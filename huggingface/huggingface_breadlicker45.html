<!doctype html>
<html class="">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
		<meta name="description" content="We’re on a journey to advance and democratize artificial intelligence through open source and open science." />
		<meta property="fb:app_id" content="1321688464574422" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@huggingface" />
		<meta name="twitter:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/breadlicker45/discord_data.png" />
		<meta property="og:title" content="breadlicker45/discord_data · Datasets at Hugging Face" />
		<meta property="og:type" content="website" />
		<meta property="og:url" content="https://huggingface.co/datasets/breadlicker45/discord_data" />
		<meta property="og:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/breadlicker45/discord_data.png" />

		<link rel="stylesheet" href="/front/build/kube-68d7aa0/style.css" />

		<link rel="preconnect" href="https://fonts.gstatic.com" />
		<link
			href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap"
			rel="stylesheet"
		/>

		<link
			rel="preload"
			href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css"
			as="style"
			onload="this.onload=null;this.rel='stylesheet'"
		/>
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		<script>const guestTheme = document.cookie.match(/theme=(\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches));</script>
<link rel="canonical" href="https://huggingface.co/datasets/breadlicker45/discord_data"> <script type="application/ld+json">{
  "@context": {
    "@language": "en",
    "@vocab": "https:\/\/schema.org\/",
    "citeAs": "cr:citeAs",
    "column": "cr:column",
    "conformsTo": "dct:conformsTo",
    "cr": "http:\/\/mlcommons.org\/croissant\/",
    "data": {
      "@id": "cr:data",
      "@type": "@json"
    },
    "dataBiases": "cr:dataBiases",
    "dataCollection": "cr:dataCollection",
    "dataType": {
      "@id": "cr:dataType",
      "@type": "@vocab"
    },
    "dct": "http:\/\/purl.org\/dc\/terms\/",
    "extract": "cr:extract",
    "field": "cr:field",
    "fileProperty": "cr:fileProperty",
    "fileObject": "cr:fileObject",
    "fileSet": "cr:fileSet",
    "format": "cr:format",
    "includes": "cr:includes",
    "isLiveDataset": "cr:isLiveDataset",
    "jsonPath": "cr:jsonPath",
    "key": "cr:key",
    "md5": "cr:md5",
    "parentField": "cr:parentField",
    "path": "cr:path",
    "personalSensitiveInformation": "cr:personalSensitiveInformation",
    "recordSet": "cr:recordSet",
    "references": "cr:references",
    "regex": "cr:regex",
    "repeated": "cr:repeated",
    "replace": "cr:replace",
    "sc": "https:\/\/schema.org\/",
    "separator": "cr:separator",
    "source": "cr:source",
    "subField": "cr:subField",
    "transform": "cr:transform"
  },
  "@type": "sc:Dataset",
  "distribution": [
    {
      "@type": "cr:FileObject",
      "@id": "repo",
      "name": "repo",
      "description": "The Hugging Face git repository.",
      "contentUrl": "https:\/\/huggingface.co\/datasets\/breadlicker45\/discord_data\/tree\/refs%2Fconvert%2Fparquet",
      "encodingFormat": "git+https",
      "sha256": "https:\/\/github.com\/mlcommons\/croissant\/issues\/80"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-default",
      "name": "parquet-files-for-config-default",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "default\/*\/*.parquet"
    }
  ],
  "recordSet": [
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "default_splits\/split_name"
      },
      "@id": "default_splits",
      "name": "default_splits",
      "description": "Splits for the default config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "default_splits\/split_name": "train"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "default",
      "name": "default",
      "description": "breadlicker45\/discord_data - 'default' subset",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default\/split",
          "name": "default\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "default\/(?:partial-)?(train)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "default_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/data",
          "name": "default\/data",
          "description": "Column 'data' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "data"
            }
          }
        }
      ]
    }
  ],
  "conformsTo": "http:\/\/mlcommons.org\/croissant\/1.0",
  "name": "discord_data",
  "description": "breadlicker45\/discord_data dataset hosted on Hugging Face and contributed by the HF Datasets community",
  "alternateName": [
    "breadlicker45\/discord_data"
  ],
  "creator": {
    "@type": "Person",
    "name": "Matthew Mitton",
    "url": "https:\/\/huggingface.co\/breadlicker45"
  },
  "keywords": [
    "100K - 1M",
    "csv",
    "Text",
    "Datasets",
    "pandas",
    "Croissant",
    "Polars",
    "🇺🇸 Region: US"
  ],
  "url": "https:\/\/huggingface.co\/datasets\/breadlicker45\/discord_data"
}</script> 

		<title>breadlicker45/discord_data · Datasets at Hugging Face</title>

		<script
			defer
			data-domain="huggingface.co"
			event-loggedIn="false"
			src="/js/script.pageview-props.js"
		></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script>
			window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer></script>
	</head>
	<body class="flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DatasetPage">
		<div class="flex min-h-dvh flex-col"><div class="SVELTE_HYDRATER contents" data-target="SystemThemeMonitor" data-props="{&quot;isLoggedIn&quot;:false}"></div>

	<div class="SVELTE_HYDRATER contents" data-target="MainHeader" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false,&quot;isPro&quot;:false}"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg">
				<span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a>
			<div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..."   spellcheck="false" type="text" value="">
	<svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
	</div>
			<div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg>
		</button>

	</div></div>
		<nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 2xl:space-x-2"><li class="hover:text-indigo-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
					Models</a>
			</li><li class="hover:text-red-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					Datasets</a>
			</li><li class="hover:text-blue-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg>
					Spaces</a>
			</li><li class="hover:text-yellow-700 max-xl:hidden"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 text-yellow-500!" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg>
					Posts</a>
			</li><li class="hover:text-yellow-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 16 16"><path d="m2.28 3.7-.3.16a.67.67 0 0 0-.34.58v8.73l.01.04.02.07.01.04.03.06.02.04.02.03.04.06.05.05.04.04.06.04.06.04.08.04.08.02h.05l.07.02h.11l.04-.01.07-.02.03-.01.07-.03.22-.12a5.33 5.33 0 0 1 5.15.1.67.67 0 0 0 .66 0 5.33 5.33 0 0 1 5.33 0 .67.67 0 0 0 1-.58V4.36a.67.67 0 0 0-.34-.5l-.3-.17v7.78a.63.63 0 0 1-.87.59 4.9 4.9 0 0 0-4.35.35l-.65.39a.29.29 0 0 1-.15.04.29.29 0 0 1-.16-.04l-.65-.4a4.9 4.9 0 0 0-4.34-.34.63.63 0 0 1-.87-.59V3.7Z" fill="currentColor" class="dark:opacity-40"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M8 3.1a5.99 5.99 0 0 0-5.3-.43.66.66 0 0 0-.42.62v8.18c0 .45.46.76.87.59a4.9 4.9 0 0 1 4.34.35l.65.39c.05.03.1.04.16.04.05 0 .1-.01.15-.04l.65-.4a4.9 4.9 0 0 1 4.35-.34.63.63 0 0 0 .86-.59V3.3a.67.67 0 0 0-.41-.62 5.99 5.99 0 0 0-5.3.43l-.3.17L8 3.1Zm.73 1.87a.43.43 0 1 0-.86 0v5.48a.43.43 0 0 0 .86 0V4.97Z" fill="currentColor" class="opacity-40 dark:opacity-100"></path><path d="M8.73 4.97a.43.43 0 1 0-.86 0v5.48a.43.43 0 1 0 .86 0V4.96Z" fill="currentColor" class="dark:opacity-40"></path></svg>
					Docs</a>
			</li><li class="hover:text-black dark:hover:text-white"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/enterprise"><svg class="mr-1.5 text-gray-400 group-hover:text-black dark:group-hover:text-white" xmlns="http://www.w3.org/2000/svg" fill="none" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 33 27"><path fill="currentColor" fill-rule="evenodd" d="M13.5.7a8.7 8.7 0 0 0-7.7 5.7L1 20.6c-1 3.1.9 5.7 4.1 5.7h15c3.3 0 6.8-2.6 7.8-5.7l4.6-14.2c1-3.1-.8-5.7-4-5.7h-15Zm1.1 5.7L9.8 20.3h9.8l1-3.1h-5.8l.8-2.5h4.8l1.1-3h-4.8l.8-2.3H23l1-3h-9.5Z" clip-rule="evenodd"></path></svg>
					Enterprise</a>
			</li>

		<li><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/pricing">Pricing
			</a></li>

		<li><div class="relative group">
	<button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button">
		<svg class=" text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>
			
		</button>
	
	
	</div></li>
		<li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li>
		<li><a class="block cursor-pointer whitespace-nowrap px-2 py-0.5 hover:text-gray-500 dark:text-gray-300 dark:hover:text-gray-100" href="/login">Log In
				</a></li>
			<li><a class="whitespace-nowrap rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up
					</a></li></ul></nav></div></header></div>
	
	
	
	<div class="SVELTE_HYDRATER contents" data-target="SSOBanner" data-props="{}"></div>
	
	

	<main class="flex flex-1 flex-col">
	<div class="SVELTE_HYDRATER contents" data-target="DatasetHeader" data-props="{&quot;activeTab&quot;:&quot;datasetCard&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;62b4b30f40e600c2272741f6&quot;,&quot;avatarUrl&quot;:&quot;/avatars/d86ca9ce3242be87bbfe541219ee2fac.svg&quot;,&quot;fullname&quot;:&quot;Matthew Mitton&quot;,&quot;name&quot;:&quot;breadlicker45&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:20},&quot;canReadRepoSettings&quot;:false,&quot;dataset&quot;:{&quot;author&quot;:&quot;breadlicker45&quot;,&quot;createdAt&quot;:&quot;2023-05-03T16:08:07.000Z&quot;,&quot;downloads&quot;:20,&quot;downloadsAllTime&quot;:478,&quot;id&quot;:&quot;breadlicker45/discord_data&quot;,&quot;isLikedByUser&quot;:false,&quot;lastModified&quot;:&quot;2023-06-27T00:37:16.000Z&quot;,&quot;likes&quot;:3,&quot;datasetsServerInfo&quot;:{&quot;viewer&quot;:&quot;viewer&quot;,&quot;numRows&quot;:452471,&quot;libraries&quot;:[&quot;datasets&quot;,&quot;pandas&quot;,&quot;mlcroissant&quot;,&quot;polars&quot;],&quot;formats&quot;:[&quot;csv&quot;],&quot;modalities&quot;:[&quot;text&quot;]},&quot;discussionsDisabled&quot;:false,&quot;repoType&quot;:&quot;dataset&quot;,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;tags&quot;:[&quot;size_categories:100K<n<1M&quot;,&quot;format:csv&quot;,&quot;modality:text&quot;,&quot;library:datasets&quot;,&quot;library:pandas&quot;,&quot;library:mlcroissant&quot;,&quot;library:polars&quot;,&quot;region:us&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;size_categories:100K<n<1M&quot;,&quot;label&quot;:&quot;100K - 1M&quot;,&quot;type&quot;:&quot;size_categories&quot;},{&quot;id&quot;:&quot;format:csv&quot;,&quot;label&quot;:&quot;csv&quot;,&quot;type&quot;:&quot;format&quot;},{&quot;id&quot;:&quot;modality:text&quot;,&quot;label&quot;:&quot;Text&quot;,&quot;type&quot;:&quot;modality&quot;},{&quot;id&quot;:&quot;library:datasets&quot;,&quot;label&quot;:&quot;Datasets&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:pandas&quot;,&quot;label&quot;:&quot;pandas&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:mlcroissant&quot;,&quot;label&quot;:&quot;Croissant&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:polars&quot;,&quot;label&quot;:&quot;Polars&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;🇺🇸 Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;}],&quot;hasBlockedOids&quot;:false,&quot;region&quot;:&quot;us&quot;,&quot;xetEnabled&quot;:false},&quot;discussionsStats&quot;:{&quot;closed&quot;:0,&quot;open&quot;:0,&quot;total&quot;:0}}"><header class="bg-linear-to-t border-b border-gray-100 pt-6 sm:pt-9 from-gray-50-to-white via-white dark:via-gray-950"><div class="container relative "><h1 class="flex flex-wrap items-center max-md:leading-tight mb-3 text-lg max-sm:gap-y-1.5 md:text-xl"><a href="/datasets" class="group flex items-center"><svg class="sm:mr-1.5 -mr-1 text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					<span class="mr-2.5 font-semibold text-gray-400 group-hover:text-gray-500 max-sm:hidden">Datasets:</span></a>
				<hr class="mx-1.5 h-2 translate-y-px rounded-sm border-r dark:border-gray-600 sm:hidden">
			<div class="group flex flex-none items-center"><div class="relative mr-1 flex items-center">

			

<span class="inline-block "><span class="contents"><a href="/breadlicker45" class="text-gray-400 hover:text-blue-600"><img alt="" class="w-3.5 h-3.5 rounded-full  flex-none" src="/avatars/d86ca9ce3242be87bbfe541219ee2fac.svg" crossorigin="anonymous"></a></span>
	</span></div>
		

<span class="inline-block "><span class="contents"><a href="/breadlicker45" class="text-gray-400 hover:text-blue-600">breadlicker45</a></span>
	</span>
		<div class="mx-0.5 text-gray-300">/</div></div>

<div class="max-w-full "><a class="break-words font-mono font-semibold hover:text-blue-600 " href="/datasets/breadlicker45/discord_data">discord_data</a>
	<button class="relative text-sm mr-4 focus:outline-hidden inline-flex cursor-pointer items-center text-sm  mx-0.5   text-gray-600 " title="Copy dataset name to clipboard" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg>
	
	</button></div>
			<div class="inline-flex items-center overflow-hidden whitespace-nowrap rounded-md border bg-white text-sm leading-none text-gray-500  mr-2"><button class="relative flex items-center overflow-hidden from-red-50 to-transparent dark:from-red-900 px-1.5 py-1 hover:bg-linear-to-t focus:outline-hidden"  title="Like"><svg class="left-1.5 absolute" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>

		
		<span class="ml-4 pl-0.5 ">like</span></button>
	<button class="focus:outline-hidden flex items-center border-l px-1.5 py-1 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="See users who liked this repository">3</button></div>




			
			
	</h1>
		<div class="mb-3 flex flex-wrap md:mb-4"><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Modalities:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?modality=modality%3Atext"><div class="tag tag-white   ">
		<svg class="text-red-700 dark:text-red-600" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.619 4.619C2.667 6.573 2.667 9.715 2.667 16s0 9.428 1.952 11.38C6.573 29.333 9.715 29.333 16 29.333s9.428 0 11.38-1.953c1.953-1.95 1.953-5.095 1.953-11.38s0-9.428-1.953-11.381C25.43 2.667 22.285 2.667 16 2.667s-9.428 0-11.381 1.952m8.65 3.714c-.573 0-1.109 0-1.546.066-.495.073-1.003.248-1.41.7-.392.436-.53.956-.59 1.452-.056.464-.056 1.04-.056 1.689V13a1 1 0 1 0 2 0v-.704c0-.724.001-1.176.041-1.505q.015-.15.061-.294a.2.2 0 0 1 .031-.061q0-.003.016-.01a.8.8 0 0 1 .203-.05c.272-.04.654-.043 1.314-.043H15v11.334h-2.333a1 1 0 1 0 0 2H20a1 1 0 0 0 0-2h-3V10.333h1.667c.66 0 1.042.003 1.314.043.123.019.18.04.203.05l.015.009a.2.2 0 0 1 .032.061c.018.05.042.14.061.295.04.329.041.781.041 1.506V13a1 1 0 1 0 2 0v-.76c0-.65 0-1.225-.056-1.69-.06-.495-.198-1.015-.59-1.453-.407-.45-.915-.625-1.41-.698-.437-.067-.973-.067-1.546-.066z" fill="currentColor"></path></svg>

	

	<span>Text</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Formats:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?format=format%3Acsv"><div class="tag tag-white   ">
		<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.508 4.486h3.98v-1h-3.98v1Zm5.004 0h3.98v-1h-3.98v1ZM5.488 6.5h-3.98v-1h3.98v1Zm1.024 0h3.98v-1h-3.98v1ZM5.488 8.514h-3.98v-1h3.98v1Zm1.024 0h3.98v-1h-3.98v1Z" fill="currentColor"></path></svg>

	

	<span>csv</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Size:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?size_categories=size_categories%3A100K%3Cn%3C1M"><div class="tag tag-white   ">

	

	<span>100K - 1M</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Libraries:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Adatasets"><div class="tag tag-white   "><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 95 88"><path fill="#fff" d="M94.25 70.08a8.28 8.28 0 0 1-.43 6.46 10.57 10.57 0 0 1-3 3.6 25.18 25.18 0 0 1-5.7 3.2 65.74 65.74 0 0 1-7.56 2.65 46.67 46.67 0 0 1-11.42 1.68c-5.42.05-10.09-1.23-13.4-4.5a40.4 40.4 0 0 1-10.14.03c-3.34 3.25-7.99 4.52-13.39 4.47a46.82 46.82 0 0 1-11.43-1.68 66.37 66.37 0 0 1-7.55-2.65c-2.28-.98-4.17-2-5.68-3.2a10.5 10.5 0 0 1-3.02-3.6c-.99-2-1.18-4.3-.42-6.46a8.54 8.54 0 0 1-.33-5.63c.25-.95.66-1.83 1.18-2.61a8.67 8.67 0 0 1 2.1-8.47 8.23 8.23 0 0 1 2.82-2.07 41.75 41.75 0 1 1 81.3-.12 8.27 8.27 0 0 1 3.11 2.19 8.7 8.7 0 0 1 2.1 8.47c.52.78.93 1.66 1.18 2.61a8.61 8.61 0 0 1-.32 5.63Z"></path><path fill="#FFD21E" d="M47.21 76.5a34.75 34.75 0 1 0 0-69.5 34.75 34.75 0 0 0 0 69.5Z"></path><path fill="#FF9D0B" d="M81.96 41.75a34.75 34.75 0 1 0-69.5 0 34.75 34.75 0 0 0 69.5 0Zm-73.5 0a38.75 38.75 0 1 1 77.5 0 38.75 38.75 0 0 1-77.5 0Z"></path><path fill="#3A3B45" d="M58.5 32.3c1.28.44 1.78 3.06 3.07 2.38a5 5 0 1 0-6.76-2.07c.61 1.15 2.55-.72 3.7-.32ZM34.95 32.3c-1.28.44-1.79 3.06-3.07 2.38a5 5 0 1 1 6.76-2.07c-.61 1.15-2.56-.72-3.7-.32Z"></path><path fill="#FF323D" d="M46.96 56.29c9.83 0 13-8.76 13-13.26 0-2.34-1.57-1.6-4.09-.36-2.33 1.15-5.46 2.74-8.9 2.74-7.19 0-13-6.88-13-2.38s3.16 13.26 13 13.26Z"></path><path fill="#3A3B45" fill-rule="evenodd" d="M39.43 54a8.7 8.7 0 0 1 5.3-4.49c.4-.12.81.57 1.24 1.28.4.68.82 1.37 1.24 1.37.45 0 .9-.68 1.33-1.35.45-.7.89-1.38 1.32-1.25a8.61 8.61 0 0 1 5 4.17c3.73-2.94 5.1-7.74 5.1-10.7 0-2.34-1.57-1.6-4.09-.36l-.14.07c-2.31 1.15-5.39 2.67-8.77 2.67s-6.45-1.52-8.77-2.67c-2.6-1.29-4.23-2.1-4.23.29 0 3.05 1.46 8.06 5.47 10.97Z" clip-rule="evenodd"></path><path fill="#FF9D0B" d="M70.71 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM24.21 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM17.52 48c-1.62 0-3.06.66-4.07 1.87a5.97 5.97 0 0 0-1.33 3.76 7.1 7.1 0 0 0-1.94-.3c-1.55 0-2.95.59-3.94 1.66a5.8 5.8 0 0 0-.8 7 5.3 5.3 0 0 0-1.79 2.82c-.24.9-.48 2.8.8 4.74a5.22 5.22 0 0 0-.37 5.02c1.02 2.32 3.57 4.14 8.52 6.1 3.07 1.22 5.89 2 5.91 2.01a44.33 44.33 0 0 0 10.93 1.6c5.86 0 10.05-1.8 12.46-5.34 3.88-5.69 3.33-10.9-1.7-15.92-2.77-2.78-4.62-6.87-5-7.77-.78-2.66-2.84-5.62-6.25-5.62a5.7 5.7 0 0 0-4.6 2.46c-1-1.26-1.98-2.25-2.86-2.82A7.4 7.4 0 0 0 17.52 48Zm0 4c.51 0 1.14.22 1.82.65 2.14 1.36 6.25 8.43 7.76 11.18.5.92 1.37 1.31 2.14 1.31 1.55 0 2.75-1.53.15-3.48-3.92-2.93-2.55-7.72-.68-8.01.08-.02.17-.02.24-.02 1.7 0 2.45 2.93 2.45 2.93s2.2 5.52 5.98 9.3c3.77 3.77 3.97 6.8 1.22 10.83-1.88 2.75-5.47 3.58-9.16 3.58-3.81 0-7.73-.9-9.92-1.46-.11-.03-13.45-3.8-11.76-7 .28-.54.75-.76 1.34-.76 2.38 0 6.7 3.54 8.57 3.54.41 0 .7-.17.83-.6.79-2.85-12.06-4.05-10.98-8.17.2-.73.71-1.02 1.44-1.02 3.14 0 10.2 5.53 11.68 5.53.11 0 .2-.03.24-.1.74-1.2.33-2.04-4.9-5.2-5.21-3.16-8.88-5.06-6.8-7.33.24-.26.58-.38 1-.38 3.17 0 10.66 6.82 10.66 6.82s2.02 2.1 3.25 2.1c.28 0 .52-.1.68-.38.86-1.46-8.06-8.22-8.56-11.01-.34-1.9.24-2.85 1.31-2.85Z"></path><path fill="#FFD21E" d="M38.6 76.69c2.75-4.04 2.55-7.07-1.22-10.84-3.78-3.77-5.98-9.3-5.98-9.3s-.82-3.2-2.69-2.9c-1.87.3-3.24 5.08.68 8.01 3.91 2.93-.78 4.92-2.29 2.17-1.5-2.75-5.62-9.82-7.76-11.18-2.13-1.35-3.63-.6-3.13 2.2.5 2.79 9.43 9.55 8.56 11-.87 1.47-3.93-1.71-3.93-1.71s-9.57-8.71-11.66-6.44c-2.08 2.27 1.59 4.17 6.8 7.33 5.23 3.16 5.64 4 4.9 5.2-.75 1.2-12.28-8.53-13.36-4.4-1.08 4.11 11.77 5.3 10.98 8.15-.8 2.85-9.06-5.38-10.74-2.18-1.7 3.21 11.65 6.98 11.76 7.01 4.3 1.12 15.25 3.49 19.08-2.12Z"></path><path fill="#FF9D0B" d="M77.4 48c1.62 0 3.07.66 4.07 1.87a5.97 5.97 0 0 1 1.33 3.76 7.1 7.1 0 0 1 1.95-.3c1.55 0 2.95.59 3.94 1.66a5.8 5.8 0 0 1 .8 7 5.3 5.3 0 0 1 1.78 2.82c.24.9.48 2.8-.8 4.74a5.22 5.22 0 0 1 .37 5.02c-1.02 2.32-3.57 4.14-8.51 6.1-3.08 1.22-5.9 2-5.92 2.01a44.33 44.33 0 0 1-10.93 1.6c-5.86 0-10.05-1.8-12.46-5.34-3.88-5.69-3.33-10.9 1.7-15.92 2.78-2.78 4.63-6.87 5.01-7.77.78-2.66 2.83-5.62 6.24-5.62a5.7 5.7 0 0 1 4.6 2.46c1-1.26 1.98-2.25 2.87-2.82A7.4 7.4 0 0 1 77.4 48Zm0 4c-.51 0-1.13.22-1.82.65-2.13 1.36-6.25 8.43-7.76 11.18a2.43 2.43 0 0 1-2.14 1.31c-1.54 0-2.75-1.53-.14-3.48 3.91-2.93 2.54-7.72.67-8.01a1.54 1.54 0 0 0-.24-.02c-1.7 0-2.45 2.93-2.45 2.93s-2.2 5.52-5.97 9.3c-3.78 3.77-3.98 6.8-1.22 10.83 1.87 2.75 5.47 3.58 9.15 3.58 3.82 0 7.73-.9 9.93-1.46.1-.03 13.45-3.8 11.76-7-.29-.54-.75-.76-1.34-.76-2.38 0-6.71 3.54-8.57 3.54-.42 0-.71-.17-.83-.6-.8-2.85 12.05-4.05 10.97-8.17-.19-.73-.7-1.02-1.44-1.02-3.14 0-10.2 5.53-11.68 5.53-.1 0-.19-.03-.23-.1-.74-1.2-.34-2.04 4.88-5.2 5.23-3.16 8.9-5.06 6.8-7.33-.23-.26-.57-.38-.98-.38-3.18 0-10.67 6.82-10.67 6.82s-2.02 2.1-3.24 2.1a.74.74 0 0 1-.68-.38c-.87-1.46 8.05-8.22 8.55-11.01.34-1.9-.24-2.85-1.31-2.85Z"></path><path fill="#FFD21E" d="M56.33 76.69c-2.75-4.04-2.56-7.07 1.22-10.84 3.77-3.77 5.97-9.3 5.97-9.3s.82-3.2 2.7-2.9c1.86.3 3.23 5.08-.68 8.01-3.92 2.93.78 4.92 2.28 2.17 1.51-2.75 5.63-9.82 7.76-11.18 2.13-1.35 3.64-.6 3.13 2.2-.5 2.79-9.42 9.55-8.55 11 .86 1.47 3.92-1.71 3.92-1.71s9.58-8.71 11.66-6.44c2.08 2.27-1.58 4.17-6.8 7.33-5.23 3.16-5.63 4-4.9 5.2.75 1.2 12.28-8.53 13.36-4.4 1.08 4.11-11.76 5.3-10.97 8.15.8 2.85 9.05-5.38 10.74-2.18 1.69 3.21-11.65 6.98-11.76 7.01-4.31 1.12-15.26 3.49-19.08-2.12Z"></path></svg>

	

	<span>Datasets</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Apandas"><div class="tag tag-white   "><svg class="text-black inline-block text-sm text-[#130754] dark:text-gray-200" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid slice" viewBox="0 0 210.21 280.43"><rect fill="currentColor" x="74.51" y="43.03" width="24.09" height="50.02"></rect><rect fill="currentColor" x="74.51" y="145.78" width="24.09" height="50.02"></rect><rect fill="#ffca00" x="74.51" y="107.65" width="24.09" height="23.6"></rect><rect fill="currentColor" x="35.81" y="84.15" width="24.09" height="166.27"></rect><rect fill="currentColor" x="112.41" y="187.05" width="24.09" height="50.02"></rect><rect fill="currentColor" x="112.41" y="84.21" width="24.09" height="50.02"></rect><rect fill="#e70488" x="112.41" y="148.84" width="24.09" height="23.6"></rect><rect fill="currentColor" x="150.3" y="30" width="24.09" height="166.27"></rect></svg>

	

	<span>pandas</span>
	

	</div></a><div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-lg rounded-br-none " type="button">
		<div class="tag tag-white   relative rounded-br-none pr-2.5"><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="none" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="#F5AB6A"></path><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="url(#paint0_radial_18_31665)"></path><g filter="url(#filter0_f_18_31665)"><path d="M22.2849 12.1817L23.4375 13.2656L24.4375 4.70312C23.5121 4.1242 23.0198 3.96369 22.6563 3.89062L22.2849 12.1817Z" fill="url(#paint1_linear_18_31665)"></path></g><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint2_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint3_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint4_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint5_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint6_linear_18_31665)"></path><g filter="url(#filter1_f_18_31665)"><path d="M13.1016 2.72656C11.862 3.06924 11.5298 3.40016 11.5298 3.40016C10.9102 3.52335 10.936 4.11525 11.2408 4.6799C13.1487 6.95202 15.1361 13.2496 18.007 14.0958C18.2707 14.1633 18.6953 14.2107 19.1797 14.2344L13.1016 2.72656Z" fill="url(#paint7_linear_18_31665)"></path></g><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint8_radial_18_31665)"></path><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint9_radial_18_31665)"></path><g filter="url(#filter2_f_18_31665)"><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint10_linear_18_31665)"></path><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint11_radial_18_31665)"></path></g><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="#EC9F6A"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint12_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint13_linear_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint14_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint15_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint16_radial_18_31665)"></path><g filter="url(#filter3_f_18_31665)"><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint17_linear_18_31665)"></path><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint18_linear_18_31665)"></path></g><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="#D79453"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint19_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint20_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint21_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint22_linear_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint23_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint24_radial_18_31665)"></path><defs><filter id="filter0_f_18_31665" x="22.0349" y="3.64062" width="2.65265" height="9.875" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter1_f_18_31665" x="10.7815" y="2.47656" width="8.64819" height="12.0078" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter2_f_18_31665" x="3.89062" y="22.5416" width="9.57812" height="2.2804" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter3_f_18_31665" x="2.45312" y="11.2538" width="12.1255" height="8.50903" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><radialGradient id="paint0_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.8125 12.9375) rotate(42.7741) scale(12.5164 7.08839)"><stop offset="0.0937591" stop-color="#C05159"></stop><stop offset="0.553697" stop-color="#F6AC6A"></stop><stop offset="0.832916" stop-color="#FFD186"></stop><stop offset="0.916927" stop-color="#FFDC87"></stop></radialGradient><linearGradient id="paint1_linear_18_31665" x1="24.7344" y1="4.67187" x2="20.8594" y2="12.8906" gradientUnits="userSpaceOnUse"><stop stop-color="#EBD67C"></stop><stop offset="0.0655686" stop-color="#FFFFA6"></stop><stop offset="0.530552" stop-color="#F8C281"></stop><stop offset="0.937338" stop-color="#E99E6B"></stop></linearGradient><radialGradient id="paint2_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.9009 13.1847) rotate(-127.648) scale(14.3438 11.7966)"><stop stop-color="#FFBE66"></stop><stop offset="1" stop-color="#E2AE5B"></stop></radialGradient><radialGradient id="paint3_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(18 11.4375) rotate(53.9726) scale(11.9013 4.84018)"><stop stop-color="#D67C63"></stop><stop offset="1" stop-color="#D97D67" stop-opacity="0"></stop></radialGradient><radialGradient id="paint4_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(23 4.1875) rotate(45.7639) scale(3.31486 5.75622)"><stop stop-color="#FFE4A6"></stop><stop offset="0.711285" stop-color="#F8B76F"></stop><stop offset="1" stop-color="#F9B870" stop-opacity="0"></stop></radialGradient><radialGradient id="paint5_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.875 12.4375) rotate(88.9391) scale(3.37558 1.29066)"><stop stop-color="#FFBC67"></stop><stop offset="1" stop-color="#FFBC67" stop-opacity="0"></stop></radialGradient><linearGradient id="paint6_linear_18_31665" x1="20.375" y1="15.6875" x2="20.125" y2="12.7813" gradientUnits="userSpaceOnUse"><stop offset="0.461609" stop-color="#B45077"></stop><stop offset="0.855389" stop-color="#B75077" stop-opacity="0"></stop></linearGradient><linearGradient id="paint7_linear_18_31665" x1="12.9375" y1="2.57056" x2="18.5625" y2="14.3891" gradientUnits="userSpaceOnUse"><stop stop-color="#DDC173"></stop><stop offset="0.485173" stop-color="#D59F65"></stop><stop offset="1" stop-color="#E49966"></stop></linearGradient><radialGradient id="paint8_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.5625 23.5) rotate(109.113) scale(6.68078 10.2578)"><stop offset="0.165756" stop-color="#FFBF7E"></stop><stop offset="0.827674" stop-color="#DF8C6D"></stop><stop offset="1" stop-color="#B05A66"></stop></radialGradient><radialGradient id="paint9_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.1875 26) rotate(41.0652) scale(8.37243 2.03649)"><stop stop-color="#FFD483"></stop><stop offset="1" stop-color="#FFD688" stop-opacity="0"></stop></radialGradient><linearGradient id="paint10_linear_18_31665" x1="3.96063" y1="23.794" x2="13.3748" y2="23.5143" gradientUnits="userSpaceOnUse"><stop stop-color="#A8716F"></stop><stop offset="0.103615" stop-color="#B37173"></stop><stop offset="0.225484" stop-color="#DB9F84"></stop><stop offset="0.799889" stop-color="#F1BB8A"></stop><stop offset="1" stop-color="#FFD780"></stop></linearGradient><radialGradient id="paint11_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4219 23.1719) rotate(-178.616) scale(3.23532 0.569081)"><stop offset="0.621498" stop-color="#AF5A3E"></stop><stop offset="1" stop-color="#B35445" stop-opacity="0"></stop></radialGradient><radialGradient id="paint12_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.625 19.125) rotate(-171.737) scale(15.2205 15.0767)"><stop offset="0.138435" stop-color="#FFB974"></stop><stop offset="0.403618" stop-color="#F2A56D"></stop><stop offset="0.925938" stop-color="#A16948"></stop></radialGradient><linearGradient id="paint13_linear_18_31665" x1="8.22184" y1="13.125" x2="6.81191" y2="15.4996" gradientUnits="userSpaceOnUse"><stop offset="0.610751" stop-color="#984847"></stop><stop offset="0.850075" stop-color="#9A4947" stop-opacity="0"></stop></linearGradient><radialGradient id="paint14_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.25 23.7461) scale(11.25 5.68361)"><stop stop-color="#C66364"></stop><stop offset="1" stop-color="#D4766B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint15_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.875 23.5313) scale(10.0937 1.29657)"><stop stop-color="#B64B4B"></stop><stop offset="1" stop-color="#C56158" stop-opacity="0"></stop></radialGradient><radialGradient id="paint16_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4375 19.875) rotate(-46.8882) scale(4.02385 7.51767)"><stop stop-color="#FFC083"></stop><stop offset="0.620218" stop-color="#FFBD7D" stop-opacity="0"></stop></radialGradient><linearGradient id="paint17_linear_18_31665" x1="2.8125" y1="13.0312" x2="14.5582" y2="18.9404" gradientUnits="userSpaceOnUse"><stop stop-color="#B89367"></stop><stop offset="1" stop-color="#C5835E"></stop></linearGradient><linearGradient id="paint18_linear_18_31665" x1="8.21875" y1="14.6406" x2="7.59349" y2="15.6717" gradientUnits="userSpaceOnUse"><stop offset="0.351552" stop-color="#A74746"></stop><stop offset="0.845198" stop-color="#A04346" stop-opacity="0"></stop></linearGradient><radialGradient id="paint19_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.5625 14.5625) rotate(140.244) scale(18.3733 13.7403)"><stop stop-color="#FDAE69"></stop><stop offset="0.729021" stop-color="#CE8C4F"></stop><stop offset="0.921546" stop-color="#AD7B45"></stop><stop offset="1" stop-color="#8B6B4A"></stop></radialGradient><radialGradient id="paint20_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.0625 7) rotate(65.3152) scale(11.0745 3.16547)"><stop offset="0.233237" stop-color="#FFD47C"></stop><stop offset="0.853648" stop-color="#FFD98B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint21_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.3125 8.875) rotate(100.886) scale(6.6191 5.57808)"><stop offset="0.128419" stop-color="#FFD88C"></stop><stop offset="0.924134" stop-color="#FFBE7B" stop-opacity="0"></stop></radialGradient><linearGradient id="paint22_linear_18_31665" x1="7.25" y1="15.1875" x2="10.7588" y2="10.3142" gradientUnits="userSpaceOnUse"><stop offset="0.142353" stop-color="#C15F4D"></stop><stop offset="1" stop-color="#D58366" stop-opacity="0"></stop></linearGradient><radialGradient id="paint23_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(8.15625 15.7813) rotate(28.5422) scale(12.5574 1.96589)"><stop offset="0.149989" stop-color="#E4745D"></stop><stop offset="0.453292" stop-color="#C8604C"></stop><stop offset="0.632597" stop-color="#C0605F"></stop><stop offset="1" stop-color="#C0605F" stop-opacity="0"></stop></radialGradient><radialGradient id="paint24_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(1.40625 2.69067) rotate(46.0943) scale(22.3963)"><stop offset="0.935802" stop-color="#C17C61" stop-opacity="0"></stop><stop offset="0.982109" stop-color="#C17C61"></stop></radialGradient></defs></svg>

	

	<span>Croissant</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	<button class="tag tag-ghost px-1! -ml-0.5 mb-1 md:mb-1.5" type="button">+ 1</button></div></div>

		<div class="flex flex-col-reverse lg:flex-row lg:items-center lg:justify-between"><div class="-mb-px flex h-12 items-center overflow-x-auto overflow-y-hidden ">
	<a class="tab-alternate active" href="/datasets/breadlicker45/discord_data"><svg class="mr-1.5 text-gray-400 flex-none" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
	Dataset card
	

	
		</a><a class="tab-alternate" href="/datasets/breadlicker45/discord_data/viewer/"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	Data Studio
	

	
		</a><a class="tab-alternate" href="/datasets/breadlicker45/discord_data/tree/main"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M21 19h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-8h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0 4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M9 19a1 1 0 0 1-1-1V6a1 1 0 0 1 2 0v12a1 1 0 0 1-1 1zm-6-4.333a1 1 0 0 1-.64-1.769L3.438 12l-1.078-.898a1 1 0 0 1 1.28-1.538l2 1.667a1 1 0 0 1 0 1.538l-2 1.667a.999.999 0 0 1-.64.231z" fill="currentColor"></path></svg>
	<span class="xl:hidden">Files</span>
		<span class="hidden xl:inline">Files and versions</span>
	

	
		</a><a class="tab-alternate" href="/datasets/breadlicker45/discord_data/discussions"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M20.6081 3C21.7684 3 22.8053 3.49196 23.5284 4.38415C23.9756 4.93678 24.4428 5.82749 24.4808 7.16133C24.9674 7.01707 25.4353 6.93643 25.8725 6.93643C26.9833 6.93643 27.9865 7.37587 28.696 8.17411C29.6075 9.19872 30.0124 10.4579 29.8361 11.7177C29.7523 12.3177 29.5581 12.8555 29.2678 13.3534C29.8798 13.8646 30.3306 14.5763 30.5485 15.4322C30.719 16.1032 30.8939 17.5006 29.9808 18.9403C30.0389 19.0342 30.0934 19.1319 30.1442 19.2318C30.6932 20.3074 30.7283 21.5229 30.2439 22.6548C29.5093 24.3704 27.6841 25.7219 24.1397 27.1727C21.9347 28.0753 19.9174 28.6523 19.8994 28.6575C16.9842 29.4379 14.3477 29.8345 12.0653 29.8345C7.87017 29.8345 4.8668 28.508 3.13831 25.8921C0.356375 21.6797 0.754104 17.8269 4.35369 14.1131C6.34591 12.058 7.67023 9.02782 7.94613 8.36275C8.50224 6.39343 9.97271 4.20438 12.4172 4.20438H12.4179C12.6236 4.20438 12.8314 4.2214 13.0364 4.25468C14.107 4.42854 15.0428 5.06476 15.7115 6.02205C16.4331 5.09583 17.134 4.359 17.7682 3.94323C18.7242 3.31737 19.6794 3 20.6081 3ZM20.6081 5.95917C20.2427 5.95917 19.7963 6.1197 19.3039 6.44225C17.7754 7.44319 14.8258 12.6772 13.7458 14.7131C13.3839 15.3952 12.7655 15.6837 12.2086 15.6837C11.1036 15.6837 10.2408 14.5497 12.1076 13.1085C14.9146 10.9402 13.9299 7.39584 12.5898 7.1776C12.5311 7.16799 12.4731 7.16355 12.4172 7.16355C11.1989 7.16355 10.6615 9.33114 10.6615 9.33114C10.6615 9.33114 9.0863 13.4148 6.38031 16.206C3.67434 18.998 3.5346 21.2388 5.50675 24.2246C6.85185 26.2606 9.42666 26.8753 12.0653 26.8753C14.8021 26.8753 17.6077 26.2139 19.1799 25.793C19.2574 25.7723 28.8193 22.984 27.6081 20.6107C27.4046 20.212 27.0693 20.0522 26.6471 20.0522C24.9416 20.0522 21.8393 22.6726 20.5057 22.6726C20.2076 22.6726 19.9976 22.5416 19.9116 22.222C19.3433 20.1173 28.552 19.2325 27.7758 16.1839C27.639 15.6445 27.2677 15.4256 26.746 15.4263C24.4923 15.4263 19.4358 19.5181 18.3759 19.5181C18.2949 19.5181 18.2368 19.4937 18.2053 19.4419C17.6743 18.557 17.9653 17.9394 21.7082 15.6009C25.4511 13.2617 28.0783 11.8545 26.5841 10.1752C26.4121 9.98141 26.1684 9.8956 25.8725 9.8956C23.6001 9.89634 18.2311 14.9403 18.2311 14.9403C18.2311 14.9403 16.7821 16.496 15.9057 16.496C15.7043 16.496 15.533 16.4139 15.4169 16.2112C14.7956 15.1296 21.1879 10.1286 21.5484 8.06535C21.7928 6.66715 21.3771 5.95917 20.6081 5.95917Z" fill="#FF9D00"></path><path d="M5.50686 24.2246C3.53472 21.2387 3.67446 18.9979 6.38043 16.206C9.08641 13.4147 10.6615 9.33111 10.6615 9.33111C10.6615 9.33111 11.2499 6.95933 12.59 7.17757C13.93 7.39581 14.9139 10.9401 12.1069 13.1084C9.29997 15.276 12.6659 16.7489 13.7459 14.713C14.8258 12.6772 17.7747 7.44316 19.304 6.44221C20.8326 5.44128 21.9089 6.00204 21.5484 8.06532C21.188 10.1286 14.795 15.1295 15.4171 16.2118C16.0391 17.2934 18.2312 14.9402 18.2312 14.9402C18.2312 14.9402 25.0907 8.49588 26.5842 10.1752C28.0776 11.8545 25.4512 13.2616 21.7082 15.6008C17.9646 17.9393 17.6744 18.557 18.2054 19.4418C18.7372 20.3266 26.9998 13.1351 27.7759 16.1838C28.5513 19.2324 19.3434 20.1173 19.9117 22.2219C20.48 24.3274 26.3979 18.2382 27.6082 20.6107C28.8193 22.9839 19.2574 25.7722 19.18 25.7929C16.0914 26.62 8.24723 28.3726 5.50686 24.2246Z" fill="#FFD21E"></path></svg>
	Community
	

	
		</a></div>
	
			</div></div></header>
</div>
	
<div class="container relative flex flex-col md:grid md:space-y-0 w-full md:grid-cols-12 md:flex-1 md:grid-rows-full space-y-4 md:gap-6 ">
		<section class="pt-6 border-gray-100 md:col-span-8 pb-24 relative break-words copiable-code-container">
				<div class="SVELTE_HYDRATER contents" data-target="UnsafeBanner" data-props="{&quot;classNames&quot;:&quot;mb-4&quot;,&quot;repoId&quot;:&quot;breadlicker45/discord_data&quot;,&quot;repoType&quot;:&quot;dataset&quot;,&quot;minLevel&quot;:&quot;unsafe&quot;}"></div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetViewer" data-props="{&quot;data&quot;:{&quot;kind&quot;:&quot;DatasetAndSampleData&quot;,&quot;datasetInfo&quot;:[{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Size of downloaded dataset files:&quot;,&quot;value&quot;:&quot;428 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;/datasets/breadlicker45/discord_data/tree/refs%2Fconvert%2Fparquet/&quot;,&quot;label&quot;:&quot;Size of the auto-converted Parquet files:&quot;,&quot;value&quot;:&quot;220 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Number of rows:&quot;,&quot;value&quot;:&quot;452,471&quot;}],&quot;partial&quot;:false,&quot;configsData&quot;:{&quot;configInfos&quot;:[{&quot;name&quot;:&quot;default&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:452471}],&quot;selectedConfig&quot;:&quot;default&quot;,&quot;hasSelectedConfigParquet&quot;:true},&quot;splitsData&quot;:{&quot;splitInfos&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;numRows&quot;:452471}],&quot;selectedSplit&quot;:&quot;train&quot;},&quot;sampleData&quot;:{&quot;dataset&quot;:&quot;breadlicker45/discord_data&quot;,&quot;config&quot;:&quot;default&quot;,&quot;split&quot;:&quot;train&quot;,&quot;capabilities&quot;:{&quot;rows&quot;:true,&quot;search&quot;:true,&quot;filter&quot;:true,&quot;statistics&quot;:true},&quot;navigation&quot;:{&quot;p&quot;:0},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzA5OSwic3ViIjoiL2RhdGFzZXRzL2JyZWFkbGlja2VyNDUvZGlzY29yZF9kYXRhIiwiZXhwIjoxNzQyOTI2Njk5LCJpc3MiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvIn0.sVs-_MzgSg5ty30M4pL-zlZGgRsuMHZCpaUzVgLcyF68UuQS48RiqvpGsrQmT3RMmDGSOcaEaJ5n3ZAekBxrCA&quot;,&quot;sampleData&quot;:{&quot;columns&quot;:[{&quot;name&quot;:&quot;data&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;data&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:14,&quot;max&quot;:24284,&quot;mean&quot;:929.24136,&quot;median&quot;:834,&quot;std&quot;:523.82049,&quot;histogram&quot;:{&quot;hist&quot;:[445054,6414,834,123,25,7,5,4,4,1],&quot;bin_edges&quot;:[14,2442,4870,7298,9726,12154,14582,17010,19438,21866,24284]}}}}],&quot;rows&quot;:[{&quot;rowIdx&quot;:0,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: You could have just redirected the welcome to a new channel yk\nJL#1976: Let's keep all the discussions in this general chat, so #🙏︱welcome channel serves as a landing page and rules page if we get more users.\nZestyLemonade#1012: **Welcome to #💬︱general**\nThis is the start of the #💬︱general channel.\nJL#1976: You learn something new every day 🙂\n『ｋｏｍｏｒｅｂｉ』#3903: where's the language list for riffusion\n『ｋｏｍｏｒｅｂｉ』#3903: i want to know what genres it supports\n『ｋｏｍｏｒｅｂｉ』#3903: found it nvm\ndb0798#7460: Where is it then?\n『ｋｏｍｏｒｅｂｉ』#3903: https://huggingface.co/riffusion/riffusion-model-v1/blob/main/tokenizer/vocab.json\n『ｋｏｍｏｒｅｂｉ』#3903: here\ndb0798#7460: Thanks!\n『ｋｏｍｏｒｅｂｉ』#3903: it has a lot of non-musical words though\ndoesn't exactly help\n『ｋｏｍｏｒｅｂｉ』#3903: not too sure how it's supposed to work&quot;}}},{&quot;rowIdx&quot;:1,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;db0798#7460: This is identical with the vocab.json of the Stable Diffusion 1.5 base model https://huggingface.co/runwayml/stable-diffusion-v1-5/raw/main/tokenizer/vocab.json\ndb0798#7460: I think the genre names probably come from somewhere else\ndep#0002: @seth (sorry if too many pings) I am trying to make my own audio seed. Could the spectogram_from_waveform (from https://github.com/hmartiro/riffusion-inference/blob/6c99dba1c81b2126a2042712ab0c35d0668bd83c/riffusion/audio.py#L89) be used to transform a WAV tensor's (I'm guessing from torchaudio.load) into a spectogram object, and then do the reverse of spectrogram_from_image to basically have a custom seed?\ndep#0002: I also see the following comment:\n```\n\&quot;\&quot;\&quot;\nCompute a spectrogram magnitude array from a spectrogram image.\nTODO(hayk): Add image_from_spectrogram and call this out as the reverse.\n\&quot;\&quot;\&quot;\n```\nI could try doing it\nalfredw#2036: Can we make it 10x better soon?\ndep#0002: I was looking into converting it to TensorRT with Volta but it seems it has 1 more layer\n『ｋｏｍｏｒｅｂｉ』#3903: could we send some songs of different genres so that the ai can generate a wider array of genres?\ni have a decent amount of obscure genres in my playlist&quot;}}},{&quot;rowIdx&quot;:2,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;alfredw#2036: what's the training set?\n『ｋｏｍｏｒｅｂｉ』#3903: vaporwave, chopped and screwed, free folk, experimental rock, art pop, etc?\n『ｋｏｍｏｒｅｂｉ』#3903: don't think the ai knows those genres too well\ndep#0002: gptchat:\n```python\ndef image_from_spectrogram(spectrogram: np.ndarray, max_volume: float = 50, power_for_image: float = 0.25) -> Image.Image:\n\&quot;\&quot;\&quot;\nCompute a spectrogram image from a spectrogram magnitude array.\n\&quot;\&quot;\&quot;\n# Reverse the power curve\ndata = np.power(spectrogram, power_for_image)\n\n# Rescale to the range 0-255\ndata = data * 255 / max_volume\n&quot;}}},{&quot;rowIdx&quot;:3,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# Invert\ndata = 255 - data\n\n# Flip Y and add a single channel\ndata = data[::-1, :, None]\n\n# Convert to an image\nreturn Image.fromarray(data.astype(np.uint8))\n```\ndep#0002: anyways I will see what I can do\ndep#0002: this is basically audio2audio\n『ｋｏｍｏｒｅｂｉ』#3903: could we help increase the dataset\ndep#0002: If they release the training code I will train it on the entirety of pandemic sound\ndb0798#7460: I would also like to have some way of adding things to the dataset\n『ｋｏｍｏｒｅｂｉ』#3903: i'd train it on my playlist + more albums that i somewhat like&quot;}}},{&quot;rowIdx&quot;:4,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Slynk#7009: omg I've been dying for something audio related to happen with all this AI hype.\n『ｋｏｍｏｒｅｂｉ』#3903: so that way the ai can endlessly churn out music i like >:D\nmy playlist would probably be too smoll for it though\nThistle Cat#9883: Hi what's happened?\nPaulinux#8579: What I'm doing wrongly? I have URL like this:\nhttps://www.riffusion.com/?%20&amp;prompt=folk&amp;%20denoising=0.05&amp;%20seedImageId=agile\nAnd sometimes this AI couldn't produce for me anything\ndb0798#7460: Try the Colab (https://colab.research.google.com/drive/1FhH3HlN8Ps_Pr9OR6Qcfbfz7utDvICl0?usp=sharing) instead, perhaps? Colab seems to work consistently\nPaulinux#8579: OK, thanks\n『ｋｏｍｏｒｅｂｉ』#3903: ooh wait i know what i'd do\ni gather all the alternative/avant-garde genres i can muster, select some albums from those genres (with the genre names + other descriptions with them) and then i'd put those in the ai\ndep#0002: overloaded\ndep#0002: If anyone wants I can host a mirror\nhayk#0058: We have this code and it's very simple, we just haven't added it to the inference repo\ndep#0002: Could it be possible for you to send it here or push it to the repo?&quot;}}},{&quot;rowIdx&quot;:5,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;hayk#0058: Yeah if you open an issue on github we will aim to get to it soon!\nAmbientArtstyles#1406: Hey @ZestyLemonade, I'm writing an article about sound design (sfx for games/movies) and Riffusion, can I use your sentience.wav clip in it?\nAmbientArtstyles#1406: I so want to collaborate on training the algorithm with my personal sound libraries. 🎚️   🎶\nThistle Cat#9883: Has the website been fixed?\nJL#1976: Works for me.\nThistle Cat#9883: Nice!\nThistle Cat#9883: I will have to check it again tonight\nTekh#3634: How do I make a continuous stream of interconnected clips with the colab?\nTekh#3634: also, is there any way to change tempo and such like on the webapp using the colab?\ndep#0002: thanks pls do so asap I cant wait\ndep#0002: in the meantime im getting things like these\ndep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053101779992186941/FINAL.png\nThistle Cat#9883: Anyone hearing a snapping noise when it goes to the next spectrogram?\nyokento#6970: Can you seed riffusion with your own clip of audio?\nApril#5244: 14gb ckpt?&quot;}}},{&quot;rowIdx&quot;:6,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: thats what I was trying\nJack Julian#8888: Crazy stuff yall\nim a musician myself, and seeing this is both interesting and 'worrying'. Love how you thought this out and put it to works.\nApril#5244: was hoping to gen using automatic1111's sd webui and perhaps finetune the model using dreambooth. but I feel like I'm in a bit over my head lol\nWereSloth#0312: you'll need to convert music to a spectrogram\nWereSloth#0312: and then, yes, you should be able to dreambooth\ndb0798#7460: Is there a script for converting your own audio sample to the right kind of spectrogram already available anywhere? There seem to be functions that do this in the Riffusion codebase but I guess they don't work as a standalone script?\ndep#0002: the image_from_spectogram function is missing\ndep#0002: https://github.com/hmartiro/riffusion-inference/issues/9\ndep#0002: supposedly they have it but they haven't added it yet\ndep#0002: Maybe tomorrow it could be ready\nlxe#0001: 👋\nlxe#0001: Just wanted to stop by and say how awesome this thing is\nlxe#0001: Wonder if something like deforum for it is in the works.\njustinethanmathews#7521: this is very interesting. i am mostly in the \&quot;afraid of AI\&quot; camp, but music is a field I understand and I can see how interesting this is.&quot;}}},{&quot;rowIdx&quot;:7,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\nthis might be a stupid question. but what was this trained on?\nApril#5244: I'm also curious about the dataset tbh\nApril#5244: also managed a small success: converting from wav file to spectrogram and back is working perfectly, and I have a working ckpt that can generate the spectrogram images. Next is to make a finetuning dataset and run it through dreambooth 🙂\ndep#0002: Can you share your convertor?\nApril#5244: https://pastebin.com/raw/0ALzwee4\nApril#5244: just a word of warning @dep I have no idea what I'm doing and this code was generated with the help of an ai and my own tinkering. might have some serious stuff wrong with it lol\nApril#5244: I've only tested it on the generated 5-second wav files that are created from the sister script\nApril#5244: also trying to re-input the generated pics doesn't work right so I have to manually save in paint and then it works for some reason lol\nApril#5244: but from my testing it seems to work well enough\nApril#5244: currently seeing if I can get it to work from mp3 and clip like the first 5 seconds or something\ndep#0002: time to train on the internet\ndep#0002: 🥂\nApril#5244: okay so I think I got it working with mp3 so I threw a whole dang song in there and it generated an image but it's much wider in resolution, and cropping it down just results in junk lol\nApril#5244: might have to limit it to 5 seconds&quot;}}},{&quot;rowIdx&quot;:8,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;April#5244: ```\ndef spectrogram_image_from_mp3(mp3_bytes: io.BytesIO, max_volume: float = 50, power_for_image: float = 0.25) -> Image.Image:\n\&quot;\&quot;\&quot;\nGenerate a spectrogram image from an MP3 file.\n\&quot;\&quot;\&quot;\n# Load MP3 file into AudioSegment object\naudio = pydub.AudioSegment.from_mp3(mp3_bytes)\n\n# Convert to mono and set frame rate\naudio = audio.set_channels(1)\naudio = audio.set_frame_rate(44100)\n\n# Extract first 5 seconds of audio data\naudio = audio[:5000]\n&quot;}}},{&quot;rowIdx&quot;:9,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# Convert to WAV and save as BytesIO object\nwav_bytes = io.BytesIO()\naudio.export(wav_bytes, format=\&quot;wav\&quot;)\nwav_bytes.seek(0)\n\n# Generate spectrogram image from WAV file\nreturn spectrogram_image_from_wav(wav_bytes, max_volume=max_volume, power_for_image=power_for_image)\n```\n```\n# Open MP3 file\nwith open('music.mp3', 'rb') as f:\nmp3_bytes = io.BytesIO(f.read())\n\n# Generate spectrogram image\nimage = spectrogram_image_from_mp3(mp3_bytes)&quot;}}},{&quot;rowIdx&quot;:10,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\n# Save image to file\nimage.save('restoredinput.png')\n```\nApril#5244: add this function and those lines at the bottom for mp3 and cutting first 5 seconds, seems to work great\na_robot_kicker#7014: any idea what could be happening here?\n```\nERROR:server:Exception on /run_inference/ [POST]\nTraceback (most recent call last):\nFile \&quot;C:\\Users\\matth\\miniconda3\\envs\\ldm\\lib\\site-packages\\flask\\app.py\&quot;, line 2525, in wsgi_app\nresponse = self.full_dispatch_request()\nFile \&quot;C:\\Users\\matth\\miniconda3\\envs\\ldm\\lib\\site-packages\\flask\\app.py\&quot;, line 1822, in full_dispatch_request\nrv = self.handle_user_exception(e)\nFile \&quot;C:\\Users\\matth\\miniconda3\\envs\\ldm\\lib\\site-packages\\flask_cors\\extension.py\&quot;, line 165, in wrapped_function\nreturn cors_after_request(app.make_response(f(*args, **kwargs)))&quot;}}},{&quot;rowIdx&quot;:11,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;File \&quot;C:\\Users\\matth\\miniconda3\\envs\\ldm\\lib\\site-packages\\flask\\app.py\&quot;, line 1820, in full_dispatch_request\nrv = self.dispatch_request()\nFile \&quot;C:\\Users\\matth\\miniconda3\\envs\\ldm\\lib\\site-packages\\flask\\app.py\&quot;, line 1796, in dispatch_request\nreturn self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\nFile \&quot;C:\\Users\\matth\\Documents\\StableDiffusion\\riffusion-inference\\riffusion\\server.py\&quot;, line 147, in run_inference\nresponse = compute(inputs)\nFile \&quot;C:\\Users\\matth\\Documents\\StableDiffusion\\riffusion-inference\\riffusion\\server.py\&quot;, line 177, in compute\nimage = MODEL.riffuse(inputs, init_image=init_image, mask_image=mask_image)\n... (lots of lines here)\nattn_output = torch.bmm(attn_probs, value_states)\nRuntimeError: expected scalar type Half but found Float\n```\na_robot_kicker#7014: ```\nFile \&quot;C:\\Users\\matth\\Documents\\StableDiffusion\\riffusion-inference\\riffusion\\prompt_weighting.py\&quot;, line 229, in get_unweighted_text_embeddings\ntext_embeddings = pipe.text_encoder(text_input)[0]&quot;}}},{&quot;rowIdx&quot;:12,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;```\nApril#5244: this is the img2wav script I'm using https://cdn.discordapp.com/attachments/1053081177772261386/1053166079804981268/audio.py\nApril#5244: I'm actually not using any other code lol\nApril#5244: so idk why/how to fix any issues with the riffusion ui stuff\ndep#0002: What error did you got\ndep#0002: I didnt got any errors but it took longer because it was larger than 512by512\ndep#0002: after resizing it it worked as normal\ndep#0002: however I can barely hear the see\ndep#0002: I had to set denoising to 0.01 to actually remember the tempo\ndep#0002: Anyways\ndep#0002: I have an A100 so I will see if I can finetune it\nApril#5244: I actually fixed the error by editing the img2wav script lol\nApril#5244: one of the things had an extra parameter for whatever reason which was messing it up\nApril#5244:  https://cdn.discordapp.com/attachments/1053081177772261386/1053175675709837412/output.wav,https://cdn.discordapp.com/attachments/1053081177772261386/1053175675986649158/clip.wav,https://cdn.discordapp.com/attachments/1053081177772261386/1053175676334788689/outputspectro.png\nApril#5244: example conversion&quot;}}},{&quot;rowIdx&quot;:13,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;April#5244: \&quot;clip.wav\&quot; is the 5 second clip from the original mp3 that's used for conversion. the image is the converted spectrum from the mp3. and output.wav is the reconverted song from the image\nApril#5244: scripts used https://cdn.discordapp.com/attachments/1053081177772261386/1053176008141963364/audio2spectro.py,https://cdn.discordapp.com/attachments/1053081177772261386/1053176008590770236/audio.py\nApril#5244: notably the script doesn't clip the first 5 seconds, but rather the next 5 after that\nApril#5244: since I wanted to avoid the sometimes slow intro that songs have lol\nApril#5244: still need to do some more work on the scripts before I can have it auto-generate some dataset images properly @.@\nApril#5244: might actually need to fetch later in the songs lol 🤔\nApril#5244: I noticed it still distorts the sound a bit...\nApril#5244: comparing: clip is the cropped audio, output.wav is the audio->img->audio convert https://cdn.discordapp.com/attachments/1053081177772261386/1053177485497475113/output.wav,https://cdn.discordapp.com/attachments/1053081177772261386/1053177485862387712/clip.wav\ndep#0002: WoohH!!!!!\ndep#0002: I dont know how to thank you\ndep#0002: and its just been a day Lol\ndep#0002: @JL may I suggest some emojis\nApril#5244: I'm sure someone smarter than me can figure out how to fix it lol\nJL#1976: Yes, let me know if you have any cool ideas for the channel\ndep#0002: Can I dm you the stickers and emojis&quot;}}},{&quot;rowIdx&quot;:14,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: I prob will also make it into a bot\ndep#0002: (riffusion)\ndep#0002: although I've also heard that another dev is also working on one\ndep#0002: u know lopho from sail?\ndep#0002: the decentralized training server\ndep#0002: I might ask him tomorrow\ndep#0002: he knows a lot about this stuff\ndep#0002: he rewrote the bucketing code himself lol\ndep#0002: yet haru didnt merged it and he deleted it\nJL#1976: Yup\ndep#0002: hm.... I dont think we should mess with the channels....\ndep#0002: but its worth the attempt\nApril#5244: 🤷‍♀️ honestly most of this code is ai generated. I don't know anything about music lol. removing that line of code seems to break the conversion entirely\nApril#5244: looking into the actual conversion a bit more it seems like the sample rate is getting changed which may be why the quality is decreasing 🤔\ndep#0002: = ( https://cdn.discordapp.com/attachments/1053081177772261386/1053183348715032576/recompiled.mp3&quot;}}},{&quot;rowIdx&quot;:15,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: og https://cdn.discordapp.com/attachments/1053081177772261386/1053183641699753984/invadercrop.wav\ndep#0002: fixed https://cdn.discordapp.com/attachments/1053081177772261386/1053184852154916894/recompiled.mp3\nApril#5244: got it pretty close https://cdn.discordapp.com/attachments/1053081177772261386/1053189308154122391/reconstructed.wav,https://cdn.discordapp.com/attachments/1053081177772261386/1053189308531613707/clip.wav\nApril#5244: there's some clipping though 🤔\nApril#5244: basically just change max volume to 80 on both scripts to get this result\ndep#0002: @April My image is on 512x501, is there anyway to fix that?\ndep#0002: or just resize in paint\ndep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053191450768187392/agile.png\nApril#5244: > audio = audio[:5119]\nApril#5244: when you're doing converting\nApril#5244: the size of the image is based on the length of the audio\nApril#5244: clipping it to 5119 seems to work\nApril#5244: I'm currently using this to get the middle of the song:\n> audio = audio[int(len(audio)/2):int(len(audio)/2)+5119]\ndep#0002: kay I will investigae more tomorrow&quot;}}},{&quot;rowIdx&quot;:16,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: maybe the first finetune\nApril#5244: I wonder if there's a way to just have the whole song in the image 🤔\nApril#5244: I guess it'd have to be a larger image...\nvai#0872: any way to fine tune this model?\nMilano#2460: hi All! are you aware of https://www.isik.dev/posts/Technoset.html ?\nMilano#2460: Technoset is a data-set of 90,933 electronic music loops, totalling around 50 hours. Each loop has a length of 1.827-seconds and is at 128bpm. The loops are from 10,000 separate electronic music tracks.\nMilano#2460: I'm wondering how best to preprocess it.\ndb0798#7460: I think this one is pretty cool actually, sounds like Autechre\njacobresch#3699: here's an extension for auto1111 which automatically converts the images to audio again 🙂\nhttps://github.com/enlyth/sd-webui-riffusion\nJL#1976: Pinned a message.\nHD#1311: this is exactly the kind of plugin I was looking for\nHD#1311: thanks\nHD#1311: I'll report if it works when I get home from work\nApril#5244: Worked for me but it messed up my sd install and python because it tried to install pytorch audio which I already had&quot;}}},{&quot;rowIdx&quot;:17,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;JeniaJitsev#1332: Great work, folks, very impressive! I am scientific lead and co-founder of LAION, datasets of which are used to train original image based stable diffusion. Very nice to see such a cool twist for getting spectrogram based training running. We would be very much interested to cooperate on that and scale it further up - just join our LAION discord : https://discord.gg/we4DaujH\nJL#1976: Welcome, happy to see you here!\nHD#1311: just tested it out and it works\nHD#1311: fun stuff\npnuts#1013: 👋\ndep#0002: So, me an lopho have been experimenting with converting audio to spectogram images\ndep#0002: @April we got better results by increasing the n_mels but it would prob not be compatible with the current model\ndep#0002: 512 (original) https://cdn.discordapp.com/attachments/1053081177772261386/1053353723319038012/invadercrop_nmels_512.png,https://cdn.discordapp.com/attachments/1053081177772261386/1053353723826552852/invader_nmels_512.wav\ndep#0002: 768 https://cdn.discordapp.com/attachments/1053081177772261386/1053353776838365224/invadercrop_nmels_768.png,https://cdn.discordapp.com/attachments/1053081177772261386/1053353777136148602/invader_nmels_768.wav\ndep#0002: 1024 https://cdn.discordapp.com/attachments/1053081177772261386/1053353811399409724/invadercrop_nmels_1024.png,https://cdn.discordapp.com/attachments/1053081177772261386/1053353811714002996/invader_nmels_1024.wav\ndep#0002: original (nothing) https://cdn.discordapp.com/attachments/1053081177772261386/1053353879825305650/invadercrop.wav\ndep#0002: you will probably need good headphones to hear the difference\ndep#0002: but you can hear one of the beats more clearly compared to 512\nJL#1976: https://www.futurepedia.io/tool/riffusion Riffusion added to futerepedia.io\ndep#0002: I'll be updating these tools here:&quot;}}},{&quot;rowIdx&quot;:18,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/chavinlo/riffusion-manipulation\nJL#1976: Let's create a post and get that pinned.\nJL#1976: **Official website:**\nhttps://riffusion.com/\n\n**Technical explanation:**\nhttps://www.riffusion.com/about\n\n**Riffusion App Github:**\nhttps://github.com/hmartiro/riffusion-app\n\n**Riffusion Inference Server Github:\n**https://github.com/hmartiro/riffusion-inference/\n\n**Developers:**&quot;}}},{&quot;rowIdx&quot;:19,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;@seth\n@hayk\n\n**HackerNews thread:**\nhttps://news.ycombinator.com/item?id=33999162\n\n**Subreddit:**\nhttps://reddit.com/r/riffusion\n\n**Riffusion manipulation tools from @dep :**\nhttps://github.com/chavinlo/riffusion-manipulation\n\n**Riffusion extension for AUTOMATIC1111 Web UI**:\nhttps://github.com/enlyth/sd-webui-riffusion\n&quot;}}},{&quot;rowIdx&quot;:20,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;**Notebook:**\nhttps://colab.research.google.com/gist/mdc202002/411d8077c3c5bd34d7c9bf244a1c240e/riffusion_music2music.ipynb\n**\nHuggingface Riffusion demo:**\nhttps://huggingface.co/spaces/anzorq/riffusion-demo\n\n(pm me if any new resources have to be added or there are any errors in current listings)\nJL#1976: https://techcrunch.com/2022/12/15/try-riffusion-an-ai-model-that-composes-music-by-visualizing-it/\nWow, TechCrunch article!\nEclipstic#9066: hi\nJL#1976: Hey\nEclipstic#9066: whats up\nEclipstic#9066: im messing around with riffusion now\nEclipstic#9066: it mostly doesnt follow my prompts\nEclipstic#9066: but sometimes it surprises me&quot;}}},{&quot;rowIdx&quot;:21,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;JL#1976: Feel free to share the best stuff you get in #🤘︱share-riffs\nEclipstic#9066: i can only say this:\n\n\nthis is cool as hell\nHD#1311: what's the song\ndep#0002: https://www.youtube.com/watch?v=jezqbMVqcLk\ndep#0002: I am messing with the script rn so I can train a whole model on him\ndep#0002: another sample\ndep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053381711276294234/planet_girl_rebuild_default.wav\ndep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053381799297949777/planet_girl.png\njoao_betelgeuse#0410: yoooooooo\nJL#1976: Hey\nTivra#3760: Good evening\ndep#0002: @seth have you considered using the 3 channels rather than just 1?&quot;}}},{&quot;rowIdx&quot;:22,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: I've been discusing about it with another dev, and it could help to \&quot;channels for 24bit amp\&quot;\ndep#0002: power 0.75 https://cdn.discordapp.com/attachments/1053081177772261386/1053421209599082496/planet_girl_rebuild.wav\ndep#0002: power 0.1 (earrape) https://cdn.discordapp.com/attachments/1053081177772261386/1053421333830180914/power01.wav\ndep#0002: power 0.4 https://cdn.discordapp.com/attachments/1053081177772261386/1053421463572586566/0.4.wav\ndep#0002: power 0.3 https://cdn.discordapp.com/attachments/1053081177772261386/1053421536628977794/0.3.wav\ndep#0002: 0.25 is the default and the best.\nSuperSonicDiscord1#4751: Do any devs here know how to do a latent space walk from one seed image to another?\nSuperSonicDiscord1#4751: `InferneceInput` implies that you can only have one seed image if you're useing the inference server.\nWereSloth#0312: y'all have made it to the big time.  Softology has added Riffusion to Visions of Chaos.  :slothrofl:\nApril#5244: the problem is the resulting image *must* be 512x512 to work properly with stable diffusion. obviously different sized spectrograms would be ideal but that really isn't possible\n『ｋｏｍｏｒｅｂｉ』#3903: question: how do you train/finetune the ai\ni want to train it on ambient/drone music\nso that it can churn out stuff i like\nApril#5244: just stick the model in dreambooth as usual. then feed it your spectrogam images in the same format\nhayk#0058: one of my favorite reddit comments https://cdn.discordapp.com/attachments/1053081177772261386/1053461992024850462/reddit_comment.png&quot;}}},{&quot;rowIdx&quot;:23,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;『ｋｏｍｏｒｅｂｉ』#3903: oh ok\nhow to convert images to spectrogram\nalso i have to import 5 second stuff right? nothing longer?\nIgnizHerz#2097: https://github.com/chavinlo/riffusion-manipulation\nApril#5244: I posted code that does this earlier. though it seems a link to something better was just posted?\nApril#5244: and yes, has to be 5 second clips\n『ｋｏｍｏｒｅｂｉ』#3903: ah ok\n『ｋｏｍｏｒｅｂｉ』#3903: another dumb question\nhow do i put it in dreambooth\nwhere do i even get dreambooth\nIgnizHerz#2097: also @dep, it would be awesome if you set it where you can generate the images for the entire song automatically rather than manually getting every 5 seconds. I did this very poorly locally myself, but I'm sure people would appreciate it.\ndep#0002: I will do this soon\ndep#0002: The next version should grab the audio files, split it in chunks of 5 seconds, make a spectogram image for each chunk, and done\ndep#0002: rn I am downloading some artists to train on\ndep#0002: also, @hayk have you looked into the possibility of using an outpainting model, rather than img2img?&quot;}}},{&quot;rowIdx&quot;:24,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: I can help you train one if needed\ndep#0002: he just logged off .-.\nIgnizHerz#2097: sweet, I did this with my very rusty python code. Only thing is dealing with getting to the end having like 2seconds or something leftover, guess you'll have to add empty noise.\ndep#0002: yeah empty noise should do\nApril#5244: https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast-DreamBooth.ipynb\nthis is the colab I use for dreambooth\n『ｋｏｍｏｒｅｂｉ』#3903: ok\nSheppy#4289: if someone sets up a eli5 local setup guide, DM me please, i feel dumb, but i don't know what i'm doing\ndep#0002: eli5?\nManly P Hall#3191: acronym for **e**xplain **l**ike **i**'m **5**\ndep#0002: a\nSheppy#4289: yeah, my b\ndep#0002: well its not that hard\ndep#0002: just follow the instructions lel\ndep#0002: install the latest version of nvm, then install npm and node, then execute the inference server first, and then the webui&quot;}}},{&quot;rowIdx&quot;:25,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: if you need help I can join the vc and explain you in depth\noliveoil2222#2222: and thats all in command line right\ndep#0002: yes\ndep#0002: (unfortunately)\noliveoil2222#2222: yup\nhayk#0058: I haven't tried it beyond a bit in the automatic1111 ui. Sometimes the longer results got repetitive, but there's a lot to play with there!\noliveoil2222#2222: ah i was lost on what node version manager was now we're in business\ndep#0002: yeah, the nextjs version that the webui uses is only supported by node 16+\ndep#0002: Heres a script to extract the tags of a song based off the filename from last.fm https://cdn.discordapp.com/attachments/1053081177772261386/1053518335624626226/message.txt\ndep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053520950714454067/image.png\ndep#0002: @April btw what did you used on auto webui\nApril#5244: I just converted the model file to a ckpt (the included one was 14gb for some reason so I didn't bother with it) and just used the converted ckpt in webui and it worked fine 🤷‍♀️\ndep#0002: what prompts\nApril#5244: ? for what exactly?\ndep#0002: like&quot;}}},{&quot;rowIdx&quot;:26,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: to generate\ndep#0002: the images\ndep#0002: in the webui\ndep#0002: @April\nApril#5244: whatever you want it to generate?\ndep#0002: You used Automatic's webUI to generate spectograms, right?\nApril#5244: yeah\nIgnizHerz#2097: usually the prompt depends on what you intend to get the result as, jazz for jazz and such\nApril#5244: ^\nIgnizHerz#2097: the model is unusually good at jazz\ndep#0002: simple as that?\nApril#5244: yeah\ndep#0002: like if I want something electro, just type the same as the riffusion webui?\ndep#0002: \&quot;Electro Pop\&quot;\nApril#5244: yes. just make sure you're using the riffusion model&quot;}}},{&quot;rowIdx&quot;:27,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: no \&quot;Spectogram of Electro Pop\&quot; then\nIgnizHerz#2097: no written rules, can run the same seed with different prompts to see what does best\nApril#5244: no. riffusion generates spectrograms by default\nIgnizHerz#2097: but uh it generates without having to say it yeah\nIgnizHerz#2097: I imagine this fancy interpolation stuff would also help me effectively audio2audio style transfer\nIgnizHerz#2097: If I do each sections under the same seed they still have a bit of disconnect that makes them jumpy (im just using the auto webui)\nnoop_noob#0479: Does anybody know what training data was used for riffusion?\ndep#0002: @April btw do you think your dreambooth finetune worked?\nApril#5244: it was decent I guess? Not really that great lol. The background music kinda just became a mess, though it seemed to get her vocals okay\nApril#5244: might need more data\nmatteo101man#6162: how would we go about generating things longer than 5 seconds?\nApril#5244: or perhaps just more proper labeling with terms or something 🤔\nApril#5244: the actual riffusion inference software should be able to do it which I am literally just starting to look into. can't seem to get it running :\\\nApril#5244: basically what you need to do is generate a couple of clips and then generate the interpolation between them to \&quot;extend\&quot; the music\nApril#5244: auto1111 won't be able to do it I think&quot;}}},{&quot;rowIdx&quot;:28,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;matteo101man#6162: Ah I see, thanks\nmatteo101man#6162: Shoot let me know if you figure it out\nmatteo101man#6162: they do have interpolating prompt scripts and seed travel but I wouldn't know if that would work or if you could somehow frankenstein that with the webui riffusion thing\nApril#5244:  https://cdn.discordapp.com/attachments/1053081177772261386/1053542594996617216/image.png\nmatteo101man#6162: I'm pretty new to all of it and I know zero python so 🤷‍♂️\nApril#5244: basically we need to generate interpolation pics between two seeds, convert all the clips into audio, and stick all the audio together\nApril#5244: auto1111 definitely can't do the convert to audio (it can, with an extension, but it's very limited). and afaik there's no way to do interpolation in there either?\nApril#5244: is there a script for that?\nApril#5244: I might try to do it manually\nApril#5244: the riffusion software is throwing errors :\\\ndep#0002: theres a func for it on the inference server\nmatteo101man#6162: yeah they have two things for what you're talking about\nmatteo101man#6162: https://github.com/EugeoSynthesisThirtyTwo/prompt-interpolation-script-for-sd-webui\nhttps://github.com/yownas/seed_travel\nApril#5244: interesting, I'll have to try it&quot;}}},{&quot;rowIdx&quot;:29,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: soon https://cdn.discordapp.com/attachments/1053081177772261386/1053554055764512878/image.png\ndep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053554170227064893/Snail_s_House____waiting_for_you_in_snowing_city._chunk_2.png\nApril#5244: testing seed travel https://cdn.discordapp.com/attachments/1053081177772261386/1053554728098865152/output.mp3\nApril#5244: this was with 5 steps\nApril#5244: ```\nimg = Image.open(\&quot;test/00000.png\&quot;)\nwav0 = wav_bytes_from_spectrogram_image(img)\nsound0 = pydub.AudioSegment.from_wav(wav0[0])\nimg = Image.open(\&quot;test/00001.png\&quot;)\nwav1 = wav_bytes_from_spectrogram_image(img)\nsound1 = pydub.AudioSegment.from_wav(wav1[0])\nimg = Image.open(\&quot;test/00002.png\&quot;)\nwav2 = wav_bytes_from_spectrogram_image(img)\nsound2 = pydub.AudioSegment.from_wav(wav2[0])\nimg = Image.open(\&quot;test/00003.png\&quot;)&quot;}}},{&quot;rowIdx&quot;:30,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;wav3 = wav_bytes_from_spectrogram_image(img)\nsound3 = pydub.AudioSegment.from_wav(wav3[0])\nimg = Image.open(\&quot;test/00004.png\&quot;)\nwav4 = wav_bytes_from_spectrogram_image(img)\nsound4 = pydub.AudioSegment.from_wav(wav4[0])\n\nsound = sound0+sound1+sound2+sound3+sound4\nmp3_bytes = io.BytesIO()\nsound.export(mp3_bytes, format=\&quot;mp3\&quot;)\nmp3_bytes.seek(0)\nwith open(\&quot;test/output.mp3\&quot;, \&quot;wb\&quot;) as outfile:\noutfile.write(mp3_bytes.getbuffer())\n```\nlazy code lmao\nmatteo101man#6162: kinda sounds like a scratched up vinyl that keeps skipping or a messed up cd lol but thats progress&quot;}}},{&quot;rowIdx&quot;:31,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;April#5244: the issue is there's no smoothing between clips :\\\nApril#5244: I imagine a higher amount of interpolating steps would smooth it out a bit, but it'd also make the song longer...\nmatteo101man#6162: what gpu do you have\nApril#5244: my gpu is bad lol. 1660ti\nmatteo101man#6162: dang\nApril#5244: I can do like one gen every 10-15 seconds\nmatteo101man#6162: i have a 3090\nmatteo101man#6162: i could try at more steps for you cause I'm curious about this as well but\nmatteo101man#6162: how did you go about the process\nApril#5244: threw the riffusion/dreamboothed model into webui and used the scripts you linked earlier to generate the interpolating/seed travel steps. that generates the various spectrogram images. after that I just used the code I just posted to convert them all into audio and append them together\nmatteo101man#6162: that actually makes sense\nmatteo101man#6162: let me make a bomb ass prompt first\nApril#5244: here's with the interpolation script between hatsuki yura and jazz lol https://cdn.discordapp.com/attachments/1053081177772261386/1053557262876151859/music.wav\nApril#5244: notable cuts\nmatteo101man#6162: can't tell which one is better lol&quot;}}},{&quot;rowIdx&quot;:32,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;April#5244: warped sound is just due to low txt2img smapling steps I think\nApril#5244: I'm running 20 steps each since it's fast though it kinda gets the best results with like 70+\nApril#5244: also this interpolation script is nice since it stitches the spectrogram images together automatically so I can just throw that through the regular spectro->audio converter without bothering to stitch stuff together or mess with multiple files\nmatteo101man#6162: ah, so which one is better\nmatteo101man#6162: seed travel or prompt interpolation?\nApril#5244: they do different things lol\nApril#5244: seed travel you only get one prompt and it just generates between seeds\nApril#5244: prompt interpolation you can give it two prompts and it'll generate the in-between\nmatteo101man#6162: well just found out my seed travel script is broken anyway\nApril#5244: rip\nApril#5244: you can do the seed travel thing with the prompt interpolation one anyway I think by just putting the same prompt twice\nApril#5244: at least, I imagine that's how it works?\nmatteo101man#6162: guess ill find out\nmatteo101man#6162: generation is actually taking pretty long though at 20 steps with prompt interpolation\nmatteo101man#6162: just 4 images but taking much longer than usual&quot;}}},{&quot;rowIdx&quot;:33,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;April#5244: tbh I don't know what settings riffusion uses by default 🤷‍♀️\nmatteo101man#6162: nah it's just interesting never really messed with the scripts i was talking about, just kinda downloaded them and read about their function\nmatteo101man#6162: ah isee why\nmatteo101man#6162: how do i execute this code within a certain directory\nApril#5244: ?\nmatteo101man#6162: your code\nmatteo101man#6162: i'm not very familiar with python\nApril#5244: this is to be used with the earlier spectro2audio code posted lol\nApril#5244: lemme get the whole script\nmatteo101man#6162: i only know LUA which is useless practically\nApril#5244:  https://cdn.discordapp.com/attachments/1053081177772261386/1053559417355903016/audio.py\nApril#5244: this is the script i'm using\nApril#5244: note the commented mess at the bottom\nApril#5244: ```\n# image = spectrogram_image_from_mp3(mp3_bytes)&quot;}}},{&quot;rowIdx&quot;:34,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;parser = argparse.ArgumentParser()\nparser.add_argument(\&quot;filename\&quot;, help=\&quot;the file to process\&quot;)\nargs = parser.parse_args()\n\n# The filename is stored in the `filename` attribute of the `args` object\nfilename = args.filename\nimg = Image.open(filename)\nwav = wav_bytes_from_spectrogram_image(img)\nwrite_bytesio_to_file(\&quot;music.wav\&quot;, wav[0])\n```\nThis is the default that i have it as which lets you just run the script specifying which file to convert. comment this out and use the other stuff below it to grab multiple files and stitch together\nApril#5244: it's a mess since it's just for my personal use lol T_T\nmatteo101man#6162: right so essentially like\nmatteo101man#6162: paste the long script above\nmatteo101man#6162: and uncomment your stuff and paste it below&quot;}}},{&quot;rowIdx&quot;:35,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;April#5244: here I fixed the commenting https://cdn.discordapp.com/attachments/1053081177772261386/1053559902334881823/audio.py\nmatteo101man#6162: uh, how do i define specific file names like if i have a folder of things named say \&quot;image (1)\&quot;\nApril#5244: just run this and have your files in the \&quot;test\&quot; folder next to it\nApril#5244: `Image.open(\&quot;test/00004.png\&quot;)`\nspecifies which file to open\nmatteo101man#6162: cool\nApril#5244: it's just hardcoded to use test/0000#.png files\nApril#5244: 0-4\nApril#5244: to make it more adaptable it'd require a bit of a rewrite lol\nApril#5244: as you can see it's just loading up each file, converting it to wav file data, converting that into pydub audio, and concatting them together, then writing as mp3\nmatteo101man#6162: right\nmatteo101man#6162: perhaps i'm a bit of a dummy\nApril#5244: outpainting test https://cdn.discordapp.com/attachments/1053081177772261386/1053562076926316554/music.wav\nApril#5244: everything past the 5s mark is outpainted\nmatteo101man#6162: Am I supposed to run it as a py file?&quot;}}},{&quot;rowIdx&quot;:36,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;April#5244: yeah the script I posted is a python script 🙂\nmatteo101man#6162: right\nmatteo101man#6162: so say\nmatteo101man#6162: i took that code verbatim\nmatteo101man#6162: and put it into notepad++ and saved it as a .py and ran it in a folder with images from 00000 to 00019 and a test folder in that same folder\nApril#5244: it's hardcoded to grab test/00000.png through test/00004.png and stitch them together. any higher won't work without modifying the script lol.\ndep#0002: https://github.com/chavinlo/riffusion-manipulation/blob/master/scraper/mass_a2i.py\nApril#5244: should be fairly simple to write a loop to loop through such files though\ndep#0002: this converts a whole audio file into multiple imgs\ndep#0002: its meant for a dataset but should be easy to edit\nmatteo101man#6162: yeah that's not the issue for me, took those extra ones out but just straight up running it causes nothing to happen; do you use a compiler or anything specific?\nmatteo101man#6162: I see I have idle and I got an unexpected indent, so perhaps that's an issue\nApril#5244: ? should just run it with python: `python audio.py`\nApril#5244: yeah if it's saying unexpected indent, the indentation somewhere is messed up\nApril#5244: since python is kinda strict about that&quot;}}},{&quot;rowIdx&quot;:37,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;April#5244: make sure it's either all spaces or all tabs and not a mix lol\nmatteo101man#6162: yeah fixed tht\nmatteo101man#6162: that*\nmatteo101man#6162: now no module named numpy\nmatteo101man#6162: ill look into that\nApril#5244: pip install numpy 🙂\nmatteo101man#6162: yep\nmatteo101man#6162: and for PIL\nmatteo101man#6162: ?\nApril#5244: I think for pil it's pip install pillow\nmatteo101man#6162: holy cow\nmatteo101man#6162: I'm missing so many things lol\nApril#5244: ```\nimport io\nimport typing as T&quot;}}},{&quot;rowIdx&quot;:38,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\nimport numpy as np\nfrom PIL import Image\nimport pydub\nfrom scipy.io import wavfile\nimport torch\nimport torchaudio\nimport argparse\n```\nApril#5244: are the imports for the file\nmatteo101man#6162: halfway there\nApril#5244: torch, argparse, scipy, pydub, pillow, numpy\nApril#5244: I think io and typing are python defaults?\nmatteo101man#6162: installed a bunch of other things\nmatteo101man#6162: now i've got torch not compiled with cuda enabled which appears to mean ughh, something&quot;}}},{&quot;rowIdx&quot;:39,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;matteo101man#6162: quite a lot of things\nApril#5244: this is why I do python stuff manually. I had torch installed already thanks to the whole auto webui stuff\nApril#5244: then again I actually code in python 🤷‍♀️\nmatteo101man#6162: yeah i'm definitely missing something\nApril#5244: https://pytorch.org/get-started/locally/\nApril#5244: pytorch has a weird install process to get it working with cuda\nApril#5244: I imagine this is why most people use auto installers for stuff lmao\nmatteo101man#6162: i'll let you know how it goes with that 😓\noliveoil2222#2222: man im so stubborn im just using the ckpt in a111 and throwing the spectrogram into img2audio.py 💀\nmatteo101man#6162: never thought installing something would be so hard\nApril#5244: this is pretty much what I've been doing though if you're just doing txt2img for the 5 second clips, keep in mind there's a webui script that can run the img2audio script automatically\noliveoil2222#2222: yeah i found it\ndb0798#7460: Earlier today I tried to install the same things too and got stuck in the same place, haven't resolved it\noliveoil2222#2222: anticipating easier stuff around the corner\nmatteo101man#6162: i installed a bunch of random stuff&quot;}}},{&quot;rowIdx&quot;:40,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;matteo101man#6162: did not help lol\noliveoil2222#2222: would love to see if dall-e 2 style clip variations could happen but I guess spectrogram2spectrogram is the closest one can get for now\nmatteo101man#6162: i get frozen solve when trying to install cuda on anaconda then it gets stuck on solving environment\nmatteo101man#6162: im gonna come back to it in a few minutes ig\nmatteo101man#6162: Right\nmatteo101man#6162: just ran a perfect install of cuda\nmatteo101man#6162: same torch not compiled\nmatteo101man#6162: finally got it to work\nmatteo101man#6162: cost 20 gb though\ndb0798#7460: What did you have to do to get it to work?\nmatteo101man#6162: https://stackoverflow.com/questions/57238344/i-have-a-gpu-and-cuda-installed-in-windows-10-but-pytorchs-torch-cuda-is-availa\nmatteo101man#6162: do pip uninstall torch in cmd\nmatteo101man#6162: then\nmatteo101man#6162:  https://cdn.discordapp.com/attachments/1053081177772261386/1053601238438133760/image.png\nmatteo101man#6162: on this site choose those and take that command&quot;}}},{&quot;rowIdx&quot;:41,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;db0798#7460: Thanks, I'll try this later\nmatteo101man#6162: @April after taking all the time just to get that darn thing working i see what you mean\nmatteo101man#6162: it doesnt really interpolate all that well\nmatteo101man#6162: also increasing to max steps doesn't really make it much better the skipping is still obvious\nApril#5244: Out painting seems to work for extending it tbh.\nmatteo101man#6162: what do you use to ~~outpaint~~? better yet how exactly are you implementing that\nLun-Sei#5355: ?\nmatteo101man#6162: like how are you converting the larger image into a music\nmatteo101man#6162: cause i just edited code and I made some frightening af stuff for 5:17AM\nIDDQD#9118: Hello, is that colab demo similar to the demo at riffusion.com in a way that it's possible to edit settings as the riffusion goes on?\nIDDQD#9118: yeah, no it's not\nIDDQD#9118: there are no colabs etc, that would allow similar kinda flow as the demo at riffusiondotcom? Can't run locally 😦\nOnusai#6441: ~~not a colab but you can run it locally <https://github.com/hmartiro/riffusion-app>~~ mb misread\nIDDQD#9118: Would be cool, but I quess no luck with a laptop gpu ;DD\nOnusai#6441: oof yea i wouldnt count on it&quot;}}},{&quot;rowIdx&quot;:42,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;IDDQD#9118: Yeah, thanks anyhow !\nIDDQD#9118: can't wait for this to get optimized n stuff. Would like to ought that there's alot that can be done on that front\nJay#0152: https://colab.research.google.com/gist/mdc202002/411d8077c3c5bd34d7c9bf244a1c240e/riffusion_music2music.ipynb\nJay#0152: finally, enjoy everyone!\nJay#0152: i know i will 🙂\nJay#0152: i fixed bugs in original notebook, credit to original author\nJay#0152: (also is *very* configurable)\nEgorV4X#6102: This is probably stupid, but what if you do a fine tune on speech / vocals and use label data as a prompt?\nJay#0152: Could someone plz pin this?\nEgorV4X#6102: and also if you train on instruments with midi/musicxml in prompt?\nEgorV4X#6102: I wonder if it will understand the meaning of this\nEgorV4X#6102: xd https://cdn.discordapp.com/attachments/1053081177772261386/1053659363937620058/image.png\nJL#1976: Pinned\ndep#0002: the author of that colab needs to use commit specific raw gets to avoid these kind of things\ncvtecvts#9059: Any musicians found any uses for Riffusion yet? So far, I've been able to chop some samples out of the generated clips, and some clips are ok inspiration for parts of songs.&quot;}}},{&quot;rowIdx&quot;:43,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: I'm not a musician but I do tinker a lot with the models and tech\n\nAt this point you can do txt2img to attempt to generate some new seeds\n\nYou can also use img2img to generate other variants of the same \&quot;beat\&quot;(?) or tempo\ndep#0002: I posted some on #🤘︱share-riffs\ndep#0002: Using songs that you like, convert them to spectrogram and use them as a seed is way more pleasant imo\ndep#0002: I've also tried fine-tuning it but so far no good changes, might try training an outpainting one soon, since the current method is just img2img and the chunks are distinguishable\nNikuson#6709: https://github.com/chavinlo/riffusion-manipulation I also can't figure out how to use it via pycharm\nNikuson#6709: Perhaps a banal question, but I'm not very good at pycharm either\nhead_robotics_AI#0742: Hello; when I tried to load/switch to a riffussion model in Automatic1111 web ui I got a loadin error ending in \&quot;AttributeError: 'NoneType' object has no attribute 'sd_model_checkpoint'\&quot;\nAny suggestions for how I might explore a soution?\nhead_robotics_AI#0742: is a .yaml file needed for the models?\ndb0798#7460: Is the model that you are loading to Automatic1111 the 14 Gb file? There are other versions with smaller files that work for me, at least: https://www.reddit.com/r/riffusion/comments/znbo75/how_to_load_model_to_automatic1111/\ndep#0002: what error u getting&quot;}}},{&quot;rowIdx&quot;:44,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: never used pycharm btw\nNikuson#6709: I get absolutely nothing.  just the terminal outputs “python” in response and nothing happens\ndep#0002: have you tried running it directly on bash or cli\ndep#0002: because on every instance I have ran it I never got that error\nNikuson#6709: just nothing happens, I don't think it's a bug.\ndep#0002: It doesn't prints something like \&quot;Original audio length\&quot;?\nNikuson#6709: how did you launch it?  I'm just afraid that if I work in the terminal of the operating system, I won't be able to integrate it into the diffusion stable in any way.\ndep#0002: python3 file2img.py -i inputaudio.wav -o outputimg.png\ndep#0002: its just python functions, even if the script doesn't works, you should be able to copy the functions into your own script\ndep#0002: and then call them\nNikuson#6709: nothing at all, just the standard \&quot;python\&quot; response to a command like \&quot;python3....\&quot;\nNikuson#6709: yes i do it.  I also tried to convert Lady Gaga's song and it didn't work either 😆\ndep#0002: Download this\ndep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053788225128374292/message.txt\ndep#0002: save it as file2audio.py&quot;}}},{&quot;rowIdx&quot;:45,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: and try running it\ndep#0002: tell me what you get\nNikuson#6709: D:\\Python\\StableVoice\\venv\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\nwarn(\&quot;Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\&quot;, RuntimeWarning)\nD:\\Python\\StableVoice\\venv\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\nwarnings.warn(\&quot;No audio backend is available.\&quot;)\nTraceback (most recent call last):\nFile \&quot;D:/Python/StableVoice/img2audio.py\&quot;, line 152, in <module>\nimage = spectrogram_image_from_file(filename)\nFile \&quot;D:/Python/StableVoice/img2audio.py\&quot;, line 122, in spectrogram_image_from_file\naudio = pydub.AudioSegment.from_file(filename)\nFile \&quot;D:\\Python\\StableVoice\\venv\\lib\\site-packages\\pydub\\audio_segment.py\&quot;, line 723, in from_file\nstdin_data = file.read()\nAttributeError: 'NoneType' object has no attribute 'read'\nThe script is executing&quot;}}},{&quot;rowIdx&quot;:46,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;About to call spectogram_image_from_file function\nLoading Audio File\n\nProcess finished with exit code 1\ndep#0002: you dont have ffmpeg installed\ndep#0002: thats why\ndep#0002: in what OS are you windows or linux\nNikuson#6709: Win\ndep#0002: https://www.wikihow.com/Install-FFmpeg-on-Windows\ndep#0002: a bit hard\nryan_helsing#7769: Hey @dep would you be willing to walk me through how to fine-tune on a custom data set?\ndep#0002: sure but the results aren't that good... yet\ndep#0002: wait\ndep#0002: rn I cant explain step by step but I can give you a brief summary of the process\nryan_helsing#7769: That would be wonderful&quot;}}},{&quot;rowIdx&quot;:47,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Nikuson#6709: nothing has changed after installation\nNikuson#6709: although it looks like windows doesn't see ffmpeg. Strange, I did everything according to the instructions\nNikuson#6709: I fully installed it and the operating system even began to see ffmpeg, but the error is the same\ndep#0002: At this point I suggest you install WSL and run it from there\ndb0798#7460: The script takes input and output paths as command line arguments, are you providing it these arguments?\nNikuson#6709: It seems to me that this is an overly complicated setup process for this task. 🤕 I think everyone would be happy to see something like a colab laptop for this project.\nApril#5244: auto1111 webui has an outpainting script. I load up the model and use the script to outpaint like normal.\nApril#5244: I manually converted the model into a ckpt using separate tools, so it put it down to 4gb like normal. I saw someone upload already converted models at a 2gb and 4gb size but idk how well those work. I avoided the 14gb ckpt since I figured there's no way I'd be able to run it\nNikuson#6709: I do everything like on github\nmatteo101man#6162: @April how did you convert the larger outpainted image to an audio file\nIgnizHerz#2097: what settings for this? when I tried I got nice garbage\nApril#5244: same script works fine 🤷‍♀️\nApril#5244: mostly default settings. denoise was set to like 0.75 I think? maybe higher? maskblur I set to like 20-40 or something to try and get it smoother. make sure you only extend to the right. I left fall off exponent and color variation as the defaults\nApril#5244: I posted an outpainting sample here\nApril#5244: since webui can only outpaint 256 pixels at a time, that means the outpainting only adds 2.56 seconds at once, so that's where your cuts are&quot;}}},{&quot;rowIdx&quot;:48,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;IgnizHerz#2097: got an example of what your spectrogram looked like?\nApril#5244: example outpainting spectro + converted song https://cdn.discordapp.com/attachments/1053081177772261386/1053814603445960824/music.wav,https://cdn.discordapp.com/attachments/1053081177772261386/1053814603890561054/test.png\nApril#5244: this one I was deliberately going for a different prompt at the end\nApril#5244: you can see that for the first half or so it's smooth and fine, and only once I switch the prompt it differs\nApril#5244: jumping between prompts doesn't seem to work that well. too hard of an edge I think\nIgnizHerz#2097: and this I presume is from a generated one to begin with?\nIgnizHerz#2097: mine might not being doing well because I'm attempting an extend from a converted audio to graph\nIgnizHerz#2097: you can tell pretty quickly where it swaps https://cdn.discordapp.com/attachments/1053081177772261386/1053815712780668991/tmpjlpy8xou.png\nApril#5244: yeah, so this is just extending a txt2img song using the same model and prompt that generated it. so the result of the outpainting should be near identical which leads to a useful continuation\nApril#5244: trying to extend an existing song will lead to issues, because there's no guarantee the model *can* generate something similar\nApril#5244: I'd try adjusting your denoising value, increase mask blur\nApril#5244: maybe do a clip interrogation to try and get the tokens that most closely match the song\nIgnizHerz#2097: true its probably struggling to get what my song is\nNikuson#6709: I still can't understand how you started it\nIgnizHerz#2097: anything remotely brass band esq tends to be a lot older fashioned than the song I'm using&quot;}}},{&quot;rowIdx&quot;:49,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Nikuson#6709: 😢\nIgnizHerz#2097: like uh with the built-in clip interrogator?\nApril#5244: yeah\nApril#5244: ? how I started it?\nIgnizHerz#2097: `a black and white photo of a square area with a pattern on it` not sure if it'd help to be fair\nIgnizHerz#2097: something tells me every graph looks this way\nApril#5244: 🤷‍♀️\nIgnizHerz#2097: also this from earlier, are you implying this script uses interpolation?\nApril#5244: no. that specific code just stitches together various spectrograms into a single audio clip\nApril#5244: the interpolation was done using a webui script\nIgnizHerz#2097: I havent touched the actual install, I had enough install issues that I gave up\nIgnizHerz#2097: I was thinking of hastily cramming code into the auto extension temp, or just wait I guess\nIgnizHerz#2097: if the interpolation is even worth doing that, If it doesn't really mend that well, I could always just crossfade in an audio editor\nApril#5244: I posted seed travel and interpolation tests a bit earlier\nApril#5244: I found seed travel worked better in that it was less skipping. the interpolation test I tried skipped hard but I think that might've been due to not enough steps and a large difference between prompts&quot;}}},{&quot;rowIdx&quot;:50,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;IgnizHerz#2097: I'd just like to keep the effective \&quot;style transfer\&quot; halfway consistent\nIgnizHerz#2097: on the stuff I've been doing\nIgnizHerz#2097: between two chunks of 5 seconds one will be quiet and then the next starts much louder\nIgnizHerz#2097: harder to fix than popping or silence\nmatteo101man#6162: Ah maybe I’m using the wrong outpainting or something\nmatteo101man#6162: Guess ill check when I get home\nmatteo101man#6162: Does outpainting like extend it to the right there or what? Mine just makes a 768 image like an upscale\nIgnizHerz#2097: in autos you can specify a direction and how much\nApril#5244: in the outpainting mk 2 script for auto1111 webui you can specify the direction to outpaint\nNikuson#6709: for some reason I just can't use https://github.com/chavinlo/riffusion-manipulation and I'm wondering how others installed it\nApril#5244: @Nikuson I'm using my own code I posted here\ndb0798#7460: That's kind of the problem that OpenAI Jukebox has too. OpenAI Jukebox can extend songs and the extended bits aren't completely unrelated to the original but they are still so random that the output sounds like a very drunken jam session rather than a song. It would be good if there was a way to make the output conform to some song structure, like verse-chorus-verse or something like that. But I don't know what exactly would need to be done to achieve that\nApril#5244: I can't imagine the approach riffusion is using will ever accomplish that. since it's just generating 5 second clips (or less) at once and it can't really \&quot;know\&quot; any larger structures\nApril#5244: you'd need something that could be trained on whole songs, and generates whole songs\nApril#5244: not 5 second clips&quot;}}},{&quot;rowIdx&quot;:51,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;db0798#7460: I think a 5 second clip could be a seed for making a loop. In the output of OpenAI Jukebox, there are usually some 5 to 10 second bits that sound great when they are looped but instead of looping, OpenAI Jukebox moves on to something unrelated, and the incoherence makes the output sound bad. So just looping a bit would already be an improvement over a stream of randomness, I think. And then perhaps it would be possible to make variations to the loop by using something like image2image. Or chopping the loop up into smaller time slices and swapping the order of some slices, and duplicating some slices. When you have two 5 second loops, one could serve as the A section and the other as the B section in a larger structure\nApril#5244: you should be able to make good loops by generating an x-tileable image when diffusing\nNikuson#6709: Can you please provide the link again?\nApril#5244: https://discord.com/channels/1053034685590143047/1053081177772261386/1053559417808900096\ndb0798#7460: That seems like a good idea\nNikuson#6709: Do you only have audio conversion or also images?\nApril#5244: again already posted earlier: https://discord.com/channels/1053034685590143047/1053081177772261386/1053176008687230978\nApril#5244: these are just my personal scripts. I know there's some more clean/nicer scripts out there somewhere posted lol\nIgnizHerz#2097: you could train an outpaint model just for riffusion couldn't you?\nApril#5244: I was just asked this in the \&quot;shinonome ai lab\&quot; server lol. Perhaps? I have no idea how well it'd go or how inpaint/outpaint models even work. I tried merging the inpainting 1.5 model with riffusion and it failed miserably\nIgnizHerz#2097: obviously lol\nApril#5244: outpainting using regular riffusion/dreamboothed model seems to work fine\nApril#5244: even then though, outpainting won't actually solve the issue mentioned\nIgnizHerz#2097: a particular outpaint would be able to mend new pieces much nicer\nApril#5244: since the length is always capped at 5.12s at most&quot;}}},{&quot;rowIdx&quot;:52,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;IgnizHerz#2097: same way with images\nNikuson#6709: no, I really like them.  At least until I ran them and there were no problems 😅\nIgnizHerz#2097: its how outpaint on images can keep a general idea and even add to them\nIgnizHerz#2097: I mean you can just keep outpainting couldn't you\nApril#5244: I showed some outpainting examples earlier iirc\nApril#5244: it works well for consistency but it won't be able to create proper song structure\nIgnizHerz#2097: music isn't too far from images\nIgnizHerz#2097: in terms of blending and keeping consistency\nIgnizHerz#2097: I think its moreso the outpaint not being so good at it on its own\nApril#5244: the problem is sd and thus riffusion is stuck with 512x512 images, meaning the model is trained on 5 second clips, which aren't enough to teach song structure\nNikuson#6709: I’ve just been writing various articles about all possible implementations of generative models with explanations for quite some time, and it became very interesting for me to release them at the end of the year.  something about audio.  Do you mind if your script gets on github and a couple of sites with articles about programming?😅 I'll try to point you out if possible\nIgnizHerz#2097: right, if it was an outpaint model trained specifically to add new pieces based off of old ones\nIgnizHerz#2097: wouldnt that work\nApril#5244: I post code with the assumption that people are gonna use it as they please. so go nuts 🙂\nApril#5244: just don't post any identifying info about me please&quot;}}},{&quot;rowIdx&quot;:53,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;IgnizHerz#2097: praise the open sourcing\nApril#5244: I like my anonymity\nApril#5244: the code I posted is mostly ripped from riffusion and chatgpt anyway 🤷‍♀️\nApril#5244: I'd formally release it but it's a mess lol\nApril#5244: normally I try to keep proper releases to things that are actually nice 😂\nNikuson#6709: ok, it will remain a secret\nNikuson#6709: By the way, it's very interesting how you can use depth2img in audio\nApril#5244: ?\nNikuson#6709: I'm just wondering how the depth2img function will behave in stable diffusion 2 on spectrograms\nJay#0152: https://colab.research.google.com/github/thx-pw/riffusion-music2music-colab/blob/main/riffusion_music2music.ipynb\nJay#0152: Everything seems to be fixed by og author, enjoy\nApril#5244: If I'm understanding this correctly, this takes a song file, splits it up, runs img2img on each section, then stitches it back together?\nApril#5244: also one thing I've been thinking is that it'd be super helpful to have a list of captions used on the training dataset of riffusion, rather than just blindly guessing what's in there\ndb0798#7460: I recently watched this video from a music theory guy on Youtube that explains why just morphing a motif into something different over time doesn't work as well as having proper song structure: https://www.youtube.com/watch?v=8Z8zOLtgvgU&amp;ab_channel=RyanLeach\nI think this depends on style, though: for some styles the outpainting approach would suit better than others&quot;}}},{&quot;rowIdx&quot;:54,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;April#5244: with regular stable diffusion it's using the laion dataset and clip which has damn near everything you'd want. but what does riffusion have?\nJay#0152: yes you're correct.\nApril#5244: it's entirely possible to keep outpainting. however you can't guarantee consistency. outpainting only makes sure the connecting bit flows well, not that the entire structure works together\ndb0798#7460: Yes\nIgnizHerz#2097: couldn't you train a model to specifically do this?\nApril#5244: if you use the same prompt and just keep outpainting, you get something workable but no overarching structure\nIgnizHerz#2097: it improves outpainting on images to use the original image as a basis as well\nApril#5244: no. sd only works up to 512x512 images, which means that you're stuck at 5s length for \&quot;knowledge\&quot;\nApril#5244: you'd have to rework how sd diffusion works\nApril#5244: to allow wider images\nJay#0152: might not be very difficult, though\nApril#5244: for a regular 3min song you'd need a 3000x512 image I think\nApril#5244: actually. I think my math is off lol\nApril#5244: 5s = 512\nIgnizHerz#2097: I figured for a proper outpaint model you'd use the original image as base anyways&quot;}}},{&quot;rowIdx&quot;:55,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;April#5244: 18432x512\nApril#5244: no that's still wrong\nApril#5244: i'm dumb\nApril#5244: actually....\nIgnizHerz#2097: which is applicable to music as it is to images. For both you'd want something that consists of new material but is not just random nonsense\nApril#5244: yes that's correct.\n5s=512\n180s/5s = 36\n36*512 = 18432?\nApril#5244: might be easier to just convert a full song and look at size lol\nApril#5244: i'm dumb af\nIgnizHerz#2097: though I'd imagine getting a song length of that size to work kek\nJay#0152: attempting style transfer w notebook to change song by the Beatles to one by the Doors lmao, wish me luck! 😆\nApril#5244: 24094x512 for a 4min song\nApril#5244: so yeah 18k is about right&quot;}}},{&quot;rowIdx&quot;:56,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;April#5244: tldr: sticking the whole dang song into sd is unworkable\nApril#5244: even though that's what you'd need to do to get song structure\ndep#0002: Not really, I mean sure SD 1.5. but SD 2.0 works at 768, and we already have aspect ratio trainers\nApril#5244: I suppose if you clip into like song sections like verses/chorus/whatever you could tag those individually, gen them using such tags, and manually stick together?\nApril#5244: though that'd still be large I think\nApril#5244: right, but 768 is already a huge jump that's computationally expensive, and that only nets you an extra 2 seconds\nApril#5244: as the math I just did shows, you'd need 18000x512 style images to do whole songs\nApril#5244: far larger than 768\nApril#5244: like 10x lmao\ndep#0002: We might be able to apply tensorrt and get a big boost as well\ndep#0002: But yeah, we are never going any higher than 2048\nApril#5244: tbh I think doing image diffusion on spectrograms is kinda a losing approach. computationally expensive for full length, very limited on smaller stuff, and spectrograms lose audio info\ndep#0002: 1024 even\nApril#5244: exactly\nApril#5244: though one thing is that sd is square aspect ratio which we don't need for music&quot;}}},{&quot;rowIdx&quot;:57,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;April#5244: 512 height is fine\nApril#5244: it's just width we need\nJay#0152: oh my god it kind of worked..... with default settings\ndep#0002: About the loss, lopho and me were talking about inserting more data on the other 2 channels\nCurrently it uses 1 channel (B&amp;W) and replicates across the 2 other channels\ndep#0002: The problem is that it would be even harder to recognize\nApril#5244: yeah I saw some talk about that earlier, using the full rgb\ndep#0002: Ah wait\nApril#5244: I don't know literally anything about music lol\ndep#0002: I was talking about width too xd\ndep#0002: 512h is ok imo\nApril#5244: as it stands, sd trains on 1:1 ratio, so 768w = 768h, which just makes it hard to scale as scaling both makes it much more pixels and expensive\nApril#5244: being able to train on non-square aspect ratio would help a lot\nApril#5244: but even then...\ndep#0002: Would?&quot;}}},{&quot;rowIdx&quot;:58,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: Anyways\n.....\nApril#5244: yeah so we don't need like 2048x2048 images, but rather 2048x512\ndep#0002: It exists\nApril#5244: oh?\ndep#0002: It's called bucketing\ndep#0002: I just said it.....\nApril#5244: doing that would definitely help\nApril#5244: but still I think it gets expensive even past 2048x512\nApril#5244: or probably before that\ndep#0002: Yeah\ndep#0002: And also that the model would struggle at such extreme aspect ratios\nApril#5244: decided to run a test with regular sd stuff and gen a 2048x512 image just to see if I can even do it lol\nApril#5244: it's taking like 3min I think lmao\nApril#5244: 2048 would be 20s lmao nowhere close to a song&quot;}}},{&quot;rowIdx&quot;:59,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;db0798#7460: I think just generating loops with Stable Diffusion and then stitching them together with some post-processing script would work better than trying to outpaint a whole song\nApril#5244: 5s loop isn't really that interesting to listen to though lol\nApril#5244: okay seems my laptop *is* able to gen a 2048x512 image 🙂\nApril#5244: took forever though @.@\ndb0798#7460: Yes, it would need variations. I think those could be created by inpainting bits of the loop, and rearranging time slices of the loop\nApril#5244: yup. though I feel like that quickly gets into \&quot;the human is making the music\&quot; territory, rather than the ai itself lol\nNikuson#6709: does anyone have any ideas to improve the sound quality?\ndb0798#7460: Yes, kind of. But if the post-processing is all done by a script instead of a human manually editing stuff, the result will still be fully computer-generated\nApril#5244: true. but if you're essentially hardcoding song structure, all the songs will end up sounding similar I think?\nNikuson#6709: one could immediately get audio from the image using a diffusion vocoder\ndb0798#7460: I guess depends on how complex the post-processing script is. A simple one would make similar-sounding output every time, a more complex one wouldn't\nApril#5244: true I suppose\nApril#5244: but I can't help but feel it's the same exact approach as making a chatbot by hardcoding in lines and responses\nApril#5244: like sure you technically get something workable, but it's really not a great solution\nApril#5244: 🤷‍♀️ I guess I'll just have to see if someone actually makes something like that&quot;}}},{&quot;rowIdx&quot;:60,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Nikuson#6709: and in what variable does the untransformed spectrogram lie here?\nApril#5244: ?\ndb0798#7460: Yes, generating a song structure the way I described is definitely more primitive compared to getting a neural network to produce a song structure. But the neural network method seems to be more difficult to do\nApril#5244: it's definitely a hard problem to solve\ndb0798#7460: Yes, I'm just throwing ideas around here and won't promise to do this myself, as I have many other unrelated things that I'm already busy with\nNikuson#6709: it’s already night for me and today I don’t seem to have time to understand your code and I just wanted to know in advance which of the variables contains the spectrogram taken from the image, but not yet converted to audio\nApril#5244: the spectrogram is the input for that particular script. it converts the spectrogram image into audio. I have no idea how the actual inner workings of the script work. spectro->audio was provided by riffusion, and audio->spectro was ai generated based on the riffusion code\nApril#5244: I know literally nothing about how this tech works or anything about music so sorry to burst bubbles there 🙏\nApril#5244: if i had to guess \&quot;spectrogram_from_image\&quot; likely is what converts the image to spectrogram data in-code\nNikuson#6709: ok, I'll study this code closer to dinner\ndep#0002: @seth Sorry to bother you, but how many steps was riffusion trained for? And at what batch size?\na_robot_kicker#7014: okay, I've been plunking away at this code for a while now and finally got it to work. No idea why but I had to reach deep into the transformers library and patch these three lines https://cdn.discordapp.com/attachments/1053081177772261386/1053874830933508129/image.png\na_robot_kicker#7014: somehow I was ending up with \&quot;attn_probs\&quot; being Float32, and \&quot;value_states\&quot; being float16\na_robot_kicker#7014: I can see that RiffusionPipeline defines its datatype as float16, so I'm not sure how attn_probs ended up being float32 🤷 https://cdn.discordapp.com/attachments/1053081177772261386/1053875188028153916/image.png\ndep#0002: I got a similar error where tensors were mismatch but it was using raw diffusers&quot;}}},{&quot;rowIdx&quot;:61,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;a_robot_kicker#7014: another change I ended up needing to do was converting the spectrogram image to float64, otherwise it would always overflow and produce nans https://cdn.discordapp.com/attachments/1053081177772261386/1053875449765318737/image.png\na_robot_kicker#7014: but, now I'm getting output spectrograms and waveforms! https://cdn.discordapp.com/attachments/1053081177772261386/1053875587392995368/image.png\na_robot_kicker#7014: will now learn how to save or play these. For learning purposes I took the guts out of the flask server and turned it into a command line interface.\na_robot_kicker#7014: okay, not too bad! now I've got wav files!! https://cdn.discordapp.com/attachments/1053081177772261386/1053876328438440056/image.png\na_robot_kicker#7014: wow, my first wav -- \&quot;Taylor Swift Beat Boxing\&quot; https://cdn.discordapp.com/attachments/1053081177772261386/1053877417183285268/output.wav\na_robot_kicker#7014: thanks, gonna have a lot of fun with this tool 🙂\njoao_betelgeuse#0410: Based\nmatteo101man#6162: @April What exact script are you using to convert the longer outpainted sequences? like say something 896x512? img2audio?\nApril#5244: same script that I posted. works for any size spectrogram lol\nmatteo101man#6162: which one chief? there's a few I remember\nmatteo101man#6162: like I remember you showed me audio.py but that ones for 4+ images\nApril#5244: I didn't at all change the conversion script lol. so... any of them? the spectrogram->audio script I'm using is straight from riffusion\nmatteo101man#6162: chavinlo ones? like riffusion-manipulation? or you mean something else\nmatteo101man#6162: might be missing another maybe ill scroll up\nmatteo101man#6162: cause definitely don't mess with img2audio.py for anything more than as is 🤮&quot;}}},{&quot;rowIdx&quot;:62,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;April#5244: https://discord.com/channels/1053034685590143047/1053081177772261386/1053559417808900096\nhttps://discord.com/channels/1053034685590143047/1053081177772261386/1053176008687230978\nApril#5244: I'm sure the riffusion-manipulation one will work fine too\nmatteo101man#6162: oh no\nApril#5244: or you can just grab audio.py from riffusion lol\nmatteo101man#6162: audio2spectro.py was the thing i was missing though (edit: which has nothing to do with what I'm doing)\nmatteo101man#6162: but nah manipulation one makes horrendous noises\nApril#5244: you can mess around with the max_volume variable. it's normally set to 50 but you can tweak it\nmatteo101man#6162: just static doing that though\nmatteo101man#6162: I'm just gonna give up on it, I tried the other audio.py but there's a lot of variables I have to edit to actually convert the file\nmatteo101man#6162: and if it's not mathematically sound it throws errors\nhayk#0058: Digging the prompts here: https://www.youtube.com/watch?v=BUBaHhDxkIc\nmatteo101man#6162: it will be nice when we can generate stuff of that quality straight from stable diffusion\nBananaBot#3675: I’ve only tried a couple spaces, but I noticed that they typically do things in increments of time (/seconds). Wouldn’t it be more useful (since we’re dealing with music) to generate things by beat/bar and bpm? Or is that not possible?\nJonestown#8964: Just trying out Riffusion now.  This is pretty impressive.  I've been following Harmonai for a while and haven't seen anything too interesting come out of it yet, but Riffusion is a big step forward.&quot;}}},{&quot;rowIdx&quot;:63,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;noop_noob#0479: @hayk @seth Sorry for the ping. May I know what the dataset was? Or if not, maybe at least what the text in the dataset looks like? I think that maybe knowing what the data looks like could lead to better prompts.\na_robot_kicker#7014: Yeah I can't find this info anywhere and it's critical to understand the limitations of this model and how it might be improved.\nouthippo#4297: damn, even if it does not adhere to prompts that well its still some great music\nouthippo#4297: Can someone explain where this spectogranm to audio script gets the \&quot;sounds\&quot; from meaning how the piano or drums sound?\nouthippo#4297: i cant wrap my head around it\nnoop_noob#0479: The X axis of the spectogram is time. The Y axis is the frequency. At a specific time (i.e., in a single column of pixels), a specific timbre of sound (which, for example, distinguishes a piano from a violin) corresponds to a specific pattern of blacks/whites/grays. Placing this pattern higher or lower corresponds to a higher or lower pitch.\nouthippo#4297: got it, but there are thousands of possible piano sounds - some are terrible, midi sounding and some are full and realistic. I wonder why the music ends up sounding pretty good in terms of sound selection and not fake. Maybe it's just that the frequencies correspond to how iyt should sound \&quot;exactly\&quot; unlike midi which just says how high or low the sound should be but not the quality of the sound.\nnoop_noob#0479: The different kinds of piano sounds correspond to slightly different timbres/patterns.\nnoop_noob#0479: something something fourier transform idk lol\nouthippo#4297: got it, so if it was trained on music that is professionak sounding than it would retain the professional sound selection\nnoop_noob#0479: Probably, yeah. Maybe prompts might affect that too.\nouthippo#4297: Thanks, I need to read more on spectograms too\nnoop_noob#0479: @outhippohttps://youtu.be/spUNpyF58BY\nnoop_noob#0479: https://www.reddit.com/r/StableDiffusion/comments/zoc365/new_riffusion_web_ui_realtime_music_generation_up/\nNikuson#6709: I don't know why, but pycharm refuses to see pydub even though I installed it&quot;}}},{&quot;rowIdx&quot;:64,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Nikuson#6709: from constant problems, I can only come to the conclusion that pycharm is far from the best IDE\nTwee#2335: damn this app is nuts\ndep#0002: https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features\ndep#0002: Might be really useful to use this for future finetuning\ndep#0002: @Jonestown ????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\ndep#0002: https://media.discordapp.net/attachments/710745951236522019/791617162364190730/image0-19.gif\nJonestown#8964: @dep Cat decided to play with a toy on the keyboard.  Sorry for spam lol\ndep#0002: lol\nNikuson#6709: there is no thematic mood at all\nTwee#2335: putting \&quot;shoegaze\&quot; on the prompt does not make shoegaze :(\nTwee#2335: pain\nSheppy#4289: yeah, training it to recognize musical modes/keycenters/time signitures/tempos should probably be a high priority\nTwee#2335: im more of a genre person\nTwee#2335: would be great if adding an amalgamation of genres in a prompt could create an accurate fusion of those genres\nTwee#2335: or simply adding a genre and recreate a song in the vein and sound of that genre&quot;}}},{&quot;rowIdx&quot;:65,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Twee#2335: and im not talking about mere meta genres like pop, rock, jazz, hip hop, etc\nTwee#2335: im talking about more specific subgenres and scenes\nSheppy#4289: that's important too\nTwee#2335: like if i added \&quot;radiohead but vaporwave\&quot;\nSheppy#4289: lol\na_robot_kicker#7014: working on a simple tkinter local GUI, will make a github fork once it is useful https://cdn.discordapp.com/attachments/1053081177772261386/1054134363387858954/image.png\na_robot_kicker#7014: so far in terms of data representation, I have no idea which prompts will work. Seems to mostly only know about electronic music and a few big name artists\na_robot_kicker#7014: but the seed can have a huge effect\nTwee#2335: nice!!\nThistle Cat#9883: https://youtu.be/nHuF927CgkM\nThistle Cat#9883: Idk why but my label is far more devious than this audiobook\nnoop_noob#0479: Huh https://fxtwitter.com/1cebell/status/1604267031238434819\nnoop_noob#0479: Found this from 4 years ago https://www.youtube.com/watch?v=YRb0XAnUpIk\npork#9581: can we use higher resolution generations to get better audio quality/longer clips?\nquique#5458: hi, is there any colab available to generate smooth audio between two prompts which also allows us to grab and use the last spectogram image to generate further audio transitions? My idea is chaining several prompts transitions like \&quot;typing -> jazz piano -> guitar riff -> rock guitar solo\&quot;? or how would you do it manually?&quot;}}},{&quot;rowIdx&quot;:66,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;IDDQD#9118: Neither does it do black metal :(((\nXIVV#9579: or hardcore punk :((((\nIDDQD#9118: Looking forwards to the future iterations of this riffusion. Also would gladly contribute if anyhow possible at some point (most likely via labelling etc. since I don't posses programming prowess). this immense.\nIDDQD#9118: \&quot;an exhiliratingly epic anti-anthem of the dysregulatory purgatory in the style of psychedelic cosmic black metal vaporwave noise mumblerap with contemporary avantgarde classical and free jazz passages by greg rutkowski trending on artstation\&quot; when??!!\nTwee#2335: post-avant jazzcore and progressive dreamfunk\nJack Julian#8888: https://everynoise.com\nTwee#2335: everynoise is ok but tbh an actually accurate genre database is RateYourMusic\nTwee#2335: sounds like ur average oranssi pazuzu album\nTwee#2335: i did try \&quot;dark synthpop with ethereal female vocals\&quot; and it sounds eerily similar to early cocteau twins\nTwee#2335: i was very impressed\nTwee#2335: cant wait until im able to extend songs thru automatic1111's ui\ncravinadventure#7884: Hi!\n\nIf anyone needs a Mix and Master of any sound for much better sound quality, please DM me!\n&quot;}}},{&quot;rowIdx&quot;:67,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;*- **BEFORE:**     (Original_Riffusion_Output_Sound)*\n*- **AFTER:**        (Mixed_Mastered_Sound)*\n\n🙂 https://cdn.discordapp.com/attachments/1053081177772261386/1054429627482910832/Original_Riffusion_Output_Sound.wav,https://cdn.discordapp.com/attachments/1053081177772261386/1054429627810062436/Mixed_Mastered_Sound.wav\npnuts#1013: what's the benefit of running it via automatic1111? I'm using the web-app from the official repo atm\npnuts#1013: nvm, just installed it to take a look\nNikuson#6709: I can't solve the problem with pydub. It doesn't see ffmpeg even though I installed everything correctly\nclambake#5510: if we can use transformers to turn words into music, could we generate an ai image from music\npnuts#1013: works pretty much out of the box here ```|0: e:                                              |\n│1: cd git                                        │\n│2: git clone https://github.com/jiaaro/pydub.git │\n│3: cd pydub                                      │\n│4: conda create -n pydub python=3.9 -y           │\n│5: conda activate pydub                          │\n│6: python setup.py build                         │&quot;}}},{&quot;rowIdx&quot;:68,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;│7: python setup.py install                       │\n│8: conda install ffmpeg                          │\n│9: python whatsthis.py                              |```\npnuts#1013: `whatsthis.py` is  the first sample in the repo```import os\nimport glob\nfrom pydub import AudioSegment\n\nvideo_dir = './samples'  # Path where the videos are located\nextension_list = ('*.mp4', '*.flv')\n\nos.chdir(video_dir)\nfor extension in extension_list:\nfor video in glob.glob(extension):\nmp3_filename = os.path.splitext(os.path.basename(video))[0] + '.mp3'\nAudioSegment.from_file(video).export(mp3_filename, format='mp3')```&quot;}}},{&quot;rowIdx&quot;:69,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;AgentA1cr#8430: does Riffusion support negative prompt weights?\nAgentA1cr#8430: Also, loving what this model can do. However, it seems to me that, given enough time, it will slowly (or sometimes not-so-slowly) drift away from the prompt and start doing its own thing, with a strong preference for percussion and piano.\nhayk#0058: Very nice! I'm super interested to automate parts of this into the riffusion-inference repo so it can generate higher quality. Are you doing this within a DAW? I'd love a breakdown of the steps so we can try to get it in code.\n\nSomewhat related, if any audio experts have a lead on a neural vocoder to try that might perform better than Griffin Lim, that's worth exploring.\nundefined#3382: I'd really be great if we could finetune the model (using EveryDream) ourselves too 🙂 but I haven't seen any info about that or I missed it\ndb0798#7460: My guess is that you have multiple versions of Python on your system and something you installed got installed for the wrong installation of Python\na_robot_kicker#7014: I've been working with a DAW as well and will post a little fork with my progress, probably later today. Biggest limitation I have with the tool so far IMO is the hard-coded 5s limit, makes it challenging to keep in sync with clips from the DAW. I'd like to eventually write a VST plugin or something to make interop a bit easier.\na_robot_kicker#7014: VST plugin would have to call some kind of API to a server running riffusion w the audio input it gets, and then async output that either to a file or the VST output buffer. A bit difficult because the VST api pretty strongly assumes realtime processing aplications.\nj.ru.s#9349: Hey all, does anyone know what music dataset Riffusion uses to fine-tune on?\nj.ru.s#9349: Making a universal vocoder is a really tricky problem, usually it needs to be specifically trained towards a specific type of audio output. In the case of Riffusion, if we know the music dataset used to do the fine tuning we could potentially train a neural vocoder on that.\nNikuson#6709: I thought so too and removed all versions except 3.9, but still does not determine\nNikuson#6709: i suggest to use univnet, it is based on diffusion too and can be used for different types of sounds\ndb0798#7460: I don't know then what goes wrong in your installation. I got the installation working for myself today but it was a troublesome process\nNikuson#6709: I did everything according to the guide from the Internet.  downloaded and added path to PATH.&quot;}}},{&quot;rowIdx&quot;:70,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Nikuson#6709: \&quot;Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\&quot;\nNikuson#6709: I installed psydub via pip, but the audio backend for it is ffmpeg and I installed it according to the guide from wikiHow\na_robot_kicker#7014: alrighty, here's my fork that has a simple local tkinter gui that can read and write .wav files: https://github.com/mklingen/riffusion-inference\na_robot_kicker#7014:  https://cdn.discordapp.com/attachments/1053081177772261386/1054498695162372116/riffusion_gui.gif\nXIVV#9579: damn\nXIVV#9579: im just waiting for this thing\nXIVV#9579: to develop\nXIVV#9579: so i can generate a sick black metal riff\nXIVV#9579: cuz\nXIVV#9579: rn\nXIVV#9579: it sounds like some kind of lounge music\nXIVV#9579: or something\nHaycoat#4808: Someone should make some sort of img2img version so you can make a doodle and turn it into a spectrogram of your text to music prompt\nnullerror#1387: apologies if this has been asked before i just joined but there is currently anyway to finetune the model with music of your choice?\nnullerror#1387: like if i wanted to retrain the model on a specific genre how would i go about converting wavs to the correct spectrogram format to fine tune the stable diffusion model on&quot;}}},{&quot;rowIdx&quot;:71,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;db0798#7460: It should be possible to use chavinlo's scripts from https://github.com/chavinlo/riffusion-manipulation to do conversions between audio and spectrograms, and to run Dreambooth to add new spectrograms to the Riffusion model\ncravinadventure#7884: Amazing! Thank you for sharing. 🙂\nnullerror#1387: thank you db0789!\nnullerror#1387: i assume this also takes care of the phase issues mentioned in the about page of the website\ndb0798#7460: I don't know if it takes care of the phase issues or not\nnullerror#1387: is the person who made this in the server by chance\nnullerror#1387: ?\nXIVV#9579: i think it's @seth\nnullerror#1387: oh i meant the chavinlo github posted above\nnullerror#1387: dmed seth with the same question but no response yet. must be busy\ndb0798#7460: I think it's @dep\ndep#0002: whats a phase issue\ndep#0002: loss in information?\nnullerror#1387: here one sec\nnullerror#1387: ill grab the quote&quot;}}},{&quot;rowIdx&quot;:72,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dep#0002: ok\nnullerror#1387:  https://cdn.discordapp.com/attachments/1053081177772261386/1054509685790744627/image.png\ndep#0002: you mean if the repo uses the griffin-lim thing to reconstruct the audio?\nnullerror#1387:  https://cdn.discordapp.com/attachments/1053081177772261386/1054509803369668618/image.png\nnullerror#1387: or to make the spectrograms\nnullerror#1387: cuz it says here the spectrograms they use take advantage of these algos or whatever they are\ndep#0002: it uses the exact same functions that the original riffuser inference server uses. The only new addition is the `image_from_spectogram` which is the inverse of `spectogram_from_image` although our implementation was unofficial but works almost the same as the one that's currently on the official repo https://cdn.discordapp.com/attachments/1053081177772261386/1054510864667000892/image.png\nnullerror#1387: ah got it perfect thank you!\nnullerror#1387: ill give this a shot\ndep#0002: there are also some tests included in the repo if you want to take a look at the reconstruction quality with different parameters\nnullerror#1387: will do\nnullerror#1387: appreciate it\nApril#5244: >people using some code I generated using an ai\nI'll never stop being amused lol. I'm honestly just surprised it works at all\nnullerror#1387: okay so ive made some images and am going to finetune the model. i think ive gone ahead and made to many tho haha&quot;}}},{&quot;rowIdx&quot;:73,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;nullerror#1387: how many images/training steps are recommended (if known)\ndb0798#7460: My first Dreambooth test run just finished. I used 53 5 second pieces of a chiptune for training, used 'techno' as the class prompt. The output sounded like a chiptune already after 750 steps. I think it got to overfitting territory quite quickly, although it didn't make an exact copy of the input tune\nnullerror#1387: shoot only 53??\ndb0798#7460: If anyone has more experience with this, I would also like to know how many images and steps are recommended\nnullerror#1387: man im over here with 1400 i think i should cut back\nnullerror#1387: would also be interested in hearing images/steps\nnullerror#1387: although i know a general rule of thumb is 100x the number of new images\nnullerror#1387: for steps\nnullerror#1387: its how many images is the question\ndb0798#7460: I think 1400 might be a good number actually, if you want to have varied output\nnullerror#1387: oh fr? hey ill be the test dummy and give it a whirl\nnullerror#1387: is probably gonna take forever tho but ill shoot my shot\ndb0798#7460: With as few as I used, basically what I got back from it was the original input with mutations and rearrangements. I think with a larger input dataset there will be more of a chance to get something completely new in the output\ndb0798#7460: I'm curious to know how your run turns out\nnullerror#1387: im gonna cut back slightly so im not wasting massive amounts of time/compute power but will report my results&quot;}}},{&quot;rowIdx&quot;:74,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;nullerror#1387: gonna go for like 200-400\ndb0798#7460: That's like in the Terminator movie where Skynet travels back in time and contributs to Skynet's code\na_robot_kicker#7014: That's awesome. I'd love to try a fine tuned chip tune model\ndb0798#7460: I'll try again later with a larger input dataset\na_robot_kicker#7014: I've noticed that this thing is learning a 3 channel image and converting it to single channel, which seems pretty wasteful. Perhaps you could even fit more time domain into the G and B channels. Phase is apparently hard, but one low hanging fruit might be to just put more time in there so you could get longer samples. Like literally R is the first 5 seconds, G the second, and B the third\ndb0798#7460: April and deb talked about something like this here earlier, would be good if they implemented it\na_robot_kicker#7014: But since it's using stable diffusion which is trained originally on natural images anything that doesn't vaguely resemble real images might be hard for it to learn.\ndenny#1553: has anyone been using SoX for audio processing? Seems super powerful for automated output\nNikuson#6709: I still don't understand how to make it work on Windows\na_robot_kicker#7014: What are you hoping to use ffmpeg for?\nhayk#0058: You should try generating riffs without img2img conditioning. I believe it's just the seed image being \&quot;sparse\&quot; that leads to it sounding more like lounge music\nryan_helsing#7769: I use it a ton on my project (https://neptunely.com)\ndenny#1553: Has been working great at concatenating audio files so far-- been wondering if there's a way to ping-pong loop though\nryan_helsing#7769: Can we supply our own seed images? Is what’s happening a sort of img2img style transfer technically?\nryan_helsing#7769: It’s extremely powerful and quick when you pipe in output .. I sometimes build commands with hundred of sub commands tying together files and it does it in under a second&quot;}}},{&quot;rowIdx&quot;:75,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;denny#1553: yeah it's super fast!\ndenny#1553: I've been impressed\nnullerror#1387: finetuned my model but ive run into the issue of idk how to run it now lmao\nnullerror#1387: i have an amd gpu so i dont think the webapp will work for me. is there any other way of running riffusion with a custom model? maybe a colab?\nnullerror#1387: https://huggingface.co/spaces/aross3/riffusion-rage\nnullerror#1387: https://huggingface.co/spaces/aross3/riffusion-rage\nnullerror#1387: apologies for two links my discord is glitchy\nnullerror#1387: but hey it worked. i trained it on yeat and carti. sounds a little off/vocoded. does it just need more training? was trained on 200 so images\nnullerror#1387: hmm actually its super phase-y. keeps like shifting up over time\nnullerror#1387: anyone know why?\ndb0798#7460: Is the model that is loaded in the linked page your trained model? If so, what's the instance prompt for it?\nnullerror#1387: yes its my trained model and all the images its trained on are called rage\nnullerror#1387: lowercase \&quot;rage\&quot;\nnullerror#1387: it only came out as 1gb compared to the 14gb main model\nnullerror#1387: so maybe thats the issue? idk im more an audio guy than ml&quot;}}},{&quot;rowIdx&quot;:76,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;db0798#7460: There are smaller versions of the Riffusion model that someone linked to on Reddit: https://www.reddit.com/r/riffusion/comments/znbo75/how_to_load_model_to_automatic1111/ . I was using the 4Gb one for training. I don't actually know what the difference is between these versions\ndb0798#7460: Here's a random output file from the chiptunes test. It's kind of low fidelity but doesn't seem phasey in exactly in the same way as yours. I guess this could be because the material it was trained on was already 8 bit, so bit reduction doesn't affect it as much https://cdn.discordapp.com/attachments/1053081177772261386/1054571256814514277/sample_8750-00.mp3\nnullerror#1387: hmm i wonder why mine is acting all weird\nnullerror#1387: did u use a colab to train it?\ndb0798#7460: I used Automatic1111 WebUI run locally, with RTX 3080\nnullerror#1387: ah see i got an amd card so im using a google colab\ndb0798#7460: I tried right now what happens it I try to do the training with the 14 Gb model file, it doesn't work on my setup because CUDA runs out of memory\nnullerror#1387: i wonder if its chkpt setup is messy oir smth\ndb0798#7460: I guess the spectrogram to audio conversion settings can make a difference to the output quality, too\nnullerror#1387: how did u convert? did u use that github u linked earlier?\nnullerror#1387: i did max settings except 512 height\nnullerror#1387: for audio to spectro\nnullerror#1387: i mean\ndb0798#7460: Yes, I used the scripts from that Github page\ndb0798#7460: For audio to spectrum I used the default settings&quot;}}},{&quot;rowIdx&quot;:77,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;db0798#7460: For spectrum to audio I used the default settings, except I reduced maxvol from 100 to 50 because otherwise the audio started clipping\nnullerror#1387: gotcha\nnullerror#1387: i tried running spec to audio but again amd gpu so i couldnt do it\ndb0798#7460: It would be handy if there was a Colab version of that Github repository\nnullerror#1387: yeah fr\nnullerror#1387: im using the dreambooth colab for fine tuning then spaces to run it\nnullerror#1387: also just some sort of how many images for fine tuning/steps ofc\nnullerror#1387: i guess im just asking for a guide at this point lmao\ndb0798#7460: Yes. I think right now this hasn't been tested enough for anyone to write a guide\nIgnizHerz#2097: haha still developing the tools to play with it I think\nIgnizHerz#2097: plus training takes time\nLAIONardo#4462: do you actually have to use Dreambooth? Would it just replace the seeds in the interference model be enough?\ndb0798#7460: How's that done? Would textual inversion do that?\nLAIONardo#4462: Maybe I am wrong here, but isn't this folder what train the model? https://github.com/riffusion/riffusion-inference/tree/main/seed_images\nLAIONardo#4462: Maybe your way of deploying DreamBot is the right thing to do&quot;}}},{&quot;rowIdx&quot;:78,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;db0798#7460: I don't know what those images are for exactly but I think replacing the image files in that directory won't do anything unless you retrain the whole Riffusion model from scratch in the way the people who created that model did\ndb0798#7460: I think textual inversion might also work in place of Dreambooth but I haven't heard of anyone trying it for Riffusion\nLAIONardo#4462: I wonder if @hayk could share some lights on what that folder does 🙂\nLAIONardo#4462: Incredible work btw!\na_robot_kicker#7014: No those are just initial images that can be used as a seed for img2img\nLAIONardo#4462: Oh I see that makes sense!\nnoop_noob#0479: https://www.youtube.com/watch?v=uGRLOMf2hSc\nnoop_noob#0479: AI music from a different team.\nMeatfucker#1381: Hello, enjoy your tool quite a bit. Also noted we have a bit of hobby crossovers when I checked your github profile. I fly fpv drones for shits n giggles.\nMeatfucker#1381: One thing I noted is the seams in between loops is a bit abrupt and had an idea about that. If you have a fast enough gpu you should be able to take two outputs and img2img the seam between them\nMeatfucker#1381: should make the transition between clips much smoother, but I think it would roughly double processing time since you would be making intermediate frames every time\nMeatfucker#1381: though you also wouldnt have to use the entire generated frame, just the edge, so maybe it wouldnt add so much\nnoop_noob#0479: What is this? https://fxtwitter.com/naklecha/status/1598956352851693568\nMeatfucker#1381: Looks like they are extracting chords from an audio file and then teaching a model to predict them\nMeatfucker#1381: a neat approach, but itll be limited to sounds that are chords Id imagine&quot;}}},{&quot;rowIdx&quot;:79,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;JL#1976: https://arstechnica.com/information-technology/2022/12/riffusions-ai-generates-music-from-text-using-visual-sonograms/ Ars Technica article\nIDDQD#9118: yes, indeed\nNikuson#6709: for one script to convert audio to image spectrogram\nXIVV#9579: how do i turn that off\nEdenoide#0166: Hi! I'm a windows user and I've been unable to make riffusion works on it... But I've created a simple Colab from the RIFFUSION MANIPULATION github for converting audio to spectrogram for model training: https://drive.google.com/file/d/1Mv3FsSiZGWt_qRv1UloG2gIawlalMlej/view?usp=share_link\nEdenoide#0166: My programming level is zero by the way\nNikuson#6709: if anyone has a script without using something like ffmpeg - i will be grateful\nNikuson#6709: this is great. but in principle, in this repository, I can only get noise\nEdenoide#0166: They look good enough but I haven't tried yet to turn them again into sound https://cdn.discordapp.com/attachments/1053081177772261386/1054723695203074108/aicumbia16_0.png\nEdenoide#0166: I've been working with 5,12 seconds loops\nEdenoide#0166: in wav format\nEdenoide#0166: for a reason I don't know when turning it into mp3 the time length changes a bit and creats an extra chunk with just white\nEdenoide#0166: so I'm only saving the first png of every loop\nNikuson#6709: I have them written badly, maybe I mixed up the sizes\nNikuson#6709: 512*512 https://cdn.discordapp.com/attachments/1053081177772261386/1054724689064366110/12bb35ac-1c9e-49ce-a576-2746ec474aeb.png&quot;}}},{&quot;rowIdx&quot;:80,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Edenoide#0166: wow\nEdenoide#0166: maybe it's something wrong with the audio. I've been generating the loops with Audacity (free sound software):\nEdenoide#0166: Just drag and drop a sound on it. Then select 4 beats for making a loop and delete the rest (In case you are generating 'four-to-the-floor' electronic music). Double click on the timeline and then Effect>Pitch &amp; tempo>Change tempo and in the last parameter Length (seconds) put on the second cell 5.119\nEdenoide#0166: Then File>Export>Export as WAV\nEdenoide#0166: The thing is your final file should be a 16bit .wav with a length of 5.119 seconds\nNikuson#6709: I did it, I didn't exactly trim the song to 5 seconds last time https://cdn.discordapp.com/attachments/1053081177772261386/1054726259298537562/7fb7aa90-d9b5-4c8d-b8dd-d94d4f35002b.png\nNikuson#6709: after reverse processing https://cdn.discordapp.com/attachments/1053081177772261386/1054726335085416468/LG.mp3\nEdenoide#0166: that looks a lot better but your clip seems shorter than 5.119 seconds so there's a silence at the end.\nNikuson#6709: I made to your img2audio notepad: https://colab.research.google.com/drive/1-REue4KpDhOMDI-v6gRytMpANoMUqFvi?usp=sharing\n\nnow all functionality is implemented here\nEdenoide#0166: great!\nNikuson#6709: in the original trimmed clip, there is also silence at the end\nNikuson#6709:  https://cdn.discordapp.com/attachments/1053081177772261386/1054727175238066196/Lady_Gaga.wav\nEdenoide#0166: perfect then&quot;}}},{&quot;rowIdx&quot;:81,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Edenoide#0166: How did you avoid the 'clipping' artifacts in the second .wav?\nNikuson#6709: don't know, i just cut the audio through this service for even 5 seconds: https://mp3cut.net/\nEdenoide#0166: I think riffusion only works with loops of 5,12 seconds (maybe I'm wrong). This means if you are not training your model with 'loopable' cuts of 94bpms (or 47bpm, 188bpm...etc)  it would sound like a patchwork but maybe in an interesting way.\nNikuson#6709: I don't want to use this for riffusion, I'm training my model\nNikuson#6709: riffusion trained a little wrong\na_robot_kicker#7014: Oh. Check out the branch I posted a bit earlier. All I did was invert the code that coverts image to wav. You will find that in my audio py. Btw it requires 16 bit 44.1 khz mono wav files. You shouldn't need ffmpeg for that as python has a native wave file reader\nNikuson#6709: Can I please have a proverb for this?\na_robot_kicker#7014: To convert other formats into that you can use vlc or audacity, although those things are probably running ffmpeg under the hood they at least install it for you\na_robot_kicker#7014: I assume you mean link. See audio.py in there, has wave to spectrogram. There are the guts of loading a wave from disk in there in gui.py as well. https://github.com/mklingen/riffusion-inference\nNikuson#6709: Thanks, I'll take a look\nHaycoat#4808: Can we get a huggingface demo for wav2spec2music?\nnullerror#1387: ooooooooooooooooooooooo the audio needs to be mono?\nnullerror#1387: hm\nnullerror#1387: wait also i assume thats a typo but it requires 44000 not 4400 sampling frequency?\nnullerror#1387: and are we sure its not the industry standard 44.1khz it has to be 44khz?&quot;}}},{&quot;rowIdx&quot;:82,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;nullerror#1387: ok checked the code can confirm it is 44.1khz\nnullerror#1387: scared me for a sec\nHaycoat#4808: Should I start a list of artists the model currently recognizes?\nHaycoat#4808: Because there's a few that are very prominent when generating with their name\na_robot_kicker#7014: Yeah 44.1 kHz\na_robot_kicker#7014: If the author would just tell us the training set, we'd know that. But I can't find the training data....\nHaycoat#4808: I know Avicii, Eminem, Post Malone, Frank Sinatra and Billie Ellish are some that generate good results\na_robot_kicker#7014: Britney Spears,  Charlie Parker, Jimi Hendrix and Aretha Franklin all worked for me. Surprisingly, Michael Jackson did not. Nirvana didn't.\na_robot_kicker#7014: Oh I tried some classical. Bach and Chopin sort of work\ndenny#1553: deadmau5 seems to work\ndenny#1553: Using 'deadmau5 melody' gives more than just droning thumps\ndenny#1553: Queen, Weezer, Kurt Cobain, backstreet boys seems to work too\nLAIONardo#4462: So a part I am confused here (for people who are training the model on a specific sample) are you just training Dreambooth with few pictures of different spectrogram? Cause what I still don't get is doesn't Dreambooth has a lot more picture than just the one you train the model with? Are you also replacing those as well with more spectrograms??\nLAIONardo#4462: Like if I am retraining Dreambooth on new spectrogram, both the instance data and the class data needs to be spectrogram only I suppose? but what is the difference there, like how do I choose what's one or another since they are both spectrograms (unlike regular profile case where the instance would be a pic of myself) https://cdn.discordapp.com/attachments/1053081177772261386/1054796709248647198/CleanShot_2022-12-20_at_11.23.202x.png\nnullerror#1387: i’m training mine on a whole genre via a dreambooth google colab. converted the songs/samples to spectrogram and then uploaded as instance data. i’m not sure what class data is my colab doesn’t have that&quot;}}},{&quot;rowIdx&quot;:83,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Twee#2335: i should make an ai-generated lo-fi hip hop livestream\nTwee#2335: nobody will tell the difference\nnullerror#1387: uploaded roughly 1400pics\nnullerror#1387: haha twee that was what i was gonna go for here in a sec\nnullerror#1387: endless lofi beats\nTwee#2335: i mostly wanna do it as a critique\nTwee#2335: of how formulaic a lot of those beats are\nnullerror#1387: could be said of any genre\nTwee#2335: yes but this model is trained on beats lol\nnullerror#1387: ?\nTwee#2335: i mean thats what ppl in the share-riffs channel said\nTwee#2335: although i mostly have an issue with a lot of cookie-cutter lo-fi hip hop thats become too oversaturated lol\nnullerror#1387: i think it’s trained on a variety of lounge music and various electronic artists as people were mentioning above\nnullerror#1387: the main model i mean\nnullerror#1387: ofc a lofi hiphop centered one could also be made&quot;}}},{&quot;rowIdx&quot;:84,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;nullerror#1387: yeye\nSemper#0669: Oh I see! Would you mind share the google collab link you used?\nTwee#2335: most of my ai ideas are mostly satirical critiques of lack of creativity within culture\nnullerror#1387: sure thing\nSemper#0669: I am interested in that question\nSemper#0669: As well\nnullerror#1387: it comes with a guide as well one sec\nTwee#2335: that or just shitposts\nnullerror#1387: https://bytexd.com/how-to-use-dreambooth-to-fine-tune-stable-diffusion-colab/\nnullerror#1387: https://bytexd.com/how-to-use-dreambooth-to-fine-tune-stable-diffusion-colab/\nnullerror#1387: apologies my links send twice cuz my discord is glitchy\nnullerror#1387: basically train it here then transfer to an instance of riffusion\nTwee#2335: unfortunately im still stuck with automatic1111's webui so i cant make anything fancy other than short clips and convert them to audio\nSemper#0669: Thank you are the best! I guess the name of the sample here is important as well right?\nSemper#0669: Like if I want to train it on jazz each sample should have an artist name to then be found in generation right?&quot;}}},{&quot;rowIdx&quot;:85,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;pnuts#1013: install it from the official repo and run the web-app locally? at least that way you'll get continuous playback\nTwee#2335: tried and ran into a lot of headaches lol\npnuts#1013: oh 🙂\nTwee#2335: also storage is an issue\nTwee#2335: all these models, man\nTwee#2335: they eat ur hard drive up\nSemper#0669: Ahah yeah\nTwee#2335: and riffusion is like\nTwee#2335: 15 gb\npnuts#1013: you're not wrong\nTwee#2335: probably the biggest one i have\nnullerror#1387: semper should all be in the guide but yeah all my images were named the same\nTwee#2335: ill try again some other time and maybe i can reach out if u know how to do it correctly?\nnullerror#1387: can’t confirm it works completely yet. did a quick test yesterday and it was getting there. trying a bigger one as of rn. when i load the bigger one i’ll send my finding as to if it worked\nnullerror#1387: also make sure ur audio is mono/16bit wav/44.1khz&quot;}}},{&quot;rowIdx&quot;:86,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;nullerror#1387: then use that github linked somewhere above called like manipulation tools for riffusion or smth to get audiotoimg for training\npnuts#1013: sure, I don't recall running into any major issues. I've got it working on 2 machines. I'm sure we can work it out\nnullerror#1387: twee do u have an amd graphics card\npnuts#1013: nvidia here, does AMD even support CUDA?\nTwee#2335: nah\nTwee#2335: i have nvidia\nTwee#2335: more specifically\nTwee#2335: i use a cloud pc with a nvidia p5000 quadro\nTwee#2335: for the ai stuff\nTwee#2335: (and gaming ofc lmao)\npnuts#1013: running locally on a 3080 here\nnullerror#1387: was gonna say if it ain’t working might be amd\nnullerror#1387: that’s my issue and why i have to use colab and stuff\nnullerror#1387: where do u rent a cloud computer from twee i’ve been looking for one\nTwee#2335: i mean this first step isnt really clear. npm install on your C home drive doesnt do anything lol https://cdn.discordapp.com/attachments/1053081177772261386/1054801152530731008/Screenshot_2022-12-20_at_11.42.17_AM.png&quot;}}},{&quot;rowIdx&quot;:87,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;pnuts#1013: https://lambdalabs.com/ seems to be one of the cheaper options\npnuts#1013: make sure you have node/npm installed, then run `npm install` from inside the folder\nTwee#2335: which folder though\nTwee#2335: the node folder?\nTwee#2335: i wish that was specified tbh\npnuts#1013: the git repo you cloned\nTwee#2335: wait\nTwee#2335: ahhhhh\nTwee#2335: yea i think i tried that too\nTwee#2335: hold on\npnuts#1013: so from inside the `riffusion-app` folder\nTwee#2335: yea screw it ill just try agian lol\nTwee#2335: oh ok\npnuts#1013: in the same folder you also want to create a file called `.env` or `.env.local` and add ```RIFFUSION_FLASK_URL=http://127.0.0.1:3013/run_inference/```\npnuts#1013: if you run the inference server on the same box the above will work&quot;}}},{&quot;rowIdx&quot;:88,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;pnuts#1013: if it's running elsewhere, add the correct IP\nnullerror#1387: thanks punts i’ve been looking at that and vast ai\nnullerror#1387: i’ll extend my search\nTwee#2335: i should had know i should had git cloned, im just still getting used to \&quot;developer unclear instructions to layman user\&quot; syndrome\nTwee#2335: do u edit the file like a textfile?\npnuts#1013: yes\nTwee#2335: ok bc\nTwee#2335: i cant seem to be able to edit it\npnuts#1013: open it in any old text editor\npnuts#1013: if you're struggling create a .txt file, add `RIFFUSION_FLASK_URL=http://127.0.0.1:3013/run_inference/` and then rename it to .env or .env.local\nTwee#2335: no i gotcha\nTwee#2335: usually im used to right clicking a file and hit the \&quot;edit\&quot; button\nTwee#2335: ok time to donwload the model but where does that get put in\npnuts#1013: right-click open with\npnuts#1013: you don't actually need the 14GB checkpoint, but if you want it `git lfs clone https://huggingface.co/riffusion/riffusion-model-v1`&quot;}}},{&quot;rowIdx&quot;:89,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Twee#2335: oh lmao\nTwee#2335: whats the checkpoint download for then\npnuts#1013: more fine=tuning perhaps? I'm pretty confident I didn't download it on the 2nd install I did.\nTwee#2335: also\nTwee#2335: wasnt this suppose to be in the inference folder\nTwee#2335: which is a separate download\npnuts#1013: also it's 14GB I only have a 10gig card.\npnuts#1013: no, that variable is to tell the app where to find the inference server\na_robot_kicker#7014: Uh, it downloads the 14gb model on startup if it can.\nHaycoat#4808: Img2spec2music should be possible to do in a huggingface demo...\na_robot_kicker#7014: Also checkpoint size doesn't seem to necessarily be the amount of vram it uses, but I could be mistaken\npnuts#1013: any idea how it is split up, as I can run it on a 10gig card. I struggle running various other checkpoints due to their size.\na_robot_kicker#7014: I'm not sure but in my case it seems to have downsampled the model to float16\npnuts#1013: too busy messing around with stuff, should really read the code\na_robot_kicker#7014: It doesn't seem to use up my 10gb of vram&quot;}}},{&quot;rowIdx&quot;:90,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Haycoat#4808: Like if you have a spectrogram of your voice, you can convert it to the style of Avicii EDM with img2img implementation\nTwee#2335:  https://cdn.discordapp.com/attachments/1053081177772261386/1054806218834718791/Screenshot_2022-12-20_at_12.03.15_PM.png\npnuts#1013: you've launched the inference server too?\nTwee#2335: well i mean thats why i asked about the inference server too lol\nTwee#2335: i was wondering if it was a necessary download\npnuts#1013: clone this fella <https://github.com/riffusion/riffusion-inference>\npnuts#1013: the first part you completed it for the front-end only\npnuts#1013: it's the inference-server that will generate your images\nnullerror#1387: there’s a smaller version of the model available\nnullerror#1387: as a chkpt file on hugface\nnullerror#1387: 4gb\nnullerror#1387: or so\nTwee#2335: cool https://cdn.discordapp.com/attachments/1053081177772261386/1054806906327285790/Screenshot_2022-12-20_at_12.05.36_PM.png\nTwee#2335: like i said\nTwee#2335: headache&quot;}}},{&quot;rowIdx&quot;:91,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;pnuts#1013: you ran 3 commands at once\nTwee#2335: i mean that usually tends to work lol\npnuts#1013: ```conda create --name riffusion-inference python=3.9\nconda activate riffusion-inference\npython -m pip install -r requirements.txt```\nTwee#2335: one command, then the other, then the other\nHaycoat#4808: What if we used the img2img part and use it in Riffusion? Then we can attempt a way to do audio2spec and use our own audio as a reference\npnuts#1013: I can't remember if I had to also install torch after this. give it a try and see what errors it throws when you try and launch it\na_robot_kicker#7014: In my case I had to separately install torch and cuda.\nTwee#2335: what step did i skipped this time lol https://cdn.discordapp.com/attachments/1053081177772261386/1054808178484838521/Screenshot_2022-12-20_at_12.10.55_PM.png\npnuts#1013: `conda install ffmpeg`\nTwee#2335: still getting the no audio backend error\nTwee#2335:  https://cdn.discordapp.com/attachments/1053081177772261386/1054808623445966918/Screenshot_2022-12-20_at_12.12.49_PM.png\nTwee#2335: do i have to install torch+cuda seperately or something\npnuts#1013: running through the steps again on my sie&quot;}}},{&quot;rowIdx&quot;:92,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;a_robot_kicker#7014: In my case that indicated needing to install torch\na_robot_kicker#7014: Specifically torch audio and cuda\npnuts#1013: `pip install --no-cache-dir --ignore-installed --force-reinstall --no-warn-conflicts torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116`\nHaycoat#4808: There's also a huggingface space for Riffusion already available! https://huggingface.co/spaces/fffiloni/spectrogram-to-music\npnuts#1013: yep, had to re-install this. but it's up and running now\npnuts#1013:  https://cdn.discordapp.com/attachments/1053081177772261386/1054810986021924914/Capture.PNG\nTwee#2335: its downloading a bunch of stuff now lol\npnuts#1013: will check in again shortly, off to grab some nom noms\nTwee#2335: laterrrr\nHaycoat#4808: My favorite artist to use is Avicii, hands down a bop to listen\nTwee#2335: lol https://cdn.discordapp.com/attachments/1053081177772261386/1054815748343738519/Screenshot_2022-12-20_at_12.41.06_PM.png\nTwee#2335: ngl i liked my results better with automatic1111's webui settings\nTwee#2335: the web app is not exactly versatile lol\npnuts#1013: it gives you continuous playback, and you can add your seed images.\nSemper#0669: For the riffusion manipulation tool how do I execute the command on all the files in a folder instead of one by one? the command is:&quot;}}},{&quot;rowIdx&quot;:93,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\npython3 file2img.py -i INPUT_AUDIO.wav -o OUTPUT_FOLDER\n\nBut using /foldername/*.wav doesn’t work\nTwee#2335: i can add spectrograms into the web app?\npnuts#1013: yes, there's a seed image folder\npnuts#1013: <https://github.com/riffusion/riffusion-inference/tree/main/seed_images>\nTwee#2335: once i add a seed image, how do i access it within the web app?\npnuts#1013: https://localhost:3000/?&amp;prompt=brazilian+Forr%C3%B3+dance&amp;seed=51209&amp;denoising=0.75&amp;seedImageId=og_beat\npnuts#1013: replace `og_beat` at the end with your seed\npnuts#1013: I think I also had to edit something else and give it an initial seed\nhayk#0058: Thanks @Meatfucker ! I think there are several good ideas for smoothing between clips, some discussion here https://github.com/orgs/riffusion/discussions/18\nhayk#0058: I'm going to be adding in a streamlit app (because I know it best over gradio, etc) to riffusion-inference that does some of the common operations for riffusion like converting from audio, generation, interpolation, etc\nNikuson#6709: For those who find it difficult to trim an audio file to 5 seconds every time, I posted a repository with this script for quick and easy trimming\n&quot;}}},{&quot;rowIdx&quot;:94,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/nikuson/trimmed\nNikuson#6709: ChatGPT generated\nLAIONardo#4462: Thank you!\nnullerror#1387: doesnt the riffusion manipulation thing already do this for spectrograms?\nnullerror#1387: unless this is meant for smth else\nPhilpax#0001: hey there! apologies if this has already been asked, but is there any information on finetuning sd/riffusion on your own collection of tagged samples? I'd like to do conditional sound effect generation and am wondering if anyone's explored this yet\nMeatfucker#1381: I've seen some people mention it. It's a standard diffusion model on terms of training though I don't know it's initial tagging\nMeatfucker#1381: You should be able to convert some things into spectrograms and train it like any other model\nPhilpax#0001: aye, that's what I suspected - just wanted to make sure there wasn't any other kind of magic\nNikuson#6709: no, spectrograms are obtained with artifacts\nNikuson#6709:  https://cdn.discordapp.com/attachments/1053081177772261386/1054837140544036955/12bb35ac-1c9e-49ce-a576-2746ec474aeb.png\nHaycoat#4808: Could you try getting ChatGPT to make a GitHub for wav2spec?\nNikuson#6709: in what sense? diffusion manipulation has all the necessary tools for transformations\nHaycoat#4808: I mean like being able to upload an audio file and use it for Riffusion\nEdenoide#0166: Export audios as .WAV 16bit&quot;}}},{&quot;rowIdx&quot;:95,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Edenoide#0166: Is this Linux? Did someone been capable of make it run on windows?\ndenny#1553: The inference server runs fine on windows through conda\ndenny#1553: Haven't tried the front end but I suspect it's fine too\nEdenoide#0166: through conda you say? I'm gonna give it a try then\nEdenoide#0166: I had as lot of problems with transformerx, pydubs etc\nEdenoide#0166: I'm gonna try it. Thanks\nnullerror#1387: nokia on im confused what do you mean spectrogram are “obtained through artifacts”?\nnullerror#1387: nikuson\ndb0798#7460: The audio to spectrogram scripts from the Riffusion Manipulation Tools Github page work okay for me. Nikuson must be doing something wrong to get artefacts\nnullerror#1387: can report that it works for me as well. likely mp3 conversion or smth with the settings on their end\nnullerror#1387: my images don’t come out like that at least i mean\nEdenoide#0166: I'm trying to run Rifussion inference server on Windows using conda. I've installed ffmpeg via conda install -c conda-forge ffmpeg and soundfile (pip install soundfile) but a lot of errors appear when running the last step:\nEdenoide#0166:  https://cdn.discordapp.com/attachments/1053081177772261386/1054870021639262219/errors.PNG\nEdenoide#0166: Any idea? I feel like I'm almost there...\na_robot_kicker#7014: you do not have cuda and torch installed.&quot;}}},{&quot;rowIdx&quot;:96,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;db0798#7460: It looks more like torch is installed but Cuda isn't\nMeatfucker#1381: pytorch website has a little command configuration thing on it to build you a command\nMeatfucker#1381: https://pytorch.org/get-started/locally/\ndb0798#7460: If I remember it right, on my computer I first had to install Cuda from NVIDIA website and after that use the command from the https://pytorch.org/get-started/locally/ page. When I didn't install Cuda from the NVIDIA page first, I got the same error message that Ketedeneden got\nEdenoide#0166: Baaam! Yeah it seems a problem with cuda https://cdn.discordapp.com/attachments/1053081177772261386/1054874669024546886/cuda.PNG\nMeatfucker#1381: the program runs in an enviroment separate from your base system enviroment\nEdenoide#0166: I see! a virtual enviroment?\nMeatfucker#1381: yep\nMeatfucker#1381: I think conda in this case, not a venv\nEdenoide#0166: right! thanks\nMeatfucker#1381: so enter your conda enviroment, and then put in the command the pytorch website tells you\nMeatfucker#1381: and it should sort out your deps\nMeatfucker#1381: if its still funny make a fresh conda enviroment\nMeatfucker#1381: sometimes old deps can gunk up the works with conda\nNikuson#6709: I have observed the dataset of singing and now I will run fine tune stable diffusion&quot;}}},{&quot;rowIdx&quot;:97,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Nikuson#6709: it seems to me that only the sampler is trained in riffusion, which gives such poor quality\nmatteo101man#6162: Anyone know of any local mashup AIs?\nnullerror#1387: rave dot dj\nnullerror#1387: been around a while its okay\nhayk#0058: 🤘 Hey riffusers! 🤘 @here\n\n@seth  and I have been absolutely blown away by the response to our little hobby project. We had no idea if this approach would even work, and to see musicians and tinkerers building on top of it and making fun sounds is a dream.\n\nWe’re still trying to keep up with everything, but a few fun notes:\n\n+ riffusion.com has been visited over a million times in the past few days and generated about a year of unique audio.\n+ Our GPUs still can’t always keep up with requests, but they are getting close!\n+ We will soon add a streamlit app that demos some of the common use cases like interpolation, img2img, and some audio transformations.\n+ We are beginning to collaborate with LAION, the people behind the dataset that trained stable diffusion, to see how we can scale up.\n+ We’ll also make GitHub issues to track a bunch of the improvement ideas we have.&quot;}}},{&quot;rowIdx&quot;:98,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;+ Attached is an awesome sample created by producer Jamison Baken incorporating outputs from Riffusion.\n\nIf you’re a software eng or musician interested in being more directly involved, feel free to send us a DM. And everyone, thanks for being here! https://cdn.discordapp.com/attachments/1053081177772261386/1054910368960499742/mix.mp3\nEdenoide#0166: Great work!!!!\nCOMEHU#2094: Good luck, im sure this is the start of something bigger\nNikuson#6709: wrote in DM.\ncravinadventure#7884: Awesome post @hayk @seth !\n\nExcited to contribute to the improvement of Riffusion and really see it take off :redrocket: Crazy that the site has been visited over 1M times and has generated about 1 yr of audio.\n\n:partyBlob:  **FUN Discord Community Idea:** :partyBlob:\n- Host competitions where the community will be able to submit and *vote, by liking a post*, on the top 10 samples weekly.\n- Additionally, the prompt should be listed with each submission.\n\n**Competitions &amp; Leaderboards info:**&quot;}}},{&quot;rowIdx&quot;:99,&quot;cells&quot;:{&quot;data&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;- set up a new “***Competitions Channel***” which includes:\n- “***Weekly Top 10 Leaderboard***”\n- Top liked posts in 1 week. Ideally you should keep track of the weekly leaderboards so anyone could go back in time to look and see who won on any given week, in the past.\n- “***All-Time Top 10 Leaderboard***”\n- Top liked posts of all-time, from every weekly competition combined.\n\nI hope something like this will be implemented because I think it would be FUN for everyone &amp; keep users active. 🙂\nPhilpax#0001: is there an standalone application that can convert a riffusion spectrograph to audio? preferably command-line\nnullerror#1387: yes\nnullerror#1387: riffusion manipulator i think its called? ill grab the link\nnullerror#1387: https://github.com/chavinlo/riffusion-manipulation\nnullerror#1387: https://github.com/chavinlo/riffusion-manipulation\nnullerror#1387: manipulation\nnullerror#1387: almost had it\ncravinadventure#7884: wait so does that software allow you to basically feed in images as input to create audio?&quot;}}}],&quot;truncated&quot;:true},&quot;paginationData&quot;:{&quot;pageIndex&quot;:0,&quot;numItemsPerPage&quot;:100,&quot;numTotalItems&quot;:452471,&quot;offset&quot;:0,&quot;length&quot;:100}},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzA5OSwic3ViIjoiL2RhdGFzZXRzL2JyZWFkbGlja2VyNDUvZGlzY29yZF9kYXRhIiwiZXhwIjoxNzQyOTI2Njk5LCJpc3MiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvIn0.sVs-_MzgSg5ty30M4pL-zlZGgRsuMHZCpaUzVgLcyF68UuQS48RiqvpGsrQmT3RMmDGSOcaEaJ5n3ZAekBxrCA&quot;,&quot;displayUrls&quot;:true},&quot;dataset&quot;:&quot;breadlicker45/discord_data&quot;,&quot;isGated&quot;:false,&quot;isPrivate&quot;:false,&quot;hasParquetFormat&quot;:false,&quot;author&quot;:{&quot;_id&quot;:&quot;62b4b30f40e600c2272741f6&quot;,&quot;avatarUrl&quot;:&quot;/avatars/d86ca9ce3242be87bbfe541219ee2fac.svg&quot;,&quot;fullname&quot;:&quot;Matthew Mitton&quot;,&quot;name&quot;:&quot;breadlicker45&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:20},&quot;compact&quot;:true}"><div class="flex flex-col overflow-hidden shadow-xs mx-auto mb-10 rounded-lg border pt-2  px-2.5"><div class="mb-2 flex flex-wrap items-center gap-2"><div class="mr-auto flex items-center"><svg class="mr-1 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	<div class="whitespace-nowrap font-semibold">Dataset Viewer</div>
	</div>
				<a href="/datasets/breadlicker45/discord_data/tree/refs%2Fconvert%2Fparquet/default" class="group mr-1 text-xs text-gray-400 max-sm:hidden"><svg class="text-[.6rem] mr-1 inline -translate-y-px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill="currentColor" d="M12 10H6.78A11 11 0 0 1 27 16h2A13 13 0 0 0 6 7.68V4H4v8h8zm8 12h5.22A11 11 0 0 1 5 16H3a13 13 0 0 0 23 8.32V28h2v-8h-8z"></path></svg>
						<span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Auto-converted</span> to Parquet
					</a>
				<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>API</button>
					<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path d="M9.80603 2.86737H3.56107C3.37704 2.86737 3.20055 2.94048 3.07042 3.0706C2.94029 3.20073 2.86719 3.37723 2.86719 3.56126V9.80622C2.86719 9.99025 2.94029 10.1667 3.07042 10.2969C3.20055 10.427 3.37704 10.5001 3.56107 10.5001H9.80603C9.99006 10.5001 10.1666 10.427 10.2967 10.2969C10.4268 10.1667 10.4999 9.99025 10.4999 9.80622V3.56126C10.4999 3.37723 10.4268 3.20073 10.2967 3.0706C10.1666 2.94048 9.99006 2.86737 9.80603 2.86737Z" fill="currentColor" fill-opacity="0.3"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M2.40942 1.66191C2.05175 1.66191 1.7618 1.95186 1.7618 2.30953V6.76191H1.43799V2.30953C1.43799 1.77303 1.87291 1.3381 2.40942 1.3381H6.45704V1.66191H2.40942Z" fill="currentColor"></path></svg>Embed</button>

					<button class="bg-linear-to-b shadow-xs flex items-center gap-1.5 rounded-full border from-white to-red-100/90 px-2 py-0.5 text-xs font-medium text-[#2D3648] transition-shadow hover:shadow-inner dark:from-gray-900 dark:to-red-800/30 dark:text-gray-100 dark:hover:shadow-inner dark:hover:shadow-red-800/30" ><svg class="h-3.5 w-3.5 text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
						<span>Data Studio</span></button></div>
		<div class="flex flex-1 flex-col overflow-hidden -mx-2.5"><div class="flex flex-1 flex-col overflow-hidden"><div class="flex min-h-0 flex-1"><div class="flex flex-1 flex-col overflow-hidden"><div class="md:-mx-2.5 flex min-w-0 flex-wrap border-t"><div class="flex min-w-0 flex-1 flex-wrap"><div class="grid flex-1 grid-cols-1 overflow-hidden text-sm md:grid-cols-2 md:place-content-center sm:mx-2.5"><label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-950 md:border-r md:border-r-0 hidden" title="default"><span class="text-gray-500">Subset (1)</span>
			<div class="flex items-center whitespace-nowrap"><span class="truncate">default</span>
				<span class="mx-2 text-gray-500">·</span>
					<span class="text-gray-500">452k rows</span>
				<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
			<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Subset (1)"><option value="default" selected>default (452k rows)</option></optgroup></select></label>
		<label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-900 md:border-r md:border-r" title="train"><div class="text-gray-500">Split (1)</div>
				<div class="flex items-center overflow-hidden whitespace-nowrap"><span class="truncate">train</span>
					<span class="mx-2 text-gray-500">·</span>
						<span class="text-gray-500">452k rows</span>
					<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
				<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Split (1)"><option value="train" selected>train (452k rows)</option></optgroup></select></label></div></div>
								</div>

							<div class="flex min-h-0 flex-1 flex-col ">
	<div class="bg-linear-to-r text-smd relative flex items-center dark:border-gray-900 dark:bg-gray-950 false border-t [&amp;:has(:focus)]:from-gray-50 [&amp;:has(:focus)]:to-transparent [&amp;:has(:focus)]:to-20% dark:[&amp;:has(:focus)]:from-gray-900"><form class="flex-1"><svg class="absolute left-3 top-1/2 transform -translate-y-1/2 pointer-events-none text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				<input disabled class="outline-hidden h-9 w-full border-none bg-transparent px-1 pl-9 pr-3 placeholder:text-gray-400 " placeholder="Search this dataset" dir="auto"></form>
			<div class="flex items-center gap-2 px-2 py-1"><button type="button" class="hover:bg-yellow-200/70 flex items-center gap-1 rounded-md border border-yellow-200 bg-yellow-100 pl-0.5 pr-1 text-[.8rem] leading-normal text-gray-700 dark:border-orange-500/25 dark:bg-orange-500/20 dark:text-gray-300 dark:hover:brightness-110 hidden"><div class="rounded-sm bg-yellow-300 px-1 font-mono text-[.7rem] font-bold text-black dark:bg-yellow-700 dark:text-gray-200">SQL
	</div>
	Console
</button></div></div>


<div class="flex flex-1 flex-col overflow-hidden min-h-64 border-t">
		

<div class="max-h-96 relative overflow-auto"><table class="w-full table-auto rounded-lg font-mono text-xs text-gray-900"><thead class="shadow-xs sticky left-0 right-0 top-0 z-1 bg-white align-top"><tr class="space-y-54 h-full min-w-fit divide-x border-b text-left"><th class="h-full max-w-sm p-2 text-left  relative w-full"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">data
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">14</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">24.3k</div>
			</div></div></div></div>
	
						</th></tr></thead>
			<tbody class="h-16 overflow-scroll"><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="0"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: You could have just redirected the welcome to a new channel yk
JL#1976: Let's keep all the discussions in this general chat, so #🙏︱welcome channel serves as a landing page and rules page if we get more users.
ZestyLemonade#1012: **Welcome to #💬︱general**
This is the start of the #💬︱general channel.
JL#1976: You learn something new every day 🙂
『ｋｏｍｏｒｅｂｉ』#3903: where's the language list for riffusion
『ｋｏｍｏｒｅｂｉ』#3903: i want to know what genres it supports
『ｋｏｍｏｒｅｂｉ』#3903: found it nvm
db0798#7460: Where is it then?
『ｋｏｍｏｒｅｂｉ』#3903: https://huggingface.co/riffusion/riffusion-model-v1/blob/main/tokenizer/vocab.json
『ｋｏｍｏｒｅｂｉ』#3903: here
db0798#7460: Thanks!
『ｋｏｍｏｒｅｂｉ』#3903: it has a lot of non-musical words though
doesn't exactly help
『ｋｏｍｏｒｅｂｉ』#3903: not too sure how it's supposed to work</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="1"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">db0798#7460: This is identical with the vocab.json of the Stable Diffusion 1.5 base model https://huggingface.co/runwayml/stable-diffusion-v1-5/raw/main/tokenizer/vocab.json
db0798#7460: I think the genre names probably come from somewhere else
dep#0002: @seth (sorry if too many pings) I am trying to make my own audio seed. Could the spectogram_from_waveform (from https://github.com/hmartiro/riffusion-inference/blob/6c99dba1c81b2126a2042712ab0c35d0668bd83c/riffusion/audio.py#L89) be used to transform a WAV tensor's (I'm guessing from torchaudio.load) into a spectogram object, and then do the reverse of spectrogram_from_image to basically have a custom seed?
dep#0002: I also see the following comment:
```
"""
Compute a spectrogram magnitude array from a spectrogram image.
TODO(hayk): Add image_from_spectrogram and call this out as the reverse.
"""
```
I could try doing it
alfredw#2036: Can we make it 10x better soon?
dep#0002: I was looking into converting it to TensorRT with Volta but it seems it has 1 more layer
『ｋｏｍｏｒｅｂｉ』#3903: could we send some songs of different genres so that the ai can generate a wider array of genres?
i have a decent amount of obscure genres in my playlist</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="2"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">alfredw#2036: what's the training set?
『ｋｏｍｏｒｅｂｉ』#3903: vaporwave, chopped and screwed, free folk, experimental rock, art pop, etc?
『ｋｏｍｏｒｅｂｉ』#3903: don't think the ai knows those genres too well
dep#0002: gptchat:
```python
def image_from_spectrogram(spectrogram: np.ndarray, max_volume: float = 50, power_for_image: float = 0.25) -> Image.Image:
"""
Compute a spectrogram image from a spectrogram magnitude array.
"""
# Reverse the power curve
data = np.power(spectrogram, power_for_image)

# Rescale to the range 0-255
data = data * 255 / max_volume
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="3"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># Invert
data = 255 - data

# Flip Y and add a single channel
data = data[::-1, :, None]

# Convert to an image
return Image.fromarray(data.astype(np.uint8))
```
dep#0002: anyways I will see what I can do
dep#0002: this is basically audio2audio
『ｋｏｍｏｒｅｂｉ』#3903: could we help increase the dataset
dep#0002: If they release the training code I will train it on the entirety of pandemic sound
db0798#7460: I would also like to have some way of adding things to the dataset
『ｋｏｍｏｒｅｂｉ』#3903: i'd train it on my playlist + more albums that i somewhat like</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="4"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Slynk#7009: omg I've been dying for something audio related to happen with all this AI hype.
『ｋｏｍｏｒｅｂｉ』#3903: so that way the ai can endlessly churn out music i like >:D
my playlist would probably be too smoll for it though
Thistle Cat#9883: Hi what's happened?
Paulinux#8579: What I'm doing wrongly? I have URL like this:
https://www.riffusion.com/?%20&amp;prompt=folk&amp;%20denoising=0.05&amp;%20seedImageId=agile
And sometimes this AI couldn't produce for me anything
db0798#7460: Try the Colab (https://colab.research.google.com/drive/1FhH3HlN8Ps_Pr9OR6Qcfbfz7utDvICl0?usp=sharing) instead, perhaps? Colab seems to work consistently
Paulinux#8579: OK, thanks
『ｋｏｍｏｒｅｂｉ』#3903: ooh wait i know what i'd do
i gather all the alternative/avant-garde genres i can muster, select some albums from those genres (with the genre names + other descriptions with them) and then i'd put those in the ai
dep#0002: overloaded
dep#0002: If anyone wants I can host a mirror
hayk#0058: We have this code and it's very simple, we just haven't added it to the inference repo
dep#0002: Could it be possible for you to send it here or push it to the repo?</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="5"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">hayk#0058: Yeah if you open an issue on github we will aim to get to it soon!
AmbientArtstyles#1406: Hey @ZestyLemonade, I'm writing an article about sound design (sfx for games/movies) and Riffusion, can I use your sentience.wav clip in it?
AmbientArtstyles#1406: I so want to collaborate on training the algorithm with my personal sound libraries. 🎚️   🎶
Thistle Cat#9883: Has the website been fixed?
JL#1976: Works for me.
Thistle Cat#9883: Nice!
Thistle Cat#9883: I will have to check it again tonight
Tekh#3634: How do I make a continuous stream of interconnected clips with the colab?
Tekh#3634: also, is there any way to change tempo and such like on the webapp using the colab?
dep#0002: thanks pls do so asap I cant wait
dep#0002: in the meantime im getting things like these
dep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053101779992186941/FINAL.png
Thistle Cat#9883: Anyone hearing a snapping noise when it goes to the next spectrogram?
yokento#6970: Can you seed riffusion with your own clip of audio?
April#5244: 14gb ckpt?</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="6"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: thats what I was trying
Jack Julian#8888: Crazy stuff yall
im a musician myself, and seeing this is both interesting and 'worrying'. Love how you thought this out and put it to works.
April#5244: was hoping to gen using automatic1111's sd webui and perhaps finetune the model using dreambooth. but I feel like I'm in a bit over my head lol
WereSloth#0312: you'll need to convert music to a spectrogram
WereSloth#0312: and then, yes, you should be able to dreambooth
db0798#7460: Is there a script for converting your own audio sample to the right kind of spectrogram already available anywhere? There seem to be functions that do this in the Riffusion codebase but I guess they don't work as a standalone script?
dep#0002: the image_from_spectogram function is missing
dep#0002: https://github.com/hmartiro/riffusion-inference/issues/9
dep#0002: supposedly they have it but they haven't added it yet
dep#0002: Maybe tomorrow it could be ready
lxe#0001: 👋
lxe#0001: Just wanted to stop by and say how awesome this thing is
lxe#0001: Wonder if something like deforum for it is in the works.
justinethanmathews#7521: this is very interesting. i am mostly in the "afraid of AI" camp, but music is a field I understand and I can see how interesting this is.</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="7"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
this might be a stupid question. but what was this trained on?
April#5244: I'm also curious about the dataset tbh
April#5244: also managed a small success: converting from wav file to spectrogram and back is working perfectly, and I have a working ckpt that can generate the spectrogram images. Next is to make a finetuning dataset and run it through dreambooth 🙂
dep#0002: Can you share your convertor?
April#5244: https://pastebin.com/raw/0ALzwee4
April#5244: just a word of warning @dep I have no idea what I'm doing and this code was generated with the help of an ai and my own tinkering. might have some serious stuff wrong with it lol
April#5244: I've only tested it on the generated 5-second wav files that are created from the sister script
April#5244: also trying to re-input the generated pics doesn't work right so I have to manually save in paint and then it works for some reason lol
April#5244: but from my testing it seems to work well enough
April#5244: currently seeing if I can get it to work from mp3 and clip like the first 5 seconds or something
dep#0002: time to train on the internet
dep#0002: 🥂
April#5244: okay so I think I got it working with mp3 so I threw a whole dang song in there and it generated an image but it's much wider in resolution, and cropping it down just results in junk lol
April#5244: might have to limit it to 5 seconds</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="8"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">April#5244: ```
def spectrogram_image_from_mp3(mp3_bytes: io.BytesIO, max_volume: float = 50, power_for_image: float = 0.25) -> Image.Image:
"""
Generate a spectrogram image from an MP3 file.
"""
# Load MP3 file into AudioSegment object
audio = pydub.AudioSegment.from_mp3(mp3_bytes)

# Convert to mono and set frame rate
audio = audio.set_channels(1)
audio = audio.set_frame_rate(44100)

# Extract first 5 seconds of audio data
audio = audio[:5000]
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="9"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># Convert to WAV and save as BytesIO object
wav_bytes = io.BytesIO()
audio.export(wav_bytes, format="wav")
wav_bytes.seek(0)

# Generate spectrogram image from WAV file
return spectrogram_image_from_wav(wav_bytes, max_volume=max_volume, power_for_image=power_for_image)
```
```
# Open MP3 file
with open('music.mp3', 'rb') as f:
mp3_bytes = io.BytesIO(f.read())

# Generate spectrogram image
image = spectrogram_image_from_mp3(mp3_bytes)</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="10"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
# Save image to file
image.save('restoredinput.png')
```
April#5244: add this function and those lines at the bottom for mp3 and cutting first 5 seconds, seems to work great
a_robot_kicker#7014: any idea what could be happening here?
```
ERROR:server:Exception on /run_inference/ [POST]
Traceback (most recent call last):
File "C:\Users\matth\miniconda3\envs\ldm\lib\site-packages\flask\app.py", line 2525, in wsgi_app
response = self.full_dispatch_request()
File "C:\Users\matth\miniconda3\envs\ldm\lib\site-packages\flask\app.py", line 1822, in full_dispatch_request
rv = self.handle_user_exception(e)
File "C:\Users\matth\miniconda3\envs\ldm\lib\site-packages\flask_cors\extension.py", line 165, in wrapped_function
return cors_after_request(app.make_response(f(*args, **kwargs)))</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="11"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">File "C:\Users\matth\miniconda3\envs\ldm\lib\site-packages\flask\app.py", line 1820, in full_dispatch_request
rv = self.dispatch_request()
File "C:\Users\matth\miniconda3\envs\ldm\lib\site-packages\flask\app.py", line 1796, in dispatch_request
return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
File "C:\Users\matth\Documents\StableDiffusion\riffusion-inference\riffusion\server.py", line 147, in run_inference
response = compute(inputs)
File "C:\Users\matth\Documents\StableDiffusion\riffusion-inference\riffusion\server.py", line 177, in compute
image = MODEL.riffuse(inputs, init_image=init_image, mask_image=mask_image)
... (lots of lines here)
attn_output = torch.bmm(attn_probs, value_states)
RuntimeError: expected scalar type Half but found Float
```
a_robot_kicker#7014: ```
File "C:\Users\matth\Documents\StableDiffusion\riffusion-inference\riffusion\prompt_weighting.py", line 229, in get_unweighted_text_embeddings
text_embeddings = pipe.text_encoder(text_input)[0]</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="12"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">```
April#5244: this is the img2wav script I'm using https://cdn.discordapp.com/attachments/1053081177772261386/1053166079804981268/audio.py
April#5244: I'm actually not using any other code lol
April#5244: so idk why/how to fix any issues with the riffusion ui stuff
dep#0002: What error did you got
dep#0002: I didnt got any errors but it took longer because it was larger than 512by512
dep#0002: after resizing it it worked as normal
dep#0002: however I can barely hear the see
dep#0002: I had to set denoising to 0.01 to actually remember the tempo
dep#0002: Anyways
dep#0002: I have an A100 so I will see if I can finetune it
April#5244: I actually fixed the error by editing the img2wav script lol
April#5244: one of the things had an extra parameter for whatever reason which was messing it up
April#5244:  https://cdn.discordapp.com/attachments/1053081177772261386/1053175675709837412/output.wav,https://cdn.discordapp.com/attachments/1053081177772261386/1053175675986649158/clip.wav,https://cdn.discordapp.com/attachments/1053081177772261386/1053175676334788689/outputspectro.png
April#5244: example conversion</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="13"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">April#5244: "clip.wav" is the 5 second clip from the original mp3 that's used for conversion. the image is the converted spectrum from the mp3. and output.wav is the reconverted song from the image
April#5244: scripts used https://cdn.discordapp.com/attachments/1053081177772261386/1053176008141963364/audio2spectro.py,https://cdn.discordapp.com/attachments/1053081177772261386/1053176008590770236/audio.py
April#5244: notably the script doesn't clip the first 5 seconds, but rather the next 5 after that
April#5244: since I wanted to avoid the sometimes slow intro that songs have lol
April#5244: still need to do some more work on the scripts before I can have it auto-generate some dataset images properly @.@
April#5244: might actually need to fetch later in the songs lol 🤔
April#5244: I noticed it still distorts the sound a bit...
April#5244: comparing: clip is the cropped audio, output.wav is the audio->img->audio convert https://cdn.discordapp.com/attachments/1053081177772261386/1053177485497475113/output.wav,https://cdn.discordapp.com/attachments/1053081177772261386/1053177485862387712/clip.wav
dep#0002: WoohH!!!!!
dep#0002: I dont know how to thank you
dep#0002: and its just been a day Lol
dep#0002: @JL may I suggest some emojis
April#5244: I'm sure someone smarter than me can figure out how to fix it lol
JL#1976: Yes, let me know if you have any cool ideas for the channel
dep#0002: Can I dm you the stickers and emojis</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="14"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: I prob will also make it into a bot
dep#0002: (riffusion)
dep#0002: although I've also heard that another dev is also working on one
dep#0002: u know lopho from sail?
dep#0002: the decentralized training server
dep#0002: I might ask him tomorrow
dep#0002: he knows a lot about this stuff
dep#0002: he rewrote the bucketing code himself lol
dep#0002: yet haru didnt merged it and he deleted it
JL#1976: Yup
dep#0002: hm.... I dont think we should mess with the channels....
dep#0002: but its worth the attempt
April#5244: 🤷‍♀️ honestly most of this code is ai generated. I don't know anything about music lol. removing that line of code seems to break the conversion entirely
April#5244: looking into the actual conversion a bit more it seems like the sample rate is getting changed which may be why the quality is decreasing 🤔
dep#0002: = ( https://cdn.discordapp.com/attachments/1053081177772261386/1053183348715032576/recompiled.mp3</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="15"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: og https://cdn.discordapp.com/attachments/1053081177772261386/1053183641699753984/invadercrop.wav
dep#0002: fixed https://cdn.discordapp.com/attachments/1053081177772261386/1053184852154916894/recompiled.mp3
April#5244: got it pretty close https://cdn.discordapp.com/attachments/1053081177772261386/1053189308154122391/reconstructed.wav,https://cdn.discordapp.com/attachments/1053081177772261386/1053189308531613707/clip.wav
April#5244: there's some clipping though 🤔
April#5244: basically just change max volume to 80 on both scripts to get this result
dep#0002: @April My image is on 512x501, is there anyway to fix that?
dep#0002: or just resize in paint
dep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053191450768187392/agile.png
April#5244: > audio = audio[:5119]
April#5244: when you're doing converting
April#5244: the size of the image is based on the length of the audio
April#5244: clipping it to 5119 seems to work
April#5244: I'm currently using this to get the middle of the song:
> audio = audio[int(len(audio)/2):int(len(audio)/2)+5119]
dep#0002: kay I will investigae more tomorrow</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="16"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: maybe the first finetune
April#5244: I wonder if there's a way to just have the whole song in the image 🤔
April#5244: I guess it'd have to be a larger image...
vai#0872: any way to fine tune this model?
Milano#2460: hi All! are you aware of https://www.isik.dev/posts/Technoset.html ?
Milano#2460: Technoset is a data-set of 90,933 electronic music loops, totalling around 50 hours. Each loop has a length of 1.827-seconds and is at 128bpm. The loops are from 10,000 separate electronic music tracks.
Milano#2460: I'm wondering how best to preprocess it.
db0798#7460: I think this one is pretty cool actually, sounds like Autechre
jacobresch#3699: here's an extension for auto1111 which automatically converts the images to audio again 🙂
https://github.com/enlyth/sd-webui-riffusion
JL#1976: Pinned a message.
HD#1311: this is exactly the kind of plugin I was looking for
HD#1311: thanks
HD#1311: I'll report if it works when I get home from work
April#5244: Worked for me but it messed up my sd install and python because it tried to install pytorch audio which I already had</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="17"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">JeniaJitsev#1332: Great work, folks, very impressive! I am scientific lead and co-founder of LAION, datasets of which are used to train original image based stable diffusion. Very nice to see such a cool twist for getting spectrogram based training running. We would be very much interested to cooperate on that and scale it further up - just join our LAION discord : https://discord.gg/we4DaujH
JL#1976: Welcome, happy to see you here!
HD#1311: just tested it out and it works
HD#1311: fun stuff
pnuts#1013: 👋
dep#0002: So, me an lopho have been experimenting with converting audio to spectogram images
dep#0002: @April we got better results by increasing the n_mels but it would prob not be compatible with the current model
dep#0002: 512 (original) https://cdn.discordapp.com/attachments/1053081177772261386/1053353723319038012/invadercrop_nmels_512.png,https://cdn.discordapp.com/attachments/1053081177772261386/1053353723826552852/invader_nmels_512.wav
dep#0002: 768 https://cdn.discordapp.com/attachments/1053081177772261386/1053353776838365224/invadercrop_nmels_768.png,https://cdn.discordapp.com/attachments/1053081177772261386/1053353777136148602/invader_nmels_768.wav
dep#0002: 1024 https://cdn.discordapp.com/attachments/1053081177772261386/1053353811399409724/invadercrop_nmels_1024.png,https://cdn.discordapp.com/attachments/1053081177772261386/1053353811714002996/invader_nmels_1024.wav
dep#0002: original (nothing) https://cdn.discordapp.com/attachments/1053081177772261386/1053353879825305650/invadercrop.wav
dep#0002: you will probably need good headphones to hear the difference
dep#0002: but you can hear one of the beats more clearly compared to 512
JL#1976: https://www.futurepedia.io/tool/riffusion Riffusion added to futerepedia.io
dep#0002: I'll be updating these tools here:</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="18"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/chavinlo/riffusion-manipulation
JL#1976: Let's create a post and get that pinned.
JL#1976: **Official website:**
https://riffusion.com/

**Technical explanation:**
https://www.riffusion.com/about

**Riffusion App Github:**
https://github.com/hmartiro/riffusion-app

**Riffusion Inference Server Github:
**https://github.com/hmartiro/riffusion-inference/

**Developers:**</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="19"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">@seth
@hayk

**HackerNews thread:**
https://news.ycombinator.com/item?id=33999162

**Subreddit:**
https://reddit.com/r/riffusion

**Riffusion manipulation tools from @dep :**
https://github.com/chavinlo/riffusion-manipulation

**Riffusion extension for AUTOMATIC1111 Web UI**:
https://github.com/enlyth/sd-webui-riffusion
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="20"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">**Notebook:**
https://colab.research.google.com/gist/mdc202002/411d8077c3c5bd34d7c9bf244a1c240e/riffusion_music2music.ipynb
**
Huggingface Riffusion demo:**
https://huggingface.co/spaces/anzorq/riffusion-demo

(pm me if any new resources have to be added or there are any errors in current listings)
JL#1976: https://techcrunch.com/2022/12/15/try-riffusion-an-ai-model-that-composes-music-by-visualizing-it/
Wow, TechCrunch article!
Eclipstic#9066: hi
JL#1976: Hey
Eclipstic#9066: whats up
Eclipstic#9066: im messing around with riffusion now
Eclipstic#9066: it mostly doesnt follow my prompts
Eclipstic#9066: but sometimes it surprises me</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="21"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">JL#1976: Feel free to share the best stuff you get in #🤘︱share-riffs
Eclipstic#9066: i can only say this:


this is cool as hell
HD#1311: what's the song
dep#0002: https://www.youtube.com/watch?v=jezqbMVqcLk
dep#0002: I am messing with the script rn so I can train a whole model on him
dep#0002: another sample
dep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053381711276294234/planet_girl_rebuild_default.wav
dep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053381799297949777/planet_girl.png
joao_betelgeuse#0410: yoooooooo
JL#1976: Hey
Tivra#3760: Good evening
dep#0002: @seth have you considered using the 3 channels rather than just 1?</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="22"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: I've been discusing about it with another dev, and it could help to "channels for 24bit amp"
dep#0002: power 0.75 https://cdn.discordapp.com/attachments/1053081177772261386/1053421209599082496/planet_girl_rebuild.wav
dep#0002: power 0.1 (earrape) https://cdn.discordapp.com/attachments/1053081177772261386/1053421333830180914/power01.wav
dep#0002: power 0.4 https://cdn.discordapp.com/attachments/1053081177772261386/1053421463572586566/0.4.wav
dep#0002: power 0.3 https://cdn.discordapp.com/attachments/1053081177772261386/1053421536628977794/0.3.wav
dep#0002: 0.25 is the default and the best.
SuperSonicDiscord1#4751: Do any devs here know how to do a latent space walk from one seed image to another?
SuperSonicDiscord1#4751: `InferneceInput` implies that you can only have one seed image if you're useing the inference server.
WereSloth#0312: y'all have made it to the big time.  Softology has added Riffusion to Visions of Chaos.  :slothrofl:
April#5244: the problem is the resulting image *must* be 512x512 to work properly with stable diffusion. obviously different sized spectrograms would be ideal but that really isn't possible
『ｋｏｍｏｒｅｂｉ』#3903: question: how do you train/finetune the ai
i want to train it on ambient/drone music
so that it can churn out stuff i like
April#5244: just stick the model in dreambooth as usual. then feed it your spectrogam images in the same format
hayk#0058: one of my favorite reddit comments https://cdn.discordapp.com/attachments/1053081177772261386/1053461992024850462/reddit_comment.png</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="23"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">『ｋｏｍｏｒｅｂｉ』#3903: oh ok
how to convert images to spectrogram
also i have to import 5 second stuff right? nothing longer?
IgnizHerz#2097: https://github.com/chavinlo/riffusion-manipulation
April#5244: I posted code that does this earlier. though it seems a link to something better was just posted?
April#5244: and yes, has to be 5 second clips
『ｋｏｍｏｒｅｂｉ』#3903: ah ok
『ｋｏｍｏｒｅｂｉ』#3903: another dumb question
how do i put it in dreambooth
where do i even get dreambooth
IgnizHerz#2097: also @dep, it would be awesome if you set it where you can generate the images for the entire song automatically rather than manually getting every 5 seconds. I did this very poorly locally myself, but I'm sure people would appreciate it.
dep#0002: I will do this soon
dep#0002: The next version should grab the audio files, split it in chunks of 5 seconds, make a spectogram image for each chunk, and done
dep#0002: rn I am downloading some artists to train on
dep#0002: also, @hayk have you looked into the possibility of using an outpainting model, rather than img2img?</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="24"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: I can help you train one if needed
dep#0002: he just logged off .-.
IgnizHerz#2097: sweet, I did this with my very rusty python code. Only thing is dealing with getting to the end having like 2seconds or something leftover, guess you'll have to add empty noise.
dep#0002: yeah empty noise should do
April#5244: https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast-DreamBooth.ipynb
this is the colab I use for dreambooth
『ｋｏｍｏｒｅｂｉ』#3903: ok
Sheppy#4289: if someone sets up a eli5 local setup guide, DM me please, i feel dumb, but i don't know what i'm doing
dep#0002: eli5?
Manly P Hall#3191: acronym for **e**xplain **l**ike **i**'m **5**
dep#0002: a
Sheppy#4289: yeah, my b
dep#0002: well its not that hard
dep#0002: just follow the instructions lel
dep#0002: install the latest version of nvm, then install npm and node, then execute the inference server first, and then the webui</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="25"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: if you need help I can join the vc and explain you in depth
oliveoil2222#2222: and thats all in command line right
dep#0002: yes
dep#0002: (unfortunately)
oliveoil2222#2222: yup
hayk#0058: I haven't tried it beyond a bit in the automatic1111 ui. Sometimes the longer results got repetitive, but there's a lot to play with there!
oliveoil2222#2222: ah i was lost on what node version manager was now we're in business
dep#0002: yeah, the nextjs version that the webui uses is only supported by node 16+
dep#0002: Heres a script to extract the tags of a song based off the filename from last.fm https://cdn.discordapp.com/attachments/1053081177772261386/1053518335624626226/message.txt
dep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053520950714454067/image.png
dep#0002: @April btw what did you used on auto webui
April#5244: I just converted the model file to a ckpt (the included one was 14gb for some reason so I didn't bother with it) and just used the converted ckpt in webui and it worked fine 🤷‍♀️
dep#0002: what prompts
April#5244: ? for what exactly?
dep#0002: like</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="26"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: to generate
dep#0002: the images
dep#0002: in the webui
dep#0002: @April
April#5244: whatever you want it to generate?
dep#0002: You used Automatic's webUI to generate spectograms, right?
April#5244: yeah
IgnizHerz#2097: usually the prompt depends on what you intend to get the result as, jazz for jazz and such
April#5244: ^
IgnizHerz#2097: the model is unusually good at jazz
dep#0002: simple as that?
April#5244: yeah
dep#0002: like if I want something electro, just type the same as the riffusion webui?
dep#0002: "Electro Pop"
April#5244: yes. just make sure you're using the riffusion model</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="27"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: no "Spectogram of Electro Pop" then
IgnizHerz#2097: no written rules, can run the same seed with different prompts to see what does best
April#5244: no. riffusion generates spectrograms by default
IgnizHerz#2097: but uh it generates without having to say it yeah
IgnizHerz#2097: I imagine this fancy interpolation stuff would also help me effectively audio2audio style transfer
IgnizHerz#2097: If I do each sections under the same seed they still have a bit of disconnect that makes them jumpy (im just using the auto webui)
noop_noob#0479: Does anybody know what training data was used for riffusion?
dep#0002: @April btw do you think your dreambooth finetune worked?
April#5244: it was decent I guess? Not really that great lol. The background music kinda just became a mess, though it seemed to get her vocals okay
April#5244: might need more data
matteo101man#6162: how would we go about generating things longer than 5 seconds?
April#5244: or perhaps just more proper labeling with terms or something 🤔
April#5244: the actual riffusion inference software should be able to do it which I am literally just starting to look into. can't seem to get it running :\
April#5244: basically what you need to do is generate a couple of clips and then generate the interpolation between them to "extend" the music
April#5244: auto1111 won't be able to do it I think</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="28"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">matteo101man#6162: Ah I see, thanks
matteo101man#6162: Shoot let me know if you figure it out
matteo101man#6162: they do have interpolating prompt scripts and seed travel but I wouldn't know if that would work or if you could somehow frankenstein that with the webui riffusion thing
April#5244:  https://cdn.discordapp.com/attachments/1053081177772261386/1053542594996617216/image.png
matteo101man#6162: I'm pretty new to all of it and I know zero python so 🤷‍♂️
April#5244: basically we need to generate interpolation pics between two seeds, convert all the clips into audio, and stick all the audio together
April#5244: auto1111 definitely can't do the convert to audio (it can, with an extension, but it's very limited). and afaik there's no way to do interpolation in there either?
April#5244: is there a script for that?
April#5244: I might try to do it manually
April#5244: the riffusion software is throwing errors :\
dep#0002: theres a func for it on the inference server
matteo101man#6162: yeah they have two things for what you're talking about
matteo101man#6162: https://github.com/EugeoSynthesisThirtyTwo/prompt-interpolation-script-for-sd-webui
https://github.com/yownas/seed_travel
April#5244: interesting, I'll have to try it</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="29"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: soon https://cdn.discordapp.com/attachments/1053081177772261386/1053554055764512878/image.png
dep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053554170227064893/Snail_s_House____waiting_for_you_in_snowing_city._chunk_2.png
April#5244: testing seed travel https://cdn.discordapp.com/attachments/1053081177772261386/1053554728098865152/output.mp3
April#5244: this was with 5 steps
April#5244: ```
img = Image.open("test/00000.png")
wav0 = wav_bytes_from_spectrogram_image(img)
sound0 = pydub.AudioSegment.from_wav(wav0[0])
img = Image.open("test/00001.png")
wav1 = wav_bytes_from_spectrogram_image(img)
sound1 = pydub.AudioSegment.from_wav(wav1[0])
img = Image.open("test/00002.png")
wav2 = wav_bytes_from_spectrogram_image(img)
sound2 = pydub.AudioSegment.from_wav(wav2[0])
img = Image.open("test/00003.png")</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="30"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">wav3 = wav_bytes_from_spectrogram_image(img)
sound3 = pydub.AudioSegment.from_wav(wav3[0])
img = Image.open("test/00004.png")
wav4 = wav_bytes_from_spectrogram_image(img)
sound4 = pydub.AudioSegment.from_wav(wav4[0])

sound = sound0+sound1+sound2+sound3+sound4
mp3_bytes = io.BytesIO()
sound.export(mp3_bytes, format="mp3")
mp3_bytes.seek(0)
with open("test/output.mp3", "wb") as outfile:
outfile.write(mp3_bytes.getbuffer())
```
lazy code lmao
matteo101man#6162: kinda sounds like a scratched up vinyl that keeps skipping or a messed up cd lol but thats progress</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="31"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">April#5244: the issue is there's no smoothing between clips :\
April#5244: I imagine a higher amount of interpolating steps would smooth it out a bit, but it'd also make the song longer...
matteo101man#6162: what gpu do you have
April#5244: my gpu is bad lol. 1660ti
matteo101man#6162: dang
April#5244: I can do like one gen every 10-15 seconds
matteo101man#6162: i have a 3090
matteo101man#6162: i could try at more steps for you cause I'm curious about this as well but
matteo101man#6162: how did you go about the process
April#5244: threw the riffusion/dreamboothed model into webui and used the scripts you linked earlier to generate the interpolating/seed travel steps. that generates the various spectrogram images. after that I just used the code I just posted to convert them all into audio and append them together
matteo101man#6162: that actually makes sense
matteo101man#6162: let me make a bomb ass prompt first
April#5244: here's with the interpolation script between hatsuki yura and jazz lol https://cdn.discordapp.com/attachments/1053081177772261386/1053557262876151859/music.wav
April#5244: notable cuts
matteo101man#6162: can't tell which one is better lol</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="32"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">April#5244: warped sound is just due to low txt2img smapling steps I think
April#5244: I'm running 20 steps each since it's fast though it kinda gets the best results with like 70+
April#5244: also this interpolation script is nice since it stitches the spectrogram images together automatically so I can just throw that through the regular spectro->audio converter without bothering to stitch stuff together or mess with multiple files
matteo101man#6162: ah, so which one is better
matteo101man#6162: seed travel or prompt interpolation?
April#5244: they do different things lol
April#5244: seed travel you only get one prompt and it just generates between seeds
April#5244: prompt interpolation you can give it two prompts and it'll generate the in-between
matteo101man#6162: well just found out my seed travel script is broken anyway
April#5244: rip
April#5244: you can do the seed travel thing with the prompt interpolation one anyway I think by just putting the same prompt twice
April#5244: at least, I imagine that's how it works?
matteo101man#6162: guess ill find out
matteo101man#6162: generation is actually taking pretty long though at 20 steps with prompt interpolation
matteo101man#6162: just 4 images but taking much longer than usual</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="33"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">April#5244: tbh I don't know what settings riffusion uses by default 🤷‍♀️
matteo101man#6162: nah it's just interesting never really messed with the scripts i was talking about, just kinda downloaded them and read about their function
matteo101man#6162: ah isee why
matteo101man#6162: how do i execute this code within a certain directory
April#5244: ?
matteo101man#6162: your code
matteo101man#6162: i'm not very familiar with python
April#5244: this is to be used with the earlier spectro2audio code posted lol
April#5244: lemme get the whole script
matteo101man#6162: i only know LUA which is useless practically
April#5244:  https://cdn.discordapp.com/attachments/1053081177772261386/1053559417355903016/audio.py
April#5244: this is the script i'm using
April#5244: note the commented mess at the bottom
April#5244: ```
# image = spectrogram_image_from_mp3(mp3_bytes)</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="34"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">parser = argparse.ArgumentParser()
parser.add_argument("filename", help="the file to process")
args = parser.parse_args()

# The filename is stored in the `filename` attribute of the `args` object
filename = args.filename
img = Image.open(filename)
wav = wav_bytes_from_spectrogram_image(img)
write_bytesio_to_file("music.wav", wav[0])
```
This is the default that i have it as which lets you just run the script specifying which file to convert. comment this out and use the other stuff below it to grab multiple files and stitch together
April#5244: it's a mess since it's just for my personal use lol T_T
matteo101man#6162: right so essentially like
matteo101man#6162: paste the long script above
matteo101man#6162: and uncomment your stuff and paste it below</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="35"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">April#5244: here I fixed the commenting https://cdn.discordapp.com/attachments/1053081177772261386/1053559902334881823/audio.py
matteo101man#6162: uh, how do i define specific file names like if i have a folder of things named say "image (1)"
April#5244: just run this and have your files in the "test" folder next to it
April#5244: `Image.open("test/00004.png")`
specifies which file to open
matteo101man#6162: cool
April#5244: it's just hardcoded to use test/0000#.png files
April#5244: 0-4
April#5244: to make it more adaptable it'd require a bit of a rewrite lol
April#5244: as you can see it's just loading up each file, converting it to wav file data, converting that into pydub audio, and concatting them together, then writing as mp3
matteo101man#6162: right
matteo101man#6162: perhaps i'm a bit of a dummy
April#5244: outpainting test https://cdn.discordapp.com/attachments/1053081177772261386/1053562076926316554/music.wav
April#5244: everything past the 5s mark is outpainted
matteo101man#6162: Am I supposed to run it as a py file?</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="36"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">April#5244: yeah the script I posted is a python script 🙂
matteo101man#6162: right
matteo101man#6162: so say
matteo101man#6162: i took that code verbatim
matteo101man#6162: and put it into notepad++ and saved it as a .py and ran it in a folder with images from 00000 to 00019 and a test folder in that same folder
April#5244: it's hardcoded to grab test/00000.png through test/00004.png and stitch them together. any higher won't work without modifying the script lol.
dep#0002: https://github.com/chavinlo/riffusion-manipulation/blob/master/scraper/mass_a2i.py
April#5244: should be fairly simple to write a loop to loop through such files though
dep#0002: this converts a whole audio file into multiple imgs
dep#0002: its meant for a dataset but should be easy to edit
matteo101man#6162: yeah that's not the issue for me, took those extra ones out but just straight up running it causes nothing to happen; do you use a compiler or anything specific?
matteo101man#6162: I see I have idle and I got an unexpected indent, so perhaps that's an issue
April#5244: ? should just run it with python: `python audio.py`
April#5244: yeah if it's saying unexpected indent, the indentation somewhere is messed up
April#5244: since python is kinda strict about that</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="37"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">April#5244: make sure it's either all spaces or all tabs and not a mix lol
matteo101man#6162: yeah fixed tht
matteo101man#6162: that*
matteo101man#6162: now no module named numpy
matteo101man#6162: ill look into that
April#5244: pip install numpy 🙂
matteo101man#6162: yep
matteo101man#6162: and for PIL
matteo101man#6162: ?
April#5244: I think for pil it's pip install pillow
matteo101man#6162: holy cow
matteo101man#6162: I'm missing so many things lol
April#5244: ```
import io
import typing as T</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="38"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
import numpy as np
from PIL import Image
import pydub
from scipy.io import wavfile
import torch
import torchaudio
import argparse
```
April#5244: are the imports for the file
matteo101man#6162: halfway there
April#5244: torch, argparse, scipy, pydub, pillow, numpy
April#5244: I think io and typing are python defaults?
matteo101man#6162: installed a bunch of other things
matteo101man#6162: now i've got torch not compiled with cuda enabled which appears to mean ughh, something</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="39"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">matteo101man#6162: quite a lot of things
April#5244: this is why I do python stuff manually. I had torch installed already thanks to the whole auto webui stuff
April#5244: then again I actually code in python 🤷‍♀️
matteo101man#6162: yeah i'm definitely missing something
April#5244: https://pytorch.org/get-started/locally/
April#5244: pytorch has a weird install process to get it working with cuda
April#5244: I imagine this is why most people use auto installers for stuff lmao
matteo101man#6162: i'll let you know how it goes with that 😓
oliveoil2222#2222: man im so stubborn im just using the ckpt in a111 and throwing the spectrogram into img2audio.py 💀
matteo101man#6162: never thought installing something would be so hard
April#5244: this is pretty much what I've been doing though if you're just doing txt2img for the 5 second clips, keep in mind there's a webui script that can run the img2audio script automatically
oliveoil2222#2222: yeah i found it
db0798#7460: Earlier today I tried to install the same things too and got stuck in the same place, haven't resolved it
oliveoil2222#2222: anticipating easier stuff around the corner
matteo101man#6162: i installed a bunch of random stuff</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="40"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">matteo101man#6162: did not help lol
oliveoil2222#2222: would love to see if dall-e 2 style clip variations could happen but I guess spectrogram2spectrogram is the closest one can get for now
matteo101man#6162: i get frozen solve when trying to install cuda on anaconda then it gets stuck on solving environment
matteo101man#6162: im gonna come back to it in a few minutes ig
matteo101man#6162: Right
matteo101man#6162: just ran a perfect install of cuda
matteo101man#6162: same torch not compiled
matteo101man#6162: finally got it to work
matteo101man#6162: cost 20 gb though
db0798#7460: What did you have to do to get it to work?
matteo101man#6162: https://stackoverflow.com/questions/57238344/i-have-a-gpu-and-cuda-installed-in-windows-10-but-pytorchs-torch-cuda-is-availa
matteo101man#6162: do pip uninstall torch in cmd
matteo101man#6162: then
matteo101man#6162:  https://cdn.discordapp.com/attachments/1053081177772261386/1053601238438133760/image.png
matteo101man#6162: on this site choose those and take that command</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="41"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">db0798#7460: Thanks, I'll try this later
matteo101man#6162: @April after taking all the time just to get that darn thing working i see what you mean
matteo101man#6162: it doesnt really interpolate all that well
matteo101man#6162: also increasing to max steps doesn't really make it much better the skipping is still obvious
April#5244: Out painting seems to work for extending it tbh.
matteo101man#6162: what do you use to ~~outpaint~~? better yet how exactly are you implementing that
Lun-Sei#5355: ?
matteo101man#6162: like how are you converting the larger image into a music
matteo101man#6162: cause i just edited code and I made some frightening af stuff for 5:17AM
IDDQD#9118: Hello, is that colab demo similar to the demo at riffusion.com in a way that it's possible to edit settings as the riffusion goes on?
IDDQD#9118: yeah, no it's not
IDDQD#9118: there are no colabs etc, that would allow similar kinda flow as the demo at riffusiondotcom? Can't run locally 😦
Onusai#6441: ~~not a colab but you can run it locally &lt;https://github.com/hmartiro/riffusion-app>~~ mb misread
IDDQD#9118: Would be cool, but I quess no luck with a laptop gpu ;DD
Onusai#6441: oof yea i wouldnt count on it</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="42"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">IDDQD#9118: Yeah, thanks anyhow !
IDDQD#9118: can't wait for this to get optimized n stuff. Would like to ought that there's alot that can be done on that front
Jay#0152: https://colab.research.google.com/gist/mdc202002/411d8077c3c5bd34d7c9bf244a1c240e/riffusion_music2music.ipynb
Jay#0152: finally, enjoy everyone!
Jay#0152: i know i will 🙂
Jay#0152: i fixed bugs in original notebook, credit to original author
Jay#0152: (also is *very* configurable)
EgorV4X#6102: This is probably stupid, but what if you do a fine tune on speech / vocals and use label data as a prompt?
Jay#0152: Could someone plz pin this?
EgorV4X#6102: and also if you train on instruments with midi/musicxml in prompt?
EgorV4X#6102: I wonder if it will understand the meaning of this
EgorV4X#6102: xd https://cdn.discordapp.com/attachments/1053081177772261386/1053659363937620058/image.png
JL#1976: Pinned
dep#0002: the author of that colab needs to use commit specific raw gets to avoid these kind of things
cvtecvts#9059: Any musicians found any uses for Riffusion yet? So far, I've been able to chop some samples out of the generated clips, and some clips are ok inspiration for parts of songs.</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="43"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: I'm not a musician but I do tinker a lot with the models and tech

At this point you can do txt2img to attempt to generate some new seeds

You can also use img2img to generate other variants of the same "beat"(?) or tempo
dep#0002: I posted some on #🤘︱share-riffs
dep#0002: Using songs that you like, convert them to spectrogram and use them as a seed is way more pleasant imo
dep#0002: I've also tried fine-tuning it but so far no good changes, might try training an outpainting one soon, since the current method is just img2img and the chunks are distinguishable
Nikuson#6709: https://github.com/chavinlo/riffusion-manipulation I also can't figure out how to use it via pycharm
Nikuson#6709: Perhaps a banal question, but I'm not very good at pycharm either
head_robotics_AI#0742: Hello; when I tried to load/switch to a riffussion model in Automatic1111 web ui I got a loadin error ending in "AttributeError: 'NoneType' object has no attribute 'sd_model_checkpoint'"
Any suggestions for how I might explore a soution?
head_robotics_AI#0742: is a .yaml file needed for the models?
db0798#7460: Is the model that you are loading to Automatic1111 the 14 Gb file? There are other versions with smaller files that work for me, at least: https://www.reddit.com/r/riffusion/comments/znbo75/how_to_load_model_to_automatic1111/
dep#0002: what error u getting</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="44"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: never used pycharm btw
Nikuson#6709: I get absolutely nothing.  just the terminal outputs “python” in response and nothing happens
dep#0002: have you tried running it directly on bash or cli
dep#0002: because on every instance I have ran it I never got that error
Nikuson#6709: just nothing happens, I don't think it's a bug.
dep#0002: It doesn't prints something like "Original audio length"?
Nikuson#6709: how did you launch it?  I'm just afraid that if I work in the terminal of the operating system, I won't be able to integrate it into the diffusion stable in any way.
dep#0002: python3 file2img.py -i inputaudio.wav -o outputimg.png
dep#0002: its just python functions, even if the script doesn't works, you should be able to copy the functions into your own script
dep#0002: and then call them
Nikuson#6709: nothing at all, just the standard "python" response to a command like "python3...."
Nikuson#6709: yes i do it.  I also tried to convert Lady Gaga's song and it didn't work either 😆
dep#0002: Download this
dep#0002:  https://cdn.discordapp.com/attachments/1053081177772261386/1053788225128374292/message.txt
dep#0002: save it as file2audio.py</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="45"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: and try running it
dep#0002: tell me what you get
Nikuson#6709: D:\Python\StableVoice\venv\lib\site-packages\pydub\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
D:\Python\StableVoice\venv\lib\site-packages\torchaudio\backend\utils.py:62: UserWarning: No audio backend is available.
warnings.warn("No audio backend is available.")
Traceback (most recent call last):
File "D:/Python/StableVoice/img2audio.py", line 152, in &lt;module>
image = spectrogram_image_from_file(filename)
File "D:/Python/StableVoice/img2audio.py", line 122, in spectrogram_image_from_file
audio = pydub.AudioSegment.from_file(filename)
File "D:\Python\StableVoice\venv\lib\site-packages\pydub\audio_segment.py", line 723, in from_file
stdin_data = file.read()
AttributeError: 'NoneType' object has no attribute 'read'
The script is executing</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="46"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">About to call spectogram_image_from_file function
Loading Audio File

Process finished with exit code 1
dep#0002: you dont have ffmpeg installed
dep#0002: thats why
dep#0002: in what OS are you windows or linux
Nikuson#6709: Win
dep#0002: https://www.wikihow.com/Install-FFmpeg-on-Windows
dep#0002: a bit hard
ryan_helsing#7769: Hey @dep would you be willing to walk me through how to fine-tune on a custom data set?
dep#0002: sure but the results aren't that good... yet
dep#0002: wait
dep#0002: rn I cant explain step by step but I can give you a brief summary of the process
ryan_helsing#7769: That would be wonderful</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="47"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Nikuson#6709: nothing has changed after installation
Nikuson#6709: although it looks like windows doesn't see ffmpeg. Strange, I did everything according to the instructions
Nikuson#6709: I fully installed it and the operating system even began to see ffmpeg, but the error is the same
dep#0002: At this point I suggest you install WSL and run it from there
db0798#7460: The script takes input and output paths as command line arguments, are you providing it these arguments?
Nikuson#6709: It seems to me that this is an overly complicated setup process for this task. 🤕 I think everyone would be happy to see something like a colab laptop for this project.
April#5244: auto1111 webui has an outpainting script. I load up the model and use the script to outpaint like normal.
April#5244: I manually converted the model into a ckpt using separate tools, so it put it down to 4gb like normal. I saw someone upload already converted models at a 2gb and 4gb size but idk how well those work. I avoided the 14gb ckpt since I figured there's no way I'd be able to run it
Nikuson#6709: I do everything like on github
matteo101man#6162: @April how did you convert the larger outpainted image to an audio file
IgnizHerz#2097: what settings for this? when I tried I got nice garbage
April#5244: same script works fine 🤷‍♀️
April#5244: mostly default settings. denoise was set to like 0.75 I think? maybe higher? maskblur I set to like 20-40 or something to try and get it smoother. make sure you only extend to the right. I left fall off exponent and color variation as the defaults
April#5244: I posted an outpainting sample here
April#5244: since webui can only outpaint 256 pixels at a time, that means the outpainting only adds 2.56 seconds at once, so that's where your cuts are</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="48"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">IgnizHerz#2097: got an example of what your spectrogram looked like?
April#5244: example outpainting spectro + converted song https://cdn.discordapp.com/attachments/1053081177772261386/1053814603445960824/music.wav,https://cdn.discordapp.com/attachments/1053081177772261386/1053814603890561054/test.png
April#5244: this one I was deliberately going for a different prompt at the end
April#5244: you can see that for the first half or so it's smooth and fine, and only once I switch the prompt it differs
April#5244: jumping between prompts doesn't seem to work that well. too hard of an edge I think
IgnizHerz#2097: and this I presume is from a generated one to begin with?
IgnizHerz#2097: mine might not being doing well because I'm attempting an extend from a converted audio to graph
IgnizHerz#2097: you can tell pretty quickly where it swaps https://cdn.discordapp.com/attachments/1053081177772261386/1053815712780668991/tmpjlpy8xou.png
April#5244: yeah, so this is just extending a txt2img song using the same model and prompt that generated it. so the result of the outpainting should be near identical which leads to a useful continuation
April#5244: trying to extend an existing song will lead to issues, because there's no guarantee the model *can* generate something similar
April#5244: I'd try adjusting your denoising value, increase mask blur
April#5244: maybe do a clip interrogation to try and get the tokens that most closely match the song
IgnizHerz#2097: true its probably struggling to get what my song is
Nikuson#6709: I still can't understand how you started it
IgnizHerz#2097: anything remotely brass band esq tends to be a lot older fashioned than the song I'm using</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="49"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Nikuson#6709: 😢
IgnizHerz#2097: like uh with the built-in clip interrogator?
April#5244: yeah
April#5244: ? how I started it?
IgnizHerz#2097: `a black and white photo of a square area with a pattern on it` not sure if it'd help to be fair
IgnizHerz#2097: something tells me every graph looks this way
April#5244: 🤷‍♀️
IgnizHerz#2097: also this from earlier, are you implying this script uses interpolation?
April#5244: no. that specific code just stitches together various spectrograms into a single audio clip
April#5244: the interpolation was done using a webui script
IgnizHerz#2097: I havent touched the actual install, I had enough install issues that I gave up
IgnizHerz#2097: I was thinking of hastily cramming code into the auto extension temp, or just wait I guess
IgnizHerz#2097: if the interpolation is even worth doing that, If it doesn't really mend that well, I could always just crossfade in an audio editor
April#5244: I posted seed travel and interpolation tests a bit earlier
April#5244: I found seed travel worked better in that it was less skipping. the interpolation test I tried skipped hard but I think that might've been due to not enough steps and a large difference between prompts</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="50"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">IgnizHerz#2097: I'd just like to keep the effective "style transfer" halfway consistent
IgnizHerz#2097: on the stuff I've been doing
IgnizHerz#2097: between two chunks of 5 seconds one will be quiet and then the next starts much louder
IgnizHerz#2097: harder to fix than popping or silence
matteo101man#6162: Ah maybe I’m using the wrong outpainting or something
matteo101man#6162: Guess ill check when I get home
matteo101man#6162: Does outpainting like extend it to the right there or what? Mine just makes a 768 image like an upscale
IgnizHerz#2097: in autos you can specify a direction and how much
April#5244: in the outpainting mk 2 script for auto1111 webui you can specify the direction to outpaint
Nikuson#6709: for some reason I just can't use https://github.com/chavinlo/riffusion-manipulation and I'm wondering how others installed it
April#5244: @Nikuson I'm using my own code I posted here
db0798#7460: That's kind of the problem that OpenAI Jukebox has too. OpenAI Jukebox can extend songs and the extended bits aren't completely unrelated to the original but they are still so random that the output sounds like a very drunken jam session rather than a song. It would be good if there was a way to make the output conform to some song structure, like verse-chorus-verse or something like that. But I don't know what exactly would need to be done to achieve that
April#5244: I can't imagine the approach riffusion is using will ever accomplish that. since it's just generating 5 second clips (or less) at once and it can't really "know" any larger structures
April#5244: you'd need something that could be trained on whole songs, and generates whole songs
April#5244: not 5 second clips</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="51"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">db0798#7460: I think a 5 second clip could be a seed for making a loop. In the output of OpenAI Jukebox, there are usually some 5 to 10 second bits that sound great when they are looped but instead of looping, OpenAI Jukebox moves on to something unrelated, and the incoherence makes the output sound bad. So just looping a bit would already be an improvement over a stream of randomness, I think. And then perhaps it would be possible to make variations to the loop by using something like image2image. Or chopping the loop up into smaller time slices and swapping the order of some slices, and duplicating some slices. When you have two 5 second loops, one could serve as the A section and the other as the B section in a larger structure
April#5244: you should be able to make good loops by generating an x-tileable image when diffusing
Nikuson#6709: Can you please provide the link again?
April#5244: https://discord.com/channels/1053034685590143047/1053081177772261386/1053559417808900096
db0798#7460: That seems like a good idea
Nikuson#6709: Do you only have audio conversion or also images?
April#5244: again already posted earlier: https://discord.com/channels/1053034685590143047/1053081177772261386/1053176008687230978
April#5244: these are just my personal scripts. I know there's some more clean/nicer scripts out there somewhere posted lol
IgnizHerz#2097: you could train an outpaint model just for riffusion couldn't you?
April#5244: I was just asked this in the "shinonome ai lab" server lol. Perhaps? I have no idea how well it'd go or how inpaint/outpaint models even work. I tried merging the inpainting 1.5 model with riffusion and it failed miserably
IgnizHerz#2097: obviously lol
April#5244: outpainting using regular riffusion/dreamboothed model seems to work fine
April#5244: even then though, outpainting won't actually solve the issue mentioned
IgnizHerz#2097: a particular outpaint would be able to mend new pieces much nicer
April#5244: since the length is always capped at 5.12s at most</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="52"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">IgnizHerz#2097: same way with images
Nikuson#6709: no, I really like them.  At least until I ran them and there were no problems 😅
IgnizHerz#2097: its how outpaint on images can keep a general idea and even add to them
IgnizHerz#2097: I mean you can just keep outpainting couldn't you
April#5244: I showed some outpainting examples earlier iirc
April#5244: it works well for consistency but it won't be able to create proper song structure
IgnizHerz#2097: music isn't too far from images
IgnizHerz#2097: in terms of blending and keeping consistency
IgnizHerz#2097: I think its moreso the outpaint not being so good at it on its own
April#5244: the problem is sd and thus riffusion is stuck with 512x512 images, meaning the model is trained on 5 second clips, which aren't enough to teach song structure
Nikuson#6709: I’ve just been writing various articles about all possible implementations of generative models with explanations for quite some time, and it became very interesting for me to release them at the end of the year.  something about audio.  Do you mind if your script gets on github and a couple of sites with articles about programming?😅 I'll try to point you out if possible
IgnizHerz#2097: right, if it was an outpaint model trained specifically to add new pieces based off of old ones
IgnizHerz#2097: wouldnt that work
April#5244: I post code with the assumption that people are gonna use it as they please. so go nuts 🙂
April#5244: just don't post any identifying info about me please</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="53"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">IgnizHerz#2097: praise the open sourcing
April#5244: I like my anonymity
April#5244: the code I posted is mostly ripped from riffusion and chatgpt anyway 🤷‍♀️
April#5244: I'd formally release it but it's a mess lol
April#5244: normally I try to keep proper releases to things that are actually nice 😂
Nikuson#6709: ok, it will remain a secret
Nikuson#6709: By the way, it's very interesting how you can use depth2img in audio
April#5244: ?
Nikuson#6709: I'm just wondering how the depth2img function will behave in stable diffusion 2 on spectrograms
Jay#0152: https://colab.research.google.com/github/thx-pw/riffusion-music2music-colab/blob/main/riffusion_music2music.ipynb
Jay#0152: Everything seems to be fixed by og author, enjoy
April#5244: If I'm understanding this correctly, this takes a song file, splits it up, runs img2img on each section, then stitches it back together?
April#5244: also one thing I've been thinking is that it'd be super helpful to have a list of captions used on the training dataset of riffusion, rather than just blindly guessing what's in there
db0798#7460: I recently watched this video from a music theory guy on Youtube that explains why just morphing a motif into something different over time doesn't work as well as having proper song structure: https://www.youtube.com/watch?v=8Z8zOLtgvgU&amp;ab_channel=RyanLeach
I think this depends on style, though: for some styles the outpainting approach would suit better than others</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="54"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">April#5244: with regular stable diffusion it's using the laion dataset and clip which has damn near everything you'd want. but what does riffusion have?
Jay#0152: yes you're correct.
April#5244: it's entirely possible to keep outpainting. however you can't guarantee consistency. outpainting only makes sure the connecting bit flows well, not that the entire structure works together
db0798#7460: Yes
IgnizHerz#2097: couldn't you train a model to specifically do this?
April#5244: if you use the same prompt and just keep outpainting, you get something workable but no overarching structure
IgnizHerz#2097: it improves outpainting on images to use the original image as a basis as well
April#5244: no. sd only works up to 512x512 images, which means that you're stuck at 5s length for "knowledge"
April#5244: you'd have to rework how sd diffusion works
April#5244: to allow wider images
Jay#0152: might not be very difficult, though
April#5244: for a regular 3min song you'd need a 3000x512 image I think
April#5244: actually. I think my math is off lol
April#5244: 5s = 512
IgnizHerz#2097: I figured for a proper outpaint model you'd use the original image as base anyways</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="55"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">April#5244: 18432x512
April#5244: no that's still wrong
April#5244: i'm dumb
April#5244: actually....
IgnizHerz#2097: which is applicable to music as it is to images. For both you'd want something that consists of new material but is not just random nonsense
April#5244: yes that's correct.
5s=512
180s/5s = 36
36*512 = 18432?
April#5244: might be easier to just convert a full song and look at size lol
April#5244: i'm dumb af
IgnizHerz#2097: though I'd imagine getting a song length of that size to work kek
Jay#0152: attempting style transfer w notebook to change song by the Beatles to one by the Doors lmao, wish me luck! 😆
April#5244: 24094x512 for a 4min song
April#5244: so yeah 18k is about right</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="56"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">April#5244: tldr: sticking the whole dang song into sd is unworkable
April#5244: even though that's what you'd need to do to get song structure
dep#0002: Not really, I mean sure SD 1.5. but SD 2.0 works at 768, and we already have aspect ratio trainers
April#5244: I suppose if you clip into like song sections like verses/chorus/whatever you could tag those individually, gen them using such tags, and manually stick together?
April#5244: though that'd still be large I think
April#5244: right, but 768 is already a huge jump that's computationally expensive, and that only nets you an extra 2 seconds
April#5244: as the math I just did shows, you'd need 18000x512 style images to do whole songs
April#5244: far larger than 768
April#5244: like 10x lmao
dep#0002: We might be able to apply tensorrt and get a big boost as well
dep#0002: But yeah, we are never going any higher than 2048
April#5244: tbh I think doing image diffusion on spectrograms is kinda a losing approach. computationally expensive for full length, very limited on smaller stuff, and spectrograms lose audio info
dep#0002: 1024 even
April#5244: exactly
April#5244: though one thing is that sd is square aspect ratio which we don't need for music</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="57"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">April#5244: 512 height is fine
April#5244: it's just width we need
Jay#0152: oh my god it kind of worked..... with default settings
dep#0002: About the loss, lopho and me were talking about inserting more data on the other 2 channels
Currently it uses 1 channel (B&amp;W) and replicates across the 2 other channels
dep#0002: The problem is that it would be even harder to recognize
April#5244: yeah I saw some talk about that earlier, using the full rgb
dep#0002: Ah wait
April#5244: I don't know literally anything about music lol
dep#0002: I was talking about width too xd
dep#0002: 512h is ok imo
April#5244: as it stands, sd trains on 1:1 ratio, so 768w = 768h, which just makes it hard to scale as scaling both makes it much more pixels and expensive
April#5244: being able to train on non-square aspect ratio would help a lot
April#5244: but even then...
dep#0002: Would?</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="58"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: Anyways
.....
April#5244: yeah so we don't need like 2048x2048 images, but rather 2048x512
dep#0002: It exists
April#5244: oh?
dep#0002: It's called bucketing
dep#0002: I just said it.....
April#5244: doing that would definitely help
April#5244: but still I think it gets expensive even past 2048x512
April#5244: or probably before that
dep#0002: Yeah
dep#0002: And also that the model would struggle at such extreme aspect ratios
April#5244: decided to run a test with regular sd stuff and gen a 2048x512 image just to see if I can even do it lol
April#5244: it's taking like 3min I think lmao
April#5244: 2048 would be 20s lmao nowhere close to a song</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="59"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">db0798#7460: I think just generating loops with Stable Diffusion and then stitching them together with some post-processing script would work better than trying to outpaint a whole song
April#5244: 5s loop isn't really that interesting to listen to though lol
April#5244: okay seems my laptop *is* able to gen a 2048x512 image 🙂
April#5244: took forever though @.@
db0798#7460: Yes, it would need variations. I think those could be created by inpainting bits of the loop, and rearranging time slices of the loop
April#5244: yup. though I feel like that quickly gets into "the human is making the music" territory, rather than the ai itself lol
Nikuson#6709: does anyone have any ideas to improve the sound quality?
db0798#7460: Yes, kind of. But if the post-processing is all done by a script instead of a human manually editing stuff, the result will still be fully computer-generated
April#5244: true. but if you're essentially hardcoding song structure, all the songs will end up sounding similar I think?
Nikuson#6709: one could immediately get audio from the image using a diffusion vocoder
db0798#7460: I guess depends on how complex the post-processing script is. A simple one would make similar-sounding output every time, a more complex one wouldn't
April#5244: true I suppose
April#5244: but I can't help but feel it's the same exact approach as making a chatbot by hardcoding in lines and responses
April#5244: like sure you technically get something workable, but it's really not a great solution
April#5244: 🤷‍♀️ I guess I'll just have to see if someone actually makes something like that</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="60"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Nikuson#6709: and in what variable does the untransformed spectrogram lie here?
April#5244: ?
db0798#7460: Yes, generating a song structure the way I described is definitely more primitive compared to getting a neural network to produce a song structure. But the neural network method seems to be more difficult to do
April#5244: it's definitely a hard problem to solve
db0798#7460: Yes, I'm just throwing ideas around here and won't promise to do this myself, as I have many other unrelated things that I'm already busy with
Nikuson#6709: it’s already night for me and today I don’t seem to have time to understand your code and I just wanted to know in advance which of the variables contains the spectrogram taken from the image, but not yet converted to audio
April#5244: the spectrogram is the input for that particular script. it converts the spectrogram image into audio. I have no idea how the actual inner workings of the script work. spectro->audio was provided by riffusion, and audio->spectro was ai generated based on the riffusion code
April#5244: I know literally nothing about how this tech works or anything about music so sorry to burst bubbles there 🙏
April#5244: if i had to guess "spectrogram_from_image" likely is what converts the image to spectrogram data in-code
Nikuson#6709: ok, I'll study this code closer to dinner
dep#0002: @seth Sorry to bother you, but how many steps was riffusion trained for? And at what batch size?
a_robot_kicker#7014: okay, I've been plunking away at this code for a while now and finally got it to work. No idea why but I had to reach deep into the transformers library and patch these three lines https://cdn.discordapp.com/attachments/1053081177772261386/1053874830933508129/image.png
a_robot_kicker#7014: somehow I was ending up with "attn_probs" being Float32, and "value_states" being float16
a_robot_kicker#7014: I can see that RiffusionPipeline defines its datatype as float16, so I'm not sure how attn_probs ended up being float32 🤷 https://cdn.discordapp.com/attachments/1053081177772261386/1053875188028153916/image.png
dep#0002: I got a similar error where tensors were mismatch but it was using raw diffusers</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="61"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">a_robot_kicker#7014: another change I ended up needing to do was converting the spectrogram image to float64, otherwise it would always overflow and produce nans https://cdn.discordapp.com/attachments/1053081177772261386/1053875449765318737/image.png
a_robot_kicker#7014: but, now I'm getting output spectrograms and waveforms! https://cdn.discordapp.com/attachments/1053081177772261386/1053875587392995368/image.png
a_robot_kicker#7014: will now learn how to save or play these. For learning purposes I took the guts out of the flask server and turned it into a command line interface.
a_robot_kicker#7014: okay, not too bad! now I've got wav files!! https://cdn.discordapp.com/attachments/1053081177772261386/1053876328438440056/image.png
a_robot_kicker#7014: wow, my first wav -- "Taylor Swift Beat Boxing" https://cdn.discordapp.com/attachments/1053081177772261386/1053877417183285268/output.wav
a_robot_kicker#7014: thanks, gonna have a lot of fun with this tool 🙂
joao_betelgeuse#0410: Based
matteo101man#6162: @April What exact script are you using to convert the longer outpainted sequences? like say something 896x512? img2audio?
April#5244: same script that I posted. works for any size spectrogram lol
matteo101man#6162: which one chief? there's a few I remember
matteo101man#6162: like I remember you showed me audio.py but that ones for 4+ images
April#5244: I didn't at all change the conversion script lol. so... any of them? the spectrogram->audio script I'm using is straight from riffusion
matteo101man#6162: chavinlo ones? like riffusion-manipulation? or you mean something else
matteo101man#6162: might be missing another maybe ill scroll up
matteo101man#6162: cause definitely don't mess with img2audio.py for anything more than as is 🤮</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="62"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">April#5244: https://discord.com/channels/1053034685590143047/1053081177772261386/1053559417808900096
https://discord.com/channels/1053034685590143047/1053081177772261386/1053176008687230978
April#5244: I'm sure the riffusion-manipulation one will work fine too
matteo101man#6162: oh no
April#5244: or you can just grab audio.py from riffusion lol
matteo101man#6162: audio2spectro.py was the thing i was missing though (edit: which has nothing to do with what I'm doing)
matteo101man#6162: but nah manipulation one makes horrendous noises
April#5244: you can mess around with the max_volume variable. it's normally set to 50 but you can tweak it
matteo101man#6162: just static doing that though
matteo101man#6162: I'm just gonna give up on it, I tried the other audio.py but there's a lot of variables I have to edit to actually convert the file
matteo101man#6162: and if it's not mathematically sound it throws errors
hayk#0058: Digging the prompts here: https://www.youtube.com/watch?v=BUBaHhDxkIc
matteo101man#6162: it will be nice when we can generate stuff of that quality straight from stable diffusion
BananaBot#3675: I’ve only tried a couple spaces, but I noticed that they typically do things in increments of time (/seconds). Wouldn’t it be more useful (since we’re dealing with music) to generate things by beat/bar and bpm? Or is that not possible?
Jonestown#8964: Just trying out Riffusion now.  This is pretty impressive.  I've been following Harmonai for a while and haven't seen anything too interesting come out of it yet, but Riffusion is a big step forward.</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="63"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">noop_noob#0479: @hayk @seth Sorry for the ping. May I know what the dataset was? Or if not, maybe at least what the text in the dataset looks like? I think that maybe knowing what the data looks like could lead to better prompts.
a_robot_kicker#7014: Yeah I can't find this info anywhere and it's critical to understand the limitations of this model and how it might be improved.
outhippo#4297: damn, even if it does not adhere to prompts that well its still some great music
outhippo#4297: Can someone explain where this spectogranm to audio script gets the "sounds" from meaning how the piano or drums sound?
outhippo#4297: i cant wrap my head around it
noop_noob#0479: The X axis of the spectogram is time. The Y axis is the frequency. At a specific time (i.e., in a single column of pixels), a specific timbre of sound (which, for example, distinguishes a piano from a violin) corresponds to a specific pattern of blacks/whites/grays. Placing this pattern higher or lower corresponds to a higher or lower pitch.
outhippo#4297: got it, but there are thousands of possible piano sounds - some are terrible, midi sounding and some are full and realistic. I wonder why the music ends up sounding pretty good in terms of sound selection and not fake. Maybe it's just that the frequencies correspond to how iyt should sound "exactly" unlike midi which just says how high or low the sound should be but not the quality of the sound.
noop_noob#0479: The different kinds of piano sounds correspond to slightly different timbres/patterns.
noop_noob#0479: something something fourier transform idk lol
outhippo#4297: got it, so if it was trained on music that is professionak sounding than it would retain the professional sound selection
noop_noob#0479: Probably, yeah. Maybe prompts might affect that too.
outhippo#4297: Thanks, I need to read more on spectograms too
noop_noob#0479: @outhippohttps://youtu.be/spUNpyF58BY
noop_noob#0479: https://www.reddit.com/r/StableDiffusion/comments/zoc365/new_riffusion_web_ui_realtime_music_generation_up/
Nikuson#6709: I don't know why, but pycharm refuses to see pydub even though I installed it</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="64"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Nikuson#6709: from constant problems, I can only come to the conclusion that pycharm is far from the best IDE
Twee#2335: damn this app is nuts
dep#0002: https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features
dep#0002: Might be really useful to use this for future finetuning
dep#0002: @Jonestown ????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
dep#0002: https://media.discordapp.net/attachments/710745951236522019/791617162364190730/image0-19.gif
Jonestown#8964: @dep Cat decided to play with a toy on the keyboard.  Sorry for spam lol
dep#0002: lol
Nikuson#6709: there is no thematic mood at all
Twee#2335: putting "shoegaze" on the prompt does not make shoegaze :(
Twee#2335: pain
Sheppy#4289: yeah, training it to recognize musical modes/keycenters/time signitures/tempos should probably be a high priority
Twee#2335: im more of a genre person
Twee#2335: would be great if adding an amalgamation of genres in a prompt could create an accurate fusion of those genres
Twee#2335: or simply adding a genre and recreate a song in the vein and sound of that genre</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="65"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Twee#2335: and im not talking about mere meta genres like pop, rock, jazz, hip hop, etc
Twee#2335: im talking about more specific subgenres and scenes
Sheppy#4289: that's important too
Twee#2335: like if i added "radiohead but vaporwave"
Sheppy#4289: lol
a_robot_kicker#7014: working on a simple tkinter local GUI, will make a github fork once it is useful https://cdn.discordapp.com/attachments/1053081177772261386/1054134363387858954/image.png
a_robot_kicker#7014: so far in terms of data representation, I have no idea which prompts will work. Seems to mostly only know about electronic music and a few big name artists
a_robot_kicker#7014: but the seed can have a huge effect
Twee#2335: nice!!
Thistle Cat#9883: https://youtu.be/nHuF927CgkM
Thistle Cat#9883: Idk why but my label is far more devious than this audiobook
noop_noob#0479: Huh https://fxtwitter.com/1cebell/status/1604267031238434819
noop_noob#0479: Found this from 4 years ago https://www.youtube.com/watch?v=YRb0XAnUpIk
pork#9581: can we use higher resolution generations to get better audio quality/longer clips?
quique#5458: hi, is there any colab available to generate smooth audio between two prompts which also allows us to grab and use the last spectogram image to generate further audio transitions? My idea is chaining several prompts transitions like "typing -> jazz piano -> guitar riff -> rock guitar solo"? or how would you do it manually?</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="66"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">IDDQD#9118: Neither does it do black metal :(((
XIVV#9579: or hardcore punk :((((
IDDQD#9118: Looking forwards to the future iterations of this riffusion. Also would gladly contribute if anyhow possible at some point (most likely via labelling etc. since I don't posses programming prowess). this immense.
IDDQD#9118: "an exhiliratingly epic anti-anthem of the dysregulatory purgatory in the style of psychedelic cosmic black metal vaporwave noise mumblerap with contemporary avantgarde classical and free jazz passages by greg rutkowski trending on artstation" when??!!
Twee#2335: post-avant jazzcore and progressive dreamfunk
Jack Julian#8888: https://everynoise.com
Twee#2335: everynoise is ok but tbh an actually accurate genre database is RateYourMusic
Twee#2335: sounds like ur average oranssi pazuzu album
Twee#2335: i did try "dark synthpop with ethereal female vocals" and it sounds eerily similar to early cocteau twins
Twee#2335: i was very impressed
Twee#2335: cant wait until im able to extend songs thru automatic1111's ui
cravinadventure#7884: Hi!

If anyone needs a Mix and Master of any sound for much better sound quality, please DM me!
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="67"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">*- **BEFORE:**     (Original_Riffusion_Output_Sound)*
*- **AFTER:**        (Mixed_Mastered_Sound)*

🙂 https://cdn.discordapp.com/attachments/1053081177772261386/1054429627482910832/Original_Riffusion_Output_Sound.wav,https://cdn.discordapp.com/attachments/1053081177772261386/1054429627810062436/Mixed_Mastered_Sound.wav
pnuts#1013: what's the benefit of running it via automatic1111? I'm using the web-app from the official repo atm
pnuts#1013: nvm, just installed it to take a look
Nikuson#6709: I can't solve the problem with pydub. It doesn't see ffmpeg even though I installed everything correctly
clambake#5510: if we can use transformers to turn words into music, could we generate an ai image from music
pnuts#1013: works pretty much out of the box here ```|0: e:                                              |
│1: cd git                                        │
│2: git clone https://github.com/jiaaro/pydub.git │
│3: cd pydub                                      │
│4: conda create -n pydub python=3.9 -y           │
│5: conda activate pydub                          │
│6: python setup.py build                         │</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="68"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">│7: python setup.py install                       │
│8: conda install ffmpeg                          │
│9: python whatsthis.py                              |```
pnuts#1013: `whatsthis.py` is  the first sample in the repo```import os
import glob
from pydub import AudioSegment

video_dir = './samples'  # Path where the videos are located
extension_list = ('*.mp4', '*.flv')

os.chdir(video_dir)
for extension in extension_list:
for video in glob.glob(extension):
mp3_filename = os.path.splitext(os.path.basename(video))[0] + '.mp3'
AudioSegment.from_file(video).export(mp3_filename, format='mp3')```</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="69"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">AgentA1cr#8430: does Riffusion support negative prompt weights?
AgentA1cr#8430: Also, loving what this model can do. However, it seems to me that, given enough time, it will slowly (or sometimes not-so-slowly) drift away from the prompt and start doing its own thing, with a strong preference for percussion and piano.
hayk#0058: Very nice! I'm super interested to automate parts of this into the riffusion-inference repo so it can generate higher quality. Are you doing this within a DAW? I'd love a breakdown of the steps so we can try to get it in code.

Somewhat related, if any audio experts have a lead on a neural vocoder to try that might perform better than Griffin Lim, that's worth exploring.
undefined#3382: I'd really be great if we could finetune the model (using EveryDream) ourselves too 🙂 but I haven't seen any info about that or I missed it
db0798#7460: My guess is that you have multiple versions of Python on your system and something you installed got installed for the wrong installation of Python
a_robot_kicker#7014: I've been working with a DAW as well and will post a little fork with my progress, probably later today. Biggest limitation I have with the tool so far IMO is the hard-coded 5s limit, makes it challenging to keep in sync with clips from the DAW. I'd like to eventually write a VST plugin or something to make interop a bit easier.
a_robot_kicker#7014: VST plugin would have to call some kind of API to a server running riffusion w the audio input it gets, and then async output that either to a file or the VST output buffer. A bit difficult because the VST api pretty strongly assumes realtime processing aplications.
j.ru.s#9349: Hey all, does anyone know what music dataset Riffusion uses to fine-tune on?
j.ru.s#9349: Making a universal vocoder is a really tricky problem, usually it needs to be specifically trained towards a specific type of audio output. In the case of Riffusion, if we know the music dataset used to do the fine tuning we could potentially train a neural vocoder on that.
Nikuson#6709: I thought so too and removed all versions except 3.9, but still does not determine
Nikuson#6709: i suggest to use univnet, it is based on diffusion too and can be used for different types of sounds
db0798#7460: I don't know then what goes wrong in your installation. I got the installation working for myself today but it was a troublesome process
Nikuson#6709: I did everything according to the guide from the Internet.  downloaded and added path to PATH.</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="70"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Nikuson#6709: "Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work"
Nikuson#6709: I installed psydub via pip, but the audio backend for it is ffmpeg and I installed it according to the guide from wikiHow
a_robot_kicker#7014: alrighty, here's my fork that has a simple local tkinter gui that can read and write .wav files: https://github.com/mklingen/riffusion-inference
a_robot_kicker#7014:  https://cdn.discordapp.com/attachments/1053081177772261386/1054498695162372116/riffusion_gui.gif
XIVV#9579: damn
XIVV#9579: im just waiting for this thing
XIVV#9579: to develop
XIVV#9579: so i can generate a sick black metal riff
XIVV#9579: cuz
XIVV#9579: rn
XIVV#9579: it sounds like some kind of lounge music
XIVV#9579: or something
Haycoat#4808: Someone should make some sort of img2img version so you can make a doodle and turn it into a spectrogram of your text to music prompt
nullerror#1387: apologies if this has been asked before i just joined but there is currently anyway to finetune the model with music of your choice?
nullerror#1387: like if i wanted to retrain the model on a specific genre how would i go about converting wavs to the correct spectrogram format to fine tune the stable diffusion model on</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="71"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">db0798#7460: It should be possible to use chavinlo's scripts from https://github.com/chavinlo/riffusion-manipulation to do conversions between audio and spectrograms, and to run Dreambooth to add new spectrograms to the Riffusion model
cravinadventure#7884: Amazing! Thank you for sharing. 🙂
nullerror#1387: thank you db0789!
nullerror#1387: i assume this also takes care of the phase issues mentioned in the about page of the website
db0798#7460: I don't know if it takes care of the phase issues or not
nullerror#1387: is the person who made this in the server by chance
nullerror#1387: ?
XIVV#9579: i think it's @seth
nullerror#1387: oh i meant the chavinlo github posted above
nullerror#1387: dmed seth with the same question but no response yet. must be busy
db0798#7460: I think it's @dep
dep#0002: whats a phase issue
dep#0002: loss in information?
nullerror#1387: here one sec
nullerror#1387: ill grab the quote</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="72"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dep#0002: ok
nullerror#1387:  https://cdn.discordapp.com/attachments/1053081177772261386/1054509685790744627/image.png
dep#0002: you mean if the repo uses the griffin-lim thing to reconstruct the audio?
nullerror#1387:  https://cdn.discordapp.com/attachments/1053081177772261386/1054509803369668618/image.png
nullerror#1387: or to make the spectrograms
nullerror#1387: cuz it says here the spectrograms they use take advantage of these algos or whatever they are
dep#0002: it uses the exact same functions that the original riffuser inference server uses. The only new addition is the `image_from_spectogram` which is the inverse of `spectogram_from_image` although our implementation was unofficial but works almost the same as the one that's currently on the official repo https://cdn.discordapp.com/attachments/1053081177772261386/1054510864667000892/image.png
nullerror#1387: ah got it perfect thank you!
nullerror#1387: ill give this a shot
dep#0002: there are also some tests included in the repo if you want to take a look at the reconstruction quality with different parameters
nullerror#1387: will do
nullerror#1387: appreciate it
April#5244: >people using some code I generated using an ai
I'll never stop being amused lol. I'm honestly just surprised it works at all
nullerror#1387: okay so ive made some images and am going to finetune the model. i think ive gone ahead and made to many tho haha</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="73"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">nullerror#1387: how many images/training steps are recommended (if known)
db0798#7460: My first Dreambooth test run just finished. I used 53 5 second pieces of a chiptune for training, used 'techno' as the class prompt. The output sounded like a chiptune already after 750 steps. I think it got to overfitting territory quite quickly, although it didn't make an exact copy of the input tune
nullerror#1387: shoot only 53??
db0798#7460: If anyone has more experience with this, I would also like to know how many images and steps are recommended
nullerror#1387: man im over here with 1400 i think i should cut back
nullerror#1387: would also be interested in hearing images/steps
nullerror#1387: although i know a general rule of thumb is 100x the number of new images
nullerror#1387: for steps
nullerror#1387: its how many images is the question
db0798#7460: I think 1400 might be a good number actually, if you want to have varied output
nullerror#1387: oh fr? hey ill be the test dummy and give it a whirl
nullerror#1387: is probably gonna take forever tho but ill shoot my shot
db0798#7460: With as few as I used, basically what I got back from it was the original input with mutations and rearrangements. I think with a larger input dataset there will be more of a chance to get something completely new in the output
db0798#7460: I'm curious to know how your run turns out
nullerror#1387: im gonna cut back slightly so im not wasting massive amounts of time/compute power but will report my results</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="74"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">nullerror#1387: gonna go for like 200-400
db0798#7460: That's like in the Terminator movie where Skynet travels back in time and contributs to Skynet's code
a_robot_kicker#7014: That's awesome. I'd love to try a fine tuned chip tune model
db0798#7460: I'll try again later with a larger input dataset
a_robot_kicker#7014: I've noticed that this thing is learning a 3 channel image and converting it to single channel, which seems pretty wasteful. Perhaps you could even fit more time domain into the G and B channels. Phase is apparently hard, but one low hanging fruit might be to just put more time in there so you could get longer samples. Like literally R is the first 5 seconds, G the second, and B the third
db0798#7460: April and deb talked about something like this here earlier, would be good if they implemented it
a_robot_kicker#7014: But since it's using stable diffusion which is trained originally on natural images anything that doesn't vaguely resemble real images might be hard for it to learn.
denny#1553: has anyone been using SoX for audio processing? Seems super powerful for automated output
Nikuson#6709: I still don't understand how to make it work on Windows
a_robot_kicker#7014: What are you hoping to use ffmpeg for?
hayk#0058: You should try generating riffs without img2img conditioning. I believe it's just the seed image being "sparse" that leads to it sounding more like lounge music
ryan_helsing#7769: I use it a ton on my project (https://neptunely.com)
denny#1553: Has been working great at concatenating audio files so far-- been wondering if there's a way to ping-pong loop though
ryan_helsing#7769: Can we supply our own seed images? Is what’s happening a sort of img2img style transfer technically?
ryan_helsing#7769: It’s extremely powerful and quick when you pipe in output .. I sometimes build commands with hundred of sub commands tying together files and it does it in under a second</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="75"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">denny#1553: yeah it's super fast!
denny#1553: I've been impressed
nullerror#1387: finetuned my model but ive run into the issue of idk how to run it now lmao
nullerror#1387: i have an amd gpu so i dont think the webapp will work for me. is there any other way of running riffusion with a custom model? maybe a colab?
nullerror#1387: https://huggingface.co/spaces/aross3/riffusion-rage
nullerror#1387: https://huggingface.co/spaces/aross3/riffusion-rage
nullerror#1387: apologies for two links my discord is glitchy
nullerror#1387: but hey it worked. i trained it on yeat and carti. sounds a little off/vocoded. does it just need more training? was trained on 200 so images
nullerror#1387: hmm actually its super phase-y. keeps like shifting up over time
nullerror#1387: anyone know why?
db0798#7460: Is the model that is loaded in the linked page your trained model? If so, what's the instance prompt for it?
nullerror#1387: yes its my trained model and all the images its trained on are called rage
nullerror#1387: lowercase "rage"
nullerror#1387: it only came out as 1gb compared to the 14gb main model
nullerror#1387: so maybe thats the issue? idk im more an audio guy than ml</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="76"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">db0798#7460: There are smaller versions of the Riffusion model that someone linked to on Reddit: https://www.reddit.com/r/riffusion/comments/znbo75/how_to_load_model_to_automatic1111/ . I was using the 4Gb one for training. I don't actually know what the difference is between these versions
db0798#7460: Here's a random output file from the chiptunes test. It's kind of low fidelity but doesn't seem phasey in exactly in the same way as yours. I guess this could be because the material it was trained on was already 8 bit, so bit reduction doesn't affect it as much https://cdn.discordapp.com/attachments/1053081177772261386/1054571256814514277/sample_8750-00.mp3
nullerror#1387: hmm i wonder why mine is acting all weird
nullerror#1387: did u use a colab to train it?
db0798#7460: I used Automatic1111 WebUI run locally, with RTX 3080
nullerror#1387: ah see i got an amd card so im using a google colab
db0798#7460: I tried right now what happens it I try to do the training with the 14 Gb model file, it doesn't work on my setup because CUDA runs out of memory
nullerror#1387: i wonder if its chkpt setup is messy oir smth
db0798#7460: I guess the spectrogram to audio conversion settings can make a difference to the output quality, too
nullerror#1387: how did u convert? did u use that github u linked earlier?
nullerror#1387: i did max settings except 512 height
nullerror#1387: for audio to spectro
nullerror#1387: i mean
db0798#7460: Yes, I used the scripts from that Github page
db0798#7460: For audio to spectrum I used the default settings</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="77"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">db0798#7460: For spectrum to audio I used the default settings, except I reduced maxvol from 100 to 50 because otherwise the audio started clipping
nullerror#1387: gotcha
nullerror#1387: i tried running spec to audio but again amd gpu so i couldnt do it
db0798#7460: It would be handy if there was a Colab version of that Github repository
nullerror#1387: yeah fr
nullerror#1387: im using the dreambooth colab for fine tuning then spaces to run it
nullerror#1387: also just some sort of how many images for fine tuning/steps ofc
nullerror#1387: i guess im just asking for a guide at this point lmao
db0798#7460: Yes. I think right now this hasn't been tested enough for anyone to write a guide
IgnizHerz#2097: haha still developing the tools to play with it I think
IgnizHerz#2097: plus training takes time
LAIONardo#4462: do you actually have to use Dreambooth? Would it just replace the seeds in the interference model be enough?
db0798#7460: How's that done? Would textual inversion do that?
LAIONardo#4462: Maybe I am wrong here, but isn't this folder what train the model? https://github.com/riffusion/riffusion-inference/tree/main/seed_images
LAIONardo#4462: Maybe your way of deploying DreamBot is the right thing to do</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="78"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">db0798#7460: I don't know what those images are for exactly but I think replacing the image files in that directory won't do anything unless you retrain the whole Riffusion model from scratch in the way the people who created that model did
db0798#7460: I think textual inversion might also work in place of Dreambooth but I haven't heard of anyone trying it for Riffusion
LAIONardo#4462: I wonder if @hayk could share some lights on what that folder does 🙂
LAIONardo#4462: Incredible work btw!
a_robot_kicker#7014: No those are just initial images that can be used as a seed for img2img
LAIONardo#4462: Oh I see that makes sense!
noop_noob#0479: https://www.youtube.com/watch?v=uGRLOMf2hSc
noop_noob#0479: AI music from a different team.
Meatfucker#1381: Hello, enjoy your tool quite a bit. Also noted we have a bit of hobby crossovers when I checked your github profile. I fly fpv drones for shits n giggles.
Meatfucker#1381: One thing I noted is the seams in between loops is a bit abrupt and had an idea about that. If you have a fast enough gpu you should be able to take two outputs and img2img the seam between them
Meatfucker#1381: should make the transition between clips much smoother, but I think it would roughly double processing time since you would be making intermediate frames every time
Meatfucker#1381: though you also wouldnt have to use the entire generated frame, just the edge, so maybe it wouldnt add so much
noop_noob#0479: What is this? https://fxtwitter.com/naklecha/status/1598956352851693568
Meatfucker#1381: Looks like they are extracting chords from an audio file and then teaching a model to predict them
Meatfucker#1381: a neat approach, but itll be limited to sounds that are chords Id imagine</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="79"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">JL#1976: https://arstechnica.com/information-technology/2022/12/riffusions-ai-generates-music-from-text-using-visual-sonograms/ Ars Technica article
IDDQD#9118: yes, indeed
Nikuson#6709: for one script to convert audio to image spectrogram
XIVV#9579: how do i turn that off
Edenoide#0166: Hi! I'm a windows user and I've been unable to make riffusion works on it... But I've created a simple Colab from the RIFFUSION MANIPULATION github for converting audio to spectrogram for model training: https://drive.google.com/file/d/1Mv3FsSiZGWt_qRv1UloG2gIawlalMlej/view?usp=share_link
Edenoide#0166: My programming level is zero by the way
Nikuson#6709: if anyone has a script without using something like ffmpeg - i will be grateful
Nikuson#6709: this is great. but in principle, in this repository, I can only get noise
Edenoide#0166: They look good enough but I haven't tried yet to turn them again into sound https://cdn.discordapp.com/attachments/1053081177772261386/1054723695203074108/aicumbia16_0.png
Edenoide#0166: I've been working with 5,12 seconds loops
Edenoide#0166: in wav format
Edenoide#0166: for a reason I don't know when turning it into mp3 the time length changes a bit and creats an extra chunk with just white
Edenoide#0166: so I'm only saving the first png of every loop
Nikuson#6709: I have them written badly, maybe I mixed up the sizes
Nikuson#6709: 512*512 https://cdn.discordapp.com/attachments/1053081177772261386/1054724689064366110/12bb35ac-1c9e-49ce-a576-2746ec474aeb.png</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="80"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Edenoide#0166: wow
Edenoide#0166: maybe it's something wrong with the audio. I've been generating the loops with Audacity (free sound software):
Edenoide#0166: Just drag and drop a sound on it. Then select 4 beats for making a loop and delete the rest (In case you are generating 'four-to-the-floor' electronic music). Double click on the timeline and then Effect>Pitch &amp; tempo>Change tempo and in the last parameter Length (seconds) put on the second cell 5.119
Edenoide#0166: Then File>Export>Export as WAV
Edenoide#0166: The thing is your final file should be a 16bit .wav with a length of 5.119 seconds
Nikuson#6709: I did it, I didn't exactly trim the song to 5 seconds last time https://cdn.discordapp.com/attachments/1053081177772261386/1054726259298537562/7fb7aa90-d9b5-4c8d-b8dd-d94d4f35002b.png
Nikuson#6709: after reverse processing https://cdn.discordapp.com/attachments/1053081177772261386/1054726335085416468/LG.mp3
Edenoide#0166: that looks a lot better but your clip seems shorter than 5.119 seconds so there's a silence at the end.
Nikuson#6709: I made to your img2audio notepad: https://colab.research.google.com/drive/1-REue4KpDhOMDI-v6gRytMpANoMUqFvi?usp=sharing

now all functionality is implemented here
Edenoide#0166: great!
Nikuson#6709: in the original trimmed clip, there is also silence at the end
Nikuson#6709:  https://cdn.discordapp.com/attachments/1053081177772261386/1054727175238066196/Lady_Gaga.wav
Edenoide#0166: perfect then</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="81"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Edenoide#0166: How did you avoid the 'clipping' artifacts in the second .wav?
Nikuson#6709: don't know, i just cut the audio through this service for even 5 seconds: https://mp3cut.net/
Edenoide#0166: I think riffusion only works with loops of 5,12 seconds (maybe I'm wrong). This means if you are not training your model with 'loopable' cuts of 94bpms (or 47bpm, 188bpm...etc)  it would sound like a patchwork but maybe in an interesting way.
Nikuson#6709: I don't want to use this for riffusion, I'm training my model
Nikuson#6709: riffusion trained a little wrong
a_robot_kicker#7014: Oh. Check out the branch I posted a bit earlier. All I did was invert the code that coverts image to wav. You will find that in my audio py. Btw it requires 16 bit 44.1 khz mono wav files. You shouldn't need ffmpeg for that as python has a native wave file reader
Nikuson#6709: Can I please have a proverb for this?
a_robot_kicker#7014: To convert other formats into that you can use vlc or audacity, although those things are probably running ffmpeg under the hood they at least install it for you
a_robot_kicker#7014: I assume you mean link. See audio.py in there, has wave to spectrogram. There are the guts of loading a wave from disk in there in gui.py as well. https://github.com/mklingen/riffusion-inference
Nikuson#6709: Thanks, I'll take a look
Haycoat#4808: Can we get a huggingface demo for wav2spec2music?
nullerror#1387: ooooooooooooooooooooooo the audio needs to be mono?
nullerror#1387: hm
nullerror#1387: wait also i assume thats a typo but it requires 44000 not 4400 sampling frequency?
nullerror#1387: and are we sure its not the industry standard 44.1khz it has to be 44khz?</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="82"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">nullerror#1387: ok checked the code can confirm it is 44.1khz
nullerror#1387: scared me for a sec
Haycoat#4808: Should I start a list of artists the model currently recognizes?
Haycoat#4808: Because there's a few that are very prominent when generating with their name
a_robot_kicker#7014: Yeah 44.1 kHz
a_robot_kicker#7014: If the author would just tell us the training set, we'd know that. But I can't find the training data....
Haycoat#4808: I know Avicii, Eminem, Post Malone, Frank Sinatra and Billie Ellish are some that generate good results
a_robot_kicker#7014: Britney Spears,  Charlie Parker, Jimi Hendrix and Aretha Franklin all worked for me. Surprisingly, Michael Jackson did not. Nirvana didn't.
a_robot_kicker#7014: Oh I tried some classical. Bach and Chopin sort of work
denny#1553: deadmau5 seems to work
denny#1553: Using 'deadmau5 melody' gives more than just droning thumps
denny#1553: Queen, Weezer, Kurt Cobain, backstreet boys seems to work too
LAIONardo#4462: So a part I am confused here (for people who are training the model on a specific sample) are you just training Dreambooth with few pictures of different spectrogram? Cause what I still don't get is doesn't Dreambooth has a lot more picture than just the one you train the model with? Are you also replacing those as well with more spectrograms??
LAIONardo#4462: Like if I am retraining Dreambooth on new spectrogram, both the instance data and the class data needs to be spectrogram only I suppose? but what is the difference there, like how do I choose what's one or another since they are both spectrograms (unlike regular profile case where the instance would be a pic of myself) https://cdn.discordapp.com/attachments/1053081177772261386/1054796709248647198/CleanShot_2022-12-20_at_11.23.202x.png
nullerror#1387: i’m training mine on a whole genre via a dreambooth google colab. converted the songs/samples to spectrogram and then uploaded as instance data. i’m not sure what class data is my colab doesn’t have that</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="83"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Twee#2335: i should make an ai-generated lo-fi hip hop livestream
Twee#2335: nobody will tell the difference
nullerror#1387: uploaded roughly 1400pics
nullerror#1387: haha twee that was what i was gonna go for here in a sec
nullerror#1387: endless lofi beats
Twee#2335: i mostly wanna do it as a critique
Twee#2335: of how formulaic a lot of those beats are
nullerror#1387: could be said of any genre
Twee#2335: yes but this model is trained on beats lol
nullerror#1387: ?
Twee#2335: i mean thats what ppl in the share-riffs channel said
Twee#2335: although i mostly have an issue with a lot of cookie-cutter lo-fi hip hop thats become too oversaturated lol
nullerror#1387: i think it’s trained on a variety of lounge music and various electronic artists as people were mentioning above
nullerror#1387: the main model i mean
nullerror#1387: ofc a lofi hiphop centered one could also be made</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="84"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">nullerror#1387: yeye
Semper#0669: Oh I see! Would you mind share the google collab link you used?
Twee#2335: most of my ai ideas are mostly satirical critiques of lack of creativity within culture
nullerror#1387: sure thing
Semper#0669: I am interested in that question
Semper#0669: As well
nullerror#1387: it comes with a guide as well one sec
Twee#2335: that or just shitposts
nullerror#1387: https://bytexd.com/how-to-use-dreambooth-to-fine-tune-stable-diffusion-colab/
nullerror#1387: https://bytexd.com/how-to-use-dreambooth-to-fine-tune-stable-diffusion-colab/
nullerror#1387: apologies my links send twice cuz my discord is glitchy
nullerror#1387: basically train it here then transfer to an instance of riffusion
Twee#2335: unfortunately im still stuck with automatic1111's webui so i cant make anything fancy other than short clips and convert them to audio
Semper#0669: Thank you are the best! I guess the name of the sample here is important as well right?
Semper#0669: Like if I want to train it on jazz each sample should have an artist name to then be found in generation right?</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="85"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">pnuts#1013: install it from the official repo and run the web-app locally? at least that way you'll get continuous playback
Twee#2335: tried and ran into a lot of headaches lol
pnuts#1013: oh 🙂
Twee#2335: also storage is an issue
Twee#2335: all these models, man
Twee#2335: they eat ur hard drive up
Semper#0669: Ahah yeah
Twee#2335: and riffusion is like
Twee#2335: 15 gb
pnuts#1013: you're not wrong
Twee#2335: probably the biggest one i have
nullerror#1387: semper should all be in the guide but yeah all my images were named the same
Twee#2335: ill try again some other time and maybe i can reach out if u know how to do it correctly?
nullerror#1387: can’t confirm it works completely yet. did a quick test yesterday and it was getting there. trying a bigger one as of rn. when i load the bigger one i’ll send my finding as to if it worked
nullerror#1387: also make sure ur audio is mono/16bit wav/44.1khz</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="86"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">nullerror#1387: then use that github linked somewhere above called like manipulation tools for riffusion or smth to get audiotoimg for training
pnuts#1013: sure, I don't recall running into any major issues. I've got it working on 2 machines. I'm sure we can work it out
nullerror#1387: twee do u have an amd graphics card
pnuts#1013: nvidia here, does AMD even support CUDA?
Twee#2335: nah
Twee#2335: i have nvidia
Twee#2335: more specifically
Twee#2335: i use a cloud pc with a nvidia p5000 quadro
Twee#2335: for the ai stuff
Twee#2335: (and gaming ofc lmao)
pnuts#1013: running locally on a 3080 here
nullerror#1387: was gonna say if it ain’t working might be amd
nullerror#1387: that’s my issue and why i have to use colab and stuff
nullerror#1387: where do u rent a cloud computer from twee i’ve been looking for one
Twee#2335: i mean this first step isnt really clear. npm install on your C home drive doesnt do anything lol https://cdn.discordapp.com/attachments/1053081177772261386/1054801152530731008/Screenshot_2022-12-20_at_11.42.17_AM.png</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="87"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">pnuts#1013: https://lambdalabs.com/ seems to be one of the cheaper options
pnuts#1013: make sure you have node/npm installed, then run `npm install` from inside the folder
Twee#2335: which folder though
Twee#2335: the node folder?
Twee#2335: i wish that was specified tbh
pnuts#1013: the git repo you cloned
Twee#2335: wait
Twee#2335: ahhhhh
Twee#2335: yea i think i tried that too
Twee#2335: hold on
pnuts#1013: so from inside the `riffusion-app` folder
Twee#2335: yea screw it ill just try agian lol
Twee#2335: oh ok
pnuts#1013: in the same folder you also want to create a file called `.env` or `.env.local` and add ```RIFFUSION_FLASK_URL=http://127.0.0.1:3013/run_inference/```
pnuts#1013: if you run the inference server on the same box the above will work</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="88"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">pnuts#1013: if it's running elsewhere, add the correct IP
nullerror#1387: thanks punts i’ve been looking at that and vast ai
nullerror#1387: i’ll extend my search
Twee#2335: i should had know i should had git cloned, im just still getting used to "developer unclear instructions to layman user" syndrome
Twee#2335: do u edit the file like a textfile?
pnuts#1013: yes
Twee#2335: ok bc
Twee#2335: i cant seem to be able to edit it
pnuts#1013: open it in any old text editor
pnuts#1013: if you're struggling create a .txt file, add `RIFFUSION_FLASK_URL=http://127.0.0.1:3013/run_inference/` and then rename it to .env or .env.local
Twee#2335: no i gotcha
Twee#2335: usually im used to right clicking a file and hit the "edit" button
Twee#2335: ok time to donwload the model but where does that get put in
pnuts#1013: right-click open with
pnuts#1013: you don't actually need the 14GB checkpoint, but if you want it `git lfs clone https://huggingface.co/riffusion/riffusion-model-v1`</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="89"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Twee#2335: oh lmao
Twee#2335: whats the checkpoint download for then
pnuts#1013: more fine=tuning perhaps? I'm pretty confident I didn't download it on the 2nd install I did.
Twee#2335: also
Twee#2335: wasnt this suppose to be in the inference folder
Twee#2335: which is a separate download
pnuts#1013: also it's 14GB I only have a 10gig card.
pnuts#1013: no, that variable is to tell the app where to find the inference server
a_robot_kicker#7014: Uh, it downloads the 14gb model on startup if it can.
Haycoat#4808: Img2spec2music should be possible to do in a huggingface demo...
a_robot_kicker#7014: Also checkpoint size doesn't seem to necessarily be the amount of vram it uses, but I could be mistaken
pnuts#1013: any idea how it is split up, as I can run it on a 10gig card. I struggle running various other checkpoints due to their size.
a_robot_kicker#7014: I'm not sure but in my case it seems to have downsampled the model to float16
pnuts#1013: too busy messing around with stuff, should really read the code
a_robot_kicker#7014: It doesn't seem to use up my 10gb of vram</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="90"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Haycoat#4808: Like if you have a spectrogram of your voice, you can convert it to the style of Avicii EDM with img2img implementation
Twee#2335:  https://cdn.discordapp.com/attachments/1053081177772261386/1054806218834718791/Screenshot_2022-12-20_at_12.03.15_PM.png
pnuts#1013: you've launched the inference server too?
Twee#2335: well i mean thats why i asked about the inference server too lol
Twee#2335: i was wondering if it was a necessary download
pnuts#1013: clone this fella &lt;https://github.com/riffusion/riffusion-inference>
pnuts#1013: the first part you completed it for the front-end only
pnuts#1013: it's the inference-server that will generate your images
nullerror#1387: there’s a smaller version of the model available
nullerror#1387: as a chkpt file on hugface
nullerror#1387: 4gb
nullerror#1387: or so
Twee#2335: cool https://cdn.discordapp.com/attachments/1053081177772261386/1054806906327285790/Screenshot_2022-12-20_at_12.05.36_PM.png
Twee#2335: like i said
Twee#2335: headache</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="91"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">pnuts#1013: you ran 3 commands at once
Twee#2335: i mean that usually tends to work lol
pnuts#1013: ```conda create --name riffusion-inference python=3.9
conda activate riffusion-inference
python -m pip install -r requirements.txt```
Twee#2335: one command, then the other, then the other
Haycoat#4808: What if we used the img2img part and use it in Riffusion? Then we can attempt a way to do audio2spec and use our own audio as a reference
pnuts#1013: I can't remember if I had to also install torch after this. give it a try and see what errors it throws when you try and launch it
a_robot_kicker#7014: In my case I had to separately install torch and cuda.
Twee#2335: what step did i skipped this time lol https://cdn.discordapp.com/attachments/1053081177772261386/1054808178484838521/Screenshot_2022-12-20_at_12.10.55_PM.png
pnuts#1013: `conda install ffmpeg`
Twee#2335: still getting the no audio backend error
Twee#2335:  https://cdn.discordapp.com/attachments/1053081177772261386/1054808623445966918/Screenshot_2022-12-20_at_12.12.49_PM.png
Twee#2335: do i have to install torch+cuda seperately or something
pnuts#1013: running through the steps again on my sie</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="92"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">a_robot_kicker#7014: In my case that indicated needing to install torch
a_robot_kicker#7014: Specifically torch audio and cuda
pnuts#1013: `pip install --no-cache-dir --ignore-installed --force-reinstall --no-warn-conflicts torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116`
Haycoat#4808: There's also a huggingface space for Riffusion already available! https://huggingface.co/spaces/fffiloni/spectrogram-to-music
pnuts#1013: yep, had to re-install this. but it's up and running now
pnuts#1013:  https://cdn.discordapp.com/attachments/1053081177772261386/1054810986021924914/Capture.PNG
Twee#2335: its downloading a bunch of stuff now lol
pnuts#1013: will check in again shortly, off to grab some nom noms
Twee#2335: laterrrr
Haycoat#4808: My favorite artist to use is Avicii, hands down a bop to listen
Twee#2335: lol https://cdn.discordapp.com/attachments/1053081177772261386/1054815748343738519/Screenshot_2022-12-20_at_12.41.06_PM.png
Twee#2335: ngl i liked my results better with automatic1111's webui settings
Twee#2335: the web app is not exactly versatile lol
pnuts#1013: it gives you continuous playback, and you can add your seed images.
Semper#0669: For the riffusion manipulation tool how do I execute the command on all the files in a folder instead of one by one? the command is:</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="93"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
python3 file2img.py -i INPUT_AUDIO.wav -o OUTPUT_FOLDER

But using /foldername/*.wav doesn’t work
Twee#2335: i can add spectrograms into the web app?
pnuts#1013: yes, there's a seed image folder
pnuts#1013: &lt;https://github.com/riffusion/riffusion-inference/tree/main/seed_images>
Twee#2335: once i add a seed image, how do i access it within the web app?
pnuts#1013: https://localhost:3000/?&amp;prompt=brazilian+Forr%C3%B3+dance&amp;seed=51209&amp;denoising=0.75&amp;seedImageId=og_beat
pnuts#1013: replace `og_beat` at the end with your seed
pnuts#1013: I think I also had to edit something else and give it an initial seed
hayk#0058: Thanks @Meatfucker ! I think there are several good ideas for smoothing between clips, some discussion here https://github.com/orgs/riffusion/discussions/18
hayk#0058: I'm going to be adding in a streamlit app (because I know it best over gradio, etc) to riffusion-inference that does some of the common operations for riffusion like converting from audio, generation, interpolation, etc
Nikuson#6709: For those who find it difficult to trim an audio file to 5 seconds every time, I posted a repository with this script for quick and easy trimming
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="94"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/nikuson/trimmed
Nikuson#6709: ChatGPT generated
LAIONardo#4462: Thank you!
nullerror#1387: doesnt the riffusion manipulation thing already do this for spectrograms?
nullerror#1387: unless this is meant for smth else
Philpax#0001: hey there! apologies if this has already been asked, but is there any information on finetuning sd/riffusion on your own collection of tagged samples? I'd like to do conditional sound effect generation and am wondering if anyone's explored this yet
Meatfucker#1381: I've seen some people mention it. It's a standard diffusion model on terms of training though I don't know it's initial tagging
Meatfucker#1381: You should be able to convert some things into spectrograms and train it like any other model
Philpax#0001: aye, that's what I suspected - just wanted to make sure there wasn't any other kind of magic
Nikuson#6709: no, spectrograms are obtained with artifacts
Nikuson#6709:  https://cdn.discordapp.com/attachments/1053081177772261386/1054837140544036955/12bb35ac-1c9e-49ce-a576-2746ec474aeb.png
Haycoat#4808: Could you try getting ChatGPT to make a GitHub for wav2spec?
Nikuson#6709: in what sense? diffusion manipulation has all the necessary tools for transformations
Haycoat#4808: I mean like being able to upload an audio file and use it for Riffusion
Edenoide#0166: Export audios as .WAV 16bit</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="95"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Edenoide#0166: Is this Linux? Did someone been capable of make it run on windows?
denny#1553: The inference server runs fine on windows through conda
denny#1553: Haven't tried the front end but I suspect it's fine too
Edenoide#0166: through conda you say? I'm gonna give it a try then
Edenoide#0166: I had as lot of problems with transformerx, pydubs etc
Edenoide#0166: I'm gonna try it. Thanks
nullerror#1387: nokia on im confused what do you mean spectrogram are “obtained through artifacts”?
nullerror#1387: nikuson
db0798#7460: The audio to spectrogram scripts from the Riffusion Manipulation Tools Github page work okay for me. Nikuson must be doing something wrong to get artefacts
nullerror#1387: can report that it works for me as well. likely mp3 conversion or smth with the settings on their end
nullerror#1387: my images don’t come out like that at least i mean
Edenoide#0166: I'm trying to run Rifussion inference server on Windows using conda. I've installed ffmpeg via conda install -c conda-forge ffmpeg and soundfile (pip install soundfile) but a lot of errors appear when running the last step:
Edenoide#0166:  https://cdn.discordapp.com/attachments/1053081177772261386/1054870021639262219/errors.PNG
Edenoide#0166: Any idea? I feel like I'm almost there...
a_robot_kicker#7014: you do not have cuda and torch installed.</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="96"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">db0798#7460: It looks more like torch is installed but Cuda isn't
Meatfucker#1381: pytorch website has a little command configuration thing on it to build you a command
Meatfucker#1381: https://pytorch.org/get-started/locally/
db0798#7460: If I remember it right, on my computer I first had to install Cuda from NVIDIA website and after that use the command from the https://pytorch.org/get-started/locally/ page. When I didn't install Cuda from the NVIDIA page first, I got the same error message that Ketedeneden got
Edenoide#0166: Baaam! Yeah it seems a problem with cuda https://cdn.discordapp.com/attachments/1053081177772261386/1054874669024546886/cuda.PNG
Meatfucker#1381: the program runs in an enviroment separate from your base system enviroment
Edenoide#0166: I see! a virtual enviroment?
Meatfucker#1381: yep
Meatfucker#1381: I think conda in this case, not a venv
Edenoide#0166: right! thanks
Meatfucker#1381: so enter your conda enviroment, and then put in the command the pytorch website tells you
Meatfucker#1381: and it should sort out your deps
Meatfucker#1381: if its still funny make a fresh conda enviroment
Meatfucker#1381: sometimes old deps can gunk up the works with conda
Nikuson#6709: I have observed the dataset of singing and now I will run fine tune stable diffusion</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="97"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Nikuson#6709: it seems to me that only the sampler is trained in riffusion, which gives such poor quality
matteo101man#6162: Anyone know of any local mashup AIs?
nullerror#1387: rave dot dj
nullerror#1387: been around a while its okay
hayk#0058: 🤘 Hey riffusers! 🤘 @here

@seth  and I have been absolutely blown away by the response to our little hobby project. We had no idea if this approach would even work, and to see musicians and tinkerers building on top of it and making fun sounds is a dream.

We’re still trying to keep up with everything, but a few fun notes:

+ riffusion.com has been visited over a million times in the past few days and generated about a year of unique audio.
+ Our GPUs still can’t always keep up with requests, but they are getting close!
+ We will soon add a streamlit app that demos some of the common use cases like interpolation, img2img, and some audio transformations.
+ We are beginning to collaborate with LAION, the people behind the dataset that trained stable diffusion, to see how we can scale up.
+ We’ll also make GitHub issues to track a bunch of the improvement ideas we have.</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="98"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">+ Attached is an awesome sample created by producer Jamison Baken incorporating outputs from Riffusion.

If you’re a software eng or musician interested in being more directly involved, feel free to send us a DM. And everyone, thanks for being here! https://cdn.discordapp.com/attachments/1053081177772261386/1054910368960499742/mix.mp3
Edenoide#0166: Great work!!!!
COMEHU#2094: Good luck, im sure this is the start of something bigger
Nikuson#6709: wrote in DM.
cravinadventure#7884: Awesome post @hayk @seth !

Excited to contribute to the improvement of Riffusion and really see it take off :redrocket: Crazy that the site has been visited over 1M times and has generated about 1 yr of audio.

:partyBlob:  **FUN Discord Community Idea:** :partyBlob:
- Host competitions where the community will be able to submit and *vote, by liking a post*, on the top 10 samples weekly.
- Additionally, the prompt should be listed with each submission.

**Competitions &amp; Leaderboards info:**</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="99"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">- set up a new “***Competitions Channel***” which includes:
- “***Weekly Top 10 Leaderboard***”
- Top liked posts in 1 week. Ideally you should keep track of the weekly leaderboards so anyone could go back in time to look and see who won on any given week, in the past.
- “***All-Time Top 10 Leaderboard***”
- Top liked posts of all-time, from every weekly competition combined.

I hope something like this will be implemented because I think it would be FUN for everyone &amp; keep users active. 🙂
Philpax#0001: is there an standalone application that can convert a riffusion spectrograph to audio? preferably command-line
nullerror#1387: yes
nullerror#1387: riffusion manipulator i think its called? ill grab the link
nullerror#1387: https://github.com/chavinlo/riffusion-manipulation
nullerror#1387: https://github.com/chavinlo/riffusion-manipulation
nullerror#1387: manipulation
nullerror#1387: almost had it
cravinadventure#7884: wait so does that software allow you to basically feed in images as input to create audio?</span></div></div>
							</td>
					</tr></tbody></table>
		<div class="bg-linear-to-b sticky left-0 border-t border-dashed border-gray-300 from-gray-100 to-white py-3 text-center font-mono text-xs dark:border-gray-700 dark:from-gray-950 dark:to-gray-900">End of preview. <a href="/datasets/breadlicker45/discord_data/viewer/default/train" class="group"><span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Expand</span>
						in <svg class="text-lg mr-0.5 inline -translate-y-px text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>Data Studio
					</a></div></div>




			<div class="bg-linear-to-b from-gray-100 to-white dark:from-gray-950 dark:to-gray-900 "><hr class="flex-none -translate-y-px border-t border-dashed border-gray-300 bg-white dark:border-gray-700 dark:bg-gray-950">
					<nav><ul class="flex select-none items-center justify-between space-x-2 text-gray-700 sm:justify-center py-1 text-center font-mono text-xs rounded-b-lg"><li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 pointer-events-none cursor-default text-gray-400 hover:text-gray-700" href=""><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg>
		Previous</a></li>
			<li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1 bg-gray-50 font-semibold ring-1 ring-inset ring-gray-200 dark:bg-gray-900 dark:text-yellow-500 dark:ring-gray-900 hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/breadlicker45/discord_data/viewer/default/train?p=0">1</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/breadlicker45/discord_data/viewer/default/train?p=1">2</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/breadlicker45/discord_data/viewer/default/train?p=2">3</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  pointer-events-none cursor-default" href="#">...</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/breadlicker45/discord_data/viewer/default/train?p=4524">4,525</a>
				</li>
			<li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 " href="/datasets/breadlicker45/discord_data/viewer/default/train?p=1">Next
		<svg class="ml-1.5 transform rotate-180" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg></a></li></ul></nav></div></div>






</div></div></div></div></div></div></div>

				<div class="from-gray-50-to-white bg-linear-to-t rounded-lg border border-dotted border-gray-200 py-24 text-center md:px-6"><p class="mb-1 mt-2">No dataset card yet</p>
						</div></section>
			
			<section class="pt-6 border-gray-100 md:pb-24 md:pl-6 md:w-64 lg:w-80 xl:w-96 flex-none order-first md:order-none md:border-l pt-3! md:pt-6!"><dl class="flex items-baseline justify-between"><dt class="text-sm text-gray-500">Downloads last month</dt><div class="mx-4 flex-1 border-b border-dotted"></div><dd class="font-semibold">20</dd></dl>
				
				<div class="divider-column-vertical"></div>
				<div class="grid grid-cols-2 gap-x-2 md:flex md:flex-row md:flex-wrap"><div class="SVELTE_HYDRATER contents" data-target="DatasetAndModelActionsDropdown" data-props="{&quot;classNames&quot;:&quot;order-last&quot;,&quot;discussionsDisabled&quot;:false,&quot;repo&quot;:{&quot;type&quot;:&quot;dataset&quot;,&quot;name&quot;:&quot;breadlicker45/discord_data&quot;},&quot;canWrite&quot;:false,&quot;canDisable&quot;:false,&quot;repoIsPrivate&quot;:false,&quot;repoIsGated&quot;:false,&quot;repoIsDisabled&quot;:false,&quot;repoIsAdminFlaggedNFAA&quot;:false,&quot;repoHasBlockedOids&quot;:false}"><div class="order-last"><div class="relative ">
	<button class="btn px-1.5 py-1.5 " type="button">
		
			<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="p-0.5" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><circle cx="16" cy="7" r="3" fill="currentColor"></circle><circle cx="16" cy="16" r="3" fill="currentColor"></circle><circle cx="16" cy="25" r="3" fill="currentColor"></circle></svg>
		
		</button>
	
	
	</div></div>













</div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetLibrary" data-props="{&quot;classNames&quot;:&quot;md:w-full xl:w-auto xl:flex-none&quot;,&quot;libraries&quot;:[{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;datasets&quot;,&quot;function&quot;:&quot;load_dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{},&quot;code&quot;:&quot;from datasets import load_dataset\n\nds = load_dataset(\&quot;breadlicker45/discord_data\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;pandas&quot;,&quot;function&quot;:&quot;pd.read_csv&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;output_file.csv&quot;}},&quot;code&quot;:&quot;import pandas as pd\n\ndf = pd.read_csv(\&quot;hf://datasets/breadlicker45/discord_data/output_file.csv\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;mlcroissant&quot;,&quot;function&quot;:&quot;Dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;record_set&quot;:&quot;default&quot;,&quot;partial&quot;:false},&quot;code&quot;:&quot;from mlcroissant import Dataset\n\nds = Dataset(jsonld=\&quot;https://huggingface.co/api/datasets/breadlicker45/discord_data/croissant\&quot;)\nrecords = ds.records(\&quot;default\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;polars&quot;,&quot;function&quot;:&quot;pl.read_csv&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;output_file.csv&quot;}},&quot;code&quot;:&quot;import polars as pl\n\ndf = pl.read_csv('hf://datasets/breadlicker45/discord_data/output_file.csv')\n&quot;}]}]}"><div class="relative md:w-full xl:w-auto xl:flex-none">
	<button class="from-gray-800! to-black! max-xl:mb-2 text-white! gap-1! border-gray-800! dark:border-gray-900!  btn w-full cursor-pointer text-sm" type="button">
		<svg class="mr-1.5 mr-0.5! " xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>
			Use this dataset
		<svg class="-mr-1 text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path d="M16.293 9.293L12 13.586L7.707 9.293l-1.414 1.414L12 16.414l5.707-5.707z" fill="currentColor"></path></svg></button>
	
	
	</div>

</div>
					
					</div>
				<div class="divider-column-vertical"></div>
					<div class="flex flex-col flex-wrap xl:flex-row"><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of downloaded dataset files:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->428 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 " href="/datasets/breadlicker45/discord_data/tree/refs%2Fconvert%2Fparquet/" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of the auto-converted Parquet files:</div>
		<div class="truncate text-sm group-hover:underline">
			<!-- HTML_TAG_START -->220 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Number of rows:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->452,471<!-- HTML_TAG_END --></div></a></div>
				
				
				
				

				
				<div class="divider-column-vertical md:hidden"></div></section></div></main>

	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container relative flex flex-col justify-between space-y-2 py-6 text-gray-500 max-md:*:self-start md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="SVELTE_HYDRATER contents" data-target="ThemeSwitcher" data-props="{&quot;theme&quot;:&quot;system&quot;,&quot;isLoggedIn&quot;:false,&quot;menuClassNames&quot;:&quot;md:-top-24&quot;,&quot;classNames&quot;:&quot;max-md:mb-5 max-md:*:self-start&quot;}">
<div class="relative inline-block max-md:mb-5 max-md:*:self-start">
	<button class="rounded-full border border-gray-100 pl-2 py-1 pr-2.5  flex items-center text-sm text-gray-500 bg-white hover:bg-purple-50 hover:border-purple-200 dark:hover:bg-gray-800 dark:hover:border-gray-950 dark:border-gray-800 " type="button">
		<svg class="mr-1.5 text-gray-500" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M6 22.5h20a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2H6a2 2 0 0 0-2 2v13.5a2 2 0 0 0 2 2ZM7 7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h18a1 1 0 0 0 1-1V8a1 1 0 0 0-1-1H7Z" fill="currentColor"></path><path d="M6 8a1 1 0 0 1 1-1h18a1 1 0 0 1 1 1v11a1 1 0 0 1-1 1H7a1 1 0 0 1-1-1V8Z" fill="currentColor" fill-opacity=".4"></path><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path></svg>
			System theme
		</button>
	
	
	</div></div>
		<div class="font-semibold text-black md:hidden">Company</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="max-md:mb-4! max-md:mt-8! group flex-none max-md:order-last"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="max-md:mt-8! font-semibold text-black md:hidden">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("\/front\/build\/kube-68d7aa0\/index.js");
			window.moonSha = "kube-68d7aa0\/";
			window.__hf_deferred = {};
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>
	</body>
</html>
