<!doctype html>
<html class="">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
		<meta name="description" content="Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science." />
		<meta property="fb:app_id" content="1321688464574422" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@huggingface" />
		<meta name="twitter:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/MaartenGr/arxiv_nlp.png" />
		<meta property="og:title" content="MaartenGr/arxiv_nlp Â· Datasets at Hugging Face" />
		<meta property="og:type" content="website" />
		<meta property="og:url" content="https://huggingface.co/datasets/MaartenGr/arxiv_nlp" />
		<meta property="og:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/MaartenGr/arxiv_nlp.png" />

		<link rel="stylesheet" href="/front/build/kube-68d7aa0/style.css" />

		<link rel="preconnect" href="https://fonts.gstatic.com" />
		<link
			href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap"
			rel="stylesheet"
		/>

		<link
			rel="preload"
			href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css"
			as="style"
			onload="this.onload=null;this.rel='stylesheet'"
		/>
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		<script>const guestTheme = document.cookie.match(/theme=(\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches));</script>
<link rel="canonical" href="https://huggingface.co/datasets/MaartenGr/arxiv_nlp"> <script type="application/ld+json">{
  "@context": {
    "@language": "en",
    "@vocab": "https:\/\/schema.org\/",
    "citeAs": "cr:citeAs",
    "column": "cr:column",
    "conformsTo": "dct:conformsTo",
    "cr": "http:\/\/mlcommons.org\/croissant\/",
    "data": {
      "@id": "cr:data",
      "@type": "@json"
    },
    "dataBiases": "cr:dataBiases",
    "dataCollection": "cr:dataCollection",
    "dataType": {
      "@id": "cr:dataType",
      "@type": "@vocab"
    },
    "dct": "http:\/\/purl.org\/dc\/terms\/",
    "extract": "cr:extract",
    "field": "cr:field",
    "fileProperty": "cr:fileProperty",
    "fileObject": "cr:fileObject",
    "fileSet": "cr:fileSet",
    "format": "cr:format",
    "includes": "cr:includes",
    "isLiveDataset": "cr:isLiveDataset",
    "jsonPath": "cr:jsonPath",
    "key": "cr:key",
    "md5": "cr:md5",
    "parentField": "cr:parentField",
    "path": "cr:path",
    "personalSensitiveInformation": "cr:personalSensitiveInformation",
    "recordSet": "cr:recordSet",
    "references": "cr:references",
    "regex": "cr:regex",
    "repeated": "cr:repeated",
    "replace": "cr:replace",
    "sc": "https:\/\/schema.org\/",
    "separator": "cr:separator",
    "source": "cr:source",
    "subField": "cr:subField",
    "transform": "cr:transform"
  },
  "@type": "sc:Dataset",
  "distribution": [
    {
      "@type": "cr:FileObject",
      "@id": "repo",
      "name": "repo",
      "description": "The Hugging Face git repository.",
      "contentUrl": "https:\/\/huggingface.co\/datasets\/MaartenGr\/arxiv_nlp\/tree\/refs%2Fconvert%2Fparquet",
      "encodingFormat": "git+https",
      "sha256": "https:\/\/github.com\/mlcommons\/croissant\/issues\/80"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-default",
      "name": "parquet-files-for-config-default",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "default\/*\/*.parquet"
    }
  ],
  "recordSet": [
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "default_splits\/split_name"
      },
      "@id": "default_splits",
      "name": "default_splits",
      "description": "Splits for the default config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "default_splits\/split_name": "train"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "default",
      "name": "default",
      "description": "MaartenGr\/arxiv_nlp - 'default' subset",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default\/split",
          "name": "default\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "default\/(?:partial-)?(train)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "default_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/Titles",
          "name": "default\/Titles",
          "description": "Column 'Titles' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "Titles"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/Abstracts",
          "name": "default\/Abstracts",
          "description": "Column 'Abstracts' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "Abstracts"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/Years",
          "name": "default\/Years",
          "description": "Column 'Years' from the Hugging Face parquet file.",
          "dataType": "sc:Integer",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "Years"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/Categories",
          "name": "default\/Categories",
          "description": "Column 'Categories' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "Categories"
            }
          }
        }
      ]
    }
  ],
  "conformsTo": "http:\/\/mlcommons.org\/croissant\/1.0",
  "name": "arxiv_nlp",
  "description": "\n\t\n\t\t\n\t\n\t\n\t\tarXiv Abstracts\n\t\n\nAbstracts for the cs.CL category of ArXiv between 1991 and 2024. This dataset was created as an instructional tool for the Clustering and Topic Modeling chapter in the upcoming\n\"Hands-On Large Language Models\" book. \nThe original dataset was retrieved here. \nThis subset will be updated towards the release of the book to make sure it captures relatively recent articles in the domain.\n",
  "alternateName": [
    "MaartenGr\/arxiv_nlp",
    "ArXiv NLP Abstracts"
  ],
  "creator": {
    "@type": "Person",
    "name": "Maarten Grootendorst",
    "url": "https:\/\/huggingface.co\/MaartenGr"
  },
  "keywords": [
    "English",
    "mit",
    "10K - 100K",
    "csv",
    "Text",
    "Datasets",
    "pandas",
    "Croissant",
    "Polars",
    "ðŸ‡ºðŸ‡¸ Region: US"
  ],
  "license": "https:\/\/choosealicense.com\/licenses\/mit\/",
  "url": "https:\/\/huggingface.co\/datasets\/MaartenGr\/arxiv_nlp"
}</script> 

		<title>MaartenGr/arxiv_nlp Â· Datasets at Hugging Face</title>

		<script
			defer
			data-domain="huggingface.co"
			event-loggedIn="false"
			src="/js/script.pageview-props.js"
		></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script>
			window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer></script>
	</head>
	<body class="flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DatasetPage">
		<div class="flex min-h-dvh flex-col"><div class="SVELTE_HYDRATER contents" data-target="SystemThemeMonitor" data-props="{&quot;isLoggedIn&quot;:false}"></div>

	<div class="SVELTE_HYDRATER contents" data-target="MainHeader" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false,&quot;isPro&quot;:false}"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg">
				<span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a>
			<div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..."   spellcheck="false" type="text" value="">
	<svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
	</div>
			<div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg>
		</button>

	</div></div>
		<nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 2xl:space-x-2"><li class="hover:text-indigo-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
					Models</a>
			</li><li class="hover:text-red-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					Datasets</a>
			</li><li class="hover:text-blue-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg>
					Spaces</a>
			</li><li class="hover:text-yellow-700 max-xl:hidden"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 text-yellow-500!" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg>
					Posts</a>
			</li><li class="hover:text-yellow-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 16 16"><path d="m2.28 3.7-.3.16a.67.67 0 0 0-.34.58v8.73l.01.04.02.07.01.04.03.06.02.04.02.03.04.06.05.05.04.04.06.04.06.04.08.04.08.02h.05l.07.02h.11l.04-.01.07-.02.03-.01.07-.03.22-.12a5.33 5.33 0 0 1 5.15.1.67.67 0 0 0 .66 0 5.33 5.33 0 0 1 5.33 0 .67.67 0 0 0 1-.58V4.36a.67.67 0 0 0-.34-.5l-.3-.17v7.78a.63.63 0 0 1-.87.59 4.9 4.9 0 0 0-4.35.35l-.65.39a.29.29 0 0 1-.15.04.29.29 0 0 1-.16-.04l-.65-.4a4.9 4.9 0 0 0-4.34-.34.63.63 0 0 1-.87-.59V3.7Z" fill="currentColor" class="dark:opacity-40"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M8 3.1a5.99 5.99 0 0 0-5.3-.43.66.66 0 0 0-.42.62v8.18c0 .45.46.76.87.59a4.9 4.9 0 0 1 4.34.35l.65.39c.05.03.1.04.16.04.05 0 .1-.01.15-.04l.65-.4a4.9 4.9 0 0 1 4.35-.34.63.63 0 0 0 .86-.59V3.3a.67.67 0 0 0-.41-.62 5.99 5.99 0 0 0-5.3.43l-.3.17L8 3.1Zm.73 1.87a.43.43 0 1 0-.86 0v5.48a.43.43 0 0 0 .86 0V4.97Z" fill="currentColor" class="opacity-40 dark:opacity-100"></path><path d="M8.73 4.97a.43.43 0 1 0-.86 0v5.48a.43.43 0 1 0 .86 0V4.96Z" fill="currentColor" class="dark:opacity-40"></path></svg>
					Docs</a>
			</li><li class="hover:text-black dark:hover:text-white"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/enterprise"><svg class="mr-1.5 text-gray-400 group-hover:text-black dark:group-hover:text-white" xmlns="http://www.w3.org/2000/svg" fill="none" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 33 27"><path fill="currentColor" fill-rule="evenodd" d="M13.5.7a8.7 8.7 0 0 0-7.7 5.7L1 20.6c-1 3.1.9 5.7 4.1 5.7h15c3.3 0 6.8-2.6 7.8-5.7l4.6-14.2c1-3.1-.8-5.7-4-5.7h-15Zm1.1 5.7L9.8 20.3h9.8l1-3.1h-5.8l.8-2.5h4.8l1.1-3h-4.8l.8-2.3H23l1-3h-9.5Z" clip-rule="evenodd"></path></svg>
					Enterprise</a>
			</li>

		<li><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/pricing">Pricing
			</a></li>

		<li><div class="relative group">
	<button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button">
		<svg class=" text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>
			
		</button>
	
	
	</div></li>
		<li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li>
		<li><a class="block cursor-pointer whitespace-nowrap px-2 py-0.5 hover:text-gray-500 dark:text-gray-300 dark:hover:text-gray-100" href="/login">Log In
				</a></li>
			<li><a class="whitespace-nowrap rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up
					</a></li></ul></nav></div></header></div>
	
	
	
	<div class="SVELTE_HYDRATER contents" data-target="SSOBanner" data-props="{}"></div>
	
	

	<main class="flex flex-1 flex-col">
	<div class="SVELTE_HYDRATER contents" data-target="DatasetHeader" data-props="{&quot;activeTab&quot;:&quot;datasetCard&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;62ea1ac3cc08a09aa6d3ec95&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62ea1ac3cc08a09aa6d3ec95/_74xXYEYLLjNVJ9zQucfn.jpeg&quot;,&quot;fullname&quot;:&quot;Maarten Grootendorst&quot;,&quot;name&quot;:&quot;MaartenGr&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:25},&quot;canReadRepoSettings&quot;:false,&quot;dataset&quot;:{&quot;author&quot;:&quot;MaartenGr&quot;,&quot;cardData&quot;:{&quot;license&quot;:&quot;mit&quot;,&quot;language&quot;:[&quot;en&quot;],&quot;pretty_name&quot;:&quot;ArXiv NLP Abstracts&quot;},&quot;cardExists&quot;:true,&quot;createdAt&quot;:&quot;2023-08-09T10:53:12.000Z&quot;,&quot;description&quot;:&quot;\n\t\n\t\t\n\t\n\t\n\t\tarXiv Abstracts\n\t\n\nAbstracts for the cs.CL category of ArXiv between 1991 and 2024. This dataset was created as an instructional tool for the Clustering and Topic Modeling chapter in the upcoming\n\&quot;Hands-On Large Language Models\&quot; book. \nThe original dataset was retrieved here. \nThis subset will be updated towards the release of the book to make sure it captures relatively recent articles in the domain.\n&quot;,&quot;downloads&quot;:1696,&quot;downloadsAllTime&quot;:5650,&quot;id&quot;:&quot;MaartenGr/arxiv_nlp&quot;,&quot;isLikedByUser&quot;:false,&quot;lastModified&quot;:&quot;2024-03-13T08:23:32.000Z&quot;,&quot;likes&quot;:4,&quot;datasetsServerInfo&quot;:{&quot;viewer&quot;:&quot;viewer&quot;,&quot;numRows&quot;:44949,&quot;libraries&quot;:[&quot;datasets&quot;,&quot;pandas&quot;,&quot;mlcroissant&quot;,&quot;polars&quot;],&quot;formats&quot;:[&quot;csv&quot;],&quot;modalities&quot;:[&quot;text&quot;]},&quot;discussionsDisabled&quot;:false,&quot;repoType&quot;:&quot;dataset&quot;,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;tags&quot;:[&quot;language:en&quot;,&quot;license:mit&quot;,&quot;size_categories:10K<n<100K&quot;,&quot;format:csv&quot;,&quot;modality:text&quot;,&quot;library:datasets&quot;,&quot;library:pandas&quot;,&quot;library:mlcroissant&quot;,&quot;library:polars&quot;,&quot;region:us&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;language:en&quot;,&quot;label&quot;:&quot;English&quot;,&quot;type&quot;:&quot;language&quot;},{&quot;id&quot;:&quot;license:mit&quot;,&quot;label&quot;:&quot;mit&quot;,&quot;type&quot;:&quot;license&quot;},{&quot;id&quot;:&quot;size_categories:10K<n<100K&quot;,&quot;label&quot;:&quot;10K - 100K&quot;,&quot;type&quot;:&quot;size_categories&quot;},{&quot;id&quot;:&quot;format:csv&quot;,&quot;label&quot;:&quot;csv&quot;,&quot;type&quot;:&quot;format&quot;},{&quot;id&quot;:&quot;modality:text&quot;,&quot;label&quot;:&quot;Text&quot;,&quot;type&quot;:&quot;modality&quot;},{&quot;id&quot;:&quot;library:datasets&quot;,&quot;label&quot;:&quot;Datasets&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:pandas&quot;,&quot;label&quot;:&quot;pandas&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:mlcroissant&quot;,&quot;label&quot;:&quot;Croissant&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:polars&quot;,&quot;label&quot;:&quot;Polars&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;ðŸ‡ºðŸ‡¸ Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;}],&quot;hasBlockedOids&quot;:false,&quot;region&quot;:&quot;us&quot;,&quot;xetEnabled&quot;:false},&quot;discussionsStats&quot;:{&quot;closed&quot;:0,&quot;open&quot;:1,&quot;total&quot;:1}}"><header class="bg-linear-to-t border-b border-gray-100 pt-6 sm:pt-9 from-gray-50-to-white via-white dark:via-gray-950"><div class="container relative "><h1 class="flex flex-wrap items-center max-md:leading-tight mb-3 text-lg max-sm:gap-y-1.5 md:text-xl"><a href="/datasets" class="group flex items-center"><svg class="sm:mr-1.5 -mr-1 text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					<span class="mr-2.5 font-semibold text-gray-400 group-hover:text-gray-500 max-sm:hidden">Datasets:</span></a>
				<hr class="mx-1.5 h-2 translate-y-px rounded-sm border-r dark:border-gray-600 sm:hidden">
			<div class="group flex flex-none items-center"><div class="relative mr-1 flex items-center">

			

<span class="inline-block "><span class="contents"><a href="/MaartenGr" class="text-gray-400 hover:text-blue-600"><img alt="" class="w-3.5 h-3.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/62ea1ac3cc08a09aa6d3ec95/_74xXYEYLLjNVJ9zQucfn.jpeg" crossorigin="anonymous"></a></span>
	</span></div>
		

<span class="inline-block "><span class="contents"><a href="/MaartenGr" class="text-gray-400 hover:text-blue-600">MaartenGr</a></span>
	</span>
		<div class="mx-0.5 text-gray-300">/</div></div>

<div class="max-w-full "><a class="break-words font-mono font-semibold hover:text-blue-600 " href="/datasets/MaartenGr/arxiv_nlp">arxiv_nlp</a>
	<button class="relative text-sm mr-4 focus:outline-hidden inline-flex cursor-pointer items-center text-sm  mx-0.5   text-gray-600 " title="Copy dataset name to clipboard" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg>
	
	</button></div>
			<div class="inline-flex items-center overflow-hidden whitespace-nowrap rounded-md border bg-white text-sm leading-none text-gray-500  mr-2"><button class="relative flex items-center overflow-hidden from-red-50 to-transparent dark:from-red-900 px-1.5 py-1 hover:bg-linear-to-t focus:outline-hidden"  title="Like"><svg class="left-1.5 absolute" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>

		
		<span class="ml-4 pl-0.5 ">like</span></button>
	<button class="focus:outline-hidden flex items-center border-l px-1.5 py-1 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="See users who liked this repository">4</button></div>




			
			
	</h1>
		<div class="mb-3 flex flex-wrap md:mb-4"><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Modalities:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?modality=modality%3Atext"><div class="tag tag-white   ">
		<svg class="text-red-700 dark:text-red-600" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.619 4.619C2.667 6.573 2.667 9.715 2.667 16s0 9.428 1.952 11.38C6.573 29.333 9.715 29.333 16 29.333s9.428 0 11.38-1.953c1.953-1.95 1.953-5.095 1.953-11.38s0-9.428-1.953-11.381C25.43 2.667 22.285 2.667 16 2.667s-9.428 0-11.381 1.952m8.65 3.714c-.573 0-1.109 0-1.546.066-.495.073-1.003.248-1.41.7-.392.436-.53.956-.59 1.452-.056.464-.056 1.04-.056 1.689V13a1 1 0 1 0 2 0v-.704c0-.724.001-1.176.041-1.505q.015-.15.061-.294a.2.2 0 0 1 .031-.061q0-.003.016-.01a.8.8 0 0 1 .203-.05c.272-.04.654-.043 1.314-.043H15v11.334h-2.333a1 1 0 1 0 0 2H20a1 1 0 0 0 0-2h-3V10.333h1.667c.66 0 1.042.003 1.314.043.123.019.18.04.203.05l.015.009a.2.2 0 0 1 .032.061c.018.05.042.14.061.295.04.329.041.781.041 1.506V13a1 1 0 1 0 2 0v-.76c0-.65 0-1.225-.056-1.69-.06-.495-.198-1.015-.59-1.453-.407-.45-.915-.625-1.41-.698-.437-.067-.973-.067-1.546-.066z" fill="currentColor"></path></svg>

	

	<span>Text</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Formats:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?format=format%3Acsv"><div class="tag tag-white   ">
		<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.508 4.486h3.98v-1h-3.98v1Zm5.004 0h3.98v-1h-3.98v1ZM5.488 6.5h-3.98v-1h3.98v1Zm1.024 0h3.98v-1h-3.98v1ZM5.488 8.514h-3.98v-1h3.98v1Zm1.024 0h3.98v-1h-3.98v1Z" fill="currentColor"></path></svg>

	

	<span>csv</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Languages:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?language=language%3Aen"><div class="tag tag-white   ">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="text-green-600/80" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 10 10"><path fill-rule="evenodd" clip-rule="evenodd" d="M0.625 5C0.625 6.16032 1.08594 7.27312 1.90641 8.09359C2.72688 8.91406 3.83968 9.375 5 9.375C6.16032 9.375 7.27312 8.91406 8.09359 8.09359C8.91406 7.27312 9.375 6.16032 9.375 5C9.375 3.83968 8.91406 2.72688 8.09359 1.90641C7.27312 1.08594 6.16032 0.625 5 0.625C3.83968 0.625 2.72688 1.08594 1.90641 1.90641C1.08594 2.72688 0.625 3.83968 0.625 5ZM7.64365 7.48027C7.61734 7.50832 7.59054 7.53598 7.56326 7.56326C7.13828 7.98824 6.61864 8.2968 6.0539 8.46842C6.29802 8.11949 6.49498 7.64804 6.63475 7.09483C7.00845 7.18834 7.35014 7.3187 7.64365 7.48027ZM8.10076 6.87776C8.37677 6.42196 8.55005 5.90894 8.60556 5.37499H6.86808C6.85542 5.71597 6.82551 6.04557 6.77971 6.35841C7.25309 6.47355 7.68808 6.6414 8.062 6.85549C8.07497 6.86283 8.08789 6.87025 8.10076 6.87776ZM6.03795 6.22536C6.07708 5.95737 6.1044 5.67232 6.11705 5.37499H3.88295C3.89666 5.69742 3.92764 6.00542 3.9722 6.29287C4.37075 6.21726 4.79213 6.17749 5.224 6.17749C5.50054 6.17749 5.77294 6.19376 6.03795 6.22536ZM4.1261 7.02673C4.34894 7.84835 4.68681 8.375 5 8.375C5.32122 8.375 5.66839 7.82101 5.8908 6.963C5.67389 6.93928 5.45082 6.92699 5.224 6.92699C4.84316 6.92699 4.47332 6.96176 4.1261 7.02673ZM3.39783 7.21853C3.53498 7.71842 3.72038 8.14579 3.9461 8.46842C3.42141 8.30898 2.93566 8.03132 2.52857 7.65192C2.77253 7.48017 3.06711 7.33382 3.39783 7.21853ZM3.23916 6.48077C3.18263 6.13193 3.14625 5.76074 3.13192 5.37499H1.39444C1.4585 5.99112 1.67936 6.57938 2.03393 7.08403C2.3706 6.83531 2.78055 6.63162 3.23916 6.48077ZM1.39444 4.62499H3.13192C3.14615 4.24204 3.18211 3.87344 3.23794 3.52681C2.77814 3.37545 2.36731 3.17096 2.03024 2.92123C1.67783 3.42469 1.45828 4.011 1.39444 4.62499ZM2.5237 2.35262C2.76812 2.52552 3.06373 2.67281 3.39584 2.78875C3.53318 2.28573 3.71928 1.85578 3.9461 1.53158C3.41932 1.69166 2.93178 1.97089 2.5237 2.35262ZM3.97101 3.71489C3.92709 4.00012 3.89654 4.30547 3.88295 4.62499H6.11705C6.10453 4.33057 6.07761 4.04818 6.03909 3.78248C5.77372 3.81417 5.50093 3.83049 5.224 3.83049C4.79169 3.83049 4.3699 3.79065 3.97101 3.71489ZM5.8928 3.04476C5.67527 3.06863 5.45151 3.08099 5.224 3.08099C4.84241 3.08099 4.47186 3.04609 4.12405 2.98086C4.34686 2.1549 4.68584 1.625 5 1.625C5.32218 1.625 5.67048 2.18233 5.8928 3.04476ZM6.78083 3.6493C6.826 3.95984 6.85552 4.28682 6.86808 4.62499H8.60556C8.55029 4.09337 8.37827 3.58251 8.10436 3.1282C8.0903 3.1364 8.07618 3.14449 8.062 3.15249C7.68838 3.36641 7.25378 3.53417 6.78083 3.6493ZM7.64858 2.52499C7.35446 2.68754 7.0117 2.81868 6.63664 2.91268C6.49676 2.35623 6.29913 1.88209 6.0539 1.53158C6.61864 1.7032 7.13828 2.01176 7.56326 2.43674C7.59224 2.46572 7.62068 2.49514 7.64858 2.52499Z" fill="currentColor"></path></svg>

	

	<span>English</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Size:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?size_categories=size_categories%3A10K%3Cn%3C100K"><div class="tag tag-white   ">

	

	<span>10K - 100K</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Libraries:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Adatasets"><div class="tag tag-white   "><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 95 88"><path fill="#fff" d="M94.25 70.08a8.28 8.28 0 0 1-.43 6.46 10.57 10.57 0 0 1-3 3.6 25.18 25.18 0 0 1-5.7 3.2 65.74 65.74 0 0 1-7.56 2.65 46.67 46.67 0 0 1-11.42 1.68c-5.42.05-10.09-1.23-13.4-4.5a40.4 40.4 0 0 1-10.14.03c-3.34 3.25-7.99 4.52-13.39 4.47a46.82 46.82 0 0 1-11.43-1.68 66.37 66.37 0 0 1-7.55-2.65c-2.28-.98-4.17-2-5.68-3.2a10.5 10.5 0 0 1-3.02-3.6c-.99-2-1.18-4.3-.42-6.46a8.54 8.54 0 0 1-.33-5.63c.25-.95.66-1.83 1.18-2.61a8.67 8.67 0 0 1 2.1-8.47 8.23 8.23 0 0 1 2.82-2.07 41.75 41.75 0 1 1 81.3-.12 8.27 8.27 0 0 1 3.11 2.19 8.7 8.7 0 0 1 2.1 8.47c.52.78.93 1.66 1.18 2.61a8.61 8.61 0 0 1-.32 5.63Z"></path><path fill="#FFD21E" d="M47.21 76.5a34.75 34.75 0 1 0 0-69.5 34.75 34.75 0 0 0 0 69.5Z"></path><path fill="#FF9D0B" d="M81.96 41.75a34.75 34.75 0 1 0-69.5 0 34.75 34.75 0 0 0 69.5 0Zm-73.5 0a38.75 38.75 0 1 1 77.5 0 38.75 38.75 0 0 1-77.5 0Z"></path><path fill="#3A3B45" d="M58.5 32.3c1.28.44 1.78 3.06 3.07 2.38a5 5 0 1 0-6.76-2.07c.61 1.15 2.55-.72 3.7-.32ZM34.95 32.3c-1.28.44-1.79 3.06-3.07 2.38a5 5 0 1 1 6.76-2.07c-.61 1.15-2.56-.72-3.7-.32Z"></path><path fill="#FF323D" d="M46.96 56.29c9.83 0 13-8.76 13-13.26 0-2.34-1.57-1.6-4.09-.36-2.33 1.15-5.46 2.74-8.9 2.74-7.19 0-13-6.88-13-2.38s3.16 13.26 13 13.26Z"></path><path fill="#3A3B45" fill-rule="evenodd" d="M39.43 54a8.7 8.7 0 0 1 5.3-4.49c.4-.12.81.57 1.24 1.28.4.68.82 1.37 1.24 1.37.45 0 .9-.68 1.33-1.35.45-.7.89-1.38 1.32-1.25a8.61 8.61 0 0 1 5 4.17c3.73-2.94 5.1-7.74 5.1-10.7 0-2.34-1.57-1.6-4.09-.36l-.14.07c-2.31 1.15-5.39 2.67-8.77 2.67s-6.45-1.52-8.77-2.67c-2.6-1.29-4.23-2.1-4.23.29 0 3.05 1.46 8.06 5.47 10.97Z" clip-rule="evenodd"></path><path fill="#FF9D0B" d="M70.71 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM24.21 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM17.52 48c-1.62 0-3.06.66-4.07 1.87a5.97 5.97 0 0 0-1.33 3.76 7.1 7.1 0 0 0-1.94-.3c-1.55 0-2.95.59-3.94 1.66a5.8 5.8 0 0 0-.8 7 5.3 5.3 0 0 0-1.79 2.82c-.24.9-.48 2.8.8 4.74a5.22 5.22 0 0 0-.37 5.02c1.02 2.32 3.57 4.14 8.52 6.1 3.07 1.22 5.89 2 5.91 2.01a44.33 44.33 0 0 0 10.93 1.6c5.86 0 10.05-1.8 12.46-5.34 3.88-5.69 3.33-10.9-1.7-15.92-2.77-2.78-4.62-6.87-5-7.77-.78-2.66-2.84-5.62-6.25-5.62a5.7 5.7 0 0 0-4.6 2.46c-1-1.26-1.98-2.25-2.86-2.82A7.4 7.4 0 0 0 17.52 48Zm0 4c.51 0 1.14.22 1.82.65 2.14 1.36 6.25 8.43 7.76 11.18.5.92 1.37 1.31 2.14 1.31 1.55 0 2.75-1.53.15-3.48-3.92-2.93-2.55-7.72-.68-8.01.08-.02.17-.02.24-.02 1.7 0 2.45 2.93 2.45 2.93s2.2 5.52 5.98 9.3c3.77 3.77 3.97 6.8 1.22 10.83-1.88 2.75-5.47 3.58-9.16 3.58-3.81 0-7.73-.9-9.92-1.46-.11-.03-13.45-3.8-11.76-7 .28-.54.75-.76 1.34-.76 2.38 0 6.7 3.54 8.57 3.54.41 0 .7-.17.83-.6.79-2.85-12.06-4.05-10.98-8.17.2-.73.71-1.02 1.44-1.02 3.14 0 10.2 5.53 11.68 5.53.11 0 .2-.03.24-.1.74-1.2.33-2.04-4.9-5.2-5.21-3.16-8.88-5.06-6.8-7.33.24-.26.58-.38 1-.38 3.17 0 10.66 6.82 10.66 6.82s2.02 2.1 3.25 2.1c.28 0 .52-.1.68-.38.86-1.46-8.06-8.22-8.56-11.01-.34-1.9.24-2.85 1.31-2.85Z"></path><path fill="#FFD21E" d="M38.6 76.69c2.75-4.04 2.55-7.07-1.22-10.84-3.78-3.77-5.98-9.3-5.98-9.3s-.82-3.2-2.69-2.9c-1.87.3-3.24 5.08.68 8.01 3.91 2.93-.78 4.92-2.29 2.17-1.5-2.75-5.62-9.82-7.76-11.18-2.13-1.35-3.63-.6-3.13 2.2.5 2.79 9.43 9.55 8.56 11-.87 1.47-3.93-1.71-3.93-1.71s-9.57-8.71-11.66-6.44c-2.08 2.27 1.59 4.17 6.8 7.33 5.23 3.16 5.64 4 4.9 5.2-.75 1.2-12.28-8.53-13.36-4.4-1.08 4.11 11.77 5.3 10.98 8.15-.8 2.85-9.06-5.38-10.74-2.18-1.7 3.21 11.65 6.98 11.76 7.01 4.3 1.12 15.25 3.49 19.08-2.12Z"></path><path fill="#FF9D0B" d="M77.4 48c1.62 0 3.07.66 4.07 1.87a5.97 5.97 0 0 1 1.33 3.76 7.1 7.1 0 0 1 1.95-.3c1.55 0 2.95.59 3.94 1.66a5.8 5.8 0 0 1 .8 7 5.3 5.3 0 0 1 1.78 2.82c.24.9.48 2.8-.8 4.74a5.22 5.22 0 0 1 .37 5.02c-1.02 2.32-3.57 4.14-8.51 6.1-3.08 1.22-5.9 2-5.92 2.01a44.33 44.33 0 0 1-10.93 1.6c-5.86 0-10.05-1.8-12.46-5.34-3.88-5.69-3.33-10.9 1.7-15.92 2.78-2.78 4.63-6.87 5.01-7.77.78-2.66 2.83-5.62 6.24-5.62a5.7 5.7 0 0 1 4.6 2.46c1-1.26 1.98-2.25 2.87-2.82A7.4 7.4 0 0 1 77.4 48Zm0 4c-.51 0-1.13.22-1.82.65-2.13 1.36-6.25 8.43-7.76 11.18a2.43 2.43 0 0 1-2.14 1.31c-1.54 0-2.75-1.53-.14-3.48 3.91-2.93 2.54-7.72.67-8.01a1.54 1.54 0 0 0-.24-.02c-1.7 0-2.45 2.93-2.45 2.93s-2.2 5.52-5.97 9.3c-3.78 3.77-3.98 6.8-1.22 10.83 1.87 2.75 5.47 3.58 9.15 3.58 3.82 0 7.73-.9 9.93-1.46.1-.03 13.45-3.8 11.76-7-.29-.54-.75-.76-1.34-.76-2.38 0-6.71 3.54-8.57 3.54-.42 0-.71-.17-.83-.6-.8-2.85 12.05-4.05 10.97-8.17-.19-.73-.7-1.02-1.44-1.02-3.14 0-10.2 5.53-11.68 5.53-.1 0-.19-.03-.23-.1-.74-1.2-.34-2.04 4.88-5.2 5.23-3.16 8.9-5.06 6.8-7.33-.23-.26-.57-.38-.98-.38-3.18 0-10.67 6.82-10.67 6.82s-2.02 2.1-3.24 2.1a.74.74 0 0 1-.68-.38c-.87-1.46 8.05-8.22 8.55-11.01.34-1.9-.24-2.85-1.31-2.85Z"></path><path fill="#FFD21E" d="M56.33 76.69c-2.75-4.04-2.56-7.07 1.22-10.84 3.77-3.77 5.97-9.3 5.97-9.3s.82-3.2 2.7-2.9c1.86.3 3.23 5.08-.68 8.01-3.92 2.93.78 4.92 2.28 2.17 1.51-2.75 5.63-9.82 7.76-11.18 2.13-1.35 3.64-.6 3.13 2.2-.5 2.79-9.42 9.55-8.55 11 .86 1.47 3.92-1.71 3.92-1.71s9.58-8.71 11.66-6.44c2.08 2.27-1.58 4.17-6.8 7.33-5.23 3.16-5.63 4-4.9 5.2.75 1.2 12.28-8.53 13.36-4.4 1.08 4.11-11.76 5.3-10.97 8.15.8 2.85 9.05-5.38 10.74-2.18 1.69 3.21-11.65 6.98-11.76 7.01-4.31 1.12-15.26 3.49-19.08-2.12Z"></path></svg>

	

	<span>Datasets</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Apandas"><div class="tag tag-white   "><svg class="text-black inline-block text-sm text-[#130754] dark:text-gray-200" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid slice" viewBox="0 0 210.21 280.43"><rect fill="currentColor" x="74.51" y="43.03" width="24.09" height="50.02"></rect><rect fill="currentColor" x="74.51" y="145.78" width="24.09" height="50.02"></rect><rect fill="#ffca00" x="74.51" y="107.65" width="24.09" height="23.6"></rect><rect fill="currentColor" x="35.81" y="84.15" width="24.09" height="166.27"></rect><rect fill="currentColor" x="112.41" y="187.05" width="24.09" height="50.02"></rect><rect fill="currentColor" x="112.41" y="84.21" width="24.09" height="50.02"></rect><rect fill="#e70488" x="112.41" y="148.84" width="24.09" height="23.6"></rect><rect fill="currentColor" x="150.3" y="30" width="24.09" height="166.27"></rect></svg>

	

	<span>pandas</span>
	

	</div></a><div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-lg rounded-br-none " type="button">
		<div class="tag tag-white   relative rounded-br-none pr-2.5"><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="none" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="#F5AB6A"></path><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="url(#paint0_radial_18_31665)"></path><g filter="url(#filter0_f_18_31665)"><path d="M22.2849 12.1817L23.4375 13.2656L24.4375 4.70312C23.5121 4.1242 23.0198 3.96369 22.6563 3.89062L22.2849 12.1817Z" fill="url(#paint1_linear_18_31665)"></path></g><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint2_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint3_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint4_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint5_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint6_linear_18_31665)"></path><g filter="url(#filter1_f_18_31665)"><path d="M13.1016 2.72656C11.862 3.06924 11.5298 3.40016 11.5298 3.40016C10.9102 3.52335 10.936 4.11525 11.2408 4.6799C13.1487 6.95202 15.1361 13.2496 18.007 14.0958C18.2707 14.1633 18.6953 14.2107 19.1797 14.2344L13.1016 2.72656Z" fill="url(#paint7_linear_18_31665)"></path></g><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint8_radial_18_31665)"></path><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint9_radial_18_31665)"></path><g filter="url(#filter2_f_18_31665)"><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint10_linear_18_31665)"></path><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint11_radial_18_31665)"></path></g><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="#EC9F6A"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint12_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint13_linear_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint14_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint15_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint16_radial_18_31665)"></path><g filter="url(#filter3_f_18_31665)"><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint17_linear_18_31665)"></path><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint18_linear_18_31665)"></path></g><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="#D79453"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint19_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint20_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint21_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint22_linear_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint23_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint24_radial_18_31665)"></path><defs><filter id="filter0_f_18_31665" x="22.0349" y="3.64062" width="2.65265" height="9.875" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter1_f_18_31665" x="10.7815" y="2.47656" width="8.64819" height="12.0078" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter2_f_18_31665" x="3.89062" y="22.5416" width="9.57812" height="2.2804" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter3_f_18_31665" x="2.45312" y="11.2538" width="12.1255" height="8.50903" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><radialGradient id="paint0_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.8125 12.9375) rotate(42.7741) scale(12.5164 7.08839)"><stop offset="0.0937591" stop-color="#C05159"></stop><stop offset="0.553697" stop-color="#F6AC6A"></stop><stop offset="0.832916" stop-color="#FFD186"></stop><stop offset="0.916927" stop-color="#FFDC87"></stop></radialGradient><linearGradient id="paint1_linear_18_31665" x1="24.7344" y1="4.67187" x2="20.8594" y2="12.8906" gradientUnits="userSpaceOnUse"><stop stop-color="#EBD67C"></stop><stop offset="0.0655686" stop-color="#FFFFA6"></stop><stop offset="0.530552" stop-color="#F8C281"></stop><stop offset="0.937338" stop-color="#E99E6B"></stop></linearGradient><radialGradient id="paint2_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.9009 13.1847) rotate(-127.648) scale(14.3438 11.7966)"><stop stop-color="#FFBE66"></stop><stop offset="1" stop-color="#E2AE5B"></stop></radialGradient><radialGradient id="paint3_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(18 11.4375) rotate(53.9726) scale(11.9013 4.84018)"><stop stop-color="#D67C63"></stop><stop offset="1" stop-color="#D97D67" stop-opacity="0"></stop></radialGradient><radialGradient id="paint4_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(23 4.1875) rotate(45.7639) scale(3.31486 5.75622)"><stop stop-color="#FFE4A6"></stop><stop offset="0.711285" stop-color="#F8B76F"></stop><stop offset="1" stop-color="#F9B870" stop-opacity="0"></stop></radialGradient><radialGradient id="paint5_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.875 12.4375) rotate(88.9391) scale(3.37558 1.29066)"><stop stop-color="#FFBC67"></stop><stop offset="1" stop-color="#FFBC67" stop-opacity="0"></stop></radialGradient><linearGradient id="paint6_linear_18_31665" x1="20.375" y1="15.6875" x2="20.125" y2="12.7813" gradientUnits="userSpaceOnUse"><stop offset="0.461609" stop-color="#B45077"></stop><stop offset="0.855389" stop-color="#B75077" stop-opacity="0"></stop></linearGradient><linearGradient id="paint7_linear_18_31665" x1="12.9375" y1="2.57056" x2="18.5625" y2="14.3891" gradientUnits="userSpaceOnUse"><stop stop-color="#DDC173"></stop><stop offset="0.485173" stop-color="#D59F65"></stop><stop offset="1" stop-color="#E49966"></stop></linearGradient><radialGradient id="paint8_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.5625 23.5) rotate(109.113) scale(6.68078 10.2578)"><stop offset="0.165756" stop-color="#FFBF7E"></stop><stop offset="0.827674" stop-color="#DF8C6D"></stop><stop offset="1" stop-color="#B05A66"></stop></radialGradient><radialGradient id="paint9_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.1875 26) rotate(41.0652) scale(8.37243 2.03649)"><stop stop-color="#FFD483"></stop><stop offset="1" stop-color="#FFD688" stop-opacity="0"></stop></radialGradient><linearGradient id="paint10_linear_18_31665" x1="3.96063" y1="23.794" x2="13.3748" y2="23.5143" gradientUnits="userSpaceOnUse"><stop stop-color="#A8716F"></stop><stop offset="0.103615" stop-color="#B37173"></stop><stop offset="0.225484" stop-color="#DB9F84"></stop><stop offset="0.799889" stop-color="#F1BB8A"></stop><stop offset="1" stop-color="#FFD780"></stop></linearGradient><radialGradient id="paint11_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4219 23.1719) rotate(-178.616) scale(3.23532 0.569081)"><stop offset="0.621498" stop-color="#AF5A3E"></stop><stop offset="1" stop-color="#B35445" stop-opacity="0"></stop></radialGradient><radialGradient id="paint12_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.625 19.125) rotate(-171.737) scale(15.2205 15.0767)"><stop offset="0.138435" stop-color="#FFB974"></stop><stop offset="0.403618" stop-color="#F2A56D"></stop><stop offset="0.925938" stop-color="#A16948"></stop></radialGradient><linearGradient id="paint13_linear_18_31665" x1="8.22184" y1="13.125" x2="6.81191" y2="15.4996" gradientUnits="userSpaceOnUse"><stop offset="0.610751" stop-color="#984847"></stop><stop offset="0.850075" stop-color="#9A4947" stop-opacity="0"></stop></linearGradient><radialGradient id="paint14_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.25 23.7461) scale(11.25 5.68361)"><stop stop-color="#C66364"></stop><stop offset="1" stop-color="#D4766B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint15_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.875 23.5313) scale(10.0937 1.29657)"><stop stop-color="#B64B4B"></stop><stop offset="1" stop-color="#C56158" stop-opacity="0"></stop></radialGradient><radialGradient id="paint16_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4375 19.875) rotate(-46.8882) scale(4.02385 7.51767)"><stop stop-color="#FFC083"></stop><stop offset="0.620218" stop-color="#FFBD7D" stop-opacity="0"></stop></radialGradient><linearGradient id="paint17_linear_18_31665" x1="2.8125" y1="13.0312" x2="14.5582" y2="18.9404" gradientUnits="userSpaceOnUse"><stop stop-color="#B89367"></stop><stop offset="1" stop-color="#C5835E"></stop></linearGradient><linearGradient id="paint18_linear_18_31665" x1="8.21875" y1="14.6406" x2="7.59349" y2="15.6717" gradientUnits="userSpaceOnUse"><stop offset="0.351552" stop-color="#A74746"></stop><stop offset="0.845198" stop-color="#A04346" stop-opacity="0"></stop></linearGradient><radialGradient id="paint19_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.5625 14.5625) rotate(140.244) scale(18.3733 13.7403)"><stop stop-color="#FDAE69"></stop><stop offset="0.729021" stop-color="#CE8C4F"></stop><stop offset="0.921546" stop-color="#AD7B45"></stop><stop offset="1" stop-color="#8B6B4A"></stop></radialGradient><radialGradient id="paint20_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.0625 7) rotate(65.3152) scale(11.0745 3.16547)"><stop offset="0.233237" stop-color="#FFD47C"></stop><stop offset="0.853648" stop-color="#FFD98B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint21_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.3125 8.875) rotate(100.886) scale(6.6191 5.57808)"><stop offset="0.128419" stop-color="#FFD88C"></stop><stop offset="0.924134" stop-color="#FFBE7B" stop-opacity="0"></stop></radialGradient><linearGradient id="paint22_linear_18_31665" x1="7.25" y1="15.1875" x2="10.7588" y2="10.3142" gradientUnits="userSpaceOnUse"><stop offset="0.142353" stop-color="#C15F4D"></stop><stop offset="1" stop-color="#D58366" stop-opacity="0"></stop></linearGradient><radialGradient id="paint23_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(8.15625 15.7813) rotate(28.5422) scale(12.5574 1.96589)"><stop offset="0.149989" stop-color="#E4745D"></stop><stop offset="0.453292" stop-color="#C8604C"></stop><stop offset="0.632597" stop-color="#C0605F"></stop><stop offset="1" stop-color="#C0605F" stop-opacity="0"></stop></radialGradient><radialGradient id="paint24_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(1.40625 2.69067) rotate(46.0943) scale(22.3963)"><stop offset="0.935802" stop-color="#C17C61" stop-opacity="0"></stop><stop offset="0.982109" stop-color="#C17C61"></stop></radialGradient></defs></svg>

	

	<span>Croissant</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	<button class="tag tag-ghost px-1! -ml-0.5 mb-1 md:mb-1.5" type="button">+ 1</button></div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">License:
	</span>
	<div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-full rounded-br-none " type="button">
		<div class="tag tag-white rounded-full  relative rounded-br-none pr-2.5">
		<svg class="text-xs text-gray-900" width="1em" height="1em" viewBox="0 0 10 10" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.46009 5.0945V6.88125C1.46009 7.25201 1.75937 7.55129 2.13012 7.55129C2.50087 7.55129 2.80016 7.25201 2.80016 6.88125V5.0945C2.80016 4.72375 2.50087 4.42446 2.13012 4.42446C1.75937 4.42446 1.46009 4.72375 1.46009 5.0945ZM4.14022 5.0945V6.88125C4.14022 7.25201 4.4395 7.55129 4.81026 7.55129C5.18101 7.55129 5.48029 7.25201 5.48029 6.88125V5.0945C5.48029 4.72375 5.18101 4.42446 4.81026 4.42446C4.4395 4.42446 4.14022 4.72375 4.14022 5.0945ZM1.23674 9.78473H8.38377C8.75452 9.78473 9.0538 9.48545 9.0538 9.1147C9.0538 8.74395 8.75452 8.44466 8.38377 8.44466H1.23674C0.865993 8.44466 0.566711 8.74395 0.566711 9.1147C0.566711 9.48545 0.865993 9.78473 1.23674 9.78473ZM6.82036 5.0945V6.88125C6.82036 7.25201 7.11964 7.55129 7.49039 7.55129C7.86114 7.55129 8.16042 7.25201 8.16042 6.88125V5.0945C8.16042 4.72375 7.86114 4.42446 7.49039 4.42446C7.11964 4.42446 6.82036 4.72375 6.82036 5.0945ZM4.39484 0.623142L0.865993 2.48137C0.682851 2.57517 0.566711 2.76725 0.566711 2.97273C0.566711 3.28094 0.816857 3.53109 1.12507 3.53109H8.49991C8.80365 3.53109 9.0538 3.28094 9.0538 2.97273C9.0538 2.76725 8.93766 2.57517 8.75452 2.48137L5.22568 0.623142C4.9666 0.484669 4.65391 0.484669 4.39484 0.623142V0.623142Z" fill="currentColor"></path></svg>

	

	<span>mit</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	</div></div>

		<div class="flex flex-col-reverse lg:flex-row lg:items-center lg:justify-between"><div class="-mb-px flex h-12 items-center overflow-x-auto overflow-y-hidden ">
	<a class="tab-alternate active" href="/datasets/MaartenGr/arxiv_nlp"><svg class="mr-1.5 text-gray-400 flex-none" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
	Dataset card
	

	
		</a><a class="tab-alternate" href="/datasets/MaartenGr/arxiv_nlp/viewer/"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	Data Studio
	

	
		</a><a class="tab-alternate" href="/datasets/MaartenGr/arxiv_nlp/tree/main"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M21 19h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-8h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0 4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M9 19a1 1 0 0 1-1-1V6a1 1 0 0 1 2 0v12a1 1 0 0 1-1 1zm-6-4.333a1 1 0 0 1-.64-1.769L3.438 12l-1.078-.898a1 1 0 0 1 1.28-1.538l2 1.667a1 1 0 0 1 0 1.538l-2 1.667a.999.999 0 0 1-.64.231z" fill="currentColor"></path></svg>
	<span class="xl:hidden">Files</span>
		<span class="hidden xl:inline">Files and versions</span>
	

	
		</a><a class="tab-alternate" href="/datasets/MaartenGr/arxiv_nlp/discussions"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M20.6081 3C21.7684 3 22.8053 3.49196 23.5284 4.38415C23.9756 4.93678 24.4428 5.82749 24.4808 7.16133C24.9674 7.01707 25.4353 6.93643 25.8725 6.93643C26.9833 6.93643 27.9865 7.37587 28.696 8.17411C29.6075 9.19872 30.0124 10.4579 29.8361 11.7177C29.7523 12.3177 29.5581 12.8555 29.2678 13.3534C29.8798 13.8646 30.3306 14.5763 30.5485 15.4322C30.719 16.1032 30.8939 17.5006 29.9808 18.9403C30.0389 19.0342 30.0934 19.1319 30.1442 19.2318C30.6932 20.3074 30.7283 21.5229 30.2439 22.6548C29.5093 24.3704 27.6841 25.7219 24.1397 27.1727C21.9347 28.0753 19.9174 28.6523 19.8994 28.6575C16.9842 29.4379 14.3477 29.8345 12.0653 29.8345C7.87017 29.8345 4.8668 28.508 3.13831 25.8921C0.356375 21.6797 0.754104 17.8269 4.35369 14.1131C6.34591 12.058 7.67023 9.02782 7.94613 8.36275C8.50224 6.39343 9.97271 4.20438 12.4172 4.20438H12.4179C12.6236 4.20438 12.8314 4.2214 13.0364 4.25468C14.107 4.42854 15.0428 5.06476 15.7115 6.02205C16.4331 5.09583 17.134 4.359 17.7682 3.94323C18.7242 3.31737 19.6794 3 20.6081 3ZM20.6081 5.95917C20.2427 5.95917 19.7963 6.1197 19.3039 6.44225C17.7754 7.44319 14.8258 12.6772 13.7458 14.7131C13.3839 15.3952 12.7655 15.6837 12.2086 15.6837C11.1036 15.6837 10.2408 14.5497 12.1076 13.1085C14.9146 10.9402 13.9299 7.39584 12.5898 7.1776C12.5311 7.16799 12.4731 7.16355 12.4172 7.16355C11.1989 7.16355 10.6615 9.33114 10.6615 9.33114C10.6615 9.33114 9.0863 13.4148 6.38031 16.206C3.67434 18.998 3.5346 21.2388 5.50675 24.2246C6.85185 26.2606 9.42666 26.8753 12.0653 26.8753C14.8021 26.8753 17.6077 26.2139 19.1799 25.793C19.2574 25.7723 28.8193 22.984 27.6081 20.6107C27.4046 20.212 27.0693 20.0522 26.6471 20.0522C24.9416 20.0522 21.8393 22.6726 20.5057 22.6726C20.2076 22.6726 19.9976 22.5416 19.9116 22.222C19.3433 20.1173 28.552 19.2325 27.7758 16.1839C27.639 15.6445 27.2677 15.4256 26.746 15.4263C24.4923 15.4263 19.4358 19.5181 18.3759 19.5181C18.2949 19.5181 18.2368 19.4937 18.2053 19.4419C17.6743 18.557 17.9653 17.9394 21.7082 15.6009C25.4511 13.2617 28.0783 11.8545 26.5841 10.1752C26.4121 9.98141 26.1684 9.8956 25.8725 9.8956C23.6001 9.89634 18.2311 14.9403 18.2311 14.9403C18.2311 14.9403 16.7821 16.496 15.9057 16.496C15.7043 16.496 15.533 16.4139 15.4169 16.2112C14.7956 15.1296 21.1879 10.1286 21.5484 8.06535C21.7928 6.66715 21.3771 5.95917 20.6081 5.95917Z" fill="#FF9D00"></path><path d="M5.50686 24.2246C3.53472 21.2387 3.67446 18.9979 6.38043 16.206C9.08641 13.4147 10.6615 9.33111 10.6615 9.33111C10.6615 9.33111 11.2499 6.95933 12.59 7.17757C13.93 7.39581 14.9139 10.9401 12.1069 13.1084C9.29997 15.276 12.6659 16.7489 13.7459 14.713C14.8258 12.6772 17.7747 7.44316 19.304 6.44221C20.8326 5.44128 21.9089 6.00204 21.5484 8.06532C21.188 10.1286 14.795 15.1295 15.4171 16.2118C16.0391 17.2934 18.2312 14.9402 18.2312 14.9402C18.2312 14.9402 25.0907 8.49588 26.5842 10.1752C28.0776 11.8545 25.4512 13.2616 21.7082 15.6008C17.9646 17.9393 17.6744 18.557 18.2054 19.4418C18.7372 20.3266 26.9998 13.1351 27.7759 16.1838C28.5513 19.2324 19.3434 20.1173 19.9117 22.2219C20.48 24.3274 26.3979 18.2382 27.6082 20.6107C28.8193 22.9839 19.2574 25.7722 19.18 25.7929C16.0914 26.62 8.24723 28.3726 5.50686 24.2246Z" fill="#FFD21E"></path></svg>
	Community
	<div class="ml-1.5 flex h-4 min-w-[1rem] items-center justify-center rounded px-1 text-xs leading-none shadow-sm bg-black text-white dark:bg-gray-800 dark:text-gray-200">1</div>

	
		</a></div>
	
			</div></div></header>
</div>
	
<div class="container relative flex flex-col md:grid md:space-y-0 w-full md:grid-cols-12 md:flex-1 md:grid-rows-full space-y-4 md:gap-6 ">
		<section class="pt-6 border-gray-100 md:col-span-8 pb-24 relative break-words copiable-code-container">
				<div class="SVELTE_HYDRATER contents" data-target="UnsafeBanner" data-props="{&quot;classNames&quot;:&quot;mb-4&quot;,&quot;repoId&quot;:&quot;MaartenGr/arxiv_nlp&quot;,&quot;repoType&quot;:&quot;dataset&quot;,&quot;minLevel&quot;:&quot;unsafe&quot;}"></div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetViewer" data-props="{&quot;data&quot;:{&quot;kind&quot;:&quot;DatasetAndSampleData&quot;,&quot;datasetInfo&quot;:[{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Size of downloaded dataset files:&quot;,&quot;value&quot;:&quot;53.2 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;/datasets/MaartenGr/arxiv_nlp/tree/refs%2Fconvert%2Fparquet/&quot;,&quot;label&quot;:&quot;Size of the auto-converted Parquet files:&quot;,&quot;value&quot;:&quot;29.7 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Number of rows:&quot;,&quot;value&quot;:&quot;44,949&quot;}],&quot;partial&quot;:false,&quot;configsData&quot;:{&quot;configInfos&quot;:[{&quot;name&quot;:&quot;default&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:44949}],&quot;selectedConfig&quot;:&quot;default&quot;,&quot;hasSelectedConfigParquet&quot;:true},&quot;splitsData&quot;:{&quot;splitInfos&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;numRows&quot;:44949}],&quot;selectedSplit&quot;:&quot;train&quot;},&quot;sampleData&quot;:{&quot;dataset&quot;:&quot;MaartenGr/arxiv_nlp&quot;,&quot;config&quot;:&quot;default&quot;,&quot;split&quot;:&quot;train&quot;,&quot;capabilities&quot;:{&quot;rows&quot;:true,&quot;search&quot;:true,&quot;filter&quot;:true,&quot;statistics&quot;:true},&quot;navigation&quot;:{&quot;p&quot;:0},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkxODcyNSwic3ViIjoiL2RhdGFzZXRzL01hYXJ0ZW5Hci9hcnhpdl9ubHAiLCJleHAiOjE3NDI5MjIzMjUsImlzcyI6Imh0dHBzOi8vaHVnZ2luZ2ZhY2UuY28ifQ.b4HsT1bmqaXMXiUmxXq_0vXv327szcn-IXEjcIP2M1PkHdSmV6cGhDXC_7hI8QHuKperOxmyJFgX5U1pDPV9CA&quot;,&quot;sampleData&quot;:{&quot;columns&quot;:[{&quot;name&quot;:&quot;Titles&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;Titles&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:6,&quot;max&quot;:220,&quot;mean&quot;:76.6137,&quot;median&quot;:76,&quot;std&quot;:23.50551,&quot;histogram&quot;:{&quot;hist&quot;:[339,4719,15925,13529,7915,2096,327,82,14,3],&quot;bin_edges&quot;:[6,28,50,72,94,116,138,160,182,204,220]}}}},{&quot;name&quot;:&quot;Abstracts&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;Abstracts&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:37,&quot;max&quot;:3263,&quot;mean&quot;:1072.06565,&quot;median&quot;:1056,&quot;std&quot;:314.92469,&quot;histogram&quot;:{&quot;hist&quot;:[263,4363,14801,16613,6817,2084,4,1,0,3],&quot;bin_edges&quot;:[37,360,683,1006,1329,1652,1975,2298,2621,2944,3263]}}}},{&quot;name&quot;:&quot;Years&quot;,&quot;align&quot;:&quot;right&quot;,&quot;type&quot;:&quot;int64&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;Years&quot;,&quot;column_type&quot;:&quot;int&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:1991,&quot;max&quot;:2024,&quot;mean&quot;:2020.65568,&quot;median&quot;:2021,&quot;std&quot;:3.17204,&quot;histogram&quot;:{&quot;hist&quot;:[5,34,187,108,410,769,6341,22450,14645],&quot;bin_edges&quot;:[1991,1995,1999,2003,2007,2011,2015,2019,2023,2024]}}}},{&quot;name&quot;:&quot;Categories&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;Categories&quot;,&quot;column_type&quot;:&quot;string_label&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;no_label_count&quot;:0,&quot;no_label_proportion&quot;:0,&quot;n_unique&quot;:1,&quot;frequencies&quot;:{&quot;Computation and Language&quot;:44949}}}}],&quot;rows&quot;:[{&quot;rowIdx&quot;:0,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Introduction to Arabic Speech Recognition Using CMUSphinx System&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper Arabic was investigated from the speech recognition problem\npoint of view. We propose a novel approach to build an Arabic Automated Speech\nRecognition System (ASR). This system is based on the open source CMU Sphinx-4,\nfrom the Carnegie Mellon University. CMU Sphinx is a large-vocabulary;\nspeaker-independent, continuous speech recognition system based on discrete\nHidden Markov Models (HMMs). We build a model using utilities from the\nOpenSource CMU Sphinx. We will demonstrate the possible adaptability of this\nsystem to Arabic voice recognition.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:1,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Arabic Speech Recognition System using CMU-Sphinx4&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper we present the creation of an Arabic version of Automated\nSpeech Recognition System (ASR). This system is based on the open source\nSphinx-4, from the Carnegie Mellon University. Which is a speech recognition\nsystem based on discrete hidden Markov models (HMMs). We investigate the\nchanges that must be made to the model to adapt Arabic voice recognition.\n  Keywords: Speech recognition, Acoustic model, Arabic language, HMMs,\nCMUSphinx-4, Artificial intelligence.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:2,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;On the Development of Text Input Method - Lessons Learned&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Intelligent Input Methods (IM) are essential for making text entries in many\nEast Asian scripts, but their application to other languages has not been fully\nexplored. This paper discusses how such tools can contribute to the development\nof computer processing of other oriental languages. We propose a design\nphilosophy that regards IM as a text service platform, and treats the study of\nIM as a cross disciplinary subject from the perspectives of software\nengineering, human-computer interaction (HCI), and natural language processing\n(NLP). We discuss these three perspectives and indicate a number of possible\nfuture research directions.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:3,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Network statistics on early English Syntax: Structural criteria&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This paper includes a reflection on the role of networks in the study of\nEnglish language acquisition, as well as a collection of practical criteria to\nannotate free-speech corpora from children utterances. At the theoretical\nlevel, the main claim of this paper is that syntactic networks should be\ninterpreted as the outcome of the use of the syntactic machinery. Thus, the\nintrinsic features of such machinery are not accessible directly from (known)\nnetwork properties. Rather, what one can see are the global patterns of its use\nand, thus, a global view of the power and organization of the underlying\ngrammar. Taking a look into more practical issues, the paper examines how to\nbuild a net from the projection of syntactic relations. Recall that, as opposed\nto adult grammars, early-child language has not a well-defined concept of\nstructure. To overcome such difficulty, we develop a set of systematic criteria\nassuming constituency hierarchy and a grammar based on lexico-thematic\nrelations. At the end, what we obtain is a well defined corpora annotation that\nenables us i) to perform statistics on the size of structures and ii) to build\na network from syntactic relations over which we can perform the standard\nmeasures of complexity. We also provide a detailed example.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:4,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Segmentation and Context of Literary and Musical Sequences&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We test a segmentation algorithm, based on the calculation of the\nJensen-Shannon divergence between probability distributions, to two symbolic\nsequences of literary and musical origin. The first sequence represents the\nsuccessive appearance of characters in a theatrical play, and the second\nrepresents the succession of tones from the twelve-tone scale in a keyboard\nsonata. The algorithm divides the sequences into segments of maximal\ncompositional divergence between them. For the play, these segments are related\nto changes in the frequency of appearance of different characters and in the\ngeographical setting of the action. For the sonata, the segments correspond to\ntonal domains and reveal in detail the characteristic tonal progression of such\nkind of musical composition.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:5,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;International Standard for a Linguistic Annotation Framework&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This paper describes the Linguistic Annotation Framework under development\nwithin ISO TC37 SC4 WG1. The Linguistic Annotation Framework is intended to\nserve as a basis for harmonizing existing language resources as well as\ndeveloping new ones.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2004,&quot;string&quot;:&quot;2,004&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:6,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Formal Model of Dictionary Structure and Content&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We show that a general model of lexical information conforms to an abstract\nmodel that reflects the hierarchy of information found in a typical dictionary\nentry. We show that this model can be mapped into a well-formed XML document,\nand how the XSL transformation language can be used to implement a semantics\ndefined over the abstract model to enable extraction and manipulation of the\ninformation in any format.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2000,&quot;string&quot;:&quot;2,000&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:7,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Practical Approach to Knowledge-based Question Answering with Natural\n  Language Understanding and Advanced Reasoning&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This research hypothesized that a practical approach in the form of a\nsolution framework known as Natural Language Understanding and Reasoning for\nIntelligence (NaLURI), which combines full-discourse natural language\nunderstanding, powerful representation formalism capable of exploiting\nontological information and reasoning approach with advanced features, will\nsolve the following problems without compromising practicality factors: 1)\nrestriction on the nature of question and response, and 2) limitation to scale\nacross domains and to real-life natural language text.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:8,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Learning Probabilistic Models of Word Sense Disambiguation&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This dissertation presents several new methods of supervised and unsupervised\nlearning of word sense disambiguation models. The supervised methods focus on\nperforming model searches through a space of probabilistic models, and the\nunsupervised methods rely on the use of Gibbs Sampling and the Expectation\nMaximization (EM) algorithm. In both the supervised and unsupervised case, the\nNaive Bayesian model is found to perform well. An explanation for this success\nis presented in terms of learning rates and bias-variance decompositions.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1998,&quot;string&quot;:&quot;1,998&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:9,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Learning Phonotactics Using ILP&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This paper describes experiments on learning Dutch phonotactic rules using\nInductive Logic Programming, a machine learning discipline based on inductive\nlogical operators. Two different ways of approaching the problem are\nexperimented with, and compared against each other as well as with related work\non the task. The results show a direct correspondence between the quality and\ninformedness of the background knowledge and the constructed theory,\ndemonstrating the ability of ILP to take good advantage of the prior domain\nknowledge available. Further research is outlined.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2002,&quot;string&quot;:&quot;2,002&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:10,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Bootstrapping Deep Lexical Resources: Resources for Courses&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We propose a range of deep lexical acquisition methods which make use of\nmorphological, syntactic and ontological language resources to model word\nsimilarity and bootstrap from a seed lexicon. The different methods are\ndeployed in learning lexical items for a precision grammar, and shown to each\nhave strengths and weaknesses over different word classes. A particular focus\nof this paper is the relative accessibility of different language resource\ntypes, and predicted ``bang for the buck'' associated with each in deep lexical\nacquisition applications.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2005,&quot;string&quot;:&quot;2,005&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:11,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Bio-linguistic transition and Baldwin effect in an evolutionary\n  naming-game model&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We examine an evolutionary naming-game model where communicating agents are\nequipped with an evolutionarily selected learning ability. Such a coupling of\nbiological and linguistic ingredients results in an abrupt transition: upon a\nsmall change of a model control parameter a poorly communicating group of\nlinguistically unskilled agents transforms into almost perfectly communicating\ngroup with large learning abilities. When learning ability is kept fixed, the\ntransition appears to be continuous. Genetic imprinting of the learning\nabilities proceeds via Baldwin effect: initially unskilled communicating agents\nlearn a language and that creates a niche in which there is an evolutionary\npressure for the increase of learning ability.Our model suggests that when\nlinguistic (or cultural) processes became intensive enough, a transition took\nplace where both linguistic performance and biological endowment of our species\nexperienced an abrupt change that perhaps triggered the rapid expansion of\nhuman civilization.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:12,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Zipf's Law and Avoidance of Excessive Synonymy&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Zipf's law states that if words of language are ranked in the order of\ndecreasing frequency in texts, the frequency of a word is inversely\nproportional to its rank. It is very robust as an experimental observation, but\nto date it escaped satisfactory theoretical explanation. We suggest that Zipf's\nlaw may arise from the evolution of word semantics dominated by expansion of\nmeanings and competition of synonyms.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:13,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;On the role of autocorrelations in texts&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The task of finding a criterion allowing to distinguish a text from an\narbitrary set of words is rather relevant in itself, for instance, in the\naspect of development of means for internet-content indexing or separating\nsignals and noise in communication channels. The Zipf law is currently\nconsidered to be the most reliable criterion of this kind [3]. At any rate,\nconventional stochastic word sets do not meet this law. The present paper deals\nwith one of possible criteria based on the determination of the degree of data\ncompression.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:14,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;On the fractal nature of mutual relevance sequences in the Internet news\n  message flows&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In the task of information retrieval the term relevance is taken to mean\nformal conformity of a document given by the retrieval system to user's\ninformation query. As a rule, the documents found by the retrieval system\nshould be submitted to the user in a certain order. Therefore, a retrieval\nperceived as a selection of documents formally solving the user's query, should\nbe supplemented with a certain procedure of processing a relevant set. It would\nbe natural to introduce a quantitative measure of document conformity to query,\ni.e. the relevance measure. Since no single rule exists for the determination\nof the relevance measure, we shall consider two of them which are the simplest\nin our opinion. The proposed approach does not suppose any restrictions and can\nbe applied to other relevance measures.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:15,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;What's in a Name?&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This paper describes experiments on identifying the language of a single name\nin isolation or in a document written in a different language. A new corpus has\nbeen compiled and made available, matching names against languages. This corpus\nis used in a series of experiments measuring the performance of general\nlanguage models and names-only language models on the language identification\ntask. Conclusions are drawn from the comparison between using general language\nmodels and names-only language models and between identifying the language of\nisolated names and the language of very short document fragments. Future\nresearch directions are outlined.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:16,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The structure of verbal sequences analyzed with unsupervised learning\n  techniques&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Data mining allows the exploration of sequences of phenomena, whereas one\nusually tends to focus on isolated phenomena or on the relation between two\nphenomena. It offers invaluable tools for theoretical analyses and exploration\nof the structure of sentences, texts, dialogues, and speech. We report here the\nresults of an attempt at using it for inspecting sequences of verbs from French\naccounts of road accidents. This analysis comes from an original approach of\nunsupervised training allowing the discovery of the structure of sequential\ndata. The entries of the analyzer were only made of the verbs appearing in the\nsentences. It provided a classification of the links between two successive\nverbs into four distinct clusters, allowing thus text segmentation. We give\nhere an interpretation of these clusters by applying a statistical analysis to\nindependent semantic annotations.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:17,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Linguistic Information Energy&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this treatment a text is considered to be a series of word impulses which\nare read at a constant rate. The brain then assembles these units of\ninformation into higher units of meaning. A classical systems approach is used\nto model an initial part of this assembly process. The concepts of linguistic\nsystem response, information energy, and ordering energy are defined and\nanalyzed. Finally, as a demonstration, information energy is used to estimate\nthe publication dates of a series of texts and the similarity of a set of\ntexts.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:18,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Generating models for temporal representations&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We discuss the use of model building for temporal representations. We chose\nPolish to illustrate our discussion because it has an interesting aspectual\nsystem, but the points we wish to make are not language specific. Rather, our\ngoal is to develop theoretical and computational tools for temporal model\nbuilding tasks in computational semantics. To this end, we present a\nfirst-order theory of time and events which is rich enough to capture\ninteresting semantic distinctions, and an algorithm which takes minimal models\nfor first-order theories and systematically attempts to ``perturb'' their\ntemporal component to provide non-minimal, but semantically significant,\nmodels.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:19,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Using Description Logics for Recognising Textual Entailment&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The aim of this paper is to show how we can handle the Recognising Textual\nEntailment (RTE) task by using Description Logics (DLs). To do this, we propose\na representation of natural language semantics in DLs inspired by existing\nrepresentations in first-order logic. But our most significant contribution is\nthe definition of two novel inference tasks: A-Box saturation and subgraph\ndetection which are crucial for our approach to RTE.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:20,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Using Synchronic and Diachronic Relations for Summarizing Multiple\n  Documents Describing Evolving Events&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper we present a fresh look at the problem of summarizing evolving\nevents from multiple sources. After a discussion concerning the nature of\nevolving events we introduce a distinction between linearly and non-linearly\nevolving events. We present then a general methodology for the automatic\ncreation of summaries from evolving events. At its heart lie the notions of\nSynchronic and Diachronic cross-document Relations (SDRs), whose aim is the\nidentification of similarities and differences between sources, from a\nsynchronical and diachronical perspective. SDRs do not connect documents or\ntextual elements found therein, but structures one might call messages.\nApplying this methodology will yield a set of messages and relations, SDRs,\nconnecting them, that is a graph which we call grid. We will show how such a\ngrid can be considered as the starting point of a Natural Language Generation\nSystem. The methodology is evaluated in two case-studies, one for linearly\nevolving events (descriptions of football matches) and another one for\nnon-linearly evolving events (terrorist incidents involving hostages). In both\ncases we evaluate the results produced by our computational systems.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:21,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Some Reflections on the Task of Content Determination in the Context of\n  Multi-Document Summarization of Evolving Events&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Despite its importance, the task of summarizing evolving events has received\nsmall attention by researchers in the field of multi-document summariztion. In\na previous paper (Afantenos et al. 2007) we have presented a methodology for\nthe automatic summarization of documents, emitted by multiple sources, which\ndescribe the evolution of an event. At the heart of this methodology lies the\nidentification of similarities and differences between the various documents,\nin two axes: the synchronic and the diachronic. This is achieved by the\nintroduction of the notion of Synchronic and Diachronic Relations. Those\nrelations connect the messages that are found in the documents, resulting thus\nin a graph which we call grid. Although the creation of the grid completes the\nDocument Planning phase of a typical NLG architecture, it can be the case that\nthe number of messages contained in a grid is very large, exceeding thus the\nrequired compression rate. In this paper we provide some initial thoughts on a\nprobabilistic model which can be applied at the Content Determination stage,\nand which tries to alleviate this problem.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:22,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Discriminative Phoneme Sequences Extraction for Non-Native Speaker's\n  Origin Classification&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper we present an automated method for the classification of the\norigin of non-native speakers. The origin of non-native speakers could be\nidentified by a human listener based on the detection of typical pronunciations\nfor each nationality. Thus we suppose the existence of several phoneme\nsequences that might allow the classification of the origin of non-native\nspeakers. Our new method is based on the extraction of discriminative sequences\nof phonemes from a non-native English speech database. These sequences are used\nto construct a probabilistic classifier for the speakers' origin. The existence\nof discriminative phone sequences in non-native speech is a significant result\nof this work. The system that we have developed achieved a significant correct\nclassification rate of 96.3% and a significant error reduction compared to some\nother tested techniques.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:23,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Combined Acoustic and Pronunciation Modelling for Non-Native Speech\n  Recognition&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper, we present several adaptation methods for non-native speech\nrecognition. We have tested pronunciation modelling, MLLR and MAP non-native\npronunciation adaptation and HMM models retraining on the HIWIRE foreign\naccented English speech database. The ``phonetic confusion'' scheme we have\ndeveloped consists in associating to each spoken phone several sequences of\nconfused phones. In our experiments, we have used different combinations of\nacoustic models representing the canonical and the foreign pronunciations:\nspoken and native models, models adapted to the non-native accent with MAP and\nMLLR. The joint use of pronunciation modelling and acoustic adaptation led to\nfurther improvements in recognition accuracy. The best combination of the above\nmentioned techniques resulted in a relative word error reduction ranging from\n46% to 71%.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:24,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Am\\'elioration des Performances des Syst\\`emes Automatiques de\n  Reconnaissance de la Parole pour la Parole Non Native&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this article, we present an approach for non native automatic speech\nrecognition (ASR). We propose two methods to adapt existing ASR systems to the\nnon-native accents. The first method is based on the modification of acoustic\nmodels through integration of acoustic models from the mother tong. The\nphonemes of the target language are pronounced in a similar manner to the\nnative language of speakers. We propose to combine the models of confused\nphonemes so that the ASR system could recognize both concurrent\npronounciations. The second method we propose is a refinment of the\npronounciation error detection through the introduction of graphemic\nconstraints. Indeed, non native speakers may rely on the writing of words in\ntheir uttering. Thus, the pronounctiation errors might depend on the characters\ncomposing the words. The average error rate reduction that we observed is\n(22.5%) relative for the sentence error rate, and 34.5% (relative) in word\nerror rate.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:25,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Can a Computer Laugh ?&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  A computer model of \&quot;a sense of humour\&quot; suggested previously\n[arXiv:0711.2058,0711.2061], relating the humorous effect with a specific\nmalfunction in information processing, is given in somewhat different\nexposition. Psychological aspects of humour are elaborated more thoroughly. The\nmechanism of laughter is formulated on the more general level. Detailed\ndiscussion is presented for the higher levels of information processing, which\nare responsible for a perception of complex samples of humour. Development of a\nsense of humour in the process of evolution is discussed.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1994,&quot;string&quot;:&quot;1,994&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:26,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Proof nets for display logic&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This paper explores several extensions of proof nets for the Lambek calculus\nin order to handle the different connectives of display logic in a natural way.\nThe new proof net calculus handles some recent additions to the Lambek\nvocabulary such as Galois connections and Grishin interactions. It concludes\nwith an exploration of the generative capacity of the Lambek-Grishin calculus,\npresenting an embedding of lexicalized tree adjoining grammars into the\nLambek-Grishin calculus.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:27,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;How to realize \&quot;a sense of humour\&quot; in computers ?&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Computer model of a \&quot;sense of humour\&quot; suggested previously [arXiv:0711.2058,\n0711.2061, 0711.2270] is raised to the level of a realistic algorithm.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:28,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Morphological annotation of Korean with Directly Maintainable Resources&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This article describes an exclusively resource-based method of morphological\nannotation of written Korean text. Korean is an agglutinative language. Our\nannotator is designed to process text before the operation of a syntactic\nparser. In its present state, it annotates one-stem words only. The output is a\ngraph of morphemes annotated with accurate linguistic information. The\ngranularity of the tagset is 3 to 5 times higher than usual tagsets. A\ncomparison with a reference annotated corpus showed that it achieves 89% recall\nwithout any corpus training. The language resources used by the system are\nlexicons of stems, transducers of suffixes and transducers of generation of\nallomorphs. All can be easily updated, which allows users to control the\nevolution of the performances of the system. It has been claimed that\nmorphological annotation of Korean text could only be performed by a\nmorphological analysis module accessing a lexicon of morphemes. We show that it\ncan also be performed directly with a lexicon of words and without applying\nmorphological rules at annotation time, which speeds up annotation to 1,210\nword/s. The lexicon of words is obtained from the maintainable language\nresources through a fully automated compilation process.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2006,&quot;string&quot;:&quot;2,006&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:29,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Lexicon management and standard formats&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  International standards for lexicon formats are in preparation. To a certain\nextent, the proposed formats converge with prior results of standardization\nprojects. However, their adequacy for (i) lexicon management and (ii)\nlexicon-driven applications have been little debated in the past, nor are they\nas a part of the present standardization effort. We examine these issues. IGM\nhas developed XML formats compatible with the emerging international standards,\nand we report experimental results on large-coverage lexica.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2005,&quot;string&quot;:&quot;2,005&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:30,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;In memoriam Maurice Gross&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Maurice Gross (1934-2001) was both a great linguist and a pioneer in natural\nlanguage processing. This article is written in homage to his memory\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2005,&quot;string&quot;:&quot;2,005&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:31,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A resource-based Korean morphological annotation system&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We describe a resource-based method of morphological annotation of written\nKorean text. Korean is an agglutinative language. The output of our system is a\ngraph of morphemes annotated with accurate linguistic information. The language\nresources used by the system can be easily updated, which allows us-ers to\ncontrol the evolution of the per-formances of the system. We show that\nmorphological annotation of Korean text can be performed directly with a\nlexicon of words and without morpho-logical rules.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2005,&quot;string&quot;:&quot;2,005&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:32,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Graphes param\\'etr\\'es et outils de lexicalisation&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Shifting to a lexicalized grammar reduces the number of parsing errors and\nimproves application results. However, such an operation affects a syntactic\nparser in all its aspects. One of our research objectives is to design a\nrealistic model for grammar lexicalization. We carried out experiments for\nwhich we used a grammar with a very simple content and formalism, and a very\ninformative syntactic lexicon, the lexicon-grammar of French elaborated by the\nLADL. Lexicalization was performed by applying the parameterized-graph\napproach. Our results tend to show that most information in the lexicon-grammar\ncan be transferred into a grammar and exploited successfully for the syntactic\nparsing of sentences.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2006,&quot;string&quot;:&quot;2,006&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:33,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Evaluation of a Grammar of French Determiners&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Existing syntactic grammars of natural languages, even with a far from\ncomplete coverage, are complex objects. Assessments of the quality of parts of\nsuch grammars are useful for the validation of their construction. We evaluated\nthe quality of a grammar of French determiners that takes the form of a\nrecursive transition network. The result of the application of this local\ngrammar gives deeper syntactic information than chunking or information\navailable in treebanks. We performed the evaluation by comparison with a corpus\nindependently annotated with information on determiners. We obtained 86%\nprecision and 92% recall on text not tagged for parts of speech.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:34,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Very strict selectional restrictions&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We discuss the characteristics and behaviour of two parallel classes of verbs\nin two Romance languages, French and Portuguese. Examples of these verbs are\nPort. abater [gado] and Fr. abattre [b\\'etail], both meaning \&quot;slaughter\n[cattle]\&quot;. In both languages, the definition of the class of verbs includes\nseveral features: - They have only one essential complement, which is a direct\nobject. - The nominal distribution of the complement is very limited, i.e., few\nnouns can be selected as head nouns of the complement. However, this selection\nis not restricted to a single noun, as would be the case for verbal idioms such\nas Fr. monter la garde \&quot;mount guard\&quot;. - We excluded from the class\nconstructions which are reductions of more complex constructions, e.g. Port.\nafinar [instrumento] com \&quot;tune [instrument] with\&quot;.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2006,&quot;string&quot;:&quot;2,006&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:35,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Outilex, plate-forme logicielle de traitement de textes \\'ecrits&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The Outilex software platform, which will be made available to research,\ndevelopment and industry, comprises software components implementing all the\nfundamental operations of written text processing: processing without lexicons,\nexploitation of lexicons and grammars, language resource management. All data\nare structured in XML formats, and also in more compact formats, either\nreadable or binary, whenever necessary; the required format converters are\nincluded in the platform; the grammar formats allow for combining statistical\napproaches with resource-based approaches. Manually constructed lexicons for\nFrench and English, originating from the LADL, and of substantial coverage,\nwill be distributed with the platform under LGPL-LR license.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2006,&quot;string&quot;:&quot;2,006&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:36,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Let's get the student into the driver's seat&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Speaking a language and achieving proficiency in another one is a highly\ncomplex process which requires the acquisition of various kinds of knowledge\nand skills, like the learning of words, rules and patterns and their connection\nto communicative goals (intentions), the usual starting point. To help the\nlearner to acquire these skills we propose an enhanced, electronic version of\nan age old method: pattern drills (henceforth PDs). While being highly regarded\nin the fifties, PDs have become unpopular since then, partially because of\ntheir lack of grounding (natural context) and rigidity. Despite these\nshortcomings we do believe in the virtues of this approach, at least with\nregard to the acquisition of basic linguistic reflexes or skills (automatisms),\nnecessary to survive in the new language. Of course, the method needs\nimprovement, and we will show here how this can be achieved. Unlike tapes or\nbooks, computers are open media, allowing for dynamic changes, taking users'\nperformances and preferences into account. Building an electronic version of\nPDs amounts to building an open resource, accomodatable to the users' ever\nchanging needs.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:37,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Valence extraction using EM selection and co-occurrence matrices&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This paper discusses two new procedures for extracting verb valences from raw\ntexts, with an application to the Polish language. The first novel technique,\nthe EM selection algorithm, performs unsupervised disambiguation of valence\nframe forests, obtained by applying a non-probabilistic deep grammar parser and\nsome post-processing to the text. The second new idea concerns filtering of\nincorrect frames detected in the parsed text and is motivated by an observation\nthat verbs which take similar arguments tend to have similar frames. This\nphenomenon is described in terms of newly introduced co-occurrence matrices.\nUsing co-occurrence matrices, we split filtering into two steps. The list of\nvalid arguments is first determined for each verb, whereas the pattern\naccording to which the arguments are combined into frames is computed in the\nfollowing stage. Our best extracted dictionary reaches an $F$-score of 45%,\ncompared to an $F$-score of 39% for the standard frame-based BHT filtering.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:38,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Framework and Resources for Natural Language Parser Evaluation&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Because of the wide variety of contemporary practices used in the automatic\nsyntactic parsing of natural languages, it has become necessary to analyze and\nevaluate the strengths and weaknesses of different approaches. This research is\nall the more necessary because there are currently no genre- and\ndomain-independent parsers that are able to analyze unrestricted text with 100%\npreciseness (I use this term to refer to the correctness of analyses assigned\nby a parser). All these factors create a need for methods and resources that\ncan be used to evaluate and compare parsing systems. This research describes:\n(1) A theoretical analysis of current achievements in parsing and parser\nevaluation. (2) A framework (called FEPa) that can be used to carry out\npractical parser evaluations and comparisons. (3) A set of new evaluation\nresources: FiEval is a Finnish treebank under construction, and MGTS and RobSet\nare parser evaluation resources in English. (4) The results of experiments in\nwhich the developed evaluation framework and the two resources for English were\nused for evaluating a set of selected parsers.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:39,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The emerging field of language dynamics&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  A simple review by a linguist, citing many articles by physicists:\nQuantitative methods, agent-based computer simulations, language dynamics,\nlanguage typology, historical linguistics\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:40,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Comparison of natural (english) and artificial (esperanto) languages.\n  A Multifractal method based analysis&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We present a comparison of two english texts, written by Lewis Carroll, one\n(Alice in wonderland) and the other (Through a looking glass), the former\ntranslated into esperanto, in order to observe whether natural and artificial\nlanguages significantly differ from each other. We construct one dimensional\ntime series like signals using either word lengths or word frequencies. We use\nthe multifractal ideas for sorting out correlations in the writings. In order\nto check the robustness of the methods we also write the corresponding shuffled\ntexts. We compare characteristic functions and e.g. observe marked differences\nin the (far from parabolic) f(alpha) curves, differences which we attribute to\nTsallis non extensive statistical features in the ''frequency time series'' and\n''length time series''. The esperanto text has more extreme vallues. A very\nrough approximation consists in modeling the texts as a random Cantor set if\nresulting from a binomial cascade of long and short words (or words and\nblanks). This leads to parameters characterizing the text style, and most\nlikely in fine the author writings.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:41,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Online-concordance \&quot;Perekhresni stezhky\&quot; (\&quot;The Cross-Paths\&quot;), a novel by\n  Ivan Franko&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In the article, theoretical principles and practical realization for the\ncompilation of the concordance to \&quot;Perekhresni stezhky\&quot; (\&quot;The Cross-Paths\&quot;), a\nnovel by Ivan Franko, are described. Two forms for the context presentation are\nproposed. The electronic version of this lexicographic work is available\nonline.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2006,&quot;string&quot;:&quot;2,006&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:42,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Robustness Evaluation of Two CCG, a PCFG and a Link Grammar Parsers&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Robustness in a parser refers to an ability to deal with exceptional\nphenomena. A parser is robust if it deals with phenomena outside its normal\nrange of inputs. This paper reports on a series of robustness evaluations of\nstate-of-the-art parsers in which we concentrated on one aspect of robustness:\nits ability to parse sentences containing misspelled words. We propose two\nmeasures for robustness evaluation based on a comparison of a parser's output\nfor grammatical input sentences and their noisy counterparts. In this paper, we\nuse these measures to compare the overall robustness of the four evaluated\nparsers, and we present an analysis of the decline in parser performance with\nincreasing error levels. Our results indicate that performance typically\ndeclines tens of percentage units when parsers are presented with texts\ncontaining misspellings. When it was tested on our purpose-built test set of\n443 sentences, the best parser in the experiment (C&amp;C parser) was able to\nreturn exactly the same parse tree for the grammatical and ungrammatical\nsentences for 60.8%, 34.0% and 14.9% of the sentences with one, two or three\nmisspelled words respectively.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:43,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Between conjecture and memento: shaping a collective emotional\n  perception of the future&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Large scale surveys of public mood are costly and often impractical to\nperform. However, the web is awash with material indicative of public mood such\nas blogs, emails, and web queries. Inexpensive content analysis on such\nextensive corpora can be used to assess public mood fluctuations. The work\npresented here is concerned with the analysis of the public mood towards the\nfuture. Using an extension of the Profile of Mood States questionnaire, we have\nextracted mood indicators from 10,741 emails submitted in 2006 to futureme.org,\na web service that allows its users to send themselves emails to be delivered\nat a later date. Our results indicate long-term optimism toward the future, but\nmedium-term apprehension and confusion.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:44,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Methods to integrate a language model with semantic information for a\n  word prediction component&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Most current word prediction systems make use of n-gram language models (LM)\nto estimate the probability of the following word in a phrase. In the past\nyears there have been many attempts to enrich such language models with further\nsyntactic or semantic information. We want to explore the predictive powers of\nLatent Semantic Analysis (LSA), a method that has been shown to provide\nreliable information on long-distance semantic dependencies between words in a\ncontext. We present and evaluate here several methods that integrate LSA-based\ninformation with a standard language model: a semantic cache, partial\nreranking, and different forms of interpolation. We found that all methods show\nsignificant improvements, compared to the 4-gram baseline, and most of them to\na simple cache model as well.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:45,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Concerning Olga, the Beautiful Little Street Dancer (Adjectives as\n  Higher-Order Polymorphic Functions)&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper we suggest a typed compositional seman-tics for nominal\ncompounds of the form [Adj Noun] that models adjectives as higher-order\npolymorphic functions, and where types are assumed to represent concepts in an\nontology that reflects our commonsense view of the world and the way we talk\nabout it in or-dinary language. In addition to [Adj Noun] compounds our\nproposal seems also to suggest a plausible explana-tion for well known\nadjective ordering restrictions.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:46,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Textual Fingerprinting with Texts from Parkin, Bassewitz, and Leander&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Current research in author profiling to discover a legal author's fingerprint\ndoes not only follow examinations based on statistical parameters only but\ninclude more and more dynamic methods that can learn and that react adaptable\nto the specific behavior of an author. But the question on how to appropriately\nrepresent a text is still one of the fundamental tasks, and the problem of\nwhich attribute should be used to fingerprint the author's style is still not\nexactly defined. In this work, we focus on linguistic selection of attributes\nto fingerprint the style of the authors Parkin, Bassewitz and Leander. We use\ntexts of the genre Fairy Tale as it has a clear style and texts of a shorter\nsize with a straightforward story-line and a simple language.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:47,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Some properties of the Ukrainian writing system&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We investigate the grapheme-phoneme relation in Ukrainian and some properties\nof the Ukrainian version of the Cyrillic alphabet.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:48,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The Generation of Textual Entailment with NLML in an Intelligent\n  Dialogue system for Language Learning CSIEC&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This research report introduces the generation of textual entailment within\nthe project CSIEC (Computer Simulation in Educational Communication), an\ninteractive web-based human-computer dialogue system with natural language for\nEnglish instruction. The generation of textual entailment (GTE) is critical to\nthe further improvement of CSIEC project. Up to now we have found few\nliteratures related with GTE. Simulating the process that a human being learns\nEnglish as a foreign language we explore our naive approach to tackle the GTE\nproblem and its algorithm within the framework of CSIEC, i.e. rule annotation\nin NLML, pattern recognition (matching), and entailment transformation. The\ntime and space complexity of our algorithm is tested with some entailment\nexamples. Further works include the rules annotation based on the English\ntextbooks and a GUI interface for normal users to edit the entailment rules.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:49,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Figuring out Actors in Text Streams: Using Collocations to establish\n  Incremental Mind-maps&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The recognition, involvement, and description of main actors influences the\nstory line of the whole text. This is of higher importance as the text per se\nrepresents a flow of words and expressions that once it is read it is lost. In\nthis respect, the understanding of a text and moreover on how the actor exactly\nbehaves is not only a major concern: as human beings try to store a given input\non short-term memory while associating diverse aspects and actors with\nincidents, the following approach represents a virtual architecture, where\ncollocations are concerned and taken as the associative completion of the\nactors' acting. Once that collocations are discovered, they become managed in\nseparated memory blocks broken down by the actors. As for human beings, the\nmemory blocks refer to associative mind-maps. We then present several priority\nfunctions to represent the actual temporal situation inside a mind-map to\nenable the user to reconstruct the recent events from the discovered temporal\nresults.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:50,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Effects of High-Order Co-occurrences on Word Semantic Similarities&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  A computational model of the construction of word meaning through exposure to\ntexts is built in order to simulate the effects of co-occurrence values on word\nsemantic similarities, paragraph by paragraph. Semantic similarity is here\nviewed as association. It turns out that the similarity between two words W1\nand W2 strongly increases with a co-occurrence, decreases with the occurrence\nof W1 without W2 or W2 without W1, and slightly increases with high-order\nco-occurrences. Therefore, operationalizing similarity as a frequency of\nco-occurrence probably introduces a bias: first, there are cases in which there\nis similarity without co-occurrence and, second, the frequency of co-occurrence\noverestimates similarity.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2006,&quot;string&quot;:&quot;2,006&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:51,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Parts-of-Speech Tagger Errors Do Not Necessarily Degrade Accuracy in\n  Extracting Information from Biomedical Text&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  A recent study reported development of Muscorian, a generic text processing\ntool for extracting protein-protein interactions from text that achieved\ncomparable performance to biomedical-specific text processing tools. This\nresult was unexpected since potential errors from a series of text analysis\nprocesses is likely to adversely affect the outcome of the entire process. Most\nbiomedical entity relationship extraction tools have used biomedical-specific\nparts-of-speech (POS) tagger as errors in POS tagging and are likely to affect\nsubsequent semantic analysis of the text, such as shallow parsing. This study\naims to evaluate the parts-of-speech (POS) tagging accuracy and attempts to\nexplore whether a comparable performance is obtained when a generic POS tagger,\nMontyTagger, was used in place of MedPost, a tagger trained in biomedical text.\nOur results demonstrated that MontyTagger, Muscorian's POS tagger, has a POS\ntagging accuracy of 83.1% when tested on biomedical text. Replacing MontyTagger\nwith MedPost did not result in a significant improvement in entity relationship\nextraction from text; precision of 55.6% from MontyTagger versus 56.8% from\nMedPost on directional relationships and 86.1% from MontyTagger compared to\n81.8% from MedPost on nondirectional relationships. This is unexpected as the\npotential for poor POS tagging by MontyTagger is likely to affect the outcome\nof the information extraction. An analysis of POS tagging errors demonstrated\nthat 78.5% of tagging errors are being compensated by shallow parsing. Thus,\ndespite 83.1% tagging accuracy, MontyTagger has a functional tagging accuracy\nof 94.6%.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:52,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Semi-Automatic Framework to Discover Epistemic Modalities in\n  Scientific Articles&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Documents in scientific newspapers are often marked by attitudes and opinions\nof the author and/or other persons, who contribute with objective and\nsubjective statements and arguments as well. In this respect, the attitude is\noften accomplished by a linguistic modality. As in languages like english,\nfrench and german, the modality is expressed by special verbs like can, must,\nmay, etc. and the subjunctive mood, an occurrence of modalities often induces\nthat these verbs take over the role of modality. This is not correct as it is\nproven that modality is the instrument of the whole sentence where both the\nadverbs, modal particles, punctuation marks, and the intonation of a sentence\ncontribute. Often, a combination of all these instruments are necessary to\nexpress a modality. In this work, we concern with the finding of modal verbs in\nscientific texts as a pre-step towards the discovery of the attitude of an\nauthor. Whereas the input will be an arbitrary text, the output consists of\nzones representing modalities.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:53,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Phoneme recognition in TIMIT with BLSTM-CTC&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We compare the performance of a recurrent neural network with the best\nresults published so far on phoneme recognition in the TIMIT database. These\npublished results have been obtained with a combination of classifiers.\nHowever, in this paper we apply a single recurrent neural network to the same\ntask. Our recurrent neural network attains an error rate of 24.6%. This result\nis not significantly different from that obtained by the other best methods,\nbut they rely on a combination of classifiers for achieving comparable\nperformance.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:54,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Feature Unification in TAG Derivation Trees&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The derivation trees of a tree adjoining grammar provide a first insight into\nthe sentence semantics, and are thus prime targets for generation systems. We\ndefine a formalism, feature-based regular tree grammars, and a translation from\nfeature based tree adjoining grammars into this new formalism. The translation\npreserves the derivation structures of the original grammar, and accounts for\nfeature unification.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:55,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Graph Algorithms for Improving Type-Logical Proof Search&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Proof nets are a graph theoretical representation of proofs in various\nfragments of type-logical grammar. In spite of this basis in graph theory,\nthere has been relatively little attention to the use of graph theoretic\nalgorithms for type-logical proof search. In this paper we will look at several\nways in which standard graph theoretic algorithms can be used to restrict the\nsearch space. In particular, we will provide an O(n4) algorithm for selecting\nan optimal axiom link at any stage in the proof search as well as a O(kn3)\nalgorithm for selecting the k best proof candidates.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2004,&quot;string&quot;:&quot;2,004&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:56,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A toolkit for a generative lexicon&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper we describe the conception of a software toolkit designed for\nthe construction, maintenance and collaborative use of a Generative Lexicon. In\norder to ease its portability and spreading use, this tool was built with free\nand open source products. We eventually tested the toolkit and showed it\nfilters the adequate form of anaphoric reference to the modifier in endocentric\ncompounds.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:57,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computational Representation of Linguistic Structures using\n  Domain-Specific Languages&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We describe a modular system for generating sentences from formal definitions\nof underlying linguistic structures using domain-specific languages. The system\nuses Java in general, Prolog for lexical entries and custom domain-specific\nlanguages based on Functional Grammar and Functional Discourse Grammar\nnotation, implemented using the ANTLR parser generator. We show how linguistic\nand technological parts can be brought together in a natural language\nprocessing system and how domain-specific languages can be used as a tool for\nconsistent formal notation in linguistic description.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:58,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Exploring a type-theoretic approach to accessibility constraint\n  modelling&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The type-theoretic modelling of DRT that [degroote06] proposed features\ncontinuations for the management of the context in which a clause has to be\ninterpreted. This approach, while keeping the standard definitions of\nquantifier scope, translates the rules of the accessibility constraints of\ndiscourse referents inside the semantic recipes. In this paper, we deal with\nadditional rules for these accessibility constraints. In particular in the case\nof discourse referents introduced by proper nouns, that negation does not\nblock, and in the case of rhetorical relations that structure discourses. We\nshow how this continuation-based approach applies to those accessibility\nconstraints and how we can consider the parallel management of various\nprinciples.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:59,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A semantic space for modeling children's semantic memory&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The goal of this paper is to present a model of children's semantic memory,\nwhich is based on a corpus reproducing the kinds of texts children are exposed\nto. After presenting the literature in the development of the semantic memory,\na preliminary French corpus of 3.2 million words is described. Similarities in\nthe resulting semantic space are compared to human data on four tests:\nassociation norms, vocabulary test, semantic judgments and memory tasks. A\nsecond corpus is described, which is composed of subcorpora corresponding to\nvarious ages. This stratified corpus is intended as a basis for developmental\nstudies. Finally, two applications of these models of semantic memory are\npresented: the first one aims at tracing the development of semantic\nsimilarities paragraph by paragraph; the second one describes an implementation\nof a model of text comprehension derived from the Construction-integration\nmodel (Kintsch, 1988, 1998) and based on such models of semantic memory.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:60,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Textual Entailment Recognizing by Theorem Proving Approach&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper we present two original methods for recognizing textual\ninference. First one is a modified resolution method such that some linguistic\nconsiderations are introduced in the unification of two atoms. The approach is\npossible due to the recent methods of transforming texts in logic formulas.\nSecond one is based on semantic relations in text, as presented in WordNet.\nSome similarities between these two methods are remarked.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2006,&quot;string&quot;:&quot;2,006&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:61,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A chain dictionary method for Word Sense Disambiguation and applications&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  A large class of unsupervised algorithms for Word Sense Disambiguation (WSD)\nis that of dictionary-based methods. Various algorithms have as the root Lesk's\nalgorithm, which exploits the sense definitions in the dictionary directly. Our\napproach uses the lexical base WordNet for a new algorithm originated in\nLesk's, namely \&quot;chain algorithm for disambiguation of all words\&quot;, CHAD. We show\nhow translation from a language into another one and also text entailment\nverification could be accomplished by this disambiguation.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:62,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;How Is Meaning Grounded in Dictionary Definitions?&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Meaning cannot be based on dictionary definitions all the way down: at some\npoint the circularity of definitions must be broken in some way, by grounding\nthe meanings of certain words in sensorimotor categories learned from\nexperience or shaped by evolution. This is the \&quot;symbol grounding problem.\&quot; We\nintroduce the concept of a reachable set -- a larger vocabulary whose meanings\ncan be learned from a smaller vocabulary through definition alone, as long as\nthe meanings of the smaller vocabulary are themselves already grounded. We\nprovide simple algorithms to compute reachable sets for any given dictionary.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:63,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computational Approaches to Measuring the Similarity of Short Contexts :\n  A Review of Applications and Methods&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Measuring the similarity of short written contexts is a fundamental problem\nin Natural Language Processing. This article provides a unifying framework by\nwhich short context problems can be categorized both by their intended\napplication and proposed solution. The goal is to show that various problems\nand methodologies that appear quite different on the surface are in fact very\nclosely related. The axes by which these categorizations are made include the\nformat of the contexts (headed versus headless), the way in which the contexts\nare to be measured (first-order versus second-order similarity), and the\ninformation used to represent the features in the contexts (micro versus macro\nviews). The unifying thread that binds together many short context applications\nand methods is the fact that similarity decisions must be made between contexts\nthat share few (if any) words in common.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2010,&quot;string&quot;:&quot;2,010&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:64,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;About the creation of a parallel bilingual corpora of web-publications&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The algorithm of the creation texts parallel corpora was presented. The\nalgorithm is based on the use of \&quot;key words\&quot; in text documents, and on the\nmeans of their automated translation. Key words were singled out by means of\nusing Russian and Ukrainian morphological dictionaries, as well as dictionaries\nof the translation of nouns for the Russian and Ukrainianlanguages. Besides, to\ncalculate the weights of the terms in the documents, empiric-statistic rules\nwere used. The algorithm under consideration was realized in the form of a\nprogram complex, integrated into the content-monitoring InfoStream system. As a\nresult, a parallel bilingual corpora of web-publications containing about 30\nthousand documents, was created\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:65,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;TuLiPA: Towards a Multi-Formalism Parsing Environment for Grammar\n  Engineering&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper, we present an open-source parsing environment (Tuebingen\nLinguistic Parsing Architecture, TuLiPA) which uses Range Concatenation Grammar\n(RCG) as a pivot formalism, thus opening the way to the parsing of several\nmildly context-sensitive formalisms. This environment currently supports\ntree-based grammars (namely Tree-Adjoining Grammars, TAG) and Multi-Component\nTree-Adjoining Grammars with Tree Tuples (TT-MCTAG)) and allows computation not\nonly of syntactic structures, but also of the corresponding semantic\nrepresentations. It is used for the development of a tree-based grammar for\nGerman.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:66,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Formal semantics of language and the Richard-Berry paradox&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The classical logical antinomy known as Richard-Berry paradox is combined\nwith plausible assumptions about the size i.e. the descriptional complexity of\nTuring machines formalizing certain sentences, to show that formalization of\nlanguage leads to contradiction.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:67,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Investigation of the Zipf-plot of the extinct Meroitic language&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The ancient and extinct language Meroitic is investigated using Zipf's Law.\nIn particular, since Meroitic is still undeciphered, the Zipf law analysis\nallows us to assess the quality of current texts and possible avenues for\nfuture investigation using statistical techniques.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2007,&quot;string&quot;:&quot;2,007&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:68,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;What It Feels Like To Hear Voices: Fond Memories of Julian Jaynes&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Julian Jaynes's profound humanitarian convictions not only prevented him from\ngoing to war, but would have prevented him from ever kicking a dog. Yet\naccording to his theory, not only are language-less dogs unconscious, but so\ntoo were the speaking/hearing Greeks in the Bicameral Era, when they heard\ngods' voices telling them what to do rather than thinking for themselves. I\nargue that to be conscious is to be able to feel, and that all mammals (and\nprobably lower vertebrates and invertebrates too) feel, hence are conscious.\nJulian Jaynes's brilliant analysis of our concepts of consciousness\nnevertheless keeps inspiring ever more inquiry and insights into the age-old\nmind/body problem and its relation to cognition and language.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:69,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Constructing word similarities in Meroitic as an aid to decipherment&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Meroitic is the still undeciphered language of the ancient civilization of\nKush. Over the years, various techniques for decipherment such as finding a\nbilingual text or cognates from modern or other ancient languages in the Sudan\nand surrounding areas has not been successful. Using techniques borrowed from\ninformation theory and natural language statistics, similar words are paired\nand attempts are made to use currently defined words to extract at least\npartial meaning from unknown words.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:70,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Open architecture for multilingual parallel texts&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Multilingual parallel texts (abbreviated to parallel texts) are linguistic\nversions of the same content (\&quot;translations\&quot;); e.g., the Maastricht Treaty in\nEnglish and Spanish are parallel texts. This document is about creating an open\narchitecture for the whole Authoring, Translation and Publishing Chain\n(ATP-chain) for the processing of parallel texts.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:71,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;On the nature of long-range letter correlations in texts&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The origin of long-range letter correlations in natural texts is studied\nusing random walk analysis and Jensen-Shannon divergence. It is concluded that\nthey result from slow variations in letter frequency distribution, which are a\nconsequence of slow variations in lexical composition within the text. These\ncorrelations are preserved by random letter shuffling within a moving window.\nAs such, they do reflect structural properties of the text, but in a very\nindirect manner.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2016,&quot;string&quot;:&quot;2,016&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:72,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Recognizing analogies, synonyms, antonyms, and associations appear to be four\ndistinct tasks, requiring distinct NLP algorithms. In the past, the four tasks\nhave been treated independently, using a wide variety of algorithms. These four\nsemantic classes, however, are a tiny sample of the full range of semantic\nphenomena, and we cannot afford to create ad hoc algorithms for each semantic\nphenomenon; we need to seek a unified approach. We propose to subsume a broad\nrange of phenomena under analogies. To limit the scope of this paper, we\nrestrict our attention to the subsumption of synonyms, antonyms, and\nassociations. We introduce a supervised corpus-based machine learning algorithm\nfor classifying analogous word pairs, and we show that it can solve\nmultiple-choice SAT analogy questions, TOEFL synonym questions, ESL\nsynonym-antonym questions, and similar-associated-both questions from cognitive\npsychology.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:73,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Using descriptive mark-up to formalize translation quality assessment&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The paper deals with using descriptive mark-up to emphasize translation\nmistakes. The author postulates the necessity to develop a standard and formal\nXML-based way of describing translation mistakes. It is considered to be\nimportant for achieving impersonal translation quality assessment. Marked-up\ntranslations can be used in corpus translation studies; moreover, automatic\ntranslation assessment based on marked-up mistakes is possible. The paper\nconcludes with setting up guidelines for further activity within the described\nfield.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:74,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Distribution of complexities in the Vai script&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In the paper, we analyze the distribution of complexities in the Vai script,\nan indigenous syllabic writing system from Liberia. It is found that the\nuniformity hypothesis for complexities fails for this script. The models using\nPoisson distribution for the number of components and hyper-Poisson\ndistribution for connections provide good fits in the case of the Vai script.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:75,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Une grammaire formelle du cr\\'eole martiniquais pour la g\\'en\\'eration\n  automatique&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this article, some first elements of a computational modelling of the\ngrammar of the Martiniquese French Creole dialect are presented. The sources of\ninspiration for the modelling is the functional description given by Damoiseau\n(1984), and Pinalie's &amp; Bernabe's (1999) grammar manual. Based on earlier works\nin text generation (Vaillant, 1997), a unification grammar formalism, namely\nTree Adjoining Grammars (TAG), and a modelling of lexical functional categories\nbased on syntactic and semantic properties, are used to implement a grammar of\nMartiniquese Creole which is used in a prototype of text generation system. One\nof the main applications of the system could be its use as a tool software\nsupporting the task of learning Creole as a second language. -- Nous\npr\\'esenterons dans cette communication les premiers travaux de mod\\'elisation\ninformatique d'une grammaire de la langue cr\\'eole martiniquaise, en nous\ninspirant des descriptions fonctionnelles de Damoiseau (1984) ainsi que du\nmanuel de Pinalie &amp; Bernab\\'e (1999). Prenant appui sur des travaux\nant\\'erieurs en g\\'en\\'eration de texte (Vaillant, 1997), nous utilisons un\nformalisme de grammaires d'unification, les grammaires d'adjonction d'arbres\n(TAG d'apr\\`es l'acronyme anglais), ainsi qu'une mod\\'elisation de cat\\'egories\nlexicales fonctionnelles \\`a base syntaxico-s\\'emantique, pour mettre en oeuvre\nune grammaire du cr\\'eole martiniquais utilisable dans une maquette de\nsyst\\`eme de g\\'en\\'eration automatique. L'un des int\\'er\\^ets principaux de ce\nsyst\\`eme pourrait \\^etre son utilisation comme logiciel outil pour l'aide \\`a\nl'apprentissage du cr\\'eole en tant que langue seconde.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2003,&quot;string&quot;:&quot;2,003&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:76,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Layered Grammar Model: Using Tree-Adjoining Grammars to Build a Common\n  Syntactic Kernel for Related Dialects&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This article describes the design of a common syntactic description for the\ncore grammar of a group of related dialects. The common description does not\nrely on an abstract sub-linguistic structure like a metagrammar: it consists in\na single FS-LTAG where the actual specific language is included as one of the\nattributes in the set of attribute types defined for the features. When the\nlang attribute is instantiated, the selected subset of the grammar is\nequivalent to the grammar of one dialect. When it is not, we have a model of a\nhybrid multidialectal linguistic system. This principle is used for a group of\ncreole languages of the West-Atlantic area, namely the French-based Creoles of\nHaiti, Guadeloupe, Martinique and French Guiana.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:77,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Analyse spectrale des textes: d\\'etection automatique des fronti\\`eres\n  de langue et de discours&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We propose a theoretical framework within which information on the vocabulary\nof a given corpus can be inferred on the basis of statistical information\ngathered on that corpus. Inferences can be made on the categories of the words\nin the vocabulary, and on their syntactical properties within particular\nlanguages. Based on the same statistical data, it is possible to build matrices\nof syntagmatic similarity (bigram transition matrices) or paradigmatic\nsimilarity (probability for any pair of words to share common contexts). When\nclustered with respect to their syntagmatic similarity, words tend to group\ninto sublanguage vocabularies, and when clustered with respect to their\nparadigmatic similarity, into syntactic or semantic classes. Experiments have\nexplored the first of these two possibilities. Their results are interpreted in\nthe frame of a Markov chain modelling of the corpus' generative processe(s): we\nshow that the results of a spectral analysis of the transition matrix can be\ninterpreted as probability distributions of words within clusters. This method\nyields a soft clustering of the vocabulary into sublanguages which contribute\nto the generation of heterogeneous corpora. As an application, we show how\nmultilingual texts can be visually segmented into linguistically homogeneous\nsegments. Our method is specifically useful in the case of related languages\nwhich happened to be mixed in corpora.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2006,&quot;string&quot;:&quot;2,006&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:78,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Soft Uncoupling of Markov Chains for Permeable Language Distinction: A\n  New Algorithm&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Without prior knowledge, distinguishing different languages may be a hard\ntask, especially when their borders are permeable. We develop an extension of\nspectral clustering -- a powerful unsupervised classification toolbox -- that\nis shown to resolve accurately the task of soft language distinction. At the\nheart of our approach, we replace the usual hard membership assignment of\nspectral clustering by a soft, probabilistic assignment, which also presents\nthe advantage to bypass a well-known complexity bottleneck of the method.\nFurthermore, our approach relies on a novel, convenient construction of a\nMarkov chain out of a corpus. Extensive experiments with a readily available\nsystem clearly display the potential of the method, which brings a visually\nappealing soft distinction of languages that may define altogether a whole\ncorpus.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2006,&quot;string&quot;:&quot;2,006&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:79,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Text as Statistical Mechanics Object&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this article we present a model of human written text based on statistical\nmechanics approach by deriving the potential energy for different parts of the\ntext using large text corpus. We have checked the results numerically and found\nthat the specific heat parameter effectively separates the closed class words\nfrom the specific terms used in the text.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:80,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Language structure in the n-object naming game&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We examine a naming game with two agents trying to establish a common\nvocabulary for n objects. Such efforts lead to the emergence of language that\nallows for an efficient communication and exhibits some degree of homonymy and\nsynonymy. Although homonymy reduces the communication efficiency, it seems to\nbe a dynamical trap that persists for a long, and perhaps indefinite, time. On\nthe other hand, synonymy does not reduce the efficiency of communication, but\nappears to be only a transient feature of the language. Thus, in our model the\nrole of synonymy decreases and in the long-time limit it becomes negligible. A\nsimilar rareness of synonymy is observed in present natural languages. The role\nof noise, that distorts the communicated words, is also examined. Although, in\ngeneral, the noise reduces the communication efficiency, it also regroups the\nwords so that they are more evenly distributed within the available \&quot;verbal\&quot;\nspace.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:81,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Assembling Actor-based Mind-Maps from Text Stream&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  For human beings, the processing of text streams of unknown size leads\ngenerally to problems because e.g. noise must be selected out, information be\ntested for its relevance or redundancy, and linguistic phenomenon like\nambiguity or the resolution of pronouns be advanced. Putting this into\nsimulation by using an artificial mind-map is a challenge, which offers the\ngate for a wide field of applications like automatic text summarization or\npunctual retrieval. In this work we present a framework that is a first step\ntowards an automatic intellect. It aims at assembling a mind-map based on\nincoming text streams and on a subject-verb-object strategy, having the verb as\nan interconnection between the adjacent nouns. The mind-map's performance is\nenriched by a pronoun resolution engine that bases on the work of D. Klein, and\nC. D. Manning.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:82,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;CoZo+ - A Content Zoning Engine for textual documents&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Content zoning can be understood as a segmentation of textual documents into\nzones. This is inspired by [6] who initially proposed an approach for the\nargumentative zoning of textual documents. With the prototypical CoZo+ engine,\nwe focus on content zoning towards an automatic processing of textual streams\nwhile considering only the actors as the zones. We gain information that can be\nused to realize an automatic recognition of content for pre-defined actors. We\nunderstand CoZo+ as a necessary pre-step towards an automatic generation of\nsummaries and to make intellectual ownership of documents detectable.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:83,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;UNL-French deconversion as transfer &amp; generation from an interlingua\n  with possible quality enhancement through offline human interaction&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We present the architecture of the UNL-French deconverter, which \&quot;generates\&quot;\nfrom the UNL interlingua by first\&quot;localizing\&quot; the UNL form for French, within\nUNL, and then applying slightly adapted but classical transfer and generation\ntechniques, implemented in GETA's Ariane-G5 environment, supplemented by some\nUNL-specific tools. Online interaction can be used during deconversion to\nenhance output quality and is now used for development purposes. We show how\ninteraction could be delayed and embedded in the postedition phase, which would\nthen interact not directly with the output text, but indirectly with several\ncomponents of the deconverter. Interacting online or offline can improve the\nquality not only of the utterance at hand, but also of the utterances processed\nlater, as various preferences may be automatically changed to let the\ndeconverter \&quot;learn\&quot;.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1999,&quot;string&quot;:&quot;1,999&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:84,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The Application of Fuzzy Logic to Collocation Extraction&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Collocations are important for many tasks of Natural language processing such\nas information retrieval, machine translation, computational lexicography etc.\nSo far many statistical methods have been used for collocation extraction.\nAlmost all the methods form a classical crisp set of collocation. We propose a\nfuzzy logic approach of collocation extraction to form a fuzzy set of\ncollocations in which each word combination has a certain grade of membership\nfor being collocation. Fuzzy logic provides an easy way to express natural\nlanguage into fuzzy logic rules. Two existing methods; Mutual information and\nt-test have been utilized for the input of the fuzzy inference system. The\nresulting membership function could be easily seen and demonstrated. To show\nthe utility of the fuzzy logic some word pairs have been examined as an\nexample. The working data has been based on a corpus of about one million words\ncontained in different novels constituting project Gutenberg available on\nwww.gutenberg.org. The proposed method has all the advantages of the two\nmethods, while overcoming their drawbacks. Hence it provides a better result\nthan the two methods.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:85,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Computational Model to Disentangle Semantic Information Embedded in\n  Word Association Norms&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Two well-known databases of semantic relationships between pairs of words\nused in psycholinguistics, feature-based and association-based, are studied as\ncomplex networks. We propose an algorithm to disentangle feature based\nrelationships from free association semantic networks. The algorithm uses the\nrich topology of the free association semantic network to produce a new set of\nrelationships between words similar to those observed in feature production\nnorms.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:86,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The Latent Relation Mapping Engine: Algorithm and Experiments&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Many AI researchers and cognitive scientists have argued that analogy is the\ncore of cognition. The most influential work on computational modeling of\nanalogy-making is Structure Mapping Theory (SMT) and its implementation in the\nStructure Mapping Engine (SME). A limitation of SME is the requirement for\ncomplex hand-coded representations. We introduce the Latent Relation Mapping\nEngine (LRME), which combines ideas from SME and Latent Relational Analysis\n(LRA) in order to remove the requirement for hand-coded representations. LRME\nbuilds analogical mappings between lists of words, using a large corpus of raw\ntext to automatically discover the semantic relations among the words. We\nevaluate LRME on a set of twenty analogical mapping problems, ten based on\nscientific analogies and ten based on common metaphors. LRME achieves\nhuman-level performance on the twenty problems. We compare LRME with a variety\nof alternative approaches and find that they are not able to reach the same\nlevel of performance.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:87,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Discovering Global Patterns in Linguistic Networks through Spectral\n  Analysis: A Case Study of the Consonant Inventories&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Recent research has shown that language and the socio-cognitive phenomena\nassociated with it can be aptly modeled and visualized through networks of\nlinguistic entities. However, most of the existing works on linguistic networks\nfocus only on the local properties of the networks. This study is an attempt to\nanalyze the structure of languages via a purely structural technique, namely\nspectral analysis, which is ideally suited for discovering the global\ncorrelations in a network. Application of this technique to PhoNet, the\nco-occurrence network of consonants, not only reveals several natural\nlinguistic principles governing the structure of the consonant inventories, but\nis also able to quantify their relative importance. We believe that this\npowerful technique can be successfully applied, in general, to study the\nstructure of natural languages.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:88,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Beyond word frequency: Bursts, lulls, and scaling in the temporal\n  distributions of words&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Background: Zipf's discovery that word frequency distributions obey a power\nlaw established parallels between biological and physical processes, and\nlanguage, laying the groundwork for a complex systems perspective on human\ncommunication. More recent research has also identified scaling regularities in\nthe dynamics underlying the successive occurrences of events, suggesting the\npossibility of similar findings for language as well.\n  Methodology/Principal Findings: By considering frequent words in USENET\ndiscussion groups and in disparate databases where the language has different\nlevels of formality, here we show that the distributions of distances between\nsuccessive occurrences of the same word display bursty deviations from a\nPoisson process and are well characterized by a stretched exponential (Weibull)\nscaling. The extent of this deviation depends strongly on semantic type -- a\nmeasure of the logicality of each word -- and less strongly on frequency. We\ndevelop a generative model of this behavior that fully determines the dynamics\nof word usage.\n  Conclusions/Significance: Recurrence patterns of words are well described by\na stretched exponential distribution of recurrence times, an empirical scaling\nthat cannot be anticipated from Zipf's law. Because the use of words provides a\nuniquely precise and powerful lens on human thought and activity, our findings\nalso have implications for other overt manifestations of collective human\ndynamics.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:89,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Statistical analysis of the Indus script using $n$-grams&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The Indus script is one of the major undeciphered scripts of the ancient\nworld. The small size of the corpus, the absence of bilingual texts, and the\nlack of definite knowledge of the underlying language has frustrated efforts at\ndecipherment since the discovery of the remains of the Indus civilisation.\nRecently, some researchers have questioned the premise that the Indus script\nencodes spoken language. Building on previous statistical approaches, we apply\nthe tools of statistical language processing, specifically $n$-gram Markov\nchains, to analyse the Indus script for syntax. Our main results are that the\nscript has well-defined signs which begin and end texts, that there is\ndirectionality and strong correlations in the sign order, and that there are\ngroups of signs which appear to have identical syntactic function. All these\nrequire no {\\it a priori} suppositions regarding the syntactic or semantic\ncontent of the signs, but follow directly from the statistical analysis. Using\ninformation theoretic measures, we find the information in the script to be\nintermediate between that of a completely random and a completely fixed\nordering of signs. Our study reveals that the Indus script is a structured sign\nsystem showing features of a formal language, but, at present, cannot\nconclusively establish that it encodes {\\it natural} language. Our $n$-gram\nMarkov model is useful for predicting signs which are missing or illegible in a\ncorpus of Indus texts. This work forms the basis for the development of a\nstochastic grammar which can be used to explore the syntax of the Indus script\nin greater detail.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2015,&quot;string&quot;:&quot;2,015&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:90,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Approaching the linguistic complexity&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We analyze the rank-frequency distributions of words in selected English and\nPolish texts. We compare scaling properties of these distributions in both\nlanguages. We also study a few small corpora of Polish literary texts and find\nthat for a corpus consisting of texts written by different authors the basic\nscaling regime is broken more strongly than in the case of comparable corpus\nconsisting of texts written by the same author. Similarly, for a corpus\nconsisting of texts translated into Polish from other languages the scaling\nregime is broken more strongly than for a comparable corpus of native Polish\ntexts. Moreover, based on the British National Corpus, we consider the\nrank-frequency distributions of the grammatically basic forms of words (lemmas)\ntagged with their proper part of speech. We find that these distributions do\nnot scale if each part of speech is analyzed separately. The only part of\nspeech that independently develops a trace of scaling is verbs.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:91,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Du corpus au dictionnaire&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this article, we propose an automatic process to build multi-lingual\nlexico-semantic resources. The goal of these resources is to browse\nsemantically textual information contained in texts of different languages.\nThis method uses a mathematical model called Atlas s\\'emantiques in order to\nrepresent the different senses of each word. It uses the linguistic relations\nbetween words to create graphs that are projected into a semantic space. These\nprojections constitute semantic maps that denote the sense trends of each given\nword. This model is fed with syntactic relations between words extracted from a\ncorpus. Therefore, the lexico-semantic resource produced describes all the\nwords and all their meanings observed in the corpus. The sense trends are\nexpressed by syntactic contexts, typical for a given meaning. The link between\neach sense trend and the utterances used to build the sense trend are also\nstored in an index. Thus all the instances of a word in a particular sense are\nlinked and can be browsed easily. And by using several corpora of different\nlanguages, several resources are built that correspond with each other through\nlanguages. It makes it possible to browse information through languages thanks\nto syntactic contexts translations (even if some of them are partial).\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:92,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Google distance between words&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Cilibrasi and Vitanyi have demonstrated that it is possible to extract the\nmeaning of words from the world-wide web. To achieve this, they rely on the\nnumber of webpages that are found through a Google search containing a given\nword and they associate the page count to the probability that the word appears\non a webpage. Thus, conditional probabilities allow them to correlate one word\nwith another word's meaning. Furthermore, they have developed a similarity\ndistance function that gauges how closely related a pair of words is. We\npresent a specific counterexample to the triangle inequality for this\nsimilarity distance function.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2015,&quot;string&quot;:&quot;2,015&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:93,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;On the Entropy of Written Spanish&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This paper reports on results on the entropy of the Spanish language. They\nare based on an analysis of natural language for n-word symbols (n = 1 to 18),\ntrigrams, digrams, and characters. The results obtained in this work are based\non the analysis of twelve different literary works in Spanish, as well as a\n279917 word news file provided by the Spanish press agency EFE. Entropy values\nare calculated by a direct method using computer processing and the probability\nlaw of large numbers. Three samples of artificial Spanish language produced by\na first-order model software source are also analyzed and compared with natural\nSpanish language.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2012,&quot;string&quot;:&quot;2,012&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:94,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Beyond Zipf's law: Modeling the structure of human language&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Human language, the most powerful communication system in history, is closely\nassociated with cognition. Written text is one of the fundamental\nmanifestations of language, and the study of its universal regularities can\ngive clues about how our brains process information and how we, as a society,\norganize and share it. Still, only classical patterns such as Zipf's law have\nbeen explored in depth. In contrast, other basic properties like the existence\nof bursts of rare words in specific documents, the topical organization of\ncollections, or the sublinear growth of vocabulary size with the length of a\ndocument, have only been studied one by one and mainly applying heuristic\nmethodologies rather than basic principles and general mechanisms. As a\nconsequence, there is a lack of understanding of linguistic processes as\ncomplex emergent phenomena. Beyond Zipf's law for word frequencies, here we\nfocus on Heaps' law, burstiness, and the topicality of document collections,\nwhich encode correlations within and across documents absent in random null\nmodels. We introduce and validate a generative model that explains the\nsimultaneous emergence of all these patterns from simple rules. As a result, we\nfind a connection between the bursty nature of rare words and the topical\norganization of texts and identify dynamic word ranking and memory across\ndocuments as key mechanisms explaining the non trivial organization of written\ntext. Our research can have broad implications and practical applications in\ncomputer science, cognitive science, and linguistics.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:95,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;New Confidence Measures for Statistical Machine Translation&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  A confidence measure is able to estimate the reliability of an hypothesis\nprovided by a machine translation system. The problem of confidence measure can\nbe seen as a process of testing : we want to decide whether the most probable\nsequence of words provided by the machine translation system is correct or not.\nIn the following we describe several original word-level confidence measures\nfor machine translation, based on mutual information, n-gram language model and\nlexical features language model. We evaluate how well they perform individually\nor together, and show that using a combination of confidence measures based on\nmutual information yields a classification error rate as low as 25.1% with an\nF-measure of 0.708.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:96,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;BagPack: A general framework to represent semantic relations&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We introduce a way to represent word pairs instantiating arbitrary semantic\nrelations that keeps track of the contexts in which the words in the pair occur\nboth together and independently. The resulting features are of sufficient\ngenerality to allow us, with the help of a standard supervised machine learning\nalgorithm, to tackle a variety of unrelated semantic tasks with good results\nand almost no task-specific tailoring.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:97,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;What's in a Message?&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper we present the first step in a larger series of experiments for\nthe induction of predicate/argument structures. The structures that we are\ninducing are very similar to the conceptual structures that are used in Frame\nSemantics (such as FrameNet). Those structures are called messages and they\nwere previously used in the context of a multi-document summarization system of\nevolving events. The series of experiments that we are proposing are\nessentially composed from two stages. In the first stage we are trying to\nextract a representative vocabulary of words. This vocabulary is later used in\nthe second stage, during which we apply to it various clustering approaches in\norder to identify the clusters of predicates and arguments--or frames and\nsemantic roles, to use the jargon of Frame Semantics. This paper presents in\ndetail and evaluates the first stage.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:98,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Syntactic variation of support verb constructions&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We report experiments about the syntactic variations of support verb\nconstructions, a special type of multiword expressions (MWEs) containing\npredicative nouns. In these expressions, the noun can occur with or without the\nverb, with no clear-cut semantic difference. We extracted from a large French\ncorpus a set of examples of the two situations and derived statistical results\nfrom these data. The extraction involved large-coverage language resources and\nfinite-state techniques. The results show that, most frequently, predicative\nnouns occur without a support verb. This fact has consequences on methods of\nextracting or recognising MWEs.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2008,&quot;string&quot;:&quot;2,008&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}},{&quot;rowIdx&quot;:99,&quot;cells&quot;:{&quot;Titles&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Network of two-Chinese-character compound words in Japanese language&quot;},&quot;Abstracts&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Some statistical properties of a network of two-Chinese-character compound\nwords in Japanese language are reported. In this network, a node represents a\nChinese character and an edge represents a two-Chinese-character compound word.\nIt is found that this network has properties of \&quot;small-world\&quot; and \&quot;scale-free.\&quot;\nA network formed by only Chinese characters for common use ({\\it joyo-kanji} in\nJapanese), which is regarded as a subclass of the original network, also has\nsmall-world property. However, a degree distribution of the network exhibits no\nclear power law. In order to reproduce disappearance of the power-law property,\na model for a selecting process of the Chinese characters for common use is\nproposed.\n&quot;},&quot;Years&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2009,&quot;string&quot;:&quot;2,009&quot;},&quot;Categories&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Computation and Language&quot;}}}],&quot;truncated&quot;:true},&quot;paginationData&quot;:{&quot;pageIndex&quot;:0,&quot;numItemsPerPage&quot;:100,&quot;numTotalItems&quot;:44949,&quot;offset&quot;:0,&quot;length&quot;:100}},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkxODcyNSwic3ViIjoiL2RhdGFzZXRzL01hYXJ0ZW5Hci9hcnhpdl9ubHAiLCJleHAiOjE3NDI5MjIzMjUsImlzcyI6Imh0dHBzOi8vaHVnZ2luZ2ZhY2UuY28ifQ.b4HsT1bmqaXMXiUmxXq_0vXv327szcn-IXEjcIP2M1PkHdSmV6cGhDXC_7hI8QHuKperOxmyJFgX5U1pDPV9CA&quot;,&quot;displayUrls&quot;:true},&quot;dataset&quot;:&quot;MaartenGr/arxiv_nlp&quot;,&quot;isGated&quot;:false,&quot;isPrivate&quot;:false,&quot;hasParquetFormat&quot;:false,&quot;author&quot;:{&quot;_id&quot;:&quot;62ea1ac3cc08a09aa6d3ec95&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62ea1ac3cc08a09aa6d3ec95/_74xXYEYLLjNVJ9zQucfn.jpeg&quot;,&quot;fullname&quot;:&quot;Maarten Grootendorst&quot;,&quot;name&quot;:&quot;MaartenGr&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:25},&quot;compact&quot;:true}"><div class="flex flex-col overflow-hidden shadow-xs mx-auto mb-10 rounded-lg border pt-2  px-2.5"><div class="mb-2 flex flex-wrap items-center gap-2"><div class="mr-auto flex items-center"><svg class="mr-1 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	<div class="whitespace-nowrap font-semibold">Dataset Viewer</div>
	</div>
				<a href="/datasets/MaartenGr/arxiv_nlp/tree/refs%2Fconvert%2Fparquet/default" class="group mr-1 text-xs text-gray-400 max-sm:hidden"><svg class="text-[.6rem] mr-1 inline -translate-y-px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill="currentColor" d="M12 10H6.78A11 11 0 0 1 27 16h2A13 13 0 0 0 6 7.68V4H4v8h8zm8 12h5.22A11 11 0 0 1 5 16H3a13 13 0 0 0 23 8.32V28h2v-8h-8z"></path></svg>
						<span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Auto-converted</span> to Parquet
					</a>
				<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>API</button>
					<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path d="M9.80603 2.86737H3.56107C3.37704 2.86737 3.20055 2.94048 3.07042 3.0706C2.94029 3.20073 2.86719 3.37723 2.86719 3.56126V9.80622C2.86719 9.99025 2.94029 10.1667 3.07042 10.2969C3.20055 10.427 3.37704 10.5001 3.56107 10.5001H9.80603C9.99006 10.5001 10.1666 10.427 10.2967 10.2969C10.4268 10.1667 10.4999 9.99025 10.4999 9.80622V3.56126C10.4999 3.37723 10.4268 3.20073 10.2967 3.0706C10.1666 2.94048 9.99006 2.86737 9.80603 2.86737Z" fill="currentColor" fill-opacity="0.3"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M2.40942 1.66191C2.05175 1.66191 1.7618 1.95186 1.7618 2.30953V6.76191H1.43799V2.30953C1.43799 1.77303 1.87291 1.3381 2.40942 1.3381H6.45704V1.66191H2.40942Z" fill="currentColor"></path></svg>Embed</button>

					<button class="bg-linear-to-b shadow-xs flex items-center gap-1.5 rounded-full border from-white to-red-100/90 px-2 py-0.5 text-xs font-medium text-[#2D3648] transition-shadow hover:shadow-inner dark:from-gray-900 dark:to-red-800/30 dark:text-gray-100 dark:hover:shadow-inner dark:hover:shadow-red-800/30" ><svg class="h-3.5 w-3.5 text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
						<span>Data Studio</span></button></div>
		<div class="flex flex-1 flex-col overflow-hidden -mx-2.5"><div class="flex flex-1 flex-col overflow-hidden"><div class="flex min-h-0 flex-1"><div class="flex flex-1 flex-col overflow-hidden"><div class="md:-mx-2.5 flex min-w-0 flex-wrap border-t"><div class="flex min-w-0 flex-1 flex-wrap"><div class="grid flex-1 grid-cols-1 overflow-hidden text-sm md:grid-cols-2 md:place-content-center sm:mx-2.5"><label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-950 md:border-r md:border-r-0 hidden" title="default"><span class="text-gray-500">Subset (1)</span>
			<div class="flex items-center whitespace-nowrap"><span class="truncate">default</span>
				<span class="mx-2 text-gray-500">Â·</span>
					<span class="text-gray-500">44.9k rows</span>
				<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
			<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Subset (1)"><option value="default" selected>default (44.9k rows)</option></optgroup></select></label>
		<label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-900 md:border-r md:border-r" title="train"><div class="text-gray-500">Split (1)</div>
				<div class="flex items-center overflow-hidden whitespace-nowrap"><span class="truncate">train</span>
					<span class="mx-2 text-gray-500">Â·</span>
						<span class="text-gray-500">44.9k rows</span>
					<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
				<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Split (1)"><option value="train" selected>train (44.9k rows)</option></optgroup></select></label></div></div>
								</div>

							<div class="flex min-h-0 flex-1 flex-col ">
	<div class="bg-linear-to-r text-smd relative flex items-center dark:border-gray-900 dark:bg-gray-950 false border-t [&amp;:has(:focus)]:from-gray-50 [&amp;:has(:focus)]:to-transparent [&amp;:has(:focus)]:to-20% dark:[&amp;:has(:focus)]:from-gray-900"><form class="flex-1"><svg class="absolute left-3 top-1/2 transform -translate-y-1/2 pointer-events-none text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				<input disabled class="outline-hidden h-9 w-full border-none bg-transparent px-1 pl-9 pr-3 placeholder:text-gray-400 " placeholder="Search this dataset" dir="auto"></form>
			<div class="flex items-center gap-2 px-2 py-1"><button type="button" class="hover:bg-yellow-200/70 flex items-center gap-1 rounded-md border border-yellow-200 bg-yellow-100 pl-0.5 pr-1 text-[.8rem] leading-normal text-gray-700 dark:border-orange-500/25 dark:bg-orange-500/20 dark:text-gray-300 dark:hover:brightness-110 hidden"><div class="rounded-sm bg-yellow-300 px-1 font-mono text-[.7rem] font-bold text-black dark:bg-yellow-700 dark:text-gray-200">SQL
	</div>
	Console
</button></div></div>


<div class="flex flex-1 flex-col overflow-hidden min-h-64 border-t">
		

<div class="max-h-96 relative overflow-auto"><table class="w-full table-auto rounded-lg font-mono text-xs text-gray-900"><thead class="shadow-xs sticky left-0 right-0 top-0 z-1 bg-white align-top"><tr class="space-y-54 h-full min-w-fit divide-x border-b text-left"><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">Titles
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['Â·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="18.295510204081634" width="11.2" height="11.704489795918366" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="3.9118367346938783" width="11.2" height="26.08816326530612" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="13.077551020408164" width="11.2" height="16.922448979591834" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="22.57795918367347" width="11.2" height="7.422040816326531" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">6</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">220</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">Abstracts
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['Â·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="19.171732980196232" width="11.2" height="10.828267019803768" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="2.835851441642088" width="11.2" height="27.164148558357912" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="15.331126226449166" width="11.2" height="14.668873773550834" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="22.73845783422621" width="11.2" height="7.2615421657737915" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">37</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">3.26k</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">Years
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>int64</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="0" y="25" width="12.666666666666666" height="5" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="14.666666666666666" y="25" width="12.666666666666666" height="5" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="29.333333333333332" y="25" width="12.666666666666666" height="5" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="44" y="25" width="12.666666666666666" height="5" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="58.666666666666664" y="25" width="12.666666666666666" height="5" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="73.33333333333333" y="25" width="12.666666666666666" height="5" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="88" y="18.656302895322938" width="12.666666666666666" height="11.34369710467706" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="102.66666666666666" y="0" width="12.666666666666666" height="30" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="117.33333333333333" y="9.039198218262808" width="12.666666666666666" height="20.960801781737192" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="13.666666666666666" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="28.333333333333332" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="43" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="57.666666666666664" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="72.33333333333333" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="87" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="101.66666666666666" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="116.33333333333333" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">1.99k</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">2.02k</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">Categories
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['Â·']">classes</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><defs><clipPath id="rounded-bar"><rect x="0" y="0" width="130" height="8" rx="4"></rect></clipPath><pattern id="hatching" patternUnits="userSpaceOnUse" patternTransform="rotate(-45)" height="1" width="5"><line y1="0" class="stroke-gray-400 dark:stroke-gray-500/80" stroke-width="3" y2="1" x1="2" x2="2"></line></pattern><pattern id="hatching-faded" patternUnits="userSpaceOnUse" patternTransform="rotate(-45)" height="1" width="5"><line y1="0" class="stroke-gray-100 dark:stroke-gray-500/20" stroke-width="3" y2="1" x1="2" x2="2"></line></pattern></defs><g height="8" style="transform: translateY(20px)" clip-path="url(#rounded-bar)"><g style="transform: scaleX(1.0153846153846153) translateX(-1px)"><g><rect class="fill-indigo-500 dark:fill-indigo-600/80" x="1" y="0" width="128" height="8" fill-opacity="1"></rect></g></g></g><g style="transform: scaleX(1.0153846153846153) translateX(-1px)"><g><rect class="fill-white cursor-pointer" x="0" y="0" width="130" height="28" fill-opacity="0"></rect></g></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 max-w-full overflow-hidden text-ellipsis whitespace-nowrap">1
				value</div></div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th></tr></thead>
			<tbody class="h-16 overflow-scroll"><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="0"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Introduction to Arabic Speech Recognition Using CMUSphinx System</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper Arabic was investigated from the speech recognition problem
point of view. We propose a novel approach to build an Arabic Automated Speech
Recognition System (ASR). This system is based on the open source CMU Sphinx-4,
from the Carnegie Mellon University. CMU Sphinx is a large-vocabulary;
speaker-independent, continuous speech recognition system based on discrete
Hidden Markov Models (HMMs). We build a model using utilities from the
OpenSource CMU Sphinx. We will demonstrate the possible adaptability of this
system to Arabic voice recognition.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="1"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Arabic Speech Recognition System using CMU-Sphinx4</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper we present the creation of an Arabic version of Automated
Speech Recognition System (ASR). This system is based on the open source
Sphinx-4, from the Carnegie Mellon University. Which is a speech recognition
system based on discrete hidden Markov models (HMMs). We investigate the
changes that must be made to the model to adapt Arabic voice recognition.
  Keywords: Speech recognition, Acoustic model, Arabic language, HMMs,
CMUSphinx-4, Artificial intelligence.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="2"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">On the Development of Text Input Method - Lessons Learned</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Intelligent Input Methods (IM) are essential for making text entries in many
East Asian scripts, but their application to other languages has not been fully
explored. This paper discusses how such tools can contribute to the development
of computer processing of other oriental languages. We propose a design
philosophy that regards IM as a text service platform, and treats the study of
IM as a cross disciplinary subject from the perspectives of software
engineering, human-computer interaction (HCI), and natural language processing
(NLP). We discuss these three perspectives and indicate a number of possible
future research directions.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="3"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Network statistics on early English Syntax: Structural criteria</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This paper includes a reflection on the role of networks in the study of
English language acquisition, as well as a collection of practical criteria to
annotate free-speech corpora from children utterances. At the theoretical
level, the main claim of this paper is that syntactic networks should be
interpreted as the outcome of the use of the syntactic machinery. Thus, the
intrinsic features of such machinery are not accessible directly from (known)
network properties. Rather, what one can see are the global patterns of its use
and, thus, a global view of the power and organization of the underlying
grammar. Taking a look into more practical issues, the paper examines how to
build a net from the projection of syntactic relations. Recall that, as opposed
to adult grammars, early-child language has not a well-defined concept of
structure. To overcome such difficulty, we develop a set of systematic criteria
assuming constituency hierarchy and a grammar based on lexico-thematic
relations. At the end, what we obtain is a well defined corpora annotation that
enables us i) to perform statistics on the size of structures and ii) to build
a network from syntactic relations over which we can perform the standard
measures of complexity. We also provide a detailed example.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="4"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Segmentation and Context of Literary and Musical Sequences</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We test a segmentation algorithm, based on the calculation of the
Jensen-Shannon divergence between probability distributions, to two symbolic
sequences of literary and musical origin. The first sequence represents the
successive appearance of characters in a theatrical play, and the second
represents the succession of tones from the twelve-tone scale in a keyboard
sonata. The algorithm divides the sequences into segments of maximal
compositional divergence between them. For the play, these segments are related
to changes in the frequency of appearance of different characters and in the
geographical setting of the action. For the sonata, the segments correspond to
tonal domains and reveal in detail the characteristic tonal progression of such
kind of musical composition.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="5"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">International Standard for a Linguistic Annotation Framework</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This paper describes the Linguistic Annotation Framework under development
within ISO TC37 SC4 WG1. The Linguistic Annotation Framework is intended to
serve as a basis for harmonizing existing language resources as well as
developing new ones.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,004</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="6"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Formal Model of Dictionary Structure and Content</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We show that a general model of lexical information conforms to an abstract
model that reflects the hierarchy of information found in a typical dictionary
entry. We show that this model can be mapped into a well-formed XML document,
and how the XSL transformation language can be used to implement a semantics
defined over the abstract model to enable extraction and manipulation of the
information in any format.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,000</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="7"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Practical Approach to Knowledge-based Question Answering with Natural
  Language Understanding and Advanced Reasoning</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This research hypothesized that a practical approach in the form of a
solution framework known as Natural Language Understanding and Reasoning for
Intelligence (NaLURI), which combines full-discourse natural language
understanding, powerful representation formalism capable of exploiting
ontological information and reasoning approach with advanced features, will
solve the following problems without compromising practicality factors: 1)
restriction on the nature of question and response, and 2) limitation to scale
across domains and to real-life natural language text.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="8"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Learning Probabilistic Models of Word Sense Disambiguation</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This dissertation presents several new methods of supervised and unsupervised
learning of word sense disambiguation models. The supervised methods focus on
performing model searches through a space of probabilistic models, and the
unsupervised methods rely on the use of Gibbs Sampling and the Expectation
Maximization (EM) algorithm. In both the supervised and unsupervised case, the
Naive Bayesian model is found to perform well. An explanation for this success
is presented in terms of learning rates and bias-variance decompositions.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1,998</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="9"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Learning Phonotactics Using ILP</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This paper describes experiments on learning Dutch phonotactic rules using
Inductive Logic Programming, a machine learning discipline based on inductive
logical operators. Two different ways of approaching the problem are
experimented with, and compared against each other as well as with related work
on the task. The results show a direct correspondence between the quality and
informedness of the background knowledge and the constructed theory,
demonstrating the ability of ILP to take good advantage of the prior domain
knowledge available. Further research is outlined.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,002</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="10"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Bootstrapping Deep Lexical Resources: Resources for Courses</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We propose a range of deep lexical acquisition methods which make use of
morphological, syntactic and ontological language resources to model word
similarity and bootstrap from a seed lexicon. The different methods are
deployed in learning lexical items for a precision grammar, and shown to each
have strengths and weaknesses over different word classes. A particular focus
of this paper is the relative accessibility of different language resource
types, and predicted ``bang for the buck'' associated with each in deep lexical
acquisition applications.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,005</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="11"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Bio-linguistic transition and Baldwin effect in an evolutionary
  naming-game model</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We examine an evolutionary naming-game model where communicating agents are
equipped with an evolutionarily selected learning ability. Such a coupling of
biological and linguistic ingredients results in an abrupt transition: upon a
small change of a model control parameter a poorly communicating group of
linguistically unskilled agents transforms into almost perfectly communicating
group with large learning abilities. When learning ability is kept fixed, the
transition appears to be continuous. Genetic imprinting of the learning
abilities proceeds via Baldwin effect: initially unskilled communicating agents
learn a language and that creates a niche in which there is an evolutionary
pressure for the increase of learning ability.Our model suggests that when
linguistic (or cultural) processes became intensive enough, a transition took
place where both linguistic performance and biological endowment of our species
experienced an abrupt change that perhaps triggered the rapid expansion of
human civilization.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="12"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Zipf's Law and Avoidance of Excessive Synonymy</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Zipf's law states that if words of language are ranked in the order of
decreasing frequency in texts, the frequency of a word is inversely
proportional to its rank. It is very robust as an experimental observation, but
to date it escaped satisfactory theoretical explanation. We suggest that Zipf's
law may arise from the evolution of word semantics dominated by expansion of
meanings and competition of synonyms.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="13"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">On the role of autocorrelations in texts</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The task of finding a criterion allowing to distinguish a text from an
arbitrary set of words is rather relevant in itself, for instance, in the
aspect of development of means for internet-content indexing or separating
signals and noise in communication channels. The Zipf law is currently
considered to be the most reliable criterion of this kind [3]. At any rate,
conventional stochastic word sets do not meet this law. The present paper deals
with one of possible criteria based on the determination of the degree of data
compression.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="14"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">On the fractal nature of mutual relevance sequences in the Internet news
  message flows</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In the task of information retrieval the term relevance is taken to mean
formal conformity of a document given by the retrieval system to user's
information query. As a rule, the documents found by the retrieval system
should be submitted to the user in a certain order. Therefore, a retrieval
perceived as a selection of documents formally solving the user's query, should
be supplemented with a certain procedure of processing a relevant set. It would
be natural to introduce a quantitative measure of document conformity to query,
i.e. the relevance measure. Since no single rule exists for the determination
of the relevance measure, we shall consider two of them which are the simplest
in our opinion. The proposed approach does not suppose any restrictions and can
be applied to other relevance measures.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="15"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">What's in a Name?</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This paper describes experiments on identifying the language of a single name
in isolation or in a document written in a different language. A new corpus has
been compiled and made available, matching names against languages. This corpus
is used in a series of experiments measuring the performance of general
language models and names-only language models on the language identification
task. Conclusions are drawn from the comparison between using general language
models and names-only language models and between identifying the language of
isolated names and the language of very short document fragments. Future
research directions are outlined.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="16"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The structure of verbal sequences analyzed with unsupervised learning
  techniques</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Data mining allows the exploration of sequences of phenomena, whereas one
usually tends to focus on isolated phenomena or on the relation between two
phenomena. It offers invaluable tools for theoretical analyses and exploration
of the structure of sentences, texts, dialogues, and speech. We report here the
results of an attempt at using it for inspecting sequences of verbs from French
accounts of road accidents. This analysis comes from an original approach of
unsupervised training allowing the discovery of the structure of sequential
data. The entries of the analyzer were only made of the verbs appearing in the
sentences. It provided a classification of the links between two successive
verbs into four distinct clusters, allowing thus text segmentation. We give
here an interpretation of these clusters by applying a statistical analysis to
independent semantic annotations.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="17"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Linguistic Information Energy</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this treatment a text is considered to be a series of word impulses which
are read at a constant rate. The brain then assembles these units of
information into higher units of meaning. A classical systems approach is used
to model an initial part of this assembly process. The concepts of linguistic
system response, information energy, and ordering energy are defined and
analyzed. Finally, as a demonstration, information energy is used to estimate
the publication dates of a series of texts and the similarity of a set of
texts.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="18"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Generating models for temporal representations</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We discuss the use of model building for temporal representations. We chose
Polish to illustrate our discussion because it has an interesting aspectual
system, but the points we wish to make are not language specific. Rather, our
goal is to develop theoretical and computational tools for temporal model
building tasks in computational semantics. To this end, we present a
first-order theory of time and events which is rich enough to capture
interesting semantic distinctions, and an algorithm which takes minimal models
for first-order theories and systematically attempts to ``perturb'' their
temporal component to provide non-minimal, but semantically significant,
models.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="19"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Using Description Logics for Recognising Textual Entailment</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The aim of this paper is to show how we can handle the Recognising Textual
Entailment (RTE) task by using Description Logics (DLs). To do this, we propose
a representation of natural language semantics in DLs inspired by existing
representations in first-order logic. But our most significant contribution is
the definition of two novel inference tasks: A-Box saturation and subgraph
detection which are crucial for our approach to RTE.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="20"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Using Synchronic and Diachronic Relations for Summarizing Multiple
  Documents Describing Evolving Events</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper we present a fresh look at the problem of summarizing evolving
events from multiple sources. After a discussion concerning the nature of
evolving events we introduce a distinction between linearly and non-linearly
evolving events. We present then a general methodology for the automatic
creation of summaries from evolving events. At its heart lie the notions of
Synchronic and Diachronic cross-document Relations (SDRs), whose aim is the
identification of similarities and differences between sources, from a
synchronical and diachronical perspective. SDRs do not connect documents or
textual elements found therein, but structures one might call messages.
Applying this methodology will yield a set of messages and relations, SDRs,
connecting them, that is a graph which we call grid. We will show how such a
grid can be considered as the starting point of a Natural Language Generation
System. The methodology is evaluated in two case-studies, one for linearly
evolving events (descriptions of football matches) and another one for
non-linearly evolving events (terrorist incidents involving hostages). In both
cases we evaluate the results produced by our computational systems.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="21"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Some Reflections on the Task of Content Determination in the Context of
  Multi-Document Summarization of Evolving Events</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Despite its importance, the task of summarizing evolving events has received
small attention by researchers in the field of multi-document summariztion. In
a previous paper (Afantenos et al. 2007) we have presented a methodology for
the automatic summarization of documents, emitted by multiple sources, which
describe the evolution of an event. At the heart of this methodology lies the
identification of similarities and differences between the various documents,
in two axes: the synchronic and the diachronic. This is achieved by the
introduction of the notion of Synchronic and Diachronic Relations. Those
relations connect the messages that are found in the documents, resulting thus
in a graph which we call grid. Although the creation of the grid completes the
Document Planning phase of a typical NLG architecture, it can be the case that
the number of messages contained in a grid is very large, exceeding thus the
required compression rate. In this paper we provide some initial thoughts on a
probabilistic model which can be applied at the Content Determination stage,
and which tries to alleviate this problem.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="22"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Discriminative Phoneme Sequences Extraction for Non-Native Speaker's
  Origin Classification</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper we present an automated method for the classification of the
origin of non-native speakers. The origin of non-native speakers could be
identified by a human listener based on the detection of typical pronunciations
for each nationality. Thus we suppose the existence of several phoneme
sequences that might allow the classification of the origin of non-native
speakers. Our new method is based on the extraction of discriminative sequences
of phonemes from a non-native English speech database. These sequences are used
to construct a probabilistic classifier for the speakers' origin. The existence
of discriminative phone sequences in non-native speech is a significant result
of this work. The system that we have developed achieved a significant correct
classification rate of 96.3% and a significant error reduction compared to some
other tested techniques.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="23"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Combined Acoustic and Pronunciation Modelling for Non-Native Speech
  Recognition</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper, we present several adaptation methods for non-native speech
recognition. We have tested pronunciation modelling, MLLR and MAP non-native
pronunciation adaptation and HMM models retraining on the HIWIRE foreign
accented English speech database. The ``phonetic confusion'' scheme we have
developed consists in associating to each spoken phone several sequences of
confused phones. In our experiments, we have used different combinations of
acoustic models representing the canonical and the foreign pronunciations:
spoken and native models, models adapted to the non-native accent with MAP and
MLLR. The joint use of pronunciation modelling and acoustic adaptation led to
further improvements in recognition accuracy. The best combination of the above
mentioned techniques resulted in a relative word error reduction ranging from
46% to 71%.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="24"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Am\'elioration des Performances des Syst\`emes Automatiques de
  Reconnaissance de la Parole pour la Parole Non Native</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this article, we present an approach for non native automatic speech
recognition (ASR). We propose two methods to adapt existing ASR systems to the
non-native accents. The first method is based on the modification of acoustic
models through integration of acoustic models from the mother tong. The
phonemes of the target language are pronounced in a similar manner to the
native language of speakers. We propose to combine the models of confused
phonemes so that the ASR system could recognize both concurrent
pronounciations. The second method we propose is a refinment of the
pronounciation error detection through the introduction of graphemic
constraints. Indeed, non native speakers may rely on the writing of words in
their uttering. Thus, the pronounctiation errors might depend on the characters
composing the words. The average error rate reduction that we observed is
(22.5%) relative for the sentence error rate, and 34.5% (relative) in word
error rate.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="25"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Can a Computer Laugh ?</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  A computer model of "a sense of humour" suggested previously
[arXiv:0711.2058,0711.2061], relating the humorous effect with a specific
malfunction in information processing, is given in somewhat different
exposition. Psychological aspects of humour are elaborated more thoroughly. The
mechanism of laughter is formulated on the more general level. Detailed
discussion is presented for the higher levels of information processing, which
are responsible for a perception of complex samples of humour. Development of a
sense of humour in the process of evolution is discussed.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1,994</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="26"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Proof nets for display logic</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This paper explores several extensions of proof nets for the Lambek calculus
in order to handle the different connectives of display logic in a natural way.
The new proof net calculus handles some recent additions to the Lambek
vocabulary such as Galois connections and Grishin interactions. It concludes
with an exploration of the generative capacity of the Lambek-Grishin calculus,
presenting an embedding of lexicalized tree adjoining grammars into the
Lambek-Grishin calculus.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="27"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">How to realize "a sense of humour" in computers ?</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Computer model of a "sense of humour" suggested previously [arXiv:0711.2058,
0711.2061, 0711.2270] is raised to the level of a realistic algorithm.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="28"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Morphological annotation of Korean with Directly Maintainable Resources</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This article describes an exclusively resource-based method of morphological
annotation of written Korean text. Korean is an agglutinative language. Our
annotator is designed to process text before the operation of a syntactic
parser. In its present state, it annotates one-stem words only. The output is a
graph of morphemes annotated with accurate linguistic information. The
granularity of the tagset is 3 to 5 times higher than usual tagsets. A
comparison with a reference annotated corpus showed that it achieves 89% recall
without any corpus training. The language resources used by the system are
lexicons of stems, transducers of suffixes and transducers of generation of
allomorphs. All can be easily updated, which allows users to control the
evolution of the performances of the system. It has been claimed that
morphological annotation of Korean text could only be performed by a
morphological analysis module accessing a lexicon of morphemes. We show that it
can also be performed directly with a lexicon of words and without applying
morphological rules at annotation time, which speeds up annotation to 1,210
word/s. The lexicon of words is obtained from the maintainable language
resources through a fully automated compilation process.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,006</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="29"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Lexicon management and standard formats</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  International standards for lexicon formats are in preparation. To a certain
extent, the proposed formats converge with prior results of standardization
projects. However, their adequacy for (i) lexicon management and (ii)
lexicon-driven applications have been little debated in the past, nor are they
as a part of the present standardization effort. We examine these issues. IGM
has developed XML formats compatible with the emerging international standards,
and we report experimental results on large-coverage lexica.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,005</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="30"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">In memoriam Maurice Gross</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Maurice Gross (1934-2001) was both a great linguist and a pioneer in natural
language processing. This article is written in homage to his memory
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,005</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="31"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A resource-based Korean morphological annotation system</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We describe a resource-based method of morphological annotation of written
Korean text. Korean is an agglutinative language. The output of our system is a
graph of morphemes annotated with accurate linguistic information. The language
resources used by the system can be easily updated, which allows us-ers to
control the evolution of the per-formances of the system. We show that
morphological annotation of Korean text can be performed directly with a
lexicon of words and without morpho-logical rules.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,005</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="32"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Graphes param\'etr\'es et outils de lexicalisation</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Shifting to a lexicalized grammar reduces the number of parsing errors and
improves application results. However, such an operation affects a syntactic
parser in all its aspects. One of our research objectives is to design a
realistic model for grammar lexicalization. We carried out experiments for
which we used a grammar with a very simple content and formalism, and a very
informative syntactic lexicon, the lexicon-grammar of French elaborated by the
LADL. Lexicalization was performed by applying the parameterized-graph
approach. Our results tend to show that most information in the lexicon-grammar
can be transferred into a grammar and exploited successfully for the syntactic
parsing of sentences.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,006</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="33"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Evaluation of a Grammar of French Determiners</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Existing syntactic grammars of natural languages, even with a far from
complete coverage, are complex objects. Assessments of the quality of parts of
such grammars are useful for the validation of their construction. We evaluated
the quality of a grammar of French determiners that takes the form of a
recursive transition network. The result of the application of this local
grammar gives deeper syntactic information than chunking or information
available in treebanks. We performed the evaluation by comparison with a corpus
independently annotated with information on determiners. We obtained 86%
precision and 92% recall on text not tagged for parts of speech.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="34"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Very strict selectional restrictions</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We discuss the characteristics and behaviour of two parallel classes of verbs
in two Romance languages, French and Portuguese. Examples of these verbs are
Port. abater [gado] and Fr. abattre [b\'etail], both meaning "slaughter
[cattle]". In both languages, the definition of the class of verbs includes
several features: - They have only one essential complement, which is a direct
object. - The nominal distribution of the complement is very limited, i.e., few
nouns can be selected as head nouns of the complement. However, this selection
is not restricted to a single noun, as would be the case for verbal idioms such
as Fr. monter la garde "mount guard". - We excluded from the class
constructions which are reductions of more complex constructions, e.g. Port.
afinar [instrumento] com "tune [instrument] with".
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,006</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="35"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Outilex, plate-forme logicielle de traitement de textes \'ecrits</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The Outilex software platform, which will be made available to research,
development and industry, comprises software components implementing all the
fundamental operations of written text processing: processing without lexicons,
exploitation of lexicons and grammars, language resource management. All data
are structured in XML formats, and also in more compact formats, either
readable or binary, whenever necessary; the required format converters are
included in the platform; the grammar formats allow for combining statistical
approaches with resource-based approaches. Manually constructed lexicons for
French and English, originating from the LADL, and of substantial coverage,
will be distributed with the platform under LGPL-LR license.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,006</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="36"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Let's get the student into the driver's seat</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Speaking a language and achieving proficiency in another one is a highly
complex process which requires the acquisition of various kinds of knowledge
and skills, like the learning of words, rules and patterns and their connection
to communicative goals (intentions), the usual starting point. To help the
learner to acquire these skills we propose an enhanced, electronic version of
an age old method: pattern drills (henceforth PDs). While being highly regarded
in the fifties, PDs have become unpopular since then, partially because of
their lack of grounding (natural context) and rigidity. Despite these
shortcomings we do believe in the virtues of this approach, at least with
regard to the acquisition of basic linguistic reflexes or skills (automatisms),
necessary to survive in the new language. Of course, the method needs
improvement, and we will show here how this can be achieved. Unlike tapes or
books, computers are open media, allowing for dynamic changes, taking users'
performances and preferences into account. Building an electronic version of
PDs amounts to building an open resource, accomodatable to the users' ever
changing needs.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="37"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Valence extraction using EM selection and co-occurrence matrices</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This paper discusses two new procedures for extracting verb valences from raw
texts, with an application to the Polish language. The first novel technique,
the EM selection algorithm, performs unsupervised disambiguation of valence
frame forests, obtained by applying a non-probabilistic deep grammar parser and
some post-processing to the text. The second new idea concerns filtering of
incorrect frames detected in the parsed text and is motivated by an observation
that verbs which take similar arguments tend to have similar frames. This
phenomenon is described in terms of newly introduced co-occurrence matrices.
Using co-occurrence matrices, we split filtering into two steps. The list of
valid arguments is first determined for each verb, whereas the pattern
according to which the arguments are combined into frames is computed in the
following stage. Our best extracted dictionary reaches an $F$-score of 45%,
compared to an $F$-score of 39% for the standard frame-based BHT filtering.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="38"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Framework and Resources for Natural Language Parser Evaluation</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Because of the wide variety of contemporary practices used in the automatic
syntactic parsing of natural languages, it has become necessary to analyze and
evaluate the strengths and weaknesses of different approaches. This research is
all the more necessary because there are currently no genre- and
domain-independent parsers that are able to analyze unrestricted text with 100%
preciseness (I use this term to refer to the correctness of analyses assigned
by a parser). All these factors create a need for methods and resources that
can be used to evaluate and compare parsing systems. This research describes:
(1) A theoretical analysis of current achievements in parsing and parser
evaluation. (2) A framework (called FEPa) that can be used to carry out
practical parser evaluations and comparisons. (3) A set of new evaluation
resources: FiEval is a Finnish treebank under construction, and MGTS and RobSet
are parser evaluation resources in English. (4) The results of experiments in
which the developed evaluation framework and the two resources for English were
used for evaluating a set of selected parsers.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="39"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The emerging field of language dynamics</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  A simple review by a linguist, citing many articles by physicists:
Quantitative methods, agent-based computer simulations, language dynamics,
language typology, historical linguistics
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="40"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Comparison of natural (english) and artificial (esperanto) languages.
  A Multifractal method based analysis</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We present a comparison of two english texts, written by Lewis Carroll, one
(Alice in wonderland) and the other (Through a looking glass), the former
translated into esperanto, in order to observe whether natural and artificial
languages significantly differ from each other. We construct one dimensional
time series like signals using either word lengths or word frequencies. We use
the multifractal ideas for sorting out correlations in the writings. In order
to check the robustness of the methods we also write the corresponding shuffled
texts. We compare characteristic functions and e.g. observe marked differences
in the (far from parabolic) f(alpha) curves, differences which we attribute to
Tsallis non extensive statistical features in the ''frequency time series'' and
''length time series''. The esperanto text has more extreme vallues. A very
rough approximation consists in modeling the texts as a random Cantor set if
resulting from a binomial cascade of long and short words (or words and
blanks). This leads to parameters characterizing the text style, and most
likely in fine the author writings.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="41"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Online-concordance "Perekhresni stezhky" ("The Cross-Paths"), a novel by
  Ivan Franko</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In the article, theoretical principles and practical realization for the
compilation of the concordance to "Perekhresni stezhky" ("The Cross-Paths"), a
novel by Ivan Franko, are described. Two forms for the context presentation are
proposed. The electronic version of this lexicographic work is available
online.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,006</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="42"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Robustness Evaluation of Two CCG, a PCFG and a Link Grammar Parsers</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Robustness in a parser refers to an ability to deal with exceptional
phenomena. A parser is robust if it deals with phenomena outside its normal
range of inputs. This paper reports on a series of robustness evaluations of
state-of-the-art parsers in which we concentrated on one aspect of robustness:
its ability to parse sentences containing misspelled words. We propose two
measures for robustness evaluation based on a comparison of a parser's output
for grammatical input sentences and their noisy counterparts. In this paper, we
use these measures to compare the overall robustness of the four evaluated
parsers, and we present an analysis of the decline in parser performance with
increasing error levels. Our results indicate that performance typically
declines tens of percentage units when parsers are presented with texts
containing misspellings. When it was tested on our purpose-built test set of
443 sentences, the best parser in the experiment (C&amp;C parser) was able to
return exactly the same parse tree for the grammatical and ungrammatical
sentences for 60.8%, 34.0% and 14.9% of the sentences with one, two or three
misspelled words respectively.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="43"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Between conjecture and memento: shaping a collective emotional
  perception of the future</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Large scale surveys of public mood are costly and often impractical to
perform. However, the web is awash with material indicative of public mood such
as blogs, emails, and web queries. Inexpensive content analysis on such
extensive corpora can be used to assess public mood fluctuations. The work
presented here is concerned with the analysis of the public mood towards the
future. Using an extension of the Profile of Mood States questionnaire, we have
extracted mood indicators from 10,741 emails submitted in 2006 to futureme.org,
a web service that allows its users to send themselves emails to be delivered
at a later date. Our results indicate long-term optimism toward the future, but
medium-term apprehension and confusion.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="44"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Methods to integrate a language model with semantic information for a
  word prediction component</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Most current word prediction systems make use of n-gram language models (LM)
to estimate the probability of the following word in a phrase. In the past
years there have been many attempts to enrich such language models with further
syntactic or semantic information. We want to explore the predictive powers of
Latent Semantic Analysis (LSA), a method that has been shown to provide
reliable information on long-distance semantic dependencies between words in a
context. We present and evaluate here several methods that integrate LSA-based
information with a standard language model: a semantic cache, partial
reranking, and different forms of interpolation. We found that all methods show
significant improvements, compared to the 4-gram baseline, and most of them to
a simple cache model as well.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="45"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Concerning Olga, the Beautiful Little Street Dancer (Adjectives as
  Higher-Order Polymorphic Functions)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper we suggest a typed compositional seman-tics for nominal
compounds of the form [Adj Noun] that models adjectives as higher-order
polymorphic functions, and where types are assumed to represent concepts in an
ontology that reflects our commonsense view of the world and the way we talk
about it in or-dinary language. In addition to [Adj Noun] compounds our
proposal seems also to suggest a plausible explana-tion for well known
adjective ordering restrictions.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="46"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Textual Fingerprinting with Texts from Parkin, Bassewitz, and Leander</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Current research in author profiling to discover a legal author's fingerprint
does not only follow examinations based on statistical parameters only but
include more and more dynamic methods that can learn and that react adaptable
to the specific behavior of an author. But the question on how to appropriately
represent a text is still one of the fundamental tasks, and the problem of
which attribute should be used to fingerprint the author's style is still not
exactly defined. In this work, we focus on linguistic selection of attributes
to fingerprint the style of the authors Parkin, Bassewitz and Leander. We use
texts of the genre Fairy Tale as it has a clear style and texts of a shorter
size with a straightforward story-line and a simple language.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="47"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Some properties of the Ukrainian writing system</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We investigate the grapheme-phoneme relation in Ukrainian and some properties
of the Ukrainian version of the Cyrillic alphabet.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="48"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The Generation of Textual Entailment with NLML in an Intelligent
  Dialogue system for Language Learning CSIEC</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This research report introduces the generation of textual entailment within
the project CSIEC (Computer Simulation in Educational Communication), an
interactive web-based human-computer dialogue system with natural language for
English instruction. The generation of textual entailment (GTE) is critical to
the further improvement of CSIEC project. Up to now we have found few
literatures related with GTE. Simulating the process that a human being learns
English as a foreign language we explore our naive approach to tackle the GTE
problem and its algorithm within the framework of CSIEC, i.e. rule annotation
in NLML, pattern recognition (matching), and entailment transformation. The
time and space complexity of our algorithm is tested with some entailment
examples. Further works include the rules annotation based on the English
textbooks and a GUI interface for normal users to edit the entailment rules.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="49"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Figuring out Actors in Text Streams: Using Collocations to establish
  Incremental Mind-maps</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The recognition, involvement, and description of main actors influences the
story line of the whole text. This is of higher importance as the text per se
represents a flow of words and expressions that once it is read it is lost. In
this respect, the understanding of a text and moreover on how the actor exactly
behaves is not only a major concern: as human beings try to store a given input
on short-term memory while associating diverse aspects and actors with
incidents, the following approach represents a virtual architecture, where
collocations are concerned and taken as the associative completion of the
actors' acting. Once that collocations are discovered, they become managed in
separated memory blocks broken down by the actors. As for human beings, the
memory blocks refer to associative mind-maps. We then present several priority
functions to represent the actual temporal situation inside a mind-map to
enable the user to reconstruct the recent events from the discovered temporal
results.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="50"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Effects of High-Order Co-occurrences on Word Semantic Similarities</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  A computational model of the construction of word meaning through exposure to
texts is built in order to simulate the effects of co-occurrence values on word
semantic similarities, paragraph by paragraph. Semantic similarity is here
viewed as association. It turns out that the similarity between two words W1
and W2 strongly increases with a co-occurrence, decreases with the occurrence
of W1 without W2 or W2 without W1, and slightly increases with high-order
co-occurrences. Therefore, operationalizing similarity as a frequency of
co-occurrence probably introduces a bias: first, there are cases in which there
is similarity without co-occurrence and, second, the frequency of co-occurrence
overestimates similarity.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,006</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="51"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Parts-of-Speech Tagger Errors Do Not Necessarily Degrade Accuracy in
  Extracting Information from Biomedical Text</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  A recent study reported development of Muscorian, a generic text processing
tool for extracting protein-protein interactions from text that achieved
comparable performance to biomedical-specific text processing tools. This
result was unexpected since potential errors from a series of text analysis
processes is likely to adversely affect the outcome of the entire process. Most
biomedical entity relationship extraction tools have used biomedical-specific
parts-of-speech (POS) tagger as errors in POS tagging and are likely to affect
subsequent semantic analysis of the text, such as shallow parsing. This study
aims to evaluate the parts-of-speech (POS) tagging accuracy and attempts to
explore whether a comparable performance is obtained when a generic POS tagger,
MontyTagger, was used in place of MedPost, a tagger trained in biomedical text.
Our results demonstrated that MontyTagger, Muscorian's POS tagger, has a POS
tagging accuracy of 83.1% when tested on biomedical text. Replacing MontyTagger
with MedPost did not result in a significant improvement in entity relationship
extraction from text; precision of 55.6% from MontyTagger versus 56.8% from
MedPost on directional relationships and 86.1% from MontyTagger compared to
81.8% from MedPost on nondirectional relationships. This is unexpected as the
potential for poor POS tagging by MontyTagger is likely to affect the outcome
of the information extraction. An analysis of POS tagging errors demonstrated
that 78.5% of tagging errors are being compensated by shallow parsing. Thus,
despite 83.1% tagging accuracy, MontyTagger has a functional tagging accuracy
of 94.6%.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="52"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Semi-Automatic Framework to Discover Epistemic Modalities in
  Scientific Articles</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Documents in scientific newspapers are often marked by attitudes and opinions
of the author and/or other persons, who contribute with objective and
subjective statements and arguments as well. In this respect, the attitude is
often accomplished by a linguistic modality. As in languages like english,
french and german, the modality is expressed by special verbs like can, must,
may, etc. and the subjunctive mood, an occurrence of modalities often induces
that these verbs take over the role of modality. This is not correct as it is
proven that modality is the instrument of the whole sentence where both the
adverbs, modal particles, punctuation marks, and the intonation of a sentence
contribute. Often, a combination of all these instruments are necessary to
express a modality. In this work, we concern with the finding of modal verbs in
scientific texts as a pre-step towards the discovery of the attitude of an
author. Whereas the input will be an arbitrary text, the output consists of
zones representing modalities.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="53"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Phoneme recognition in TIMIT with BLSTM-CTC</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We compare the performance of a recurrent neural network with the best
results published so far on phoneme recognition in the TIMIT database. These
published results have been obtained with a combination of classifiers.
However, in this paper we apply a single recurrent neural network to the same
task. Our recurrent neural network attains an error rate of 24.6%. This result
is not significantly different from that obtained by the other best methods,
but they rely on a combination of classifiers for achieving comparable
performance.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="54"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Feature Unification in TAG Derivation Trees</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The derivation trees of a tree adjoining grammar provide a first insight into
the sentence semantics, and are thus prime targets for generation systems. We
define a formalism, feature-based regular tree grammars, and a translation from
feature based tree adjoining grammars into this new formalism. The translation
preserves the derivation structures of the original grammar, and accounts for
feature unification.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="55"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Graph Algorithms for Improving Type-Logical Proof Search</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Proof nets are a graph theoretical representation of proofs in various
fragments of type-logical grammar. In spite of this basis in graph theory,
there has been relatively little attention to the use of graph theoretic
algorithms for type-logical proof search. In this paper we will look at several
ways in which standard graph theoretic algorithms can be used to restrict the
search space. In particular, we will provide an O(n4) algorithm for selecting
an optimal axiom link at any stage in the proof search as well as a O(kn3)
algorithm for selecting the k best proof candidates.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,004</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="56"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A toolkit for a generative lexicon</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper we describe the conception of a software toolkit designed for
the construction, maintenance and collaborative use of a Generative Lexicon. In
order to ease its portability and spreading use, this tool was built with free
and open source products. We eventually tested the toolkit and showed it
filters the adequate form of anaphoric reference to the modifier in endocentric
compounds.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="57"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computational Representation of Linguistic Structures using
  Domain-Specific Languages</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We describe a modular system for generating sentences from formal definitions
of underlying linguistic structures using domain-specific languages. The system
uses Java in general, Prolog for lexical entries and custom domain-specific
languages based on Functional Grammar and Functional Discourse Grammar
notation, implemented using the ANTLR parser generator. We show how linguistic
and technological parts can be brought together in a natural language
processing system and how domain-specific languages can be used as a tool for
consistent formal notation in linguistic description.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="58"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Exploring a type-theoretic approach to accessibility constraint
  modelling</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The type-theoretic modelling of DRT that [degroote06] proposed features
continuations for the management of the context in which a clause has to be
interpreted. This approach, while keeping the standard definitions of
quantifier scope, translates the rules of the accessibility constraints of
discourse referents inside the semantic recipes. In this paper, we deal with
additional rules for these accessibility constraints. In particular in the case
of discourse referents introduced by proper nouns, that negation does not
block, and in the case of rhetorical relations that structure discourses. We
show how this continuation-based approach applies to those accessibility
constraints and how we can consider the parallel management of various
principles.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="59"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A semantic space for modeling children's semantic memory</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The goal of this paper is to present a model of children's semantic memory,
which is based on a corpus reproducing the kinds of texts children are exposed
to. After presenting the literature in the development of the semantic memory,
a preliminary French corpus of 3.2 million words is described. Similarities in
the resulting semantic space are compared to human data on four tests:
association norms, vocabulary test, semantic judgments and memory tasks. A
second corpus is described, which is composed of subcorpora corresponding to
various ages. This stratified corpus is intended as a basis for developmental
studies. Finally, two applications of these models of semantic memory are
presented: the first one aims at tracing the development of semantic
similarities paragraph by paragraph; the second one describes an implementation
of a model of text comprehension derived from the Construction-integration
model (Kintsch, 1988, 1998) and based on such models of semantic memory.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="60"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Textual Entailment Recognizing by Theorem Proving Approach</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper we present two original methods for recognizing textual
inference. First one is a modified resolution method such that some linguistic
considerations are introduced in the unification of two atoms. The approach is
possible due to the recent methods of transforming texts in logic formulas.
Second one is based on semantic relations in text, as presented in WordNet.
Some similarities between these two methods are remarked.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,006</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="61"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A chain dictionary method for Word Sense Disambiguation and applications</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  A large class of unsupervised algorithms for Word Sense Disambiguation (WSD)
is that of dictionary-based methods. Various algorithms have as the root Lesk's
algorithm, which exploits the sense definitions in the dictionary directly. Our
approach uses the lexical base WordNet for a new algorithm originated in
Lesk's, namely "chain algorithm for disambiguation of all words", CHAD. We show
how translation from a language into another one and also text entailment
verification could be accomplished by this disambiguation.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="62"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">How Is Meaning Grounded in Dictionary Definitions?</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Meaning cannot be based on dictionary definitions all the way down: at some
point the circularity of definitions must be broken in some way, by grounding
the meanings of certain words in sensorimotor categories learned from
experience or shaped by evolution. This is the "symbol grounding problem." We
introduce the concept of a reachable set -- a larger vocabulary whose meanings
can be learned from a smaller vocabulary through definition alone, as long as
the meanings of the smaller vocabulary are themselves already grounded. We
provide simple algorithms to compute reachable sets for any given dictionary.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="63"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computational Approaches to Measuring the Similarity of Short Contexts :
  A Review of Applications and Methods</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Measuring the similarity of short written contexts is a fundamental problem
in Natural Language Processing. This article provides a unifying framework by
which short context problems can be categorized both by their intended
application and proposed solution. The goal is to show that various problems
and methodologies that appear quite different on the surface are in fact very
closely related. The axes by which these categorizations are made include the
format of the contexts (headed versus headless), the way in which the contexts
are to be measured (first-order versus second-order similarity), and the
information used to represent the features in the contexts (micro versus macro
views). The unifying thread that binds together many short context applications
and methods is the fact that similarity decisions must be made between contexts
that share few (if any) words in common.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,010</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="64"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">About the creation of a parallel bilingual corpora of web-publications</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The algorithm of the creation texts parallel corpora was presented. The
algorithm is based on the use of "key words" in text documents, and on the
means of their automated translation. Key words were singled out by means of
using Russian and Ukrainian morphological dictionaries, as well as dictionaries
of the translation of nouns for the Russian and Ukrainianlanguages. Besides, to
calculate the weights of the terms in the documents, empiric-statistic rules
were used. The algorithm under consideration was realized in the form of a
program complex, integrated into the content-monitoring InfoStream system. As a
result, a parallel bilingual corpora of web-publications containing about 30
thousand documents, was created
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="65"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">TuLiPA: Towards a Multi-Formalism Parsing Environment for Grammar
  Engineering</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper, we present an open-source parsing environment (Tuebingen
Linguistic Parsing Architecture, TuLiPA) which uses Range Concatenation Grammar
(RCG) as a pivot formalism, thus opening the way to the parsing of several
mildly context-sensitive formalisms. This environment currently supports
tree-based grammars (namely Tree-Adjoining Grammars, TAG) and Multi-Component
Tree-Adjoining Grammars with Tree Tuples (TT-MCTAG)) and allows computation not
only of syntactic structures, but also of the corresponding semantic
representations. It is used for the development of a tree-based grammar for
German.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="66"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Formal semantics of language and the Richard-Berry paradox</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The classical logical antinomy known as Richard-Berry paradox is combined
with plausible assumptions about the size i.e. the descriptional complexity of
Turing machines formalizing certain sentences, to show that formalization of
language leads to contradiction.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="67"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Investigation of the Zipf-plot of the extinct Meroitic language</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The ancient and extinct language Meroitic is investigated using Zipf's Law.
In particular, since Meroitic is still undeciphered, the Zipf law analysis
allows us to assess the quality of current texts and possible avenues for
future investigation using statistical techniques.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,007</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="68"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">What It Feels Like To Hear Voices: Fond Memories of Julian Jaynes</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Julian Jaynes's profound humanitarian convictions not only prevented him from
going to war, but would have prevented him from ever kicking a dog. Yet
according to his theory, not only are language-less dogs unconscious, but so
too were the speaking/hearing Greeks in the Bicameral Era, when they heard
gods' voices telling them what to do rather than thinking for themselves. I
argue that to be conscious is to be able to feel, and that all mammals (and
probably lower vertebrates and invertebrates too) feel, hence are conscious.
Julian Jaynes's brilliant analysis of our concepts of consciousness
nevertheless keeps inspiring ever more inquiry and insights into the age-old
mind/body problem and its relation to cognition and language.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="69"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Constructing word similarities in Meroitic as an aid to decipherment</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Meroitic is the still undeciphered language of the ancient civilization of
Kush. Over the years, various techniques for decipherment such as finding a
bilingual text or cognates from modern or other ancient languages in the Sudan
and surrounding areas has not been successful. Using techniques borrowed from
information theory and natural language statistics, similar words are paired
and attempts are made to use currently defined words to extract at least
partial meaning from unknown words.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="70"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Open architecture for multilingual parallel texts</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Multilingual parallel texts (abbreviated to parallel texts) are linguistic
versions of the same content ("translations"); e.g., the Maastricht Treaty in
English and Spanish are parallel texts. This document is about creating an open
architecture for the whole Authoring, Translation and Publishing Chain
(ATP-chain) for the processing of parallel texts.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="71"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">On the nature of long-range letter correlations in texts</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The origin of long-range letter correlations in natural texts is studied
using random walk analysis and Jensen-Shannon divergence. It is concluded that
they result from slow variations in letter frequency distribution, which are a
consequence of slow variations in lexical composition within the text. These
correlations are preserved by random letter shuffling within a moving window.
As such, they do reflect structural properties of the text, but in a very
indirect manner.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,016</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="72"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Recognizing analogies, synonyms, antonyms, and associations appear to be four
distinct tasks, requiring distinct NLP algorithms. In the past, the four tasks
have been treated independently, using a wide variety of algorithms. These four
semantic classes, however, are a tiny sample of the full range of semantic
phenomena, and we cannot afford to create ad hoc algorithms for each semantic
phenomenon; we need to seek a unified approach. We propose to subsume a broad
range of phenomena under analogies. To limit the scope of this paper, we
restrict our attention to the subsumption of synonyms, antonyms, and
associations. We introduce a supervised corpus-based machine learning algorithm
for classifying analogous word pairs, and we show that it can solve
multiple-choice SAT analogy questions, TOEFL synonym questions, ESL
synonym-antonym questions, and similar-associated-both questions from cognitive
psychology.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="73"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Using descriptive mark-up to formalize translation quality assessment</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The paper deals with using descriptive mark-up to emphasize translation
mistakes. The author postulates the necessity to develop a standard and formal
XML-based way of describing translation mistakes. It is considered to be
important for achieving impersonal translation quality assessment. Marked-up
translations can be used in corpus translation studies; moreover, automatic
translation assessment based on marked-up mistakes is possible. The paper
concludes with setting up guidelines for further activity within the described
field.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="74"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Distribution of complexities in the Vai script</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In the paper, we analyze the distribution of complexities in the Vai script,
an indigenous syllabic writing system from Liberia. It is found that the
uniformity hypothesis for complexities fails for this script. The models using
Poisson distribution for the number of components and hyper-Poisson
distribution for connections provide good fits in the case of the Vai script.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="75"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Une grammaire formelle du cr\'eole martiniquais pour la g\'en\'eration
  automatique</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this article, some first elements of a computational modelling of the
grammar of the Martiniquese French Creole dialect are presented. The sources of
inspiration for the modelling is the functional description given by Damoiseau
(1984), and Pinalie's &amp; Bernabe's (1999) grammar manual. Based on earlier works
in text generation (Vaillant, 1997), a unification grammar formalism, namely
Tree Adjoining Grammars (TAG), and a modelling of lexical functional categories
based on syntactic and semantic properties, are used to implement a grammar of
Martiniquese Creole which is used in a prototype of text generation system. One
of the main applications of the system could be its use as a tool software
supporting the task of learning Creole as a second language. -- Nous
pr\'esenterons dans cette communication les premiers travaux de mod\'elisation
informatique d'une grammaire de la langue cr\'eole martiniquaise, en nous
inspirant des descriptions fonctionnelles de Damoiseau (1984) ainsi que du
manuel de Pinalie &amp; Bernab\'e (1999). Prenant appui sur des travaux
ant\'erieurs en g\'en\'eration de texte (Vaillant, 1997), nous utilisons un
formalisme de grammaires d'unification, les grammaires d'adjonction d'arbres
(TAG d'apr\`es l'acronyme anglais), ainsi qu'une mod\'elisation de cat\'egories
lexicales fonctionnelles \`a base syntaxico-s\'emantique, pour mettre en oeuvre
une grammaire du cr\'eole martiniquais utilisable dans une maquette de
syst\`eme de g\'en\'eration automatique. L'un des int\'er\^ets principaux de ce
syst\`eme pourrait \^etre son utilisation comme logiciel outil pour l'aide \`a
l'apprentissage du cr\'eole en tant que langue seconde.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,003</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="76"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Layered Grammar Model: Using Tree-Adjoining Grammars to Build a Common
  Syntactic Kernel for Related Dialects</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This article describes the design of a common syntactic description for the
core grammar of a group of related dialects. The common description does not
rely on an abstract sub-linguistic structure like a metagrammar: it consists in
a single FS-LTAG where the actual specific language is included as one of the
attributes in the set of attribute types defined for the features. When the
lang attribute is instantiated, the selected subset of the grammar is
equivalent to the grammar of one dialect. When it is not, we have a model of a
hybrid multidialectal linguistic system. This principle is used for a group of
creole languages of the West-Atlantic area, namely the French-based Creoles of
Haiti, Guadeloupe, Martinique and French Guiana.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="77"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Analyse spectrale des textes: d\'etection automatique des fronti\`eres
  de langue et de discours</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We propose a theoretical framework within which information on the vocabulary
of a given corpus can be inferred on the basis of statistical information
gathered on that corpus. Inferences can be made on the categories of the words
in the vocabulary, and on their syntactical properties within particular
languages. Based on the same statistical data, it is possible to build matrices
of syntagmatic similarity (bigram transition matrices) or paradigmatic
similarity (probability for any pair of words to share common contexts). When
clustered with respect to their syntagmatic similarity, words tend to group
into sublanguage vocabularies, and when clustered with respect to their
paradigmatic similarity, into syntactic or semantic classes. Experiments have
explored the first of these two possibilities. Their results are interpreted in
the frame of a Markov chain modelling of the corpus' generative processe(s): we
show that the results of a spectral analysis of the transition matrix can be
interpreted as probability distributions of words within clusters. This method
yields a soft clustering of the vocabulary into sublanguages which contribute
to the generation of heterogeneous corpora. As an application, we show how
multilingual texts can be visually segmented into linguistically homogeneous
segments. Our method is specifically useful in the case of related languages
which happened to be mixed in corpora.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,006</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="78"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Soft Uncoupling of Markov Chains for Permeable Language Distinction: A
  New Algorithm</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Without prior knowledge, distinguishing different languages may be a hard
task, especially when their borders are permeable. We develop an extension of
spectral clustering -- a powerful unsupervised classification toolbox -- that
is shown to resolve accurately the task of soft language distinction. At the
heart of our approach, we replace the usual hard membership assignment of
spectral clustering by a soft, probabilistic assignment, which also presents
the advantage to bypass a well-known complexity bottleneck of the method.
Furthermore, our approach relies on a novel, convenient construction of a
Markov chain out of a corpus. Extensive experiments with a readily available
system clearly display the potential of the method, which brings a visually
appealing soft distinction of languages that may define altogether a whole
corpus.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,006</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="79"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Text as Statistical Mechanics Object</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this article we present a model of human written text based on statistical
mechanics approach by deriving the potential energy for different parts of the
text using large text corpus. We have checked the results numerically and found
that the specific heat parameter effectively separates the closed class words
from the specific terms used in the text.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="80"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Language structure in the n-object naming game</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We examine a naming game with two agents trying to establish a common
vocabulary for n objects. Such efforts lead to the emergence of language that
allows for an efficient communication and exhibits some degree of homonymy and
synonymy. Although homonymy reduces the communication efficiency, it seems to
be a dynamical trap that persists for a long, and perhaps indefinite, time. On
the other hand, synonymy does not reduce the efficiency of communication, but
appears to be only a transient feature of the language. Thus, in our model the
role of synonymy decreases and in the long-time limit it becomes negligible. A
similar rareness of synonymy is observed in present natural languages. The role
of noise, that distorts the communicated words, is also examined. Although, in
general, the noise reduces the communication efficiency, it also regroups the
words so that they are more evenly distributed within the available "verbal"
space.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="81"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Assembling Actor-based Mind-Maps from Text Stream</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  For human beings, the processing of text streams of unknown size leads
generally to problems because e.g. noise must be selected out, information be
tested for its relevance or redundancy, and linguistic phenomenon like
ambiguity or the resolution of pronouns be advanced. Putting this into
simulation by using an artificial mind-map is a challenge, which offers the
gate for a wide field of applications like automatic text summarization or
punctual retrieval. In this work we present a framework that is a first step
towards an automatic intellect. It aims at assembling a mind-map based on
incoming text streams and on a subject-verb-object strategy, having the verb as
an interconnection between the adjacent nouns. The mind-map's performance is
enriched by a pronoun resolution engine that bases on the work of D. Klein, and
C. D. Manning.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="82"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">CoZo+ - A Content Zoning Engine for textual documents</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Content zoning can be understood as a segmentation of textual documents into
zones. This is inspired by [6] who initially proposed an approach for the
argumentative zoning of textual documents. With the prototypical CoZo+ engine,
we focus on content zoning towards an automatic processing of textual streams
while considering only the actors as the zones. We gain information that can be
used to realize an automatic recognition of content for pre-defined actors. We
understand CoZo+ as a necessary pre-step towards an automatic generation of
summaries and to make intellectual ownership of documents detectable.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="83"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">UNL-French deconversion as transfer &amp; generation from an interlingua
  with possible quality enhancement through offline human interaction</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We present the architecture of the UNL-French deconverter, which "generates"
from the UNL interlingua by first"localizing" the UNL form for French, within
UNL, and then applying slightly adapted but classical transfer and generation
techniques, implemented in GETA's Ariane-G5 environment, supplemented by some
UNL-specific tools. Online interaction can be used during deconversion to
enhance output quality and is now used for development purposes. We show how
interaction could be delayed and embedded in the postedition phase, which would
then interact not directly with the output text, but indirectly with several
components of the deconverter. Interacting online or offline can improve the
quality not only of the utterance at hand, but also of the utterances processed
later, as various preferences may be automatically changed to let the
deconverter "learn".
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1,999</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="84"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The Application of Fuzzy Logic to Collocation Extraction</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Collocations are important for many tasks of Natural language processing such
as information retrieval, machine translation, computational lexicography etc.
So far many statistical methods have been used for collocation extraction.
Almost all the methods form a classical crisp set of collocation. We propose a
fuzzy logic approach of collocation extraction to form a fuzzy set of
collocations in which each word combination has a certain grade of membership
for being collocation. Fuzzy logic provides an easy way to express natural
language into fuzzy logic rules. Two existing methods; Mutual information and
t-test have been utilized for the input of the fuzzy inference system. The
resulting membership function could be easily seen and demonstrated. To show
the utility of the fuzzy logic some word pairs have been examined as an
example. The working data has been based on a corpus of about one million words
contained in different novels constituting project Gutenberg available on
www.gutenberg.org. The proposed method has all the advantages of the two
methods, while overcoming their drawbacks. Hence it provides a better result
than the two methods.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="85"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Computational Model to Disentangle Semantic Information Embedded in
  Word Association Norms</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Two well-known databases of semantic relationships between pairs of words
used in psycholinguistics, feature-based and association-based, are studied as
complex networks. We propose an algorithm to disentangle feature based
relationships from free association semantic networks. The algorithm uses the
rich topology of the free association semantic network to produce a new set of
relationships between words similar to those observed in feature production
norms.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="86"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The Latent Relation Mapping Engine: Algorithm and Experiments</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Many AI researchers and cognitive scientists have argued that analogy is the
core of cognition. The most influential work on computational modeling of
analogy-making is Structure Mapping Theory (SMT) and its implementation in the
Structure Mapping Engine (SME). A limitation of SME is the requirement for
complex hand-coded representations. We introduce the Latent Relation Mapping
Engine (LRME), which combines ideas from SME and Latent Relational Analysis
(LRA) in order to remove the requirement for hand-coded representations. LRME
builds analogical mappings between lists of words, using a large corpus of raw
text to automatically discover the semantic relations among the words. We
evaluate LRME on a set of twenty analogical mapping problems, ten based on
scientific analogies and ten based on common metaphors. LRME achieves
human-level performance on the twenty problems. We compare LRME with a variety
of alternative approaches and find that they are not able to reach the same
level of performance.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="87"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Discovering Global Patterns in Linguistic Networks through Spectral
  Analysis: A Case Study of the Consonant Inventories</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Recent research has shown that language and the socio-cognitive phenomena
associated with it can be aptly modeled and visualized through networks of
linguistic entities. However, most of the existing works on linguistic networks
focus only on the local properties of the networks. This study is an attempt to
analyze the structure of languages via a purely structural technique, namely
spectral analysis, which is ideally suited for discovering the global
correlations in a network. Application of this technique to PhoNet, the
co-occurrence network of consonants, not only reveals several natural
linguistic principles governing the structure of the consonant inventories, but
is also able to quantify their relative importance. We believe that this
powerful technique can be successfully applied, in general, to study the
structure of natural languages.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="88"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Beyond word frequency: Bursts, lulls, and scaling in the temporal
  distributions of words</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Background: Zipf's discovery that word frequency distributions obey a power
law established parallels between biological and physical processes, and
language, laying the groundwork for a complex systems perspective on human
communication. More recent research has also identified scaling regularities in
the dynamics underlying the successive occurrences of events, suggesting the
possibility of similar findings for language as well.
  Methodology/Principal Findings: By considering frequent words in USENET
discussion groups and in disparate databases where the language has different
levels of formality, here we show that the distributions of distances between
successive occurrences of the same word display bursty deviations from a
Poisson process and are well characterized by a stretched exponential (Weibull)
scaling. The extent of this deviation depends strongly on semantic type -- a
measure of the logicality of each word -- and less strongly on frequency. We
develop a generative model of this behavior that fully determines the dynamics
of word usage.
  Conclusions/Significance: Recurrence patterns of words are well described by
a stretched exponential distribution of recurrence times, an empirical scaling
that cannot be anticipated from Zipf's law. Because the use of words provides a
uniquely precise and powerful lens on human thought and activity, our findings
also have implications for other overt manifestations of collective human
dynamics.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="89"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Statistical analysis of the Indus script using $n$-grams</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The Indus script is one of the major undeciphered scripts of the ancient
world. The small size of the corpus, the absence of bilingual texts, and the
lack of definite knowledge of the underlying language has frustrated efforts at
decipherment since the discovery of the remains of the Indus civilisation.
Recently, some researchers have questioned the premise that the Indus script
encodes spoken language. Building on previous statistical approaches, we apply
the tools of statistical language processing, specifically $n$-gram Markov
chains, to analyse the Indus script for syntax. Our main results are that the
script has well-defined signs which begin and end texts, that there is
directionality and strong correlations in the sign order, and that there are
groups of signs which appear to have identical syntactic function. All these
require no {\it a priori} suppositions regarding the syntactic or semantic
content of the signs, but follow directly from the statistical analysis. Using
information theoretic measures, we find the information in the script to be
intermediate between that of a completely random and a completely fixed
ordering of signs. Our study reveals that the Indus script is a structured sign
system showing features of a formal language, but, at present, cannot
conclusively establish that it encodes {\it natural} language. Our $n$-gram
Markov model is useful for predicting signs which are missing or illegible in a
corpus of Indus texts. This work forms the basis for the development of a
stochastic grammar which can be used to explore the syntax of the Indus script
in greater detail.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,015</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="90"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Approaching the linguistic complexity</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We analyze the rank-frequency distributions of words in selected English and
Polish texts. We compare scaling properties of these distributions in both
languages. We also study a few small corpora of Polish literary texts and find
that for a corpus consisting of texts written by different authors the basic
scaling regime is broken more strongly than in the case of comparable corpus
consisting of texts written by the same author. Similarly, for a corpus
consisting of texts translated into Polish from other languages the scaling
regime is broken more strongly than for a comparable corpus of native Polish
texts. Moreover, based on the British National Corpus, we consider the
rank-frequency distributions of the grammatically basic forms of words (lemmas)
tagged with their proper part of speech. We find that these distributions do
not scale if each part of speech is analyzed separately. The only part of
speech that independently develops a trace of scaling is verbs.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="91"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Du corpus au dictionnaire</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this article, we propose an automatic process to build multi-lingual
lexico-semantic resources. The goal of these resources is to browse
semantically textual information contained in texts of different languages.
This method uses a mathematical model called Atlas s\'emantiques in order to
represent the different senses of each word. It uses the linguistic relations
between words to create graphs that are projected into a semantic space. These
projections constitute semantic maps that denote the sense trends of each given
word. This model is fed with syntactic relations between words extracted from a
corpus. Therefore, the lexico-semantic resource produced describes all the
words and all their meanings observed in the corpus. The sense trends are
expressed by syntactic contexts, typical for a given meaning. The link between
each sense trend and the utterances used to build the sense trend are also
stored in an index. Thus all the instances of a word in a particular sense are
linked and can be browsed easily. And by using several corpora of different
languages, several resources are built that correspond with each other through
languages. It makes it possible to browse information through languages thanks
to syntactic contexts translations (even if some of them are partial).
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="92"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Google distance between words</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Cilibrasi and Vitanyi have demonstrated that it is possible to extract the
meaning of words from the world-wide web. To achieve this, they rely on the
number of webpages that are found through a Google search containing a given
word and they associate the page count to the probability that the word appears
on a webpage. Thus, conditional probabilities allow them to correlate one word
with another word's meaning. Furthermore, they have developed a similarity
distance function that gauges how closely related a pair of words is. We
present a specific counterexample to the triangle inequality for this
similarity distance function.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,015</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="93"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">On the Entropy of Written Spanish</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This paper reports on results on the entropy of the Spanish language. They
are based on an analysis of natural language for n-word symbols (n = 1 to 18),
trigrams, digrams, and characters. The results obtained in this work are based
on the analysis of twelve different literary works in Spanish, as well as a
279917 word news file provided by the Spanish press agency EFE. Entropy values
are calculated by a direct method using computer processing and the probability
law of large numbers. Three samples of artificial Spanish language produced by
a first-order model software source are also analyzed and compared with natural
Spanish language.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,012</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="94"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Beyond Zipf's law: Modeling the structure of human language</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Human language, the most powerful communication system in history, is closely
associated with cognition. Written text is one of the fundamental
manifestations of language, and the study of its universal regularities can
give clues about how our brains process information and how we, as a society,
organize and share it. Still, only classical patterns such as Zipf's law have
been explored in depth. In contrast, other basic properties like the existence
of bursts of rare words in specific documents, the topical organization of
collections, or the sublinear growth of vocabulary size with the length of a
document, have only been studied one by one and mainly applying heuristic
methodologies rather than basic principles and general mechanisms. As a
consequence, there is a lack of understanding of linguistic processes as
complex emergent phenomena. Beyond Zipf's law for word frequencies, here we
focus on Heaps' law, burstiness, and the topicality of document collections,
which encode correlations within and across documents absent in random null
models. We introduce and validate a generative model that explains the
simultaneous emergence of all these patterns from simple rules. As a result, we
find a connection between the bursty nature of rare words and the topical
organization of texts and identify dynamic word ranking and memory across
documents as key mechanisms explaining the non trivial organization of written
text. Our research can have broad implications and practical applications in
computer science, cognitive science, and linguistics.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="95"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">New Confidence Measures for Statistical Machine Translation</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  A confidence measure is able to estimate the reliability of an hypothesis
provided by a machine translation system. The problem of confidence measure can
be seen as a process of testing : we want to decide whether the most probable
sequence of words provided by the machine translation system is correct or not.
In the following we describe several original word-level confidence measures
for machine translation, based on mutual information, n-gram language model and
lexical features language model. We evaluate how well they perform individually
or together, and show that using a combination of confidence measures based on
mutual information yields a classification error rate as low as 25.1% with an
F-measure of 0.708.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="96"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">BagPack: A general framework to represent semantic relations</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We introduce a way to represent word pairs instantiating arbitrary semantic
relations that keeps track of the contexts in which the words in the pair occur
both together and independently. The resulting features are of sufficient
generality to allow us, with the help of a standard supervised machine learning
algorithm, to tackle a variety of unrelated semantic tasks with good results
and almost no task-specific tailoring.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="97"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">What's in a Message?</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper we present the first step in a larger series of experiments for
the induction of predicate/argument structures. The structures that we are
inducing are very similar to the conceptual structures that are used in Frame
Semantics (such as FrameNet). Those structures are called messages and they
were previously used in the context of a multi-document summarization system of
evolving events. The series of experiments that we are proposing are
essentially composed from two stages. In the first stage we are trying to
extract a representative vocabulary of words. This vocabulary is later used in
the second stage, during which we apply to it various clustering approaches in
order to identify the clusters of predicates and arguments--or frames and
semantic roles, to use the jargon of Frame Semantics. This paper presents in
detail and evaluates the first stage.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="98"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Syntactic variation of support verb constructions</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We report experiments about the syntactic variations of support verb
constructions, a special type of multiword expressions (MWEs) containing
predicative nouns. In these expressions, the noun can occur with or without the
verb, with no clear-cut semantic difference. We extracted from a large French
corpus a set of examples of the two situations and derived statistical results
from these data. The extraction involved large-coverage language resources and
finite-state techniques. The results show that, most frequently, predicative
nouns occur without a support verb. This fact has consequences on methods of
extracting or recognising MWEs.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,008</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="99"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Network of two-Chinese-character compound words in Japanese language</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Some statistical properties of a network of two-Chinese-character compound
words in Japanese language are reported. In this network, a node represents a
Chinese character and an edge represents a two-Chinese-character compound word.
It is found that this network has properties of "small-world" and "scale-free."
A network formed by only Chinese characters for common use ({\it joyo-kanji} in
Japanese), which is regarded as a subclass of the original network, also has
small-world property. However, a degree distribution of the network exhibits no
clear power law. In order to reproduce disappearance of the power-law property,
a model for a selecting process of the Chinese characters for common use is
proposed.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2,009</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Computation and Language</span></div></div>
							</td>
					</tr></tbody></table>
		<div class="bg-linear-to-b sticky left-0 border-t border-dashed border-gray-300 from-gray-100 to-white py-3 text-center font-mono text-xs dark:border-gray-700 dark:from-gray-950 dark:to-gray-900">End of preview. <a href="/datasets/MaartenGr/arxiv_nlp/viewer/default/train" class="group"><span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Expand</span>
						in <svg class="text-lg mr-0.5 inline -translate-y-px text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>Data Studio
					</a></div></div>




			<div class="bg-linear-to-b from-gray-100 to-white dark:from-gray-950 dark:to-gray-900 "><hr class="flex-none -translate-y-px border-t border-dashed border-gray-300 bg-white dark:border-gray-700 dark:bg-gray-950">
					<nav><ul class="flex select-none items-center justify-between space-x-2 text-gray-700 sm:justify-center py-1 text-center font-mono text-xs rounded-b-lg"><li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 pointer-events-none cursor-default text-gray-400 hover:text-gray-700" href=""><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg>
		Previous</a></li>
			<li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1 bg-gray-50 font-semibold ring-1 ring-inset ring-gray-200 dark:bg-gray-900 dark:text-yellow-500 dark:ring-gray-900 hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/MaartenGr/arxiv_nlp/viewer/default/train?p=0">1</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/MaartenGr/arxiv_nlp/viewer/default/train?p=1">2</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/MaartenGr/arxiv_nlp/viewer/default/train?p=2">3</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  pointer-events-none cursor-default" href="#">...</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/MaartenGr/arxiv_nlp/viewer/default/train?p=449">450</a>
				</li>
			<li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 " href="/datasets/MaartenGr/arxiv_nlp/viewer/default/train?p=1">Next
		<svg class="ml-1.5 transform rotate-180" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg></a></li></ul></nav></div></div>






</div></div></div></div></div></div></div>

				
					<div class="SVELTE_HYDRATER contents" data-target="RepoCodeCopy" data-props="{}"><div></div></div>
					

					<div class="SVELTE_HYDRATER contents" data-target="SideNavigation" data-props="{&quot;titleTree&quot;:[],&quot;classNames&quot;:&quot;top-6&quot;}">

</div>
					<div class="2xl:pr-6"><div class="prose pl-6 -ml-6 hf-sanitized hf-sanitized-Fb_qcGDnPF27J0chGBgRz">
	<!-- HTML_TAG_START --><h1 class="relative group flex items-center">
	<a rel="nofollow" href="#arxiv-abstracts" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="arxiv-abstracts">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		arXiv Abstracts
	</span>
</h1>
<p>Abstracts for the <code>cs.CL</code> category of ArXiv between 1991 and 2024. This dataset was created as an instructional tool for the Clustering and Topic Modeling chapter in the upcoming
<a rel="nofollow" href="https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/">"Hands-On Large Language Models"</a> book. </p>
<p>The original dataset was retrieved <a rel="nofollow" href="https://www.kaggle.com/datasets/Cornell-University/arxiv">here</a>. </p>
<p>This subset will be updated towards the release of the book to make sure it captures relatively recent articles in the domain.</p>
<!-- HTML_TAG_END --></div>
</div></section>
			
			<section class="pt-6 border-gray-100 md:pb-24 md:pl-6 md:w-64 lg:w-80 xl:w-96 flex-none order-first md:order-none md:border-l pt-3! md:pt-6!"><dl class="flex items-baseline justify-between"><dt class="text-sm text-gray-500">Downloads last month</dt><div class="mx-4 flex-1 border-b border-dotted"></div><dd class="font-semibold">1,696</dd></dl>
				
				<div class="divider-column-vertical"></div>
				<div class="grid grid-cols-2 gap-x-2 md:flex md:flex-row md:flex-wrap"><div class="SVELTE_HYDRATER contents" data-target="DatasetAndModelActionsDropdown" data-props="{&quot;classNames&quot;:&quot;order-last&quot;,&quot;discussionsDisabled&quot;:false,&quot;repo&quot;:{&quot;type&quot;:&quot;dataset&quot;,&quot;name&quot;:&quot;MaartenGr/arxiv_nlp&quot;},&quot;canWrite&quot;:false,&quot;canDisable&quot;:false,&quot;repoIsPrivate&quot;:false,&quot;repoIsGated&quot;:false,&quot;repoIsDisabled&quot;:false,&quot;repoIsAdminFlaggedNFAA&quot;:false,&quot;repoHasBlockedOids&quot;:false}"><div class="order-last"><div class="relative ">
	<button class="btn px-1.5 py-1.5 " type="button">
		
			<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="p-0.5" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><circle cx="16" cy="7" r="3" fill="currentColor"></circle><circle cx="16" cy="16" r="3" fill="currentColor"></circle><circle cx="16" cy="25" r="3" fill="currentColor"></circle></svg>
		
		</button>
	
	
	</div></div>













</div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetLibrary" data-props="{&quot;classNames&quot;:&quot;md:w-full xl:w-auto xl:flex-none&quot;,&quot;libraries&quot;:[{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;datasets&quot;,&quot;function&quot;:&quot;load_dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{},&quot;code&quot;:&quot;from datasets import load_dataset\n\nds = load_dataset(\&quot;MaartenGr/arxiv_nlp\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;pandas&quot;,&quot;function&quot;:&quot;pd.read_csv&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;data.csv&quot;}},&quot;code&quot;:&quot;import pandas as pd\n\ndf = pd.read_csv(\&quot;hf://datasets/MaartenGr/arxiv_nlp/data.csv\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;mlcroissant&quot;,&quot;function&quot;:&quot;Dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;record_set&quot;:&quot;default&quot;,&quot;partial&quot;:false},&quot;code&quot;:&quot;from mlcroissant import Dataset\n\nds = Dataset(jsonld=\&quot;https://huggingface.co/api/datasets/MaartenGr/arxiv_nlp/croissant\&quot;)\nrecords = ds.records(\&quot;default\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;polars&quot;,&quot;function&quot;:&quot;pl.read_csv&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;data.csv&quot;}},&quot;code&quot;:&quot;import polars as pl\n\ndf = pl.read_csv('hf://datasets/MaartenGr/arxiv_nlp/data.csv')\n&quot;}]}]}"><div class="relative md:w-full xl:w-auto xl:flex-none">
	<button class="from-gray-800! to-black! max-xl:mb-2 text-white! gap-1! border-gray-800! dark:border-gray-900!  btn w-full cursor-pointer text-sm" type="button">
		<svg class="mr-1.5 mr-0.5! " xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>
			Use this dataset
		<svg class="-mr-1 text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path d="M16.293 9.293L12 13.586L7.707 9.293l-1.414 1.414L12 16.414l5.707-5.707z" fill="currentColor"></path></svg></button>
	
	
	</div>

</div>
					
					</div>
				<div class="divider-column-vertical"></div>
					<div class="flex flex-col flex-wrap xl:flex-row"><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of downloaded dataset files:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->53.2 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 " href="/datasets/MaartenGr/arxiv_nlp/tree/refs%2Fconvert%2Fparquet/" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of the auto-converted Parquet files:</div>
		<div class="truncate text-sm group-hover:underline">
			<!-- HTML_TAG_START -->29.7 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Number of rows:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->44,949<!-- HTML_TAG_END --></div></a></div>
				
				
				
				

				
				<div class="divider-column-vertical md:hidden"></div></section></div></main>

	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container relative flex flex-col justify-between space-y-2 py-6 text-gray-500 max-md:*:self-start md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="SVELTE_HYDRATER contents" data-target="ThemeSwitcher" data-props="{&quot;theme&quot;:&quot;system&quot;,&quot;isLoggedIn&quot;:false,&quot;menuClassNames&quot;:&quot;md:-top-24&quot;,&quot;classNames&quot;:&quot;max-md:mb-5 max-md:*:self-start&quot;}">
<div class="relative inline-block max-md:mb-5 max-md:*:self-start">
	<button class="rounded-full border border-gray-100 pl-2 py-1 pr-2.5  flex items-center text-sm text-gray-500 bg-white hover:bg-purple-50 hover:border-purple-200 dark:hover:bg-gray-800 dark:hover:border-gray-950 dark:border-gray-800 " type="button">
		<svg class="mr-1.5 text-gray-500" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M6 22.5h20a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2H6a2 2 0 0 0-2 2v13.5a2 2 0 0 0 2 2ZM7 7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h18a1 1 0 0 0 1-1V8a1 1 0 0 0-1-1H7Z" fill="currentColor"></path><path d="M6 8a1 1 0 0 1 1-1h18a1 1 0 0 1 1 1v11a1 1 0 0 1-1 1H7a1 1 0 0 1-1-1V8Z" fill="currentColor" fill-opacity=".4"></path><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path></svg>
			System theme
		</button>
	
	
	</div></div>
		<div class="font-semibold text-black md:hidden">Company</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="max-md:mb-4! max-md:mt-8! group flex-none max-md:order-last"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="max-md:mt-8! font-semibold text-black md:hidden">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("\/front\/build\/kube-68d7aa0\/index.js");
			window.moonSha = "kube-68d7aa0\/";
			window.__hf_deferred = {};
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>
	</body>
</html>
