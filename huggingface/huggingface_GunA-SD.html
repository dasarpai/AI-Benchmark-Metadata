<!doctype html>
<html class="">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
		<meta name="description" content="Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science." />
		<meta property="fb:app_id" content="1321688464574422" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@huggingface" />
		<meta name="twitter:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/GunA-SD/bash_code.png" />
		<meta property="og:title" content="GunA-SD/bash_code Â· Datasets at Hugging Face" />
		<meta property="og:type" content="website" />
		<meta property="og:url" content="https://huggingface.co/datasets/GunA-SD/bash_code" />
		<meta property="og:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/GunA-SD/bash_code.png" />

		<link rel="stylesheet" href="/front/build/kube-68d7aa0/style.css" />

		<link rel="preconnect" href="https://fonts.gstatic.com" />
		<link
			href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap"
			rel="stylesheet"
		/>

		<link
			rel="preload"
			href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css"
			as="style"
			onload="this.onload=null;this.rel='stylesheet'"
		/>
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		<script>const guestTheme = document.cookie.match(/theme=(\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches));</script>
<link rel="canonical" href="https://huggingface.co/datasets/GunA-SD/bash_code"> <script type="application/ld+json">{
  "@context": {
    "@language": "en",
    "@vocab": "https:\/\/schema.org\/",
    "citeAs": "cr:citeAs",
    "column": "cr:column",
    "conformsTo": "dct:conformsTo",
    "cr": "http:\/\/mlcommons.org\/croissant\/",
    "data": {
      "@id": "cr:data",
      "@type": "@json"
    },
    "dataBiases": "cr:dataBiases",
    "dataCollection": "cr:dataCollection",
    "dataType": {
      "@id": "cr:dataType",
      "@type": "@vocab"
    },
    "dct": "http:\/\/purl.org\/dc\/terms\/",
    "extract": "cr:extract",
    "field": "cr:field",
    "fileProperty": "cr:fileProperty",
    "fileObject": "cr:fileObject",
    "fileSet": "cr:fileSet",
    "format": "cr:format",
    "includes": "cr:includes",
    "isLiveDataset": "cr:isLiveDataset",
    "jsonPath": "cr:jsonPath",
    "key": "cr:key",
    "md5": "cr:md5",
    "parentField": "cr:parentField",
    "path": "cr:path",
    "personalSensitiveInformation": "cr:personalSensitiveInformation",
    "recordSet": "cr:recordSet",
    "references": "cr:references",
    "regex": "cr:regex",
    "repeated": "cr:repeated",
    "replace": "cr:replace",
    "sc": "https:\/\/schema.org\/",
    "separator": "cr:separator",
    "source": "cr:source",
    "subField": "cr:subField",
    "transform": "cr:transform"
  },
  "@type": "sc:Dataset",
  "distribution": [
    {
      "@type": "cr:FileObject",
      "@id": "repo",
      "name": "repo",
      "description": "The Hugging Face git repository.",
      "contentUrl": "https:\/\/huggingface.co\/datasets\/GunA-SD\/bash_code\/tree\/refs%2Fconvert%2Fparquet",
      "encodingFormat": "git+https",
      "sha256": "https:\/\/github.com\/mlcommons\/croissant\/issues\/80"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-default",
      "name": "parquet-files-for-config-default",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "default\/*\/*.parquet"
    }
  ],
  "recordSet": [
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "default_splits\/split_name"
      },
      "@id": "default_splits",
      "name": "default_splits",
      "description": "Splits for the default config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "default_splits\/split_name": "train"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "default",
      "name": "default",
      "description": "GunA-SD\/bash_code - 'default' subset",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default\/split",
          "name": "default\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "default\/(?:partial-)?(train)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "default_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/content",
          "name": "default\/content",
          "description": "Column 'content' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "content"
            }
          }
        }
      ]
    }
  ],
  "conformsTo": "http:\/\/mlcommons.org\/croissant\/1.0",
  "name": "bash_code",
  "description": "This dataset is a collection of bash programs from various GitHub repositories and open source projects.\nThe dataset might contain harmful code.\n",
  "alternateName": [
    "GunA-SD\/bash_code"
  ],
  "creator": {
    "@type": "Person",
    "name": "gunasekar",
    "url": "https:\/\/huggingface.co\/GunA-SD"
  },
  "keywords": [
    "text-generation",
    "English",
    "apache-2.0",
    "100K - 1M",
    "parquet",
    "Text",
    "Datasets",
    "Dask",
    "Croissant",
    "Polars",
    "ðŸ‡ºðŸ‡¸ Region: US",
    "Bash"
  ],
  "license": "https:\/\/choosealicense.com\/licenses\/apache-2.0\/",
  "url": "https:\/\/huggingface.co\/datasets\/GunA-SD\/bash_code"
}</script> 

		<title>GunA-SD/bash_code Â· Datasets at Hugging Face</title>

		<script
			defer
			data-domain="huggingface.co"
			event-loggedIn="false"
			src="/js/script.pageview-props.js"
		></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script>
			window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer></script>
	</head>
	<body class="flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DatasetPage">
		<div class="flex min-h-dvh flex-col"><div class="SVELTE_HYDRATER contents" data-target="SystemThemeMonitor" data-props="{&quot;isLoggedIn&quot;:false}"></div>

	<div class="SVELTE_HYDRATER contents" data-target="MainHeader" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false,&quot;isPro&quot;:false}"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg">
				<span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a>
			<div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..."   spellcheck="false" type="text" value="">
	<svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
	</div>
			<div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg>
		</button>

	</div></div>
		<nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 2xl:space-x-2"><li class="hover:text-indigo-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
					Models</a>
			</li><li class="hover:text-red-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					Datasets</a>
			</li><li class="hover:text-blue-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg>
					Spaces</a>
			</li><li class="hover:text-yellow-700 max-xl:hidden"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 text-yellow-500!" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg>
					Posts</a>
			</li><li class="hover:text-yellow-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 16 16"><path d="m2.28 3.7-.3.16a.67.67 0 0 0-.34.58v8.73l.01.04.02.07.01.04.03.06.02.04.02.03.04.06.05.05.04.04.06.04.06.04.08.04.08.02h.05l.07.02h.11l.04-.01.07-.02.03-.01.07-.03.22-.12a5.33 5.33 0 0 1 5.15.1.67.67 0 0 0 .66 0 5.33 5.33 0 0 1 5.33 0 .67.67 0 0 0 1-.58V4.36a.67.67 0 0 0-.34-.5l-.3-.17v7.78a.63.63 0 0 1-.87.59 4.9 4.9 0 0 0-4.35.35l-.65.39a.29.29 0 0 1-.15.04.29.29 0 0 1-.16-.04l-.65-.4a4.9 4.9 0 0 0-4.34-.34.63.63 0 0 1-.87-.59V3.7Z" fill="currentColor" class="dark:opacity-40"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M8 3.1a5.99 5.99 0 0 0-5.3-.43.66.66 0 0 0-.42.62v8.18c0 .45.46.76.87.59a4.9 4.9 0 0 1 4.34.35l.65.39c.05.03.1.04.16.04.05 0 .1-.01.15-.04l.65-.4a4.9 4.9 0 0 1 4.35-.34.63.63 0 0 0 .86-.59V3.3a.67.67 0 0 0-.41-.62 5.99 5.99 0 0 0-5.3.43l-.3.17L8 3.1Zm.73 1.87a.43.43 0 1 0-.86 0v5.48a.43.43 0 0 0 .86 0V4.97Z" fill="currentColor" class="opacity-40 dark:opacity-100"></path><path d="M8.73 4.97a.43.43 0 1 0-.86 0v5.48a.43.43 0 1 0 .86 0V4.96Z" fill="currentColor" class="dark:opacity-40"></path></svg>
					Docs</a>
			</li><li class="hover:text-black dark:hover:text-white"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/enterprise"><svg class="mr-1.5 text-gray-400 group-hover:text-black dark:group-hover:text-white" xmlns="http://www.w3.org/2000/svg" fill="none" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 33 27"><path fill="currentColor" fill-rule="evenodd" d="M13.5.7a8.7 8.7 0 0 0-7.7 5.7L1 20.6c-1 3.1.9 5.7 4.1 5.7h15c3.3 0 6.8-2.6 7.8-5.7l4.6-14.2c1-3.1-.8-5.7-4-5.7h-15Zm1.1 5.7L9.8 20.3h9.8l1-3.1h-5.8l.8-2.5h4.8l1.1-3h-4.8l.8-2.3H23l1-3h-9.5Z" clip-rule="evenodd"></path></svg>
					Enterprise</a>
			</li>

		<li><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/pricing">Pricing
			</a></li>

		<li><div class="relative group">
	<button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button">
		<svg class=" text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>
			
		</button>
	
	
	</div></li>
		<li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li>
		<li><a class="block cursor-pointer whitespace-nowrap px-2 py-0.5 hover:text-gray-500 dark:text-gray-300 dark:hover:text-gray-100" href="/login">Log In
				</a></li>
			<li><a class="whitespace-nowrap rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up
					</a></li></ul></nav></div></header></div>
	
	
	
	<div class="SVELTE_HYDRATER contents" data-target="SSOBanner" data-props="{}"></div>
	
	

	<main class="flex flex-1 flex-col">
	<div class="SVELTE_HYDRATER contents" data-target="DatasetHeader" data-props="{&quot;activeTab&quot;:&quot;datasetCard&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;64395f66b9ac1d55f41e5cc4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64395f66b9ac1d55f41e5cc4/qhzWbKjN0zyRIlwpg8JRe.png&quot;,&quot;fullname&quot;:&quot;gunasekar&quot;,&quot;name&quot;:&quot;GunA-SD&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false},&quot;canReadRepoSettings&quot;:false,&quot;dataset&quot;:{&quot;author&quot;:&quot;GunA-SD&quot;,&quot;cardData&quot;:{&quot;language&quot;:[&quot;en&quot;],&quot;task_categories&quot;:[&quot;text-generation&quot;],&quot;tags&quot;:[&quot;Bash&quot;],&quot;dataset_info&quot;:{&quot;features&quot;:[{&quot;name&quot;:&quot;content&quot;,&quot;dtype&quot;:&quot;string&quot;}],&quot;splits&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;num_bytes&quot;:845687877,&quot;num_examples&quot;:342720}],&quot;download_size&quot;:347876940,&quot;dataset_size&quot;:845687877},&quot;configs&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;train&quot;,&quot;path&quot;:&quot;data/train-*&quot;}]}],&quot;license&quot;:&quot;apache-2.0&quot;,&quot;size_categories&quot;:[&quot;100K<n<1M&quot;]},&quot;cardExists&quot;:true,&quot;createdAt&quot;:&quot;2023-07-06T08:10:53.000Z&quot;,&quot;description&quot;:&quot;This dataset is a collection of bash programs from various GitHub repositories and open source projects.\nThe dataset might contain harmful code.\n&quot;,&quot;downloads&quot;:133,&quot;downloadsAllTime&quot;:879,&quot;id&quot;:&quot;GunA-SD/bash_code&quot;,&quot;isLikedByUser&quot;:false,&quot;lastModified&quot;:&quot;2024-05-03T16:51:03.000Z&quot;,&quot;likes&quot;:3,&quot;datasetsServerInfo&quot;:{&quot;viewer&quot;:&quot;viewer&quot;,&quot;numRows&quot;:342720,&quot;libraries&quot;:[&quot;datasets&quot;,&quot;dask&quot;,&quot;mlcroissant&quot;,&quot;polars&quot;],&quot;formats&quot;:[&quot;parquet&quot;],&quot;modalities&quot;:[&quot;text&quot;]},&quot;discussionsDisabled&quot;:false,&quot;repoType&quot;:&quot;dataset&quot;,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;tags&quot;:[&quot;task_categories:text-generation&quot;,&quot;language:en&quot;,&quot;license:apache-2.0&quot;,&quot;size_categories:100K<n<1M&quot;,&quot;format:parquet&quot;,&quot;modality:text&quot;,&quot;library:datasets&quot;,&quot;library:dask&quot;,&quot;library:mlcroissant&quot;,&quot;library:polars&quot;,&quot;region:us&quot;,&quot;Bash&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;task_categories:text-generation&quot;,&quot;label&quot;:&quot;text-generation&quot;,&quot;type&quot;:&quot;task_categories&quot;,&quot;subType&quot;:&quot;nlp&quot;},{&quot;id&quot;:&quot;language:en&quot;,&quot;label&quot;:&quot;English&quot;,&quot;type&quot;:&quot;language&quot;},{&quot;id&quot;:&quot;license:apache-2.0&quot;,&quot;label&quot;:&quot;apache-2.0&quot;,&quot;type&quot;:&quot;license&quot;},{&quot;id&quot;:&quot;size_categories:100K<n<1M&quot;,&quot;label&quot;:&quot;100K - 1M&quot;,&quot;type&quot;:&quot;size_categories&quot;},{&quot;id&quot;:&quot;format:parquet&quot;,&quot;label&quot;:&quot;parquet&quot;,&quot;type&quot;:&quot;format&quot;},{&quot;id&quot;:&quot;modality:text&quot;,&quot;label&quot;:&quot;Text&quot;,&quot;type&quot;:&quot;modality&quot;},{&quot;id&quot;:&quot;library:datasets&quot;,&quot;label&quot;:&quot;Datasets&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:dask&quot;,&quot;label&quot;:&quot;Dask&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:mlcroissant&quot;,&quot;label&quot;:&quot;Croissant&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:polars&quot;,&quot;label&quot;:&quot;Polars&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;ðŸ‡ºðŸ‡¸ Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;},{&quot;id&quot;:&quot;Bash&quot;,&quot;label&quot;:&quot;Bash&quot;,&quot;type&quot;:&quot;other&quot;}],&quot;hasBlockedOids&quot;:false,&quot;region&quot;:&quot;us&quot;,&quot;xetEnabled&quot;:false},&quot;discussionsStats&quot;:{&quot;closed&quot;:0,&quot;open&quot;:1,&quot;total&quot;:1}}"><header class="bg-linear-to-t border-b border-gray-100 pt-6 sm:pt-9 from-gray-50-to-white via-white dark:via-gray-950"><div class="container relative "><h1 class="flex flex-wrap items-center max-md:leading-tight mb-3 text-lg max-sm:gap-y-1.5 md:text-xl"><a href="/datasets" class="group flex items-center"><svg class="sm:mr-1.5 -mr-1 text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					<span class="mr-2.5 font-semibold text-gray-400 group-hover:text-gray-500 max-sm:hidden">Datasets:</span></a>
				<hr class="mx-1.5 h-2 translate-y-px rounded-sm border-r dark:border-gray-600 sm:hidden">
			<div class="group flex flex-none items-center"><div class="relative mr-1 flex items-center">

			

<span class="inline-block "><span class="contents"><a href="/GunA-SD" class="text-gray-400 hover:text-blue-600"><img alt="" class="w-3.5 h-3.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/64395f66b9ac1d55f41e5cc4/qhzWbKjN0zyRIlwpg8JRe.png" crossorigin="anonymous"></a></span>
	</span></div>
		

<span class="inline-block "><span class="contents"><a href="/GunA-SD" class="text-gray-400 hover:text-blue-600">GunA-SD</a></span>
	</span>
		<div class="mx-0.5 text-gray-300">/</div></div>

<div class="max-w-full "><a class="break-words font-mono font-semibold hover:text-blue-600 " href="/datasets/GunA-SD/bash_code">bash_code</a>
	<button class="relative text-sm mr-4 focus:outline-hidden inline-flex cursor-pointer items-center text-sm  mx-0.5   text-gray-600 " title="Copy dataset name to clipboard" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg>
	
	</button></div>
			<div class="inline-flex items-center overflow-hidden whitespace-nowrap rounded-md border bg-white text-sm leading-none text-gray-500  mr-2"><button class="relative flex items-center overflow-hidden from-red-50 to-transparent dark:from-red-900 px-1.5 py-1 hover:bg-linear-to-t focus:outline-hidden"  title="Like"><svg class="left-1.5 absolute" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>

		
		<span class="ml-4 pl-0.5 ">like</span></button>
	<button class="focus:outline-hidden flex items-center border-l px-1.5 py-1 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="See users who liked this repository">3</button></div>




			
			
	</h1>
		<div class="mb-3 flex flex-wrap md:mb-4"><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Tasks:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?task_categories=task_categories%3Atext-generation"><div class="tag tag-white   "><div class="tag-ico -ml-2 tag-ico-indigo"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 18 18"><path d="M16.2607 8.08202L14.468 6.28928C14.3063 6.12804 14.0873 6.03749 13.859 6.03749C13.6307 6.03749 13.4117 6.12804 13.25 6.28928L5.6375 13.904V16.9125H8.64607L16.2607 9.30002C16.422 9.13836 16.5125 8.91935 16.5125 8.69102C16.5125 8.4627 16.422 8.24369 16.2607 8.08202V8.08202ZM8.1953 15.825H6.725V14.3547L11.858 9.22118L13.3288 10.6915L8.1953 15.825ZM14.0982 9.92262L12.6279 8.45232L13.8606 7.21964L15.3309 8.68994L14.0982 9.92262Z"></path><path d="M6.18125 9.84373H7.26875V6.03748H8.9V4.94998H4.55V6.03748H6.18125V9.84373Z"></path><path d="M4.55 11.475H2.375V2.775H11.075V4.95H12.1625V2.775C12.1625 2.48658 12.0479 2.20997 11.844 2.00602C11.64 1.80208 11.3634 1.6875 11.075 1.6875H2.375C2.08658 1.6875 1.80997 1.80208 1.60602 2.00602C1.40207 2.20997 1.2875 2.48658 1.2875 2.775V11.475C1.2875 11.7634 1.40207 12.04 1.60602 12.244C1.80997 12.4479 2.08658 12.5625 2.375 12.5625H4.55V11.475Z"></path></svg></div>

	

	<span>Text Generation</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Modalities:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?modality=modality%3Atext"><div class="tag tag-white   ">
		<svg class="text-red-700 dark:text-red-600" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.619 4.619C2.667 6.573 2.667 9.715 2.667 16s0 9.428 1.952 11.38C6.573 29.333 9.715 29.333 16 29.333s9.428 0 11.38-1.953c1.953-1.95 1.953-5.095 1.953-11.38s0-9.428-1.953-11.381C25.43 2.667 22.285 2.667 16 2.667s-9.428 0-11.381 1.952m8.65 3.714c-.573 0-1.109 0-1.546.066-.495.073-1.003.248-1.41.7-.392.436-.53.956-.59 1.452-.056.464-.056 1.04-.056 1.689V13a1 1 0 1 0 2 0v-.704c0-.724.001-1.176.041-1.505q.015-.15.061-.294a.2.2 0 0 1 .031-.061q0-.003.016-.01a.8.8 0 0 1 .203-.05c.272-.04.654-.043 1.314-.043H15v11.334h-2.333a1 1 0 1 0 0 2H20a1 1 0 0 0 0-2h-3V10.333h1.667c.66 0 1.042.003 1.314.043.123.019.18.04.203.05l.015.009a.2.2 0 0 1 .032.061c.018.05.042.14.061.295.04.329.041.781.041 1.506V13a1 1 0 1 0 2 0v-.76c0-.65 0-1.225-.056-1.69-.06-.495-.198-1.015-.59-1.453-.407-.45-.915-.625-1.41-.698-.437-.067-.973-.067-1.546-.066z" fill="currentColor"></path></svg>

	

	<span>Text</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Formats:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?format=format%3Aparquet"><div class="tag tag-white   ">
		<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 34 34"><path fill-rule="evenodd" clip-rule="evenodd" d="m17.97 18.44-3.98-2.3-7.1 3.78 3.98 2.3 7.1-3.78Zm15.1-1.4-3.99-2.3-16.22 8.63 3.98 2.3 16.22-8.63Zm-5.98-3.45-3.97-2.3-7.1 3.78 3.98 2.3 7.1-3.78Zm-9.94-5.74 3.98 2.3-11.16 5.93L6 13.78l11.16-5.93Zm-13.19 7 3.98 2.3-3.04 1.62-3.98-2.3 3.04-1.61Z" fill="currentColor"></path></svg>

	

	<span>parquet</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Languages:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?language=language%3Aen"><div class="tag tag-white   ">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="text-green-600/80" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 10 10"><path fill-rule="evenodd" clip-rule="evenodd" d="M0.625 5C0.625 6.16032 1.08594 7.27312 1.90641 8.09359C2.72688 8.91406 3.83968 9.375 5 9.375C6.16032 9.375 7.27312 8.91406 8.09359 8.09359C8.91406 7.27312 9.375 6.16032 9.375 5C9.375 3.83968 8.91406 2.72688 8.09359 1.90641C7.27312 1.08594 6.16032 0.625 5 0.625C3.83968 0.625 2.72688 1.08594 1.90641 1.90641C1.08594 2.72688 0.625 3.83968 0.625 5ZM7.64365 7.48027C7.61734 7.50832 7.59054 7.53598 7.56326 7.56326C7.13828 7.98824 6.61864 8.2968 6.0539 8.46842C6.29802 8.11949 6.49498 7.64804 6.63475 7.09483C7.00845 7.18834 7.35014 7.3187 7.64365 7.48027ZM8.10076 6.87776C8.37677 6.42196 8.55005 5.90894 8.60556 5.37499H6.86808C6.85542 5.71597 6.82551 6.04557 6.77971 6.35841C7.25309 6.47355 7.68808 6.6414 8.062 6.85549C8.07497 6.86283 8.08789 6.87025 8.10076 6.87776ZM6.03795 6.22536C6.07708 5.95737 6.1044 5.67232 6.11705 5.37499H3.88295C3.89666 5.69742 3.92764 6.00542 3.9722 6.29287C4.37075 6.21726 4.79213 6.17749 5.224 6.17749C5.50054 6.17749 5.77294 6.19376 6.03795 6.22536ZM4.1261 7.02673C4.34894 7.84835 4.68681 8.375 5 8.375C5.32122 8.375 5.66839 7.82101 5.8908 6.963C5.67389 6.93928 5.45082 6.92699 5.224 6.92699C4.84316 6.92699 4.47332 6.96176 4.1261 7.02673ZM3.39783 7.21853C3.53498 7.71842 3.72038 8.14579 3.9461 8.46842C3.42141 8.30898 2.93566 8.03132 2.52857 7.65192C2.77253 7.48017 3.06711 7.33382 3.39783 7.21853ZM3.23916 6.48077C3.18263 6.13193 3.14625 5.76074 3.13192 5.37499H1.39444C1.4585 5.99112 1.67936 6.57938 2.03393 7.08403C2.3706 6.83531 2.78055 6.63162 3.23916 6.48077ZM1.39444 4.62499H3.13192C3.14615 4.24204 3.18211 3.87344 3.23794 3.52681C2.77814 3.37545 2.36731 3.17096 2.03024 2.92123C1.67783 3.42469 1.45828 4.011 1.39444 4.62499ZM2.5237 2.35262C2.76812 2.52552 3.06373 2.67281 3.39584 2.78875C3.53318 2.28573 3.71928 1.85578 3.9461 1.53158C3.41932 1.69166 2.93178 1.97089 2.5237 2.35262ZM3.97101 3.71489C3.92709 4.00012 3.89654 4.30547 3.88295 4.62499H6.11705C6.10453 4.33057 6.07761 4.04818 6.03909 3.78248C5.77372 3.81417 5.50093 3.83049 5.224 3.83049C4.79169 3.83049 4.3699 3.79065 3.97101 3.71489ZM5.8928 3.04476C5.67527 3.06863 5.45151 3.08099 5.224 3.08099C4.84241 3.08099 4.47186 3.04609 4.12405 2.98086C4.34686 2.1549 4.68584 1.625 5 1.625C5.32218 1.625 5.67048 2.18233 5.8928 3.04476ZM6.78083 3.6493C6.826 3.95984 6.85552 4.28682 6.86808 4.62499H8.60556C8.55029 4.09337 8.37827 3.58251 8.10436 3.1282C8.0903 3.1364 8.07618 3.14449 8.062 3.15249C7.68838 3.36641 7.25378 3.53417 6.78083 3.6493ZM7.64858 2.52499C7.35446 2.68754 7.0117 2.81868 6.63664 2.91268C6.49676 2.35623 6.29913 1.88209 6.0539 1.53158C6.61864 1.7032 7.13828 2.01176 7.56326 2.43674C7.59224 2.46572 7.62068 2.49514 7.64858 2.52499Z" fill="currentColor"></path></svg>

	

	<span>English</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Size:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?size_categories=size_categories%3A100K%3Cn%3C1M"><div class="tag tag-white   ">

	

	<span>100K - 1M</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Tags:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?other=Bash"><div class="tag tag-white   ">

	

	<span>Bash</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Libraries:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Adatasets"><div class="tag tag-white   "><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 95 88"><path fill="#fff" d="M94.25 70.08a8.28 8.28 0 0 1-.43 6.46 10.57 10.57 0 0 1-3 3.6 25.18 25.18 0 0 1-5.7 3.2 65.74 65.74 0 0 1-7.56 2.65 46.67 46.67 0 0 1-11.42 1.68c-5.42.05-10.09-1.23-13.4-4.5a40.4 40.4 0 0 1-10.14.03c-3.34 3.25-7.99 4.52-13.39 4.47a46.82 46.82 0 0 1-11.43-1.68 66.37 66.37 0 0 1-7.55-2.65c-2.28-.98-4.17-2-5.68-3.2a10.5 10.5 0 0 1-3.02-3.6c-.99-2-1.18-4.3-.42-6.46a8.54 8.54 0 0 1-.33-5.63c.25-.95.66-1.83 1.18-2.61a8.67 8.67 0 0 1 2.1-8.47 8.23 8.23 0 0 1 2.82-2.07 41.75 41.75 0 1 1 81.3-.12 8.27 8.27 0 0 1 3.11 2.19 8.7 8.7 0 0 1 2.1 8.47c.52.78.93 1.66 1.18 2.61a8.61 8.61 0 0 1-.32 5.63Z"></path><path fill="#FFD21E" d="M47.21 76.5a34.75 34.75 0 1 0 0-69.5 34.75 34.75 0 0 0 0 69.5Z"></path><path fill="#FF9D0B" d="M81.96 41.75a34.75 34.75 0 1 0-69.5 0 34.75 34.75 0 0 0 69.5 0Zm-73.5 0a38.75 38.75 0 1 1 77.5 0 38.75 38.75 0 0 1-77.5 0Z"></path><path fill="#3A3B45" d="M58.5 32.3c1.28.44 1.78 3.06 3.07 2.38a5 5 0 1 0-6.76-2.07c.61 1.15 2.55-.72 3.7-.32ZM34.95 32.3c-1.28.44-1.79 3.06-3.07 2.38a5 5 0 1 1 6.76-2.07c-.61 1.15-2.56-.72-3.7-.32Z"></path><path fill="#FF323D" d="M46.96 56.29c9.83 0 13-8.76 13-13.26 0-2.34-1.57-1.6-4.09-.36-2.33 1.15-5.46 2.74-8.9 2.74-7.19 0-13-6.88-13-2.38s3.16 13.26 13 13.26Z"></path><path fill="#3A3B45" fill-rule="evenodd" d="M39.43 54a8.7 8.7 0 0 1 5.3-4.49c.4-.12.81.57 1.24 1.28.4.68.82 1.37 1.24 1.37.45 0 .9-.68 1.33-1.35.45-.7.89-1.38 1.32-1.25a8.61 8.61 0 0 1 5 4.17c3.73-2.94 5.1-7.74 5.1-10.7 0-2.34-1.57-1.6-4.09-.36l-.14.07c-2.31 1.15-5.39 2.67-8.77 2.67s-6.45-1.52-8.77-2.67c-2.6-1.29-4.23-2.1-4.23.29 0 3.05 1.46 8.06 5.47 10.97Z" clip-rule="evenodd"></path><path fill="#FF9D0B" d="M70.71 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM24.21 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM17.52 48c-1.62 0-3.06.66-4.07 1.87a5.97 5.97 0 0 0-1.33 3.76 7.1 7.1 0 0 0-1.94-.3c-1.55 0-2.95.59-3.94 1.66a5.8 5.8 0 0 0-.8 7 5.3 5.3 0 0 0-1.79 2.82c-.24.9-.48 2.8.8 4.74a5.22 5.22 0 0 0-.37 5.02c1.02 2.32 3.57 4.14 8.52 6.1 3.07 1.22 5.89 2 5.91 2.01a44.33 44.33 0 0 0 10.93 1.6c5.86 0 10.05-1.8 12.46-5.34 3.88-5.69 3.33-10.9-1.7-15.92-2.77-2.78-4.62-6.87-5-7.77-.78-2.66-2.84-5.62-6.25-5.62a5.7 5.7 0 0 0-4.6 2.46c-1-1.26-1.98-2.25-2.86-2.82A7.4 7.4 0 0 0 17.52 48Zm0 4c.51 0 1.14.22 1.82.65 2.14 1.36 6.25 8.43 7.76 11.18.5.92 1.37 1.31 2.14 1.31 1.55 0 2.75-1.53.15-3.48-3.92-2.93-2.55-7.72-.68-8.01.08-.02.17-.02.24-.02 1.7 0 2.45 2.93 2.45 2.93s2.2 5.52 5.98 9.3c3.77 3.77 3.97 6.8 1.22 10.83-1.88 2.75-5.47 3.58-9.16 3.58-3.81 0-7.73-.9-9.92-1.46-.11-.03-13.45-3.8-11.76-7 .28-.54.75-.76 1.34-.76 2.38 0 6.7 3.54 8.57 3.54.41 0 .7-.17.83-.6.79-2.85-12.06-4.05-10.98-8.17.2-.73.71-1.02 1.44-1.02 3.14 0 10.2 5.53 11.68 5.53.11 0 .2-.03.24-.1.74-1.2.33-2.04-4.9-5.2-5.21-3.16-8.88-5.06-6.8-7.33.24-.26.58-.38 1-.38 3.17 0 10.66 6.82 10.66 6.82s2.02 2.1 3.25 2.1c.28 0 .52-.1.68-.38.86-1.46-8.06-8.22-8.56-11.01-.34-1.9.24-2.85 1.31-2.85Z"></path><path fill="#FFD21E" d="M38.6 76.69c2.75-4.04 2.55-7.07-1.22-10.84-3.78-3.77-5.98-9.3-5.98-9.3s-.82-3.2-2.69-2.9c-1.87.3-3.24 5.08.68 8.01 3.91 2.93-.78 4.92-2.29 2.17-1.5-2.75-5.62-9.82-7.76-11.18-2.13-1.35-3.63-.6-3.13 2.2.5 2.79 9.43 9.55 8.56 11-.87 1.47-3.93-1.71-3.93-1.71s-9.57-8.71-11.66-6.44c-2.08 2.27 1.59 4.17 6.8 7.33 5.23 3.16 5.64 4 4.9 5.2-.75 1.2-12.28-8.53-13.36-4.4-1.08 4.11 11.77 5.3 10.98 8.15-.8 2.85-9.06-5.38-10.74-2.18-1.7 3.21 11.65 6.98 11.76 7.01 4.3 1.12 15.25 3.49 19.08-2.12Z"></path><path fill="#FF9D0B" d="M77.4 48c1.62 0 3.07.66 4.07 1.87a5.97 5.97 0 0 1 1.33 3.76 7.1 7.1 0 0 1 1.95-.3c1.55 0 2.95.59 3.94 1.66a5.8 5.8 0 0 1 .8 7 5.3 5.3 0 0 1 1.78 2.82c.24.9.48 2.8-.8 4.74a5.22 5.22 0 0 1 .37 5.02c-1.02 2.32-3.57 4.14-8.51 6.1-3.08 1.22-5.9 2-5.92 2.01a44.33 44.33 0 0 1-10.93 1.6c-5.86 0-10.05-1.8-12.46-5.34-3.88-5.69-3.33-10.9 1.7-15.92 2.78-2.78 4.63-6.87 5.01-7.77.78-2.66 2.83-5.62 6.24-5.62a5.7 5.7 0 0 1 4.6 2.46c1-1.26 1.98-2.25 2.87-2.82A7.4 7.4 0 0 1 77.4 48Zm0 4c-.51 0-1.13.22-1.82.65-2.13 1.36-6.25 8.43-7.76 11.18a2.43 2.43 0 0 1-2.14 1.31c-1.54 0-2.75-1.53-.14-3.48 3.91-2.93 2.54-7.72.67-8.01a1.54 1.54 0 0 0-.24-.02c-1.7 0-2.45 2.93-2.45 2.93s-2.2 5.52-5.97 9.3c-3.78 3.77-3.98 6.8-1.22 10.83 1.87 2.75 5.47 3.58 9.15 3.58 3.82 0 7.73-.9 9.93-1.46.1-.03 13.45-3.8 11.76-7-.29-.54-.75-.76-1.34-.76-2.38 0-6.71 3.54-8.57 3.54-.42 0-.71-.17-.83-.6-.8-2.85 12.05-4.05 10.97-8.17-.19-.73-.7-1.02-1.44-1.02-3.14 0-10.2 5.53-11.68 5.53-.1 0-.19-.03-.23-.1-.74-1.2-.34-2.04 4.88-5.2 5.23-3.16 8.9-5.06 6.8-7.33-.23-.26-.57-.38-.98-.38-3.18 0-10.67 6.82-10.67 6.82s-2.02 2.1-3.24 2.1a.74.74 0 0 1-.68-.38c-.87-1.46 8.05-8.22 8.55-11.01.34-1.9-.24-2.85-1.31-2.85Z"></path><path fill="#FFD21E" d="M56.33 76.69c-2.75-4.04-2.56-7.07 1.22-10.84 3.77-3.77 5.97-9.3 5.97-9.3s.82-3.2 2.7-2.9c1.86.3 3.23 5.08-.68 8.01-3.92 2.93.78 4.92 2.28 2.17 1.51-2.75 5.63-9.82 7.76-11.18 2.13-1.35 3.64-.6 3.13 2.2-.5 2.79-9.42 9.55-8.55 11 .86 1.47 3.92-1.71 3.92-1.71s9.58-8.71 11.66-6.44c2.08 2.27-1.58 4.17-6.8 7.33-5.23 3.16-5.63 4-4.9 5.2.75 1.2 12.28-8.53 13.36-4.4 1.08 4.11-11.76 5.3-10.97 8.15.8 2.85 9.05-5.38 10.74-2.18 1.69 3.21-11.65 6.98-11.76 7.01-4.31 1.12-15.26 3.49-19.08-2.12Z"></path></svg>

	

	<span>Datasets</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Adask"><div class="tag tag-white   "><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path d="m3.368 3.694 2.965-1.71A.095.095 0 0 0 6.38 1.9V.875a.49.49 0 0 0-.183-.394.468.468 0 0 0-.523-.034L1.526 2.84a.471.471 0 0 0-.235.408l-.002 5.405c0 .151.062.302.183.394.157.118.358.13.524.035l.878-.507a.095.095 0 0 0 .048-.082V4.465a.89.89 0 0 1 .446-.771Z" fill="#FFC11E"></path><path d="M10.475 2.919a.47.47 0 0 0-.47 0L5.856 5.312a.473.473 0 0 0-.236.408l-.002 5.425c0 .17.088.323.236.408a.466.466 0 0 0 .471 0l4.147-2.393a.473.473 0 0 0 .236-.408l.002-5.425a.467.467 0 0 0-.236-.408Z" fill="#EF1161"></path><path d="m5.647 4.949 2.737-1.58a.095.095 0 0 0 .047-.082V2.093a.49.49 0 0 0-.183-.394.468.468 0 0 0-.523-.035l-1.135.655-3.013 1.738a.471.471 0 0 0-.236.408v4.083L3.34 9.87c0 .152.062.302.183.394.157.118.358.13.524.035l1.106-.639a.094.094 0 0 0 .047-.082l.001-3.859a.89.89 0 0 1 .446-.77Z" fill="#FC6E6B"></path></svg>

	

	<span>Dask</span>
	

	</div></a><div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-lg rounded-br-none " type="button">
		<div class="tag tag-white   relative rounded-br-none pr-2.5"><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="none" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="#F5AB6A"></path><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="url(#paint0_radial_18_31665)"></path><g filter="url(#filter0_f_18_31665)"><path d="M22.2849 12.1817L23.4375 13.2656L24.4375 4.70312C23.5121 4.1242 23.0198 3.96369 22.6563 3.89062L22.2849 12.1817Z" fill="url(#paint1_linear_18_31665)"></path></g><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint2_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint3_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint4_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint5_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint6_linear_18_31665)"></path><g filter="url(#filter1_f_18_31665)"><path d="M13.1016 2.72656C11.862 3.06924 11.5298 3.40016 11.5298 3.40016C10.9102 3.52335 10.936 4.11525 11.2408 4.6799C13.1487 6.95202 15.1361 13.2496 18.007 14.0958C18.2707 14.1633 18.6953 14.2107 19.1797 14.2344L13.1016 2.72656Z" fill="url(#paint7_linear_18_31665)"></path></g><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint8_radial_18_31665)"></path><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint9_radial_18_31665)"></path><g filter="url(#filter2_f_18_31665)"><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint10_linear_18_31665)"></path><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint11_radial_18_31665)"></path></g><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="#EC9F6A"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint12_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint13_linear_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint14_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint15_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint16_radial_18_31665)"></path><g filter="url(#filter3_f_18_31665)"><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint17_linear_18_31665)"></path><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint18_linear_18_31665)"></path></g><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="#D79453"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint19_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint20_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint21_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint22_linear_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint23_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint24_radial_18_31665)"></path><defs><filter id="filter0_f_18_31665" x="22.0349" y="3.64062" width="2.65265" height="9.875" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter1_f_18_31665" x="10.7815" y="2.47656" width="8.64819" height="12.0078" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter2_f_18_31665" x="3.89062" y="22.5416" width="9.57812" height="2.2804" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter3_f_18_31665" x="2.45312" y="11.2538" width="12.1255" height="8.50903" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><radialGradient id="paint0_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.8125 12.9375) rotate(42.7741) scale(12.5164 7.08839)"><stop offset="0.0937591" stop-color="#C05159"></stop><stop offset="0.553697" stop-color="#F6AC6A"></stop><stop offset="0.832916" stop-color="#FFD186"></stop><stop offset="0.916927" stop-color="#FFDC87"></stop></radialGradient><linearGradient id="paint1_linear_18_31665" x1="24.7344" y1="4.67187" x2="20.8594" y2="12.8906" gradientUnits="userSpaceOnUse"><stop stop-color="#EBD67C"></stop><stop offset="0.0655686" stop-color="#FFFFA6"></stop><stop offset="0.530552" stop-color="#F8C281"></stop><stop offset="0.937338" stop-color="#E99E6B"></stop></linearGradient><radialGradient id="paint2_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.9009 13.1847) rotate(-127.648) scale(14.3438 11.7966)"><stop stop-color="#FFBE66"></stop><stop offset="1" stop-color="#E2AE5B"></stop></radialGradient><radialGradient id="paint3_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(18 11.4375) rotate(53.9726) scale(11.9013 4.84018)"><stop stop-color="#D67C63"></stop><stop offset="1" stop-color="#D97D67" stop-opacity="0"></stop></radialGradient><radialGradient id="paint4_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(23 4.1875) rotate(45.7639) scale(3.31486 5.75622)"><stop stop-color="#FFE4A6"></stop><stop offset="0.711285" stop-color="#F8B76F"></stop><stop offset="1" stop-color="#F9B870" stop-opacity="0"></stop></radialGradient><radialGradient id="paint5_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.875 12.4375) rotate(88.9391) scale(3.37558 1.29066)"><stop stop-color="#FFBC67"></stop><stop offset="1" stop-color="#FFBC67" stop-opacity="0"></stop></radialGradient><linearGradient id="paint6_linear_18_31665" x1="20.375" y1="15.6875" x2="20.125" y2="12.7813" gradientUnits="userSpaceOnUse"><stop offset="0.461609" stop-color="#B45077"></stop><stop offset="0.855389" stop-color="#B75077" stop-opacity="0"></stop></linearGradient><linearGradient id="paint7_linear_18_31665" x1="12.9375" y1="2.57056" x2="18.5625" y2="14.3891" gradientUnits="userSpaceOnUse"><stop stop-color="#DDC173"></stop><stop offset="0.485173" stop-color="#D59F65"></stop><stop offset="1" stop-color="#E49966"></stop></linearGradient><radialGradient id="paint8_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.5625 23.5) rotate(109.113) scale(6.68078 10.2578)"><stop offset="0.165756" stop-color="#FFBF7E"></stop><stop offset="0.827674" stop-color="#DF8C6D"></stop><stop offset="1" stop-color="#B05A66"></stop></radialGradient><radialGradient id="paint9_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.1875 26) rotate(41.0652) scale(8.37243 2.03649)"><stop stop-color="#FFD483"></stop><stop offset="1" stop-color="#FFD688" stop-opacity="0"></stop></radialGradient><linearGradient id="paint10_linear_18_31665" x1="3.96063" y1="23.794" x2="13.3748" y2="23.5143" gradientUnits="userSpaceOnUse"><stop stop-color="#A8716F"></stop><stop offset="0.103615" stop-color="#B37173"></stop><stop offset="0.225484" stop-color="#DB9F84"></stop><stop offset="0.799889" stop-color="#F1BB8A"></stop><stop offset="1" stop-color="#FFD780"></stop></linearGradient><radialGradient id="paint11_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4219 23.1719) rotate(-178.616) scale(3.23532 0.569081)"><stop offset="0.621498" stop-color="#AF5A3E"></stop><stop offset="1" stop-color="#B35445" stop-opacity="0"></stop></radialGradient><radialGradient id="paint12_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.625 19.125) rotate(-171.737) scale(15.2205 15.0767)"><stop offset="0.138435" stop-color="#FFB974"></stop><stop offset="0.403618" stop-color="#F2A56D"></stop><stop offset="0.925938" stop-color="#A16948"></stop></radialGradient><linearGradient id="paint13_linear_18_31665" x1="8.22184" y1="13.125" x2="6.81191" y2="15.4996" gradientUnits="userSpaceOnUse"><stop offset="0.610751" stop-color="#984847"></stop><stop offset="0.850075" stop-color="#9A4947" stop-opacity="0"></stop></linearGradient><radialGradient id="paint14_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.25 23.7461) scale(11.25 5.68361)"><stop stop-color="#C66364"></stop><stop offset="1" stop-color="#D4766B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint15_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.875 23.5313) scale(10.0937 1.29657)"><stop stop-color="#B64B4B"></stop><stop offset="1" stop-color="#C56158" stop-opacity="0"></stop></radialGradient><radialGradient id="paint16_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4375 19.875) rotate(-46.8882) scale(4.02385 7.51767)"><stop stop-color="#FFC083"></stop><stop offset="0.620218" stop-color="#FFBD7D" stop-opacity="0"></stop></radialGradient><linearGradient id="paint17_linear_18_31665" x1="2.8125" y1="13.0312" x2="14.5582" y2="18.9404" gradientUnits="userSpaceOnUse"><stop stop-color="#B89367"></stop><stop offset="1" stop-color="#C5835E"></stop></linearGradient><linearGradient id="paint18_linear_18_31665" x1="8.21875" y1="14.6406" x2="7.59349" y2="15.6717" gradientUnits="userSpaceOnUse"><stop offset="0.351552" stop-color="#A74746"></stop><stop offset="0.845198" stop-color="#A04346" stop-opacity="0"></stop></linearGradient><radialGradient id="paint19_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.5625 14.5625) rotate(140.244) scale(18.3733 13.7403)"><stop stop-color="#FDAE69"></stop><stop offset="0.729021" stop-color="#CE8C4F"></stop><stop offset="0.921546" stop-color="#AD7B45"></stop><stop offset="1" stop-color="#8B6B4A"></stop></radialGradient><radialGradient id="paint20_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.0625 7) rotate(65.3152) scale(11.0745 3.16547)"><stop offset="0.233237" stop-color="#FFD47C"></stop><stop offset="0.853648" stop-color="#FFD98B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint21_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.3125 8.875) rotate(100.886) scale(6.6191 5.57808)"><stop offset="0.128419" stop-color="#FFD88C"></stop><stop offset="0.924134" stop-color="#FFBE7B" stop-opacity="0"></stop></radialGradient><linearGradient id="paint22_linear_18_31665" x1="7.25" y1="15.1875" x2="10.7588" y2="10.3142" gradientUnits="userSpaceOnUse"><stop offset="0.142353" stop-color="#C15F4D"></stop><stop offset="1" stop-color="#D58366" stop-opacity="0"></stop></linearGradient><radialGradient id="paint23_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(8.15625 15.7813) rotate(28.5422) scale(12.5574 1.96589)"><stop offset="0.149989" stop-color="#E4745D"></stop><stop offset="0.453292" stop-color="#C8604C"></stop><stop offset="0.632597" stop-color="#C0605F"></stop><stop offset="1" stop-color="#C0605F" stop-opacity="0"></stop></radialGradient><radialGradient id="paint24_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(1.40625 2.69067) rotate(46.0943) scale(22.3963)"><stop offset="0.935802" stop-color="#C17C61" stop-opacity="0"></stop><stop offset="0.982109" stop-color="#C17C61"></stop></radialGradient></defs></svg>

	

	<span>Croissant</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	<button class="tag tag-ghost px-1! -ml-0.5 mb-1 md:mb-1.5" type="button">+ 1</button></div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">License:
	</span>
	<div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-full rounded-br-none " type="button">
		<div class="tag tag-white rounded-full  relative rounded-br-none pr-2.5">
		<svg class="text-xs text-gray-900" width="1em" height="1em" viewBox="0 0 10 10" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.46009 5.0945V6.88125C1.46009 7.25201 1.75937 7.55129 2.13012 7.55129C2.50087 7.55129 2.80016 7.25201 2.80016 6.88125V5.0945C2.80016 4.72375 2.50087 4.42446 2.13012 4.42446C1.75937 4.42446 1.46009 4.72375 1.46009 5.0945ZM4.14022 5.0945V6.88125C4.14022 7.25201 4.4395 7.55129 4.81026 7.55129C5.18101 7.55129 5.48029 7.25201 5.48029 6.88125V5.0945C5.48029 4.72375 5.18101 4.42446 4.81026 4.42446C4.4395 4.42446 4.14022 4.72375 4.14022 5.0945ZM1.23674 9.78473H8.38377C8.75452 9.78473 9.0538 9.48545 9.0538 9.1147C9.0538 8.74395 8.75452 8.44466 8.38377 8.44466H1.23674C0.865993 8.44466 0.566711 8.74395 0.566711 9.1147C0.566711 9.48545 0.865993 9.78473 1.23674 9.78473ZM6.82036 5.0945V6.88125C6.82036 7.25201 7.11964 7.55129 7.49039 7.55129C7.86114 7.55129 8.16042 7.25201 8.16042 6.88125V5.0945C8.16042 4.72375 7.86114 4.42446 7.49039 4.42446C7.11964 4.42446 6.82036 4.72375 6.82036 5.0945ZM4.39484 0.623142L0.865993 2.48137C0.682851 2.57517 0.566711 2.76725 0.566711 2.97273C0.566711 3.28094 0.816857 3.53109 1.12507 3.53109H8.49991C8.80365 3.53109 9.0538 3.28094 9.0538 2.97273C9.0538 2.76725 8.93766 2.57517 8.75452 2.48137L5.22568 0.623142C4.9666 0.484669 4.65391 0.484669 4.39484 0.623142V0.623142Z" fill="currentColor"></path></svg>

	

	<span>apache-2.0</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	</div></div>

		<div class="flex flex-col-reverse lg:flex-row lg:items-center lg:justify-between"><div class="-mb-px flex h-12 items-center overflow-x-auto overflow-y-hidden ">
	<a class="tab-alternate active" href="/datasets/GunA-SD/bash_code"><svg class="mr-1.5 text-gray-400 flex-none" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
	Dataset card
	

	
		</a><a class="tab-alternate" href="/datasets/GunA-SD/bash_code/viewer/"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	Data Studio
	

	
		</a><a class="tab-alternate" href="/datasets/GunA-SD/bash_code/tree/main"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M21 19h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-8h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0 4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M9 19a1 1 0 0 1-1-1V6a1 1 0 0 1 2 0v12a1 1 0 0 1-1 1zm-6-4.333a1 1 0 0 1-.64-1.769L3.438 12l-1.078-.898a1 1 0 0 1 1.28-1.538l2 1.667a1 1 0 0 1 0 1.538l-2 1.667a.999.999 0 0 1-.64.231z" fill="currentColor"></path></svg>
	<span class="xl:hidden">Files</span>
		<span class="hidden xl:inline">Files and versions</span>
	

	
		</a><a class="tab-alternate" href="/datasets/GunA-SD/bash_code/discussions"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M20.6081 3C21.7684 3 22.8053 3.49196 23.5284 4.38415C23.9756 4.93678 24.4428 5.82749 24.4808 7.16133C24.9674 7.01707 25.4353 6.93643 25.8725 6.93643C26.9833 6.93643 27.9865 7.37587 28.696 8.17411C29.6075 9.19872 30.0124 10.4579 29.8361 11.7177C29.7523 12.3177 29.5581 12.8555 29.2678 13.3534C29.8798 13.8646 30.3306 14.5763 30.5485 15.4322C30.719 16.1032 30.8939 17.5006 29.9808 18.9403C30.0389 19.0342 30.0934 19.1319 30.1442 19.2318C30.6932 20.3074 30.7283 21.5229 30.2439 22.6548C29.5093 24.3704 27.6841 25.7219 24.1397 27.1727C21.9347 28.0753 19.9174 28.6523 19.8994 28.6575C16.9842 29.4379 14.3477 29.8345 12.0653 29.8345C7.87017 29.8345 4.8668 28.508 3.13831 25.8921C0.356375 21.6797 0.754104 17.8269 4.35369 14.1131C6.34591 12.058 7.67023 9.02782 7.94613 8.36275C8.50224 6.39343 9.97271 4.20438 12.4172 4.20438H12.4179C12.6236 4.20438 12.8314 4.2214 13.0364 4.25468C14.107 4.42854 15.0428 5.06476 15.7115 6.02205C16.4331 5.09583 17.134 4.359 17.7682 3.94323C18.7242 3.31737 19.6794 3 20.6081 3ZM20.6081 5.95917C20.2427 5.95917 19.7963 6.1197 19.3039 6.44225C17.7754 7.44319 14.8258 12.6772 13.7458 14.7131C13.3839 15.3952 12.7655 15.6837 12.2086 15.6837C11.1036 15.6837 10.2408 14.5497 12.1076 13.1085C14.9146 10.9402 13.9299 7.39584 12.5898 7.1776C12.5311 7.16799 12.4731 7.16355 12.4172 7.16355C11.1989 7.16355 10.6615 9.33114 10.6615 9.33114C10.6615 9.33114 9.0863 13.4148 6.38031 16.206C3.67434 18.998 3.5346 21.2388 5.50675 24.2246C6.85185 26.2606 9.42666 26.8753 12.0653 26.8753C14.8021 26.8753 17.6077 26.2139 19.1799 25.793C19.2574 25.7723 28.8193 22.984 27.6081 20.6107C27.4046 20.212 27.0693 20.0522 26.6471 20.0522C24.9416 20.0522 21.8393 22.6726 20.5057 22.6726C20.2076 22.6726 19.9976 22.5416 19.9116 22.222C19.3433 20.1173 28.552 19.2325 27.7758 16.1839C27.639 15.6445 27.2677 15.4256 26.746 15.4263C24.4923 15.4263 19.4358 19.5181 18.3759 19.5181C18.2949 19.5181 18.2368 19.4937 18.2053 19.4419C17.6743 18.557 17.9653 17.9394 21.7082 15.6009C25.4511 13.2617 28.0783 11.8545 26.5841 10.1752C26.4121 9.98141 26.1684 9.8956 25.8725 9.8956C23.6001 9.89634 18.2311 14.9403 18.2311 14.9403C18.2311 14.9403 16.7821 16.496 15.9057 16.496C15.7043 16.496 15.533 16.4139 15.4169 16.2112C14.7956 15.1296 21.1879 10.1286 21.5484 8.06535C21.7928 6.66715 21.3771 5.95917 20.6081 5.95917Z" fill="#FF9D00"></path><path d="M5.50686 24.2246C3.53472 21.2387 3.67446 18.9979 6.38043 16.206C9.08641 13.4147 10.6615 9.33111 10.6615 9.33111C10.6615 9.33111 11.2499 6.95933 12.59 7.17757C13.93 7.39581 14.9139 10.9401 12.1069 13.1084C9.29997 15.276 12.6659 16.7489 13.7459 14.713C14.8258 12.6772 17.7747 7.44316 19.304 6.44221C20.8326 5.44128 21.9089 6.00204 21.5484 8.06532C21.188 10.1286 14.795 15.1295 15.4171 16.2118C16.0391 17.2934 18.2312 14.9402 18.2312 14.9402C18.2312 14.9402 25.0907 8.49588 26.5842 10.1752C28.0776 11.8545 25.4512 13.2616 21.7082 15.6008C17.9646 17.9393 17.6744 18.557 18.2054 19.4418C18.7372 20.3266 26.9998 13.1351 27.7759 16.1838C28.5513 19.2324 19.3434 20.1173 19.9117 22.2219C20.48 24.3274 26.3979 18.2382 27.6082 20.6107C28.8193 22.9839 19.2574 25.7722 19.18 25.7929C16.0914 26.62 8.24723 28.3726 5.50686 24.2246Z" fill="#FFD21E"></path></svg>
	Community
	<div class="ml-1.5 flex h-4 min-w-[1rem] items-center justify-center rounded px-1 text-xs leading-none shadow-sm bg-black text-white dark:bg-gray-800 dark:text-gray-200">1</div>

	
		</a></div>
	
			</div></div></header>
</div>
	
<div class="container relative flex flex-col md:grid md:space-y-0 w-full md:grid-cols-12 md:flex-1 md:grid-rows-full space-y-4 md:gap-6 ">
		<section class="pt-6 border-gray-100 md:col-span-8 pb-24 relative break-words copiable-code-container">
				<div class="SVELTE_HYDRATER contents" data-target="UnsafeBanner" data-props="{&quot;classNames&quot;:&quot;mb-4&quot;,&quot;repoId&quot;:&quot;GunA-SD/bash_code&quot;,&quot;repoType&quot;:&quot;dataset&quot;,&quot;minLevel&quot;:&quot;unsafe&quot;}"></div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetViewer" data-props="{&quot;data&quot;:{&quot;kind&quot;:&quot;DatasetAndSampleData&quot;,&quot;datasetInfo&quot;:[{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Size of downloaded dataset files:&quot;,&quot;value&quot;:&quot;348 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;/datasets/GunA-SD/bash_code/tree/refs%2Fconvert%2Fparquet/&quot;,&quot;label&quot;:&quot;Size of the auto-converted Parquet files:&quot;,&quot;value&quot;:&quot;348 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Number of rows:&quot;,&quot;value&quot;:&quot;342,720&quot;}],&quot;partial&quot;:false,&quot;configsData&quot;:{&quot;configInfos&quot;:[{&quot;name&quot;:&quot;default&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:342720}],&quot;selectedConfig&quot;:&quot;default&quot;,&quot;hasSelectedConfigParquet&quot;:true},&quot;splitsData&quot;:{&quot;splitInfos&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;numRows&quot;:342720}],&quot;selectedSplit&quot;:&quot;train&quot;},&quot;sampleData&quot;:{&quot;dataset&quot;:&quot;GunA-SD/bash_code&quot;,&quot;config&quot;:&quot;default&quot;,&quot;split&quot;:&quot;train&quot;,&quot;capabilities&quot;:{&quot;rows&quot;:true,&quot;search&quot;:true,&quot;filter&quot;:true,&quot;statistics&quot;:true},&quot;navigation&quot;:{&quot;p&quot;:0},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzEyOSwic3ViIjoiL2RhdGFzZXRzL0d1bkEtU0QvYmFzaF9jb2RlIiwiZXhwIjoxNzQyOTI2NzI5LCJpc3MiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvIn0.WyEBYPrjjocUDYOJFtB4BQD3yPs_elkAzEY9qHBwcDaXeDFQPKCctZneKzCC1mWs7e3i3vEX60vWbC91ezESDQ&quot;,&quot;sampleData&quot;:{&quot;columns&quot;:[{&quot;name&quot;:&quot;content&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;content&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:3,&quot;nan_proportion&quot;:0.00001,&quot;min&quot;:1,&quot;max&quot;:1016182,&quot;mean&quot;:2445.19926,&quot;median&quot;:751,&quot;std&quot;:11132.68154,&quot;histogram&quot;:{&quot;hist&quot;:[342280,253,109,41,12,5,4,4,4,5],&quot;bin_edges&quot;:[1,101620,203239,304858,406477,508096,609715,711334,812953,914572,1016182]}}}}],&quot;rows&quot;:[{&quot;rowIdx&quot;:0,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\n# Running skydns based on instructions at: https://testdatamanagement.wordpress.com/2015/09/01/running-kubernetes-in-docker-with-dns-on-a-single-node/\n\nPWD=`pwd`\nBASEDIR=`readlink -e $(dirname ${0})`\ncd ${BASEDIR}\n\nKUBECTL='docker exec hyperkube /hyperkube kubectl'\n\n#RUN_SKYDNS=\&quot;yes\&quot;\nRUN_SKYDNS=\&quot;no\&quot;\n\n# DNS_ARGUMENTS needs to be passed when Kubernetes is setup.\nif [ \&quot;${RUN_SKYDNS}\&quot; = \&quot;yes\&quot; ]; then\n\tDNS_ARGUMENTS=\&quot;--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\&quot;\nelse\n\tDNS_ARGUMENTS=\&quot;\&quot;\nfi\n\nwait_until_k8s_ready() {\n\t# Wait until kubernetes is up and fully responsive\n\twhile :\n\tdo\n\t\t${KUBECTL} get nodes 2>/dev/null | grep -q '127.0.0.1'\n\t\tif [ \&quot;${?}\&quot; = \&quot;0\&quot; ]; then\n\t\t\tbreak\n\t\telse\n\t\t\techo \&quot;sleeping for 5 seconds (waiting for kubernetes to start)\&quot;\n\t\t\tsleep 5\n\t\tfi\n\tdone\n\techo \&quot;kubernetes nodes:\&quot;\n\t${KUBECTL} get nodes\n}\n\n\nif [ \&quot;${RUN_SKYDNS}\&quot; = \&quot;yes\&quot; ]; then\n\twait_until_k8s_ready\n\n\techo \&quot;Launch kube2sky...\&quot;\n\tdocker run -d --net=host gcr.io/google_containers/kube2sky:1.11 --kube_master_url=http://127.0.0.1:8080 --domain=cluster.local\n\n\techo \&quot;\&quot;\n\n\techo \&quot;Launch SkyDNS...\&quot;\n\tdocker run -d --net=host gcr.io/google_containers/skydns:2015-03-11-001 --machines=http://localhost:4001 --addr=0.0.0.0:53 --domain=cluster.local\nelse\n\ttrue\nfi\n\ncd ${PWD}\n&quot;}}},{&quot;rowIdx&quot;:1,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n# cp -i /etc/kubernetes/admin.conf /root/kube-admin.conf\nkubectl --kubeconfig /root/kube-admin.conf $*\n&quot;}}},{&quot;rowIdx&quot;:2,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\nset -e\n\nif [ \&quot;$1\&quot; = \&quot;/opt/logstash/bin/logstash\&quot; ]; then\n    exec \&quot;$1\&quot; agent -f /opt/conf/logstash.conf\nelse\n    exec \&quot;$@\&quot;\nfi&quot;}}},{&quot;rowIdx&quot;:3,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#---------#\n# Seqdiag #\n#---------#\n\n# Watches for files named *.diag in the given directory (recursive) and generates the\n# corresponding PNG file.\n# $1: the folder to watch (Default: pwd)\n# shellcheck disable=SC2034\nseqWatch() {\n  local folder=\&quot;$1\&quot;\n\n  [[ -n \&quot;$folder\&quot; ]] || {\n    printfc 'Folder not defined, it was set to pwd\\n' \&quot;$YELLOW\&quot;\n    folder=\&quot;$(pwd)\&quot;;\n  }\n\n  inotifywait -rm \&quot;$folder\&quot; -e close_write |\n  while read path action file; do\n    if [[ \&quot;$file\&quot; =~ .*\\.diag$ ]]; then\n      seqdiag \&quot;$path$file\&quot; --no-transparency -a\n    fi\n  done\n}\n\n# Inits a seqdiag file with the preferences defined in seqdiag.init.\n# Uses: $TOOLING\n# $1: the file to be created (absolute path)\nseqInit() {\n  local filePath=\&quot;${1?Missing path to file}\&quot;\n\n  mkdir -p \&quot;$(dirname \&quot;$filePath\&quot;)\&quot;\n  cp \&quot;$TOOLING/bashrc/Utils/seqdiag.init\&quot; \&quot;$(basename \&quot;$filePath\&quot;)\&quot;\n}\n&quot;}}},{&quot;rowIdx&quot;:4,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n# Author: Eason Yi\n# Date: 2017-05-17\n\npbpaste|awk '!/^[ ]*$/'|pbcopy|pbpaste\n&quot;}}},{&quot;rowIdx&quot;:5,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/usr/bin/env bash\n\nPWD_DIR=$(pwd)\nfunction cleanup {\n    cd \&quot;$PWD_DIR\&quot;\n}\ntrap cleanup EXIT\n\nGREP=grep\nSED=sed\nAWK=awk\nMAKE=make\n\n# Fixup ancient Bash\n# https://unix.stackexchange.com/q/468579/56041\nif [[ -z \&quot;$BASH_SOURCE\&quot; ]]; then\n\tBASH_SOURCE=\&quot;$0\&quot;\nfi\n\n# Fixup, Solaris and friends\nif [[ (-d /usr/xpg4/bin) ]]; then\n\tSED=/usr/xpg4/bin/sed\n\tAWK=/usr/xpg4/bin/awk\n\tGREP=/usr/xpg4/bin/grep\nelif [[ (-d /usr/bin/posix) ]]; then\n\tSED=/usr/bin/posix/sed\n\tAWK=/usr/bin/posix/awk\n\tGREP=/usr/bin/posix/grep\nfi\n\n# Fixup for sed and \&quot;illegal byte sequence\&quot;\nIS_DARWIN=$(uname -s | \&quot;$GREP\&quot; -i -c darwin)\nif [[ \&quot;$IS_DARWIN\&quot; -ne 0 ]]; then\n\texport LC_ALL=C\nfi\n\n# Fixup for Solaris and BSDs\n# Fixup for Solaris and BSDs\nif [[ ! -z $(command -v gmake) ]]; then\n\tMAKE=gmake\nelse\n\tMAKE=make\nfi\n\n# Fixup for missing libtool\nif [[ ! -z $(command -v libtoolize) ]]; then\n\tLIBTOOLIZE=$(command -v libtoolize)\nelif [[ ! -z $(command -v glibtoolize) ]]; then\n\tLIBTOOLIZE=$(command -v glibtoolize)\nelif [[ ! -z $(command -v libtool) ]]; then\n\tLIBTOOLIZE=$(command -v libtool)\nelif [[ ! -z $(command -v glibtool) ]]; then\n\tLIBTOOLIZE=$(command -v glibtool)\nfi\n\n# Fecth the three required files\nif ! wget --no-check-certificate 'https://raw.githubusercontent.com/noloader/cryptopp-autotools/master/Makefile.am' -O Makefile.am; then\n\techo \&quot;Makefile.am download failed\&quot;\n\t[[ \&quot;$0\&quot; = \&quot;${BASH_SOURCE[0]}\&quot; ]] &amp;&amp; exit 1 || return 1\nfi\n\nif ! wget --no-check-certificate 'https://raw.githubusercontent.com/noloader/cryptopp-autotools/master/configure.ac' -O configure.ac; then\n\techo \&quot;configure.ac download failed\&quot;\n\t[[ \&quot;$0\&quot; = \&quot;${BASH_SOURCE[0]}\&quot; ]] &amp;&amp; exit 1 || return 1\nfi\n\nif ! wget --no-check-certificate 'https://raw.githubusercontent.com/noloader/cryptopp-autotools/master/libcryptopp.pc.in' -O libcryptopp.pc.in; then\n\techo \&quot;libcryptopp.pc.in download failed\&quot;\n\t[[ \&quot;$0\&quot; = \&quot;${BASH_SOURCE[0]}\&quot; ]] &amp;&amp; exit 1 || return 1\nfi\n\nmkdir -p m4/\n\nif [[ -z $(command -v autoupdate) ]]; then\n\techo \&quot;Cannot find autoupdate. Things may fail.\&quot;\nfi\n\nif [[ -z \&quot;$LIBTOOLIZE\&quot; ]]; then\n\techo \&quot;Cannot find libtoolize. Things may fail.\&quot;\nfi\n\nif [[ -z $(command -v autoreconf) ]]; then\n\techo \&quot;Cannot find autoreconf. Things may fail.\&quot;\nfi\n\necho \&quot;Running autoupdate\&quot;\nif ! autoupdate 2>/dev/null; then\n\techo \&quot;autoupdate failed.\&quot;\n\t[[ \&quot;$0\&quot; = \&quot;${BASH_SOURCE[0]}\&quot; ]] &amp;&amp; exit 1 || return 1\nfi\n\necho \&quot;Running libtoolize\&quot;\nif ! \&quot;$LIBTOOLIZE\&quot; 2>/dev/null; then\n\techo \&quot;libtoolize failed.\&quot;\n\t[[ \&quot;$0\&quot; = \&quot;${BASH_SOURCE[0]}\&quot; ]] &amp;&amp; exit 1 || return 1\nfi\n\n# Run autoreconf twice on failure. Also see\n# https://github.com/tracebox/tracebox/issues/57\necho \&quot;Running autoreconf\&quot;\nif ! autoreconf 2>/dev/null; then\n\techo \&quot;autoreconf failed, running again.\&quot;\n\tif ! autoreconf -fi; then\n\t\techo \&quot;autoreconf failed, again.\&quot;\n\t\t[[ \&quot;$0\&quot; = \&quot;${BASH_SOURCE[0]}\&quot; ]] &amp;&amp; exit 1 || return 1\n\tfi\nfi\n\n# Sparc need +w\nif [[ -e config.sub ]]; then\n\tchmod +w config.sub\nfi\nif [[ -e config.guess ]]; then\n\tchmod +w config.guess\nfi\n\n# Update config.sub config.guess. GNU recommends using the latest for all projects.\necho \&quot;Updating config.sub\&quot;\nwget --no-check-certificate 'https://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.sub' -O config.sub\n\nif [[ -e config.sub ]]; then\n\tchmod +x config.sub\nfi\n\necho \&quot;Updating config.guess\&quot;\nwget --no-check-certificate 'https://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.guess' -O config.guess\n\nif [[ -e config.guess ]]; then\n\tchmod +x config.guess\nfi\n\nif ! ./configure; then\n\techo \&quot;configure failed.\&quot;\n\t[[ \&quot;$0\&quot; = \&quot;${BASH_SOURCE[0]}\&quot; ]] &amp;&amp; exit 1 || return 1\nfi\n\n\&quot;$MAKE\&quot; clean 2>/dev/null\n\nif ! \&quot;$MAKE\&quot; -j2 -f Makefile; then\n\techo \&quot;make failed.\&quot;\n\t[[ \&quot;$0\&quot; = \&quot;${BASH_SOURCE[0]}\&quot; ]] &amp;&amp; exit 1 || return 1\nfi\n\nif ! ./cryptest v; then\n\techo \&quot;cryptest v failed.\&quot;\n\t[[ \&quot;$0\&quot; = \&quot;${BASH_SOURCE[0]}\&quot; ]] &amp;&amp; exit 1 || return 1\nfi\n\nif ! ./cryptest tv all; then\n\techo \&quot;cryptest tv all failed.\&quot;\n\t[[ \&quot;$0\&quot; = \&quot;${BASH_SOURCE[0]}\&quot; ]] &amp;&amp; exit 1 || return 1\nfi\n\n# Return success\n[[ \&quot;$0\&quot; = \&quot;${BASH_SOURCE[0]}\&quot; ]] &amp;&amp; exit 0 || return 0\n&quot;}}},{&quot;rowIdx&quot;:6,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\n# Copyright 2018 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \&quot;License\&quot;);\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# This shell script is used to build a cluster and create a namespace from our\n# argo workflow\n\nset -o errexit\nset -o nounset\nset -o pipefail\n\nCLUSTER_NAME=\&quot;${CLUSTER_NAME}\&quot;\nZONE=\&quot;${GCP_ZONE}\&quot;\nPROJECT=\&quot;${GCP_PROJECT}\&quot;\nNAMESPACE=\&quot;${DEPLOY_NAMESPACE}\&quot;\n\necho \&quot;Activating service-account\&quot;\ngcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}\necho \&quot;Creating GPU cluster\&quot;\ngcloud --project ${PROJECT} beta container clusters create ${CLUSTER_NAME} \\\n    --zone ${ZONE} \\\n    --machine-type=n1-standard-8 \\\n    --num-nodes=6 \\\n    --cluster-version 1.14\necho \&quot;Configuring kubectl\&quot;\ngcloud --project ${PROJECT} container clusters get-credentials ${CLUSTER_NAME} \\\n    --zone ${ZONE}\n&quot;}}},{&quot;rowIdx&quot;:7,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/sh\nprog=wave1D_u0_s.py\n\ngrep 'if n == 90:' $prog\nif [ $? -ne 0 ]; then\n  echo \&quot;insert if n == 90: st.savefig('frame_C%s.pdf' % C) in $prog\&quot;\n  exit\nfi\n\nC_values=\&quot;1.0 0.95 0.2 1.0015\&quot;\nfor C in $C_values; do\npython $prog $C\nscitools movie output_file=index.html fps=2 frame*.png\nscitools movie encoder=convert output_file=movie.gif fps=4 frame*.png\ndir=guitar_C$C\nrm -rf $dir\nmkdir $dir\nmv movie.gif index.html frame*.png $dir\ndone\nscitools rename frame_C wave1D_guitar_C frame_C*.pdf\n&quot;}}},{&quot;rowIdx&quot;:8,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;export KUBERNETES_SERVICE_HOST=master.serverless-6e97.openshiftworkshop.com\nexport KUBERNETES_SERVICE_PORT=443\nexport KUBERNETES_CLIENT_SERVICEACCOUNT_ROOT=$(pwd)/istio\nexport COOLSTORE_GW_ENDPOINT=http://istio-ingressgateway-istio-system.apps.serverless-6e97.openshiftworkshop.com\n#export COOLSTORE_SCENARIOS_ENDPOINT=http://scenarios-coolstore.apps.serverless-6e97.openshiftworkshop.com\nexport COOLSTORE_SCENARIOS_ENDPOINT=http://localhost:8080\nexport OPENSHIFT_BUILD_NAMESPACE=coolstore-ng\nexport BASE_DOMAIN=apps.serverless-6e97.openshiftworkshop.com\nexport WEB_UI_CUSTOM_PORT=8090\n\nnpm run dev&quot;}}},{&quot;rowIdx&quot;:9,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# Combined file for easier scripting\nexport MQSI_SIGNAL_EXCLUSIONS=11\nexport MQSI_NO_CACHE_SUPPORT=1\n\n. /opt/ibm/ace-12/server/bin/mqsiprofile\n\nexport LD_LIBRARY_PATH=/lib:/opt/ibm/java/jre/lib/amd64/compressedrefs:/opt/ibm/java/jre/lib/amd64:$LD_LIBRARY_PATH\n\n# Not really ibmjava-related, but still needed\nexport LD_LIBRARY_PATH=/usr/glibc-compat/zlib-only:$LD_LIBRARY_PATH\n&quot;}}},{&quot;rowIdx&quot;:10,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n#\n# This file is open source software, licensed to you under the terms\n# of the Apache License, Version 2.0 (the \&quot;License\&quot;).  See the NOTICE file\n# distributed with this work for additional information regarding copyright\n# ownership.  You may not use this file except in compliance with the License.\n#\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \&quot;AS IS\&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n# os-release may be missing in container environment by default.\nif [ -f \&quot;/etc/os-release\&quot; ]; then\n    . /etc/os-release\nelif [ -f \&quot;/etc/arch-release\&quot; ]; then\n    export ID=arch\nelse\n    echo \&quot;/etc/os-release missing.\&quot;\n    exit 1\nfi\n\ndebian_packages=(\n    ninja-build\n    ragel\n    libhwloc-dev\n    libnuma-dev\n    libpciaccess-dev\n    libcrypto++-dev\n    libboost-all-dev\n    libxml2-dev\n    xfslibs-dev\n    libgnutls28-dev\n    liblz4-dev\n    libsctp-dev\n    gcc\n    make\n    python3\n    systemtap-sdt-dev\n    libtool\n    cmake\n    libyaml-cpp-dev\n    libc-ares-dev\n    stow\n    g++\n    libfmt-dev\n    diffutils\n    valgrind\n    doxygen\n    openssl\n    pkg-config\n)\n\n# seastar doesn't directly depend on these packages. They are\n# needed because we want to link seastar statically and pkg-config\n# has no way of saying \&quot;static seastar, but dynamic transitive\n# dependencies\&quot;. They provide the various .so -> .so.ver symbolic\n# links.\ntransitive=(libtool-ltdl-devel trousers-devel libidn2-devel libunistring-devel)\n\nredhat_packages=(\n    hwloc-devel\n    numactl-devel\n    libpciaccess-devel\n    cryptopp-devel\n    libxml2-devel\n    xfsprogs-devel\n    gnutls-devel\n    lksctp-tools-devel\n    lz4-devel\n    gcc\n    g++\n    make\n    python3\n    systemtap-sdt-devel\n    libtool\n    cmake\n    yaml-cpp-devel\n    c-ares-devel\n    stow\n    diffutils\n    doxygen\n    openssl\n    fmt-devel\n    boost-devel\n    valgrind-devel\n    \&quot;${transitive[@]}\&quot;\n)\n\nfedora_packages=(\n    \&quot;${redhat_packages[@]}\&quot;\n    gcc-c++\n    ninja-build\n    ragel\n    boost-devel\n    fmt-devel\n    libubsan\n    libasan\n    libatomic\n    valgrind-devel\n)\n\ncentos7_packages=(\n    \&quot;${redhat_packages[@]}\&quot;\n    ninja-build\n    ragel\n    cmake3\n    rh-mongodb36-boost-devel\n    devtoolset-9-gcc-c++\n    devtoolset-9-libubsan\n    devtoolset-9-libasan\n    devtoolset-9-libatomic\n)\n\ncentos8_packages=(\n    \&quot;${redhat_packages[@]}\&quot;\n    ninja-build\n    ragel\n    gcc-toolset-9-gcc\n    gcc-toolset-9-gcc-c++\n    gcc-toolset-9-libubsan-devel\n    gcc-toolset-9-libasan-devel\n    gcc-toolset-9-libatomic-devel\n)\n\n# 1) glibc 2.30-3 has sys/sdt.h (systemtap include)\n#    some old containers may contain glibc older,\n#    so enforce update on that one.\n# 2) if problems with signatures, ensure having fresh\n#    archlinux-keyring: pacman -Sy archlinux-keyring &amp;&amp; pacman -Syyu\n# 3) aur installations require having sudo and being\n#    a sudoer. makepkg does not work otherwise.\narch_packages=(\n    gcc\n    ninja\n    ragel\n    boost\n    boost-libs\n    hwloc\n    numactl\n    libpciaccess\n    crypto++\n    libxml2\n    xfsprogs\n    gnutls\n    lksctp-tools\n    lz4\n    make\n    libtool\n    cmake\n    yaml-cpp\n    stow\n    c-ares\n    pkgconf\n    fmt\n    python3\n    glibc\n    filesystem\n    valgrind\n    openssl\n)\n\nopensuse_packages=(\n    c-ares-devel\n    cmake\n    hwloc-devel\n    libboost_filesystem1_66_0\n    libboost_filesystem1_66_0-devel\n    libboost_program_options1_66_0\n    libboost_program_options1_66_0-devel\n    libboost_system1_66_0\n    libboost_system1_66_0-devel\n    libboost_test1_66_0\n    libboost_test1_66_0-devel\n    libboost_thread1_66_0\n    libboost_thread1_66_0-devel\n    libcryptopp-devel\n    libboost_atomic1_66_0\n    libboost_atomic1_66_0-devel\n    libboost_date_time1_66_0\n    libboost_date_time1_66_0-devel\n    libboost_chrono1_66_0\n    libboost_chrono1_66_0-devel\n    libgnutls-devel\n    libgnutlsxx28\n    liblz4-devel\n    libnuma-devel\n    lksctp-tools-devel\n    ninja\n    ragel\n    xfsprogs-devel\n    yaml-cpp-devel\n    libtool\n    stow\n    openssl\n)\n\ncase \&quot;$ID\&quot; in\n    ubuntu|debian|pop)\n        apt-get install -y \&quot;${debian_packages[@]}\&quot;\n    ;;\n    fedora)\n        dnf install -y \&quot;${fedora_packages[@]}\&quot;\n    ;;\n    rhel|centos|amzn)\n        if [ \&quot;$VERSION_ID\&quot; = \&quot;7\&quot; ]; then\n            yum install -y epel-release centos-release-scl scl-utils\n            yum install -y \&quot;${centos7_packages[@]}\&quot;\n        elif [ \&quot;${VERSION_ID%%.*}\&quot; = \&quot;8\&quot; ]; then\n            dnf install -y epel-release\n            dnf install -y \&quot;${centos8_packages[@]} ${arch_packages[@]}\&quot;\n        elif [ \&quot;$VERSION_ID\&quot; = \&quot;2\&quot; ]; then\n            yum install -y epel-release centos-release-scl scl-utils\n            yum install -y \&quot;${centos8_packages[@]} ${arch_packages[@]}\&quot;\n        fi\n    ;;\n    opensuse-leap)\n        zypper install -y \&quot;${opensuse_packages[@]}\&quot;\n    ;;\n    arch|manjaro)\n        if [ \&quot;$EUID\&quot; -eq \&quot;0\&quot; ]; then\n            pacman -Sy --needed --noconfirm \&quot;${arch_packages[@]}\&quot;\n        else\n            echo \&quot;seastar: running without root. Skipping main dependencies (pacman).\&quot; 1>&amp;2\n        fi\n    ;;\n    *)\n        echo \&quot;Your system ($ID) is not supported by this script. Please install dependencies manually.\&quot;\n        exit 1\n    ;;\nesac\n&quot;}}},{&quot;rowIdx&quot;:11,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/usr/bin/env bash\nset -e\n\nfunction check_command() {\n  if ! command -v $1 >/dev/null; then\n    echo -e \&quot;Install \\033[1m$1\\033[0m\&quot;\n    exit 1\n  fi\n}\n\ncheck_command mvn\ncheck_command jq\ncheck_command yq\ncheck_command yarn\ncheck_command npm\ncheck_command docker\n\nif [[ \&quot;$#\&quot; != \&quot;1\&quot; ]] || [[ ! \&quot;$1\&quot; =~ ^(patch|minor|major)$ ]]; then\n  echo \&quot;Usage: $0 patch|minor|major\&quot;\n  exit 1\nfi\n\nif [[ $(git status --porcelain) ]]; then\n  echo -e \&quot;The repository has changes. Commit first...\\033[0;31mAborting!\\033[0m\&quot;\n  exit 1\nfi\n\ngit pull --rebase\n\ncd paperboy-project-generator\nyarn\nnpm version $1\nnpm publish\n\ncd ../paperboy-core\nyarn\nyarn build\nnpm version $1\nversion=$(cat package.json | jq -r .version)\nnpm publish\n\ncd ../paperboy-magnolia-module\nmvn versions:set -DnewVersion=${version} -DgenerateBackupPoms=false\n\ncd ../paperboy-cli\ncat package.json | jq \&quot;.version = \\\&quot;$version\\\&quot; | .dependencies.\\\&quot;@neoskop/paperboy\\\&quot; = \\\&quot;$version\\\&quot;\&quot; >package.json.new\nmv package.json.new package.json\nyarn\n\nsed -i.bak \&quot;s/version('[[:digit:]]\\+\\.[[:digit:]]\\+\\.[[:digit:]]\\+')/version('$version')/g\&quot; paperboy-cli.js\nrm -rf paperboy-cli.js.bak\nnpm publish\n\ncd ../paperboy-push-service\ncat package.json | jq \&quot;.version = \\\&quot;$version\\\&quot;\&quot; >package.json.new\nmv package.json.new package.json\nyarn\nyarn build\nnpm publish\ndocker build -t neoskop/paperboy-push-service:$version .\ndocker build -t neoskop/paperboy-push-service:latest .\ndocker push neoskop/paperboy-push-service:$version\ndocker push neoskop/paperboy-push-service:latest\n\ncd ../paperboy-docker\nsed -i \&quot;s/ENV PAPERBOY_VERSION=[[:digit:]]\\+\\.[[:digit:]]\\+\\.[[:digit:]]\\+/ENV PAPERBOY_VERSION=$version/\&quot; Dockerfile\ndocker build -t neoskop/paperboy:$version .\ndocker build -t neoskop/paperboy:latest .\ndocker push neoskop/paperboy:$version\ndocker push neoskop/paperboy:latest\n\ncd ../paperboy-helm\nyq eval -i \&quot;.version=\\\&quot;$version\\\&quot;\&quot; ./Chart.yaml\nyq eval -i \&quot;.appVersion=\\\&quot;$version\\\&quot;\&quot; ./Chart.yaml\nyq eval -i \&quot;.image.tag=\\\&quot;$version\\\&quot;\&quot; ./values.yaml\n\ncd ../\ngit add .\ngit commit -m \&quot;chore: Bump version to ${version}.\&quot;\ngit tag ${version}\ngit push origin $version\ngit pull --rebase\ngit push\n\nhelm package paperboy-helm --destination .deploy\ncr upload -o neoskop -r paperboy -p .deploy\ngit checkout gh-pages\ncr index -i ./index.yaml -p .deploy -o neoskop -r paperboy -c https://neoskop.github.io/paperboy/\ngit add index.yaml\ngit commit -m \&quot;chore: Bump version to ${version}.\&quot;\ngit push\ngit checkout master\nrm -rf .deploy/\n\nHELM_CHARTS_DIR=../neoskop-helm-charts\n[ -d $HELM_CHARTS_DIR ] || git clone git@github.com:neoskop/helm-charts.git $HELM_CHARTS_DIR\ncd $HELM_CHARTS_DIR\n./update-index.sh\ncd - &amp;>/dev/null&quot;}}},{&quot;rowIdx&quot;:12,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\n. /HolismHolding/Infra/Scripts/Message.sh\n\nfunction LinkConnectionStrings()\n{\n    Info \&quot;Linking ConnectionStrings.json ...\&quot;;\n    sudo ln -s -f /$Organization/Common/ConnectionStrings.json /$Organization/$Repository/ConnectionStrings.json\n    Divide\n}&quot;}}},{&quot;rowIdx&quot;:13,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/usr/bin/env bash\n\n# get run options\nwhile test $# -gt 0; do\n  case \&quot;$1\&quot; in\n    -h|--help)\n      echo \&quot;pac-man$ docker-test - run lambda package\&quot;\n      echo \&quot; \&quot;\n      echo \&quot;pac-man$ docker-test [options]\&quot;\n      echo \&quot; \&quot;\n      echo \&quot;options:\&quot;\n      echo \&quot;-h, --help                show brief help\&quot;\n      echo \&quot;-b, --build               build lambda package prior to running\&quot;\n      exit 0\n      ;;\n    -b|--build)\n      shift\n      export PACMAN_BUILD=1\n      ;;\n    *)\n      break\n      ;;\n  esac\ndone\n\n# cd to pac-man directory\ncd \&quot;$(dirname \&quot;$0\&quot;)\&quot;\n\nif [[ -n ${PACMAN_BUILD} &amp;&amp; \&quot;${PACMAN_BUILD}\&quot;==\&quot;1\&quot; ]]; then\n  # build lambda package\n  docker run --rm \\\n      -v ${PWD}:/code \\\n      -v ${HOME}/.cargo/registry:/root/.cargo/registry \\\n      -v ${HOME}/.cargo/git:/root/.cargo/git \\\n      softprops/lambda-rust &amp;&amp; \\\n  unzip -o \\\n      target/lambda/release/pac-man.zip \\\n      -d /tmp/lambda &amp;&amp; \\\n  echo \&quot;Enter Payload Then Press CTRL-D...\&quot; &amp;&amp; \\\n  docker run \\\n      -i -e DOCKER_LAMBDA_USE_STDIN=1 \\\n      -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\\n      -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\\n      --rm \\\n      -v /tmp/lambda:/var/task \\\n      lambci/lambda:provided\nelse\n  echo \&quot;Enter Payload Then Press CTRL-D...\&quot; &amp;&amp; \\\n  docker run \\\n      -i -e DOCKER_LAMBDA_USE_STDIN=1 \\\n      -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\\n      -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\\n      --rm \\\n      -v /tmp/lambda:/var/task \\\n      lambci/lambda:provided\nfi\n&quot;}}},{&quot;rowIdx&quot;:14,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\nprintf \&quot;testing '$1'\\n\&quot;\n\nprintf \&quot;testing python... \&quot;\n\nppp_file=$1\n\npy_output=$(python3 $ppp_file 2>&amp;1)\npy_exit_code=$?\n\nif [ \&quot;$py_exit_code\&quot; -eq \&quot;0\&quot; ]; then\n\tprintf \&quot;succeeded\\n\&quot;\nelse\n\tprintf \&quot;FAILED!\\n\&quot;\nfi\n\nprintf \&quot;testing C++... \&quot;\n\ncpp_comp_output=$(g++ -x c++ -std=c++14 $ppp_file -o tmp_bin 2>&amp;1)\ncpp_comp_exit_code=$?\n\ncpp_run_output=\&quot;\&quot;\ncpp_run_exit_code=1\n\nif [ \&quot;$cpp_comp_exit_code\&quot; -eq \&quot;0\&quot; ]; then\n\tcpp_run_output=$(./tmp_bin 2>&amp;1)\n\tcpp_run_exit_code=$?\n\t\n\tif [ \&quot;$cpp_run_exit_code\&quot; -eq \&quot;0\&quot; ]; then\n\t\tprintf \&quot;succeeded\\n\&quot;\n\telse\n\t\tprintf \&quot;CRASHED!\\n\&quot;\n\tfi\n\t\n\trm tmp_bin\nelse\n\tprintf \&quot;FAILED TO COMPILE!\\n\&quot;\nfi\n\nif [ \&quot;$py_exit_code\&quot; -eq \&quot;0\&quot; ] &amp;&amp; [ \&quot;$cpp_run_exit_code\&quot; -eq \&quot;0\&quot; ] &amp;&amp; [ \&quot;$py_output\&quot; = \&quot;$cpp_run_output\&quot; ]; then\n\t\n\tprintf \&quot;Python and C++ outputs match\\n\&quot;\n\tprintf \&quot;________\\n\&quot;\n\tprintf \&quot; output \\__________________________________________\\n\\n\&quot;\n\tprintf \&quot;$py_output\\n\&quot;\n\tprintf \&quot;___________________________________________________\\n\&quot;\nelse\n\t\n\tif [ \&quot;$py_exit_code\&quot; -eq \&quot;0\&quot; ] &amp;&amp; [ \&quot;$cpp_run_exit_code\&quot; -eq \&quot;0\&quot; ]; then\n\t\tprintf \&quot;Python and C++ outputs DO NOT MATCH!\\n\&quot;\n\tfi\n\t\n\tprintf \&quot;_______________\\n\&quot;\n\tprintf \&quot; Python output \\___________________________________\\n\\n\&quot;\n\tprintf \&quot;$py_output\\n\&quot;\n\tprintf \&quot;___________________________________________________\\n\&quot;\n\t\n\tif [ \&quot;$cpp_comp_exit_code\&quot; -ne \&quot;0\&quot; ]; then\n\t\tprintf \&quot;_____________________\\n\&quot;\n\t\tprintf \&quot; C++ compiler output \\_____________________________\\n\\n\&quot;\n\t\tprintf \&quot;$cpp_comp_output\\n\&quot;\n\t\tprintf \&quot;___________________________________________________\\n\&quot;\n\telse\n\t\tprintf \&quot;____________\\n\&quot;\n\t\tprintf \&quot; C++ output \\______________________________________\\n\\n\&quot;\n\t\tprintf \&quot;$cpp_run_output\\n\&quot;\n\t\tprintf \&quot;___________________________________________________\\n\&quot;\n\tfi\nfi\n\nprintf \&quot;\\n\&quot;\n\n\n\n&quot;}}},{&quot;rowIdx&quot;:15,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\nexport HOME=/root/\nsource $HOME/.bashrc\nsource $HOME/conda/bin/activate\nconda activate tali\n\ncd $CODE_DIR\ngit pull\npip install -r $CODE_DIR/requirements.txt\n\nsource $CODE_DIR/setup_scripts/setup_base_experiment_disk.sh\nsource $CODE_DIR/setup_scripts/setup_wandb_credentials.sh\n\ncd $CODE_DIR\n\nfuser -k /dev/nvidia*; \\\npython $CODE_DIR/run.py \\\nhydra.verbose=True \\\ntrainer=default \\\nresume=True \\\nbatch_size=8 \\\ntrainer.gpus=4 \\\ntrainer.auto_scale_batch_size=True \\\ndatamodule.dataset_config.rescan_paths=True \\\ndatamodule.prefetch_factor=3 \\\ndatamodule.num_workers=48 \\\nmodel=deci_modus_prime_resnet50 \\\ndatamodule.dataset_config.dataset_size_identifier=base \\\ndatamodule.dataset_config.modality_config.image=True \\\ndatamodule.dataset_config.modality_config.text=True \\\ndatamodule.dataset_config.modality_config.audio=True \\\ndatamodule.dataset_config.modality_config.video=True\n\n&quot;}}},{&quot;rowIdx&quot;:16,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/usr/bin/env bash\n\nwatch --color --beep 'bash script/ci.sh t'\n&quot;}}},{&quot;rowIdx&quot;:17,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/sh\n\n# generate sim input\necho \&quot;(1/5) generating simulation input data\&quot;\ncd tests\n./sim_input.py sim_points.txt sim_input.h || exit $?\ncd ..\n\n# compile simulation\necho \&quot;(2/5) compiling the simulation program\&quot;\n./compile.sh sim || exit $?\n\n# flash target with simulation program\necho \&quot;(3/5) flashing the target\&quot;\n./flash.sh sim || exit $?\n\n# redirect tether output to file\necho \&quot;(4/5) running the simulation\&quot;\n./tether.py --format-csv tests/sim_output.csv || exit $?\n\n# run tests\necho \&quot;(5/5) checking the simulation output\&quot;\n./tests/sim_tests.py tests/sim_output.csv || exit $?\n&quot;}}},{&quot;rowIdx&quot;:18,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n# ------------------------------------------------------------------------------\n# SCM Breeze - Streamline your SCM workflow.\n# Copyright 2011 Nathan Broadbent (http://madebynathan.com). All Rights Reserved.\n# Released under the LGPL (GNU Lesser General Public License)\n# ------------------------------------------------------------------------------\n#\n# Unit tests for git shell scripts\n\nexport scmbDir=\&quot;$( cd -P \&quot;$( dirname \&quot;$0\&quot; )\&quot; &amp;&amp; pwd )/../../..\&quot;\n\n# Zsh compatibility\nif [ -n \&quot;${ZSH_VERSION:-}\&quot; ]; then shell=\&quot;zsh\&quot;; SHUNIT_PARENT=$0; setopt shwordsplit; fi\n\n# Load test helpers\nsource \&quot;$scmbDir/test/support/test_helper.sh\&quot;\n\n# Load functions to test\nsource \&quot;$scmbDir/lib/scm_breeze.sh\&quot;\nsource \&quot;$scmbDir/lib/git/repo_index.sh\&quot;\n\n\n# Setup and tear down\n#-----------------------------------------------------------------------------\noneTimeSetUp() {\n  GIT_REPO_DIR=$(mktemp -d -t scm_breeze.XXXXXXXXXX)\n  GIT_REPOS=\&quot;/tmp/test_repo_1:/tmp/test_repo_11\&quot;\n  git_status_command=\&quot;git status\&quot;\n\n  git_index_file=\&quot;$GIT_REPO_DIR/.git_index\&quot;\n\n  silentGitCommands\n\n  cd $GIT_REPO_DIR\n  # Setup test repos in temp repo dir\n  for repo in github bitbucket source_forge TestCaps; do\n    mkdir $repo; cd $repo; git init; cd - > /dev/null\n  done\n\n  # Add some nested dirs for testing resursive tab completion\n  mkdir -p github/videos/octocat/live_action\n  # Add hidden dir to test that '.git' is filtered, but other hidden dirs are available.\n  mkdir -p github/.im_hidden\n\n  # Setup a test repo with some submodules\n  # (just a dummy '.gitmodules' file and some nested .git directories)\n  mkdir submodules_everywhere\n  cd submodules_everywhere\n  git init\n  cat > .gitmodules <<EOF\n[submodule \&quot;very/nested/directory/red_submodule\&quot;]\n[submodule \&quot;very/nested/directory/green_submodule\&quot;]\n[submodule \&quot;very/nested/directory/blue_submodule\&quot;]\nEOF\n  mkdir -p \&quot;very/nested/directory\&quot;\n  cd \&quot;very/nested/directory\&quot;\n  for repo in red_submodule green_submodule blue_submodule; do\n    mkdir $repo; cd $repo; git init; cd - > /dev/null\n  done\n\n  # Setup some custom repos outside the main repo dir\n  IFS=\&quot;:\&quot;\n  for dir in $GIT_REPOS; do\n    mkdir -p $dir; cd $dir; git init;\n  done\n  unset IFS\n\n  verboseGitCommands\n\n  cd \&quot;$orig_cwd\&quot;\n}\n\noneTimeTearDown() {\n  rm -rf \&quot;${GIT_REPO_DIR}\&quot;\n  IFS=\&quot;:\&quot;\n  for dir in $GIT_REPOS; do rm -rf $dir; done\n  unset IFS\n}\n\nensureIndex() {\n  _check_git_index\n}\n\nindex_no_newlines() {\n  tr \&quot;\\\\n\&quot; \&quot; \&quot; < $git_index_file\n}\n\n\n#-----------------------------------------------------------------------------\n# Unit tests\n#-----------------------------------------------------------------------------\n\ntest_repo_index_command() {\n  git_index --rebuild > /dev/null\n\n  # Test that all repos are detected, and sorted alphabetically\n  assertIncludes \&quot;$(index_no_newlines)\&quot; \&quot;bitbucket.*\\\nblue_submodule.*\\\ngithub.*\\\ngreen_submodule.*\\\nred_submodule.*\\\nsource_forge.*\\\nsubmodules_everywhere.*\\\ntest_repo_11.*\\\ntest_repo_1\&quot;\n\n}\n\ntest_check_git_index() {\n  ensureIndex\n  echo \&quot;should not be regenerated\&quot; >> $git_index_file\n  _check_git_index\n  # Test that index is not rebuilt unless empty\n  assertIncludes \&quot;$(index_no_newlines)\&quot; \&quot;should not be regenerated\&quot;\n  rm $git_index_file\n  # Test the index is rebuilt\n  _check_git_index\n  assertTrue \&quot;[ -f $git_index_file ]\&quot;\n}\n\ntest_git_index_count() {\n  assertEquals \&quot;10\&quot; \&quot;$(_git_index_count)\&quot;\n}\n\ntest_repo_list() {\n  ensureIndex\n  list=$(git_index --list)\n  assertIncludes \&quot;$list\&quot; \&quot;bitbucket\&quot;      || return\n  assertIncludes \&quot;$list\&quot; \&quot;blue_submodule\&quot; || return\n  assertIncludes \&quot;$list\&quot; \&quot;test_repo_11\&quot;\n}\n\n# Test matching rules for changing directory\ntest_git_index_changing_directory() {\n  ensureIndex\n  git_index \&quot;github\&quot;;       assertEquals \&quot;$GIT_REPO_DIR/github\&quot; \&quot;$PWD\&quot;\n  git_index \&quot;github/\&quot;;      assertEquals \&quot;$GIT_REPO_DIR/github\&quot; \&quot;$PWD\&quot;\n  git_index \&quot;bucket\&quot;;       assertEquals \&quot;$GIT_REPO_DIR/bitbucket\&quot; \&quot;$PWD\&quot;\n  git_index \&quot;testcaps\&quot;;     assertEquals \&quot;$GIT_REPO_DIR/TestCaps\&quot; \&quot;$PWD\&quot;\n  git_index \&quot;green_sub\&quot;;    assertEquals \&quot;$GIT_REPO_DIR/submodules_everywhere/very/nested/directory/green_submodule\&quot; \&quot;$PWD\&quot;\n  git_index \&quot;_submod\&quot;;      assertEquals \&quot;$GIT_REPO_DIR/submodules_everywhere/very/nested/directory/blue_submodule\&quot; \&quot;$PWD\&quot;\n  git_index \&quot;test_repo_1\&quot;;  assertEquals \&quot;/tmp/test_repo_1\&quot; \&quot;$PWD\&quot;\n  git_index \&quot;test_repo_11\&quot;; assertEquals \&quot;/tmp/test_repo_11\&quot; \&quot;$PWD\&quot;\n  git_index \&quot;test_repo_\&quot;;   assertEquals \&quot;/tmp/test_repo_11\&quot; \&quot;$PWD\&quot;\n  git_index \&quot;github/videos/octocat/live_action\&quot;; assertEquals \&quot;$GIT_REPO_DIR/github/videos/octocat/live_action\&quot; \&quot;$PWD\&quot;\n}\n\ntest_git_index_tab_completion() {\n  # Only run tab completion test for bash\n  if [[ \&quot;$0\&quot; == *bash ]]; then\n    ensureIndex\n    COMP_CWORD=0\n\n    # Test that '--' commands have tab completion\n    COMP_WORDS=\&quot;--\&quot;\n    _git_index_tab_completion\n    assertEquals \&quot;Incorrect number of tab-completed '--' commands\&quot; \&quot;5\&quot; \&quot;$(tab_completions | wc -w)\&quot;\n\n    COMP_WORDS=\&quot;gith\&quot;\n    _git_index_tab_completion\n    assertIncludes \&quot;$(tab_completions)\&quot; \&quot;github/\&quot;\n\n    # Test completion for project sub-directories when project ends with '/'\n    COMP_WORDS=\&quot;github/\&quot;\n    _git_index_tab_completion\n    assertIncludes    \&quot;$(tab_completions)\&quot; \&quot;github/videos/\&quot;\n    # Check that '.git/' is filtered from completion, but other hidden dirs are available\n    assertNotIncludes \&quot;$(tab_completions)\&quot; \&quot;github/.git/\&quot;\n    assertIncludes    \&quot;$(tab_completions)\&quot; \&quot;github/.im_hidden/\&quot;\n\n    COMP_WORDS=\&quot;github/videos/\&quot;\n    _git_index_tab_completion\n    assertIncludes \&quot;$(tab_completions)\&quot; \&quot;github/videos/octocat/\&quot;\n\n\n    # Test that completion checks for other matching projects even if one matches perfectly\n    COMP_WORDS=\&quot;test_repo_1\&quot;\n    _git_index_tab_completion\n    assertIncludes \&quot;$(tab_completions)\&quot; \&quot;test_repo_1/ test_repo_11/\&quot;\n  fi\n}\n\n\n# Test changing to top-level directory (when arg begins with '/')\ntest_changing_to_top_level_directory() {\n  mkdir \&quot;$GIT_REPO_DIR/gems\&quot;\n  git_index \&quot;/gems\&quot;\n  assertEquals \&quot;$GIT_REPO_DIR/gems\&quot; \&quot;$PWD\&quot;\n}\n\n\n# load and run shUnit2\n# Call this function to run tests\nsource \&quot;$scmbDir/test/support/shunit2\&quot;\n\n&quot;}}},{&quot;rowIdx&quot;:19,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n# Boost for compiling 32-bit binaries on 64-bit:\n#   ./bootstrap.sh\n#   ./b2 link=static address-model=32 stage\n\nset -eu\n\nfunction boost-static\n{\n  sed -i 's/^\\(oakfoam_LDADD =\\) \\(.*\\) \\($(HOARD_LIB).*\\)$/\\1 -Wl,-Bstatic \\2 -Wl,-Bdynamic -pthread \\3/' Makefile\n}\n\nVER=`cat config.h | sed -n 's/.*PACKAGE_VERSION \\\&quot;\\(.*\\)\\\&quot;.*/\\1/p'`\nPREV_CONFIGURE=`cat config.log | head | sed -n 's/\\s*$ //p'`\necho \&quot;configure was: $PREV_CONFIGURE\&quot;\n\nDEBINPUT=\&quot;0\noakfoam@gmail.com\n5\nBSD\n6\ngames\n7\ni386\n\&quot;\n\nBOOST_ROOT=/data/opt/boost_1_47_0 $PREV_CONFIGURE --with-web 'CPPFLAGS=-m32' 'LDFLAGS=-m32 -pthread'\nboost-static\necho \&quot;$DEBINPUT\&quot; | sudo checkinstall --nodoc --install=no make install\nsudo chmod a+rw oakfoam oakfoam_*.deb\n\nNAME=oakfoam_${VER}_i386\n\nrm -f ${NAME}.tar.gz\nmkdir ${NAME}\n\n# BOOST_ROOT=/data/opt/boost_1_47_0 $PREV_CONFIGURE --with-web 'CPPFLAGS=-m32' 'LDFLAGS=-m32 -pthread'\n# boost-static\nmake install DESTDIR=`pwd`/${NAME}\n\nfind ${NAME}/ -type f | grep -v 'menu\\|applications\\|www' | xargs -n1 -I{} mv {} $NAME/\nfind ${NAME}/ -type d -name www | xargs -n1 -I{} mv {} $NAME/\n\nsed -i '/^cd \\.\\./d;/^bin=\&quot;.*/d;s/$bin/\\./' ${NAME}/oakfoam-web\nmv ${NAME}/oakfoam-web ${NAME}/run.sh\n\ntar -czf ${NAME}.tar.gz ${NAME}/\nrm -r ${NAME}/\n\nif [ \&quot;`uname -m`\&quot; == \&quot;x86_64\&quot; ]; then\n  DEBINPUT=\&quot;0\n  oakfoam@gmail.com\n  5\n  BSD\n  6\n  games\n  \&quot;\n\n  $PREV_CONFIGURE --with-web\n  boost-static\n  make clean\n  echo \&quot;$DEBINPUT\&quot; | sudo checkinstall --nodoc --install=no make install\n  sudo chmod a+rw oakfoam oakfoam_*.deb\n\n  NAME=oakfoam_${VER}_amd64\n\n  rm -f ${NAME}.tar.gz\n  mkdir ${NAME}\n\n  # $PREV_CONFIGURE --with-web\n  # boost-static\n  make install DESTDIR=`pwd`/${NAME}\n\n  find ${NAME}/ -type f | grep -v 'menu\\|applications\\|www' | xargs -n1 -I{} mv {} $NAME/\n  find ${NAME}/ -type d -name www | xargs -n1 -I{} mv {} $NAME/\n\n  sed -i '/^cd \\.\\./d;/^bin=\&quot;.*/d;s/$bin/\\./' ${NAME}/oakfoam-web\n  mv ${NAME}/oakfoam-web ${NAME}/run.sh\n\n  tar -czf ${NAME}.tar.gz ${NAME}/\n  rm -r ${NAME}/\n  make clean\nfi\n\n$PREV_CONFIGURE\n\n&quot;}}},{&quot;rowIdx&quot;:20,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/sh\n\nif [ \&quot;$USER\&quot; != \&quot;root\&quot; ]\nthen\n     echo \&quot;This installer must be run with root privileges. Please run sudo $0\&quot;\n     return 1\nfi\n\n# Ensure the libraries we use are installed\napt install python3 python3-rpi.gpio python3-requests\n\naddgroup --system doorbot\nadduser --system --ingroup gpio doorbot\n\nfor N in doorbot.ini.example doorbot.py doorbot.service ringtest.py\n    do cp $N /home/doorbot\n    chown doorbot:doorbot /home/doorbot/$N\ndone\n\nif [ -f /etc/systemd/system/doorbot.service ]\n    then echo \&quot;Unit file already exists, skipping\&quot;\n    else ln /home/doorbot/doorbot.service /etc/systemd/system/\nfi\nsystemctl daemon-reload\n\n&quot;}}},{&quot;rowIdx&quot;:21,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\nfor file in *.gv\ndo\n    name=${file%%.*}\n    dot -Tsvg:cairo:cairo $name.gv > ../output/$name.svg\ndone\n&quot;}}},{&quot;rowIdx&quot;:22,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;python transformers/examples/language-modeling/run_language_modeling.py --model_type gpt2 --tokenizer_name model-configs/1024-config --config_name model-configs/1024-config/config.json --train_data_file ../data/wikitext-103-raw/wiki.train.raw --eval_data_file ../data/wikitext-103-raw/wiki.valid.raw --output_dir train-outputs/512+0+512-shuffled-N/13-model --do_train --do_eval --evaluate_during_training --per_device_train_batch_size 3 --per_device_eval_batch_size 3 --num_train_epochs 10 --dataloader_drop_last --save_steps 500 --save_total_limit 20 --augmented --augmentation_function shuffle_remove_all_but_nouns_first_half --train_function augmented_training --eval_function augmented_eval --seed 13&quot;}}},{&quot;rowIdx&quot;:23,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\ndieharder -d 206 -g 27 -S 844198761\n&quot;}}},{&quot;rowIdx&quot;:24,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/usr/bin/env bash\n\nset -eu\nset -o pipefail\n\nEULA=${EULA:-false}\nHEAP_SIZE=${HEAP_SIZE:-1024}\nJVM_OPTS=${JVM_OPTS:-}\nRCON_PASSWORD=${RCON_PASSWORD:-}\nSERVER_OPTS=${SERVER_OPTS:-}\n\ncd $(pwd)/config\n\nif [ $(ls -1 ../overrides | wc -l) != \&quot;0\&quot; ]; then\n    echo \&quot;Copying configuration overrides...\&quot;\n    for file in ../overrides/*; do\n        echo \&quot;    $(basename ${file})\&quot;\n        cp ${file} .\n    done\n    echo \&quot;done!\&quot;\nfi\n\nif [ -n \&quot;$RCON_PASSWORD\&quot; ]; then\n    echo \&quot;rcon.password=${RCON_PASSWORD}\&quot; >> server.properties\nfi\n\necho \&quot;Copying configuration defaults...\&quot;\nfor file in ../defaults/*; do\n    if [ ! -f \&quot;$(basename ${file})\&quot; ]; then\n        echo \&quot;    $(basename ${file})\&quot;\n        cp ${file} .\n    fi\ndone\necho \&quot;done!\&quot;\n\nif ! grep -q eula=true eula.txt; then\n    if [ \&quot;$EULA\&quot; != \&quot;true\&quot; ]; then\n        echo \&quot;You must accept the Minecraft EULA to run the server! Read it at:\&quot;\n        echo \&quot;> https://account.mojang.com/documents/minecraft_eula\&quot;\n        echo \&quot;and then restart the server with EULA=true to accept the EULA.\&quot;\n        exit 1\n    else\n        sed -e \&quot;/^eula=/ s/=.*$/=${EULA}/\&quot; -i\&quot;\&quot; eula.txt\n    fi\nfi\n\nsed -e \&quot;/^(query\\.|server-)port=/ s/\\d+/25565/\&quot; \\\n    -e \&quot;/^rcon.port=/ s/\\d+/25575/\&quot; \\\n    -i\&quot;\&quot; server.properties\n\nNURSERY_MINIMUM=$((${HEAP_SIZE} / 2))\nNURSERY_MAXIMUM=$((${HEAP_SIZE} * 4 / 5))\n\nJVM_OPTS=\&quot;${JVM_OPTS} -Xms${HEAP_SIZE}M -Xmx${HEAP_SIZE}M -Xmns${NURSERY_MINIMUM}M -Xmnx${NURSERY_MAXIMUM}M\&quot;\nJVM_OPTS=\&quot;${JVM_OPTS} -Xgc:concurrentScavenge -Xgc:dnssExpectedTimeRatioMaximum=3 -Xgc:scvNoAdaptiveTenure\&quot;\nJVM_OPTS=\&quot;${JVM_OPTS} -Xdisableexplicitjc -Xtune:virtualized -Dlog4j.configurationFile=log4j2.xml\&quot;\nSERVER_OPTS=\&quot;--universe ../server --plugins ../plugins ${SERVER_OPTS}\&quot;\n\nexec mc-server-runner java ${JVM_OPTS} -jar ../bin/paperclip.jar ${SERVER_OPTS}\n&quot;}}},{&quot;rowIdx&quot;:25,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/usr/bin/env bash\nexport DOKKU_QUIET_OUTPUT=1\nexport DOKKU_ROOT=\&quot;$(cd \&quot;$(dirname \&quot;${BASH_SOURCE[0]}\&quot;)\&quot; &amp;&amp; pwd)/dokku\&quot;\nexport DOKKU_VERSION=${DOKKU_VERSION:-\&quot;master\&quot;}\nexport PATH=\&quot;$(cd \&quot;$(dirname \&quot;${BASH_SOURCE[0]}\&quot;)\&quot; &amp;&amp; pwd)/bin:$(cd \&quot;$(dirname \&quot;${BASH_SOURCE[0]}\&quot;)\&quot; &amp;&amp; pwd)/dokku:$PATH\&quot;\nexport PLUGIN_COMMAND_PREFIX=\&quot;s3\&quot;\nexport PLUGIN_PATH=\&quot;$DOKKU_ROOT/plugins\&quot;\nexport PLUGIN_ENABLED_PATH=\&quot;$PLUGIN_PATH\&quot;\nexport PLUGIN_AVAILABLE_PATH=\&quot;$PLUGIN_PATH\&quot;\nexport PLUGIN_CORE_AVAILABLE_PATH=\&quot;$PLUGIN_PATH\&quot;\nexport S3RVER_ROOT=\&quot;$(cd \&quot;$(dirname \&quot;${BASH_SOURCE[0]}\&quot;)\&quot; &amp;&amp; pwd)/fixtures\&quot;\nexport PLUGIN_DATA_ROOT=\&quot;$S3RVER_ROOT\&quot;\nif [[ \&quot;$(uname)\&quot; == \&quot;Darwin\&quot; ]]; then\n  export PLUGN_URL=\&quot;https://github.com/dokku/plugn/releases/download/v0.2.1/plugn_0.2.1_darwin_x86_64.tgz\&quot;\nelse\n  export PLUGN_URL=\&quot;https://github.com/dokku/plugn/releases/download/v0.2.1/plugn_0.2.1_linux_x86_64.tgz\&quot;\nfi\n\nmkdir -p \&quot;$PLUGIN_DATA_ROOT\&quot;\nrm -rf \&quot;${PLUGIN_DATA_ROOT:?}\&quot;/*\n\nflunk() {\n  { if [ \&quot;$#\&quot; -eq 0 ]; then cat -\n    else echo \&quot;$*\&quot;\n    fi\n  }\n  return 1\n}\n\nassert_equal() {\n  if [ \&quot;$1\&quot; != \&quot;$2\&quot; ]; then\n    { echo \&quot;expected: $1\&quot;\n      echo \&quot;actual:   $2\&quot;\n    } | flunk\n  fi\n}\n\nassert_exit_status() {\n  assert_equal \&quot;$status\&quot; \&quot;$1\&quot;\n}\n\nassert_success() {\n  if [ \&quot;$status\&quot; -ne 0 ]; then\n    flunk \&quot;command failed with exit status $status\&quot;\n  elif [ \&quot;$#\&quot; -gt 0 ]; then\n    assert_output \&quot;$1\&quot;\n  fi\n}\n\nassert_exists() {\n  if [ ! -f \&quot;$1\&quot; ]; then\n    flunk \&quot;expected file to exist: $1\&quot;\n  fi\n}\n\nassert_contains() {\n  if [[ \&quot;$1\&quot; != *\&quot;$2\&quot;* ]]; then\n    flunk \&quot;expected $2 to be in: $1\&quot;\n  fi\n}\n\nassert_output() {\n  local expected\n  if [ $# -eq 0 ]; then expected=\&quot;$(cat -)\&quot;\n  else expected=\&quot;$1\&quot;\n  fi\n  assert_equal \&quot;$expected\&quot; \&quot;$output\&quot;\n}\n&quot;}}},{&quot;rowIdx&quot;:26,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\ngrunt\nrm test/res/js/pagenav.js\ncp pagenav.js test/res/js/pagenav.js\ncp pagenav.min.js test/res/js/pagenav.min.js&quot;}}},{&quot;rowIdx&quot;:27,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\nprintCommandHelp() {\n    echo \&quot;Command Help:\&quot;\n    echo -e \&quot;source patchPeerDeployment.sh <peerSubscriptionID> <peerResourceGroup> <peerAKSClusterName>\&quot;\n    echo\n    echo \&quot;Arguments:\&quot;\n    echo -e \&quot;\\tpeerSubscriptionID    : Subscription ID of AKS-HLF peer template deployment\&quot;\n    echo -e \&quot;\\tpeerResourceGroup     : Resource group of AKS-HLF peer template deployment\&quot;\n    echo -e \&quot;\\tpeerAKSClusterName    : AKS Cluster name of AKS-HLF peer template deployment\&quot;\n}\n\nPEER_ORG_SUBSCRIPTION=$1\nPEER_ORG_RESOURCE_GROUP=$2\nPEER_ORG_AKS_NAME=$3\n\nif [ -z $PEER_ORG_SUBSCRIPTION ] || [ -z $PEER_ORG_RESOURCE_GROUP ] || [ -z $PEER_ORG_AKS_NAME ]; then\n    echo\n    echo \&quot;Peer organization subscription, resource group and AKS cluster name cannot be empty!\&quot;\n    echo\n\n    printCommandHelp\n\n    return;\nfi\n\nif ! command -v az &amp;> /dev/null; then\n    echo\n    echo \&quot;Command \\\&quot;az\\\&quot; not found! Please download Azure CLI for your system.\&quot;\n    echo \&quot;To setup Azure CLI after installation, run: az login with valid credentials!\&quot;\n    echo\n\n    return;\nfi\n\naz aks get-credentials --resource-group $PEER_ORG_RESOURCE_GROUP \\\n                       --name $PEER_ORG_AKS_NAME \\\n                       --subscription $PEER_ORG_SUBSCRIPTION\nres=$?\nif [ $res -ne 0 ]; then\n    echo\n    echo \&quot;Switching to AKS cluster config failed with error code: $res!\&quot;\n    echo\n\n    printCommandHelp\n    \n    return\nfi\n\nns=hlf\ndeployments=\&quot;$(kubectl get deploy -n $ns -o=jsonpath='{.items[*].metadata.name}')\&quot;\n\nfor deployment in $deployments; do\n    resource=deploy/$deployment\n\n    if [[ $deployment == peer* ]]; then\n        echo \&quot;Updating\&quot; $deployment\n\n        kubectl scale -n $ns $resource --replicas=0\n        kubectl rollout status -n $ns $resource -w\n\n        kubectl patch deployment $deployment -n $ns -p \\\n        '{\&quot;spec\&quot;: { \&quot;template\&quot;: { \&quot;spec\&quot;: { \&quot;containers\&quot;: [ { \&quot;name\&quot;:\&quot;'$deployment'\&quot;, \&quot;env\&quot;: [{ \&quot;name\&quot;: \&quot;CORE_CHAINCODE_BUILDER\&quot;, \&quot;value\&quot;: \&quot;hlfakstemplateoss.azurecr.io/hyperledger/fabric-ccenv:1.4.4\&quot; }, { \&quot;name\&quot;: \&quot;CORE_CHAINCODE_GOLANG_RUNTIME\&quot;, \&quot;value\&quot;: \&quot;hlfakstemplateoss.azurecr.io/hyperledger/fabric-baseos:amd64-0.4.18\&quot; }, { \&quot;name\&quot;: \&quot;CORE_CHAINCODE_NODE_RUNTIME\&quot;, \&quot;value\&quot;: \&quot;hlfakstemplateoss.azurecr.io/hyperledger/fabric-baseimage:amd64-0.4.18\&quot; }, { \&quot;name\&quot;: \&quot;CORE_CHAINCODE_JAVA_RUNTIME\&quot;, \&quot;value\&quot;: \&quot;\&quot; }, { \&quot;name\&quot;: \&quot;CORE_CHAINCODE_CAR_RUNTIME\&quot;, \&quot;value\&quot;: \&quot;\&quot; }] } ] } } } }'\n\n        kubectl scale -n $ns $resource --replicas=1\n        kubectl rollout status -n $ns $resource -w\n    fi\ndone&quot;}}},{&quot;rowIdx&quot;:28,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/sh\nset -e\n\necho \&quot;mkdir -p ${CONFIGURATION_BUILD_DIR}/${FRAMEWORKS_FOLDER_PATH}\&quot;\nmkdir -p \&quot;${CONFIGURATION_BUILD_DIR}/${FRAMEWORKS_FOLDER_PATH}\&quot;\n\nSWIFT_STDLIB_PATH=\&quot;${DT_TOOLCHAIN_DIR}/usr/lib/swift/${PLATFORM_NAME}\&quot;\n\ninstall_framework()\n{\n  if [ -r \&quot;${BUILT_PRODUCTS_DIR}/$1\&quot; ]; then\n    local source=\&quot;${BUILT_PRODUCTS_DIR}/$1\&quot;\n  elif [ -r \&quot;${BUILT_PRODUCTS_DIR}/$(basename \&quot;$1\&quot;)\&quot; ]; then\n    local source=\&quot;${BUILT_PRODUCTS_DIR}/$(basename \&quot;$1\&quot;)\&quot;\n  elif [ -r \&quot;$1\&quot; ]; then\n    local source=\&quot;$1\&quot;\n  fi\n\n  local destination=\&quot;${TARGET_BUILD_DIR}/${FRAMEWORKS_FOLDER_PATH}\&quot;\n\n  if [ -L \&quot;${source}\&quot; ]; then\n      echo \&quot;Symlinked...\&quot;\n      source=\&quot;$(readlink \&quot;${source}\&quot;)\&quot;\n  fi\n\n  # use filter instead of exclude so missing patterns dont' throw errors\n  echo \&quot;rsync -av --filter \\\&quot;- CVS/\\\&quot; --filter \\\&quot;- .svn/\\\&quot; --filter \\\&quot;- .git/\\\&quot; --filter \\\&quot;- .hg/\\\&quot; --filter \\\&quot;- Headers\\\&quot; --filter \\\&quot;- PrivateHeaders\\\&quot; --filter \\\&quot;- Modules\\\&quot; \\\&quot;${source}\\\&quot; \\\&quot;${destination}\\\&quot;\&quot;\n  rsync -av --filter \&quot;- CVS/\&quot; --filter \&quot;- .svn/\&quot; --filter \&quot;- .git/\&quot; --filter \&quot;- .hg/\&quot; --filter \&quot;- Headers\&quot; --filter \&quot;- PrivateHeaders\&quot; --filter \&quot;- Modules\&quot; \&quot;${source}\&quot; \&quot;${destination}\&quot;\n\n  local basename\n  basename=\&quot;$(basename -s .framework \&quot;$1\&quot;)\&quot;\n  binary=\&quot;${destination}/${basename}.framework/${basename}\&quot;\n  if ! [ -r \&quot;$binary\&quot; ]; then\n    binary=\&quot;${destination}/${basename}\&quot;\n  fi\n\n  # Strip invalid architectures so \&quot;fat\&quot; simulator / device frameworks work on device\n  if [[ \&quot;$(file \&quot;$binary\&quot;)\&quot; == *\&quot;dynamically linked shared library\&quot;* ]]; then\n    strip_invalid_archs \&quot;$binary\&quot;\n  fi\n\n  # Resign the code if required by the build settings to avoid unstable apps\n  code_sign_if_enabled \&quot;${destination}/$(basename \&quot;$1\&quot;)\&quot;\n\n  # Embed linked Swift runtime libraries. No longer necessary as of Xcode 7.\n  if [ \&quot;${XCODE_VERSION_MAJOR}\&quot; -lt 7 ]; then\n    local swift_runtime_libs\n    swift_runtime_libs=$(xcrun otool -LX \&quot;$binary\&quot; | grep --color=never @rpath/libswift | sed -E s/@rpath\\\\/\\(.+dylib\\).*/\\\\1/g | uniq -u  &amp;&amp; exit ${PIPESTATUS[0]})\n    for lib in $swift_runtime_libs; do\n      echo \&quot;rsync -auv \\\&quot;${SWIFT_STDLIB_PATH}/${lib}\\\&quot; \\\&quot;${destination}\\\&quot;\&quot;\n      rsync -auv \&quot;${SWIFT_STDLIB_PATH}/${lib}\&quot; \&quot;${destination}\&quot;\n      code_sign_if_enabled \&quot;${destination}/${lib}\&quot;\n    done\n  fi\n}\n\n# Signs a framework with the provided identity\ncode_sign_if_enabled() {\n  if [ -n \&quot;${EXPANDED_CODE_SIGN_IDENTITY}\&quot; -a \&quot;${CODE_SIGNING_REQUIRED}\&quot; != \&quot;NO\&quot; -a \&quot;${CODE_SIGNING_ALLOWED}\&quot; != \&quot;NO\&quot; ]; then\n    # Use the current code_sign_identitiy\n    echo \&quot;Code Signing $1 with Identity ${EXPANDED_CODE_SIGN_IDENTITY_NAME}\&quot;\n    local code_sign_cmd=\&quot;/usr/bin/codesign --force --sign ${EXPANDED_CODE_SIGN_IDENTITY} ${OTHER_CODE_SIGN_FLAGS} --preserve-metadata=identifier,entitlements '$1'\&quot;\n\n    if [ \&quot;${COCOAPODS_PARALLEL_CODE_SIGN}\&quot; == \&quot;true\&quot; ]; then\n      code_sign_cmd=\&quot;$code_sign_cmd &amp;\&quot;\n    fi\n    echo \&quot;$code_sign_cmd\&quot;\n    eval \&quot;$code_sign_cmd\&quot;\n  fi\n}\n\n# Strip invalid architectures\nstrip_invalid_archs() {\n  binary=\&quot;$1\&quot;\n  # Get architectures for current file\n  archs=\&quot;$(lipo -info \&quot;$binary\&quot; | rev | cut -d ':' -f1 | rev)\&quot;\n  stripped=\&quot;\&quot;\n  for arch in $archs; do\n    if ! [[ \&quot;${VALID_ARCHS}\&quot; == *\&quot;$arch\&quot;* ]]; then\n      # Strip non-valid architectures in-place\n      lipo -remove \&quot;$arch\&quot; -output \&quot;$binary\&quot; \&quot;$binary\&quot; || exit 1\n      stripped=\&quot;$stripped $arch\&quot;\n    fi\n  done\n  if [[ \&quot;$stripped\&quot; ]]; then\n    echo \&quot;Stripped $binary of architectures:$stripped\&quot;\n  fi\n}\n\n\nif [[ \&quot;$CONFIGURATION\&quot; == \&quot;Debug\&quot; ]]; then\n  install_framework \&quot;$BUILT_PRODUCTS_DIR/MyFirstCocoaPod/MyFirstCocoaPod.framework\&quot;\nfi\nif [[ \&quot;$CONFIGURATION\&quot; == \&quot;Release\&quot; ]]; then\n  install_framework \&quot;$BUILT_PRODUCTS_DIR/MyFirstCocoaPod/MyFirstCocoaPod.framework\&quot;\nfi\nif [ \&quot;${COCOAPODS_PARALLEL_CODE_SIGN}\&quot; == \&quot;true\&quot; ]; then\n  wait\nfi\n&quot;}}},{&quot;rowIdx&quot;:29,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;. inc/common.sh\n\nif ! $XB_BIN --help 2>&amp;1 | grep -q debug-sync; then\n    echo \&quot;Requires --debug-sync support\&quot; > $SKIPPED_REASON\n    exit $SKIPPED_EXIT_CODE\nfi\n\nstart_server --innodb_log_file_size=1M --innodb_thread_concurrency=1 \\\n    --innodb_log_buffer_size=1M\n\nload_dbase_schema sakila\nload_dbase_data sakila\nmkdir $topdir/backup\n\nrun_cmd_expect_failure $XB_BIN $XB_ARGS --datadir=$mysql_datadir --backup \\\n    --innodb_log_file_size=1M --target-dir=$topdir/backup \\\n    --debug-sync=\&quot;xtrabackup_copy_logfile_pause\&quot; &amp;\n\njob_pid=$!\n\npid_file=$topdir/backup/xtrabackup_debug_sync\n\n# Wait for xtrabackup to suspend\ni=0\nwhile [ ! -r \&quot;$pid_file\&quot; ]\ndo\n    sleep 1\n    i=$((i+1))\n    echo \&quot;Waited $i seconds for $pid_file to be created\&quot;\ndone\n\nxb_pid=`cat $pid_file`\n\n# Create 4M+ of log data\n\n$MYSQL $MYSQL_ARGS -Ns -e \&quot;CREATE TABLE tmp1 ENGINE=InnoDB SELECT * FROM payment\&quot; sakila\n$MYSQL $MYSQL_ARGS -Ns -e \&quot;CREATE TABLE tmp2 ENGINE=InnoDB SELECT * FROM payment\&quot; sakila\n$MYSQL $MYSQL_ARGS -Ns -e \&quot;CREATE TABLE tmp3 ENGINE=InnoDB SELECT * FROM payment\&quot; sakila\n\n# Resume the xtrabackup process\nvlog \&quot;Resuming xtrabackup\&quot;\nkill -SIGCONT $xb_pid\n\n# wait's return code will be the code returned by the background process\nrun_cmd wait $job_pid\n&quot;}}},{&quot;rowIdx&quot;:30,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/sh\n\n# \n# Vivado(TM)\n# runme.sh: a Vivado-generated Runs Script for UNIX\n# Copyright 1986-2020 Xilinx, Inc. All Rights Reserved.\n# \n\nif [ -z \&quot;$PATH\&quot; ]; then\n  PATH=/home/varun/tools/XilinX/Vitis/2020.2/bin:/home/varun/tools/XilinX/Vivado/2020.2/ids_lite/ISE/bin/lin64:/home/varun/tools/XilinX/Vivado/2020.2/bin\nelse\n  PATH=/home/varun/tools/XilinX/Vitis/2020.2/bin:/home/varun/tools/XilinX/Vivado/2020.2/ids_lite/ISE/bin/lin64:/home/varun/tools/XilinX/Vivado/2020.2/bin:$PATH\nfi\nexport PATH\n\nif [ -z \&quot;$LD_LIBRARY_PATH\&quot; ]; then\n  LD_LIBRARY_PATH=\nelse\n  LD_LIBRARY_PATH=:$LD_LIBRARY_PATH\nfi\nexport LD_LIBRARY_PATH\n\nHD_PWD='/home/varun/coding/fpga/xylinx/pynq_z1/mpsoc_only_pl_counter/mpsoc_only_pl_counter.runs/synth_1'\ncd \&quot;$HD_PWD\&quot;\n\nHD_LOG=runme.log\n/bin/touch $HD_LOG\n\nISEStep=\&quot;./ISEWrap.sh\&quot;\nEAStep()\n{\n     $ISEStep $HD_LOG \&quot;$@\&quot; >> $HD_LOG 2>&amp;1\n     if [ $? -ne 0 ]\n     then\n         exit\n     fi\n}\n\nEAStep vivado -log counter.vds -m64 -product Vivado -mode batch -messageDb vivado.pb -notrace -source counter.tcl\n&quot;}}},{&quot;rowIdx&quot;:31,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\nif [[ $target_platform =~ linux.* ]] || [[ $target_platform == win-32 ]] || [[ $target_platform == win-64 ]] || [[ $target_platform == osx-64 ]]; then\n  export DISABLE_AUTOBREW=1\n  $R CMD INSTALL --build .\nelse\n  mkdir -p $PREFIX/lib/R/library/rgbif\n  mv * $PREFIX/lib/R/library/rgbif\n  if [[ $target_platform == osx-64 ]]; then\n    pushd $PREFIX\n      for libdir in lib/R/lib lib/R/modules lib/R/library lib/R/bin/exec sysroot/usr/lib; do\n        pushd $libdir || exit 1\n          for SHARED_LIB in $(find . -type f -iname \&quot;*.dylib\&quot; -or -iname \&quot;*.so\&quot; -or -iname \&quot;R\&quot;); do\n            echo \&quot;fixing SHARED_LIB $SHARED_LIB\&quot;\n            install_name_tool -change /Library/Frameworks/R.framework/Versions/3.5.0-MRO/Resources/lib/libR.dylib \&quot;$PREFIX\&quot;/lib/R/lib/libR.dylib $SHARED_LIB || true\n            install_name_tool -change /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libR.dylib \&quot;$PREFIX\&quot;/lib/R/lib/libR.dylib $SHARED_LIB || true\n            install_name_tool -change /usr/local/clang4/lib/libomp.dylib \&quot;$PREFIX\&quot;/lib/libomp.dylib $SHARED_LIB || true\n            install_name_tool -change /usr/local/gfortran/lib/libgfortran.3.dylib \&quot;$PREFIX\&quot;/lib/libgfortran.3.dylib $SHARED_LIB || true\n            install_name_tool -change /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libquadmath.0.dylib \&quot;$PREFIX\&quot;/lib/libquadmath.0.dylib $SHARED_LIB || true\n            install_name_tool -change /usr/local/gfortran/lib/libquadmath.0.dylib \&quot;$PREFIX\&quot;/lib/libquadmath.0.dylib $SHARED_LIB || true\n            install_name_tool -change /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libgfortran.3.dylib \&quot;$PREFIX\&quot;/lib/libgfortran.3.dylib $SHARED_LIB || true\n            install_name_tool -change /usr/lib/libgcc_s.1.dylib \&quot;$PREFIX\&quot;/lib/libgcc_s.1.dylib $SHARED_LIB || true\n            install_name_tool -change /usr/lib/libiconv.2.dylib \&quot;$PREFIX\&quot;/sysroot/usr/lib/libiconv.2.dylib $SHARED_LIB || true\n            install_name_tool -change /usr/lib/libncurses.5.4.dylib \&quot;$PREFIX\&quot;/sysroot/usr/lib/libncurses.5.4.dylib $SHARED_LIB || true\n            install_name_tool -change /usr/lib/libicucore.A.dylib \&quot;$PREFIX\&quot;/sysroot/usr/lib/libicucore.A.dylib $SHARED_LIB || true\n            install_name_tool -change /usr/lib/libexpat.1.dylib \&quot;$PREFIX\&quot;/lib/libexpat.1.dylib $SHARED_LIB || true\n            install_name_tool -change /usr/lib/libcurl.4.dylib \&quot;$PREFIX\&quot;/lib/libcurl.4.dylib $SHARED_LIB || true\n            install_name_tool -change /usr/lib/libc++.1.dylib \&quot;$PREFIX\&quot;/lib/libc++.1.dylib $SHARED_LIB || true\n            install_name_tool -change /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libc++.1.dylib \&quot;$PREFIX\&quot;/lib/libc++.1.dylib $SHARED_LIB || true\n          done\n        popd\n      done\n    popd\n  fi\nfi\n&quot;}}},{&quot;rowIdx&quot;:32,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/sh\n# Script which does some configuration of the system for laptops/desktops\n\ntest -e /sbin/rc-update\nuse_openrc=$?\n\nsetupZFSArc(){\n  # Tune ZFS ARC \n  ###############################################\n  grep -q \&quot;vfs.zfs.arc_max=\&quot; \&quot;/boot/loader.conf\&quot;\n  if [ $? -eq 0 ] ; then\n    return 0 #Do not overwrite current ARC settings\n  fi\n\n  # Get system memory in bytes\n  sysMem=`sysctl hw.physmem | cut -w -f 2`\n  # Get that in MB\n  sysMem=`expr $sysMem / 1024 / 1024`\n  # Set some default zArc sizes based upon RAM of system\n  if [ $sysMem -lt 1024 ] ; then\n    zArc=\&quot;128\&quot;\n  elif [ $sysMem -lt 4096 ] ; then\n    zArc=\&quot;256\&quot;\n  else\n    zArc=\&quot;512\&quot;\n  fi\n\n  echo \&quot;# Tune ZFS Arc Size - Change to adjust memory used for disk cache\&quot; >> /boot/loader.conf\n  echo \&quot;vfs.zfs.arc_max=\\\&quot;${zArc}M\\\&quot;\&quot; >> /boot/loader.conf\n}\n\nsetupPowerd(){\n  if [ ${use_openrc} -eq 0 ] ; then\n    rc-update | grep -q powerd\n  else\n    grep -q -E 'powerd(xx)?_enable=\&quot;YES\&quot;'\n  fi\n  if [ $? -eq 0 ] ; then\n    #one of the powerd[++] service is already setup\n    return\n  fi\n  p_service=\&quot;powerd\&quot;\n  if [ ${use_openrc} -eq 0 ] ; then\n    if [ -e \&quot;/usr/local/etc/init.d/powerd++\&quot; ] ; then\n      #The alternative powerd++ service is installed - use that instead\n      p_service=\&quot;powerd++\&quot;\n    fi\n    rc-update add ${p_service} default\n  else\n    if [ -e \&quot;/usr/local/etc/rc.d/powerdxx\&quot; ] ; then\n      p_service=\&quot;powerdxx\&quot;\n    fi\n    sysrc \&quot;${p_service}_enable=YES\&quot;\n  fi\n}\n\nsetupXProfile(){\n  local _script=\&quot;/usr/local/bin/setup-xorg-session\&quot;\n  # Check all the .xprofile files in the user home dirs\n  # And make sure they launch the x session setup script\n  for _hd in $(ls /usr/home)\n  do\n    if [ ! -e \&quot;/usr/home/${_hd}/.xprofile\&quot; ] ; then continue; fi\n    grep -q \&quot;${_script}\&quot; \&quot;/usr/home/${_hd}/.xprofile\&quot;\n    if [ $? -ne 0 ] ; then\n      echo \&quot;\nif [ -e \\\&quot;${_script}\\\&quot; ] ; then\n  . ${_script}\nfi\n\&quot; >> \&quot;/usr/home/${_hd}/.xprofile\&quot;\n    fi\n  done\n  #Now make sure the default ~/.xprofile exists and/or is setup\n  if [ ! -e \&quot;/usr/share/skel/dot.xprofile\&quot; ] ; then\n    echo \&quot;# Graphical session setup\n# Created by Project Trident\n# ===================\nif [ -e \\\&quot;${_script}\\\&quot; ] ; then\n  . ${_script}\nfi\n\&quot; >> \&quot;/usr/share/skel/dot.xprofile\&quot;\n\n  else\n    grep -q \&quot;${_script}\&quot; \&quot;/usr/share/skel/dot.xprofile\&quot;\n    if [ $? -ne 0 ] ; then\n      echo \&quot;\nif [ -e \\\&quot;${_script}\\\&quot; ] ; then\n  . ${_script}\nfi\n\&quot; >> \&quot;/usr/share/skel/dot.xprofile\&quot;\n    fi\n  fi\n}\n\nsetupWlan(){\n  # Check for any new wifi devices to setup\n  for wnic in `sysctl -n net.wlan.devices 2>/dev/null`\n  do\n    #See if this device is already configured\n    grep -q \&quot;wlans_${wnic}\&quot; /etc/rc.conf\n    if [ $? -ne 0 ] ; then\n      # New wifi device - determine the next number for it\n      grep -qE \&quot;^wlans_\&quot; /etc/rc.conf\n      if [ $? -eq 0 ] ; then\n        WLANCOUNT=`cat /etc/rc.conf | grep -E \&quot;^wlans_\&quot; | wc -l | awk '{print $1}'`\n      else\n        WLANCOUNT=\&quot;0\&quot;\n      fi\n      WLAN=\&quot;wlan${WLANCOUNT}\&quot;\n      # Save the wlan interface\n      echo \&quot;wlans_${wnic}=\\\&quot;${WLAN}\\\&quot;\&quot; >> /etc/rc.conf\n      echo \&quot;ifconfig_${WLAN}=\\\&quot;WPA DHCP\\\&quot;\&quot; >> /etc/rc.conf\n      echo \&quot;ifconfig_${WLAN}_ipv6=\\\&quot;inet6 accept_rtadv\\\&quot;\&quot; >> /etc/rc.conf\n    fi\n  done\n}\n\nsetupLan(){\n  for nic in `ifconfig -l`\n  do\n    #Ignore loopback devices\n    echo ${nic} | grep -qE \&quot;lo[0-9]\&quot;\n    if [ 0 -eq $? ] ; then continue; fi\n    #See if this device is already configured\n    sysrc -ci \&quot;ifconfig_${nic}\&quot;\n    if [ $? -ne 0 ] ; then\n      # New ethernet device\n      sysrc \&quot;ifconfig_${nic}=DHCP\&quot;\n      sysrc \&quot;ifconfig_${nic}_ipv6=inet6 accept_rtadv\&quot;\n    fi\n  done\n}\n\n#figure out if this is a laptop, desktop, or VM (VMWare or VirtualBox only at the moment)\npciconf -lv | grep -qiE \&quot;(vmware|innotek)\&quot;\nif [ $? -eq 0 ] ; then\n  type=\&quot;vm\&quot;\nelse\n  devinfo | grep -q acpi_acad0\n  if [ $? -eq 0 ] ; then\n    type=\&quot;laptop\&quot;\n  else\n    type=\&quot;desktop\&quot;\n  fi\nfi\n\n################################################\n# Verify generic init\n################################################\n\nif [ ! -d \&quot;/usr/home\&quot; ] ; then\n   mkdir /usr/home\nfi\n\n# Setup /home link (for people used to Linux, and some applications)\nif [ ! -e \&quot;/home\&quot; ] ; then\n  ln -s /usr/home /home\nfi\n\n#Check/set the ZFS arc size\nsetupZFSArc\n\n#Turn on power management service (if one is not already setup)\nif [ \&quot;type\&quot; != \&quot;vm\&quot; ] ; then\n  setupPowerd\nfi\n\nif [ \&quot;${type}\&quot; = \&quot;laptop\&quot; ] ; then\n  # Laptop system\n  # TO-DO  \nelse\n  # Desktop system\n  # TO-DO\nfi\n\n#setup the networking interfaces\nsetupLan\nsetupWlan\nsetupXProfile\n\n#Perform the system sanity check\n/usr/local/share/trident/scripts/system-sanity-check.sh\n\n\n#TrueOS 18.06-18.08 Bug Bypass (8/23/18 - Ken Moore)\n# - replace \&quot;DHCP\&quot; with \&quot;SYNCDHCP\&quot; in the default-installed /etc/rc.conf\n#sed -i '' 's|\&quot;DHCP|\&quot;SYNCDHCP|g' /etc/rc.conf\n#sed -i '' 's| DHCP\&quot;| SYNCDHCP\&quot;|g' /etc/rc.conf\n\n#Now ensure the system services are all setup properly\n/usr/local/share/trident/scripts/validate-services.sh /usr/local/etc/trident/required-services /usr/local/etc/trident/recommended-services\n&quot;}}},{&quot;rowIdx&quot;:33,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# bash/zsh git prompt support\n#\n# Copyright (C) 2006,2007 Shawn O. Pearce <spearce@spearce.org>\n# Distributed under the GNU General Public License, version 2.0.\n#\n# This script allows you to see repository status in your prompt.\n#\n# To enable:\n#\n#    1) Copy this file to somewhere (e.g. ~/.git-prompt.sh).\n#    2) Add the following line to your .bashrc/.zshrc:\n#        source ~/.git-prompt.sh\n#    3a) Change your PS1 to call __git_ps1 as\n#        command-substitution:\n#        Bash: PS1='[\\u@\\h \\W$(__git_ps1 \&quot; (%s)\&quot;)]\\$ '\n#        ZSH:  setopt PROMPT_SUBST ; PS1='[%n@%m %c$(__git_ps1 \&quot; (%s)\&quot;)]\\$ '\n#        the optional argument will be used as format string.\n#    3b) Alternatively, for a slightly faster prompt, __git_ps1 can\n#        be used for PROMPT_COMMAND in Bash or for precmd() in Zsh\n#        with two parameters, <pre> and <post>, which are strings\n#        you would put in $PS1 before and after the status string\n#        generated by the git-prompt machinery.  e.g.\n#        Bash: PROMPT_COMMAND='__git_ps1 \&quot;\\u@\\h:\\w\&quot; \&quot;\\\\\\$ \&quot;'\n#          will show username, at-sign, host, colon, cwd, then\n#          various status string, followed by dollar and SP, as\n#          your prompt.\n#        ZSH:  precmd () { __git_ps1 \&quot;%n\&quot; \&quot;:%~$ \&quot; \&quot;|%s\&quot; }\n#          will show username, pipe, then various status string,\n#          followed by colon, cwd, dollar and SP, as your prompt.\n#        Optionally, you can supply a third argument with a printf\n#        format string to finetune the output of the branch status\n#\n# The repository status will be displayed only if you are currently in a\n# git repository. The %s token is the placeholder for the shown status.\n#\n# The prompt status always includes the current branch name.\n#\n# In addition, if you set GIT_PS1_SHOWDIRTYSTATE to a nonempty value,\n# unstaged (*) and staged (+) changes will be shown next to the branch\n# name.  You can configure this per-repository with the\n# bash.showDirtyState variable, which defaults to true once\n# GIT_PS1_SHOWDIRTYSTATE is enabled.\n#\n# You can also see if currently something is stashed, by setting\n# GIT_PS1_SHOWSTASHSTATE to a nonempty value. If something is stashed,\n# then a '$' will be shown next to the branch name.\n#\n# If you would like to see if there're untracked files, then you can set\n# GIT_PS1_SHOWUNTRACKEDFILES to a nonempty value. If there're untracked\n# files, then a '%' will be shown next to the branch name.  You can\n# configure this per-repository with the bash.showUntrackedFiles\n# variable, which defaults to true once GIT_PS1_SHOWUNTRACKEDFILES is\n# enabled.\n#\n# If you would like to see the difference between HEAD and its upstream,\n# set GIT_PS1_SHOWUPSTREAM=\&quot;auto\&quot;.  A \&quot;<\&quot; indicates you are behind, \&quot;>\&quot;\n# indicates you are ahead, \&quot;<>\&quot; indicates you have diverged and \&quot;=\&quot;\n# indicates that there is no difference. You can further control\n# behaviour by setting GIT_PS1_SHOWUPSTREAM to a space-separated list\n# of values:\n#\n#     verbose       show number of commits ahead/behind (+/-) upstream\n#     name          if verbose, then also show the upstream abbrev name\n#     legacy        don't use the '--count' option available in recent\n#                   versions of git-rev-list\n#     git           always compare HEAD to @{upstream}\n#     svn           always compare HEAD to your SVN upstream\n#\n# You can change the separator between the branch name and the above\n# state symbols by setting GIT_PS1_STATESEPARATOR. The default separator\n# is SP.\n#\n# By default, __git_ps1 will compare HEAD to your SVN upstream if it can\n# find one, or @{upstream} otherwise.  Once you have set\n# GIT_PS1_SHOWUPSTREAM, you can override it on a per-repository basis by\n# setting the bash.showUpstream config variable.\n#\n# If you would like to see more information about the identity of\n# commits checked out as a detached HEAD, set GIT_PS1_DESCRIBE_STYLE\n# to one of these values:\n#\n#     contains      relative to newer annotated tag (v1.6.3.2~35)\n#     branch        relative to newer tag or branch (master~4)\n#     describe      relative to older annotated tag (v1.6.3.1-13-gdd42c2f)\n#     tag           relative to any older tag (v1.6.3.1-13-gdd42c2f)\n#     default       exactly matching tag\n#\n# If you would like a colored hint about the current dirty state, set\n# GIT_PS1_SHOWCOLORHINTS to a nonempty value. The colors are based on\n# the colored output of \&quot;git status -sb\&quot; and are available only when\n# using __git_ps1 for PROMPT_COMMAND or precmd.\n#\n# If you would like __git_ps1 to do nothing in the case when the current\n# directory is set up to be ignored by git, then set\n# GIT_PS1_HIDE_IF_PWD_IGNORED to a nonempty value. Override this on the\n# repository level by setting bash.hideIfPwdIgnored to \&quot;false\&quot;.\n\n# check whether printf supports -v\n__git_printf_supports_v=\nprintf -v __git_printf_supports_v -- '%s' yes >/dev/null 2>&amp;1\n\n# stores the divergence from upstream in $p\n# used by GIT_PS1_SHOWUPSTREAM\n__git_ps1_show_upstream ()\n{\n\tlocal key value\n\tlocal svn_remote svn_url_pattern count n\n\tlocal upstream=git legacy=\&quot;\&quot; verbose=\&quot;\&quot; name=\&quot;\&quot;\n\n\tsvn_remote=()\n\t# get some config options from git-config\n\tlocal output=\&quot;$(git config -z --get-regexp '^(svn-remote\\..*\\.url|bash\\.showupstream)$' 2>/dev/null | tr '\\0\\n' '\\n ')\&quot;\n\twhile read -r key value; do\n\t\tcase \&quot;$key\&quot; in\n\t\tbash.showupstream)\n\t\t\tGIT_PS1_SHOWUPSTREAM=\&quot;$value\&quot;\n\t\t\tif [[ -z \&quot;${GIT_PS1_SHOWUPSTREAM}\&quot; ]]; then\n\t\t\t\tp=\&quot;\&quot;\n\t\t\t\treturn\n\t\t\tfi\n\t\t\t;;\n\t\tsvn-remote.*.url)\n\t\t\tsvn_remote[$((${#svn_remote[@]} + 1))]=\&quot;$value\&quot;\n\t\t\tsvn_url_pattern=\&quot;$svn_url_pattern\\\\|$value\&quot;\n\t\t\tupstream=svn+git # default upstream is SVN if available, else git\n\t\t\t;;\n\t\tesac\n\tdone <<< \&quot;$output\&quot;\n\n\t# parse configuration values\n\tfor option in ${GIT_PS1_SHOWUPSTREAM}; do\n\t\tcase \&quot;$option\&quot; in\n\t\tgit|svn) upstream=\&quot;$option\&quot; ;;\n\t\tverbose) verbose=1 ;;\n\t\tlegacy)  legacy=1  ;;\n\t\tname)    name=1 ;;\n\t\tesac\n\tdone\n\n\t# Find our upstream\n\tcase \&quot;$upstream\&quot; in\n\tgit)    upstream=\&quot;@{upstream}\&quot; ;;\n\tsvn*)\n\t\t# get the upstream from the \&quot;git-svn-id: ...\&quot; in a commit message\n\t\t# (git-svn uses essentially the same procedure internally)\n\t\tlocal -a svn_upstream\n\t\tsvn_upstream=($(git log --first-parent -1 \\\n\t\t\t\t\t--grep=\&quot;^git-svn-id: \\(${svn_url_pattern#??}\\)\&quot; 2>/dev/null))\n\t\tif [[ 0 -ne ${#svn_upstream[@]} ]]; then\n\t\t\tsvn_upstream=${svn_upstream[${#svn_upstream[@]} - 2]}\n\t\t\tsvn_upstream=${svn_upstream%@*}\n\t\t\tlocal n_stop=\&quot;${#svn_remote[@]}\&quot;\n\t\t\tfor ((n=1; n <= n_stop; n++)); do\n\t\t\t\tsvn_upstream=${svn_upstream#${svn_remote[$n]}}\n\t\t\tdone\n\n\t\t\tif [[ -z \&quot;$svn_upstream\&quot; ]]; then\n\t\t\t\t# default branch name for checkouts with no layout:\n\t\t\t\tupstream=${GIT_SVN_ID:-git-svn}\n\t\t\telse\n\t\t\t\tupstream=${svn_upstream#/}\n\t\t\tfi\n\t\telif [[ \&quot;svn+git\&quot; = \&quot;$upstream\&quot; ]]; then\n\t\t\tupstream=\&quot;@{upstream}\&quot;\n\t\tfi\n\t\t;;\n\tesac\n\n\t# Find how many commits we are ahead/behind our upstream\n\tif [[ -z \&quot;$legacy\&quot; ]]; then\n\t\tcount=\&quot;$(git rev-list --count --left-right \\\n\t\t\t\t\&quot;$upstream\&quot;...HEAD 2>/dev/null)\&quot;\n\telse\n\t\t# produce equivalent output to --count for older versions of git\n\t\tlocal commits\n\t\tif commits=\&quot;$(git rev-list --left-right \&quot;$upstream\&quot;...HEAD 2>/dev/null)\&quot;\n\t\tthen\n\t\t\tlocal commit behind=0 ahead=0\n\t\t\tfor commit in $commits\n\t\t\tdo\n\t\t\t\tcase \&quot;$commit\&quot; in\n\t\t\t\t\&quot;<\&quot;*) ((behind++)) ;;\n\t\t\t\t*)    ((ahead++))  ;;\n\t\t\t\tesac\n\t\t\tdone\n\t\t\tcount=\&quot;$behind\t$ahead\&quot;\n\t\telse\n\t\t\tcount=\&quot;\&quot;\n\t\tfi\n\tfi\n\n\t# calculate the result\n\tif [[ -z \&quot;$verbose\&quot; ]]; then\n\t\tcase \&quot;$count\&quot; in\n\t\t\&quot;\&quot;) # no upstream\n\t\t\tp=\&quot;\&quot; ;;\n\t\t\&quot;0\t0\&quot;) # equal to upstream\n\t\t\tp=\&quot;=\&quot; ;;\n\t\t\&quot;0\t\&quot;*) # ahead of upstream\n\t\t\tp=\&quot;>\&quot; ;;\n\t\t*\&quot;\t0\&quot;) # behind upstream\n\t\t\tp=\&quot;<\&quot; ;;\n\t\t*)\t    # diverged from upstream\n\t\t\tp=\&quot;<>\&quot; ;;\n\t\tesac\n\telse\n\t\tcase \&quot;$count\&quot; in\n\t\t\&quot;\&quot;) # no upstream\n\t\t\tp=\&quot;\&quot; ;;\n\t\t\&quot;0\t0\&quot;) # equal to upstream\n\t\t\tp=\&quot; u=\&quot; ;;\n\t\t\&quot;0\t\&quot;*) # ahead of upstream\n\t\t\tp=\&quot; u+${count#0\t}\&quot; ;;\n\t\t*\&quot;\t0\&quot;) # behind upstream\n\t\t\tp=\&quot; u-${count%\t0}\&quot; ;;\n\t\t*)\t    # diverged from upstream\n\t\t\tp=\&quot; u+${count#*\t}-${count%\t*}\&quot; ;;\n\t\tesac\n\t\tif [[ -n \&quot;$count\&quot; &amp;&amp; -n \&quot;$name\&quot; ]]; then\n\t\t\t__git_ps1_upstream_name=$(git rev-parse \\\n\t\t\t\t--abbrev-ref \&quot;$upstream\&quot; 2>/dev/null)\n\t\t\tif [ \&quot;$pcmode\&quot; = yes ] &amp;&amp; [ \&quot;$ps1_expanded\&quot; = yes ]; then\n\t\t\t\tp=\&quot;$p \\${__git_ps1_upstream_name}\&quot;\n\t\t\telse\n\t\t\t\tp=\&quot;$p ${__git_ps1_upstream_name}\&quot;\n\t\t\t\t# not needed anymore; keep user's\n\t\t\t\t# environment clean\n\t\t\t\tunset __git_ps1_upstream_name\n\t\t\tfi\n\t\tfi\n\tfi\n\n}\n\n# Helper function that is meant to be called from __git_ps1.  It\n# injects color codes into the appropriate gitstring variables used\n# to build a gitstring.\n__git_ps1_colorize_gitstring ()\n{\n\tif [[ -n \&quot;${ZSH_VERSION-}\&quot; ]]; then\n\t\tlocal c_red='%F{red}'\n\t\tlocal c_green='%F{green}'\n\t\tlocal c_lblue='%F{blue}'\n\t\tlocal c_clear='%f'\n\telse\n\t\t# Using \\[ and \\] around colors is necessary to prevent\n\t\t# issues with command line editing/browsing/completion!\n\t\tlocal c_red='\\[\\e[31m\\]'\n\t\tlocal c_green='\\[\\e[32m\\]'\n\t\tlocal c_lblue='\\[\\e[1;34m\\]'\n\t\tlocal c_clear='\\[\\e[0m\\]'\n\tfi\n\tlocal bad_color=$c_red\n\tlocal ok_color=$c_green\n\tlocal flags_color=\&quot;$c_lblue\&quot;\n\n\tlocal branch_color=\&quot;\&quot;\n\tif [ \&quot;$detached\&quot; = no ]; then\n\t\tbranch_color=\&quot;$ok_color\&quot;\n\telse\n\t\tbranch_color=\&quot;$bad_color\&quot;\n\tfi\n\tc=\&quot;$branch_color$c\&quot;\n\n\tz=\&quot;$c_clear$z\&quot;\n\tif [ \&quot;$w\&quot; = \&quot;*\&quot; ]; then\n\t\tw=\&quot;$bad_color$w\&quot;\n\tfi\n\tif [ -n \&quot;$i\&quot; ]; then\n\t\ti=\&quot;$ok_color$i\&quot;\n\tfi\n\tif [ -n \&quot;$s\&quot; ]; then\n\t\ts=\&quot;$flags_color$s\&quot;\n\tfi\n\tif [ -n \&quot;$u\&quot; ]; then\n\t\tu=\&quot;$bad_color$u\&quot;\n\tfi\n\tr=\&quot;$c_clear$r\&quot;\n}\n\n# Helper function to read the first line of a file into a variable.\n# __git_eread requires 2 arguments, the file path and the name of the\n# variable, in that order.\n__git_eread ()\n{\n\ttest -r \&quot;$1\&quot; &amp;&amp; IFS=$'\\r\\n' read \&quot;$2\&quot; <\&quot;$1\&quot;\n}\n\n# __git_ps1 accepts 0 or 1 arguments (i.e., format string)\n# when called from PS1 using command substitution\n# in this mode it prints text to add to bash PS1 prompt (includes branch name)\n#\n# __git_ps1 requires 2 or 3 arguments when called from PROMPT_COMMAND (pc)\n# in that case it _sets_ PS1. The arguments are parts of a PS1 string.\n# when two arguments are given, the first is prepended and the second appended\n# to the state string when assigned to PS1.\n# The optional third parameter will be used as printf format string to further\n# customize the output of the git-status string.\n# In this mode you can request colored hints using GIT_PS1_SHOWCOLORHINTS=true\n__git_ps1 ()\n{\n\t# preserve exit status\n\tlocal exit=$?\n\tlocal pcmode=no\n\tlocal detached=no\n\tlocal ps1pc_start='\\u@\\h:\\w '\n\tlocal ps1pc_end='\\$ '\n\tlocal printf_format=' (%s)'\n\n\tcase \&quot;$#\&quot; in\n\t\t2|3)\tpcmode=yes\n\t\t\tps1pc_start=\&quot;$1\&quot;\n\t\t\tps1pc_end=\&quot;$2\&quot;\n\t\t\tprintf_format=\&quot;${3:-$printf_format}\&quot;\n\t\t\t# set PS1 to a plain prompt so that we can\n\t\t\t# simply return early if the prompt should not\n\t\t\t# be decorated\n\t\t\tPS1=\&quot;$ps1pc_start$ps1pc_end\&quot;\n\t\t;;\n\t\t0|1)\tprintf_format=\&quot;${1:-$printf_format}\&quot;\n\t\t;;\n\t\t*)\treturn $exit\n\t\t;;\n\tesac\n\n\t# ps1_expanded:  This variable is set to 'yes' if the shell\n\t# subjects the value of PS1 to parameter expansion:\n\t#\n\t#   * bash does unless the promptvars option is disabled\n\t#   * zsh does not unless the PROMPT_SUBST option is set\n\t#   * POSIX shells always do\n\t#\n\t# If the shell would expand the contents of PS1 when drawing\n\t# the prompt, a raw ref name must not be included in PS1.\n\t# This protects the user from arbitrary code execution via\n\t# specially crafted ref names.  For example, a ref named\n\t# 'refs/heads/$(IFS=_;cmd=sudo_rm_-rf_/;$cmd)' might cause the\n\t# shell to execute 'sudo rm -rf /' when the prompt is drawn.\n\t#\n\t# Instead, the ref name should be placed in a separate global\n\t# variable (in the __git_ps1_* namespace to avoid colliding\n\t# with the user's environment) and that variable should be\n\t# referenced from PS1.  For example:\n\t#\n\t#     __git_ps1_foo=$(do_something_to_get_ref_name)\n\t#     PS1=\&quot;...stuff...\\${__git_ps1_foo}...stuff...\&quot;\n\t#\n\t# If the shell does not expand the contents of PS1, the raw\n\t# ref name must be included in PS1.\n\t#\n\t# The value of this variable is only relevant when in pcmode.\n\t#\n\t# Assume that the shell follows the POSIX specification and\n\t# expands PS1 unless determined otherwise.  (This is more\n\t# likely to be correct if the user has a non-bash, non-zsh\n\t# shell and safer than the alternative if the assumption is\n\t# incorrect.)\n\t#\n\tlocal ps1_expanded=yes\n\t[ -z \&quot;${ZSH_VERSION-}\&quot; ] || [[ -o PROMPT_SUBST ]] || ps1_expanded=no\n\t[ -z \&quot;${BASH_VERSION-}\&quot; ] || shopt -q promptvars || ps1_expanded=no\n\n\tlocal repo_info rev_parse_exit_code\n\trepo_info=\&quot;$(git rev-parse --git-dir --is-inside-git-dir \\\n\t\t--is-bare-repository --is-inside-work-tree \\\n\t\t--short HEAD 2>/dev/null)\&quot;\n\trev_parse_exit_code=\&quot;$?\&quot;\n\n\tif [ -z \&quot;$repo_info\&quot; ]; then\n\t\treturn $exit\n\tfi\n\n\tlocal short_sha=\&quot;\&quot;\n\tif [ \&quot;$rev_parse_exit_code\&quot; = \&quot;0\&quot; ]; then\n\t\tshort_sha=\&quot;${repo_info##*$'\\n'}\&quot;\n\t\trepo_info=\&quot;${repo_info%$'\\n'*}\&quot;\n\tfi\n\tlocal inside_worktree=\&quot;${repo_info##*$'\\n'}\&quot;\n\trepo_info=\&quot;${repo_info%$'\\n'*}\&quot;\n\tlocal bare_repo=\&quot;${repo_info##*$'\\n'}\&quot;\n\trepo_info=\&quot;${repo_info%$'\\n'*}\&quot;\n\tlocal inside_gitdir=\&quot;${repo_info##*$'\\n'}\&quot;\n\tlocal g=\&quot;${repo_info%$'\\n'*}\&quot;\n\n\tif [ \&quot;true\&quot; = \&quot;$inside_worktree\&quot; ] &amp;&amp;\n\t   [ -n \&quot;${GIT_PS1_HIDE_IF_PWD_IGNORED-}\&quot; ] &amp;&amp;\n\t   [ \&quot;$(git config --bool bash.hideIfPwdIgnored)\&quot; != \&quot;false\&quot; ] &amp;&amp;\n\t   git check-ignore -q .\n\tthen\n\t\treturn $exit\n\tfi\n\n\tlocal r=\&quot;\&quot;\n\tlocal b=\&quot;\&quot;\n\tlocal step=\&quot;\&quot;\n\tlocal total=\&quot;\&quot;\n\tif [ -d \&quot;$g/rebase-merge\&quot; ]; then\n\t\t__git_eread \&quot;$g/rebase-merge/head-name\&quot; b\n\t\t__git_eread \&quot;$g/rebase-merge/msgnum\&quot; step\n\t\t__git_eread \&quot;$g/rebase-merge/end\&quot; total\n\t\tif [ -f \&quot;$g/rebase-merge/interactive\&quot; ]; then\n\t\t\tr=\&quot;|REBASE-i\&quot;\n\t\telse\n\t\t\tr=\&quot;|REBASE-m\&quot;\n\t\tfi\n\telse\n\t\tif [ -d \&quot;$g/rebase-apply\&quot; ]; then\n\t\t\t__git_eread \&quot;$g/rebase-apply/next\&quot; step\n\t\t\t__git_eread \&quot;$g/rebase-apply/last\&quot; total\n\t\t\tif [ -f \&quot;$g/rebase-apply/rebasing\&quot; ]; then\n\t\t\t\t__git_eread \&quot;$g/rebase-apply/head-name\&quot; b\n\t\t\t\tr=\&quot;|REBASE\&quot;\n\t\t\telif [ -f \&quot;$g/rebase-apply/applying\&quot; ]; then\n\t\t\t\tr=\&quot;|AM\&quot;\n\t\t\telse\n\t\t\t\tr=\&quot;|AM/REBASE\&quot;\n\t\t\tfi\n\t\telif [ -f \&quot;$g/MERGE_HEAD\&quot; ]; then\n\t\t\tr=\&quot;|MERGING\&quot;\n\t\telif [ -f \&quot;$g/CHERRY_PICK_HEAD\&quot; ]; then\n\t\t\tr=\&quot;|CHERRY-PICKING\&quot;\n\t\telif [ -f \&quot;$g/REVERT_HEAD\&quot; ]; then\n\t\t\tr=\&quot;|REVERTING\&quot;\n\t\telif [ -f \&quot;$g/BISECT_LOG\&quot; ]; then\n\t\t\tr=\&quot;|BISECTING\&quot;\n\t\tfi\n\n\t\tif [ -n \&quot;$b\&quot; ]; then\n\t\t\t:\n\t\telif [ -h \&quot;$g/HEAD\&quot; ]; then\n\t\t\t# symlink symbolic ref\n\t\t\tb=\&quot;$(git symbolic-ref HEAD 2>/dev/null)\&quot;\n\t\telse\n\t\t\tlocal head=\&quot;\&quot;\n\t\t\tif ! __git_eread \&quot;$g/HEAD\&quot; head; then\n\t\t\t\treturn $exit\n\t\t\tfi\n\t\t\t# is it a symbolic ref?\n\t\t\tb=\&quot;${head#ref: }\&quot;\n\t\t\tif [ \&quot;$head\&quot; = \&quot;$b\&quot; ]; then\n\t\t\t\tdetached=yes\n\t\t\t\tb=\&quot;$(\n\t\t\t\tcase \&quot;${GIT_PS1_DESCRIBE_STYLE-}\&quot; in\n\t\t\t\t(contains)\n\t\t\t\t\tgit describe --contains HEAD ;;\n\t\t\t\t(branch)\n\t\t\t\t\tgit describe --contains --all HEAD ;;\n\t\t\t\t(tag)\n\t\t\t\t\tgit describe --tags HEAD ;;\n\t\t\t\t(describe)\n\t\t\t\t\tgit describe HEAD ;;\n\t\t\t\t(* | default)\n\t\t\t\t\tgit describe --tags --exact-match HEAD ;;\n\t\t\t\tesac 2>/dev/null)\&quot; ||\n\n\t\t\t\tb=\&quot;$short_sha...\&quot;\n\t\t\t\tb=\&quot;($b)\&quot;\n\t\t\tfi\n\t\tfi\n\tfi\n\n\tif [ -n \&quot;$step\&quot; ] &amp;&amp; [ -n \&quot;$total\&quot; ]; then\n\t\tr=\&quot;$r $step/$total\&quot;\n\tfi\n\n\tlocal w=\&quot;\&quot;\n\tlocal i=\&quot;\&quot;\n\tlocal s=\&quot;\&quot;\n\tlocal u=\&quot;\&quot;\n\tlocal c=\&quot;\&quot;\n\tlocal p=\&quot;\&quot;\n\n\tif [ \&quot;true\&quot; = \&quot;$inside_gitdir\&quot; ]; then\n\t\tif [ \&quot;true\&quot; = \&quot;$bare_repo\&quot; ]; then\n\t\t\tc=\&quot;BARE:\&quot;\n\t\telse\n\t\t\tb=\&quot;GIT_DIR!\&quot;\n\t\tfi\n\telif [ \&quot;true\&quot; = \&quot;$inside_worktree\&quot; ]; then\n\t\tif [ -n \&quot;${GIT_PS1_SHOWDIRTYSTATE-}\&quot; ] &amp;&amp;\n\t\t   [ \&quot;$(git config --bool bash.showDirtyState)\&quot; != \&quot;false\&quot; ]\n\t\tthen\n\t\t\tgit diff --no-ext-diff --quiet || w=\&quot;*\&quot;\n\t\t\tgit diff --no-ext-diff --cached --quiet || i=\&quot;+\&quot;\n\t\t\tif [ -z \&quot;$short_sha\&quot; ] &amp;&amp; [ -z \&quot;$i\&quot; ]; then\n\t\t\t\ti=\&quot;#\&quot;\n\t\t\tfi\n\t\tfi\n\t\tif [ -n \&quot;${GIT_PS1_SHOWSTASHSTATE-}\&quot; ] &amp;&amp;\n\t\t   git rev-parse --verify --quiet refs/stash >/dev/null\n\t\tthen\n\t\t\ts=\&quot;$\&quot;\n\t\tfi\n\n\t\tif [ -n \&quot;${GIT_PS1_SHOWUNTRACKEDFILES-}\&quot; ] &amp;&amp;\n\t\t   [ \&quot;$(git config --bool bash.showUntrackedFiles)\&quot; != \&quot;false\&quot; ] &amp;&amp;\n\t\t   git ls-files --others --exclude-standard --directory --no-empty-directory --error-unmatch -- ':/*' >/dev/null 2>/dev/null\n\t\tthen\n\t\t\tu=\&quot;%${ZSH_VERSION+%}\&quot;\n\t\tfi\n\n\t\tif [ -n \&quot;${GIT_PS1_SHOWUPSTREAM-}\&quot; ]; then\n\t\t\t__git_ps1_show_upstream\n\t\tfi\n\tfi\n\n\tlocal z=\&quot;${GIT_PS1_STATESEPARATOR-\&quot; \&quot;}\&quot;\n\n\t# NO color option unless in PROMPT_COMMAND mode or it's Zsh\n\tif [ -n \&quot;${GIT_PS1_SHOWCOLORHINTS-}\&quot; ]; then\n\t\tif [ \&quot;$pcmode\&quot; = yes ] || [ -n \&quot;${ZSH_VERSION-}\&quot; ]; then\n\t\t\t__git_ps1_colorize_gitstring\n\t\tfi\n\tfi\n\n\tb=${b##refs/heads/}\n\tif [ \&quot;$pcmode\&quot; = yes ] &amp;&amp; [ \&quot;$ps1_expanded\&quot; = yes ]; then\n\t\t__git_ps1_branch_name=$b\n\t\tb=\&quot;\\${__git_ps1_branch_name}\&quot;\n\tfi\n\n\tlocal f=\&quot;$w$i$s$u\&quot;\n\tlocal gitstring=\&quot;$c$b${f:+$z$f}$r$p\&quot;\n\n\tif [ \&quot;$pcmode\&quot; = yes ]; then\n\t\tif [ \&quot;${__git_printf_supports_v-}\&quot; != yes ]; then\n\t\t\tgitstring=$(printf -- \&quot;$printf_format\&quot; \&quot;$gitstring\&quot;)\n\t\telse\n\t\t\tprintf -v gitstring -- \&quot;$printf_format\&quot; \&quot;$gitstring\&quot;\n\t\tfi\n\t\tPS1=\&quot;$ps1pc_start$gitstring$ps1pc_end\&quot;\n\telse\n\t\tprintf -- \&quot;$printf_format\&quot; \&quot;$gitstring\&quot;\n\tfi\n\n\treturn $exit\n}\n&quot;}}},{&quot;rowIdx&quot;:34,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\n#Copyright (c) 2016, Allgeyer Tobias, Aumann Florian, Borella Jocelyn, Karrenbauer Oliver, Marek Felix, Meissner Pascal, Stroh Daniel, Trautmann Jeremias\n#All rights reserved.\n#\n#Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n#\n#1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n#\n#2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other #materials provided with the distribution.\n#\n#3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific #prior written permission.\n#\n#THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \&quot;AS IS\&quot; AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED #WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, #INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR #PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) #ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n# read http://unix.stackexchange.com/questions/17116/prevent-pane-window-from-closing-when-command-completes-tmux\n# Starts additional simulation modules that cannot be launched before gazebo is running\n\n# wait for gazebo and rviz\n\nsleep 5\n\n# while [[ ! $(rosservice list | grep gaz) ]]; do sleep 1; done;\n# while [[ ! $(xwininfo -root -all | grep rviz) ]]; do sleep 1; done;\n\n\n# make sure log folder exist, so all modules start safely\nexport logFolder=~/log\nmkdir -p ${logFolder}\n\n#Starts scene recognition, pose prediction and provides a pane for interfaces with object localization simulation.\ntmux new-window -n 'ism' \ntmux send-keys -t asr:ism 'script -c \&quot;roslaunch --wait asr_recognizer_prediction_ism rp_ism_node.launch\&quot; -f '\&quot;${logFolder}\&quot;'/ism.log' C-m\ntmux split-window -t asr:ism\ntmux send-keys -t asr:ism.1 'echo Perform service calls to asr_fake_object_recognition from here.' C-m\n\n#Starts next-best-view calculation and world model.\ntmux new-window -n 'nbv'\ntmux send-keys -t asr:nbv 'script -c \&quot;roslaunch --wait asr_next_best_view next_best_view_core_sim.launch\&quot; -f '\&quot;${logFolder}\&quot;'/nbv.log' C-m\ntmux split-window -t asr:nbv\ntmux send-keys -t asr:nbv.1 'script -c \&quot;roslaunch --wait asr_world_model world_model.launch\&quot; -f '\&quot;${logFolder}\&quot;'/world_model.log' C-m\n\n#Starts visualization server to publish the room model.\ntmux new-window -n 'viz_server'\ntmux send-keys -t asr:viz_server 'script -c \&quot;roslaunch --wait asr_visualization_server visualization.launch\&quot; -f '\&quot;${logFolder}\&quot;'/viz_server.log'  C-m\n\n#Starts state machine that controls all other components, required for active scene recognition.\ntmux new-window -n 'state_machine'\ntmux send-keys -t asr:state_machine 'script -c \&quot;roslaunch --wait asr_state_machine scene_exploration_sim.launch\&quot; -f '\&quot;${logFolder}\&quot;'/state_machine.log'  C-m\n\n#Starts direct_search_manager that handles the direct search\ntmux new-window -n 'direct_search_manager'\ntmux send-keys -t asr:direct_search_manager 'script -c \&quot;roslaunch --wait asr_direct_search_manager direct_search_manager.launch\&quot; -f '\&quot;${logFolder}\&quot;'/direct_search_manager.log'  C-m\n\n&quot;}}},{&quot;rowIdx&quot;:35,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/sh\n\ndocker push robodomo/icomfort-microservice\n\n&quot;}}},{&quot;rowIdx&quot;:36,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/usr/bin/env bash\n\nset -e\nset -o pipefail\n\n# vars\nPYTHON=python\nPIP=pip\nVENV_NAME=\n\n# process options\nwhile getopts \&quot;h?3e:\&quot; opt; do\n    case \&quot;$opt\&quot; in\n        h|\\?)\n            echo \&quot;install.sh parameters\&quot;\n            echo \&quot;\&quot;\n            echo \&quot;-3 install for Python 3.3+\&quot;\n            echo \&quot;-e [environment name] install to a virtual environment\&quot;\n            echo \&quot;\&quot;\n            exit 1\n            ;;\n        3)\n            PYTHON=python3\n            PIP=pip3\n            ;;\n        e)\n            VENV_NAME=$OPTARG\n            ;;\n    esac\ndone\nshift $((OPTIND-1))\n[ \&quot;$1\&quot; = \&quot;--\&quot; ] &amp;&amp; shift\n\n# check to ensure this is not being run directly as root\nif [ $(id -u) -eq 0 ]; then\n    echo \&quot;Installation cannot be performed as root or via sudo.\&quot;\n    echo \&quot;Please install as a regular user.\&quot;\n    exit 1\nfi\n\n# check for sudo\nif hash sudo 2> /dev/null; then\n    echo \&quot;sudo found.\&quot;\nelse\n    echo \&quot;sudo not found. Please install sudo first before proceeding.\&quot;\n    exit 1\nfi\n\n# check that shipyard.py is in cwd\nif [ ! -f $PWD/shipyard.py ]; then\n    echo \&quot;shipyard.py not found in $PWD.\&quot;\n    echo \&quot;Please run install.sh from the same directory as shipyard.py.\&quot;\n    exit 1\nfi\n\n# check for python\nif hash $PYTHON 2> /dev/null; then\n    echo \&quot;Installing for $PYTHON.\&quot;\nelse\n    echo \&quot;$PYTHON not found, please install $PYTHON first with your system software installer.\&quot;\n    exit 1\nfi\n\n# check for anaconda\nset +e\nANACONDA=0\n$PYTHON -c \&quot;from __future__ import print_function; import sys; print(sys.version)\&quot; | grep -Ei 'anaconda|continuum'\nif [ $? -eq 0 ]; then\n    # check for conda\n    if hash conda 2> /dev/null; then\n        echo \&quot;Anaconda environment detected.\&quot;\n    else\n        echo \&quot;Anaconda environment detected, but conda command not found.\&quot;\n        exit 1\n    fi\n    if [ -z $VENV_NAME ]; then\n        echo \&quot;Virtual environment name must be supplied for Anaconda installations.\&quot;\n        exit 1\n    fi\n    ANACONDA=1\n    PIP=pip\nfi\nset -e\n\n# perform some virtual env parameter checks\nINSTALL_VENV_BIN=0\nif [ ! -z $VENV_NAME ]; then\n    # check if virtual env, env is not named shipyard\n    if [ \&quot;$VENV_NAME\&quot; == \&quot;shipyard\&quot; ]; then\n        echo \&quot;Virtual environment name cannot be shipyard. Please use a different virtual environment name.\&quot;\n        exit 1\n    fi\n    # check for virtualenv executable\n    if [ $ANACONDA -eq 0 ]; then\n        if hash virtualenv 2> /dev/null; then\n            echo \&quot;virtualenv found.\&quot;\n        else\n            echo \&quot;virtualenv not found.\&quot;\n            INSTALL_VENV_BIN=1\n        fi\n    fi\nfi\n\n# try to get /etc/lsb-release\nif [ -e /etc/lsb-release ]; then\n    . /etc/lsb-release\nelse\n    if [ -e /etc/os-release ]; then\n        . /etc/os-release\n        DISTRIB_ID=$ID\n        DISTRIB_RELEASE=$VERSION_ID\n    fi\nfi\n\nif [ -z ${DISTRIB_ID+x} ] || [ -z ${DISTRIB_RELEASE+x} ]; then\n    echo \&quot;Unknown DISTRIB_ID or DISTRIB_RELEASE.\&quot;\n    echo \&quot;Please refer to the Installation documentation for manual installation steps.\&quot;\n    exit 1\nfi\n\n# lowercase vars\nDISTRIB_ID=${DISTRIB_ID,,}\nDISTRIB_RELEASE=${DISTRIB_RELEASE,,}\n\n# install requisite packages from distro repo\nif [ $DISTRIB_ID == \&quot;ubuntu\&quot; ] || [ $DISTRIB_ID == \&quot;debian\&quot; ]; then\n    sudo apt-get update\n    if [ $PYTHON == \&quot;python\&quot; ]; then\n        PYTHON_PKGS=\&quot;libpython-dev python-dev\&quot;\n        if [ $ANACONDA -eq 0 ]; then\n            PYTHON_PKGS=\&quot;$PYTHON_PKGS python-pip\&quot;\n        fi\n    else\n        PYTHON_PKGS=\&quot;libpython3-dev python3-dev\&quot;\n        if [ $ANACONDA -eq 0 ]; then\n            PYTHON_PKGS=\&quot;$PYTHON_PKGS python3-pip\&quot;\n        fi\n    fi\n    sudo apt-get install -y --no-install-recommends \\\n        build-essential libssl-dev libffi-dev openssl \\\n        openssh-client rsync $PYTHON_PKGS\nelif [ $DISTRIB_ID == \&quot;centos\&quot; ] || [ $DISTRIB_ID == \&quot;rhel\&quot; ]; then\n    if [ $PYTHON == \&quot;python\&quot; ]; then\n        PYTHON_PKGS=\&quot;python-devel\&quot;\n    else\n        if [ $(yum list installed epel-release) -ne 0 ]; then\n            echo \&quot;epel-release package not installed.\&quot;\n            echo \&quot;Please install the epel-release package or refer to the Installation documentation for manual installation steps\&quot;.\n            exit 1\n        fi\n        if [ $(yum list installed python34) -ne 0 ]; then\n            echo \&quot;python34 epel package not installed.\&quot;\n            echo \&quot;Please install the python34 epel package or refer to the Installation documentation for manual installation steps.\&quot;\n            exit 1\n        fi\n        PYTHON_PKGS=\&quot;python34-devel\&quot;\n    fi\n    sudo yum install -y gcc openssl-devel libffi-devel openssl \\\n        openssh-clients rsync $PYTHON_PKGS\n    if [ $ANACONDA -eq 0 ]; then\n        curl -fSsL https://bootstrap.pypa.io/get-pip.py | sudo $PYTHON\n    fi\nelif [ $DISTRIB_ID == \&quot;opensuse\&quot; ] || [ $DISTRIB_ID == \&quot;sles\&quot; ]; then\n    sudo zypper ref\n    if [ $PYTHON == \&quot;python\&quot; ]; then\n        PYTHON_PKGS=\&quot;python-devel\&quot;\n    else\n        PYTHON_PKGS=\&quot;python3-devel\&quot;\n    fi\n    sudo zypper -n in gcc libopenssl-devel libffi48-devel openssl \\\n        openssh rsync $PYTHON_PKGS\n    if [ $ANACONDA -eq 0 ]; then\n        curl -fSsL https://bootstrap.pypa.io/get-pip.py | sudo $PYTHON\n    fi\nelse\n    echo \&quot;Unsupported distribution.\&quot;\n    echo \&quot;Please refer to the Installation documentation for manual installation steps.\&quot;\n    exit 1\nfi\n\n# create virtual env if required and install required python packages\nif [ ! -z $VENV_NAME ]; then\n    # install virtual env if required\n    if [ $INSTALL_VENV_BIN -eq 1 ]; then\n        sudo $PIP install virtualenv\n    fi\n    if [ $ANACONDA -eq 0 ]; then\n        # create venv if it doesn't exist\n        virtualenv -p $PYTHON $VENV_NAME\n        source $VENV_NAME/bin/activate\n        $PIP install --upgrade pip setuptools\n        $PIP install --upgrade -r requirements.txt\n        deactivate\n    else\n        # create conda env\n        set +e\n        conda create --yes --name $VENV_NAME\n        set -e\n        source activate $VENV_NAME\n        conda install --yes pip\n        # temporary workaround with pip requirements upgrading setuptools and\n        # conda pip failing to reference the old setuptools version\n        set +e\n        $PIP install --upgrade setuptools\n        set -e\n        $PIP install --upgrade -r requirements.txt\n        source deactivate $VENV_NAME\n    fi\nelse\n    sudo $PIP install --upgrade pip setuptools\n    $PIP install --upgrade --user -r requirements.txt\nfi\n\n# create shipyard script\ncat > shipyard << EOF\n#!/usr/bin/env bash\n\nset -e\nset -f\n\nBATCH_SHIPYARD_ROOT_DIR=$PWD\nVENV_NAME=$VENV_NAME\n\nEOF\ncat >> shipyard << 'EOF'\nif [ -z $BATCH_SHIPYARD_ROOT_DIR ]; then\n    echo Batch Shipyard root directory not set.\n    echo Please rerun the install.sh script.\n    exit 1\nfi\n\nEOF\n\nif [ ! -z $VENV_NAME ]; then\n    if [ $ANACONDA -eq 0 ]; then\ncat >> shipyard << 'EOF'\nsource $BATCH_SHIPYARD_ROOT_DIR/$VENV_NAME/bin/activate\nEOF\n    else\ncat >> shipyard << 'EOF'\nsource activate $VENV_NAME\nEOF\n    fi\nfi\n\nif [ $PYTHON == \&quot;python\&quot; ]; then\ncat >> shipyard << 'EOF'\npython $BATCH_SHIPYARD_ROOT_DIR/shipyard.py $*\nEOF\nelse\ncat >> shipyard << 'EOF'\npython3 $BATCH_SHIPYARD_ROOT_DIR/shipyard.py $*\nEOF\nfi\n\nif [ ! -z $VENV_NAME ]; then\n    if [ $ANACONDA -eq 0 ]; then\ncat >> shipyard << 'EOF'\ndeactivate\nEOF\n    else\ncat >> shipyard << 'EOF'\nsource deactivate $VENV_NAME\nEOF\n    fi\nfi\n\nchmod 755 shipyard\n\necho \&quot;\&quot;\nif [ -z $VENV_NAME ]; then\n    echo '>> Please add $HOME/.local/bin to your $PATH. You can do this '\n    echo '>> permanently in your shell rc script, e.g., .bashrc for bash shells.'\n    echo \&quot;\&quot;\nfi\necho \&quot;>> Install complete for $PYTHON. Please run Batch Shipyard as: $PWD/shipyard\&quot;\n&quot;}}},{&quot;rowIdx&quot;:37,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\necho \&quot;\&quot;\necho \&quot;Applying migration AssociatedEnterpriseCheckYourAnswers\&quot;\n\necho \&quot;Adding routes to conf/app.routes\&quot;\necho \&quot;\&quot; >> ../conf/app.routes\necho \&quot;GET        /associatedEnterpriseCheckYourAnswers                       controllers.AssociatedEnterpriseCheckYourAnswersController.onPageLoad()\&quot; >> ../conf/app.routes\n\necho \&quot;Adding messages to conf.messages\&quot;\necho \&quot;\&quot; >> ../conf/messages.en\necho \&quot;associatedEnterpriseCheckYourAnswers.title = associatedEnterpriseCheckYourAnswers\&quot; >> ../conf/messages.en\necho \&quot;associatedEnterpriseCheckYourAnswers.heading = associatedEnterpriseCheckYourAnswers\&quot; >> ../conf/messages.en\n\necho \&quot;Migration AssociatedEnterpriseCheckYourAnswers completed\&quot;\n&quot;}}},{&quot;rowIdx&quot;:38,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;rsync -avzP --update * ericmjl@rous:~/github/protein-convolutional-nets --exclude-from rsync_exclude.txt\n\nrsync -avzP --update ericmjl@rous:~/github/protein-convolutional-nets/* ./ --exclude-from rsync_exclude.txt\n&quot;}}},{&quot;rowIdx&quot;:39,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n# LICENSE UPL 1.0\n#\n# Copyright (c) 2019 Oracle and/or its affiliates. All rights reserved.\n#\n# Since: January, 2018\n# Author: sanjay.singh@oracle.com, paramdeep.saini@oracle.com\n# Description: Add a Grid node and add Oracle Database instance based on following parameters:\n#              $PUBLIC_HOSTNAME\n#              $PUBLIC_IP\n#\n# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS HEADER.\n#\n\n####################### Variables and Constants #################\ndeclare -r FALSE=1\ndeclare -r TRUE=0\ndeclare -x GRID_USER='grid'          ## Default gris user is grid.\ndeclare -x DB_USER='oracle'      ## default oracle user is oracle.\ndeclare -r ETCHOSTS=\&quot;/etc/hosts\&quot;     ## /etc/hosts file location.\ndeclare -r RAC_ENV_FILE=\&quot;/etc/rac_env_vars\&quot;   ## RACENV FILE NAME\ndeclare -x GIMR_DB_FLAG='false'      ## GIMR DB Check by default is false\ndeclare -x DOMAIN                    ## Domain name will be computed based on hostname -d, otherwise pass it as env variable.\ndeclare -x PUBLIC_IP                 ## Computed based on Node name.\ndeclare -x PUBLIC_HOSTNAME           ## PUBLIC HOSTNAME set based on hostname\ndeclare -x EXISTING_CLS_NODE         ## Computed during the program execution.\ndeclare -x EXISTING_CLS_NODES        ## You must all the exisitng nodes of the cluster in comma separated strings. Otherwise installation will fail.\ndeclare -x DHCP_CONF='false'         ## Pass env variable where value set to true for DHCP based installation.\ndeclare -x NODE_VIP                  ## Pass it as env variable.\ndeclare -x VIP_HOSTNAME              ## Pass as env variable.\ndeclare -x SCAN_NAME                 ## Pass it as env variable.\ndeclare -x SCAN_IP                   ## Pass as env variable if you do not have DNS server. Otherwise, do not pass this variable.\ndeclare -x SINGLENIC='false'         ## Default value is false as we should use 2 nics if possible for better performance.\ndeclare -x PRIV_IP                   ## Pass PRIV_IP is not using SINGLE NIC\ndeclare -x CONFIGURE_GNS='false'     ## Default value set to false. However, under DSC checks, it is reverted to true.\ndeclare -x COMMON_SCRIPTS            ## COMMON SCRIPT Locations. Pass this env variable if you have custom responsefile for grid and other scripts for DB.\ndeclare -x PRIV_HOSTNAME             ## if SINGLENIC=true then PRIV and PUB hostname will be same. Otherise pass it as env variable.\ndeclare -x CMAN_HOSTNAME             ## If you want to use connection manager to proxy the DB connections\ndeclare -x CMAN_IP                   ## CMAN_IP if you want to use connection manager to proxy the DB connections\ndeclare -x OS_PASSWORD               ## if not passed as env variable, it will be set to PASSWORD\ndeclare -x GRID_PASSWORD             ## if not passed as env variable , it will be set to OS_PASSWORD\ndeclare -x ORACLE_PASSWORD           ## if not passed as env variable, it will be set to OS_PASSWORD\ndeclare -x PASSWORD                  ## If not passed as env variable , it will be set as system generated password\ndeclare -x CLUSTER_TYPE='STANDARD'   ## Default instllation is STANDARD. You can pass DOMAIn or MEMBERDB.\ndeclare -x GRID_RESPONSE_FILE        ## IF you pass this env variable then user based responsefile will be used. default location is COMMON_SCRIPTS.\ndeclare -x SCRIPT_ROOT               ## SCRIPT_ROOT will be set as per your COMMON_SCRIPTS.Do not Pass env variable SCRIPT_ROOT.\ndeclare -r OSDBA='dba'\ndeclare -r OSASM='asmadmin'\ndeclare -r INSTALL_TYPE='CRS_ADDNODE'\ndeclare -r IPMI_FLAG='false'\ndeclare -r ASM_STORAGE_OPTION='ASM'\ndeclare -r GIMR_ON_NAS='false'\ndeclare -x SCAN_TYPE='LOCAL_SCAN'\ndeclare -x SHARED_SCAN\ndeclare -x DB_ASM_DISKGROUP='DATA'\ndeclare -x CONFIGURE_AFD_FLAG='false'\ndeclare -x CONFIGURE_RHPS_FLAG='false'\ndeclare -x EXECUTE_ROOT_SCRIPT_FLAG='fasle'\ndeclare -x EXECUTE_ROOT_SCRIPT_METHOD='ROOT'\ndeclare -x IGNORE_CVU_CHECKS='true'           ## Ignore CVU Checks\ndeclare -x SECRET_VOLUME='/run/secrets/'      ## Secret Volume\ndeclare -x PWD_KEY='pwd.key'                  ## PWD Key File\ndeclare -x ORACLE_PWD_FILE\ndeclare -x GRID_PWD_FILE\ndeclare -x REMOVE_OS_PWD_FILES='false'\ndeclare -x COMMON_OS_PWD_FILE='common_os_pwdfile.enc'\ndeclare -x CRS_CONFIG_NODES\ndeclare -x ANSIBLE_INSTALL='false'\ndeclare -x RUN_DBCA='true'\n\nprogname=$(basename \&quot;$0\&quot;)\n###################### Variabes and Constants declaration ends here  ####################\n\n\n############Sourcing Env file##########\nif [ -f \&quot;/etc/rac_env_vars\&quot; ]; then\nsource \&quot;/etc/rac_env_vars\&quot;\nfi\n##########Source ENV file ends here####\n\n\n###################Capture Process id and source functions.sh###############\nsource \&quot;$SCRIPT_DIR/functions.sh\&quot;\n###########################sourcing of functions.sh ends here##############\n\n####error_exit function sends a TERM signal, which is caught by trap command and returns exit status 15\&quot;####\ntrap '{ exit 15; }' TERM\n###########################trap code ends here##########################\n\nall_check()\n{\ncheck_pub_host_name\ncheck_cls_node_names\ncheck_ip_env_vars\ncheck_passwd_env_vars\ncheck_rspfile_env_vars\ncheck_db_env_vars\n}\n\n#####################Function related to public hostname, IP and domain name check begin here ########\n\ncheck_pub_host_name()\n{\nlocal domain_name\nlocal stat\n\nif [ -z \&quot;${PUBLIC_IP}\&quot; ]; then\n    PUBLIC_IP=$(dig +short \&quot;$(hostname)\&quot;)\n    print_message \&quot;Public IP is set to ${PUBLIC_IP}\&quot;\nelse\n    print_message \&quot;Public IP is set to ${PUBLIC_IP}\&quot;\nfi\n\nif [ -z \&quot;${PUBLIC_HOSTNAME}\&quot; ]; then\n  PUBLIC_HOSTNAME=$(hostname)\n  print_message \&quot;RAC Node PUBLIC Hostname is set to ${PUBLIC_HOSTNAME}\&quot;\n else\n  print_message \&quot;RAC Node PUBLIC Hostname is set to ${PUBLIC_HOSTNAME}\&quot;\nfi\n\nif [ -z \&quot;${DOMAIN}\&quot; ]; then\ndomain_name=$(hostname -d)\n if [ -z \&quot;${domain_name}\&quot; ];then\n   print_message  \&quot;Domain name is not defined. Setting Domain to 'example.com'\&quot;\n    DOMAIN=\&quot;example.com\&quot;\n else\n    DOMAIN=${domain_name}\nfi\n else\n print_message \&quot;Domain is defined to $DOMAIN\&quot;\nfi\n\n}\n\n############### Function related to public hostname, IP and domain checks ends here ##########\n\n############## Function related to check exisitng cls nodes begin here #######################\ncheck_cls_node_names()\n{\nif [ -z \&quot;${EXISTING_CLS_NODES}\&quot; ]; then\n\terror_exit \&quot;For Node Addition, please provide the existing clustered node name.\&quot;\nelse\n\t\n   if isStringExist ${EXISTING_CLS_NODES} ${PUBLIC_HOSTNAME}; then\n\t  error_exit \&quot;EXISTING_CLS_NODES ${EXISTING_CLS_NODES} contains new node name ${PUBLIC_HOSTNAME}\&quot;\n   fi\n\nprint_message \&quot;Setting Existing Cluster Node for node addition operation. This will be retrieved from ${EXISTING_CLS_NODES}\&quot;\n\nEXISTING_CLS_NODE=\&quot;$( cut -d ',' -f 1 <<< \&quot;$EXISTING_CLS_NODES\&quot; )\&quot;\n\nif [ -z \&quot;${EXISTING_CLS_NODE}\&quot; ]; then\n   error_exit \&quot; Existing Node Name of the cluster not set or set to empty string\&quot;\nelse\n   print_message \&quot;Existing Node Name of the cluster is set to ${EXISTING_CLS_NODE}\&quot;\n\nif resolveip ${EXISTING_CLS_NODE}; then\n print_message \&quot;Existing Cluster node resolved to IP. Check passed\&quot;\nelse\n  error_exit \&quot;Existing Cluster node does not resolved to IP. Check Failed\&quot;\nfi\nfi\nfi\n}\n\n############## Function related to check exisitng cls nodes begin here #######################\n\ncheck_ip_env_vars ()\n{\nif [ \&quot;${DHCP_CONF}\&quot; != 'true' ]; then\n  print_message \&quot;Default setting of AUTO GNS VIP set to false. If you want to use AUTO GNS VIP, please pass DHCP_CONF as an env parameter set to true\&quot;\n  DHCP_CONF=false\nif [ -z \&quot;${NODE_VIP}\&quot; ]; then\n   error_exit \&quot;RAC Node ViP is not set or set to empty string\&quot;\nelse\n   print_message \&quot;RAC VIP set to ${NODE_VIP}\&quot;\nfi\n\nif [ -z \&quot;${VIP_HOSTNAME}\&quot; ]; then\n   error_exit \&quot;RAC Node Vip hostname is not set ot set to empty string\&quot;\nelse\n   print_message \&quot;RAC Node VIP hostname is set to ${VIP_HOSTNAME} \&quot;\nfi\n\nif [ -z ${SCAN_NAME} ]; then\n  print_message \&quot;SCAN_NAME set to the empty string\&quot;\nelse\n  print_message \&quot;SCAN_NAME name is ${SCAN_NAME}\&quot;\nfi\n\nif resolveip ${SCAN_NAME}; then\n print_message \&quot;SCAN Name resolving to IP. Check Passed!\&quot;\nelse\n  error_exit \&quot;SCAN Name not resolving to IP. Check Failed!\&quot;\nfi\n\nif [ -z ${SCAN_IP} ]; then\n   print_message \&quot;SCAN_IP set to the empty string\&quot;\nelse\n  print_message \&quot;SCAN_IP name is ${SCAN_IP}\&quot;\nfi\nfi\n\nif [ \&quot;${SINGLENIC}\&quot; == 'true' ];then\nPRIV_IP=${PUBLIC_IP}\nPRIV_HOSTNAME=${PUBLIC_HOSTNAME}\nfi\n\nif [ -z \&quot;${PRIV_IP}\&quot; ]; then\n   error_exit \&quot;RAC Node private ip is not set ot set to empty string\&quot;\nelse\n  print_message \&quot;RAC Node PRIV IP is set to ${PRIV_IP} \&quot;\nfi\n\nif [ -z \&quot;${PRIV_HOSTNAME}\&quot; ]; then\n   error_exit \&quot;RAC Node private hostname is not set ot set to empty string\&quot;\nelse\n  print_message \&quot;RAC Node private hostname is set to ${PRIV_HOSTNAME}\&quot;\nfi\n\n\nif [ -z ${CMAN_HOSTNAME} ]; then\n  print_message  \&quot;CMAN_NAME set to the empty string\&quot;\nelse\n  print_message \&quot;CMAN_HOSTNAME name is ${CMAN_HOSTNAME}\&quot;\nfi\n\nif [ -z ${CMAN_IP} ]; then\n   print_message \&quot;CMAN_IP set to the empty string\&quot;\nelse\n  print_message \&quot;CMAN_IP name is ${CMAN_IP}\&quot;\nfi\n\n}\n################check ip env vars function  ends here ############################\n\n################ Check passwd env vars function  begin here ######################\ncheck_passwd_env_vars()\n{\nif [ -f \&quot;${SECRET_VOLUME}/${COMMON_OS_PWD_FILE}\&quot; ]; then\ncmd='openssl enc -d -aes-256-cbc -in \&quot;${SECRET_VOLUME}/${COMMON_OS_PWD_FILE}\&quot; -out /tmp/${COMMON_OS_PWD_FILE} -pass file:\&quot;${SECRET_VOLUME}/${PWD_KEY}\&quot;'\n\neval $cmd\n\nif [ $? -eq 0 ]; then\nprint_message \&quot;Password file generated\&quot;\nelse\nerror_exit \&quot;Error occurred during common os password file generation\&quot;\nfi\n\nread PASSWORD < /tmp/${COMMON_OS_PWD_FILE}\nrm -f /tmp/${COMMON_OS_PWD_FILE}\nelse\n print_message \&quot;Password is empty string\&quot;\n PASSWORD=O$(openssl rand -base64 6 | tr -d \&quot;=+/\&quot;)_1\nfi\n\nif [ ! -z \&quot;${GRID_PWD_FILE}\&quot; ]; then\ncmd='openssl enc -d -aes-256-cbc -in \&quot;${SECRET_VOLUME}/${GRID_PWD_FILE}\&quot; -out \&quot;/tmp/${GRID_PWD_FILE}\&quot; -pass file:\&quot;${SECRET_VOLUME}/${PWD_KEY}\&quot;'\n\neval $cmd\n\nif [ $? -eq 0 ]; then\nprint_message \&quot;Password file generated\&quot;\nelse\nerror_exit \&quot;Error occurred during Grid password file generation\&quot;\nfi\n\nread GRID_PASSWORD < /tmp/${GRID_PWD_FILE}\nrm -f /tmp/${GRID_PWD_FILE}\nelse\n  GRID_PASSWORD=\&quot;${PASSWORD}\&quot;\n  print_message \&quot;Common OS Password string is set for Grid user\&quot;\nfi\n\nif [ ! -z \&quot;${ORACLE_PWD_FILE}\&quot; ]; then\ncmd='openssl enc -d -aes-256-cbc -in \&quot;${SECRET_VOLUME}/${ORACLE_PWD_FILE}\&quot; -out \&quot;/tmp/${ORACLE_PWD_FILE}\&quot; -pass file:\&quot;${SECRET_VOLUME}/${PWD_KEY}\&quot;'\n\neval $cmd\n\nif [ $? -eq 0 ]; then\nprint_message \&quot;Password file generated\&quot;\nelse\nerror_exit \&quot;Error occurred during Oracle  password file generation\&quot;\nfi\n\nread ORACLE_PASSWORD < /tmp/${ORACLE_PWD_FILE}\nrm -f /tmp/${GRID_PWD_FILE}\nelse\n  ORACLE_PASSWORD=\&quot;${PASSWORD}\&quot;\n  print_message \&quot;Common OS Password string is set for  Oracle user\&quot;\nfi\n\nif [ \&quot;${REMOVE_OS_PWD_FILES}\&quot; == 'true' ]; then\nrm -f  ${SECRET_VOLUME}/${COMMON_OS_PWD_FILE}\nrm -f ${SECRET_VOLUME}/${PWD_KEY}\nfi\n\n}\n\n############### Check password env vars function ends here ########################\n\n############### Check grid Response file function begin here ######################\ncheck_rspfile_env_vars ()\n{\nif [ -z \&quot;${GRID_RESPONSE_FILE}\&quot; ];then\nprint_message \&quot;GRID_RESPONSE_FILE env variable set to empty. $progname will use standard cluster responsefile\&quot;\nelse\nif [ -f $COMMON_SCRIPTS/$GRID_RESPONSE_FILE ];then\ncp $COMMON_SCRIPTS/$GRID_RESPONSE_FILE $logdir/$GRID_RESPONSE_FILE\nelse\nerror_exit \&quot;$COMMON_SCRIPTS/$GRID_RESPONSE_FILE does not exist\&quot;\nfi\nfi\n\nif [ -z \&quot;${SCRIPT_ROOT}\&quot; ]; then\nSCRIPT_ROOT=$COMMON_SCRIPTS\nprint_message \&quot;Location for User script SCRIPT_ROOT set to $COMMON_SCRIPTS\&quot;\nelse\nprint_message \&quot;Location for User script SCRIPT_ROOT set to $SCRIPT_ROOT\&quot;\nfi\n\n}\n\n############ Check responsefile function end here ######################\n\n########### Check db env vars function begin here #######################\ncheck_db_env_vars ()\n{\nif [ $CLUSTER_TYPE == 'MEMBERDB' ]; then\nprint_message \&quot;Checking StorageOption for MEMBERDB Cluster\&quot;\n\nif [ -z \&quot;${STORAGE_OPTIONS_FOR_MEMBERDB}\&quot; ]; then\nprint_message \&quot;Storage Options is set to STORAGE_OPTIONS_FOR_MEMBERDB\&quot;\nelse\nprint_message \&quot;Storage Options is set to STORAGE_OPTIONS_FOR_MEMBERDB\&quot;\nfi\n\nfi\nif [ -z \&quot;${ORACLE_SID}\&quot; ]; then\n   print_message \&quot;ORACLE_SID is not defined\&quot;\nelse\n  print_message \&quot;ORACLE_SID is set to $ORACLE_SID\&quot;\nfi\n\n}\n\n################# Check db env vars end here ##################################\n\n################ All Check Functions end here #####################################\n\n\n########################################### SSH Function begin here ########################\nsetupSSH()\n{\nlocal password\nlocal ssh_pid\nlocal stat\n\nif [ -z $CRS_NODES ]; then\n  CRS_NODES=$PUBLIC_HOSTNAME\nfi\n\n\nIFS=', ' read -r -a CLUSTER_NODES  <<< \&quot;$EXISTING_CLS_NODES\&quot;\nEXISTING_CLS_NODES+=\&quot;,$CRS_NODES\&quot;\nCLUSTER_NODES=$(echo $EXISTING_CLS_NODES | tr ',' ' ')\n\nprint_message \&quot;Cluster Nodes are $CLUSTER_NODES\&quot;\nprint_message \&quot;Running SSH setup for $GRID_USER user between nodes ${CLUSTER_NODES}\&quot;\ncmd='su - $GRID_USER -c \&quot;$EXPECT $SCRIPT_DIR/$SETUPSSH $GRID_USER \\\&quot;$GRID_HOME/oui/prov/resources/scripts\\\&quot;  \\\&quot;${CLUSTER_NODES}\\\&quot;  \\\&quot;$GRID_PASSWORD\\\&quot;\&quot;'\n(eval $cmd) &amp;\nssh_pid=$!\nwait $ssh_pid\nstat=$?\n\nif [ \&quot;${stat}\&quot; -ne 0 ]; then\nerror_exit \&quot;ssh setup for Grid user failed!, please make sure you have pass the corect password. You need to make sure that password must be same on all the clustered nodes or the nodes set in existing_cls_nodes env variable for $GRID_USER  user\&quot;\nfi\n\nprint_message \&quot;Running SSH setup for $DB_USER user between nodes ${CLUSTER_NODES[@]}\&quot;\ncmd='su - $DB_USER -c \&quot;$EXPECT $SCRIPT_DIR/$SETUPSSH $DB_USER \\\&quot;$DB_HOME/oui/prov/resources/scripts\\\&quot;  \\\&quot;${CLUSTER_NODES}\\\&quot;  \\\&quot;$ORACLE_PASSWORD\\\&quot;\&quot;'\n(eval $cmd) &amp;\nssh_pid=$!\nwait $ssh_pid\nstat=$?\n\nif [ \&quot;${stat}\&quot; -ne 0 ]; then\nerror_exit \&quot;ssh setup for Oracle  user failed!, please make sure you have pass the corect password. You need to make sure that password must be same on all the clustered nodes or the nodes set in existing_cls_nodes env variable for $DB_USER user\&quot;\nfi\n}\n\ncheckSSH ()\n{\n\nlocal password\nlocal ssh_pid\nlocal stat\nlocal status\n\nIFS=', ' read -r -a CLUSTER_NODES  <<< \&quot;$EXISTING_CLS_NODES\&quot;\nEXISTING_CLS_NODES+=\&quot;,$PUBLIC_HOSTNAME\&quot;\nCLUSTER_NODES=$(echo $EXISTING_CLS_NODES | tr ',' ' ')\n\ncmd='su - $GRID_USER -c \&quot;ssh -o BatchMode=yes -o ConnectTimeout=5 $GRID_USER@$node echo ok 2>&amp;1\&quot;'\necho $cmd\n\nfor node in ${CLUSTER_NODES}\ndo\n\nstatus=$(eval $cmd)\n\nif [[ $status == ok ]] ; then\n  print_message \&quot;SSH check fine for the $node\&quot;\n  \nelif [[ $status == \&quot;Permission denied\&quot;* ]] ; then\n   error_exit \&quot;SSH check failed for the $GRID_USER@$node beuase of permission denied error! SSH setup did not complete sucessfully\&quot; \nelse\n   error_exit \&quot;SSH check failed for the $GRID_USER@$node! Error occurred during SSH setup\&quot;\nfi\n\ndone\n\nstatus=\&quot;NA\&quot;\ncmd='su - $DB_USER -c \&quot;ssh -o BatchMode=yes -o ConnectTimeout=5 $DB_USER@$node echo ok 2>&amp;1\&quot;'\n echo $cmd\nfor node in ${CLUSTER_NODES}\ndo\n\nstatus=$(eval $cmd)\n\nif [[ $status == ok ]] ; then\n  print_message \&quot;SSH check fine for the $DB_USER@$node\&quot;\nelif [[ $status == \&quot;Permission denied\&quot;* ]] ; then\n   error_exit \&quot;SSH check failed for the $DB_USER@$node becuase of permission denied error! SSH setup did not complete sucessfully\&quot;\nelse\n   error_exit \&quot;SSH check failed for the $DB_USER@$node! Error occurred during SSH setup\&quot;\nfi\n\ndone\n\n}\n\n######################################  SSH Function End here ####################################\n\n######################Add Node Functions ####################################\nrunorainstroot()\n{\n$INVENTORY/orainstRoot.sh\n}\n\nrunrootsh ()\n{\n\nlocal ORACLE_HOME=$1\nlocal USER=$2\n\nif [ -z $CRS_NODES ]; then\n  CLUSTER_NODES=$PUBLIC_HOSTNAME\nelse\n  IFS=', ' read -r -a CLUSTER_NODES <<< \&quot;$CRS_NODES\&quot;\nfi\n\nprint_message \&quot;Nodes in the cluster ${CLUSTER_NODES[@]}\&quot;\nfor node in \&quot;${CLUSTER_NODES[@]}\&quot;; do\ncmd='su - $USER -c \&quot;ssh $node sudo $ORACLE_HOME/root.sh\&quot;'\neval $cmd\ndone\n\n}\n\ngenerate_response_file ()\n{\ncp $SCRIPT_DIR/$ADDNODE_RSP $logdir/$ADDNODE_RSP\nchmod 666 $logdir/$ADDNODE_RSP\n\n\nif [ -z \&quot;${GRID_RESPONSE_FILE}\&quot; ]; then\n\nif [ -z ${CRS_CONFIG_NODES} ]; then\n   CRS_CONFIG_NODES=\&quot;$PUBLIC_HOSTNAME:$VIP_HOSTNAME:HUB\&quot;\n   print_message \&quot;Clustered Nodes are set to $CRS_CONFIG_NODES\&quot;\nfi\n\nsed -i -e \&quot;s|###INVENTORY###|$INVENTORY|g\&quot; $logdir/$ADDNODE_RSP\nsed -i -e \&quot;s|###GRID_BASE###|$GRID_BASE|g\&quot; $logdir/$ADDNODE_RSP\nsed -i -r \&quot;s|###PUBLIC_HOSTNAME###|$PUBLIC_HOSTNAME|g\&quot;  $logdir/$ADDNODE_RSP\nsed -i -r \&quot;s|###HOSTNAME_VIP###|$VIP_HOSTNAME|g\&quot;  $logdir/$ADDNODE_RSP\nsed -i -e \&quot;s|###INSTALL_TYPE###|$INSTALL_TYPE|g\&quot; $logdir/$ADDNODE_RSP\nsed -i -e \&quot;s|###OSDBA###|$OSDBA|g\&quot; $logdir/$ADDNODE_RSP\nsed -i -e \&quot;s|###OSOPER###|$OSOPER|g\&quot; $logdir/$ADDNODE_RSP\nsed -i -e \&quot;s|###OSASM###|$OSASM|g\&quot; $logdir/$ADDNODE_RSP\nsed -i -e \&quot;s|###SCAN_TYPE###|$SCAN_TYPE|g\&quot; $logdir/$ADDNODE_RSP\nsed -i -e \&quot;s|###SHARED_SCAN_FILE###|$SHARED_SCAN_FILE|g\&quot; $logdir/$ADDNODE_RSP\nsed -i -e \&quot;s|###DB_ASM_DISKGROUP###|$DB_ASM_DISKGROUP|g\&quot; $logdir/$ADDNODE_RSP\nsed -i -e \&quot;s|###CONFIGURE_AFD_FLAG###|$CONFIGURE_AFD_FLAG|g\&quot; $logdir/$ADDNODE_RSP\nsed -i -e \&quot;s|###CONFIGURE_RHPS_FLAG###|$CONFIGURE_RHPS_FLAG|g\&quot; $logdir/$ADDNODE_RSP\nsed -i -e \&quot;s|###EXECUTE_ROOT_SCRIPT_FLAG###|$EXECUTE_ROOT_SCRIPT_FLAG|g\&quot; $logdir/$ADDNODE_RSP\nsed -i -e \&quot;s|###EXECUTE_ROOT_SCRIPT_METHOD###|$EXECUTE_ROOT_SCRIPT_METHOD|g\&quot; $logdir/$ADDNODE_RSP\nsed -i -e \&quot;s|###CRS_CONFIG_NODES###|$CRS_CONFIG_NODES|g\&quot; $logdir/$ADDNODE_RSP\nfi\n\n}\n\n###### Cluster Verification function #######\nCheckRemoteCluster ()\n{\nlocal cmd;\nlocal stat;\nlocal node=$EXISTING_CLS_NODE\nlocal oracle_home=$GRID_HOME\nlocal ORACLE_HOME=$GRID_HOME\n\nprint_message \&quot;Checking Cluster\&quot;\n\ncmd='su - $GRID_USER -c \&quot;ssh $node \\\&quot;$ORACLE_HOME/bin/crsctl check crs\\\&quot;\&quot;'\neval $cmd\n\nif [ $?  -eq 0 ];then\nprint_message \&quot;Cluster Check on remote node passed\&quot;\nelse\nerror_exit \&quot;Cluster Check on remote node failed\&quot;\nfi\n\ncmd='su - $GRID_USER -c \&quot;ssh $node \\\&quot;$ORACLE_HOME/bin/crsctl check cluster\\\&quot;\&quot;'\neval $cmd\n\nif [ $? -eq 0 ]; then\nprint_message \&quot;Cluster Check went fine\&quot;\nelse\nerror_exit \&quot;Cluster  Check failed!\&quot;\nfi\n\nif [ ${GIMR_DB_FLAG} == 'true' ]; then\n\n   cmd='su - $GRID_USER -c \&quot;ssh $node \\\&quot;$ORACLE_HOME/bin/srvctl status mgmtdb\\\&quot;\&quot;'\n   eval $cmd\n\n    if [ $? -eq 0 ]; then\n        print_message \&quot;MGMTDB Check went fine\&quot;\n    else\n         error_exit \&quot;MGMTDB Check failed!\&quot;\n    fi\nfi\n\ncmd='su - $GRID_USER -c \&quot;ssh $node \\\&quot;$ORACLE_HOME/bin/crsctl check crsd\\\&quot;\&quot;'\neval $cmd\n\nif [ $? -eq 0 ]; then\nprint_message \&quot;CRSD Check went fine\&quot;\nelse\nerror_exit \&quot;CRSD Check failed!\&quot;\nfi\n\n\ncmd='su - $GRID_USER -c \&quot;ssh $node \\\&quot;$ORACLE_HOME/bin/crsctl check cssd\\\&quot;\&quot;'\neval $cmd\n\nif [ $? -eq 0 ]; then\nprint_message \&quot;CSSD Check went fine\&quot;\nelse\nerror_exit \&quot;CSSD Check failed!\&quot;\nfi\n\ncmd='su - $GRID_USER -c \&quot;ssh $node \\\&quot;$ORACLE_HOME/bin/crsctl check evmd\\\&quot;\&quot;'\neval $cmd\n\nif [ $? -eq 0 ]; then\nprint_message \&quot;EVMD Check went fine\&quot;\nelse\nerror_exit \&quot;EVMD Check failed\&quot;\nfi\n\n}\n\nsetDevicePermissions ()\n{\n\nlocal cmd\nlocal state=3\n\nif [ -z $CRS_NODES ]; then\n  CLUSTER_NODES=$PUBLIC_HOSTNAME\nelse\n  IFS=', ' read -r -a CLUSTER_NODES <<< \&quot;$CRS_NODES\&quot;\nfi\n\nprint_message \&quot;Nodes in the cluster ${CLUSTER_NODES[@]}\&quot;\nfor node in \&quot;${CLUSTER_NODES[@]}\&quot;; do\nprint_message \&quot;Setting Device permissions for RAC Install  on $node\&quot;\n\nif [ ! -z \&quot;${GIMR_DEVICE_LIST}\&quot; ];then\n\nprint_message \&quot;Preapring GIMR Device list\&quot;\nIFS=', ' read -r -a devices <<< \&quot;$GIMR_DEVICE_LIST\&quot;\n        local arr_device=${#devices[@]}\nif [ $arr_device -ne 0 ]; then\n        for device in \&quot;${devices[@]}\&quot;\n        do\n        print_message \&quot;Changing Disk permission and ownership\&quot;\n        cmd='su - $GRID_USER -c \&quot;ssh $node sudo chown $GRID_USER:asmadmin $device\&quot;'\n        print_message \&quot;Command : $cmd execute on $node\&quot;\n        eval $cmd\n        unset cmd\n        cmd='su - $GRID_USER -c \&quot;ssh $node sudo chmod 660 $device\&quot;'\n        print_message \&quot;Command : $cmd execute on $node\&quot;\n        eval $cmd\n        unset cmd\n        print_message \&quot;Populate Rac Env Vars on Remote Hosts\&quot;\n        cmd='su - $GRID_USER -c \&quot;ssh $node sudo echo \\\&quot;export GIMR_DEVICE_LIST=${GIMR_DEVICE_LIST}\\\&quot; >> /etc/rac_env_vars\&quot;'\n        print_message \&quot;Command : $cmd execute on $node\&quot;\n        eval $cmd\n        unset cmd\n       done\nfi\n\nfi\n\nif [ ! -z \&quot;${ASM_DEVICE_LIST}\&quot; ];then\n\nprint_message \&quot;Preapring ASM Device list\&quot;\nIFS=', ' read -r -a devices <<< \&quot;$ASM_DEVICE_LIST\&quot;\n        local arr_device=${#devices[@]}\nif [ $arr_device -ne 0 ]; then\n        for device in \&quot;${devices[@]}\&quot;\n        do\n        print_message \&quot;Changing Disk permission and ownership\&quot;\n        cmd='su - $GRID_USER -c \&quot;ssh $node sudo chown $GRID_USER:asmadmin $device\&quot;'\n        print_message \&quot;Command : $cmd execute on $node\&quot;\n        eval $cmd\n        unset cmd\n        cmd='su - $GRID_USER -c \&quot;ssh $node sudo chmod 660 $device\&quot;'\n        print_message \&quot;Command : $cmd execute on $node\&quot;\n        eval $cmd\n        unset cmd\n        print_message \&quot;Populate Rac Env Vars on Remote Hosts\&quot;\n        cmd='su - $GRID_USER -c \&quot;ssh $node sudo echo \\\&quot;export ASM_DEVICE_LIST=${ASM_DEVICE_LIST}\\\&quot; >> /etc/rac_env_vars\&quot;'\n        print_message \&quot;Command : $cmd execute on $node\&quot;\n        eval $cmd\n        unset cmd\n       done\nfi\n\nfi\n\ndone\n\n}\n\ncheckCluster ()\n{\nlocal cmd;\nlocal stat;\nlocal oracle_home=$GRID_HOME\n\nprint_message \&quot;Checking Cluster\&quot;\n\ncmd='su - $GRID_USER -c \&quot;$GRID_HOME/bin/crsctl check crs\&quot;'\neval $cmd\n\nif [ $?  -eq 0 ];then\nprint_message \&quot;Cluster Check passed\&quot;\nelse\nerror_exit \&quot;Cluster Check failed\&quot;\nfi\n\ncmd='su - $GRID_USER -c \&quot;$GRID_HOME/bin/crsctl check cluster\&quot;'\neval $cmd\n\nif [ $? -eq 0 ]; then\nprint_message \&quot;Cluster Check went fine\&quot;\nelse\nerror_exit \&quot;Cluster  Check failed!\&quot;\nfi\n\nif [ ${GIMR_DB_FLAG} == 'true' ]; then\n   cmd='su - $GRID_USER -c \&quot;$GRID_HOME/bin/srvctl status mgmtdb\&quot;'\n    eval $cmd\n\n   if [ $? -eq 0 ]; then\n      print_message \&quot;MGMTDB Check went fine\&quot;\n   else\n      error_exit \&quot;MGMTDB Check failed!\&quot;\n    fi\nfi\n\ncmd='su - $GRID_USER -c \&quot;$GRID_HOME/bin/crsctl check crsd\&quot;'\neval $cmd\n\nif [ $? -eq 0 ]; then\nprint_message \&quot;CRSD Check went fine\&quot;\nelse\nerror_exit \&quot;CRSD Check failed!\&quot;\nfi\n\ncmd='su - $GRID_USER -c \&quot;$GRID_HOME/bin/crsctl check cssd\&quot;'\neval $cmd\n\nif [ $? -eq 0 ]; then\nprint_message \&quot;CSSD Check went fine\&quot;\nelse\nerror_exit \&quot;CSSD Check failed!\&quot;\nfi\n\ncmd='su - $GRID_USER -c \&quot;$GRID_HOME/bin/crsctl check evmd\&quot;'\neval $cmd\n\nif [ $? -eq 0 ]; then\nprint_message \&quot;EVMD Check went fine\&quot;\nelse\nerror_exit \&quot;EVMD Check failed\&quot;\nfi\n\nprint_message \&quot;Removing $logdir/cluvfy_check.txt as cluster check has passed\&quot;\nrm -f $logdir/cluvfy_check.txt\n\n}\n\ncheckClusterClass ()\n{\nprint_message \&quot;Checking Cluster Class\&quot;\nlocal cluster_class\n\ncmd='su - $GRID_USER -c \&quot;$GRID_HOME/bin/crsctl get cluster class\&quot;'\ncluster_class=$(eval $cmd)\nprint_message \&quot;Cluster class is $cluster_class\&quot;\nCLUSTER_TYPE=$(echo $cluster_class | awk -F \\' '{ print $2 }' | awk '{ print $1 }')\n}\n\n\n###### Grid install &amp; Cluster Verification utility Function #######\ncluvfyCheck()\n{\n\nlocal node=$EXISTING_CLS_NODE\nlocal responsefile=$logdir/$ADDNODE_RSP\nlocal hostname=$PUBLIC_HOSTNAME\nlocal vip_hostname=$VIP_HOSTNAME\nlocal cmd\nlocal stat\n\nif [ -z $CRS_NODES ]; then\n  CLUSTER_NODES=$PUBLIC_HOSTNAME\nelse\n  IFS=', ' read -r -a CLUSTER_NODES <<< \&quot;$CRS_NODES\&quot;\nfi\n\nif [ -f \&quot;$logdir/cluvfy_check.txt\&quot; ]; then\nprint_message \&quot;Moving any exisiting cluvfy $logdir/cluvfy_check.txt to $logdir/cluvfy_check_$TIMESTAMP.txt\&quot;\nmv $logdir/cluvfy_check.txt $logdir/cluvfy_check.\&quot;$(date +%Y%m%d-%H%M%S)\&quot;.txt\nfi\n\n#cmd='su - $GRID_USER -c \&quot;ssh $node  \\\&quot;$GRID_HOME/runcluvfy.sh stage -pre nodeadd -n $hostname -vip $vip_hostname\\\&quot; | tee -a $logdir/cluvfy_check.txt\&quot;'\n#eval $cmd\n\nprint_message \&quot;Nodes in the cluster ${CLUSTER_NODES[@]}\&quot;\nfor cls_node in \&quot;${CLUSTER_NODES[@]}\&quot;; do\nprint_message \&quot;ssh to the node $node and executing cvu checks on $cls_node\&quot;\ncmd='su - $GRID_USER -c \&quot;ssh $node  \\\&quot;$GRID_HOME/runcluvfy.sh stage -pre nodeadd -n $cls_node\\\&quot; | tee -a $logdir/cluvfy_check.txt\&quot;'\neval $cmd\ndone\n\nprint_message \&quot;Checking $logdir/cluvfy_check.txt if there is any failed check.\&quot;\nFAILED_CMDS=$(sed -n -f - $logdir/cluvfy_check.txt << EOF\n /.*FAILED.*/ {\np\n}\nEOF\n)\n\ncat $logdir/cluvfy_check.txt > $STD_OUT_FILE\n\nif [[ ${IGNORE_CVU_CHECKS} == 'true' ]]; then\nprint_message \&quot;CVU Checks are ignored as IGNORE_CVU_CHECKS set to true. It is recommended to set IGNORE_CVU_CHECKS to false and meet all the cvu checks requirement. RAC installation might fail, if there are failed cvu checks.\&quot;\nelse\nif [[ $FAILED_CMDS =~ .*FAILED*. ]]\nthen\nprint_message \&quot;cluvfy failed for following  \\n $FAILED_CMDS\&quot;\nerror_exit \&quot;Pre Checks failed for Grid installation, please check $logdir/cluvfy_check.txt\&quot;\nfi\nfi\n}\n\naddGridNode ()\n{\n\nlocal node=$EXISTING_CLS_NODE\nlocal responsefile=$logdir/$ADDNODE_RSP\nlocal hostname=$PUBLIC_HOSTNAME\nlocal vip_hostname=$VIP_HOSTNAME\nlocal cmd\nlocal stat\n\nprint_message \&quot;Copying $responsefile on remote node $node\&quot;\ncmd='su - $GRID_USER -c \&quot;scp $responsefile $node:$logdir\&quot;'\neval $cmd\n\nprint_message \&quot;Running GridSetup.sh on $node to add the node to existing cluster\&quot;\ncmd='su - $GRID_USER -c \&quot;ssh $node  \\\&quot;$GRID_HOME/gridSetup.sh -silent -waitForCompletion -noCopy -skipPrereqs -responseFile $responsefile\\\&quot; | tee -a $logfile\&quot;'\neval $cmd\n\nprint_message \&quot;Node Addition performed. removing Responsefile\&quot;\nrm -f $responsefile\ncmd='su - $GRID_USER -c \&quot;ssh $node \\\&quot;rm -f $responsefile\\\&quot;\&quot;'\n#eval $cmd\n\n}\n\n###########DB Node Addition Functions##############\naddDBNode ()\n{\nlocal node=$EXISTING_CLS_NODE\n\nif [ -z $CRS_NODES ]; then\n   new_node_hostname=$PUBLIC_HOSTNAME\nelse\n   new_node_hostname=$CRS_NODES\nfi\n\nlocal stat=3\nlocal cmd\n\ncmd='su - $DB_USER -c \&quot;ssh $node \\\&quot;$DB_HOME/addnode/addnode.sh \\\&quot;CLUSTER_NEW_NODES={$new_node_hostname}\\\&quot; -skipPrereqs -waitForCompletion -ignoreSysPrereqs -noCopy  -silent\\\&quot; | tee -a $logfile\&quot;'\neval $cmd\n\nif [ $? -eq 0 ]; then\nprint_message \&quot;Node Addition went fine for $new_node_hostname\&quot;\nelse\nerror_exit \&quot;Node Addition failed for $new_node_hostname\&quot;\nfi\n}\n\naddDBInst ()\n{\n# Check whether ORACLE_SID is passed on\nlocal HOSTNAME=$PUBLIC_HOSTNAME\nlocal node=$EXISTING_CLS_NODE\nlocal stat=3\nlocal cmd\n\nif [ -z $CRS_NODES ]; then\n  CLUSTER_NODES=$PUBLIC_HOSTNAME\nelse\n  CLUSTER_NODES=$( echo $CRS_NODES | tr ',' ' ' )\nfi\n\nif [ -z \&quot;${ORACLE_SID}\&quot; ];then\n error_exit \&quot;ORACLE SID is not defined. Cannot Add Instance\&quot;\nfi\n\nif [ -z \&quot;${HOSTNAME}\&quot; ]; then\nerror_exit \&quot;Hostname is not defined\&quot;\nfi\n\n\nfor new_node in \&quot;${CLUSTER_NODES[@]}\&quot;; do\nprint_message \&quot;Adding DB Instance on $node\&quot;\ncmd='su - $DB_USER -c \&quot;ssh $node \\\&quot;$DB_HOME/bin/dbca -addInstance -silent  -nodeName $new_node  -gdbName $ORACLE_SID\\\&quot; | tee -a $logfile\&quot;'\neval $cmd\ndone\n\n}\n\ncheckDBStatus ()\n{\nlocal status\n\nif [ -f \&quot;/tmp/db_status.txt\&quot; ]; then\nstatus=$(cat /tmp/db_status.txt)\nelse\nstatus=\&quot;NOT OPEN\&quot;\nfi\n\nrm -f /tmp/db_status.txt\n\n# SQL Plus execution was successful and database is open\nif [ \&quot;$status\&quot; = \&quot;OPEN\&quot; ]; then\n   print_message \&quot;#################################################################\&quot;\n   print_message \&quot; Oracle Database $ORACLE_SID is up and running on $(hostname)    \&quot;\n   print_message \&quot;#################################################################\&quot;\n# Database is not open\nelse\n   error_exit \&quot;$ORACLE_SID is not up and running on $(hostname)\&quot;\nfi\n\n}\n\n\nsetremotelistener ()\n{\nlocal status\nlocal cmd\n\nif resolveip $CMAN_HOSTNAME; then\nprint_message \&quot;Executing script to set the remote listener\&quot;\nsu - $DB_USER -c \&quot;$SCRIPT_DIR/$REMOTE_LISTENER_FILE $ORACLE_SID $SCAN_NAME $CMAN_HOSTNAME.$DOMAIN\&quot;\nfi\n\n}\n\n########################## DB Functions End here ##########################\n\n###################################\n# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! #\n############# MAIN ################\n# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! #\n###################################\n\n\n###### Etc Host and other Checks and setup before proceeding installation #####\nall_check\nprint_message \&quot;Setting random password for root/$GRID_USER/$DB_USER user\&quot;\nprint_message \&quot;Setting random password for $GRID_USER user\&quot;\nsetpasswd $GRID_USER  $GRID_PASSWORD\nprint_message \&quot;Setting random password for $DB_USER user\&quot;\nsetpasswd $DB_USER $ORACLE_PASSWORD\nprint_message \&quot;Setting random password for root user\&quot;\nsetpasswd root $PASSWORD\n\n####  Setting up SSH #######\nsetupSSH\ncheckSSH\n\n#### Grid Node Addition #####\nprint_message \&quot;Setting Device permission to grid and asmadmin on all the cluster nodes\&quot;\nsetDevicePermissions\nprint_message \&quot;Checking Cluster Status on $EXISTING_CLS_NODE\&quot;\nCheckRemoteCluster\nprint_message \&quot;Generating Responsefile for node addition\&quot;\ngenerate_response_file\nprint_message \&quot;Running Cluster verification utility for new node $PUBLIC_HOSTNAME on $EXISTING_CLS_NODE\&quot;\ncluvfyCheck\nprint_message \&quot;Running Node Addition and cluvfy test for node $PUBLIC_HOSTNAME\&quot;\naddGridNode\nprint_message \&quot;Running root.sh on node $PUBLIC_HOSTNAME\&quot;\nrunrootsh $GRID_HOME  $GRID_USER\ncheckCluster\nprint_message \&quot;Checking Cluster Class\&quot;\ncheckClusterClass\nprint_message \&quot;Running User Script for $GRID_USER user\&quot;\nsu - $GRID_USER -c \&quot;$SCRIPT_DIR/$USER_SCRIPTS_FILE $GRID_SCRIPT_ROOT GRID\&quot;\n\n###### DB Node Addition ######\nif [ \&quot;${CLUSTER_TYPE}\&quot; != 'Domain' ]; then\nif [ \&quot;${RUN_DBCA}\&quot; == 'true' ]; then\nprint_message  \&quot;Performing DB Node addition\&quot;\naddDBNode\nprint_message \&quot;Running root.sh\&quot;\nrunrootsh $DB_HOME $DB_USER\nprint_message \&quot;Adding DB Instance\&quot;\naddDBInst \nprint_message \&quot;Checking DB status\&quot;\nsu - $DB_USER -c \&quot;$SCRIPT_DIR/$CHECK_DB_FILE $ORACLE_SID\&quot;\ncheckDBStatus\nprint_message \&quot;Running User Script for $DB_USER user\&quot;\nsu - $DB_USER -c \&quot;$SCRIPT_DIR/$USER_SCRIPTS_FILE $DB_SCRIPT_ROOT DB\&quot;\nprint_message \&quot;Setting Remote Listener\&quot;\nsetremotelistener\nfi\nfi\necho $TRUE\n&quot;}}},{&quot;rowIdx&quot;:40,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/sh\n\nif [ $# -ne 2 ]; then\n    echo \&quot;invalid parameter\&quot;\n    exit 1\nfi\n\nDATE=$1\nGROUP_ID=$2\n\nwhich gdate > /dev/null 2>&amp;1\nif [ $? -eq 1 ]; then\n    echo \&quot;not found gdate command\&quot;\n    exit 1\nfi\n\ngdate +%Y%m%d --date \&quot;${DATE}\&quot;\nif [ $? -eq 1 ]; then\n    echo \&quot;invalid date: ${DATE}\&quot;\n    exit 1\nfi\n\nAPI_URL=\&quot;http://localhost:9000/programs?date=${DATE}&amp;groupId=${GROUP_ID}\&quot;\n\nwhich jq > /dev/null 2>&amp;1\nif [ $? -eq 1 ]; then\n    echo \&quot;not found jq command\&quot;\n    exit 1\nfi\n\ncurl \&quot;${API_URL}\&quot; | jq -r \&quot;.[] | \\\&quot;\\(.start_time|split(\\\&quot;T\\\&quot;)|.[1])ã€œ\\(.end_time|split(\\\&quot;T\\\&quot;)|.[1]) [\\(.channel.name)] \\(.name) / \\(.title)\\\&quot;\&quot;\n\nexit 0\n&quot;}}},{&quot;rowIdx&quot;:41,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;python transformers/examples/language-modeling/run_language_modeling.py --model_name_or_path train-outputs/1024+0+512-STWS/model --tokenizer_name model-configs/1536-config --eval_data_file ../data/wikitext-103-raw/wiki.valid.raw --output_dir eval-outputs/1024+0+512-STWS/512+512+512-HPMI-256 --do_eval --per_device_eval_batch_size 1 --dataloader_drop_last --augmented --augmentation_function shuffle_within_sentences_high_pmi_first_third_sixth --eval_function last_sixth_eval&quot;}}},{&quot;rowIdx&quot;:42,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/usr/bin/env bash\n# Copyright 2015 Cloudera Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \&quot;License\&quot;);\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Exit on non-true return value\nset -e\n# Exit on reference to uninitialized variable\nset -u\n\nset -o pipefail\n\nsource $SOURCE_DIR/functions.sh\nTHIS_DIR=\&quot;$( cd \&quot;$( dirname \&quot;$0\&quot; )\&quot; &amp;&amp; pwd )\&quot;\nprepare $THIS_DIR\n\nif needs_build_package ; then\n  # Download the dependency from S3\n  download_dependency $PACKAGE \&quot;${PACKAGE_STRING}.tar.gz\&quot; $THIS_DIR\n\n  setup_package_build $PACKAGE $PACKAGE_VERSION\n\n  # Snappy switched to CMake. Detect this and use CMake for newer releases.\n  if [ -e CMakeLists.txt ]; then\n    # Snappy's CMake builds either shared or static but not both. Build\n    # each separately.\n    mkdir -p build_shared\n    pushd build_shared\n    wrap cmake -DBUILD_SHARED_LIBS=ON -DCMAKE_BUILD_TYPE=RELEASE \\\n               -DCMAKE_INSTALL_PREFIX=$LOCAL_INSTALL ..\n    wrap make -C . -j${BUILD_THREADS:-4} install\n    popd\n\n    mkdir -p build_static\n    pushd build_static\n    wrap cmake -DCMAKE_BUILD_TYPE=RELEASE -DCMAKE_INSTALL_PREFIX=$LOCAL_INSTALL ..\n    wrap make -C . -j${BUILD_THREADS:-4} install\n    popd\n  else\n    wrap ./configure --with-pic --prefix=$LOCAL_INSTALL\n    wrap make -j${BUILD_THREADS:-4} install\n  fi\n\n  finalize_package_build $PACKAGE $PACKAGE_VERSION\nfi\n&quot;}}},{&quot;rowIdx&quot;:43,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/ash -xe\n\nget_from_event() {\n  jq -r \&quot;$1\&quot; \&quot;${GITHUB_EVENT_PATH}\&quot;\n}\n\nif jq --exit-status '.inputs.deployment_id' \&quot;$GITHUB_EVENT_PATH\&quot; >/dev/null; then\n  MONOPOLIS_URL=\&quot;https://github-api.monopolis.cloud/rollout/start/$(get_from_event '.repository.full_name')/$(get_from_event '.inputs.deployment_id')\&quot;\n  echo ::set-output name=environment_map::$(curl --fail -X POST \&quot;${MONOPOLIS_URL}\&quot; -H \&quot;Authorization: Bearer ${GITHUB_TOKEN}\&quot;)\nelse\n  echo ::set-output name=environment_map::\&quot;{}\&quot;\nfi\n\n&quot;}}},{&quot;rowIdx&quot;:44,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/usr/bin/env bash\r\n#shellcheck disable=SC2128\r\n#shellcheck source=/dev/null\r\nset -x\r\nsource ../dapp-test-common.sh\r\nsource \&quot;../x2ethereum/publicTest.sh\&quot;\r\n\r\nsendAddress=\&quot;12qyocayNF7Lv6C9qW4avxs2E7U41fKSfv\&quot;\r\nsendPriKey=\&quot;0x4257d8692ef7fe13c68b65d6a52f03933db2fa5ce8faf210b5b8b80c721ced01\&quot;\r\nMAIN_HTTP=\&quot;\&quot;\r\nchain33SenderAddr=\&quot;14KEKbYtKKQm4wMthSK9J4La4nAiidGozt\&quot;\r\n# validatorsAddr=[\&quot;0x92c8b16afd6d423652559c6e266cbe1c29bfd84f\&quot;, \&quot;0x0df9a824699bc5878232c9e612fe1a5346a5a368\&quot;, \&quot;0xcb074cb21cdddf3ce9c3c0a7ac4497d633c9d9f1\&quot;, \&quot;0xd9dab021e74ecf475788ed7b61356056b2095830\&quot;]\r\nethValidatorAddrKeyA=\&quot;3fa21584ae2e4fd74db9b58e2386f5481607dfa4d7ba0617aaa7858e5025dc1e\&quot;\r\nethValidatorAddrKeyB=\&quot;a5f3063552f4483cfc20ac4f40f45b798791379862219de9e915c64722c1d400\&quot;\r\nethValidatorAddrKeyC=\&quot;bbf5e65539e9af0eb0cfac30bad475111054b09c11d668fc0731d54ea777471e\&quot;\r\nethValidatorAddrKeyD=\&quot;c9fa31d7984edf81b8ef3b40c761f1847f6fcd5711ab2462da97dc458f1f896b\&quot;\r\n#      chain33           10 bty    \r\nchain33Validator1=\&quot;1H4zzzQEQQR2FxXwppiMRXcvqLvqzxK2nv\&quot;\r\nchain33Validator2=\&quot;1Nq5AhTgVNvYaWQqih8ZQQEaRk3CFhTDHp\&quot;\r\nchain33Validator3=\&quot;16nmxjF58z5oKK9m44cGy241zMSJWPN1Ty\&quot;\r\nchain33Validator4=\&quot;182nAEMxF1JWWxEWdu4jvd68aZhQumS97H\&quot;\r\nchain33ValidatorKey1=\&quot;0x260124d9c619b0088241ffe2f1d7dc56b0b6100c88c342040387cd62b8ba35a3\&quot;\r\nchain33ValidatorKey2=\&quot;0x7812f8c688048943f1c168f8f2f76f44912de1f0ff8b12358b213118081869b2\&quot;\r\nchain33ValidatorKey3=\&quot;0xd44c8f3d8cac5d9c7fef7b0a0bf7be0909372ec6368064f742193de0bddeb2d1\&quot;\r\nchain33ValidatorKey4=\&quot;0xaad36689ca332026d4a4ceee62c8a91bac7bc100906b25a181a7f28b8552b53e\&quot;\r\nethReceiverAddr1=\&quot;0xa4ea64a583f6e51c3799335b28a8f0529570a635\&quot;\r\nethReceiverAddrKey1=\&quot;355b876d7cbcb930d5dfab767f66336ce327e082cbaa1877210c1bae89b1df71\&quot;\r\nethReceiverAddr2=\&quot;0x0c05ba5c230fdaa503b53702af1962e08d0c60bf\&quot;\r\n#ethReceiverAddrKey2=\&quot;9dc6df3a8ab139a54d8a984f54958ae0661f880229bf3bdbb886b87d58b56a08\&quot;\r\nmaturityDegree=5\r\n#portRelayer=19999\r\nethUrl=\&quot;\&quot;\r\n\r\nCLIA_HTTP=\&quot;\&quot;\r\nCLIB_HTTP=\&quot;\&quot;\r\nCLIC_HTTP=\&quot;\&quot;\r\nCLID_HTTP=\&quot;\&quot;\r\n\r\n# $1 sendAddress, $2 balance\r\nfunction queryExecBalance() {\r\n    local resp=\&quot;\&quot;\r\n    chain33_QueryExecBalance \&quot;${1}\&quot; \&quot;x2ethereum\&quot; \&quot;$MAIN_HTTP\&quot;\r\n    # shellcheck disable=SC2155\r\n    local balance=$(echo \&quot;$resp\&quot; | jq -r \&quot;.result\&quot; | jq \&quot;.[].balance\&quot;)\r\n    if [ \&quot;${balance}\&quot; != \&quot;${2}\&quot; ]; then\r\n        echo_rst \&quot;queryExecBalance\&quot; \&quot;1\&quot; \&quot;${balance} != ${2}\&quot;\r\n    fi\r\n}\r\n\r\n# $1 chain33Address, $2 balance\r\nfunction queryChain33Balance() {\r\n    local resp=\&quot;\&quot;\r\n    chain33_QueryBalance \&quot;${1}\&quot; \&quot;${MAIN_HTTP}\&quot;\r\n    # shellcheck disable=SC2155\r\n    local balance=$(echo $resp | jq -r \&quot;.result.execAccount\&quot; | jq \&quot;.[].account.balance\&quot;)\r\n    if [ \&quot;${balance}\&quot; != \&quot;${2}\&quot; ]; then\r\n        echo_rst \&quot;queryChain33Balance\&quot; \&quot;1\&quot; \&quot;${balance} != ${2}\&quot;\r\n    fi\r\n}\r\n\r\n# $1 req , $2 balance\r\nfunction queryRelayerBalance() {\r\n    chain33_Http \&quot;${1}\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;GetBalance\&quot; \&quot;.result.balance\&quot;\r\n    if [ \&quot;${RETURN_RESP}\&quot; != \&quot;${2}\&quot; ]; then\r\n        echo_rst \&quot;queryRelayerBalance\&quot; \&quot;1\&quot; \&quot;${RETURN_RESP} != ${2}\&quot;\r\n        copyErrLogs\r\n    fi\r\n}\r\n\r\n# $1 req , $2 balance\r\nfunction queryChain33X2ethBalance() {\r\n    chain33_Http \&quot;${req}\&quot; ${MAIN_HTTP} '(.error|not) and (.result != null)' \&quot;GetBalance\&quot; \&quot;.result\&quot;\r\n    # shellcheck disable=SC2155\r\n    local balance=$(echo \&quot;${RETURN_RESP}\&quot; | jq -r \&quot;.res\&quot; | jq \&quot;.[].balance\&quot; | sed 's/\\\&quot;//g')\r\n    if [ \&quot;${balance}\&quot; != \&quot;${2}\&quot; ]; then\r\n        echo_rst \&quot;queryChain33X2ethBalance\&quot; \&quot;1\&quot; \&quot;${balance} != ${2}\&quot;\r\n    fi\r\n}\r\n\r\nfunction start_ebrelayerA() {\r\n    docker cp \&quot;./x2ethereum/relayer.toml\&quot; \&quot;${dockerNamePrefix}_ebrelayera_rpc_1\&quot;:/root/relayer.toml\r\n    start_docker_ebrelayer \&quot;${dockerNamePrefix}_ebrelayera_rpc_1\&quot; \&quot;/root/ebrelayer\&quot; \&quot;./x2ethereum/ebrelayera.log\&quot;\r\n    sleep 5\r\n}\r\n\r\nfunction StartRelayerAndDeploy() {\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME begin ===========${NOC}\&quot;\r\n\r\n    cp ../x2ethereum/* ./x2ethereum/\r\n    for dockerName in ganachetest ebrelayera ebrelayerb ebrelayerc ebrelayerd; do\r\n        line=$(delete_line_show \&quot;./x2ethereum/docker-compose-x2ethereum.yml\&quot; \&quot;${dockerName}:\&quot;)\r\n        sed -i ''\&quot;${line}\&quot;' a \\ \\ '${dockerName}'_rpc:' \&quot;./x2ethereum/docker-compose-x2ethereum.yml\&quot;\r\n    done\r\n\r\n    docker-compose -f ./x2ethereum/docker-compose-x2ethereum.yml up --build -d\r\n    sleep 5\r\n\r\n    # change EthProvider url\r\n    dockerAddr=$(get_docker_addr \&quot;${dockerNamePrefix}_ganachetest_rpc_1\&quot;)\r\n    ethUrl=\&quot;http://${dockerAddr}:8545\&quot;\r\n\r\n    #    relayer.toml     \r\n    updata_relayer_a_toml \&quot;${dockerAddr}\&quot; \&quot;${dockerNamePrefix}_ebrelayera_rpc_1\&quot; \&quot;./x2ethereum/relayer.toml\&quot;\r\n\r\n    line=$(delete_line_show \&quot;./x2ethereum/relayer.toml\&quot; \&quot;localhost:9901\&quot;)\r\n    sed -i ''\&quot;${line}\&quot;' a JrpcBindAddr=\&quot;:9901\&quot;' \&quot;./x2ethereum/relayer.toml\&quot;\r\n    # start ebrelayer A\r\n    start_ebrelayerA\r\n\r\n    ebrelayeraRpcHost=$(get_docker_addr \&quot;${dockerNamePrefix}_ebrelayera_rpc_1\&quot;)\r\n    if [[ ${ebrelayeraRpcHost} == \&quot;\&quot; ]]; then\r\n        echo -e \&quot;${RED}ebrelayeraRpcHost a is empty${NOC}\&quot;\r\n    fi\r\n    CLIA_HTTP=\&quot;http://${ebrelayeraRpcHost}:9901\&quot;\r\n\r\n    #     \r\n    InitAndDeploy\r\n\r\n    #    BridgeRegistry   \r\n    local req='{\&quot;method\&quot;:\&quot;Manager.ShowBridgeRegistryAddr\&quot;,\&quot;params\&quot;:[{}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;$FUNCNAME\&quot; \&quot;.result.addr\&quot;\r\n    local BridgeRegistry=\&quot;$RETURN_RESP\&quot;\r\n\r\n    # kill ebrelayer A\r\n    kill_docker_ebrelayer \&quot;${dockerNamePrefix}_ebrelayera_rpc_1\&quot;\r\n    sleep 1\r\n\r\n    #    relayer.toml     \r\n    updata_relayer_toml \&quot;${BridgeRegistry}\&quot; ${maturityDegree} \&quot;./x2ethereum/relayer.toml\&quot;\r\n    #   \r\n    start_ebrelayerA\r\n\r\n    # start ebrelayer B C D\r\n    for name in b c d; do\r\n        local file=\&quot;./x2ethereum/relayer$name.toml\&quot;\r\n        cp './x2ethereum/relayer.toml' \&quot;${file}\&quot;\r\n\r\n        #              \r\n        for deleteName in \&quot;deployerPrivateKey\&quot; \&quot;operatorAddr\&quot; \&quot;validatorsAddr\&quot; \&quot;initPowers\&quot; \&quot;deployerPrivateKey\&quot; \&quot;deploy\&quot;; do\r\n            delete_line \&quot;${file}\&quot; \&quot;${deleteName}\&quot;\r\n        done\r\n\r\n        sed -i 's/x2ethereum/x2ethereum'${name}'/g' \&quot;${file}\&quot;\r\n\r\n        pushHost=$(get_docker_addr \&quot;${dockerNamePrefix}_ebrelayer${name}_rpc_1\&quot;)\r\n        line=$(delete_line_show \&quot;${file}\&quot; \&quot;pushHost\&quot;)\r\n        sed -i ''\&quot;${line}\&quot;' a pushHost=\&quot;http://'\&quot;${pushHost}\&quot;':20000\&quot;' \&quot;${file}\&quot;\r\n\r\n        line=$(delete_line_show \&quot;${file}\&quot; \&quot;pushBind\&quot;)\r\n        sed -i ''\&quot;${line}\&quot;' a pushBind=\&quot;'\&quot;${pushHost}\&quot;':20000\&quot;' \&quot;${file}\&quot;\r\n\r\n        docker cp \&quot;${file}\&quot; \&quot;${dockerNamePrefix}_ebrelayer${name}_rpc_1\&quot;:/root/relayer.toml\r\n        start_docker_ebrelayer \&quot;${dockerNamePrefix}_ebrelayer${name}_rpc_1\&quot; \&quot;/root/ebrelayer\&quot; \&quot;./x2ethereum/ebrelayer${name}.log\&quot;\r\n    done\r\n    sleep 5\r\n\r\n    ebrelayeraRpcHost=$(get_docker_addr \&quot;${dockerNamePrefix}_ebrelayera_rpc_1\&quot;)\r\n    CLIA_HTTP=\&quot;http://${ebrelayeraRpcHost}:9901\&quot;\r\n    ebrelayeraRpcHost=$(get_docker_addr \&quot;${dockerNamePrefix}_ebrelayerb_rpc_1\&quot;)\r\n    CLIB_HTTP=\&quot;http://${ebrelayeraRpcHost}:9901\&quot;\r\n    ebrelayeraRpcHost=$(get_docker_addr \&quot;${dockerNamePrefix}_ebrelayerc_rpc_1\&quot;)\r\n    CLIC_HTTP=\&quot;http://${ebrelayeraRpcHost}:9901\&quot;\r\n    ebrelayeraRpcHost=$(get_docker_addr \&quot;${dockerNamePrefix}_ebrelayerd_rpc_1\&quot;)\r\n    CLID_HTTP=\&quot;http://${ebrelayeraRpcHost}:9901\&quot;\r\n\r\n    docker ps -a\r\n\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME end ===========${NOC}\&quot;\r\n}\r\n\r\nfunction InitAndDeploy() {\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME begin ===========${NOC}\&quot;\r\n    local req='{\&quot;method\&quot;:\&quot;Manager.SetPassphase\&quot;,\&quot;params\&quot;:[{\&quot;Passphase\&quot;:\&quot;123456hzj\&quot;}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;SetPassphase\&quot; \&quot;.result\&quot;\r\n\r\n    local req='{\&quot;method\&quot;:\&quot;Manager.Unlock\&quot;,\&quot;params\&quot;:[\&quot;123456hzj\&quot;]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;Unlock\&quot; \&quot;.result\&quot;\r\n\r\n    local req='{\&quot;method\&quot;:\&quot;Manager.DeployContrcts\&quot;,\&quot;params\&quot;:[{}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;$FUNCNAME\&quot; \&quot;.result\&quot;\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME end ===========${NOC}\&quot;\r\n}\r\n\r\n# chian33         \r\nfunction InitChain33Vilators() {\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME begin ===========${NOC}\&quot;\r\n    #    chain33Validators       \r\n    chain33_ImportPrivkey \&quot;${chain33ValidatorKey1}\&quot; \&quot;${chain33Validator1}\&quot; \&quot;tokenAddr\&quot; \&quot;${MAIN_HTTP}\&quot;\r\n    chain33_ImportPrivkey \&quot;${chain33ValidatorKey2}\&quot; \&quot;${chain33Validator2}\&quot; \&quot;tokenAddr\&quot; \&quot;${MAIN_HTTP}\&quot;\r\n    chain33_ImportPrivkey \&quot;${chain33ValidatorKey3}\&quot; \&quot;${chain33Validator3}\&quot; \&quot;tokenAddr\&quot; \&quot;${MAIN_HTTP}\&quot;\r\n    chain33_ImportPrivkey \&quot;${chain33ValidatorKey4}\&quot; \&quot;${chain33Validator4}\&quot; \&quot;tokenAddr\&quot; \&quot;${MAIN_HTTP}\&quot;\r\n\r\n    # SetConsensusThreshold\r\n    tx=$(curl -ksd '{\&quot;method\&quot;:\&quot;Chain33.CreateTransaction\&quot;,\&quot;params\&quot;:[{\&quot;execer\&quot;:\&quot;x2ethereum\&quot;,\&quot;actionName\&quot;:\&quot;SetConsensusThreshold\&quot;,\&quot;payload\&quot;:{\&quot;consensusThreshold\&quot;:\&quot;80\&quot;}}]}' ${MAIN_HTTP} | jq -r \&quot;.result\&quot;)\r\n    chain33_SignAndSendTxWait \&quot;$tx\&quot; \&quot;$sendPriKey\&quot; ${MAIN_HTTP} \&quot;SetConsensusThreshold\&quot;\r\n\r\n    # add a validator\r\n    tx=$(curl -ksd '{\&quot;method\&quot;:\&quot;Chain33.CreateTransaction\&quot;,\&quot;params\&quot;:[{\&quot;execer\&quot;:\&quot;x2ethereum\&quot;,\&quot;actionName\&quot;:\&quot;AddValidator\&quot;,\&quot;payload\&quot;:{\&quot;address\&quot;:\&quot;'${chain33Validator1}'\&quot;,\&quot;power\&quot;:\&quot;25\&quot;}}]}' ${MAIN_HTTP} | jq -r \&quot;.result\&quot;)\r\n    chain33_SignAndSendTxWait \&quot;$tx\&quot; \&quot;$sendPriKey\&quot; ${MAIN_HTTP} \&quot;AddValidator\&quot;\r\n    tx=$(curl -ksd '{\&quot;method\&quot;:\&quot;Chain33.CreateTransaction\&quot;,\&quot;params\&quot;:[{\&quot;execer\&quot;:\&quot;x2ethereum\&quot;,\&quot;actionName\&quot;:\&quot;AddValidator\&quot;,\&quot;payload\&quot;:{\&quot;address\&quot;:\&quot;'${chain33Validator2}'\&quot;,\&quot;power\&quot;:\&quot;25\&quot;}}]}' ${MAIN_HTTP} | jq -r \&quot;.result\&quot;)\r\n    chain33_SignAndSendTxWait \&quot;$tx\&quot; \&quot;$sendPriKey\&quot; ${MAIN_HTTP} \&quot;AddValidator\&quot;\r\n    tx=$(curl -ksd '{\&quot;method\&quot;:\&quot;Chain33.CreateTransaction\&quot;,\&quot;params\&quot;:[{\&quot;execer\&quot;:\&quot;x2ethereum\&quot;,\&quot;actionName\&quot;:\&quot;AddValidator\&quot;,\&quot;payload\&quot;:{\&quot;address\&quot;:\&quot;'${chain33Validator3}'\&quot;,\&quot;power\&quot;:\&quot;25\&quot;}}]}' ${MAIN_HTTP} | jq -r \&quot;.result\&quot;)\r\n    chain33_SignAndSendTxWait \&quot;$tx\&quot; \&quot;$sendPriKey\&quot; ${MAIN_HTTP} \&quot;AddValidator\&quot;\r\n    tx=$(curl -ksd '{\&quot;method\&quot;:\&quot;Chain33.CreateTransaction\&quot;,\&quot;params\&quot;:[{\&quot;execer\&quot;:\&quot;x2ethereum\&quot;,\&quot;actionName\&quot;:\&quot;AddValidator\&quot;,\&quot;payload\&quot;:{\&quot;address\&quot;:\&quot;'${chain33Validator4}'\&quot;,\&quot;power\&quot;:\&quot;25\&quot;}}]}' ${MAIN_HTTP} | jq -r \&quot;.result\&quot;)\r\n    chain33_SignAndSendTxWait \&quot;$tx\&quot; \&quot;$sendPriKey\&quot; ${MAIN_HTTP} \&quot;AddValidator\&quot;\r\n\r\n    # query Validators\r\n    chain33_Http '{\&quot;method\&quot;:\&quot;Chain33.Query\&quot;,\&quot;params\&quot;:[{\&quot;execer\&quot;:\&quot;x2ethereum\&quot;,\&quot;funcName\&quot;:\&quot;GetTotalPower\&quot;,\&quot;payload\&quot;:{}}]}' ${MAIN_HTTP} '(.error|not) and (.result != null)' \&quot;GetTotalPower\&quot; \&quot;.result.totalPower\&quot;\r\n    if [ \&quot;${RETURN_RESP}\&quot; != \&quot;100\&quot; ]; then\r\n        echo -e \&quot;${RED}=========== GetTotalPower err: TotalPower = $RETURN_RESP ===========${NOC}\&quot;\r\n    fi\r\n\r\n    # cions     x2ethereum     \r\n    x2eth_addr=$(curl -ksd '{\&quot;method\&quot;:\&quot;Chain33.ConvertExectoAddr\&quot;,\&quot;params\&quot;:[{\&quot;execname\&quot;:\&quot;x2ethereum\&quot;}]}' ${MAIN_HTTP} | jq -r \&quot;.result\&quot;)\r\n    chain33_SendToAddress \&quot;${sendAddress}\&quot; \&quot;${x2eth_addr}\&quot; 20000000000 \&quot;${MAIN_HTTP}\&quot;\r\n    queryExecBalance \&quot;${sendAddress}\&quot; \&quot;20000000000\&quot;\r\n\r\n    # chain33Validator      \r\n    chain33_applyCoins \&quot;${chain33Validator1}\&quot; 1000000000 \&quot;${MAIN_HTTP}\&quot;\r\n    queryChain33Balance \&quot;${chain33Validator1}\&quot; \&quot;1000000000\&quot;\r\n    chain33_applyCoins \&quot;${chain33Validator2}\&quot; 1000000000 \&quot;${MAIN_HTTP}\&quot;\r\n    queryChain33Balance \&quot;${chain33Validator2}\&quot; \&quot;1000000000\&quot;\r\n    chain33_applyCoins \&quot;${chain33Validator3}\&quot; 1000000000 \&quot;${MAIN_HTTP}\&quot;\r\n    queryChain33Balance \&quot;${chain33Validator3}\&quot; \&quot;1000000000\&quot;\r\n    chain33_applyCoins \&quot;${chain33Validator4}\&quot; 1000000000 \&quot;${MAIN_HTTP}\&quot;\r\n    queryChain33Balance \&quot;${chain33Validator4}\&quot; \&quot;1000000000\&quot;\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME end ===========${NOC}\&quot;\r\n}\r\n\r\nfunction EthImportKey() {\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME begin ===========${NOC}\&quot;\r\n\r\n    #   \r\n    local req='{\&quot;method\&quot;:\&quot;Manager.SetPassphase\&quot;,\&quot;params\&quot;:[{\&quot;Passphase\&quot;:\&quot;123456hzj\&quot;}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIB_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;SetPassphase\&quot; \&quot;.result\&quot;\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIC_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;SetPassphase\&quot; \&quot;.result\&quot;\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLID_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;SetPassphase\&quot; \&quot;.result\&quot;\r\n    req='{\&quot;method\&quot;:\&quot;Manager.Unlock\&quot;,\&quot;params\&quot;:[\&quot;123456hzj\&quot;]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;Unlock\&quot; \&quot;.result\&quot;\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIB_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;Unlock\&quot; \&quot;.result\&quot;\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIC_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;Unlock\&quot; \&quot;.result\&quot;\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLID_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;Unlock\&quot; \&quot;.result\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.ImportChain33PrivateKey4EthRelayer\&quot;,\&quot;params\&quot;:[\&quot;'${chain33ValidatorKey1}'\&quot;]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;ImportChain33PrivateKey4EthRelayer\&quot; \&quot;.result\&quot;\r\n    req='{\&quot;method\&quot;:\&quot;Manager.ImportChain33PrivateKey4EthRelayer\&quot;,\&quot;params\&quot;:[\&quot;'${chain33ValidatorKey2}'\&quot;]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIB_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;ImportChain33PrivateKey4EthRelayer\&quot; \&quot;.result\&quot;\r\n    req='{\&quot;method\&quot;:\&quot;Manager.ImportChain33PrivateKey4EthRelayer\&quot;,\&quot;params\&quot;:[\&quot;'${chain33ValidatorKey3}'\&quot;]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIC_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;ImportChain33PrivateKey4EthRelayer\&quot; \&quot;.result\&quot;\r\n    req='{\&quot;method\&quot;:\&quot;Manager.ImportChain33PrivateKey4EthRelayer\&quot;,\&quot;params\&quot;:[\&quot;'${chain33ValidatorKey4}'\&quot;]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLID_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;ImportChain33PrivateKey4EthRelayer\&quot; \&quot;.result\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.ImportChain33RelayerPrivateKey\&quot;,\&quot;params\&quot;:[{\&quot;privateKey\&quot;:\&quot;'${ethValidatorAddrKeyA}'\&quot;}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;ImportChain33RelayerPrivateKey\&quot; \&quot;.result\&quot;\r\n    req='{\&quot;method\&quot;:\&quot;Manager.ImportChain33RelayerPrivateKey\&quot;,\&quot;params\&quot;:[{\&quot;privateKey\&quot;:\&quot;'${ethValidatorAddrKeyB}'\&quot;}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIB_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;ImportChain33RelayerPrivateKey\&quot; \&quot;.result\&quot;\r\n    req='{\&quot;method\&quot;:\&quot;Manager.ImportChain33RelayerPrivateKey\&quot;,\&quot;params\&quot;:[{\&quot;privateKey\&quot;:\&quot;'${ethValidatorAddrKeyC}'\&quot;}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIC_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;ImportChain33RelayerPrivateKey\&quot; \&quot;.result\&quot;\r\n    req='{\&quot;method\&quot;:\&quot;Manager.ImportChain33RelayerPrivateKey\&quot;,\&quot;params\&quot;:[{\&quot;privateKey\&quot;:\&quot;'${ethValidatorAddrKeyD}'\&quot;}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLID_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;ImportChain33RelayerPrivateKey\&quot; \&quot;.result\&quot;\r\n\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME end ===========${NOC}\&quot;\r\n}\r\n\r\nfunction TestChain33ToEthAssets() {\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME begin ===========${NOC}\&quot;\r\n    # token4chain33           bty\r\n    local req='{\&quot;method\&quot;:\&quot;Manager.CreateBridgeToken\&quot;,\&quot;params\&quot;:[\&quot;coins.bty\&quot;]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;CreateBridgeToken\&quot; \&quot;.result.addr\&quot;\r\n    tokenAddrBty=${RETURN_RESP}\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${ethReceiverAddr1}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddrBty}'\&quot;}]}'\r\n    queryRelayerBalance \&quot;$req\&quot; \&quot;0\&quot;\r\n\r\n    # chain33 lock bty\r\n    #shellcheck disable=SC2086\r\n    tx=$(curl -ksd '{\&quot;method\&quot;:\&quot;Chain33.CreateTransaction\&quot;,\&quot;params\&quot;:[{\&quot;execer\&quot;:\&quot;x2ethereum\&quot;,\&quot;actionName\&quot;:\&quot;Chain33ToEthLock\&quot;,\&quot;payload\&quot;:{\&quot;TokenContract\&quot;:\&quot;'${tokenAddrBty}'\&quot;,\&quot;Chain33Sender\&quot;:\&quot;'${sendPriKey}'\&quot;,\&quot;EthereumReceiver\&quot;:\&quot;'${ethReceiverAddr1}'\&quot;,\&quot;Amount\&quot;:\&quot;500000000\&quot;,\&quot;IssuerDotSymbol\&quot;:\&quot;coins.bty\&quot;,\&quot;Decimals\&quot;:\&quot;8\&quot;}}]}' ${MAIN_HTTP} | jq -r \&quot;.result\&quot;)\r\n    chain33_SignAndSendTxWait \&quot;$tx\&quot; \&quot;$sendPriKey\&quot; ${MAIN_HTTP} \&quot;Chain33ToEthLock\&quot;\r\n\r\n    queryExecBalance \&quot;${sendAddress}\&quot; \&quot;19500000000\&quot;\r\n\r\n    eth_block_wait $((maturityDegree + 2)) \&quot;${ethUrl}\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${ethReceiverAddr1}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddrBty}'\&quot;}]}'\r\n    queryRelayerBalance \&quot;$req\&quot; \&quot;5\&quot;\r\n\r\n    # eth burn\r\n    req='{\&quot;method\&quot;:\&quot;Manager.Burn\&quot;,\&quot;params\&quot;:[{\&quot;ownerKey\&quot;:\&quot;'${ethReceiverAddrKey1}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddrBty}'\&quot;,\&quot;chain33Receiver\&quot;:\&quot;'${chain33SenderAddr}'\&quot;,\&quot;amount\&quot;:\&quot;500000000\&quot;}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;Burn\&quot; \&quot;.result\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${ethReceiverAddr1}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddrBty}'\&quot;}]}'\r\n    queryRelayerBalance \&quot;$req\&quot; \&quot;0\&quot;\r\n\r\n    # eth    10    \r\n    eth_block_wait $((maturityDegree + 2)) \&quot;${ethUrl}\&quot;\r\n\r\n    queryExecBalance \&quot;${chain33SenderAddr}\&quot; \&quot;500000000\&quot;\r\n\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME end ===========${NOC}\&quot;\r\n}\r\n\r\n# eth to chain33\r\n#          ,    chain33    ,   eth   \r\nfunction TestETH2Chain33Assets() {\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME begin ===========${NOC}\&quot;\r\n    local req='{\&quot;method\&quot;:\&quot;Manager.ShowBridgeBankAddr\&quot;,\&quot;params\&quot;:[{}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;ShowBridgeBankAddr\&quot; \&quot;.result.addr\&quot;\r\n    bridgeBankAddr=\&quot;${RETURN_RESP}\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${bridgeBankAddr}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;\&quot;}]}'\r\n    queryRelayerBalance \&quot;$req\&quot; \&quot;0\&quot;\r\n\r\n    # eth lock 0.1\r\n    req='{\&quot;method\&quot;:\&quot;Manager.LockEthErc20Asset\&quot;,\&quot;params\&quot;:[{\&quot;ownerKey\&quot;:\&quot;'${ethReceiverAddrKey1}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;\&quot;,\&quot;amount\&quot;:\&quot;100000000000000000\&quot;,\&quot;chain33Receiver\&quot;:\&quot;'${sendAddress}'\&quot;}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;LockEthErc20Asset\&quot; \&quot;.result\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${bridgeBankAddr}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;\&quot;}]}'\r\n    queryRelayerBalance \&quot;$req\&quot; \&quot;0.1\&quot;\r\n\r\n    # eth    10    \r\n    eth_block_wait $((maturityDegree + 2)) \&quot;${ethUrl}\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Chain33.Query\&quot;,\&quot;params\&quot;:[{\&quot;execer\&quot;:\&quot;x2ethereum\&quot;,\&quot;funcName\&quot;:\&quot;GetRelayerBalance\&quot;,\&quot;payload\&quot;:{\&quot;tokenSymbol\&quot;:\&quot;eth\&quot;,\&quot;address\&quot;:\&quot;'${sendAddress}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;0x0000000000000000000000000000000000000000\&quot;}}]}'\r\n    queryChain33X2ethBalance \&quot;${req}\&quot; \&quot;0.1\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${ethReceiverAddr2}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;\&quot;}]}'\r\n    chain33_Http \&quot;${req}\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;GetBalance\&quot; \&quot;.result.balance\&quot;\r\n    local balance=${RETURN_RESP}\r\n\r\n    #    burn 0.1\r\n    tx=$(curl -ksd '{\&quot;method\&quot;:\&quot;Chain33.CreateTransaction\&quot;,\&quot;params\&quot;:[{\&quot;execer\&quot;:\&quot;x2ethereum\&quot;,\&quot;actionName\&quot;:\&quot;Chain33ToEthBurn\&quot;,\&quot;payload\&quot;:{\&quot;TokenContract\&quot;:\&quot;0x0000000000000000000000000000000000000000\&quot;,\&quot;Chain33Sender\&quot;:\&quot;'${sendPriKey}'\&quot;,\&quot;EthereumReceiver\&quot;:\&quot;'${ethReceiverAddr2}'\&quot;,\&quot;Amount\&quot;:\&quot;10000000\&quot;,\&quot;IssuerDotSymbol\&quot;:\&quot;eth\&quot;,\&quot;Decimals\&quot;:\&quot;18\&quot;}}]}' ${MAIN_HTTP} | jq -r \&quot;.result\&quot;)\r\n    chain33_SignAndSendTxWait \&quot;$tx\&quot; \&quot;$sendPriKey\&quot; ${MAIN_HTTP} \&quot;Chain33ToEthBurn\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Chain33.Query\&quot;,\&quot;params\&quot;:[{\&quot;execer\&quot;:\&quot;x2ethereum\&quot;,\&quot;funcName\&quot;:\&quot;GetRelayerBalance\&quot;,\&quot;payload\&quot;:{\&quot;tokenSymbol\&quot;:\&quot;eth\&quot;,\&quot;address\&quot;:\&quot;'${sendAddress}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;0x0000000000000000000000000000000000000000\&quot;}}]}'\r\n    queryChain33X2ethBalance \&quot;${req}\&quot; \&quot;0\&quot;\r\n\r\n    eth_block_wait $((maturityDegree + 2)) \&quot;${ethUrl}\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${bridgeBankAddr}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;\&quot;}]}'\r\n    queryRelayerBalance \&quot;$req\&quot; \&quot;0\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${ethReceiverAddr2}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;\&quot;}]}'\r\n    #queryRelayerBalance \&quot;$req\&quot; \&quot;$(echo \&quot;${balance}+0.1\&quot; | bc)\&quot;\r\n    queryRelayerBalance \&quot;$req\&quot; \&quot;100.1\&quot;\r\n\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME end ===========${NOC}\&quot;\r\n}\r\n\r\nfunction TestETH2Chain33Erc20() {\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME begin ===========${NOC}\&quot;\r\n    # token4erc20   chain33     token,   mint\r\n    local req='{\&quot;method\&quot;:\&quot;Manager.CreateERC20Token\&quot;,\&quot;params\&quot;:[\&quot;testc\&quot;]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;CreateERC20Token\&quot; \&quot;.result.addr\&quot;\r\n    tokenAddr=\&quot;${RETURN_RESP}\&quot;\r\n\r\n    #     1000\r\n    req='{\&quot;method\&quot;:\&quot;Manager.MintErc20\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${ethReceiverAddr1}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddr}'\&quot;,\&quot;amount\&quot;:\&quot;100000000000\&quot;}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;MintErc20\&quot; \&quot;.result.addr\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${ethReceiverAddr1}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddr}'\&quot;}]}'\r\n    queryRelayerBalance \&quot;$req\&quot; \&quot;1000\&quot;\r\n\r\n    local req='{\&quot;method\&quot;:\&quot;Manager.ShowBridgeBankAddr\&quot;,\&quot;params\&quot;:[{}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;ShowBridgeBankAddr\&quot; \&quot;.result.addr\&quot;\r\n    bridgeBankAddr=\&quot;${RETURN_RESP}\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${bridgeBankAddr}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddr}'\&quot;}]}'\r\n    queryRelayerBalance \&quot;$req\&quot; \&quot;0\&quot;\r\n\r\n    # lock 100\r\n    req='{\&quot;method\&quot;:\&quot;Manager.LockEthErc20Asset\&quot;,\&quot;params\&quot;:[{\&quot;ownerKey\&quot;:\&quot;'${ethReceiverAddrKey1}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddr}'\&quot;,\&quot;amount\&quot;:\&quot;10000000000\&quot;,\&quot;chain33Receiver\&quot;:\&quot;'${chain33Validator1}'\&quot;}]}'\r\n    chain33_Http \&quot;$req\&quot; \&quot;${CLIA_HTTP}\&quot; '(.error|not) and (.result != null)' \&quot;LockEthErc20Asset\&quot; \&quot;.result\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${ethReceiverAddr1}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddr}'\&quot;}]}'\r\n    queryRelayerBalance \&quot;$req\&quot; \&quot;900\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${bridgeBankAddr}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddr}'\&quot;}]}'\r\n    queryRelayerBalance \&quot;$req\&quot; \&quot;100\&quot;\r\n\r\n    # eth    10    \r\n    eth_block_wait $((maturityDegree + 2)) \&quot;${ethUrl}\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Chain33.Query\&quot;,\&quot;params\&quot;:[{\&quot;execer\&quot;:\&quot;x2ethereum\&quot;,\&quot;funcName\&quot;:\&quot;GetRelayerBalance\&quot;,\&quot;payload\&quot;:{\&quot;tokenSymbol\&quot;:\&quot;testc\&quot;,\&quot;address\&quot;:\&quot;'${chain33Validator1}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddr}'\&quot;}}]}'\r\n    queryChain33X2ethBalance \&quot;${req}\&quot; \&quot;100\&quot;\r\n\r\n    # chain33 burn 100\r\n    #shellcheck disable=SC2086\r\n    tx=$(curl -ksd '{\&quot;method\&quot;:\&quot;Chain33.CreateTransaction\&quot;,\&quot;params\&quot;:[{\&quot;execer\&quot;:\&quot;x2ethereum\&quot;,\&quot;actionName\&quot;:\&quot;Chain33ToEthBurn\&quot;,\&quot;payload\&quot;:{\&quot;TokenContract\&quot;:\&quot;'${tokenAddr}'\&quot;,\&quot;Chain33Sender\&quot;:\&quot;'${chain33ValidatorKey1}'\&quot;,\&quot;EthereumReceiver\&quot;:\&quot;'${ethReceiverAddr2}'\&quot;,\&quot;Amount\&quot;:\&quot;10000000000\&quot;,\&quot;IssuerDotSymbol\&quot;:\&quot;testc\&quot;,\&quot;Decimals\&quot;:\&quot;8\&quot;}}]}' ${MAIN_HTTP} | jq -r \&quot;.result\&quot;)\r\n    chain33_SignAndSendTxWait \&quot;$tx\&quot; \&quot;$chain33ValidatorKey1\&quot; ${MAIN_HTTP} \&quot;Chain33ToEthBurn\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Chain33.Query\&quot;,\&quot;params\&quot;:[{\&quot;execer\&quot;:\&quot;x2ethereum\&quot;,\&quot;funcName\&quot;:\&quot;GetRelayerBalance\&quot;,\&quot;payload\&quot;:{\&quot;tokenSymbol\&quot;:\&quot;testc\&quot;,\&quot;address\&quot;:\&quot;'${chain33Validator1}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddr}'\&quot;}}]}'\r\n    queryChain33X2ethBalance \&quot;${req}\&quot; \&quot;0\&quot;\r\n\r\n    eth_block_wait $((maturityDegree + 2)) \&quot;${ethUrl}\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${ethReceiverAddr2}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddr}'\&quot;}]}'\r\n    queryRelayerBalance \&quot;$req\&quot; \&quot;100\&quot;\r\n\r\n    req='{\&quot;method\&quot;:\&quot;Manager.GetBalance\&quot;,\&quot;params\&quot;:[{\&quot;owner\&quot;:\&quot;'${bridgeBankAddr}'\&quot;,\&quot;tokenAddr\&quot;:\&quot;'${tokenAddr}'\&quot;}]}'\r\n    queryRelayerBalance \&quot;$req\&quot; \&quot;0\&quot;\r\n\r\n    echo -e \&quot;${GRE}=========== $FUNCNAME end ===========${NOC}\&quot;\r\n}\r\n\r\nfunction rpc_test() {\r\n    set +e\r\n    set -x\r\n    chain33_RpcTestBegin x2ethereum\r\n    MAIN_HTTP=\&quot;$1\&quot;\r\n    dockerNamePrefix=\&quot;$2\&quot;\r\n    echo \&quot;main_ip=$MAIN_HTTP\&quot;\r\n\r\n    ispara=$(echo '\&quot;'\&quot;${MAIN_HTTP}\&quot;'\&quot;' | jq '.|contains(\&quot;8901\&quot;)')\r\n    if [ \&quot;$ispara\&quot; == false ]; then\r\n        # init\r\n        StartRelayerAndDeploy\r\n        InitChain33Vilators\r\n        EthImportKey\r\n\r\n        # test\r\n        TestChain33ToEthAssets\r\n        TestETH2Chain33Assets\r\n        TestETH2Chain33Erc20\r\n\r\n        copyErrLogs\r\n\r\n        docker-compose -f ./x2ethereum/docker-compose-x2ethereum.yml down\r\n    fi\r\n    chain33_RpcTestRst x2ethereum \&quot;$CASE_ERR\&quot;\r\n}\r\n\r\nchain33_debug_function rpc_test \&quot;$1\&quot; \&quot;$2\&quot;\r\n&quot;}}},{&quot;rowIdx&quot;:45,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;set -e\n\nSCRIPT_DIR=\&quot;$( cd \&quot;$( dirname \&quot;${BASH_SOURCE[0]}\&quot; )\&quot; &amp;&amp; pwd )\&quot;\n\nsource $SCRIPT_DIR/set-kubeconfig.sh\n\nopen http://127.0.0.1:4040\n\nkubectl port-forward -n kube-system \&quot;$(kubectl get -n kube-system pod --selector=weave-scope-component=app -o jsonpath='{.items..metadata.name}')\&quot; 4040\n&quot;}}},{&quot;rowIdx&quot;:46,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;setup_emsdk()\n{\n    if [ ! -d \&quot;./tools/emsdk\&quot; ]\n    then\n        if [ ! -d \&quot;./tools\&quot; ]\n        then\n            mkdir ./tools\n        fi\n\n        cd ./tools        \n        git clone https://github.com/emscripten-core/emsdk.git\n        \n        cd ./emsdk\n        git pull\n\n        ./emsdk install latest\n        ./emsdk activate latest\n        \n        cd ../..\n\n    fi\n}\n&quot;}}},{&quot;rowIdx&quot;:47,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=16\n#SBATCH --mem=8gb\n\nmodule load fastqc\nmodule load trimgalore\nmodule load samtools\nmodule load bowtie2\nmodule load bedtools\n\n### Fastqc for untrimmed files\ncd /gpfs/group/pipkin/hdiao/T_Cell_ChIP/0_fastq\nfastq_untrimmed_1=SRR1731132_1.fastq\nfastqc $fastq_untrimmed_1\n\n### Trim Galore\ntrim_galore --length 24 --stringency 3 $fastq_untrimmed_1\ntrim_fastq_end1=/gpfs/group/pipkin/hdiao/T_Cell_ChIP/0_fastq/SRR1731132_1_trimmed.fq\n\n### Fastqc for trimmed files\nfastqc $trim_fastq_end1\n\n### Bowtie2 alignment\ncd /gpfs/group/pipkin/hdiao/T_Cell_ChIP/1_bowtie2\nbowtie2_index=/gpfs/group/pipkin/hdiao/ref_resources/mm/release102/GRCm38\nsam_name=SRR1731132.sam\nbowtie2 -p 16 -x $bowtie2_index -U $trim_fastq_end1 -S $sam_name\n\n### Convert/sort/filter\nbam_name=SRR1731132.bam\nbam_name_srt=SRR1731132_srt.sam\nsam_name_srt_dupr=SRR1731132_srt_dupr.sam\nbam_name_srt_dupr=SRR1731132_srt_dupr.bam\nflb_bam_name=SRR1731132_srt_dupr_flb.bam\nblacklist_bed=/gpfs/group/pipkin/hdiao/ref_resources/mm/mm10_blacklisted_2016_nochr.bed\n\nsamtools view -bS $sam_name > $bam_name\nsamtools sort $bam_name -o $bam_name_srt\nsamtools rmdup -S $bam_name_srt $sam_name_srt_dupr\nsamtools view -bS $sam_name_srt_dupr > $bam_name_srt_dupr\nbedtools intersect -abam $bam_name_srt_dupr -b $blacklist_bed -v > $flb_bam_name\n\n### Remove intermediate files\nfilesize=$(stat -c%s $flb_bam_name)\nif (( filesize > 10000 )) \nthen\n    rm $sam_name\n    rm $bam_name\n    rm $bam_name_srt\n    rm $sam_name_srt_dupr\n    rm $bam_name_srt_dupr\n    rm $trim_fastq_end1\nfi\n&quot;}}},{&quot;rowIdx&quot;:48,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash -x\n\n# Exit on error\nset -e\n\n# Test code goes here\nrm -rf valid_lowisbet valid_lowisbet_?.log\nmkdir -p valid_lowisbet\n\nextra_opts=\&quot;--no-shuffle --seed 1111 --maxi-batch 1 --maxi-batch-sort none\&quot;\nextra_opts=\&quot;$extra_opts --dim-emb 64 --dim-rnn 128 --mini-batch 32\&quot;\nextra_opts=\&quot;$extra_opts --cost-type ce-mean --disp-label-counts false --clip-norm 0\&quot;\n\n\n# Files for the validation sets are swapped intentionally\n$MRT_MARIAN/marian $extra_opts \\\n    -m valid_lowisbet/model.npz -t $MRT_DATA/train.max50.{en,de} -v vocab.en.yml vocab.de.yml \\\n    --disp-freq 10 --valid-freq 30 --after-batches 160 --early-stopping 2 \\\n    --valid-metrics cross-entropy --valid-sets $MRT_DATA/europarl.de-en/toy.bpe.{de,en} --valid-mini-batch 64 \\\n    --valid-log valid_lowisbet_1.log\n\ntest -e valid_lowisbet/model.npz\ntest -e valid_lowisbet/model.npz.yml\ntest -e valid_lowisbet_1.log\n\ncp valid_lowisbet/model.npz.progress.yml valid_lowisbet/model.npz.progress.yml.bac\ncat valid_lowisbet_1.log | $MRT_TOOLS/strip-timestamps.sh | grep \&quot;cross-entropy\&quot; > valid_lowisbet.out\n\n# Files for the validation sets are swapped intentionally\n$MRT_MARIAN/marian $extra_opts \\\n    -m valid_lowisbet/model.npz -t $MRT_DATA/train.max50.{en,de} -v vocab.en.yml vocab.de.yml \\\n    --disp-freq 10 --valid-freq 30 --after-batches 320 --early-stopping 4 \\\n    --valid-metrics cross-entropy --valid-sets $MRT_DATA/europarl.de-en/toy.bpe.{de,en} --valid-mini-batch 64 \\\n    --valid-log valid_lowisbet_2.log\n\ntest -e valid_lowisbet/model.npz\ntest -e valid_lowisbet_2.log\n\ncat valid_lowisbet_2.log | $MRT_TOOLS/strip-timestamps.sh | grep \&quot;cross-entropy\&quot; >> valid_lowisbet.out\n$MRT_TOOLS/diff-nums.py -p 0.1 valid_lowisbet.out valid_lowisbet.expected -o valid_lowisbet.diff\n\n# Exit with success code\nexit 0\n&quot;}}},{&quot;rowIdx&quot;:49,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/usr/bin/env bash\n\n# --------------------------------------------------------------------------\n#  Copyright 2013-2016 Sam Deane, Elegant Chaos. All rights reserved.\n#  This source code is distributed under the terms of Elegant Chaos's\n#  liberal license: http://www.elegantchaos.com/license/liberal\n# --------------------------------------------------------------------------\n\n## This script is an attempt to automate picking up the latest version of the \&quot;develop\&quot; branch for a module, given that it might be on a detatch HEAD at the time.\n##\n## It performs the following steps:\n##\n## - rebase on the local develop branch\n## - save this to a temporary branch\n## - switch to the local develop branch\n## - merge in the temporary branch - this should be a fast forward\n## - remove the temporary branch\n## - rebase on the remote \&quot;develop\&quot; from origin\n## - push the resulting changed branch back to origin\n\ncheck() {\n    if [[ $1 != 0 ]]; then\n      echo \&quot;failed: $2\&quot;\n      exit $1\n    fi\n}\n\nstatus=`git status --porcelain`\n\nif [[ \&quot;$status\&quot; != \&quot;\&quot; ]]; then\n    echo \&quot;You have local changes. Commit them first.\&quot;\n    exit 1\nfi\n\n# we may start on something that isn't the develop branch\n# possibly a detached HEAD\n\n# try to apply any changes on top of our local develop\n\ngit rebase develop\ncheck $? \&quot;rebasing on develop\&quot;\n\n# now fast forward develop to the merged place\n\ngit checkout -b develop-temp\ncheck $? \&quot;making temp branch\&quot;\n\ngit checkout develop\ncheck $? \&quot;switching back to develop\&quot;\n\ngit merge develop-temp\ncheck $? \&quot;merging local changes\&quot;\n\ngit branch -d develop-temp\ncheck $? \&quot;removing temp branch\&quot;\n\n# we should now be on a local develop branch incorporating any local changes\necho fetching latest revisions\ngit fetch\n\n# try to rebase again on top of any remote changes\n\ngit rebase\ncheck $? \&quot;rebasing on origin/develop\&quot;\n\n# if that worked, push back the merged version\n\ngit push\ncheck $? \&quot;pushing\&quot;\n\n\n&quot;}}},{&quot;rowIdx&quot;:50,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\nCUDA_VISIBLE_DEVICES=\&quot;0\&quot; python runner.py \n\n&quot;}}},{&quot;rowIdx&quot;:51,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\nrm -rf dist || exit 0;\nmkdir dist;\nnpm run build\ncp ./ops/CNAME ./dist\n( cd dist\n git init\n git add .\n git commit -m \&quot;Deployed to Github Pages\&quot;\n git push --force git@github.com:anvaka/npmgraph.an.git master:gh-pages\n)\n&quot;}}},{&quot;rowIdx&quot;:52,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;find . -name '*.go' -exec sed -i 's?k8s.io/kubernetes/plugin/pkg/scheduler?github.com/KubeDevice/kube-scheduler/pkg?g' {} +\n&quot;}}},{&quot;rowIdx&quot;:53,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/usr/bin/env bash\n\n#\n#\tMetaCall Configuration Environment Bash Script by Parra Studios\n#\tConfigure and install MetaCall environment script utility.\n#\n#\tCopyright (C) 2016 - 2021 Vicente Eduardo Ferrer Garcia <vic798@gmail.com>\n#\n#\tLicensed under the Apache License, Version 2.0 (the \&quot;License\&quot;);\n#\tyou may not use this file except in compliance with the License.\n#\tYou may obtain a copy of the License at\n#\n#\t\thttp://www.apache.org/licenses/LICENSE-2.0\n#\n#\tUnless required by applicable law or agreed to in writing, software\n#\tdistributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n#\tWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#\tSee the License for the specific language governing permissions and\n#\tlimitations under the License.\n#\n\nROOT_DIR=$(pwd)\n\nRUN_AS_ROOT=0\nSUDO_CMD=sudo\nINSTALL_APT=1\nINSTALL_PYTHON=0\nINSTALL_RUBY=0\nINSTALL_NETCORE=0\nINSTALL_NETCORE2=0\nINSTALL_NETCORE5=0\nINSTALL_V8=0\nINSTALL_NODEJS=0\nINSTALL_TYPESCRIPT=0\nINSTALL_FILE=0\nINSTALL_RPC=0\nINSTALL_PORTS=0\nINSTALL_CLEAN=0\nSHOW_HELP=0\nPROGNAME=$(basename $0)\n\n# Install and mark packages to avoid autoremove\nsub_apt_install_hold(){\n\t$SUDO_CMD apt-get -y install --no-install-recommends $@\n\t$SUDO_CMD apt-mark hold $@\n}\n\n# Base packages\nsub_apt(){\n\techo \&quot;configure apt\&quot;\n\tcd $ROOT_DIR\n\t$SUDO_CMD apt-get update &amp;&amp; apt-get -y install --no-install-recommends wget gpg apt-transport-https\n}\n\n# Python\nsub_python(){\n\techo \&quot;configure python\&quot;\n\tcd $ROOT_DIR\n\tsub_apt_install_hold python3 libpython3.7\n}\n\n# Ruby\nsub_ruby(){\n\techo \&quot;configure ruby\&quot;\n\tcd $ROOT_DIR\n\n\t# TODO: Remove this when using ruby2.5 (not available yet because it fails on loading a script with a malloc error)\n\t$SUDO_CMD mv /etc/apt/sources.list /etc/apt/sources.list.backup\n\t$SUDO_CMD sh -c \&quot;echo \\\&quot;deb http://ftp.debian.org/debian/ stretch main\\\&quot; > /etc/apt/sources.list\&quot;\n\t$SUDO_CMD sh -c \&quot;echo \\\&quot;deb-src http://ftp.debian.org/debian/ stretch main\\\&quot; >> /etc/apt/sources.list\&quot;\n\t$SUDO_CMD sh -c \&quot;echo \\\&quot;deb http://security.debian.org/debian-security stretch/updates main\\\&quot; >> /etc/apt/sources.list\&quot;\n\t$SUDO_CMD sh -c \&quot;echo \\\&quot;deb-src http://security.debian.org/debian-security stretch/updates main\\\&quot; >> /etc/apt/sources.list\&quot;\n\t$SUDO_CMD sh -c \&quot;echo \\\&quot;deb http://ftp.debian.org/debian/ stretch-updates main\\\&quot; >> /etc/apt/sources.list\&quot;\n\t$SUDO_CMD sh -c \&quot;echo \\\&quot;deb-src http://ftp.debian.org/debian/ stretch-updates main\\\&quot; >> /etc/apt/sources.list\&quot;\n\n\t$SUDO_CMD apt-get update\n\t# sub_apt_install_hold ruby2.5 libruby2.5\n\t$SUDO_CMD apt-get -y install --no-install-recommends --allow-remove-essential --allow-downgrades libssl1.1 libffi6 zlib1g libyaml-0-2 libgmp10=2:6.1.2+dfsg-1 libreadline7 libxml2 libncurses5 libtinfo5 ruby2.3 libruby2.3\n\t$SUDO_CMD apt-mark hold libssl1.1 libffi6 zlib1g libyaml-0-2 libgmp10 libreadline7 libxml2 libncurses5 libtinfo5 ruby2.3 libruby2.3\n\n\t# TODO: Remove this when using ruby2.5 (not available yet because it fails on loading a script with a malloc error)\n\t$SUDO_CMD mv /etc/apt/sources.list.backup /etc/apt/sources.list\n}\n\n# NetCore\nsub_netcore(){\n\techo \&quot;configure netcore\&quot;\n\tcd $ROOT_DIR\n\n\t# Debian Stretch\n\n\tsub_apt_install_hold libc6 libcurl3 libgcc1 libgssapi-krb5-2 libicu57 \\\n\t\tliblttng-ust0 libssl1.0.2 libstdc++6 libunwind8 libuuid1 zlib1g ca-certificates\n\n\t# Install .NET Core Runtime 1.x\n\tDOTNET_VERSION=1.1.10\n\tDOTNET_DOWNLOAD_URL=https://dotnetcli.blob.core.windows.net/dotnet/Runtime/$DOTNET_VERSION/dotnet-debian.9-x64.$DOTNET_VERSION.tar.gz\n\n\twget $DOTNET_DOWNLOAD_URL -O dotnet.tar.gz\n\tmkdir -p /usr/share/dotnet\n\ttar -zxf dotnet.tar.gz -C /usr/share/dotnet\n\trm dotnet.tar.gz\n\tln -s /usr/share/dotnet/dotnet /usr/bin/dotnet\n}\n\n# NetCore 2\nsub_netcore2(){\n\techo \&quot;configure netcore 2\&quot;\n\tcd $ROOT_DIR\n\n\t# Install NET Core Runtime 2.x\n\twget https://packages.microsoft.com/config/debian/10/packages-microsoft-prod.deb -O packages-microsoft-prod.deb\n\t$SUDO_CMD dpkg -i packages-microsoft-prod.deb\n\trm packages-microsoft-prod.deb\n\n\t$SUDO_CMD apt-get update\n\tsub_apt_install_hold dotnet-runtime-2.2=2.2.8-1\n}\n\n# NetCore 5\nsub_netcore5(){\n\techo \&quot;configure netcore 5\&quot;\n\tcd $ROOT_DIR\n\n\t# Install NET Core Runtime 5.x\n\twget https://packages.microsoft.com/config/debian/10/packages-microsoft-prod.deb -O packages-microsoft-prod.deb\n\t$SUDO_CMD dpkg -i packages-microsoft-prod.deb\n\trm packages-microsoft-prod.deb\n\n\t$SUDO_CMD apt-get update\n\tsub_apt_install_hold dotnet-runtime-5.0=5.0.9-1\n}\n\n# V8\nsub_v8(){\n\techo \&quot;configure v8\&quot;\n\t# TODO\n}\n\n# NodeJS\nsub_nodejs(){\n\techo \&quot;configure node\&quot;\n\n\t# Install NodeJS library\n\tsub_apt_install_hold libnode83\n}\n\n# TypeScript\nsub_typescript(){\n\techo \&quot;configure typescript\&quot;\n\t# Nothing needed, node_modules are local to the path,\n\t# runtime is located in /usr/local/lib, and node builtins\n\t# are already compiled in the runtime\n}\n\n# File\nsub_file(){\n\techo \&quot;configure file\&quot;\n\t# Nothing needed\n}\n\n# RPC\nsub_rpc(){\n\techo \&quot;configure rpc\&quot;\n\n\tsub_apt_install_hold libcurl4\n}\n\n# Ports\nsub_ports(){\n\techo \&quot;configure ports\&quot;\n\n\t# Nothing needed, there are no dependencies for ports by now\n}\n\n# Install\nsub_install(){\n\tif [ $RUN_AS_ROOT = 1 ]; then\n\t\tSUDO_CMD=\&quot;\&quot;\n\tfi\n\tif [ $INSTALL_APT = 1 ]; then\n\t\tsub_apt\n\tfi\n\tif [ $INSTALL_PYTHON = 1 ]; then\n\t\tsub_python\n\tfi\n\tif [ $INSTALL_RUBY = 1 ]; then\n\t\tsub_ruby\n\tfi\n\tif [ $INSTALL_NETCORE = 1 ]; then\n\t\tsub_netcore\n\tfi\n\tif [ $INSTALL_NETCORE2 = 1 ]; then\n\t\tsub_netcore2\n\tfi\n\tif [ $INSTALL_NETCORE5 = 1 ]; then\n\t\tsub_netcore5\n\tfi\n\tif [ $INSTALL_V8 = 1 ]; then\n\t\tsub_v8\n\tfi\n\tif [ $INSTALL_NODEJS = 1 ]; then\n\t\tsub_nodejs\n\tfi\n\tif [ $INSTALL_TYPESCRIPT = 1 ]; then\n\t\tsub_typescript\n\tfi\n\tif [ $INSTALL_FILE = 1 ]; then\n\t\tsub_file\n\tfi\n\tif [ $INSTALL_RPC = 1 ]; then\n\t\tsub_rpc\n\tfi\n\tif [ $INSTALL_PORTS = 1 ]; then\n\t\tsub_ports\n\tfi\n\tif [ $INSTALL_CLEAN = 1 ]; then\n\t\tsub_clean\n\tfi\n\n\techo \&quot;install finished in workspace $ROOT_DIR\&quot;\n}\n\n# Clean dependencies\nsub_clean(){\n\techo \&quot;clean dependencies\&quot;\n\n\t$SUDO_CMD apt-get -y remove wget gpg\n\t$SUDO_CMD apt-get -y autoclean\n}\n\n# Configuration\nsub_options(){\n\tfor var in \&quot;$@\&quot;\n\tdo\n\t\tif [ \&quot;$var\&quot; = 'root' ]; then\n\t\t\techo \&quot;running as root\&quot;\n\t\t\tRUN_AS_ROOT=1\n\t\tfi\n\t\tif [ \&quot;$var\&quot; = 'base' ]; then\n\t\t\techo \&quot;apt selected\&quot;\n\t\t\tINSTALL_APT=1\n\t\tfi\n\t\tif [ \&quot;$var\&quot; = 'python' ]; then\n\t\t\techo \&quot;python selected\&quot;\n\t\t\tINSTALL_PYTHON=1\n\t\tfi\n\t\tif [ \&quot;$var\&quot; = 'ruby' ]; then\n\t\t\techo \&quot;ruby selected\&quot;\n\t\t\tINSTALL_RUBY=1\n\t\tfi\n\t\tif [ \&quot;$var\&quot; = 'netcore' ]; then\n\t\t\techo \&quot;netcore selected\&quot;\n\t\t\tINSTALL_NETCORE=1\n\t\tfi\n\t\tif [ \&quot;$var\&quot; = 'netcore2' ]; then\n\t\t\techo \&quot;netcore 2 selected\&quot;\n\t\t\tINSTALL_NETCORE2=1\n\t\tfi\n\t\tif [ \&quot;$var\&quot; = 'netcore5' ]; then\n\t\t\techo \&quot;netcore 5 selected\&quot;\n\t\t\tINSTALL_NETCORE5=1\n\t\tfi\n\t\tif [ \&quot;$var\&quot; = 'v8' ]; then\n\t\t\techo \&quot;v8 selected\&quot;\n\t\t\tINSTALL_V8=1\n\t\tfi\n\t\tif [ \&quot;$var\&quot; = 'nodejs' ]; then\n\t\t\techo \&quot;nodejs selected\&quot;\n\t\t\tINSTALL_NODEJS=1\n\t\tfi\n\t\tif [ \&quot;$var\&quot; = 'typescript' ]; then\n\t\t\techo \&quot;typescript selected\&quot;\n\t\t\tINSTALL_TYPESCRIPT=1\n\t\tfi\n\t\tif [ \&quot;$var\&quot; = 'file' ]; then\n\t\t\techo \&quot;file selected\&quot;\n\t\t\tINSTALL_FILE=1\n\t\tfi\n\t\tif [ \&quot;$var\&quot; = 'rpc' ]; then\n\t\t\techo \&quot;rpc selected\&quot;\n\t\t\tINSTALL_RPC=1\n\t\tfi\n\t\tif [ \&quot;$var\&quot; = 'ports' ]; then\n\t\t\techo \&quot;ports selected\&quot;\n\t\t\tINSTALL_PORTS=1\n\t\tfi\n\t\tif [ \&quot;$var\&quot; = 'clean' ]; then\n\t\t\techo \&quot;clean selected\&quot;\n\t\t\tINSTALL_CLEAN=1\n\t\tfi\n\tdone\n}\n\n# Help\nsub_help() {\n\techo \&quot;Usage: `basename \&quot;$0\&quot;` list of component\&quot;\n\techo \&quot;Components:\&quot;\n\techo \&quot;\troot\&quot;\n\techo \&quot;\tbase\&quot;\n\techo \&quot;\tpython\&quot;\n\techo \&quot;\truby\&quot;\n\techo \&quot;\tnetcore\&quot;\n\techo \&quot;\tnetcore2\&quot;\n\techo \&quot;\tv8\&quot;\n\techo \&quot;\tnodejs\&quot;\n\techo \&quot;\ttypescript\&quot;\n\techo \&quot;\tfile\&quot;\n\techo \&quot;\trpc\&quot;\n\techo \&quot;\tports\&quot;\n\techo \&quot;\tclean\&quot;\n\techo \&quot;\&quot;\n}\n\ncase \&quot;$#\&quot; in\n\t0)\n\t\tsub_help\n\t\t;;\n\t*)\n\t\tsub_options $@\n\t\tsub_install\n\t\t;;\nesac\n&quot;}}},{&quot;rowIdx&quot;:54,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#\n# Copyright (C) 2013 Julian Atienza Herrero <j.atienza at har.mrc.ac.uk>\n#\n# MEDICAL RESEARCH COUNCIL UK MRC\n#\n# Harwell Mammalian Genetics Unit\n#\n# http://www.har.mrc.ac.uk\n#\n# Licensed under the Apache License, Version 2.0 (the \&quot;License\&quot;); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n#\n\n\nrm -rf `find . -type d -name .svn`\n\nsvn delete Â svn://source.har.mrc.ac.uk/PhenoDCC/exportlibrary/trunk/exportlibrary -m \&quot;working copy broken\&quot;\n\nsvn import exportlibrary svn://source.har.mrc.ac.uk/PhenoDCC/exportlibrary/trunk/exportlibrary -m \&quot;first commit after broken working copy\&quot;\n\nsvn co  svn://source.har.mrc.ac.uk/PhenoDCC/exportlibrary/trunk/exportlibrary\n\n\n\nsvn delete Â svn://source.har.mrc.ac.uk/PhenoDCC/exportlibrary/trunk/exportlibrary/exportlibrary.utils/src/test/db/\n\nsvn delete Â svn://source.har.mrc.ac.uk/PhenoDCC/exportlibrary/trunk/exportlibrary/exportlibrary.xmlvalidation/hsqldb/ -m \&quot;remove test dabases\&quot;\n\nsvn delete Â svn://source.har.mrc.ac.uk/PhenoDCC/exportlibrary/trunk/exportlibrary/exportlibrary.xmlvalidation/src/main/generated/ -m \&quot;removed generated classes\&quot;\n&quot;}}},{&quot;rowIdx&quot;:55,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/sh\n#NEWROOT=:pserver:$(id -un)@cvs.bh.exept.de:/cvs/stx\n\nif [ -z \&quot;$1\&quot; ]; then\n    echo \&quot;Common CVS roots:\&quot;\n    # Do not show these to other people, these are useless for them\n    # anyway\n    if [ \&quot;$USER\&quot; = \&quot;jv\&quot; ]; then\n\techo \&quot; :ext:vrany@exeptn:/cvs/stx       \&quot;\n\techo\n\techo \&quot; :ext:vrany@dialin.exept.de:/cvs/stx           \&quot;\n\techo\n    fi\ncat <<EOF\n :pserver:cvs@cvs.smalltalk-x.de:/cvs/stx\n     (public eXept CVS, synced once a day. Use this if unsure)\n\n :ext:swing.fit.cvut.cz/var/local/cvs\n     (SWING mirror. Use this if you have shell account\n      on swing.fit.cvut.cz)\n\nEOF\n    echo -n \&quot;Enter new CVS root (or Ctrl-C to abort): \&quot;\n    read answer\nelse\n    answer=\&quot;$1\&quot;\nfi\n\nif [ ! -z \&quot;$answer\&quot; ]; then\n    echo \&quot;$answer\&quot; > /tmp/chcvs.$$\n    find . -name CVS -type d -exec cp /tmp/chcvs.$$ {}/Root \\;\n    rm /tmp/chcvs.$$\nelse\n    echo \&quot;Nothing changed\&quot;\n    exit 1\t\nfi\n&quot;}}},{&quot;rowIdx&quot;:56,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\nREGEX=\&quot;https:\\/\\/[a-z]{7}.*.iso\&quot;\nLink=$(wget -qO- www.debian.org | grep -Eo $REGEX)\necho $Link\nwget $Link -P ~/\n\n#wget -qO- www.debian.org |\n#grep -Eoi '<a [^>]+>' |\n#grep -Eo 'href=\&quot;[^\\\&quot;]+\&quot;' |\n#grep -Eo '(http|https)://[^/\&quot;]+'\n&quot;}}},{&quot;rowIdx&quot;:57,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\nkubectl get secret --namespace monitoring grafana -o jsonpath=\&quot;{.data.admin-password}\&quot; | base64 --decode ; echo\n\nexport POD_NAME=$(kubectl get pods --namespace monitoring -l \&quot;app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\&quot; -o jsonpath=\&quot;{.items[0].metadata.name}\&quot;)\nkubectl --namespace monitoring port-forward $POD_NAME 3000&quot;}}},{&quot;rowIdx&quot;:58,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;data_directory=../../model_training_data/Fine-ET/RS_NDS_from_2\nckpt_directory=$data_directory/ckpts\ndataset_file=../../datasets/sampled_datasets/RS_NDS_from_2.json.gz\n\nmkdir -p $data_directory\nmkdir -p $ckpt_directory\n\n\n# Model parameters\nrnn_hidden_neurons=200\nkeep_prob=0.5\nlearning_rate=0.002\nbatch_size=500\nchar_embedding_size=200\nchar_rnn_hidden_neurons=50\njoint_embedding_size=500\nepochs=15   # A number such that approx 19-20 million EMs are used in training\nsave_checkpoint_after=600000 # Total entities in RS_NDS_from_2: 1291232\n\n# http://nlp.stanford.edu/data/glove.840B.300d.zip\nglove_vector_file_path=/hdd1/word_vectors/glove.42B.300d/glove.42B.300d.txt\n\n\n# Step 1: Generate local variables such as word to number dictionary etc.\n#echo \&quot;Generate local variables required for model\&quot;\n#python Fine-ET/src/data_processing/json_to_tfrecord.py prepare_local_variables $dataset_file  $glove_vector_file_path unk $data_directory/ --lowercase\n\n# Step 2: Convert Training data into TFRecord format.\n#echo \&quot;Converting Train data to TFRecord\&quot;\n#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ $dataset_file\n\n# Step 3: Convert development and testing data into TFRecord format.\n#echo \&quot;Converting Dev and Test data to TFRecord\&quot;\n#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ ../../datasets/1k-WFB-g/fner_dev.json --test_data\n#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ ../../datasets/1k-WFB-g/fner_test.json --test_data\n#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ ../../datasets/figer_gold.json --test_data\n\n# Step 4 (Optional): Convert the development and testing data with entities identified by Fine-ED models to TFRecord format. The following files have to first generated by the Fine-ED model trained on the same dataset. Note that, if the Fine-ED model is retrained, these results file needs to be updated.\n#echo \&quot;Step 4: Pipeline use of FgED results.\&quot;\n#python Fine-ET/src/detect_entities.py ../../datasets/figer_gold.json ../../results/Fine-ED/lstm_crf/RS_NDS_from_2/figer.conll $data_directory/figer_gold_lstm_crf.json\n#python Fine-ET/src/detect_entities.py ../../datasets/1k-WFB-g/fner_dev.json ../../results/Fine-ED/lstm_crf/RS_NDS_from_2/fner_dev.conll $data_directory/fner_dev_lstm_crf.json\n#python Fine-ET/src/detect_entities.py ../../datasets/1k-WFB-g/fner_test.json ../../results/Fine-ED/lstm_crf/RS_NDS_from_2/fner_test.conll $data_directory/fner_test_lstm_crf.json\n#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ $data_directory/fner_dev_lstm_crf.json --test_data\n#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ $data_directory/fner_test_lstm_crf.json --test_data\n#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ $data_directory/figer_gold_lstm_crf.json --test_data\n\n# Run train test procedure 5 times\nfor ((i=1; i<=5; i++)); do\n  # Do not emit '_run_' from model ckpt name\n  # format: prefix_run_suffix\n  model_ckpt_name=checkpoint_run_$i\n\n#  echo \&quot;Training a FNET model\&quot;\n#  time python Fine-ET/src/main_fnet_train.py  $data_directory/ $ckpt_directory/$model_ckpt_name/ 'RS_NDS_from_2.json*.tfrecord' $rnn_hidden_neurons $keep_prob $learning_rate $batch_size $char_embedding_size $char_rnn_hidden_neurons $joint_embedding_size $epochs $save_checkpoint_after --use_mention --use_clean\n\n#  echo \&quot;Testing a FNET model on dev data.\&quot;\n#  python Fine-ET/src/main_fnet_test.py $ckpt_directory/$model_ckpt_name/ $data_directory/fner_dev.json_0.tfrecord\n\n#  echo \&quot;Testing a FNET model on dev data with entities detected by a Fine-ED model. (Optional)\&quot;\n#  python Fine-ET/src/main_fnet_test.py $ckpt_directory/$model_ckpt_name/ $data_directory/fner_dev_lstm_crf.json_0.tfrecord\n\n#  echo \&quot;Testing a FNET model on test data.\&quot;\n#  python Fine-ET/src/main_fnet_test.py $ckpt_directory/$model_ckpt_name/ $data_directory/fner_test.json_0.tfrecord\n\n#  echo \&quot;Testing a FNET model on test data with entities detected by a Fine-ED model. (Optional)\&quot;\n#  python Fine-ET/src/main_fnet_test.py $ckpt_directory/$model_ckpt_name/ $data_directory/fner_test_lstm_crf.json_0.tfrecord\n\n#  echo \&quot;Testing a FNET model on figer gold data.\&quot;\n#  python Fine-ET/src/main_fnet_test.py $ckpt_directory/$model_ckpt_name/ $data_directory/figer_gold.json_0.tfrecord\n\n#  echo \&quot;Testing a FNET model on figer data with entities detected by a Fine-ED model. (Optional)\&quot;\n#  python Fine-ET/src/main_fnet_test.py $ckpt_directory/$model_ckpt_name/ $data_directory/figer_gold_lstm_crf.json_0.tfrecord\n\n  # The final_result file contains the result on the development set based on the strict, macro and micro F1 metrics.\n#  echo \&quot;Report results FNER dev data.\&quot;\n#  bash Fine-ET/src/scripts/report_result_fnet.bash $ckpt_directory/$model_ckpt_name/fner_dev.json_0.tfrecord/ ../../datasets/1k-WFB-g/fner_dev.json 0 > $ckpt_directory/$model_ckpt_name/fner_dev.json_0.tfrecord/final_result.txt\n\n#  echo \&quot;Report results FNER dev data with entities detected by a Fine-ED model. (Optional)\&quot;\n#  bash Fine-ET/src/scripts/report_result_fnet.bash $ckpt_directory/$model_ckpt_name/fner_dev_lstm_crf.json_0.tfrecord/ ../../datasets/1k-WFB-g/fner_dev.json 0 > $ckpt_directory/$model_ckpt_name/fner_dev_lstm_crf.json_0.tfrecord/final_result.txt\n\n  # The final_result file contains the result on the test set based on the strict, macro and micro F1 metrics.\n#  echo \&quot;Report results FNER eval data.\&quot;\n#  bash Fine-ET/src/scripts/report_result_fnet.bash $ckpt_directory/$model_ckpt_name/fner_test.json_0.tfrecord/ ../../datasets/1k-WFB-g/fner_test.json 0 > $ckpt_directory/$model_ckpt_name/fner_test.json_0.tfrecord/final_result.txt\n\n#  echo \&quot;Report results FNER eval data with entities detected by a Fine-ED model. (Optional)\&quot;\n#  bash Fine-ET/src/scripts/report_result_fnet.bash $ckpt_directory/$model_ckpt_name/fner_test_lstm_crf.json_0.tfrecord/ ../../datasets/1k-WFB-g/fner_test.json 0 > $ckpt_directory/$model_ckpt_name/fner_test_lstm_crf.json_0.tfrecord/final_result.txt\n\n#  echo \&quot;Report results figer data.\&quot;\n#  bash Fine-ET/src/scripts/report_result_fnet.bash $ckpt_directory/$model_ckpt_name/figer_gold.json_0.tfrecord/ ../../datasets/figer_gold.json 0 ../../datasets/label_patch_figer_to_fner.txt > $ckpt_directory/$model_ckpt_name/figer_gold.json_0.tfrecord/final_result.txt\n\n#  echo \&quot;Report results figer data with entities detected by a Fine-ED model. (Optional)\&quot;\n#  bash Fine-ET/src/scripts/report_result_fnet.bash $ckpt_directory/$model_ckpt_name/figer_gold_lstm_crf.json_0.tfrecord/ ../../datasets/figer_gold.json 0 ../../datasets/label_patch_figer_to_fner.txt > $ckpt_directory/$model_ckpt_name/figer_gold_lstm_crf.json_0.tfrecord/final_result.txt\ndone\n\n&quot;}}},{&quot;rowIdx&quot;:59,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\nset -e\n\nBLUE='\\033[1;34m'\nNC='\\033[0m'\n\nscript_dir=\&quot;$( cd \&quot;$( dirname \&quot;${BASH_SOURCE[0]}\&quot;  )\&quot; >/dev/null 2>&amp;1 &amp;&amp; pwd )\&quot;\npython_dir=\&quot;$script_dir/occlum_instance/image/opt/python-occlum\&quot;\n\ncd occlum_instance &amp;&amp; rm -rf image\ncopy_bom -f ../pytorch.yaml --root image --include-dir /opt/occlum/etc/template\n\nif [ ! -d $python_dir ];then\n    echo \&quot;Error: cannot stat '$python_dir' directory\&quot;\n    exit 1\nfi\n\nnew_json=\&quot;$(jq '.resource_limits.user_space_size = \&quot;6000MB\&quot; |\n                .resource_limits.kernel_space_heap_size = \&quot;256MB\&quot; |\n                .process.default_mmap_size = \&quot;4000MB\&quot; |\n                .env.default += [\&quot;PYTHONHOME=/opt/python-occlum\&quot;]' Occlum.json)\&quot; &amp;&amp; \\\necho \&quot;${new_json}\&quot; > Occlum.json\nocclum build\n\n# Run the python demo\necho -e \&quot;${BLUE}occlum run /bin/python3 demo.py${NC}\&quot;\nocclum run /bin/python3 demo.py\n&quot;}}},{&quot;rowIdx&quot;:60,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# set the android paths globally\nexport ANDROID_HOME=\&quot;/opt/android-sdk-linux\&quot;\nexport PATH=$PATH:$ANDROID_HOME/tools\nexport PATH=$PATH:$ANDROID_HOME/platform-tools\nexport PATH=$PATH:$ANDROID_HOME/build-tools/24.0.0\n&quot;}}},{&quot;rowIdx&quot;:61,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\n# Hi!\n# If you're reading this, you're probably interested in what's \n# going on within this script. We've provided what we hope are useful\n# comments inline, as well as color-coded relevant shell output.\n# We hope it's useful for you, but if you have any questions or suggestions\n# please open an issue on https:/github.com/MicrosoftDocs/mslearn-aspnet-core.\n#\n\n## Start\ncd ~\n\n# dotnet SDK version\ndeclare -x dotnetSdkVersion=\&quot;3.1.403\&quot;\n\n# Module name\ndeclare moduleName=\&quot;microservices-logging-aspnet-core\&quot;\n\n# Any other declarations we need\ndeclare -x gitBranch=\&quot;live\&quot;\ndeclare initScript=https://raw.githubusercontent.com/MicrosoftDocs/mslearn-aspnet-core/$gitBranch/infrastructure/scripts/initenvironment.sh\ndeclare suppressAzureResources=true\ndeclare rootLocation=~/clouddrive\ndeclare editorHomeLocation=$rootLocation/aspnet-learn/src\n\nif [ -d \&quot;$rootLocation/aspnet-learn\&quot; ]; then\n    echo \&quot;$rootLocation/aspnet-learn/ already exists!\&quot;\n    echo \&quot; \&quot;\n    echo \&quot;Before running this script, please remove or rename the existing $rootLocation/aspnet-learn/ directory as follows:\&quot;\n    echo \&quot;Remove: rm -r $rootLocation/aspnet-learn/\&quot;\n    echo \&quot;Rename: mv $rootLocation/aspnet-learn/ ~/clouddrive/new-name-here/ \&quot;\n    echo \&quot; \&quot;\nelse\n    # Backup .bashrc\n    cp ~/.bashrc ~/.bashrc.bak.$moduleName\n\n    # Grab and run initenvironment.sh\n    . <(wget -q -O - $initScript)\n\n    # Download\n    downloadStarterApp\n\n    # Set location to ~/clouddrive\n    cd $editorHomeLocation\n\n    # Launch editor so the user can see the code\n    code .\n\n    # Run eshop-learn quickstart to deploy to AKS\n    $editorHomeLocation/deploy/k8s/quickstart.sh --resource-group eshop-learn-rg --location centralus\n\n    # Create ACR resource\n    $editorHomeLocation/deploy/k8s/create-acr.sh\n\n    # Display URLs to user\n    cat ~/clouddrive/aspnet-learn/deployment-urls.txt\nfi\n&quot;}}},{&quot;rowIdx&quot;:62,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\nset -e\nsource $(dirname $0)/lib.sh\n\nreq_env_var \&quot;\n    USER $USER\n    HOME $HOME\n    ENVLIB $ENVLIB\n    SCRIPT_BASE $SCRIPT_BASE\n    CIRRUS_REPO_NAME $CIRRUS_REPO_NAME\n    CIRRUS_CHANGE_IN_REPO $CIRRUS_CHANGE_IN_REPO\n    CIRRUS_WORKING_DIR $CIRRUS_WORKING_DIR\n\&quot;\n\n[[ \&quot;$SHELL\&quot; =~ \&quot;bash\&quot; ]] || chsh -s /bin/bash\n\ncd \&quot;$CIRRUS_WORKING_DIR\&quot;  # for clarity of initial conditions\n\n# Verify basic dependencies\nfor depbin in gcc rsync sha256sum curl make\ndo\n    if ! type -P \&quot;$depbin\&quot; &amp;> /dev/null\n    then\n        echo \&quot;***** WARNING: $depbin binary not found in $PATH *****\&quot;\n    fi\ndone\n\n# Setup env. vars common to all tasks/scripts/platforms and\n# ensure they return for every following script execution.\nMARK=\&quot;# Added by $0, manual changes will be lost.\&quot;\ntouch \&quot;$HOME/$ENVLIB\&quot;\nif ! grep -q \&quot;$MARK\&quot; \&quot;$HOME/$ENVLIB\&quot;\nthen\n    cp \&quot;$HOME/$ENVLIB\&quot; \&quot;$HOME/${ENVLIB}_original\&quot;\n    # N/B: Single-quote items evaluated every time, double-quotes only once (right now).\n    for envstr in \\\n        \&quot;$MARK\&quot; \\\n        \&quot;export SRC=\\\&quot;$CIRRUS_WORKING_DIR\\\&quot;\&quot; \\\n        \&quot;export OS_RELEASE_ID=\\\&quot;$(os_release_id)\\\&quot;\&quot; \\\n        \&quot;export OS_RELEASE_VER=\\\&quot;$(os_release_ver)\\\&quot;\&quot; \\\n        \&quot;export OS_REL_VER=\\\&quot;$(os_release_id)-$(os_release_ver)\\\&quot;\&quot; \\\n        \&quot;export BUILT_IMAGE_SUFFIX=\\\&quot;-$CIRRUS_REPO_NAME-${CIRRUS_CHANGE_IN_REPO:0:8}\\\&quot;\&quot;\n    do\n        # Make permanent in later shells, and set in current shell\n        X=$(echo \&quot;$envstr\&quot; | tee -a \&quot;$HOME/$ENVLIB\&quot;) &amp;&amp; eval \&quot;$X\&quot; &amp;&amp; echo \&quot;$X\&quot;\n    done\n\n    # Do the same for golang env. vars\n    go env | while read envline\n    do\n        X=$(echo \&quot;export $envline\&quot; | tee -a \&quot;$HOME/$ENVLIB\&quot;) &amp;&amp; eval \&quot;$X\&quot; &amp;&amp; echo \&quot;$X\&quot;\n    done\n\n    show_env_vars\n\n    # Nothing further required on image-builder VM\n    if ((IMAGE_BUILD))\n    then\n        exit 0\n    fi\n\n    # Owner/mode may have changed\n    setup_gopath\n\n    case \&quot;$OS_REL_VER\&quot; in\n        fedora-29)\n            install_testing_deps\n            build_and_replace_conmon\n\n            cd \&quot;$CRIO_SRC\&quot;  # cri-o source\n            echo \&quot;Building binaries required for testing\&quot;\n            ooe.sh make test-binaries\n\n            echo \&quot;Configuring firewall/networking for integration tests\&quot;\n            ooe.sh iptables -F\n            ooe.sh iptables -t nat -I POSTROUTING -s 127.0.0.1 ! -d 127.0.0.1 -j MASQUERADE\n            echo \&quot;Setting read_only flag to false\&quot;\n            sudo sed -i 's/read_only = true/read_only = false/g' /etc/crio/crio.conf\n            echo \&quot;Removing nodev flag\&quot;\n            sudo sed -i 's/nodev//g' /etc/containers/storage.conf\n            iptables -L -n -v\n            ;;\n        *) bad_os_id_ver ;;\n    esac\n\n    # Verify nothing was set empty\n    # N/B: Some multi-user environment variables are pre-cooked into /etc/environ\n    #      (see setup_gopath in $SCRIPT_BASE/lib.sh)\n    req_env_var \&quot;\n        OS_RELEASE_ID $OS_RELEASE_ID\n        OS_RELEASE_VER $OS_RELEASE_VER\n        OS_REL_VER $OS_REL_VER\n        BUILT_IMAGE_SUFFIX $BUILT_IMAGE_SUFFIX\n    \&quot;\nfi\n\necho \&quot;***** TESTING STARTS: $(date --iso-8601=seconds)\&quot;\n&quot;}}},{&quot;rowIdx&quot;:63,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;export MALI_LIBS=/scratch/ZCU102_PL/ZynqMP/build/tmp/sysroots/plnx_aarch64/usr/lib\nexport MALI_INCLUDE=/scratch/ZCU102_PL/ZynqMP/build/tmp/sysroots/plnx_aarch64/usr/include\nexport ARM_CROSS_COMPILER_PATH=/proj/petalinux/petalinux-v2017.1_daily_latest/petalinux-v2017.1-final/tools/linux-i386/aarch64-linux-gnu/bin/aarch64-linux-gnu-g++\nexport ARM_CROSS_COMPILER_PATH_C=/proj/petalinux/petalinux-v2017.1_daily_latest/petalinux-v2017.1-final/tools/linux-i386/aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc\nexport CXXFLAGS=-DENABLE_FBDEV\n\n\n&quot;}}},{&quot;rowIdx&quot;:64,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash -ve\n\n./node_modules/.bin/marionette-mocha \\\n  --host-log stdout \\\n  --host $(pwd)/node_modules/graphene-marionette-runner/host/index.js \\\n  --runtime ./graphene/Contents/MacOS/graphene \\\n  --start-manifest http://localhost:6060/manifest.webapp \\\n  $(find test -name '*_test.js') $@;\n&quot;}}},{&quot;rowIdx&quot;:65,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \&quot;License\&quot;); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \&quot;AS IS\&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\nfind . -regex '\\.\\/[P|T].CL-.*\\.sh' -exec  {} -o results-Latency/ --csv \\; \n&quot;}}},{&quot;rowIdx&quot;:66,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/sh\n\necho \&quot;/patched-lib\&quot; > /etc/ld.so.conf.d/000-patched-lib.conf &amp;&amp; \\\nmkdir -p \&quot;/patched-lib\&quot; &amp;&amp; \\\nPATCH_OUTPUT_DIR=/patched-lib /usr/local/bin/patch.sh &amp;&amp; \\\ncd /patched-lib &amp;&amp; \\\nfor f in * ; do\n    suffix=\&quot;${f##*.so}\&quot;\n    name=\&quot;$(basename \&quot;$f\&quot; \&quot;$suffix\&quot;)\&quot;\n    [ -h \&quot;$name\&quot; ] || ln -sf \&quot;$f\&quot; \&quot;$name\&quot;\n    [ -h \&quot;$name\&quot; ] || ln -sf \&quot;$f\&quot; \&quot;$name.1\&quot;\ndone &amp;&amp; \\\nldconfig\n[ \&quot;$OLDPWD\&quot; ] &amp;&amp; cd -\nexec \&quot;$@\&quot;\n&quot;}}},{&quot;rowIdx&quot;:67,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;export NGPUS=8\npython3 -m torch.distributed.launch --nproc_per_node=$NGPUS tools/train_net.py --config-file configs/e2e_faster_rcnn_DETNAS_COCO_FPN_300M_search.yaml  OUTPUT_DIR models/DETNAS_COCO_FPN_300M_1x_search\n&quot;}}},{&quot;rowIdx&quot;:68,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# === === === === ===\n# Capture stdout and stderr to log files\n## I don't totally understand this line. Sourced from https://stackoverflow.com/a/2364896/100596 as of 2020-07-05:\nexec 3>&amp;1 4>&amp;2\n\n## Redirect stdout to log\nexec 1>/tmp/dotfiles-install-stdout.log\n\n## Redirect stderr to log\nexec 2>/tmp/dotfiles-install-stderr.log\n\n\n# === === === === ===\n# Make sure PowerShell is installed\nsudo chmod a+x ./posix/require-pwsh.sh\n. ./posix/require-pwsh.sh\n\n\n# === === === === ===\n# Make sure Git is installed\nsudo chmod a+x ./posix/require-git.sh\n. ./posix/require-git.sh\n\n\n# === === === === ===\n# Restore stdout and stderr\nexec 1>&amp;3 2>&amp;4\n\n## Close the unused descriptors\nexec 3>&amp;- 4>&amp;-\n&quot;}}},{&quot;rowIdx&quot;:69,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/usr/bin/env bash\nDIR=\&quot;$(cd \&quot;$(dirname \&quot;$0\&quot;)\&quot; >/dev/null 2>&amp;1 &amp;&amp; pwd)\&quot;\nGO_SRC=\&quot;$DIR/..\&quot;\n\nrm -rf \&quot;$GO_SRC/.go\&quot;\n&quot;}}},{&quot;rowIdx&quot;:70,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\n# Get script arguments\nPARAMS=\&quot;\&quot;\nwhile ((\&quot;$#\&quot;)); do\n    [[ $1 == --*=* ]] &amp;&amp; set -- \&quot;${1%%=*}\&quot; \&quot;${1#*=}\&quot; \&quot;${@:2}\&quot;\n    case \&quot;$1\&quot; in\n    --organization-url)\n        if [ -n \&quot;$2\&quot; ] &amp;&amp; [ \&quot;${2:0:1}\&quot; != \&quot;-\&quot; ]; then\n            organization_url=$2\n            shift 2\n        else\n            echo \&quot;Error: Argument for $1 is missing\&quot; >&amp;2\n            exit 1\n        fi\n        ;;\n    --project-name)\n        if [ -n \&quot;$2\&quot; ] &amp;&amp; [ \&quot;${2:0:1}\&quot; != \&quot;-\&quot; ]; then\n            project_name=$2\n            shift 2\n        else\n            echo \&quot;Error: Argument for $1 is missing\&quot; >&amp;2\n            exit 1\n        fi\n        ;;\n    --repository-name)\n        if [ -n \&quot;$2\&quot; ] &amp;&amp; [ \&quot;${2:0:1}\&quot; != \&quot;-\&quot; ]; then\n            repository_name=$2\n            shift 2\n        else\n            echo \&quot;Error: Argument for $1 is missing\&quot; >&amp;2\n            exit 1\n        fi\n        ;;\n    --pull-request-title)\n        if [ -n \&quot;$2\&quot; ] &amp;&amp; [ \&quot;${2:0:1}\&quot; != \&quot;-\&quot; ]; then\n            pull_request_title=$2\n            shift 2\n        else\n            echo \&quot;Error: Argument for $1 is missing\&quot; >&amp;2\n            exit 1\n        fi\n        ;;\n    --branch-name)\n        if [ -n \&quot;$2\&quot; ] &amp;&amp; [ \&quot;${2:0:1}\&quot; != \&quot;-\&quot; ]; then\n            branch_name=$2\n            shift 2\n        else\n            echo \&quot;Error: Argument for $1 is missing\&quot; >&amp;2\n            exit 1\n        fi\n        ;;\n    --source-folder-path)\n        if [ -n \&quot;$2\&quot; ] &amp;&amp; [ \&quot;${2:0:1}\&quot; != \&quot;-\&quot; ]; then\n            source_folder_path=$2\n            shift 2\n        else\n            echo \&quot;Error: Argument for $1 is missing\&quot; >&amp;2\n            exit 1\n        fi\n        ;;\n    --temporary-branch-name)\n        if [ -n \&quot;$2\&quot; ] &amp;&amp; [ \&quot;${2:0:1}\&quot; != \&quot;-\&quot; ]; then\n            temporary_branch_name=$2\n            shift 2\n        else\n            echo \&quot;Error: Argument for $1 is missing\&quot; >&amp;2\n            exit 1\n        fi\n        ;;\n    --temporary-folder-path)\n        if [ -n \&quot;$2\&quot; ] &amp;&amp; [ \&quot;${2:0:1}\&quot; != \&quot;-\&quot; ]; then\n            temporary_folder_path=$2\n            shift 2\n        else\n            echo \&quot;Error: Argument for $1 is missing\&quot; >&amp;2\n            exit 1\n        fi\n        ;;\n    --overwrite-subfolder)\n        if [ -n \&quot;$2\&quot; ] &amp;&amp; [ \&quot;${2:0:1}\&quot; != \&quot;-\&quot; ]; then\n            overwrite_subfolder=$2\n            shift 2\n        else\n            echo \&quot;Error: Argument for $1 is missing\&quot; >&amp;2\n            exit 1\n        fi\n      \n        ;;\n    -*) # unsupported flags\n        echo \&quot;Error: Unsupported flag $1\&quot; >&amp;2\n        exit 1\n        ;;\n    *) # preserve positional arguments\n        PARAMS=\&quot;$PARAMS \&quot;\&quot;$1\&quot;\&quot;\&quot;\n        shift\n        ;;\n    esac\ndone\neval set -- \&quot;$PARAMS\&quot;\nset -e -o pipefail\n\necho \&quot;Installing Azure DevOps extension...\&quot;\naz extension add --name \&quot;azure-devops\&quot;\naz devops configure --defaults organization=\&quot;${organization_url}\&quot; project=\&quot;${project_name}\&quot;\n\necho \&quot;Creating folder ${temporary_folder_path}...\&quot;\nmkdir -p \&quot;${temporary_folder_path}\&quot;\n\necho \&quot;Cloning branch ${branch_name}...\&quot;\nclone_url=$(az repos show --repository \&quot;${repository_name}\&quot; --query \&quot;webUrl\&quot; --output tsv)\nauthenticated_clone_url=${clone_url/\\/\\////$AZURE_DEVOPS_EXT_PAT@}\ngit clone --branch \&quot;${branch_name}\&quot; --depth 1 \&quot;${authenticated_clone_url}\&quot; \&quot;${temporary_folder_path}\&quot;\n\necho \&quot;Creating temporary branch ${temporary_branch_name} from ${branch_name}...\&quot;\ngit -C \&quot;${temporary_folder_path}\&quot; checkout -b \&quot;${temporary_branch_name}\&quot; \&quot;${branch_name}\&quot;\n\n\necho \&quot;Overwrite folder set to $overwrite_subfolder; deleting its contents...\&quot;\nrm -rfv \&quot;${temporary_folder_path:?}/${overwrite_subfolder:?}\&quot;/*\n\n\necho \&quot;Copying source folder ${source_folder_path} contents to temporary folder ${temporary_folder_path}...\&quot;\ncp -r \&quot;${source_folder_path}\&quot;/* \&quot;${temporary_folder_path}\&quot;/\n\necho \&quot;Validating that changes exist to be published...\&quot;\nif [[ ! $(git -C \&quot;${temporary_folder_path}\&quot; status --porcelain | head -1) ]]; then\n    echo \&quot;No changes exist to be published.\&quot;\n    exit 0\nfi\n\necho \&quot;Setting git user information...\&quot;\ngit config --global user.email \&quot;azuredevopsagent@azuredevops.com\&quot;\ngit config --global user.name \&quot;Azure Devops agent\&quot;\n\necho \&quot;Adding changes...\&quot;\ngit -C \&quot;${temporary_folder_path}\&quot; add --all\n\necho \&quot;Commiting changes...\&quot;\ngit -C \&quot;${temporary_folder_path}\&quot; commit --message \&quot;Initial commit\&quot;\n\necho \&quot;Pushing changes...\&quot;\ngit -C \&quot;${temporary_folder_path}\&quot; push --set-upstream origin \&quot;${temporary_branch_name}\&quot;\n\necho \&quot;Creating pull request...\&quot;\naz repos pr create --source-branch \&quot;${temporary_branch_name}\&quot; --target-branch \&quot;${branch_name}\&quot; --title \&quot;${pull_request_title}\&quot; --squash --delete-source-branch \&quot;true\&quot; --repository \&quot;${repository_name}\&quot;\n\necho \&quot;Deleting temporary folder contents...\&quot;\nrm -rf \&quot;${temporary_folder_path}/{*,.*}\&quot;\n\necho \&quot;Execution complete.\&quot;&quot;}}},{&quot;rowIdx&quot;:71,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;export DETECTRON2_DATASETS=~/datasets\npython train_detectron.py --num-gpus 2 \\\n        --config-file ./detectron_configs/COCO-Keypoints/keypoint_rcnn_resnet50_triplet_attention_FPN_1x.yaml \\\n\t#--eval-only MODEL.WEIGHTS ./output/model_final.pth\n&quot;}}},{&quot;rowIdx&quot;:72,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\n# Copyright 2021 VMware Tanzu Community Edition contributors. All Rights Reserved.\n# SPDX-License-Identifier: Apache-2.0\n\nset -e\n\nMY_DIR=\&quot;$( cd \&quot;$( dirname \&quot;${BASH_SOURCE[0]}\&quot; )\&quot; >/dev/null 2>&amp;1 &amp;&amp; pwd )\&quot;\nTCE_REPO_PATH=${MY_DIR}/../../../../..\n\nTCE_VERSION=\&quot;v0.9.1\&quot;\n\necho \&quot;Installing TCE ${TCE_VERSION}\&quot;\n\nBUILD_OS=$(uname -s | tr '[:upper:]' '[:lower:]')\nTCE_RELEASE_TAR_BALL=\&quot;tce-${BUILD_OS}-amd64-${TCE_VERSION}.tar.gz\&quot;\nTCE_RELEASE_DIR=\&quot;tce-${BUILD_OS}-amd64-${TCE_VERSION}\&quot;\nINSTALLATION_DIR=\&quot;${MY_DIR}/tce-installation\&quot;\n\n\&quot;${TCE_REPO_PATH}\&quot;/hack/get-tce-release.sh ${TCE_VERSION} \&quot;${BUILD_OS}\&quot;-amd64\n\nmkdir -p \&quot;${INSTALLATION_DIR}\&quot;\ntar xzvf \&quot;${TCE_RELEASE_TAR_BALL}\&quot; --directory=\&quot;${INSTALLATION_DIR}\&quot;\n\n\&quot;${INSTALLATION_DIR}\&quot;/\&quot;${TCE_RELEASE_DIR}\&quot;/install.sh || { error \&quot;Unexpected failure during TCE installation\&quot;; exit 1; }\n\necho \&quot;TCE version: \&quot;\ntanzu standalone-cluster version || { error \&quot;Unexpected failure during TCE installation\&quot;; exit 1; }\n\nTANZU_DIAGNOSTICS_PLUGIN_DIR=${MY_DIR}/..\nTANZU_DIAGNOSTICS_BIN=${MY_DIR}/tanzu-diagnostics-e2e-bin\n\necho \&quot;Entering ${TANZU_DIAGNOSTICS_PLUGIN_DIR} directory to build tanzu diagnostics plugin\&quot;\npushd \&quot;${TANZU_DIAGNOSTICS_PLUGIN_DIR}\&quot;\n\ngo build -o \&quot;${TANZU_DIAGNOSTICS_BIN}\&quot; -v\n\necho \&quot;Finished building tanzu diagnostics plugin. Leaving ${TANZU_DIAGNOSTICS_PLUGIN_DIR}\&quot;\npopd\n&quot;}}},{&quot;rowIdx&quot;:73,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\n./configure --prefix=${PREFIX}\n\nmake -j${CPU_COUNT}\nmake check -j${CPU_COUNT}\nmake install -j${CPU_COUNT}\n&quot;}}},{&quot;rowIdx&quot;:74,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/sh\n\n# This script requires google's cloud_sql_proxy on your PATH. One way to set this up:\n# $ brew cask install google-cloud-sdk\n# $ gcloud components install cloud_sql_proxy\n# $ PATH=\&quot;$PATH:/usr/local/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin\&quot;\n# OR\n# $ ln -s /usr/local/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin/cloud_sql_proxy /usr/local/bin/cloud_sql_proxy\n#\n# Use this script to connect to cloud Postgres for a specific Catalog instance.\n# For example, to connect to the dev catalog database, run:\n# $ ENV=dev ./db-connect.sh\n#\n# The proxy will continue to run until you quit it using ^C.\n#\n# The default port used is 5431, to avoid conflicting with a locally running postgres which\n# defaults to port 5432. Use the environment variable PORT to override this setting.\n\n: \&quot;${ENV:?}\&quot;\nPORT=${PORT:-5431}\n\nVAULT_PATH=\&quot;secret/dsde/terra/kernel/${ENV}/${ENV}/catalog/postgres\&quot;\n\nINSTANCE=$(vault read -field=data -format=json \&quot;${VAULT_PATH}/instance\&quot; |\n           jq -r '\&quot;\\(.project):\\(.region):\\(.name)\&quot;')=tcp:$PORT\n\nDB_CREDS_DATA=$(vault read -field=data -format=json \&quot;${VAULT_PATH}/db-creds\&quot;)\n\nJDBC_URL=jdbc:postgresql://localhost:$PORT/$(echo \&quot;${DB_CREDS_DATA}\&quot; |\n          jq -r '\&quot;\\(.db)?user=\\(.username)&amp;password=\\(.password)\&quot;')\n\nPSQL_COMMAND=$(echo \&quot;${DB_CREDS_DATA}\&quot; |\n          jq -r '\&quot;psql postgresql://\\(.username):\\(.password)@localhost/\\(.db)\\\\?port=\&quot;')$PORT\n\necho \&quot;Starting a proxy for $ENV. Connect using: \\\&quot;$JDBC_URL\\\&quot; or run: \\\&quot;$PSQL_COMMAND\\\&quot;\&quot;\n\ncloud_sql_proxy -instances=\&quot;${INSTANCE}\&quot; -dir=/tmp\n&quot;}}},{&quot;rowIdx&quot;:75,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\nexec /usr/local/bin/preoomkiller &amp;\n\nwhile :; do sleep 1; done\n&quot;}}},{&quot;rowIdx&quot;:76,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/bin/bash\n\n# Copyright 2017-2020 Authors of Cilium\n# SPDX-License-Identifier: Apache-2.0\n\nset -o errexit\nset -o pipefail\nset -o nounset\n\nMAKER_IMAGE=\&quot;${MAKER_IMAGE:-docker.io/cilium/image-maker:3e2ea4f151593908c362307a1de22e68610d955c}\&quot;\n\nif [ \&quot;$#\&quot; -ne 1 ] ; then\n  echo \&quot;$0 supports exactly 1 argument\&quot;\n  exit 1\nfi\n\nroot_dir=\&quot;$(git rev-parse --show-toplevel)\&quot;\n\nif [ -z \&quot;${MAKER_CONTAINER+x}\&quot; ] ; then\n   exec docker run --env DOCKER_HUB_PUBLIC_ACCESS_ONLY=true --env QUAY_PUBLIC_ACCESS_ONLY=true --rm --volume \&quot;${root_dir}:/src\&quot; --workdir /src \&quot;${MAKER_IMAGE}\&quot; \&quot;/src/scripts/$(basename \&quot;${0}\&quot;)\&quot; \&quot;${1}\&quot;\nfi\n\ncrane digest \&quot;${1}\&quot; 2> /dev/null\n&quot;}}},{&quot;rowIdx&quot;:77,&quot;cells&quot;:{&quot;content&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;#!/usr/bin/env bash\n\n# Copyright 2017   Hainan Xu\n# Apache 2.0\n\n# This script rescores lattices with KALDI RNNLM trained on reversed text.\n# The input directory should already be rescored with a forward RNNLM, preferably\n# with the pruned algorithm, since smaller lattices make rescoring much faster.\n# An example of the forward pruned rescoring is at\n# egs/swbd/s5c/local/rnnlm/run_tdnn_lstm.sh\n# One example script for backward RNNLM rescoring is at\n# egs/swbd/s5c/local/rnnlm/run_tdnn_lstm_back.sh\n\n# Begin configuration section.\ncmd=run.pl\nskip_scoring=false\nmax_ngram_order=4 # Approximate the lattice-rescoring by limiting the max-ngram-order\n                  # if it's set, it merges histories in the lattice if they share\n                  # the same ngram history and this prevents the lattice from \n                  # exploding exponentially. Details of the n-gram approximation\n                  # method are described in section 2.3 of the paper\n                  # http://www.danielpovey.com/files/2018_icassp_lattice_pruning.pdm\n\nweight=0.5  # Interpolation weight for RNNLM.\nnormalize=false # If true, we add a normalization step to the output of the RNNLM\n                # so that it adds up to *exactly* 1. Note that this is not necessary\n                # as in our RNNLM setup, a properly trained network would automatically\n                # have its normalization term close to 1. The details of this\n                # could be found at http://www.danielpovey.com/files/2018_icassp_rnnlm.pdf\nscoring_opts=\n\n# End configuration section.\n\necho \&quot;$0 $@\&quot;  # Print the command line for logging\n\n. ./utils/parse_options.sh\n\nif [ $# != 5 ]; then\n   echo \&quot;Does language model rescoring of lattices (remove old LM, add new LM)\&quot;\n   echo \&quot;with Kaldi RNNLM trained on reversed text. See comments in file for details\&quot;\n   echo \&quot;\&quot;\n   echo \&quot;Usage: $0 [options] <old-lang-dir> <rnnlm-dir> \\\\\&quot;\n   echo \&quot;                   <data-dir> <input-decode-dir> <output-decode-dir>\&quot;\n   echo \&quot; e.g.: $0 data/lang_tg exp/rnnlm_lstm/ data/test \\\\\&quot;\n   echo \&quot;                   exp/tri3/test_rnnlm_forward exp/tri3/test_rnnlm_bidirection\&quot;\n   echo \&quot;options: [--cmd (run.pl|queue.pl [queue opts])]\&quot;\n   exit 1;\nfi\n\n[ -f path.sh ] &amp;&amp; . ./path.sh;\n\noldlang=$1\nrnnlm_dir=$2\ndata=$3\nindir=$4\noutdir=$5\n\noldlm=$oldlang/G.fst\nif [ ! -f $oldlm ]; then\n  echo \&quot;$0: file $oldlm not found; using $oldlang/G.carpa\&quot;\n  oldlm=$oldlang/G.carpa\nfi\n\n[ ! -f $oldlm ] &amp;&amp; echo \&quot;$0: Missing file $oldlm\&quot; &amp;&amp; exit 1;\n[ ! -f $rnnlm_dir/final.raw ] &amp;&amp; echo \&quot;$0: Missing file $rnnlm_dir/final.raw\&quot; &amp;&amp; exit 1;\n[ ! -f $rnnlm_dir/feat_embedding.final.mat ] &amp;&amp; [ ! -f $rnnlm_dir/word_embedding.final.mat ] &amp;&amp; echo \&quot;$0: Missing word embedding file\&quot; &amp;&amp; exit 1;\n\n[ ! -f $oldlang/words.txt ] &amp;&amp;\\\n  echo \&quot;$0: Missing file $oldlang/words.txt\&quot; &amp;&amp; exit 1;\n! ls $indir/lat.*.gz >/dev/null &amp;&amp;\\\n  echo \&quot;$0: No lattices input directory $indir\&quot; &amp;&amp; exit 1;\nawk -v n=$0 -v w=$weight 'BEGIN {if (w < 0 || w > 1) {\n  print n\&quot;: Interpolation weight should be in the range of [0, 1]\&quot;; exit 1;}}' \\\n  || exit 1;\n\nnormalize_opt=\nif $normalize; then\n  normalize_opt=\&quot;--normalize-probs=true\&quot;\nfi\noldlm_command=\&quot;fstproject --project_output=true $oldlm |\&quot;\nspecial_symbol_opts=$(cat $rnnlm_dir/special_symbol_opts.txt)\n\nword_embedding=\nif [ -f $rnnlm_dir/word_embedding.final.mat ]; then\n  word_embedding=$rnnlm_dir/word_embedding.final.mat\nelse\n  word_embedding=\&quot;'rnnlm-get-word-embedding $rnnlm_dir/word_feats.txt $rnnlm_dir/feat_embedding.final.mat -|'\&quot;\nfi\n\nmkdir -p $outdir/log\nnj=`cat $indir/num_jobs` || exit 1;\ncp $indir/num_jobs $outdir\n\n# In order to rescore with a backward RNNLM, we first remove the original LM\n# scores with lattice-lmrescore, before reversing the lattices\noldlm_weight=$(perl -e \&quot;print -1.0 * $weight;\&quot;)\nif [ \&quot;$oldlm\&quot; == \&quot;$oldlang/G.fst\&quot; ]; then\n  $cmd JOB=1:$nj $outdir/log/rescorelm.JOB.log \\\n    lattice-lmrescore --lm-scale=$oldlm_weight \\\n    \&quot;ark:gunzip -c $indir/lat.JOB.gz|\&quot; \&quot;$oldlm_command\&quot; ark:-  \\| \\\n    lattice-reverse ark:- ark:- \\| \\\n    lattice-lmrescore-kaldi-rnnlm --lm-scale=$weight $special_symbol_opts \\\n    --max-ngram-order=$max_ngram_order $normalize_opt \\\n    $word_embedding \&quot;$rnnlm_dir/final.raw\&quot; ark:- ark:- \\| \\\n    lattice-reverse ark:- \&quot;ark,t:|gzip -c>$outdir/lat.JOB.gz\&quot; || exit 1;\nelse\n  $cmd JOB=1:$nj $outdir/log/rescorelm.JOB.log \\\n    lattice-lmrescore-const-arpa --lm-scale=$oldlm_weight \\\n    \&quot;ark:gunzip -c $indir/lat.JOB.gz|\&quot; \&quot;$oldlm\&quot; ark:-  \\| \\\n    lattice-reverse ark:- ark:- \\| \\\n    lattice-lmrescore-kaldi-rnnlm --lm-scale=$weight $special_symbol_opts \\\n    --max-ngram-order=$max_ngram_order $normalize_opt \\\n    $word_embedding \&quot;$rnnlm_dir/final.raw\&quot; ark:- ark:- \\| \\\n    lattice-reverse ark:- \&quot;ark,t:|gzip -c>$outdir/lat.JOB.gz\&quot; || exit 1;\nfi\n\nif ! $skip_scoring ; then\n  err_msg=\&quot;$0: Not scoring because local/score.sh does not exist or not executable.\&quot;\n  [ ! -x local/score.sh ] &amp;&amp; echo $err_msg &amp;&amp; exit 1;\n  echo local/score.sh --cmd \&quot;$cmd\&quot; $scoring_opts $data $oldlang $outdir\n  local/score.sh --cmd \&quot;$cmd\&quot; $scoring_opts $data $oldlang $outdir\nelse\n  echo \&quot;$0: Not scoring because --skip-scoring was specified.\&quot;\nfi\n\nexit 0;\n&quot;}}}],&quot;truncated&quot;:true},&quot;paginationData&quot;:{&quot;pageIndex&quot;:0,&quot;numItemsPerPage&quot;:100,&quot;numTotalItems&quot;:342720,&quot;offset&quot;:0,&quot;length&quot;:100}},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzEyOSwic3ViIjoiL2RhdGFzZXRzL0d1bkEtU0QvYmFzaF9jb2RlIiwiZXhwIjoxNzQyOTI2NzI5LCJpc3MiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvIn0.WyEBYPrjjocUDYOJFtB4BQD3yPs_elkAzEY9qHBwcDaXeDFQPKCctZneKzCC1mWs7e3i3vEX60vWbC91ezESDQ&quot;,&quot;displayUrls&quot;:true},&quot;dataset&quot;:&quot;GunA-SD/bash_code&quot;,&quot;isGated&quot;:false,&quot;isPrivate&quot;:false,&quot;hasParquetFormat&quot;:true,&quot;author&quot;:{&quot;_id&quot;:&quot;64395f66b9ac1d55f41e5cc4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64395f66b9ac1d55f41e5cc4/qhzWbKjN0zyRIlwpg8JRe.png&quot;,&quot;fullname&quot;:&quot;gunasekar&quot;,&quot;name&quot;:&quot;GunA-SD&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false},&quot;compact&quot;:true}"><div class="flex flex-col overflow-hidden shadow-xs mx-auto mb-10 rounded-lg border pt-2  px-2.5"><div class="mb-2 flex flex-wrap items-center gap-2"><div class="mr-auto flex items-center"><svg class="mr-1 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	<div class="whitespace-nowrap font-semibold">Dataset Viewer</div>
	</div>
				<a href="/datasets/GunA-SD/bash_code/tree/refs%2Fconvert%2Fparquet/default" class="group mr-1 text-xs text-gray-400 max-sm:hidden"><svg class="text-[.6rem] mr-1 inline -translate-y-px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill="currentColor" d="M12 10H6.78A11 11 0 0 1 27 16h2A13 13 0 0 0 6 7.68V4H4v8h8zm8 12h5.22A11 11 0 0 1 5 16H3a13 13 0 0 0 23 8.32V28h2v-8h-8z"></path></svg>
						<span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Auto-converted</span> to Parquet
					</a>
				<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>API</button>
					<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path d="M9.80603 2.86737H3.56107C3.37704 2.86737 3.20055 2.94048 3.07042 3.0706C2.94029 3.20073 2.86719 3.37723 2.86719 3.56126V9.80622C2.86719 9.99025 2.94029 10.1667 3.07042 10.2969C3.20055 10.427 3.37704 10.5001 3.56107 10.5001H9.80603C9.99006 10.5001 10.1666 10.427 10.2967 10.2969C10.4268 10.1667 10.4999 9.99025 10.4999 9.80622V3.56126C10.4999 3.37723 10.4268 3.20073 10.2967 3.0706C10.1666 2.94048 9.99006 2.86737 9.80603 2.86737Z" fill="currentColor" fill-opacity="0.3"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M2.40942 1.66191C2.05175 1.66191 1.7618 1.95186 1.7618 2.30953V6.76191H1.43799V2.30953C1.43799 1.77303 1.87291 1.3381 2.40942 1.3381H6.45704V1.66191H2.40942Z" fill="currentColor"></path></svg>Embed</button>

					<button class="bg-linear-to-b shadow-xs flex items-center gap-1.5 rounded-full border from-white to-red-100/90 px-2 py-0.5 text-xs font-medium text-[#2D3648] transition-shadow hover:shadow-inner dark:from-gray-900 dark:to-red-800/30 dark:text-gray-100 dark:hover:shadow-inner dark:hover:shadow-red-800/30" ><svg class="h-3.5 w-3.5 text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
						<span>Data Studio</span></button></div>
		<div class="flex flex-1 flex-col overflow-hidden -mx-2.5"><div class="flex flex-1 flex-col overflow-hidden"><div class="flex min-h-0 flex-1"><div class="flex flex-1 flex-col overflow-hidden"><div class="md:-mx-2.5 flex min-w-0 flex-wrap border-t"><div class="flex min-w-0 flex-1 flex-wrap"><div class="grid flex-1 grid-cols-1 overflow-hidden text-sm md:grid-cols-2 md:place-content-center sm:mx-2.5"><label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-950 md:border-r md:border-r-0 hidden" title="default"><span class="text-gray-500">Subset (1)</span>
			<div class="flex items-center whitespace-nowrap"><span class="truncate">default</span>
				<span class="mx-2 text-gray-500">Â·</span>
					<span class="text-gray-500">343k rows</span>
				<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
			<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Subset (1)"><option value="default" selected>default (343k rows)</option></optgroup></select></label>
		<label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-900 md:border-r md:border-r" title="train"><div class="text-gray-500">Split (1)</div>
				<div class="flex items-center overflow-hidden whitespace-nowrap"><span class="truncate">train</span>
					<span class="mx-2 text-gray-500">Â·</span>
						<span class="text-gray-500">343k rows</span>
					<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
				<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Split (1)"><option value="train" selected>train (343k rows)</option></optgroup></select></label></div></div>
								</div>

							<div class="flex min-h-0 flex-1 flex-col ">
	<div class="bg-linear-to-r text-smd relative flex items-center dark:border-gray-900 dark:bg-gray-950 false border-t [&amp;:has(:focus)]:from-gray-50 [&amp;:has(:focus)]:to-transparent [&amp;:has(:focus)]:to-20% dark:[&amp;:has(:focus)]:from-gray-900"><form class="flex-1"><svg class="absolute left-3 top-1/2 transform -translate-y-1/2 pointer-events-none text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				<input disabled class="outline-hidden h-9 w-full border-none bg-transparent px-1 pl-9 pr-3 placeholder:text-gray-400 " placeholder="Search this dataset" dir="auto"></form>
			<div class="flex items-center gap-2 px-2 py-1"><button type="button" class="hover:bg-yellow-200/70 flex items-center gap-1 rounded-md border border-yellow-200 bg-yellow-100 pl-0.5 pr-1 text-[.8rem] leading-normal text-gray-700 dark:border-orange-500/25 dark:bg-orange-500/20 dark:text-gray-300 dark:hover:brightness-110 hidden"><div class="rounded-sm bg-yellow-300 px-1 font-mono text-[.7rem] font-bold text-black dark:bg-yellow-700 dark:text-gray-200">SQL
	</div>
	Console
</button></div></div>


<div class="flex flex-1 flex-col overflow-hidden min-h-64 border-t">
		

<div class="max-h-96 relative overflow-auto"><table class="w-full table-auto rounded-lg font-mono text-xs text-gray-900"><thead class="shadow-xs sticky left-0 right-0 top-0 z-1 bg-white align-top"><tr class="space-y-54 h-full min-w-fit divide-x border-b text-left"><th class="h-full max-w-sm p-2 text-left  relative w-full"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">content
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['Â·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="10" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="12" y="25" width="10" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="24" y="25" width="10" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="36" y="25" width="10" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="48" y="25" width="10" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="60" y="25" width="10" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="72" y="25" width="10" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="84" y="25" width="10" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="96" y="25" width="10" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="108" y="25" width="10" height="5" fill-opacity="1"></rect><rect class=" fill-gray-200 dark:fill-gray-500/80" rx="2" x="120" y="25" width="10" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="11" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="23" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="35" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="47" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="59" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="71" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="83" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="95" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="107" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-gray-200 cursor-pointer" x="119" y="0" width="12" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 54px">1</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 12px; max-width: 54px">1.02M</div>
			<div class="absolute -translate-x-1/2" style="left: 125px">âŒ€</div></div></div></div></div>
	
						</th></tr></thead>
			<tbody class="h-16 overflow-scroll"><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="0"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

# Running skydns based on instructions at: https://testdatamanagement.wordpress.com/2015/09/01/running-kubernetes-in-docker-with-dns-on-a-single-node/

PWD=`pwd`
BASEDIR=`readlink -e $(dirname ${0})`
cd ${BASEDIR}

KUBECTL='docker exec hyperkube /hyperkube kubectl'

#RUN_SKYDNS="yes"
RUN_SKYDNS="no"

# DNS_ARGUMENTS needs to be passed when Kubernetes is setup.
if [ "${RUN_SKYDNS}" = "yes" ]; then
	DNS_ARGUMENTS="--cluster-dns=10.0.0.10 --cluster-domain=cluster.local"
else
	DNS_ARGUMENTS=""
fi

wait_until_k8s_ready() {
	# Wait until kubernetes is up and fully responsive
	while :
	do
		${KUBECTL} get nodes 2>/dev/null | grep -q '127.0.0.1'
		if [ "${?}" = "0" ]; then
			break
		else
			echo "sleeping for 5 seconds (waiting for kubernetes to start)"
			sleep 5
		fi
	done
	echo "kubernetes nodes:"
	${KUBECTL} get nodes
}


if [ "${RUN_SKYDNS}" = "yes" ]; then
	wait_until_k8s_ready

	echo "Launch kube2sky..."
	docker run -d --net=host gcr.io/google_containers/kube2sky:1.11 --kube_master_url=http://127.0.0.1:8080 --domain=cluster.local

	echo ""

	echo "Launch SkyDNS..."
	docker run -d --net=host gcr.io/google_containers/skydns:2015-03-11-001 --machines=http://localhost:4001 --addr=0.0.0.0:53 --domain=cluster.local
else
	true
fi

cd ${PWD}
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="1"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
# cp -i /etc/kubernetes/admin.conf /root/kube-admin.conf
kubectl --kubeconfig /root/kube-admin.conf $*
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="2"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
set -e

if [ "$1" = "/opt/logstash/bin/logstash" ]; then
    exec "$1" agent -f /opt/conf/logstash.conf
else
    exec "$@"
fi</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="3"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#---------#
# Seqdiag #
#---------#

# Watches for files named *.diag in the given directory (recursive) and generates the
# corresponding PNG file.
# $1: the folder to watch (Default: pwd)
# shellcheck disable=SC2034
seqWatch() {
  local folder="$1"

  [[ -n "$folder" ]] || {
    printfc 'Folder not defined, it was set to pwd\n' "$YELLOW"
    folder="$(pwd)";
  }

  inotifywait -rm "$folder" -e close_write |
  while read path action file; do
    if [[ "$file" =~ .*\.diag$ ]]; then
      seqdiag "$path$file" --no-transparency -a
    fi
  done
}

# Inits a seqdiag file with the preferences defined in seqdiag.init.
# Uses: $TOOLING
# $1: the file to be created (absolute path)
seqInit() {
  local filePath="${1?Missing path to file}"

  mkdir -p "$(dirname "$filePath")"
  cp "$TOOLING/bashrc/Utils/seqdiag.init" "$(basename "$filePath")"
}
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="4"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
# Author: Eason Yi
# Date: 2017-05-17

pbpaste|awk '!/^[ ]*$/'|pbcopy|pbpaste
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="5"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/usr/bin/env bash

PWD_DIR=$(pwd)
function cleanup {
    cd "$PWD_DIR"
}
trap cleanup EXIT

GREP=grep
SED=sed
AWK=awk
MAKE=make

# Fixup ancient Bash
# https://unix.stackexchange.com/q/468579/56041
if [[ -z "$BASH_SOURCE" ]]; then
	BASH_SOURCE="$0"
fi

# Fixup, Solaris and friends
if [[ (-d /usr/xpg4/bin) ]]; then
	SED=/usr/xpg4/bin/sed
	AWK=/usr/xpg4/bin/awk
	GREP=/usr/xpg4/bin/grep
elif [[ (-d /usr/bin/posix) ]]; then
	SED=/usr/bin/posix/sed
	AWK=/usr/bin/posix/awk
	GREP=/usr/bin/posix/grep
fi

# Fixup for sed and "illegal byte sequence"
IS_DARWIN=$(uname -s | "$GREP" -i -c darwin)
if [[ "$IS_DARWIN" -ne 0 ]]; then
	export LC_ALL=C
fi

# Fixup for Solaris and BSDs
# Fixup for Solaris and BSDs
if [[ ! -z $(command -v gmake) ]]; then
	MAKE=gmake
else
	MAKE=make
fi

# Fixup for missing libtool
if [[ ! -z $(command -v libtoolize) ]]; then
	LIBTOOLIZE=$(command -v libtoolize)
elif [[ ! -z $(command -v glibtoolize) ]]; then
	LIBTOOLIZE=$(command -v glibtoolize)
elif [[ ! -z $(command -v libtool) ]]; then
	LIBTOOLIZE=$(command -v libtool)
elif [[ ! -z $(command -v glibtool) ]]; then
	LIBTOOLIZE=$(command -v glibtool)
fi

# Fecth the three required files
if ! wget --no-check-certificate 'https://raw.githubusercontent.com/noloader/cryptopp-autotools/master/Makefile.am' -O Makefile.am; then
	echo "Makefile.am download failed"
	[[ "$0" = "${BASH_SOURCE[0]}" ]] &amp;&amp; exit 1 || return 1
fi

if ! wget --no-check-certificate 'https://raw.githubusercontent.com/noloader/cryptopp-autotools/master/configure.ac' -O configure.ac; then
	echo "configure.ac download failed"
	[[ "$0" = "${BASH_SOURCE[0]}" ]] &amp;&amp; exit 1 || return 1
fi

if ! wget --no-check-certificate 'https://raw.githubusercontent.com/noloader/cryptopp-autotools/master/libcryptopp.pc.in' -O libcryptopp.pc.in; then
	echo "libcryptopp.pc.in download failed"
	[[ "$0" = "${BASH_SOURCE[0]}" ]] &amp;&amp; exit 1 || return 1
fi

mkdir -p m4/

if [[ -z $(command -v autoupdate) ]]; then
	echo "Cannot find autoupdate. Things may fail."
fi

if [[ -z "$LIBTOOLIZE" ]]; then
	echo "Cannot find libtoolize. Things may fail."
fi

if [[ -z $(command -v autoreconf) ]]; then
	echo "Cannot find autoreconf. Things may fail."
fi

echo "Running autoupdate"
if ! autoupdate 2>/dev/null; then
	echo "autoupdate failed."
	[[ "$0" = "${BASH_SOURCE[0]}" ]] &amp;&amp; exit 1 || return 1
fi

echo "Running libtoolize"
if ! "$LIBTOOLIZE" 2>/dev/null; then
	echo "libtoolize failed."
	[[ "$0" = "${BASH_SOURCE[0]}" ]] &amp;&amp; exit 1 || return 1
fi

# Run autoreconf twice on failure. Also see
# https://github.com/tracebox/tracebox/issues/57
echo "Running autoreconf"
if ! autoreconf 2>/dev/null; then
	echo "autoreconf failed, running again."
	if ! autoreconf -fi; then
		echo "autoreconf failed, again."
		[[ "$0" = "${BASH_SOURCE[0]}" ]] &amp;&amp; exit 1 || return 1
	fi
fi

# Sparc need +w
if [[ -e config.sub ]]; then
	chmod +w config.sub
fi
if [[ -e config.guess ]]; then
	chmod +w config.guess
fi

# Update config.sub config.guess. GNU recommends using the latest for all projects.
echo "Updating config.sub"
wget --no-check-certificate 'https://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.sub' -O config.sub

if [[ -e config.sub ]]; then
	chmod +x config.sub
fi

echo "Updating config.guess"
wget --no-check-certificate 'https://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.guess' -O config.guess

if [[ -e config.guess ]]; then
	chmod +x config.guess
fi

if ! ./configure; then
	echo "configure failed."
	[[ "$0" = "${BASH_SOURCE[0]}" ]] &amp;&amp; exit 1 || return 1
fi

"$MAKE" clean 2>/dev/null

if ! "$MAKE" -j2 -f Makefile; then
	echo "make failed."
	[[ "$0" = "${BASH_SOURCE[0]}" ]] &amp;&amp; exit 1 || return 1
fi

if ! ./cryptest v; then
	echo "cryptest v failed."
	[[ "$0" = "${BASH_SOURCE[0]}" ]] &amp;&amp; exit 1 || return 1
fi

if ! ./cryptest tv all; then
	echo "cryptest tv all failed."
	[[ "$0" = "${BASH_SOURCE[0]}" ]] &amp;&amp; exit 1 || return 1
fi

# Return success
[[ "$0" = "${BASH_SOURCE[0]}" ]] &amp;&amp; exit 0 || return 0
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="6"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

# Copyright 2018 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This shell script is used to build a cluster and create a namespace from our
# argo workflow

set -o errexit
set -o nounset
set -o pipefail

CLUSTER_NAME="${CLUSTER_NAME}"
ZONE="${GCP_ZONE}"
PROJECT="${GCP_PROJECT}"
NAMESPACE="${DEPLOY_NAMESPACE}"

echo "Activating service-account"
gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}
echo "Creating GPU cluster"
gcloud --project ${PROJECT} beta container clusters create ${CLUSTER_NAME} \
    --zone ${ZONE} \
    --machine-type=n1-standard-8 \
    --num-nodes=6 \
    --cluster-version 1.14
echo "Configuring kubectl"
gcloud --project ${PROJECT} container clusters get-credentials ${CLUSTER_NAME} \
    --zone ${ZONE}
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="7"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/sh
prog=wave1D_u0_s.py

grep 'if n == 90:' $prog
if [ $? -ne 0 ]; then
  echo "insert if n == 90: st.savefig('frame_C%s.pdf' % C) in $prog"
  exit
fi

C_values="1.0 0.95 0.2 1.0015"
for C in $C_values; do
python $prog $C
scitools movie output_file=index.html fps=2 frame*.png
scitools movie encoder=convert output_file=movie.gif fps=4 frame*.png
dir=guitar_C$C
rm -rf $dir
mkdir $dir
mv movie.gif index.html frame*.png $dir
done
scitools rename frame_C wave1D_guitar_C frame_C*.pdf
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="8"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">export KUBERNETES_SERVICE_HOST=master.serverless-6e97.openshiftworkshop.com
export KUBERNETES_SERVICE_PORT=443
export KUBERNETES_CLIENT_SERVICEACCOUNT_ROOT=$(pwd)/istio
export COOLSTORE_GW_ENDPOINT=http://istio-ingressgateway-istio-system.apps.serverless-6e97.openshiftworkshop.com
#export COOLSTORE_SCENARIOS_ENDPOINT=http://scenarios-coolstore.apps.serverless-6e97.openshiftworkshop.com
export COOLSTORE_SCENARIOS_ENDPOINT=http://localhost:8080
export OPENSHIFT_BUILD_NAMESPACE=coolstore-ng
export BASE_DOMAIN=apps.serverless-6e97.openshiftworkshop.com
export WEB_UI_CUSTOM_PORT=8090

npm run dev</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="9"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># Combined file for easier scripting
export MQSI_SIGNAL_EXCLUSIONS=11
export MQSI_NO_CACHE_SUPPORT=1

. /opt/ibm/ace-12/server/bin/mqsiprofile

export LD_LIBRARY_PATH=/lib:/opt/ibm/java/jre/lib/amd64/compressedrefs:/opt/ibm/java/jre/lib/amd64:$LD_LIBRARY_PATH

# Not really ibmjava-related, but still needed
export LD_LIBRARY_PATH=/usr/glibc-compat/zlib-only:$LD_LIBRARY_PATH
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="10"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
#
# This file is open source software, licensed to you under the terms
# of the Apache License, Version 2.0 (the "License").  See the NOTICE file
# distributed with this work for additional information regarding copyright
# ownership.  You may not use this file except in compliance with the License.
#
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# os-release may be missing in container environment by default.
if [ -f "/etc/os-release" ]; then
    . /etc/os-release
elif [ -f "/etc/arch-release" ]; then
    export ID=arch
else
    echo "/etc/os-release missing."
    exit 1
fi

debian_packages=(
    ninja-build
    ragel
    libhwloc-dev
    libnuma-dev
    libpciaccess-dev
    libcrypto++-dev
    libboost-all-dev
    libxml2-dev
    xfslibs-dev
    libgnutls28-dev
    liblz4-dev
    libsctp-dev
    gcc
    make
    python3
    systemtap-sdt-dev
    libtool
    cmake
    libyaml-cpp-dev
    libc-ares-dev
    stow
    g++
    libfmt-dev
    diffutils
    valgrind
    doxygen
    openssl
    pkg-config
)

# seastar doesn't directly depend on these packages. They are
# needed because we want to link seastar statically and pkg-config
# has no way of saying "static seastar, but dynamic transitive
# dependencies". They provide the various .so -> .so.ver symbolic
# links.
transitive=(libtool-ltdl-devel trousers-devel libidn2-devel libunistring-devel)

redhat_packages=(
    hwloc-devel
    numactl-devel
    libpciaccess-devel
    cryptopp-devel
    libxml2-devel
    xfsprogs-devel
    gnutls-devel
    lksctp-tools-devel
    lz4-devel
    gcc
    g++
    make
    python3
    systemtap-sdt-devel
    libtool
    cmake
    yaml-cpp-devel
    c-ares-devel
    stow
    diffutils
    doxygen
    openssl
    fmt-devel
    boost-devel
    valgrind-devel
    "${transitive[@]}"
)

fedora_packages=(
    "${redhat_packages[@]}"
    gcc-c++
    ninja-build
    ragel
    boost-devel
    fmt-devel
    libubsan
    libasan
    libatomic
    valgrind-devel
)

centos7_packages=(
    "${redhat_packages[@]}"
    ninja-build
    ragel
    cmake3
    rh-mongodb36-boost-devel
    devtoolset-9-gcc-c++
    devtoolset-9-libubsan
    devtoolset-9-libasan
    devtoolset-9-libatomic
)

centos8_packages=(
    "${redhat_packages[@]}"
    ninja-build
    ragel
    gcc-toolset-9-gcc
    gcc-toolset-9-gcc-c++
    gcc-toolset-9-libubsan-devel
    gcc-toolset-9-libasan-devel
    gcc-toolset-9-libatomic-devel
)

# 1) glibc 2.30-3 has sys/sdt.h (systemtap include)
#    some old containers may contain glibc older,
#    so enforce update on that one.
# 2) if problems with signatures, ensure having fresh
#    archlinux-keyring: pacman -Sy archlinux-keyring &amp;&amp; pacman -Syyu
# 3) aur installations require having sudo and being
#    a sudoer. makepkg does not work otherwise.
arch_packages=(
    gcc
    ninja
    ragel
    boost
    boost-libs
    hwloc
    numactl
    libpciaccess
    crypto++
    libxml2
    xfsprogs
    gnutls
    lksctp-tools
    lz4
    make
    libtool
    cmake
    yaml-cpp
    stow
    c-ares
    pkgconf
    fmt
    python3
    glibc
    filesystem
    valgrind
    openssl
)

opensuse_packages=(
    c-ares-devel
    cmake
    hwloc-devel
    libboost_filesystem1_66_0
    libboost_filesystem1_66_0-devel
    libboost_program_options1_66_0
    libboost_program_options1_66_0-devel
    libboost_system1_66_0
    libboost_system1_66_0-devel
    libboost_test1_66_0
    libboost_test1_66_0-devel
    libboost_thread1_66_0
    libboost_thread1_66_0-devel
    libcryptopp-devel
    libboost_atomic1_66_0
    libboost_atomic1_66_0-devel
    libboost_date_time1_66_0
    libboost_date_time1_66_0-devel
    libboost_chrono1_66_0
    libboost_chrono1_66_0-devel
    libgnutls-devel
    libgnutlsxx28
    liblz4-devel
    libnuma-devel
    lksctp-tools-devel
    ninja
    ragel
    xfsprogs-devel
    yaml-cpp-devel
    libtool
    stow
    openssl
)

case "$ID" in
    ubuntu|debian|pop)
        apt-get install -y "${debian_packages[@]}"
    ;;
    fedora)
        dnf install -y "${fedora_packages[@]}"
    ;;
    rhel|centos|amzn)
        if [ "$VERSION_ID" = "7" ]; then
            yum install -y epel-release centos-release-scl scl-utils
            yum install -y "${centos7_packages[@]}"
        elif [ "${VERSION_ID%%.*}" = "8" ]; then
            dnf install -y epel-release
            dnf install -y "${centos8_packages[@]} ${arch_packages[@]}"
        elif [ "$VERSION_ID" = "2" ]; then
            yum install -y epel-release centos-release-scl scl-utils
            yum install -y "${centos8_packages[@]} ${arch_packages[@]}"
        fi
    ;;
    opensuse-leap)
        zypper install -y "${opensuse_packages[@]}"
    ;;
    arch|manjaro)
        if [ "$EUID" -eq "0" ]; then
            pacman -Sy --needed --noconfirm "${arch_packages[@]}"
        else
            echo "seastar: running without root. Skipping main dependencies (pacman)." 1>&amp;2
        fi
    ;;
    *)
        echo "Your system ($ID) is not supported by this script. Please install dependencies manually."
        exit 1
    ;;
esac
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="11"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/usr/bin/env bash
set -e

function check_command() {
  if ! command -v $1 >/dev/null; then
    echo -e "Install \033[1m$1\033[0m"
    exit 1
  fi
}

check_command mvn
check_command jq
check_command yq
check_command yarn
check_command npm
check_command docker

if [[ "$#" != "1" ]] || [[ ! "$1" =~ ^(patch|minor|major)$ ]]; then
  echo "Usage: $0 patch|minor|major"
  exit 1
fi

if [[ $(git status --porcelain) ]]; then
  echo -e "The repository has changes. Commit first...\033[0;31mAborting!\033[0m"
  exit 1
fi

git pull --rebase

cd paperboy-project-generator
yarn
npm version $1
npm publish

cd ../paperboy-core
yarn
yarn build
npm version $1
version=$(cat package.json | jq -r .version)
npm publish

cd ../paperboy-magnolia-module
mvn versions:set -DnewVersion=${version} -DgenerateBackupPoms=false

cd ../paperboy-cli
cat package.json | jq ".version = \"$version\" | .dependencies.\"@neoskop/paperboy\" = \"$version\"" >package.json.new
mv package.json.new package.json
yarn

sed -i.bak "s/version('[[:digit:]]\+\.[[:digit:]]\+\.[[:digit:]]\+')/version('$version')/g" paperboy-cli.js
rm -rf paperboy-cli.js.bak
npm publish

cd ../paperboy-push-service
cat package.json | jq ".version = \"$version\"" >package.json.new
mv package.json.new package.json
yarn
yarn build
npm publish
docker build -t neoskop/paperboy-push-service:$version .
docker build -t neoskop/paperboy-push-service:latest .
docker push neoskop/paperboy-push-service:$version
docker push neoskop/paperboy-push-service:latest

cd ../paperboy-docker
sed -i "s/ENV PAPERBOY_VERSION=[[:digit:]]\+\.[[:digit:]]\+\.[[:digit:]]\+/ENV PAPERBOY_VERSION=$version/" Dockerfile
docker build -t neoskop/paperboy:$version .
docker build -t neoskop/paperboy:latest .
docker push neoskop/paperboy:$version
docker push neoskop/paperboy:latest

cd ../paperboy-helm
yq eval -i ".version=\"$version\"" ./Chart.yaml
yq eval -i ".appVersion=\"$version\"" ./Chart.yaml
yq eval -i ".image.tag=\"$version\"" ./values.yaml

cd ../
git add .
git commit -m "chore: Bump version to ${version}."
git tag ${version}
git push origin $version
git pull --rebase
git push

helm package paperboy-helm --destination .deploy
cr upload -o neoskop -r paperboy -p .deploy
git checkout gh-pages
cr index -i ./index.yaml -p .deploy -o neoskop -r paperboy -c https://neoskop.github.io/paperboy/
git add index.yaml
git commit -m "chore: Bump version to ${version}."
git push
git checkout master
rm -rf .deploy/

HELM_CHARTS_DIR=../neoskop-helm-charts
[ -d $HELM_CHARTS_DIR ] || git clone git@github.com:neoskop/helm-charts.git $HELM_CHARTS_DIR
cd $HELM_CHARTS_DIR
./update-index.sh
cd - &amp;>/dev/null</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="12"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

. /HolismHolding/Infra/Scripts/Message.sh

function LinkConnectionStrings()
{
    Info "Linking ConnectionStrings.json ...";
    sudo ln -s -f /$Organization/Common/ConnectionStrings.json /$Organization/$Repository/ConnectionStrings.json
    Divide
}</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="13"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/usr/bin/env bash

# get run options
while test $# -gt 0; do
  case "$1" in
    -h|--help)
      echo "pac-man$ docker-test - run lambda package"
      echo " "
      echo "pac-man$ docker-test [options]"
      echo " "
      echo "options:"
      echo "-h, --help                show brief help"
      echo "-b, --build               build lambda package prior to running"
      exit 0
      ;;
    -b|--build)
      shift
      export PACMAN_BUILD=1
      ;;
    *)
      break
      ;;
  esac
done

# cd to pac-man directory
cd "$(dirname "$0")"

if [[ -n ${PACMAN_BUILD} &amp;&amp; "${PACMAN_BUILD}"=="1" ]]; then
  # build lambda package
  docker run --rm \
      -v ${PWD}:/code \
      -v ${HOME}/.cargo/registry:/root/.cargo/registry \
      -v ${HOME}/.cargo/git:/root/.cargo/git \
      softprops/lambda-rust &amp;&amp; \
  unzip -o \
      target/lambda/release/pac-man.zip \
      -d /tmp/lambda &amp;&amp; \
  echo "Enter Payload Then Press CTRL-D..." &amp;&amp; \
  docker run \
      -i -e DOCKER_LAMBDA_USE_STDIN=1 \
      -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
      -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
      --rm \
      -v /tmp/lambda:/var/task \
      lambci/lambda:provided
else
  echo "Enter Payload Then Press CTRL-D..." &amp;&amp; \
  docker run \
      -i -e DOCKER_LAMBDA_USE_STDIN=1 \
      -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
      -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
      --rm \
      -v /tmp/lambda:/var/task \
      lambci/lambda:provided
fi
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="14"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
printf "testing '$1'\n"

printf "testing python... "

ppp_file=$1

py_output=$(python3 $ppp_file 2>&amp;1)
py_exit_code=$?

if [ "$py_exit_code" -eq "0" ]; then
	printf "succeeded\n"
else
	printf "FAILED!\n"
fi

printf "testing C++... "

cpp_comp_output=$(g++ -x c++ -std=c++14 $ppp_file -o tmp_bin 2>&amp;1)
cpp_comp_exit_code=$?

cpp_run_output=""
cpp_run_exit_code=1

if [ "$cpp_comp_exit_code" -eq "0" ]; then
	cpp_run_output=$(./tmp_bin 2>&amp;1)
	cpp_run_exit_code=$?
	
	if [ "$cpp_run_exit_code" -eq "0" ]; then
		printf "succeeded\n"
	else
		printf "CRASHED!\n"
	fi
	
	rm tmp_bin
else
	printf "FAILED TO COMPILE!\n"
fi

if [ "$py_exit_code" -eq "0" ] &amp;&amp; [ "$cpp_run_exit_code" -eq "0" ] &amp;&amp; [ "$py_output" = "$cpp_run_output" ]; then
	
	printf "Python and C++ outputs match\n"
	printf "________\n"
	printf " output \__________________________________________\n\n"
	printf "$py_output\n"
	printf "___________________________________________________\n"
else
	
	if [ "$py_exit_code" -eq "0" ] &amp;&amp; [ "$cpp_run_exit_code" -eq "0" ]; then
		printf "Python and C++ outputs DO NOT MATCH!\n"
	fi
	
	printf "_______________\n"
	printf " Python output \___________________________________\n\n"
	printf "$py_output\n"
	printf "___________________________________________________\n"
	
	if [ "$cpp_comp_exit_code" -ne "0" ]; then
		printf "_____________________\n"
		printf " C++ compiler output \_____________________________\n\n"
		printf "$cpp_comp_output\n"
		printf "___________________________________________________\n"
	else
		printf "____________\n"
		printf " C++ output \______________________________________\n\n"
		printf "$cpp_run_output\n"
		printf "___________________________________________________\n"
	fi
fi

printf "\n"



</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="15"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
export HOME=/root/
source $HOME/.bashrc
source $HOME/conda/bin/activate
conda activate tali

cd $CODE_DIR
git pull
pip install -r $CODE_DIR/requirements.txt

source $CODE_DIR/setup_scripts/setup_base_experiment_disk.sh
source $CODE_DIR/setup_scripts/setup_wandb_credentials.sh

cd $CODE_DIR

fuser -k /dev/nvidia*; \
python $CODE_DIR/run.py \
hydra.verbose=True \
trainer=default \
resume=True \
batch_size=8 \
trainer.gpus=4 \
trainer.auto_scale_batch_size=True \
datamodule.dataset_config.rescan_paths=True \
datamodule.prefetch_factor=3 \
datamodule.num_workers=48 \
model=deci_modus_prime_resnet50 \
datamodule.dataset_config.dataset_size_identifier=base \
datamodule.dataset_config.modality_config.image=True \
datamodule.dataset_config.modality_config.text=True \
datamodule.dataset_config.modality_config.audio=True \
datamodule.dataset_config.modality_config.video=True

</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="16"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/usr/bin/env bash

watch --color --beep 'bash script/ci.sh t'
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="17"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/sh

# generate sim input
echo "(1/5) generating simulation input data"
cd tests
./sim_input.py sim_points.txt sim_input.h || exit $?
cd ..

# compile simulation
echo "(2/5) compiling the simulation program"
./compile.sh sim || exit $?

# flash target with simulation program
echo "(3/5) flashing the target"
./flash.sh sim || exit $?

# redirect tether output to file
echo "(4/5) running the simulation"
./tether.py --format-csv tests/sim_output.csv || exit $?

# run tests
echo "(5/5) checking the simulation output"
./tests/sim_tests.py tests/sim_output.csv || exit $?
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="18"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
# ------------------------------------------------------------------------------
# SCM Breeze - Streamline your SCM workflow.
# Copyright 2011 Nathan Broadbent (http://madebynathan.com). All Rights Reserved.
# Released under the LGPL (GNU Lesser General Public License)
# ------------------------------------------------------------------------------
#
# Unit tests for git shell scripts

export scmbDir="$( cd -P "$( dirname "$0" )" &amp;&amp; pwd )/../../.."

# Zsh compatibility
if [ -n "${ZSH_VERSION:-}" ]; then shell="zsh"; SHUNIT_PARENT=$0; setopt shwordsplit; fi

# Load test helpers
source "$scmbDir/test/support/test_helper.sh"

# Load functions to test
source "$scmbDir/lib/scm_breeze.sh"
source "$scmbDir/lib/git/repo_index.sh"


# Setup and tear down
#-----------------------------------------------------------------------------
oneTimeSetUp() {
  GIT_REPO_DIR=$(mktemp -d -t scm_breeze.XXXXXXXXXX)
  GIT_REPOS="/tmp/test_repo_1:/tmp/test_repo_11"
  git_status_command="git status"

  git_index_file="$GIT_REPO_DIR/.git_index"

  silentGitCommands

  cd $GIT_REPO_DIR
  # Setup test repos in temp repo dir
  for repo in github bitbucket source_forge TestCaps; do
    mkdir $repo; cd $repo; git init; cd - > /dev/null
  done

  # Add some nested dirs for testing resursive tab completion
  mkdir -p github/videos/octocat/live_action
  # Add hidden dir to test that '.git' is filtered, but other hidden dirs are available.
  mkdir -p github/.im_hidden

  # Setup a test repo with some submodules
  # (just a dummy '.gitmodules' file and some nested .git directories)
  mkdir submodules_everywhere
  cd submodules_everywhere
  git init
  cat > .gitmodules &lt;&lt;EOF
[submodule "very/nested/directory/red_submodule"]
[submodule "very/nested/directory/green_submodule"]
[submodule "very/nested/directory/blue_submodule"]
EOF
  mkdir -p "very/nested/directory"
  cd "very/nested/directory"
  for repo in red_submodule green_submodule blue_submodule; do
    mkdir $repo; cd $repo; git init; cd - > /dev/null
  done

  # Setup some custom repos outside the main repo dir
  IFS=":"
  for dir in $GIT_REPOS; do
    mkdir -p $dir; cd $dir; git init;
  done
  unset IFS

  verboseGitCommands

  cd "$orig_cwd"
}

oneTimeTearDown() {
  rm -rf "${GIT_REPO_DIR}"
  IFS=":"
  for dir in $GIT_REPOS; do rm -rf $dir; done
  unset IFS
}

ensureIndex() {
  _check_git_index
}

index_no_newlines() {
  tr "\\n" " " &lt; $git_index_file
}


#-----------------------------------------------------------------------------
# Unit tests
#-----------------------------------------------------------------------------

test_repo_index_command() {
  git_index --rebuild > /dev/null

  # Test that all repos are detected, and sorted alphabetically
  assertIncludes "$(index_no_newlines)" "bitbucket.*\
blue_submodule.*\
github.*\
green_submodule.*\
red_submodule.*\
source_forge.*\
submodules_everywhere.*\
test_repo_11.*\
test_repo_1"

}

test_check_git_index() {
  ensureIndex
  echo "should not be regenerated" >> $git_index_file
  _check_git_index
  # Test that index is not rebuilt unless empty
  assertIncludes "$(index_no_newlines)" "should not be regenerated"
  rm $git_index_file
  # Test the index is rebuilt
  _check_git_index
  assertTrue "[ -f $git_index_file ]"
}

test_git_index_count() {
  assertEquals "10" "$(_git_index_count)"
}

test_repo_list() {
  ensureIndex
  list=$(git_index --list)
  assertIncludes "$list" "bitbucket"      || return
  assertIncludes "$list" "blue_submodule" || return
  assertIncludes "$list" "test_repo_11"
}

# Test matching rules for changing directory
test_git_index_changing_directory() {
  ensureIndex
  git_index "github";       assertEquals "$GIT_REPO_DIR/github" "$PWD"
  git_index "github/";      assertEquals "$GIT_REPO_DIR/github" "$PWD"
  git_index "bucket";       assertEquals "$GIT_REPO_DIR/bitbucket" "$PWD"
  git_index "testcaps";     assertEquals "$GIT_REPO_DIR/TestCaps" "$PWD"
  git_index "green_sub";    assertEquals "$GIT_REPO_DIR/submodules_everywhere/very/nested/directory/green_submodule" "$PWD"
  git_index "_submod";      assertEquals "$GIT_REPO_DIR/submodules_everywhere/very/nested/directory/blue_submodule" "$PWD"
  git_index "test_repo_1";  assertEquals "/tmp/test_repo_1" "$PWD"
  git_index "test_repo_11"; assertEquals "/tmp/test_repo_11" "$PWD"
  git_index "test_repo_";   assertEquals "/tmp/test_repo_11" "$PWD"
  git_index "github/videos/octocat/live_action"; assertEquals "$GIT_REPO_DIR/github/videos/octocat/live_action" "$PWD"
}

test_git_index_tab_completion() {
  # Only run tab completion test for bash
  if [[ "$0" == *bash ]]; then
    ensureIndex
    COMP_CWORD=0

    # Test that '--' commands have tab completion
    COMP_WORDS="--"
    _git_index_tab_completion
    assertEquals "Incorrect number of tab-completed '--' commands" "5" "$(tab_completions | wc -w)"

    COMP_WORDS="gith"
    _git_index_tab_completion
    assertIncludes "$(tab_completions)" "github/"

    # Test completion for project sub-directories when project ends with '/'
    COMP_WORDS="github/"
    _git_index_tab_completion
    assertIncludes    "$(tab_completions)" "github/videos/"
    # Check that '.git/' is filtered from completion, but other hidden dirs are available
    assertNotIncludes "$(tab_completions)" "github/.git/"
    assertIncludes    "$(tab_completions)" "github/.im_hidden/"

    COMP_WORDS="github/videos/"
    _git_index_tab_completion
    assertIncludes "$(tab_completions)" "github/videos/octocat/"


    # Test that completion checks for other matching projects even if one matches perfectly
    COMP_WORDS="test_repo_1"
    _git_index_tab_completion
    assertIncludes "$(tab_completions)" "test_repo_1/ test_repo_11/"
  fi
}


# Test changing to top-level directory (when arg begins with '/')
test_changing_to_top_level_directory() {
  mkdir "$GIT_REPO_DIR/gems"
  git_index "/gems"
  assertEquals "$GIT_REPO_DIR/gems" "$PWD"
}


# load and run shUnit2
# Call this function to run tests
source "$scmbDir/test/support/shunit2"

</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="19"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
# Boost for compiling 32-bit binaries on 64-bit:
#   ./bootstrap.sh
#   ./b2 link=static address-model=32 stage

set -eu

function boost-static
{
  sed -i 's/^\(oakfoam_LDADD =\) \(.*\) \($(HOARD_LIB).*\)$/\1 -Wl,-Bstatic \2 -Wl,-Bdynamic -pthread \3/' Makefile
}

VER=`cat config.h | sed -n 's/.*PACKAGE_VERSION \"\(.*\)\".*/\1/p'`
PREV_CONFIGURE=`cat config.log | head | sed -n 's/\s*$ //p'`
echo "configure was: $PREV_CONFIGURE"

DEBINPUT="0
oakfoam@gmail.com
5
BSD
6
games
7
i386
"

BOOST_ROOT=/data/opt/boost_1_47_0 $PREV_CONFIGURE --with-web 'CPPFLAGS=-m32' 'LDFLAGS=-m32 -pthread'
boost-static
echo "$DEBINPUT" | sudo checkinstall --nodoc --install=no make install
sudo chmod a+rw oakfoam oakfoam_*.deb

NAME=oakfoam_${VER}_i386

rm -f ${NAME}.tar.gz
mkdir ${NAME}

# BOOST_ROOT=/data/opt/boost_1_47_0 $PREV_CONFIGURE --with-web 'CPPFLAGS=-m32' 'LDFLAGS=-m32 -pthread'
# boost-static
make install DESTDIR=`pwd`/${NAME}

find ${NAME}/ -type f | grep -v 'menu\|applications\|www' | xargs -n1 -I{} mv {} $NAME/
find ${NAME}/ -type d -name www | xargs -n1 -I{} mv {} $NAME/

sed -i '/^cd \.\./d;/^bin=".*/d;s/$bin/\./' ${NAME}/oakfoam-web
mv ${NAME}/oakfoam-web ${NAME}/run.sh

tar -czf ${NAME}.tar.gz ${NAME}/
rm -r ${NAME}/

if [ "`uname -m`" == "x86_64" ]; then
  DEBINPUT="0
  oakfoam@gmail.com
  5
  BSD
  6
  games
  "

  $PREV_CONFIGURE --with-web
  boost-static
  make clean
  echo "$DEBINPUT" | sudo checkinstall --nodoc --install=no make install
  sudo chmod a+rw oakfoam oakfoam_*.deb

  NAME=oakfoam_${VER}_amd64

  rm -f ${NAME}.tar.gz
  mkdir ${NAME}

  # $PREV_CONFIGURE --with-web
  # boost-static
  make install DESTDIR=`pwd`/${NAME}

  find ${NAME}/ -type f | grep -v 'menu\|applications\|www' | xargs -n1 -I{} mv {} $NAME/
  find ${NAME}/ -type d -name www | xargs -n1 -I{} mv {} $NAME/

  sed -i '/^cd \.\./d;/^bin=".*/d;s/$bin/\./' ${NAME}/oakfoam-web
  mv ${NAME}/oakfoam-web ${NAME}/run.sh

  tar -czf ${NAME}.tar.gz ${NAME}/
  rm -r ${NAME}/
  make clean
fi

$PREV_CONFIGURE

</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="20"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/sh

if [ "$USER" != "root" ]
then
     echo "This installer must be run with root privileges. Please run sudo $0"
     return 1
fi

# Ensure the libraries we use are installed
apt install python3 python3-rpi.gpio python3-requests

addgroup --system doorbot
adduser --system --ingroup gpio doorbot

for N in doorbot.ini.example doorbot.py doorbot.service ringtest.py
    do cp $N /home/doorbot
    chown doorbot:doorbot /home/doorbot/$N
done

if [ -f /etc/systemd/system/doorbot.service ]
    then echo "Unit file already exists, skipping"
    else ln /home/doorbot/doorbot.service /etc/systemd/system/
fi
systemctl daemon-reload

</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="21"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
for file in *.gv
do
    name=${file%%.*}
    dot -Tsvg:cairo:cairo $name.gv > ../output/$name.svg
done
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="22"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">python transformers/examples/language-modeling/run_language_modeling.py --model_type gpt2 --tokenizer_name model-configs/1024-config --config_name model-configs/1024-config/config.json --train_data_file ../data/wikitext-103-raw/wiki.train.raw --eval_data_file ../data/wikitext-103-raw/wiki.valid.raw --output_dir train-outputs/512+0+512-shuffled-N/13-model --do_train --do_eval --evaluate_during_training --per_device_train_batch_size 3 --per_device_eval_batch_size 3 --num_train_epochs 10 --dataloader_drop_last --save_steps 500 --save_total_limit 20 --augmented --augmentation_function shuffle_remove_all_but_nouns_first_half --train_function augmented_training --eval_function augmented_eval --seed 13</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="23"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
dieharder -d 206 -g 27 -S 844198761
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="24"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/usr/bin/env bash

set -eu
set -o pipefail

EULA=${EULA:-false}
HEAP_SIZE=${HEAP_SIZE:-1024}
JVM_OPTS=${JVM_OPTS:-}
RCON_PASSWORD=${RCON_PASSWORD:-}
SERVER_OPTS=${SERVER_OPTS:-}

cd $(pwd)/config

if [ $(ls -1 ../overrides | wc -l) != "0" ]; then
    echo "Copying configuration overrides..."
    for file in ../overrides/*; do
        echo "    $(basename ${file})"
        cp ${file} .
    done
    echo "done!"
fi

if [ -n "$RCON_PASSWORD" ]; then
    echo "rcon.password=${RCON_PASSWORD}" >> server.properties
fi

echo "Copying configuration defaults..."
for file in ../defaults/*; do
    if [ ! -f "$(basename ${file})" ]; then
        echo "    $(basename ${file})"
        cp ${file} .
    fi
done
echo "done!"

if ! grep -q eula=true eula.txt; then
    if [ "$EULA" != "true" ]; then
        echo "You must accept the Minecraft EULA to run the server! Read it at:"
        echo "> https://account.mojang.com/documents/minecraft_eula"
        echo "and then restart the server with EULA=true to accept the EULA."
        exit 1
    else
        sed -e "/^eula=/ s/=.*$/=${EULA}/" -i"" eula.txt
    fi
fi

sed -e "/^(query\.|server-)port=/ s/\d+/25565/" \
    -e "/^rcon.port=/ s/\d+/25575/" \
    -i"" server.properties

NURSERY_MINIMUM=$((${HEAP_SIZE} / 2))
NURSERY_MAXIMUM=$((${HEAP_SIZE} * 4 / 5))

JVM_OPTS="${JVM_OPTS} -Xms${HEAP_SIZE}M -Xmx${HEAP_SIZE}M -Xmns${NURSERY_MINIMUM}M -Xmnx${NURSERY_MAXIMUM}M"
JVM_OPTS="${JVM_OPTS} -Xgc:concurrentScavenge -Xgc:dnssExpectedTimeRatioMaximum=3 -Xgc:scvNoAdaptiveTenure"
JVM_OPTS="${JVM_OPTS} -Xdisableexplicitjc -Xtune:virtualized -Dlog4j.configurationFile=log4j2.xml"
SERVER_OPTS="--universe ../server --plugins ../plugins ${SERVER_OPTS}"

exec mc-server-runner java ${JVM_OPTS} -jar ../bin/paperclip.jar ${SERVER_OPTS}
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="25"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/usr/bin/env bash
export DOKKU_QUIET_OUTPUT=1
export DOKKU_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" &amp;&amp; pwd)/dokku"
export DOKKU_VERSION=${DOKKU_VERSION:-"master"}
export PATH="$(cd "$(dirname "${BASH_SOURCE[0]}")" &amp;&amp; pwd)/bin:$(cd "$(dirname "${BASH_SOURCE[0]}")" &amp;&amp; pwd)/dokku:$PATH"
export PLUGIN_COMMAND_PREFIX="s3"
export PLUGIN_PATH="$DOKKU_ROOT/plugins"
export PLUGIN_ENABLED_PATH="$PLUGIN_PATH"
export PLUGIN_AVAILABLE_PATH="$PLUGIN_PATH"
export PLUGIN_CORE_AVAILABLE_PATH="$PLUGIN_PATH"
export S3RVER_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" &amp;&amp; pwd)/fixtures"
export PLUGIN_DATA_ROOT="$S3RVER_ROOT"
if [[ "$(uname)" == "Darwin" ]]; then
  export PLUGN_URL="https://github.com/dokku/plugn/releases/download/v0.2.1/plugn_0.2.1_darwin_x86_64.tgz"
else
  export PLUGN_URL="https://github.com/dokku/plugn/releases/download/v0.2.1/plugn_0.2.1_linux_x86_64.tgz"
fi

mkdir -p "$PLUGIN_DATA_ROOT"
rm -rf "${PLUGIN_DATA_ROOT:?}"/*

flunk() {
  { if [ "$#" -eq 0 ]; then cat -
    else echo "$*"
    fi
  }
  return 1
}

assert_equal() {
  if [ "$1" != "$2" ]; then
    { echo "expected: $1"
      echo "actual:   $2"
    } | flunk
  fi
}

assert_exit_status() {
  assert_equal "$status" "$1"
}

assert_success() {
  if [ "$status" -ne 0 ]; then
    flunk "command failed with exit status $status"
  elif [ "$#" -gt 0 ]; then
    assert_output "$1"
  fi
}

assert_exists() {
  if [ ! -f "$1" ]; then
    flunk "expected file to exist: $1"
  fi
}

assert_contains() {
  if [[ "$1" != *"$2"* ]]; then
    flunk "expected $2 to be in: $1"
  fi
}

assert_output() {
  local expected
  if [ $# -eq 0 ]; then expected="$(cat -)"
  else expected="$1"
  fi
  assert_equal "$expected" "$output"
}
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="26"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
grunt
rm test/res/js/pagenav.js
cp pagenav.js test/res/js/pagenav.js
cp pagenav.min.js test/res/js/pagenav.min.js</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="27"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

printCommandHelp() {
    echo "Command Help:"
    echo -e "source patchPeerDeployment.sh &lt;peerSubscriptionID> &lt;peerResourceGroup> &lt;peerAKSClusterName>"
    echo
    echo "Arguments:"
    echo -e "\tpeerSubscriptionID    : Subscription ID of AKS-HLF peer template deployment"
    echo -e "\tpeerResourceGroup     : Resource group of AKS-HLF peer template deployment"
    echo -e "\tpeerAKSClusterName    : AKS Cluster name of AKS-HLF peer template deployment"
}

PEER_ORG_SUBSCRIPTION=$1
PEER_ORG_RESOURCE_GROUP=$2
PEER_ORG_AKS_NAME=$3

if [ -z $PEER_ORG_SUBSCRIPTION ] || [ -z $PEER_ORG_RESOURCE_GROUP ] || [ -z $PEER_ORG_AKS_NAME ]; then
    echo
    echo "Peer organization subscription, resource group and AKS cluster name cannot be empty!"
    echo

    printCommandHelp

    return;
fi

if ! command -v az &amp;> /dev/null; then
    echo
    echo "Command \"az\" not found! Please download Azure CLI for your system."
    echo "To setup Azure CLI after installation, run: az login with valid credentials!"
    echo

    return;
fi

az aks get-credentials --resource-group $PEER_ORG_RESOURCE_GROUP \
                       --name $PEER_ORG_AKS_NAME \
                       --subscription $PEER_ORG_SUBSCRIPTION
res=$?
if [ $res -ne 0 ]; then
    echo
    echo "Switching to AKS cluster config failed with error code: $res!"
    echo

    printCommandHelp
    
    return
fi

ns=hlf
deployments="$(kubectl get deploy -n $ns -o=jsonpath='{.items[*].metadata.name}')"

for deployment in $deployments; do
    resource=deploy/$deployment

    if [[ $deployment == peer* ]]; then
        echo "Updating" $deployment

        kubectl scale -n $ns $resource --replicas=0
        kubectl rollout status -n $ns $resource -w

        kubectl patch deployment $deployment -n $ns -p \
        '{"spec": { "template": { "spec": { "containers": [ { "name":"'$deployment'", "env": [{ "name": "CORE_CHAINCODE_BUILDER", "value": "hlfakstemplateoss.azurecr.io/hyperledger/fabric-ccenv:1.4.4" }, { "name": "CORE_CHAINCODE_GOLANG_RUNTIME", "value": "hlfakstemplateoss.azurecr.io/hyperledger/fabric-baseos:amd64-0.4.18" }, { "name": "CORE_CHAINCODE_NODE_RUNTIME", "value": "hlfakstemplateoss.azurecr.io/hyperledger/fabric-baseimage:amd64-0.4.18" }, { "name": "CORE_CHAINCODE_JAVA_RUNTIME", "value": "" }, { "name": "CORE_CHAINCODE_CAR_RUNTIME", "value": "" }] } ] } } } }'

        kubectl scale -n $ns $resource --replicas=1
        kubectl rollout status -n $ns $resource -w
    fi
done</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="28"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/sh
set -e

echo "mkdir -p ${CONFIGURATION_BUILD_DIR}/${FRAMEWORKS_FOLDER_PATH}"
mkdir -p "${CONFIGURATION_BUILD_DIR}/${FRAMEWORKS_FOLDER_PATH}"

SWIFT_STDLIB_PATH="${DT_TOOLCHAIN_DIR}/usr/lib/swift/${PLATFORM_NAME}"

install_framework()
{
  if [ -r "${BUILT_PRODUCTS_DIR}/$1" ]; then
    local source="${BUILT_PRODUCTS_DIR}/$1"
  elif [ -r "${BUILT_PRODUCTS_DIR}/$(basename "$1")" ]; then
    local source="${BUILT_PRODUCTS_DIR}/$(basename "$1")"
  elif [ -r "$1" ]; then
    local source="$1"
  fi

  local destination="${TARGET_BUILD_DIR}/${FRAMEWORKS_FOLDER_PATH}"

  if [ -L "${source}" ]; then
      echo "Symlinked..."
      source="$(readlink "${source}")"
  fi

  # use filter instead of exclude so missing patterns dont' throw errors
  echo "rsync -av --filter \"- CVS/\" --filter \"- .svn/\" --filter \"- .git/\" --filter \"- .hg/\" --filter \"- Headers\" --filter \"- PrivateHeaders\" --filter \"- Modules\" \"${source}\" \"${destination}\""
  rsync -av --filter "- CVS/" --filter "- .svn/" --filter "- .git/" --filter "- .hg/" --filter "- Headers" --filter "- PrivateHeaders" --filter "- Modules" "${source}" "${destination}"

  local basename
  basename="$(basename -s .framework "$1")"
  binary="${destination}/${basename}.framework/${basename}"
  if ! [ -r "$binary" ]; then
    binary="${destination}/${basename}"
  fi

  # Strip invalid architectures so "fat" simulator / device frameworks work on device
  if [[ "$(file "$binary")" == *"dynamically linked shared library"* ]]; then
    strip_invalid_archs "$binary"
  fi

  # Resign the code if required by the build settings to avoid unstable apps
  code_sign_if_enabled "${destination}/$(basename "$1")"

  # Embed linked Swift runtime libraries. No longer necessary as of Xcode 7.
  if [ "${XCODE_VERSION_MAJOR}" -lt 7 ]; then
    local swift_runtime_libs
    swift_runtime_libs=$(xcrun otool -LX "$binary" | grep --color=never @rpath/libswift | sed -E s/@rpath\\/\(.+dylib\).*/\\1/g | uniq -u  &amp;&amp; exit ${PIPESTATUS[0]})
    for lib in $swift_runtime_libs; do
      echo "rsync -auv \"${SWIFT_STDLIB_PATH}/${lib}\" \"${destination}\""
      rsync -auv "${SWIFT_STDLIB_PATH}/${lib}" "${destination}"
      code_sign_if_enabled "${destination}/${lib}"
    done
  fi
}

# Signs a framework with the provided identity
code_sign_if_enabled() {
  if [ -n "${EXPANDED_CODE_SIGN_IDENTITY}" -a "${CODE_SIGNING_REQUIRED}" != "NO" -a "${CODE_SIGNING_ALLOWED}" != "NO" ]; then
    # Use the current code_sign_identitiy
    echo "Code Signing $1 with Identity ${EXPANDED_CODE_SIGN_IDENTITY_NAME}"
    local code_sign_cmd="/usr/bin/codesign --force --sign ${EXPANDED_CODE_SIGN_IDENTITY} ${OTHER_CODE_SIGN_FLAGS} --preserve-metadata=identifier,entitlements '$1'"

    if [ "${COCOAPODS_PARALLEL_CODE_SIGN}" == "true" ]; then
      code_sign_cmd="$code_sign_cmd &amp;"
    fi
    echo "$code_sign_cmd"
    eval "$code_sign_cmd"
  fi
}

# Strip invalid architectures
strip_invalid_archs() {
  binary="$1"
  # Get architectures for current file
  archs="$(lipo -info "$binary" | rev | cut -d ':' -f1 | rev)"
  stripped=""
  for arch in $archs; do
    if ! [[ "${VALID_ARCHS}" == *"$arch"* ]]; then
      # Strip non-valid architectures in-place
      lipo -remove "$arch" -output "$binary" "$binary" || exit 1
      stripped="$stripped $arch"
    fi
  done
  if [[ "$stripped" ]]; then
    echo "Stripped $binary of architectures:$stripped"
  fi
}


if [[ "$CONFIGURATION" == "Debug" ]]; then
  install_framework "$BUILT_PRODUCTS_DIR/MyFirstCocoaPod/MyFirstCocoaPod.framework"
fi
if [[ "$CONFIGURATION" == "Release" ]]; then
  install_framework "$BUILT_PRODUCTS_DIR/MyFirstCocoaPod/MyFirstCocoaPod.framework"
fi
if [ "${COCOAPODS_PARALLEL_CODE_SIGN}" == "true" ]; then
  wait
fi
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="29"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">. inc/common.sh

if ! $XB_BIN --help 2>&amp;1 | grep -q debug-sync; then
    echo "Requires --debug-sync support" > $SKIPPED_REASON
    exit $SKIPPED_EXIT_CODE
fi

start_server --innodb_log_file_size=1M --innodb_thread_concurrency=1 \
    --innodb_log_buffer_size=1M

load_dbase_schema sakila
load_dbase_data sakila
mkdir $topdir/backup

run_cmd_expect_failure $XB_BIN $XB_ARGS --datadir=$mysql_datadir --backup \
    --innodb_log_file_size=1M --target-dir=$topdir/backup \
    --debug-sync="xtrabackup_copy_logfile_pause" &amp;

job_pid=$!

pid_file=$topdir/backup/xtrabackup_debug_sync

# Wait for xtrabackup to suspend
i=0
while [ ! -r "$pid_file" ]
do
    sleep 1
    i=$((i+1))
    echo "Waited $i seconds for $pid_file to be created"
done

xb_pid=`cat $pid_file`

# Create 4M+ of log data

$MYSQL $MYSQL_ARGS -Ns -e "CREATE TABLE tmp1 ENGINE=InnoDB SELECT * FROM payment" sakila
$MYSQL $MYSQL_ARGS -Ns -e "CREATE TABLE tmp2 ENGINE=InnoDB SELECT * FROM payment" sakila
$MYSQL $MYSQL_ARGS -Ns -e "CREATE TABLE tmp3 ENGINE=InnoDB SELECT * FROM payment" sakila

# Resume the xtrabackup process
vlog "Resuming xtrabackup"
kill -SIGCONT $xb_pid

# wait's return code will be the code returned by the background process
run_cmd wait $job_pid
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="30"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/sh

# 
# Vivado(TM)
# runme.sh: a Vivado-generated Runs Script for UNIX
# Copyright 1986-2020 Xilinx, Inc. All Rights Reserved.
# 

if [ -z "$PATH" ]; then
  PATH=/home/varun/tools/XilinX/Vitis/2020.2/bin:/home/varun/tools/XilinX/Vivado/2020.2/ids_lite/ISE/bin/lin64:/home/varun/tools/XilinX/Vivado/2020.2/bin
else
  PATH=/home/varun/tools/XilinX/Vitis/2020.2/bin:/home/varun/tools/XilinX/Vivado/2020.2/ids_lite/ISE/bin/lin64:/home/varun/tools/XilinX/Vivado/2020.2/bin:$PATH
fi
export PATH

if [ -z "$LD_LIBRARY_PATH" ]; then
  LD_LIBRARY_PATH=
else
  LD_LIBRARY_PATH=:$LD_LIBRARY_PATH
fi
export LD_LIBRARY_PATH

HD_PWD='/home/varun/coding/fpga/xylinx/pynq_z1/mpsoc_only_pl_counter/mpsoc_only_pl_counter.runs/synth_1'
cd "$HD_PWD"

HD_LOG=runme.log
/bin/touch $HD_LOG

ISEStep="./ISEWrap.sh"
EAStep()
{
     $ISEStep $HD_LOG "$@" >> $HD_LOG 2>&amp;1
     if [ $? -ne 0 ]
     then
         exit
     fi
}

EAStep vivado -log counter.vds -m64 -product Vivado -mode batch -messageDb vivado.pb -notrace -source counter.tcl
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="31"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
if [[ $target_platform =~ linux.* ]] || [[ $target_platform == win-32 ]] || [[ $target_platform == win-64 ]] || [[ $target_platform == osx-64 ]]; then
  export DISABLE_AUTOBREW=1
  $R CMD INSTALL --build .
else
  mkdir -p $PREFIX/lib/R/library/rgbif
  mv * $PREFIX/lib/R/library/rgbif
  if [[ $target_platform == osx-64 ]]; then
    pushd $PREFIX
      for libdir in lib/R/lib lib/R/modules lib/R/library lib/R/bin/exec sysroot/usr/lib; do
        pushd $libdir || exit 1
          for SHARED_LIB in $(find . -type f -iname "*.dylib" -or -iname "*.so" -or -iname "R"); do
            echo "fixing SHARED_LIB $SHARED_LIB"
            install_name_tool -change /Library/Frameworks/R.framework/Versions/3.5.0-MRO/Resources/lib/libR.dylib "$PREFIX"/lib/R/lib/libR.dylib $SHARED_LIB || true
            install_name_tool -change /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libR.dylib "$PREFIX"/lib/R/lib/libR.dylib $SHARED_LIB || true
            install_name_tool -change /usr/local/clang4/lib/libomp.dylib "$PREFIX"/lib/libomp.dylib $SHARED_LIB || true
            install_name_tool -change /usr/local/gfortran/lib/libgfortran.3.dylib "$PREFIX"/lib/libgfortran.3.dylib $SHARED_LIB || true
            install_name_tool -change /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libquadmath.0.dylib "$PREFIX"/lib/libquadmath.0.dylib $SHARED_LIB || true
            install_name_tool -change /usr/local/gfortran/lib/libquadmath.0.dylib "$PREFIX"/lib/libquadmath.0.dylib $SHARED_LIB || true
            install_name_tool -change /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libgfortran.3.dylib "$PREFIX"/lib/libgfortran.3.dylib $SHARED_LIB || true
            install_name_tool -change /usr/lib/libgcc_s.1.dylib "$PREFIX"/lib/libgcc_s.1.dylib $SHARED_LIB || true
            install_name_tool -change /usr/lib/libiconv.2.dylib "$PREFIX"/sysroot/usr/lib/libiconv.2.dylib $SHARED_LIB || true
            install_name_tool -change /usr/lib/libncurses.5.4.dylib "$PREFIX"/sysroot/usr/lib/libncurses.5.4.dylib $SHARED_LIB || true
            install_name_tool -change /usr/lib/libicucore.A.dylib "$PREFIX"/sysroot/usr/lib/libicucore.A.dylib $SHARED_LIB || true
            install_name_tool -change /usr/lib/libexpat.1.dylib "$PREFIX"/lib/libexpat.1.dylib $SHARED_LIB || true
            install_name_tool -change /usr/lib/libcurl.4.dylib "$PREFIX"/lib/libcurl.4.dylib $SHARED_LIB || true
            install_name_tool -change /usr/lib/libc++.1.dylib "$PREFIX"/lib/libc++.1.dylib $SHARED_LIB || true
            install_name_tool -change /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libc++.1.dylib "$PREFIX"/lib/libc++.1.dylib $SHARED_LIB || true
          done
        popd
      done
    popd
  fi
fi
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="32"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/sh
# Script which does some configuration of the system for laptops/desktops

test -e /sbin/rc-update
use_openrc=$?

setupZFSArc(){
  # Tune ZFS ARC 
  ###############################################
  grep -q "vfs.zfs.arc_max=" "/boot/loader.conf"
  if [ $? -eq 0 ] ; then
    return 0 #Do not overwrite current ARC settings
  fi

  # Get system memory in bytes
  sysMem=`sysctl hw.physmem | cut -w -f 2`
  # Get that in MB
  sysMem=`expr $sysMem / 1024 / 1024`
  # Set some default zArc sizes based upon RAM of system
  if [ $sysMem -lt 1024 ] ; then
    zArc="128"
  elif [ $sysMem -lt 4096 ] ; then
    zArc="256"
  else
    zArc="512"
  fi

  echo "# Tune ZFS Arc Size - Change to adjust memory used for disk cache" >> /boot/loader.conf
  echo "vfs.zfs.arc_max=\"${zArc}M\"" >> /boot/loader.conf
}

setupPowerd(){
  if [ ${use_openrc} -eq 0 ] ; then
    rc-update | grep -q powerd
  else
    grep -q -E 'powerd(xx)?_enable="YES"'
  fi
  if [ $? -eq 0 ] ; then
    #one of the powerd[++] service is already setup
    return
  fi
  p_service="powerd"
  if [ ${use_openrc} -eq 0 ] ; then
    if [ -e "/usr/local/etc/init.d/powerd++" ] ; then
      #The alternative powerd++ service is installed - use that instead
      p_service="powerd++"
    fi
    rc-update add ${p_service} default
  else
    if [ -e "/usr/local/etc/rc.d/powerdxx" ] ; then
      p_service="powerdxx"
    fi
    sysrc "${p_service}_enable=YES"
  fi
}

setupXProfile(){
  local _script="/usr/local/bin/setup-xorg-session"
  # Check all the .xprofile files in the user home dirs
  # And make sure they launch the x session setup script
  for _hd in $(ls /usr/home)
  do
    if [ ! -e "/usr/home/${_hd}/.xprofile" ] ; then continue; fi
    grep -q "${_script}" "/usr/home/${_hd}/.xprofile"
    if [ $? -ne 0 ] ; then
      echo "
if [ -e \"${_script}\" ] ; then
  . ${_script}
fi
" >> "/usr/home/${_hd}/.xprofile"
    fi
  done
  #Now make sure the default ~/.xprofile exists and/or is setup
  if [ ! -e "/usr/share/skel/dot.xprofile" ] ; then
    echo "# Graphical session setup
# Created by Project Trident
# ===================
if [ -e \"${_script}\" ] ; then
  . ${_script}
fi
" >> "/usr/share/skel/dot.xprofile"

  else
    grep -q "${_script}" "/usr/share/skel/dot.xprofile"
    if [ $? -ne 0 ] ; then
      echo "
if [ -e \"${_script}\" ] ; then
  . ${_script}
fi
" >> "/usr/share/skel/dot.xprofile"
    fi
  fi
}

setupWlan(){
  # Check for any new wifi devices to setup
  for wnic in `sysctl -n net.wlan.devices 2>/dev/null`
  do
    #See if this device is already configured
    grep -q "wlans_${wnic}" /etc/rc.conf
    if [ $? -ne 0 ] ; then
      # New wifi device - determine the next number for it
      grep -qE "^wlans_" /etc/rc.conf
      if [ $? -eq 0 ] ; then
        WLANCOUNT=`cat /etc/rc.conf | grep -E "^wlans_" | wc -l | awk '{print $1}'`
      else
        WLANCOUNT="0"
      fi
      WLAN="wlan${WLANCOUNT}"
      # Save the wlan interface
      echo "wlans_${wnic}=\"${WLAN}\"" >> /etc/rc.conf
      echo "ifconfig_${WLAN}=\"WPA DHCP\"" >> /etc/rc.conf
      echo "ifconfig_${WLAN}_ipv6=\"inet6 accept_rtadv\"" >> /etc/rc.conf
    fi
  done
}

setupLan(){
  for nic in `ifconfig -l`
  do
    #Ignore loopback devices
    echo ${nic} | grep -qE "lo[0-9]"
    if [ 0 -eq $? ] ; then continue; fi
    #See if this device is already configured
    sysrc -ci "ifconfig_${nic}"
    if [ $? -ne 0 ] ; then
      # New ethernet device
      sysrc "ifconfig_${nic}=DHCP"
      sysrc "ifconfig_${nic}_ipv6=inet6 accept_rtadv"
    fi
  done
}

#figure out if this is a laptop, desktop, or VM (VMWare or VirtualBox only at the moment)
pciconf -lv | grep -qiE "(vmware|innotek)"
if [ $? -eq 0 ] ; then
  type="vm"
else
  devinfo | grep -q acpi_acad0
  if [ $? -eq 0 ] ; then
    type="laptop"
  else
    type="desktop"
  fi
fi

################################################
# Verify generic init
################################################

if [ ! -d "/usr/home" ] ; then
   mkdir /usr/home
fi

# Setup /home link (for people used to Linux, and some applications)
if [ ! -e "/home" ] ; then
  ln -s /usr/home /home
fi

#Check/set the ZFS arc size
setupZFSArc

#Turn on power management service (if one is not already setup)
if [ "type" != "vm" ] ; then
  setupPowerd
fi

if [ "${type}" = "laptop" ] ; then
  # Laptop system
  # TO-DO  
else
  # Desktop system
  # TO-DO
fi

#setup the networking interfaces
setupLan
setupWlan
setupXProfile

#Perform the system sanity check
/usr/local/share/trident/scripts/system-sanity-check.sh


#TrueOS 18.06-18.08 Bug Bypass (8/23/18 - Ken Moore)
# - replace "DHCP" with "SYNCDHCP" in the default-installed /etc/rc.conf
#sed -i '' 's|"DHCP|"SYNCDHCP|g' /etc/rc.conf
#sed -i '' 's| DHCP"| SYNCDHCP"|g' /etc/rc.conf

#Now ensure the system services are all setup properly
/usr/local/share/trident/scripts/validate-services.sh /usr/local/etc/trident/required-services /usr/local/etc/trident/recommended-services
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="33"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># bash/zsh git prompt support
#
# Copyright (C) 2006,2007 Shawn O. Pearce &lt;spearce@spearce.org>
# Distributed under the GNU General Public License, version 2.0.
#
# This script allows you to see repository status in your prompt.
#
# To enable:
#
#    1) Copy this file to somewhere (e.g. ~/.git-prompt.sh).
#    2) Add the following line to your .bashrc/.zshrc:
#        source ~/.git-prompt.sh
#    3a) Change your PS1 to call __git_ps1 as
#        command-substitution:
#        Bash: PS1='[\u@\h \W$(__git_ps1 " (%s)")]\$ '
#        ZSH:  setopt PROMPT_SUBST ; PS1='[%n@%m %c$(__git_ps1 " (%s)")]\$ '
#        the optional argument will be used as format string.
#    3b) Alternatively, for a slightly faster prompt, __git_ps1 can
#        be used for PROMPT_COMMAND in Bash or for precmd() in Zsh
#        with two parameters, &lt;pre> and &lt;post>, which are strings
#        you would put in $PS1 before and after the status string
#        generated by the git-prompt machinery.  e.g.
#        Bash: PROMPT_COMMAND='__git_ps1 "\u@\h:\w" "\\\$ "'
#          will show username, at-sign, host, colon, cwd, then
#          various status string, followed by dollar and SP, as
#          your prompt.
#        ZSH:  precmd () { __git_ps1 "%n" ":%~$ " "|%s" }
#          will show username, pipe, then various status string,
#          followed by colon, cwd, dollar and SP, as your prompt.
#        Optionally, you can supply a third argument with a printf
#        format string to finetune the output of the branch status
#
# The repository status will be displayed only if you are currently in a
# git repository. The %s token is the placeholder for the shown status.
#
# The prompt status always includes the current branch name.
#
# In addition, if you set GIT_PS1_SHOWDIRTYSTATE to a nonempty value,
# unstaged (*) and staged (+) changes will be shown next to the branch
# name.  You can configure this per-repository with the
# bash.showDirtyState variable, which defaults to true once
# GIT_PS1_SHOWDIRTYSTATE is enabled.
#
# You can also see if currently something is stashed, by setting
# GIT_PS1_SHOWSTASHSTATE to a nonempty value. If something is stashed,
# then a '$' will be shown next to the branch name.
#
# If you would like to see if there're untracked files, then you can set
# GIT_PS1_SHOWUNTRACKEDFILES to a nonempty value. If there're untracked
# files, then a '%' will be shown next to the branch name.  You can
# configure this per-repository with the bash.showUntrackedFiles
# variable, which defaults to true once GIT_PS1_SHOWUNTRACKEDFILES is
# enabled.
#
# If you would like to see the difference between HEAD and its upstream,
# set GIT_PS1_SHOWUPSTREAM="auto".  A "&lt;" indicates you are behind, ">"
# indicates you are ahead, "&lt;>" indicates you have diverged and "="
# indicates that there is no difference. You can further control
# behaviour by setting GIT_PS1_SHOWUPSTREAM to a space-separated list
# of values:
#
#     verbose       show number of commits ahead/behind (+/-) upstream
#     name          if verbose, then also show the upstream abbrev name
#     legacy        don't use the '--count' option available in recent
#                   versions of git-rev-list
#     git           always compare HEAD to @{upstream}
#     svn           always compare HEAD to your SVN upstream
#
# You can change the separator between the branch name and the above
# state symbols by setting GIT_PS1_STATESEPARATOR. The default separator
# is SP.
#
# By default, __git_ps1 will compare HEAD to your SVN upstream if it can
# find one, or @{upstream} otherwise.  Once you have set
# GIT_PS1_SHOWUPSTREAM, you can override it on a per-repository basis by
# setting the bash.showUpstream config variable.
#
# If you would like to see more information about the identity of
# commits checked out as a detached HEAD, set GIT_PS1_DESCRIBE_STYLE
# to one of these values:
#
#     contains      relative to newer annotated tag (v1.6.3.2~35)
#     branch        relative to newer tag or branch (master~4)
#     describe      relative to older annotated tag (v1.6.3.1-13-gdd42c2f)
#     tag           relative to any older tag (v1.6.3.1-13-gdd42c2f)
#     default       exactly matching tag
#
# If you would like a colored hint about the current dirty state, set
# GIT_PS1_SHOWCOLORHINTS to a nonempty value. The colors are based on
# the colored output of "git status -sb" and are available only when
# using __git_ps1 for PROMPT_COMMAND or precmd.
#
# If you would like __git_ps1 to do nothing in the case when the current
# directory is set up to be ignored by git, then set
# GIT_PS1_HIDE_IF_PWD_IGNORED to a nonempty value. Override this on the
# repository level by setting bash.hideIfPwdIgnored to "false".

# check whether printf supports -v
__git_printf_supports_v=
printf -v __git_printf_supports_v -- '%s' yes >/dev/null 2>&amp;1

# stores the divergence from upstream in $p
# used by GIT_PS1_SHOWUPSTREAM
__git_ps1_show_upstream ()
{
	local key value
	local svn_remote svn_url_pattern count n
	local upstream=git legacy="" verbose="" name=""

	svn_remote=()
	# get some config options from git-config
	local output="$(git config -z --get-regexp '^(svn-remote\..*\.url|bash\.showupstream)$' 2>/dev/null | tr '\0\n' '\n ')"
	while read -r key value; do
		case "$key" in
		bash.showupstream)
			GIT_PS1_SHOWUPSTREAM="$value"
			if [[ -z "${GIT_PS1_SHOWUPSTREAM}" ]]; then
				p=""
				return
			fi
			;;
		svn-remote.*.url)
			svn_remote[$((${#svn_remote[@]} + 1))]="$value"
			svn_url_pattern="$svn_url_pattern\\|$value"
			upstream=svn+git # default upstream is SVN if available, else git
			;;
		esac
	done &lt;&lt;&lt; "$output"

	# parse configuration values
	for option in ${GIT_PS1_SHOWUPSTREAM}; do
		case "$option" in
		git|svn) upstream="$option" ;;
		verbose) verbose=1 ;;
		legacy)  legacy=1  ;;
		name)    name=1 ;;
		esac
	done

	# Find our upstream
	case "$upstream" in
	git)    upstream="@{upstream}" ;;
	svn*)
		# get the upstream from the "git-svn-id: ..." in a commit message
		# (git-svn uses essentially the same procedure internally)
		local -a svn_upstream
		svn_upstream=($(git log --first-parent -1 \
					--grep="^git-svn-id: \(${svn_url_pattern#??}\)" 2>/dev/null))
		if [[ 0 -ne ${#svn_upstream[@]} ]]; then
			svn_upstream=${svn_upstream[${#svn_upstream[@]} - 2]}
			svn_upstream=${svn_upstream%@*}
			local n_stop="${#svn_remote[@]}"
			for ((n=1; n &lt;= n_stop; n++)); do
				svn_upstream=${svn_upstream#${svn_remote[$n]}}
			done

			if [[ -z "$svn_upstream" ]]; then
				# default branch name for checkouts with no layout:
				upstream=${GIT_SVN_ID:-git-svn}
			else
				upstream=${svn_upstream#/}
			fi
		elif [[ "svn+git" = "$upstream" ]]; then
			upstream="@{upstream}"
		fi
		;;
	esac

	# Find how many commits we are ahead/behind our upstream
	if [[ -z "$legacy" ]]; then
		count="$(git rev-list --count --left-right \
				"$upstream"...HEAD 2>/dev/null)"
	else
		# produce equivalent output to --count for older versions of git
		local commits
		if commits="$(git rev-list --left-right "$upstream"...HEAD 2>/dev/null)"
		then
			local commit behind=0 ahead=0
			for commit in $commits
			do
				case "$commit" in
				"&lt;"*) ((behind++)) ;;
				*)    ((ahead++))  ;;
				esac
			done
			count="$behind	$ahead"
		else
			count=""
		fi
	fi

	# calculate the result
	if [[ -z "$verbose" ]]; then
		case "$count" in
		"") # no upstream
			p="" ;;
		"0	0") # equal to upstream
			p="=" ;;
		"0	"*) # ahead of upstream
			p=">" ;;
		*"	0") # behind upstream
			p="&lt;" ;;
		*)	    # diverged from upstream
			p="&lt;>" ;;
		esac
	else
		case "$count" in
		"") # no upstream
			p="" ;;
		"0	0") # equal to upstream
			p=" u=" ;;
		"0	"*) # ahead of upstream
			p=" u+${count#0	}" ;;
		*"	0") # behind upstream
			p=" u-${count%	0}" ;;
		*)	    # diverged from upstream
			p=" u+${count#*	}-${count%	*}" ;;
		esac
		if [[ -n "$count" &amp;&amp; -n "$name" ]]; then
			__git_ps1_upstream_name=$(git rev-parse \
				--abbrev-ref "$upstream" 2>/dev/null)
			if [ "$pcmode" = yes ] &amp;&amp; [ "$ps1_expanded" = yes ]; then
				p="$p \${__git_ps1_upstream_name}"
			else
				p="$p ${__git_ps1_upstream_name}"
				# not needed anymore; keep user's
				# environment clean
				unset __git_ps1_upstream_name
			fi
		fi
	fi

}

# Helper function that is meant to be called from __git_ps1.  It
# injects color codes into the appropriate gitstring variables used
# to build a gitstring.
__git_ps1_colorize_gitstring ()
{
	if [[ -n "${ZSH_VERSION-}" ]]; then
		local c_red='%F{red}'
		local c_green='%F{green}'
		local c_lblue='%F{blue}'
		local c_clear='%f'
	else
		# Using \[ and \] around colors is necessary to prevent
		# issues with command line editing/browsing/completion!
		local c_red='\[\e[31m\]'
		local c_green='\[\e[32m\]'
		local c_lblue='\[\e[1;34m\]'
		local c_clear='\[\e[0m\]'
	fi
	local bad_color=$c_red
	local ok_color=$c_green
	local flags_color="$c_lblue"

	local branch_color=""
	if [ "$detached" = no ]; then
		branch_color="$ok_color"
	else
		branch_color="$bad_color"
	fi
	c="$branch_color$c"

	z="$c_clear$z"
	if [ "$w" = "*" ]; then
		w="$bad_color$w"
	fi
	if [ -n "$i" ]; then
		i="$ok_color$i"
	fi
	if [ -n "$s" ]; then
		s="$flags_color$s"
	fi
	if [ -n "$u" ]; then
		u="$bad_color$u"
	fi
	r="$c_clear$r"
}

# Helper function to read the first line of a file into a variable.
# __git_eread requires 2 arguments, the file path and the name of the
# variable, in that order.
__git_eread ()
{
	test -r "$1" &amp;&amp; IFS=$'\r\n' read "$2" &lt;"$1"
}

# __git_ps1 accepts 0 or 1 arguments (i.e., format string)
# when called from PS1 using command substitution
# in this mode it prints text to add to bash PS1 prompt (includes branch name)
#
# __git_ps1 requires 2 or 3 arguments when called from PROMPT_COMMAND (pc)
# in that case it _sets_ PS1. The arguments are parts of a PS1 string.
# when two arguments are given, the first is prepended and the second appended
# to the state string when assigned to PS1.
# The optional third parameter will be used as printf format string to further
# customize the output of the git-status string.
# In this mode you can request colored hints using GIT_PS1_SHOWCOLORHINTS=true
__git_ps1 ()
{
	# preserve exit status
	local exit=$?
	local pcmode=no
	local detached=no
	local ps1pc_start='\u@\h:\w '
	local ps1pc_end='\$ '
	local printf_format=' (%s)'

	case "$#" in
		2|3)	pcmode=yes
			ps1pc_start="$1"
			ps1pc_end="$2"
			printf_format="${3:-$printf_format}"
			# set PS1 to a plain prompt so that we can
			# simply return early if the prompt should not
			# be decorated
			PS1="$ps1pc_start$ps1pc_end"
		;;
		0|1)	printf_format="${1:-$printf_format}"
		;;
		*)	return $exit
		;;
	esac

	# ps1_expanded:  This variable is set to 'yes' if the shell
	# subjects the value of PS1 to parameter expansion:
	#
	#   * bash does unless the promptvars option is disabled
	#   * zsh does not unless the PROMPT_SUBST option is set
	#   * POSIX shells always do
	#
	# If the shell would expand the contents of PS1 when drawing
	# the prompt, a raw ref name must not be included in PS1.
	# This protects the user from arbitrary code execution via
	# specially crafted ref names.  For example, a ref named
	# 'refs/heads/$(IFS=_;cmd=sudo_rm_-rf_/;$cmd)' might cause the
	# shell to execute 'sudo rm -rf /' when the prompt is drawn.
	#
	# Instead, the ref name should be placed in a separate global
	# variable (in the __git_ps1_* namespace to avoid colliding
	# with the user's environment) and that variable should be
	# referenced from PS1.  For example:
	#
	#     __git_ps1_foo=$(do_something_to_get_ref_name)
	#     PS1="...stuff...\${__git_ps1_foo}...stuff..."
	#
	# If the shell does not expand the contents of PS1, the raw
	# ref name must be included in PS1.
	#
	# The value of this variable is only relevant when in pcmode.
	#
	# Assume that the shell follows the POSIX specification and
	# expands PS1 unless determined otherwise.  (This is more
	# likely to be correct if the user has a non-bash, non-zsh
	# shell and safer than the alternative if the assumption is
	# incorrect.)
	#
	local ps1_expanded=yes
	[ -z "${ZSH_VERSION-}" ] || [[ -o PROMPT_SUBST ]] || ps1_expanded=no
	[ -z "${BASH_VERSION-}" ] || shopt -q promptvars || ps1_expanded=no

	local repo_info rev_parse_exit_code
	repo_info="$(git rev-parse --git-dir --is-inside-git-dir \
		--is-bare-repository --is-inside-work-tree \
		--short HEAD 2>/dev/null)"
	rev_parse_exit_code="$?"

	if [ -z "$repo_info" ]; then
		return $exit
	fi

	local short_sha=""
	if [ "$rev_parse_exit_code" = "0" ]; then
		short_sha="${repo_info##*$'\n'}"
		repo_info="${repo_info%$'\n'*}"
	fi
	local inside_worktree="${repo_info##*$'\n'}"
	repo_info="${repo_info%$'\n'*}"
	local bare_repo="${repo_info##*$'\n'}"
	repo_info="${repo_info%$'\n'*}"
	local inside_gitdir="${repo_info##*$'\n'}"
	local g="${repo_info%$'\n'*}"

	if [ "true" = "$inside_worktree" ] &amp;&amp;
	   [ -n "${GIT_PS1_HIDE_IF_PWD_IGNORED-}" ] &amp;&amp;
	   [ "$(git config --bool bash.hideIfPwdIgnored)" != "false" ] &amp;&amp;
	   git check-ignore -q .
	then
		return $exit
	fi

	local r=""
	local b=""
	local step=""
	local total=""
	if [ -d "$g/rebase-merge" ]; then
		__git_eread "$g/rebase-merge/head-name" b
		__git_eread "$g/rebase-merge/msgnum" step
		__git_eread "$g/rebase-merge/end" total
		if [ -f "$g/rebase-merge/interactive" ]; then
			r="|REBASE-i"
		else
			r="|REBASE-m"
		fi
	else
		if [ -d "$g/rebase-apply" ]; then
			__git_eread "$g/rebase-apply/next" step
			__git_eread "$g/rebase-apply/last" total
			if [ -f "$g/rebase-apply/rebasing" ]; then
				__git_eread "$g/rebase-apply/head-name" b
				r="|REBASE"
			elif [ -f "$g/rebase-apply/applying" ]; then
				r="|AM"
			else
				r="|AM/REBASE"
			fi
		elif [ -f "$g/MERGE_HEAD" ]; then
			r="|MERGING"
		elif [ -f "$g/CHERRY_PICK_HEAD" ]; then
			r="|CHERRY-PICKING"
		elif [ -f "$g/REVERT_HEAD" ]; then
			r="|REVERTING"
		elif [ -f "$g/BISECT_LOG" ]; then
			r="|BISECTING"
		fi

		if [ -n "$b" ]; then
			:
		elif [ -h "$g/HEAD" ]; then
			# symlink symbolic ref
			b="$(git symbolic-ref HEAD 2>/dev/null)"
		else
			local head=""
			if ! __git_eread "$g/HEAD" head; then
				return $exit
			fi
			# is it a symbolic ref?
			b="${head#ref: }"
			if [ "$head" = "$b" ]; then
				detached=yes
				b="$(
				case "${GIT_PS1_DESCRIBE_STYLE-}" in
				(contains)
					git describe --contains HEAD ;;
				(branch)
					git describe --contains --all HEAD ;;
				(tag)
					git describe --tags HEAD ;;
				(describe)
					git describe HEAD ;;
				(* | default)
					git describe --tags --exact-match HEAD ;;
				esac 2>/dev/null)" ||

				b="$short_sha..."
				b="($b)"
			fi
		fi
	fi

	if [ -n "$step" ] &amp;&amp; [ -n "$total" ]; then
		r="$r $step/$total"
	fi

	local w=""
	local i=""
	local s=""
	local u=""
	local c=""
	local p=""

	if [ "true" = "$inside_gitdir" ]; then
		if [ "true" = "$bare_repo" ]; then
			c="BARE:"
		else
			b="GIT_DIR!"
		fi
	elif [ "true" = "$inside_worktree" ]; then
		if [ -n "${GIT_PS1_SHOWDIRTYSTATE-}" ] &amp;&amp;
		   [ "$(git config --bool bash.showDirtyState)" != "false" ]
		then
			git diff --no-ext-diff --quiet || w="*"
			git diff --no-ext-diff --cached --quiet || i="+"
			if [ -z "$short_sha" ] &amp;&amp; [ -z "$i" ]; then
				i="#"
			fi
		fi
		if [ -n "${GIT_PS1_SHOWSTASHSTATE-}" ] &amp;&amp;
		   git rev-parse --verify --quiet refs/stash >/dev/null
		then
			s="$"
		fi

		if [ -n "${GIT_PS1_SHOWUNTRACKEDFILES-}" ] &amp;&amp;
		   [ "$(git config --bool bash.showUntrackedFiles)" != "false" ] &amp;&amp;
		   git ls-files --others --exclude-standard --directory --no-empty-directory --error-unmatch -- ':/*' >/dev/null 2>/dev/null
		then
			u="%${ZSH_VERSION+%}"
		fi

		if [ -n "${GIT_PS1_SHOWUPSTREAM-}" ]; then
			__git_ps1_show_upstream
		fi
	fi

	local z="${GIT_PS1_STATESEPARATOR-" "}"

	# NO color option unless in PROMPT_COMMAND mode or it's Zsh
	if [ -n "${GIT_PS1_SHOWCOLORHINTS-}" ]; then
		if [ "$pcmode" = yes ] || [ -n "${ZSH_VERSION-}" ]; then
			__git_ps1_colorize_gitstring
		fi
	fi

	b=${b##refs/heads/}
	if [ "$pcmode" = yes ] &amp;&amp; [ "$ps1_expanded" = yes ]; then
		__git_ps1_branch_name=$b
		b="\${__git_ps1_branch_name}"
	fi

	local f="$w$i$s$u"
	local gitstring="$c$b${f:+$z$f}$r$p"

	if [ "$pcmode" = yes ]; then
		if [ "${__git_printf_supports_v-}" != yes ]; then
			gitstring=$(printf -- "$printf_format" "$gitstring")
		else
			printf -v gitstring -- "$printf_format" "$gitstring"
		fi
		PS1="$ps1pc_start$gitstring$ps1pc_end"
	else
		printf -- "$printf_format" "$gitstring"
	fi

	return $exit
}
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="34"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

#Copyright (c) 2016, Allgeyer Tobias, Aumann Florian, Borella Jocelyn, Karrenbauer Oliver, Marek Felix, Meissner Pascal, Stroh Daniel, Trautmann Jeremias
#All rights reserved.
#
#Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
#
#1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
#
#2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other #materials provided with the distribution.
#
#3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific #prior written permission.
#
#THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED #WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, #INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR #PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) #ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

# read http://unix.stackexchange.com/questions/17116/prevent-pane-window-from-closing-when-command-completes-tmux
# Starts additional simulation modules that cannot be launched before gazebo is running

# wait for gazebo and rviz

sleep 5

# while [[ ! $(rosservice list | grep gaz) ]]; do sleep 1; done;
# while [[ ! $(xwininfo -root -all | grep rviz) ]]; do sleep 1; done;


# make sure log folder exist, so all modules start safely
export logFolder=~/log
mkdir -p ${logFolder}

#Starts scene recognition, pose prediction and provides a pane for interfaces with object localization simulation.
tmux new-window -n 'ism' 
tmux send-keys -t asr:ism 'script -c "roslaunch --wait asr_recognizer_prediction_ism rp_ism_node.launch" -f '"${logFolder}"'/ism.log' C-m
tmux split-window -t asr:ism
tmux send-keys -t asr:ism.1 'echo Perform service calls to asr_fake_object_recognition from here.' C-m

#Starts next-best-view calculation and world model.
tmux new-window -n 'nbv'
tmux send-keys -t asr:nbv 'script -c "roslaunch --wait asr_next_best_view next_best_view_core_sim.launch" -f '"${logFolder}"'/nbv.log' C-m
tmux split-window -t asr:nbv
tmux send-keys -t asr:nbv.1 'script -c "roslaunch --wait asr_world_model world_model.launch" -f '"${logFolder}"'/world_model.log' C-m

#Starts visualization server to publish the room model.
tmux new-window -n 'viz_server'
tmux send-keys -t asr:viz_server 'script -c "roslaunch --wait asr_visualization_server visualization.launch" -f '"${logFolder}"'/viz_server.log'  C-m

#Starts state machine that controls all other components, required for active scene recognition.
tmux new-window -n 'state_machine'
tmux send-keys -t asr:state_machine 'script -c "roslaunch --wait asr_state_machine scene_exploration_sim.launch" -f '"${logFolder}"'/state_machine.log'  C-m

#Starts direct_search_manager that handles the direct search
tmux new-window -n 'direct_search_manager'
tmux send-keys -t asr:direct_search_manager 'script -c "roslaunch --wait asr_direct_search_manager direct_search_manager.launch" -f '"${logFolder}"'/direct_search_manager.log'  C-m

</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="35"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/sh

docker push robodomo/icomfort-microservice

</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="36"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/usr/bin/env bash

set -e
set -o pipefail

# vars
PYTHON=python
PIP=pip
VENV_NAME=

# process options
while getopts "h?3e:" opt; do
    case "$opt" in
        h|\?)
            echo "install.sh parameters"
            echo ""
            echo "-3 install for Python 3.3+"
            echo "-e [environment name] install to a virtual environment"
            echo ""
            exit 1
            ;;
        3)
            PYTHON=python3
            PIP=pip3
            ;;
        e)
            VENV_NAME=$OPTARG
            ;;
    esac
done
shift $((OPTIND-1))
[ "$1" = "--" ] &amp;&amp; shift

# check to ensure this is not being run directly as root
if [ $(id -u) -eq 0 ]; then
    echo "Installation cannot be performed as root or via sudo."
    echo "Please install as a regular user."
    exit 1
fi

# check for sudo
if hash sudo 2> /dev/null; then
    echo "sudo found."
else
    echo "sudo not found. Please install sudo first before proceeding."
    exit 1
fi

# check that shipyard.py is in cwd
if [ ! -f $PWD/shipyard.py ]; then
    echo "shipyard.py not found in $PWD."
    echo "Please run install.sh from the same directory as shipyard.py."
    exit 1
fi

# check for python
if hash $PYTHON 2> /dev/null; then
    echo "Installing for $PYTHON."
else
    echo "$PYTHON not found, please install $PYTHON first with your system software installer."
    exit 1
fi

# check for anaconda
set +e
ANACONDA=0
$PYTHON -c "from __future__ import print_function; import sys; print(sys.version)" | grep -Ei 'anaconda|continuum'
if [ $? -eq 0 ]; then
    # check for conda
    if hash conda 2> /dev/null; then
        echo "Anaconda environment detected."
    else
        echo "Anaconda environment detected, but conda command not found."
        exit 1
    fi
    if [ -z $VENV_NAME ]; then
        echo "Virtual environment name must be supplied for Anaconda installations."
        exit 1
    fi
    ANACONDA=1
    PIP=pip
fi
set -e

# perform some virtual env parameter checks
INSTALL_VENV_BIN=0
if [ ! -z $VENV_NAME ]; then
    # check if virtual env, env is not named shipyard
    if [ "$VENV_NAME" == "shipyard" ]; then
        echo "Virtual environment name cannot be shipyard. Please use a different virtual environment name."
        exit 1
    fi
    # check for virtualenv executable
    if [ $ANACONDA -eq 0 ]; then
        if hash virtualenv 2> /dev/null; then
            echo "virtualenv found."
        else
            echo "virtualenv not found."
            INSTALL_VENV_BIN=1
        fi
    fi
fi

# try to get /etc/lsb-release
if [ -e /etc/lsb-release ]; then
    . /etc/lsb-release
else
    if [ -e /etc/os-release ]; then
        . /etc/os-release
        DISTRIB_ID=$ID
        DISTRIB_RELEASE=$VERSION_ID
    fi
fi

if [ -z ${DISTRIB_ID+x} ] || [ -z ${DISTRIB_RELEASE+x} ]; then
    echo "Unknown DISTRIB_ID or DISTRIB_RELEASE."
    echo "Please refer to the Installation documentation for manual installation steps."
    exit 1
fi

# lowercase vars
DISTRIB_ID=${DISTRIB_ID,,}
DISTRIB_RELEASE=${DISTRIB_RELEASE,,}

# install requisite packages from distro repo
if [ $DISTRIB_ID == "ubuntu" ] || [ $DISTRIB_ID == "debian" ]; then
    sudo apt-get update
    if [ $PYTHON == "python" ]; then
        PYTHON_PKGS="libpython-dev python-dev"
        if [ $ANACONDA -eq 0 ]; then
            PYTHON_PKGS="$PYTHON_PKGS python-pip"
        fi
    else
        PYTHON_PKGS="libpython3-dev python3-dev"
        if [ $ANACONDA -eq 0 ]; then
            PYTHON_PKGS="$PYTHON_PKGS python3-pip"
        fi
    fi
    sudo apt-get install -y --no-install-recommends \
        build-essential libssl-dev libffi-dev openssl \
        openssh-client rsync $PYTHON_PKGS
elif [ $DISTRIB_ID == "centos" ] || [ $DISTRIB_ID == "rhel" ]; then
    if [ $PYTHON == "python" ]; then
        PYTHON_PKGS="python-devel"
    else
        if [ $(yum list installed epel-release) -ne 0 ]; then
            echo "epel-release package not installed."
            echo "Please install the epel-release package or refer to the Installation documentation for manual installation steps".
            exit 1
        fi
        if [ $(yum list installed python34) -ne 0 ]; then
            echo "python34 epel package not installed."
            echo "Please install the python34 epel package or refer to the Installation documentation for manual installation steps."
            exit 1
        fi
        PYTHON_PKGS="python34-devel"
    fi
    sudo yum install -y gcc openssl-devel libffi-devel openssl \
        openssh-clients rsync $PYTHON_PKGS
    if [ $ANACONDA -eq 0 ]; then
        curl -fSsL https://bootstrap.pypa.io/get-pip.py | sudo $PYTHON
    fi
elif [ $DISTRIB_ID == "opensuse" ] || [ $DISTRIB_ID == "sles" ]; then
    sudo zypper ref
    if [ $PYTHON == "python" ]; then
        PYTHON_PKGS="python-devel"
    else
        PYTHON_PKGS="python3-devel"
    fi
    sudo zypper -n in gcc libopenssl-devel libffi48-devel openssl \
        openssh rsync $PYTHON_PKGS
    if [ $ANACONDA -eq 0 ]; then
        curl -fSsL https://bootstrap.pypa.io/get-pip.py | sudo $PYTHON
    fi
else
    echo "Unsupported distribution."
    echo "Please refer to the Installation documentation for manual installation steps."
    exit 1
fi

# create virtual env if required and install required python packages
if [ ! -z $VENV_NAME ]; then
    # install virtual env if required
    if [ $INSTALL_VENV_BIN -eq 1 ]; then
        sudo $PIP install virtualenv
    fi
    if [ $ANACONDA -eq 0 ]; then
        # create venv if it doesn't exist
        virtualenv -p $PYTHON $VENV_NAME
        source $VENV_NAME/bin/activate
        $PIP install --upgrade pip setuptools
        $PIP install --upgrade -r requirements.txt
        deactivate
    else
        # create conda env
        set +e
        conda create --yes --name $VENV_NAME
        set -e
        source activate $VENV_NAME
        conda install --yes pip
        # temporary workaround with pip requirements upgrading setuptools and
        # conda pip failing to reference the old setuptools version
        set +e
        $PIP install --upgrade setuptools
        set -e
        $PIP install --upgrade -r requirements.txt
        source deactivate $VENV_NAME
    fi
else
    sudo $PIP install --upgrade pip setuptools
    $PIP install --upgrade --user -r requirements.txt
fi

# create shipyard script
cat > shipyard &lt;&lt; EOF
#!/usr/bin/env bash

set -e
set -f

BATCH_SHIPYARD_ROOT_DIR=$PWD
VENV_NAME=$VENV_NAME

EOF
cat >> shipyard &lt;&lt; 'EOF'
if [ -z $BATCH_SHIPYARD_ROOT_DIR ]; then
    echo Batch Shipyard root directory not set.
    echo Please rerun the install.sh script.
    exit 1
fi

EOF

if [ ! -z $VENV_NAME ]; then
    if [ $ANACONDA -eq 0 ]; then
cat >> shipyard &lt;&lt; 'EOF'
source $BATCH_SHIPYARD_ROOT_DIR/$VENV_NAME/bin/activate
EOF
    else
cat >> shipyard &lt;&lt; 'EOF'
source activate $VENV_NAME
EOF
    fi
fi

if [ $PYTHON == "python" ]; then
cat >> shipyard &lt;&lt; 'EOF'
python $BATCH_SHIPYARD_ROOT_DIR/shipyard.py $*
EOF
else
cat >> shipyard &lt;&lt; 'EOF'
python3 $BATCH_SHIPYARD_ROOT_DIR/shipyard.py $*
EOF
fi

if [ ! -z $VENV_NAME ]; then
    if [ $ANACONDA -eq 0 ]; then
cat >> shipyard &lt;&lt; 'EOF'
deactivate
EOF
    else
cat >> shipyard &lt;&lt; 'EOF'
source deactivate $VENV_NAME
EOF
    fi
fi

chmod 755 shipyard

echo ""
if [ -z $VENV_NAME ]; then
    echo '>> Please add $HOME/.local/bin to your $PATH. You can do this '
    echo '>> permanently in your shell rc script, e.g., .bashrc for bash shells.'
    echo ""
fi
echo ">> Install complete for $PYTHON. Please run Batch Shipyard as: $PWD/shipyard"
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="37"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

echo ""
echo "Applying migration AssociatedEnterpriseCheckYourAnswers"

echo "Adding routes to conf/app.routes"
echo "" >> ../conf/app.routes
echo "GET        /associatedEnterpriseCheckYourAnswers                       controllers.AssociatedEnterpriseCheckYourAnswersController.onPageLoad()" >> ../conf/app.routes

echo "Adding messages to conf.messages"
echo "" >> ../conf/messages.en
echo "associatedEnterpriseCheckYourAnswers.title = associatedEnterpriseCheckYourAnswers" >> ../conf/messages.en
echo "associatedEnterpriseCheckYourAnswers.heading = associatedEnterpriseCheckYourAnswers" >> ../conf/messages.en

echo "Migration AssociatedEnterpriseCheckYourAnswers completed"
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="38"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">rsync -avzP --update * ericmjl@rous:~/github/protein-convolutional-nets --exclude-from rsync_exclude.txt

rsync -avzP --update ericmjl@rous:~/github/protein-convolutional-nets/* ./ --exclude-from rsync_exclude.txt
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="39"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
# LICENSE UPL 1.0
#
# Copyright (c) 2019 Oracle and/or its affiliates. All rights reserved.
#
# Since: January, 2018
# Author: sanjay.singh@oracle.com, paramdeep.saini@oracle.com
# Description: Add a Grid node and add Oracle Database instance based on following parameters:
#              $PUBLIC_HOSTNAME
#              $PUBLIC_IP
#
# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS HEADER.
#

####################### Variables and Constants #################
declare -r FALSE=1
declare -r TRUE=0
declare -x GRID_USER='grid'          ## Default gris user is grid.
declare -x DB_USER='oracle'      ## default oracle user is oracle.
declare -r ETCHOSTS="/etc/hosts"     ## /etc/hosts file location.
declare -r RAC_ENV_FILE="/etc/rac_env_vars"   ## RACENV FILE NAME
declare -x GIMR_DB_FLAG='false'      ## GIMR DB Check by default is false
declare -x DOMAIN                    ## Domain name will be computed based on hostname -d, otherwise pass it as env variable.
declare -x PUBLIC_IP                 ## Computed based on Node name.
declare -x PUBLIC_HOSTNAME           ## PUBLIC HOSTNAME set based on hostname
declare -x EXISTING_CLS_NODE         ## Computed during the program execution.
declare -x EXISTING_CLS_NODES        ## You must all the exisitng nodes of the cluster in comma separated strings. Otherwise installation will fail.
declare -x DHCP_CONF='false'         ## Pass env variable where value set to true for DHCP based installation.
declare -x NODE_VIP                  ## Pass it as env variable.
declare -x VIP_HOSTNAME              ## Pass as env variable.
declare -x SCAN_NAME                 ## Pass it as env variable.
declare -x SCAN_IP                   ## Pass as env variable if you do not have DNS server. Otherwise, do not pass this variable.
declare -x SINGLENIC='false'         ## Default value is false as we should use 2 nics if possible for better performance.
declare -x PRIV_IP                   ## Pass PRIV_IP is not using SINGLE NIC
declare -x CONFIGURE_GNS='false'     ## Default value set to false. However, under DSC checks, it is reverted to true.
declare -x COMMON_SCRIPTS            ## COMMON SCRIPT Locations. Pass this env variable if you have custom responsefile for grid and other scripts for DB.
declare -x PRIV_HOSTNAME             ## if SINGLENIC=true then PRIV and PUB hostname will be same. Otherise pass it as env variable.
declare -x CMAN_HOSTNAME             ## If you want to use connection manager to proxy the DB connections
declare -x CMAN_IP                   ## CMAN_IP if you want to use connection manager to proxy the DB connections
declare -x OS_PASSWORD               ## if not passed as env variable, it will be set to PASSWORD
declare -x GRID_PASSWORD             ## if not passed as env variable , it will be set to OS_PASSWORD
declare -x ORACLE_PASSWORD           ## if not passed as env variable, it will be set to OS_PASSWORD
declare -x PASSWORD                  ## If not passed as env variable , it will be set as system generated password
declare -x CLUSTER_TYPE='STANDARD'   ## Default instllation is STANDARD. You can pass DOMAIn or MEMBERDB.
declare -x GRID_RESPONSE_FILE        ## IF you pass this env variable then user based responsefile will be used. default location is COMMON_SCRIPTS.
declare -x SCRIPT_ROOT               ## SCRIPT_ROOT will be set as per your COMMON_SCRIPTS.Do not Pass env variable SCRIPT_ROOT.
declare -r OSDBA='dba'
declare -r OSASM='asmadmin'
declare -r INSTALL_TYPE='CRS_ADDNODE'
declare -r IPMI_FLAG='false'
declare -r ASM_STORAGE_OPTION='ASM'
declare -r GIMR_ON_NAS='false'
declare -x SCAN_TYPE='LOCAL_SCAN'
declare -x SHARED_SCAN
declare -x DB_ASM_DISKGROUP='DATA'
declare -x CONFIGURE_AFD_FLAG='false'
declare -x CONFIGURE_RHPS_FLAG='false'
declare -x EXECUTE_ROOT_SCRIPT_FLAG='fasle'
declare -x EXECUTE_ROOT_SCRIPT_METHOD='ROOT'
declare -x IGNORE_CVU_CHECKS='true'           ## Ignore CVU Checks
declare -x SECRET_VOLUME='/run/secrets/'      ## Secret Volume
declare -x PWD_KEY='pwd.key'                  ## PWD Key File
declare -x ORACLE_PWD_FILE
declare -x GRID_PWD_FILE
declare -x REMOVE_OS_PWD_FILES='false'
declare -x COMMON_OS_PWD_FILE='common_os_pwdfile.enc'
declare -x CRS_CONFIG_NODES
declare -x ANSIBLE_INSTALL='false'
declare -x RUN_DBCA='true'

progname=$(basename "$0")
###################### Variabes and Constants declaration ends here  ####################


############Sourcing Env file##########
if [ -f "/etc/rac_env_vars" ]; then
source "/etc/rac_env_vars"
fi
##########Source ENV file ends here####


###################Capture Process id and source functions.sh###############
source "$SCRIPT_DIR/functions.sh"
###########################sourcing of functions.sh ends here##############

####error_exit function sends a TERM signal, which is caught by trap command and returns exit status 15"####
trap '{ exit 15; }' TERM
###########################trap code ends here##########################

all_check()
{
check_pub_host_name
check_cls_node_names
check_ip_env_vars
check_passwd_env_vars
check_rspfile_env_vars
check_db_env_vars
}

#####################Function related to public hostname, IP and domain name check begin here ########

check_pub_host_name()
{
local domain_name
local stat

if [ -z "${PUBLIC_IP}" ]; then
    PUBLIC_IP=$(dig +short "$(hostname)")
    print_message "Public IP is set to ${PUBLIC_IP}"
else
    print_message "Public IP is set to ${PUBLIC_IP}"
fi

if [ -z "${PUBLIC_HOSTNAME}" ]; then
  PUBLIC_HOSTNAME=$(hostname)
  print_message "RAC Node PUBLIC Hostname is set to ${PUBLIC_HOSTNAME}"
 else
  print_message "RAC Node PUBLIC Hostname is set to ${PUBLIC_HOSTNAME}"
fi

if [ -z "${DOMAIN}" ]; then
domain_name=$(hostname -d)
 if [ -z "${domain_name}" ];then
   print_message  "Domain name is not defined. Setting Domain to 'example.com'"
    DOMAIN="example.com"
 else
    DOMAIN=${domain_name}
fi
 else
 print_message "Domain is defined to $DOMAIN"
fi

}

############### Function related to public hostname, IP and domain checks ends here ##########

############## Function related to check exisitng cls nodes begin here #######################
check_cls_node_names()
{
if [ -z "${EXISTING_CLS_NODES}" ]; then
	error_exit "For Node Addition, please provide the existing clustered node name."
else
	
   if isStringExist ${EXISTING_CLS_NODES} ${PUBLIC_HOSTNAME}; then
	  error_exit "EXISTING_CLS_NODES ${EXISTING_CLS_NODES} contains new node name ${PUBLIC_HOSTNAME}"
   fi

print_message "Setting Existing Cluster Node for node addition operation. This will be retrieved from ${EXISTING_CLS_NODES}"

EXISTING_CLS_NODE="$( cut -d ',' -f 1 &lt;&lt;&lt; "$EXISTING_CLS_NODES" )"

if [ -z "${EXISTING_CLS_NODE}" ]; then
   error_exit " Existing Node Name of the cluster not set or set to empty string"
else
   print_message "Existing Node Name of the cluster is set to ${EXISTING_CLS_NODE}"

if resolveip ${EXISTING_CLS_NODE}; then
 print_message "Existing Cluster node resolved to IP. Check passed"
else
  error_exit "Existing Cluster node does not resolved to IP. Check Failed"
fi
fi
fi
}

############## Function related to check exisitng cls nodes begin here #######################

check_ip_env_vars ()
{
if [ "${DHCP_CONF}" != 'true' ]; then
  print_message "Default setting of AUTO GNS VIP set to false. If you want to use AUTO GNS VIP, please pass DHCP_CONF as an env parameter set to true"
  DHCP_CONF=false
if [ -z "${NODE_VIP}" ]; then
   error_exit "RAC Node ViP is not set or set to empty string"
else
   print_message "RAC VIP set to ${NODE_VIP}"
fi

if [ -z "${VIP_HOSTNAME}" ]; then
   error_exit "RAC Node Vip hostname is not set ot set to empty string"
else
   print_message "RAC Node VIP hostname is set to ${VIP_HOSTNAME} "
fi

if [ -z ${SCAN_NAME} ]; then
  print_message "SCAN_NAME set to the empty string"
else
  print_message "SCAN_NAME name is ${SCAN_NAME}"
fi

if resolveip ${SCAN_NAME}; then
 print_message "SCAN Name resolving to IP. Check Passed!"
else
  error_exit "SCAN Name not resolving to IP. Check Failed!"
fi

if [ -z ${SCAN_IP} ]; then
   print_message "SCAN_IP set to the empty string"
else
  print_message "SCAN_IP name is ${SCAN_IP}"
fi
fi

if [ "${SINGLENIC}" == 'true' ];then
PRIV_IP=${PUBLIC_IP}
PRIV_HOSTNAME=${PUBLIC_HOSTNAME}
fi

if [ -z "${PRIV_IP}" ]; then
   error_exit "RAC Node private ip is not set ot set to empty string"
else
  print_message "RAC Node PRIV IP is set to ${PRIV_IP} "
fi

if [ -z "${PRIV_HOSTNAME}" ]; then
   error_exit "RAC Node private hostname is not set ot set to empty string"
else
  print_message "RAC Node private hostname is set to ${PRIV_HOSTNAME}"
fi


if [ -z ${CMAN_HOSTNAME} ]; then
  print_message  "CMAN_NAME set to the empty string"
else
  print_message "CMAN_HOSTNAME name is ${CMAN_HOSTNAME}"
fi

if [ -z ${CMAN_IP} ]; then
   print_message "CMAN_IP set to the empty string"
else
  print_message "CMAN_IP name is ${CMAN_IP}"
fi

}
################check ip env vars function  ends here ############################

################ Check passwd env vars function  begin here ######################
check_passwd_env_vars()
{
if [ -f "${SECRET_VOLUME}/${COMMON_OS_PWD_FILE}" ]; then
cmd='openssl enc -d -aes-256-cbc -in "${SECRET_VOLUME}/${COMMON_OS_PWD_FILE}" -out /tmp/${COMMON_OS_PWD_FILE} -pass file:"${SECRET_VOLUME}/${PWD_KEY}"'

eval $cmd

if [ $? -eq 0 ]; then
print_message "Password file generated"
else
error_exit "Error occurred during common os password file generation"
fi

read PASSWORD &lt; /tmp/${COMMON_OS_PWD_FILE}
rm -f /tmp/${COMMON_OS_PWD_FILE}
else
 print_message "Password is empty string"
 PASSWORD=O$(openssl rand -base64 6 | tr -d "=+/")_1
fi

if [ ! -z "${GRID_PWD_FILE}" ]; then
cmd='openssl enc -d -aes-256-cbc -in "${SECRET_VOLUME}/${GRID_PWD_FILE}" -out "/tmp/${GRID_PWD_FILE}" -pass file:"${SECRET_VOLUME}/${PWD_KEY}"'

eval $cmd

if [ $? -eq 0 ]; then
print_message "Password file generated"
else
error_exit "Error occurred during Grid password file generation"
fi

read GRID_PASSWORD &lt; /tmp/${GRID_PWD_FILE}
rm -f /tmp/${GRID_PWD_FILE}
else
  GRID_PASSWORD="${PASSWORD}"
  print_message "Common OS Password string is set for Grid user"
fi

if [ ! -z "${ORACLE_PWD_FILE}" ]; then
cmd='openssl enc -d -aes-256-cbc -in "${SECRET_VOLUME}/${ORACLE_PWD_FILE}" -out "/tmp/${ORACLE_PWD_FILE}" -pass file:"${SECRET_VOLUME}/${PWD_KEY}"'

eval $cmd

if [ $? -eq 0 ]; then
print_message "Password file generated"
else
error_exit "Error occurred during Oracle  password file generation"
fi

read ORACLE_PASSWORD &lt; /tmp/${ORACLE_PWD_FILE}
rm -f /tmp/${GRID_PWD_FILE}
else
  ORACLE_PASSWORD="${PASSWORD}"
  print_message "Common OS Password string is set for  Oracle user"
fi

if [ "${REMOVE_OS_PWD_FILES}" == 'true' ]; then
rm -f  ${SECRET_VOLUME}/${COMMON_OS_PWD_FILE}
rm -f ${SECRET_VOLUME}/${PWD_KEY}
fi

}

############### Check password env vars function ends here ########################

############### Check grid Response file function begin here ######################
check_rspfile_env_vars ()
{
if [ -z "${GRID_RESPONSE_FILE}" ];then
print_message "GRID_RESPONSE_FILE env variable set to empty. $progname will use standard cluster responsefile"
else
if [ -f $COMMON_SCRIPTS/$GRID_RESPONSE_FILE ];then
cp $COMMON_SCRIPTS/$GRID_RESPONSE_FILE $logdir/$GRID_RESPONSE_FILE
else
error_exit "$COMMON_SCRIPTS/$GRID_RESPONSE_FILE does not exist"
fi
fi

if [ -z "${SCRIPT_ROOT}" ]; then
SCRIPT_ROOT=$COMMON_SCRIPTS
print_message "Location for User script SCRIPT_ROOT set to $COMMON_SCRIPTS"
else
print_message "Location for User script SCRIPT_ROOT set to $SCRIPT_ROOT"
fi

}

############ Check responsefile function end here ######################

########### Check db env vars function begin here #######################
check_db_env_vars ()
{
if [ $CLUSTER_TYPE == 'MEMBERDB' ]; then
print_message "Checking StorageOption for MEMBERDB Cluster"

if [ -z "${STORAGE_OPTIONS_FOR_MEMBERDB}" ]; then
print_message "Storage Options is set to STORAGE_OPTIONS_FOR_MEMBERDB"
else
print_message "Storage Options is set to STORAGE_OPTIONS_FOR_MEMBERDB"
fi

fi
if [ -z "${ORACLE_SID}" ]; then
   print_message "ORACLE_SID is not defined"
else
  print_message "ORACLE_SID is set to $ORACLE_SID"
fi

}

################# Check db env vars end here ##################################

################ All Check Functions end here #####################################


########################################### SSH Function begin here ########################
setupSSH()
{
local password
local ssh_pid
local stat

if [ -z $CRS_NODES ]; then
  CRS_NODES=$PUBLIC_HOSTNAME
fi


IFS=', ' read -r -a CLUSTER_NODES  &lt;&lt;&lt; "$EXISTING_CLS_NODES"
EXISTING_CLS_NODES+=",$CRS_NODES"
CLUSTER_NODES=$(echo $EXISTING_CLS_NODES | tr ',' ' ')

print_message "Cluster Nodes are $CLUSTER_NODES"
print_message "Running SSH setup for $GRID_USER user between nodes ${CLUSTER_NODES}"
cmd='su - $GRID_USER -c "$EXPECT $SCRIPT_DIR/$SETUPSSH $GRID_USER \"$GRID_HOME/oui/prov/resources/scripts\"  \"${CLUSTER_NODES}\"  \"$GRID_PASSWORD\""'
(eval $cmd) &amp;
ssh_pid=$!
wait $ssh_pid
stat=$?

if [ "${stat}" -ne 0 ]; then
error_exit "ssh setup for Grid user failed!, please make sure you have pass the corect password. You need to make sure that password must be same on all the clustered nodes or the nodes set in existing_cls_nodes env variable for $GRID_USER  user"
fi

print_message "Running SSH setup for $DB_USER user between nodes ${CLUSTER_NODES[@]}"
cmd='su - $DB_USER -c "$EXPECT $SCRIPT_DIR/$SETUPSSH $DB_USER \"$DB_HOME/oui/prov/resources/scripts\"  \"${CLUSTER_NODES}\"  \"$ORACLE_PASSWORD\""'
(eval $cmd) &amp;
ssh_pid=$!
wait $ssh_pid
stat=$?

if [ "${stat}" -ne 0 ]; then
error_exit "ssh setup for Oracle  user failed!, please make sure you have pass the corect password. You need to make sure that password must be same on all the clustered nodes or the nodes set in existing_cls_nodes env variable for $DB_USER user"
fi
}

checkSSH ()
{

local password
local ssh_pid
local stat
local status

IFS=', ' read -r -a CLUSTER_NODES  &lt;&lt;&lt; "$EXISTING_CLS_NODES"
EXISTING_CLS_NODES+=",$PUBLIC_HOSTNAME"
CLUSTER_NODES=$(echo $EXISTING_CLS_NODES | tr ',' ' ')

cmd='su - $GRID_USER -c "ssh -o BatchMode=yes -o ConnectTimeout=5 $GRID_USER@$node echo ok 2>&amp;1"'
echo $cmd

for node in ${CLUSTER_NODES}
do

status=$(eval $cmd)

if [[ $status == ok ]] ; then
  print_message "SSH check fine for the $node"
  
elif [[ $status == "Permission denied"* ]] ; then
   error_exit "SSH check failed for the $GRID_USER@$node beuase of permission denied error! SSH setup did not complete sucessfully" 
else
   error_exit "SSH check failed for the $GRID_USER@$node! Error occurred during SSH setup"
fi

done

status="NA"
cmd='su - $DB_USER -c "ssh -o BatchMode=yes -o ConnectTimeout=5 $DB_USER@$node echo ok 2>&amp;1"'
 echo $cmd
for node in ${CLUSTER_NODES}
do

status=$(eval $cmd)

if [[ $status == ok ]] ; then
  print_message "SSH check fine for the $DB_USER@$node"
elif [[ $status == "Permission denied"* ]] ; then
   error_exit "SSH check failed for the $DB_USER@$node becuase of permission denied error! SSH setup did not complete sucessfully"
else
   error_exit "SSH check failed for the $DB_USER@$node! Error occurred during SSH setup"
fi

done

}

######################################  SSH Function End here ####################################

######################Add Node Functions ####################################
runorainstroot()
{
$INVENTORY/orainstRoot.sh
}

runrootsh ()
{

local ORACLE_HOME=$1
local USER=$2

if [ -z $CRS_NODES ]; then
  CLUSTER_NODES=$PUBLIC_HOSTNAME
else
  IFS=', ' read -r -a CLUSTER_NODES &lt;&lt;&lt; "$CRS_NODES"
fi

print_message "Nodes in the cluster ${CLUSTER_NODES[@]}"
for node in "${CLUSTER_NODES[@]}"; do
cmd='su - $USER -c "ssh $node sudo $ORACLE_HOME/root.sh"'
eval $cmd
done

}

generate_response_file ()
{
cp $SCRIPT_DIR/$ADDNODE_RSP $logdir/$ADDNODE_RSP
chmod 666 $logdir/$ADDNODE_RSP


if [ -z "${GRID_RESPONSE_FILE}" ]; then

if [ -z ${CRS_CONFIG_NODES} ]; then
   CRS_CONFIG_NODES="$PUBLIC_HOSTNAME:$VIP_HOSTNAME:HUB"
   print_message "Clustered Nodes are set to $CRS_CONFIG_NODES"
fi

sed -i -e "s|###INVENTORY###|$INVENTORY|g" $logdir/$ADDNODE_RSP
sed -i -e "s|###GRID_BASE###|$GRID_BASE|g" $logdir/$ADDNODE_RSP
sed -i -r "s|###PUBLIC_HOSTNAME###|$PUBLIC_HOSTNAME|g"  $logdir/$ADDNODE_RSP
sed -i -r "s|###HOSTNAME_VIP###|$VIP_HOSTNAME|g"  $logdir/$ADDNODE_RSP
sed -i -e "s|###INSTALL_TYPE###|$INSTALL_TYPE|g" $logdir/$ADDNODE_RSP
sed -i -e "s|###OSDBA###|$OSDBA|g" $logdir/$ADDNODE_RSP
sed -i -e "s|###OSOPER###|$OSOPER|g" $logdir/$ADDNODE_RSP
sed -i -e "s|###OSASM###|$OSASM|g" $logdir/$ADDNODE_RSP
sed -i -e "s|###SCAN_TYPE###|$SCAN_TYPE|g" $logdir/$ADDNODE_RSP
sed -i -e "s|###SHARED_SCAN_FILE###|$SHARED_SCAN_FILE|g" $logdir/$ADDNODE_RSP
sed -i -e "s|###DB_ASM_DISKGROUP###|$DB_ASM_DISKGROUP|g" $logdir/$ADDNODE_RSP
sed -i -e "s|###CONFIGURE_AFD_FLAG###|$CONFIGURE_AFD_FLAG|g" $logdir/$ADDNODE_RSP
sed -i -e "s|###CONFIGURE_RHPS_FLAG###|$CONFIGURE_RHPS_FLAG|g" $logdir/$ADDNODE_RSP
sed -i -e "s|###EXECUTE_ROOT_SCRIPT_FLAG###|$EXECUTE_ROOT_SCRIPT_FLAG|g" $logdir/$ADDNODE_RSP
sed -i -e "s|###EXECUTE_ROOT_SCRIPT_METHOD###|$EXECUTE_ROOT_SCRIPT_METHOD|g" $logdir/$ADDNODE_RSP
sed -i -e "s|###CRS_CONFIG_NODES###|$CRS_CONFIG_NODES|g" $logdir/$ADDNODE_RSP
fi

}

###### Cluster Verification function #######
CheckRemoteCluster ()
{
local cmd;
local stat;
local node=$EXISTING_CLS_NODE
local oracle_home=$GRID_HOME
local ORACLE_HOME=$GRID_HOME

print_message "Checking Cluster"

cmd='su - $GRID_USER -c "ssh $node \"$ORACLE_HOME/bin/crsctl check crs\""'
eval $cmd

if [ $?  -eq 0 ];then
print_message "Cluster Check on remote node passed"
else
error_exit "Cluster Check on remote node failed"
fi

cmd='su - $GRID_USER -c "ssh $node \"$ORACLE_HOME/bin/crsctl check cluster\""'
eval $cmd

if [ $? -eq 0 ]; then
print_message "Cluster Check went fine"
else
error_exit "Cluster  Check failed!"
fi

if [ ${GIMR_DB_FLAG} == 'true' ]; then

   cmd='su - $GRID_USER -c "ssh $node \"$ORACLE_HOME/bin/srvctl status mgmtdb\""'
   eval $cmd

    if [ $? -eq 0 ]; then
        print_message "MGMTDB Check went fine"
    else
         error_exit "MGMTDB Check failed!"
    fi
fi

cmd='su - $GRID_USER -c "ssh $node \"$ORACLE_HOME/bin/crsctl check crsd\""'
eval $cmd

if [ $? -eq 0 ]; then
print_message "CRSD Check went fine"
else
error_exit "CRSD Check failed!"
fi


cmd='su - $GRID_USER -c "ssh $node \"$ORACLE_HOME/bin/crsctl check cssd\""'
eval $cmd

if [ $? -eq 0 ]; then
print_message "CSSD Check went fine"
else
error_exit "CSSD Check failed!"
fi

cmd='su - $GRID_USER -c "ssh $node \"$ORACLE_HOME/bin/crsctl check evmd\""'
eval $cmd

if [ $? -eq 0 ]; then
print_message "EVMD Check went fine"
else
error_exit "EVMD Check failed"
fi

}

setDevicePermissions ()
{

local cmd
local state=3

if [ -z $CRS_NODES ]; then
  CLUSTER_NODES=$PUBLIC_HOSTNAME
else
  IFS=', ' read -r -a CLUSTER_NODES &lt;&lt;&lt; "$CRS_NODES"
fi

print_message "Nodes in the cluster ${CLUSTER_NODES[@]}"
for node in "${CLUSTER_NODES[@]}"; do
print_message "Setting Device permissions for RAC Install  on $node"

if [ ! -z "${GIMR_DEVICE_LIST}" ];then

print_message "Preapring GIMR Device list"
IFS=', ' read -r -a devices &lt;&lt;&lt; "$GIMR_DEVICE_LIST"
        local arr_device=${#devices[@]}
if [ $arr_device -ne 0 ]; then
        for device in "${devices[@]}"
        do
        print_message "Changing Disk permission and ownership"
        cmd='su - $GRID_USER -c "ssh $node sudo chown $GRID_USER:asmadmin $device"'
        print_message "Command : $cmd execute on $node"
        eval $cmd
        unset cmd
        cmd='su - $GRID_USER -c "ssh $node sudo chmod 660 $device"'
        print_message "Command : $cmd execute on $node"
        eval $cmd
        unset cmd
        print_message "Populate Rac Env Vars on Remote Hosts"
        cmd='su - $GRID_USER -c "ssh $node sudo echo \"export GIMR_DEVICE_LIST=${GIMR_DEVICE_LIST}\" >> /etc/rac_env_vars"'
        print_message "Command : $cmd execute on $node"
        eval $cmd
        unset cmd
       done
fi

fi

if [ ! -z "${ASM_DEVICE_LIST}" ];then

print_message "Preapring ASM Device list"
IFS=', ' read -r -a devices &lt;&lt;&lt; "$ASM_DEVICE_LIST"
        local arr_device=${#devices[@]}
if [ $arr_device -ne 0 ]; then
        for device in "${devices[@]}"
        do
        print_message "Changing Disk permission and ownership"
        cmd='su - $GRID_USER -c "ssh $node sudo chown $GRID_USER:asmadmin $device"'
        print_message "Command : $cmd execute on $node"
        eval $cmd
        unset cmd
        cmd='su - $GRID_USER -c "ssh $node sudo chmod 660 $device"'
        print_message "Command : $cmd execute on $node"
        eval $cmd
        unset cmd
        print_message "Populate Rac Env Vars on Remote Hosts"
        cmd='su - $GRID_USER -c "ssh $node sudo echo \"export ASM_DEVICE_LIST=${ASM_DEVICE_LIST}\" >> /etc/rac_env_vars"'
        print_message "Command : $cmd execute on $node"
        eval $cmd
        unset cmd
       done
fi

fi

done

}

checkCluster ()
{
local cmd;
local stat;
local oracle_home=$GRID_HOME

print_message "Checking Cluster"

cmd='su - $GRID_USER -c "$GRID_HOME/bin/crsctl check crs"'
eval $cmd

if [ $?  -eq 0 ];then
print_message "Cluster Check passed"
else
error_exit "Cluster Check failed"
fi

cmd='su - $GRID_USER -c "$GRID_HOME/bin/crsctl check cluster"'
eval $cmd

if [ $? -eq 0 ]; then
print_message "Cluster Check went fine"
else
error_exit "Cluster  Check failed!"
fi

if [ ${GIMR_DB_FLAG} == 'true' ]; then
   cmd='su - $GRID_USER -c "$GRID_HOME/bin/srvctl status mgmtdb"'
    eval $cmd

   if [ $? -eq 0 ]; then
      print_message "MGMTDB Check went fine"
   else
      error_exit "MGMTDB Check failed!"
    fi
fi

cmd='su - $GRID_USER -c "$GRID_HOME/bin/crsctl check crsd"'
eval $cmd

if [ $? -eq 0 ]; then
print_message "CRSD Check went fine"
else
error_exit "CRSD Check failed!"
fi

cmd='su - $GRID_USER -c "$GRID_HOME/bin/crsctl check cssd"'
eval $cmd

if [ $? -eq 0 ]; then
print_message "CSSD Check went fine"
else
error_exit "CSSD Check failed!"
fi

cmd='su - $GRID_USER -c "$GRID_HOME/bin/crsctl check evmd"'
eval $cmd

if [ $? -eq 0 ]; then
print_message "EVMD Check went fine"
else
error_exit "EVMD Check failed"
fi

print_message "Removing $logdir/cluvfy_check.txt as cluster check has passed"
rm -f $logdir/cluvfy_check.txt

}

checkClusterClass ()
{
print_message "Checking Cluster Class"
local cluster_class

cmd='su - $GRID_USER -c "$GRID_HOME/bin/crsctl get cluster class"'
cluster_class=$(eval $cmd)
print_message "Cluster class is $cluster_class"
CLUSTER_TYPE=$(echo $cluster_class | awk -F \' '{ print $2 }' | awk '{ print $1 }')
}


###### Grid install &amp; Cluster Verification utility Function #######
cluvfyCheck()
{

local node=$EXISTING_CLS_NODE
local responsefile=$logdir/$ADDNODE_RSP
local hostname=$PUBLIC_HOSTNAME
local vip_hostname=$VIP_HOSTNAME
local cmd
local stat

if [ -z $CRS_NODES ]; then
  CLUSTER_NODES=$PUBLIC_HOSTNAME
else
  IFS=', ' read -r -a CLUSTER_NODES &lt;&lt;&lt; "$CRS_NODES"
fi

if [ -f "$logdir/cluvfy_check.txt" ]; then
print_message "Moving any exisiting cluvfy $logdir/cluvfy_check.txt to $logdir/cluvfy_check_$TIMESTAMP.txt"
mv $logdir/cluvfy_check.txt $logdir/cluvfy_check."$(date +%Y%m%d-%H%M%S)".txt
fi

#cmd='su - $GRID_USER -c "ssh $node  \"$GRID_HOME/runcluvfy.sh stage -pre nodeadd -n $hostname -vip $vip_hostname\" | tee -a $logdir/cluvfy_check.txt"'
#eval $cmd

print_message "Nodes in the cluster ${CLUSTER_NODES[@]}"
for cls_node in "${CLUSTER_NODES[@]}"; do
print_message "ssh to the node $node and executing cvu checks on $cls_node"
cmd='su - $GRID_USER -c "ssh $node  \"$GRID_HOME/runcluvfy.sh stage -pre nodeadd -n $cls_node\" | tee -a $logdir/cluvfy_check.txt"'
eval $cmd
done

print_message "Checking $logdir/cluvfy_check.txt if there is any failed check."
FAILED_CMDS=$(sed -n -f - $logdir/cluvfy_check.txt &lt;&lt; EOF
 /.*FAILED.*/ {
p
}
EOF
)

cat $logdir/cluvfy_check.txt > $STD_OUT_FILE

if [[ ${IGNORE_CVU_CHECKS} == 'true' ]]; then
print_message "CVU Checks are ignored as IGNORE_CVU_CHECKS set to true. It is recommended to set IGNORE_CVU_CHECKS to false and meet all the cvu checks requirement. RAC installation might fail, if there are failed cvu checks."
else
if [[ $FAILED_CMDS =~ .*FAILED*. ]]
then
print_message "cluvfy failed for following  \n $FAILED_CMDS"
error_exit "Pre Checks failed for Grid installation, please check $logdir/cluvfy_check.txt"
fi
fi
}

addGridNode ()
{

local node=$EXISTING_CLS_NODE
local responsefile=$logdir/$ADDNODE_RSP
local hostname=$PUBLIC_HOSTNAME
local vip_hostname=$VIP_HOSTNAME
local cmd
local stat

print_message "Copying $responsefile on remote node $node"
cmd='su - $GRID_USER -c "scp $responsefile $node:$logdir"'
eval $cmd

print_message "Running GridSetup.sh on $node to add the node to existing cluster"
cmd='su - $GRID_USER -c "ssh $node  \"$GRID_HOME/gridSetup.sh -silent -waitForCompletion -noCopy -skipPrereqs -responseFile $responsefile\" | tee -a $logfile"'
eval $cmd

print_message "Node Addition performed. removing Responsefile"
rm -f $responsefile
cmd='su - $GRID_USER -c "ssh $node \"rm -f $responsefile\""'
#eval $cmd

}

###########DB Node Addition Functions##############
addDBNode ()
{
local node=$EXISTING_CLS_NODE

if [ -z $CRS_NODES ]; then
   new_node_hostname=$PUBLIC_HOSTNAME
else
   new_node_hostname=$CRS_NODES
fi

local stat=3
local cmd

cmd='su - $DB_USER -c "ssh $node \"$DB_HOME/addnode/addnode.sh \"CLUSTER_NEW_NODES={$new_node_hostname}\" -skipPrereqs -waitForCompletion -ignoreSysPrereqs -noCopy  -silent\" | tee -a $logfile"'
eval $cmd

if [ $? -eq 0 ]; then
print_message "Node Addition went fine for $new_node_hostname"
else
error_exit "Node Addition failed for $new_node_hostname"
fi
}

addDBInst ()
{
# Check whether ORACLE_SID is passed on
local HOSTNAME=$PUBLIC_HOSTNAME
local node=$EXISTING_CLS_NODE
local stat=3
local cmd

if [ -z $CRS_NODES ]; then
  CLUSTER_NODES=$PUBLIC_HOSTNAME
else
  CLUSTER_NODES=$( echo $CRS_NODES | tr ',' ' ' )
fi

if [ -z "${ORACLE_SID}" ];then
 error_exit "ORACLE SID is not defined. Cannot Add Instance"
fi

if [ -z "${HOSTNAME}" ]; then
error_exit "Hostname is not defined"
fi


for new_node in "${CLUSTER_NODES[@]}"; do
print_message "Adding DB Instance on $node"
cmd='su - $DB_USER -c "ssh $node \"$DB_HOME/bin/dbca -addInstance -silent  -nodeName $new_node  -gdbName $ORACLE_SID\" | tee -a $logfile"'
eval $cmd
done

}

checkDBStatus ()
{
local status

if [ -f "/tmp/db_status.txt" ]; then
status=$(cat /tmp/db_status.txt)
else
status="NOT OPEN"
fi

rm -f /tmp/db_status.txt

# SQL Plus execution was successful and database is open
if [ "$status" = "OPEN" ]; then
   print_message "#################################################################"
   print_message " Oracle Database $ORACLE_SID is up and running on $(hostname)    "
   print_message "#################################################################"
# Database is not open
else
   error_exit "$ORACLE_SID is not up and running on $(hostname)"
fi

}


setremotelistener ()
{
local status
local cmd

if resolveip $CMAN_HOSTNAME; then
print_message "Executing script to set the remote listener"
su - $DB_USER -c "$SCRIPT_DIR/$REMOTE_LISTENER_FILE $ORACLE_SID $SCAN_NAME $CMAN_HOSTNAME.$DOMAIN"
fi

}

########################## DB Functions End here ##########################

###################################
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! #
############# MAIN ################
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! #
###################################


###### Etc Host and other Checks and setup before proceeding installation #####
all_check
print_message "Setting random password for root/$GRID_USER/$DB_USER user"
print_message "Setting random password for $GRID_USER user"
setpasswd $GRID_USER  $GRID_PASSWORD
print_message "Setting random password for $DB_USER user"
setpasswd $DB_USER $ORACLE_PASSWORD
print_message "Setting random password for root user"
setpasswd root $PASSWORD

####  Setting up SSH #######
setupSSH
checkSSH

#### Grid Node Addition #####
print_message "Setting Device permission to grid and asmadmin on all the cluster nodes"
setDevicePermissions
print_message "Checking Cluster Status on $EXISTING_CLS_NODE"
CheckRemoteCluster
print_message "Generating Responsefile for node addition"
generate_response_file
print_message "Running Cluster verification utility for new node $PUBLIC_HOSTNAME on $EXISTING_CLS_NODE"
cluvfyCheck
print_message "Running Node Addition and cluvfy test for node $PUBLIC_HOSTNAME"
addGridNode
print_message "Running root.sh on node $PUBLIC_HOSTNAME"
runrootsh $GRID_HOME  $GRID_USER
checkCluster
print_message "Checking Cluster Class"
checkClusterClass
print_message "Running User Script for $GRID_USER user"
su - $GRID_USER -c "$SCRIPT_DIR/$USER_SCRIPTS_FILE $GRID_SCRIPT_ROOT GRID"

###### DB Node Addition ######
if [ "${CLUSTER_TYPE}" != 'Domain' ]; then
if [ "${RUN_DBCA}" == 'true' ]; then
print_message  "Performing DB Node addition"
addDBNode
print_message "Running root.sh"
runrootsh $DB_HOME $DB_USER
print_message "Adding DB Instance"
addDBInst 
print_message "Checking DB status"
su - $DB_USER -c "$SCRIPT_DIR/$CHECK_DB_FILE $ORACLE_SID"
checkDBStatus
print_message "Running User Script for $DB_USER user"
su - $DB_USER -c "$SCRIPT_DIR/$USER_SCRIPTS_FILE $DB_SCRIPT_ROOT DB"
print_message "Setting Remote Listener"
setremotelistener
fi
fi
echo $TRUE
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="40"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/sh

if [ $# -ne 2 ]; then
    echo "invalid parameter"
    exit 1
fi

DATE=$1
GROUP_ID=$2

which gdate > /dev/null 2>&amp;1
if [ $? -eq 1 ]; then
    echo "not found gdate command"
    exit 1
fi

gdate +%Y%m%d --date "${DATE}"
if [ $? -eq 1 ]; then
    echo "invalid date: ${DATE}"
    exit 1
fi

API_URL="http://localhost:9000/programs?date=${DATE}&amp;groupId=${GROUP_ID}"

which jq > /dev/null 2>&amp;1
if [ $? -eq 1 ]; then
    echo "not found jq command"
    exit 1
fi

curl "${API_URL}" | jq -r ".[] | \"\(.start_time|split(\"T\")|.[1])ã€œ\(.end_time|split(\"T\")|.[1]) [\(.channel.name)] \(.name) / \(.title)\""

exit 0
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="41"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">python transformers/examples/language-modeling/run_language_modeling.py --model_name_or_path train-outputs/1024+0+512-STWS/model --tokenizer_name model-configs/1536-config --eval_data_file ../data/wikitext-103-raw/wiki.valid.raw --output_dir eval-outputs/1024+0+512-STWS/512+512+512-HPMI-256 --do_eval --per_device_eval_batch_size 1 --dataloader_drop_last --augmented --augmentation_function shuffle_within_sentences_high_pmi_first_third_sixth --eval_function last_sixth_eval</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="42"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/usr/bin/env bash
# Copyright 2015 Cloudera Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Exit on non-true return value
set -e
# Exit on reference to uninitialized variable
set -u

set -o pipefail

source $SOURCE_DIR/functions.sh
THIS_DIR="$( cd "$( dirname "$0" )" &amp;&amp; pwd )"
prepare $THIS_DIR

if needs_build_package ; then
  # Download the dependency from S3
  download_dependency $PACKAGE "${PACKAGE_STRING}.tar.gz" $THIS_DIR

  setup_package_build $PACKAGE $PACKAGE_VERSION

  # Snappy switched to CMake. Detect this and use CMake for newer releases.
  if [ -e CMakeLists.txt ]; then
    # Snappy's CMake builds either shared or static but not both. Build
    # each separately.
    mkdir -p build_shared
    pushd build_shared
    wrap cmake -DBUILD_SHARED_LIBS=ON -DCMAKE_BUILD_TYPE=RELEASE \
               -DCMAKE_INSTALL_PREFIX=$LOCAL_INSTALL ..
    wrap make -C . -j${BUILD_THREADS:-4} install
    popd

    mkdir -p build_static
    pushd build_static
    wrap cmake -DCMAKE_BUILD_TYPE=RELEASE -DCMAKE_INSTALL_PREFIX=$LOCAL_INSTALL ..
    wrap make -C . -j${BUILD_THREADS:-4} install
    popd
  else
    wrap ./configure --with-pic --prefix=$LOCAL_INSTALL
    wrap make -j${BUILD_THREADS:-4} install
  fi

  finalize_package_build $PACKAGE $PACKAGE_VERSION
fi
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="43"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/ash -xe

get_from_event() {
  jq -r "$1" "${GITHUB_EVENT_PATH}"
}

if jq --exit-status '.inputs.deployment_id' "$GITHUB_EVENT_PATH" >/dev/null; then
  MONOPOLIS_URL="https://github-api.monopolis.cloud/rollout/start/$(get_from_event '.repository.full_name')/$(get_from_event '.inputs.deployment_id')"
  echo ::set-output name=environment_map::$(curl --fail -X POST "${MONOPOLIS_URL}" -H "Authorization: Bearer ${GITHUB_TOKEN}")
else
  echo ::set-output name=environment_map::"{}"
fi

</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="44"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/usr/bin/env bash
#shellcheck disable=SC2128
#shellcheck source=/dev/null
set -x
source ../dapp-test-common.sh
source "../x2ethereum/publicTest.sh"

sendAddress="12qyocayNF7Lv6C9qW4avxs2E7U41fKSfv"
sendPriKey="0x4257d8692ef7fe13c68b65d6a52f03933db2fa5ce8faf210b5b8b80c721ced01"
MAIN_HTTP=""
chain33SenderAddr="14KEKbYtKKQm4wMthSK9J4La4nAiidGozt"
# validatorsAddr=["0x92c8b16afd6d423652559c6e266cbe1c29bfd84f", "0x0df9a824699bc5878232c9e612fe1a5346a5a368", "0xcb074cb21cdddf3ce9c3c0a7ac4497d633c9d9f1", "0xd9dab021e74ecf475788ed7b61356056b2095830"]
ethValidatorAddrKeyA="3fa21584ae2e4fd74db9b58e2386f5481607dfa4d7ba0617aaa7858e5025dc1e"
ethValidatorAddrKeyB="a5f3063552f4483cfc20ac4f40f45b798791379862219de9e915c64722c1d400"
ethValidatorAddrKeyC="bbf5e65539e9af0eb0cfac30bad475111054b09c11d668fc0731d54ea777471e"
ethValidatorAddrKeyD="c9fa31d7984edf81b8ef3b40c761f1847f6fcd5711ab2462da97dc458f1f896b"
#      chain33           10 bty    
chain33Validator1="1H4zzzQEQQR2FxXwppiMRXcvqLvqzxK2nv"
chain33Validator2="1Nq5AhTgVNvYaWQqih8ZQQEaRk3CFhTDHp"
chain33Validator3="16nmxjF58z5oKK9m44cGy241zMSJWPN1Ty"
chain33Validator4="182nAEMxF1JWWxEWdu4jvd68aZhQumS97H"
chain33ValidatorKey1="0x260124d9c619b0088241ffe2f1d7dc56b0b6100c88c342040387cd62b8ba35a3"
chain33ValidatorKey2="0x7812f8c688048943f1c168f8f2f76f44912de1f0ff8b12358b213118081869b2"
chain33ValidatorKey3="0xd44c8f3d8cac5d9c7fef7b0a0bf7be0909372ec6368064f742193de0bddeb2d1"
chain33ValidatorKey4="0xaad36689ca332026d4a4ceee62c8a91bac7bc100906b25a181a7f28b8552b53e"
ethReceiverAddr1="0xa4ea64a583f6e51c3799335b28a8f0529570a635"
ethReceiverAddrKey1="355b876d7cbcb930d5dfab767f66336ce327e082cbaa1877210c1bae89b1df71"
ethReceiverAddr2="0x0c05ba5c230fdaa503b53702af1962e08d0c60bf"
#ethReceiverAddrKey2="9dc6df3a8ab139a54d8a984f54958ae0661f880229bf3bdbb886b87d58b56a08"
maturityDegree=5
#portRelayer=19999
ethUrl=""

CLIA_HTTP=""
CLIB_HTTP=""
CLIC_HTTP=""
CLID_HTTP=""

# $1 sendAddress, $2 balance
function queryExecBalance() {
    local resp=""
    chain33_QueryExecBalance "${1}" "x2ethereum" "$MAIN_HTTP"
    # shellcheck disable=SC2155
    local balance=$(echo "$resp" | jq -r ".result" | jq ".[].balance")
    if [ "${balance}" != "${2}" ]; then
        echo_rst "queryExecBalance" "1" "${balance} != ${2}"
    fi
}

# $1 chain33Address, $2 balance
function queryChain33Balance() {
    local resp=""
    chain33_QueryBalance "${1}" "${MAIN_HTTP}"
    # shellcheck disable=SC2155
    local balance=$(echo $resp | jq -r ".result.execAccount" | jq ".[].account.balance")
    if [ "${balance}" != "${2}" ]; then
        echo_rst "queryChain33Balance" "1" "${balance} != ${2}"
    fi
}

# $1 req , $2 balance
function queryRelayerBalance() {
    chain33_Http "${1}" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "GetBalance" ".result.balance"
    if [ "${RETURN_RESP}" != "${2}" ]; then
        echo_rst "queryRelayerBalance" "1" "${RETURN_RESP} != ${2}"
        copyErrLogs
    fi
}

# $1 req , $2 balance
function queryChain33X2ethBalance() {
    chain33_Http "${req}" ${MAIN_HTTP} '(.error|not) and (.result != null)' "GetBalance" ".result"
    # shellcheck disable=SC2155
    local balance=$(echo "${RETURN_RESP}" | jq -r ".res" | jq ".[].balance" | sed 's/\"//g')
    if [ "${balance}" != "${2}" ]; then
        echo_rst "queryChain33X2ethBalance" "1" "${balance} != ${2}"
    fi
}

function start_ebrelayerA() {
    docker cp "./x2ethereum/relayer.toml" "${dockerNamePrefix}_ebrelayera_rpc_1":/root/relayer.toml
    start_docker_ebrelayer "${dockerNamePrefix}_ebrelayera_rpc_1" "/root/ebrelayer" "./x2ethereum/ebrelayera.log"
    sleep 5
}

function StartRelayerAndDeploy() {
    echo -e "${GRE}=========== $FUNCNAME begin ===========${NOC}"

    cp ../x2ethereum/* ./x2ethereum/
    for dockerName in ganachetest ebrelayera ebrelayerb ebrelayerc ebrelayerd; do
        line=$(delete_line_show "./x2ethereum/docker-compose-x2ethereum.yml" "${dockerName}:")
        sed -i ''"${line}"' a \ \ '${dockerName}'_rpc:' "./x2ethereum/docker-compose-x2ethereum.yml"
    done

    docker-compose -f ./x2ethereum/docker-compose-x2ethereum.yml up --build -d
    sleep 5

    # change EthProvider url
    dockerAddr=$(get_docker_addr "${dockerNamePrefix}_ganachetest_rpc_1")
    ethUrl="http://${dockerAddr}:8545"

    #    relayer.toml     
    updata_relayer_a_toml "${dockerAddr}" "${dockerNamePrefix}_ebrelayera_rpc_1" "./x2ethereum/relayer.toml"

    line=$(delete_line_show "./x2ethereum/relayer.toml" "localhost:9901")
    sed -i ''"${line}"' a JrpcBindAddr=":9901"' "./x2ethereum/relayer.toml"
    # start ebrelayer A
    start_ebrelayerA

    ebrelayeraRpcHost=$(get_docker_addr "${dockerNamePrefix}_ebrelayera_rpc_1")
    if [[ ${ebrelayeraRpcHost} == "" ]]; then
        echo -e "${RED}ebrelayeraRpcHost a is empty${NOC}"
    fi
    CLIA_HTTP="http://${ebrelayeraRpcHost}:9901"

    #     
    InitAndDeploy

    #    BridgeRegistry   
    local req='{"method":"Manager.ShowBridgeRegistryAddr","params":[{}]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "$FUNCNAME" ".result.addr"
    local BridgeRegistry="$RETURN_RESP"

    # kill ebrelayer A
    kill_docker_ebrelayer "${dockerNamePrefix}_ebrelayera_rpc_1"
    sleep 1

    #    relayer.toml     
    updata_relayer_toml "${BridgeRegistry}" ${maturityDegree} "./x2ethereum/relayer.toml"
    #   
    start_ebrelayerA

    # start ebrelayer B C D
    for name in b c d; do
        local file="./x2ethereum/relayer$name.toml"
        cp './x2ethereum/relayer.toml' "${file}"

        #              
        for deleteName in "deployerPrivateKey" "operatorAddr" "validatorsAddr" "initPowers" "deployerPrivateKey" "deploy"; do
            delete_line "${file}" "${deleteName}"
        done

        sed -i 's/x2ethereum/x2ethereum'${name}'/g' "${file}"

        pushHost=$(get_docker_addr "${dockerNamePrefix}_ebrelayer${name}_rpc_1")
        line=$(delete_line_show "${file}" "pushHost")
        sed -i ''"${line}"' a pushHost="http://'"${pushHost}"':20000"' "${file}"

        line=$(delete_line_show "${file}" "pushBind")
        sed -i ''"${line}"' a pushBind="'"${pushHost}"':20000"' "${file}"

        docker cp "${file}" "${dockerNamePrefix}_ebrelayer${name}_rpc_1":/root/relayer.toml
        start_docker_ebrelayer "${dockerNamePrefix}_ebrelayer${name}_rpc_1" "/root/ebrelayer" "./x2ethereum/ebrelayer${name}.log"
    done
    sleep 5

    ebrelayeraRpcHost=$(get_docker_addr "${dockerNamePrefix}_ebrelayera_rpc_1")
    CLIA_HTTP="http://${ebrelayeraRpcHost}:9901"
    ebrelayeraRpcHost=$(get_docker_addr "${dockerNamePrefix}_ebrelayerb_rpc_1")
    CLIB_HTTP="http://${ebrelayeraRpcHost}:9901"
    ebrelayeraRpcHost=$(get_docker_addr "${dockerNamePrefix}_ebrelayerc_rpc_1")
    CLIC_HTTP="http://${ebrelayeraRpcHost}:9901"
    ebrelayeraRpcHost=$(get_docker_addr "${dockerNamePrefix}_ebrelayerd_rpc_1")
    CLID_HTTP="http://${ebrelayeraRpcHost}:9901"

    docker ps -a

    echo -e "${GRE}=========== $FUNCNAME end ===========${NOC}"
}

function InitAndDeploy() {
    echo -e "${GRE}=========== $FUNCNAME begin ===========${NOC}"
    local req='{"method":"Manager.SetPassphase","params":[{"Passphase":"123456hzj"}]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "SetPassphase" ".result"

    local req='{"method":"Manager.Unlock","params":["123456hzj"]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "Unlock" ".result"

    local req='{"method":"Manager.DeployContrcts","params":[{}]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "$FUNCNAME" ".result"
    echo -e "${GRE}=========== $FUNCNAME end ===========${NOC}"
}

# chian33         
function InitChain33Vilators() {
    echo -e "${GRE}=========== $FUNCNAME begin ===========${NOC}"
    #    chain33Validators       
    chain33_ImportPrivkey "${chain33ValidatorKey1}" "${chain33Validator1}" "tokenAddr" "${MAIN_HTTP}"
    chain33_ImportPrivkey "${chain33ValidatorKey2}" "${chain33Validator2}" "tokenAddr" "${MAIN_HTTP}"
    chain33_ImportPrivkey "${chain33ValidatorKey3}" "${chain33Validator3}" "tokenAddr" "${MAIN_HTTP}"
    chain33_ImportPrivkey "${chain33ValidatorKey4}" "${chain33Validator4}" "tokenAddr" "${MAIN_HTTP}"

    # SetConsensusThreshold
    tx=$(curl -ksd '{"method":"Chain33.CreateTransaction","params":[{"execer":"x2ethereum","actionName":"SetConsensusThreshold","payload":{"consensusThreshold":"80"}}]}' ${MAIN_HTTP} | jq -r ".result")
    chain33_SignAndSendTxWait "$tx" "$sendPriKey" ${MAIN_HTTP} "SetConsensusThreshold"

    # add a validator
    tx=$(curl -ksd '{"method":"Chain33.CreateTransaction","params":[{"execer":"x2ethereum","actionName":"AddValidator","payload":{"address":"'${chain33Validator1}'","power":"25"}}]}' ${MAIN_HTTP} | jq -r ".result")
    chain33_SignAndSendTxWait "$tx" "$sendPriKey" ${MAIN_HTTP} "AddValidator"
    tx=$(curl -ksd '{"method":"Chain33.CreateTransaction","params":[{"execer":"x2ethereum","actionName":"AddValidator","payload":{"address":"'${chain33Validator2}'","power":"25"}}]}' ${MAIN_HTTP} | jq -r ".result")
    chain33_SignAndSendTxWait "$tx" "$sendPriKey" ${MAIN_HTTP} "AddValidator"
    tx=$(curl -ksd '{"method":"Chain33.CreateTransaction","params":[{"execer":"x2ethereum","actionName":"AddValidator","payload":{"address":"'${chain33Validator3}'","power":"25"}}]}' ${MAIN_HTTP} | jq -r ".result")
    chain33_SignAndSendTxWait "$tx" "$sendPriKey" ${MAIN_HTTP} "AddValidator"
    tx=$(curl -ksd '{"method":"Chain33.CreateTransaction","params":[{"execer":"x2ethereum","actionName":"AddValidator","payload":{"address":"'${chain33Validator4}'","power":"25"}}]}' ${MAIN_HTTP} | jq -r ".result")
    chain33_SignAndSendTxWait "$tx" "$sendPriKey" ${MAIN_HTTP} "AddValidator"

    # query Validators
    chain33_Http '{"method":"Chain33.Query","params":[{"execer":"x2ethereum","funcName":"GetTotalPower","payload":{}}]}' ${MAIN_HTTP} '(.error|not) and (.result != null)' "GetTotalPower" ".result.totalPower"
    if [ "${RETURN_RESP}" != "100" ]; then
        echo -e "${RED}=========== GetTotalPower err: TotalPower = $RETURN_RESP ===========${NOC}"
    fi

    # cions     x2ethereum     
    x2eth_addr=$(curl -ksd '{"method":"Chain33.ConvertExectoAddr","params":[{"execname":"x2ethereum"}]}' ${MAIN_HTTP} | jq -r ".result")
    chain33_SendToAddress "${sendAddress}" "${x2eth_addr}" 20000000000 "${MAIN_HTTP}"
    queryExecBalance "${sendAddress}" "20000000000"

    # chain33Validator      
    chain33_applyCoins "${chain33Validator1}" 1000000000 "${MAIN_HTTP}"
    queryChain33Balance "${chain33Validator1}" "1000000000"
    chain33_applyCoins "${chain33Validator2}" 1000000000 "${MAIN_HTTP}"
    queryChain33Balance "${chain33Validator2}" "1000000000"
    chain33_applyCoins "${chain33Validator3}" 1000000000 "${MAIN_HTTP}"
    queryChain33Balance "${chain33Validator3}" "1000000000"
    chain33_applyCoins "${chain33Validator4}" 1000000000 "${MAIN_HTTP}"
    queryChain33Balance "${chain33Validator4}" "1000000000"
    echo -e "${GRE}=========== $FUNCNAME end ===========${NOC}"
}

function EthImportKey() {
    echo -e "${GRE}=========== $FUNCNAME begin ===========${NOC}"

    #   
    local req='{"method":"Manager.SetPassphase","params":[{"Passphase":"123456hzj"}]}'
    chain33_Http "$req" "${CLIB_HTTP}" '(.error|not) and (.result != null)' "SetPassphase" ".result"
    chain33_Http "$req" "${CLIC_HTTP}" '(.error|not) and (.result != null)' "SetPassphase" ".result"
    chain33_Http "$req" "${CLID_HTTP}" '(.error|not) and (.result != null)' "SetPassphase" ".result"
    req='{"method":"Manager.Unlock","params":["123456hzj"]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "Unlock" ".result"
    chain33_Http "$req" "${CLIB_HTTP}" '(.error|not) and (.result != null)' "Unlock" ".result"
    chain33_Http "$req" "${CLIC_HTTP}" '(.error|not) and (.result != null)' "Unlock" ".result"
    chain33_Http "$req" "${CLID_HTTP}" '(.error|not) and (.result != null)' "Unlock" ".result"

    req='{"method":"Manager.ImportChain33PrivateKey4EthRelayer","params":["'${chain33ValidatorKey1}'"]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "ImportChain33PrivateKey4EthRelayer" ".result"
    req='{"method":"Manager.ImportChain33PrivateKey4EthRelayer","params":["'${chain33ValidatorKey2}'"]}'
    chain33_Http "$req" "${CLIB_HTTP}" '(.error|not) and (.result != null)' "ImportChain33PrivateKey4EthRelayer" ".result"
    req='{"method":"Manager.ImportChain33PrivateKey4EthRelayer","params":["'${chain33ValidatorKey3}'"]}'
    chain33_Http "$req" "${CLIC_HTTP}" '(.error|not) and (.result != null)' "ImportChain33PrivateKey4EthRelayer" ".result"
    req='{"method":"Manager.ImportChain33PrivateKey4EthRelayer","params":["'${chain33ValidatorKey4}'"]}'
    chain33_Http "$req" "${CLID_HTTP}" '(.error|not) and (.result != null)' "ImportChain33PrivateKey4EthRelayer" ".result"

    req='{"method":"Manager.ImportChain33RelayerPrivateKey","params":[{"privateKey":"'${ethValidatorAddrKeyA}'"}]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "ImportChain33RelayerPrivateKey" ".result"
    req='{"method":"Manager.ImportChain33RelayerPrivateKey","params":[{"privateKey":"'${ethValidatorAddrKeyB}'"}]}'
    chain33_Http "$req" "${CLIB_HTTP}" '(.error|not) and (.result != null)' "ImportChain33RelayerPrivateKey" ".result"
    req='{"method":"Manager.ImportChain33RelayerPrivateKey","params":[{"privateKey":"'${ethValidatorAddrKeyC}'"}]}'
    chain33_Http "$req" "${CLIC_HTTP}" '(.error|not) and (.result != null)' "ImportChain33RelayerPrivateKey" ".result"
    req='{"method":"Manager.ImportChain33RelayerPrivateKey","params":[{"privateKey":"'${ethValidatorAddrKeyD}'"}]}'
    chain33_Http "$req" "${CLID_HTTP}" '(.error|not) and (.result != null)' "ImportChain33RelayerPrivateKey" ".result"

    echo -e "${GRE}=========== $FUNCNAME end ===========${NOC}"
}

function TestChain33ToEthAssets() {
    echo -e "${GRE}=========== $FUNCNAME begin ===========${NOC}"
    # token4chain33           bty
    local req='{"method":"Manager.CreateBridgeToken","params":["coins.bty"]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "CreateBridgeToken" ".result.addr"
    tokenAddrBty=${RETURN_RESP}

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${ethReceiverAddr1}'","tokenAddr":"'${tokenAddrBty}'"}]}'
    queryRelayerBalance "$req" "0"

    # chain33 lock bty
    #shellcheck disable=SC2086
    tx=$(curl -ksd '{"method":"Chain33.CreateTransaction","params":[{"execer":"x2ethereum","actionName":"Chain33ToEthLock","payload":{"TokenContract":"'${tokenAddrBty}'","Chain33Sender":"'${sendPriKey}'","EthereumReceiver":"'${ethReceiverAddr1}'","Amount":"500000000","IssuerDotSymbol":"coins.bty","Decimals":"8"}}]}' ${MAIN_HTTP} | jq -r ".result")
    chain33_SignAndSendTxWait "$tx" "$sendPriKey" ${MAIN_HTTP} "Chain33ToEthLock"

    queryExecBalance "${sendAddress}" "19500000000"

    eth_block_wait $((maturityDegree + 2)) "${ethUrl}"

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${ethReceiverAddr1}'","tokenAddr":"'${tokenAddrBty}'"}]}'
    queryRelayerBalance "$req" "5"

    # eth burn
    req='{"method":"Manager.Burn","params":[{"ownerKey":"'${ethReceiverAddrKey1}'","tokenAddr":"'${tokenAddrBty}'","chain33Receiver":"'${chain33SenderAddr}'","amount":"500000000"}]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "Burn" ".result"

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${ethReceiverAddr1}'","tokenAddr":"'${tokenAddrBty}'"}]}'
    queryRelayerBalance "$req" "0"

    # eth    10    
    eth_block_wait $((maturityDegree + 2)) "${ethUrl}"

    queryExecBalance "${chain33SenderAddr}" "500000000"

    echo -e "${GRE}=========== $FUNCNAME end ===========${NOC}"
}

# eth to chain33
#          ,    chain33    ,   eth   
function TestETH2Chain33Assets() {
    echo -e "${GRE}=========== $FUNCNAME begin ===========${NOC}"
    local req='{"method":"Manager.ShowBridgeBankAddr","params":[{}]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "ShowBridgeBankAddr" ".result.addr"
    bridgeBankAddr="${RETURN_RESP}"

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${bridgeBankAddr}'","tokenAddr":""}]}'
    queryRelayerBalance "$req" "0"

    # eth lock 0.1
    req='{"method":"Manager.LockEthErc20Asset","params":[{"ownerKey":"'${ethReceiverAddrKey1}'","tokenAddr":"","amount":"100000000000000000","chain33Receiver":"'${sendAddress}'"}]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "LockEthErc20Asset" ".result"

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${bridgeBankAddr}'","tokenAddr":""}]}'
    queryRelayerBalance "$req" "0.1"

    # eth    10    
    eth_block_wait $((maturityDegree + 2)) "${ethUrl}"

    req='{"method":"Chain33.Query","params":[{"execer":"x2ethereum","funcName":"GetRelayerBalance","payload":{"tokenSymbol":"eth","address":"'${sendAddress}'","tokenAddr":"0x0000000000000000000000000000000000000000"}}]}'
    queryChain33X2ethBalance "${req}" "0.1"

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${ethReceiverAddr2}'","tokenAddr":""}]}'
    chain33_Http "${req}" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "GetBalance" ".result.balance"
    local balance=${RETURN_RESP}

    #    burn 0.1
    tx=$(curl -ksd '{"method":"Chain33.CreateTransaction","params":[{"execer":"x2ethereum","actionName":"Chain33ToEthBurn","payload":{"TokenContract":"0x0000000000000000000000000000000000000000","Chain33Sender":"'${sendPriKey}'","EthereumReceiver":"'${ethReceiverAddr2}'","Amount":"10000000","IssuerDotSymbol":"eth","Decimals":"18"}}]}' ${MAIN_HTTP} | jq -r ".result")
    chain33_SignAndSendTxWait "$tx" "$sendPriKey" ${MAIN_HTTP} "Chain33ToEthBurn"

    req='{"method":"Chain33.Query","params":[{"execer":"x2ethereum","funcName":"GetRelayerBalance","payload":{"tokenSymbol":"eth","address":"'${sendAddress}'","tokenAddr":"0x0000000000000000000000000000000000000000"}}]}'
    queryChain33X2ethBalance "${req}" "0"

    eth_block_wait $((maturityDegree + 2)) "${ethUrl}"

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${bridgeBankAddr}'","tokenAddr":""}]}'
    queryRelayerBalance "$req" "0"

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${ethReceiverAddr2}'","tokenAddr":""}]}'
    #queryRelayerBalance "$req" "$(echo "${balance}+0.1" | bc)"
    queryRelayerBalance "$req" "100.1"

    echo -e "${GRE}=========== $FUNCNAME end ===========${NOC}"
}

function TestETH2Chain33Erc20() {
    echo -e "${GRE}=========== $FUNCNAME begin ===========${NOC}"
    # token4erc20   chain33     token,   mint
    local req='{"method":"Manager.CreateERC20Token","params":["testc"]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "CreateERC20Token" ".result.addr"
    tokenAddr="${RETURN_RESP}"

    #     1000
    req='{"method":"Manager.MintErc20","params":[{"owner":"'${ethReceiverAddr1}'","tokenAddr":"'${tokenAddr}'","amount":"100000000000"}]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "MintErc20" ".result.addr"

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${ethReceiverAddr1}'","tokenAddr":"'${tokenAddr}'"}]}'
    queryRelayerBalance "$req" "1000"

    local req='{"method":"Manager.ShowBridgeBankAddr","params":[{}]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "ShowBridgeBankAddr" ".result.addr"
    bridgeBankAddr="${RETURN_RESP}"

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${bridgeBankAddr}'","tokenAddr":"'${tokenAddr}'"}]}'
    queryRelayerBalance "$req" "0"

    # lock 100
    req='{"method":"Manager.LockEthErc20Asset","params":[{"ownerKey":"'${ethReceiverAddrKey1}'","tokenAddr":"'${tokenAddr}'","amount":"10000000000","chain33Receiver":"'${chain33Validator1}'"}]}'
    chain33_Http "$req" "${CLIA_HTTP}" '(.error|not) and (.result != null)' "LockEthErc20Asset" ".result"

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${ethReceiverAddr1}'","tokenAddr":"'${tokenAddr}'"}]}'
    queryRelayerBalance "$req" "900"

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${bridgeBankAddr}'","tokenAddr":"'${tokenAddr}'"}]}'
    queryRelayerBalance "$req" "100"

    # eth    10    
    eth_block_wait $((maturityDegree + 2)) "${ethUrl}"

    req='{"method":"Chain33.Query","params":[{"execer":"x2ethereum","funcName":"GetRelayerBalance","payload":{"tokenSymbol":"testc","address":"'${chain33Validator1}'","tokenAddr":"'${tokenAddr}'"}}]}'
    queryChain33X2ethBalance "${req}" "100"

    # chain33 burn 100
    #shellcheck disable=SC2086
    tx=$(curl -ksd '{"method":"Chain33.CreateTransaction","params":[{"execer":"x2ethereum","actionName":"Chain33ToEthBurn","payload":{"TokenContract":"'${tokenAddr}'","Chain33Sender":"'${chain33ValidatorKey1}'","EthereumReceiver":"'${ethReceiverAddr2}'","Amount":"10000000000","IssuerDotSymbol":"testc","Decimals":"8"}}]}' ${MAIN_HTTP} | jq -r ".result")
    chain33_SignAndSendTxWait "$tx" "$chain33ValidatorKey1" ${MAIN_HTTP} "Chain33ToEthBurn"

    req='{"method":"Chain33.Query","params":[{"execer":"x2ethereum","funcName":"GetRelayerBalance","payload":{"tokenSymbol":"testc","address":"'${chain33Validator1}'","tokenAddr":"'${tokenAddr}'"}}]}'
    queryChain33X2ethBalance "${req}" "0"

    eth_block_wait $((maturityDegree + 2)) "${ethUrl}"

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${ethReceiverAddr2}'","tokenAddr":"'${tokenAddr}'"}]}'
    queryRelayerBalance "$req" "100"

    req='{"method":"Manager.GetBalance","params":[{"owner":"'${bridgeBankAddr}'","tokenAddr":"'${tokenAddr}'"}]}'
    queryRelayerBalance "$req" "0"

    echo -e "${GRE}=========== $FUNCNAME end ===========${NOC}"
}

function rpc_test() {
    set +e
    set -x
    chain33_RpcTestBegin x2ethereum
    MAIN_HTTP="$1"
    dockerNamePrefix="$2"
    echo "main_ip=$MAIN_HTTP"

    ispara=$(echo '"'"${MAIN_HTTP}"'"' | jq '.|contains("8901")')
    if [ "$ispara" == false ]; then
        # init
        StartRelayerAndDeploy
        InitChain33Vilators
        EthImportKey

        # test
        TestChain33ToEthAssets
        TestETH2Chain33Assets
        TestETH2Chain33Erc20

        copyErrLogs

        docker-compose -f ./x2ethereum/docker-compose-x2ethereum.yml down
    fi
    chain33_RpcTestRst x2ethereum "$CASE_ERR"
}

chain33_debug_function rpc_test "$1" "$2"
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="45"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">set -e

SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &amp;&amp; pwd )"

source $SCRIPT_DIR/set-kubeconfig.sh

open http://127.0.0.1:4040

kubectl port-forward -n kube-system "$(kubectl get -n kube-system pod --selector=weave-scope-component=app -o jsonpath='{.items..metadata.name}')" 4040
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="46"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">setup_emsdk()
{
    if [ ! -d "./tools/emsdk" ]
    then
        if [ ! -d "./tools" ]
        then
            mkdir ./tools
        fi

        cd ./tools        
        git clone https://github.com/emscripten-core/emsdk.git
        
        cd ./emsdk
        git pull

        ./emsdk install latest
        ./emsdk activate latest
        
        cd ../..

    fi
}
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="47"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks=16
#SBATCH --mem=8gb

module load fastqc
module load trimgalore
module load samtools
module load bowtie2
module load bedtools

### Fastqc for untrimmed files
cd /gpfs/group/pipkin/hdiao/T_Cell_ChIP/0_fastq
fastq_untrimmed_1=SRR1731132_1.fastq
fastqc $fastq_untrimmed_1

### Trim Galore
trim_galore --length 24 --stringency 3 $fastq_untrimmed_1
trim_fastq_end1=/gpfs/group/pipkin/hdiao/T_Cell_ChIP/0_fastq/SRR1731132_1_trimmed.fq

### Fastqc for trimmed files
fastqc $trim_fastq_end1

### Bowtie2 alignment
cd /gpfs/group/pipkin/hdiao/T_Cell_ChIP/1_bowtie2
bowtie2_index=/gpfs/group/pipkin/hdiao/ref_resources/mm/release102/GRCm38
sam_name=SRR1731132.sam
bowtie2 -p 16 -x $bowtie2_index -U $trim_fastq_end1 -S $sam_name

### Convert/sort/filter
bam_name=SRR1731132.bam
bam_name_srt=SRR1731132_srt.sam
sam_name_srt_dupr=SRR1731132_srt_dupr.sam
bam_name_srt_dupr=SRR1731132_srt_dupr.bam
flb_bam_name=SRR1731132_srt_dupr_flb.bam
blacklist_bed=/gpfs/group/pipkin/hdiao/ref_resources/mm/mm10_blacklisted_2016_nochr.bed

samtools view -bS $sam_name > $bam_name
samtools sort $bam_name -o $bam_name_srt
samtools rmdup -S $bam_name_srt $sam_name_srt_dupr
samtools view -bS $sam_name_srt_dupr > $bam_name_srt_dupr
bedtools intersect -abam $bam_name_srt_dupr -b $blacklist_bed -v > $flb_bam_name

### Remove intermediate files
filesize=$(stat -c%s $flb_bam_name)
if (( filesize > 10000 )) 
then
    rm $sam_name
    rm $bam_name
    rm $bam_name_srt
    rm $sam_name_srt_dupr
    rm $bam_name_srt_dupr
    rm $trim_fastq_end1
fi
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="48"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash -x

# Exit on error
set -e

# Test code goes here
rm -rf valid_lowisbet valid_lowisbet_?.log
mkdir -p valid_lowisbet

extra_opts="--no-shuffle --seed 1111 --maxi-batch 1 --maxi-batch-sort none"
extra_opts="$extra_opts --dim-emb 64 --dim-rnn 128 --mini-batch 32"
extra_opts="$extra_opts --cost-type ce-mean --disp-label-counts false --clip-norm 0"


# Files for the validation sets are swapped intentionally
$MRT_MARIAN/marian $extra_opts \
    -m valid_lowisbet/model.npz -t $MRT_DATA/train.max50.{en,de} -v vocab.en.yml vocab.de.yml \
    --disp-freq 10 --valid-freq 30 --after-batches 160 --early-stopping 2 \
    --valid-metrics cross-entropy --valid-sets $MRT_DATA/europarl.de-en/toy.bpe.{de,en} --valid-mini-batch 64 \
    --valid-log valid_lowisbet_1.log

test -e valid_lowisbet/model.npz
test -e valid_lowisbet/model.npz.yml
test -e valid_lowisbet_1.log

cp valid_lowisbet/model.npz.progress.yml valid_lowisbet/model.npz.progress.yml.bac
cat valid_lowisbet_1.log | $MRT_TOOLS/strip-timestamps.sh | grep "cross-entropy" > valid_lowisbet.out

# Files for the validation sets are swapped intentionally
$MRT_MARIAN/marian $extra_opts \
    -m valid_lowisbet/model.npz -t $MRT_DATA/train.max50.{en,de} -v vocab.en.yml vocab.de.yml \
    --disp-freq 10 --valid-freq 30 --after-batches 320 --early-stopping 4 \
    --valid-metrics cross-entropy --valid-sets $MRT_DATA/europarl.de-en/toy.bpe.{de,en} --valid-mini-batch 64 \
    --valid-log valid_lowisbet_2.log

test -e valid_lowisbet/model.npz
test -e valid_lowisbet_2.log

cat valid_lowisbet_2.log | $MRT_TOOLS/strip-timestamps.sh | grep "cross-entropy" >> valid_lowisbet.out
$MRT_TOOLS/diff-nums.py -p 0.1 valid_lowisbet.out valid_lowisbet.expected -o valid_lowisbet.diff

# Exit with success code
exit 0
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="49"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/usr/bin/env bash

# --------------------------------------------------------------------------
#  Copyright 2013-2016 Sam Deane, Elegant Chaos. All rights reserved.
#  This source code is distributed under the terms of Elegant Chaos's
#  liberal license: http://www.elegantchaos.com/license/liberal
# --------------------------------------------------------------------------

## This script is an attempt to automate picking up the latest version of the "develop" branch for a module, given that it might be on a detatch HEAD at the time.
##
## It performs the following steps:
##
## - rebase on the local develop branch
## - save this to a temporary branch
## - switch to the local develop branch
## - merge in the temporary branch - this should be a fast forward
## - remove the temporary branch
## - rebase on the remote "develop" from origin
## - push the resulting changed branch back to origin

check() {
    if [[ $1 != 0 ]]; then
      echo "failed: $2"
      exit $1
    fi
}

status=`git status --porcelain`

if [[ "$status" != "" ]]; then
    echo "You have local changes. Commit them first."
    exit 1
fi

# we may start on something that isn't the develop branch
# possibly a detached HEAD

# try to apply any changes on top of our local develop

git rebase develop
check $? "rebasing on develop"

# now fast forward develop to the merged place

git checkout -b develop-temp
check $? "making temp branch"

git checkout develop
check $? "switching back to develop"

git merge develop-temp
check $? "merging local changes"

git branch -d develop-temp
check $? "removing temp branch"

# we should now be on a local develop branch incorporating any local changes
echo fetching latest revisions
git fetch

# try to rebase again on top of any remote changes

git rebase
check $? "rebasing on origin/develop"

# if that worked, push back the merged version

git push
check $? "pushing"


</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="50"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
CUDA_VISIBLE_DEVICES="0" python runner.py 

</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="51"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
rm -rf dist || exit 0;
mkdir dist;
npm run build
cp ./ops/CNAME ./dist
( cd dist
 git init
 git add .
 git commit -m "Deployed to Github Pages"
 git push --force git@github.com:anvaka/npmgraph.an.git master:gh-pages
)
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="52"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">find . -name '*.go' -exec sed -i 's?k8s.io/kubernetes/plugin/pkg/scheduler?github.com/KubeDevice/kube-scheduler/pkg?g' {} +
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="53"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/usr/bin/env bash

#
#	MetaCall Configuration Environment Bash Script by Parra Studios
#	Configure and install MetaCall environment script utility.
#
#	Copyright (C) 2016 - 2021 Vicente Eduardo Ferrer Garcia &lt;vic798@gmail.com>
#
#	Licensed under the Apache License, Version 2.0 (the "License");
#	you may not use this file except in compliance with the License.
#	You may obtain a copy of the License at
#
#		http://www.apache.org/licenses/LICENSE-2.0
#
#	Unless required by applicable law or agreed to in writing, software
#	distributed under the License is distributed on an "AS IS" BASIS,
#	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#	See the License for the specific language governing permissions and
#	limitations under the License.
#

ROOT_DIR=$(pwd)

RUN_AS_ROOT=0
SUDO_CMD=sudo
INSTALL_APT=1
INSTALL_PYTHON=0
INSTALL_RUBY=0
INSTALL_NETCORE=0
INSTALL_NETCORE2=0
INSTALL_NETCORE5=0
INSTALL_V8=0
INSTALL_NODEJS=0
INSTALL_TYPESCRIPT=0
INSTALL_FILE=0
INSTALL_RPC=0
INSTALL_PORTS=0
INSTALL_CLEAN=0
SHOW_HELP=0
PROGNAME=$(basename $0)

# Install and mark packages to avoid autoremove
sub_apt_install_hold(){
	$SUDO_CMD apt-get -y install --no-install-recommends $@
	$SUDO_CMD apt-mark hold $@
}

# Base packages
sub_apt(){
	echo "configure apt"
	cd $ROOT_DIR
	$SUDO_CMD apt-get update &amp;&amp; apt-get -y install --no-install-recommends wget gpg apt-transport-https
}

# Python
sub_python(){
	echo "configure python"
	cd $ROOT_DIR
	sub_apt_install_hold python3 libpython3.7
}

# Ruby
sub_ruby(){
	echo "configure ruby"
	cd $ROOT_DIR

	# TODO: Remove this when using ruby2.5 (not available yet because it fails on loading a script with a malloc error)
	$SUDO_CMD mv /etc/apt/sources.list /etc/apt/sources.list.backup
	$SUDO_CMD sh -c "echo \"deb http://ftp.debian.org/debian/ stretch main\" > /etc/apt/sources.list"
	$SUDO_CMD sh -c "echo \"deb-src http://ftp.debian.org/debian/ stretch main\" >> /etc/apt/sources.list"
	$SUDO_CMD sh -c "echo \"deb http://security.debian.org/debian-security stretch/updates main\" >> /etc/apt/sources.list"
	$SUDO_CMD sh -c "echo \"deb-src http://security.debian.org/debian-security stretch/updates main\" >> /etc/apt/sources.list"
	$SUDO_CMD sh -c "echo \"deb http://ftp.debian.org/debian/ stretch-updates main\" >> /etc/apt/sources.list"
	$SUDO_CMD sh -c "echo \"deb-src http://ftp.debian.org/debian/ stretch-updates main\" >> /etc/apt/sources.list"

	$SUDO_CMD apt-get update
	# sub_apt_install_hold ruby2.5 libruby2.5
	$SUDO_CMD apt-get -y install --no-install-recommends --allow-remove-essential --allow-downgrades libssl1.1 libffi6 zlib1g libyaml-0-2 libgmp10=2:6.1.2+dfsg-1 libreadline7 libxml2 libncurses5 libtinfo5 ruby2.3 libruby2.3
	$SUDO_CMD apt-mark hold libssl1.1 libffi6 zlib1g libyaml-0-2 libgmp10 libreadline7 libxml2 libncurses5 libtinfo5 ruby2.3 libruby2.3

	# TODO: Remove this when using ruby2.5 (not available yet because it fails on loading a script with a malloc error)
	$SUDO_CMD mv /etc/apt/sources.list.backup /etc/apt/sources.list
}

# NetCore
sub_netcore(){
	echo "configure netcore"
	cd $ROOT_DIR

	# Debian Stretch

	sub_apt_install_hold libc6 libcurl3 libgcc1 libgssapi-krb5-2 libicu57 \
		liblttng-ust0 libssl1.0.2 libstdc++6 libunwind8 libuuid1 zlib1g ca-certificates

	# Install .NET Core Runtime 1.x
	DOTNET_VERSION=1.1.10
	DOTNET_DOWNLOAD_URL=https://dotnetcli.blob.core.windows.net/dotnet/Runtime/$DOTNET_VERSION/dotnet-debian.9-x64.$DOTNET_VERSION.tar.gz

	wget $DOTNET_DOWNLOAD_URL -O dotnet.tar.gz
	mkdir -p /usr/share/dotnet
	tar -zxf dotnet.tar.gz -C /usr/share/dotnet
	rm dotnet.tar.gz
	ln -s /usr/share/dotnet/dotnet /usr/bin/dotnet
}

# NetCore 2
sub_netcore2(){
	echo "configure netcore 2"
	cd $ROOT_DIR

	# Install NET Core Runtime 2.x
	wget https://packages.microsoft.com/config/debian/10/packages-microsoft-prod.deb -O packages-microsoft-prod.deb
	$SUDO_CMD dpkg -i packages-microsoft-prod.deb
	rm packages-microsoft-prod.deb

	$SUDO_CMD apt-get update
	sub_apt_install_hold dotnet-runtime-2.2=2.2.8-1
}

# NetCore 5
sub_netcore5(){
	echo "configure netcore 5"
	cd $ROOT_DIR

	# Install NET Core Runtime 5.x
	wget https://packages.microsoft.com/config/debian/10/packages-microsoft-prod.deb -O packages-microsoft-prod.deb
	$SUDO_CMD dpkg -i packages-microsoft-prod.deb
	rm packages-microsoft-prod.deb

	$SUDO_CMD apt-get update
	sub_apt_install_hold dotnet-runtime-5.0=5.0.9-1
}

# V8
sub_v8(){
	echo "configure v8"
	# TODO
}

# NodeJS
sub_nodejs(){
	echo "configure node"

	# Install NodeJS library
	sub_apt_install_hold libnode83
}

# TypeScript
sub_typescript(){
	echo "configure typescript"
	# Nothing needed, node_modules are local to the path,
	# runtime is located in /usr/local/lib, and node builtins
	# are already compiled in the runtime
}

# File
sub_file(){
	echo "configure file"
	# Nothing needed
}

# RPC
sub_rpc(){
	echo "configure rpc"

	sub_apt_install_hold libcurl4
}

# Ports
sub_ports(){
	echo "configure ports"

	# Nothing needed, there are no dependencies for ports by now
}

# Install
sub_install(){
	if [ $RUN_AS_ROOT = 1 ]; then
		SUDO_CMD=""
	fi
	if [ $INSTALL_APT = 1 ]; then
		sub_apt
	fi
	if [ $INSTALL_PYTHON = 1 ]; then
		sub_python
	fi
	if [ $INSTALL_RUBY = 1 ]; then
		sub_ruby
	fi
	if [ $INSTALL_NETCORE = 1 ]; then
		sub_netcore
	fi
	if [ $INSTALL_NETCORE2 = 1 ]; then
		sub_netcore2
	fi
	if [ $INSTALL_NETCORE5 = 1 ]; then
		sub_netcore5
	fi
	if [ $INSTALL_V8 = 1 ]; then
		sub_v8
	fi
	if [ $INSTALL_NODEJS = 1 ]; then
		sub_nodejs
	fi
	if [ $INSTALL_TYPESCRIPT = 1 ]; then
		sub_typescript
	fi
	if [ $INSTALL_FILE = 1 ]; then
		sub_file
	fi
	if [ $INSTALL_RPC = 1 ]; then
		sub_rpc
	fi
	if [ $INSTALL_PORTS = 1 ]; then
		sub_ports
	fi
	if [ $INSTALL_CLEAN = 1 ]; then
		sub_clean
	fi

	echo "install finished in workspace $ROOT_DIR"
}

# Clean dependencies
sub_clean(){
	echo "clean dependencies"

	$SUDO_CMD apt-get -y remove wget gpg
	$SUDO_CMD apt-get -y autoclean
}

# Configuration
sub_options(){
	for var in "$@"
	do
		if [ "$var" = 'root' ]; then
			echo "running as root"
			RUN_AS_ROOT=1
		fi
		if [ "$var" = 'base' ]; then
			echo "apt selected"
			INSTALL_APT=1
		fi
		if [ "$var" = 'python' ]; then
			echo "python selected"
			INSTALL_PYTHON=1
		fi
		if [ "$var" = 'ruby' ]; then
			echo "ruby selected"
			INSTALL_RUBY=1
		fi
		if [ "$var" = 'netcore' ]; then
			echo "netcore selected"
			INSTALL_NETCORE=1
		fi
		if [ "$var" = 'netcore2' ]; then
			echo "netcore 2 selected"
			INSTALL_NETCORE2=1
		fi
		if [ "$var" = 'netcore5' ]; then
			echo "netcore 5 selected"
			INSTALL_NETCORE5=1
		fi
		if [ "$var" = 'v8' ]; then
			echo "v8 selected"
			INSTALL_V8=1
		fi
		if [ "$var" = 'nodejs' ]; then
			echo "nodejs selected"
			INSTALL_NODEJS=1
		fi
		if [ "$var" = 'typescript' ]; then
			echo "typescript selected"
			INSTALL_TYPESCRIPT=1
		fi
		if [ "$var" = 'file' ]; then
			echo "file selected"
			INSTALL_FILE=1
		fi
		if [ "$var" = 'rpc' ]; then
			echo "rpc selected"
			INSTALL_RPC=1
		fi
		if [ "$var" = 'ports' ]; then
			echo "ports selected"
			INSTALL_PORTS=1
		fi
		if [ "$var" = 'clean' ]; then
			echo "clean selected"
			INSTALL_CLEAN=1
		fi
	done
}

# Help
sub_help() {
	echo "Usage: `basename "$0"` list of component"
	echo "Components:"
	echo "	root"
	echo "	base"
	echo "	python"
	echo "	ruby"
	echo "	netcore"
	echo "	netcore2"
	echo "	v8"
	echo "	nodejs"
	echo "	typescript"
	echo "	file"
	echo "	rpc"
	echo "	ports"
	echo "	clean"
	echo ""
}

case "$#" in
	0)
		sub_help
		;;
	*)
		sub_options $@
		sub_install
		;;
esac
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="54"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#
# Copyright (C) 2013 Julian Atienza Herrero &lt;j.atienza at har.mrc.ac.uk>
#
# MEDICAL RESEARCH COUNCIL UK MRC
#
# Harwell Mammalian Genetics Unit
#
# http://www.har.mrc.ac.uk
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
#


rm -rf `find . -type d -name .svn`

svn delete Â svn://source.har.mrc.ac.uk/PhenoDCC/exportlibrary/trunk/exportlibrary -m "working copy broken"

svn import exportlibrary svn://source.har.mrc.ac.uk/PhenoDCC/exportlibrary/trunk/exportlibrary -m "first commit after broken working copy"

svn co  svn://source.har.mrc.ac.uk/PhenoDCC/exportlibrary/trunk/exportlibrary



svn delete Â svn://source.har.mrc.ac.uk/PhenoDCC/exportlibrary/trunk/exportlibrary/exportlibrary.utils/src/test/db/

svn delete Â svn://source.har.mrc.ac.uk/PhenoDCC/exportlibrary/trunk/exportlibrary/exportlibrary.xmlvalidation/hsqldb/ -m "remove test dabases"

svn delete Â svn://source.har.mrc.ac.uk/PhenoDCC/exportlibrary/trunk/exportlibrary/exportlibrary.xmlvalidation/src/main/generated/ -m "removed generated classes"
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="55"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/sh
#NEWROOT=:pserver:$(id -un)@cvs.bh.exept.de:/cvs/stx

if [ -z "$1" ]; then
    echo "Common CVS roots:"
    # Do not show these to other people, these are useless for them
    # anyway
    if [ "$USER" = "jv" ]; then
	echo " :ext:vrany@exeptn:/cvs/stx       "
	echo
	echo " :ext:vrany@dialin.exept.de:/cvs/stx           "
	echo
    fi
cat &lt;&lt;EOF
 :pserver:cvs@cvs.smalltalk-x.de:/cvs/stx
     (public eXept CVS, synced once a day. Use this if unsure)

 :ext:swing.fit.cvut.cz/var/local/cvs
     (SWING mirror. Use this if you have shell account
      on swing.fit.cvut.cz)

EOF
    echo -n "Enter new CVS root (or Ctrl-C to abort): "
    read answer
else
    answer="$1"
fi

if [ ! -z "$answer" ]; then
    echo "$answer" > /tmp/chcvs.$$
    find . -name CVS -type d -exec cp /tmp/chcvs.$$ {}/Root \;
    rm /tmp/chcvs.$$
else
    echo "Nothing changed"
    exit 1	
fi
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="56"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

REGEX="https:\/\/[a-z]{7}.*.iso"
Link=$(wget -qO- www.debian.org | grep -Eo $REGEX)
echo $Link
wget $Link -P ~/

#wget -qO- www.debian.org |
#grep -Eoi '&lt;a [^>]+>' |
#grep -Eo 'href="[^\"]+"' |
#grep -Eo '(http|https)://[^/"]+'
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="57"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

kubectl get secret --namespace monitoring grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana" -o jsonpath="{.items[0].metadata.name}")
kubectl --namespace monitoring port-forward $POD_NAME 3000</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="58"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">data_directory=../../model_training_data/Fine-ET/RS_NDS_from_2
ckpt_directory=$data_directory/ckpts
dataset_file=../../datasets/sampled_datasets/RS_NDS_from_2.json.gz

mkdir -p $data_directory
mkdir -p $ckpt_directory


# Model parameters
rnn_hidden_neurons=200
keep_prob=0.5
learning_rate=0.002
batch_size=500
char_embedding_size=200
char_rnn_hidden_neurons=50
joint_embedding_size=500
epochs=15   # A number such that approx 19-20 million EMs are used in training
save_checkpoint_after=600000 # Total entities in RS_NDS_from_2: 1291232

# http://nlp.stanford.edu/data/glove.840B.300d.zip
glove_vector_file_path=/hdd1/word_vectors/glove.42B.300d/glove.42B.300d.txt


# Step 1: Generate local variables such as word to number dictionary etc.
#echo "Generate local variables required for model"
#python Fine-ET/src/data_processing/json_to_tfrecord.py prepare_local_variables $dataset_file  $glove_vector_file_path unk $data_directory/ --lowercase

# Step 2: Convert Training data into TFRecord format.
#echo "Converting Train data to TFRecord"
#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ $dataset_file

# Step 3: Convert development and testing data into TFRecord format.
#echo "Converting Dev and Test data to TFRecord"
#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ ../../datasets/1k-WFB-g/fner_dev.json --test_data
#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ ../../datasets/1k-WFB-g/fner_test.json --test_data
#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ ../../datasets/figer_gold.json --test_data

# Step 4 (Optional): Convert the development and testing data with entities identified by Fine-ED models to TFRecord format. The following files have to first generated by the Fine-ED model trained on the same dataset. Note that, if the Fine-ED model is retrained, these results file needs to be updated.
#echo "Step 4: Pipeline use of FgED results."
#python Fine-ET/src/detect_entities.py ../../datasets/figer_gold.json ../../results/Fine-ED/lstm_crf/RS_NDS_from_2/figer.conll $data_directory/figer_gold_lstm_crf.json
#python Fine-ET/src/detect_entities.py ../../datasets/1k-WFB-g/fner_dev.json ../../results/Fine-ED/lstm_crf/RS_NDS_from_2/fner_dev.conll $data_directory/fner_dev_lstm_crf.json
#python Fine-ET/src/detect_entities.py ../../datasets/1k-WFB-g/fner_test.json ../../results/Fine-ED/lstm_crf/RS_NDS_from_2/fner_test.conll $data_directory/fner_test_lstm_crf.json
#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ $data_directory/fner_dev_lstm_crf.json --test_data
#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ $data_directory/fner_test_lstm_crf.json --test_data
#python Fine-ET/src/data_processing/json_to_tfrecord.py afet_data $data_directory/ $data_directory/figer_gold_lstm_crf.json --test_data

# Run train test procedure 5 times
for ((i=1; i&lt;=5; i++)); do
  # Do not emit '_run_' from model ckpt name
  # format: prefix_run_suffix
  model_ckpt_name=checkpoint_run_$i

#  echo "Training a FNET model"
#  time python Fine-ET/src/main_fnet_train.py  $data_directory/ $ckpt_directory/$model_ckpt_name/ 'RS_NDS_from_2.json*.tfrecord' $rnn_hidden_neurons $keep_prob $learning_rate $batch_size $char_embedding_size $char_rnn_hidden_neurons $joint_embedding_size $epochs $save_checkpoint_after --use_mention --use_clean

#  echo "Testing a FNET model on dev data."
#  python Fine-ET/src/main_fnet_test.py $ckpt_directory/$model_ckpt_name/ $data_directory/fner_dev.json_0.tfrecord

#  echo "Testing a FNET model on dev data with entities detected by a Fine-ED model. (Optional)"
#  python Fine-ET/src/main_fnet_test.py $ckpt_directory/$model_ckpt_name/ $data_directory/fner_dev_lstm_crf.json_0.tfrecord

#  echo "Testing a FNET model on test data."
#  python Fine-ET/src/main_fnet_test.py $ckpt_directory/$model_ckpt_name/ $data_directory/fner_test.json_0.tfrecord

#  echo "Testing a FNET model on test data with entities detected by a Fine-ED model. (Optional)"
#  python Fine-ET/src/main_fnet_test.py $ckpt_directory/$model_ckpt_name/ $data_directory/fner_test_lstm_crf.json_0.tfrecord

#  echo "Testing a FNET model on figer gold data."
#  python Fine-ET/src/main_fnet_test.py $ckpt_directory/$model_ckpt_name/ $data_directory/figer_gold.json_0.tfrecord

#  echo "Testing a FNET model on figer data with entities detected by a Fine-ED model. (Optional)"
#  python Fine-ET/src/main_fnet_test.py $ckpt_directory/$model_ckpt_name/ $data_directory/figer_gold_lstm_crf.json_0.tfrecord

  # The final_result file contains the result on the development set based on the strict, macro and micro F1 metrics.
#  echo "Report results FNER dev data."
#  bash Fine-ET/src/scripts/report_result_fnet.bash $ckpt_directory/$model_ckpt_name/fner_dev.json_0.tfrecord/ ../../datasets/1k-WFB-g/fner_dev.json 0 > $ckpt_directory/$model_ckpt_name/fner_dev.json_0.tfrecord/final_result.txt

#  echo "Report results FNER dev data with entities detected by a Fine-ED model. (Optional)"
#  bash Fine-ET/src/scripts/report_result_fnet.bash $ckpt_directory/$model_ckpt_name/fner_dev_lstm_crf.json_0.tfrecord/ ../../datasets/1k-WFB-g/fner_dev.json 0 > $ckpt_directory/$model_ckpt_name/fner_dev_lstm_crf.json_0.tfrecord/final_result.txt

  # The final_result file contains the result on the test set based on the strict, macro and micro F1 metrics.
#  echo "Report results FNER eval data."
#  bash Fine-ET/src/scripts/report_result_fnet.bash $ckpt_directory/$model_ckpt_name/fner_test.json_0.tfrecord/ ../../datasets/1k-WFB-g/fner_test.json 0 > $ckpt_directory/$model_ckpt_name/fner_test.json_0.tfrecord/final_result.txt

#  echo "Report results FNER eval data with entities detected by a Fine-ED model. (Optional)"
#  bash Fine-ET/src/scripts/report_result_fnet.bash $ckpt_directory/$model_ckpt_name/fner_test_lstm_crf.json_0.tfrecord/ ../../datasets/1k-WFB-g/fner_test.json 0 > $ckpt_directory/$model_ckpt_name/fner_test_lstm_crf.json_0.tfrecord/final_result.txt

#  echo "Report results figer data."
#  bash Fine-ET/src/scripts/report_result_fnet.bash $ckpt_directory/$model_ckpt_name/figer_gold.json_0.tfrecord/ ../../datasets/figer_gold.json 0 ../../datasets/label_patch_figer_to_fner.txt > $ckpt_directory/$model_ckpt_name/figer_gold.json_0.tfrecord/final_result.txt

#  echo "Report results figer data with entities detected by a Fine-ED model. (Optional)"
#  bash Fine-ET/src/scripts/report_result_fnet.bash $ckpt_directory/$model_ckpt_name/figer_gold_lstm_crf.json_0.tfrecord/ ../../datasets/figer_gold.json 0 ../../datasets/label_patch_figer_to_fner.txt > $ckpt_directory/$model_ckpt_name/figer_gold_lstm_crf.json_0.tfrecord/final_result.txt
done

</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="59"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
set -e

BLUE='\033[1;34m'
NC='\033[0m'

script_dir="$( cd "$( dirname "${BASH_SOURCE[0]}"  )" >/dev/null 2>&amp;1 &amp;&amp; pwd )"
python_dir="$script_dir/occlum_instance/image/opt/python-occlum"

cd occlum_instance &amp;&amp; rm -rf image
copy_bom -f ../pytorch.yaml --root image --include-dir /opt/occlum/etc/template

if [ ! -d $python_dir ];then
    echo "Error: cannot stat '$python_dir' directory"
    exit 1
fi

new_json="$(jq '.resource_limits.user_space_size = "6000MB" |
                .resource_limits.kernel_space_heap_size = "256MB" |
                .process.default_mmap_size = "4000MB" |
                .env.default += ["PYTHONHOME=/opt/python-occlum"]' Occlum.json)" &amp;&amp; \
echo "${new_json}" > Occlum.json
occlum build

# Run the python demo
echo -e "${BLUE}occlum run /bin/python3 demo.py${NC}"
occlum run /bin/python3 demo.py
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="60"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># set the android paths globally
export ANDROID_HOME="/opt/android-sdk-linux"
export PATH=$PATH:$ANDROID_HOME/tools
export PATH=$PATH:$ANDROID_HOME/platform-tools
export PATH=$PATH:$ANDROID_HOME/build-tools/24.0.0
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="61"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

# Hi!
# If you're reading this, you're probably interested in what's 
# going on within this script. We've provided what we hope are useful
# comments inline, as well as color-coded relevant shell output.
# We hope it's useful for you, but if you have any questions or suggestions
# please open an issue on https:/github.com/MicrosoftDocs/mslearn-aspnet-core.
#

## Start
cd ~

# dotnet SDK version
declare -x dotnetSdkVersion="3.1.403"

# Module name
declare moduleName="microservices-logging-aspnet-core"

# Any other declarations we need
declare -x gitBranch="live"
declare initScript=https://raw.githubusercontent.com/MicrosoftDocs/mslearn-aspnet-core/$gitBranch/infrastructure/scripts/initenvironment.sh
declare suppressAzureResources=true
declare rootLocation=~/clouddrive
declare editorHomeLocation=$rootLocation/aspnet-learn/src

if [ -d "$rootLocation/aspnet-learn" ]; then
    echo "$rootLocation/aspnet-learn/ already exists!"
    echo " "
    echo "Before running this script, please remove or rename the existing $rootLocation/aspnet-learn/ directory as follows:"
    echo "Remove: rm -r $rootLocation/aspnet-learn/"
    echo "Rename: mv $rootLocation/aspnet-learn/ ~/clouddrive/new-name-here/ "
    echo " "
else
    # Backup .bashrc
    cp ~/.bashrc ~/.bashrc.bak.$moduleName

    # Grab and run initenvironment.sh
    . &lt;(wget -q -O - $initScript)

    # Download
    downloadStarterApp

    # Set location to ~/clouddrive
    cd $editorHomeLocation

    # Launch editor so the user can see the code
    code .

    # Run eshop-learn quickstart to deploy to AKS
    $editorHomeLocation/deploy/k8s/quickstart.sh --resource-group eshop-learn-rg --location centralus

    # Create ACR resource
    $editorHomeLocation/deploy/k8s/create-acr.sh

    # Display URLs to user
    cat ~/clouddrive/aspnet-learn/deployment-urls.txt
fi
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="62"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

set -e
source $(dirname $0)/lib.sh

req_env_var "
    USER $USER
    HOME $HOME
    ENVLIB $ENVLIB
    SCRIPT_BASE $SCRIPT_BASE
    CIRRUS_REPO_NAME $CIRRUS_REPO_NAME
    CIRRUS_CHANGE_IN_REPO $CIRRUS_CHANGE_IN_REPO
    CIRRUS_WORKING_DIR $CIRRUS_WORKING_DIR
"

[[ "$SHELL" =~ "bash" ]] || chsh -s /bin/bash

cd "$CIRRUS_WORKING_DIR"  # for clarity of initial conditions

# Verify basic dependencies
for depbin in gcc rsync sha256sum curl make
do
    if ! type -P "$depbin" &amp;> /dev/null
    then
        echo "***** WARNING: $depbin binary not found in $PATH *****"
    fi
done

# Setup env. vars common to all tasks/scripts/platforms and
# ensure they return for every following script execution.
MARK="# Added by $0, manual changes will be lost."
touch "$HOME/$ENVLIB"
if ! grep -q "$MARK" "$HOME/$ENVLIB"
then
    cp "$HOME/$ENVLIB" "$HOME/${ENVLIB}_original"
    # N/B: Single-quote items evaluated every time, double-quotes only once (right now).
    for envstr in \
        "$MARK" \
        "export SRC=\"$CIRRUS_WORKING_DIR\"" \
        "export OS_RELEASE_ID=\"$(os_release_id)\"" \
        "export OS_RELEASE_VER=\"$(os_release_ver)\"" \
        "export OS_REL_VER=\"$(os_release_id)-$(os_release_ver)\"" \
        "export BUILT_IMAGE_SUFFIX=\"-$CIRRUS_REPO_NAME-${CIRRUS_CHANGE_IN_REPO:0:8}\""
    do
        # Make permanent in later shells, and set in current shell
        X=$(echo "$envstr" | tee -a "$HOME/$ENVLIB") &amp;&amp; eval "$X" &amp;&amp; echo "$X"
    done

    # Do the same for golang env. vars
    go env | while read envline
    do
        X=$(echo "export $envline" | tee -a "$HOME/$ENVLIB") &amp;&amp; eval "$X" &amp;&amp; echo "$X"
    done

    show_env_vars

    # Nothing further required on image-builder VM
    if ((IMAGE_BUILD))
    then
        exit 0
    fi

    # Owner/mode may have changed
    setup_gopath

    case "$OS_REL_VER" in
        fedora-29)
            install_testing_deps
            build_and_replace_conmon

            cd "$CRIO_SRC"  # cri-o source
            echo "Building binaries required for testing"
            ooe.sh make test-binaries

            echo "Configuring firewall/networking for integration tests"
            ooe.sh iptables -F
            ooe.sh iptables -t nat -I POSTROUTING -s 127.0.0.1 ! -d 127.0.0.1 -j MASQUERADE
            echo "Setting read_only flag to false"
            sudo sed -i 's/read_only = true/read_only = false/g' /etc/crio/crio.conf
            echo "Removing nodev flag"
            sudo sed -i 's/nodev//g' /etc/containers/storage.conf
            iptables -L -n -v
            ;;
        *) bad_os_id_ver ;;
    esac

    # Verify nothing was set empty
    # N/B: Some multi-user environment variables are pre-cooked into /etc/environ
    #      (see setup_gopath in $SCRIPT_BASE/lib.sh)
    req_env_var "
        OS_RELEASE_ID $OS_RELEASE_ID
        OS_RELEASE_VER $OS_RELEASE_VER
        OS_REL_VER $OS_REL_VER
        BUILT_IMAGE_SUFFIX $BUILT_IMAGE_SUFFIX
    "
fi

echo "***** TESTING STARTS: $(date --iso-8601=seconds)"
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="63"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">export MALI_LIBS=/scratch/ZCU102_PL/ZynqMP/build/tmp/sysroots/plnx_aarch64/usr/lib
export MALI_INCLUDE=/scratch/ZCU102_PL/ZynqMP/build/tmp/sysroots/plnx_aarch64/usr/include
export ARM_CROSS_COMPILER_PATH=/proj/petalinux/petalinux-v2017.1_daily_latest/petalinux-v2017.1-final/tools/linux-i386/aarch64-linux-gnu/bin/aarch64-linux-gnu-g++
export ARM_CROSS_COMPILER_PATH_C=/proj/petalinux/petalinux-v2017.1_daily_latest/petalinux-v2017.1-final/tools/linux-i386/aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc
export CXXFLAGS=-DENABLE_FBDEV


</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="64"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash -ve

./node_modules/.bin/marionette-mocha \
  --host-log stdout \
  --host $(pwd)/node_modules/graphene-marionette-runner/host/index.js \
  --runtime ./graphene/Contents/MacOS/graphene \
  --start-manifest http://localhost:6060/manifest.webapp \
  $(find test -name '*_test.js') $@;
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="65"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

find . -regex '\.\/[P|T].CL-.*\.sh' -exec  {} -o results-Latency/ --csv \; 
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="66"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/sh

echo "/patched-lib" > /etc/ld.so.conf.d/000-patched-lib.conf &amp;&amp; \
mkdir -p "/patched-lib" &amp;&amp; \
PATCH_OUTPUT_DIR=/patched-lib /usr/local/bin/patch.sh &amp;&amp; \
cd /patched-lib &amp;&amp; \
for f in * ; do
    suffix="${f##*.so}"
    name="$(basename "$f" "$suffix")"
    [ -h "$name" ] || ln -sf "$f" "$name"
    [ -h "$name" ] || ln -sf "$f" "$name.1"
done &amp;&amp; \
ldconfig
[ "$OLDPWD" ] &amp;&amp; cd -
exec "$@"
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="67"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">export NGPUS=8
python3 -m torch.distributed.launch --nproc_per_node=$NGPUS tools/train_net.py --config-file configs/e2e_faster_rcnn_DETNAS_COCO_FPN_300M_search.yaml  OUTPUT_DIR models/DETNAS_COCO_FPN_300M_1x_search
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="68"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># === === === === ===
# Capture stdout and stderr to log files
## I don't totally understand this line. Sourced from https://stackoverflow.com/a/2364896/100596 as of 2020-07-05:
exec 3>&amp;1 4>&amp;2

## Redirect stdout to log
exec 1>/tmp/dotfiles-install-stdout.log

## Redirect stderr to log
exec 2>/tmp/dotfiles-install-stderr.log


# === === === === ===
# Make sure PowerShell is installed
sudo chmod a+x ./posix/require-pwsh.sh
. ./posix/require-pwsh.sh


# === === === === ===
# Make sure Git is installed
sudo chmod a+x ./posix/require-git.sh
. ./posix/require-git.sh


# === === === === ===
# Restore stdout and stderr
exec 1>&amp;3 2>&amp;4

## Close the unused descriptors
exec 3>&amp;- 4>&amp;-
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="69"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/usr/bin/env bash
DIR="$(cd "$(dirname "$0")" >/dev/null 2>&amp;1 &amp;&amp; pwd)"
GO_SRC="$DIR/.."

rm -rf "$GO_SRC/.go"
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="70"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

# Get script arguments
PARAMS=""
while (("$#")); do
    [[ $1 == --*=* ]] &amp;&amp; set -- "${1%%=*}" "${1#*=}" "${@:2}"
    case "$1" in
    --organization-url)
        if [ -n "$2" ] &amp;&amp; [ "${2:0:1}" != "-" ]; then
            organization_url=$2
            shift 2
        else
            echo "Error: Argument for $1 is missing" >&amp;2
            exit 1
        fi
        ;;
    --project-name)
        if [ -n "$2" ] &amp;&amp; [ "${2:0:1}" != "-" ]; then
            project_name=$2
            shift 2
        else
            echo "Error: Argument for $1 is missing" >&amp;2
            exit 1
        fi
        ;;
    --repository-name)
        if [ -n "$2" ] &amp;&amp; [ "${2:0:1}" != "-" ]; then
            repository_name=$2
            shift 2
        else
            echo "Error: Argument for $1 is missing" >&amp;2
            exit 1
        fi
        ;;
    --pull-request-title)
        if [ -n "$2" ] &amp;&amp; [ "${2:0:1}" != "-" ]; then
            pull_request_title=$2
            shift 2
        else
            echo "Error: Argument for $1 is missing" >&amp;2
            exit 1
        fi
        ;;
    --branch-name)
        if [ -n "$2" ] &amp;&amp; [ "${2:0:1}" != "-" ]; then
            branch_name=$2
            shift 2
        else
            echo "Error: Argument for $1 is missing" >&amp;2
            exit 1
        fi
        ;;
    --source-folder-path)
        if [ -n "$2" ] &amp;&amp; [ "${2:0:1}" != "-" ]; then
            source_folder_path=$2
            shift 2
        else
            echo "Error: Argument for $1 is missing" >&amp;2
            exit 1
        fi
        ;;
    --temporary-branch-name)
        if [ -n "$2" ] &amp;&amp; [ "${2:0:1}" != "-" ]; then
            temporary_branch_name=$2
            shift 2
        else
            echo "Error: Argument for $1 is missing" >&amp;2
            exit 1
        fi
        ;;
    --temporary-folder-path)
        if [ -n "$2" ] &amp;&amp; [ "${2:0:1}" != "-" ]; then
            temporary_folder_path=$2
            shift 2
        else
            echo "Error: Argument for $1 is missing" >&amp;2
            exit 1
        fi
        ;;
    --overwrite-subfolder)
        if [ -n "$2" ] &amp;&amp; [ "${2:0:1}" != "-" ]; then
            overwrite_subfolder=$2
            shift 2
        else
            echo "Error: Argument for $1 is missing" >&amp;2
            exit 1
        fi
      
        ;;
    -*) # unsupported flags
        echo "Error: Unsupported flag $1" >&amp;2
        exit 1
        ;;
    *) # preserve positional arguments
        PARAMS="$PARAMS ""$1"""
        shift
        ;;
    esac
done
eval set -- "$PARAMS"
set -e -o pipefail

echo "Installing Azure DevOps extension..."
az extension add --name "azure-devops"
az devops configure --defaults organization="${organization_url}" project="${project_name}"

echo "Creating folder ${temporary_folder_path}..."
mkdir -p "${temporary_folder_path}"

echo "Cloning branch ${branch_name}..."
clone_url=$(az repos show --repository "${repository_name}" --query "webUrl" --output tsv)
authenticated_clone_url=${clone_url/\/\////$AZURE_DEVOPS_EXT_PAT@}
git clone --branch "${branch_name}" --depth 1 "${authenticated_clone_url}" "${temporary_folder_path}"

echo "Creating temporary branch ${temporary_branch_name} from ${branch_name}..."
git -C "${temporary_folder_path}" checkout -b "${temporary_branch_name}" "${branch_name}"


echo "Overwrite folder set to $overwrite_subfolder; deleting its contents..."
rm -rfv "${temporary_folder_path:?}/${overwrite_subfolder:?}"/*


echo "Copying source folder ${source_folder_path} contents to temporary folder ${temporary_folder_path}..."
cp -r "${source_folder_path}"/* "${temporary_folder_path}"/

echo "Validating that changes exist to be published..."
if [[ ! $(git -C "${temporary_folder_path}" status --porcelain | head -1) ]]; then
    echo "No changes exist to be published."
    exit 0
fi

echo "Setting git user information..."
git config --global user.email "azuredevopsagent@azuredevops.com"
git config --global user.name "Azure Devops agent"

echo "Adding changes..."
git -C "${temporary_folder_path}" add --all

echo "Commiting changes..."
git -C "${temporary_folder_path}" commit --message "Initial commit"

echo "Pushing changes..."
git -C "${temporary_folder_path}" push --set-upstream origin "${temporary_branch_name}"

echo "Creating pull request..."
az repos pr create --source-branch "${temporary_branch_name}" --target-branch "${branch_name}" --title "${pull_request_title}" --squash --delete-source-branch "true" --repository "${repository_name}"

echo "Deleting temporary folder contents..."
rm -rf "${temporary_folder_path}/{*,.*}"

echo "Execution complete."</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="71"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">export DETECTRON2_DATASETS=~/datasets
python train_detectron.py --num-gpus 2 \
        --config-file ./detectron_configs/COCO-Keypoints/keypoint_rcnn_resnet50_triplet_attention_FPN_1x.yaml \
	#--eval-only MODEL.WEIGHTS ./output/model_final.pth
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="72"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

# Copyright 2021 VMware Tanzu Community Edition contributors. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

set -e

MY_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&amp;1 &amp;&amp; pwd )"
TCE_REPO_PATH=${MY_DIR}/../../../../..

TCE_VERSION="v0.9.1"

echo "Installing TCE ${TCE_VERSION}"

BUILD_OS=$(uname -s | tr '[:upper:]' '[:lower:]')
TCE_RELEASE_TAR_BALL="tce-${BUILD_OS}-amd64-${TCE_VERSION}.tar.gz"
TCE_RELEASE_DIR="tce-${BUILD_OS}-amd64-${TCE_VERSION}"
INSTALLATION_DIR="${MY_DIR}/tce-installation"

"${TCE_REPO_PATH}"/hack/get-tce-release.sh ${TCE_VERSION} "${BUILD_OS}"-amd64

mkdir -p "${INSTALLATION_DIR}"
tar xzvf "${TCE_RELEASE_TAR_BALL}" --directory="${INSTALLATION_DIR}"

"${INSTALLATION_DIR}"/"${TCE_RELEASE_DIR}"/install.sh || { error "Unexpected failure during TCE installation"; exit 1; }

echo "TCE version: "
tanzu standalone-cluster version || { error "Unexpected failure during TCE installation"; exit 1; }

TANZU_DIAGNOSTICS_PLUGIN_DIR=${MY_DIR}/..
TANZU_DIAGNOSTICS_BIN=${MY_DIR}/tanzu-diagnostics-e2e-bin

echo "Entering ${TANZU_DIAGNOSTICS_PLUGIN_DIR} directory to build tanzu diagnostics plugin"
pushd "${TANZU_DIAGNOSTICS_PLUGIN_DIR}"

go build -o "${TANZU_DIAGNOSTICS_BIN}" -v

echo "Finished building tanzu diagnostics plugin. Leaving ${TANZU_DIAGNOSTICS_PLUGIN_DIR}"
popd
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="73"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

./configure --prefix=${PREFIX}

make -j${CPU_COUNT}
make check -j${CPU_COUNT}
make install -j${CPU_COUNT}
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="74"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/sh

# This script requires google's cloud_sql_proxy on your PATH. One way to set this up:
# $ brew cask install google-cloud-sdk
# $ gcloud components install cloud_sql_proxy
# $ PATH="$PATH:/usr/local/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin"
# OR
# $ ln -s /usr/local/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin/cloud_sql_proxy /usr/local/bin/cloud_sql_proxy
#
# Use this script to connect to cloud Postgres for a specific Catalog instance.
# For example, to connect to the dev catalog database, run:
# $ ENV=dev ./db-connect.sh
#
# The proxy will continue to run until you quit it using ^C.
#
# The default port used is 5431, to avoid conflicting with a locally running postgres which
# defaults to port 5432. Use the environment variable PORT to override this setting.

: "${ENV:?}"
PORT=${PORT:-5431}

VAULT_PATH="secret/dsde/terra/kernel/${ENV}/${ENV}/catalog/postgres"

INSTANCE=$(vault read -field=data -format=json "${VAULT_PATH}/instance" |
           jq -r '"\(.project):\(.region):\(.name)"')=tcp:$PORT

DB_CREDS_DATA=$(vault read -field=data -format=json "${VAULT_PATH}/db-creds")

JDBC_URL=jdbc:postgresql://localhost:$PORT/$(echo "${DB_CREDS_DATA}" |
          jq -r '"\(.db)?user=\(.username)&amp;password=\(.password)"')

PSQL_COMMAND=$(echo "${DB_CREDS_DATA}" |
          jq -r '"psql postgresql://\(.username):\(.password)@localhost/\(.db)\\?port="')$PORT

echo "Starting a proxy for $ENV. Connect using: \"$JDBC_URL\" or run: \"$PSQL_COMMAND\""

cloud_sql_proxy -instances="${INSTANCE}" -dir=/tmp
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="75"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

exec /usr/local/bin/preoomkiller &amp;

while :; do sleep 1; done
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="76"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/bin/bash

# Copyright 2017-2020 Authors of Cilium
# SPDX-License-Identifier: Apache-2.0

set -o errexit
set -o pipefail
set -o nounset

MAKER_IMAGE="${MAKER_IMAGE:-docker.io/cilium/image-maker:3e2ea4f151593908c362307a1de22e68610d955c}"

if [ "$#" -ne 1 ] ; then
  echo "$0 supports exactly 1 argument"
  exit 1
fi

root_dir="$(git rev-parse --show-toplevel)"

if [ -z "${MAKER_CONTAINER+x}" ] ; then
   exec docker run --env DOCKER_HUB_PUBLIC_ACCESS_ONLY=true --env QUAY_PUBLIC_ACCESS_ONLY=true --rm --volume "${root_dir}:/src" --workdir /src "${MAKER_IMAGE}" "/src/scripts/$(basename "${0}")" "${1}"
fi

crane digest "${1}" 2> /dev/null
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="77"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">#!/usr/bin/env bash

# Copyright 2017   Hainan Xu
# Apache 2.0

# This script rescores lattices with KALDI RNNLM trained on reversed text.
# The input directory should already be rescored with a forward RNNLM, preferably
# with the pruned algorithm, since smaller lattices make rescoring much faster.
# An example of the forward pruned rescoring is at
# egs/swbd/s5c/local/rnnlm/run_tdnn_lstm.sh
# One example script for backward RNNLM rescoring is at
# egs/swbd/s5c/local/rnnlm/run_tdnn_lstm_back.sh

# Begin configuration section.
cmd=run.pl
skip_scoring=false
max_ngram_order=4 # Approximate the lattice-rescoring by limiting the max-ngram-order
                  # if it's set, it merges histories in the lattice if they share
                  # the same ngram history and this prevents the lattice from 
                  # exploding exponentially. Details of the n-gram approximation
                  # method are described in section 2.3 of the paper
                  # http://www.danielpovey.com/files/2018_icassp_lattice_pruning.pdm

weight=0.5  # Interpolation weight for RNNLM.
normalize=false # If true, we add a normalization step to the output of the RNNLM
                # so that it adds up to *exactly* 1. Note that this is not necessary
                # as in our RNNLM setup, a properly trained network would automatically
                # have its normalization term close to 1. The details of this
                # could be found at http://www.danielpovey.com/files/2018_icassp_rnnlm.pdf
scoring_opts=

# End configuration section.

echo "$0 $@"  # Print the command line for logging

. ./utils/parse_options.sh

if [ $# != 5 ]; then
   echo "Does language model rescoring of lattices (remove old LM, add new LM)"
   echo "with Kaldi RNNLM trained on reversed text. See comments in file for details"
   echo ""
   echo "Usage: $0 [options] &lt;old-lang-dir> &lt;rnnlm-dir> \\"
   echo "                   &lt;data-dir> &lt;input-decode-dir> &lt;output-decode-dir>"
   echo " e.g.: $0 data/lang_tg exp/rnnlm_lstm/ data/test \\"
   echo "                   exp/tri3/test_rnnlm_forward exp/tri3/test_rnnlm_bidirection"
   echo "options: [--cmd (run.pl|queue.pl [queue opts])]"
   exit 1;
fi

[ -f path.sh ] &amp;&amp; . ./path.sh;

oldlang=$1
rnnlm_dir=$2
data=$3
indir=$4
outdir=$5

oldlm=$oldlang/G.fst
if [ ! -f $oldlm ]; then
  echo "$0: file $oldlm not found; using $oldlang/G.carpa"
  oldlm=$oldlang/G.carpa
fi

[ ! -f $oldlm ] &amp;&amp; echo "$0: Missing file $oldlm" &amp;&amp; exit 1;
[ ! -f $rnnlm_dir/final.raw ] &amp;&amp; echo "$0: Missing file $rnnlm_dir/final.raw" &amp;&amp; exit 1;
[ ! -f $rnnlm_dir/feat_embedding.final.mat ] &amp;&amp; [ ! -f $rnnlm_dir/word_embedding.final.mat ] &amp;&amp; echo "$0: Missing word embedding file" &amp;&amp; exit 1;

[ ! -f $oldlang/words.txt ] &amp;&amp;\
  echo "$0: Missing file $oldlang/words.txt" &amp;&amp; exit 1;
! ls $indir/lat.*.gz >/dev/null &amp;&amp;\
  echo "$0: No lattices input directory $indir" &amp;&amp; exit 1;
awk -v n=$0 -v w=$weight 'BEGIN {if (w &lt; 0 || w > 1) {
  print n": Interpolation weight should be in the range of [0, 1]"; exit 1;}}' \
  || exit 1;

normalize_opt=
if $normalize; then
  normalize_opt="--normalize-probs=true"
fi
oldlm_command="fstproject --project_output=true $oldlm |"
special_symbol_opts=$(cat $rnnlm_dir/special_symbol_opts.txt)

word_embedding=
if [ -f $rnnlm_dir/word_embedding.final.mat ]; then
  word_embedding=$rnnlm_dir/word_embedding.final.mat
else
  word_embedding="'rnnlm-get-word-embedding $rnnlm_dir/word_feats.txt $rnnlm_dir/feat_embedding.final.mat -|'"
fi

mkdir -p $outdir/log
nj=`cat $indir/num_jobs` || exit 1;
cp $indir/num_jobs $outdir

# In order to rescore with a backward RNNLM, we first remove the original LM
# scores with lattice-lmrescore, before reversing the lattices
oldlm_weight=$(perl -e "print -1.0 * $weight;")
if [ "$oldlm" == "$oldlang/G.fst" ]; then
  $cmd JOB=1:$nj $outdir/log/rescorelm.JOB.log \
    lattice-lmrescore --lm-scale=$oldlm_weight \
    "ark:gunzip -c $indir/lat.JOB.gz|" "$oldlm_command" ark:-  \| \
    lattice-reverse ark:- ark:- \| \
    lattice-lmrescore-kaldi-rnnlm --lm-scale=$weight $special_symbol_opts \
    --max-ngram-order=$max_ngram_order $normalize_opt \
    $word_embedding "$rnnlm_dir/final.raw" ark:- ark:- \| \
    lattice-reverse ark:- "ark,t:|gzip -c>$outdir/lat.JOB.gz" || exit 1;
else
  $cmd JOB=1:$nj $outdir/log/rescorelm.JOB.log \
    lattice-lmrescore-const-arpa --lm-scale=$oldlm_weight \
    "ark:gunzip -c $indir/lat.JOB.gz|" "$oldlm" ark:-  \| \
    lattice-reverse ark:- ark:- \| \
    lattice-lmrescore-kaldi-rnnlm --lm-scale=$weight $special_symbol_opts \
    --max-ngram-order=$max_ngram_order $normalize_opt \
    $word_embedding "$rnnlm_dir/final.raw" ark:- ark:- \| \
    lattice-reverse ark:- "ark,t:|gzip -c>$outdir/lat.JOB.gz" || exit 1;
fi

if ! $skip_scoring ; then
  err_msg="$0: Not scoring because local/score.sh does not exist or not executable."
  [ ! -x local/score.sh ] &amp;&amp; echo $err_msg &amp;&amp; exit 1;
  echo local/score.sh --cmd "$cmd" $scoring_opts $data $oldlang $outdir
  local/score.sh --cmd "$cmd" $scoring_opts $data $oldlang $outdir
else
  echo "$0: Not scoring because --skip-scoring was specified."
fi

exit 0;
</span></div></div>
							</td>
					</tr></tbody></table>
		<div class="bg-linear-to-b sticky left-0 border-t border-dashed border-gray-300 from-gray-100 to-white py-3 text-center font-mono text-xs dark:border-gray-700 dark:from-gray-950 dark:to-gray-900">End of preview. <a href="/datasets/GunA-SD/bash_code/viewer/default/train" class="group"><span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Expand</span>
						in <svg class="text-lg mr-0.5 inline -translate-y-px text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>Data Studio
					</a></div></div>




			<div class="bg-linear-to-b from-gray-100 to-white dark:from-gray-950 dark:to-gray-900 "><hr class="flex-none -translate-y-px border-t border-dashed border-gray-300 bg-white dark:border-gray-700 dark:bg-gray-950">
					<nav><ul class="flex select-none items-center justify-between space-x-2 text-gray-700 sm:justify-center py-1 text-center font-mono text-xs rounded-b-lg"><li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 pointer-events-none cursor-default text-gray-400 hover:text-gray-700" href=""><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg>
		Previous</a></li>
			<li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1 bg-gray-50 font-semibold ring-1 ring-inset ring-gray-200 dark:bg-gray-900 dark:text-yellow-500 dark:ring-gray-900 hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/GunA-SD/bash_code/viewer/default/train?p=0">1</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/GunA-SD/bash_code/viewer/default/train?p=1">2</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/GunA-SD/bash_code/viewer/default/train?p=2">3</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  pointer-events-none cursor-default" href="#">...</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/GunA-SD/bash_code/viewer/default/train?p=3427">3,428</a>
				</li>
			<li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 " href="/datasets/GunA-SD/bash_code/viewer/default/train?p=1">Next
		<svg class="ml-1.5 transform rotate-180" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg></a></li></ul></nav></div></div>






</div></div></div></div></div></div></div>

				
					<div class="SVELTE_HYDRATER contents" data-target="RepoCodeCopy" data-props="{}"><div></div></div>
					

					<div class="SVELTE_HYDRATER contents" data-target="SideNavigation" data-props="{&quot;titleTree&quot;:[],&quot;classNames&quot;:&quot;top-6&quot;}">

</div>
					<div class="2xl:pr-6"><div class="prose pl-6 -ml-6 hf-sanitized hf-sanitized-3G0zAJJ_pZcb9a52Jk0Bv">
	<!-- HTML_TAG_START --><p>This dataset is a collection of bash programs from various GitHub repositories and open source projects.
The dataset might contain harmful code.</p>
<!-- HTML_TAG_END --></div>
</div></section>
			
			<section class="pt-6 border-gray-100 md:pb-24 md:pl-6 md:w-64 lg:w-80 xl:w-96 flex-none order-first md:order-none md:border-l pt-3! md:pt-6!"><dl class="flex items-baseline justify-between"><dt class="text-sm text-gray-500">Downloads last month</dt><div class="mx-4 flex-1 border-b border-dotted"></div><dd class="font-semibold">133</dd></dl>
				
				<div class="divider-column-vertical"></div>
				<div class="grid grid-cols-2 gap-x-2 md:flex md:flex-row md:flex-wrap"><div class="SVELTE_HYDRATER contents" data-target="DatasetAndModelActionsDropdown" data-props="{&quot;classNames&quot;:&quot;order-last&quot;,&quot;discussionsDisabled&quot;:false,&quot;repo&quot;:{&quot;type&quot;:&quot;dataset&quot;,&quot;name&quot;:&quot;GunA-SD/bash_code&quot;},&quot;canWrite&quot;:false,&quot;canDisable&quot;:false,&quot;repoIsPrivate&quot;:false,&quot;repoIsGated&quot;:false,&quot;repoIsDisabled&quot;:false,&quot;repoIsAdminFlaggedNFAA&quot;:false,&quot;repoHasBlockedOids&quot;:false}"><div class="order-last"><div class="relative ">
	<button class="btn px-1.5 py-1.5 " type="button">
		
			<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="p-0.5" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><circle cx="16" cy="7" r="3" fill="currentColor"></circle><circle cx="16" cy="16" r="3" fill="currentColor"></circle><circle cx="16" cy="25" r="3" fill="currentColor"></circle></svg>
		
		</button>
	
	
	</div></div>













</div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetLibrary" data-props="{&quot;classNames&quot;:&quot;md:w-full xl:w-auto xl:flex-none&quot;,&quot;libraries&quot;:[{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;datasets&quot;,&quot;function&quot;:&quot;load_dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{},&quot;code&quot;:&quot;from datasets import load_dataset\n\nds = load_dataset(\&quot;GunA-SD/bash_code\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;dask&quot;,&quot;function&quot;:&quot;dd.read_parquet&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;data/train-*.parquet&quot;}},&quot;code&quot;:&quot;import dask.dataframe as dd\n\ndf = dd.read_parquet(\&quot;hf://datasets/GunA-SD/bash_code/data/train-*.parquet\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;mlcroissant&quot;,&quot;function&quot;:&quot;Dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;record_set&quot;:&quot;default&quot;,&quot;partial&quot;:false},&quot;code&quot;:&quot;from mlcroissant import Dataset\n\nds = Dataset(jsonld=\&quot;https://huggingface.co/api/datasets/GunA-SD/bash_code/croissant\&quot;)\nrecords = ds.records(\&quot;default\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;polars&quot;,&quot;function&quot;:&quot;pl.read_parquet&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;data/train-*.parquet&quot;}},&quot;code&quot;:&quot;import polars as pl\n\ndf = pl.read_parquet('hf://datasets/GunA-SD/bash_code/data/train-*.parquet')\n&quot;}]}]}"><div class="relative md:w-full xl:w-auto xl:flex-none">
	<button class="from-gray-800! to-black! max-xl:mb-2 text-white! gap-1! border-gray-800! dark:border-gray-900!  btn w-full cursor-pointer text-sm" type="button">
		<svg class="mr-1.5 mr-0.5! " xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>
			Use this dataset
		<svg class="-mr-1 text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path d="M16.293 9.293L12 13.586L7.707 9.293l-1.414 1.414L12 16.414l5.707-5.707z" fill="currentColor"></path></svg></button>
	
	
	</div>

</div>
					
					</div>
				<div class="divider-column-vertical"></div>
					<div class="flex flex-col flex-wrap xl:flex-row"><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of downloaded dataset files:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->348 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 " href="/datasets/GunA-SD/bash_code/tree/refs%2Fconvert%2Fparquet/" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of the auto-converted Parquet files:</div>
		<div class="truncate text-sm group-hover:underline">
			<!-- HTML_TAG_START -->348 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Number of rows:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->342,720<!-- HTML_TAG_END --></div></a></div>
				
				
				
				

				
				<div class="divider-column-vertical md:hidden"></div></section></div></main>

	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container relative flex flex-col justify-between space-y-2 py-6 text-gray-500 max-md:*:self-start md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="SVELTE_HYDRATER contents" data-target="ThemeSwitcher" data-props="{&quot;theme&quot;:&quot;system&quot;,&quot;isLoggedIn&quot;:false,&quot;menuClassNames&quot;:&quot;md:-top-24&quot;,&quot;classNames&quot;:&quot;max-md:mb-5 max-md:*:self-start&quot;}">
<div class="relative inline-block max-md:mb-5 max-md:*:self-start">
	<button class="rounded-full border border-gray-100 pl-2 py-1 pr-2.5  flex items-center text-sm text-gray-500 bg-white hover:bg-purple-50 hover:border-purple-200 dark:hover:bg-gray-800 dark:hover:border-gray-950 dark:border-gray-800 " type="button">
		<svg class="mr-1.5 text-gray-500" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M6 22.5h20a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2H6a2 2 0 0 0-2 2v13.5a2 2 0 0 0 2 2ZM7 7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h18a1 1 0 0 0 1-1V8a1 1 0 0 0-1-1H7Z" fill="currentColor"></path><path d="M6 8a1 1 0 0 1 1-1h18a1 1 0 0 1 1 1v11a1 1 0 0 1-1 1H7a1 1 0 0 1-1-1V8Z" fill="currentColor" fill-opacity=".4"></path><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path></svg>
			System theme
		</button>
	
	
	</div></div>
		<div class="font-semibold text-black md:hidden">Company</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="max-md:mb-4! max-md:mt-8! group flex-none max-md:order-last"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="max-md:mt-8! font-semibold text-black md:hidden">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("\/front\/build\/kube-68d7aa0\/index.js");
			window.moonSha = "kube-68d7aa0\/";
			window.__hf_deferred = {};
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>
	</body>
</html>
