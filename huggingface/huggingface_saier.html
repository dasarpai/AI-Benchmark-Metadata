<!doctype html>
<html class="">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
		<meta name="description" content="Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science." />
		<meta property="fb:app_id" content="1321688464574422" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@huggingface" />
		<meta name="twitter:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/saier/unarXive_imrad_clf.png" />
		<meta property="og:title" content="saier/unarXive_imrad_clf Â· Datasets at Hugging Face" />
		<meta property="og:type" content="website" />
		<meta property="og:url" content="https://huggingface.co/datasets/saier/unarXive_imrad_clf" />
		<meta property="og:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/saier/unarXive_imrad_clf.png" />

		<link rel="stylesheet" href="/front/build/kube-68d7aa0/style.css" />

		<link rel="preconnect" href="https://fonts.gstatic.com" />
		<link
			href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap"
			rel="stylesheet"
		/>

		<link
			rel="preload"
			href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css"
			as="style"
			onload="this.onload=null;this.rel='stylesheet'"
		/>
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		<script>const guestTheme = document.cookie.match(/theme=(\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches));</script>
<link rel="canonical" href="https://huggingface.co/datasets/saier/unarXive_imrad_clf"> <script type="application/ld+json">{
  "@context": {
    "@language": "en",
    "@vocab": "https:\/\/schema.org\/",
    "citeAs": "cr:citeAs",
    "column": "cr:column",
    "conformsTo": "dct:conformsTo",
    "cr": "http:\/\/mlcommons.org\/croissant\/",
    "data": {
      "@id": "cr:data",
      "@type": "@json"
    },
    "dataBiases": "cr:dataBiases",
    "dataCollection": "cr:dataCollection",
    "dataType": {
      "@id": "cr:dataType",
      "@type": "@vocab"
    },
    "dct": "http:\/\/purl.org\/dc\/terms\/",
    "extract": "cr:extract",
    "field": "cr:field",
    "fileProperty": "cr:fileProperty",
    "fileObject": "cr:fileObject",
    "fileSet": "cr:fileSet",
    "format": "cr:format",
    "includes": "cr:includes",
    "isLiveDataset": "cr:isLiveDataset",
    "jsonPath": "cr:jsonPath",
    "key": "cr:key",
    "md5": "cr:md5",
    "parentField": "cr:parentField",
    "path": "cr:path",
    "personalSensitiveInformation": "cr:personalSensitiveInformation",
    "recordSet": "cr:recordSet",
    "references": "cr:references",
    "regex": "cr:regex",
    "repeated": "cr:repeated",
    "replace": "cr:replace",
    "sc": "https:\/\/schema.org\/",
    "separator": "cr:separator",
    "source": "cr:source",
    "subField": "cr:subField",
    "transform": "cr:transform"
  },
  "@type": "sc:Dataset",
  "distribution": [
    {
      "@type": "cr:FileObject",
      "@id": "repo",
      "name": "repo",
      "description": "The Hugging Face git repository.",
      "contentUrl": "https:\/\/huggingface.co\/datasets\/saier\/unarXive_imrad_clf\/tree\/refs%2Fconvert%2Fparquet",
      "encodingFormat": "git+https",
      "sha256": "https:\/\/github.com\/mlcommons\/croissant\/issues\/80"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-default",
      "name": "parquet-files-for-config-default",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "default\/*\/*.parquet"
    }
  ],
  "recordSet": [
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "default_splits\/split_name"
      },
      "@id": "default_splits",
      "name": "default_splits",
      "description": "Splits for the default config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "default_splits\/split_name": "train"
        },
        {
          "default_splits\/split_name": "validation"
        },
        {
          "default_splits\/split_name": "test"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "default",
      "name": "default",
      "description": "saier\/unarXive_imrad_clf - 'default' subset\n\nAdditional information:\n- 3 splits: train, validation, test",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default\/split",
          "name": "default\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "default\/(?:partial-)?(train|validation|test)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "default_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/_id",
          "name": "default\/_id",
          "description": "Column '_id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "_id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/text",
          "name": "default\/text",
          "description": "Column 'text' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "text"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/label",
          "name": "default\/label",
          "description": "Column 'label' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "label"
            }
          }
        }
      ]
    }
  ],
  "conformsTo": "http:\/\/mlcommons.org\/croissant\/1.0",
  "name": "unarXive_imrad_clf",
  "description": "\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for unarXive IMRaD classification\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe unarXive IMRaD classification dataset contains 530k paragraphs from computer science papers and the IMRaD section they originate from. The paragraphs are derived from unarXive.\nThe dataset can be used as follows.\nfrom datasets import load_dataset\n\nimrad_data = load_dataset('saier\/unarXive_imrad_clf')\nimrad_data = imrad_data.class_encode_column('label')  # assign target label column\nimrad_data =â€¦ See the full description on the dataset page: https:\/\/huggingface.co\/datasets\/saier\/unarXive_imrad_clf.",
  "alternateName": [
    "saier\/unarXive_imrad_clf",
    "unarXive IMRaD classification"
  ],
  "creator": {
    "@type": "Person",
    "name": "Tarek Saier",
    "url": "https:\/\/huggingface.co\/saier"
  },
  "keywords": [
    "text-classification",
    "multi-class-classification",
    "machine-generated",
    "found",
    "monolingual",
    "extended|10.5281\/zenodo.7752615",
    "English",
    "cc-by-sa-4.0",
    "100K - 1M",
    "json",
    "Text",
    "Datasets",
    "pandas",
    "Croissant",
    "Polars",
    "arxiv:2303.14957",
    "ðŸ‡ºðŸ‡¸ Region: US",
    "arXiv.org",
    "arXiv",
    "IMRaD",
    "publication",
    "paper",
    "preprint",
    "section",
    "physics",
    "mathematics",
    "computer science",
    "cs"
  ],
  "license": "https:\/\/choosealicense.com\/licenses\/cc-by-sa-4.0\/",
  "sameAs": "https:\/\/github.com\/IllDepence\/unarXive",
  "url": "https:\/\/huggingface.co\/datasets\/saier\/unarXive_imrad_clf"
}</script> 

		<title>saier/unarXive_imrad_clf Â· Datasets at Hugging Face</title>

		<script
			defer
			data-domain="huggingface.co"
			event-loggedIn="false"
			src="/js/script.pageview-props.js"
		></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script>
			window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer></script>
	</head>
	<body class="flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DatasetPage">
		<div class="flex min-h-dvh flex-col"><div class="SVELTE_HYDRATER contents" data-target="SystemThemeMonitor" data-props="{&quot;isLoggedIn&quot;:false}"></div>

	<div class="SVELTE_HYDRATER contents" data-target="MainHeader" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false,&quot;isPro&quot;:false}"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg">
				<span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a>
			<div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..."   spellcheck="false" type="text" value="">
	<svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
	</div>
			<div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg>
		</button>

	</div></div>
		<nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 2xl:space-x-2"><li class="hover:text-indigo-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
					Models</a>
			</li><li class="hover:text-red-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					Datasets</a>
			</li><li class="hover:text-blue-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg>
					Spaces</a>
			</li><li class="hover:text-yellow-700 max-xl:hidden"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 text-yellow-500!" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg>
					Posts</a>
			</li><li class="hover:text-yellow-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 16 16"><path d="m2.28 3.7-.3.16a.67.67 0 0 0-.34.58v8.73l.01.04.02.07.01.04.03.06.02.04.02.03.04.06.05.05.04.04.06.04.06.04.08.04.08.02h.05l.07.02h.11l.04-.01.07-.02.03-.01.07-.03.22-.12a5.33 5.33 0 0 1 5.15.1.67.67 0 0 0 .66 0 5.33 5.33 0 0 1 5.33 0 .67.67 0 0 0 1-.58V4.36a.67.67 0 0 0-.34-.5l-.3-.17v7.78a.63.63 0 0 1-.87.59 4.9 4.9 0 0 0-4.35.35l-.65.39a.29.29 0 0 1-.15.04.29.29 0 0 1-.16-.04l-.65-.4a4.9 4.9 0 0 0-4.34-.34.63.63 0 0 1-.87-.59V3.7Z" fill="currentColor" class="dark:opacity-40"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M8 3.1a5.99 5.99 0 0 0-5.3-.43.66.66 0 0 0-.42.62v8.18c0 .45.46.76.87.59a4.9 4.9 0 0 1 4.34.35l.65.39c.05.03.1.04.16.04.05 0 .1-.01.15-.04l.65-.4a4.9 4.9 0 0 1 4.35-.34.63.63 0 0 0 .86-.59V3.3a.67.67 0 0 0-.41-.62 5.99 5.99 0 0 0-5.3.43l-.3.17L8 3.1Zm.73 1.87a.43.43 0 1 0-.86 0v5.48a.43.43 0 0 0 .86 0V4.97Z" fill="currentColor" class="opacity-40 dark:opacity-100"></path><path d="M8.73 4.97a.43.43 0 1 0-.86 0v5.48a.43.43 0 1 0 .86 0V4.96Z" fill="currentColor" class="dark:opacity-40"></path></svg>
					Docs</a>
			</li><li class="hover:text-black dark:hover:text-white"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/enterprise"><svg class="mr-1.5 text-gray-400 group-hover:text-black dark:group-hover:text-white" xmlns="http://www.w3.org/2000/svg" fill="none" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 33 27"><path fill="currentColor" fill-rule="evenodd" d="M13.5.7a8.7 8.7 0 0 0-7.7 5.7L1 20.6c-1 3.1.9 5.7 4.1 5.7h15c3.3 0 6.8-2.6 7.8-5.7l4.6-14.2c1-3.1-.8-5.7-4-5.7h-15Zm1.1 5.7L9.8 20.3h9.8l1-3.1h-5.8l.8-2.5h4.8l1.1-3h-4.8l.8-2.3H23l1-3h-9.5Z" clip-rule="evenodd"></path></svg>
					Enterprise</a>
			</li>

		<li><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/pricing">Pricing
			</a></li>

		<li><div class="relative group">
	<button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button">
		<svg class=" text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>
			
		</button>
	
	
	</div></li>
		<li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li>
		<li><a class="block cursor-pointer whitespace-nowrap px-2 py-0.5 hover:text-gray-500 dark:text-gray-300 dark:hover:text-gray-100" href="/login">Log In
				</a></li>
			<li><a class="whitespace-nowrap rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up
					</a></li></ul></nav></div></header></div>
	
	
	
	<div class="SVELTE_HYDRATER contents" data-target="SSOBanner" data-props="{}"></div>
	
	

	<main class="flex flex-1 flex-col">
	<div class="SVELTE_HYDRATER contents" data-target="DatasetHeader" data-props="{&quot;activeTab&quot;:&quot;datasetCard&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63c56270cf44a436ae49d26c&quot;,&quot;avatarUrl&quot;:&quot;/avatars/57faf531eab7c992d4273747c8691abc.svg&quot;,&quot;fullname&quot;:&quot;Tarek Saier&quot;,&quot;name&quot;:&quot;saier&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;canReadRepoSettings&quot;:false,&quot;dataset&quot;:{&quot;author&quot;:&quot;saier&quot;,&quot;cardData&quot;:{&quot;annotations_creators&quot;:[&quot;machine-generated&quot;],&quot;language&quot;:[&quot;en&quot;],&quot;language_creators&quot;:[&quot;found&quot;],&quot;license&quot;:[&quot;cc-by-sa-4.0&quot;],&quot;multilinguality&quot;:[&quot;monolingual&quot;],&quot;pretty_name&quot;:&quot;unarXive IMRaD classification&quot;,&quot;size_categories&quot;:[&quot;100K<n<1M&quot;],&quot;tags&quot;:[&quot;arXiv.org&quot;,&quot;arXiv&quot;,&quot;IMRaD&quot;,&quot;publication&quot;,&quot;paper&quot;,&quot;preprint&quot;,&quot;section&quot;,&quot;physics&quot;,&quot;mathematics&quot;,&quot;computer science&quot;,&quot;cs&quot;],&quot;task_categories&quot;:[&quot;text-classification&quot;],&quot;task_ids&quot;:[&quot;multi-class-classification&quot;],&quot;source_datasets&quot;:[&quot;extended|10.5281/zenodo.7752615&quot;],&quot;dataset_info&quot;:{&quot;features&quot;:[{&quot;name&quot;:&quot;_id&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;text&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;label&quot;,&quot;dtype&quot;:&quot;string&quot;}],&quot;splits&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;num_bytes&quot;:451908280,&quot;num_examples&quot;:520053},{&quot;name&quot;:&quot;test&quot;,&quot;num_bytes&quot;:4650429,&quot;num_examples&quot;:5000},{&quot;name&quot;:&quot;validation&quot;,&quot;num_bytes&quot;:4315597,&quot;num_examples&quot;:5001}],&quot;download_size&quot;:482376743,&quot;dataset_size&quot;:460874306}},&quot;cardExists&quot;:true,&quot;createdAt&quot;:&quot;2023-03-24T11:30:56.000Z&quot;,&quot;description&quot;:&quot;\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for unarXive IMRaD classification\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe unarXive IMRaD classification dataset contains 530k paragraphs from computer science papers and the IMRaD section they originate from. The paragraphs are derived from unarXive.\nThe dataset can be used as follows.\nfrom datasets import load_dataset\n\nimrad_data = load_dataset('saier/unarXive_imrad_clf')\nimrad_data = imrad_data.class_encode_column('label')  # assign target label column\nimrad_data =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/saier/unarXive_imrad_clf.&quot;,&quot;downloads&quot;:223,&quot;downloadsAllTime&quot;:4593,&quot;id&quot;:&quot;saier/unarXive_imrad_clf&quot;,&quot;isLikedByUser&quot;:false,&quot;lastModified&quot;:&quot;2023-04-02T00:56:43.000Z&quot;,&quot;likes&quot;:7,&quot;datasetsServerInfo&quot;:{&quot;viewer&quot;:&quot;viewer&quot;,&quot;numRows&quot;:530054,&quot;libraries&quot;:[&quot;datasets&quot;,&quot;pandas&quot;,&quot;mlcroissant&quot;,&quot;polars&quot;],&quot;formats&quot;:[&quot;json&quot;],&quot;modalities&quot;:[&quot;text&quot;]},&quot;discussionsDisabled&quot;:false,&quot;repoType&quot;:&quot;dataset&quot;,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;tags&quot;:[&quot;task_categories:text-classification&quot;,&quot;task_ids:multi-class-classification&quot;,&quot;annotations_creators:machine-generated&quot;,&quot;language_creators:found&quot;,&quot;multilinguality:monolingual&quot;,&quot;source_datasets:extended|10.5281/zenodo.7752615&quot;,&quot;language:en&quot;,&quot;license:cc-by-sa-4.0&quot;,&quot;size_categories:100K<n<1M&quot;,&quot;format:json&quot;,&quot;modality:text&quot;,&quot;library:datasets&quot;,&quot;library:pandas&quot;,&quot;library:mlcroissant&quot;,&quot;library:polars&quot;,&quot;arxiv:2303.14957&quot;,&quot;region:us&quot;,&quot;arXiv.org&quot;,&quot;arXiv&quot;,&quot;IMRaD&quot;,&quot;publication&quot;,&quot;paper&quot;,&quot;preprint&quot;,&quot;section&quot;,&quot;physics&quot;,&quot;mathematics&quot;,&quot;computer science&quot;,&quot;cs&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;task_categories:text-classification&quot;,&quot;label&quot;:&quot;text-classification&quot;,&quot;type&quot;:&quot;task_categories&quot;,&quot;subType&quot;:&quot;nlp&quot;},{&quot;id&quot;:&quot;task_ids:multi-class-classification&quot;,&quot;label&quot;:&quot;multi-class-classification&quot;,&quot;type&quot;:&quot;task_ids&quot;},{&quot;id&quot;:&quot;annotations_creators:machine-generated&quot;,&quot;label&quot;:&quot;machine-generated&quot;,&quot;type&quot;:&quot;annotations_creators&quot;},{&quot;id&quot;:&quot;language_creators:found&quot;,&quot;label&quot;:&quot;found&quot;,&quot;type&quot;:&quot;language_creators&quot;},{&quot;id&quot;:&quot;multilinguality:monolingual&quot;,&quot;label&quot;:&quot;monolingual&quot;,&quot;type&quot;:&quot;multilinguality&quot;},{&quot;id&quot;:&quot;source_datasets:extended|10.5281/zenodo.7752615&quot;,&quot;label&quot;:&quot;extended|10.5281/zenodo.7752615&quot;,&quot;type&quot;:&quot;source_datasets&quot;},{&quot;id&quot;:&quot;language:en&quot;,&quot;label&quot;:&quot;English&quot;,&quot;type&quot;:&quot;language&quot;},{&quot;id&quot;:&quot;license:cc-by-sa-4.0&quot;,&quot;label&quot;:&quot;cc-by-sa-4.0&quot;,&quot;type&quot;:&quot;license&quot;},{&quot;id&quot;:&quot;size_categories:100K<n<1M&quot;,&quot;label&quot;:&quot;100K - 1M&quot;,&quot;type&quot;:&quot;size_categories&quot;},{&quot;id&quot;:&quot;format:json&quot;,&quot;label&quot;:&quot;json&quot;,&quot;type&quot;:&quot;format&quot;},{&quot;id&quot;:&quot;modality:text&quot;,&quot;label&quot;:&quot;Text&quot;,&quot;type&quot;:&quot;modality&quot;},{&quot;id&quot;:&quot;library:datasets&quot;,&quot;label&quot;:&quot;Datasets&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:pandas&quot;,&quot;label&quot;:&quot;pandas&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:mlcroissant&quot;,&quot;label&quot;:&quot;Croissant&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:polars&quot;,&quot;label&quot;:&quot;Polars&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;arxiv:2303.14957&quot;,&quot;label&quot;:&quot;arxiv:2303.14957&quot;,&quot;type&quot;:&quot;arxiv&quot;,&quot;extra&quot;:{&quot;paperTitle&quot;:&quot;unarXive 2022: All arXiv Publications Pre-Processed for NLP, Including\n  Structured Full-Text and Citation Network&quot;}},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;ðŸ‡ºðŸ‡¸ Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;},{&quot;id&quot;:&quot;arXiv.org&quot;,&quot;label&quot;:&quot;arXiv.org&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;arXiv&quot;,&quot;label&quot;:&quot;arXiv&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;IMRaD&quot;,&quot;label&quot;:&quot;IMRaD&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;publication&quot;,&quot;label&quot;:&quot;publication&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;paper&quot;,&quot;label&quot;:&quot;paper&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;preprint&quot;,&quot;label&quot;:&quot;preprint&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;section&quot;,&quot;label&quot;:&quot;section&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;physics&quot;,&quot;label&quot;:&quot;physics&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;mathematics&quot;,&quot;label&quot;:&quot;mathematics&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;computer science&quot;,&quot;label&quot;:&quot;computer science&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;cs&quot;,&quot;label&quot;:&quot;cs&quot;,&quot;type&quot;:&quot;other&quot;}],&quot;homepage&quot;:&quot;https://github.com/IllDepence/unarXive&quot;,&quot;hasBlockedOids&quot;:false,&quot;region&quot;:&quot;us&quot;,&quot;xetEnabled&quot;:false},&quot;discussionsStats&quot;:{&quot;closed&quot;:0,&quot;open&quot;:0,&quot;total&quot;:0}}"><header class="bg-linear-to-t border-b border-gray-100 pt-6 sm:pt-9 from-gray-50-to-white via-white dark:via-gray-950"><div class="container relative "><h1 class="flex flex-wrap items-center max-md:leading-tight mb-3 text-lg max-sm:gap-y-1.5 md:text-xl"><a href="/datasets" class="group flex items-center"><svg class="sm:mr-1.5 -mr-1 text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					<span class="mr-2.5 font-semibold text-gray-400 group-hover:text-gray-500 max-sm:hidden">Datasets:</span></a>
				<hr class="mx-1.5 h-2 translate-y-px rounded-sm border-r dark:border-gray-600 sm:hidden">
			<div class="group flex flex-none items-center"><div class="relative mr-1 flex items-center">

			

<span class="inline-block "><span class="contents"><a href="/saier" class="text-gray-400 hover:text-blue-600"><img alt="" class="w-3.5 h-3.5 rounded-full  flex-none" src="/avatars/57faf531eab7c992d4273747c8691abc.svg" crossorigin="anonymous"></a></span>
	</span></div>
		

<span class="inline-block "><span class="contents"><a href="/saier" class="text-gray-400 hover:text-blue-600">saier</a></span>
	</span>
		<div class="mx-0.5 text-gray-300">/</div></div>

<div class="max-w-full "><a class="break-words font-mono font-semibold hover:text-blue-600 " href="/datasets/saier/unarXive_imrad_clf">unarXive_imrad_clf</a>
	<button class="relative text-sm mr-4 focus:outline-hidden inline-flex cursor-pointer items-center text-sm  mx-0.5   text-gray-600 " title="Copy dataset name to clipboard" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg>
	
	</button></div>
			<div class="inline-flex items-center overflow-hidden whitespace-nowrap rounded-md border bg-white text-sm leading-none text-gray-500  mr-2"><button class="relative flex items-center overflow-hidden from-red-50 to-transparent dark:from-red-900 px-1.5 py-1 hover:bg-linear-to-t focus:outline-hidden"  title="Like"><svg class="left-1.5 absolute" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>

		
		<span class="ml-4 pl-0.5 ">like</span></button>
	<button class="focus:outline-hidden flex items-center border-l px-1.5 py-1 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="See users who liked this repository">7</button></div>




			
			
	</h1>
		<div class="mb-3 flex flex-wrap md:mb-4"><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Tasks:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?task_categories=task_categories%3Atext-classification"><div class="tag tag-white   "><div class="tag-ico -ml-2 tag-ico-orange"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><circle cx="10" cy="20" r="2" fill="currentColor"></circle><circle cx="10" cy="28" r="2" fill="currentColor"></circle><circle cx="10" cy="14" r="2" fill="currentColor"></circle><circle cx="28" cy="4" r="2" fill="currentColor"></circle><circle cx="22" cy="6" r="2" fill="currentColor"></circle><circle cx="28" cy="10" r="2" fill="currentColor"></circle><circle cx="20" cy="12" r="2" fill="currentColor"></circle><circle cx="28" cy="22" r="2" fill="currentColor"></circle><circle cx="26" cy="28" r="2" fill="currentColor"></circle><circle cx="20" cy="26" r="2" fill="currentColor"></circle><circle cx="22" cy="20" r="2" fill="currentColor"></circle><circle cx="16" cy="4" r="2" fill="currentColor"></circle><circle cx="4" cy="24" r="2" fill="currentColor"></circle><circle cx="4" cy="16" r="2" fill="currentColor"></circle></svg></div>

	

	<span>Text Classification</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Modalities:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?modality=modality%3Atext"><div class="tag tag-white   ">
		<svg class="text-red-700 dark:text-red-600" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.619 4.619C2.667 6.573 2.667 9.715 2.667 16s0 9.428 1.952 11.38C6.573 29.333 9.715 29.333 16 29.333s9.428 0 11.38-1.953c1.953-1.95 1.953-5.095 1.953-11.38s0-9.428-1.953-11.381C25.43 2.667 22.285 2.667 16 2.667s-9.428 0-11.381 1.952m8.65 3.714c-.573 0-1.109 0-1.546.066-.495.073-1.003.248-1.41.7-.392.436-.53.956-.59 1.452-.056.464-.056 1.04-.056 1.689V13a1 1 0 1 0 2 0v-.704c0-.724.001-1.176.041-1.505q.015-.15.061-.294a.2.2 0 0 1 .031-.061q0-.003.016-.01a.8.8 0 0 1 .203-.05c.272-.04.654-.043 1.314-.043H15v11.334h-2.333a1 1 0 1 0 0 2H20a1 1 0 0 0 0-2h-3V10.333h1.667c.66 0 1.042.003 1.314.043.123.019.18.04.203.05l.015.009a.2.2 0 0 1 .032.061c.018.05.042.14.061.295.04.329.041.781.041 1.506V13a1 1 0 1 0 2 0v-.76c0-.65 0-1.225-.056-1.69-.06-.495-.198-1.015-.59-1.453-.407-.45-.915-.625-1.41-.698-.437-.067-.973-.067-1.546-.066z" fill="currentColor"></path></svg>

	

	<span>Text</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Formats:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?format=format%3Ajson"><div class="tag tag-white   ">
		<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.917 2.25h-.834v.833h.834v2.084A.833.833 0 0 0 9.75 6a.833.833 0 0 0-.833.833v2.084h-.834v.833h.834c.446-.113.833-.375.833-.833V7.25a.833.833 0 0 1 .833-.833H11v-.834h-.417a.833.833 0 0 1-.833-.833V3.083a.833.833 0 0 0-.833-.833Zm-5.834 0a.833.833 0 0 0-.833.833V4.75a.833.833 0 0 1-.833.833H1v.834h.417a.833.833 0 0 1 .833.833v1.667a.833.833 0 0 0 .833.833h.834v-.833h-.834V6.833A.833.833 0 0 0 2.25 6a.833.833 0 0 0 .833-.833V3.083h.834V2.25h-.834ZM6 7.25a.417.417 0 1 0 0 .833.417.417 0 0 0 0-.833Zm1.667 0a.417.417 0 1 0 0 .833.417.417 0 0 0 0-.833Zm-3.334 0a.417.417 0 1 0 0 .833.417.417 0 0 0 0-.833Z" fill="currentColor"></path></svg>

	

	<span>json</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Sub-tasks:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?task_ids=task_ids%3Amulti-class-classification"><div class="tag tag-white   ">

	

	<span>multi-class-classification</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Languages:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?language=language%3Aen"><div class="tag tag-white   ">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="text-green-600/80" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 10 10"><path fill-rule="evenodd" clip-rule="evenodd" d="M0.625 5C0.625 6.16032 1.08594 7.27312 1.90641 8.09359C2.72688 8.91406 3.83968 9.375 5 9.375C6.16032 9.375 7.27312 8.91406 8.09359 8.09359C8.91406 7.27312 9.375 6.16032 9.375 5C9.375 3.83968 8.91406 2.72688 8.09359 1.90641C7.27312 1.08594 6.16032 0.625 5 0.625C3.83968 0.625 2.72688 1.08594 1.90641 1.90641C1.08594 2.72688 0.625 3.83968 0.625 5ZM7.64365 7.48027C7.61734 7.50832 7.59054 7.53598 7.56326 7.56326C7.13828 7.98824 6.61864 8.2968 6.0539 8.46842C6.29802 8.11949 6.49498 7.64804 6.63475 7.09483C7.00845 7.18834 7.35014 7.3187 7.64365 7.48027ZM8.10076 6.87776C8.37677 6.42196 8.55005 5.90894 8.60556 5.37499H6.86808C6.85542 5.71597 6.82551 6.04557 6.77971 6.35841C7.25309 6.47355 7.68808 6.6414 8.062 6.85549C8.07497 6.86283 8.08789 6.87025 8.10076 6.87776ZM6.03795 6.22536C6.07708 5.95737 6.1044 5.67232 6.11705 5.37499H3.88295C3.89666 5.69742 3.92764 6.00542 3.9722 6.29287C4.37075 6.21726 4.79213 6.17749 5.224 6.17749C5.50054 6.17749 5.77294 6.19376 6.03795 6.22536ZM4.1261 7.02673C4.34894 7.84835 4.68681 8.375 5 8.375C5.32122 8.375 5.66839 7.82101 5.8908 6.963C5.67389 6.93928 5.45082 6.92699 5.224 6.92699C4.84316 6.92699 4.47332 6.96176 4.1261 7.02673ZM3.39783 7.21853C3.53498 7.71842 3.72038 8.14579 3.9461 8.46842C3.42141 8.30898 2.93566 8.03132 2.52857 7.65192C2.77253 7.48017 3.06711 7.33382 3.39783 7.21853ZM3.23916 6.48077C3.18263 6.13193 3.14625 5.76074 3.13192 5.37499H1.39444C1.4585 5.99112 1.67936 6.57938 2.03393 7.08403C2.3706 6.83531 2.78055 6.63162 3.23916 6.48077ZM1.39444 4.62499H3.13192C3.14615 4.24204 3.18211 3.87344 3.23794 3.52681C2.77814 3.37545 2.36731 3.17096 2.03024 2.92123C1.67783 3.42469 1.45828 4.011 1.39444 4.62499ZM2.5237 2.35262C2.76812 2.52552 3.06373 2.67281 3.39584 2.78875C3.53318 2.28573 3.71928 1.85578 3.9461 1.53158C3.41932 1.69166 2.93178 1.97089 2.5237 2.35262ZM3.97101 3.71489C3.92709 4.00012 3.89654 4.30547 3.88295 4.62499H6.11705C6.10453 4.33057 6.07761 4.04818 6.03909 3.78248C5.77372 3.81417 5.50093 3.83049 5.224 3.83049C4.79169 3.83049 4.3699 3.79065 3.97101 3.71489ZM5.8928 3.04476C5.67527 3.06863 5.45151 3.08099 5.224 3.08099C4.84241 3.08099 4.47186 3.04609 4.12405 2.98086C4.34686 2.1549 4.68584 1.625 5 1.625C5.32218 1.625 5.67048 2.18233 5.8928 3.04476ZM6.78083 3.6493C6.826 3.95984 6.85552 4.28682 6.86808 4.62499H8.60556C8.55029 4.09337 8.37827 3.58251 8.10436 3.1282C8.0903 3.1364 8.07618 3.14449 8.062 3.15249C7.68838 3.36641 7.25378 3.53417 6.78083 3.6493ZM7.64858 2.52499C7.35446 2.68754 7.0117 2.81868 6.63664 2.91268C6.49676 2.35623 6.29913 1.88209 6.0539 1.53158C6.61864 1.7032 7.13828 2.01176 7.56326 2.43674C7.59224 2.46572 7.62068 2.49514 7.64858 2.52499Z" fill="currentColor"></path></svg>

	

	<span>English</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Size:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?size_categories=size_categories%3A100K%3Cn%3C1M"><div class="tag tag-white   ">

	

	<span>100K - 1M</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">ArXiv:
	</span>
	

	<div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-full rounded-br-none " type="button">
		<div class="tag tag-white rounded-full  relative rounded-br-none pr-2.5">
		<svg class="-mr-1 text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet" fill="none"><path fill="currentColor" fill-rule="evenodd" d="M8.007 1.814a1.176 1.176 0 0 0-.732-.266H3.088c-.64 0-1.153.512-1.153 1.152v6.803c0 .64.513 1.152 1.153 1.152h5.54c.632 0 1.144-.511 1.144-1.152V3.816c0-.338-.137-.658-.412-.887L8.007 1.814Zm-1.875 1.81c0 .695.55 1.253 1.244 1.253h.983a.567.567 0 0 1 .553.585v4.041c0 .165-.119.302-.283.302h-5.55c-.156 0-.275-.137-.275-.302V2.7a.284.284 0 0 1 .284-.301h2.468a.574.574 0 0 1 .434.19.567.567 0 0 1 .142.395v.64Z" clip-rule="evenodd"></path><path fill="currentColor" fill-opacity=".2" fill-rule="evenodd" d="M6.132 3.624c0 .695.55 1.253 1.244 1.253h.97a.567.567 0 0 1 .566.585v4.041c0 .165-.119.302-.283.302h-5.55c-.156 0-.275-.137-.275-.302V2.7a.284.284 0 0 1 .284-.301h2.468a.567.567 0 0 1 .576.585v.64Z" clip-rule="evenodd"></path></svg>

	

	<span class="-mr-2 text-gray-400">arxiv:</span>
		<span>2303.14957</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Tags:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?other=arXiv.org"><div class="tag tag-white   ">

	

	<span>arXiv.org</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?other=arXiv"><div class="tag tag-white   ">

	

	<span>arXiv</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?other=IMRaD"><div class="tag tag-white   ">

	

	<span>IMRaD</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?other=publication"><div class="tag tag-white   ">

	

	<span>publication</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?other=paper"><div class="tag tag-white   ">

	

	<span>paper</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?other=preprint"><div class="tag tag-white   ">

	

	<span>preprint</span>
	

	</div></a>

	<button class="tag tag-ghost px-1! -ml-0.5 mb-1 md:mb-1.5" type="button">+ 5</button></div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Libraries:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Adatasets"><div class="tag tag-white   "><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 95 88"><path fill="#fff" d="M94.25 70.08a8.28 8.28 0 0 1-.43 6.46 10.57 10.57 0 0 1-3 3.6 25.18 25.18 0 0 1-5.7 3.2 65.74 65.74 0 0 1-7.56 2.65 46.67 46.67 0 0 1-11.42 1.68c-5.42.05-10.09-1.23-13.4-4.5a40.4 40.4 0 0 1-10.14.03c-3.34 3.25-7.99 4.52-13.39 4.47a46.82 46.82 0 0 1-11.43-1.68 66.37 66.37 0 0 1-7.55-2.65c-2.28-.98-4.17-2-5.68-3.2a10.5 10.5 0 0 1-3.02-3.6c-.99-2-1.18-4.3-.42-6.46a8.54 8.54 0 0 1-.33-5.63c.25-.95.66-1.83 1.18-2.61a8.67 8.67 0 0 1 2.1-8.47 8.23 8.23 0 0 1 2.82-2.07 41.75 41.75 0 1 1 81.3-.12 8.27 8.27 0 0 1 3.11 2.19 8.7 8.7 0 0 1 2.1 8.47c.52.78.93 1.66 1.18 2.61a8.61 8.61 0 0 1-.32 5.63Z"></path><path fill="#FFD21E" d="M47.21 76.5a34.75 34.75 0 1 0 0-69.5 34.75 34.75 0 0 0 0 69.5Z"></path><path fill="#FF9D0B" d="M81.96 41.75a34.75 34.75 0 1 0-69.5 0 34.75 34.75 0 0 0 69.5 0Zm-73.5 0a38.75 38.75 0 1 1 77.5 0 38.75 38.75 0 0 1-77.5 0Z"></path><path fill="#3A3B45" d="M58.5 32.3c1.28.44 1.78 3.06 3.07 2.38a5 5 0 1 0-6.76-2.07c.61 1.15 2.55-.72 3.7-.32ZM34.95 32.3c-1.28.44-1.79 3.06-3.07 2.38a5 5 0 1 1 6.76-2.07c-.61 1.15-2.56-.72-3.7-.32Z"></path><path fill="#FF323D" d="M46.96 56.29c9.83 0 13-8.76 13-13.26 0-2.34-1.57-1.6-4.09-.36-2.33 1.15-5.46 2.74-8.9 2.74-7.19 0-13-6.88-13-2.38s3.16 13.26 13 13.26Z"></path><path fill="#3A3B45" fill-rule="evenodd" d="M39.43 54a8.7 8.7 0 0 1 5.3-4.49c.4-.12.81.57 1.24 1.28.4.68.82 1.37 1.24 1.37.45 0 .9-.68 1.33-1.35.45-.7.89-1.38 1.32-1.25a8.61 8.61 0 0 1 5 4.17c3.73-2.94 5.1-7.74 5.1-10.7 0-2.34-1.57-1.6-4.09-.36l-.14.07c-2.31 1.15-5.39 2.67-8.77 2.67s-6.45-1.52-8.77-2.67c-2.6-1.29-4.23-2.1-4.23.29 0 3.05 1.46 8.06 5.47 10.97Z" clip-rule="evenodd"></path><path fill="#FF9D0B" d="M70.71 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM24.21 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM17.52 48c-1.62 0-3.06.66-4.07 1.87a5.97 5.97 0 0 0-1.33 3.76 7.1 7.1 0 0 0-1.94-.3c-1.55 0-2.95.59-3.94 1.66a5.8 5.8 0 0 0-.8 7 5.3 5.3 0 0 0-1.79 2.82c-.24.9-.48 2.8.8 4.74a5.22 5.22 0 0 0-.37 5.02c1.02 2.32 3.57 4.14 8.52 6.1 3.07 1.22 5.89 2 5.91 2.01a44.33 44.33 0 0 0 10.93 1.6c5.86 0 10.05-1.8 12.46-5.34 3.88-5.69 3.33-10.9-1.7-15.92-2.77-2.78-4.62-6.87-5-7.77-.78-2.66-2.84-5.62-6.25-5.62a5.7 5.7 0 0 0-4.6 2.46c-1-1.26-1.98-2.25-2.86-2.82A7.4 7.4 0 0 0 17.52 48Zm0 4c.51 0 1.14.22 1.82.65 2.14 1.36 6.25 8.43 7.76 11.18.5.92 1.37 1.31 2.14 1.31 1.55 0 2.75-1.53.15-3.48-3.92-2.93-2.55-7.72-.68-8.01.08-.02.17-.02.24-.02 1.7 0 2.45 2.93 2.45 2.93s2.2 5.52 5.98 9.3c3.77 3.77 3.97 6.8 1.22 10.83-1.88 2.75-5.47 3.58-9.16 3.58-3.81 0-7.73-.9-9.92-1.46-.11-.03-13.45-3.8-11.76-7 .28-.54.75-.76 1.34-.76 2.38 0 6.7 3.54 8.57 3.54.41 0 .7-.17.83-.6.79-2.85-12.06-4.05-10.98-8.17.2-.73.71-1.02 1.44-1.02 3.14 0 10.2 5.53 11.68 5.53.11 0 .2-.03.24-.1.74-1.2.33-2.04-4.9-5.2-5.21-3.16-8.88-5.06-6.8-7.33.24-.26.58-.38 1-.38 3.17 0 10.66 6.82 10.66 6.82s2.02 2.1 3.25 2.1c.28 0 .52-.1.68-.38.86-1.46-8.06-8.22-8.56-11.01-.34-1.9.24-2.85 1.31-2.85Z"></path><path fill="#FFD21E" d="M38.6 76.69c2.75-4.04 2.55-7.07-1.22-10.84-3.78-3.77-5.98-9.3-5.98-9.3s-.82-3.2-2.69-2.9c-1.87.3-3.24 5.08.68 8.01 3.91 2.93-.78 4.92-2.29 2.17-1.5-2.75-5.62-9.82-7.76-11.18-2.13-1.35-3.63-.6-3.13 2.2.5 2.79 9.43 9.55 8.56 11-.87 1.47-3.93-1.71-3.93-1.71s-9.57-8.71-11.66-6.44c-2.08 2.27 1.59 4.17 6.8 7.33 5.23 3.16 5.64 4 4.9 5.2-.75 1.2-12.28-8.53-13.36-4.4-1.08 4.11 11.77 5.3 10.98 8.15-.8 2.85-9.06-5.38-10.74-2.18-1.7 3.21 11.65 6.98 11.76 7.01 4.3 1.12 15.25 3.49 19.08-2.12Z"></path><path fill="#FF9D0B" d="M77.4 48c1.62 0 3.07.66 4.07 1.87a5.97 5.97 0 0 1 1.33 3.76 7.1 7.1 0 0 1 1.95-.3c1.55 0 2.95.59 3.94 1.66a5.8 5.8 0 0 1 .8 7 5.3 5.3 0 0 1 1.78 2.82c.24.9.48 2.8-.8 4.74a5.22 5.22 0 0 1 .37 5.02c-1.02 2.32-3.57 4.14-8.51 6.1-3.08 1.22-5.9 2-5.92 2.01a44.33 44.33 0 0 1-10.93 1.6c-5.86 0-10.05-1.8-12.46-5.34-3.88-5.69-3.33-10.9 1.7-15.92 2.78-2.78 4.63-6.87 5.01-7.77.78-2.66 2.83-5.62 6.24-5.62a5.7 5.7 0 0 1 4.6 2.46c1-1.26 1.98-2.25 2.87-2.82A7.4 7.4 0 0 1 77.4 48Zm0 4c-.51 0-1.13.22-1.82.65-2.13 1.36-6.25 8.43-7.76 11.18a2.43 2.43 0 0 1-2.14 1.31c-1.54 0-2.75-1.53-.14-3.48 3.91-2.93 2.54-7.72.67-8.01a1.54 1.54 0 0 0-.24-.02c-1.7 0-2.45 2.93-2.45 2.93s-2.2 5.52-5.97 9.3c-3.78 3.77-3.98 6.8-1.22 10.83 1.87 2.75 5.47 3.58 9.15 3.58 3.82 0 7.73-.9 9.93-1.46.1-.03 13.45-3.8 11.76-7-.29-.54-.75-.76-1.34-.76-2.38 0-6.71 3.54-8.57 3.54-.42 0-.71-.17-.83-.6-.8-2.85 12.05-4.05 10.97-8.17-.19-.73-.7-1.02-1.44-1.02-3.14 0-10.2 5.53-11.68 5.53-.1 0-.19-.03-.23-.1-.74-1.2-.34-2.04 4.88-5.2 5.23-3.16 8.9-5.06 6.8-7.33-.23-.26-.57-.38-.98-.38-3.18 0-10.67 6.82-10.67 6.82s-2.02 2.1-3.24 2.1a.74.74 0 0 1-.68-.38c-.87-1.46 8.05-8.22 8.55-11.01.34-1.9-.24-2.85-1.31-2.85Z"></path><path fill="#FFD21E" d="M56.33 76.69c-2.75-4.04-2.56-7.07 1.22-10.84 3.77-3.77 5.97-9.3 5.97-9.3s.82-3.2 2.7-2.9c1.86.3 3.23 5.08-.68 8.01-3.92 2.93.78 4.92 2.28 2.17 1.51-2.75 5.63-9.82 7.76-11.18 2.13-1.35 3.64-.6 3.13 2.2-.5 2.79-9.42 9.55-8.55 11 .86 1.47 3.92-1.71 3.92-1.71s9.58-8.71 11.66-6.44c2.08 2.27-1.58 4.17-6.8 7.33-5.23 3.16-5.63 4-4.9 5.2.75 1.2 12.28-8.53 13.36-4.4 1.08 4.11-11.76 5.3-10.97 8.15.8 2.85 9.05-5.38 10.74-2.18 1.69 3.21-11.65 6.98-11.76 7.01-4.31 1.12-15.26 3.49-19.08-2.12Z"></path></svg>

	

	<span>Datasets</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Apandas"><div class="tag tag-white   "><svg class="text-black inline-block text-sm text-[#130754] dark:text-gray-200" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid slice" viewBox="0 0 210.21 280.43"><rect fill="currentColor" x="74.51" y="43.03" width="24.09" height="50.02"></rect><rect fill="currentColor" x="74.51" y="145.78" width="24.09" height="50.02"></rect><rect fill="#ffca00" x="74.51" y="107.65" width="24.09" height="23.6"></rect><rect fill="currentColor" x="35.81" y="84.15" width="24.09" height="166.27"></rect><rect fill="currentColor" x="112.41" y="187.05" width="24.09" height="50.02"></rect><rect fill="currentColor" x="112.41" y="84.21" width="24.09" height="50.02"></rect><rect fill="#e70488" x="112.41" y="148.84" width="24.09" height="23.6"></rect><rect fill="currentColor" x="150.3" y="30" width="24.09" height="166.27"></rect></svg>

	

	<span>pandas</span>
	

	</div></a><div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-lg rounded-br-none " type="button">
		<div class="tag tag-white   relative rounded-br-none pr-2.5"><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="none" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="#F5AB6A"></path><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="url(#paint0_radial_18_31665)"></path><g filter="url(#filter0_f_18_31665)"><path d="M22.2849 12.1817L23.4375 13.2656L24.4375 4.70312C23.5121 4.1242 23.0198 3.96369 22.6563 3.89062L22.2849 12.1817Z" fill="url(#paint1_linear_18_31665)"></path></g><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint2_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint3_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint4_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint5_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint6_linear_18_31665)"></path><g filter="url(#filter1_f_18_31665)"><path d="M13.1016 2.72656C11.862 3.06924 11.5298 3.40016 11.5298 3.40016C10.9102 3.52335 10.936 4.11525 11.2408 4.6799C13.1487 6.95202 15.1361 13.2496 18.007 14.0958C18.2707 14.1633 18.6953 14.2107 19.1797 14.2344L13.1016 2.72656Z" fill="url(#paint7_linear_18_31665)"></path></g><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint8_radial_18_31665)"></path><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint9_radial_18_31665)"></path><g filter="url(#filter2_f_18_31665)"><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint10_linear_18_31665)"></path><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint11_radial_18_31665)"></path></g><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="#EC9F6A"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint12_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint13_linear_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint14_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint15_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint16_radial_18_31665)"></path><g filter="url(#filter3_f_18_31665)"><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint17_linear_18_31665)"></path><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint18_linear_18_31665)"></path></g><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="#D79453"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint19_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint20_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint21_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint22_linear_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint23_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint24_radial_18_31665)"></path><defs><filter id="filter0_f_18_31665" x="22.0349" y="3.64062" width="2.65265" height="9.875" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter1_f_18_31665" x="10.7815" y="2.47656" width="8.64819" height="12.0078" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter2_f_18_31665" x="3.89062" y="22.5416" width="9.57812" height="2.2804" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter3_f_18_31665" x="2.45312" y="11.2538" width="12.1255" height="8.50903" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><radialGradient id="paint0_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.8125 12.9375) rotate(42.7741) scale(12.5164 7.08839)"><stop offset="0.0937591" stop-color="#C05159"></stop><stop offset="0.553697" stop-color="#F6AC6A"></stop><stop offset="0.832916" stop-color="#FFD186"></stop><stop offset="0.916927" stop-color="#FFDC87"></stop></radialGradient><linearGradient id="paint1_linear_18_31665" x1="24.7344" y1="4.67187" x2="20.8594" y2="12.8906" gradientUnits="userSpaceOnUse"><stop stop-color="#EBD67C"></stop><stop offset="0.0655686" stop-color="#FFFFA6"></stop><stop offset="0.530552" stop-color="#F8C281"></stop><stop offset="0.937338" stop-color="#E99E6B"></stop></linearGradient><radialGradient id="paint2_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.9009 13.1847) rotate(-127.648) scale(14.3438 11.7966)"><stop stop-color="#FFBE66"></stop><stop offset="1" stop-color="#E2AE5B"></stop></radialGradient><radialGradient id="paint3_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(18 11.4375) rotate(53.9726) scale(11.9013 4.84018)"><stop stop-color="#D67C63"></stop><stop offset="1" stop-color="#D97D67" stop-opacity="0"></stop></radialGradient><radialGradient id="paint4_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(23 4.1875) rotate(45.7639) scale(3.31486 5.75622)"><stop stop-color="#FFE4A6"></stop><stop offset="0.711285" stop-color="#F8B76F"></stop><stop offset="1" stop-color="#F9B870" stop-opacity="0"></stop></radialGradient><radialGradient id="paint5_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.875 12.4375) rotate(88.9391) scale(3.37558 1.29066)"><stop stop-color="#FFBC67"></stop><stop offset="1" stop-color="#FFBC67" stop-opacity="0"></stop></radialGradient><linearGradient id="paint6_linear_18_31665" x1="20.375" y1="15.6875" x2="20.125" y2="12.7813" gradientUnits="userSpaceOnUse"><stop offset="0.461609" stop-color="#B45077"></stop><stop offset="0.855389" stop-color="#B75077" stop-opacity="0"></stop></linearGradient><linearGradient id="paint7_linear_18_31665" x1="12.9375" y1="2.57056" x2="18.5625" y2="14.3891" gradientUnits="userSpaceOnUse"><stop stop-color="#DDC173"></stop><stop offset="0.485173" stop-color="#D59F65"></stop><stop offset="1" stop-color="#E49966"></stop></linearGradient><radialGradient id="paint8_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.5625 23.5) rotate(109.113) scale(6.68078 10.2578)"><stop offset="0.165756" stop-color="#FFBF7E"></stop><stop offset="0.827674" stop-color="#DF8C6D"></stop><stop offset="1" stop-color="#B05A66"></stop></radialGradient><radialGradient id="paint9_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.1875 26) rotate(41.0652) scale(8.37243 2.03649)"><stop stop-color="#FFD483"></stop><stop offset="1" stop-color="#FFD688" stop-opacity="0"></stop></radialGradient><linearGradient id="paint10_linear_18_31665" x1="3.96063" y1="23.794" x2="13.3748" y2="23.5143" gradientUnits="userSpaceOnUse"><stop stop-color="#A8716F"></stop><stop offset="0.103615" stop-color="#B37173"></stop><stop offset="0.225484" stop-color="#DB9F84"></stop><stop offset="0.799889" stop-color="#F1BB8A"></stop><stop offset="1" stop-color="#FFD780"></stop></linearGradient><radialGradient id="paint11_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4219 23.1719) rotate(-178.616) scale(3.23532 0.569081)"><stop offset="0.621498" stop-color="#AF5A3E"></stop><stop offset="1" stop-color="#B35445" stop-opacity="0"></stop></radialGradient><radialGradient id="paint12_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.625 19.125) rotate(-171.737) scale(15.2205 15.0767)"><stop offset="0.138435" stop-color="#FFB974"></stop><stop offset="0.403618" stop-color="#F2A56D"></stop><stop offset="0.925938" stop-color="#A16948"></stop></radialGradient><linearGradient id="paint13_linear_18_31665" x1="8.22184" y1="13.125" x2="6.81191" y2="15.4996" gradientUnits="userSpaceOnUse"><stop offset="0.610751" stop-color="#984847"></stop><stop offset="0.850075" stop-color="#9A4947" stop-opacity="0"></stop></linearGradient><radialGradient id="paint14_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.25 23.7461) scale(11.25 5.68361)"><stop stop-color="#C66364"></stop><stop offset="1" stop-color="#D4766B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint15_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.875 23.5313) scale(10.0937 1.29657)"><stop stop-color="#B64B4B"></stop><stop offset="1" stop-color="#C56158" stop-opacity="0"></stop></radialGradient><radialGradient id="paint16_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4375 19.875) rotate(-46.8882) scale(4.02385 7.51767)"><stop stop-color="#FFC083"></stop><stop offset="0.620218" stop-color="#FFBD7D" stop-opacity="0"></stop></radialGradient><linearGradient id="paint17_linear_18_31665" x1="2.8125" y1="13.0312" x2="14.5582" y2="18.9404" gradientUnits="userSpaceOnUse"><stop stop-color="#B89367"></stop><stop offset="1" stop-color="#C5835E"></stop></linearGradient><linearGradient id="paint18_linear_18_31665" x1="8.21875" y1="14.6406" x2="7.59349" y2="15.6717" gradientUnits="userSpaceOnUse"><stop offset="0.351552" stop-color="#A74746"></stop><stop offset="0.845198" stop-color="#A04346" stop-opacity="0"></stop></linearGradient><radialGradient id="paint19_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.5625 14.5625) rotate(140.244) scale(18.3733 13.7403)"><stop stop-color="#FDAE69"></stop><stop offset="0.729021" stop-color="#CE8C4F"></stop><stop offset="0.921546" stop-color="#AD7B45"></stop><stop offset="1" stop-color="#8B6B4A"></stop></radialGradient><radialGradient id="paint20_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.0625 7) rotate(65.3152) scale(11.0745 3.16547)"><stop offset="0.233237" stop-color="#FFD47C"></stop><stop offset="0.853648" stop-color="#FFD98B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint21_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.3125 8.875) rotate(100.886) scale(6.6191 5.57808)"><stop offset="0.128419" stop-color="#FFD88C"></stop><stop offset="0.924134" stop-color="#FFBE7B" stop-opacity="0"></stop></radialGradient><linearGradient id="paint22_linear_18_31665" x1="7.25" y1="15.1875" x2="10.7588" y2="10.3142" gradientUnits="userSpaceOnUse"><stop offset="0.142353" stop-color="#C15F4D"></stop><stop offset="1" stop-color="#D58366" stop-opacity="0"></stop></linearGradient><radialGradient id="paint23_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(8.15625 15.7813) rotate(28.5422) scale(12.5574 1.96589)"><stop offset="0.149989" stop-color="#E4745D"></stop><stop offset="0.453292" stop-color="#C8604C"></stop><stop offset="0.632597" stop-color="#C0605F"></stop><stop offset="1" stop-color="#C0605F" stop-opacity="0"></stop></radialGradient><radialGradient id="paint24_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(1.40625 2.69067) rotate(46.0943) scale(22.3963)"><stop offset="0.935802" stop-color="#C17C61" stop-opacity="0"></stop><stop offset="0.982109" stop-color="#C17C61"></stop></radialGradient></defs></svg>

	

	<span>Croissant</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	<button class="tag tag-ghost px-1! -ml-0.5 mb-1 md:mb-1.5" type="button">+ 1</button></div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">License:
	</span>
	<div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-full rounded-br-none " type="button">
		<div class="tag tag-white rounded-full  relative rounded-br-none pr-2.5">
		<svg class="text-xs text-gray-900" width="1em" height="1em" viewBox="0 0 10 10" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.46009 5.0945V6.88125C1.46009 7.25201 1.75937 7.55129 2.13012 7.55129C2.50087 7.55129 2.80016 7.25201 2.80016 6.88125V5.0945C2.80016 4.72375 2.50087 4.42446 2.13012 4.42446C1.75937 4.42446 1.46009 4.72375 1.46009 5.0945ZM4.14022 5.0945V6.88125C4.14022 7.25201 4.4395 7.55129 4.81026 7.55129C5.18101 7.55129 5.48029 7.25201 5.48029 6.88125V5.0945C5.48029 4.72375 5.18101 4.42446 4.81026 4.42446C4.4395 4.42446 4.14022 4.72375 4.14022 5.0945ZM1.23674 9.78473H8.38377C8.75452 9.78473 9.0538 9.48545 9.0538 9.1147C9.0538 8.74395 8.75452 8.44466 8.38377 8.44466H1.23674C0.865993 8.44466 0.566711 8.74395 0.566711 9.1147C0.566711 9.48545 0.865993 9.78473 1.23674 9.78473ZM6.82036 5.0945V6.88125C6.82036 7.25201 7.11964 7.55129 7.49039 7.55129C7.86114 7.55129 8.16042 7.25201 8.16042 6.88125V5.0945C8.16042 4.72375 7.86114 4.42446 7.49039 4.42446C7.11964 4.42446 6.82036 4.72375 6.82036 5.0945ZM4.39484 0.623142L0.865993 2.48137C0.682851 2.57517 0.566711 2.76725 0.566711 2.97273C0.566711 3.28094 0.816857 3.53109 1.12507 3.53109H8.49991C8.80365 3.53109 9.0538 3.28094 9.0538 2.97273C9.0538 2.76725 8.93766 2.57517 8.75452 2.48137L5.22568 0.623142C4.9666 0.484669 4.65391 0.484669 4.39484 0.623142V0.623142Z" fill="currentColor"></path></svg>

	

	<span>cc-by-sa-4.0</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	</div></div>

		<div class="flex flex-col-reverse lg:flex-row lg:items-center lg:justify-between"><div class="-mb-px flex h-12 items-center overflow-x-auto overflow-y-hidden ">
	<a class="tab-alternate active" href="/datasets/saier/unarXive_imrad_clf"><svg class="mr-1.5 text-gray-400 flex-none" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
	Dataset card
	

	
		</a><a class="tab-alternate" href="/datasets/saier/unarXive_imrad_clf/viewer/"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	Data Studio
	

	
		</a><a class="tab-alternate" href="/datasets/saier/unarXive_imrad_clf/tree/main"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M21 19h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-8h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0 4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M9 19a1 1 0 0 1-1-1V6a1 1 0 0 1 2 0v12a1 1 0 0 1-1 1zm-6-4.333a1 1 0 0 1-.64-1.769L3.438 12l-1.078-.898a1 1 0 0 1 1.28-1.538l2 1.667a1 1 0 0 1 0 1.538l-2 1.667a.999.999 0 0 1-.64.231z" fill="currentColor"></path></svg>
	<span class="xl:hidden">Files</span>
		<span class="hidden xl:inline">Files and versions</span>
	

	
		</a><a class="tab-alternate" href="/datasets/saier/unarXive_imrad_clf/discussions"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M20.6081 3C21.7684 3 22.8053 3.49196 23.5284 4.38415C23.9756 4.93678 24.4428 5.82749 24.4808 7.16133C24.9674 7.01707 25.4353 6.93643 25.8725 6.93643C26.9833 6.93643 27.9865 7.37587 28.696 8.17411C29.6075 9.19872 30.0124 10.4579 29.8361 11.7177C29.7523 12.3177 29.5581 12.8555 29.2678 13.3534C29.8798 13.8646 30.3306 14.5763 30.5485 15.4322C30.719 16.1032 30.8939 17.5006 29.9808 18.9403C30.0389 19.0342 30.0934 19.1319 30.1442 19.2318C30.6932 20.3074 30.7283 21.5229 30.2439 22.6548C29.5093 24.3704 27.6841 25.7219 24.1397 27.1727C21.9347 28.0753 19.9174 28.6523 19.8994 28.6575C16.9842 29.4379 14.3477 29.8345 12.0653 29.8345C7.87017 29.8345 4.8668 28.508 3.13831 25.8921C0.356375 21.6797 0.754104 17.8269 4.35369 14.1131C6.34591 12.058 7.67023 9.02782 7.94613 8.36275C8.50224 6.39343 9.97271 4.20438 12.4172 4.20438H12.4179C12.6236 4.20438 12.8314 4.2214 13.0364 4.25468C14.107 4.42854 15.0428 5.06476 15.7115 6.02205C16.4331 5.09583 17.134 4.359 17.7682 3.94323C18.7242 3.31737 19.6794 3 20.6081 3ZM20.6081 5.95917C20.2427 5.95917 19.7963 6.1197 19.3039 6.44225C17.7754 7.44319 14.8258 12.6772 13.7458 14.7131C13.3839 15.3952 12.7655 15.6837 12.2086 15.6837C11.1036 15.6837 10.2408 14.5497 12.1076 13.1085C14.9146 10.9402 13.9299 7.39584 12.5898 7.1776C12.5311 7.16799 12.4731 7.16355 12.4172 7.16355C11.1989 7.16355 10.6615 9.33114 10.6615 9.33114C10.6615 9.33114 9.0863 13.4148 6.38031 16.206C3.67434 18.998 3.5346 21.2388 5.50675 24.2246C6.85185 26.2606 9.42666 26.8753 12.0653 26.8753C14.8021 26.8753 17.6077 26.2139 19.1799 25.793C19.2574 25.7723 28.8193 22.984 27.6081 20.6107C27.4046 20.212 27.0693 20.0522 26.6471 20.0522C24.9416 20.0522 21.8393 22.6726 20.5057 22.6726C20.2076 22.6726 19.9976 22.5416 19.9116 22.222C19.3433 20.1173 28.552 19.2325 27.7758 16.1839C27.639 15.6445 27.2677 15.4256 26.746 15.4263C24.4923 15.4263 19.4358 19.5181 18.3759 19.5181C18.2949 19.5181 18.2368 19.4937 18.2053 19.4419C17.6743 18.557 17.9653 17.9394 21.7082 15.6009C25.4511 13.2617 28.0783 11.8545 26.5841 10.1752C26.4121 9.98141 26.1684 9.8956 25.8725 9.8956C23.6001 9.89634 18.2311 14.9403 18.2311 14.9403C18.2311 14.9403 16.7821 16.496 15.9057 16.496C15.7043 16.496 15.533 16.4139 15.4169 16.2112C14.7956 15.1296 21.1879 10.1286 21.5484 8.06535C21.7928 6.66715 21.3771 5.95917 20.6081 5.95917Z" fill="#FF9D00"></path><path d="M5.50686 24.2246C3.53472 21.2387 3.67446 18.9979 6.38043 16.206C9.08641 13.4147 10.6615 9.33111 10.6615 9.33111C10.6615 9.33111 11.2499 6.95933 12.59 7.17757C13.93 7.39581 14.9139 10.9401 12.1069 13.1084C9.29997 15.276 12.6659 16.7489 13.7459 14.713C14.8258 12.6772 17.7747 7.44316 19.304 6.44221C20.8326 5.44128 21.9089 6.00204 21.5484 8.06532C21.188 10.1286 14.795 15.1295 15.4171 16.2118C16.0391 17.2934 18.2312 14.9402 18.2312 14.9402C18.2312 14.9402 25.0907 8.49588 26.5842 10.1752C28.0776 11.8545 25.4512 13.2616 21.7082 15.6008C17.9646 17.9393 17.6744 18.557 18.2054 19.4418C18.7372 20.3266 26.9998 13.1351 27.7759 16.1838C28.5513 19.2324 19.3434 20.1173 19.9117 22.2219C20.48 24.3274 26.3979 18.2382 27.6082 20.6107C28.8193 22.9839 19.2574 25.7722 19.18 25.7929C16.0914 26.62 8.24723 28.3726 5.50686 24.2246Z" fill="#FFD21E"></path></svg>
	Community
	

	
		</a></div>
	
			</div></div></header>
</div>
	
<div class="container relative flex flex-col md:grid md:space-y-0 w-full md:grid-cols-12 md:flex-1 md:grid-rows-full space-y-4 md:gap-6 ">
		<section class="pt-6 border-gray-100 md:col-span-8 pb-24 relative break-words copiable-code-container">
				<div class="SVELTE_HYDRATER contents" data-target="UnsafeBanner" data-props="{&quot;classNames&quot;:&quot;mb-4&quot;,&quot;repoId&quot;:&quot;saier/unarXive_imrad_clf&quot;,&quot;repoType&quot;:&quot;dataset&quot;,&quot;minLevel&quot;:&quot;unsafe&quot;}"></div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetViewer" data-props="{&quot;data&quot;:{&quot;kind&quot;:&quot;DatasetAndSampleData&quot;,&quot;datasetInfo&quot;:[{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Size of downloaded dataset files:&quot;,&quot;value&quot;:&quot;482 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;/datasets/saier/unarXive_imrad_clf/tree/refs%2Fconvert%2Fparquet/&quot;,&quot;label&quot;:&quot;Size of the auto-converted Parquet files:&quot;,&quot;value&quot;:&quot;247 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Number of rows:&quot;,&quot;value&quot;:&quot;530,054&quot;}],&quot;partial&quot;:false,&quot;configsData&quot;:{&quot;configInfos&quot;:[{&quot;name&quot;:&quot;default&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:530054}],&quot;selectedConfig&quot;:&quot;default&quot;,&quot;hasSelectedConfigParquet&quot;:true},&quot;splitsData&quot;:{&quot;splitInfos&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;numRows&quot;:520053},{&quot;name&quot;:&quot;validation&quot;,&quot;numRows&quot;:5001},{&quot;name&quot;:&quot;test&quot;,&quot;numRows&quot;:5000}],&quot;selectedSplit&quot;:&quot;train&quot;},&quot;sampleData&quot;:{&quot;dataset&quot;:&quot;saier/unarXive_imrad_clf&quot;,&quot;config&quot;:&quot;default&quot;,&quot;split&quot;:&quot;train&quot;,&quot;capabilities&quot;:{&quot;rows&quot;:true,&quot;search&quot;:true,&quot;filter&quot;:true,&quot;statistics&quot;:true},&quot;navigation&quot;:{&quot;p&quot;:0},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzA4MCwic3ViIjoiL2RhdGFzZXRzL3NhaWVyL3VuYXJYaXZlX2ltcmFkX2NsZiIsImV4cCI6MTc0MjkyNjY4MCwiaXNzIjoiaHR0cHM6Ly9odWdnaW5nZmFjZS5jbyJ9.JBFB3XCzduvrSj80JMxcdY_t8NCjO_QODp4mXE4fJIT6aHxvfoyB3CixHg0ByXvri2Y76ISBrUFeTclprUsGDA&quot;,&quot;sampleData&quot;:{&quot;columns&quot;:[{&quot;name&quot;:&quot;_id&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;_id&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:36,&quot;max&quot;:36,&quot;mean&quot;:36,&quot;median&quot;:36,&quot;std&quot;:0,&quot;histogram&quot;:{&quot;hist&quot;:[520053],&quot;bin_edges&quot;:[36,36]}}}},{&quot;name&quot;:&quot;text&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;text&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:200,&quot;max&quot;:327882,&quot;mean&quot;:818.23772,&quot;median&quot;:656,&quot;std&quot;:2343.09811,&quot;histogram&quot;:{&quot;hist&quot;:[519615,314,76,17,12,6,7,5,0,1],&quot;bin_edges&quot;:[200,32969,65738,98507,131276,164045,196814,229583,262352,295121,327882]}}}},{&quot;name&quot;:&quot;label&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;label&quot;,&quot;column_type&quot;:&quot;string_label&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;no_label_count&quot;:0,&quot;no_label_proportion&quot;:0,&quot;n_unique&quot;:5,&quot;frequencies&quot;:{&quot;m&quot;:62863,&quot;r&quot;:38814,&quot;i&quot;:264799,&quot;d&quot;:89131,&quot;w&quot;:64446}}}}],&quot;rows&quot;:[{&quot;rowIdx&quot;:0,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;7b7f37ca-e70e-416d-927f-2fa3db62e3d5&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Food recommendation has become an essential method to help users adopt healthy dietary habitsÂ [1]}.\nThe task of computationally providing food and diet recommendations is challenging, as thousands of food items/ingredients have to be collected, combined in innovative ways, and reasoned overÂ [2]}.\nFurthermore, there are many facets to the foods we consume, such as our ethnic identities, socio-demographic backgrounds, life-long preferences, all of which can inform our perspectives about the food we choose to consume to lead healthy lives.\nFood recommendation can get even more complicated when the food options available to an individual are further constrained because of a group setting\n(e.g., the seafood allergy of one family member may preclude recipes including shrimp to be recommended to the whole group). There is `no size fits all,' and even dietetic professionals have raised concerns that such varied dimensions need to be incorporated in their food recommendation adviceÂ [3]}.\nSuch varied dimensions set up a need to provide food-related explanations to enhance the users' trust in recommendations made by food recommender systems, both automatic and human-driven, as users are more likely to follow the advice when the reasons for the advice are provided understandably.\nAlthough explanations could help users trust in recommendations and encourage them to follow good eating habits, the inclusion of explanations into food recommender systems has not yet received the interest it deserves in the available literatureÂ [1]}.\nTherefore, this work aims to bridge the gap between existing food recommendation systems by providing semantic modeling of explanations required in the complex and ever-expanding food and diet domain.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:1,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0f83b85b-e18d-4055-b5aa-9bf7096c42cb&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;We introduce and discuss the FEO that extends the Explanation Ontology [1]} and the FoodKG (a food knowledge graph that uses a variety of food sources) [2]} to model explanations in the food domain, a connection that is currently lacking in the current literatureÂ [3]}.\nOur ontology can be classified under the post-hoc wing of Explainable Artificial Intelligence(XAI) and aims to interpret the results of black box AI recommender systems in a human-understandable manner [4]}, [5]}. Accordingly, using a recommender system agnostic model, we aim to retroactively create connections between the system and the recommendation,including modeling user details, such as allergies and likes, system details, such as location and time, and question details, such as parameters.\nWe then assemble explanations by querying the ontology for different templates of knowledge types, defined using formalizations of explanation types.\nWe add structure to the auxiliary modeling of the user, and the system, which we find are important components of comprehensive explanations to represent a range of explanation types [1]} (e.g., contextual, contrastive, counterfactual explanations) to model food-specific explanations, which would complement personalized, knowledge-based food recommendation applications such as the `Health Coach,' a healthy food recommendation serviceÂ [7]}.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:2,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;61d8f60a-917b-4807-87c2-5b7e5c07f0ae&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Prior work has shown that users seek answers and reasoning for nutrition and food questions they might have\nÂ [1]}, [2]}, [3]}, [4]}, [5]}, [6]}.\nHowever, users are increasingly concerned with the evidence and reasoning that lead to those claims.\nApplying logic, reasoning, and querying on food and culinary arts have captured information, such as food categorization in FoodOnÂ [7]}, recipes, and associated information in RecipeDBÂ [8]}, and have brought together disparate sources of food informationÂ [9]}, [10]}.\nIn the area of food recommendations, many existing approaches recommend recipes based on the recipe content (e.g., ingredients) Â [5]}, [1]}, [6]}, user behavior history (e.g., eating history)Â [1]}, [2]}, [3]}, or dietary preferencesÂ [4]}, [3]}. However, none of these systems provide the rationales for why the food was recommended, as these systems are utilizing black-box, deep learning models. Conversely, while there are systems that employ post hoc XAI methods to provide explanations for opaque AI systems [19]}, [20]}, they have not yet been applied in the food recommendation domain.\nOur work differs from such previous works because we leverage a greater degree of explicit, semantic information about foods and other related semantically annotated data in generating explanations about recommending a food item or answering specific questions about a particular recommendation.\nAdditionally, the need to provide more user-centered explanations that help users improve trust and understanding of AI systems, and the information used for recommendations, has been gaining attention [21]} lately. There have been some conceptual frameworks [22]} and ontologies [23]} that attempt to model explanations from an end-user perspective. In our work, we aim to ground these more general-purpose efforts for the food domain.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;w&quot;}}},{&quot;rowIdx&quot;:3,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;13508314-3af5-446f-b353-7d1105b1ce8e&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;[1]} describe a system based on logical reasoning that supports monitoring the users' behaviors and persuades them to follow healthy lifestyles, including recommending suitable food items, with natural language explanations[1]}. Their system performs reasoning to understand whether the users follow an unhealthy behavior regarding a food intake input.\nThen the system generates the persuasion message with explanations using natural language templatesÂ [1]}.\nOur proposed, ontology-based method for generating explanations is complementary to their approach because we provide support for various types of explanations, not just trace-based explanations derived from templates for explaining the reasoner result.\nWe believe that by supporting different types of explanations, system developers will be supporting more user-friendly interfaces for personalized, consumer-facing applications [4]}.\nThe primary aim for FEO is usage in more interactive or conversational food recommendations, for example, in a personalized health recommendation app.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;w&quot;}}},{&quot;rowIdx&quot;:4,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1587047b-085e-41aa-8e80-6481f716f583&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;We employ a task-based evaluation [1]} for our ontology using three main competency questions, each aimed at addressing a different explanation type that we attempted to extract from our model in FEO, as detailed in the following section. We have used competency questions as our method of evaluation as they are the accepted standard to â€œevaluate the ontological commitments that have been made\&quot; [2]}.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;m&quot;}}},{&quot;rowIdx&quot;:5,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;f4dc6ca4-ecfd-46cc-aa29-eda892367f86&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;As our model endeavors to provide explanations and context to users that get lost in black box AI models, we chose to evaluate the FEO by its ability to provide responses to a subset of important explanation types. In tbl:explanationTable, we have included a list of previously identified explanation types and the corresponding questions that might require an explanation for its food-related recommendation.\nPost-hoc explanations provide an approximation of the rationales that users might be looking for [1]}, which is what we wanted to tackle with the competency questions.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;m&quot;}}},{&quot;rowIdx&quot;:6,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;62489ef1-231c-48cb-991b-8eb8488137e0&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;We support our choice in the selection of a subset of explanation types from Table REF  for our evaluation via competency questions with observations from recent advances in the machine learning community, where we noticed that there is a focus on methods that generate contrastive and counterfactual explanations [1]}. Moreover, contrastive, counterfactual, and contextual explanations also contain explanations with scientific evidence, everyday evidence, and system trace. Therefore, an evaluation using these explanation types would also allow FEO to include other explanation types. Hence, we have completed our initial modeling to allow for contrastive, counterfactual, and contextual explanations and framed the evaluation of our ontology by these explanation types.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;m&quot;}}},{&quot;rowIdx&quot;:7,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;da1f6953-f422-4460-8d0b-a6c422d9755a&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;We undertook this process against the recommendations generated by the Health Coach Application, which uses machine learning techniques to assess users' dietary needs and provide recommendationsÂ [1]}.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;m&quot;}}},{&quot;rowIdx&quot;:8,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;74f18154-445c-46c4-ac77-820d2a63878d&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;In this paper, we have discussed an ontology modeling for food and diet recommendation explanations, which aims to model and then be used to generate explanations specific to the context of the users and setting.\nFEO is a domain-specific ontology where the domain concepts are abstracted up in a manner so that they can be comprehensively exposed to a user in the form of a diverse range of explanations. The class and property relationships that we detailed\nenable using simple queries to get explanations that explore many different variables. We strove to maintain the simplicity of the queries in order to ensure that a non-technical user can access explanations just as effectively as a technical user. From the modeling perspective, we found that the food domain would benefit from semantically bound explanations because of the variety of questions that a user might ask and the corresponding variety of explanations that they might require. From the user perspective, we chose the food domain specifically because food and diet are something that a growing number of people are concerned with, and we believe that our ontology can empower users to make informed decisions from their food choices. We plan to continue this work to extend the range of explanations that we can provide and increase accessibility to the tool by incorporating it into a more user-facing recommendation environment.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:9,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;b38859e2-3072-4811-b009-8f13df445a9f&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Photolithography is one of the most important processes for semiconductor manufacturing Â . An exemplar photolithography system is shown in Fig.Â REF . A laser beam that goes through the integrated circuit patterns on the reticle is projected on the wafer so that the patterns are printed onto the wafer. During this process, the stage carrying the wafer (the wafer stage) needs to move steadily and precisely so that the patterns are printed accuratelyÂ [1]}.\n<FIGURE>&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:10,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;42a82b45-2d55-43c8-857a-6e6ae00d0db7&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;With the technological development of the semiconductor industry, manufacturers demand more precise performance from the wafer stage. To achieve the goal, researchers have developed many control strategies and applied them on the wafer stage, including iterative learning control (ILC)Â [1]}, [2]}, [3]}, [4]}, sliding mode control (SMC)Â [5]}, [6]}, [7]}, [8]}, \\(H \\infty \\)  feedback controlÂ [9]}, multi-rate controlÂ [10]} and so on. Among them, SMC has attracted great attention for its simple implementation and robust performance in the presence of uncertainties and external disturbancesÂ [11]}, [12]}.\nBeyond the basic SMC structureÂ [13]}, many advanced SMC strategies such as the modified reaching lawsÂ [14]}, boundary layer techniqueÂ [15]}, and super-twisting algorithm (STA)Â [16]} have also been proposed. The STA, focusing on improving the dynamics of the sliding variables, has been considered as one of the most effective approaches for the well-known chattering phenomenonÂ [17]}. It is also robust with respect to bounded uncertainties and disturbancesÂ [18]}, [19]}, and has been implemented successfully in practiceÂ [18]}, [21]}.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:11,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;a30dbe41-0845-4204-b74b-18f570de6865&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;To further improve the performance of SMC, fractional-order calculus is introduced to improve the state dynamics in the sliding surface and is combined with the STAÂ [1]}, [2]}. Although there are some theoretical research and successful precedents for the application of fractional-order super-twisting algorithm (FOSTA)Â [2]}, [4]}, the parametric uncertainties are not always taken into consideration, or their bounds are assumed to be small. When the amplitudes of the uncertainties or disturbances are rather large, the sliding variable in STA cannot converge to the predefined sliding surface. Instead, it only converges to an uncertainty region around the sliding surface, which inevitably brings positioning error to the systemÂ [5]}. Previous researches have tried to reduce the range of the uncertainty region to improve the precisionÂ [6]}, but the negative influence from large uncertainties remains.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:12,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;a4a45dd6-43db-4077-bdbf-07be5ada5db6&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Aiming to improve the control performance (i.e., high precision and robust performance) in the presence of large model uncertainties and disturbances, a novel adaptive neural network and fractional-order super-twisting algorithm (ANN-FSA) is proposed in this paper. Firstly, we use the radial basis function (RBF) neural network to approximate the uncertainties and disturbances in the system, and a corresponding fractional-order super-twisting controller is designed to compensate for uncertainties and disturbances. The stability of the proposed control strategy is also analyzed. Moreover, to guarantee the global convergence of the closed-loop system, an adaptive law is designed. At last, we apply the proposed controller to a wafer stage testbed. Experimental results show that the controller performs well and is robust against disturbances.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:13,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;fc40adda-f282-413d-b8bc-29f0affdf1e1&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The remainder of this paper is organized as follows: SectionÂ  provides the model of the wafer stage. SectionÂ  presents the proposed controller, and the stability analysis of the controller. SectionÂ  displays the experimental setup and the experimental results with the proposed controller. Finally, SectionÂ  presents conclusions.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:14,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;9ab84482-14ea-46f1-90f7-0358109893dc&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The overall structure of our experimental system is depicted in Fig.Â REF . The control algorithm is programmed in the LabView environment on the host computer. The host computer is connected with the remote controller (PXI 7831, from National Instruments) via Ethernet, so that the control algorithm can be deployed in the LabView Real-Time system in the remote controller. The output of the controller is amplified by an amplifierÂ (TA330, from Trust Automation) and applied to the wafer stage testbed. The position of the moving part of the wafer stage is measured by a laser ranging systemÂ (from Keysight), and the measuring results are fed back to the remote controller. The nominal parameters of the wafer stage have been identified as \\(\\bar{A}=-1.092~s^{-1}\\)  and \\(\\bar{B}=3.9124~m/(s^2\\cdot A)\\) .\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;m&quot;}}},{&quot;rowIdx&quot;:15,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;e652aa8a-b9f6-4b10-915a-3e8fc5bcaea5&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;We implement the traditional PID controller, the SMC, the advanced FOSTA, and the proposed ANN-FSA to the wafer stage testbed to investigate the effectiveness of the proposed controller. The reference trajectory is shown in Fig.Â REF . The scan length is set as \\(0.04\\)  m, and the scan velocity is set as \\(0.032\\) Â m/s. The parameters in each controller are tuned so that the best performance of each controller is achieved. The sampling interval of the experiments are set as 1 ms. For the RBF neural network, the number of the hidden nodes is set as 5, other parameters are selected as \\(c_1=[-3~-1~0~1~3]\\) , \\(c_2=[-7~-3~0~3~7]\\) , \\(b=[50~50~50~50~50]\\) , \\(\\rho =0.2\\)  and the initial value of \\(\\mathbf {W}\\)  is set as \\(\\mathbf {0}\\) . In the fractional-order super-twisting algorithm, \\(\\eta \\)  and \\(a\\)  are selected as \\(\\frac{1}{2}\\) , and other parameters are tuned as \\(h_1=500\\) , \\(h_2=30\\) , \\(\\alpha _1=0.001\\)  and \\(\\alpha _2=175\\) .\n<FIGURE><FIGURE><FIGURE>&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;r&quot;}}},{&quot;rowIdx&quot;:16,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;4af855ba-1917-4364-b0d1-ab02667d4fd3&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Moreover, to study the robustness of these controllers, with all the parameters maintained the same, an extra external sinusoidal disturbance is generated and applied to the system. The amplitude and frequency of the disturbance signal are set as \\(0.03\\) Â m (rather large compared with referce signal) and 1Â Hz, respectively. We denote the situation without extra disturbance as CaseÂ 1 and the situation with the additional disturbance as CaseÂ 2. The tracking performance in these two cases is shown in Fig.Â REF  and Fig.Â REF , respectively.\n<TABLE>&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;r&quot;}}},{&quot;rowIdx&quot;:17,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;56d37cd8-83fe-4a3e-9a2a-2d35e7a964f4&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;In Fig.Â REF , we note that all the four controllers have large tracking errors when the scanning velocity changes. The peak error of SMC is the largest among the four controllers, which is at around 50Â \\(\\mu \\) m. Tracking error via the proposed ANN-FSA is the smallest, about 35Â \\(\\mu \\) m. We also note that the errors of SMC and ANN-FSA decay faster than the PID controller, but that the error via the PID controller is smoother than the other two controllers when the tracking errors are small, i.e., closer to zero. Figure REF  shows that even in the presence of disturbances, the ANN-FSA can achieve the smallest tracking error. Moreover, from Fig.Â REF  and Fig.Â REF , we note that the tracking error increases when disturbances exist. This means that the PID controller is not as robust enough as SMC and ANN-FSA. To quantitatively describe the robustness, we calculate the root mean square (RMS) errors of each controller in both cases and the results are displayed in Table.Â REF . We can see that compared with traditional SMC, the precision of FOSTA is significantly improved. This is due to the introduction of the fractional-order sliding surface and the super-twisting algorithm. Further, the proposed ANN-FSA has the smallest RMS error in both CaseÂ 1 and CaseÂ 2. According to the difference values between the RMS errors in CaseÂ 1 and CaseÂ 2, we can conclude that SMC and FOSTA strategy is more robust than the PID controller. ANN-FSA remains precise in the presence of the large external disturbances. The results and comparison prove that the proposed control scheme achieves the best performance in terms of precision and robustness.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;r&quot;}}},{&quot;rowIdx&quot;:18,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;445f6fda-7cc7-4613-b911-4257b1b73aa9&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;In this paper, an adaptive neural-network and fractional-order super-twisting algorithm was proposed and applied to a precision motion system. In this way, not only the dynamics of the states on the sliding surface was improved via the super-twisting algorithm, but also unknown model uncertainties and disturbances of the system were well compensated. Moreover, an adaptive law was derived for the neural-network-based controller so that the closed-loop system is globally convergent. Both stability analysis and experimental verification were provided. The comparison results among a PID controller, a conventional SMC, an advanced FOSTA and the proposed ANN-FSA showed that the proposed controller could achieve higher precision and better robustness than conventional controllers.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:19,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d04d3ac9-d3db-4a35-9cc5-52568934052a&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The number field sieve is the most efficient method known for solving the integer\nfactorization problem and the discrete logarithm problem in a finite field,\nin the most general case. However there are many different variants of the number field\nsieve, depending on the context. Recently, the Tower Number Field Sieve (TNFS) was\nsuggested as a novel approach to computing discrete logs in a finite field of\nextension degree \\(> 1\\) . The Extended Tower Number Field Sieve (ExTNFS) is a variant\nof TNFS which applies when the extension degree \\(n\\)  is composite, and gives the best known\nruntime complexity in the medium characteristic case (see below).\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:20,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;b888a7a7-6fd5-4e32-89df-a763ea276ff9&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;We briefly discuss the asymptotics of number field sieve-type algorithms.\nWe define the following function:\n\\(L_{p^n}(\\alpha ,c) = \\exp ((c+o(1))(\\log {p^n})^\\alpha (\\log {\\log {p^n}})^{1-\\alpha }).\\) \n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:21,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;e3f7862f-da28-4b54-99a4-474b51e0d15c&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;This function describes the asymptotic complexity of a subexponential function in\n\\(\\log {p^n}\\) , which is used to asses the complexity of the number field sieve for\ncomputing discrete logs in \\(\\mathbb {F}_{p^n}\\) . For a given \\(p^n\\) , there are two important\nboundaries, respectively for \\(\\alpha = 1/3\\)  and \\(\\alpha = 2/3\\) . We then have 3 cases:\nsmall characteristic, when \\(p < L_{p^n}(1/3,\\cdot )\\) , medium characteristic, when\n\\(L_{p^n}(1/3,\\cdot ) < p < L_{p^n}(2/3,\\cdot )\\) , and large characteristic,\nwhen \\(p > L_{p^n}(2/3,\\cdot )\\) . This work relates to the medium characteristic case.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:22,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;c1e2cd57-d423-4fbf-838a-94857b91ae4e&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The structure of this paper is as follows: In Section 2, we give an overview\nof computing discrete logs using ExTNFS. In section 3, sieving in a 4d box (orthotope)\nis described, and we give implementation details. In section 4, we describe the\ndescent step in detail. In section 5 we give details of the record computation\nin \\(\\mathbb {F}_{p^4}\\)  of 512 bits field size. In section 6 we conclude and outline future\npossible work.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:23,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;a5705097-c41c-4e30-ad7a-8b60fe2905b4&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;We implemented the key components of the Extended Tower Number Field Sieve and together\nwith linear algebra components of CADO-NFS demonstrated a total discrete log break\nin a finite field \\(\\mathbb {F}_{p^4}\\)  of size 512 bits, a new record. This provides another\ndata point in the evaluation of security of systems dependent on the intractability\nof discrete log attacks in extension fields. Whereas the recent articles\n[1]}, [2]} show that asymptotically, sieving in a d-dimensional\nsphere is optimal as \\(d \\rightarrow \\infty \\) , there seems to be room at lower\ndimensions for sieving in an orthotope to remain competitive. We did not optimize our code\nparticularly well and there are probably further gains in sieving speed possible.\nOne such idea is to replace our list/sort approach with bucket sieving\n[3]}, which would\nat least improve the memory footprint (although for our sieving dimensions this was\nnot a problem). The parameters for sieving were\ntuned in an ad-hoc way and a finer examination of optimal parameters would be interesting.\nIt would be a fairly easy change to adjust the sieving shape to best suit a given\nspecial-\\(\\mathfrak {q}\\) , for rectangular sieving orthotopes, improving the relation yield.\nFinally, we did not exploit the common Galois automorphism of the sieving polynomials,\nwhich would have cut the sieving time in half.\nThe overall timings of the key stages of our computation are shown in\ntable REF .\n<TABLE>&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:24,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;2ccd93b0-7f49-401c-b8ee-6cf316cb9ade&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Neural methods for generating entity embeddings have become the dominant approach to representing entities, with embeddings learned through methods such as pretraining, task-based training, and encoding knowledge graphs [1]}, [2]}, [3]}.\nThese embeddings can be compared extrinsically by performance on a downstream task, such as entity linking (EL).\nHowever, performance depends on several factors, such as the architecture of the model they are used in and how the data is preprocessed, making direct comparison of the embeddings hard.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:25,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;49515884-2caf-4584-b156-05c7eede690a&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Another way to compare these embeddings is intrinsically using probing tasks [1]}, [2]}, which have been used to examine entity embeddings for information such as an entity's type, relation to other entities, and factual information [3]}, [4]}, [5]}, [6]}.\nThese prior examinations have often examined only a few methods, and some propose tasks that can only be applied to certain classes of embeddings, such as those produced from a mention of an entity in context.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:26,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;9f0a29f7-d76d-4a93-a126-b10c958003c1&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;We address these gaps by comparing a wide range of entity embedding methods for semantic information using both probing tasks as well as downstream task performance.\nWe propose a set of probing tasks derived simply from Wikipedia and DBPedia, which can be applied to any method that produces a single embedding per entity.\nWe use these to compare eight entity embedding methods based on a diverse set of model architectures, learning objectives, and knowledge sources.\nWe evaluate how these differences are reflected in performance on predicting information like entity types, relationships, and context words.\nWe find that type information is extremely well encoded by most methods and that this can lead to inflated performance on other probing tasks.\nWe propose a method to counteract this and show that it allows a more reliable estimate of the encoded information.\nFinally, we evaluate the embeddings on two EL tasks to directly compare their performance when used in different model architectures, identifying some that generalize well across multiple architectures and others that perform particularly well on one task.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:27,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;58f8acc2-f63f-43df-919a-0f82dafd1491&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;We aim to provide a clear comparison of the strengths and weaknesses of various entity embedding methods and the information they encode to guide future work.\nOur probing task datasets, embeddings, and code are available online.https://github.com/AJRunge523/entitylens\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:28,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;b01f524d-cd9b-49c2-b55f-d6030c20b885&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Many of our embedding methods have been evaluated on EL tasks in prior work, either in a separate model or as full EL models themselves.\nHowever, direct comparison of the impact of of the embeddings on EL performance is confounded by differences in the architectures which leverage the embeddings, as well as difficult to reproduce differences in candidate selection, data preprocessing, and other implementation details.\nTo address this, we evaluate all of our embeddings in a consistent framework, testing them on two standard datasets in three different EL model architectures to directly compare the contribution of the embeddings to performance on the downstream task and how well they perform across different model architectures.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;m&quot;}}},{&quot;rowIdx&quot;:29,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;776298b8-cc2e-4cb1-9315-d6455ccebb64&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;We test the embeddings using three EL models on two standard EL datasets, the AIDA-CoNLL 2003 dataset [1]} and the TAC-KBP 2010 dataset [2]}.\nTwo of our EL models are the CNN and RNN EL models used to generate our task-learned embeddings.\nOur third is a transformer model based on the RELIC model of [3]} that encodes a 128-word context window around the entity mention using uncased DistilBERT-base [4]}https://huggingface.co/distilbert-base-uncased.\nWe compare the embedding of the CLS token in the final layer to a separate entity embedding for each candidate entity using a weighted cosine similarity.\nTo compare the impact of the entity embeddings, we replace the candidate document convolution in the CNN model or the randomly initialized embeddings in the RNN and transformer models with the pretrained embeddings during training.\nDetails about dataset preprocessing, candidate selection, and model training can be found in Appendix .\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;m&quot;}}},{&quot;rowIdx&quot;:30,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;42f87d63-7602-4205-b341-1a2cfb5bd6d5&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;In this work, we propose a new set of probing tasks for evaluating entity embeddings which can be applied to any method that creates one embedding per entity.\nUsing these tasks, we find that entity type information is one of the strongest signals present in all but one of the embedding models, followed by coarse information about how likely an entity is to be mentioned.\nWe show that the embeddings are particularly able to use entity type information to bootstrap their way to improved performance on entity relationship and factual information prediction tasks and propose methods to counteract this to more accurately estimate how well they encode relationships and facts.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:31,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;94138ed2-c856-4c13-919d-aafbc8d72777&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Overall, we find that while BERT-based entity embeddings perform well on many of these tasks, their high performance can often be attributed to strong entity type information encoding.\nMore specialized models such as Wikipedia2Vec are better able to detect and identify relationships, while the embeddings of [1]} better capture the lexical and distributional semantics of entities.\nAdditionally, we provide a direct comparison of the embeddings on two downstream EL tasks, where the models that performed well on the probing tasks such as Ganea, Wiki2V, and BERT performed best on the downstream tasks.\nWe find that the best performing embedding model depends greatly on the surrounding architecture and encourage future practitioners to directly compare newly proposed methods with prior models in a consistent architecture, rather than only compare results.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:32,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;cea9e13d-9656-4fc0-b6f0-7e137bcc04cd&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Our work provides insight into the information encoded by static entity embeddings, but entities can change over time, sometimes quite significantly.\nOne future line of work we would like to pursue using our tests is to investigate how changes in entities over time can be reflected in the embeddings, and how those changes could be modeled as transformations in the embedding space.\nContext-based embeddings in particular could then be dynamically updated with new information, instead of being retrained from scratch.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:33,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;55d4f39c-c6c2-4370-b6ec-f2881f0a142e&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Bipolar disorder (BD) is a mental health condition that causes extreme mood swings like emotional highs (mania, hypomania), lows (depression), mixed episodes where depression and manic symptoms occur together. The diagnosis of bipolar disorder requires lengthy observations on the patient. Otherwise, it can be mistaken with other mental disorders like anxiety or depression. The disease affects 2% of the population, and sub-threshold forms (recurrent hypomania episodes without major depressive episodes) affect an additional 2%Â [1]}. It is ranked as one of the top ten diseases of disability-adjusted life year (DALY) indicator among young adults, according to World Health OrganizationÂ [2]}. It takes 10 years on average to diagnose bipolar disorder after the first symptomsÂ [3]}.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:34,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;90748098-c9f9-463c-88e7-7602a0e028d0&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;In bipolar disorder, the clinical appearance of the patients changes based on the moods they are in. The changes are seen in both their sound and visual appearance, as well as the energy level changes. In the manic episode, the speech of the patient becomes louder, rushed, or pressured. The patient can be very cheerful, furious, or overly confident. The movements of the patient become more active, exaggerated, and they tend to wear very colorful clothes. Feelings and the state of mind change quickly. Racing thoughts, reduced need for sleep, lack of attention, increase in targeted activity (work, school, personal life) are some situations patients can experience in the manic episode. These symptoms return to a normal state during the remission stateÂ [1]}.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:35,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;be8152d4-ad99-4786-8cce-08a683992dfa&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Today, the diagnosis of mental health disorders rely on questionnaires done by psychiatrists and reports from patients and their caregivers. Psychiatrists perform some tests to collect information about the patient's cognitive, neurophysiological, and emotional situationsÂ [1]}. But these reports are subjective, and there is a need for more systematic and objective diagnosis methods. Especially, with the COVID-19 pandemic, remote treatment and diagnosis gain importance, which can be achieved using automated methods.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:36,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;5f241116-9423-4597-bd70-e3959432727e&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;One of the tools used to rate the severity of the manic episodes of a patient is the Young Mania Rating Scale (YMRS). During the interviews, psychiatrists observe the patient's symptoms and give ratings to them. The 11 items in YMRS assess the elevated mood, increased motor activity-energy, sexual interest, sleep, irritability, speech rate and amount, language-thought disorder, content, disruptive-aggressive behavior, appearance, and insight. Most of these can be observed from speech patterns, body or facial movements, and the content of what was spoken during the interview.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:37,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;25a9b2d4-7baf-4d5c-b789-10443582124c&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Recent advancements in technologies like social media, smartphones, wearable devices, and improvements in recording techniques like better cameras, neuroimaging techniques, microphones enable us to gather good quality data from people during their everyday lives. This creates an opportunity to create tools to monitor the symptoms of the patients in longer periods, screen patients before they see the psychiatrists, complement clinicians in the diagnosis, and capture their behaviors in situations where they cannot act or hide the symptoms.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:38,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;9e3e27f1-7c93-4de6-9384-b8717ec2f287&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;In recent years, there are many works on diagnosing psychiatric disorders like Alzheimer's disease, anxiety, attention deficit hyperactivity disorder, autism spectrum disorder, depression, obsessive-compulsive disorder, bipolar disorderÂ [1]} using machine learning (ML) techniques. The datasets used for the detection of the diseases contain linguistic, auditory, and visual information. Adapted from real life, using the modalities together with fusion techniques improves the results as explained in Chapter .\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:39,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;cae9c630-e4ac-4bc4-85a2-214978176e0c&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Assessment of mental health disorders using machine learning methods has been an active research area. Many researchers are working on recognizing mental health disorders varying from depression, Alzheimerâ€™s disease, anxiety to bipolar disorder. The interdisciplinary research between psychiatrists and computer scientists helps to create new datasets and bringing insights from the medical domain to artificial intelligence.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;w&quot;}}},{&quot;rowIdx&quot;:40,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;3f3d47a9-0011-484d-bccc-7d32c4593500&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;In this chapter, we introduce the features used in audio, textual, and visual modalities, preprocessing, feature selection methods applied to the dataset. After that, we explain the ELM algorithm used as a classification method, cross-validation technique used to evaluate the results, and modality fusion methods applied to improve the unimodal results.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;m&quot;}}},{&quot;rowIdx&quot;:41,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;ae647d7b-e1fd-42fe-ad88-53a0686f9d60&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;During this thesis, we worked on the classification of bipolar disorder episodes (mania, hypomania, depression) using the BD dataset that contains video recordings of the bipolar disorder patients while they are interviewed by their psychiatrists. During the interviews, the patients perform seven different tasks. The tasks are designed in a way that they elicit both positive and negative emotions in the patients, and some tasks are emotionally neutral.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:42,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;e799b2ad-acf7-41c0-a755-187eb7579bb1&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;We showed that multimodality improves the generalizability of the classification of bipolar disorder. The information coming from acoustic, textual, and visual modalities complement each other and improve the performance of the unimodal systems. The results suggests that using all three modalities together gives the best performance, however a fusion model of the linguistic, and acoustic modalities still perform well while requiring less information.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:43,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;e12eb7c5-29f1-4029-9585-f893f486341a&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;As a classification algorithm, we use fusion of weighted and unweighted ELMs. ELM was a good fit for this problem, since it is a 2-level neural and prone to overfitting. The data imbalance creates a need for a weighted model, however weighted ELM mostly favor the minority class. So using the fusion of weighted and unweighted ELMs, the optimum point is found.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:44,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;644700f7-5da7-4590-bad1-8ed5728efe1b&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The best performing model is achieved using eGEMAPS10, LIWC, and FAU features using the fusion of weighted and unweighted kernel ELMS, and fused using majority voting as a late fusion process. We achieve 64.8% test set UAR on this configuration, which is the best result achieved on the BD dataset as can be seen in FigureÂ REF . The results suggest that benefiting from all three modalities is useful, since the first 13 best performing model is achieved on the fusion models of three modalities. However, the 14th highest score on the Table\nÂ REF  uses only linguistic and acoustic modalities. So, it is possible to use only audio recordings of the patients, like phone recordings and achieve promising results from the fusion of linguistic and acoustic modalities. Besides, the MM1 scores on Table REF  shows that, fusion of modalities increase the maximum scores achieved on a single modality in all the configurations.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:45,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;258da887-57b6-4668-a9f7-691395eb7e86&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;eGEMAPS is a commonly used minimalistic acoustic feature set. So we used it for the audio classification, and in the fusion experiments. Besides, we summarized eGEMAPS LLDs with the 10 functionals presented inÂ [1]}. We achieved a better performance using eGEMAPS10 feature set, which shows that eGEMAPS LLDs can give better results when summarized with different functionals. eGEMAPS, and eGEMAPS10 feature sets contain 88, and 230 features respectively. So, a higher feature size may help finding better features that generalize better to the dataset.\n<FIGURE>&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:46,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;579163b4-b6bf-4010-b0b2-cc5eb1ae82d4&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;These results are still not high enough to use in a real-world application as a decision system. One of the main difficulties was the small size of the BD corpus. There are 25, 38, and 41 clips in the dataset for the remission, hypomania, and mania classes respectively, which is not enough to generalize with a high certainty. The dataset is collected in a real-life scenario. So there were some noises, and in some cases the clinician explains things about the questions to the patients, so her voice can be heard as well. These issues are expected to be present if a real-life application is created, so the natural recording setup makes this database valuable. Another difficulty stems from missing information in some clips, where patients do not answer some of the questions. In one of the test case clips, the patient does not answer any question at all. This can be used as a feature as well. However in our method, it caused a poor performance.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:47,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0fbed13e-eef2-4764-9f77-131976893f47&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Besides the clip level evaluation, we look for the effect of the tasks separately, and by grouping the same emotion eliciting tasks during the classification. Since some tasks are not performed in every clip, the number of clips per task are different. To be able to compare the results among the task groups and the entire clips results, we assign the middle class label to the missing clips. Since the dataset size is already small, this distorted the final scores somewhat. Still, from the task level experiments we can see that emotion eliciting tasks are more useful in the classification of BD for all three modalities, as expected. In order to increase the dataset size, we also used the task groups as separate data points and performed classification. However, the results were not better than the entire clip level results, which shows that the information obtained from longer clips is necessary for learning.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:48,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;5fa8e7bc-f488-4c83-bdba-5e584336fe3a&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Our final best performing model contains information from three different modalities, and each modality is represented using feature vectors with various sizes, which causes poor explainability of the model. It is especially important to create explainable models in medical domain.\nAs a further study, the explainability of the system can be investigated, which also gives insights to the psychiatrists about the features used in the classification, and the best performing ones can be adapted in their decision making progresses.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:49,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;e349de23-f7a4-4f0a-b4ca-1d2787a3ca63&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Deep learning has emerged as a powerful tool for many industrial and scientific applications.\nHowever, deep learning requires large centralized datasets, whose collection can be intrusive, for training.\nThe finalized model can either be deployed on a server or edge devices.\nFederated learning circumvents this problem by shifting compute responsibility onto clients, letting a central server aggregate the resulting artifacts.\nIn FedAvg, edge devices train on locally available data, while the server averages the finished models [1]}.\nThis is repeated for multiple rounds of communication.\nThe server never has possession of any potentially sensitive data.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:50,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;ce0478f1-e907-4432-a48d-af90aa633f35&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;When data is independently and identically distributed (IID), federated learning algorithms converge rapidly.\nFedAvg takes as a few as 18 communication rounds to reach 99% accuracy for 100 device federated MNIST [1]}.\nWhen the client devices are statistically heterogeneous, learning a single global model becomes very difficult [2]}.\nIn such cases, it is more natural to learn personalized models.\nStill, there are circumstances where a single global model is desired.\nFor example, different online businesses might want a model capable of flagging a wide spectrum of fraudulent schemes.\nsince fraudsters are often repeat offenders, scams attempted on one platform may be reused on others.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:51,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;eb5bb278-8016-4107-9bf3-f8f8acb4db17&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Techniques for faster federated learning on non-IID data range from the simple to the complex.\nOn the simple end, Momentum Federated Learning averages the momenta of different devices into a global momentum which is distributed at the start of each round [1]}.\nThis enables clients to use momentum gradient descent as their optimizer, provably increasing the rate of convergence.\nOn the complex end, SCAFFOLD uses the gradient of the global model as a control variate to address drifting among client updates [2]}.\nNotably these two methods double the amount of information submitted by devices to the server.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:52,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;aab22439-f975-4110-a62e-82b21f734c88&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;To deal with the communication and scalability challenges introduced by above methods, efforts has been made to reduce the amount of rounds required for server-client communication [1]}.\nFedPAQ has made an initial effort [2]} to periodically average and quantize the client models before making the server update.\nThen, periodic averaging for both server and client models followed up quickly [3]}.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:53,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;fa8703b9-1117-475f-ae8d-fb939bbabdb8&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;In this paper, we take a different approach, using server averaging to accelerate convergence.\nWe justify the technique using heuristic arguments and experimentally show that it reaches a given test accuracy faster than FedAvg.\nAdditionally, we propose decay epochs for reducing client computation while maintaining non-IID performance.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:54,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;afbc9576-f872-454f-a6b2-9e602eb5fd11&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The history of stochastic gradient methods dates back to 1951, and is usually mentioned as Robbins-Monro process [1]}.\nOne technique that has historically been used to improve SGD convergence is iterate averaging [2]}, [3]}, [4]}, also often referred to as Polyak-Rupert averaging.\nRecently, the stability of an averaging scheme that considers a non-uniform average of the iterates is discussed [5]}.\nA weighted average is applied which decays in a geometric manner.\nNeu et. al., show that the same regularizing effect can be done for SGD with the linear least-squares regression problem.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;w&quot;}}},{&quot;rowIdx&quot;:55,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d42322ba-5a02-48d3-a75a-c3df36c7fc52&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Federated learning techniques heavily rely on above mentioned averaging schemes.\nFedAvg is the most popular aggregation method that averages parameters of local models element-wise.\nThere exists two major branches for improving FedAvg [1]}.\nOne is lead by FedProx [2]} that applies a proximal term to the local lost function of each client and thresholding the local updates.\nAnother approach is to proposes different averaging schemes to either save the communication cost or to improve the performance [3]}.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;w&quot;}}},{&quot;rowIdx&quot;:56,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;3af8883d-25e6-4ddf-87d4-02c16284a622&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Safa et. al. explore iterate averaging in the context of block-cyclic SGD [1]}.\nMost federated learning algorithms assume that clients are chosen uniformly.\nIn practice, devices conduct local training only when idle, with devices falling into blocks according to their timezone.\nMore formally, we want to minimize\n\\(\\operatorname{\\mathbb {E}}_{z \\sim \\mathcal {D}} f(w, z) \\text{ where } \\mathcal {D} = \\sum _{i=1}^{m} \\mathcal {D}_i\\) \n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;w&quot;}}},{&quot;rowIdx&quot;:57,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;59c00e2d-549f-499e-ad1c-358e07e5f086&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;while sampling \\(n\\)  points from \\(\\mathcal {D}_1, \\dots , \\mathcal {D}_m\\)  in order for \\(K\\)  cycles.\nIn this block-cyclic setting, SGD is worse by a factor of \\(\\sqrt{mn/K}\\) .\nHowever, learning personalized models for each block using Averaged SGD [1]}â€”taking the average of all SGD iterate as the final model parametersâ€”provides the same performance guarantees as SGD with IID sampling.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;w&quot;}}},{&quot;rowIdx&quot;:58,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;71a05e05-e4d3-457c-9904-d06d7a14eb68&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Stochastic Weight Averaging (SWA) applies Averaged SGD to deep learning [1]}.\nIzmailov et. al. note that SGD generally converges to points near the boundary of a wide flat region and that optima width has been conjectured to correlate with generalization.\nThe average of the SGD iterates then lies at the the center of this flat region.\nMoreover, to ensure coverage of this flat region, SWA uses a cyclic or a high constant learning rate.\nThis algorithm has the benefit of low computational overheadâ€”only the moving average needs to be recordedâ€“and simplicity.\nSWA does not improve the rate of convergence compared to SGD.\nIn fact, SWA converges to worse but better generalizing optima than SGD.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;w&quot;}}},{&quot;rowIdx&quot;:59,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;3436b808-2afd-400b-b6bd-e1bd450df09e&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;This paper improves upon an existing federated learning algorithm by performing periodic server-side averaging.\nThe proposed adaptation of FedAvg has three major benefits: (1) it uses iterate averaging for accelerated convergence, (2) it learns a better generalizing optima than SGD, (3) the effectiveness of FL is increased due to recycling of previously participating clients.\nWe empirically show that server averaging takes fewer rounds than FedAvg to a desired accuracy level.\nIn addition, we propose epoch decay to lower the computation costs for each client.\nEpoch decay limits the number of updates, similar to learning rate decay for SGD, and reduces the amount of computation by up to 40%.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:60,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;3bc8f89c-0854-4158-813c-809894baa58a&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;In the future, we wish to extend the server averaging to both various neural network types (i.e. attention, LSTM, etc.) and layer-wise building blocks (i.e. batch normalization layers, etc.).\nIn addition, we wish to investigate the performance of epoch decay paired with state-of-the-art update methods such as match averaging [1]}.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:61,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;3a6811f9-90ac-47c5-9b37-07f95925330c&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Contour is one of the most important object descriptors, along with texture and color. The boundary of an object in an image is encoded in contour description, which is useful in various applications, such as image retrieval [1]}, [2]}, [3]}, recognition [4]}, [5]}, [6]}, and segmentation [7]}, [8]}, [9]}, [10]}, [11]}. It is desirable to represent object boundaries compactly, as well as faithfully, but it is challenging to design such contour descriptors due to the diversity and complexity of object shapes.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:62,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;6782fdc5-35b5-4b87-8d30-ef3704395e2a&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Early contour descriptors were developed mainly for image retrieval [1]}, [2]}, [3]}, [4]}. An object contour can be simply represented based on the area, circularity, and/or eccentricity of the object [5]}. For more precise description, there are several approaches, including shape signature [6]}, [7]}, [8]}, structural analysis [9]}, [10]}, [11]}, [12]}, [13]}, spectral analysis [2]}, [3]}, and curvature scale space (CSS) [1]}, [17]}.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:63,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;29a190e8-5669-4900-951c-759d211e625e&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Recently, contour descriptors have been incorporated into deep-learning-based object detection, tracking, and segmentation systems. In [1]}, bounding boxes are replaced by polygons to enclose objects more tightly. In [2]}, ellipse fitting is done to produce a rotated box of a target object to be tracked. For instance segmentation, contour-based techniques have been proposed that represent pixelwise masks by contour descriptors based on shape signature [3]} or polynomial fitting [4]}. Even though these descriptors can localize an object effectively, they may fail to reconstruct the object boundary faithfully. Also, they consider the structural information of an individual object only, without exploiting the shape correlation between different objects.\n<FIGURE>&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:64,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;258c9349-2eae-453e-aeba-bcc3e101a13d&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;In this paper, we propose novel contour descriptors, called eigencontours, based on low-rank approximation. First, we construct a contour matrix containing all object boundaries in a training set. Second, we decompose the contour matrix into eigencontours, based on the best rank-\\(M\\)  approximation of singular value decomposition (SVD) [1]}. Then, each contour is represented by a linear combination of the \\(M\\)  eigencontours, as illustrated in Figure REF . Also, we incorporate the eigencontours into an instance segmentation framework. Experimental results demonstrate that the proposed eigencontours can represent object boundaries more effectively and more efficiently than the existing contour descriptors [2]}, [3]}. Moreover, utilizing the existing framework of YOLOv3 [4]}, the proposed algorithm yields promising instance segmentation performances on various datasets â€” KINS [5]}, SBD [6]}, and COCO2017 [7]}.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:65,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;ab1985af-9e02-4e82-8632-0ee559db99f2&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\nWe propose the notion of eigencontours â€” data-driven contour descriptors based on SVD â€” to represent object boundaries as faithfully as possible with a limited number of coefficients.\n\nThe proposed algorithm can represent object boundaries more effectively and more efficiently than the existing contour descriptors.\n\nThe proposed algorithm outperforms conventional contour-based techniques in instance segmentation.\n\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:66,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;2ac7b224-3e3b-4914-bf7a-07ee5c450339&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The goal of contour description is to represent the boundary of an object in an image compactly and faithfully. Simple contour descriptors are based on the area, circularity, and/or eccentricity of an object [1]}, and basic geometric shapes, such as rectangles and ellipses, can be also used. However, these simple descriptors cannot preserve the original shape of an object faithfully [2]}, [3]}.\nFor more sophisticated description, there are four types of approaches: shape signature [4]}, [5]}, [6]}, structural analysis [7]}, [8]}, [9]}, [10]}, spectral analysis [11]}, [12]}, and CSS [13]}, [14]}.\nFirst, a shape signature is a one-dimensional function derived from the boundary coordinates of an object. For example, a polar coordinate system is set up with respect to the centroid of an object. Then, the object boundary is represented by the \\((r, \\theta )\\)  graph, called the centroidal profile [4]}. Also, an object shape can be represented by the angle between the tangent vector at each contour point and the \\(x\\) -axis [5]}. Second, structural methods divide an object boundary into segments and approximate each segment to encode the whole boundary. In [7]}, the boundary is represented by a sequence of unit vectors with a few possible directions. In [8]}, polygonal approximation is performed to globally minimize the errors from an approximated polygon to the original boundary. In [9]}, segments of an object contour are represented by cubic polynomials. Third, in spectral methods, boundary coordinates are transformed to a spectral domain. In [11]}, a wavelet transform is used for contour description.\nIn [12]}, the Fourier descriptors are derived from the Fourier series of centroidal profiles. Fourth, in CSS [13]}, a boundary is smoothed by a Gaussian filter with a varying standard deviation. Then, the boundary is represented by the curvature zero-crossing points of the smoothed curve at each standard deviation.\n<FIGURE>&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;w&quot;}}},{&quot;rowIdx&quot;:67,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;20ac1d8a-251c-477a-9798-6480b8a96975&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Recently, attempts have been made to improve the performances of deep-learning-based vision systems. In [1]}, a bounding box for object detection is replaced by an octagon to enclose an object more tightly via polygonal approximation. In [2]}, a rotated box for a target object is determined based on ellipse fitting, in order to cope with object deformation in a visual tracking system. For instance segmentation, contour-based approaches [3]}, [4]} have been developed, which reformulate the pixelwise classification task as the boundary regression of an object. To this end, these methods encode segmentation masks into contour descriptors. In [4]}, centroidal profiles are used to describe object boundaries.\nIn [3]}, each segment of a boundary is represented by a few coefficients based on polynomial fitting. Although these methods are computationally efficient for localizing object instances, they often fail to reconstruct the boundaries of the object shapes faithfully.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;w&quot;}}},{&quot;rowIdx&quot;:68,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;5a8c8f29-1f54-4289-a6a3-1c9880e45646&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The proposed algorithm aims to represent an object boundary as faithfully as possible by employing as few coefficients as possible. To this end, we develop eigencontours based on the best low-rank approximation property of SVD.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;w&quot;}}},{&quot;rowIdx&quot;:69,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;6f1d02ce-f89b-4930-a418-867fa8daf441&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Dimension of eigencontour space (\\(M\\) ):\nTableÂ REF  lists the AUC-\\(\\cal {F}\\)  performances of the proposed algorithm on the SBD validation dataset according to the dimension, \\(M\\) , of the eigencontour space. At \\(M=10\\) , the proposed algorithm yields poor scores, since object boundaries are too simplified and not sufficiently accurate. At \\(M=20\\) , it provides the best results. At \\(M=30\\) , it yields similarly good results. However, at \\(M=40\\) , the performances are degraded further, which indicates that a high-dimensional space does not always lead to better results. It is more challenging to regress more variables reliably. There is a tradeoff between accuracy and reliability. In this test, \\(M=20\\)  achieves a good tradeoff.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;m&quot;}}},{&quot;rowIdx&quot;:70,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1ebd7eb0-42ed-44ec-a646-4dba1cd36855&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Categorical eigencontour space:\nThe proposed eigencontours are data-driven descriptors, which depend on the distribution of object contours in a dataset. Thus, different eigencontours are obtained for different data. Let us consider two options for constructing eigencontour spaces: categorial construction and universal construction.\nIn the categorial construction, eigencontours are determined for each category in a dataset. In the universal construction, they are determined for all instances in all categories.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;m&quot;}}},{&quot;rowIdx&quot;:71,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;389c1547-abc7-4ef9-a5d2-781bb9c944c4&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;For the two options, \\(\\cal {F}\\)  score curves are presented according to the dimension \\(M\\)  in the supplemental document. Table REF  compares the area under curve performances of the \\(\\cal {F}\\)  curves up to \\(M=18\\) . The categorial construction provides better performances than the universal construction, because it considers similar shapes in the same category only. In COCO2017, the gap between the two options is the smallest. This is because some object shapes are not properly represented due to occlusions and thus COCO2017 objects exhibit low intra-category correlation. In contrast, in KINS, whole contours are well represented because occluded regions are also annotated. Hence, the gap between the two options is the largest.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;m&quot;}}},{&quot;rowIdx&quot;:72,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;7cd6bac9-81a1-482a-9c69-605195030d7f&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Limitations:\nThe proposed eigencontours represent typical contour patterns in a dataset. Thus, if object contour patterns differ among datasets, the eigencontours for a dataset may be effective for that particular dataset only. To assess the dependency of eigencontours on a dataset, we conduct cross-validation tests between datasets in the supplemental document.\n<TABLE><TABLE>&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;m&quot;}}},{&quot;rowIdx&quot;:73,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d5854744-09f5-4bf0-bb33-ee4ed71bd3b7&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;We proposed novel contour descriptors, called eigencontours, based on low-rank approximation. First, we constructed a contour matrix containing all contours in a training set. Second, we approximated the contour matrix, by performing the best rank-\\(M\\)  approximation. Third, we represent an object boundary by a linear combination of the \\(M\\)  eigencontours. Experimental results demonstrated that the proposed eigencontours can represent object boundaries more effectively and more faithfully than the existing methods. Moreover, the proposed algorithm yields meaningful instance segmentation performances.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:74,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;ff7c1911-a348-434c-ae84-e2924bb12d28&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Scientists increasingly use machine learning (ML) in their daily work. This development is not limited to natural sciences like ecology  or neuroscience , but also extends to social sciences such as psychology  and archaeology .\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:75,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;7151e7c7-ce93-46b8-97cc-2571a79df978&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Â Â Â Â In particular, when building predictive models for problems with complex data structures, ML outcompetes classical statistical models in both performance and convenience. Impressive recent examples of successful prediction models in science include the automated particle tracking at CERN , or DeepMind's AlphaFold, which has essentially solved the protein structure prediction challenge CASP . In such examples, some see a paradigm shift towards theory-free science that â€œlets the data speakâ€ , , , . Indeed, prediction is one of the core aims of science , , but so are, as philosophers of science and statisticians emphasize, explanation and knowledge generation , , . Focusing exclusively on prediction may therefore represent a historical step back , .\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:76,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;004cc382-3ba7-4af2-b4ff-26ba95978428&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Â Â Â Â What hinders scientists from using ML models to gain real-world insights is the model complexity and the unclear connection between the model and the described phenomenon â€” the so-called opacity problem , . Interpretable machine learning (IML, also called XAI for eXplainable artificial intelligence) aims to solve the opacity problem by analyzing model elements or inspecting model properties . Various expectations are put into IML by different stakeholders with diverse goals , including scientists , ML engineers , regulatory bodies , and laypeople . Due to this diversity of goals, stakeholders, and requirements, IML has been criticized for lacking a well-defined goal .\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:77,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;f1f5210a-6d48-4dfa-8b17-32b06daf335f&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Â Â Â Â Nevertheless, scientists increasingly use IML techniques in their research.e.g. for predicting personality traits from smartphone usage , forecasting crop yield , , or analyzing seasonal precipitation forecasts  Although researchers are aware that their IML analysis remains just a model description, it is often implied that the explanations, associations, or effects found also extend to the corresponding real-world properties. Unfortunately, drawing inferences with IML can currently be epistemically problematic because the interpretation techniques are not defined for that purpose . In particular, the difference between model-only versus phenomenon explanations is often unclear ,  and a theory to quantify the uncertainty of interpretations is lacking .\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:78,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;7d25e83e-cbfb-47e4-8d54-426ebdd8b9b1&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;In recent years, digital libraries have moved towards open science and open access with several large scholarly datasets being constructed. Most popular datasets include millions of papers, authors, venues, and other information. Their large size and heterogeneous contents make it very challenging to effectively manage, explore, and utilize these datasets. The knowledge graph has emerged as a universal data format for representing knowledge about entities and their relationships in such complicated data. The main part of a knowledge graph is a collection of triples, with each triple \\( (h, t, r) \\)  denoting the fact that relation \\( r \\)  exists between head entity \\( h \\)  and tail entity \\( t \\) . This can also be formalized as a labeled directed multigraph where each triple \\( (h, t, r) \\)  represents a directed edge from node \\( h \\)  to node \\( t \\)  with label \\( r \\) . Therefore, it is straightforward to build knowledge graphs for scholarly data by representing natural connections between scholarly entities with triples such as (AuthorA, Paper1, write) and (Paper1, Paper2, cite).\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:79,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;be8647fa-47ab-4abc-9692-04fdb62a7f74&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Notably, instead of using knowledge graphs directly in some tasks, we can model them by knowledge graph embedding methods, which represent entities and relations as embedding vectors in semantic space, then model the interactions between them to solve the knowledge graph completion task. There are many approaches [1]} to modeling the interactions between embedding vectors resulting in many knowledge graph embedding methods such as ComplEx [2]} and CP\\( _h \\)  [3]}. In the case of word embedding methods such as word2vec, embedding vectors are known to contain rich semantic information that enables them to be used in many semantic applications [4]}. However, the semantic structures in the knowledge graph embedding space are not well-studied, thus knowledge graph embeddings are only used for knowledge graph completion but remain absent in the toolbox for data analysis of heterogeneous data in general and scholarly data in particular, although they have the potential to be highly effective and efficient. In this paper, we address these issues by providing a theoretical understanding of their semantic structures and designing a general semantic query framework to support data exploration.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:80,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;38cf6de4-fd73-40f8-b931-114da0e747e1&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;For theoretical analysis, we first analyze the state-of-the-art knowledge graph embedding model CP\\( _h \\)  [1]} in comparison to the popular word embedding model word2vec skipgram [2]} to explain its components and provide understandings to its semantic structures. We then define the semantic queries on the knowledge graph embedding spaces, which are algebraic operations between the embedding vectors in the knowledge graph embedding space to solve queries such as similarity and analogy between the entities on the original datasets.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:81,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;4bc58645-1d7f-431c-a254-22ba0bdf2eaf&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Based on our theoretical results, we design a general framework for data exploration on scholarly data by semantic queries on knowledge graph embedding space. The main component in this framework is the conversion between the data exploration tasks and the semantic queries. We first outline the semantic query solutions to some traditional data exploration tasks, such as similar paper prediction and similar author prediction. We then propose a group of new interesting tasks, such as analogy query and analogy browsing, and discuss how they can be used in modern digital libraries.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:82,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;388310ad-34a1-4321-b88d-de5d4f83751a&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;In this paper, we studied the application of knowledge graph embedding in exploratory data analysis. We analyzed the CP\\( _h \\)  model and provided understandings to its semantic structures. We then defined the semantic queries on knowledge graph embedding space to efficiently approximate some operations on heterogeneous data such as scholarly data. We designed a general framework to systematically apply semantic queries to solve scholarly data exploration tasks. Finally, we outlined and discussed the solutions to some traditional and pioneering exploration tasks emerged from the semantic structures of the knowledge graph embedding space.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:83,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;764a6cda-e150-4341-971b-9e971c906cee&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;This paper is dedicated to the theoretical foundation of a new approach and discussions of emerging tasks, whereas experiments and evaluations are left for the future work. There are several other promising directions for future research. One direction is to explore new tasks or new solutions of traditional tasks using the proposed method. Another direction is to implement the proposed exploration tasks on real-life digital libraries for online evaluation.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:84,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;82e5ee19-669b-425c-87bd-1e6f8f7e7b70&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\nIn recent years, machine learning algorithms have been increasingly used to inform decisions with far-reaching consequences (e.g. whether to release someone from prison or grant them a loan), raising concerns about their compliance with laws, regulations, societal norms, and ethical values. Specifically, machine learning algorithms have been found to discriminate against certain â€œsensitiveâ€ demographic groups (e.g. racial minorities), prompting a profusion of algorithmic fairness researchÂ [1]}, [2]}, [3]}, [4]}, [5]}, [6]}, [7]}, [8]}, [9]}, [10]}, [11]}, [12]}, [13]}, [14]}, [15]}, [16]}. Algorithmic fairness literature aims to develop fair machine learning algorithms that output non-discriminatory predictions.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:85,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;274c01a8-01f4-4118-9db4-166c0b5c7d41&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\nFair learning algorithms typically need access to the sensitive data in order to ensure that the trained model is non-discriminatory.\nHowever, consumer privacy laws (such as the E.U. General Data Protection Regulation) restrict the use of sensitive demographic data in algorithmic decision-making. These two requirementsâ€“fair algorithms trained with private dataâ€“presents a quandary: how can we train a model to be fair to a certain demographic if we don't even know which of our training examples belong to that group?\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:86,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;5d0af241-6a11-4aa4-a538-97564180275c&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\nThe works ofÂ [1]}, [2]} proposed a solution to this quandary using secure multi-party computation (MPC), which allows the learner to train a fair model without directly accessing the sensitive attributes.\nUnfortunately, asÂ [3]} observed, MPC does not prevent the trained model from leaking sensitive data. For example, with MPC, the output of the trained model could be used to infer the race of an individual in the training data setÂ [4]}, [5]}, [6]}, [7]}.\nTo prevent such leaks, [3]} argued for the use of differential privacyÂ [9]} in fair learning. Differential privacy (DP) provides a strong guarantee that no company (or adversary) can learn much more about any individual than they could have learned had that individual's data never been used.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:87,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;b7f3d840-9d11-446b-883d-9c0863e156a3&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\nSinceÂ [1]}, several follow-up works have\nproposed alternate approaches to DP fair learningÂ [2]}, [3]}, [4]}, [5]}, [6]}, [7]}. As shown inÂ fig: related work table,\neach of these approaches suffers from at least two critical shortcomings.\nIn particular, none of these methods have convergence guarantees when mini-batches of data are used in training. In training large-scale models, memory and efficiency constraints require the use of small minibatches in each iteration of training (i.e. stochastic optimization). Thus, existing DP fair learning methods cannot be used in such settings since they require computations on the full training data set in every iteration. SeeÂ app: related work for a more comprehensive discussion of related work.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:88,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;9a11b4e1-1524-4c43-9243-1799245cd66a&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Our Contributions: In this work, we propose a novel algorithmic framework for DP fair learning. Our approach builds on the non-private fair learning method ofÂ [1]}. We consider a regularized empirical risk minimization (ERM) problem where the regularizer penalizes fairness violations, as measured by the Exponential RÃ©nyi Mutual Information.\nUsing a result fromÂ [1]}, we reformulate this fair ERM problem as a min-max optimization problem. Then, we use an efficient differentially private variation of stochastic gradient descent-ascent (DP-SGDA) to solve this fair ERM min-max objective.\nThe main features of our algorithm are:\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:89,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;7a3e3a77-3253-43e7-8c7e-f3bf397601a2&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\nGuaranteed convergence for any privacy and fairness level, even when mini-batches of data are used in each iteration of training (i.e. stochastic optimization setting). As discussed, stochastic optimization is essential in large-scale machine learning scenarios. Our algorithm is the first stochastic DP fair learning method with provable convergence.\n\nFlexibility to handle non-binary classification with multiple (non-binary) sensitive attributes (e.g. race and gender) under different fairness notions such as demographic parity or equalized odds. In each of these cases, our algorithm is guaranteed to converge.\n\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:90,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;9e737725-9354-43db-b300-bcc4d256d71c&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Empirically, we show that our method outperforms the previous state-of-the-art methods in terms of fairness vs. accuracy trade-off across all privacy levels. Moreover, our algorithm is capable of training with mini-batch updates and can handle non-binary target and non-binary sensitive attributes. By contrast, existing DP fairness algorithms could not converge in our stochastic/non-binary experiment.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:91,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;ec259b81-6d2f-4e81-af8e-e1a77f3cafd2&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\nA byproduct of our algorithmic developments and analyses is the first DP convergent algorithm for nonconvex min-max optimization: namely, we provide an upper bound on the stationarity gap of DP-SGDA for solving problems of the form \\(\\min _{\\theta } \\max _{W} F(\\theta , W)\\) , where \\(F(\\cdot , W)\\)  is non-convex. We expect this result to be of independent interest to the DP optimization community. Prior works that provide convergence results for DP min-max problems have assumed that \\(F(\\cdot , W)\\)  is either (strongly) convexÂ [1]}, [2]} or satisfies a generalization of strong convexity known as the Polyak-Åojasiewicz (PL) conditionÂ [3]}.\n<FIGURE>&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:92,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;8264eb8d-21ec-406d-a29c-0bc4a813b2f9&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\nIn this section, we evaluate the performance of our proposed approach (DP-FERMI) in terms of the fairness violation vs. test error for different privacy levels. We present our results in two parts: In SectionÂ REF , we assess the performance of our method in training logistic regression models on several benchmark tabular datasets. Since this is a standard setup that existing DP fairness algorithms can handle, we are able to compare our method against the state-of-the-art baselines. We find that DP-FERMI consistently outperforms all state-of-the-art baselines across all data sets and all privacy levels. These observations hold for both demographic parity and equalized odds fairness notions.\nIn SectionÂ REF , we showcase the scalability of DP-FERMI by using it to train a deep convolutional neural network for classification on a large image dataset. InÂ app: experiments, we give detailed descriptions of the data sets, experimental setups and training procedure, along with additional results.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;m&quot;}}},{&quot;rowIdx&quot;:93,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;95322e5f-18a1-4653-93a5-c0963a4d6e4e&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\nMotivated by pressing legal, ethical, and social considerations, we studied the challenging problem of learning fair models with differentially private demographic data. We observed that existing works suffer from a few crucial limitations that render their approaches impractical for large-scale problems. Specifically, existing approaches require full batches of data in each iteration (and/or exponential runtime) in order to provide convergence/accuracy guarantees. We addressed these limitations by deriving a DP stochastic optimization algorithm for fair learning, and rigorously proved the convergence of the proposed method. Our convergence guarantee holds even for non-binary classification (with any hypothesis class, even infinite VC dimension, c.f.Â [1]}) with multiple sensitive attributes and access to random minibatches of data in each iteration. Finally, we evaluated our method in extensive numerical experiments and found that it significantly outperforms the previous state-of-the-art models, in terms of fairness-accuracy tradeoff. Further, our method provided stable results in a larger scale experiment with small batch size and non-binary targets/sensitive attributes. The potential societal impacts and limitations of our work are discussed inÂ app: societal impacts.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;d&quot;}}},{&quot;rowIdx&quot;:94,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;298d9497-25cd-47fc-b2f7-785d95521f43&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The study of differentially private fair learning algorithms was initiated byÂ [1]}. [1]} considered equalized odds and proposed two DP algorithms: 1) an \\(\\epsilon \\) -DP post-processing approach derived fromÂ [3]}; and 2) an \\((\\epsilon , \\delta )\\) -DP in-processing approach based onÂ [4]}. The major drawback of their post-processing approach is the unrealistic requirement that the algorithm have access to the sensitive attributes at test time, whichÂ [1]} admits â€œisn't feasible (or legal) in certain applications.â€ Additionally, post-processing approaches are known to suffer from inferior fairness-accuracy tradeoffs compared with in-processing methods. While the in-processing method ofÂ [1]} does not require access to sensitive attributes at test time, it comes with a different set of disadvantages: 1) it is limited to binary classification; 2) its theoretical performance guarantees require the use of the computationally inefficient (i.e. exponential-time) exponential mechanismÂ [7]}; 3) its theoretical performance guarantees require computations on the full training set and do not permit mini-batch implementations; 4) it requires the hypothesis class \\(\\mathcal {H}\\)  to have finite VC dimension.\nIn this work, we propose the first algorithm that overcomes all of these pitfalls: our algorithm is amenable to multi-way classification with multiple sensitive attributes, computationally efficient, and comes with convergence guarantees that hold even when mini-batches of \\(m < n\\)  samples are used in each iteration of training, and even when \\(\\text{VC}(\\mathcal {H}) = \\infty \\) . Furthermore, our framework is flexible enough to accommodate many notions of group fairness besides equalized odds (e.g. demographic parity, accuracy parity).\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;w&quot;}}},{&quot;rowIdx&quot;:95,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;bf2aabcd-99b3-47a4-bbd9-21252209eca5&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;FollowingÂ [1]}, several works have proposed other DP fair learning algorithms. None of these works have managed to simultaneously address all the shortcomings of the method ofÂ [1]}. The work ofÂ [3]} proposed DP and fair binary logistic regression, but did not provide any theoretical convergence/performance guarantees. The work ofÂ [4]} combined aspects of bothÂ [5]} andÂ [6]} in a two-step locally differentially private fairness algorithm. Their approach is limited to binary classification.\nMoreover, their algorithm requires \\(n/2\\)  samples in each iteration (of their in-processing step), making it impractical for large-scale problems. More recently,Â [7]} devised another DP in-processing method based on lagrange duality, which covers non-binary classification problems. In a subsequent work,Â [8]} studied the effect of DP on accuracy parity in ERM, and proposed using a regularizer to promote fairness. Finally, Â [9]} provided a semi-supervised fair â€œPrivate Aggregation of Teacher Ensemblesâ€ framework. A shortcoming of each of these three most recent works is their lack of theoretical convergence or accuracy guarantees. In another vein, some works have observed the disparate impact of privacy constraints on demographic subgroupsÂ [10]}, [11]}.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;w&quot;}}},{&quot;rowIdx&quot;:96,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;52f44d2d-dce4-49a1-ac6d-bbe03471f633&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Nam dui ligula, fringilla a, euismod sodales,\nsollicitudin vel, wisi. Morbi auctor lorem non justo. Nam lacus\nlibero, pretium at, lobortis vitae, ultricies et, tellus. Donec\naliquet, tortor sed accumsan bibendum, erat ligula aliquet magna,\nvitae ornare odio metus a mi. Morbi ac orci et nisl hendrerit\nmollis. Suspendisse ut massa. Cras nec ante. Pellentesque a nulla.\nCum sociis natoque penatibus et magnis dis parturient montes,\nnascetur ridiculus mus. Aliquam tincidunt urna. Nulla ullamcorper\nvestibulum turpis. Pellentesque cursus luctus mauris.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:97,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;67c47121-3a1c-4195-9489-4cb56370daa8&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Nulla malesuada porttitor diam. Donec felis\nerat, congue non, volutpat at, tincidunt tristique, libero. Vivamus\nviverra fermentum felis. Donec nonummy pellentesque ante. Phasellus\nadipiscing semper elit. Proin fermentum massa ac quam. Sed diam\nturpis, molestie vitae, placerat a, molestie nec, leo. Maecenas\nlacinia. Nam ipsum ligula, eleifend at, accumsan nec, suscipit a,\nipsum. Morbi blandit ligula feugiat magna. Nunc eleifend consequat\nlorem. Sed lacinia nulla vitae enim. Pellentesque tincidunt purus\nvel magna. Integer non enim. Praesent euismod nunc eu purus. Donec\nbibendum quam in tellus. Nullam cursus pulvinar lectus. Donec et mi.\nNam vulputate metus eu enim. Vestibulum pellentesque felis eu\nmassa.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:98,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;580035ca-35ab-4e20-90a2-4c76e1a94bdf&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Concurrent with steady progress towards improving the accuracy and efficiency of 3D object detector algorithms [1]}, [2]}, [3]}, [4]}, [5]}, [6]}, [7]}, [8]}, [9]}, [10]}, [11]}, LiDAR sensor hardware has been improving in maximum range and fidelity, in order to meet the needs of safe, high speed driving. Some of the latest commercial LiDARs can sense up to 250mÂ [12]} and 300mÂ [13]} in all directions around the vehicle. This large volume coverage places strong demands for efficient and accurate 3D detection methods.\n<FIGURE>&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}},{&quot;rowIdx&quot;:99,&quot;cells&quot;:{&quot;_id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;b8e291bf-5052-45b9-9ebc-b15fd15d9e8e&quot;},&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Grid based methods [1]}, [2]}, [3]}, [4]}, [5]} divide the 3D space into voxels or pillars, each of these being optionally encoded using PointNet [6]}. Dense convolutions are applied on the grid to extract features. This approach is inefficient for large grids which are needed for long range sensing or small object detection. Sparse convolutions [7]} scale better to large detection ranges but are usually slow due to the inefficiencies of applying to all points.\nRange images are native, dense representations, suitable for processing point clouds captured by a single LiDAR. Range image based methods [8]}, [9]} perform convolutions directly over the range in order to extract point cloud features.\nSuch models scale well with distance, but tend to perform less well in occlusion handling, accurate object localization, and for size estimation. A second stage, refining a set of initial candidate detections, can help mitigate some of these quality issues, at the expense of significant computational cost.\n&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;i&quot;}}}],&quot;truncated&quot;:true},&quot;paginationData&quot;:{&quot;pageIndex&quot;:0,&quot;numItemsPerPage&quot;:100,&quot;numTotalItems&quot;:520053,&quot;offset&quot;:0,&quot;length&quot;:100}},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzA4MCwic3ViIjoiL2RhdGFzZXRzL3NhaWVyL3VuYXJYaXZlX2ltcmFkX2NsZiIsImV4cCI6MTc0MjkyNjY4MCwiaXNzIjoiaHR0cHM6Ly9odWdnaW5nZmFjZS5jbyJ9.JBFB3XCzduvrSj80JMxcdY_t8NCjO_QODp4mXE4fJIT6aHxvfoyB3CixHg0ByXvri2Y76ISBrUFeTclprUsGDA&quot;,&quot;displayUrls&quot;:true},&quot;dataset&quot;:&quot;saier/unarXive_imrad_clf&quot;,&quot;isGated&quot;:false,&quot;isPrivate&quot;:false,&quot;hasParquetFormat&quot;:false,&quot;author&quot;:{&quot;_id&quot;:&quot;63c56270cf44a436ae49d26c&quot;,&quot;avatarUrl&quot;:&quot;/avatars/57faf531eab7c992d4273747c8691abc.svg&quot;,&quot;fullname&quot;:&quot;Tarek Saier&quot;,&quot;name&quot;:&quot;saier&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;compact&quot;:true}"><div class="flex flex-col overflow-hidden shadow-xs mx-auto mb-10 rounded-lg border pt-2  px-2.5"><div class="mb-2 flex flex-wrap items-center gap-2"><div class="mr-auto flex items-center"><svg class="mr-1 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	<div class="whitespace-nowrap font-semibold">Dataset Viewer</div>
	</div>
				<a href="/datasets/saier/unarXive_imrad_clf/tree/refs%2Fconvert%2Fparquet/default" class="group mr-1 text-xs text-gray-400 max-sm:hidden"><svg class="text-[.6rem] mr-1 inline -translate-y-px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill="currentColor" d="M12 10H6.78A11 11 0 0 1 27 16h2A13 13 0 0 0 6 7.68V4H4v8h8zm8 12h5.22A11 11 0 0 1 5 16H3a13 13 0 0 0 23 8.32V28h2v-8h-8z"></path></svg>
						<span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Auto-converted</span> to Parquet
					</a>
				<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>API</button>
					<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path d="M9.80603 2.86737H3.56107C3.37704 2.86737 3.20055 2.94048 3.07042 3.0706C2.94029 3.20073 2.86719 3.37723 2.86719 3.56126V9.80622C2.86719 9.99025 2.94029 10.1667 3.07042 10.2969C3.20055 10.427 3.37704 10.5001 3.56107 10.5001H9.80603C9.99006 10.5001 10.1666 10.427 10.2967 10.2969C10.4268 10.1667 10.4999 9.99025 10.4999 9.80622V3.56126C10.4999 3.37723 10.4268 3.20073 10.2967 3.0706C10.1666 2.94048 9.99006 2.86737 9.80603 2.86737Z" fill="currentColor" fill-opacity="0.3"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M2.40942 1.66191C2.05175 1.66191 1.7618 1.95186 1.7618 2.30953V6.76191H1.43799V2.30953C1.43799 1.77303 1.87291 1.3381 2.40942 1.3381H6.45704V1.66191H2.40942Z" fill="currentColor"></path></svg>Embed</button>

					<button class="bg-linear-to-b shadow-xs flex items-center gap-1.5 rounded-full border from-white to-red-100/90 px-2 py-0.5 text-xs font-medium text-[#2D3648] transition-shadow hover:shadow-inner dark:from-gray-900 dark:to-red-800/30 dark:text-gray-100 dark:hover:shadow-inner dark:hover:shadow-red-800/30" ><svg class="h-3.5 w-3.5 text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
						<span>Data Studio</span></button></div>
		<div class="flex flex-1 flex-col overflow-hidden -mx-2.5"><div class="flex flex-1 flex-col overflow-hidden"><div class="flex min-h-0 flex-1"><div class="flex flex-1 flex-col overflow-hidden"><div class="md:-mx-2.5 flex min-w-0 flex-wrap border-t"><div class="flex min-w-0 flex-1 flex-wrap"><div class="grid flex-1 grid-cols-1 overflow-hidden text-sm md:grid-cols-2 md:place-content-center sm:mx-2.5"><label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-950 md:border-r md:border-r-0 hidden" title="default"><span class="text-gray-500">Subset (1)</span>
			<div class="flex items-center whitespace-nowrap"><span class="truncate">default</span>
				<span class="mx-2 text-gray-500">Â·</span>
					<span class="text-gray-500">530k rows</span>
				<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
			<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Subset (1)"><option value="default" selected>default (530k rows)</option></optgroup></select></label>
		<label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-900 md:border-r md:border-r" title="train"><div class="text-gray-500">Split (3)</div>
				<div class="flex items-center overflow-hidden whitespace-nowrap"><span class="truncate">train</span>
					<span class="mx-2 text-gray-500">Â·</span>
						<span class="text-gray-500">520k rows</span>
					<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
				<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Split (3)"><option value="train" selected>train (520k rows)</option><option value="validation" >validation (5k rows)</option><option value="test" >test (5k rows)</option></optgroup></select></label></div></div>
								</div>

							<div class="flex min-h-0 flex-1 flex-col ">
	<div class="bg-linear-to-r text-smd relative flex items-center dark:border-gray-900 dark:bg-gray-950 false border-t [&amp;:has(:focus)]:from-gray-50 [&amp;:has(:focus)]:to-transparent [&amp;:has(:focus)]:to-20% dark:[&amp;:has(:focus)]:from-gray-900"><form class="flex-1"><svg class="absolute left-3 top-1/2 transform -translate-y-1/2 pointer-events-none text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				<input disabled class="outline-hidden h-9 w-full border-none bg-transparent px-1 pl-9 pr-3 placeholder:text-gray-400 " placeholder="Search this dataset" dir="auto"></form>
			<div class="flex items-center gap-2 px-2 py-1"><button type="button" class="hover:bg-yellow-200/70 flex items-center gap-1 rounded-md border border-yellow-200 bg-yellow-100 pl-0.5 pr-1 text-[.8rem] leading-normal text-gray-700 dark:border-orange-500/25 dark:bg-orange-500/20 dark:text-gray-300 dark:hover:brightness-110 hidden"><div class="rounded-sm bg-yellow-300 px-1 font-mono text-[.7rem] font-bold text-black dark:bg-yellow-700 dark:text-gray-200">SQL
	</div>
	Console
</button></div></div>


<div class="flex flex-1 flex-col overflow-hidden min-h-64 border-t">
		

<div class="max-h-96 relative overflow-auto"><table class="w-full table-auto rounded-lg font-mono text-xs text-gray-900"><thead class="shadow-xs sticky left-0 right-0 top-0 z-1 bg-white align-top"><tr class="space-y-54 h-full min-w-fit divide-x border-b text-left"><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">_id
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['Â·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="130" height="30" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="132" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">36</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">36</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">text
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['Â·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">200</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">328k</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">label
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['Â·']">classes</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><defs><clipPath id="rounded-bar"><rect x="0" y="0" width="130" height="8" rx="4"></rect></clipPath><pattern id="hatching" patternUnits="userSpaceOnUse" patternTransform="rotate(-45)" height="1" width="5"><line y1="0" class="stroke-gray-400 dark:stroke-gray-500/80" stroke-width="3" y2="1" x1="2" x2="2"></line></pattern><pattern id="hatching-faded" patternUnits="userSpaceOnUse" patternTransform="rotate(-45)" height="1" width="5"><line y1="0" class="stroke-gray-100 dark:stroke-gray-500/20" stroke-width="3" y2="1" x1="2" x2="2"></line></pattern></defs><g height="8" style="transform: translateY(20px)" clip-path="url(#rounded-bar)"><g style="transform: scaleX(1.0153846153846153) translateX(-1px)"><g><rect class="fill-indigo-500 dark:fill-indigo-600/80" x="1" y="0" width="64.19300340542215" height="8" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" x="67.19300340542215" y="0" width="20.280479105014297" height="8" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" x="89.47348251043644" y="0" width="14.109858033700412" height="8" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" x="105.58334054413686" y="0" width="13.714148365647349" height="8" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" x="121.2974889097842" y="0" width="7.702511090215804" height="8" fill-opacity="1"></rect></g></g></g><g style="transform: scaleX(1.0153846153846153) translateX(-1px)"><g><rect class="fill-white cursor-pointer" x="0" y="0" width="66.19300340542215" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="66.19300340542215" y="0" width="22.280479105014297" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="88.47348251043644" y="0" width="16.109858033700412" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="104.58334054413686" y="0" width="15.714148365647349" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="120.2974889097842" y="0" width="9.702511090215804" height="28" fill-opacity="0"></rect></g></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 max-w-full overflow-hidden text-ellipsis whitespace-nowrap">5
				values</div></div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th></tr></thead>
			<tbody class="h-16 overflow-scroll"><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="0"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">7b7f37ca-e70e-416d-927f-2fa3db62e3d5</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Food recommendation has become an essential method to help users adopt healthy dietary habitsÂ [1]}.
The task of computationally providing food and diet recommendations is challenging, as thousands of food items/ingredients have to be collected, combined in innovative ways, and reasoned overÂ [2]}.
Furthermore, there are many facets to the foods we consume, such as our ethnic identities, socio-demographic backgrounds, life-long preferences, all of which can inform our perspectives about the food we choose to consume to lead healthy lives.
Food recommendation can get even more complicated when the food options available to an individual are further constrained because of a group setting
(e.g., the seafood allergy of one family member may preclude recipes including shrimp to be recommended to the whole group). There is `no size fits all,' and even dietetic professionals have raised concerns that such varied dimensions need to be incorporated in their food recommendation adviceÂ [3]}.
Such varied dimensions set up a need to provide food-related explanations to enhance the users' trust in recommendations made by food recommender systems, both automatic and human-driven, as users are more likely to follow the advice when the reasons for the advice are provided understandably.
Although explanations could help users trust in recommendations and encourage them to follow good eating habits, the inclusion of explanations into food recommender systems has not yet received the interest it deserves in the available literatureÂ [1]}.
Therefore, this work aims to bridge the gap between existing food recommendation systems by providing semantic modeling of explanations required in the complex and ever-expanding food and diet domain.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="1"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0f83b85b-e18d-4055-b5aa-9bf7096c42cb</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">We introduce and discuss the FEO that extends the Explanation Ontology [1]} and the FoodKG (a food knowledge graph that uses a variety of food sources) [2]} to model explanations in the food domain, a connection that is currently lacking in the current literatureÂ [3]}.
Our ontology can be classified under the post-hoc wing of Explainable Artificial Intelligence(XAI) and aims to interpret the results of black box AI recommender systems in a human-understandable manner [4]}, [5]}. Accordingly, using a recommender system agnostic model, we aim to retroactively create connections between the system and the recommendation,including modeling user details, such as allergies and likes, system details, such as location and time, and question details, such as parameters.
We then assemble explanations by querying the ontology for different templates of knowledge types, defined using formalizations of explanation types.
We add structure to the auxiliary modeling of the user, and the system, which we find are important components of comprehensive explanations to represent a range of explanation types [1]} (e.g., contextual, contrastive, counterfactual explanations) to model food-specific explanations, which would complement personalized, knowledge-based food recommendation applications such as the `Health Coach,' a healthy food recommendation serviceÂ [7]}.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="2"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">61d8f60a-917b-4807-87c2-5b7e5c07f0ae</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Prior work has shown that users seek answers and reasoning for nutrition and food questions they might have
Â [1]}, [2]}, [3]}, [4]}, [5]}, [6]}.
However, users are increasingly concerned with the evidence and reasoning that lead to those claims.
Applying logic, reasoning, and querying on food and culinary arts have captured information, such as food categorization in FoodOnÂ [7]}, recipes, and associated information in RecipeDBÂ [8]}, and have brought together disparate sources of food informationÂ [9]}, [10]}.
In the area of food recommendations, many existing approaches recommend recipes based on the recipe content (e.g., ingredients) Â [5]}, [1]}, [6]}, user behavior history (e.g., eating history)Â [1]}, [2]}, [3]}, or dietary preferencesÂ [4]}, [3]}. However, none of these systems provide the rationales for why the food was recommended, as these systems are utilizing black-box, deep learning models. Conversely, while there are systems that employ post hoc XAI methods to provide explanations for opaque AI systems [19]}, [20]}, they have not yet been applied in the food recommendation domain.
Our work differs from such previous works because we leverage a greater degree of explicit, semantic information about foods and other related semantically annotated data in generating explanations about recommending a food item or answering specific questions about a particular recommendation.
Additionally, the need to provide more user-centered explanations that help users improve trust and understanding of AI systems, and the information used for recommendations, has been gaining attention [21]} lately. There have been some conceptual frameworks [22]} and ontologies [23]} that attempt to model explanations from an end-user perspective. In our work, we aim to ground these more general-purpose efforts for the food domain.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">w</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="3"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">13508314-3af5-446f-b353-7d1105b1ce8e</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">[1]} describe a system based on logical reasoning that supports monitoring the users' behaviors and persuades them to follow healthy lifestyles, including recommending suitable food items, with natural language explanations[1]}. Their system performs reasoning to understand whether the users follow an unhealthy behavior regarding a food intake input.
Then the system generates the persuasion message with explanations using natural language templatesÂ [1]}.
Our proposed, ontology-based method for generating explanations is complementary to their approach because we provide support for various types of explanations, not just trace-based explanations derived from templates for explaining the reasoner result.
We believe that by supporting different types of explanations, system developers will be supporting more user-friendly interfaces for personalized, consumer-facing applications [4]}.
The primary aim for FEO is usage in more interactive or conversational food recommendations, for example, in a personalized health recommendation app.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">w</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="4"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1587047b-085e-41aa-8e80-6481f716f583</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">We employ a task-based evaluation [1]} for our ontology using three main competency questions, each aimed at addressing a different explanation type that we attempted to extract from our model in FEO, as detailed in the following section. We have used competency questions as our method of evaluation as they are the accepted standard to â€œevaluate the ontological commitments that have been made" [2]}.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">m</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="5"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">f4dc6ca4-ecfd-46cc-aa29-eda892367f86</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">As our model endeavors to provide explanations and context to users that get lost in black box AI models, we chose to evaluate the FEO by its ability to provide responses to a subset of important explanation types. In tbl:explanationTable, we have included a list of previously identified explanation types and the corresponding questions that might require an explanation for its food-related recommendation.
Post-hoc explanations provide an approximation of the rationales that users might be looking for [1]}, which is what we wanted to tackle with the competency questions.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">m</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="6"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">62489ef1-231c-48cb-991b-8eb8488137e0</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">We support our choice in the selection of a subset of explanation types from Table REF  for our evaluation via competency questions with observations from recent advances in the machine learning community, where we noticed that there is a focus on methods that generate contrastive and counterfactual explanations [1]}. Moreover, contrastive, counterfactual, and contextual explanations also contain explanations with scientific evidence, everyday evidence, and system trace. Therefore, an evaluation using these explanation types would also allow FEO to include other explanation types. Hence, we have completed our initial modeling to allow for contrastive, counterfactual, and contextual explanations and framed the evaluation of our ontology by these explanation types.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">m</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="7"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">da1f6953-f422-4460-8d0b-a6c422d9755a</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">We undertook this process against the recommendations generated by the Health Coach Application, which uses machine learning techniques to assess users' dietary needs and provide recommendationsÂ [1]}.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">m</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="8"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">74f18154-445c-46c4-ac77-820d2a63878d</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">In this paper, we have discussed an ontology modeling for food and diet recommendation explanations, which aims to model and then be used to generate explanations specific to the context of the users and setting.
FEO is a domain-specific ontology where the domain concepts are abstracted up in a manner so that they can be comprehensively exposed to a user in the form of a diverse range of explanations. The class and property relationships that we detailed
enable using simple queries to get explanations that explore many different variables. We strove to maintain the simplicity of the queries in order to ensure that a non-technical user can access explanations just as effectively as a technical user. From the modeling perspective, we found that the food domain would benefit from semantically bound explanations because of the variety of questions that a user might ask and the corresponding variety of explanations that they might require. From the user perspective, we chose the food domain specifically because food and diet are something that a growing number of people are concerned with, and we believe that our ontology can empower users to make informed decisions from their food choices. We plan to continue this work to extend the range of explanations that we can provide and increase accessibility to the tool by incorporating it into a more user-facing recommendation environment.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="9"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">b38859e2-3072-4811-b009-8f13df445a9f</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Photolithography is one of the most important processes for semiconductor manufacturing Â . An exemplar photolithography system is shown in Fig.Â REF . A laser beam that goes through the integrated circuit patterns on the reticle is projected on the wafer so that the patterns are printed onto the wafer. During this process, the stage carrying the wafer (the wafer stage) needs to move steadily and precisely so that the patterns are printed accuratelyÂ [1]}.
&lt;FIGURE></span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="10"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">42a82b45-2d55-43c8-857a-6e6ae00d0db7</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">With the technological development of the semiconductor industry, manufacturers demand more precise performance from the wafer stage. To achieve the goal, researchers have developed many control strategies and applied them on the wafer stage, including iterative learning control (ILC)Â [1]}, [2]}, [3]}, [4]}, sliding mode control (SMC)Â [5]}, [6]}, [7]}, [8]}, \(H \infty \)  feedback controlÂ [9]}, multi-rate controlÂ [10]} and so on. Among them, SMC has attracted great attention for its simple implementation and robust performance in the presence of uncertainties and external disturbancesÂ [11]}, [12]}.
Beyond the basic SMC structureÂ [13]}, many advanced SMC strategies such as the modified reaching lawsÂ [14]}, boundary layer techniqueÂ [15]}, and super-twisting algorithm (STA)Â [16]} have also been proposed. The STA, focusing on improving the dynamics of the sliding variables, has been considered as one of the most effective approaches for the well-known chattering phenomenonÂ [17]}. It is also robust with respect to bounded uncertainties and disturbancesÂ [18]}, [19]}, and has been implemented successfully in practiceÂ [18]}, [21]}.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="11"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">a30dbe41-0845-4204-b74b-18f570de6865</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">To further improve the performance of SMC, fractional-order calculus is introduced to improve the state dynamics in the sliding surface and is combined with the STAÂ [1]}, [2]}. Although there are some theoretical research and successful precedents for the application of fractional-order super-twisting algorithm (FOSTA)Â [2]}, [4]}, the parametric uncertainties are not always taken into consideration, or their bounds are assumed to be small. When the amplitudes of the uncertainties or disturbances are rather large, the sliding variable in STA cannot converge to the predefined sliding surface. Instead, it only converges to an uncertainty region around the sliding surface, which inevitably brings positioning error to the systemÂ [5]}. Previous researches have tried to reduce the range of the uncertainty region to improve the precisionÂ [6]}, but the negative influence from large uncertainties remains.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="12"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">a4a45dd6-43db-4077-bdbf-07be5ada5db6</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Aiming to improve the control performance (i.e., high precision and robust performance) in the presence of large model uncertainties and disturbances, a novel adaptive neural network and fractional-order super-twisting algorithm (ANN-FSA) is proposed in this paper. Firstly, we use the radial basis function (RBF) neural network to approximate the uncertainties and disturbances in the system, and a corresponding fractional-order super-twisting controller is designed to compensate for uncertainties and disturbances. The stability of the proposed control strategy is also analyzed. Moreover, to guarantee the global convergence of the closed-loop system, an adaptive law is designed. At last, we apply the proposed controller to a wafer stage testbed. Experimental results show that the controller performs well and is robust against disturbances.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="13"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">fc40adda-f282-413d-b8bc-29f0affdf1e1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The remainder of this paper is organized as follows: SectionÂ  provides the model of the wafer stage. SectionÂ  presents the proposed controller, and the stability analysis of the controller. SectionÂ  displays the experimental setup and the experimental results with the proposed controller. Finally, SectionÂ  presents conclusions.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="14"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">9ab84482-14ea-46f1-90f7-0358109893dc</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The overall structure of our experimental system is depicted in Fig.Â REF . The control algorithm is programmed in the LabView environment on the host computer. The host computer is connected with the remote controller (PXI 7831, from National Instruments) via Ethernet, so that the control algorithm can be deployed in the LabView Real-Time system in the remote controller. The output of the controller is amplified by an amplifierÂ (TA330, from Trust Automation) and applied to the wafer stage testbed. The position of the moving part of the wafer stage is measured by a laser ranging systemÂ (from Keysight), and the measuring results are fed back to the remote controller. The nominal parameters of the wafer stage have been identified as \(\bar{A}=-1.092~s^{-1}\)  and \(\bar{B}=3.9124~m/(s^2\cdot A)\) .
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">m</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="15"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">e652aa8a-b9f6-4b10-915a-3e8fc5bcaea5</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">We implement the traditional PID controller, the SMC, the advanced FOSTA, and the proposed ANN-FSA to the wafer stage testbed to investigate the effectiveness of the proposed controller. The reference trajectory is shown in Fig.Â REF . The scan length is set as \(0.04\)  m, and the scan velocity is set as \(0.032\) Â m/s. The parameters in each controller are tuned so that the best performance of each controller is achieved. The sampling interval of the experiments are set as 1 ms. For the RBF neural network, the number of the hidden nodes is set as 5, other parameters are selected as \(c_1=[-3~-1~0~1~3]\) , \(c_2=[-7~-3~0~3~7]\) , \(b=[50~50~50~50~50]\) , \(\rho =0.2\)  and the initial value of \(\mathbf {W}\)  is set as \(\mathbf {0}\) . In the fractional-order super-twisting algorithm, \(\eta \)  and \(a\)  are selected as \(\frac{1}{2}\) , and other parameters are tuned as \(h_1=500\) , \(h_2=30\) , \(\alpha _1=0.001\)  and \(\alpha _2=175\) .
&lt;FIGURE>&lt;FIGURE>&lt;FIGURE></span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">r</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="16"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">4af855ba-1917-4364-b0d1-ab02667d4fd3</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Moreover, to study the robustness of these controllers, with all the parameters maintained the same, an extra external sinusoidal disturbance is generated and applied to the system. The amplitude and frequency of the disturbance signal are set as \(0.03\) Â m (rather large compared with referce signal) and 1Â Hz, respectively. We denote the situation without extra disturbance as CaseÂ 1 and the situation with the additional disturbance as CaseÂ 2. The tracking performance in these two cases is shown in Fig.Â REF  and Fig.Â REF , respectively.
&lt;TABLE></span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">r</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="17"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">56d37cd8-83fe-4a3e-9a2a-2d35e7a964f4</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">In Fig.Â REF , we note that all the four controllers have large tracking errors when the scanning velocity changes. The peak error of SMC is the largest among the four controllers, which is at around 50Â \(\mu \) m. Tracking error via the proposed ANN-FSA is the smallest, about 35Â \(\mu \) m. We also note that the errors of SMC and ANN-FSA decay faster than the PID controller, but that the error via the PID controller is smoother than the other two controllers when the tracking errors are small, i.e., closer to zero. Figure REF  shows that even in the presence of disturbances, the ANN-FSA can achieve the smallest tracking error. Moreover, from Fig.Â REF  and Fig.Â REF , we note that the tracking error increases when disturbances exist. This means that the PID controller is not as robust enough as SMC and ANN-FSA. To quantitatively describe the robustness, we calculate the root mean square (RMS) errors of each controller in both cases and the results are displayed in Table.Â REF . We can see that compared with traditional SMC, the precision of FOSTA is significantly improved. This is due to the introduction of the fractional-order sliding surface and the super-twisting algorithm. Further, the proposed ANN-FSA has the smallest RMS error in both CaseÂ 1 and CaseÂ 2. According to the difference values between the RMS errors in CaseÂ 1 and CaseÂ 2, we can conclude that SMC and FOSTA strategy is more robust than the PID controller. ANN-FSA remains precise in the presence of the large external disturbances. The results and comparison prove that the proposed control scheme achieves the best performance in terms of precision and robustness.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">r</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="18"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">445f6fda-7cc7-4613-b911-4257b1b73aa9</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">In this paper, an adaptive neural-network and fractional-order super-twisting algorithm was proposed and applied to a precision motion system. In this way, not only the dynamics of the states on the sliding surface was improved via the super-twisting algorithm, but also unknown model uncertainties and disturbances of the system were well compensated. Moreover, an adaptive law was derived for the neural-network-based controller so that the closed-loop system is globally convergent. Both stability analysis and experimental verification were provided. The comparison results among a PID controller, a conventional SMC, an advanced FOSTA and the proposed ANN-FSA showed that the proposed controller could achieve higher precision and better robustness than conventional controllers.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="19"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d04d3ac9-d3db-4a35-9cc5-52568934052a</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The number field sieve is the most efficient method known for solving the integer
factorization problem and the discrete logarithm problem in a finite field,
in the most general case. However there are many different variants of the number field
sieve, depending on the context. Recently, the Tower Number Field Sieve (TNFS) was
suggested as a novel approach to computing discrete logs in a finite field of
extension degree \(> 1\) . The Extended Tower Number Field Sieve (ExTNFS) is a variant
of TNFS which applies when the extension degree \(n\)  is composite, and gives the best known
runtime complexity in the medium characteristic case (see below).
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="20"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">b888a7a7-6fd5-4e32-89df-a763ea276ff9</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">We briefly discuss the asymptotics of number field sieve-type algorithms.
We define the following function:
\(L_{p^n}(\alpha ,c) = \exp ((c+o(1))(\log {p^n})^\alpha (\log {\log {p^n}})^{1-\alpha }).\) 
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="21"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">e3f7862f-da28-4b54-99a4-474b51e0d15c</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">This function describes the asymptotic complexity of a subexponential function in
\(\log {p^n}\) , which is used to asses the complexity of the number field sieve for
computing discrete logs in \(\mathbb {F}_{p^n}\) . For a given \(p^n\) , there are two important
boundaries, respectively for \(\alpha = 1/3\)  and \(\alpha = 2/3\) . We then have 3 cases:
small characteristic, when \(p &lt; L_{p^n}(1/3,\cdot )\) , medium characteristic, when
\(L_{p^n}(1/3,\cdot ) &lt; p &lt; L_{p^n}(2/3,\cdot )\) , and large characteristic,
when \(p > L_{p^n}(2/3,\cdot )\) . This work relates to the medium characteristic case.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="22"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">c1e2cd57-d423-4fbf-838a-94857b91ae4e</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The structure of this paper is as follows: In Section 2, we give an overview
of computing discrete logs using ExTNFS. In section 3, sieving in a 4d box (orthotope)
is described, and we give implementation details. In section 4, we describe the
descent step in detail. In section 5 we give details of the record computation
in \(\mathbb {F}_{p^4}\)  of 512 bits field size. In section 6 we conclude and outline future
possible work.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="23"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">a5705097-c41c-4e30-ad7a-8b60fe2905b4</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">We implemented the key components of the Extended Tower Number Field Sieve and together
with linear algebra components of CADO-NFS demonstrated a total discrete log break
in a finite field \(\mathbb {F}_{p^4}\)  of size 512 bits, a new record. This provides another
data point in the evaluation of security of systems dependent on the intractability
of discrete log attacks in extension fields. Whereas the recent articles
[1]}, [2]} show that asymptotically, sieving in a d-dimensional
sphere is optimal as \(d \rightarrow \infty \) , there seems to be room at lower
dimensions for sieving in an orthotope to remain competitive. We did not optimize our code
particularly well and there are probably further gains in sieving speed possible.
One such idea is to replace our list/sort approach with bucket sieving
[3]}, which would
at least improve the memory footprint (although for our sieving dimensions this was
not a problem). The parameters for sieving were
tuned in an ad-hoc way and a finer examination of optimal parameters would be interesting.
It would be a fairly easy change to adjust the sieving shape to best suit a given
special-\(\mathfrak {q}\) , for rectangular sieving orthotopes, improving the relation yield.
Finally, we did not exploit the common Galois automorphism of the sieving polynomials,
which would have cut the sieving time in half.
The overall timings of the key stages of our computation are shown in
table REF .
&lt;TABLE></span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="24"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">2ccd93b0-7f49-401c-b8ee-6cf316cb9ade</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Neural methods for generating entity embeddings have become the dominant approach to representing entities, with embeddings learned through methods such as pretraining, task-based training, and encoding knowledge graphs [1]}, [2]}, [3]}.
These embeddings can be compared extrinsically by performance on a downstream task, such as entity linking (EL).
However, performance depends on several factors, such as the architecture of the model they are used in and how the data is preprocessed, making direct comparison of the embeddings hard.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="25"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">49515884-2caf-4584-b156-05c7eede690a</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Another way to compare these embeddings is intrinsically using probing tasks [1]}, [2]}, which have been used to examine entity embeddings for information such as an entity's type, relation to other entities, and factual information [3]}, [4]}, [5]}, [6]}.
These prior examinations have often examined only a few methods, and some propose tasks that can only be applied to certain classes of embeddings, such as those produced from a mention of an entity in context.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="26"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">9f0a29f7-d76d-4a93-a126-b10c958003c1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">We address these gaps by comparing a wide range of entity embedding methods for semantic information using both probing tasks as well as downstream task performance.
We propose a set of probing tasks derived simply from Wikipedia and DBPedia, which can be applied to any method that produces a single embedding per entity.
We use these to compare eight entity embedding methods based on a diverse set of model architectures, learning objectives, and knowledge sources.
We evaluate how these differences are reflected in performance on predicting information like entity types, relationships, and context words.
We find that type information is extremely well encoded by most methods and that this can lead to inflated performance on other probing tasks.
We propose a method to counteract this and show that it allows a more reliable estimate of the encoded information.
Finally, we evaluate the embeddings on two EL tasks to directly compare their performance when used in different model architectures, identifying some that generalize well across multiple architectures and others that perform particularly well on one task.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="27"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">58f8acc2-f63f-43df-919a-0f82dafd1491</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">We aim to provide a clear comparison of the strengths and weaknesses of various entity embedding methods and the information they encode to guide future work.
Our probing task datasets, embeddings, and code are available online.https://github.com/AJRunge523/entitylens
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="28"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">b01f524d-cd9b-49c2-b55f-d6030c20b885</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Many of our embedding methods have been evaluated on EL tasks in prior work, either in a separate model or as full EL models themselves.
However, direct comparison of the impact of of the embeddings on EL performance is confounded by differences in the architectures which leverage the embeddings, as well as difficult to reproduce differences in candidate selection, data preprocessing, and other implementation details.
To address this, we evaluate all of our embeddings in a consistent framework, testing them on two standard datasets in three different EL model architectures to directly compare the contribution of the embeddings to performance on the downstream task and how well they perform across different model architectures.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">m</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="29"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">776298b8-cc2e-4cb1-9315-d6455ccebb64</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">We test the embeddings using three EL models on two standard EL datasets, the AIDA-CoNLL 2003 dataset [1]} and the TAC-KBP 2010 dataset [2]}.
Two of our EL models are the CNN and RNN EL models used to generate our task-learned embeddings.
Our third is a transformer model based on the RELIC model of [3]} that encodes a 128-word context window around the entity mention using uncased DistilBERT-base [4]}https://huggingface.co/distilbert-base-uncased.
We compare the embedding of the CLS token in the final layer to a separate entity embedding for each candidate entity using a weighted cosine similarity.
To compare the impact of the entity embeddings, we replace the candidate document convolution in the CNN model or the randomly initialized embeddings in the RNN and transformer models with the pretrained embeddings during training.
Details about dataset preprocessing, candidate selection, and model training can be found in Appendix .
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">m</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="30"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">42f87d63-7602-4205-b341-1a2cfb5bd6d5</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">In this work, we propose a new set of probing tasks for evaluating entity embeddings which can be applied to any method that creates one embedding per entity.
Using these tasks, we find that entity type information is one of the strongest signals present in all but one of the embedding models, followed by coarse information about how likely an entity is to be mentioned.
We show that the embeddings are particularly able to use entity type information to bootstrap their way to improved performance on entity relationship and factual information prediction tasks and propose methods to counteract this to more accurately estimate how well they encode relationships and facts.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="31"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">94138ed2-c856-4c13-919d-aafbc8d72777</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Overall, we find that while BERT-based entity embeddings perform well on many of these tasks, their high performance can often be attributed to strong entity type information encoding.
More specialized models such as Wikipedia2Vec are better able to detect and identify relationships, while the embeddings of [1]} better capture the lexical and distributional semantics of entities.
Additionally, we provide a direct comparison of the embeddings on two downstream EL tasks, where the models that performed well on the probing tasks such as Ganea, Wiki2V, and BERT performed best on the downstream tasks.
We find that the best performing embedding model depends greatly on the surrounding architecture and encourage future practitioners to directly compare newly proposed methods with prior models in a consistent architecture, rather than only compare results.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="32"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">cea9e13d-9656-4fc0-b6f0-7e137bcc04cd</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Our work provides insight into the information encoded by static entity embeddings, but entities can change over time, sometimes quite significantly.
One future line of work we would like to pursue using our tests is to investigate how changes in entities over time can be reflected in the embeddings, and how those changes could be modeled as transformations in the embedding space.
Context-based embeddings in particular could then be dynamically updated with new information, instead of being retrained from scratch.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="33"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">55d4f39c-c6c2-4370-b6ec-f2881f0a142e</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Bipolar disorder (BD) is a mental health condition that causes extreme mood swings like emotional highs (mania, hypomania), lows (depression), mixed episodes where depression and manic symptoms occur together. The diagnosis of bipolar disorder requires lengthy observations on the patient. Otherwise, it can be mistaken with other mental disorders like anxiety or depression. The disease affects 2% of the population, and sub-threshold forms (recurrent hypomania episodes without major depressive episodes) affect an additional 2%Â [1]}. It is ranked as one of the top ten diseases of disability-adjusted life year (DALY) indicator among young adults, according to World Health OrganizationÂ [2]}. It takes 10 years on average to diagnose bipolar disorder after the first symptomsÂ [3]}.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="34"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">90748098-c9f9-463c-88e7-7602a0e028d0</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">In bipolar disorder, the clinical appearance of the patients changes based on the moods they are in. The changes are seen in both their sound and visual appearance, as well as the energy level changes. In the manic episode, the speech of the patient becomes louder, rushed, or pressured. The patient can be very cheerful, furious, or overly confident. The movements of the patient become more active, exaggerated, and they tend to wear very colorful clothes. Feelings and the state of mind change quickly. Racing thoughts, reduced need for sleep, lack of attention, increase in targeted activity (work, school, personal life) are some situations patients can experience in the manic episode. These symptoms return to a normal state during the remission stateÂ [1]}.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="35"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">be8152d4-ad99-4786-8cce-08a683992dfa</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Today, the diagnosis of mental health disorders rely on questionnaires done by psychiatrists and reports from patients and their caregivers. Psychiatrists perform some tests to collect information about the patient's cognitive, neurophysiological, and emotional situationsÂ [1]}. But these reports are subjective, and there is a need for more systematic and objective diagnosis methods. Especially, with the COVID-19 pandemic, remote treatment and diagnosis gain importance, which can be achieved using automated methods.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="36"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">5f241116-9423-4597-bd70-e3959432727e</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">One of the tools used to rate the severity of the manic episodes of a patient is the Young Mania Rating Scale (YMRS). During the interviews, psychiatrists observe the patient's symptoms and give ratings to them. The 11 items in YMRS assess the elevated mood, increased motor activity-energy, sexual interest, sleep, irritability, speech rate and amount, language-thought disorder, content, disruptive-aggressive behavior, appearance, and insight. Most of these can be observed from speech patterns, body or facial movements, and the content of what was spoken during the interview.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="37"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">25a9b2d4-7baf-4d5c-b789-10443582124c</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Recent advancements in technologies like social media, smartphones, wearable devices, and improvements in recording techniques like better cameras, neuroimaging techniques, microphones enable us to gather good quality data from people during their everyday lives. This creates an opportunity to create tools to monitor the symptoms of the patients in longer periods, screen patients before they see the psychiatrists, complement clinicians in the diagnosis, and capture their behaviors in situations where they cannot act or hide the symptoms.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="38"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">9e3e27f1-7c93-4de6-9384-b8717ec2f287</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">In recent years, there are many works on diagnosing psychiatric disorders like Alzheimer's disease, anxiety, attention deficit hyperactivity disorder, autism spectrum disorder, depression, obsessive-compulsive disorder, bipolar disorderÂ [1]} using machine learning (ML) techniques. The datasets used for the detection of the diseases contain linguistic, auditory, and visual information. Adapted from real life, using the modalities together with fusion techniques improves the results as explained in Chapter .
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="39"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">cae9c630-e4ac-4bc4-85a2-214978176e0c</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Assessment of mental health disorders using machine learning methods has been an active research area. Many researchers are working on recognizing mental health disorders varying from depression, Alzheimerâ€™s disease, anxiety to bipolar disorder. The interdisciplinary research between psychiatrists and computer scientists helps to create new datasets and bringing insights from the medical domain to artificial intelligence.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">w</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="40"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">3f3d47a9-0011-484d-bccc-7d32c4593500</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">In this chapter, we introduce the features used in audio, textual, and visual modalities, preprocessing, feature selection methods applied to the dataset. After that, we explain the ELM algorithm used as a classification method, cross-validation technique used to evaluate the results, and modality fusion methods applied to improve the unimodal results.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">m</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="41"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">ae647d7b-e1fd-42fe-ad88-53a0686f9d60</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">During this thesis, we worked on the classification of bipolar disorder episodes (mania, hypomania, depression) using the BD dataset that contains video recordings of the bipolar disorder patients while they are interviewed by their psychiatrists. During the interviews, the patients perform seven different tasks. The tasks are designed in a way that they elicit both positive and negative emotions in the patients, and some tasks are emotionally neutral.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="42"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">e799b2ad-acf7-41c0-a755-187eb7579bb1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">We showed that multimodality improves the generalizability of the classification of bipolar disorder. The information coming from acoustic, textual, and visual modalities complement each other and improve the performance of the unimodal systems. The results suggests that using all three modalities together gives the best performance, however a fusion model of the linguistic, and acoustic modalities still perform well while requiring less information.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="43"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">e12eb7c5-29f1-4029-9585-f893f486341a</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">As a classification algorithm, we use fusion of weighted and unweighted ELMs. ELM was a good fit for this problem, since it is a 2-level neural and prone to overfitting. The data imbalance creates a need for a weighted model, however weighted ELM mostly favor the minority class. So using the fusion of weighted and unweighted ELMs, the optimum point is found.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="44"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">644700f7-5da7-4590-bad1-8ed5728efe1b</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The best performing model is achieved using eGEMAPS10, LIWC, and FAU features using the fusion of weighted and unweighted kernel ELMS, and fused using majority voting as a late fusion process. We achieve 64.8% test set UAR on this configuration, which is the best result achieved on the BD dataset as can be seen in FigureÂ REF . The results suggest that benefiting from all three modalities is useful, since the first 13 best performing model is achieved on the fusion models of three modalities. However, the 14th highest score on the Table
Â REF  uses only linguistic and acoustic modalities. So, it is possible to use only audio recordings of the patients, like phone recordings and achieve promising results from the fusion of linguistic and acoustic modalities. Besides, the MM1 scores on Table REF  shows that, fusion of modalities increase the maximum scores achieved on a single modality in all the configurations.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="45"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">258da887-57b6-4668-a9f7-691395eb7e86</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">eGEMAPS is a commonly used minimalistic acoustic feature set. So we used it for the audio classification, and in the fusion experiments. Besides, we summarized eGEMAPS LLDs with the 10 functionals presented inÂ [1]}. We achieved a better performance using eGEMAPS10 feature set, which shows that eGEMAPS LLDs can give better results when summarized with different functionals. eGEMAPS, and eGEMAPS10 feature sets contain 88, and 230 features respectively. So, a higher feature size may help finding better features that generalize better to the dataset.
&lt;FIGURE></span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="46"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">579163b4-b6bf-4010-b0b2-cc5eb1ae82d4</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">These results are still not high enough to use in a real-world application as a decision system. One of the main difficulties was the small size of the BD corpus. There are 25, 38, and 41 clips in the dataset for the remission, hypomania, and mania classes respectively, which is not enough to generalize with a high certainty. The dataset is collected in a real-life scenario. So there were some noises, and in some cases the clinician explains things about the questions to the patients, so her voice can be heard as well. These issues are expected to be present if a real-life application is created, so the natural recording setup makes this database valuable. Another difficulty stems from missing information in some clips, where patients do not answer some of the questions. In one of the test case clips, the patient does not answer any question at all. This can be used as a feature as well. However in our method, it caused a poor performance.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="47"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0fbed13e-eef2-4764-9f77-131976893f47</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Besides the clip level evaluation, we look for the effect of the tasks separately, and by grouping the same emotion eliciting tasks during the classification. Since some tasks are not performed in every clip, the number of clips per task are different. To be able to compare the results among the task groups and the entire clips results, we assign the middle class label to the missing clips. Since the dataset size is already small, this distorted the final scores somewhat. Still, from the task level experiments we can see that emotion eliciting tasks are more useful in the classification of BD for all three modalities, as expected. In order to increase the dataset size, we also used the task groups as separate data points and performed classification. However, the results were not better than the entire clip level results, which shows that the information obtained from longer clips is necessary for learning.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="48"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">5fa8e7bc-f488-4c83-bdba-5e584336fe3a</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Our final best performing model contains information from three different modalities, and each modality is represented using feature vectors with various sizes, which causes poor explainability of the model. It is especially important to create explainable models in medical domain.
As a further study, the explainability of the system can be investigated, which also gives insights to the psychiatrists about the features used in the classification, and the best performing ones can be adapted in their decision making progresses.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="49"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">e349de23-f7a4-4f0a-b4ca-1d2787a3ca63</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Deep learning has emerged as a powerful tool for many industrial and scientific applications.
However, deep learning requires large centralized datasets, whose collection can be intrusive, for training.
The finalized model can either be deployed on a server or edge devices.
Federated learning circumvents this problem by shifting compute responsibility onto clients, letting a central server aggregate the resulting artifacts.
In FedAvg, edge devices train on locally available data, while the server averages the finished models [1]}.
This is repeated for multiple rounds of communication.
The server never has possession of any potentially sensitive data.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="50"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">ce0478f1-e907-4432-a48d-af90aa633f35</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">When data is independently and identically distributed (IID), federated learning algorithms converge rapidly.
FedAvg takes as a few as 18 communication rounds to reach 99% accuracy for 100 device federated MNIST [1]}.
When the client devices are statistically heterogeneous, learning a single global model becomes very difficult [2]}.
In such cases, it is more natural to learn personalized models.
Still, there are circumstances where a single global model is desired.
For example, different online businesses might want a model capable of flagging a wide spectrum of fraudulent schemes.
since fraudsters are often repeat offenders, scams attempted on one platform may be reused on others.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="51"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">eb5bb278-8016-4107-9bf3-f8f8acb4db17</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Techniques for faster federated learning on non-IID data range from the simple to the complex.
On the simple end, Momentum Federated Learning averages the momenta of different devices into a global momentum which is distributed at the start of each round [1]}.
This enables clients to use momentum gradient descent as their optimizer, provably increasing the rate of convergence.
On the complex end, SCAFFOLD uses the gradient of the global model as a control variate to address drifting among client updates [2]}.
Notably these two methods double the amount of information submitted by devices to the server.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="52"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">aab22439-f975-4110-a62e-82b21f734c88</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">To deal with the communication and scalability challenges introduced by above methods, efforts has been made to reduce the amount of rounds required for server-client communication [1]}.
FedPAQ has made an initial effort [2]} to periodically average and quantize the client models before making the server update.
Then, periodic averaging for both server and client models followed up quickly [3]}.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="53"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">fa8703b9-1117-475f-ae8d-fb939bbabdb8</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">In this paper, we take a different approach, using server averaging to accelerate convergence.
We justify the technique using heuristic arguments and experimentally show that it reaches a given test accuracy faster than FedAvg.
Additionally, we propose decay epochs for reducing client computation while maintaining non-IID performance.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="54"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">afbc9576-f872-454f-a6b2-9e602eb5fd11</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The history of stochastic gradient methods dates back to 1951, and is usually mentioned as Robbins-Monro process [1]}.
One technique that has historically been used to improve SGD convergence is iterate averaging [2]}, [3]}, [4]}, also often referred to as Polyak-Rupert averaging.
Recently, the stability of an averaging scheme that considers a non-uniform average of the iterates is discussed [5]}.
A weighted average is applied which decays in a geometric manner.
Neu et. al., show that the same regularizing effect can be done for SGD with the linear least-squares regression problem.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">w</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="55"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d42322ba-5a02-48d3-a75a-c3df36c7fc52</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Federated learning techniques heavily rely on above mentioned averaging schemes.
FedAvg is the most popular aggregation method that averages parameters of local models element-wise.
There exists two major branches for improving FedAvg [1]}.
One is lead by FedProx [2]} that applies a proximal term to the local lost function of each client and thresholding the local updates.
Another approach is to proposes different averaging schemes to either save the communication cost or to improve the performance [3]}.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">w</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="56"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">3af8883d-25e6-4ddf-87d4-02c16284a622</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Safa et. al. explore iterate averaging in the context of block-cyclic SGD [1]}.
Most federated learning algorithms assume that clients are chosen uniformly.
In practice, devices conduct local training only when idle, with devices falling into blocks according to their timezone.
More formally, we want to minimize
\(\operatorname{\mathbb {E}}_{z \sim \mathcal {D}} f(w, z) \text{ where } \mathcal {D} = \sum _{i=1}^{m} \mathcal {D}_i\) 
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">w</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="57"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">59c00e2d-549f-499e-ad1c-358e07e5f086</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">while sampling \(n\)  points from \(\mathcal {D}_1, \dots , \mathcal {D}_m\)  in order for \(K\)  cycles.
In this block-cyclic setting, SGD is worse by a factor of \(\sqrt{mn/K}\) .
However, learning personalized models for each block using Averaged SGD [1]}â€”taking the average of all SGD iterate as the final model parametersâ€”provides the same performance guarantees as SGD with IID sampling.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">w</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="58"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">71a05e05-e4d3-457c-9904-d06d7a14eb68</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Stochastic Weight Averaging (SWA) applies Averaged SGD to deep learning [1]}.
Izmailov et. al. note that SGD generally converges to points near the boundary of a wide flat region and that optima width has been conjectured to correlate with generalization.
The average of the SGD iterates then lies at the the center of this flat region.
Moreover, to ensure coverage of this flat region, SWA uses a cyclic or a high constant learning rate.
This algorithm has the benefit of low computational overheadâ€”only the moving average needs to be recordedâ€“and simplicity.
SWA does not improve the rate of convergence compared to SGD.
In fact, SWA converges to worse but better generalizing optima than SGD.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">w</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="59"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">3436b808-2afd-400b-b6bd-e1bd450df09e</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">This paper improves upon an existing federated learning algorithm by performing periodic server-side averaging.
The proposed adaptation of FedAvg has three major benefits: (1) it uses iterate averaging for accelerated convergence, (2) it learns a better generalizing optima than SGD, (3) the effectiveness of FL is increased due to recycling of previously participating clients.
We empirically show that server averaging takes fewer rounds than FedAvg to a desired accuracy level.
In addition, we propose epoch decay to lower the computation costs for each client.
Epoch decay limits the number of updates, similar to learning rate decay for SGD, and reduces the amount of computation by up to 40%.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="60"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">3bc8f89c-0854-4158-813c-809894baa58a</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">In the future, we wish to extend the server averaging to both various neural network types (i.e. attention, LSTM, etc.) and layer-wise building blocks (i.e. batch normalization layers, etc.).
In addition, we wish to investigate the performance of epoch decay paired with state-of-the-art update methods such as match averaging [1]}.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="61"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">3a6811f9-90ac-47c5-9b37-07f95925330c</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Contour is one of the most important object descriptors, along with texture and color. The boundary of an object in an image is encoded in contour description, which is useful in various applications, such as image retrieval [1]}, [2]}, [3]}, recognition [4]}, [5]}, [6]}, and segmentation [7]}, [8]}, [9]}, [10]}, [11]}. It is desirable to represent object boundaries compactly, as well as faithfully, but it is challenging to design such contour descriptors due to the diversity and complexity of object shapes.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="62"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">6782fdc5-35b5-4b87-8d30-ef3704395e2a</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Early contour descriptors were developed mainly for image retrieval [1]}, [2]}, [3]}, [4]}. An object contour can be simply represented based on the area, circularity, and/or eccentricity of the object [5]}. For more precise description, there are several approaches, including shape signature [6]}, [7]}, [8]}, structural analysis [9]}, [10]}, [11]}, [12]}, [13]}, spectral analysis [2]}, [3]}, and curvature scale space (CSS) [1]}, [17]}.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="63"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">29a190e8-5669-4900-951c-759d211e625e</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Recently, contour descriptors have been incorporated into deep-learning-based object detection, tracking, and segmentation systems. In [1]}, bounding boxes are replaced by polygons to enclose objects more tightly. In [2]}, ellipse fitting is done to produce a rotated box of a target object to be tracked. For instance segmentation, contour-based techniques have been proposed that represent pixelwise masks by contour descriptors based on shape signature [3]} or polynomial fitting [4]}. Even though these descriptors can localize an object effectively, they may fail to reconstruct the object boundary faithfully. Also, they consider the structural information of an individual object only, without exploiting the shape correlation between different objects.
&lt;FIGURE></span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="64"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">258c9349-2eae-453e-aeba-bcc3e101a13d</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">In this paper, we propose novel contour descriptors, called eigencontours, based on low-rank approximation. First, we construct a contour matrix containing all object boundaries in a training set. Second, we decompose the contour matrix into eigencontours, based on the best rank-\(M\)  approximation of singular value decomposition (SVD) [1]}. Then, each contour is represented by a linear combination of the \(M\)  eigencontours, as illustrated in Figure REF . Also, we incorporate the eigencontours into an instance segmentation framework. Experimental results demonstrate that the proposed eigencontours can represent object boundaries more effectively and more efficiently than the existing contour descriptors [2]}, [3]}. Moreover, utilizing the existing framework of YOLOv3 [4]}, the proposed algorithm yields promising instance segmentation performances on various datasets â€” KINS [5]}, SBD [6]}, and COCO2017 [7]}.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="65"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">ab1985af-9e02-4e82-8632-0ee559db99f2</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
We propose the notion of eigencontours â€” data-driven contour descriptors based on SVD â€” to represent object boundaries as faithfully as possible with a limited number of coefficients.

The proposed algorithm can represent object boundaries more effectively and more efficiently than the existing contour descriptors.

The proposed algorithm outperforms conventional contour-based techniques in instance segmentation.

</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="66"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">2ac7b224-3e3b-4914-bf7a-07ee5c450339</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The goal of contour description is to represent the boundary of an object in an image compactly and faithfully. Simple contour descriptors are based on the area, circularity, and/or eccentricity of an object [1]}, and basic geometric shapes, such as rectangles and ellipses, can be also used. However, these simple descriptors cannot preserve the original shape of an object faithfully [2]}, [3]}.
For more sophisticated description, there are four types of approaches: shape signature [4]}, [5]}, [6]}, structural analysis [7]}, [8]}, [9]}, [10]}, spectral analysis [11]}, [12]}, and CSS [13]}, [14]}.
First, a shape signature is a one-dimensional function derived from the boundary coordinates of an object. For example, a polar coordinate system is set up with respect to the centroid of an object. Then, the object boundary is represented by the \((r, \theta )\)  graph, called the centroidal profile [4]}. Also, an object shape can be represented by the angle between the tangent vector at each contour point and the \(x\) -axis [5]}. Second, structural methods divide an object boundary into segments and approximate each segment to encode the whole boundary. In [7]}, the boundary is represented by a sequence of unit vectors with a few possible directions. In [8]}, polygonal approximation is performed to globally minimize the errors from an approximated polygon to the original boundary. In [9]}, segments of an object contour are represented by cubic polynomials. Third, in spectral methods, boundary coordinates are transformed to a spectral domain. In [11]}, a wavelet transform is used for contour description.
In [12]}, the Fourier descriptors are derived from the Fourier series of centroidal profiles. Fourth, in CSS [13]}, a boundary is smoothed by a Gaussian filter with a varying standard deviation. Then, the boundary is represented by the curvature zero-crossing points of the smoothed curve at each standard deviation.
&lt;FIGURE></span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">w</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="67"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">20ac1d8a-251c-477a-9798-6480b8a96975</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Recently, attempts have been made to improve the performances of deep-learning-based vision systems. In [1]}, a bounding box for object detection is replaced by an octagon to enclose an object more tightly via polygonal approximation. In [2]}, a rotated box for a target object is determined based on ellipse fitting, in order to cope with object deformation in a visual tracking system. For instance segmentation, contour-based approaches [3]}, [4]} have been developed, which reformulate the pixelwise classification task as the boundary regression of an object. To this end, these methods encode segmentation masks into contour descriptors. In [4]}, centroidal profiles are used to describe object boundaries.
In [3]}, each segment of a boundary is represented by a few coefficients based on polynomial fitting. Although these methods are computationally efficient for localizing object instances, they often fail to reconstruct the boundaries of the object shapes faithfully.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">w</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="68"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">5a8c8f29-1f54-4289-a6a3-1c9880e45646</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The proposed algorithm aims to represent an object boundary as faithfully as possible by employing as few coefficients as possible. To this end, we develop eigencontours based on the best low-rank approximation property of SVD.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">w</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="69"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">6f1d02ce-f89b-4930-a418-867fa8daf441</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Dimension of eigencontour space (\(M\) ):
TableÂ REF  lists the AUC-\(\cal {F}\)  performances of the proposed algorithm on the SBD validation dataset according to the dimension, \(M\) , of the eigencontour space. At \(M=10\) , the proposed algorithm yields poor scores, since object boundaries are too simplified and not sufficiently accurate. At \(M=20\) , it provides the best results. At \(M=30\) , it yields similarly good results. However, at \(M=40\) , the performances are degraded further, which indicates that a high-dimensional space does not always lead to better results. It is more challenging to regress more variables reliably. There is a tradeoff between accuracy and reliability. In this test, \(M=20\)  achieves a good tradeoff.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">m</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="70"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1ebd7eb0-42ed-44ec-a646-4dba1cd36855</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Categorical eigencontour space:
The proposed eigencontours are data-driven descriptors, which depend on the distribution of object contours in a dataset. Thus, different eigencontours are obtained for different data. Let us consider two options for constructing eigencontour spaces: categorial construction and universal construction.
In the categorial construction, eigencontours are determined for each category in a dataset. In the universal construction, they are determined for all instances in all categories.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">m</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="71"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">389c1547-abc7-4ef9-a5d2-781bb9c944c4</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">For the two options, \(\cal {F}\)  score curves are presented according to the dimension \(M\)  in the supplemental document. Table REF  compares the area under curve performances of the \(\cal {F}\)  curves up to \(M=18\) . The categorial construction provides better performances than the universal construction, because it considers similar shapes in the same category only. In COCO2017, the gap between the two options is the smallest. This is because some object shapes are not properly represented due to occlusions and thus COCO2017 objects exhibit low intra-category correlation. In contrast, in KINS, whole contours are well represented because occluded regions are also annotated. Hence, the gap between the two options is the largest.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">m</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="72"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">7cd6bac9-81a1-482a-9c69-605195030d7f</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Limitations:
The proposed eigencontours represent typical contour patterns in a dataset. Thus, if object contour patterns differ among datasets, the eigencontours for a dataset may be effective for that particular dataset only. To assess the dependency of eigencontours on a dataset, we conduct cross-validation tests between datasets in the supplemental document.
&lt;TABLE>&lt;TABLE></span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">m</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="73"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d5854744-09f5-4bf0-bb33-ee4ed71bd3b7</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">We proposed novel contour descriptors, called eigencontours, based on low-rank approximation. First, we constructed a contour matrix containing all contours in a training set. Second, we approximated the contour matrix, by performing the best rank-\(M\)  approximation. Third, we represent an object boundary by a linear combination of the \(M\)  eigencontours. Experimental results demonstrated that the proposed eigencontours can represent object boundaries more effectively and more faithfully than the existing methods. Moreover, the proposed algorithm yields meaningful instance segmentation performances.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="74"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">ff7c1911-a348-434c-ae84-e2924bb12d28</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Scientists increasingly use machine learning (ML) in their daily work. This development is not limited to natural sciences like ecology  or neuroscience , but also extends to social sciences such as psychology  and archaeology .
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="75"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">7151e7c7-ce93-46b8-97cc-2571a79df978</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Â Â Â Â In particular, when building predictive models for problems with complex data structures, ML outcompetes classical statistical models in both performance and convenience. Impressive recent examples of successful prediction models in science include the automated particle tracking at CERN , or DeepMind's AlphaFold, which has essentially solved the protein structure prediction challenge CASP . In such examples, some see a paradigm shift towards theory-free science that â€œlets the data speakâ€ , , , . Indeed, prediction is one of the core aims of science , , but so are, as philosophers of science and statisticians emphasize, explanation and knowledge generation , , . Focusing exclusively on prediction may therefore represent a historical step back , .
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="76"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">004cc382-3ba7-4af2-b4ff-26ba95978428</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Â Â Â Â What hinders scientists from using ML models to gain real-world insights is the model complexity and the unclear connection between the model and the described phenomenon â€” the so-called opacity problem , . Interpretable machine learning (IML, also called XAI for eXplainable artificial intelligence) aims to solve the opacity problem by analyzing model elements or inspecting model properties . Various expectations are put into IML by different stakeholders with diverse goals , including scientists , ML engineers , regulatory bodies , and laypeople . Due to this diversity of goals, stakeholders, and requirements, IML has been criticized for lacking a well-defined goal .
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="77"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">f1f5210a-6d48-4dfa-8b17-32b06daf335f</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Â Â Â Â Nevertheless, scientists increasingly use IML techniques in their research.e.g. for predicting personality traits from smartphone usage , forecasting crop yield , , or analyzing seasonal precipitation forecasts  Although researchers are aware that their IML analysis remains just a model description, it is often implied that the explanations, associations, or effects found also extend to the corresponding real-world properties. Unfortunately, drawing inferences with IML can currently be epistemically problematic because the interpretation techniques are not defined for that purpose . In particular, the difference between model-only versus phenomenon explanations is often unclear ,  and a theory to quantify the uncertainty of interpretations is lacking .
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="78"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">7d25e83e-cbfb-47e4-8d54-426ebdd8b9b1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">In recent years, digital libraries have moved towards open science and open access with several large scholarly datasets being constructed. Most popular datasets include millions of papers, authors, venues, and other information. Their large size and heterogeneous contents make it very challenging to effectively manage, explore, and utilize these datasets. The knowledge graph has emerged as a universal data format for representing knowledge about entities and their relationships in such complicated data. The main part of a knowledge graph is a collection of triples, with each triple \( (h, t, r) \)  denoting the fact that relation \( r \)  exists between head entity \( h \)  and tail entity \( t \) . This can also be formalized as a labeled directed multigraph where each triple \( (h, t, r) \)  represents a directed edge from node \( h \)  to node \( t \)  with label \( r \) . Therefore, it is straightforward to build knowledge graphs for scholarly data by representing natural connections between scholarly entities with triples such as (AuthorA, Paper1, write) and (Paper1, Paper2, cite).
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="79"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">be8647fa-47ab-4abc-9692-04fdb62a7f74</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Notably, instead of using knowledge graphs directly in some tasks, we can model them by knowledge graph embedding methods, which represent entities and relations as embedding vectors in semantic space, then model the interactions between them to solve the knowledge graph completion task. There are many approaches [1]} to modeling the interactions between embedding vectors resulting in many knowledge graph embedding methods such as ComplEx [2]} and CP\( _h \)  [3]}. In the case of word embedding methods such as word2vec, embedding vectors are known to contain rich semantic information that enables them to be used in many semantic applications [4]}. However, the semantic structures in the knowledge graph embedding space are not well-studied, thus knowledge graph embeddings are only used for knowledge graph completion but remain absent in the toolbox for data analysis of heterogeneous data in general and scholarly data in particular, although they have the potential to be highly effective and efficient. In this paper, we address these issues by providing a theoretical understanding of their semantic structures and designing a general semantic query framework to support data exploration.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="80"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">38cf6de4-fd73-40f8-b931-114da0e747e1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">For theoretical analysis, we first analyze the state-of-the-art knowledge graph embedding model CP\( _h \)  [1]} in comparison to the popular word embedding model word2vec skipgram [2]} to explain its components and provide understandings to its semantic structures. We then define the semantic queries on the knowledge graph embedding spaces, which are algebraic operations between the embedding vectors in the knowledge graph embedding space to solve queries such as similarity and analogy between the entities on the original datasets.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="81"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">4bc58645-1d7f-431c-a254-22ba0bdf2eaf</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Based on our theoretical results, we design a general framework for data exploration on scholarly data by semantic queries on knowledge graph embedding space. The main component in this framework is the conversion between the data exploration tasks and the semantic queries. We first outline the semantic query solutions to some traditional data exploration tasks, such as similar paper prediction and similar author prediction. We then propose a group of new interesting tasks, such as analogy query and analogy browsing, and discuss how they can be used in modern digital libraries.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="82"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">388310ad-34a1-4321-b88d-de5d4f83751a</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">In this paper, we studied the application of knowledge graph embedding in exploratory data analysis. We analyzed the CP\( _h \)  model and provided understandings to its semantic structures. We then defined the semantic queries on knowledge graph embedding space to efficiently approximate some operations on heterogeneous data such as scholarly data. We designed a general framework to systematically apply semantic queries to solve scholarly data exploration tasks. Finally, we outlined and discussed the solutions to some traditional and pioneering exploration tasks emerged from the semantic structures of the knowledge graph embedding space.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="83"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">764a6cda-e150-4341-971b-9e971c906cee</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">This paper is dedicated to the theoretical foundation of a new approach and discussions of emerging tasks, whereas experiments and evaluations are left for the future work. There are several other promising directions for future research. One direction is to explore new tasks or new solutions of traditional tasks using the proposed method. Another direction is to implement the proposed exploration tasks on real-life digital libraries for online evaluation.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="84"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">82e5ee19-669b-425c-87bd-1e6f8f7e7b70</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
In recent years, machine learning algorithms have been increasingly used to inform decisions with far-reaching consequences (e.g. whether to release someone from prison or grant them a loan), raising concerns about their compliance with laws, regulations, societal norms, and ethical values. Specifically, machine learning algorithms have been found to discriminate against certain â€œsensitiveâ€ demographic groups (e.g. racial minorities), prompting a profusion of algorithmic fairness researchÂ [1]}, [2]}, [3]}, [4]}, [5]}, [6]}, [7]}, [8]}, [9]}, [10]}, [11]}, [12]}, [13]}, [14]}, [15]}, [16]}. Algorithmic fairness literature aims to develop fair machine learning algorithms that output non-discriminatory predictions.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="85"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">274c01a8-01f4-4118-9db4-166c0b5c7d41</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
Fair learning algorithms typically need access to the sensitive data in order to ensure that the trained model is non-discriminatory.
However, consumer privacy laws (such as the E.U. General Data Protection Regulation) restrict the use of sensitive demographic data in algorithmic decision-making. These two requirementsâ€“fair algorithms trained with private dataâ€“presents a quandary: how can we train a model to be fair to a certain demographic if we don't even know which of our training examples belong to that group?
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="86"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">5d0af241-6a11-4aa4-a538-97564180275c</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
The works ofÂ [1]}, [2]} proposed a solution to this quandary using secure multi-party computation (MPC), which allows the learner to train a fair model without directly accessing the sensitive attributes.
Unfortunately, asÂ [3]} observed, MPC does not prevent the trained model from leaking sensitive data. For example, with MPC, the output of the trained model could be used to infer the race of an individual in the training data setÂ [4]}, [5]}, [6]}, [7]}.
To prevent such leaks, [3]} argued for the use of differential privacyÂ [9]} in fair learning. Differential privacy (DP) provides a strong guarantee that no company (or adversary) can learn much more about any individual than they could have learned had that individual's data never been used.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="87"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">b7f3d840-9d11-446b-883d-9c0863e156a3</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
SinceÂ [1]}, several follow-up works have
proposed alternate approaches to DP fair learningÂ [2]}, [3]}, [4]}, [5]}, [6]}, [7]}. As shown inÂ fig: related work table,
each of these approaches suffers from at least two critical shortcomings.
In particular, none of these methods have convergence guarantees when mini-batches of data are used in training. In training large-scale models, memory and efficiency constraints require the use of small minibatches in each iteration of training (i.e. stochastic optimization). Thus, existing DP fair learning methods cannot be used in such settings since they require computations on the full training data set in every iteration. SeeÂ app: related work for a more comprehensive discussion of related work.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="88"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">9a11b4e1-1524-4c43-9243-1799245cd66a</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Our Contributions: In this work, we propose a novel algorithmic framework for DP fair learning. Our approach builds on the non-private fair learning method ofÂ [1]}. We consider a regularized empirical risk minimization (ERM) problem where the regularizer penalizes fairness violations, as measured by the Exponential RÃ©nyi Mutual Information.
Using a result fromÂ [1]}, we reformulate this fair ERM problem as a min-max optimization problem. Then, we use an efficient differentially private variation of stochastic gradient descent-ascent (DP-SGDA) to solve this fair ERM min-max objective.
The main features of our algorithm are:
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="89"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">7a3e3a77-3253-43e7-8c7e-f3bf397601a2</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
Guaranteed convergence for any privacy and fairness level, even when mini-batches of data are used in each iteration of training (i.e. stochastic optimization setting). As discussed, stochastic optimization is essential in large-scale machine learning scenarios. Our algorithm is the first stochastic DP fair learning method with provable convergence.

Flexibility to handle non-binary classification with multiple (non-binary) sensitive attributes (e.g. race and gender) under different fairness notions such as demographic parity or equalized odds. In each of these cases, our algorithm is guaranteed to converge.

</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="90"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">9e737725-9354-43db-b300-bcc4d256d71c</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Empirically, we show that our method outperforms the previous state-of-the-art methods in terms of fairness vs. accuracy trade-off across all privacy levels. Moreover, our algorithm is capable of training with mini-batch updates and can handle non-binary target and non-binary sensitive attributes. By contrast, existing DP fairness algorithms could not converge in our stochastic/non-binary experiment.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="91"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">ec259b81-6d2f-4e81-af8e-e1a77f3cafd2</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
A byproduct of our algorithmic developments and analyses is the first DP convergent algorithm for nonconvex min-max optimization: namely, we provide an upper bound on the stationarity gap of DP-SGDA for solving problems of the form \(\min _{\theta } \max _{W} F(\theta , W)\) , where \(F(\cdot , W)\)  is non-convex. We expect this result to be of independent interest to the DP optimization community. Prior works that provide convergence results for DP min-max problems have assumed that \(F(\cdot , W)\)  is either (strongly) convexÂ [1]}, [2]} or satisfies a generalization of strong convexity known as the Polyak-Åojasiewicz (PL) conditionÂ [3]}.
&lt;FIGURE></span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="92"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">8264eb8d-21ec-406d-a29c-0bc4a813b2f9</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
In this section, we evaluate the performance of our proposed approach (DP-FERMI) in terms of the fairness violation vs. test error for different privacy levels. We present our results in two parts: In SectionÂ REF , we assess the performance of our method in training logistic regression models on several benchmark tabular datasets. Since this is a standard setup that existing DP fairness algorithms can handle, we are able to compare our method against the state-of-the-art baselines. We find that DP-FERMI consistently outperforms all state-of-the-art baselines across all data sets and all privacy levels. These observations hold for both demographic parity and equalized odds fairness notions.
In SectionÂ REF , we showcase the scalability of DP-FERMI by using it to train a deep convolutional neural network for classification on a large image dataset. InÂ app: experiments, we give detailed descriptions of the data sets, experimental setups and training procedure, along with additional results.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">m</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="93"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">95322e5f-18a1-4653-93a5-c0963a4d6e4e</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">
Motivated by pressing legal, ethical, and social considerations, we studied the challenging problem of learning fair models with differentially private demographic data. We observed that existing works suffer from a few crucial limitations that render their approaches impractical for large-scale problems. Specifically, existing approaches require full batches of data in each iteration (and/or exponential runtime) in order to provide convergence/accuracy guarantees. We addressed these limitations by deriving a DP stochastic optimization algorithm for fair learning, and rigorously proved the convergence of the proposed method. Our convergence guarantee holds even for non-binary classification (with any hypothesis class, even infinite VC dimension, c.f.Â [1]}) with multiple sensitive attributes and access to random minibatches of data in each iteration. Finally, we evaluated our method in extensive numerical experiments and found that it significantly outperforms the previous state-of-the-art models, in terms of fairness-accuracy tradeoff. Further, our method provided stable results in a larger scale experiment with small batch size and non-binary targets/sensitive attributes. The potential societal impacts and limitations of our work are discussed inÂ app: societal impacts.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">d</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="94"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">298d9497-25cd-47fc-b2f7-785d95521f43</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The study of differentially private fair learning algorithms was initiated byÂ [1]}. [1]} considered equalized odds and proposed two DP algorithms: 1) an \(\epsilon \) -DP post-processing approach derived fromÂ [3]}; and 2) an \((\epsilon , \delta )\) -DP in-processing approach based onÂ [4]}. The major drawback of their post-processing approach is the unrealistic requirement that the algorithm have access to the sensitive attributes at test time, whichÂ [1]} admits â€œisn't feasible (or legal) in certain applications.â€ Additionally, post-processing approaches are known to suffer from inferior fairness-accuracy tradeoffs compared with in-processing methods. While the in-processing method ofÂ [1]} does not require access to sensitive attributes at test time, it comes with a different set of disadvantages: 1) it is limited to binary classification; 2) its theoretical performance guarantees require the use of the computationally inefficient (i.e. exponential-time) exponential mechanismÂ [7]}; 3) its theoretical performance guarantees require computations on the full training set and do not permit mini-batch implementations; 4) it requires the hypothesis class \(\mathcal {H}\)  to have finite VC dimension.
In this work, we propose the first algorithm that overcomes all of these pitfalls: our algorithm is amenable to multi-way classification with multiple sensitive attributes, computationally efficient, and comes with convergence guarantees that hold even when mini-batches of \(m &lt; n\)  samples are used in each iteration of training, and even when \(\text{VC}(\mathcal {H}) = \infty \) . Furthermore, our framework is flexible enough to accommodate many notions of group fairness besides equalized odds (e.g. demographic parity, accuracy parity).
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">w</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="95"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">bf2aabcd-99b3-47a4-bbd9-21252209eca5</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">FollowingÂ [1]}, several works have proposed other DP fair learning algorithms. None of these works have managed to simultaneously address all the shortcomings of the method ofÂ [1]}. The work ofÂ [3]} proposed DP and fair binary logistic regression, but did not provide any theoretical convergence/performance guarantees. The work ofÂ [4]} combined aspects of bothÂ [5]} andÂ [6]} in a two-step locally differentially private fairness algorithm. Their approach is limited to binary classification.
Moreover, their algorithm requires \(n/2\)  samples in each iteration (of their in-processing step), making it impractical for large-scale problems. More recently,Â [7]} devised another DP in-processing method based on lagrange duality, which covers non-binary classification problems. In a subsequent work,Â [8]} studied the effect of DP on accuracy parity in ERM, and proposed using a regularizer to promote fairness. Finally, Â [9]} provided a semi-supervised fair â€œPrivate Aggregation of Teacher Ensemblesâ€ framework. A shortcoming of each of these three most recent works is their lack of theoretical convergence or accuracy guarantees. In another vein, some works have observed the disparate impact of privacy constraints on demographic subgroupsÂ [10]}, [11]}.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">w</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="96"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">52f44d2d-dce4-49a1-ac6d-bbe03471f633</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Nam dui ligula, fringilla a, euismod sodales,
sollicitudin vel, wisi. Morbi auctor lorem non justo. Nam lacus
libero, pretium at, lobortis vitae, ultricies et, tellus. Donec
aliquet, tortor sed accumsan bibendum, erat ligula aliquet magna,
vitae ornare odio metus a mi. Morbi ac orci et nisl hendrerit
mollis. Suspendisse ut massa. Cras nec ante. Pellentesque a nulla.
Cum sociis natoque penatibus et magnis dis parturient montes,
nascetur ridiculus mus. Aliquam tincidunt urna. Nulla ullamcorper
vestibulum turpis. Pellentesque cursus luctus mauris.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="97"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">67c47121-3a1c-4195-9489-4cb56370daa8</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Nulla malesuada porttitor diam. Donec felis
erat, congue non, volutpat at, tincidunt tristique, libero. Vivamus
viverra fermentum felis. Donec nonummy pellentesque ante. Phasellus
adipiscing semper elit. Proin fermentum massa ac quam. Sed diam
turpis, molestie vitae, placerat a, molestie nec, leo. Maecenas
lacinia. Nam ipsum ligula, eleifend at, accumsan nec, suscipit a,
ipsum. Morbi blandit ligula feugiat magna. Nunc eleifend consequat
lorem. Sed lacinia nulla vitae enim. Pellentesque tincidunt purus
vel magna. Integer non enim. Praesent euismod nunc eu purus. Donec
bibendum quam in tellus. Nullam cursus pulvinar lectus. Donec et mi.
Nam vulputate metus eu enim. Vestibulum pellentesque felis eu
massa.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="98"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">580035ca-35ab-4e20-90a2-4c76e1a94bdf</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Concurrent with steady progress towards improving the accuracy and efficiency of 3D object detector algorithms [1]}, [2]}, [3]}, [4]}, [5]}, [6]}, [7]}, [8]}, [9]}, [10]}, [11]}, LiDAR sensor hardware has been improving in maximum range and fidelity, in order to meet the needs of safe, high speed driving. Some of the latest commercial LiDARs can sense up to 250mÂ [12]} and 300mÂ [13]} in all directions around the vehicle. This large volume coverage places strong demands for efficient and accurate 3D detection methods.
&lt;FIGURE></span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="99"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">b8e291bf-5052-45b9-9ebc-b15fd15d9e8e</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Grid based methods [1]}, [2]}, [3]}, [4]}, [5]} divide the 3D space into voxels or pillars, each of these being optionally encoded using PointNet [6]}. Dense convolutions are applied on the grid to extract features. This approach is inefficient for large grids which are needed for long range sensing or small object detection. Sparse convolutions [7]} scale better to large detection ranges but are usually slow due to the inefficiencies of applying to all points.
Range images are native, dense representations, suitable for processing point clouds captured by a single LiDAR. Range image based methods [8]}, [9]} perform convolutions directly over the range in order to extract point cloud features.
Such models scale well with distance, but tend to perform less well in occlusion handling, accurate object localization, and for size estimation. A second stage, refining a set of initial candidate detections, can help mitigate some of these quality issues, at the expense of significant computational cost.
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">i</span></div></div>
							</td>
					</tr></tbody></table>
		<div class="bg-linear-to-b sticky left-0 border-t border-dashed border-gray-300 from-gray-100 to-white py-3 text-center font-mono text-xs dark:border-gray-700 dark:from-gray-950 dark:to-gray-900">End of preview. <a href="/datasets/saier/unarXive_imrad_clf/viewer/default/train" class="group"><span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Expand</span>
						in <svg class="text-lg mr-0.5 inline -translate-y-px text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>Data Studio
					</a></div></div>




			<div class="bg-linear-to-b from-gray-100 to-white dark:from-gray-950 dark:to-gray-900 "><hr class="flex-none -translate-y-px border-t border-dashed border-gray-300 bg-white dark:border-gray-700 dark:bg-gray-950">
					<nav><ul class="flex select-none items-center justify-between space-x-2 text-gray-700 sm:justify-center py-1 text-center font-mono text-xs rounded-b-lg"><li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 pointer-events-none cursor-default text-gray-400 hover:text-gray-700" href=""><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg>
		Previous</a></li>
			<li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1 bg-gray-50 font-semibold ring-1 ring-inset ring-gray-200 dark:bg-gray-900 dark:text-yellow-500 dark:ring-gray-900 hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/saier/unarXive_imrad_clf/viewer/default/train?p=0">1</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/saier/unarXive_imrad_clf/viewer/default/train?p=1">2</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/saier/unarXive_imrad_clf/viewer/default/train?p=2">3</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  pointer-events-none cursor-default" href="#">...</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/saier/unarXive_imrad_clf/viewer/default/train?p=5200">5,201</a>
				</li>
			<li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 " href="/datasets/saier/unarXive_imrad_clf/viewer/default/train?p=1">Next
		<svg class="ml-1.5 transform rotate-180" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg></a></li></ul></nav></div></div>






</div></div></div></div></div></div></div>

				
					<div class="SVELTE_HYDRATER contents" data-target="RepoCodeCopy" data-props="{}"><div></div></div>
					

					<div class="SVELTE_HYDRATER contents" data-target="SideNavigation" data-props="{&quot;titleTree&quot;:[{&quot;id&quot;:&quot;dataset-structure&quot;,&quot;label&quot;:&quot;Dataset Structure&quot;,&quot;children&quot;:[{&quot;id&quot;:&quot;data-instances&quot;,&quot;label&quot;:&quot;Data Instances&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Data Instances&quot;},{&quot;id&quot;:&quot;data-splits&quot;,&quot;label&quot;:&quot;Data Splits&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Data Splits&quot;}],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Dataset Structure&quot;},{&quot;id&quot;:&quot;dataset-creation&quot;,&quot;label&quot;:&quot;Dataset Creation&quot;,&quot;children&quot;:[{&quot;id&quot;:&quot;source-data&quot;,&quot;label&quot;:&quot;Source Data&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Source Data&quot;},{&quot;id&quot;:&quot;annotations&quot;,&quot;label&quot;:&quot;Annotations&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Annotations&quot;}],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Dataset Creation&quot;},{&quot;id&quot;:&quot;considerations-for-using-the-data&quot;,&quot;label&quot;:&quot;Considerations for Using the Data&quot;,&quot;children&quot;:[{&quot;id&quot;:&quot;discussion-and-biases&quot;,&quot;label&quot;:&quot;Discussion and Biases&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Discussion and Biases&quot;},{&quot;id&quot;:&quot;other-known-limitations&quot;,&quot;label&quot;:&quot;Other Known Limitations&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Other Known Limitations&quot;}],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Considerations for Using the Data&quot;},{&quot;id&quot;:&quot;additional-information&quot;,&quot;label&quot;:&quot;Additional Information&quot;,&quot;children&quot;:[{&quot;id&quot;:&quot;licensing-information&quot;,&quot;label&quot;:&quot;Licensing information&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Licensing information&quot;},{&quot;id&quot;:&quot;citation-information&quot;,&quot;label&quot;:&quot;Citation Information&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Citation Information&quot;}],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Additional Information&quot;}],&quot;classNames&quot;:&quot;top-6&quot;}">

<div class="absolute -left-12 bottom-0 top-0 z-10 top-6"><div class="sticky top-4 flex"><div class="h-7 pt-[0.175rem]">
				<span class="peer" tabindex="0"><button class="select-none hover:cursor-pointer"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-lg opacity-80 hover:opacity-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg></button></span>
				<div class="invisible w-0 -translate-x-24 -translate-y-6 overflow-hidden rounded-xl border bg-white transition-transform hover:visible hover:w-52 hover:translate-x-0 peer-focus-within:visible peer-focus-within:w-52 peer-focus-within:translate-x-0"><nav aria-label="Secondary" class="max-h-[550px] overflow-y-auto p-3"><ul><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 *:break-words hover:underline active:text-gray-900 dark:active:text-gray-200" href="#dataset-structure" title="Dataset Structure"><!-- HTML_TAG_START -->Dataset Structure<!-- HTML_TAG_END --></a>
									<ul class="pl-1"><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="#data-instances" title="Data Instances"><!-- HTML_TAG_START -->Data Instances<!-- HTML_TAG_END --></a>
												<ul class="pl-2"></ul>
											</li><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="#data-splits" title="Data Splits"><!-- HTML_TAG_START -->Data Splits<!-- HTML_TAG_END --></a>
												<ul class="pl-2"></ul>
											</li></ul>
								</li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 *:break-words hover:underline active:text-gray-900 dark:active:text-gray-200" href="#dataset-creation" title="Dataset Creation"><!-- HTML_TAG_START -->Dataset Creation<!-- HTML_TAG_END --></a>
									<ul class="pl-1"><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="#source-data" title="Source Data"><!-- HTML_TAG_START -->Source Data<!-- HTML_TAG_END --></a>
												<ul class="pl-2"></ul>
											</li><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="#annotations" title="Annotations"><!-- HTML_TAG_START -->Annotations<!-- HTML_TAG_END --></a>
												<ul class="pl-2"></ul>
											</li></ul>
								</li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 *:break-words hover:underline active:text-gray-900 dark:active:text-gray-200" href="#considerations-for-using-the-data" title="Considerations for Using the Data"><!-- HTML_TAG_START -->Considerations for Using the Data<!-- HTML_TAG_END --></a>
									<ul class="pl-1"><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="#discussion-and-biases" title="Discussion and Biases"><!-- HTML_TAG_START -->Discussion and Biases<!-- HTML_TAG_END --></a>
												<ul class="pl-2"></ul>
											</li><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="#other-known-limitations" title="Other Known Limitations"><!-- HTML_TAG_START -->Other Known Limitations<!-- HTML_TAG_END --></a>
												<ul class="pl-2"></ul>
											</li></ul>
								</li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 *:break-words hover:underline active:text-gray-900 dark:active:text-gray-200" href="#additional-information" title="Additional Information"><!-- HTML_TAG_START -->Additional Information<!-- HTML_TAG_END --></a>
									<ul class="pl-1"><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="#licensing-information" title="Licensing information"><!-- HTML_TAG_START -->Licensing information<!-- HTML_TAG_END --></a>
												<ul class="pl-2"></ul>
											</li><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="#citation-information" title="Citation Information"><!-- HTML_TAG_START -->Citation Information<!-- HTML_TAG_END --></a>
												<ul class="pl-2"></ul>
											</li></ul>
								</li></ul></nav></div></div></div></div></div>
					<div class="2xl:pr-6"><div class="prose pl-6 -ml-6 hf-sanitized hf-sanitized-A5YP4gcBhjJ-umnNS47be">
	<!-- HTML_TAG_START --><h1 class="relative group flex items-center">
	<a rel="nofollow" href="#dataset-card-for-unarxive-imrad-classification" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-card-for-unarxive-imrad-classification">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Card for unarXive IMRaD classification
	</span>
</h1>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="#dataset-summary" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-summary">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Summary
	</span>
</h3>
<p>The unarXive IMRaD classification dataset contains 530k paragraphs from computer science papers and the IMRaD section they originate from. The paragraphs are derived from <a rel="nofollow" href="https://github.com/IllDepence/unarXive">unarXive</a>.</p>
<p>The dataset can be used as follows.</p>
<pre><code>from datasets import load_dataset

imrad_data = load_dataset('saier/unarXive_imrad_clf')
imrad_data = imrad_data.class_encode_column('label')  # assign target label column
imrad_data = imrad_data.remove_columns('_id')         # remove sample ID column
</code></pre>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="#dataset-structure" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-structure">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Structure
	</span>
</h2>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="#data-instances" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="data-instances">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Data Instances
	</span>
</h3>
<p>Each data instance contains the paragraphâ€™s text as well as one of the labels ('i', 'm', 'r', 'd', 'w' â€” for Introduction, Methods, Results, Discussion and Related Work). An example is shown below.</p>
<pre><code>{'_id': '789f68e7-a1cc-4072-b07d-ecffc3e7ca38',
 'label': 'm',
 'text': 'To link the mentions encoded by BERT to the KGE entities, we define '
         'an entity linking loss as cross-entropy between self-supervised '
         'entity labels and similarities obtained from the linker in KGE '
         'space:\n'
         '\\(\\mathcal {L}_{EL}=\\sum -\\log \\dfrac{\\exp (h_m^{proj}\\cdot '
         '\\textbf {e})}{\\sum _{\\textbf {e}_j\\in \\mathcal {E}} \\exp '
         '(h_m^{proj}\\cdot \\textbf {e}_j)}\\) \n'}
</code></pre>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="#data-splits" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="data-splits">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Data Splits
	</span>
</h3>
<p>The data is split into training, development, and testing data as follows.</p>
<ul>
<li>Training: 520,053 instances</li>
<li>Development: 5000 instances</li>
<li>Testing: 5001 instances</li>
</ul>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="#dataset-creation" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-creation">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Creation
	</span>
</h2>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="#source-data" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="source-data">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Source Data
	</span>
</h3>
<p>The paragraph texts are extracted from the data set <a rel="nofollow" href="https://github.com/IllDepence/unarXive">unarXive</a>.</p>
<h4 class="relative group flex items-center">
	<a rel="nofollow" href="#who-are-the-source-language-producers" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="who-are-the-source-language-producers">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Who are the source language producers?
	</span>
</h4>
<p>The paragraphs were written by the authors of the arXiv papers. In file <code>license_info.jsonl</code> author and text licensing information can be found for all samples, An example is shown below.</p>
<pre><code>
{'authors': 'Yusuke Sekikawa, Teppei Suzuki',
 'license': 'http://creativecommons.org/licenses/by/4.0/',
 'paper_arxiv_id': '2011.09852',
 'sample_ids': ['cc375518-347c-43d0-bfb2-f88564d66df8',
                '18dc073e-a48e-488e-b34c-e5fc3cb8a4ca',
                '0c2e89b3-d863-4bc2-9e11-8f6c48d867cb',
                'd85e46cf-b11d-49b6-801b-089aa2dd037d',
                '92915cea-17ab-4a98-aad2-417f6cdd53d2',
                'e88cb422-47b7-4f69-9b0b-fbddf8140d98',
                '4f5094a4-0e6e-46ae-a34d-e15ce0b9803c',
                '59003494-096f-4a7c-ad65-342b74eed561',
                '6a99b3f5-217e-4d3d-a770-693483ef8670']}
</code></pre>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="#annotations" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="annotations">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Annotations
	</span>
</h3>
<p>Class labels were automatically determined (<a rel="nofollow" href="https://github.com/IllDepence/unarXive/blob/master/src/utility_scripts/ml_tasks_prep_data.py">see implementation</a>).</p>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="#considerations-for-using-the-data" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="considerations-for-using-the-data">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Considerations for Using the Data
	</span>
</h2>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="#discussion-and-biases" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="discussion-and-biases">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Discussion and Biases
	</span>
</h3>
<p>Because only paragraphs unambiguously assignable to one of the IMRaD classeswere used, a certain selection bias is to be expected in the data.</p>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="#other-known-limitations" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="other-known-limitations">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Other Known Limitations
	</span>
</h3>
<p>Depending on authorsâ€™ writing styles as well LaTeX processing quirks, paragraphs can vary in length a significantly.</p>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="#additional-information" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="additional-information">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Additional Information
	</span>
</h2>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="#licensing-information" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="licensing-information">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Licensing information
	</span>
</h3>
<p>The dataset is released under the Creative Commons Attribution-ShareAlike 4.0.</p>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="#citation-information" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="citation-information">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Citation Information
	</span>
</h3>
<pre><code>@inproceedings{Saier2023unarXive,
  author        = {Saier, Tarek and Krause, Johan and F\"{a}rber, Michael},
  title         = {{unarXive 2022: All arXiv Publications Pre-Processed for NLP, Including Structured Full-Text and Citation Network}},
  booktitle     = {Proceedings of the 23rd ACM/IEEE Joint Conference on Digital Libraries},
  year          = {2023},
  series        = {JCDL '23}
}
</code></pre>
<!-- HTML_TAG_END --></div>
</div></section>
			
			<section class="pt-6 border-gray-100 md:pb-24 md:pl-6 md:w-64 lg:w-80 xl:w-96 flex-none order-first md:order-none md:border-l pt-3! md:pt-6!"><dl class="flex items-baseline justify-between"><dt class="text-sm text-gray-500">Downloads last month</dt><div class="mx-4 flex-1 border-b border-dotted"></div><dd class="font-semibold">223</dd></dl>
				
				<div class="divider-column-vertical"></div>
				<div class="grid grid-cols-2 gap-x-2 md:flex md:flex-row md:flex-wrap"><div class="SVELTE_HYDRATER contents" data-target="DatasetAndModelActionsDropdown" data-props="{&quot;classNames&quot;:&quot;order-last&quot;,&quot;discussionsDisabled&quot;:false,&quot;repo&quot;:{&quot;type&quot;:&quot;dataset&quot;,&quot;name&quot;:&quot;saier/unarXive_imrad_clf&quot;},&quot;canWrite&quot;:false,&quot;canDisable&quot;:false,&quot;repoIsPrivate&quot;:false,&quot;repoIsGated&quot;:false,&quot;repoIsDisabled&quot;:false,&quot;repoIsAdminFlaggedNFAA&quot;:false,&quot;repoHasBlockedOids&quot;:false}"><div class="order-last"><div class="relative ">
	<button class="btn px-1.5 py-1.5 " type="button">
		
			<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="p-0.5" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><circle cx="16" cy="7" r="3" fill="currentColor"></circle><circle cx="16" cy="16" r="3" fill="currentColor"></circle><circle cx="16" cy="25" r="3" fill="currentColor"></circle></svg>
		
		</button>
	
	
	</div></div>













</div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetLibrary" data-props="{&quot;classNames&quot;:&quot;md:w-full xl:w-auto xl:flex-none&quot;,&quot;libraries&quot;:[{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;datasets&quot;,&quot;function&quot;:&quot;load_dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{},&quot;code&quot;:&quot;from datasets import load_dataset\n\nds = load_dataset(\&quot;saier/unarXive_imrad_clf\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;pandas&quot;,&quot;function&quot;:&quot;pd.read_json&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;data/train.jsonl&quot;,&quot;validation&quot;:&quot;data/dev.jsonl&quot;,&quot;test&quot;:&quot;data/test.jsonl&quot;},&quot;lines&quot;:true},&quot;code&quot;:&quot;import pandas as pd\n\nsplits = {'train': 'data/train.jsonl', 'validation': 'data/dev.jsonl', 'test': 'data/test.jsonl'}\ndf = pd.read_json(\&quot;hf://datasets/saier/unarXive_imrad_clf/\&quot; + splits[\&quot;train\&quot;, lines=True])&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;mlcroissant&quot;,&quot;function&quot;:&quot;Dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;record_set&quot;:&quot;default&quot;,&quot;partial&quot;:false},&quot;code&quot;:&quot;from mlcroissant import Dataset\n\nds = Dataset(jsonld=\&quot;https://huggingface.co/api/datasets/saier/unarXive_imrad_clf/croissant\&quot;)\nrecords = ds.records(\&quot;default\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;polars&quot;,&quot;function&quot;:&quot;pl.read_ndjson&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;data/train.jsonl&quot;,&quot;validation&quot;:&quot;data/dev.jsonl&quot;,&quot;test&quot;:&quot;data/test.jsonl&quot;}},&quot;code&quot;:&quot;import polars as pl\n\nsplits = {'train': 'data/train.jsonl', 'validation': 'data/dev.jsonl', 'test': 'data/test.jsonl'}\ndf = pl.read_ndjson('hf://datasets/saier/unarXive_imrad_clf/' + splits['train'])\n&quot;}]}]}"><div class="relative md:w-full xl:w-auto xl:flex-none">
	<button class="from-gray-800! to-black! max-xl:mb-2 text-white! gap-1! border-gray-800! dark:border-gray-900!  btn w-full cursor-pointer text-sm" type="button">
		<svg class="mr-1.5 mr-0.5! " xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>
			Use this dataset
		<svg class="-mr-1 text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path d="M16.293 9.293L12 13.586L7.707 9.293l-1.414 1.414L12 16.414l5.707-5.707z" fill="currentColor"></path></svg></button>
	
	
	</div>

</div>
					
					</div>
				<div class="divider-column-vertical"></div>
					<div class="flex flex-col flex-wrap xl:flex-row"><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 " href="https://github.com/IllDepence/unarXive" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Homepage:</div>
		<div class="truncate text-sm group-hover:underline">
			<!-- HTML_TAG_START -->github.com<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 " href="https://arxiv.org/abs/2303.14957" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Paper:</div>
		<div class="truncate text-sm group-hover:underline">
			<!-- HTML_TAG_START -->unarXive 2022: All arXiv Publications Pre-Processed for NLP, Including Structured Full-Text and Citation Network<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of downloaded dataset files:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->482 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 " href="/datasets/saier/unarXive_imrad_clf/tree/refs%2Fconvert%2Fparquet/" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of the auto-converted Parquet files:</div>
		<div class="truncate text-sm group-hover:underline">
			<!-- HTML_TAG_START -->247 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Number of rows:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->530,054<!-- HTML_TAG_END --></div></a></div>
				
				
				<div class="divider-column-vertical"></div>
					<h2 class="text-smd mb-5 flex items-baseline overflow-hidden whitespace-nowrap font-semibold text-gray-800"><svg class="mr-1.5 text-sm inline self-center flex-none text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
						Models trained or fine-tuned on
						<span class="ml-1 truncate font-mono text-[0.87rem] font-medium">saier/unarXive_imrad_clf</span></h2>

					<div class="space-y-3"><div class=""><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/davidschulte/ESM_saier__unarXive_imrad_clf_default"><div class="w-full truncate"><header class="flex items-center mb-1" title="davidschulte/ESM_saier__unarXive_imrad_clf_default"><img alt="" class="w-3 h-3 rounded-full mr-1.5 flex-none flex-none" src="/avatars/ab7fb2af582f38cbc33debb349fb67f2.svg" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">davidschulte/ESM_saier__unarXive_imrad_clf_default</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400">
	
				<span class="truncate">Updated
					<time datetime="2025-03-25T10:55:32" title="Tue, 25 Mar 2025 10:55:32 GMT">about 6 hours ago</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					17
				
	
				

				</div></div>
		
	</a></article>
							</div>
						</div>
				

				
				<div class="divider-column-vertical md:hidden"></div></section></div></main>

	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container relative flex flex-col justify-between space-y-2 py-6 text-gray-500 max-md:*:self-start md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="SVELTE_HYDRATER contents" data-target="ThemeSwitcher" data-props="{&quot;theme&quot;:&quot;system&quot;,&quot;isLoggedIn&quot;:false,&quot;menuClassNames&quot;:&quot;md:-top-24&quot;,&quot;classNames&quot;:&quot;max-md:mb-5 max-md:*:self-start&quot;}">
<div class="relative inline-block max-md:mb-5 max-md:*:self-start">
	<button class="rounded-full border border-gray-100 pl-2 py-1 pr-2.5  flex items-center text-sm text-gray-500 bg-white hover:bg-purple-50 hover:border-purple-200 dark:hover:bg-gray-800 dark:hover:border-gray-950 dark:border-gray-800 " type="button">
		<svg class="mr-1.5 text-gray-500" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M6 22.5h20a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2H6a2 2 0 0 0-2 2v13.5a2 2 0 0 0 2 2ZM7 7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h18a1 1 0 0 0 1-1V8a1 1 0 0 0-1-1H7Z" fill="currentColor"></path><path d="M6 8a1 1 0 0 1 1-1h18a1 1 0 0 1 1 1v11a1 1 0 0 1-1 1H7a1 1 0 0 1-1-1V8Z" fill="currentColor" fill-opacity=".4"></path><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path></svg>
			System theme
		</button>
	
	
	</div></div>
		<div class="font-semibold text-black md:hidden">Company</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="max-md:mb-4! max-md:mt-8! group flex-none max-md:order-last"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="max-md:mt-8! font-semibold text-black md:hidden">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("\/front\/build\/kube-68d7aa0\/index.js");
			window.moonSha = "kube-68d7aa0\/";
			window.__hf_deferred = {};
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>
	</body>
</html>
