<!doctype html>
<html class="">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
		<meta name="description" content="We’re on a journey to advance and democratize artificial intelligence through open source and open science." />
		<meta property="fb:app_id" content="1321688464574422" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@huggingface" />
		<meta name="twitter:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/GPUMODE/Inductor_Created_Data_Permissive.png" />
		<meta property="og:title" content="GPUMODE/Inductor_Created_Data_Permissive · Datasets at Hugging Face" />
		<meta property="og:type" content="website" />
		<meta property="og:url" content="https://huggingface.co/datasets/GPUMODE/Inductor_Created_Data_Permissive" />
		<meta property="og:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/GPUMODE/Inductor_Created_Data_Permissive.png" />

		<link rel="stylesheet" href="/front/build/kube-68d7aa0/style.css" />

		<link rel="preconnect" href="https://fonts.gstatic.com" />
		<link
			href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap"
			rel="stylesheet"
		/>

		<link
			rel="preload"
			href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css"
			as="style"
			onload="this.onload=null;this.rel='stylesheet'"
		/>
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		<script>const guestTheme = document.cookie.match(/theme=(\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches));</script>
<link rel="canonical" href="https://huggingface.co/datasets/GPUMODE/Inductor_Created_Data_Permissive"> <script type="application/ld+json">{
  "@context": {
    "@language": "en",
    "@vocab": "https:\/\/schema.org\/",
    "citeAs": "cr:citeAs",
    "column": "cr:column",
    "conformsTo": "dct:conformsTo",
    "cr": "http:\/\/mlcommons.org\/croissant\/",
    "data": {
      "@id": "cr:data",
      "@type": "@json"
    },
    "dataBiases": "cr:dataBiases",
    "dataCollection": "cr:dataCollection",
    "dataType": {
      "@id": "cr:dataType",
      "@type": "@vocab"
    },
    "dct": "http:\/\/purl.org\/dc\/terms\/",
    "extract": "cr:extract",
    "field": "cr:field",
    "fileProperty": "cr:fileProperty",
    "fileObject": "cr:fileObject",
    "fileSet": "cr:fileSet",
    "format": "cr:format",
    "includes": "cr:includes",
    "isLiveDataset": "cr:isLiveDataset",
    "jsonPath": "cr:jsonPath",
    "key": "cr:key",
    "md5": "cr:md5",
    "parentField": "cr:parentField",
    "path": "cr:path",
    "personalSensitiveInformation": "cr:personalSensitiveInformation",
    "recordSet": "cr:recordSet",
    "references": "cr:references",
    "regex": "cr:regex",
    "repeated": "cr:repeated",
    "replace": "cr:replace",
    "sc": "https:\/\/schema.org\/",
    "separator": "cr:separator",
    "source": "cr:source",
    "subField": "cr:subField",
    "transform": "cr:transform"
  },
  "@type": "sc:Dataset",
  "distribution": [
    {
      "@type": "cr:FileObject",
      "@id": "repo",
      "name": "repo",
      "description": "The Hugging Face git repository.",
      "contentUrl": "https:\/\/huggingface.co\/datasets\/GPUMODE\/Inductor_Created_Data_Permissive\/tree\/refs%2Fconvert%2Fparquet",
      "encodingFormat": "git+https",
      "sha256": "https:\/\/github.com\/mlcommons\/croissant\/issues\/80"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-default",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "default\/*\/*.parquet"
    }
  ],
  "recordSet": [
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "default_splits\/split_name"
      },
      "@id": "default_splits",
      "name": "default_splits",
      "description": "Splits for the default config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default_splits\/split_name",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "default_splits\/split_name": "train"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "default",
      "description": "GPUMODE\/Inductor_Created_Data_Permissive - 'default' subset",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default\/split",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "default\/(?:partial-)?(train)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "default_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/entry_point",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "entry_point"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/original_triton_code",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "original_triton_code"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/python_code",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "python_code"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/triton_code",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "triton_code"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/repo_name",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "repo_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/module_name",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "module_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/synthetic",
          "dataType": "sc:Boolean",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "synthetic"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/uuid",
          "dataType": "sc:Integer",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "uuid"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/licenses",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "licenses"
            }
          },
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "default\/stars",
          "dataType": "sc:Integer",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "stars"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/sha",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "sha"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/repo_link",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "repo_link"
            }
          }
        }
      ]
    }
  ],
  "conformsTo": "http:\/\/mlcommons.org\/croissant\/1.0",
  "name": "Inductor_Created_Data_Permissive",
  "description": "\n\t\n\t\t\n\t\tOverview\n\t\n\ndataset_permissive{.json\/.parquet} is a curated collection of pairs of pytorch programs and equivalent triton code (generated by torch inductor) which can be used to train models to translate pytorch code to triton code.\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\nThe dataset was created through the following process:\n\nRepository Collection: PyTorch repositories were collected from GitHub using repositories (and associated hashes) from the Stack v1.\nPyTorch Module Extraction: We extracted… See the full description on the dataset page: https:\/\/huggingface.co\/datasets\/GPUMODE\/Inductor_Created_Data_Permissive.",
  "alternateName": [
    "GPUMODE\/Inductor_Created_Data_Permissive"
  ],
  "creator": {
    "@type": "Organization",
    "name": "GPU MODE",
    "url": "https:\/\/huggingface.co\/GPUMODE"
  },
  "keywords": [
    "English",
    "mit",
    "10K - 100K",
    "parquet",
    "Tabular",
    "Text",
    "Datasets",
    "pandas",
    "Croissant",
    "Polars",
    "🇺🇸 Region: US"
  ],
  "license": "https:\/\/choosealicense.com\/licenses\/mit\/",
  "url": "https:\/\/huggingface.co\/datasets\/GPUMODE\/Inductor_Created_Data_Permissive"
}</script> 

		<title>GPUMODE/Inductor_Created_Data_Permissive · Datasets at Hugging Face</title>

		<script
			defer
			data-domain="huggingface.co"
			event-loggedIn="false"
			src="/js/script.pageview-props.js"
		></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script>
			window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer></script>
	</head>
	<body class="flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DatasetPage">
		<div class="flex min-h-dvh flex-col"><div class="SVELTE_HYDRATER contents" data-target="SystemThemeMonitor" data-props="{&quot;isLoggedIn&quot;:false}"></div>

	<div class="SVELTE_HYDRATER contents" data-target="MainHeader" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false,&quot;isPro&quot;:false}"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg">
				<span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a>
			<div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..."   spellcheck="false" type="text" value="">
	<svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
	</div>
			<div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg>
		</button>

	</div></div>
		<nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 2xl:space-x-2"><li class="hover:text-indigo-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
					Models</a>
			</li><li class="hover:text-red-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					Datasets</a>
			</li><li class="hover:text-blue-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg>
					Spaces</a>
			</li><li class="hover:text-yellow-700 max-xl:hidden"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 text-yellow-500!" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg>
					Posts</a>
			</li><li class="hover:text-yellow-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 16 16"><path d="m2.28 3.7-.3.16a.67.67 0 0 0-.34.58v8.73l.01.04.02.07.01.04.03.06.02.04.02.03.04.06.05.05.04.04.06.04.06.04.08.04.08.02h.05l.07.02h.11l.04-.01.07-.02.03-.01.07-.03.22-.12a5.33 5.33 0 0 1 5.15.1.67.67 0 0 0 .66 0 5.33 5.33 0 0 1 5.33 0 .67.67 0 0 0 1-.58V4.36a.67.67 0 0 0-.34-.5l-.3-.17v7.78a.63.63 0 0 1-.87.59 4.9 4.9 0 0 0-4.35.35l-.65.39a.29.29 0 0 1-.15.04.29.29 0 0 1-.16-.04l-.65-.4a4.9 4.9 0 0 0-4.34-.34.63.63 0 0 1-.87-.59V3.7Z" fill="currentColor" class="dark:opacity-40"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M8 3.1a5.99 5.99 0 0 0-5.3-.43.66.66 0 0 0-.42.62v8.18c0 .45.46.76.87.59a4.9 4.9 0 0 1 4.34.35l.65.39c.05.03.1.04.16.04.05 0 .1-.01.15-.04l.65-.4a4.9 4.9 0 0 1 4.35-.34.63.63 0 0 0 .86-.59V3.3a.67.67 0 0 0-.41-.62 5.99 5.99 0 0 0-5.3.43l-.3.17L8 3.1Zm.73 1.87a.43.43 0 1 0-.86 0v5.48a.43.43 0 0 0 .86 0V4.97Z" fill="currentColor" class="opacity-40 dark:opacity-100"></path><path d="M8.73 4.97a.43.43 0 1 0-.86 0v5.48a.43.43 0 1 0 .86 0V4.96Z" fill="currentColor" class="dark:opacity-40"></path></svg>
					Docs</a>
			</li><li class="hover:text-black dark:hover:text-white"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/enterprise"><svg class="mr-1.5 text-gray-400 group-hover:text-black dark:group-hover:text-white" xmlns="http://www.w3.org/2000/svg" fill="none" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 33 27"><path fill="currentColor" fill-rule="evenodd" d="M13.5.7a8.7 8.7 0 0 0-7.7 5.7L1 20.6c-1 3.1.9 5.7 4.1 5.7h15c3.3 0 6.8-2.6 7.8-5.7l4.6-14.2c1-3.1-.8-5.7-4-5.7h-15Zm1.1 5.7L9.8 20.3h9.8l1-3.1h-5.8l.8-2.5h4.8l1.1-3h-4.8l.8-2.3H23l1-3h-9.5Z" clip-rule="evenodd"></path></svg>
					Enterprise</a>
			</li>

		<li><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/pricing">Pricing
			</a></li>

		<li><div class="relative group">
	<button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button">
		<svg class=" text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>
			
		</button>
	
	
	</div></li>
		<li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li>
		<li><a class="block cursor-pointer whitespace-nowrap px-2 py-0.5 hover:text-gray-500 dark:text-gray-300 dark:hover:text-gray-100" href="/login">Log In
				</a></li>
			<li><a class="whitespace-nowrap rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up
					</a></li></ul></nav></div></header></div>
	
	
	
	<div class="SVELTE_HYDRATER contents" data-target="SSOBanner" data-props="{}"></div>
	
	

	<main class="flex flex-1 flex-col">
	<div class="SVELTE_HYDRATER contents" data-target="DatasetHeader" data-props="{&quot;activeTab&quot;:&quot;datasetCard&quot;,&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/608cdf95bf3d1d8d1aca928f/OPvrzyMsSCrzBPfa3VX3-.jpeg&quot;,&quot;fullname&quot;:&quot;GPU MODE&quot;,&quot;name&quot;:&quot;GPUMODE&quot;,&quot;type&quot;:&quot;org&quot;,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;isEnterprise&quot;:false,&quot;followerCount&quot;:5},&quot;canReadRepoSettings&quot;:false,&quot;dataset&quot;:{&quot;author&quot;:&quot;GPUMODE&quot;,&quot;cardData&quot;:{&quot;license&quot;:&quot;mit&quot;,&quot;language&quot;:[&quot;en&quot;]},&quot;cardExists&quot;:true,&quot;createdAt&quot;:&quot;2025-03-14T22:55:06.000Z&quot;,&quot;description&quot;:&quot;\n\t\n\t\t\n\t\tOverview\n\t\n\ndataset_permissive{.json/.parquet} is a curated collection of pairs of pytorch programs and equivalent triton code (generated by torch inductor) which can be used to train models to translate pytorch code to triton code.\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\nThe dataset was created through the following process:\n\nRepository Collection: PyTorch repositories were collected from GitHub using repositories (and associated hashes) from the Stack v1.\nPyTorch Module Extraction: We extracted… See the full description on the dataset page: https://huggingface.co/datasets/GPUMODE/Inductor_Created_Data_Permissive.&quot;,&quot;downloads&quot;:39,&quot;downloadsAllTime&quot;:39,&quot;id&quot;:&quot;GPUMODE/Inductor_Created_Data_Permissive&quot;,&quot;isLikedByUser&quot;:false,&quot;lastModified&quot;:&quot;2025-03-14T23:14:43.000Z&quot;,&quot;likes&quot;:4,&quot;datasetsServerInfo&quot;:{&quot;viewer&quot;:&quot;viewer&quot;,&quot;numRows&quot;:18162,&quot;libraries&quot;:[&quot;datasets&quot;,&quot;pandas&quot;,&quot;mlcroissant&quot;,&quot;polars&quot;],&quot;formats&quot;:[&quot;parquet&quot;],&quot;modalities&quot;:[&quot;tabular&quot;,&quot;text&quot;]},&quot;discussionsDisabled&quot;:false,&quot;repoType&quot;:&quot;dataset&quot;,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;tags&quot;:[&quot;language:en&quot;,&quot;license:mit&quot;,&quot;size_categories:10K<n<100K&quot;,&quot;format:parquet&quot;,&quot;modality:tabular&quot;,&quot;modality:text&quot;,&quot;library:datasets&quot;,&quot;library:pandas&quot;,&quot;library:mlcroissant&quot;,&quot;library:polars&quot;,&quot;region:us&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;language:en&quot;,&quot;label&quot;:&quot;English&quot;,&quot;type&quot;:&quot;language&quot;},{&quot;id&quot;:&quot;license:mit&quot;,&quot;label&quot;:&quot;mit&quot;,&quot;type&quot;:&quot;license&quot;},{&quot;id&quot;:&quot;size_categories:10K<n<100K&quot;,&quot;label&quot;:&quot;10K - 100K&quot;,&quot;type&quot;:&quot;size_categories&quot;},{&quot;id&quot;:&quot;format:parquet&quot;,&quot;label&quot;:&quot;parquet&quot;,&quot;type&quot;:&quot;format&quot;},{&quot;id&quot;:&quot;modality:tabular&quot;,&quot;label&quot;:&quot;Tabular&quot;,&quot;type&quot;:&quot;modality&quot;},{&quot;id&quot;:&quot;modality:text&quot;,&quot;label&quot;:&quot;Text&quot;,&quot;type&quot;:&quot;modality&quot;},{&quot;id&quot;:&quot;library:datasets&quot;,&quot;label&quot;:&quot;Datasets&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:pandas&quot;,&quot;label&quot;:&quot;pandas&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:mlcroissant&quot;,&quot;label&quot;:&quot;Croissant&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:polars&quot;,&quot;label&quot;:&quot;Polars&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;🇺🇸 Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;}],&quot;hasBlockedOids&quot;:false,&quot;region&quot;:&quot;us&quot;,&quot;xetEnabled&quot;:false},&quot;discussionsStats&quot;:{&quot;closed&quot;:0,&quot;open&quot;:0,&quot;total&quot;:0}}"><header class="bg-linear-to-t border-b border-gray-100 pt-6 sm:pt-9 from-gray-50-to-white via-white dark:via-gray-950"><div class="container relative "><h1 class="flex flex-wrap items-center max-md:leading-tight mb-3 text-lg max-sm:gap-y-1.5 md:text-xl"><a href="/datasets" class="group flex items-center"><svg class="sm:mr-1.5 -mr-1 text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					<span class="mr-2.5 font-semibold text-gray-400 group-hover:text-gray-500 max-sm:hidden">Datasets:</span></a>
				<hr class="mx-1.5 h-2 translate-y-px rounded-sm border-r dark:border-gray-600 sm:hidden">
			<div class="group flex flex-none items-center"><div class="relative mr-1 flex items-center">

			

<span class="inline-block "><span class="contents"><a href="/GPUMODE" class="text-gray-400 hover:text-blue-600"><img alt="" class="w-3.5 h-3.5 rounded-sm  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/608cdf95bf3d1d8d1aca928f/OPvrzyMsSCrzBPfa3VX3-.jpeg" crossorigin="anonymous"></a></span>
	</span></div>
		

<span class="inline-block "><span class="contents"><a href="/GPUMODE" class="text-gray-400 hover:text-blue-600">GPUMODE</a></span>
	</span>
		<div class="mx-0.5 text-gray-300">/</div></div>

<div class="max-w-full "><a class="break-words font-mono font-semibold hover:text-blue-600 " href="/datasets/GPUMODE/Inductor_Created_Data_Permissive">Inductor_Created_Data_Permissive</a>
	<button class="relative text-sm mr-4 focus:outline-hidden inline-flex cursor-pointer items-center text-sm  mx-0.5   text-gray-600 " title="Copy dataset name to clipboard" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg>
	
	</button></div>
			<div class="inline-flex items-center overflow-hidden whitespace-nowrap rounded-md border bg-white text-sm leading-none text-gray-500  mr-2"><button class="relative flex items-center overflow-hidden from-red-50 to-transparent dark:from-red-900 px-1.5 py-1 hover:bg-linear-to-t focus:outline-hidden"  title="Like"><svg class="left-1.5 absolute" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>

		
		<span class="ml-4 pl-0.5 ">like</span></button>
	<button class="focus:outline-hidden flex items-center border-l px-1.5 py-1 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="See users who liked this repository">4</button></div>




			<div class="relative flex items-center gap-1.5  "><div class="mr-2 inline-flex h-6 items-center overflow-hidden whitespace-nowrap rounded-md border text-sm text-gray-500"><button class="focus:outline-hidden relative flex h-full max-w-56 items-center gap-1.5 overflow-hidden px-1.5 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" type="button" ><div class="flex h-full flex-1 items-center justify-center ">Follow</div>
		<img alt="" class="rounded-xs size-3 flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/608cdf95bf3d1d8d1aca928f/OPvrzyMsSCrzBPfa3VX3-.jpeg">
		<span class="truncate">GPU MODE</span></button>
	<button class="focus:outline-hidden flex h-full items-center border-l pl-1.5 pr-1.5 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="Show GPU MODE's followers" type="button">5</button></div>

		</div>
			
	</h1>
		<div class="mb-3 flex flex-wrap md:mb-4"><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Modalities:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?modality=modality%3Atabular"><div class="tag tag-white   ">
		<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.619 4.619C2.667 6.572 2.667 9.715 2.667 16s0 9.43 1.952 11.38C6.572 29.333 9.715 29.333 16 29.333s9.43 0 11.38-1.953c1.953-1.952 1.953-5.095 1.953-11.38 0-6.285 0-9.427-1.953-11.381C25.428 2.667 22.285 2.667 16 2.667c-6.285 0-9.427 0-11.381 1.952ZM16.615 10a1 1 0 1 1 0 2h-1.23a1 1 0 1 1 0-2h1.23Zm0 5a1 1 0 1 1 0 2h-1.23a1 1 0 1 1 0-2h1.23Zm3.154 1a1 1 0 0 0 1 1H22a1 1 0 1 0 0-2h-1.23a1 1 0 0 0-1 1Zm-3.154 4a1 1 0 1 1 0 2h-1.23a1 1 0 1 1 0-2h1.23Zm3.154 1a1 1 0 0 0 1 1H22a1 1 0 1 0 0-2h-1.23a1 1 0 0 0-1 1Zm0-10a1 1 0 0 0 1 1H22a1 1 0 1 0 0-2h-1.23a1 1 0 0 0-1 1Zm-8.538-1H10a1 1 0 1 0 0 2h1.23a1 1 0 1 0 0-2Zm0 5H10a1 1 0 1 0 0 2h1.23a1 1 0 1 0 0-2Zm0 5H10a1 1 0 1 0 0 2h1.23a1 1 0 1 0 0-2Z" fill="currentColor"></path></svg>

	

	<span>Tabular</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?modality=modality%3Atext"><div class="tag tag-white   ">
		<svg class="text-red-700 dark:text-red-600" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.619 4.619C2.667 6.573 2.667 9.715 2.667 16s0 9.428 1.952 11.38C6.573 29.333 9.715 29.333 16 29.333s9.428 0 11.38-1.953c1.953-1.95 1.953-5.095 1.953-11.38s0-9.428-1.953-11.381C25.43 2.667 22.285 2.667 16 2.667s-9.428 0-11.381 1.952m8.65 3.714c-.573 0-1.109 0-1.546.066-.495.073-1.003.248-1.41.7-.392.436-.53.956-.59 1.452-.056.464-.056 1.04-.056 1.689V13a1 1 0 1 0 2 0v-.704c0-.724.001-1.176.041-1.505q.015-.15.061-.294a.2.2 0 0 1 .031-.061q0-.003.016-.01a.8.8 0 0 1 .203-.05c.272-.04.654-.043 1.314-.043H15v11.334h-2.333a1 1 0 1 0 0 2H20a1 1 0 0 0 0-2h-3V10.333h1.667c.66 0 1.042.003 1.314.043.123.019.18.04.203.05l.015.009a.2.2 0 0 1 .032.061c.018.05.042.14.061.295.04.329.041.781.041 1.506V13a1 1 0 1 0 2 0v-.76c0-.65 0-1.225-.056-1.69-.06-.495-.198-1.015-.59-1.453-.407-.45-.915-.625-1.41-.698-.437-.067-.973-.067-1.546-.066z" fill="currentColor"></path></svg>

	

	<span>Text</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Formats:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?format=format%3Aparquet"><div class="tag tag-white   ">
		<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 34 34"><path fill-rule="evenodd" clip-rule="evenodd" d="m17.97 18.44-3.98-2.3-7.1 3.78 3.98 2.3 7.1-3.78Zm15.1-1.4-3.99-2.3-16.22 8.63 3.98 2.3 16.22-8.63Zm-5.98-3.45-3.97-2.3-7.1 3.78 3.98 2.3 7.1-3.78Zm-9.94-5.74 3.98 2.3-11.16 5.93L6 13.78l11.16-5.93Zm-13.19 7 3.98 2.3-3.04 1.62-3.98-2.3 3.04-1.61Z" fill="currentColor"></path></svg>

	

	<span>parquet</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Languages:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?language=language%3Aen"><div class="tag tag-white   ">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="text-green-600/80" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 10 10"><path fill-rule="evenodd" clip-rule="evenodd" d="M0.625 5C0.625 6.16032 1.08594 7.27312 1.90641 8.09359C2.72688 8.91406 3.83968 9.375 5 9.375C6.16032 9.375 7.27312 8.91406 8.09359 8.09359C8.91406 7.27312 9.375 6.16032 9.375 5C9.375 3.83968 8.91406 2.72688 8.09359 1.90641C7.27312 1.08594 6.16032 0.625 5 0.625C3.83968 0.625 2.72688 1.08594 1.90641 1.90641C1.08594 2.72688 0.625 3.83968 0.625 5ZM7.64365 7.48027C7.61734 7.50832 7.59054 7.53598 7.56326 7.56326C7.13828 7.98824 6.61864 8.2968 6.0539 8.46842C6.29802 8.11949 6.49498 7.64804 6.63475 7.09483C7.00845 7.18834 7.35014 7.3187 7.64365 7.48027ZM8.10076 6.87776C8.37677 6.42196 8.55005 5.90894 8.60556 5.37499H6.86808C6.85542 5.71597 6.82551 6.04557 6.77971 6.35841C7.25309 6.47355 7.68808 6.6414 8.062 6.85549C8.07497 6.86283 8.08789 6.87025 8.10076 6.87776ZM6.03795 6.22536C6.07708 5.95737 6.1044 5.67232 6.11705 5.37499H3.88295C3.89666 5.69742 3.92764 6.00542 3.9722 6.29287C4.37075 6.21726 4.79213 6.17749 5.224 6.17749C5.50054 6.17749 5.77294 6.19376 6.03795 6.22536ZM4.1261 7.02673C4.34894 7.84835 4.68681 8.375 5 8.375C5.32122 8.375 5.66839 7.82101 5.8908 6.963C5.67389 6.93928 5.45082 6.92699 5.224 6.92699C4.84316 6.92699 4.47332 6.96176 4.1261 7.02673ZM3.39783 7.21853C3.53498 7.71842 3.72038 8.14579 3.9461 8.46842C3.42141 8.30898 2.93566 8.03132 2.52857 7.65192C2.77253 7.48017 3.06711 7.33382 3.39783 7.21853ZM3.23916 6.48077C3.18263 6.13193 3.14625 5.76074 3.13192 5.37499H1.39444C1.4585 5.99112 1.67936 6.57938 2.03393 7.08403C2.3706 6.83531 2.78055 6.63162 3.23916 6.48077ZM1.39444 4.62499H3.13192C3.14615 4.24204 3.18211 3.87344 3.23794 3.52681C2.77814 3.37545 2.36731 3.17096 2.03024 2.92123C1.67783 3.42469 1.45828 4.011 1.39444 4.62499ZM2.5237 2.35262C2.76812 2.52552 3.06373 2.67281 3.39584 2.78875C3.53318 2.28573 3.71928 1.85578 3.9461 1.53158C3.41932 1.69166 2.93178 1.97089 2.5237 2.35262ZM3.97101 3.71489C3.92709 4.00012 3.89654 4.30547 3.88295 4.62499H6.11705C6.10453 4.33057 6.07761 4.04818 6.03909 3.78248C5.77372 3.81417 5.50093 3.83049 5.224 3.83049C4.79169 3.83049 4.3699 3.79065 3.97101 3.71489ZM5.8928 3.04476C5.67527 3.06863 5.45151 3.08099 5.224 3.08099C4.84241 3.08099 4.47186 3.04609 4.12405 2.98086C4.34686 2.1549 4.68584 1.625 5 1.625C5.32218 1.625 5.67048 2.18233 5.8928 3.04476ZM6.78083 3.6493C6.826 3.95984 6.85552 4.28682 6.86808 4.62499H8.60556C8.55029 4.09337 8.37827 3.58251 8.10436 3.1282C8.0903 3.1364 8.07618 3.14449 8.062 3.15249C7.68838 3.36641 7.25378 3.53417 6.78083 3.6493ZM7.64858 2.52499C7.35446 2.68754 7.0117 2.81868 6.63664 2.91268C6.49676 2.35623 6.29913 1.88209 6.0539 1.53158C6.61864 1.7032 7.13828 2.01176 7.56326 2.43674C7.59224 2.46572 7.62068 2.49514 7.64858 2.52499Z" fill="currentColor"></path></svg>

	

	<span>English</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Size:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?size_categories=size_categories%3A10K%3Cn%3C100K"><div class="tag tag-white   ">

	

	<span>10K - 100K</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Libraries:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Adatasets"><div class="tag tag-white   "><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 95 88"><path fill="#fff" d="M94.25 70.08a8.28 8.28 0 0 1-.43 6.46 10.57 10.57 0 0 1-3 3.6 25.18 25.18 0 0 1-5.7 3.2 65.74 65.74 0 0 1-7.56 2.65 46.67 46.67 0 0 1-11.42 1.68c-5.42.05-10.09-1.23-13.4-4.5a40.4 40.4 0 0 1-10.14.03c-3.34 3.25-7.99 4.52-13.39 4.47a46.82 46.82 0 0 1-11.43-1.68 66.37 66.37 0 0 1-7.55-2.65c-2.28-.98-4.17-2-5.68-3.2a10.5 10.5 0 0 1-3.02-3.6c-.99-2-1.18-4.3-.42-6.46a8.54 8.54 0 0 1-.33-5.63c.25-.95.66-1.83 1.18-2.61a8.67 8.67 0 0 1 2.1-8.47 8.23 8.23 0 0 1 2.82-2.07 41.75 41.75 0 1 1 81.3-.12 8.27 8.27 0 0 1 3.11 2.19 8.7 8.7 0 0 1 2.1 8.47c.52.78.93 1.66 1.18 2.61a8.61 8.61 0 0 1-.32 5.63Z"></path><path fill="#FFD21E" d="M47.21 76.5a34.75 34.75 0 1 0 0-69.5 34.75 34.75 0 0 0 0 69.5Z"></path><path fill="#FF9D0B" d="M81.96 41.75a34.75 34.75 0 1 0-69.5 0 34.75 34.75 0 0 0 69.5 0Zm-73.5 0a38.75 38.75 0 1 1 77.5 0 38.75 38.75 0 0 1-77.5 0Z"></path><path fill="#3A3B45" d="M58.5 32.3c1.28.44 1.78 3.06 3.07 2.38a5 5 0 1 0-6.76-2.07c.61 1.15 2.55-.72 3.7-.32ZM34.95 32.3c-1.28.44-1.79 3.06-3.07 2.38a5 5 0 1 1 6.76-2.07c-.61 1.15-2.56-.72-3.7-.32Z"></path><path fill="#FF323D" d="M46.96 56.29c9.83 0 13-8.76 13-13.26 0-2.34-1.57-1.6-4.09-.36-2.33 1.15-5.46 2.74-8.9 2.74-7.19 0-13-6.88-13-2.38s3.16 13.26 13 13.26Z"></path><path fill="#3A3B45" fill-rule="evenodd" d="M39.43 54a8.7 8.7 0 0 1 5.3-4.49c.4-.12.81.57 1.24 1.28.4.68.82 1.37 1.24 1.37.45 0 .9-.68 1.33-1.35.45-.7.89-1.38 1.32-1.25a8.61 8.61 0 0 1 5 4.17c3.73-2.94 5.1-7.74 5.1-10.7 0-2.34-1.57-1.6-4.09-.36l-.14.07c-2.31 1.15-5.39 2.67-8.77 2.67s-6.45-1.52-8.77-2.67c-2.6-1.29-4.23-2.1-4.23.29 0 3.05 1.46 8.06 5.47 10.97Z" clip-rule="evenodd"></path><path fill="#FF9D0B" d="M70.71 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM24.21 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM17.52 48c-1.62 0-3.06.66-4.07 1.87a5.97 5.97 0 0 0-1.33 3.76 7.1 7.1 0 0 0-1.94-.3c-1.55 0-2.95.59-3.94 1.66a5.8 5.8 0 0 0-.8 7 5.3 5.3 0 0 0-1.79 2.82c-.24.9-.48 2.8.8 4.74a5.22 5.22 0 0 0-.37 5.02c1.02 2.32 3.57 4.14 8.52 6.1 3.07 1.22 5.89 2 5.91 2.01a44.33 44.33 0 0 0 10.93 1.6c5.86 0 10.05-1.8 12.46-5.34 3.88-5.69 3.33-10.9-1.7-15.92-2.77-2.78-4.62-6.87-5-7.77-.78-2.66-2.84-5.62-6.25-5.62a5.7 5.7 0 0 0-4.6 2.46c-1-1.26-1.98-2.25-2.86-2.82A7.4 7.4 0 0 0 17.52 48Zm0 4c.51 0 1.14.22 1.82.65 2.14 1.36 6.25 8.43 7.76 11.18.5.92 1.37 1.31 2.14 1.31 1.55 0 2.75-1.53.15-3.48-3.92-2.93-2.55-7.72-.68-8.01.08-.02.17-.02.24-.02 1.7 0 2.45 2.93 2.45 2.93s2.2 5.52 5.98 9.3c3.77 3.77 3.97 6.8 1.22 10.83-1.88 2.75-5.47 3.58-9.16 3.58-3.81 0-7.73-.9-9.92-1.46-.11-.03-13.45-3.8-11.76-7 .28-.54.75-.76 1.34-.76 2.38 0 6.7 3.54 8.57 3.54.41 0 .7-.17.83-.6.79-2.85-12.06-4.05-10.98-8.17.2-.73.71-1.02 1.44-1.02 3.14 0 10.2 5.53 11.68 5.53.11 0 .2-.03.24-.1.74-1.2.33-2.04-4.9-5.2-5.21-3.16-8.88-5.06-6.8-7.33.24-.26.58-.38 1-.38 3.17 0 10.66 6.82 10.66 6.82s2.02 2.1 3.25 2.1c.28 0 .52-.1.68-.38.86-1.46-8.06-8.22-8.56-11.01-.34-1.9.24-2.85 1.31-2.85Z"></path><path fill="#FFD21E" d="M38.6 76.69c2.75-4.04 2.55-7.07-1.22-10.84-3.78-3.77-5.98-9.3-5.98-9.3s-.82-3.2-2.69-2.9c-1.87.3-3.24 5.08.68 8.01 3.91 2.93-.78 4.92-2.29 2.17-1.5-2.75-5.62-9.82-7.76-11.18-2.13-1.35-3.63-.6-3.13 2.2.5 2.79 9.43 9.55 8.56 11-.87 1.47-3.93-1.71-3.93-1.71s-9.57-8.71-11.66-6.44c-2.08 2.27 1.59 4.17 6.8 7.33 5.23 3.16 5.64 4 4.9 5.2-.75 1.2-12.28-8.53-13.36-4.4-1.08 4.11 11.77 5.3 10.98 8.15-.8 2.85-9.06-5.38-10.74-2.18-1.7 3.21 11.65 6.98 11.76 7.01 4.3 1.12 15.25 3.49 19.08-2.12Z"></path><path fill="#FF9D0B" d="M77.4 48c1.62 0 3.07.66 4.07 1.87a5.97 5.97 0 0 1 1.33 3.76 7.1 7.1 0 0 1 1.95-.3c1.55 0 2.95.59 3.94 1.66a5.8 5.8 0 0 1 .8 7 5.3 5.3 0 0 1 1.78 2.82c.24.9.48 2.8-.8 4.74a5.22 5.22 0 0 1 .37 5.02c-1.02 2.32-3.57 4.14-8.51 6.1-3.08 1.22-5.9 2-5.92 2.01a44.33 44.33 0 0 1-10.93 1.6c-5.86 0-10.05-1.8-12.46-5.34-3.88-5.69-3.33-10.9 1.7-15.92 2.78-2.78 4.63-6.87 5.01-7.77.78-2.66 2.83-5.62 6.24-5.62a5.7 5.7 0 0 1 4.6 2.46c1-1.26 1.98-2.25 2.87-2.82A7.4 7.4 0 0 1 77.4 48Zm0 4c-.51 0-1.13.22-1.82.65-2.13 1.36-6.25 8.43-7.76 11.18a2.43 2.43 0 0 1-2.14 1.31c-1.54 0-2.75-1.53-.14-3.48 3.91-2.93 2.54-7.72.67-8.01a1.54 1.54 0 0 0-.24-.02c-1.7 0-2.45 2.93-2.45 2.93s-2.2 5.52-5.97 9.3c-3.78 3.77-3.98 6.8-1.22 10.83 1.87 2.75 5.47 3.58 9.15 3.58 3.82 0 7.73-.9 9.93-1.46.1-.03 13.45-3.8 11.76-7-.29-.54-.75-.76-1.34-.76-2.38 0-6.71 3.54-8.57 3.54-.42 0-.71-.17-.83-.6-.8-2.85 12.05-4.05 10.97-8.17-.19-.73-.7-1.02-1.44-1.02-3.14 0-10.2 5.53-11.68 5.53-.1 0-.19-.03-.23-.1-.74-1.2-.34-2.04 4.88-5.2 5.23-3.16 8.9-5.06 6.8-7.33-.23-.26-.57-.38-.98-.38-3.18 0-10.67 6.82-10.67 6.82s-2.02 2.1-3.24 2.1a.74.74 0 0 1-.68-.38c-.87-1.46 8.05-8.22 8.55-11.01.34-1.9-.24-2.85-1.31-2.85Z"></path><path fill="#FFD21E" d="M56.33 76.69c-2.75-4.04-2.56-7.07 1.22-10.84 3.77-3.77 5.97-9.3 5.97-9.3s.82-3.2 2.7-2.9c1.86.3 3.23 5.08-.68 8.01-3.92 2.93.78 4.92 2.28 2.17 1.51-2.75 5.63-9.82 7.76-11.18 2.13-1.35 3.64-.6 3.13 2.2-.5 2.79-9.42 9.55-8.55 11 .86 1.47 3.92-1.71 3.92-1.71s9.58-8.71 11.66-6.44c2.08 2.27-1.58 4.17-6.8 7.33-5.23 3.16-5.63 4-4.9 5.2.75 1.2 12.28-8.53 13.36-4.4 1.08 4.11-11.76 5.3-10.97 8.15.8 2.85 9.05-5.38 10.74-2.18 1.69 3.21-11.65 6.98-11.76 7.01-4.31 1.12-15.26 3.49-19.08-2.12Z"></path></svg>

	

	<span>Datasets</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Apandas"><div class="tag tag-white   "><svg class="text-black inline-block text-sm text-[#130754] dark:text-gray-200" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid slice" viewBox="0 0 210.21 280.43"><rect fill="currentColor" x="74.51" y="43.03" width="24.09" height="50.02"></rect><rect fill="currentColor" x="74.51" y="145.78" width="24.09" height="50.02"></rect><rect fill="#ffca00" x="74.51" y="107.65" width="24.09" height="23.6"></rect><rect fill="currentColor" x="35.81" y="84.15" width="24.09" height="166.27"></rect><rect fill="currentColor" x="112.41" y="187.05" width="24.09" height="50.02"></rect><rect fill="currentColor" x="112.41" y="84.21" width="24.09" height="50.02"></rect><rect fill="#e70488" x="112.41" y="148.84" width="24.09" height="23.6"></rect><rect fill="currentColor" x="150.3" y="30" width="24.09" height="166.27"></rect></svg>

	

	<span>pandas</span>
	

	</div></a><div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-lg rounded-br-none " type="button">
		<div class="tag tag-white   relative rounded-br-none pr-2.5"><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="none" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="#F5AB6A"></path><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="url(#paint0_radial_18_31665)"></path><g filter="url(#filter0_f_18_31665)"><path d="M22.2849 12.1817L23.4375 13.2656L24.4375 4.70312C23.5121 4.1242 23.0198 3.96369 22.6563 3.89062L22.2849 12.1817Z" fill="url(#paint1_linear_18_31665)"></path></g><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint2_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint3_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint4_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint5_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint6_linear_18_31665)"></path><g filter="url(#filter1_f_18_31665)"><path d="M13.1016 2.72656C11.862 3.06924 11.5298 3.40016 11.5298 3.40016C10.9102 3.52335 10.936 4.11525 11.2408 4.6799C13.1487 6.95202 15.1361 13.2496 18.007 14.0958C18.2707 14.1633 18.6953 14.2107 19.1797 14.2344L13.1016 2.72656Z" fill="url(#paint7_linear_18_31665)"></path></g><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint8_radial_18_31665)"></path><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint9_radial_18_31665)"></path><g filter="url(#filter2_f_18_31665)"><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint10_linear_18_31665)"></path><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint11_radial_18_31665)"></path></g><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="#EC9F6A"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint12_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint13_linear_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint14_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint15_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint16_radial_18_31665)"></path><g filter="url(#filter3_f_18_31665)"><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint17_linear_18_31665)"></path><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint18_linear_18_31665)"></path></g><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="#D79453"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint19_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint20_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint21_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint22_linear_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint23_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint24_radial_18_31665)"></path><defs><filter id="filter0_f_18_31665" x="22.0349" y="3.64062" width="2.65265" height="9.875" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter1_f_18_31665" x="10.7815" y="2.47656" width="8.64819" height="12.0078" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter2_f_18_31665" x="3.89062" y="22.5416" width="9.57812" height="2.2804" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter3_f_18_31665" x="2.45312" y="11.2538" width="12.1255" height="8.50903" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><radialGradient id="paint0_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.8125 12.9375) rotate(42.7741) scale(12.5164 7.08839)"><stop offset="0.0937591" stop-color="#C05159"></stop><stop offset="0.553697" stop-color="#F6AC6A"></stop><stop offset="0.832916" stop-color="#FFD186"></stop><stop offset="0.916927" stop-color="#FFDC87"></stop></radialGradient><linearGradient id="paint1_linear_18_31665" x1="24.7344" y1="4.67187" x2="20.8594" y2="12.8906" gradientUnits="userSpaceOnUse"><stop stop-color="#EBD67C"></stop><stop offset="0.0655686" stop-color="#FFFFA6"></stop><stop offset="0.530552" stop-color="#F8C281"></stop><stop offset="0.937338" stop-color="#E99E6B"></stop></linearGradient><radialGradient id="paint2_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.9009 13.1847) rotate(-127.648) scale(14.3438 11.7966)"><stop stop-color="#FFBE66"></stop><stop offset="1" stop-color="#E2AE5B"></stop></radialGradient><radialGradient id="paint3_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(18 11.4375) rotate(53.9726) scale(11.9013 4.84018)"><stop stop-color="#D67C63"></stop><stop offset="1" stop-color="#D97D67" stop-opacity="0"></stop></radialGradient><radialGradient id="paint4_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(23 4.1875) rotate(45.7639) scale(3.31486 5.75622)"><stop stop-color="#FFE4A6"></stop><stop offset="0.711285" stop-color="#F8B76F"></stop><stop offset="1" stop-color="#F9B870" stop-opacity="0"></stop></radialGradient><radialGradient id="paint5_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.875 12.4375) rotate(88.9391) scale(3.37558 1.29066)"><stop stop-color="#FFBC67"></stop><stop offset="1" stop-color="#FFBC67" stop-opacity="0"></stop></radialGradient><linearGradient id="paint6_linear_18_31665" x1="20.375" y1="15.6875" x2="20.125" y2="12.7813" gradientUnits="userSpaceOnUse"><stop offset="0.461609" stop-color="#B45077"></stop><stop offset="0.855389" stop-color="#B75077" stop-opacity="0"></stop></linearGradient><linearGradient id="paint7_linear_18_31665" x1="12.9375" y1="2.57056" x2="18.5625" y2="14.3891" gradientUnits="userSpaceOnUse"><stop stop-color="#DDC173"></stop><stop offset="0.485173" stop-color="#D59F65"></stop><stop offset="1" stop-color="#E49966"></stop></linearGradient><radialGradient id="paint8_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.5625 23.5) rotate(109.113) scale(6.68078 10.2578)"><stop offset="0.165756" stop-color="#FFBF7E"></stop><stop offset="0.827674" stop-color="#DF8C6D"></stop><stop offset="1" stop-color="#B05A66"></stop></radialGradient><radialGradient id="paint9_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.1875 26) rotate(41.0652) scale(8.37243 2.03649)"><stop stop-color="#FFD483"></stop><stop offset="1" stop-color="#FFD688" stop-opacity="0"></stop></radialGradient><linearGradient id="paint10_linear_18_31665" x1="3.96063" y1="23.794" x2="13.3748" y2="23.5143" gradientUnits="userSpaceOnUse"><stop stop-color="#A8716F"></stop><stop offset="0.103615" stop-color="#B37173"></stop><stop offset="0.225484" stop-color="#DB9F84"></stop><stop offset="0.799889" stop-color="#F1BB8A"></stop><stop offset="1" stop-color="#FFD780"></stop></linearGradient><radialGradient id="paint11_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4219 23.1719) rotate(-178.616) scale(3.23532 0.569081)"><stop offset="0.621498" stop-color="#AF5A3E"></stop><stop offset="1" stop-color="#B35445" stop-opacity="0"></stop></radialGradient><radialGradient id="paint12_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.625 19.125) rotate(-171.737) scale(15.2205 15.0767)"><stop offset="0.138435" stop-color="#FFB974"></stop><stop offset="0.403618" stop-color="#F2A56D"></stop><stop offset="0.925938" stop-color="#A16948"></stop></radialGradient><linearGradient id="paint13_linear_18_31665" x1="8.22184" y1="13.125" x2="6.81191" y2="15.4996" gradientUnits="userSpaceOnUse"><stop offset="0.610751" stop-color="#984847"></stop><stop offset="0.850075" stop-color="#9A4947" stop-opacity="0"></stop></linearGradient><radialGradient id="paint14_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.25 23.7461) scale(11.25 5.68361)"><stop stop-color="#C66364"></stop><stop offset="1" stop-color="#D4766B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint15_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.875 23.5313) scale(10.0937 1.29657)"><stop stop-color="#B64B4B"></stop><stop offset="1" stop-color="#C56158" stop-opacity="0"></stop></radialGradient><radialGradient id="paint16_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4375 19.875) rotate(-46.8882) scale(4.02385 7.51767)"><stop stop-color="#FFC083"></stop><stop offset="0.620218" stop-color="#FFBD7D" stop-opacity="0"></stop></radialGradient><linearGradient id="paint17_linear_18_31665" x1="2.8125" y1="13.0312" x2="14.5582" y2="18.9404" gradientUnits="userSpaceOnUse"><stop stop-color="#B89367"></stop><stop offset="1" stop-color="#C5835E"></stop></linearGradient><linearGradient id="paint18_linear_18_31665" x1="8.21875" y1="14.6406" x2="7.59349" y2="15.6717" gradientUnits="userSpaceOnUse"><stop offset="0.351552" stop-color="#A74746"></stop><stop offset="0.845198" stop-color="#A04346" stop-opacity="0"></stop></linearGradient><radialGradient id="paint19_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.5625 14.5625) rotate(140.244) scale(18.3733 13.7403)"><stop stop-color="#FDAE69"></stop><stop offset="0.729021" stop-color="#CE8C4F"></stop><stop offset="0.921546" stop-color="#AD7B45"></stop><stop offset="1" stop-color="#8B6B4A"></stop></radialGradient><radialGradient id="paint20_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.0625 7) rotate(65.3152) scale(11.0745 3.16547)"><stop offset="0.233237" stop-color="#FFD47C"></stop><stop offset="0.853648" stop-color="#FFD98B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint21_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.3125 8.875) rotate(100.886) scale(6.6191 5.57808)"><stop offset="0.128419" stop-color="#FFD88C"></stop><stop offset="0.924134" stop-color="#FFBE7B" stop-opacity="0"></stop></radialGradient><linearGradient id="paint22_linear_18_31665" x1="7.25" y1="15.1875" x2="10.7588" y2="10.3142" gradientUnits="userSpaceOnUse"><stop offset="0.142353" stop-color="#C15F4D"></stop><stop offset="1" stop-color="#D58366" stop-opacity="0"></stop></linearGradient><radialGradient id="paint23_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(8.15625 15.7813) rotate(28.5422) scale(12.5574 1.96589)"><stop offset="0.149989" stop-color="#E4745D"></stop><stop offset="0.453292" stop-color="#C8604C"></stop><stop offset="0.632597" stop-color="#C0605F"></stop><stop offset="1" stop-color="#C0605F" stop-opacity="0"></stop></radialGradient><radialGradient id="paint24_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(1.40625 2.69067) rotate(46.0943) scale(22.3963)"><stop offset="0.935802" stop-color="#C17C61" stop-opacity="0"></stop><stop offset="0.982109" stop-color="#C17C61"></stop></radialGradient></defs></svg>

	

	<span>Croissant</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	<button class="tag tag-ghost px-1! -ml-0.5 mb-1 md:mb-1.5" type="button">+ 1</button></div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">License:
	</span>
	<div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-full rounded-br-none " type="button">
		<div class="tag tag-white rounded-full  relative rounded-br-none pr-2.5">
		<svg class="text-xs text-gray-900" width="1em" height="1em" viewBox="0 0 10 10" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.46009 5.0945V6.88125C1.46009 7.25201 1.75937 7.55129 2.13012 7.55129C2.50087 7.55129 2.80016 7.25201 2.80016 6.88125V5.0945C2.80016 4.72375 2.50087 4.42446 2.13012 4.42446C1.75937 4.42446 1.46009 4.72375 1.46009 5.0945ZM4.14022 5.0945V6.88125C4.14022 7.25201 4.4395 7.55129 4.81026 7.55129C5.18101 7.55129 5.48029 7.25201 5.48029 6.88125V5.0945C5.48029 4.72375 5.18101 4.42446 4.81026 4.42446C4.4395 4.42446 4.14022 4.72375 4.14022 5.0945ZM1.23674 9.78473H8.38377C8.75452 9.78473 9.0538 9.48545 9.0538 9.1147C9.0538 8.74395 8.75452 8.44466 8.38377 8.44466H1.23674C0.865993 8.44466 0.566711 8.74395 0.566711 9.1147C0.566711 9.48545 0.865993 9.78473 1.23674 9.78473ZM6.82036 5.0945V6.88125C6.82036 7.25201 7.11964 7.55129 7.49039 7.55129C7.86114 7.55129 8.16042 7.25201 8.16042 6.88125V5.0945C8.16042 4.72375 7.86114 4.42446 7.49039 4.42446C7.11964 4.42446 6.82036 4.72375 6.82036 5.0945ZM4.39484 0.623142L0.865993 2.48137C0.682851 2.57517 0.566711 2.76725 0.566711 2.97273C0.566711 3.28094 0.816857 3.53109 1.12507 3.53109H8.49991C8.80365 3.53109 9.0538 3.28094 9.0538 2.97273C9.0538 2.76725 8.93766 2.57517 8.75452 2.48137L5.22568 0.623142C4.9666 0.484669 4.65391 0.484669 4.39484 0.623142V0.623142Z" fill="currentColor"></path></svg>

	

	<span>mit</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	</div></div>

		<div class="flex flex-col-reverse lg:flex-row lg:items-center lg:justify-between"><div class="-mb-px flex h-12 items-center overflow-x-auto overflow-y-hidden ">
	<a class="tab-alternate active" href="/datasets/GPUMODE/Inductor_Created_Data_Permissive"><svg class="mr-1.5 text-gray-400 flex-none" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
	Dataset card
	

	
		</a><a class="tab-alternate" href="/datasets/GPUMODE/Inductor_Created_Data_Permissive/viewer/"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	Data Studio
	

	
		</a><a class="tab-alternate" href="/datasets/GPUMODE/Inductor_Created_Data_Permissive/tree/main"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M21 19h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-8h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0 4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M9 19a1 1 0 0 1-1-1V6a1 1 0 0 1 2 0v12a1 1 0 0 1-1 1zm-6-4.333a1 1 0 0 1-.64-1.769L3.438 12l-1.078-.898a1 1 0 0 1 1.28-1.538l2 1.667a1 1 0 0 1 0 1.538l-2 1.667a.999.999 0 0 1-.64.231z" fill="currentColor"></path></svg>
	<span class="xl:hidden">Files</span>
		<span class="hidden xl:inline">Files and versions</span>
	

	
		</a><a class="tab-alternate" href="/datasets/GPUMODE/Inductor_Created_Data_Permissive/discussions"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M20.6081 3C21.7684 3 22.8053 3.49196 23.5284 4.38415C23.9756 4.93678 24.4428 5.82749 24.4808 7.16133C24.9674 7.01707 25.4353 6.93643 25.8725 6.93643C26.9833 6.93643 27.9865 7.37587 28.696 8.17411C29.6075 9.19872 30.0124 10.4579 29.8361 11.7177C29.7523 12.3177 29.5581 12.8555 29.2678 13.3534C29.8798 13.8646 30.3306 14.5763 30.5485 15.4322C30.719 16.1032 30.8939 17.5006 29.9808 18.9403C30.0389 19.0342 30.0934 19.1319 30.1442 19.2318C30.6932 20.3074 30.7283 21.5229 30.2439 22.6548C29.5093 24.3704 27.6841 25.7219 24.1397 27.1727C21.9347 28.0753 19.9174 28.6523 19.8994 28.6575C16.9842 29.4379 14.3477 29.8345 12.0653 29.8345C7.87017 29.8345 4.8668 28.508 3.13831 25.8921C0.356375 21.6797 0.754104 17.8269 4.35369 14.1131C6.34591 12.058 7.67023 9.02782 7.94613 8.36275C8.50224 6.39343 9.97271 4.20438 12.4172 4.20438H12.4179C12.6236 4.20438 12.8314 4.2214 13.0364 4.25468C14.107 4.42854 15.0428 5.06476 15.7115 6.02205C16.4331 5.09583 17.134 4.359 17.7682 3.94323C18.7242 3.31737 19.6794 3 20.6081 3ZM20.6081 5.95917C20.2427 5.95917 19.7963 6.1197 19.3039 6.44225C17.7754 7.44319 14.8258 12.6772 13.7458 14.7131C13.3839 15.3952 12.7655 15.6837 12.2086 15.6837C11.1036 15.6837 10.2408 14.5497 12.1076 13.1085C14.9146 10.9402 13.9299 7.39584 12.5898 7.1776C12.5311 7.16799 12.4731 7.16355 12.4172 7.16355C11.1989 7.16355 10.6615 9.33114 10.6615 9.33114C10.6615 9.33114 9.0863 13.4148 6.38031 16.206C3.67434 18.998 3.5346 21.2388 5.50675 24.2246C6.85185 26.2606 9.42666 26.8753 12.0653 26.8753C14.8021 26.8753 17.6077 26.2139 19.1799 25.793C19.2574 25.7723 28.8193 22.984 27.6081 20.6107C27.4046 20.212 27.0693 20.0522 26.6471 20.0522C24.9416 20.0522 21.8393 22.6726 20.5057 22.6726C20.2076 22.6726 19.9976 22.5416 19.9116 22.222C19.3433 20.1173 28.552 19.2325 27.7758 16.1839C27.639 15.6445 27.2677 15.4256 26.746 15.4263C24.4923 15.4263 19.4358 19.5181 18.3759 19.5181C18.2949 19.5181 18.2368 19.4937 18.2053 19.4419C17.6743 18.557 17.9653 17.9394 21.7082 15.6009C25.4511 13.2617 28.0783 11.8545 26.5841 10.1752C26.4121 9.98141 26.1684 9.8956 25.8725 9.8956C23.6001 9.89634 18.2311 14.9403 18.2311 14.9403C18.2311 14.9403 16.7821 16.496 15.9057 16.496C15.7043 16.496 15.533 16.4139 15.4169 16.2112C14.7956 15.1296 21.1879 10.1286 21.5484 8.06535C21.7928 6.66715 21.3771 5.95917 20.6081 5.95917Z" fill="#FF9D00"></path><path d="M5.50686 24.2246C3.53472 21.2387 3.67446 18.9979 6.38043 16.206C9.08641 13.4147 10.6615 9.33111 10.6615 9.33111C10.6615 9.33111 11.2499 6.95933 12.59 7.17757C13.93 7.39581 14.9139 10.9401 12.1069 13.1084C9.29997 15.276 12.6659 16.7489 13.7459 14.713C14.8258 12.6772 17.7747 7.44316 19.304 6.44221C20.8326 5.44128 21.9089 6.00204 21.5484 8.06532C21.188 10.1286 14.795 15.1295 15.4171 16.2118C16.0391 17.2934 18.2312 14.9402 18.2312 14.9402C18.2312 14.9402 25.0907 8.49588 26.5842 10.1752C28.0776 11.8545 25.4512 13.2616 21.7082 15.6008C17.9646 17.9393 17.6744 18.557 18.2054 19.4418C18.7372 20.3266 26.9998 13.1351 27.7759 16.1838C28.5513 19.2324 19.3434 20.1173 19.9117 22.2219C20.48 24.3274 26.3979 18.2382 27.6082 20.6107C28.8193 22.9839 19.2574 25.7722 19.18 25.7929C16.0914 26.62 8.24723 28.3726 5.50686 24.2246Z" fill="#FFD21E"></path></svg>
	Community
	

	
		</a></div>
	
			</div></div></header>


</div>
	
<div class="container relative flex flex-col md:grid md:space-y-0 w-full md:grid-cols-12 md:flex-1 md:grid-rows-full space-y-4 md:gap-6 ">
		<section class="pt-6 border-gray-100 md:col-span-8 pb-24 relative break-words copiable-code-container">
				<div class="SVELTE_HYDRATER contents" data-target="UnsafeBanner" data-props="{&quot;classNames&quot;:&quot;mb-4&quot;,&quot;repoId&quot;:&quot;GPUMODE/Inductor_Created_Data_Permissive&quot;,&quot;repoType&quot;:&quot;dataset&quot;,&quot;minLevel&quot;:&quot;unsafe&quot;}"></div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetViewer" data-props="{&quot;data&quot;:{&quot;kind&quot;:&quot;DatasetAndSampleData&quot;,&quot;datasetInfo&quot;:[{&quot;isValid&quot;:true,&quot;href&quot;:&quot;/datasets/GPUMODE/Inductor_Created_Data_Permissive/tree/refs%2Fconvert%2Fparquet/&quot;,&quot;label&quot;:&quot;Size of the auto-converted Parquet files:&quot;,&quot;value&quot;:&quot;87.7 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Number of rows:&quot;,&quot;value&quot;:&quot;18,162&quot;}],&quot;partial&quot;:false,&quot;configsData&quot;:{&quot;configInfos&quot;:[{&quot;name&quot;:&quot;default&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:18162}],&quot;selectedConfig&quot;:&quot;default&quot;,&quot;hasSelectedConfigParquet&quot;:true},&quot;splitsData&quot;:{&quot;splitInfos&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;numRows&quot;:18162}],&quot;selectedSplit&quot;:&quot;train&quot;},&quot;sampleData&quot;:{&quot;dataset&quot;:&quot;GPUMODE/Inductor_Created_Data_Permissive&quot;,&quot;config&quot;:&quot;default&quot;,&quot;split&quot;:&quot;train&quot;,&quot;capabilities&quot;:{&quot;rows&quot;:true,&quot;search&quot;:true,&quot;filter&quot;:true,&quot;statistics&quot;:true},&quot;navigation&quot;:{&quot;p&quot;:0},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkxODYyNiwic3ViIjoiL2RhdGFzZXRzL0dQVU1PREUvSW5kdWN0b3JfQ3JlYXRlZF9EYXRhX1Blcm1pc3NpdmUiLCJleHAiOjE3NDI5MjIyMjYsImlzcyI6Imh0dHBzOi8vaHVnZ2luZ2ZhY2UuY28ifQ.JS5bD0txjqMA3sChOV8xqlF9n1tkhWb1nTHUxqB5yUPZ6BeWsP3hEoiTB977lM_FYLyWgdBh4Aii4O8CdacrCw&quot;,&quot;sampleData&quot;:{&quot;columns&quot;:[{&quot;name&quot;:&quot;entry_point&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;entry_point&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:1,&quot;max&quot;:65,&quot;mean&quot;:11.96377,&quot;median&quot;:11,&quot;std&quot;:6.2222,&quot;histogram&quot;:{&quot;hist&quot;:[4349,8277,4075,1254,173,19,2,0,6,7],&quot;bin_edges&quot;:[1,8,15,22,29,36,43,50,57,64,65]}}}},{&quot;name&quot;:&quot;original_triton_code&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;original_triton_code&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:4499,&quot;max&quot;:619461,&quot;mean&quot;:13893.78549,&quot;median&quot;:9367.5,&quot;std&quot;:18089.28204,&quot;histogram&quot;:{&quot;hist&quot;:[17889,203,36,17,10,6,0,0,0,1],&quot;bin_edges&quot;:[4499,65996,127493,188990,250487,311984,373481,434978,496475,557972,619461]}}}},{&quot;name&quot;:&quot;python_code&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;python_code&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:208,&quot;max&quot;:60883,&quot;mean&quot;:1578.29732,&quot;median&quot;:1004,&quot;std&quot;:2000.77891,&quot;histogram&quot;:{&quot;hist&quot;:[17700,381,44,20,12,3,0,0,0,2],&quot;bin_edges&quot;:[208,6276,12344,18412,24480,30548,36616,42684,48752,54820,60883]}}}},{&quot;name&quot;:&quot;triton_code&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;triton_code&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:1145,&quot;max&quot;:275292,&quot;mean&quot;:6456.71204,&quot;median&quot;:4284,&quot;std&quot;:8970.36563,&quot;histogram&quot;:{&quot;hist&quot;:[17831,244,50,18,3,9,3,3,0,1],&quot;bin_edges&quot;:[1145,28560,55975,83390,110805,138220,165635,193050,220465,247880,275292]}}}},{&quot;name&quot;:&quot;repo_name&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;repo_name&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:7,&quot;max&quot;:115,&quot;mean&quot;:24.24568,&quot;median&quot;:22,&quot;std&quot;:10.1335,&quot;histogram&quot;:{&quot;hist&quot;:[4677,8788,3299,1037,225,62,44,20,8,2],&quot;bin_edges&quot;:[7,18,29,40,51,62,73,84,95,106,115]}}}},{&quot;name&quot;:&quot;module_name&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;module_name&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:1,&quot;max&quot;:65,&quot;mean&quot;:11.96377,&quot;median&quot;:11,&quot;std&quot;:6.2222,&quot;histogram&quot;:{&quot;hist&quot;:[4349,8277,4075,1254,173,19,2,0,6,7],&quot;bin_edges&quot;:[1,8,15,22,29,36,43,50,57,64,65]}}}},{&quot;name&quot;:&quot;synthetic&quot;,&quot;align&quot;:&quot;right&quot;,&quot;type&quot;:&quot;bool&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;synthetic&quot;,&quot;column_type&quot;:&quot;bool&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;frequencies&quot;:{&quot;False&quot;:18162}}}},{&quot;name&quot;:&quot;uuid&quot;,&quot;align&quot;:&quot;right&quot;,&quot;type&quot;:&quot;int64&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;uuid&quot;,&quot;column_type&quot;:&quot;int&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:0,&quot;max&quot;:18453,&quot;mean&quot;:9246.95072,&quot;median&quot;:9261.5,&quot;std&quot;:5322.36749,&quot;histogram&quot;:{&quot;hist&quot;:[1794,1817,1809,1824,1805,1826,1818,1834,1823,1812],&quot;bin_edges&quot;:[0,1846,3692,5538,7384,9230,11076,12922,14768,16614,18453]}}}},{&quot;name&quot;:&quot;licenses&quot;,&quot;align&quot;:&quot;left&quot;,&quot;type&quot;:&quot;sequence&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;licenses&quot;,&quot;column_type&quot;:&quot;list&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:1,&quot;max&quot;:6,&quot;mean&quot;:1.01393,&quot;median&quot;:1,&quot;std&quot;:0.14417,&quot;histogram&quot;:{&quot;hist&quot;:[17959,161,38,2,0,2],&quot;bin_edges&quot;:[1,2,3,4,5,6,6]}}}},{&quot;name&quot;:&quot;stars&quot;,&quot;align&quot;:&quot;right&quot;,&quot;type&quot;:&quot;int64&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;stars&quot;,&quot;column_type&quot;:&quot;int&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:0,&quot;max&quot;:19783,&quot;mean&quot;:110.66689,&quot;median&quot;:1,&quot;std&quot;:609.31285,&quot;histogram&quot;:{&quot;hist&quot;:[17887,194,46,22,7,1,0,0,1,4],&quot;bin_edges&quot;:[0,1979,3958,5937,7916,9895,11874,13853,15832,17811,19783]}}}},{&quot;name&quot;:&quot;sha&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;sha&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:40,&quot;max&quot;:40,&quot;mean&quot;:40,&quot;median&quot;:40,&quot;std&quot;:0,&quot;histogram&quot;:{&quot;hist&quot;:[18162],&quot;bin_edges&quot;:[40,40]}}}},{&quot;name&quot;:&quot;repo_link&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;repo_link&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:72,&quot;max&quot;:180,&quot;mean&quot;:89.24568,&quot;median&quot;:87,&quot;std&quot;:10.1335,&quot;histogram&quot;:{&quot;hist&quot;:[4677,8788,3299,1037,225,62,44,20,8,2],&quot;bin_edges&quot;:[72,83,94,105,116,127,138,149,160,171,180]}}}}],&quot;rows&quot;:[{&quot;rowIdx&quot;:0,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;SumAggregator&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_inference']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/7o/c7otc5ij6whexgxcr56vlxp2l7hzg3oc4onljp557uc6wncu5gvg.py\n# Topologically Sorted Source Nodes: [sum_1], Original ATen: [aten.sum]\n# Source node to ATen node mapping:\n#   sum_1 => sum_1\n# Graph fragment:\n#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%arg0_1, [1]), kwargs = {})\ntriton_poi_fused_sum_0 = async_compile.triton('triton_poi_fused_sum_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[64], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_sum_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_sum_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 64\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex % 16\n    x1 = (xindex // 16)\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (x0 + (64*x1)), xmask)\n    tmp1 = tl.load(in_ptr0 + (16 + x0 + (64*x1)), xmask)\n    tmp3 = tl.load(in_ptr0 + (32 + x0 + (64*x1)), xmask)\n    tmp5 = tl.load(in_ptr0 + (48 + x0 + (64*x1)), xmask)\n    tmp2 = tmp0 + tmp1\n    tmp4 = tmp2 + tmp3\n    tmp6 = tmp4 + tmp5\n    tl.store(out_ptr0 + (x2), tmp6, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4), (16, 4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [sum_1], Original ATen: [aten.sum]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_sum_0.run(arg0_1, buf0, 64, grid=grid(64), stream=stream0)\n        del arg0_1\n    return (buf0, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([arg0_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport torch.nn as nn\n\n\nclass SumAggregator(nn.Module):\n\n    def __init__(self):\n        super(SumAggregator, self).__init__()\n\n    def forward(self, neighbor):\n        return torch.sum(neighbor, dim=1)\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 4, 4])]\n\n\ndef get_init_inputs():\n    return [[], {}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_sum_0(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 64\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex % 16\n    x1 = xindex // 16\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (x0 + 64 * x1), xmask)\n    tmp1 = tl.load(in_ptr0 + (16 + x0 + 64 * x1), xmask)\n    tmp3 = tl.load(in_ptr0 + (32 + x0 + 64 * x1), xmask)\n    tmp5 = tl.load(in_ptr0 + (48 + x0 + 64 * x1), xmask)\n    tmp2 = tmp0 + tmp1\n    tmp4 = tmp2 + tmp3\n    tmp6 = tmp4 + tmp5\n    tl.store(out_ptr0 + x2, tmp6, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4), (16, 4, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_sum_0[grid(64)](arg0_1, buf0, 64, XBLOCK=64,\n            num_warps=1, num_stages=1)\n        del arg0_1\n    return buf0,\n\n\nclass SumAggregatorNew(nn.Module):\n\n    def __init__(self):\n        super(SumAggregatorNew, self).__init__()\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;AlexMinhao/NAS_GNN&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;SumAggregator&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;Apache-2.0&quot;],&quot;string&quot;:&quot;[\n  \&quot;Apache-2.0\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;89183988a96e1d6baed910ab3843c13282f8b077&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/AlexMinhao/NAS_GNN/tree/89183988a96e1d6baed910ab3843c13282f8b077&quot;}}},{&quot;rowIdx&quot;:1,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;LinearEmbedding&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/xz/cxz7i3qbiizfbbzvas22bbwy5nxzvmtfdg5vhhiye56dk4hdonst.py\n# Topologically Sorted Source Nodes: [mul], Original ATen: [aten.mul]\n# Source node to ATen node mapping:\n#   mul => mul\n# Graph fragment:\n#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_1, 2.0), kwargs = {})\ntriton_poi_fused_mul_0 = async_compile.triton('triton_poi_fused_mul_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[256], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_0', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_mul_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x0 = xindex % 4\n    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)\n    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = 2.0\n    tmp4 = tmp2 * tmp3\n    tl.store(in_out_ptr0 + (x2), tmp4, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    primals_1, primals_2, primals_3 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4), (4, 1))\n    assert_size_stride(primals_2, (4, ), (1, ))\n    assert_size_stride(primals_3, (4, 4, 4, 4), (64, 16, 4, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((64, 4), (4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [], Original ATen: []\n        extern_kernels.mm(reinterpret_tensor(primals_3, (64, 4), (4, 1), 0), reinterpret_tensor(primals_1, (4, 4), (1, 4), 0), out=buf0)\n        del primals_1\n        buf1 = reinterpret_tensor(buf0, (4, 4, 4, 4), (64, 16, 4, 1), 0); del buf0  # reuse\n        # Topologically Sorted Source Nodes: [mul], Original ATen: [aten.mul]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_mul_0.run(buf1, primals_2, 256, grid=grid(256), stream=stream0)\n        del primals_2\n    return (buf1, reinterpret_tensor(primals_3, (64, 4), (4, 1), 0), )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import math\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\n\n\nclass LinearEmbedding(nn.Module):\n\n    def __init__(self, inp_size, d_model):\n        super(LinearEmbedding, self).__init__()\n        self.lut = nn.Linear(inp_size, d_model)\n        self.d_model = d_model\n\n    def forward(self, x):\n        return self.lut(x) * math.sqrt(self.d_model)\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 4, 4])]\n\n\ndef get_init_inputs():\n    return [[], {'inp_size': 4, 'd_model': 4}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nimport torch.utils.data\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_mul_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x0 = xindex % 4\n    tmp0 = tl.load(in_out_ptr0 + x2, xmask)\n    tmp1 = tl.load(in_ptr0 + x0, xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = 2.0\n    tmp4 = tmp2 * tmp3\n    tl.store(in_out_ptr0 + x2, tmp4, xmask)\n\n\ndef call(args):\n    primals_1, primals_2, primals_3 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4), (4, 1))\n    assert_size_stride(primals_2, (4,), (1,))\n    assert_size_stride(primals_3, (4, 4, 4, 4), (64, 16, 4, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((64, 4), (4, 1), torch.float32)\n        extern_kernels.mm(reinterpret_tensor(primals_3, (64, 4), (4, 1), 0),\n            reinterpret_tensor(primals_1, (4, 4), (1, 4), 0), out=buf0)\n        del primals_1\n        buf1 = reinterpret_tensor(buf0, (4, 4, 4, 4), (64, 16, 4, 1), 0)\n        del buf0\n        get_raw_stream(0)\n        triton_poi_fused_mul_0[grid(256)](buf1, primals_2, 256, XBLOCK=256,\n            num_warps=4, num_stages=1)\n        del primals_2\n    return buf1, reinterpret_tensor(primals_3, (64, 4), (4, 1), 0)\n\n\nclass LinearEmbeddingNew(nn.Module):\n\n    def __init__(self, inp_size, d_model):\n        super(LinearEmbeddingNew, self).__init__()\n        self.lut = nn.Linear(inp_size, d_model)\n        self.d_model = d_model\n\n    def forward(self, input_0):\n        primals_1 = self.lut.weight\n        primals_2 = self.lut.bias\n        primals_3 = input_0\n        output = call([primals_1, primals_2, primals_3])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Akhil-Raj/Trajectory-Transformer&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;LinearEmbedding&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1,&quot;string&quot;:&quot;1&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;MIT&quot;],&quot;string&quot;:&quot;[\n  \&quot;MIT\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dd09fda99443f6afb59d962026573162219ea6a9&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/Akhil-Raj/Trajectory-Transformer/tree/dd09fda99443f6afb59d962026573162219ea6a9&quot;}}},{&quot;rowIdx&quot;:2,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;CustomizeLayer&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/ne/cnem3ewvyvy2ry3vceibv7j67edhj5jen3qlzvzibg5tpkhxkwsq.py\n# Topologically Sorted Source Nodes: [pow_1, sum_1, norm, x, mul, add], Original ATen: [aten.pow, aten.sum, aten.sqrt, aten.div, aten.mul, aten.add]\n# Source node to ATen node mapping:\n#   add => add\n#   mul => mul\n#   norm => sqrt\n#   pow_1 => pow_1\n#   sum_1 => sum_1\n#   x => div\n# Graph fragment:\n#   %pow_1 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%primals_1, 2), kwargs = {})\n#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%pow_1, [1], True), kwargs = {})\n#   %sqrt : [num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%sum_1,), kwargs = {})\n#   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%primals_1, %sqrt), kwargs = {})\n#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div, %primals_2), kwargs = {})\n#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul, %primals_3), kwargs = {})\ntriton_poi_fused_add_div_mul_pow_sqrt_sum_0 = async_compile.triton('triton_poi_fused_add_div_mul_pow_sqrt_sum_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[256], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_div_mul_pow_sqrt_sum_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_add_div_mul_pow_sqrt_sum_0(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x4 = xindex\n    x3 = (xindex // 64)\n    x5 = xindex % 16\n    x0 = xindex % 4\n    tmp0 = tl.load(in_ptr0 + (x4), xmask)\n    tmp1 = tl.load(in_ptr0 + (x5 + (64*x3)), xmask, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr0 + (16 + x5 + (64*x3)), xmask, eviction_policy='evict_last')\n    tmp6 = tl.load(in_ptr0 + (32 + x5 + (64*x3)), xmask, eviction_policy='evict_last')\n    tmp9 = tl.load(in_ptr0 + (48 + x5 + (64*x3)), xmask, eviction_policy='evict_last')\n    tmp14 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')\n    tmp16 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')\n    tmp2 = tmp1 * tmp1\n    tmp4 = tmp3 * tmp3\n    tmp5 = tmp2 + tmp4\n    tmp7 = tmp6 * tmp6\n    tmp8 = tmp5 + tmp7\n    tmp10 = tmp9 * tmp9\n    tmp11 = tmp8 + tmp10\n    tmp12 = libdevice.sqrt(tmp11)\n    tmp13 = tmp0 / tmp12\n    tmp15 = tmp13 * tmp14\n    tmp17 = tmp15 + tmp16\n    tl.store(out_ptr0 + (x4), tmp17, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    primals_1, primals_2, primals_3 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_2, (4, ), (1, ))\n    assert_size_stride(primals_3, (4, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [pow_1, sum_1, norm, x, mul, add], Original ATen: [aten.pow, aten.sum, aten.sqrt, aten.div, aten.mul, aten.add]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_add_div_mul_pow_sqrt_sum_0.run(primals_1, primals_2, primals_3, buf0, 256, grid=grid(256), stream=stream0)\n        del primals_2\n        del primals_3\n    return (buf0, primals_1, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport torch.nn as nn\n\n\nclass CustomizeLayer(nn.Module):\n\n    def __init__(self, in_dim):\n        super().__init__()\n        self.in_dim = in_dim\n        self.scale = nn.Parameter(torch.Tensor(self.in_dim))\n        self.bias = nn.Parameter(torch.Tensor(self.in_dim))\n\n    def forward(self, x):\n        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()\n        x = torch.div(x, norm)\n        return x * self.scale + self.bias\n\n    def __repr__(self):\n        return 'CustomizedLayer(in_dim=%d)' % self.in_dim\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 4, 4])]\n\n\ndef get_init_inputs():\n    return [[], {'in_dim': 4}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_add_div_mul_pow_sqrt_sum_0(in_ptr0, in_ptr1, in_ptr2,\n    out_ptr0, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x4 = xindex\n    x3 = xindex // 64\n    x5 = xindex % 16\n    x0 = xindex % 4\n    tmp0 = tl.load(in_ptr0 + x4, xmask)\n    tmp1 = tl.load(in_ptr0 + (x5 + 64 * x3), xmask, eviction_policy=\n        'evict_last')\n    tmp3 = tl.load(in_ptr0 + (16 + x5 + 64 * x3), xmask, eviction_policy=\n        'evict_last')\n    tmp6 = tl.load(in_ptr0 + (32 + x5 + 64 * x3), xmask, eviction_policy=\n        'evict_last')\n    tmp9 = tl.load(in_ptr0 + (48 + x5 + 64 * x3), xmask, eviction_policy=\n        'evict_last')\n    tmp14 = tl.load(in_ptr1 + x0, xmask, eviction_policy='evict_last')\n    tmp16 = tl.load(in_ptr2 + x0, xmask, eviction_policy='evict_last')\n    tmp2 = tmp1 * tmp1\n    tmp4 = tmp3 * tmp3\n    tmp5 = tmp2 + tmp4\n    tmp7 = tmp6 * tmp6\n    tmp8 = tmp5 + tmp7\n    tmp10 = tmp9 * tmp9\n    tmp11 = tmp8 + tmp10\n    tmp12 = libdevice.sqrt(tmp11)\n    tmp13 = tmp0 / tmp12\n    tmp15 = tmp13 * tmp14\n    tmp17 = tmp15 + tmp16\n    tl.store(out_ptr0 + x4, tmp17, xmask)\n\n\ndef call(args):\n    primals_1, primals_2, primals_3 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_2, (4,), (1,))\n    assert_size_stride(primals_3, (4,), (1,))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_add_div_mul_pow_sqrt_sum_0[grid(256)](primals_1,\n            primals_2, primals_3, buf0, 256, XBLOCK=128, num_warps=4,\n            num_stages=1)\n        del primals_2\n        del primals_3\n    return buf0, primals_1\n\n\nclass CustomizeLayerNew(nn.Module):\n\n    def __init__(self, in_dim):\n        super().__init__()\n        self.in_dim = in_dim\n        self.scale = nn.Parameter(torch.Tensor(self.in_dim))\n        self.bias = nn.Parameter(torch.Tensor(self.in_dim))\n\n    def __repr__(self):\n        return 'CustomizedLayer(in_dim=%d)' % self.in_dim\n\n    def forward(self, input_0):\n        primals_2 = self.scale\n        primals_3 = self.bias\n        primals_1 = input_0\n        output = call([primals_1, primals_2, primals_3])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Abhishekvats1997/Torch-Pruning&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;CustomizeLayer&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2,&quot;string&quot;:&quot;2&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;MIT&quot;],&quot;string&quot;:&quot;[\n  \&quot;MIT\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;b322a42d1c9032cc9644332d33a9662ca6ed44ac&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/Abhishekvats1997/Torch-Pruning/tree/b322a42d1c9032cc9644332d33a9662ca6ed44ac&quot;}}},{&quot;rowIdx&quot;:3,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;LayerNorm&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/yq/cyqficfzvi7m26ugfmcjnrvh3fwc36rw64jccffjyl3m2zuwe2to.py\n# Topologically Sorted Source Nodes: [mean, std, sub, mul, add, truediv, add_1], Original ATen: [aten.mean, aten.std, aten.sub, aten.mul, aten.add, aten.div]\n# Source node to ATen node mapping:\n#   add => add\n#   add_1 => add_1\n#   mean => mean\n#   mul => mul\n#   std => sqrt, var\n#   sub => sub\n#   truediv => div\n# Graph fragment:\n#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%primals_1, [-1], True), kwargs = {})\n#   %var : [num_users=1] = call_function[target=torch.ops.aten.var.correction](args = (%primals_1, [-1]), kwargs = {correction: 1.0, keepdim: True})\n#   %sqrt : [num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%var,), kwargs = {})\n#   %sub : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%primals_1, %mean), kwargs = {})\n#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_2, %sub), kwargs = {})\n#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%sqrt, 1e-05), kwargs = {})\n#   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%mul, %add), kwargs = {})\n#   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div, %primals_3), kwargs = {})\ntriton_poi_fused_add_div_mean_mul_std_sub_0 = async_compile.triton('triton_poi_fused_add_div_mean_mul_std_sub_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[256], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_div_mean_mul_std_sub_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_add_div_mean_mul_std_sub_0(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex % 4\n    x2 = xindex\n    x1 = (xindex // 4)\n    tmp0 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr1 + (x2), xmask)\n    tmp2 = tl.load(in_ptr1 + (4*x1), xmask, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr1 + (1 + (4*x1)), xmask, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr1 + (2 + (4*x1)), xmask, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr1 + (3 + (4*x1)), xmask, eviction_policy='evict_last')\n    tmp30 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')\n    tmp4 = tmp2 + tmp3\n    tmp6 = tmp4 + tmp5\n    tmp8 = tmp6 + tmp7\n    tmp9 = 4.0\n    tmp10 = tmp8 / tmp9\n    tmp11 = tmp1 - tmp10\n    tmp12 = tmp0 * tmp11\n    tmp13 = tmp2 - tmp10\n    tmp14 = tmp13 * tmp13\n    tmp15 = tmp3 - tmp10\n    tmp16 = tmp15 * tmp15\n    tmp17 = tmp14 + tmp16\n    tmp18 = tmp5 - tmp10\n    tmp19 = tmp18 * tmp18\n    tmp20 = tmp17 + tmp19\n    tmp21 = tmp7 - tmp10\n    tmp22 = tmp21 * tmp21\n    tmp23 = tmp20 + tmp22\n    tmp24 = 3.0\n    tmp25 = tmp23 / tmp24\n    tmp26 = libdevice.sqrt(tmp25)\n    tmp27 = 1e-05\n    tmp28 = tmp26 + tmp27\n    tmp29 = tmp12 / tmp28\n    tmp31 = tmp29 + tmp30\n    tl.store(out_ptr0 + (x2), tmp31, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    primals_1, primals_2, primals_3 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_2, (4, ), (1, ))\n    assert_size_stride(primals_3, (4, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [mean, std, sub, mul, add, truediv, add_1], Original ATen: [aten.mean, aten.std, aten.sub, aten.mul, aten.add, aten.div]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_add_div_mean_mul_std_sub_0.run(primals_2, primals_1, primals_3, buf0, 256, grid=grid(256), stream=stream0)\n        del primals_2\n        del primals_3\n    return (buf0, primals_1, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport torch.nn as nn\n\n\nclass LayerNorm(nn.Module):\n\n    def __init__(self, weights, eps=1e-05):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(weights))\n        self.beta = nn.Parameter(torch.zeros(weights))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 4, 4])]\n\n\ndef get_init_inputs():\n    return [[], {'weights': 4}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_add_div_mean_mul_std_sub_0(in_ptr0, in_ptr1, in_ptr2,\n    out_ptr0, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex % 4\n    x2 = xindex\n    x1 = xindex // 4\n    tmp0 = tl.load(in_ptr0 + x0, xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr1 + x2, xmask)\n    tmp2 = tl.load(in_ptr1 + 4 * x1, xmask, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr1 + (1 + 4 * x1), xmask, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr1 + (2 + 4 * x1), xmask, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr1 + (3 + 4 * x1), xmask, eviction_policy='evict_last')\n    tmp30 = tl.load(in_ptr2 + x0, xmask, eviction_policy='evict_last')\n    tmp4 = tmp2 + tmp3\n    tmp6 = tmp4 + tmp5\n    tmp8 = tmp6 + tmp7\n    tmp9 = 4.0\n    tmp10 = tmp8 / tmp9\n    tmp11 = tmp1 - tmp10\n    tmp12 = tmp0 * tmp11\n    tmp13 = tmp2 - tmp10\n    tmp14 = tmp13 * tmp13\n    tmp15 = tmp3 - tmp10\n    tmp16 = tmp15 * tmp15\n    tmp17 = tmp14 + tmp16\n    tmp18 = tmp5 - tmp10\n    tmp19 = tmp18 * tmp18\n    tmp20 = tmp17 + tmp19\n    tmp21 = tmp7 - tmp10\n    tmp22 = tmp21 * tmp21\n    tmp23 = tmp20 + tmp22\n    tmp24 = 3.0\n    tmp25 = tmp23 / tmp24\n    tmp26 = libdevice.sqrt(tmp25)\n    tmp27 = 1e-05\n    tmp28 = tmp26 + tmp27\n    tmp29 = tmp12 / tmp28\n    tmp31 = tmp29 + tmp30\n    tl.store(out_ptr0 + x2, tmp31, xmask)\n\n\ndef call(args):\n    primals_1, primals_2, primals_3 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_2, (4,), (1,))\n    assert_size_stride(primals_3, (4,), (1,))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_add_div_mean_mul_std_sub_0[grid(256)](primals_2,\n            primals_1, primals_3, buf0, 256, XBLOCK=256, num_warps=4,\n            num_stages=1)\n        del primals_2\n        del primals_3\n    return buf0, primals_1\n\n\nclass LayerNormNew(nn.Module):\n\n    def __init__(self, weights, eps=1e-05):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(weights))\n        self.beta = nn.Parameter(torch.zeros(weights))\n        self.eps = eps\n\n    def forward(self, input_0):\n        primals_2 = self.gamma\n        primals_3 = self.beta\n        primals_1 = input_0\n        output = call([primals_1, primals_2, primals_3])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;AWilcke/Dissertation&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;LayerNorm&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:3,&quot;string&quot;:&quot;3&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;MIT&quot;],&quot;string&quot;:&quot;[\n  \&quot;MIT\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;b85ad38a7f336ee290d5883f5e942f54e140d0d0&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/AWilcke/Dissertation/tree/b85ad38a7f336ee290d5883f5e942f54e140d0d0&quot;}}},{&quot;rowIdx&quot;:4,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;LayerNorm&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/df/cdfcie57v6pcdd6oeaz4mvlgksxgyuxzmlv5bklwemyulqhtcxta.py\n# Topologically Sorted Source Nodes: [mean, std, sub, mul, add, truediv, add_1], Original ATen: [aten.mean, aten.std, aten.sub, aten.mul, aten.add, aten.div]\n# Source node to ATen node mapping:\n#   add => add\n#   add_1 => add_1\n#   mean => mean\n#   mul => mul\n#   std => sqrt, var\n#   sub => sub\n#   truediv => div\n# Graph fragment:\n#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%primals_1, [-1], True), kwargs = {})\n#   %var : [num_users=1] = call_function[target=torch.ops.aten.var.correction](args = (%primals_1, [-1]), kwargs = {correction: 1.0, keepdim: True})\n#   %sqrt : [num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%var,), kwargs = {})\n#   %sub : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%primals_1, %mean), kwargs = {})\n#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_2, %sub), kwargs = {})\n#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%sqrt, 1e-06), kwargs = {})\n#   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%mul, %add), kwargs = {})\n#   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div, %primals_3), kwargs = {})\ntriton_poi_fused_add_div_mean_mul_std_sub_0 = async_compile.triton('triton_poi_fused_add_div_mean_mul_std_sub_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[256], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_div_mean_mul_std_sub_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_add_div_mean_mul_std_sub_0(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex % 4\n    x2 = xindex\n    x1 = (xindex // 4)\n    tmp0 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr1 + (x2), xmask)\n    tmp2 = tl.load(in_ptr1 + (4*x1), xmask, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr1 + (1 + (4*x1)), xmask, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr1 + (2 + (4*x1)), xmask, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr1 + (3 + (4*x1)), xmask, eviction_policy='evict_last')\n    tmp30 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')\n    tmp4 = tmp2 + tmp3\n    tmp6 = tmp4 + tmp5\n    tmp8 = tmp6 + tmp7\n    tmp9 = 4.0\n    tmp10 = tmp8 / tmp9\n    tmp11 = tmp1 - tmp10\n    tmp12 = tmp0 * tmp11\n    tmp13 = tmp2 - tmp10\n    tmp14 = tmp13 * tmp13\n    tmp15 = tmp3 - tmp10\n    tmp16 = tmp15 * tmp15\n    tmp17 = tmp14 + tmp16\n    tmp18 = tmp5 - tmp10\n    tmp19 = tmp18 * tmp18\n    tmp20 = tmp17 + tmp19\n    tmp21 = tmp7 - tmp10\n    tmp22 = tmp21 * tmp21\n    tmp23 = tmp20 + tmp22\n    tmp24 = 3.0\n    tmp25 = tmp23 / tmp24\n    tmp26 = libdevice.sqrt(tmp25)\n    tmp27 = 1e-06\n    tmp28 = tmp26 + tmp27\n    tmp29 = tmp12 / tmp28\n    tmp31 = tmp29 + tmp30\n    tl.store(out_ptr0 + (x2), tmp31, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    primals_1, primals_2, primals_3 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_2, (4, ), (1, ))\n    assert_size_stride(primals_3, (4, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [mean, std, sub, mul, add, truediv, add_1], Original ATen: [aten.mean, aten.std, aten.sub, aten.mul, aten.add, aten.div]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_add_div_mean_mul_std_sub_0.run(primals_2, primals_1, primals_3, buf0, 256, grid=grid(256), stream=stream0)\n        del primals_2\n        del primals_3\n    return (buf0, primals_1, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport torch.utils.data\nimport torch.nn as nn\n\n\nclass LayerNorm(nn.Module):\n    \&quot;\&quot;\&quot;\n    Construct a layernorm module (See citation for details).\n    \&quot;\&quot;\&quot;\n\n    def __init__(self, features, eps=1e-06):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        std = x.std(dim=-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 4, 4])]\n\n\ndef get_init_inputs():\n    return [[], {'features': 4}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.utils.data\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_add_div_mean_mul_std_sub_0(in_ptr0, in_ptr1, in_ptr2,\n    out_ptr0, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex % 4\n    x2 = xindex\n    x1 = xindex // 4\n    tmp0 = tl.load(in_ptr0 + x0, xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr1 + x2, xmask)\n    tmp2 = tl.load(in_ptr1 + 4 * x1, xmask, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr1 + (1 + 4 * x1), xmask, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr1 + (2 + 4 * x1), xmask, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr1 + (3 + 4 * x1), xmask, eviction_policy='evict_last')\n    tmp30 = tl.load(in_ptr2 + x0, xmask, eviction_policy='evict_last')\n    tmp4 = tmp2 + tmp3\n    tmp6 = tmp4 + tmp5\n    tmp8 = tmp6 + tmp7\n    tmp9 = 4.0\n    tmp10 = tmp8 / tmp9\n    tmp11 = tmp1 - tmp10\n    tmp12 = tmp0 * tmp11\n    tmp13 = tmp2 - tmp10\n    tmp14 = tmp13 * tmp13\n    tmp15 = tmp3 - tmp10\n    tmp16 = tmp15 * tmp15\n    tmp17 = tmp14 + tmp16\n    tmp18 = tmp5 - tmp10\n    tmp19 = tmp18 * tmp18\n    tmp20 = tmp17 + tmp19\n    tmp21 = tmp7 - tmp10\n    tmp22 = tmp21 * tmp21\n    tmp23 = tmp20 + tmp22\n    tmp24 = 3.0\n    tmp25 = tmp23 / tmp24\n    tmp26 = libdevice.sqrt(tmp25)\n    tmp27 = 1e-06\n    tmp28 = tmp26 + tmp27\n    tmp29 = tmp12 / tmp28\n    tmp31 = tmp29 + tmp30\n    tl.store(out_ptr0 + x2, tmp31, xmask)\n\n\ndef call(args):\n    primals_1, primals_2, primals_3 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_2, (4,), (1,))\n    assert_size_stride(primals_3, (4,), (1,))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_add_div_mean_mul_std_sub_0[grid(256)](primals_2,\n            primals_1, primals_3, buf0, 256, XBLOCK=256, num_warps=4,\n            num_stages=1)\n        del primals_2\n        del primals_3\n    return buf0, primals_1\n\n\nclass LayerNormNew(nn.Module):\n    \&quot;\&quot;\&quot;\n    Construct a layernorm module (See citation for details).\n    \&quot;\&quot;\&quot;\n\n    def __init__(self, features, eps=1e-06):\n        super(LayerNormNew, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, input_0):\n        primals_2 = self.a_2\n        primals_3 = self.b_2\n        primals_1 = input_0\n        output = call([primals_1, primals_2, primals_3])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Akhil-Raj/Trajectory-Transformer&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;LayerNorm&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:4,&quot;string&quot;:&quot;4&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;MIT&quot;],&quot;string&quot;:&quot;[\n  \&quot;MIT\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;dd09fda99443f6afb59d962026573162219ea6a9&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/Akhil-Raj/Trajectory-Transformer/tree/dd09fda99443f6afb59d962026573162219ea6a9&quot;}}},{&quot;rowIdx&quot;:5,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Norm&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/dg/cdgw6x7nju4bzp2wyuwgeanbco7zcjis6yiusovvnpz6zw3yjd3l.py\n# Topologically Sorted Source Nodes: [u, sub], Original ATen: [aten.mean, aten.sub]\n# Source node to ATen node mapping:\n#   sub => sub\n#   u => mean\n# Graph fragment:\n#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%primals_1, [-1], True), kwargs = {})\n#   %sub : [num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%primals_1, %mean), kwargs = {})\ntriton_poi_fused_mean_sub_0 = async_compile.triton('triton_poi_fused_mean_sub_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[256], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mean_sub_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_mean_sub_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x1 = (xindex // 4)\n    tmp0 = tl.load(in_ptr0 + (x2), xmask)\n    tmp1 = tl.load(in_ptr0 + (4*x1), xmask, eviction_policy='evict_last')\n    tmp2 = tl.load(in_ptr0 + (1 + (4*x1)), xmask, eviction_policy='evict_last')\n    tmp4 = tl.load(in_ptr0 + (2 + (4*x1)), xmask, eviction_policy='evict_last')\n    tmp6 = tl.load(in_ptr0 + (3 + (4*x1)), xmask, eviction_policy='evict_last')\n    tmp3 = tmp1 + tmp2\n    tmp5 = tmp3 + tmp4\n    tmp7 = tmp5 + tmp6\n    tmp8 = 4.0\n    tmp9 = tmp7 / tmp8\n    tmp10 = tmp0 - tmp9\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n''', device_str='cuda')\n\n\n# kernel path: runs/run_shard_6/inductor_cache/v3/cv3kidzozprmsfiixppams3wqi7ij566hegofbo3ur5gfdrhxiik.py\n# Topologically Sorted Source Nodes: [square, s, add, rsqrt, x, mul_1, x_1], Original ATen: [aten.pow, aten.mean, aten.add, aten.rsqrt, aten.mul]\n# Source node to ATen node mapping:\n#   add => add\n#   mul_1 => mul_1\n#   rsqrt => rsqrt\n#   s => mean_1\n#   square => pow_1\n#   x => mul\n#   x_1 => add_1\n# Graph fragment:\n#   %pow_1 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%sub, 2), kwargs = {})\n#   %mean_1 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_1, [-1], True), kwargs = {})\n#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean_1, 1e-05), kwargs = {})\n#   %rsqrt : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add,), kwargs = {})\n#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub, %rsqrt), kwargs = {})\n#   %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul, %primals_2), kwargs = {})\n#   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1, %primals_3), kwargs = {})\ntriton_poi_fused_add_mean_mul_pow_rsqrt_1 = async_compile.triton('triton_poi_fused_add_mean_mul_pow_rsqrt_1', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[256], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mean_mul_pow_rsqrt_1', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_add_mean_mul_pow_rsqrt_1(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x1 = (xindex // 4)\n    x0 = xindex % 4\n    tmp0 = tl.load(in_ptr0 + (x2), xmask)\n    tmp1 = tl.load(in_ptr0 + (4*x1), xmask, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr0 + (1 + (4*x1)), xmask, eviction_policy='evict_last')\n    tmp6 = tl.load(in_ptr0 + (2 + (4*x1)), xmask, eviction_policy='evict_last')\n    tmp9 = tl.load(in_ptr0 + (3 + (4*x1)), xmask, eviction_policy='evict_last')\n    tmp18 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')\n    tmp20 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')\n    tmp2 = tmp1 * tmp1\n    tmp4 = tmp3 * tmp3\n    tmp5 = tmp2 + tmp4\n    tmp7 = tmp6 * tmp6\n    tmp8 = tmp5 + tmp7\n    tmp10 = tmp9 * tmp9\n    tmp11 = tmp8 + tmp10\n    tmp12 = 4.0\n    tmp13 = tmp11 / tmp12\n    tmp14 = 1e-05\n    tmp15 = tmp13 + tmp14\n    tmp16 = libdevice.rsqrt(tmp15)\n    tmp17 = tmp0 * tmp16\n    tmp19 = tmp17 * tmp18\n    tmp21 = tmp19 + tmp20\n    tl.store(out_ptr0 + (x2), tmp21, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    primals_1, primals_2, primals_3 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_2, (4, ), (1, ))\n    assert_size_stride(primals_3, (4, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [u, sub], Original ATen: [aten.mean, aten.sub]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_mean_sub_0.run(primals_1, buf0, 256, grid=grid(256), stream=stream0)\n        buf1 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [square, s, add, rsqrt, x, mul_1, x_1], Original ATen: [aten.pow, aten.mean, aten.add, aten.rsqrt, aten.mul]\n        triton_poi_fused_add_mean_mul_pow_rsqrt_1.run(buf0, primals_2, primals_3, buf1, 256, grid=grid(256), stream=stream0)\n        del buf0\n        del primals_2\n        del primals_3\n    return (buf1, primals_1, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport torch.nn as nn\n\n\nclass Norm(nn.Module):\n\n    def __init__(self, n_state, axis=-1, epsilon=1e-05):\n        super().__init__()\n        self.n_state = n_state\n        self.g = nn.Parameter(torch.ones([self.n_state]))\n        self.b = nn.Parameter(torch.zeros([self.n_state]))\n        self.axis = axis\n        self.epsilon = epsilon\n\n    def forward(self, x):\n        u = torch.mean(x, dim=self.axis, keepdim=True)\n        s = torch.mean(torch.square(x - u), dim=self.axis, keepdim=True)\n        x = (x - u) * torch.rsqrt(s + self.epsilon)\n        x = x * self.g + self.b\n        return x\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 4, 4])]\n\n\ndef get_init_inputs():\n    return [[], {'n_state': 4}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_mean_sub_0(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr\n    ):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x1 = xindex // 4\n    tmp0 = tl.load(in_ptr0 + x2, xmask)\n    tmp1 = tl.load(in_ptr0 + 4 * x1, xmask, eviction_policy='evict_last')\n    tmp2 = tl.load(in_ptr0 + (1 + 4 * x1), xmask, eviction_policy='evict_last')\n    tmp4 = tl.load(in_ptr0 + (2 + 4 * x1), xmask, eviction_policy='evict_last')\n    tmp6 = tl.load(in_ptr0 + (3 + 4 * x1), xmask, eviction_policy='evict_last')\n    tmp3 = tmp1 + tmp2\n    tmp5 = tmp3 + tmp4\n    tmp7 = tmp5 + tmp6\n    tmp8 = 4.0\n    tmp9 = tmp7 / tmp8\n    tmp10 = tmp0 - tmp9\n    tl.store(out_ptr0 + x2, tmp10, xmask)\n\n\n@triton.jit\ndef triton_poi_fused_add_mean_mul_pow_rsqrt_1(in_ptr0, in_ptr1, in_ptr2,\n    out_ptr0, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x1 = xindex // 4\n    x0 = xindex % 4\n    tmp0 = tl.load(in_ptr0 + x2, xmask)\n    tmp1 = tl.load(in_ptr0 + 4 * x1, xmask, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr0 + (1 + 4 * x1), xmask, eviction_policy='evict_last')\n    tmp6 = tl.load(in_ptr0 + (2 + 4 * x1), xmask, eviction_policy='evict_last')\n    tmp9 = tl.load(in_ptr0 + (3 + 4 * x1), xmask, eviction_policy='evict_last')\n    tmp18 = tl.load(in_ptr1 + x0, xmask, eviction_policy='evict_last')\n    tmp20 = tl.load(in_ptr2 + x0, xmask, eviction_policy='evict_last')\n    tmp2 = tmp1 * tmp1\n    tmp4 = tmp3 * tmp3\n    tmp5 = tmp2 + tmp4\n    tmp7 = tmp6 * tmp6\n    tmp8 = tmp5 + tmp7\n    tmp10 = tmp9 * tmp9\n    tmp11 = tmp8 + tmp10\n    tmp12 = 4.0\n    tmp13 = tmp11 / tmp12\n    tmp14 = 1e-05\n    tmp15 = tmp13 + tmp14\n    tmp16 = libdevice.rsqrt(tmp15)\n    tmp17 = tmp0 * tmp16\n    tmp19 = tmp17 * tmp18\n    tmp21 = tmp19 + tmp20\n    tl.store(out_ptr0 + x2, tmp21, xmask)\n\n\ndef call(args):\n    primals_1, primals_2, primals_3 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_2, (4,), (1,))\n    assert_size_stride(primals_3, (4,), (1,))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_mean_sub_0[grid(256)](primals_1, buf0, 256, XBLOCK\n            =128, num_warps=4, num_stages=1)\n        buf1 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        triton_poi_fused_add_mean_mul_pow_rsqrt_1[grid(256)](buf0,\n            primals_2, primals_3, buf1, 256, XBLOCK=256, num_warps=4,\n            num_stages=1)\n        del buf0\n        del primals_2\n        del primals_3\n    return buf1, primals_1\n\n\nclass NormNew(nn.Module):\n\n    def __init__(self, n_state, axis=-1, epsilon=1e-05):\n        super().__init__()\n        self.n_state = n_state\n        self.g = nn.Parameter(torch.ones([self.n_state]))\n        self.b = nn.Parameter(torch.zeros([self.n_state]))\n        self.axis = axis\n        self.epsilon = epsilon\n\n    def forward(self, input_0):\n        primals_2 = self.g\n        primals_3 = self.b\n        primals_1 = input_0\n        output = call([primals_1, primals_2, primals_3])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Aalanli/MusicGeneration&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Norm&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:5,&quot;string&quot;:&quot;5&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;MIT&quot;],&quot;string&quot;:&quot;[\n  \&quot;MIT\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;7d268322d692013d8ac6e70be31741cea519fa28&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/Aalanli/MusicGeneration/tree/7d268322d692013d8ac6e70be31741cea519fa28&quot;}}},{&quot;rowIdx&quot;:6,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;BehlerAngular&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_inference']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/u6/cu6io2odujubs4jc63atgai7gg2fi2mzfw74zg77lxaq5wh3dsrx.py\n# Topologically Sorted Source Nodes: [cat], Original ATen: [aten.cat]\n# Source node to ATen node mapping:\n#   cat => cat\n# Graph fragment:\n#   %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%mul, %mul_1], -1), kwargs = {})\ntriton_poi_fused_cat_0 = async_compile.triton('triton_poi_fused_cat_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[512], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 512\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex % 2\n    x1 = (xindex // 2)\n    x2 = xindex\n    tmp0 = x0\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 1, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x1), tmp4 &amp; xmask, eviction_policy='evict_last', other=0.0)\n    tmp6 = 1.0\n    tmp7 = tmp6 - tmp5\n    tmp8 = tmp7 * tmp6\n    tmp9 = tl.full(tmp8.shape, 0.0, tmp8.dtype)\n    tmp10 = tl.where(tmp4, tmp8, tmp9)\n    tmp11 = tmp0 >= tmp3\n    tmp12 = tl.full([1], 2, tl.int64)\n    tmp13 = tmp0 < tmp12\n    tmp14 = tl.load(in_ptr0 + (x1), tmp11 &amp; xmask, eviction_policy='evict_last', other=0.0)\n    tmp15 = tmp14 + tmp6\n    tmp16 = tmp15 * tmp6\n    tmp17 = tl.full(tmp16.shape, 0.0, tmp16.dtype)\n    tmp18 = tl.where(tmp11, tmp16, tmp17)\n    tmp19 = tl.where(tmp4, tmp10, tmp18)\n    tl.store(out_ptr0 + (x2), tmp19, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4, 4, 2), (128, 32, 8, 2, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [cat], Original ATen: [aten.cat]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_cat_0.run(arg0_1, buf0, 512, grid=grid(512), stream=stream0)\n        del arg0_1\n    return (buf0, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([arg0_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nfrom torch import nn as nn\n\n\nclass BehlerAngular(nn.Module):\n    \&quot;\&quot;\&quot;\n    Compute Behler type angular contribution of the angle spanned by three atoms:\n\n    :math:`2^{(1-\\\\zeta)} (1 + \\\\lambda \\\\cos( {\\\\theta}_{ijk} ) )^\\\\zeta`\n\n    Sets of zetas with lambdas of -1 and +1 are generated automatically.\n\n    Args:\n        zetas (set of int): Set of exponents used to compute angular Behler term (default={1})\n\n    \&quot;\&quot;\&quot;\n\n    def __init__(self, zetas={1}):\n        super(BehlerAngular, self).__init__()\n        self.zetas = zetas\n\n    def forward(self, cos_theta):\n        \&quot;\&quot;\&quot;\n        Args:\n            cos_theta (torch.Tensor): Cosines between all pairs of neighbors of the central atom.\n\n        Returns:\n            torch.Tensor: Tensor containing values of the angular filters.\n        \&quot;\&quot;\&quot;\n        angular_pos = [(2 ** (1 - zeta) * ((1.0 - cos_theta) ** zeta).\n            unsqueeze(-1)) for zeta in self.zetas]\n        angular_neg = [(2 ** (1 - zeta) * ((1.0 + cos_theta) ** zeta).\n            unsqueeze(-1)) for zeta in self.zetas]\n        angular_all = angular_pos + angular_neg\n        return torch.cat(angular_all, -1)\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 4, 4])]\n\n\ndef get_init_inputs():\n    return [[], {}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch import nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_cat_0(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 512\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex % 2\n    x1 = xindex // 2\n    x2 = xindex\n    tmp0 = x0\n    tl.full([1], 0, tl.int64)\n    tmp3 = tl.full([1], 1, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + x1, tmp4 &amp; xmask, eviction_policy='evict_last',\n        other=0.0)\n    tmp6 = 1.0\n    tmp7 = tmp6 - tmp5\n    tmp8 = tmp7 * tmp6\n    tmp9 = tl.full(tmp8.shape, 0.0, tmp8.dtype)\n    tmp10 = tl.where(tmp4, tmp8, tmp9)\n    tmp11 = tmp0 >= tmp3\n    tl.full([1], 2, tl.int64)\n    tmp14 = tl.load(in_ptr0 + x1, tmp11 &amp; xmask, eviction_policy=\n        'evict_last', other=0.0)\n    tmp15 = tmp14 + tmp6\n    tmp16 = tmp15 * tmp6\n    tmp17 = tl.full(tmp16.shape, 0.0, tmp16.dtype)\n    tmp18 = tl.where(tmp11, tmp16, tmp17)\n    tmp19 = tl.where(tmp4, tmp10, tmp18)\n    tl.store(out_ptr0 + x2, tmp19, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4, 4, 2), (128, 32, 8, 2, 1),\n            torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_cat_0[grid(512)](arg0_1, buf0, 512, XBLOCK=128,\n            num_warps=4, num_stages=1)\n        del arg0_1\n    return buf0,\n\n\nclass BehlerAngularNew(nn.Module):\n    \&quot;\&quot;\&quot;\n    Compute Behler type angular contribution of the angle spanned by three atoms:\n\n    :math:`2^{(1-\\\\zeta)} (1 + \\\\lambda \\\\cos( {\\\\theta}_{ijk} ) )^\\\\zeta`\n\n    Sets of zetas with lambdas of -1 and +1 are generated automatically.\n\n    Args:\n        zetas (set of int): Set of exponents used to compute angular Behler term (default={1})\n\n    \&quot;\&quot;\&quot;\n\n    def __init__(self, zetas={1}):\n        super(BehlerAngularNew, self).__init__()\n        self.zetas = zetas\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;AlexanderDKazakov/schnetpack&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;BehlerAngular&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:6,&quot;string&quot;:&quot;6&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;MIT&quot;],&quot;string&quot;:&quot;[\n  \&quot;MIT\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;97b82469d977981b500e439a6c93696d8dac8a3f&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/AlexanderDKazakov/schnetpack/tree/97b82469d977981b500e439a6c93696d8dac8a3f&quot;}}},{&quot;rowIdx&quot;:7,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;BottleneckBlock&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/3w/c3w6ed26vbtzgzigkkyuiuwphm777x7aoogcxkvoly3ia7qifnzg.py\n# Topologically Sorted Source Nodes: [out_1], Original ATen: [aten.relu]\n# Source node to ATen node mapping:\n#   out_1 => relu\n# Graph fragment:\n#   %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})\ntriton_poi_fused_relu_0 = async_compile.triton('triton_poi_fused_relu_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[16], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_relu_0', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_relu_0(in_out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.full([1], 0, tl.int32)\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tl.store(in_out_ptr0 + (x0), tmp2, xmask)\n''', device_str='cuda')\n\n\n# kernel path: runs/run_shard_6/inductor_cache/ad/cadccuyhl7stcp3nyqfgohiwbiv5ckfzxsye27ithwsill6dvmh4.py\n# Topologically Sorted Source Nodes: [out_2, out_3], Original ATen: [aten.convolution, aten.relu]\n# Source node to ATen node mapping:\n#   out_2 => convolution_1\n#   out_3 => relu_1\n# Graph fragment:\n#   %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %primals_3, %primals_4, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})\n#   %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_1,), kwargs = {})\ntriton_poi_fused_convolution_relu_1 = async_compile.triton('triton_poi_fused_convolution_relu_1', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[4], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_relu_1', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_convolution_relu_1(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 4\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.load(in_ptr0 + (0))\n    tmp2 = tl.broadcast_to(tmp1, [XBLOCK])\n    tmp3 = tmp0 + tmp2\n    tmp4 = tl.full([1], 0, tl.int32)\n    tmp5 = triton_helpers.maximum(tmp4, tmp3)\n    tl.store(in_out_ptr0 + (x0), tmp5, xmask)\n''', device_str='cuda')\n\n\n# kernel path: runs/run_shard_6/inductor_cache/5n/c5nsnnscsravxsqs7m7r23cp6lbfgo27lpzms4oosykw3b6atpvf.py\n# Topologically Sorted Source Nodes: [out_5, out_6], Original ATen: [aten.add, aten.relu, aten.threshold_backward]\n# Source node to ATen node mapping:\n#   out_5 => add\n#   out_6 => relu_2\n# Graph fragment:\n#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%convolution_2, %primals_1), kwargs = {})\n#   %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add,), kwargs = {})\n#   %le : [num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_2, 0), kwargs = {})\ntriton_poi_fused_add_relu_threshold_backward_2 = async_compile.triton('triton_poi_fused_add_relu_threshold_backward_2', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[64], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*i1', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_relu_threshold_backward_2', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_add_relu_threshold_backward_2(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 64\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x1 = (xindex // 4)\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (x1), xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr1 + (x2), xmask)\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tmp5 = 0.0\n    tmp6 = tmp4 <= tmp5\n    tl.store(out_ptr0 + (x2), tmp4, xmask)\n    tl.store(out_ptr1 + (x2), tmp6, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 2, 2), (16, 4, 2, 1))\n    assert_size_stride(primals_2, (1, 4, 1, 1), (4, 1, 1, 1))\n    assert_size_stride(primals_3, (1, 1, 3, 3), (9, 9, 3, 1))\n    assert_size_stride(primals_4, (1, ), (1, ))\n    assert_size_stride(primals_5, (4, 1, 1, 1), (1, 1, 1, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        # Topologically Sorted Source Nodes: [out], Original ATen: [aten.convolution]\n        buf0 = extern_kernels.convolution(primals_1, primals_2, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf0, (4, 1, 2, 2), (4, 4, 2, 1))\n        buf1 = buf0; del buf0  # reuse\n        # Topologically Sorted Source Nodes: [out_1], Original ATen: [aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_relu_0.run(buf1, 16, grid=grid(16), stream=stream0)\n        # Topologically Sorted Source Nodes: [out_2], Original ATen: [aten.convolution]\n        buf2 = extern_kernels.convolution(buf1, primals_3, stride=(2, 2), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf2, (4, 1, 1, 1), (1, 1, 1, 1))\n        buf3 = buf2; del buf2  # reuse\n        # Topologically Sorted Source Nodes: [out_2, out_3], Original ATen: [aten.convolution, aten.relu]\n        triton_poi_fused_convolution_relu_1.run(buf3, primals_4, 4, grid=grid(4), stream=stream0)\n        del primals_4\n        # Topologically Sorted Source Nodes: [out_4], Original ATen: [aten.convolution]\n        buf4 = extern_kernels.convolution(buf3, primals_5, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf4, (4, 4, 1, 1), (4, 1, 1, 1))\n        buf5 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.float32)\n        buf6 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.bool)\n        # Topologically Sorted Source Nodes: [out_5, out_6], Original ATen: [aten.add, aten.relu, aten.threshold_backward]\n        triton_poi_fused_add_relu_threshold_backward_2.run(buf4, primals_1, buf5, buf6, 64, grid=grid(64), stream=stream0)\n        del buf4\n    return (buf5, primals_1, primals_2, primals_3, primals_5, buf1, buf3, buf6, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((4, 4, 2, 2), (16, 4, 2, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((1, 4, 1, 1), (4, 1, 1, 1), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((1, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_4 = rand_strided((1, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_5 = rand_strided((4, 1, 1, 1), (1, 1, 1, 1), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import math\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data.distributed\n\n\ndef init_layer(L):\n    if isinstance(L, nn.Conv2d):\n        n = L.kernel_size[0] * L.kernel_size[1] * L.out_channels\n        L.weight.data.normal_(0, math.sqrt(2.0 / float(n)))\n    elif isinstance(L, nn.BatchNorm2d):\n        L.weight.data.fill_(1)\n        L.bias.data.fill_(0)\n\n\nclass BottleneckBlock(nn.Module):\n\n    def __init__(self, indim, outdim, half_res):\n        super(BottleneckBlock, self).__init__()\n        bottleneckdim = int(outdim / 4)\n        self.indim = indim\n        self.outdim = outdim\n        self.C1 = nn.Conv2d(indim, bottleneckdim, kernel_size=1, bias=False)\n        self.BN1 = nn.Identity()\n        self.C2 = nn.Conv2d(bottleneckdim, bottleneckdim, kernel_size=3,\n            stride=2 if half_res else 1, padding=1)\n        self.BN2 = nn.Identity()\n        self.C3 = nn.Conv2d(bottleneckdim, outdim, kernel_size=1, bias=False)\n        self.BN3 = nn.Identity()\n        self.relu = nn.ReLU()\n        self.parametrized_layers = [self.C1, self.BN1, self.C2, self.BN2,\n            self.C3, self.BN3]\n        self.half_res = half_res\n        if indim != outdim:\n            self.shortcut = nn.Conv2d(indim, outdim, 1, stride=2 if\n                half_res else 1, bias=False)\n            self.parametrized_layers.append(self.shortcut)\n            self.shortcut_type = '1x1'\n        else:\n            self.shortcut_type = 'identity'\n        for layer in self.parametrized_layers:\n            init_layer(layer)\n\n    def forward(self, x):\n        short_out = x if self.shortcut_type == 'identity' else self.shortcut(x)\n        out = self.C1(x)\n        out = self.BN1(out)\n        out = self.relu(out)\n        out = self.C2(out)\n        out = self.BN2(out)\n        out = self.relu(out)\n        out = self.C3(out)\n        out = self.BN3(out)\n        out = out + short_out\n        out = self.relu(out)\n        return out\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 2, 2])]\n\n\ndef get_init_inputs():\n    return [[], {'indim': 4, 'outdim': 4, 'half_res': 4}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nimport math\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data.distributed\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_relu_0(in_out_ptr0, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + x0, xmask)\n    tmp1 = tl.full([1], 0, tl.int32)\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tl.store(in_out_ptr0 + x0, tmp2, xmask)\n\n\n@triton.jit\ndef triton_poi_fused_convolution_relu_1(in_out_ptr0, in_ptr0, xnumel,\n    XBLOCK: tl.constexpr):\n    xnumel = 4\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + x0, xmask)\n    tmp1 = tl.load(in_ptr0 + 0)\n    tmp2 = tl.broadcast_to(tmp1, [XBLOCK])\n    tmp3 = tmp0 + tmp2\n    tmp4 = tl.full([1], 0, tl.int32)\n    tmp5 = triton_helpers.maximum(tmp4, tmp3)\n    tl.store(in_out_ptr0 + x0, tmp5, xmask)\n\n\n@triton.jit\ndef triton_poi_fused_add_relu_threshold_backward_2(in_ptr0, in_ptr1,\n    out_ptr0, out_ptr1, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 64\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x1 = xindex // 4\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + x1, xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr1 + x2, xmask)\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tmp5 = 0.0\n    tmp6 = tmp4 <= tmp5\n    tl.store(out_ptr0 + x2, tmp4, xmask)\n    tl.store(out_ptr1 + x2, tmp6, xmask)\n\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 2, 2), (16, 4, 2, 1))\n    assert_size_stride(primals_2, (1, 4, 1, 1), (4, 1, 1, 1))\n    assert_size_stride(primals_3, (1, 1, 3, 3), (9, 9, 3, 1))\n    assert_size_stride(primals_4, (1,), (1,))\n    assert_size_stride(primals_5, (4, 1, 1, 1), (1, 1, 1, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = extern_kernels.convolution(primals_1, primals_2, stride=(1, \n            1), padding=(0, 0), dilation=(1, 1), transposed=False,\n            output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf0, (4, 1, 2, 2), (4, 4, 2, 1))\n        buf1 = buf0\n        del buf0\n        get_raw_stream(0)\n        triton_poi_fused_relu_0[grid(16)](buf1, 16, XBLOCK=16, num_warps=1,\n            num_stages=1)\n        buf2 = extern_kernels.convolution(buf1, primals_3, stride=(2, 2),\n            padding=(1, 1), dilation=(1, 1), transposed=False,\n            output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf2, (4, 1, 1, 1), (1, 1, 1, 1))\n        buf3 = buf2\n        del buf2\n        triton_poi_fused_convolution_relu_1[grid(4)](buf3, primals_4, 4,\n            XBLOCK=4, num_warps=1, num_stages=1)\n        del primals_4\n        buf4 = extern_kernels.convolution(buf3, primals_5, stride=(1, 1),\n            padding=(0, 0), dilation=(1, 1), transposed=False,\n            output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf4, (4, 4, 1, 1), (4, 1, 1, 1))\n        buf5 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.float32)\n        buf6 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.bool)\n        triton_poi_fused_add_relu_threshold_backward_2[grid(64)](buf4,\n            primals_1, buf5, buf6, 64, XBLOCK=64, num_warps=1, num_stages=1)\n        del buf4\n    return buf5, primals_1, primals_2, primals_3, primals_5, buf1, buf3, buf6\n\n\ndef init_layer(L):\n    if isinstance(L, nn.Conv2d):\n        n = L.kernel_size[0] * L.kernel_size[1] * L.out_channels\n        L.weight.data.normal_(0, math.sqrt(2.0 / float(n)))\n    elif isinstance(L, nn.BatchNorm2d):\n        L.weight.data.fill_(1)\n        L.bias.data.fill_(0)\n\n\nclass BottleneckBlockNew(nn.Module):\n\n    def __init__(self, indim, outdim, half_res):\n        super(BottleneckBlockNew, self).__init__()\n        bottleneckdim = int(outdim / 4)\n        self.indim = indim\n        self.outdim = outdim\n        self.C1 = nn.Conv2d(indim, bottleneckdim, kernel_size=1, bias=False)\n        self.BN1 = nn.Identity()\n        self.C2 = nn.Conv2d(bottleneckdim, bottleneckdim, kernel_size=3,\n            stride=2 if half_res else 1, padding=1)\n        self.BN2 = nn.Identity()\n        self.C3 = nn.Conv2d(bottleneckdim, outdim, kernel_size=1, bias=False)\n        self.BN3 = nn.Identity()\n        self.relu = nn.ReLU()\n        self.parametrized_layers = [self.C1, self.BN1, self.C2, self.BN2,\n            self.C3, self.BN3]\n        self.half_res = half_res\n        if indim != outdim:\n            self.shortcut = nn.Conv2d(indim, outdim, 1, stride=2 if\n                half_res else 1, bias=False)\n            self.parametrized_layers.append(self.shortcut)\n            self.shortcut_type = '1x1'\n        else:\n            self.shortcut_type = 'identity'\n        for layer in self.parametrized_layers:\n            init_layer(layer)\n\n    def forward(self, input_0):\n        primals_2 = self.C1.weight\n        primals_3 = self.C2.weight\n        primals_4 = self.C2.bias\n        primals_5 = self.C3.weight\n        primals_1 = input_0\n        output = call([primals_1, primals_2, primals_3, primals_4, primals_5])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Aamer98/FeatureNorm&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;BottleneckBlock&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:7,&quot;string&quot;:&quot;7&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;MIT&quot;],&quot;string&quot;:&quot;[\n  \&quot;MIT\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;fbf3d3b4cef81b3351347d272eb51b6cdd9f0cc5&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/Aamer98/FeatureNorm/tree/fbf3d3b4cef81b3351347d272eb51b6cdd9f0cc5&quot;}}},{&quot;rowIdx&quot;:8,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Mlp&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/sq/csqquhpniarhypnbdhu3ry5jhmzns75umbxvduqvlyaridzrevc5.py\n# Topologically Sorted Source Nodes: [h], Original ATen: [aten.gelu]\n# Source node to ATen node mapping:\n#   h => add_1, erf, mul, mul_1, mul_2\n# Graph fragment:\n#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_2, 0.5), kwargs = {})\n#   %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_2, 0.7071067811865476), kwargs = {})\n#   %erf : [num_users=1] = call_function[target=torch.ops.aten.erf.default](args = (%mul_1,), kwargs = {})\n#   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%erf, 1), kwargs = {})\n#   %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul, %add_1), kwargs = {})\ntriton_poi_fused_gelu_0 = async_compile.triton('triton_poi_fused_gelu_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[256], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_gelu_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_gelu_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x0 = xindex % 4\n    tmp0 = tl.load(in_ptr0 + (x2), xmask)\n    tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = 0.5\n    tmp4 = tmp2 * tmp3\n    tmp5 = 0.7071067811865476\n    tmp6 = tmp2 * tmp5\n    tmp7 = libdevice.erf(tmp6)\n    tmp8 = 1.0\n    tmp9 = tmp7 + tmp8\n    tmp10 = tmp4 * tmp9\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_2, (1, 4, 4), (16, 4, 1))\n    assert_size_stride(primals_3, (4, ), (1, ))\n    assert_size_stride(primals_4, (1, 4, 4), (16, 4, 1))\n    assert_size_stride(primals_5, (4, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((64, 4), (4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n        extern_kernels.mm(reinterpret_tensor(primals_1, (64, 4), (4, 1), 0), reinterpret_tensor(primals_2, (4, 4), (4, 1), 0), out=buf0)\n        del primals_2\n        buf1 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [h], Original ATen: [aten.gelu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_gelu_0.run(buf0, primals_3, buf1, 256, grid=grid(256), stream=stream0)\n        buf2 = empty_strided_cuda((64, 4), (4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [], Original ATen: []\n        extern_kernels.addmm(primals_5, reinterpret_tensor(buf1, (64, 4), (4, 1), 0), reinterpret_tensor(primals_4, (4, 4), (4, 1), 0), alpha=1, beta=1, out=buf2)\n        del primals_5\n    return (reinterpret_tensor(buf2, (4, 4, 4, 4), (64, 16, 4, 1), 0), primals_3, buf0, reinterpret_tensor(buf1, (4, 64), (1, 4), 0), reinterpret_tensor(primals_4, (4, 4), (1, 4), 0), reinterpret_tensor(primals_1, (4, 64), (1, 4), 0), )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((1, 4, 4), (16, 4, 1), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_4 = rand_strided((1, 4, 4), (16, 4, 1), device='cuda:0', dtype=torch.float32)\n    primals_5 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport torch.nn as nn\n\n\nclass Conv1d(nn.Module):\n\n    def __init__(self, nf, nx, stdev=0.02):\n        super().__init__()\n        self.nf = nf\n        self.nx = nx\n        self.stdev = stdev\n        self.w = nn.Parameter(torch.normal(size=[1, self.nx, self.nf], mean\n            =0.0, std=self.stdev))\n        self.b = nn.Parameter(torch.zeros([self.nf]))\n\n    def forward(self, x: 'torch.Tensor'):\n        shape = x.size()\n        start, nx = shape[:-1], shape[-1]\n        return torch.reshape(torch.matmul(torch.reshape(x, [-1, nx]), torch\n            .reshape(self.w, [-1, self.nf])) + self.b, start + (self.nf,))\n\n\nclass Mlp(nn.Module):\n\n    def __init__(self, input_dim, proj_dim):\n        super().__init__()\n        self.input_dim = input_dim\n        self.proj_dim = proj_dim\n        self.conv_fc = Conv1d(self.proj_dim, self.input_dim)\n        self.conv_proj = Conv1d(self.input_dim, self.proj_dim)\n\n    def forward(self, x):\n        h = nn.functional.gelu(self.conv_fc(x))\n        return self.conv_proj(h)\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 4, 4])]\n\n\ndef get_init_inputs():\n    return [[], {'input_dim': 4, 'proj_dim': 4}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_gelu_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK: tl.\n    constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x0 = xindex % 4\n    tmp0 = tl.load(in_ptr0 + x2, xmask)\n    tmp1 = tl.load(in_ptr1 + x0, xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = 0.5\n    tmp4 = tmp2 * tmp3\n    tmp5 = 0.7071067811865476\n    tmp6 = tmp2 * tmp5\n    tmp7 = libdevice.erf(tmp6)\n    tmp8 = 1.0\n    tmp9 = tmp7 + tmp8\n    tmp10 = tmp4 * tmp9\n    tl.store(out_ptr0 + x2, tmp10, xmask)\n\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_2, (1, 4, 4), (16, 4, 1))\n    assert_size_stride(primals_3, (4,), (1,))\n    assert_size_stride(primals_4, (1, 4, 4), (16, 4, 1))\n    assert_size_stride(primals_5, (4,), (1,))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((64, 4), (4, 1), torch.float32)\n        extern_kernels.mm(reinterpret_tensor(primals_1, (64, 4), (4, 1), 0),\n            reinterpret_tensor(primals_2, (4, 4), (4, 1), 0), out=buf0)\n        del primals_2\n        buf1 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_gelu_0[grid(256)](buf0, primals_3, buf1, 256,\n            XBLOCK=128, num_warps=4, num_stages=1)\n        buf2 = empty_strided_cuda((64, 4), (4, 1), torch.float32)\n        extern_kernels.addmm(primals_5, reinterpret_tensor(buf1, (64, 4), (\n            4, 1), 0), reinterpret_tensor(primals_4, (4, 4), (4, 1), 0),\n            alpha=1, beta=1, out=buf2)\n        del primals_5\n    return reinterpret_tensor(buf2, (4, 4, 4, 4), (64, 16, 4, 1), 0\n        ), primals_3, buf0, reinterpret_tensor(buf1, (4, 64), (1, 4), 0\n        ), reinterpret_tensor(primals_4, (4, 4), (1, 4), 0\n        ), reinterpret_tensor(primals_1, (4, 64), (1, 4), 0)\n\n\nclass Conv1d(nn.Module):\n\n    def __init__(self, nf, nx, stdev=0.02):\n        super().__init__()\n        self.nf = nf\n        self.nx = nx\n        self.stdev = stdev\n        self.w = nn.Parameter(torch.normal(size=[1, self.nx, self.nf], mean\n            =0.0, std=self.stdev))\n        self.b = nn.Parameter(torch.zeros([self.nf]))\n\n    def forward(self, x: 'torch.Tensor'):\n        shape = x.size()\n        start, nx = shape[:-1], shape[-1]\n        return torch.reshape(torch.matmul(torch.reshape(x, [-1, nx]), torch\n            .reshape(self.w, [-1, self.nf])) + self.b, start + (self.nf,))\n\n\nclass MlpNew(nn.Module):\n\n    def __init__(self, input_dim, proj_dim):\n        super().__init__()\n        self.input_dim = input_dim\n        self.proj_dim = proj_dim\n        self.conv_fc = Conv1d(self.proj_dim, self.input_dim)\n        self.conv_proj = Conv1d(self.input_dim, self.proj_dim)\n\n    def forward(self, input_0):\n        primals_2 = self.conv_fc.w\n        primals_3 = self.conv_fc.b\n        primals_4 = self.conv_proj.w\n        primals_5 = self.conv_proj.b\n        primals_1 = input_0\n        output = call([primals_1, primals_2, primals_3, primals_4, primals_5])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Aalanli/MusicGeneration&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Mlp&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:9,&quot;string&quot;:&quot;9&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;MIT&quot;],&quot;string&quot;:&quot;[\n  \&quot;MIT\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;7d268322d692013d8ac6e70be31741cea519fa28&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/Aalanli/MusicGeneration/tree/7d268322d692013d8ac6e70be31741cea519fa28&quot;}}},{&quot;rowIdx&quot;:9,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;GCN&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/ei/cei4vnxgrn55dm7jkw3qn2lp2qox5qamtqida5hfzp6upgn6nb24.py\n# Topologically Sorted Source Nodes: [x], Original ATen: [aten.elu]\n# Source node to ATen node mapping:\n#   x => expm1, gt, mul, mul_2, where\n# Graph fragment:\n#   %gt : [num_users=1] = call_function[target=torch.ops.aten.gt.Scalar](args = (%mm, 0), kwargs = {})\n#   %mul : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mm, 1.0), kwargs = {})\n#   %expm1 : [num_users=1] = call_function[target=torch.ops.aten.expm1.default](args = (%mul,), kwargs = {})\n#   %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%expm1, 1.0), kwargs = {})\n#   %where : [num_users=2] = call_function[target=torch.ops.aten.where.self](args = (%gt, %mul, %mul_2), kwargs = {})\ntriton_poi_fused_elu_0 = async_compile.triton('triton_poi_fused_elu_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[16], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_elu_0', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_elu_0(in_out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = 0.0\n    tmp2 = tmp0 > tmp1\n    tmp3 = 1.0\n    tmp4 = tmp0 * tmp3\n    tmp5 = libdevice.expm1(tmp4)\n    tmp6 = tmp5 * tmp3\n    tmp7 = tl.where(tmp2, tmp4, tmp6)\n    tl.store(in_out_ptr0 + (x0), tmp7, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4), (4, 1))\n    assert_size_stride(primals_2, (4, ), (1, ))\n    assert_size_stride(primals_3, (4, 4), (4, 1))\n    assert_size_stride(primals_4, (4, 4), (4, 1))\n    assert_size_stride(primals_5, (4, 4), (4, 1))\n    assert_size_stride(primals_6, (4, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4), (4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [support], Original ATen: [aten.addmm]\n        extern_kernels.addmm(primals_2, primals_3, reinterpret_tensor(primals_1, (4, 4), (1, 4), 0), alpha=1, beta=1, out=buf0)\n        del primals_1\n        del primals_2\n        buf1 = empty_strided_cuda((4, 4), (4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [output], Original ATen: [aten.mm]\n        extern_kernels.mm(primals_4, buf0, out=buf1)\n        buf2 = buf1; del buf1  # reuse\n        # Topologically Sorted Source Nodes: [x], Original ATen: [aten.elu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_elu_0.run(buf2, 16, grid=grid(16), stream=stream0)\n        buf3 = buf0; del buf0  # reuse\n        # Topologically Sorted Source Nodes: [support_1], Original ATen: [aten.addmm]\n        extern_kernels.addmm(primals_6, buf2, reinterpret_tensor(primals_5, (4, 4), (1, 4), 0), alpha=1, beta=1, out=buf3)\n        del primals_6\n        buf4 = empty_strided_cuda((4, 4), (4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [output_1], Original ATen: [aten.mm]\n        extern_kernels.mm(primals_4, buf3, out=buf4)\n        del buf3\n    return (buf4, primals_3, buf2, reinterpret_tensor(primals_4, (4, 4), (1, 4), 0), primals_5, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)\n    primals_4 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)\n    primals_5 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)\n    primals_6 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;from torch.nn import Module\nimport math\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\n\nclass GraphConvolution(Module):\n    \&quot;\&quot;\&quot;\n    A Graph Convolution Layer (GCN)\n    \&quot;\&quot;\&quot;\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.W = nn.Linear(in_features, out_features, bias=bias)\n        self.init()\n\n    def init(self):\n        stdv = 1.0 / math.sqrt(self.W.weight.size(1))\n        self.W.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, adj):\n        support = self.W(input)\n        output = torch.spmm(adj, support)\n        return output\n\n\nclass GCN(nn.Module):\n    \&quot;\&quot;\&quot;\n    A Two-layer GCN.\n    \&quot;\&quot;\&quot;\n\n    def __init__(self, nfeat, nhid, nclass, dropout):\n        super(GCN, self).__init__()\n        self.gc1 = GraphConvolution(nfeat, nhid)\n        self.gc2 = GraphConvolution(nhid, nclass)\n        self.dropout = dropout\n        self.elu = torch.nn.ELU(inplace=True)\n\n    def forward(self, x, adj, use_relu=True):\n        x = self.gc1(x, adj)\n        if use_relu:\n            x = self.elu(x)\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = self.gc2(x, adj)\n        return x\n\n\ndef get_inputs():\n    return [torch.rand([4, 4]), torch.rand([4, 4])]\n\n\ndef get_init_inputs():\n    return [[], {'nfeat': 4, 'nhid': 4, 'nclass': 4, 'dropout': 0.5}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\nfrom torch.nn import Module\nimport math\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_elu_0(in_out_ptr0, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + x0, xmask)\n    tmp1 = 0.0\n    tmp2 = tmp0 > tmp1\n    tmp3 = 1.0\n    tmp4 = tmp0 * tmp3\n    tmp5 = libdevice.expm1(tmp4)\n    tmp6 = tmp5 * tmp3\n    tmp7 = tl.where(tmp2, tmp4, tmp6)\n    tl.store(in_out_ptr0 + x0, tmp7, xmask)\n\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4), (4, 1))\n    assert_size_stride(primals_2, (4,), (1,))\n    assert_size_stride(primals_3, (4, 4), (4, 1))\n    assert_size_stride(primals_4, (4, 4), (4, 1))\n    assert_size_stride(primals_5, (4, 4), (4, 1))\n    assert_size_stride(primals_6, (4,), (1,))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4), (4, 1), torch.float32)\n        extern_kernels.addmm(primals_2, primals_3, reinterpret_tensor(\n            primals_1, (4, 4), (1, 4), 0), alpha=1, beta=1, out=buf0)\n        del primals_1\n        del primals_2\n        buf1 = empty_strided_cuda((4, 4), (4, 1), torch.float32)\n        extern_kernels.mm(primals_4, buf0, out=buf1)\n        buf2 = buf1\n        del buf1\n        get_raw_stream(0)\n        triton_poi_fused_elu_0[grid(16)](buf2, 16, XBLOCK=16, num_warps=1,\n            num_stages=1)\n        buf3 = buf0\n        del buf0\n        extern_kernels.addmm(primals_6, buf2, reinterpret_tensor(primals_5,\n            (4, 4), (1, 4), 0), alpha=1, beta=1, out=buf3)\n        del primals_6\n        buf4 = empty_strided_cuda((4, 4), (4, 1), torch.float32)\n        extern_kernels.mm(primals_4, buf3, out=buf4)\n        del buf3\n    return buf4, primals_3, buf2, reinterpret_tensor(primals_4, (4, 4), (1,\n        4), 0), primals_5\n\n\nclass GraphConvolution(Module):\n    \&quot;\&quot;\&quot;\n    A Graph Convolution Layer (GCN)\n    \&quot;\&quot;\&quot;\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.W = nn.Linear(in_features, out_features, bias=bias)\n        self.init()\n\n    def init(self):\n        stdv = 1.0 / math.sqrt(self.W.weight.size(1))\n        self.W.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, adj):\n        support = self.W(input)\n        output = torch.spmm(adj, support)\n        return output\n\n\nclass GCNNew(nn.Module):\n    \&quot;\&quot;\&quot;\n    A Two-layer GCN.\n    \&quot;\&quot;\&quot;\n\n    def __init__(self, nfeat, nhid, nclass, dropout):\n        super(GCNNew, self).__init__()\n        self.gc1 = GraphConvolution(nfeat, nhid)\n        self.gc2 = GraphConvolution(nhid, nclass)\n        self.dropout = dropout\n        self.elu = torch.nn.ELU(inplace=True)\n\n    def forward(self, input_0, input_1):\n        primals_1 = self.gc1.W.weight\n        primals_2 = self.gc1.W.bias\n        primals_3 = self.gc2.W.weight\n        primals_6 = self.gc2.W.bias\n        primals_4 = input_0\n        primals_5 = input_1\n        output = call([primals_1, primals_2, primals_3, primals_4,\n            primals_5, primals_6])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;AlexMinhao/NAS_GNN&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;GCN&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:10,&quot;string&quot;:&quot;10&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;Apache-2.0&quot;],&quot;string&quot;:&quot;[\n  \&quot;Apache-2.0\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;89183988a96e1d6baed910ab3843c13282f8b077&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/AlexMinhao/NAS_GNN/tree/89183988a96e1d6baed910ab3843c13282f8b077&quot;}}},{&quot;rowIdx&quot;:10,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;SimpleBlock&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/3w/c3w6ed26vbtzgzigkkyuiuwphm777x7aoogcxkvoly3ia7qifnzg.py\n# Topologically Sorted Source Nodes: [out_1], Original ATen: [aten.relu]\n# Source node to ATen node mapping:\n#   out_1 => relu\n# Graph fragment:\n#   %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})\ntriton_poi_fused_relu_0 = async_compile.triton('triton_poi_fused_relu_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[16], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_relu_0', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_relu_0(in_out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.full([1], 0, tl.int32)\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tl.store(in_out_ptr0 + (x0), tmp2, xmask)\n''', device_str='cuda')\n\n\n# kernel path: runs/run_shard_6/inductor_cache/l4/cl4s6iwd4ja3i3vrrev5xqre3x2k7a3yguenikvia4suomlqy7ty.py\n# Topologically Sorted Source Nodes: [out_3, out_4], Original ATen: [aten.add, aten.relu, aten.threshold_backward]\n# Source node to ATen node mapping:\n#   out_3 => add\n#   out_4 => relu_1\n# Graph fragment:\n#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%convolution_1, %primals_2), kwargs = {})\n#   %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add,), kwargs = {})\n#   %le : [num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_1, 0), kwargs = {})\ntriton_poi_fused_add_relu_threshold_backward_1 = async_compile.triton('triton_poi_fused_add_relu_threshold_backward_1', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[64], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*i1', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_relu_threshold_backward_1', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_add_relu_threshold_backward_1(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 64\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x1 = (xindex // 4)\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (x1), xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr1 + (x2), xmask)\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tmp5 = 0.0\n    tmp6 = tmp4 <= tmp5\n    tl.store(out_ptr0 + (x2), tmp4, xmask)\n    tl.store(out_ptr1 + (x2), tmp6, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    primals_1, primals_2, primals_3 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 3, 3), (36, 9, 3, 1))\n    assert_size_stride(primals_2, (4, 4, 2, 2), (16, 4, 2, 1))\n    assert_size_stride(primals_3, (4, 4, 3, 3), (36, 9, 3, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        # Topologically Sorted Source Nodes: [out], Original ATen: [aten.convolution]\n        buf0 = extern_kernels.convolution(primals_2, primals_1, stride=(2, 2), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf0, (4, 4, 1, 1), (4, 1, 1, 1))\n        buf1 = buf0; del buf0  # reuse\n        # Topologically Sorted Source Nodes: [out_1], Original ATen: [aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_relu_0.run(buf1, 16, grid=grid(16), stream=stream0)\n        # Topologically Sorted Source Nodes: [out_2], Original ATen: [aten.convolution]\n        buf2 = extern_kernels.convolution(buf1, primals_3, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf2, (4, 4, 1, 1), (4, 1, 1, 1))\n        buf3 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.float32)\n        buf4 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.bool)\n        # Topologically Sorted Source Nodes: [out_3, out_4], Original ATen: [aten.add, aten.relu, aten.threshold_backward]\n        triton_poi_fused_add_relu_threshold_backward_1.run(buf2, primals_2, buf3, buf4, 64, grid=grid(64), stream=stream0)\n        del buf2\n    return (buf3, primals_1, primals_2, primals_3, buf1, buf4, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((4, 4, 3, 3), (36, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((4, 4, 2, 2), (16, 4, 2, 1), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((4, 4, 3, 3), (36, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import math\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data.distributed\n\n\ndef init_layer(L):\n    if isinstance(L, nn.Conv2d):\n        n = L.kernel_size[0] * L.kernel_size[1] * L.out_channels\n        L.weight.data.normal_(0, math.sqrt(2.0 / float(n)))\n    elif isinstance(L, nn.BatchNorm2d):\n        L.weight.data.fill_(1)\n        L.bias.data.fill_(0)\n\n\nclass SimpleBlock(nn.Module):\n    maml = False\n\n    def __init__(self, indim, outdim, half_res):\n        super(SimpleBlock, self).__init__()\n        self.indim = indim\n        self.outdim = outdim\n        self.C1 = nn.Conv2d(indim, outdim, kernel_size=3, stride=2 if\n            half_res else 1, padding=1, bias=False)\n        self.BN1 = nn.Identity()\n        self.C2 = nn.Conv2d(outdim, outdim, kernel_size=3, padding=1, bias=\n            False)\n        self.BN2 = nn.Identity()\n        self.relu1 = nn.ReLU(inplace=True)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.parametrized_layers = [self.C1, self.C2, self.BN1, self.BN2]\n        self.half_res = half_res\n        if indim != outdim:\n            self.shortcut = nn.Conv2d(indim, outdim, 1, 2 if half_res else \n                1, bias=False)\n            self.BNshortcut = nn.Identity()\n            self.parametrized_layers.append(self.shortcut)\n            self.parametrized_layers.append(self.BNshortcut)\n            self.shortcut_type = '1x1'\n        else:\n            self.shortcut_type = 'identity'\n        for layer in self.parametrized_layers:\n            init_layer(layer)\n\n    def forward(self, x):\n        out = self.C1(x)\n        out = self.BN1(out)\n        out = self.relu1(out)\n        out = self.C2(out)\n        out = self.BN2(out)\n        short_out = x if self.shortcut_type == 'identity' else self.BNshortcut(\n            self.shortcut(x))\n        out = out + short_out\n        out = self.relu2(out)\n        return out\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 2, 2])]\n\n\ndef get_init_inputs():\n    return [[], {'indim': 4, 'outdim': 4, 'half_res': 4}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nimport math\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data.distributed\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_relu_0(in_out_ptr0, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 16\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + x0, xmask)\n    tmp1 = tl.full([1], 0, tl.int32)\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tl.store(in_out_ptr0 + x0, tmp2, xmask)\n\n\n@triton.jit\ndef triton_poi_fused_add_relu_threshold_backward_1(in_ptr0, in_ptr1,\n    out_ptr0, out_ptr1, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 64\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x1 = xindex // 4\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + x1, xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr1 + x2, xmask)\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tmp5 = 0.0\n    tmp6 = tmp4 <= tmp5\n    tl.store(out_ptr0 + x2, tmp4, xmask)\n    tl.store(out_ptr1 + x2, tmp6, xmask)\n\n\ndef call(args):\n    primals_1, primals_2, primals_3 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4, 3, 3), (36, 9, 3, 1))\n    assert_size_stride(primals_2, (4, 4, 2, 2), (16, 4, 2, 1))\n    assert_size_stride(primals_3, (4, 4, 3, 3), (36, 9, 3, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = extern_kernels.convolution(primals_2, primals_1, stride=(2, \n            2), padding=(1, 1), dilation=(1, 1), transposed=False,\n            output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf0, (4, 4, 1, 1), (4, 1, 1, 1))\n        buf1 = buf0\n        del buf0\n        get_raw_stream(0)\n        triton_poi_fused_relu_0[grid(16)](buf1, 16, XBLOCK=16, num_warps=1,\n            num_stages=1)\n        buf2 = extern_kernels.convolution(buf1, primals_3, stride=(1, 1),\n            padding=(1, 1), dilation=(1, 1), transposed=False,\n            output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf2, (4, 4, 1, 1), (4, 1, 1, 1))\n        buf3 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.float32)\n        buf4 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.bool)\n        triton_poi_fused_add_relu_threshold_backward_1[grid(64)](buf2,\n            primals_2, buf3, buf4, 64, XBLOCK=64, num_warps=1, num_stages=1)\n        del buf2\n    return buf3, primals_1, primals_2, primals_3, buf1, buf4\n\n\ndef init_layer(L):\n    if isinstance(L, nn.Conv2d):\n        n = L.kernel_size[0] * L.kernel_size[1] * L.out_channels\n        L.weight.data.normal_(0, math.sqrt(2.0 / float(n)))\n    elif isinstance(L, nn.BatchNorm2d):\n        L.weight.data.fill_(1)\n        L.bias.data.fill_(0)\n\n\nclass SimpleBlockNew(nn.Module):\n    maml = False\n\n    def __init__(self, indim, outdim, half_res):\n        super(SimpleBlockNew, self).__init__()\n        self.indim = indim\n        self.outdim = outdim\n        self.C1 = nn.Conv2d(indim, outdim, kernel_size=3, stride=2 if\n            half_res else 1, padding=1, bias=False)\n        self.BN1 = nn.Identity()\n        self.C2 = nn.Conv2d(outdim, outdim, kernel_size=3, padding=1, bias=\n            False)\n        self.BN2 = nn.Identity()\n        self.relu1 = nn.ReLU(inplace=True)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.parametrized_layers = [self.C1, self.C2, self.BN1, self.BN2]\n        self.half_res = half_res\n        if indim != outdim:\n            self.shortcut = nn.Conv2d(indim, outdim, 1, 2 if half_res else \n                1, bias=False)\n            self.BNshortcut = nn.Identity()\n            self.parametrized_layers.append(self.shortcut)\n            self.parametrized_layers.append(self.BNshortcut)\n            self.shortcut_type = '1x1'\n        else:\n            self.shortcut_type = 'identity'\n        for layer in self.parametrized_layers:\n            init_layer(layer)\n\n    def forward(self, input_0):\n        primals_1 = self.C1.weight\n        primals_3 = self.C2.weight\n        primals_2 = input_0\n        output = call([primals_1, primals_2, primals_3])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Aamer98/FeatureNorm&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;SimpleBlock&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:12,&quot;string&quot;:&quot;12&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;MIT&quot;],&quot;string&quot;:&quot;[\n  \&quot;MIT\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;fbf3d3b4cef81b3351347d272eb51b6cdd9f0cc5&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/Aamer98/FeatureNorm/tree/fbf3d3b4cef81b3351347d272eb51b6cdd9f0cc5&quot;}}},{&quot;rowIdx&quot;:11,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Aggregate&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_inference']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/wq/cwq3pmymyuowy4xq7gkn2nwrwrq6bpjfz67zohrfhdkptqayuc4m.py\n# Topologically Sorted Source Nodes: [y], Original ATen: [aten.sum]\n# Source node to ATen node mapping:\n#   y => sum_1\n# Graph fragment:\n#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%arg0_1, [4]), kwargs = {})\ntriton_poi_fused_sum_0 = async_compile.triton('triton_poi_fused_sum_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[256], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_sum_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_sum_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (4*x0), xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + (4*x0)), xmask, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr0 + (2 + (4*x0)), xmask, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr0 + (3 + (4*x0)), xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp4 = tmp2 + tmp3\n    tmp6 = tmp4 + tmp5\n    tl.store(out_ptr0 + (x0), tmp6, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (4, 4, 4, 4, 4), (256, 64, 16, 4, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [y], Original ATen: [aten.sum]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_sum_0.run(arg0_1, buf0, 256, grid=grid(256), stream=stream0)\n        del arg0_1\n    return (buf0, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((4, 4, 4, 4, 4), (256, 64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([arg0_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nfrom torch import nn as nn\n\n\nclass Aggregate(nn.Module):\n    \&quot;\&quot;\&quot;Pooling layer based on sum or average with optional masking.\n\n    Args:\n        axis (int): axis along which pooling is done.\n        mean (bool, optional): if True, use average instead for sum pooling.\n        keepdim (bool, optional): whether the output tensor has dim retained or not.\n\n    \&quot;\&quot;\&quot;\n\n    def __init__(self, axis, mean=False, keepdim=True):\n        super(Aggregate, self).__init__()\n        self.average = mean\n        self.axis = axis\n        self.keepdim = keepdim\n\n    def forward(self, input, mask=None):\n        \&quot;\&quot;\&quot;Compute layer output.\n\n        Args:\n            input (torch.Tensor): input data.\n            mask (torch.Tensor, optional): mask to be applied; e.g. neighbors mask.\n\n        Returns:\n            torch.Tensor: layer output.\n\n        \&quot;\&quot;\&quot;\n        if mask is not None:\n            input = input * mask[..., None]\n        y = torch.sum(input, self.axis)\n        if self.average:\n            if mask is not None:\n                N = torch.sum(mask, self.axis, keepdim=self.keepdim)\n                N = torch.max(N, other=torch.ones_like(N))\n            else:\n                N = input.size(self.axis)\n            y = y / N\n        return y\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 4, 4, 4])]\n\n\ndef get_init_inputs():\n    return [[], {'axis': 4}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch import nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_sum_0(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + 4 * x0, xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 4 * x0), xmask, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr0 + (2 + 4 * x0), xmask, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr0 + (3 + 4 * x0), xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp4 = tmp2 + tmp3\n    tmp6 = tmp4 + tmp5\n    tl.store(out_ptr0 + x0, tmp6, xmask)\n\n\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (4, 4, 4, 4, 4), (256, 64, 16, 4, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_sum_0[grid(256)](arg0_1, buf0, 256, XBLOCK=128,\n            num_warps=4, num_stages=1)\n        del arg0_1\n    return buf0,\n\n\nclass AggregateNew(nn.Module):\n    \&quot;\&quot;\&quot;Pooling layer based on sum or average with optional masking.\n\n    Args:\n        axis (int): axis along which pooling is done.\n        mean (bool, optional): if True, use average instead for sum pooling.\n        keepdim (bool, optional): whether the output tensor has dim retained or not.\n\n    \&quot;\&quot;\&quot;\n\n    def __init__(self, axis, mean=False, keepdim=True):\n        super(AggregateNew, self).__init__()\n        self.average = mean\n        self.axis = axis\n        self.keepdim = keepdim\n\n    def forward(self, input_0):\n        arg0_1 = input_0\n        output = call([arg0_1])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;AlexanderDKazakov/schnetpack&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Aggregate&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:13,&quot;string&quot;:&quot;13&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;MIT&quot;],&quot;string&quot;:&quot;[\n  \&quot;MIT\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;97b82469d977981b500e439a6c93696d8dac8a3f&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/AlexanderDKazakov/schnetpack/tree/97b82469d977981b500e439a6c93696d8dac8a3f&quot;}}},{&quot;rowIdx&quot;:12,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;KaggleAccuracy&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_inference']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/tb/ctbghj42v5gqfb4hjrtagncr6rwduqbnmvrhbowejkjnd2qzwaye.py\n# Topologically Sorted Source Nodes: [x_3], Original ATen: [aten.mean]\n# Source node to ATen node mapping:\n#   x_3 => mean\n# Graph fragment:\n#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%view, [1]), kwargs = {})\ntriton_red_fused_mean_0 = async_compile.triton('triton_red_fused_mean_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.reduction(\n    size_hints=[8192, 512],\n    reduction_hint=ReductionHint.DEFAULT,\n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mean_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 0, 'num_reduction': 1, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}\n)\n@triton.jit\ndef triton_red_fused_mean_0(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):\n    xnumel = 5776\n    rnumel = 484\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex % 1444\n    x1 = (xindex // 1444)\n    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r2 = rindex\n        tmp0 = (11*(x0 // 38)) + ((r2 // 11) % 11)\n        tmp1 = tmp0.to(tl.float32)\n        tmp2 = 0.009569377990430622\n        tmp3 = tmp1 * tmp2\n        tmp4 = tmp3.to(tl.int32)\n        tmp5 = (11*(x0 % 38)) + (r2 % 11)\n        tmp6 = tmp5.to(tl.float32)\n        tmp7 = tmp6 * tmp2\n        tmp8 = tmp7.to(tl.int32)\n        tmp9 = tl.load(in_ptr0 + (tmp8 + (4*tmp4) + (16*(r2 // 121)) + (64*x1)), rmask &amp; xmask, eviction_policy='evict_last', other=0.0)\n        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])\n        tmp12 = _tmp11 + tmp10\n        _tmp11 = tl.where(rmask &amp; xmask, tmp12, _tmp11)\n    tmp11 = tl.sum(_tmp11, 1)[:, None]\n    tl.store(out_ptr0 + (x0 + (1472*x1)), tmp11, xmask)\n''', device_str='cuda')\n\n\n# kernel path: runs/run_shard_6/inductor_cache/q7/cq7ks2q74csjgm3oa6yac2keazqoatflpcmxe7azl6dscnvlb422.py\n# Topologically Sorted Source Nodes: [x_3, gt, x_4, x_8, gt_1, y, eq, float_5, acc], Original ATen: [aten.mean, aten.gt, aten._to_copy, aten.eq]\n# Source node to ATen node mapping:\n#   acc => mean_2\n#   eq => eq\n#   float_5 => convert_element_type_10\n#   gt => gt\n#   gt_1 => gt_1\n#   x_3 => mean\n#   x_4 => convert_element_type_4\n#   x_8 => mean_1\n#   y => convert_element_type_9\n# Graph fragment:\n#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%view, [1]), kwargs = {})\n#   %gt : [num_users=1] = call_function[target=torch.ops.aten.gt.Scalar](args = (%mean, 0.25), kwargs = {})\n#   %convert_element_type_4 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%gt, torch.float32), kwargs = {})\n#   %mean_1 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%view_1, [1]), kwargs = {})\n#   %gt_1 : [num_users=1] = call_function[target=torch.ops.aten.gt.Scalar](args = (%mean_1, 0.25), kwargs = {})\n#   %convert_element_type_9 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%gt_1, torch.float32), kwargs = {})\n#   %eq : [num_users=1] = call_function[target=torch.ops.aten.eq.Tensor](args = (%convert_element_type_4, %convert_element_type_9), kwargs = {})\n#   %convert_element_type_10 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%eq, torch.float32), kwargs = {})\n#   %mean_2 : [num_users=1] = call_function[target=torch.ops.aten.mean.default](args = (%convert_element_type_10,), kwargs = {})\ntriton_red_fused__to_copy_eq_gt_mean_1 = async_compile.triton('triton_red_fused__to_copy_eq_gt_mean_1', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.reduction(\n    size_hints=[1, 8192],\n    reduction_hint=ReductionHint.INNER,\n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {3: 1}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(3,))]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused__to_copy_eq_gt_mean_1', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}\n)\n@triton.jit\ndef triton_red_fused__to_copy_eq_gt_mean_1(in_out_ptr0, in_ptr0, in_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):\n    xnumel = 1\n    rnumel = 5776\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    _tmp13 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r0 = rindex % 1444\n        r1 = (rindex // 1444)\n        tmp0 = tl.load(in_ptr0 + (r0 + (1472*r1)), rmask, eviction_policy='evict_first', other=0.0)\n        tmp6 = tl.load(in_ptr1 + (r0 + (1472*r1)), rmask, eviction_policy='evict_first', other=0.0)\n        tmp1 = 484.0\n        tmp2 = tmp0 / tmp1\n        tmp3 = 0.25\n        tmp4 = tmp2 > tmp3\n        tmp5 = tmp4.to(tl.float32)\n        tmp7 = tmp6 / tmp1\n        tmp8 = tmp7 > tmp3\n        tmp9 = tmp8.to(tl.float32)\n        tmp10 = tmp5 == tmp9\n        tmp11 = tmp10.to(tl.float32)\n        tmp12 = tl.broadcast_to(tmp11, [XBLOCK, RBLOCK])\n        tmp14 = _tmp13 + tmp12\n        _tmp13 = tl.where(rmask, tmp14, _tmp13)\n    tmp13 = tl.sum(_tmp13, 1)[:, None]\n    tmp15 = 5776.0\n    tmp16 = tmp13 / tmp15\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + (tl.full([XBLOCK, 1], 0, tl.int32)), tmp16, None)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(arg1_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 1444), (1472, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_3], Original ATen: [aten.mean]\n        stream0 = get_raw_stream(0)\n        triton_red_fused_mean_0.run(arg0_1, buf0, 5776, 484, grid=grid(5776), stream=stream0)\n        del arg0_1\n        buf1 = empty_strided_cuda((4, 1444), (1472, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_8], Original ATen: [aten.mean]\n        triton_red_fused_mean_0.run(arg1_1, buf1, 5776, 484, grid=grid(5776), stream=stream0)\n        del arg1_1\n        buf2 = empty_strided_cuda((), (), torch.float32)\n        buf3 = buf2; del buf2  # reuse\n        # Topologically Sorted Source Nodes: [x_3, gt, x_4, x_8, gt_1, y, eq, float_5, acc], Original ATen: [aten.mean, aten.gt, aten._to_copy, aten.eq]\n        triton_red_fused__to_copy_eq_gt_mean_1.run(buf3, buf0, buf1, 1, 5776, grid=grid(1), stream=stream0)\n        del buf0\n        del buf1\n    return (buf3, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    arg1_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nfrom torch import Tensor\nfrom torch import nn\n\n\nclass KaggleAccuracy(nn.Module):\n\n    def __init__(self, threshold: 'float'=0.25, num_patches: 'int'=38, size:\n        'int'=418) ->None:\n        super().__init__()\n        self.threshold = threshold\n        self.num_patches = num_patches\n        self.patch_size = size // num_patches\n        self.resize = nn.Upsample(size=size)\n        self.unfold = nn.Unfold(kernel_size=self.patch_size, stride=self.\n            patch_size)\n\n    def forward(self, x: 'Tensor', y: 'Tensor') ->Tensor:\n        x = self.to_patch_value(x)\n        y = self.to_patch_value(y)\n        acc = torch.mean((x == y).float())\n        return acc\n\n    def to_patch_value(self, x: 'Tensor') ->Tensor:\n        x = x.float()\n        x = self.resize(x)\n        x = self.unfold(x)\n        x = x.mean(dim=1)\n        return (x > self.threshold).float()\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])]\n\n\ndef get_init_inputs():\n    return [[], {}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch import Tensor\nfrom torch import nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_red_fused_mean_0(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK: tl.\n    constexpr, RBLOCK: tl.constexpr):\n    xnumel = 5776\n    rnumel = 484\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex % 1444\n    x1 = xindex // 1444\n    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r2 = rindex\n        tmp0 = 11 * (x0 // 38) + r2 // 11 % 11\n        tmp1 = tmp0.to(tl.float32)\n        tmp2 = 0.009569377990430622\n        tmp3 = tmp1 * tmp2\n        tmp4 = tmp3.to(tl.int32)\n        tmp5 = 11 * (x0 % 38) + r2 % 11\n        tmp6 = tmp5.to(tl.float32)\n        tmp7 = tmp6 * tmp2\n        tmp8 = tmp7.to(tl.int32)\n        tmp9 = tl.load(in_ptr0 + (tmp8 + 4 * tmp4 + 16 * (r2 // 121) + 64 *\n            x1), rmask &amp; xmask, eviction_policy='evict_last', other=0.0)\n        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])\n        tmp12 = _tmp11 + tmp10\n        _tmp11 = tl.where(rmask &amp; xmask, tmp12, _tmp11)\n    tmp11 = tl.sum(_tmp11, 1)[:, None]\n    tl.store(out_ptr0 + (x0 + 1472 * x1), tmp11, xmask)\n\n\n@triton.jit\ndef triton_red_fused__to_copy_eq_gt_mean_1(in_out_ptr0, in_ptr0, in_ptr1,\n    xnumel, rnumel, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):\n    rnumel = 5776\n    xoffset = tl.program_id(0) * XBLOCK\n    xoffset + tl.arange(0, XBLOCK)[:, None]\n    tl.full([XBLOCK, RBLOCK], True, tl.int1)\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    _tmp13 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r0 = rindex % 1444\n        r1 = rindex // 1444\n        tmp0 = tl.load(in_ptr0 + (r0 + 1472 * r1), rmask, eviction_policy=\n            'evict_first', other=0.0)\n        tmp6 = tl.load(in_ptr1 + (r0 + 1472 * r1), rmask, eviction_policy=\n            'evict_first', other=0.0)\n        tmp1 = 484.0\n        tmp2 = tmp0 / tmp1\n        tmp3 = 0.25\n        tmp4 = tmp2 > tmp3\n        tmp5 = tmp4.to(tl.float32)\n        tmp7 = tmp6 / tmp1\n        tmp8 = tmp7 > tmp3\n        tmp9 = tmp8.to(tl.float32)\n        tmp10 = tmp5 == tmp9\n        tmp11 = tmp10.to(tl.float32)\n        tmp12 = tl.broadcast_to(tmp11, [XBLOCK, RBLOCK])\n        tmp14 = _tmp13 + tmp12\n        _tmp13 = tl.where(rmask, tmp14, _tmp13)\n    tmp13 = tl.sum(_tmp13, 1)[:, None]\n    tmp15 = 5776.0\n    tmp16 = tmp13 / tmp15\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp16, None)\n\n\ndef call(args):\n    arg0_1, arg1_1 = args\n    args.clear()\n    assert_size_stride(arg0_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(arg1_1, (4, 4, 4, 4), (64, 16, 4, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 1444), (1472, 1), torch.float32)\n        get_raw_stream(0)\n        triton_red_fused_mean_0[grid(5776)](arg0_1, buf0, 5776, 484, XBLOCK\n            =64, RBLOCK=64, num_warps=16, num_stages=1)\n        del arg0_1\n        buf1 = empty_strided_cuda((4, 1444), (1472, 1), torch.float32)\n        triton_red_fused_mean_0[grid(5776)](arg1_1, buf1, 5776, 484, XBLOCK\n            =64, RBLOCK=64, num_warps=16, num_stages=1)\n        del arg1_1\n        buf2 = empty_strided_cuda((), (), torch.float32)\n        buf3 = buf2\n        del buf2\n        triton_red_fused__to_copy_eq_gt_mean_1[grid(1)](buf3, buf0, buf1, 1,\n            5776, XBLOCK=1, RBLOCK=2048, num_warps=16, num_stages=1)\n        del buf0\n        del buf1\n    return buf3,\n\n\nclass KaggleAccuracyNew(nn.Module):\n\n    def __init__(self, threshold: 'float'=0.25, num_patches: 'int'=38, size:\n        'int'=418) ->None:\n        super().__init__()\n        self.threshold = threshold\n        self.num_patches = num_patches\n        self.patch_size = size // num_patches\n        self.resize = nn.Upsample(size=size)\n        self.unfold = nn.Unfold(kernel_size=self.patch_size, stride=self.\n            patch_size)\n\n    def to_patch_value(self, x: 'Tensor') ->Tensor:\n        x = x.float()\n        x = self.resize(x)\n        x = self.unfold(x)\n        x = x.mean(dim=1)\n        return (x > self.threshold).float()\n\n    def forward(self, input_0, input_1):\n        arg0_1 = input_0\n        arg1_1 = input_1\n        output = call([arg0_1, arg1_1])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;AlessandroRuzzi/Computational-Intelligence-Lab-2021&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;KaggleAccuracy&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:14,&quot;string&quot;:&quot;14&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;MIT&quot;],&quot;string&quot;:&quot;[\n  \&quot;MIT\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;ed9dae37618e0ca2f01c4e336df4354e77e00c1f&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/AlessandroRuzzi/Computational-Intelligence-Lab-2021/tree/ed9dae37618e0ca2f01c4e336df4354e77e00c1f&quot;}}},{&quot;rowIdx&quot;:13,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;CosAttention&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/tn/ctnflfpgkxcj4tzximbprecg4kfrgg46s62bv5hdfn6wxh7xshw6.py\n# Topologically Sorted Source Nodes: [mul, mul_1, alpha, alpha_1], Original ATen: [aten.mul, aten.sum]\n# Source node to ATen node mapping:\n#   alpha => mul_2\n#   alpha_1 => sum_1\n#   mul => mul\n#   mul_1 => mul_1\n# Graph fragment:\n#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_2, %primals_1), kwargs = {})\n#   %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul, %primals_3), kwargs = {})\n#   %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1, %primals_4), kwargs = {})\n#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_2, [-1]), kwargs = {})\ntriton_poi_fused_mul_sum_0 = async_compile.triton('triton_poi_fused_mul_sum_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[64], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_sum_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 16, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_mul_sum_0(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 64\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x0 = xindex % 4\n    tmp0 = tl.load(in_ptr0 + (4*x2), xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr1 + (4*x0), xmask, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr2 + (4*x2), xmask, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr3 + (4*x0), xmask, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr0 + (1 + (4*x2)), xmask, eviction_policy='evict_last')\n    tmp8 = tl.load(in_ptr1 + (1 + (4*x0)), xmask, eviction_policy='evict_last')\n    tmp10 = tl.load(in_ptr2 + (1 + (4*x2)), xmask, eviction_policy='evict_last')\n    tmp12 = tl.load(in_ptr3 + (1 + (4*x0)), xmask, eviction_policy='evict_last')\n    tmp15 = tl.load(in_ptr0 + (2 + (4*x2)), xmask, eviction_policy='evict_last')\n    tmp16 = tl.load(in_ptr1 + (2 + (4*x0)), xmask, eviction_policy='evict_last')\n    tmp18 = tl.load(in_ptr2 + (2 + (4*x2)), xmask, eviction_policy='evict_last')\n    tmp20 = tl.load(in_ptr3 + (2 + (4*x0)), xmask, eviction_policy='evict_last')\n    tmp23 = tl.load(in_ptr0 + (3 + (4*x2)), xmask, eviction_policy='evict_last')\n    tmp24 = tl.load(in_ptr1 + (3 + (4*x0)), xmask, eviction_policy='evict_last')\n    tmp26 = tl.load(in_ptr2 + (3 + (4*x2)), xmask, eviction_policy='evict_last')\n    tmp28 = tl.load(in_ptr3 + (3 + (4*x0)), xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 * tmp1\n    tmp4 = tmp2 * tmp3\n    tmp6 = tmp4 * tmp5\n    tmp9 = tmp7 * tmp8\n    tmp11 = tmp9 * tmp10\n    tmp13 = tmp11 * tmp12\n    tmp14 = tmp6 + tmp13\n    tmp17 = tmp15 * tmp16\n    tmp19 = tmp17 * tmp18\n    tmp21 = tmp19 * tmp20\n    tmp22 = tmp14 + tmp21\n    tmp25 = tmp23 * tmp24\n    tmp27 = tmp25 * tmp26\n    tmp29 = tmp27 * tmp28\n    tmp30 = tmp22 + tmp29\n    tl.store(out_ptr0 + (x2), tmp30, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4 = args\n    args.clear()\n    assert_size_stride(primals_1, (1, 4, 4), (16, 4, 1))\n    assert_size_stride(primals_2, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_3, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_4, (1, 4, 4), (16, 4, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4), (16, 4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [mul, mul_1, alpha, alpha_1], Original ATen: [aten.mul, aten.sum]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_mul_sum_0.run(primals_2, primals_1, primals_3, primals_4, buf0, 64, grid=grid(64), stream=stream0)\n    return (buf0, primals_1, primals_2, primals_3, primals_4, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((1, 4, 4), (16, 4, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    primals_4 = rand_strided((1, 4, 4), (16, 4, 1), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3, primals_4])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.nn import Parameter\n\n\nclass ConstAttention(nn.Module):\n\n    def __init__(self, **kwargs):\n        super(ConstAttention, self).__init__()\n\n    def forward(self, neighbor_vecs, self_vecs):\n        return 1\n\n\nclass GatAttention(ConstAttention):\n\n    def __init__(self, num_heads, out_channels):\n        super(GatAttention, self).__init__()\n        self.num_heads = num_heads\n        self.out_channels = out_channels\n        self.att_self_weight = Parameter(torch.Tensor(1, self.num_heads,\n            self.out_channels))\n        self.att_neighbor_weight = Parameter(torch.Tensor(1, self.num_heads,\n            self.out_channels))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        pass\n\n    def forward(self, neighbor_vecs, self_vecs):\n        alpha = (self_vecs * self.att_self_weight).sum(dim=-1) + (neighbor_vecs\n             * self.att_neighbor_weight).sum(dim=-1)\n        alpha = F.leaky_relu(alpha, negative_slope=0.2)\n        return alpha\n\n\nclass CosAttention(GatAttention):\n\n    def forward(self, neighbor_vecs, self_vecs):\n        alpha = (neighbor_vecs * self.att_neighbor_weight * self_vecs *\n            self.att_self_weight)\n        alpha = alpha.sum(dim=-1)\n        return alpha\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])]\n\n\ndef get_init_inputs():\n    return [[], {'num_heads': 4, 'out_channels': 4}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.nn import Parameter\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n\n\n@triton.jit\ndef triton_poi_fused_mul_sum_0(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0,\n    xnumel, XBLOCK: tl.constexpr):\n    xnumel = 64\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x0 = xindex % 4\n    tmp0 = tl.load(in_ptr0 + 4 * x2, xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr1 + 4 * x0, xmask, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr2 + 4 * x2, xmask, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr3 + 4 * x0, xmask, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr0 + (1 + 4 * x2), xmask, eviction_policy='evict_last')\n    tmp8 = tl.load(in_ptr1 + (1 + 4 * x0), xmask, eviction_policy='evict_last')\n    tmp10 = tl.load(in_ptr2 + (1 + 4 * x2), xmask, eviction_policy='evict_last'\n        )\n    tmp12 = tl.load(in_ptr3 + (1 + 4 * x0), xmask, eviction_policy='evict_last'\n        )\n    tmp15 = tl.load(in_ptr0 + (2 + 4 * x2), xmask, eviction_policy='evict_last'\n        )\n    tmp16 = tl.load(in_ptr1 + (2 + 4 * x0), xmask, eviction_policy='evict_last'\n        )\n    tmp18 = tl.load(in_ptr2 + (2 + 4 * x2), xmask, eviction_policy='evict_last'\n        )\n    tmp20 = tl.load(in_ptr3 + (2 + 4 * x0), xmask, eviction_policy='evict_last'\n        )\n    tmp23 = tl.load(in_ptr0 + (3 + 4 * x2), xmask, eviction_policy='evict_last'\n        )\n    tmp24 = tl.load(in_ptr1 + (3 + 4 * x0), xmask, eviction_policy='evict_last'\n        )\n    tmp26 = tl.load(in_ptr2 + (3 + 4 * x2), xmask, eviction_policy='evict_last'\n        )\n    tmp28 = tl.load(in_ptr3 + (3 + 4 * x0), xmask, eviction_policy='evict_last'\n        )\n    tmp2 = tmp0 * tmp1\n    tmp4 = tmp2 * tmp3\n    tmp6 = tmp4 * tmp5\n    tmp9 = tmp7 * tmp8\n    tmp11 = tmp9 * tmp10\n    tmp13 = tmp11 * tmp12\n    tmp14 = tmp6 + tmp13\n    tmp17 = tmp15 * tmp16\n    tmp19 = tmp17 * tmp18\n    tmp21 = tmp19 * tmp20\n    tmp22 = tmp14 + tmp21\n    tmp25 = tmp23 * tmp24\n    tmp27 = tmp25 * tmp26\n    tmp29 = tmp27 * tmp28\n    tmp30 = tmp22 + tmp29\n    tl.store(out_ptr0 + x2, tmp30, xmask)\n\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4 = args\n    args.clear()\n    assert_size_stride(primals_1, (1, 4, 4), (16, 4, 1))\n    assert_size_stride(primals_2, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_3, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_4, (1, 4, 4), (16, 4, 1))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((4, 4, 4), (16, 4, 1), torch.float32)\n        get_raw_stream(0)\n        triton_poi_fused_mul_sum_0[grid(64)](primals_2, primals_1,\n            primals_3, primals_4, buf0, 64, XBLOCK=64, num_warps=1,\n            num_stages=1)\n    return buf0, primals_1, primals_2, primals_3, primals_4\n\n\nclass ConstAttention(nn.Module):\n\n    def __init__(self, **kwargs):\n        super(ConstAttention, self).__init__()\n\n    def forward(self, neighbor_vecs, self_vecs):\n        return 1\n\n\nclass GatAttention(ConstAttention):\n\n    def __init__(self, num_heads, out_channels):\n        super(GatAttention, self).__init__()\n        self.num_heads = num_heads\n        self.out_channels = out_channels\n        self.att_self_weight = Parameter(torch.Tensor(1, self.num_heads,\n            self.out_channels))\n        self.att_neighbor_weight = Parameter(torch.Tensor(1, self.num_heads,\n            self.out_channels))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        pass\n\n    def forward(self, neighbor_vecs, self_vecs):\n        alpha = (self_vecs * self.att_self_weight).sum(dim=-1) + (neighbor_vecs\n             * self.att_neighbor_weight).sum(dim=-1)\n        alpha = F.leaky_relu(alpha, negative_slope=0.2)\n        return alpha\n\n\nclass CosAttentionNew(GatAttention):\n\n    def forward(self, input_0, input_1):\n        primals_1 = self.att_self_weight\n        primals_4 = self.att_neighbor_weight\n        primals_2 = input_0\n        primals_3 = input_1\n        output = call([primals_1, primals_2, primals_3, primals_4])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;AlexMinhao/NAS_GNN&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;CosAttention&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:15,&quot;string&quot;:&quot;15&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;Apache-2.0&quot;],&quot;string&quot;:&quot;[\n  \&quot;Apache-2.0\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;89183988a96e1d6baed910ab3843c13282f8b077&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/AlexMinhao/NAS_GNN/tree/89183988a96e1d6baed910ab3843c13282f8b077&quot;}}},{&quot;rowIdx&quot;:14,&quot;cells&quot;:{&quot;entry_point&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;FullyConnectedNet&quot;},&quot;original_triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;# AOT ID: ['0_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\n\n\n# kernel path: runs/run_shard_6/inductor_cache/r3/cr3febcwm3t44fuoitsx3ou2p6xg4sk4f7unagmmrvffasxf47te.py\n# Topologically Sorted Source Nodes: [x], Original ATen: [aten.relu, aten.threshold_backward]\n# Source node to ATen node mapping:\n#   x => relu\n# Graph fragment:\n#   %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%view_1,), kwargs = {})\n#   %le : [num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu, 0), kwargs = {})\ntriton_poi_fused_relu_threshold_backward_0 = async_compile.triton('triton_poi_fused_relu_threshold_backward_0', '''\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties\n\n@triton_heuristics.pointwise(\n    size_hints=[256], \n    filename=__file__,\n    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},\n    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_relu_threshold_backward_0', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_relu_threshold_backward_0(in_out_ptr0, in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x0 = xindex % 4\n    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)\n    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tmp5 = 0.0\n    tmp6 = tmp4 <= tmp5\n    tl.store(in_out_ptr0 + (x2), tmp4, xmask)\n    tl.store(out_ptr0 + (x2), tmp6, xmask)\n''', device_str='cuda')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4), (4, 1))\n    assert_size_stride(primals_2, (4, ), (1, ))\n    assert_size_stride(primals_3, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_4, (4, 4), (4, 1))\n    assert_size_stride(primals_5, (4, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((64, 4), (4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [], Original ATen: []\n        extern_kernels.mm(reinterpret_tensor(primals_3, (64, 4), (4, 1), 0), reinterpret_tensor(primals_1, (4, 4), (1, 4), 0), out=buf0)\n        del primals_1\n        buf1 = reinterpret_tensor(buf0, (4, 4, 4, 4), (64, 16, 4, 1), 0); del buf0  # reuse\n        buf3 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.bool)\n        # Topologically Sorted Source Nodes: [x], Original ATen: [aten.relu, aten.threshold_backward]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_relu_threshold_backward_0.run(buf1, primals_2, buf3, 256, grid=grid(256), stream=stream0)\n        del primals_2\n        buf2 = empty_strided_cuda((64, 4), (4, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [y_hat], Original ATen: [aten.addmm]\n        extern_kernels.addmm(primals_5, reinterpret_tensor(buf1, (64, 4), (4, 1), 0), reinterpret_tensor(primals_4, (4, 4), (1, 4), 0), alpha=1, beta=1, out=buf2)\n        del primals_5\n    return (reinterpret_tensor(buf2, (4, 4, 4, 4), (64, 16, 4, 1), 0), reinterpret_tensor(primals_3, (64, 4), (4, 1), 0), reinterpret_tensor(buf1, (64, 4), (4, 1), 0), primals_4, buf3, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)\n    primals_4 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)\n    primals_5 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \&quot;__main__\&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n&quot;},&quot;python_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FullyConnectedNet(nn.Module):\n    \&quot;\&quot;\&quot;https://github.com/VainF/Torch-Pruning/issues/21\&quot;\&quot;\&quot;\n\n    def __init__(self, input_size, num_classes, HIDDEN_UNITS):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, HIDDEN_UNITS)\n        self.fc2 = nn.Linear(HIDDEN_UNITS, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        y_hat = self.fc2(x)\n        return y_hat\n\n\ndef get_inputs():\n    return [torch.rand([4, 4, 4, 4])]\n\n\ndef get_init_inputs():\n    return [[], {'input_size': 4, 'num_classes': 4, 'HIDDEN_UNITS': 4}]\n&quot;},&quot;triton_code&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;import torch\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import grid\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nimport torch.nn as nn\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n\n\n@triton.jit\ndef triton_poi_fused_relu_threshold_backward_0(in_out_ptr0, in_ptr0,\n    out_ptr0, xnumel, XBLOCK: tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x0 = xindex % 4\n    tmp0 = tl.load(in_out_ptr0 + x2, xmask)\n    tmp1 = tl.load(in_ptr0 + x0, xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tmp5 = 0.0\n    tmp6 = tmp4 <= tmp5\n    tl.store(in_out_ptr0 + x2, tmp4, xmask)\n    tl.store(out_ptr0 + x2, tmp6, xmask)\n\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5 = args\n    args.clear()\n    assert_size_stride(primals_1, (4, 4), (4, 1))\n    assert_size_stride(primals_2, (4,), (1,))\n    assert_size_stride(primals_3, (4, 4, 4, 4), (64, 16, 4, 1))\n    assert_size_stride(primals_4, (4, 4), (4, 1))\n    assert_size_stride(primals_5, (4,), (1,))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((64, 4), (4, 1), torch.float32)\n        extern_kernels.mm(reinterpret_tensor(primals_3, (64, 4), (4, 1), 0),\n            reinterpret_tensor(primals_1, (4, 4), (1, 4), 0), out=buf0)\n        del primals_1\n        buf1 = reinterpret_tensor(buf0, (4, 4, 4, 4), (64, 16, 4, 1), 0)\n        del buf0\n        buf3 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.bool)\n        get_raw_stream(0)\n        triton_poi_fused_relu_threshold_backward_0[grid(256)](buf1,\n            primals_2, buf3, 256, XBLOCK=256, num_warps=4, num_stages=1)\n        del primals_2\n        buf2 = empty_strided_cuda((64, 4), (4, 1), torch.float32)\n        extern_kernels.addmm(primals_5, reinterpret_tensor(buf1, (64, 4), (\n            4, 1), 0), reinterpret_tensor(primals_4, (4, 4), (1, 4), 0),\n            alpha=1, beta=1, out=buf2)\n        del primals_5\n    return reinterpret_tensor(buf2, (4, 4, 4, 4), (64, 16, 4, 1), 0\n        ), reinterpret_tensor(primals_3, (64, 4), (4, 1), 0\n        ), reinterpret_tensor(buf1, (64, 4), (4, 1), 0), primals_4, buf3\n\n\nclass FullyConnectedNetNew(nn.Module):\n    \&quot;\&quot;\&quot;https://github.com/VainF/Torch-Pruning/issues/21\&quot;\&quot;\&quot;\n\n    def __init__(self, input_size, num_classes, HIDDEN_UNITS):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, HIDDEN_UNITS)\n        self.fc2 = nn.Linear(HIDDEN_UNITS, num_classes)\n\n    def forward(self, input_0):\n        primals_1 = self.fc1.weight\n        primals_2 = self.fc1.bias\n        primals_4 = self.fc2.weight\n        primals_5 = self.fc2.bias\n        primals_3 = input_0\n        output = call([primals_1, primals_2, primals_3, primals_4, primals_5])\n        return output[0]\n&quot;},&quot;repo_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Abhishekvats1997/Torch-Pruning&quot;},&quot;module_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;FullyConnectedNet&quot;},&quot;synthetic&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;false&quot;},&quot;uuid&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:16,&quot;string&quot;:&quot;16&quot;},&quot;licenses&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;MIT&quot;],&quot;string&quot;:&quot;[\n  \&quot;MIT\&quot;\n]&quot;},&quot;stars&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;sha&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;b322a42d1c9032cc9644332d33a9662ca6ed44ac&quot;},&quot;repo_link&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;https://github.com/Abhishekvats1997/Torch-Pruning/tree/b322a42d1c9032cc9644332d33a9662ca6ed44ac&quot;}}}],&quot;truncated&quot;:true},&quot;paginationData&quot;:{&quot;pageIndex&quot;:0,&quot;numItemsPerPage&quot;:100,&quot;numTotalItems&quot;:18162,&quot;offset&quot;:0,&quot;length&quot;:100}},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkxODYyNiwic3ViIjoiL2RhdGFzZXRzL0dQVU1PREUvSW5kdWN0b3JfQ3JlYXRlZF9EYXRhX1Blcm1pc3NpdmUiLCJleHAiOjE3NDI5MjIyMjYsImlzcyI6Imh0dHBzOi8vaHVnZ2luZ2ZhY2UuY28ifQ.JS5bD0txjqMA3sChOV8xqlF9n1tkhWb1nTHUxqB5yUPZ6BeWsP3hEoiTB977lM_FYLyWgdBh4Aii4O8CdacrCw&quot;,&quot;displayUrls&quot;:true},&quot;dataset&quot;:&quot;GPUMODE/Inductor_Created_Data_Permissive&quot;,&quot;isGated&quot;:false,&quot;isPrivate&quot;:false,&quot;hasParquetFormat&quot;:true,&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/608cdf95bf3d1d8d1aca928f/OPvrzyMsSCrzBPfa3VX3-.jpeg&quot;,&quot;fullname&quot;:&quot;GPU MODE&quot;,&quot;name&quot;:&quot;GPUMODE&quot;,&quot;type&quot;:&quot;org&quot;,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;isEnterprise&quot;:false,&quot;followerCount&quot;:5},&quot;compact&quot;:true}"><div class="flex flex-col overflow-hidden shadow-xs mx-auto mb-10 rounded-lg border pt-2  px-2.5"><div class="mb-2 flex flex-wrap items-center gap-2"><div class="mr-auto flex items-center"><svg class="mr-1 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	<div class="whitespace-nowrap font-semibold">Dataset Viewer</div>
	</div>
				<a href="/datasets/GPUMODE/Inductor_Created_Data_Permissive/tree/refs%2Fconvert%2Fparquet/default" class="group mr-1 text-xs text-gray-400 max-sm:hidden"><svg class="text-[.6rem] mr-1 inline -translate-y-px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill="currentColor" d="M12 10H6.78A11 11 0 0 1 27 16h2A13 13 0 0 0 6 7.68V4H4v8h8zm8 12h5.22A11 11 0 0 1 5 16H3a13 13 0 0 0 23 8.32V28h2v-8h-8z"></path></svg>
						<span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Auto-converted</span> to Parquet
					</a>
				<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>API</button>
					<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path d="M9.80603 2.86737H3.56107C3.37704 2.86737 3.20055 2.94048 3.07042 3.0706C2.94029 3.20073 2.86719 3.37723 2.86719 3.56126V9.80622C2.86719 9.99025 2.94029 10.1667 3.07042 10.2969C3.20055 10.427 3.37704 10.5001 3.56107 10.5001H9.80603C9.99006 10.5001 10.1666 10.427 10.2967 10.2969C10.4268 10.1667 10.4999 9.99025 10.4999 9.80622V3.56126C10.4999 3.37723 10.4268 3.20073 10.2967 3.0706C10.1666 2.94048 9.99006 2.86737 9.80603 2.86737Z" fill="currentColor" fill-opacity="0.3"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M2.40942 1.66191C2.05175 1.66191 1.7618 1.95186 1.7618 2.30953V6.76191H1.43799V2.30953C1.43799 1.77303 1.87291 1.3381 2.40942 1.3381H6.45704V1.66191H2.40942Z" fill="currentColor"></path></svg>Embed</button>

					<button class="bg-linear-to-b shadow-xs flex items-center gap-1.5 rounded-full border from-white to-red-100/90 px-2 py-0.5 text-xs font-medium text-[#2D3648] transition-shadow hover:shadow-inner dark:from-gray-900 dark:to-red-800/30 dark:text-gray-100 dark:hover:shadow-inner dark:hover:shadow-red-800/30" ><svg class="h-3.5 w-3.5 text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
						<span>Data Studio</span></button></div>
		<div class="flex flex-1 flex-col overflow-hidden -mx-2.5"><div class="flex flex-1 flex-col overflow-hidden"><div class="flex min-h-0 flex-1"><div class="flex flex-1 flex-col overflow-hidden"><div class="md:-mx-2.5 flex min-w-0 flex-wrap border-t"><div class="flex min-w-0 flex-1 flex-wrap"><div class="grid flex-1 grid-cols-1 overflow-hidden text-sm md:grid-cols-2 md:place-content-center sm:mx-2.5"><label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-950 md:border-r md:border-r-0 hidden" title="default"><span class="text-gray-500">Subset (1)</span>
			<div class="flex items-center whitespace-nowrap"><span class="truncate">default</span>
				<span class="mx-2 text-gray-500">·</span>
					<span class="text-gray-500">18.2k rows</span>
				<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
			<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Subset (1)"><option value="default" selected>default (18.2k rows)</option></optgroup></select></label>
		<label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-900 md:border-r md:border-r" title="train"><div class="text-gray-500">Split (1)</div>
				<div class="flex items-center overflow-hidden whitespace-nowrap"><span class="truncate">train</span>
					<span class="mx-2 text-gray-500">·</span>
						<span class="text-gray-500">18.2k rows</span>
					<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
				<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Split (1)"><option value="train" selected>train (18.2k rows)</option></optgroup></select></label></div></div>
								</div>

							<div class="flex min-h-0 flex-1 flex-col ">
	<div class="bg-linear-to-r text-smd relative flex items-center dark:border-gray-900 dark:bg-gray-950 false border-t [&amp;:has(:focus)]:from-gray-50 [&amp;:has(:focus)]:to-transparent [&amp;:has(:focus)]:to-20% dark:[&amp;:has(:focus)]:from-gray-900"><form class="flex-1"><svg class="absolute left-3 top-1/2 transform -translate-y-1/2 pointer-events-none text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				<input disabled class="outline-hidden h-9 w-full border-none bg-transparent px-1 pl-9 pr-3 placeholder:text-gray-400 " placeholder="Search this dataset" dir="auto"></form>
			<div class="flex items-center gap-2 px-2 py-1"><button type="button" class="hover:bg-yellow-200/70 flex items-center gap-1 rounded-md border border-yellow-200 bg-yellow-100 pl-0.5 pr-1 text-[.8rem] leading-normal text-gray-700 dark:border-orange-500/25 dark:bg-orange-500/20 dark:text-gray-300 dark:hover:brightness-110 hidden"><div class="rounded-sm bg-yellow-300 px-1 font-mono text-[.7rem] font-bold text-black dark:bg-yellow-700 dark:text-gray-200">SQL
	</div>
	Console
</button></div></div>


<div class="flex flex-1 flex-col overflow-hidden min-h-64 border-t">
		

<div class="max-h-96 relative overflow-auto"><table class="w-full table-auto rounded-lg font-mono text-xs text-gray-900"><thead class="shadow-xs sticky left-0 right-0 top-0 z-1 bg-white align-top"><tr class="space-y-54 h-full min-w-fit divide-x border-b text-left"><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">entry_point
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="12.338770085779872" width="11.2" height="17.66122991422013" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="13.19946840642745" width="11.2" height="16.80053159357255" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="22.060891627401233" width="11.2" height="7.939108372598767" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">1</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">65</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">original_triton_code
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">4.5k</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">619k</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">python_code
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">208</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">60.9k</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">triton_code
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">1.15k</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">275k</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">repo_name
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="12.162721893491124" width="11.2" height="17.837278106508876" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="16.2396449704142" width="11.2" height="13.7603550295858" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="22.931952662721894" width="11.2" height="7.068047337278106" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">7</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">115</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">module_name
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="12.338770085779872" width="11.2" height="17.66122991422013" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="13.19946840642745" width="11.2" height="16.80053159357255" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="22.060891627401233" width="11.2" height="7.939108372598767" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">1</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">65</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">synthetic
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>bool</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><defs><clipPath id="rounded-bar"><rect x="0" y="0" width="130" height="8" rx="4"></rect></clipPath><pattern id="hatching" patternUnits="userSpaceOnUse" patternTransform="rotate(-45)" height="1" width="5"><line y1="0" class="stroke-gray-400 dark:stroke-gray-500/80" stroke-width="3" y2="1" x1="2" x2="2"></line></pattern><pattern id="hatching-faded" patternUnits="userSpaceOnUse" patternTransform="rotate(-45)" height="1" width="5"><line y1="0" class="stroke-gray-100 dark:stroke-gray-500/20" stroke-width="3" y2="1" x1="2" x2="2"></line></pattern></defs><g height="8" style="transform: translateY(20px)" clip-path="url(#rounded-bar)"><g style="transform: scaleX(1.0153846153846153) translateX(-1px)"><g><rect class="fill-indigo-500 dark:fill-indigo-600/80" x="1" y="0" width="128" height="8" fill-opacity="1"></rect></g></g></g><g style="transform: scaleX(1.0153846153846153) translateX(-1px)"><g><rect class="fill-white cursor-pointer" x="0" y="0" width="130" height="28" fill-opacity="0"></rect></g></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 max-w-full overflow-hidden text-ellipsis whitespace-nowrap">1
				class</div></div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">uuid
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>int64</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="0" y="0.5670665212649943" width="11.2" height="29.432933478735006" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="13.2" y="0.2410032715376218" width="11.2" height="29.758996728462378" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="26.4" y="0.3544165757906228" width="11.2" height="29.645583424209377" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="39.599999999999994" y="0.1417666303162477" width="11.2" height="29.858233369683752" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="52.8" y="0.41112322791711975" width="11.2" height="29.58887677208288" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="66" y="0.11341330425299745" width="11.2" height="29.886586695747003" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="79.19999999999999" y="0.22682660850599845" width="11.2" height="29.773173391494" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="92.39999999999999" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="105.6" y="0.1559432933478746" width="11.2" height="29.844056706652125" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="118.8" y="0.31188658669574565" width="11.2" height="29.688113413304254" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">0</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">18.5k</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">licenses
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>sequence</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="20" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="22" y="25" width="20" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="44" y="25" width="20" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="25" width="20" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="88" y="26" width="20" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="110" y="25" width="20" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="22" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="21" y="0" width="22" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="43" y="0" width="22" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="22" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="87" y="0" width="22" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="109" y="0" width="22" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">1</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">6</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">stars
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>int64</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="0" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="13.2" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="26.4" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="39.599999999999994" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="66" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="79.19999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="92.39999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">0</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">19.8k</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">sha
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="130" height="30" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="132" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">40</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">40</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">repo_link
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="12.162721893491124" width="11.2" height="17.837278106508876" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="16.2396449704142" width="11.2" height="13.7603550295858" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="22.931952662721894" width="11.2" height="7.068047337278106" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">72</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">180</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th></tr></thead>
			<tbody class="h-16 overflow-scroll"><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="0"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">SumAggregator</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_inference']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/7o/c7otc5ij6whexgxcr56vlxp2l7hzg3oc4onljp557uc6wncu5gvg.py
# Topologically Sorted Source Nodes: [sum_1], Original ATen: [aten.sum]
# Source node to ATen node mapping:
#   sum_1 => sum_1
# Graph fragment:
#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%arg0_1, [1]), kwargs = {})
triton_poi_fused_sum_0 = async_compile.triton('triton_poi_fused_sum_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[64], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_sum_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_sum_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex % 16
    x1 = (xindex // 16)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (64*x1)), xmask)
    tmp1 = tl.load(in_ptr0 + (16 + x0 + (64*x1)), xmask)
    tmp3 = tl.load(in_ptr0 + (32 + x0 + (64*x1)), xmask)
    tmp5 = tl.load(in_ptr0 + (48 + x0 + (64*x1)), xmask)
    tmp2 = tmp0 + tmp1
    tmp4 = tmp2 + tmp3
    tmp6 = tmp4 + tmp5
    tl.store(out_ptr0 + (x2), tmp6, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, = args
    args.clear()
    assert_size_stride(arg0_1, (4, 4, 4, 4), (64, 16, 4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4), (16, 4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [sum_1], Original ATen: [aten.sum]
        stream0 = get_raw_stream(0)
        triton_poi_fused_sum_0.run(arg0_1, buf0, 64, grid=grid(64), stream=stream0)
        del arg0_1
    return (buf0, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([arg0_1])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import torch.nn as nn


class SumAggregator(nn.Module):

    def __init__(self):
        super(SumAggregator, self).__init__()

    def forward(self, neighbor):
        return torch.sum(neighbor, dim=1)


def get_inputs():
    return [torch.rand([4, 4, 4, 4])]


def get_init_inputs():
    return [[], {}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch.nn as nn
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda


@triton.jit
def triton_poi_fused_sum_0(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):
    xnumel = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex % 16
    x1 = xindex // 16
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 64 * x1), xmask)
    tmp1 = tl.load(in_ptr0 + (16 + x0 + 64 * x1), xmask)
    tmp3 = tl.load(in_ptr0 + (32 + x0 + 64 * x1), xmask)
    tmp5 = tl.load(in_ptr0 + (48 + x0 + 64 * x1), xmask)
    tmp2 = tmp0 + tmp1
    tmp4 = tmp2 + tmp3
    tmp6 = tmp4 + tmp5
    tl.store(out_ptr0 + x2, tmp6, xmask)


def call(args):
    arg0_1, = args
    args.clear()
    assert_size_stride(arg0_1, (4, 4, 4, 4), (64, 16, 4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4), (16, 4, 1), torch.float32)
        get_raw_stream(0)
        triton_poi_fused_sum_0[grid(64)](arg0_1, buf0, 64, XBLOCK=64,
            num_warps=1, num_stages=1)
        del arg0_1
    return buf0,


class SumAggregatorNew(nn.Module):

    def __init__(self):
        super(SumAggregatorNew, self).__init__()

    def forward(self, input_0):
        arg0_1 = input_0
        output = call([arg0_1])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">AlexMinhao/NAS_GNN</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">SumAggregator</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "Apache-2.0"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">89183988a96e1d6baed910ab3843c13282f8b077</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/AlexMinhao/NAS_GNN/tree/89183988a96e1d6baed910ab3843c13282f8b077</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="1"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">LinearEmbedding</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_forward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/xz/cxz7i3qbiizfbbzvas22bbwy5nxzvmtfdg5vhhiye56dk4hdonst.py
# Topologically Sorted Source Nodes: [mul], Original ATen: [aten.mul]
# Source node to ATen node mapping:
#   mul => mul
# Graph fragment:
#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_1, 2.0), kwargs = {})
triton_poi_fused_mul_0 = async_compile.triton('triton_poi_fused_mul_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[256], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_0', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_mul_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x2 = xindex
    x0 = xindex % 4
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp3 = 2.0
    tmp4 = tmp2 * tmp3
    tl.store(in_out_ptr0 + (x2), tmp4, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    primals_1, primals_2, primals_3 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4), (4, 1))
    assert_size_stride(primals_2, (4, ), (1, ))
    assert_size_stride(primals_3, (4, 4, 4, 4), (64, 16, 4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((64, 4), (4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(reinterpret_tensor(primals_3, (64, 4), (4, 1), 0), reinterpret_tensor(primals_1, (4, 4), (1, 4), 0), out=buf0)
        del primals_1
        buf1 = reinterpret_tensor(buf0, (4, 4, 4, 4), (64, 16, 4, 1), 0); del buf0  # reuse
        # Topologically Sorted Source Nodes: [mul], Original ATen: [aten.mul]
        stream0 = get_raw_stream(0)
        triton_poi_fused_mul_0.run(buf1, primals_2, 256, grid=grid(256), stream=stream0)
        del primals_2
    return (buf1, reinterpret_tensor(primals_3, (64, 4), (4, 1), 0), )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)
    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_3 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([primals_1, primals_2, primals_3])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import math
import torch
import torch.utils.data
import torch.nn as nn


class LinearEmbedding(nn.Module):

    def __init__(self, inp_size, d_model):
        super(LinearEmbedding, self).__init__()
        self.lut = nn.Linear(inp_size, d_model)
        self.d_model = d_model

    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)


def get_inputs():
    return [torch.rand([4, 4, 4, 4])]


def get_init_inputs():
    return [[], {'inp_size': 4, 'd_model': 4}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
from torch._inductor.select_algorithm import extern_kernels
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch.utils.data
import torch.nn as nn
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor


@triton.jit
def triton_poi_fused_mul_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK: tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x2 = xindex
    x0 = xindex % 4
    tmp0 = tl.load(in_out_ptr0 + x2, xmask)
    tmp1 = tl.load(in_ptr0 + x0, xmask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp3 = 2.0
    tmp4 = tmp2 * tmp3
    tl.store(in_out_ptr0 + x2, tmp4, xmask)


def call(args):
    primals_1, primals_2, primals_3 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4), (4, 1))
    assert_size_stride(primals_2, (4,), (1,))
    assert_size_stride(primals_3, (4, 4, 4, 4), (64, 16, 4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((64, 4), (4, 1), torch.float32)
        extern_kernels.mm(reinterpret_tensor(primals_3, (64, 4), (4, 1), 0),
            reinterpret_tensor(primals_1, (4, 4), (1, 4), 0), out=buf0)
        del primals_1
        buf1 = reinterpret_tensor(buf0, (4, 4, 4, 4), (64, 16, 4, 1), 0)
        del buf0
        get_raw_stream(0)
        triton_poi_fused_mul_0[grid(256)](buf1, primals_2, 256, XBLOCK=256,
            num_warps=4, num_stages=1)
        del primals_2
    return buf1, reinterpret_tensor(primals_3, (64, 4), (4, 1), 0)


class LinearEmbeddingNew(nn.Module):

    def __init__(self, inp_size, d_model):
        super(LinearEmbeddingNew, self).__init__()
        self.lut = nn.Linear(inp_size, d_model)
        self.d_model = d_model

    def forward(self, input_0):
        primals_1 = self.lut.weight
        primals_2 = self.lut.bias
        primals_3 = input_0
        output = call([primals_1, primals_2, primals_3])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Akhil-Raj/Trajectory-Transformer</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">LinearEmbedding</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "MIT"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dd09fda99443f6afb59d962026573162219ea6a9</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/Akhil-Raj/Trajectory-Transformer/tree/dd09fda99443f6afb59d962026573162219ea6a9</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="2"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">CustomizeLayer</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_forward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/ne/cnem3ewvyvy2ry3vceibv7j67edhj5jen3qlzvzibg5tpkhxkwsq.py
# Topologically Sorted Source Nodes: [pow_1, sum_1, norm, x, mul, add], Original ATen: [aten.pow, aten.sum, aten.sqrt, aten.div, aten.mul, aten.add]
# Source node to ATen node mapping:
#   add => add
#   mul => mul
#   norm => sqrt
#   pow_1 => pow_1
#   sum_1 => sum_1
#   x => div
# Graph fragment:
#   %pow_1 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%primals_1, 2), kwargs = {})
#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%pow_1, [1], True), kwargs = {})
#   %sqrt : [num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%sum_1,), kwargs = {})
#   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%primals_1, %sqrt), kwargs = {})
#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div, %primals_2), kwargs = {})
#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul, %primals_3), kwargs = {})
triton_poi_fused_add_div_mul_pow_sqrt_sum_0 = async_compile.triton('triton_poi_fused_add_div_mul_pow_sqrt_sum_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[256], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_div_mul_pow_sqrt_sum_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_div_mul_pow_sqrt_sum_0(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x4 = xindex
    x3 = (xindex // 64)
    x5 = xindex % 16
    x0 = xindex % 4
    tmp0 = tl.load(in_ptr0 + (x4), xmask)
    tmp1 = tl.load(in_ptr0 + (x5 + (64*x3)), xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr0 + (16 + x5 + (64*x3)), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr0 + (32 + x5 + (64*x3)), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr0 + (48 + x5 + (64*x3)), xmask, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1 * tmp1
    tmp4 = tmp3 * tmp3
    tmp5 = tmp2 + tmp4
    tmp7 = tmp6 * tmp6
    tmp8 = tmp5 + tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 + tmp10
    tmp12 = libdevice.sqrt(tmp11)
    tmp13 = tmp0 / tmp12
    tmp15 = tmp13 * tmp14
    tmp17 = tmp15 + tmp16
    tl.store(out_ptr0 + (x4), tmp17, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    primals_1, primals_2, primals_3 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_2, (4, ), (1, ))
    assert_size_stride(primals_3, (4, ), (1, ))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [pow_1, sum_1, norm, x, mul, add], Original ATen: [aten.pow, aten.sum, aten.sqrt, aten.div, aten.mul, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_div_mul_pow_sqrt_sum_0.run(primals_1, primals_2, primals_3, buf0, 256, grid=grid(256), stream=stream0)
        del primals_2
        del primals_3
    return (buf0, primals_1, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_3 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([primals_1, primals_2, primals_3])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import torch.nn as nn


class CustomizeLayer(nn.Module):

    def __init__(self, in_dim):
        super().__init__()
        self.in_dim = in_dim
        self.scale = nn.Parameter(torch.Tensor(self.in_dim))
        self.bias = nn.Parameter(torch.Tensor(self.in_dim))

    def forward(self, x):
        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()
        x = torch.div(x, norm)
        return x * self.scale + self.bias

    def __repr__(self):
        return 'CustomizedLayer(in_dim=%d)' % self.in_dim


def get_inputs():
    return [torch.rand([4, 4, 4, 4])]


def get_init_inputs():
    return [[], {'in_dim': 4}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._inductor.runtime.triton_helpers import libdevice
import torch.nn as nn
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda


@triton.jit
def triton_poi_fused_add_div_mul_pow_sqrt_sum_0(in_ptr0, in_ptr1, in_ptr2,
    out_ptr0, xnumel, XBLOCK: tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x4 = xindex
    x3 = xindex // 64
    x5 = xindex % 16
    x0 = xindex % 4
    tmp0 = tl.load(in_ptr0 + x4, xmask)
    tmp1 = tl.load(in_ptr0 + (x5 + 64 * x3), xmask, eviction_policy=
        'evict_last')
    tmp3 = tl.load(in_ptr0 + (16 + x5 + 64 * x3), xmask, eviction_policy=
        'evict_last')
    tmp6 = tl.load(in_ptr0 + (32 + x5 + 64 * x3), xmask, eviction_policy=
        'evict_last')
    tmp9 = tl.load(in_ptr0 + (48 + x5 + 64 * x3), xmask, eviction_policy=
        'evict_last')
    tmp14 = tl.load(in_ptr1 + x0, xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr2 + x0, xmask, eviction_policy='evict_last')
    tmp2 = tmp1 * tmp1
    tmp4 = tmp3 * tmp3
    tmp5 = tmp2 + tmp4
    tmp7 = tmp6 * tmp6
    tmp8 = tmp5 + tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 + tmp10
    tmp12 = libdevice.sqrt(tmp11)
    tmp13 = tmp0 / tmp12
    tmp15 = tmp13 * tmp14
    tmp17 = tmp15 + tmp16
    tl.store(out_ptr0 + x4, tmp17, xmask)


def call(args):
    primals_1, primals_2, primals_3 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_2, (4,), (1,))
    assert_size_stride(primals_3, (4,), (1,))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        get_raw_stream(0)
        triton_poi_fused_add_div_mul_pow_sqrt_sum_0[grid(256)](primals_1,
            primals_2, primals_3, buf0, 256, XBLOCK=128, num_warps=4,
            num_stages=1)
        del primals_2
        del primals_3
    return buf0, primals_1


class CustomizeLayerNew(nn.Module):

    def __init__(self, in_dim):
        super().__init__()
        self.in_dim = in_dim
        self.scale = nn.Parameter(torch.Tensor(self.in_dim))
        self.bias = nn.Parameter(torch.Tensor(self.in_dim))

    def __repr__(self):
        return 'CustomizedLayer(in_dim=%d)' % self.in_dim

    def forward(self, input_0):
        primals_2 = self.scale
        primals_3 = self.bias
        primals_1 = input_0
        output = call([primals_1, primals_2, primals_3])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Abhishekvats1997/Torch-Pruning</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">CustomizeLayer</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "MIT"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">b322a42d1c9032cc9644332d33a9662ca6ed44ac</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/Abhishekvats1997/Torch-Pruning/tree/b322a42d1c9032cc9644332d33a9662ca6ed44ac</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="3"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">LayerNorm</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_forward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/yq/cyqficfzvi7m26ugfmcjnrvh3fwc36rw64jccffjyl3m2zuwe2to.py
# Topologically Sorted Source Nodes: [mean, std, sub, mul, add, truediv, add_1], Original ATen: [aten.mean, aten.std, aten.sub, aten.mul, aten.add, aten.div]
# Source node to ATen node mapping:
#   add => add
#   add_1 => add_1
#   mean => mean
#   mul => mul
#   std => sqrt, var
#   sub => sub
#   truediv => div
# Graph fragment:
#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%primals_1, [-1], True), kwargs = {})
#   %var : [num_users=1] = call_function[target=torch.ops.aten.var.correction](args = (%primals_1, [-1]), kwargs = {correction: 1.0, keepdim: True})
#   %sqrt : [num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%var,), kwargs = {})
#   %sub : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%primals_1, %mean), kwargs = {})
#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_2, %sub), kwargs = {})
#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%sqrt, 1e-05), kwargs = {})
#   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%mul, %add), kwargs = {})
#   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div, %primals_3), kwargs = {})
triton_poi_fused_add_div_mean_mul_std_sub_0 = async_compile.triton('triton_poi_fused_add_div_mean_mul_std_sub_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[256], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_div_mean_mul_std_sub_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_div_mean_mul_std_sub_0(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex % 4
    x2 = xindex
    x1 = (xindex // 4)
    tmp0 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2), xmask)
    tmp2 = tl.load(in_ptr1 + (4*x1), xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (1 + (4*x1)), xmask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr1 + (2 + (4*x1)), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr1 + (3 + (4*x1)), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp4 = tmp2 + tmp3
    tmp6 = tmp4 + tmp5
    tmp8 = tmp6 + tmp7
    tmp9 = 4.0
    tmp10 = tmp8 / tmp9
    tmp11 = tmp1 - tmp10
    tmp12 = tmp0 * tmp11
    tmp13 = tmp2 - tmp10
    tmp14 = tmp13 * tmp13
    tmp15 = tmp3 - tmp10
    tmp16 = tmp15 * tmp15
    tmp17 = tmp14 + tmp16
    tmp18 = tmp5 - tmp10
    tmp19 = tmp18 * tmp18
    tmp20 = tmp17 + tmp19
    tmp21 = tmp7 - tmp10
    tmp22 = tmp21 * tmp21
    tmp23 = tmp20 + tmp22
    tmp24 = 3.0
    tmp25 = tmp23 / tmp24
    tmp26 = libdevice.sqrt(tmp25)
    tmp27 = 1e-05
    tmp28 = tmp26 + tmp27
    tmp29 = tmp12 / tmp28
    tmp31 = tmp29 + tmp30
    tl.store(out_ptr0 + (x2), tmp31, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    primals_1, primals_2, primals_3 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_2, (4, ), (1, ))
    assert_size_stride(primals_3, (4, ), (1, ))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [mean, std, sub, mul, add, truediv, add_1], Original ATen: [aten.mean, aten.std, aten.sub, aten.mul, aten.add, aten.div]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_div_mean_mul_std_sub_0.run(primals_2, primals_1, primals_3, buf0, 256, grid=grid(256), stream=stream0)
        del primals_2
        del primals_3
    return (buf0, primals_1, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_3 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([primals_1, primals_2, primals_3])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import torch.nn as nn


class LayerNorm(nn.Module):

    def __init__(self, weights, eps=1e-05):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(weights))
        self.beta = nn.Parameter(torch.zeros(weights))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta


def get_inputs():
    return [torch.rand([4, 4, 4, 4])]


def get_init_inputs():
    return [[], {'weights': 4}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._inductor.runtime.triton_helpers import libdevice
import torch.nn as nn
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda


@triton.jit
def triton_poi_fused_add_div_mean_mul_std_sub_0(in_ptr0, in_ptr1, in_ptr2,
    out_ptr0, xnumel, XBLOCK: tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex % 4
    x2 = xindex
    x1 = xindex // 4
    tmp0 = tl.load(in_ptr0 + x0, xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + x2, xmask)
    tmp2 = tl.load(in_ptr1 + 4 * x1, xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (1 + 4 * x1), xmask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr1 + (2 + 4 * x1), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr1 + (3 + 4 * x1), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr2 + x0, xmask, eviction_policy='evict_last')
    tmp4 = tmp2 + tmp3
    tmp6 = tmp4 + tmp5
    tmp8 = tmp6 + tmp7
    tmp9 = 4.0
    tmp10 = tmp8 / tmp9
    tmp11 = tmp1 - tmp10
    tmp12 = tmp0 * tmp11
    tmp13 = tmp2 - tmp10
    tmp14 = tmp13 * tmp13
    tmp15 = tmp3 - tmp10
    tmp16 = tmp15 * tmp15
    tmp17 = tmp14 + tmp16
    tmp18 = tmp5 - tmp10
    tmp19 = tmp18 * tmp18
    tmp20 = tmp17 + tmp19
    tmp21 = tmp7 - tmp10
    tmp22 = tmp21 * tmp21
    tmp23 = tmp20 + tmp22
    tmp24 = 3.0
    tmp25 = tmp23 / tmp24
    tmp26 = libdevice.sqrt(tmp25)
    tmp27 = 1e-05
    tmp28 = tmp26 + tmp27
    tmp29 = tmp12 / tmp28
    tmp31 = tmp29 + tmp30
    tl.store(out_ptr0 + x2, tmp31, xmask)


def call(args):
    primals_1, primals_2, primals_3 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_2, (4,), (1,))
    assert_size_stride(primals_3, (4,), (1,))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        get_raw_stream(0)
        triton_poi_fused_add_div_mean_mul_std_sub_0[grid(256)](primals_2,
            primals_1, primals_3, buf0, 256, XBLOCK=256, num_warps=4,
            num_stages=1)
        del primals_2
        del primals_3
    return buf0, primals_1


class LayerNormNew(nn.Module):

    def __init__(self, weights, eps=1e-05):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(weights))
        self.beta = nn.Parameter(torch.zeros(weights))
        self.eps = eps

    def forward(self, input_0):
        primals_2 = self.gamma
        primals_3 = self.beta
        primals_1 = input_0
        output = call([primals_1, primals_2, primals_3])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">AWilcke/Dissertation</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">LayerNorm</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">3</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "MIT"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">b85ad38a7f336ee290d5883f5e942f54e140d0d0</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/AWilcke/Dissertation/tree/b85ad38a7f336ee290d5883f5e942f54e140d0d0</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="4"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">LayerNorm</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_forward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/df/cdfcie57v6pcdd6oeaz4mvlgksxgyuxzmlv5bklwemyulqhtcxta.py
# Topologically Sorted Source Nodes: [mean, std, sub, mul, add, truediv, add_1], Original ATen: [aten.mean, aten.std, aten.sub, aten.mul, aten.add, aten.div]
# Source node to ATen node mapping:
#   add => add
#   add_1 => add_1
#   mean => mean
#   mul => mul
#   std => sqrt, var
#   sub => sub
#   truediv => div
# Graph fragment:
#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%primals_1, [-1], True), kwargs = {})
#   %var : [num_users=1] = call_function[target=torch.ops.aten.var.correction](args = (%primals_1, [-1]), kwargs = {correction: 1.0, keepdim: True})
#   %sqrt : [num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%var,), kwargs = {})
#   %sub : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%primals_1, %mean), kwargs = {})
#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_2, %sub), kwargs = {})
#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%sqrt, 1e-06), kwargs = {})
#   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%mul, %add), kwargs = {})
#   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div, %primals_3), kwargs = {})
triton_poi_fused_add_div_mean_mul_std_sub_0 = async_compile.triton('triton_poi_fused_add_div_mean_mul_std_sub_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[256], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_div_mean_mul_std_sub_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_div_mean_mul_std_sub_0(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex % 4
    x2 = xindex
    x1 = (xindex // 4)
    tmp0 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2), xmask)
    tmp2 = tl.load(in_ptr1 + (4*x1), xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (1 + (4*x1)), xmask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr1 + (2 + (4*x1)), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr1 + (3 + (4*x1)), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp4 = tmp2 + tmp3
    tmp6 = tmp4 + tmp5
    tmp8 = tmp6 + tmp7
    tmp9 = 4.0
    tmp10 = tmp8 / tmp9
    tmp11 = tmp1 - tmp10
    tmp12 = tmp0 * tmp11
    tmp13 = tmp2 - tmp10
    tmp14 = tmp13 * tmp13
    tmp15 = tmp3 - tmp10
    tmp16 = tmp15 * tmp15
    tmp17 = tmp14 + tmp16
    tmp18 = tmp5 - tmp10
    tmp19 = tmp18 * tmp18
    tmp20 = tmp17 + tmp19
    tmp21 = tmp7 - tmp10
    tmp22 = tmp21 * tmp21
    tmp23 = tmp20 + tmp22
    tmp24 = 3.0
    tmp25 = tmp23 / tmp24
    tmp26 = libdevice.sqrt(tmp25)
    tmp27 = 1e-06
    tmp28 = tmp26 + tmp27
    tmp29 = tmp12 / tmp28
    tmp31 = tmp29 + tmp30
    tl.store(out_ptr0 + (x2), tmp31, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    primals_1, primals_2, primals_3 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_2, (4, ), (1, ))
    assert_size_stride(primals_3, (4, ), (1, ))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [mean, std, sub, mul, add, truediv, add_1], Original ATen: [aten.mean, aten.std, aten.sub, aten.mul, aten.add, aten.div]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_div_mean_mul_std_sub_0.run(primals_2, primals_1, primals_3, buf0, 256, grid=grid(256), stream=stream0)
        del primals_2
        del primals_3
    return (buf0, primals_1, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_3 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([primals_1, primals_2, primals_3])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import torch.utils.data
import torch.nn as nn


class LayerNorm(nn.Module):
    """
    Construct a layernorm module (See citation for details).
    """

    def __init__(self, features, eps=1e-06):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        std = x.std(dim=-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2


def get_inputs():
    return [torch.rand([4, 4, 4, 4])]


def get_init_inputs():
    return [[], {'features': 4}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._inductor.runtime.triton_helpers import libdevice
import torch.utils.data
import torch.nn as nn
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda


@triton.jit
def triton_poi_fused_add_div_mean_mul_std_sub_0(in_ptr0, in_ptr1, in_ptr2,
    out_ptr0, xnumel, XBLOCK: tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex % 4
    x2 = xindex
    x1 = xindex // 4
    tmp0 = tl.load(in_ptr0 + x0, xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + x2, xmask)
    tmp2 = tl.load(in_ptr1 + 4 * x1, xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (1 + 4 * x1), xmask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr1 + (2 + 4 * x1), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr1 + (3 + 4 * x1), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr2 + x0, xmask, eviction_policy='evict_last')
    tmp4 = tmp2 + tmp3
    tmp6 = tmp4 + tmp5
    tmp8 = tmp6 + tmp7
    tmp9 = 4.0
    tmp10 = tmp8 / tmp9
    tmp11 = tmp1 - tmp10
    tmp12 = tmp0 * tmp11
    tmp13 = tmp2 - tmp10
    tmp14 = tmp13 * tmp13
    tmp15 = tmp3 - tmp10
    tmp16 = tmp15 * tmp15
    tmp17 = tmp14 + tmp16
    tmp18 = tmp5 - tmp10
    tmp19 = tmp18 * tmp18
    tmp20 = tmp17 + tmp19
    tmp21 = tmp7 - tmp10
    tmp22 = tmp21 * tmp21
    tmp23 = tmp20 + tmp22
    tmp24 = 3.0
    tmp25 = tmp23 / tmp24
    tmp26 = libdevice.sqrt(tmp25)
    tmp27 = 1e-06
    tmp28 = tmp26 + tmp27
    tmp29 = tmp12 / tmp28
    tmp31 = tmp29 + tmp30
    tl.store(out_ptr0 + x2, tmp31, xmask)


def call(args):
    primals_1, primals_2, primals_3 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_2, (4,), (1,))
    assert_size_stride(primals_3, (4,), (1,))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        get_raw_stream(0)
        triton_poi_fused_add_div_mean_mul_std_sub_0[grid(256)](primals_2,
            primals_1, primals_3, buf0, 256, XBLOCK=256, num_warps=4,
            num_stages=1)
        del primals_2
        del primals_3
    return buf0, primals_1


class LayerNormNew(nn.Module):
    """
    Construct a layernorm module (See citation for details).
    """

    def __init__(self, features, eps=1e-06):
        super(LayerNormNew, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, input_0):
        primals_2 = self.a_2
        primals_3 = self.b_2
        primals_1 = input_0
        output = call([primals_1, primals_2, primals_3])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Akhil-Raj/Trajectory-Transformer</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">LayerNorm</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">4</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "MIT"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">dd09fda99443f6afb59d962026573162219ea6a9</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/Akhil-Raj/Trajectory-Transformer/tree/dd09fda99443f6afb59d962026573162219ea6a9</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="5"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Norm</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_forward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/dg/cdgw6x7nju4bzp2wyuwgeanbco7zcjis6yiusovvnpz6zw3yjd3l.py
# Topologically Sorted Source Nodes: [u, sub], Original ATen: [aten.mean, aten.sub]
# Source node to ATen node mapping:
#   sub => sub
#   u => mean
# Graph fragment:
#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%primals_1, [-1], True), kwargs = {})
#   %sub : [num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%primals_1, %mean), kwargs = {})
triton_poi_fused_mean_sub_0 = async_compile.triton('triton_poi_fused_mean_sub_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[256], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mean_sub_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_mean_sub_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x2 = xindex
    x1 = (xindex // 4)
    tmp0 = tl.load(in_ptr0 + (x2), xmask)
    tmp1 = tl.load(in_ptr0 + (4*x1), xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr0 + (1 + (4*x1)), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr0 + (2 + (4*x1)), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr0 + (3 + (4*x1)), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp5 = tmp3 + tmp4
    tmp7 = tmp5 + tmp6
    tmp8 = 4.0
    tmp9 = tmp7 / tmp8
    tmp10 = tmp0 - tmp9
    tl.store(out_ptr0 + (x2), tmp10, xmask)
''', device_str='cuda')


# kernel path: runs/run_shard_6/inductor_cache/v3/cv3kidzozprmsfiixppams3wqi7ij566hegofbo3ur5gfdrhxiik.py
# Topologically Sorted Source Nodes: [square, s, add, rsqrt, x, mul_1, x_1], Original ATen: [aten.pow, aten.mean, aten.add, aten.rsqrt, aten.mul]
# Source node to ATen node mapping:
#   add => add
#   mul_1 => mul_1
#   rsqrt => rsqrt
#   s => mean_1
#   square => pow_1
#   x => mul
#   x_1 => add_1
# Graph fragment:
#   %pow_1 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%sub, 2), kwargs = {})
#   %mean_1 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_1, [-1], True), kwargs = {})
#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean_1, 1e-05), kwargs = {})
#   %rsqrt : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add,), kwargs = {})
#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub, %rsqrt), kwargs = {})
#   %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul, %primals_2), kwargs = {})
#   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1, %primals_3), kwargs = {})
triton_poi_fused_add_mean_mul_pow_rsqrt_1 = async_compile.triton('triton_poi_fused_add_mean_mul_pow_rsqrt_1', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[256], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mean_mul_pow_rsqrt_1', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_mean_mul_pow_rsqrt_1(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x2 = xindex
    x1 = (xindex // 4)
    x0 = xindex % 4
    tmp0 = tl.load(in_ptr0 + (x2), xmask)
    tmp1 = tl.load(in_ptr0 + (4*x1), xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr0 + (1 + (4*x1)), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr0 + (2 + (4*x1)), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr0 + (3 + (4*x1)), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1 * tmp1
    tmp4 = tmp3 * tmp3
    tmp5 = tmp2 + tmp4
    tmp7 = tmp6 * tmp6
    tmp8 = tmp5 + tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 + tmp10
    tmp12 = 4.0
    tmp13 = tmp11 / tmp12
    tmp14 = 1e-05
    tmp15 = tmp13 + tmp14
    tmp16 = libdevice.rsqrt(tmp15)
    tmp17 = tmp0 * tmp16
    tmp19 = tmp17 * tmp18
    tmp21 = tmp19 + tmp20
    tl.store(out_ptr0 + (x2), tmp21, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    primals_1, primals_2, primals_3 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_2, (4, ), (1, ))
    assert_size_stride(primals_3, (4, ), (1, ))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [u, sub], Original ATen: [aten.mean, aten.sub]
        stream0 = get_raw_stream(0)
        triton_poi_fused_mean_sub_0.run(primals_1, buf0, 256, grid=grid(256), stream=stream0)
        buf1 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [square, s, add, rsqrt, x, mul_1, x_1], Original ATen: [aten.pow, aten.mean, aten.add, aten.rsqrt, aten.mul]
        triton_poi_fused_add_mean_mul_pow_rsqrt_1.run(buf0, primals_2, primals_3, buf1, 256, grid=grid(256), stream=stream0)
        del buf0
        del primals_2
        del primals_3
    return (buf1, primals_1, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_3 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([primals_1, primals_2, primals_3])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import torch.nn as nn


class Norm(nn.Module):

    def __init__(self, n_state, axis=-1, epsilon=1e-05):
        super().__init__()
        self.n_state = n_state
        self.g = nn.Parameter(torch.ones([self.n_state]))
        self.b = nn.Parameter(torch.zeros([self.n_state]))
        self.axis = axis
        self.epsilon = epsilon

    def forward(self, x):
        u = torch.mean(x, dim=self.axis, keepdim=True)
        s = torch.mean(torch.square(x - u), dim=self.axis, keepdim=True)
        x = (x - u) * torch.rsqrt(s + self.epsilon)
        x = x * self.g + self.b
        return x


def get_inputs():
    return [torch.rand([4, 4, 4, 4])]


def get_init_inputs():
    return [[], {'n_state': 4}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._inductor.runtime.triton_helpers import libdevice
import torch.nn as nn
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda


@triton.jit
def triton_poi_fused_mean_sub_0(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr
    ):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x2 = xindex
    x1 = xindex // 4
    tmp0 = tl.load(in_ptr0 + x2, xmask)
    tmp1 = tl.load(in_ptr0 + 4 * x1, xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr0 + (1 + 4 * x1), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr0 + (2 + 4 * x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr0 + (3 + 4 * x1), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp5 = tmp3 + tmp4
    tmp7 = tmp5 + tmp6
    tmp8 = 4.0
    tmp9 = tmp7 / tmp8
    tmp10 = tmp0 - tmp9
    tl.store(out_ptr0 + x2, tmp10, xmask)


@triton.jit
def triton_poi_fused_add_mean_mul_pow_rsqrt_1(in_ptr0, in_ptr1, in_ptr2,
    out_ptr0, xnumel, XBLOCK: tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x2 = xindex
    x1 = xindex // 4
    x0 = xindex % 4
    tmp0 = tl.load(in_ptr0 + x2, xmask)
    tmp1 = tl.load(in_ptr0 + 4 * x1, xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr0 + (1 + 4 * x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr0 + (2 + 4 * x1), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr0 + (3 + 4 * x1), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr1 + x0, xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr2 + x0, xmask, eviction_policy='evict_last')
    tmp2 = tmp1 * tmp1
    tmp4 = tmp3 * tmp3
    tmp5 = tmp2 + tmp4
    tmp7 = tmp6 * tmp6
    tmp8 = tmp5 + tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 + tmp10
    tmp12 = 4.0
    tmp13 = tmp11 / tmp12
    tmp14 = 1e-05
    tmp15 = tmp13 + tmp14
    tmp16 = libdevice.rsqrt(tmp15)
    tmp17 = tmp0 * tmp16
    tmp19 = tmp17 * tmp18
    tmp21 = tmp19 + tmp20
    tl.store(out_ptr0 + x2, tmp21, xmask)


def call(args):
    primals_1, primals_2, primals_3 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_2, (4,), (1,))
    assert_size_stride(primals_3, (4,), (1,))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        get_raw_stream(0)
        triton_poi_fused_mean_sub_0[grid(256)](primals_1, buf0, 256, XBLOCK
            =128, num_warps=4, num_stages=1)
        buf1 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        triton_poi_fused_add_mean_mul_pow_rsqrt_1[grid(256)](buf0,
            primals_2, primals_3, buf1, 256, XBLOCK=256, num_warps=4,
            num_stages=1)
        del buf0
        del primals_2
        del primals_3
    return buf1, primals_1


class NormNew(nn.Module):

    def __init__(self, n_state, axis=-1, epsilon=1e-05):
        super().__init__()
        self.n_state = n_state
        self.g = nn.Parameter(torch.ones([self.n_state]))
        self.b = nn.Parameter(torch.zeros([self.n_state]))
        self.axis = axis
        self.epsilon = epsilon

    def forward(self, input_0):
        primals_2 = self.g
        primals_3 = self.b
        primals_1 = input_0
        output = call([primals_1, primals_2, primals_3])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Aalanli/MusicGeneration</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Norm</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">5</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "MIT"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">7d268322d692013d8ac6e70be31741cea519fa28</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/Aalanli/MusicGeneration/tree/7d268322d692013d8ac6e70be31741cea519fa28</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="6"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">BehlerAngular</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_inference']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/u6/cu6io2odujubs4jc63atgai7gg2fi2mzfw74zg77lxaq5wh3dsrx.py
# Topologically Sorted Source Nodes: [cat], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat => cat
# Graph fragment:
#   %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%mul, %mul_1], -1), kwargs = {})
triton_poi_fused_cat_0 = async_compile.triton('triton_poi_fused_cat_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[512], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex % 2
    x1 = (xindex // 2)
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 1, tl.int64)
    tmp4 = tmp0 &lt; tmp3
    tmp5 = tl.load(in_ptr0 + (x1), tmp4 &amp; xmask, eviction_policy='evict_last', other=0.0)
    tmp6 = 1.0
    tmp7 = tmp6 - tmp5
    tmp8 = tmp7 * tmp6
    tmp9 = tl.full(tmp8.shape, 0.0, tmp8.dtype)
    tmp10 = tl.where(tmp4, tmp8, tmp9)
    tmp11 = tmp0 >= tmp3
    tmp12 = tl.full([1], 2, tl.int64)
    tmp13 = tmp0 &lt; tmp12
    tmp14 = tl.load(in_ptr0 + (x1), tmp11 &amp; xmask, eviction_policy='evict_last', other=0.0)
    tmp15 = tmp14 + tmp6
    tmp16 = tmp15 * tmp6
    tmp17 = tl.full(tmp16.shape, 0.0, tmp16.dtype)
    tmp18 = tl.where(tmp11, tmp16, tmp17)
    tmp19 = tl.where(tmp4, tmp10, tmp18)
    tl.store(out_ptr0 + (x2), tmp19, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, = args
    args.clear()
    assert_size_stride(arg0_1, (4, 4, 4, 4), (64, 16, 4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4, 4, 2), (128, 32, 8, 2, 1), torch.float32)
        # Topologically Sorted Source Nodes: [cat], Original ATen: [aten.cat]
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_0.run(arg0_1, buf0, 512, grid=grid(512), stream=stream0)
        del arg0_1
    return (buf0, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([arg0_1])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
from torch import nn as nn


class BehlerAngular(nn.Module):
    """
    Compute Behler type angular contribution of the angle spanned by three atoms:

    :math:`2^{(1-\\zeta)} (1 + \\lambda \\cos( {\\theta}_{ijk} ) )^\\zeta`

    Sets of zetas with lambdas of -1 and +1 are generated automatically.

    Args:
        zetas (set of int): Set of exponents used to compute angular Behler term (default={1})

    """

    def __init__(self, zetas={1}):
        super(BehlerAngular, self).__init__()
        self.zetas = zetas

    def forward(self, cos_theta):
        """
        Args:
            cos_theta (torch.Tensor): Cosines between all pairs of neighbors of the central atom.

        Returns:
            torch.Tensor: Tensor containing values of the angular filters.
        """
        angular_pos = [(2 ** (1 - zeta) * ((1.0 - cos_theta) ** zeta).
            unsqueeze(-1)) for zeta in self.zetas]
        angular_neg = [(2 ** (1 - zeta) * ((1.0 + cos_theta) ** zeta).
            unsqueeze(-1)) for zeta in self.zetas]
        angular_all = angular_pos + angular_neg
        return torch.cat(angular_all, -1)


def get_inputs():
    return [torch.rand([4, 4, 4, 4])]


def get_init_inputs():
    return [[], {}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch import nn as nn
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda


@triton.jit
def triton_poi_fused_cat_0(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):
    xnumel = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex % 2
    x1 = xindex // 2
    x2 = xindex
    tmp0 = x0
    tl.full([1], 0, tl.int64)
    tmp3 = tl.full([1], 1, tl.int64)
    tmp4 = tmp0 &lt; tmp3
    tmp5 = tl.load(in_ptr0 + x1, tmp4 &amp; xmask, eviction_policy='evict_last',
        other=0.0)
    tmp6 = 1.0
    tmp7 = tmp6 - tmp5
    tmp8 = tmp7 * tmp6
    tmp9 = tl.full(tmp8.shape, 0.0, tmp8.dtype)
    tmp10 = tl.where(tmp4, tmp8, tmp9)
    tmp11 = tmp0 >= tmp3
    tl.full([1], 2, tl.int64)
    tmp14 = tl.load(in_ptr0 + x1, tmp11 &amp; xmask, eviction_policy=
        'evict_last', other=0.0)
    tmp15 = tmp14 + tmp6
    tmp16 = tmp15 * tmp6
    tmp17 = tl.full(tmp16.shape, 0.0, tmp16.dtype)
    tmp18 = tl.where(tmp11, tmp16, tmp17)
    tmp19 = tl.where(tmp4, tmp10, tmp18)
    tl.store(out_ptr0 + x2, tmp19, xmask)


def call(args):
    arg0_1, = args
    args.clear()
    assert_size_stride(arg0_1, (4, 4, 4, 4), (64, 16, 4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4, 4, 2), (128, 32, 8, 2, 1),
            torch.float32)
        get_raw_stream(0)
        triton_poi_fused_cat_0[grid(512)](arg0_1, buf0, 512, XBLOCK=128,
            num_warps=4, num_stages=1)
        del arg0_1
    return buf0,


class BehlerAngularNew(nn.Module):
    """
    Compute Behler type angular contribution of the angle spanned by three atoms:

    :math:`2^{(1-\\zeta)} (1 + \\lambda \\cos( {\\theta}_{ijk} ) )^\\zeta`

    Sets of zetas with lambdas of -1 and +1 are generated automatically.

    Args:
        zetas (set of int): Set of exponents used to compute angular Behler term (default={1})

    """

    def __init__(self, zetas={1}):
        super(BehlerAngularNew, self).__init__()
        self.zetas = zetas

    def forward(self, input_0):
        arg0_1 = input_0
        output = call([arg0_1])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">AlexanderDKazakov/schnetpack</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">BehlerAngular</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">6</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "MIT"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">97b82469d977981b500e439a6c93696d8dac8a3f</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/AlexanderDKazakov/schnetpack/tree/97b82469d977981b500e439a6c93696d8dac8a3f</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="7"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">BottleneckBlock</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_forward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/3w/c3w6ed26vbtzgzigkkyuiuwphm777x7aoogcxkvoly3ia7qifnzg.py
# Topologically Sorted Source Nodes: [out_1], Original ATen: [aten.relu]
# Source node to ATen node mapping:
#   out_1 => relu
# Graph fragment:
#   %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})
triton_poi_fused_relu_0 = async_compile.triton('triton_poi_fused_relu_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[16], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_relu_0', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_relu_0(in_out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)
    tmp1 = tl.full([1], 0, tl.int32)
    tmp2 = triton_helpers.maximum(tmp1, tmp0)
    tl.store(in_out_ptr0 + (x0), tmp2, xmask)
''', device_str='cuda')


# kernel path: runs/run_shard_6/inductor_cache/ad/cadccuyhl7stcp3nyqfgohiwbiv5ckfzxsye27ithwsill6dvmh4.py
# Topologically Sorted Source Nodes: [out_2, out_3], Original ATen: [aten.convolution, aten.relu]
# Source node to ATen node mapping:
#   out_2 => convolution_1
#   out_3 => relu_1
# Graph fragment:
#   %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %primals_3, %primals_4, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
#   %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_1,), kwargs = {})
triton_poi_fused_convolution_relu_1 = async_compile.triton('triton_poi_fused_convolution_relu_1', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[4], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_relu_1', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_relu_1(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)
    tmp1 = tl.load(in_ptr0 + (0))
    tmp2 = tl.broadcast_to(tmp1, [XBLOCK])
    tmp3 = tmp0 + tmp2
    tmp4 = tl.full([1], 0, tl.int32)
    tmp5 = triton_helpers.maximum(tmp4, tmp3)
    tl.store(in_out_ptr0 + (x0), tmp5, xmask)
''', device_str='cuda')


# kernel path: runs/run_shard_6/inductor_cache/5n/c5nsnnscsravxsqs7m7r23cp6lbfgo27lpzms4oosykw3b6atpvf.py
# Topologically Sorted Source Nodes: [out_5, out_6], Original ATen: [aten.add, aten.relu, aten.threshold_backward]
# Source node to ATen node mapping:
#   out_5 => add
#   out_6 => relu_2
# Graph fragment:
#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%convolution_2, %primals_1), kwargs = {})
#   %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add,), kwargs = {})
#   %le : [num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_2, 0), kwargs = {})
triton_poi_fused_add_relu_threshold_backward_2 = async_compile.triton('triton_poi_fused_add_relu_threshold_backward_2', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[64], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*i1', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_relu_threshold_backward_2', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_relu_threshold_backward_2(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x1 = (xindex // 4)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x1), xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2), xmask)
    tmp2 = tmp0 + tmp1
    tmp3 = tl.full([1], 0, tl.int32)
    tmp4 = triton_helpers.maximum(tmp3, tmp2)
    tmp5 = 0.0
    tmp6 = tmp4 &lt;= tmp5
    tl.store(out_ptr0 + (x2), tmp4, xmask)
    tl.store(out_ptr1 + (x2), tmp6, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    primals_1, primals_2, primals_3, primals_4, primals_5 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 2, 2), (16, 4, 2, 1))
    assert_size_stride(primals_2, (1, 4, 1, 1), (4, 1, 1, 1))
    assert_size_stride(primals_3, (1, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_4, (1, ), (1, ))
    assert_size_stride(primals_5, (4, 1, 1, 1), (1, 1, 1, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        # Topologically Sorted Source Nodes: [out], Original ATen: [aten.convolution]
        buf0 = extern_kernels.convolution(primals_1, primals_2, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
        assert_size_stride(buf0, (4, 1, 2, 2), (4, 4, 2, 1))
        buf1 = buf0; del buf0  # reuse
        # Topologically Sorted Source Nodes: [out_1], Original ATen: [aten.relu]
        stream0 = get_raw_stream(0)
        triton_poi_fused_relu_0.run(buf1, 16, grid=grid(16), stream=stream0)
        # Topologically Sorted Source Nodes: [out_2], Original ATen: [aten.convolution]
        buf2 = extern_kernels.convolution(buf1, primals_3, stride=(2, 2), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
        assert_size_stride(buf2, (4, 1, 1, 1), (1, 1, 1, 1))
        buf3 = buf2; del buf2  # reuse
        # Topologically Sorted Source Nodes: [out_2, out_3], Original ATen: [aten.convolution, aten.relu]
        triton_poi_fused_convolution_relu_1.run(buf3, primals_4, 4, grid=grid(4), stream=stream0)
        del primals_4
        # Topologically Sorted Source Nodes: [out_4], Original ATen: [aten.convolution]
        buf4 = extern_kernels.convolution(buf3, primals_5, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
        assert_size_stride(buf4, (4, 4, 1, 1), (4, 1, 1, 1))
        buf5 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.float32)
        buf6 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.bool)
        # Topologically Sorted Source Nodes: [out_5, out_6], Original ATen: [aten.add, aten.relu, aten.threshold_backward]
        triton_poi_fused_add_relu_threshold_backward_2.run(buf4, primals_1, buf5, buf6, 64, grid=grid(64), stream=stream0)
        del buf4
    return (buf5, primals_1, primals_2, primals_3, primals_5, buf1, buf3, buf6, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((4, 4, 2, 2), (16, 4, 2, 1), device='cuda:0', dtype=torch.float32)
    primals_2 = rand_strided((1, 4, 1, 1), (4, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_3 = rand_strided((1, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_4 = rand_strided((1, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_5 = rand_strided((4, 1, 1, 1), (1, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import math
import torch
import torch.nn as nn
import torch.utils.data
import torch.nn.parallel
import torch.optim
import torch.utils.data.distributed


def init_layer(L):
    if isinstance(L, nn.Conv2d):
        n = L.kernel_size[0] * L.kernel_size[1] * L.out_channels
        L.weight.data.normal_(0, math.sqrt(2.0 / float(n)))
    elif isinstance(L, nn.BatchNorm2d):
        L.weight.data.fill_(1)
        L.bias.data.fill_(0)


class BottleneckBlock(nn.Module):

    def __init__(self, indim, outdim, half_res):
        super(BottleneckBlock, self).__init__()
        bottleneckdim = int(outdim / 4)
        self.indim = indim
        self.outdim = outdim
        self.C1 = nn.Conv2d(indim, bottleneckdim, kernel_size=1, bias=False)
        self.BN1 = nn.Identity()
        self.C2 = nn.Conv2d(bottleneckdim, bottleneckdim, kernel_size=3,
            stride=2 if half_res else 1, padding=1)
        self.BN2 = nn.Identity()
        self.C3 = nn.Conv2d(bottleneckdim, outdim, kernel_size=1, bias=False)
        self.BN3 = nn.Identity()
        self.relu = nn.ReLU()
        self.parametrized_layers = [self.C1, self.BN1, self.C2, self.BN2,
            self.C3, self.BN3]
        self.half_res = half_res
        if indim != outdim:
            self.shortcut = nn.Conv2d(indim, outdim, 1, stride=2 if
                half_res else 1, bias=False)
            self.parametrized_layers.append(self.shortcut)
            self.shortcut_type = '1x1'
        else:
            self.shortcut_type = 'identity'
        for layer in self.parametrized_layers:
            init_layer(layer)

    def forward(self, x):
        short_out = x if self.shortcut_type == 'identity' else self.shortcut(x)
        out = self.C1(x)
        out = self.BN1(out)
        out = self.relu(out)
        out = self.C2(out)
        out = self.BN2(out)
        out = self.relu(out)
        out = self.C3(out)
        out = self.BN3(out)
        out = out + short_out
        out = self.relu(out)
        return out


def get_inputs():
    return [torch.rand([4, 4, 2, 2])]


def get_init_inputs():
    return [[], {'indim': 4, 'outdim': 4, 'half_res': 4}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
from torch._inductor.select_algorithm import extern_kernels
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._inductor.runtime import triton_helpers
import math
import torch.nn as nn
import torch.utils.data
import torch.nn.parallel
import torch.optim
import torch.utils.data.distributed
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda


@triton.jit
def triton_poi_fused_relu_0(in_out_ptr0, xnumel, XBLOCK: tl.constexpr):
    xnumel = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + x0, xmask)
    tmp1 = tl.full([1], 0, tl.int32)
    tmp2 = triton_helpers.maximum(tmp1, tmp0)
    tl.store(in_out_ptr0 + x0, tmp2, xmask)


@triton.jit
def triton_poi_fused_convolution_relu_1(in_out_ptr0, in_ptr0, xnumel,
    XBLOCK: tl.constexpr):
    xnumel = 4
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + x0, xmask)
    tmp1 = tl.load(in_ptr0 + 0)
    tmp2 = tl.broadcast_to(tmp1, [XBLOCK])
    tmp3 = tmp0 + tmp2
    tmp4 = tl.full([1], 0, tl.int32)
    tmp5 = triton_helpers.maximum(tmp4, tmp3)
    tl.store(in_out_ptr0 + x0, tmp5, xmask)


@triton.jit
def triton_poi_fused_add_relu_threshold_backward_2(in_ptr0, in_ptr1,
    out_ptr0, out_ptr1, xnumel, XBLOCK: tl.constexpr):
    xnumel = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x1 = xindex // 4
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + x1, xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + x2, xmask)
    tmp2 = tmp0 + tmp1
    tmp3 = tl.full([1], 0, tl.int32)
    tmp4 = triton_helpers.maximum(tmp3, tmp2)
    tmp5 = 0.0
    tmp6 = tmp4 &lt;= tmp5
    tl.store(out_ptr0 + x2, tmp4, xmask)
    tl.store(out_ptr1 + x2, tmp6, xmask)


def call(args):
    primals_1, primals_2, primals_3, primals_4, primals_5 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 2, 2), (16, 4, 2, 1))
    assert_size_stride(primals_2, (1, 4, 1, 1), (4, 1, 1, 1))
    assert_size_stride(primals_3, (1, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_4, (1,), (1,))
    assert_size_stride(primals_5, (4, 1, 1, 1), (1, 1, 1, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = extern_kernels.convolution(primals_1, primals_2, stride=(1, 
            1), padding=(0, 0), dilation=(1, 1), transposed=False,
            output_padding=(0, 0), groups=1, bias=None)
        assert_size_stride(buf0, (4, 1, 2, 2), (4, 4, 2, 1))
        buf1 = buf0
        del buf0
        get_raw_stream(0)
        triton_poi_fused_relu_0[grid(16)](buf1, 16, XBLOCK=16, num_warps=1,
            num_stages=1)
        buf2 = extern_kernels.convolution(buf1, primals_3, stride=(2, 2),
            padding=(1, 1), dilation=(1, 1), transposed=False,
            output_padding=(0, 0), groups=1, bias=None)
        assert_size_stride(buf2, (4, 1, 1, 1), (1, 1, 1, 1))
        buf3 = buf2
        del buf2
        triton_poi_fused_convolution_relu_1[grid(4)](buf3, primals_4, 4,
            XBLOCK=4, num_warps=1, num_stages=1)
        del primals_4
        buf4 = extern_kernels.convolution(buf3, primals_5, stride=(1, 1),
            padding=(0, 0), dilation=(1, 1), transposed=False,
            output_padding=(0, 0), groups=1, bias=None)
        assert_size_stride(buf4, (4, 4, 1, 1), (4, 1, 1, 1))
        buf5 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.float32)
        buf6 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.bool)
        triton_poi_fused_add_relu_threshold_backward_2[grid(64)](buf4,
            primals_1, buf5, buf6, 64, XBLOCK=64, num_warps=1, num_stages=1)
        del buf4
    return buf5, primals_1, primals_2, primals_3, primals_5, buf1, buf3, buf6


def init_layer(L):
    if isinstance(L, nn.Conv2d):
        n = L.kernel_size[0] * L.kernel_size[1] * L.out_channels
        L.weight.data.normal_(0, math.sqrt(2.0 / float(n)))
    elif isinstance(L, nn.BatchNorm2d):
        L.weight.data.fill_(1)
        L.bias.data.fill_(0)


class BottleneckBlockNew(nn.Module):

    def __init__(self, indim, outdim, half_res):
        super(BottleneckBlockNew, self).__init__()
        bottleneckdim = int(outdim / 4)
        self.indim = indim
        self.outdim = outdim
        self.C1 = nn.Conv2d(indim, bottleneckdim, kernel_size=1, bias=False)
        self.BN1 = nn.Identity()
        self.C2 = nn.Conv2d(bottleneckdim, bottleneckdim, kernel_size=3,
            stride=2 if half_res else 1, padding=1)
        self.BN2 = nn.Identity()
        self.C3 = nn.Conv2d(bottleneckdim, outdim, kernel_size=1, bias=False)
        self.BN3 = nn.Identity()
        self.relu = nn.ReLU()
        self.parametrized_layers = [self.C1, self.BN1, self.C2, self.BN2,
            self.C3, self.BN3]
        self.half_res = half_res
        if indim != outdim:
            self.shortcut = nn.Conv2d(indim, outdim, 1, stride=2 if
                half_res else 1, bias=False)
            self.parametrized_layers.append(self.shortcut)
            self.shortcut_type = '1x1'
        else:
            self.shortcut_type = 'identity'
        for layer in self.parametrized_layers:
            init_layer(layer)

    def forward(self, input_0):
        primals_2 = self.C1.weight
        primals_3 = self.C2.weight
        primals_4 = self.C2.bias
        primals_5 = self.C3.weight
        primals_1 = input_0
        output = call([primals_1, primals_2, primals_3, primals_4, primals_5])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Aamer98/FeatureNorm</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">BottleneckBlock</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">7</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "MIT"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">fbf3d3b4cef81b3351347d272eb51b6cdd9f0cc5</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/Aamer98/FeatureNorm/tree/fbf3d3b4cef81b3351347d272eb51b6cdd9f0cc5</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="8"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Mlp</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_forward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/sq/csqquhpniarhypnbdhu3ry5jhmzns75umbxvduqvlyaridzrevc5.py
# Topologically Sorted Source Nodes: [h], Original ATen: [aten.gelu]
# Source node to ATen node mapping:
#   h => add_1, erf, mul, mul_1, mul_2
# Graph fragment:
#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_2, 0.5), kwargs = {})
#   %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_2, 0.7071067811865476), kwargs = {})
#   %erf : [num_users=1] = call_function[target=torch.ops.aten.erf.default](args = (%mul_1,), kwargs = {})
#   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%erf, 1), kwargs = {})
#   %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul, %add_1), kwargs = {})
triton_poi_fused_gelu_0 = async_compile.triton('triton_poi_fused_gelu_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[256], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_gelu_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_gelu_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x2 = xindex
    x0 = xindex % 4
    tmp0 = tl.load(in_ptr0 + (x2), xmask)
    tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp3 = 0.5
    tmp4 = tmp2 * tmp3
    tmp5 = 0.7071067811865476
    tmp6 = tmp2 * tmp5
    tmp7 = libdevice.erf(tmp6)
    tmp8 = 1.0
    tmp9 = tmp7 + tmp8
    tmp10 = tmp4 * tmp9
    tl.store(out_ptr0 + (x2), tmp10, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    primals_1, primals_2, primals_3, primals_4, primals_5 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_2, (1, 4, 4), (16, 4, 1))
    assert_size_stride(primals_3, (4, ), (1, ))
    assert_size_stride(primals_4, (1, 4, 4), (16, 4, 1))
    assert_size_stride(primals_5, (4, ), (1, ))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((64, 4), (4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]
        extern_kernels.mm(reinterpret_tensor(primals_1, (64, 4), (4, 1), 0), reinterpret_tensor(primals_2, (4, 4), (4, 1), 0), out=buf0)
        del primals_2
        buf1 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [h], Original ATen: [aten.gelu]
        stream0 = get_raw_stream(0)
        triton_poi_fused_gelu_0.run(buf0, primals_3, buf1, 256, grid=grid(256), stream=stream0)
        buf2 = empty_strided_cuda((64, 4), (4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [], Original ATen: []
        extern_kernels.addmm(primals_5, reinterpret_tensor(buf1, (64, 4), (4, 1), 0), reinterpret_tensor(primals_4, (4, 4), (4, 1), 0), alpha=1, beta=1, out=buf2)
        del primals_5
    return (reinterpret_tensor(buf2, (4, 4, 4, 4), (64, 16, 4, 1), 0), primals_3, buf0, reinterpret_tensor(buf1, (4, 64), (1, 4), 0), reinterpret_tensor(primals_4, (4, 4), (1, 4), 0), reinterpret_tensor(primals_1, (4, 64), (1, 4), 0), )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    primals_2 = rand_strided((1, 4, 4), (16, 4, 1), device='cuda:0', dtype=torch.float32)
    primals_3 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_4 = rand_strided((1, 4, 4), (16, 4, 1), device='cuda:0', dtype=torch.float32)
    primals_5 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import torch.nn as nn


class Conv1d(nn.Module):

    def __init__(self, nf, nx, stdev=0.02):
        super().__init__()
        self.nf = nf
        self.nx = nx
        self.stdev = stdev
        self.w = nn.Parameter(torch.normal(size=[1, self.nx, self.nf], mean
            =0.0, std=self.stdev))
        self.b = nn.Parameter(torch.zeros([self.nf]))

    def forward(self, x: 'torch.Tensor'):
        shape = x.size()
        start, nx = shape[:-1], shape[-1]
        return torch.reshape(torch.matmul(torch.reshape(x, [-1, nx]), torch
            .reshape(self.w, [-1, self.nf])) + self.b, start + (self.nf,))


class Mlp(nn.Module):

    def __init__(self, input_dim, proj_dim):
        super().__init__()
        self.input_dim = input_dim
        self.proj_dim = proj_dim
        self.conv_fc = Conv1d(self.proj_dim, self.input_dim)
        self.conv_proj = Conv1d(self.input_dim, self.proj_dim)

    def forward(self, x):
        h = nn.functional.gelu(self.conv_fc(x))
        return self.conv_proj(h)


def get_inputs():
    return [torch.rand([4, 4, 4, 4])]


def get_init_inputs():
    return [[], {'input_dim': 4, 'proj_dim': 4}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
from torch._inductor.select_algorithm import extern_kernels
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._inductor.runtime.triton_helpers import libdevice
import torch.nn as nn
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor


@triton.jit
def triton_poi_fused_gelu_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK: tl.
    constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x2 = xindex
    x0 = xindex % 4
    tmp0 = tl.load(in_ptr0 + x2, xmask)
    tmp1 = tl.load(in_ptr1 + x0, xmask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp3 = 0.5
    tmp4 = tmp2 * tmp3
    tmp5 = 0.7071067811865476
    tmp6 = tmp2 * tmp5
    tmp7 = libdevice.erf(tmp6)
    tmp8 = 1.0
    tmp9 = tmp7 + tmp8
    tmp10 = tmp4 * tmp9
    tl.store(out_ptr0 + x2, tmp10, xmask)


def call(args):
    primals_1, primals_2, primals_3, primals_4, primals_5 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_2, (1, 4, 4), (16, 4, 1))
    assert_size_stride(primals_3, (4,), (1,))
    assert_size_stride(primals_4, (1, 4, 4), (16, 4, 1))
    assert_size_stride(primals_5, (4,), (1,))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((64, 4), (4, 1), torch.float32)
        extern_kernels.mm(reinterpret_tensor(primals_1, (64, 4), (4, 1), 0),
            reinterpret_tensor(primals_2, (4, 4), (4, 1), 0), out=buf0)
        del primals_2
        buf1 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        get_raw_stream(0)
        triton_poi_fused_gelu_0[grid(256)](buf0, primals_3, buf1, 256,
            XBLOCK=128, num_warps=4, num_stages=1)
        buf2 = empty_strided_cuda((64, 4), (4, 1), torch.float32)
        extern_kernels.addmm(primals_5, reinterpret_tensor(buf1, (64, 4), (
            4, 1), 0), reinterpret_tensor(primals_4, (4, 4), (4, 1), 0),
            alpha=1, beta=1, out=buf2)
        del primals_5
    return reinterpret_tensor(buf2, (4, 4, 4, 4), (64, 16, 4, 1), 0
        ), primals_3, buf0, reinterpret_tensor(buf1, (4, 64), (1, 4), 0
        ), reinterpret_tensor(primals_4, (4, 4), (1, 4), 0
        ), reinterpret_tensor(primals_1, (4, 64), (1, 4), 0)


class Conv1d(nn.Module):

    def __init__(self, nf, nx, stdev=0.02):
        super().__init__()
        self.nf = nf
        self.nx = nx
        self.stdev = stdev
        self.w = nn.Parameter(torch.normal(size=[1, self.nx, self.nf], mean
            =0.0, std=self.stdev))
        self.b = nn.Parameter(torch.zeros([self.nf]))

    def forward(self, x: 'torch.Tensor'):
        shape = x.size()
        start, nx = shape[:-1], shape[-1]
        return torch.reshape(torch.matmul(torch.reshape(x, [-1, nx]), torch
            .reshape(self.w, [-1, self.nf])) + self.b, start + (self.nf,))


class MlpNew(nn.Module):

    def __init__(self, input_dim, proj_dim):
        super().__init__()
        self.input_dim = input_dim
        self.proj_dim = proj_dim
        self.conv_fc = Conv1d(self.proj_dim, self.input_dim)
        self.conv_proj = Conv1d(self.input_dim, self.proj_dim)

    def forward(self, input_0):
        primals_2 = self.conv_fc.w
        primals_3 = self.conv_fc.b
        primals_4 = self.conv_proj.w
        primals_5 = self.conv_proj.b
        primals_1 = input_0
        output = call([primals_1, primals_2, primals_3, primals_4, primals_5])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Aalanli/MusicGeneration</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Mlp</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">9</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "MIT"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">7d268322d692013d8ac6e70be31741cea519fa28</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/Aalanli/MusicGeneration/tree/7d268322d692013d8ac6e70be31741cea519fa28</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="9"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">GCN</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_forward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/ei/cei4vnxgrn55dm7jkw3qn2lp2qox5qamtqida5hfzp6upgn6nb24.py
# Topologically Sorted Source Nodes: [x], Original ATen: [aten.elu]
# Source node to ATen node mapping:
#   x => expm1, gt, mul, mul_2, where
# Graph fragment:
#   %gt : [num_users=1] = call_function[target=torch.ops.aten.gt.Scalar](args = (%mm, 0), kwargs = {})
#   %mul : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mm, 1.0), kwargs = {})
#   %expm1 : [num_users=1] = call_function[target=torch.ops.aten.expm1.default](args = (%mul,), kwargs = {})
#   %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%expm1, 1.0), kwargs = {})
#   %where : [num_users=2] = call_function[target=torch.ops.aten.where.self](args = (%gt, %mul, %mul_2), kwargs = {})
triton_poi_fused_elu_0 = async_compile.triton('triton_poi_fused_elu_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[16], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_elu_0', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_elu_0(in_out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)
    tmp1 = 0.0
    tmp2 = tmp0 > tmp1
    tmp3 = 1.0
    tmp4 = tmp0 * tmp3
    tmp5 = libdevice.expm1(tmp4)
    tmp6 = tmp5 * tmp3
    tmp7 = tl.where(tmp2, tmp4, tmp6)
    tl.store(in_out_ptr0 + (x0), tmp7, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4), (4, 1))
    assert_size_stride(primals_2, (4, ), (1, ))
    assert_size_stride(primals_3, (4, 4), (4, 1))
    assert_size_stride(primals_4, (4, 4), (4, 1))
    assert_size_stride(primals_5, (4, 4), (4, 1))
    assert_size_stride(primals_6, (4, ), (1, ))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4), (4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [support], Original ATen: [aten.addmm]
        extern_kernels.addmm(primals_2, primals_3, reinterpret_tensor(primals_1, (4, 4), (1, 4), 0), alpha=1, beta=1, out=buf0)
        del primals_1
        del primals_2
        buf1 = empty_strided_cuda((4, 4), (4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [output], Original ATen: [aten.mm]
        extern_kernels.mm(primals_4, buf0, out=buf1)
        buf2 = buf1; del buf1  # reuse
        # Topologically Sorted Source Nodes: [x], Original ATen: [aten.elu]
        stream0 = get_raw_stream(0)
        triton_poi_fused_elu_0.run(buf2, 16, grid=grid(16), stream=stream0)
        buf3 = buf0; del buf0  # reuse
        # Topologically Sorted Source Nodes: [support_1], Original ATen: [aten.addmm]
        extern_kernels.addmm(primals_6, buf2, reinterpret_tensor(primals_5, (4, 4), (1, 4), 0), alpha=1, beta=1, out=buf3)
        del primals_6
        buf4 = empty_strided_cuda((4, 4), (4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [output_1], Original ATen: [aten.mm]
        extern_kernels.mm(primals_4, buf3, out=buf4)
        del buf3
    return (buf4, primals_3, buf2, reinterpret_tensor(primals_4, (4, 4), (1, 4), 0), primals_5, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)
    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_3 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)
    primals_4 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)
    primals_5 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)
    primals_6 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">from torch.nn import Module
import math
import torch
import torch.nn.functional as F
import torch.nn as nn


class GraphConvolution(Module):
    """
    A Graph Convolution Layer (GCN)
    """

    def __init__(self, in_features, out_features, bias=True):
        super(GraphConvolution, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.W = nn.Linear(in_features, out_features, bias=bias)
        self.init()

    def init(self):
        stdv = 1.0 / math.sqrt(self.W.weight.size(1))
        self.W.weight.data.uniform_(-stdv, stdv)

    def forward(self, input, adj):
        support = self.W(input)
        output = torch.spmm(adj, support)
        return output


class GCN(nn.Module):
    """
    A Two-layer GCN.
    """

    def __init__(self, nfeat, nhid, nclass, dropout):
        super(GCN, self).__init__()
        self.gc1 = GraphConvolution(nfeat, nhid)
        self.gc2 = GraphConvolution(nhid, nclass)
        self.dropout = dropout
        self.elu = torch.nn.ELU(inplace=True)

    def forward(self, x, adj, use_relu=True):
        x = self.gc1(x, adj)
        if use_relu:
            x = self.elu(x)
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.gc2(x, adj)
        return x


def get_inputs():
    return [torch.rand([4, 4]), torch.rand([4, 4])]


def get_init_inputs():
    return [[], {'nfeat': 4, 'nhid': 4, 'nclass': 4, 'dropout': 0.5}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
from torch._inductor.select_algorithm import extern_kernels
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._inductor.runtime.triton_helpers import libdevice
from torch.nn import Module
import math
import torch.nn as nn
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor


@triton.jit
def triton_poi_fused_elu_0(in_out_ptr0, xnumel, XBLOCK: tl.constexpr):
    xnumel = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + x0, xmask)
    tmp1 = 0.0
    tmp2 = tmp0 > tmp1
    tmp3 = 1.0
    tmp4 = tmp0 * tmp3
    tmp5 = libdevice.expm1(tmp4)
    tmp6 = tmp5 * tmp3
    tmp7 = tl.where(tmp2, tmp4, tmp6)
    tl.store(in_out_ptr0 + x0, tmp7, xmask)


def call(args):
    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4), (4, 1))
    assert_size_stride(primals_2, (4,), (1,))
    assert_size_stride(primals_3, (4, 4), (4, 1))
    assert_size_stride(primals_4, (4, 4), (4, 1))
    assert_size_stride(primals_5, (4, 4), (4, 1))
    assert_size_stride(primals_6, (4,), (1,))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4), (4, 1), torch.float32)
        extern_kernels.addmm(primals_2, primals_3, reinterpret_tensor(
            primals_1, (4, 4), (1, 4), 0), alpha=1, beta=1, out=buf0)
        del primals_1
        del primals_2
        buf1 = empty_strided_cuda((4, 4), (4, 1), torch.float32)
        extern_kernels.mm(primals_4, buf0, out=buf1)
        buf2 = buf1
        del buf1
        get_raw_stream(0)
        triton_poi_fused_elu_0[grid(16)](buf2, 16, XBLOCK=16, num_warps=1,
            num_stages=1)
        buf3 = buf0
        del buf0
        extern_kernels.addmm(primals_6, buf2, reinterpret_tensor(primals_5,
            (4, 4), (1, 4), 0), alpha=1, beta=1, out=buf3)
        del primals_6
        buf4 = empty_strided_cuda((4, 4), (4, 1), torch.float32)
        extern_kernels.mm(primals_4, buf3, out=buf4)
        del buf3
    return buf4, primals_3, buf2, reinterpret_tensor(primals_4, (4, 4), (1,
        4), 0), primals_5


class GraphConvolution(Module):
    """
    A Graph Convolution Layer (GCN)
    """

    def __init__(self, in_features, out_features, bias=True):
        super(GraphConvolution, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.W = nn.Linear(in_features, out_features, bias=bias)
        self.init()

    def init(self):
        stdv = 1.0 / math.sqrt(self.W.weight.size(1))
        self.W.weight.data.uniform_(-stdv, stdv)

    def forward(self, input, adj):
        support = self.W(input)
        output = torch.spmm(adj, support)
        return output


class GCNNew(nn.Module):
    """
    A Two-layer GCN.
    """

    def __init__(self, nfeat, nhid, nclass, dropout):
        super(GCNNew, self).__init__()
        self.gc1 = GraphConvolution(nfeat, nhid)
        self.gc2 = GraphConvolution(nhid, nclass)
        self.dropout = dropout
        self.elu = torch.nn.ELU(inplace=True)

    def forward(self, input_0, input_1):
        primals_1 = self.gc1.W.weight
        primals_2 = self.gc1.W.bias
        primals_3 = self.gc2.W.weight
        primals_6 = self.gc2.W.bias
        primals_4 = input_0
        primals_5 = input_1
        output = call([primals_1, primals_2, primals_3, primals_4,
            primals_5, primals_6])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">AlexMinhao/NAS_GNN</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">GCN</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">10</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "Apache-2.0"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">89183988a96e1d6baed910ab3843c13282f8b077</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/AlexMinhao/NAS_GNN/tree/89183988a96e1d6baed910ab3843c13282f8b077</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="10"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">SimpleBlock</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_forward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/3w/c3w6ed26vbtzgzigkkyuiuwphm777x7aoogcxkvoly3ia7qifnzg.py
# Topologically Sorted Source Nodes: [out_1], Original ATen: [aten.relu]
# Source node to ATen node mapping:
#   out_1 => relu
# Graph fragment:
#   %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})
triton_poi_fused_relu_0 = async_compile.triton('triton_poi_fused_relu_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[16], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_relu_0', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_relu_0(in_out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)
    tmp1 = tl.full([1], 0, tl.int32)
    tmp2 = triton_helpers.maximum(tmp1, tmp0)
    tl.store(in_out_ptr0 + (x0), tmp2, xmask)
''', device_str='cuda')


# kernel path: runs/run_shard_6/inductor_cache/l4/cl4s6iwd4ja3i3vrrev5xqre3x2k7a3yguenikvia4suomlqy7ty.py
# Topologically Sorted Source Nodes: [out_3, out_4], Original ATen: [aten.add, aten.relu, aten.threshold_backward]
# Source node to ATen node mapping:
#   out_3 => add
#   out_4 => relu_1
# Graph fragment:
#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%convolution_1, %primals_2), kwargs = {})
#   %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add,), kwargs = {})
#   %le : [num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_1, 0), kwargs = {})
triton_poi_fused_add_relu_threshold_backward_1 = async_compile.triton('triton_poi_fused_add_relu_threshold_backward_1', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[64], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*i1', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_relu_threshold_backward_1', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_relu_threshold_backward_1(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x1 = (xindex // 4)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x1), xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2), xmask)
    tmp2 = tmp0 + tmp1
    tmp3 = tl.full([1], 0, tl.int32)
    tmp4 = triton_helpers.maximum(tmp3, tmp2)
    tmp5 = 0.0
    tmp6 = tmp4 &lt;= tmp5
    tl.store(out_ptr0 + (x2), tmp4, xmask)
    tl.store(out_ptr1 + (x2), tmp6, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    primals_1, primals_2, primals_3 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 3, 3), (36, 9, 3, 1))
    assert_size_stride(primals_2, (4, 4, 2, 2), (16, 4, 2, 1))
    assert_size_stride(primals_3, (4, 4, 3, 3), (36, 9, 3, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        # Topologically Sorted Source Nodes: [out], Original ATen: [aten.convolution]
        buf0 = extern_kernels.convolution(primals_2, primals_1, stride=(2, 2), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
        assert_size_stride(buf0, (4, 4, 1, 1), (4, 1, 1, 1))
        buf1 = buf0; del buf0  # reuse
        # Topologically Sorted Source Nodes: [out_1], Original ATen: [aten.relu]
        stream0 = get_raw_stream(0)
        triton_poi_fused_relu_0.run(buf1, 16, grid=grid(16), stream=stream0)
        # Topologically Sorted Source Nodes: [out_2], Original ATen: [aten.convolution]
        buf2 = extern_kernels.convolution(buf1, primals_3, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
        assert_size_stride(buf2, (4, 4, 1, 1), (4, 1, 1, 1))
        buf3 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.float32)
        buf4 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.bool)
        # Topologically Sorted Source Nodes: [out_3, out_4], Original ATen: [aten.add, aten.relu, aten.threshold_backward]
        triton_poi_fused_add_relu_threshold_backward_1.run(buf2, primals_2, buf3, buf4, 64, grid=grid(64), stream=stream0)
        del buf2
    return (buf3, primals_1, primals_2, primals_3, buf1, buf4, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((4, 4, 3, 3), (36, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_2 = rand_strided((4, 4, 2, 2), (16, 4, 2, 1), device='cuda:0', dtype=torch.float32)
    primals_3 = rand_strided((4, 4, 3, 3), (36, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([primals_1, primals_2, primals_3])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import math
import torch
import torch.nn as nn
import torch.utils.data
import torch.nn.parallel
import torch.optim
import torch.utils.data.distributed


def init_layer(L):
    if isinstance(L, nn.Conv2d):
        n = L.kernel_size[0] * L.kernel_size[1] * L.out_channels
        L.weight.data.normal_(0, math.sqrt(2.0 / float(n)))
    elif isinstance(L, nn.BatchNorm2d):
        L.weight.data.fill_(1)
        L.bias.data.fill_(0)


class SimpleBlock(nn.Module):
    maml = False

    def __init__(self, indim, outdim, half_res):
        super(SimpleBlock, self).__init__()
        self.indim = indim
        self.outdim = outdim
        self.C1 = nn.Conv2d(indim, outdim, kernel_size=3, stride=2 if
            half_res else 1, padding=1, bias=False)
        self.BN1 = nn.Identity()
        self.C2 = nn.Conv2d(outdim, outdim, kernel_size=3, padding=1, bias=
            False)
        self.BN2 = nn.Identity()
        self.relu1 = nn.ReLU(inplace=True)
        self.relu2 = nn.ReLU(inplace=True)
        self.parametrized_layers = [self.C1, self.C2, self.BN1, self.BN2]
        self.half_res = half_res
        if indim != outdim:
            self.shortcut = nn.Conv2d(indim, outdim, 1, 2 if half_res else 
                1, bias=False)
            self.BNshortcut = nn.Identity()
            self.parametrized_layers.append(self.shortcut)
            self.parametrized_layers.append(self.BNshortcut)
            self.shortcut_type = '1x1'
        else:
            self.shortcut_type = 'identity'
        for layer in self.parametrized_layers:
            init_layer(layer)

    def forward(self, x):
        out = self.C1(x)
        out = self.BN1(out)
        out = self.relu1(out)
        out = self.C2(out)
        out = self.BN2(out)
        short_out = x if self.shortcut_type == 'identity' else self.BNshortcut(
            self.shortcut(x))
        out = out + short_out
        out = self.relu2(out)
        return out


def get_inputs():
    return [torch.rand([4, 4, 2, 2])]


def get_init_inputs():
    return [[], {'indim': 4, 'outdim': 4, 'half_res': 4}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
from torch._inductor.select_algorithm import extern_kernels
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._inductor.runtime import triton_helpers
import math
import torch.nn as nn
import torch.utils.data
import torch.nn.parallel
import torch.optim
import torch.utils.data.distributed
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda


@triton.jit
def triton_poi_fused_relu_0(in_out_ptr0, xnumel, XBLOCK: tl.constexpr):
    xnumel = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + x0, xmask)
    tmp1 = tl.full([1], 0, tl.int32)
    tmp2 = triton_helpers.maximum(tmp1, tmp0)
    tl.store(in_out_ptr0 + x0, tmp2, xmask)


@triton.jit
def triton_poi_fused_add_relu_threshold_backward_1(in_ptr0, in_ptr1,
    out_ptr0, out_ptr1, xnumel, XBLOCK: tl.constexpr):
    xnumel = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x1 = xindex // 4
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + x1, xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + x2, xmask)
    tmp2 = tmp0 + tmp1
    tmp3 = tl.full([1], 0, tl.int32)
    tmp4 = triton_helpers.maximum(tmp3, tmp2)
    tmp5 = 0.0
    tmp6 = tmp4 &lt;= tmp5
    tl.store(out_ptr0 + x2, tmp4, xmask)
    tl.store(out_ptr1 + x2, tmp6, xmask)


def call(args):
    primals_1, primals_2, primals_3 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4, 3, 3), (36, 9, 3, 1))
    assert_size_stride(primals_2, (4, 4, 2, 2), (16, 4, 2, 1))
    assert_size_stride(primals_3, (4, 4, 3, 3), (36, 9, 3, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = extern_kernels.convolution(primals_2, primals_1, stride=(2, 
            2), padding=(1, 1), dilation=(1, 1), transposed=False,
            output_padding=(0, 0), groups=1, bias=None)
        assert_size_stride(buf0, (4, 4, 1, 1), (4, 1, 1, 1))
        buf1 = buf0
        del buf0
        get_raw_stream(0)
        triton_poi_fused_relu_0[grid(16)](buf1, 16, XBLOCK=16, num_warps=1,
            num_stages=1)
        buf2 = extern_kernels.convolution(buf1, primals_3, stride=(1, 1),
            padding=(1, 1), dilation=(1, 1), transposed=False,
            output_padding=(0, 0), groups=1, bias=None)
        assert_size_stride(buf2, (4, 4, 1, 1), (4, 1, 1, 1))
        buf3 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.float32)
        buf4 = empty_strided_cuda((4, 4, 2, 2), (16, 4, 2, 1), torch.bool)
        triton_poi_fused_add_relu_threshold_backward_1[grid(64)](buf2,
            primals_2, buf3, buf4, 64, XBLOCK=64, num_warps=1, num_stages=1)
        del buf2
    return buf3, primals_1, primals_2, primals_3, buf1, buf4


def init_layer(L):
    if isinstance(L, nn.Conv2d):
        n = L.kernel_size[0] * L.kernel_size[1] * L.out_channels
        L.weight.data.normal_(0, math.sqrt(2.0 / float(n)))
    elif isinstance(L, nn.BatchNorm2d):
        L.weight.data.fill_(1)
        L.bias.data.fill_(0)


class SimpleBlockNew(nn.Module):
    maml = False

    def __init__(self, indim, outdim, half_res):
        super(SimpleBlockNew, self).__init__()
        self.indim = indim
        self.outdim = outdim
        self.C1 = nn.Conv2d(indim, outdim, kernel_size=3, stride=2 if
            half_res else 1, padding=1, bias=False)
        self.BN1 = nn.Identity()
        self.C2 = nn.Conv2d(outdim, outdim, kernel_size=3, padding=1, bias=
            False)
        self.BN2 = nn.Identity()
        self.relu1 = nn.ReLU(inplace=True)
        self.relu2 = nn.ReLU(inplace=True)
        self.parametrized_layers = [self.C1, self.C2, self.BN1, self.BN2]
        self.half_res = half_res
        if indim != outdim:
            self.shortcut = nn.Conv2d(indim, outdim, 1, 2 if half_res else 
                1, bias=False)
            self.BNshortcut = nn.Identity()
            self.parametrized_layers.append(self.shortcut)
            self.parametrized_layers.append(self.BNshortcut)
            self.shortcut_type = '1x1'
        else:
            self.shortcut_type = 'identity'
        for layer in self.parametrized_layers:
            init_layer(layer)

    def forward(self, input_0):
        primals_1 = self.C1.weight
        primals_3 = self.C2.weight
        primals_2 = input_0
        output = call([primals_1, primals_2, primals_3])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Aamer98/FeatureNorm</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">SimpleBlock</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">12</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "MIT"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">fbf3d3b4cef81b3351347d272eb51b6cdd9f0cc5</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/Aamer98/FeatureNorm/tree/fbf3d3b4cef81b3351347d272eb51b6cdd9f0cc5</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="11"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Aggregate</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_inference']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/wq/cwq3pmymyuowy4xq7gkn2nwrwrq6bpjfz67zohrfhdkptqayuc4m.py
# Topologically Sorted Source Nodes: [y], Original ATen: [aten.sum]
# Source node to ATen node mapping:
#   y => sum_1
# Graph fragment:
#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%arg0_1, [4]), kwargs = {})
triton_poi_fused_sum_0 = async_compile.triton('triton_poi_fused_sum_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[256], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_sum_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_sum_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (4*x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr0 + (1 + (4*x0)), xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr0 + (2 + (4*x0)), xmask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr0 + (3 + (4*x0)), xmask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp4 = tmp2 + tmp3
    tmp6 = tmp4 + tmp5
    tl.store(out_ptr0 + (x0), tmp6, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, = args
    args.clear()
    assert_size_stride(arg0_1, (4, 4, 4, 4, 4), (256, 64, 16, 4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [y], Original ATen: [aten.sum]
        stream0 = get_raw_stream(0)
        triton_poi_fused_sum_0.run(arg0_1, buf0, 256, grid=grid(256), stream=stream0)
        del arg0_1
    return (buf0, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((4, 4, 4, 4, 4), (256, 64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([arg0_1])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
from torch import nn as nn


class Aggregate(nn.Module):
    """Pooling layer based on sum or average with optional masking.

    Args:
        axis (int): axis along which pooling is done.
        mean (bool, optional): if True, use average instead for sum pooling.
        keepdim (bool, optional): whether the output tensor has dim retained or not.

    """

    def __init__(self, axis, mean=False, keepdim=True):
        super(Aggregate, self).__init__()
        self.average = mean
        self.axis = axis
        self.keepdim = keepdim

    def forward(self, input, mask=None):
        """Compute layer output.

        Args:
            input (torch.Tensor): input data.
            mask (torch.Tensor, optional): mask to be applied; e.g. neighbors mask.

        Returns:
            torch.Tensor: layer output.

        """
        if mask is not None:
            input = input * mask[..., None]
        y = torch.sum(input, self.axis)
        if self.average:
            if mask is not None:
                N = torch.sum(mask, self.axis, keepdim=self.keepdim)
                N = torch.max(N, other=torch.ones_like(N))
            else:
                N = input.size(self.axis)
            y = y / N
        return y


def get_inputs():
    return [torch.rand([4, 4, 4, 4, 4])]


def get_init_inputs():
    return [[], {'axis': 4}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch import nn as nn
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda


@triton.jit
def triton_poi_fused_sum_0(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + 4 * x0, xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr0 + (1 + 4 * x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr0 + (2 + 4 * x0), xmask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr0 + (3 + 4 * x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp4 = tmp2 + tmp3
    tmp6 = tmp4 + tmp5
    tl.store(out_ptr0 + x0, tmp6, xmask)


def call(args):
    arg0_1, = args
    args.clear()
    assert_size_stride(arg0_1, (4, 4, 4, 4, 4), (256, 64, 16, 4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.float32)
        get_raw_stream(0)
        triton_poi_fused_sum_0[grid(256)](arg0_1, buf0, 256, XBLOCK=128,
            num_warps=4, num_stages=1)
        del arg0_1
    return buf0,


class AggregateNew(nn.Module):
    """Pooling layer based on sum or average with optional masking.

    Args:
        axis (int): axis along which pooling is done.
        mean (bool, optional): if True, use average instead for sum pooling.
        keepdim (bool, optional): whether the output tensor has dim retained or not.

    """

    def __init__(self, axis, mean=False, keepdim=True):
        super(AggregateNew, self).__init__()
        self.average = mean
        self.axis = axis
        self.keepdim = keepdim

    def forward(self, input_0):
        arg0_1 = input_0
        output = call([arg0_1])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">AlexanderDKazakov/schnetpack</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Aggregate</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">13</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "MIT"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">97b82469d977981b500e439a6c93696d8dac8a3f</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/AlexanderDKazakov/schnetpack/tree/97b82469d977981b500e439a6c93696d8dac8a3f</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="12"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">KaggleAccuracy</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_inference']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/tb/ctbghj42v5gqfb4hjrtagncr6rwduqbnmvrhbowejkjnd2qzwaye.py
# Topologically Sorted Source Nodes: [x_3], Original ATen: [aten.mean]
# Source node to ATen node mapping:
#   x_3 => mean
# Graph fragment:
#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%view, [1]), kwargs = {})
triton_red_fused_mean_0 = async_compile.triton('triton_red_fused_mean_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.reduction(
    size_hints=[8192, 512],
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mean_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 0, 'num_reduction': 1, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
)
@triton.jit
def triton_red_fused_mean_0(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 5776
    rnumel = 484
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex &lt; xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 1444
    x1 = (xindex // 1444)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex &lt; rnumel
        r2 = rindex
        tmp0 = (11*(x0 // 38)) + ((r2 // 11) % 11)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = 0.009569377990430622
        tmp3 = tmp1 * tmp2
        tmp4 = tmp3.to(tl.int32)
        tmp5 = (11*(x0 % 38)) + (r2 % 11)
        tmp6 = tmp5.to(tl.float32)
        tmp7 = tmp6 * tmp2
        tmp8 = tmp7.to(tl.int32)
        tmp9 = tl.load(in_ptr0 + (tmp8 + (4*tmp4) + (16*(r2 // 121)) + (64*x1)), rmask &amp; xmask, eviction_policy='evict_last', other=0.0)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask &amp; xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x0 + (1472*x1)), tmp11, xmask)
''', device_str='cuda')


# kernel path: runs/run_shard_6/inductor_cache/q7/cq7ks2q74csjgm3oa6yac2keazqoatflpcmxe7azl6dscnvlb422.py
# Topologically Sorted Source Nodes: [x_3, gt, x_4, x_8, gt_1, y, eq, float_5, acc], Original ATen: [aten.mean, aten.gt, aten._to_copy, aten.eq]
# Source node to ATen node mapping:
#   acc => mean_2
#   eq => eq
#   float_5 => convert_element_type_10
#   gt => gt
#   gt_1 => gt_1
#   x_3 => mean
#   x_4 => convert_element_type_4
#   x_8 => mean_1
#   y => convert_element_type_9
# Graph fragment:
#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%view, [1]), kwargs = {})
#   %gt : [num_users=1] = call_function[target=torch.ops.aten.gt.Scalar](args = (%mean, 0.25), kwargs = {})
#   %convert_element_type_4 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%gt, torch.float32), kwargs = {})
#   %mean_1 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%view_1, [1]), kwargs = {})
#   %gt_1 : [num_users=1] = call_function[target=torch.ops.aten.gt.Scalar](args = (%mean_1, 0.25), kwargs = {})
#   %convert_element_type_9 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%gt_1, torch.float32), kwargs = {})
#   %eq : [num_users=1] = call_function[target=torch.ops.aten.eq.Tensor](args = (%convert_element_type_4, %convert_element_type_9), kwargs = {})
#   %convert_element_type_10 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%eq, torch.float32), kwargs = {})
#   %mean_2 : [num_users=1] = call_function[target=torch.ops.aten.mean.default](args = (%convert_element_type_10,), kwargs = {})
triton_red_fused__to_copy_eq_gt_mean_1 = async_compile.triton('triton_red_fused__to_copy_eq_gt_mean_1', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.reduction(
    size_hints=[1, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {3: 1}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused__to_copy_eq_gt_mean_1', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
)
@triton.jit
def triton_red_fused__to_copy_eq_gt_mean_1(in_out_ptr0, in_ptr0, in_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1
    rnumel = 5776
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, RBLOCK], True, tl.int1)
    rbase = tl.arange(0, RBLOCK)[None, :]
    _tmp13 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex &lt; rnumel
        r0 = rindex % 1444
        r1 = (rindex // 1444)
        tmp0 = tl.load(in_ptr0 + (r0 + (1472*r1)), rmask, eviction_policy='evict_first', other=0.0)
        tmp6 = tl.load(in_ptr1 + (r0 + (1472*r1)), rmask, eviction_policy='evict_first', other=0.0)
        tmp1 = 484.0
        tmp2 = tmp0 / tmp1
        tmp3 = 0.25
        tmp4 = tmp2 > tmp3
        tmp5 = tmp4.to(tl.float32)
        tmp7 = tmp6 / tmp1
        tmp8 = tmp7 > tmp3
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp5 == tmp9
        tmp11 = tmp10.to(tl.float32)
        tmp12 = tl.broadcast_to(tmp11, [XBLOCK, RBLOCK])
        tmp14 = _tmp13 + tmp12
        _tmp13 = tl.where(rmask, tmp14, _tmp13)
    tmp13 = tl.sum(_tmp13, 1)[:, None]
    tmp15 = 5776.0
    tmp16 = tmp13 / tmp15
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (tl.full([XBLOCK, 1], 0, tl.int32)), tmp16, None)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, arg1_1 = args
    args.clear()
    assert_size_stride(arg0_1, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(arg1_1, (4, 4, 4, 4), (64, 16, 4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 1444), (1472, 1), torch.float32)
        # Topologically Sorted Source Nodes: [x_3], Original ATen: [aten.mean]
        stream0 = get_raw_stream(0)
        triton_red_fused_mean_0.run(arg0_1, buf0, 5776, 484, grid=grid(5776), stream=stream0)
        del arg0_1
        buf1 = empty_strided_cuda((4, 1444), (1472, 1), torch.float32)
        # Topologically Sorted Source Nodes: [x_8], Original ATen: [aten.mean]
        triton_red_fused_mean_0.run(arg1_1, buf1, 5776, 484, grid=grid(5776), stream=stream0)
        del arg1_1
        buf2 = empty_strided_cuda((), (), torch.float32)
        buf3 = buf2; del buf2  # reuse
        # Topologically Sorted Source Nodes: [x_3, gt, x_4, x_8, gt_1, y, eq, float_5, acc], Original ATen: [aten.mean, aten.gt, aten._to_copy, aten.eq]
        triton_red_fused__to_copy_eq_gt_mean_1.run(buf3, buf0, buf1, 1, 5776, grid=grid(1), stream=stream0)
        del buf0
        del buf1
    return (buf3, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    arg1_1 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([arg0_1, arg1_1])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
from torch import Tensor
from torch import nn


class KaggleAccuracy(nn.Module):

    def __init__(self, threshold: 'float'=0.25, num_patches: 'int'=38, size:
        'int'=418) ->None:
        super().__init__()
        self.threshold = threshold
        self.num_patches = num_patches
        self.patch_size = size // num_patches
        self.resize = nn.Upsample(size=size)
        self.unfold = nn.Unfold(kernel_size=self.patch_size, stride=self.
            patch_size)

    def forward(self, x: 'Tensor', y: 'Tensor') ->Tensor:
        x = self.to_patch_value(x)
        y = self.to_patch_value(y)
        acc = torch.mean((x == y).float())
        return acc

    def to_patch_value(self, x: 'Tensor') ->Tensor:
        x = x.float()
        x = self.resize(x)
        x = self.unfold(x)
        x = x.mean(dim=1)
        return (x > self.threshold).float()


def get_inputs():
    return [torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])]


def get_init_inputs():
    return [[], {}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch import Tensor
from torch import nn
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda


@triton.jit
def triton_red_fused_mean_0(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK: tl.
    constexpr, RBLOCK: tl.constexpr):
    xnumel = 5776
    rnumel = 484
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex &lt; xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 1444
    x1 = xindex // 1444
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex &lt; rnumel
        r2 = rindex
        tmp0 = 11 * (x0 // 38) + r2 // 11 % 11
        tmp1 = tmp0.to(tl.float32)
        tmp2 = 0.009569377990430622
        tmp3 = tmp1 * tmp2
        tmp4 = tmp3.to(tl.int32)
        tmp5 = 11 * (x0 % 38) + r2 % 11
        tmp6 = tmp5.to(tl.float32)
        tmp7 = tmp6 * tmp2
        tmp8 = tmp7.to(tl.int32)
        tmp9 = tl.load(in_ptr0 + (tmp8 + 4 * tmp4 + 16 * (r2 // 121) + 64 *
            x1), rmask &amp; xmask, eviction_policy='evict_last', other=0.0)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask &amp; xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x0 + 1472 * x1), tmp11, xmask)


@triton.jit
def triton_red_fused__to_copy_eq_gt_mean_1(in_out_ptr0, in_ptr0, in_ptr1,
    xnumel, rnumel, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):
    rnumel = 5776
    xoffset = tl.program_id(0) * XBLOCK
    xoffset + tl.arange(0, XBLOCK)[:, None]
    tl.full([XBLOCK, RBLOCK], True, tl.int1)
    rbase = tl.arange(0, RBLOCK)[None, :]
    _tmp13 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex &lt; rnumel
        r0 = rindex % 1444
        r1 = rindex // 1444
        tmp0 = tl.load(in_ptr0 + (r0 + 1472 * r1), rmask, eviction_policy=
            'evict_first', other=0.0)
        tmp6 = tl.load(in_ptr1 + (r0 + 1472 * r1), rmask, eviction_policy=
            'evict_first', other=0.0)
        tmp1 = 484.0
        tmp2 = tmp0 / tmp1
        tmp3 = 0.25
        tmp4 = tmp2 > tmp3
        tmp5 = tmp4.to(tl.float32)
        tmp7 = tmp6 / tmp1
        tmp8 = tmp7 > tmp3
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp5 == tmp9
        tmp11 = tmp10.to(tl.float32)
        tmp12 = tl.broadcast_to(tmp11, [XBLOCK, RBLOCK])
        tmp14 = _tmp13 + tmp12
        _tmp13 = tl.where(rmask, tmp14, _tmp13)
    tmp13 = tl.sum(_tmp13, 1)[:, None]
    tmp15 = 5776.0
    tmp16 = tmp13 / tmp15
    tl.debug_barrier()
    tl.store(in_out_ptr0 + tl.full([XBLOCK, 1], 0, tl.int32), tmp16, None)


def call(args):
    arg0_1, arg1_1 = args
    args.clear()
    assert_size_stride(arg0_1, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(arg1_1, (4, 4, 4, 4), (64, 16, 4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 1444), (1472, 1), torch.float32)
        get_raw_stream(0)
        triton_red_fused_mean_0[grid(5776)](arg0_1, buf0, 5776, 484, XBLOCK
            =64, RBLOCK=64, num_warps=16, num_stages=1)
        del arg0_1
        buf1 = empty_strided_cuda((4, 1444), (1472, 1), torch.float32)
        triton_red_fused_mean_0[grid(5776)](arg1_1, buf1, 5776, 484, XBLOCK
            =64, RBLOCK=64, num_warps=16, num_stages=1)
        del arg1_1
        buf2 = empty_strided_cuda((), (), torch.float32)
        buf3 = buf2
        del buf2
        triton_red_fused__to_copy_eq_gt_mean_1[grid(1)](buf3, buf0, buf1, 1,
            5776, XBLOCK=1, RBLOCK=2048, num_warps=16, num_stages=1)
        del buf0
        del buf1
    return buf3,


class KaggleAccuracyNew(nn.Module):

    def __init__(self, threshold: 'float'=0.25, num_patches: 'int'=38, size:
        'int'=418) ->None:
        super().__init__()
        self.threshold = threshold
        self.num_patches = num_patches
        self.patch_size = size // num_patches
        self.resize = nn.Upsample(size=size)
        self.unfold = nn.Unfold(kernel_size=self.patch_size, stride=self.
            patch_size)

    def to_patch_value(self, x: 'Tensor') ->Tensor:
        x = x.float()
        x = self.resize(x)
        x = self.unfold(x)
        x = x.mean(dim=1)
        return (x > self.threshold).float()

    def forward(self, input_0, input_1):
        arg0_1 = input_0
        arg1_1 = input_1
        output = call([arg0_1, arg1_1])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">AlessandroRuzzi/Computational-Intelligence-Lab-2021</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">KaggleAccuracy</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">14</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "MIT"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">ed9dae37618e0ca2f01c4e336df4354e77e00c1f</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/AlessandroRuzzi/Computational-Intelligence-Lab-2021/tree/ed9dae37618e0ca2f01c4e336df4354e77e00c1f</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="13"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">CosAttention</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_forward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/tn/ctnflfpgkxcj4tzximbprecg4kfrgg46s62bv5hdfn6wxh7xshw6.py
# Topologically Sorted Source Nodes: [mul, mul_1, alpha, alpha_1], Original ATen: [aten.mul, aten.sum]
# Source node to ATen node mapping:
#   alpha => mul_2
#   alpha_1 => sum_1
#   mul => mul
#   mul_1 => mul_1
# Graph fragment:
#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_2, %primals_1), kwargs = {})
#   %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul, %primals_3), kwargs = {})
#   %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1, %primals_4), kwargs = {})
#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_2, [-1]), kwargs = {})
triton_poi_fused_mul_sum_0 = async_compile.triton('triton_poi_fused_mul_sum_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[64], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_sum_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 16, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_mul_sum_0(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x2 = xindex
    x0 = xindex % 4
    tmp0 = tl.load(in_ptr0 + (4*x2), xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (4*x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr2 + (4*x2), xmask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (4*x0), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr0 + (1 + (4*x2)), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr1 + (1 + (4*x0)), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr2 + (1 + (4*x2)), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr3 + (1 + (4*x0)), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr0 + (2 + (4*x2)), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr1 + (2 + (4*x0)), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr2 + (2 + (4*x2)), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr3 + (2 + (4*x0)), xmask, eviction_policy='evict_last')
    tmp23 = tl.load(in_ptr0 + (3 + (4*x2)), xmask, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr1 + (3 + (4*x0)), xmask, eviction_policy='evict_last')
    tmp26 = tl.load(in_ptr2 + (3 + (4*x2)), xmask, eviction_policy='evict_last')
    tmp28 = tl.load(in_ptr3 + (3 + (4*x0)), xmask, eviction_policy='evict_last')
    tmp2 = tmp0 * tmp1
    tmp4 = tmp2 * tmp3
    tmp6 = tmp4 * tmp5
    tmp9 = tmp7 * tmp8
    tmp11 = tmp9 * tmp10
    tmp13 = tmp11 * tmp12
    tmp14 = tmp6 + tmp13
    tmp17 = tmp15 * tmp16
    tmp19 = tmp17 * tmp18
    tmp21 = tmp19 * tmp20
    tmp22 = tmp14 + tmp21
    tmp25 = tmp23 * tmp24
    tmp27 = tmp25 * tmp26
    tmp29 = tmp27 * tmp28
    tmp30 = tmp22 + tmp29
    tl.store(out_ptr0 + (x2), tmp30, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    primals_1, primals_2, primals_3, primals_4 = args
    args.clear()
    assert_size_stride(primals_1, (1, 4, 4), (16, 4, 1))
    assert_size_stride(primals_2, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_3, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_4, (1, 4, 4), (16, 4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4), (16, 4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [mul, mul_1, alpha, alpha_1], Original ATen: [aten.mul, aten.sum]
        stream0 = get_raw_stream(0)
        triton_poi_fused_mul_sum_0.run(primals_2, primals_1, primals_3, primals_4, buf0, 64, grid=grid(64), stream=stream0)
    return (buf0, primals_1, primals_2, primals_3, primals_4, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((1, 4, 4), (16, 4, 1), device='cuda:0', dtype=torch.float32)
    primals_2 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    primals_3 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    primals_4 = rand_strided((1, 4, 4), (16, 4, 1), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([primals_1, primals_2, primals_3, primals_4])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.nn import Parameter


class ConstAttention(nn.Module):

    def __init__(self, **kwargs):
        super(ConstAttention, self).__init__()

    def forward(self, neighbor_vecs, self_vecs):
        return 1


class GatAttention(ConstAttention):

    def __init__(self, num_heads, out_channels):
        super(GatAttention, self).__init__()
        self.num_heads = num_heads
        self.out_channels = out_channels
        self.att_self_weight = Parameter(torch.Tensor(1, self.num_heads,
            self.out_channels))
        self.att_neighbor_weight = Parameter(torch.Tensor(1, self.num_heads,
            self.out_channels))
        self.reset_parameters()

    def reset_parameters(self):
        pass

    def forward(self, neighbor_vecs, self_vecs):
        alpha = (self_vecs * self.att_self_weight).sum(dim=-1) + (neighbor_vecs
             * self.att_neighbor_weight).sum(dim=-1)
        alpha = F.leaky_relu(alpha, negative_slope=0.2)
        return alpha


class CosAttention(GatAttention):

    def forward(self, neighbor_vecs, self_vecs):
        alpha = (neighbor_vecs * self.att_neighbor_weight * self_vecs *
            self.att_self_weight)
        alpha = alpha.sum(dim=-1)
        return alpha


def get_inputs():
    return [torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])]


def get_init_inputs():
    return [[], {'num_heads': 4, 'out_channels': 4}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch.nn.functional as F
import torch.nn as nn
from torch.nn import Parameter
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda


@triton.jit
def triton_poi_fused_mul_sum_0(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0,
    xnumel, XBLOCK: tl.constexpr):
    xnumel = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x2 = xindex
    x0 = xindex % 4
    tmp0 = tl.load(in_ptr0 + 4 * x2, xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + 4 * x0, xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr2 + 4 * x2, xmask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + 4 * x0, xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr0 + (1 + 4 * x2), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr1 + (1 + 4 * x0), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr2 + (1 + 4 * x2), xmask, eviction_policy='evict_last'
        )
    tmp12 = tl.load(in_ptr3 + (1 + 4 * x0), xmask, eviction_policy='evict_last'
        )
    tmp15 = tl.load(in_ptr0 + (2 + 4 * x2), xmask, eviction_policy='evict_last'
        )
    tmp16 = tl.load(in_ptr1 + (2 + 4 * x0), xmask, eviction_policy='evict_last'
        )
    tmp18 = tl.load(in_ptr2 + (2 + 4 * x2), xmask, eviction_policy='evict_last'
        )
    tmp20 = tl.load(in_ptr3 + (2 + 4 * x0), xmask, eviction_policy='evict_last'
        )
    tmp23 = tl.load(in_ptr0 + (3 + 4 * x2), xmask, eviction_policy='evict_last'
        )
    tmp24 = tl.load(in_ptr1 + (3 + 4 * x0), xmask, eviction_policy='evict_last'
        )
    tmp26 = tl.load(in_ptr2 + (3 + 4 * x2), xmask, eviction_policy='evict_last'
        )
    tmp28 = tl.load(in_ptr3 + (3 + 4 * x0), xmask, eviction_policy='evict_last'
        )
    tmp2 = tmp0 * tmp1
    tmp4 = tmp2 * tmp3
    tmp6 = tmp4 * tmp5
    tmp9 = tmp7 * tmp8
    tmp11 = tmp9 * tmp10
    tmp13 = tmp11 * tmp12
    tmp14 = tmp6 + tmp13
    tmp17 = tmp15 * tmp16
    tmp19 = tmp17 * tmp18
    tmp21 = tmp19 * tmp20
    tmp22 = tmp14 + tmp21
    tmp25 = tmp23 * tmp24
    tmp27 = tmp25 * tmp26
    tmp29 = tmp27 * tmp28
    tmp30 = tmp22 + tmp29
    tl.store(out_ptr0 + x2, tmp30, xmask)


def call(args):
    primals_1, primals_2, primals_3, primals_4 = args
    args.clear()
    assert_size_stride(primals_1, (1, 4, 4), (16, 4, 1))
    assert_size_stride(primals_2, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_3, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_4, (1, 4, 4), (16, 4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((4, 4, 4), (16, 4, 1), torch.float32)
        get_raw_stream(0)
        triton_poi_fused_mul_sum_0[grid(64)](primals_2, primals_1,
            primals_3, primals_4, buf0, 64, XBLOCK=64, num_warps=1,
            num_stages=1)
    return buf0, primals_1, primals_2, primals_3, primals_4


class ConstAttention(nn.Module):

    def __init__(self, **kwargs):
        super(ConstAttention, self).__init__()

    def forward(self, neighbor_vecs, self_vecs):
        return 1


class GatAttention(ConstAttention):

    def __init__(self, num_heads, out_channels):
        super(GatAttention, self).__init__()
        self.num_heads = num_heads
        self.out_channels = out_channels
        self.att_self_weight = Parameter(torch.Tensor(1, self.num_heads,
            self.out_channels))
        self.att_neighbor_weight = Parameter(torch.Tensor(1, self.num_heads,
            self.out_channels))
        self.reset_parameters()

    def reset_parameters(self):
        pass

    def forward(self, neighbor_vecs, self_vecs):
        alpha = (self_vecs * self.att_self_weight).sum(dim=-1) + (neighbor_vecs
             * self.att_neighbor_weight).sum(dim=-1)
        alpha = F.leaky_relu(alpha, negative_slope=0.2)
        return alpha


class CosAttentionNew(GatAttention):

    def forward(self, input_0, input_1):
        primals_1 = self.att_self_weight
        primals_4 = self.att_neighbor_weight
        primals_2 = input_0
        primals_3 = input_1
        output = call([primals_1, primals_2, primals_3, primals_4])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">AlexMinhao/NAS_GNN</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">CosAttention</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">15</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "Apache-2.0"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">89183988a96e1d6baed910ab3843c13282f8b077</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/AlexMinhao/NAS_GNN/tree/89183988a96e1d6baed910ab3843c13282f8b077</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="14"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">FullyConnectedNet</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""># AOT ID: ['0_forward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()


# kernel path: runs/run_shard_6/inductor_cache/r3/cr3febcwm3t44fuoitsx3ou2p6xg4sk4f7unagmmrvffasxf47te.py
# Topologically Sorted Source Nodes: [x], Original ATen: [aten.relu, aten.threshold_backward]
# Source node to ATen node mapping:
#   x => relu
# Graph fragment:
#   %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%view_1,), kwargs = {})
#   %le : [num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu, 0), kwargs = {})
triton_poi_fused_relu_threshold_backward_0 = async_compile.triton('triton_poi_fused_relu_threshold_backward_0', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

@triton_heuristics.pointwise(
    size_hints=[256], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_relu_threshold_backward_0', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_relu_threshold_backward_0(in_out_ptr0, in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x2 = xindex
    x0 = xindex % 4
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp3 = tl.full([1], 0, tl.int32)
    tmp4 = triton_helpers.maximum(tmp3, tmp2)
    tmp5 = 0.0
    tmp6 = tmp4 &lt;= tmp5
    tl.store(in_out_ptr0 + (x2), tmp4, xmask)
    tl.store(out_ptr0 + (x2), tmp6, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    primals_1, primals_2, primals_3, primals_4, primals_5 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4), (4, 1))
    assert_size_stride(primals_2, (4, ), (1, ))
    assert_size_stride(primals_3, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_4, (4, 4), (4, 1))
    assert_size_stride(primals_5, (4, ), (1, ))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((64, 4), (4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(reinterpret_tensor(primals_3, (64, 4), (4, 1), 0), reinterpret_tensor(primals_1, (4, 4), (1, 4), 0), out=buf0)
        del primals_1
        buf1 = reinterpret_tensor(buf0, (4, 4, 4, 4), (64, 16, 4, 1), 0); del buf0  # reuse
        buf3 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.bool)
        # Topologically Sorted Source Nodes: [x], Original ATen: [aten.relu, aten.threshold_backward]
        stream0 = get_raw_stream(0)
        triton_poi_fused_relu_threshold_backward_0.run(buf1, primals_2, buf3, 256, grid=grid(256), stream=stream0)
        del primals_2
        buf2 = empty_strided_cuda((64, 4), (4, 1), torch.float32)
        # Topologically Sorted Source Nodes: [y_hat], Original ATen: [aten.addmm]
        extern_kernels.addmm(primals_5, reinterpret_tensor(buf1, (64, 4), (4, 1), 0), reinterpret_tensor(primals_4, (4, 4), (1, 4), 0), alpha=1, beta=1, out=buf2)
        del primals_5
    return (reinterpret_tensor(buf2, (4, 4, 4, 4), (64, 16, 4, 1), 0), reinterpret_tensor(primals_3, (64, 4), (4, 1), 0), reinterpret_tensor(buf1, (64, 4), (4, 1), 0), primals_4, buf3, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)
    primals_2 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_3 = rand_strided((4, 4, 4, 4), (64, 16, 4, 1), device='cuda:0', dtype=torch.float32)
    primals_4 = rand_strided((4, 4), (4, 1), device='cuda:0', dtype=torch.float32)
    primals_5 = rand_strided((4, ), (1, ), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
import torch.nn as nn
import torch.nn.functional as F


class FullyConnectedNet(nn.Module):
    """https://github.com/VainF/Torch-Pruning/issues/21"""

    def __init__(self, input_size, num_classes, HIDDEN_UNITS):
        super().__init__()
        self.fc1 = nn.Linear(input_size, HIDDEN_UNITS)
        self.fc2 = nn.Linear(HIDDEN_UNITS, num_classes)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        y_hat = self.fc2(x)
        return y_hat


def get_inputs():
    return [torch.rand([4, 4, 4, 4])]


def get_init_inputs():
    return [[], {'input_size': 4, 'num_classes': 4, 'HIDDEN_UNITS': 4}]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">import torch
from torch._inductor.select_algorithm import extern_kernels
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._inductor.runtime import triton_helpers
import torch.nn as nn
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor


@triton.jit
def triton_poi_fused_relu_threshold_backward_0(in_out_ptr0, in_ptr0,
    out_ptr0, xnumel, XBLOCK: tl.constexpr):
    xnumel = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x2 = xindex
    x0 = xindex % 4
    tmp0 = tl.load(in_out_ptr0 + x2, xmask)
    tmp1 = tl.load(in_ptr0 + x0, xmask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp3 = tl.full([1], 0, tl.int32)
    tmp4 = triton_helpers.maximum(tmp3, tmp2)
    tmp5 = 0.0
    tmp6 = tmp4 &lt;= tmp5
    tl.store(in_out_ptr0 + x2, tmp4, xmask)
    tl.store(out_ptr0 + x2, tmp6, xmask)


def call(args):
    primals_1, primals_2, primals_3, primals_4, primals_5 = args
    args.clear()
    assert_size_stride(primals_1, (4, 4), (4, 1))
    assert_size_stride(primals_2, (4,), (1,))
    assert_size_stride(primals_3, (4, 4, 4, 4), (64, 16, 4, 1))
    assert_size_stride(primals_4, (4, 4), (4, 1))
    assert_size_stride(primals_5, (4,), (1,))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((64, 4), (4, 1), torch.float32)
        extern_kernels.mm(reinterpret_tensor(primals_3, (64, 4), (4, 1), 0),
            reinterpret_tensor(primals_1, (4, 4), (1, 4), 0), out=buf0)
        del primals_1
        buf1 = reinterpret_tensor(buf0, (4, 4, 4, 4), (64, 16, 4, 1), 0)
        del buf0
        buf3 = empty_strided_cuda((4, 4, 4, 4), (64, 16, 4, 1), torch.bool)
        get_raw_stream(0)
        triton_poi_fused_relu_threshold_backward_0[grid(256)](buf1,
            primals_2, buf3, 256, XBLOCK=256, num_warps=4, num_stages=1)
        del primals_2
        buf2 = empty_strided_cuda((64, 4), (4, 1), torch.float32)
        extern_kernels.addmm(primals_5, reinterpret_tensor(buf1, (64, 4), (
            4, 1), 0), reinterpret_tensor(primals_4, (4, 4), (1, 4), 0),
            alpha=1, beta=1, out=buf2)
        del primals_5
    return reinterpret_tensor(buf2, (4, 4, 4, 4), (64, 16, 4, 1), 0
        ), reinterpret_tensor(primals_3, (64, 4), (4, 1), 0
        ), reinterpret_tensor(buf1, (64, 4), (4, 1), 0), primals_4, buf3


class FullyConnectedNetNew(nn.Module):
    """https://github.com/VainF/Torch-Pruning/issues/21"""

    def __init__(self, input_size, num_classes, HIDDEN_UNITS):
        super().__init__()
        self.fc1 = nn.Linear(input_size, HIDDEN_UNITS)
        self.fc2 = nn.Linear(HIDDEN_UNITS, num_classes)

    def forward(self, input_0):
        primals_1 = self.fc1.weight
        primals_2 = self.fc1.bias
        primals_4 = self.fc2.weight
        primals_5 = self.fc2.bias
        primals_3 = input_0
        output = call([primals_1, primals_2, primals_3, primals_4, primals_5])
        return output[0]
</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Abhishekvats1997/Torch-Pruning</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">FullyConnectedNet</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto"><span class="">false</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">16</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "MIT"
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">b322a42d1c9032cc9644332d33a9662ca6ed44ac</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">https://github.com/Abhishekvats1997/Torch-Pruning/tree/b322a42d1c9032cc9644332d33a9662ca6ed44ac</span></div></div>
							</td>
					</tr></tbody></table>
		<div class="bg-linear-to-b sticky left-0 border-t border-dashed border-gray-300 from-gray-100 to-white py-3 text-center font-mono text-xs dark:border-gray-700 dark:from-gray-950 dark:to-gray-900">End of preview. <a href="/datasets/GPUMODE/Inductor_Created_Data_Permissive/viewer/default/train" class="group"><span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Expand</span>
						in <svg class="text-lg mr-0.5 inline -translate-y-px text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>Data Studio
					</a></div></div>




			<div class="bg-linear-to-b from-gray-100 to-white dark:from-gray-950 dark:to-gray-900 "><hr class="flex-none -translate-y-px border-t border-dashed border-gray-300 bg-white dark:border-gray-700 dark:bg-gray-950">
					<nav><ul class="flex select-none items-center justify-between space-x-2 text-gray-700 sm:justify-center py-1 text-center font-mono text-xs rounded-b-lg"><li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 pointer-events-none cursor-default text-gray-400 hover:text-gray-700" href=""><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg>
		Previous</a></li>
			<li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1 bg-gray-50 font-semibold ring-1 ring-inset ring-gray-200 dark:bg-gray-900 dark:text-yellow-500 dark:ring-gray-900 hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/GPUMODE/Inductor_Created_Data_Permissive/viewer/default/train?p=0">1</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/GPUMODE/Inductor_Created_Data_Permissive/viewer/default/train?p=1">2</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/GPUMODE/Inductor_Created_Data_Permissive/viewer/default/train?p=2">3</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  pointer-events-none cursor-default" href="#">...</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/GPUMODE/Inductor_Created_Data_Permissive/viewer/default/train?p=181">182</a>
				</li>
			<li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 " href="/datasets/GPUMODE/Inductor_Created_Data_Permissive/viewer/default/train?p=1">Next
		<svg class="ml-1.5 transform rotate-180" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg></a></li></ul></nav></div></div>






</div></div></div></div></div></div></div>

				
					<div class="SVELTE_HYDRATER contents" data-target="RepoCodeCopy" data-props="{}"><div></div></div>
					

					<div class="SVELTE_HYDRATER contents" data-target="SideNavigation" data-props="{&quot;titleTree&quot;:[{&quot;id&quot;:&quot;overview&quot;,&quot;label&quot;:&quot;Overview&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Overview&quot;},{&quot;id&quot;:&quot;dataset-creation&quot;,&quot;label&quot;:&quot;Dataset Creation&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Dataset Creation&quot;},{&quot;id&quot;:&quot;data-structure&quot;,&quot;label&quot;:&quot;Data Structure&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Data Structure&quot;},{&quot;id&quot;:&quot;file-formats&quot;,&quot;label&quot;:&quot;File Formats&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;File Formats&quot;},{&quot;id&quot;:&quot;usage-examples&quot;,&quot;label&quot;:&quot;Usage Examples&quot;,&quot;children&quot;:[{&quot;id&quot;:&quot;loading-the-dataset-in-python&quot;,&quot;label&quot;:&quot;Loading the Dataset in Python&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Loading the Dataset in Python&quot;}],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Usage Examples&quot;},{&quot;id&quot;:&quot;license-information&quot;,&quot;label&quot;:&quot;License Information&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;License Information&quot;}],&quot;classNames&quot;:&quot;top-6&quot;}">

<div class="absolute -left-12 bottom-0 top-0 z-10 top-6"><div class="sticky top-4 flex"><div class="h-7 pt-[0.175rem]">
				<span class="peer" tabindex="0"><button class="select-none hover:cursor-pointer"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-lg opacity-80 hover:opacity-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg></button></span>
				<div class="invisible w-0 -translate-x-24 -translate-y-6 overflow-hidden rounded-xl border bg-white transition-transform hover:visible hover:w-52 hover:translate-x-0 peer-focus-within:visible peer-focus-within:w-52 peer-focus-within:translate-x-0"><nav aria-label="Secondary" class="max-h-[550px] overflow-y-auto p-3"><ul><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 *:break-words hover:underline active:text-gray-900 dark:active:text-gray-200" href="#overview" title="Overview"><!-- HTML_TAG_START -->Overview<!-- HTML_TAG_END --></a>
									<ul class="pl-1"></ul>
								</li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 *:break-words hover:underline active:text-gray-900 dark:active:text-gray-200" href="#dataset-creation" title="Dataset Creation"><!-- HTML_TAG_START -->Dataset Creation<!-- HTML_TAG_END --></a>
									<ul class="pl-1"></ul>
								</li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 *:break-words hover:underline active:text-gray-900 dark:active:text-gray-200" href="#data-structure" title="Data Structure"><!-- HTML_TAG_START -->Data Structure<!-- HTML_TAG_END --></a>
									<ul class="pl-1"></ul>
								</li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 *:break-words hover:underline active:text-gray-900 dark:active:text-gray-200" href="#file-formats" title="File Formats"><!-- HTML_TAG_START -->File Formats<!-- HTML_TAG_END --></a>
									<ul class="pl-1"></ul>
								</li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 *:break-words hover:underline active:text-gray-900 dark:active:text-gray-200" href="#usage-examples" title="Usage Examples"><!-- HTML_TAG_START -->Usage Examples<!-- HTML_TAG_END --></a>
									<ul class="pl-1"><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="#loading-the-dataset-in-python" title="Loading the Dataset in Python"><!-- HTML_TAG_START -->Loading the Dataset in Python<!-- HTML_TAG_END --></a>
												<ul class="pl-2"></ul>
											</li></ul>
								</li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 *:break-words hover:underline active:text-gray-900 dark:active:text-gray-200" href="#license-information" title="License Information"><!-- HTML_TAG_START -->License Information<!-- HTML_TAG_END --></a>
									<ul class="pl-1"></ul>
								</li></ul></nav></div></div></div></div></div>
					<div class="2xl:pr-6"><div class="prose pl-6 -ml-6 hf-sanitized hf-sanitized-Paq3auN95U80Hsj8ItACQ">
	<!-- HTML_TAG_START --><h2 class="relative group flex items-center">
	<a rel="nofollow" href="#overview" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="overview">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Overview
	</span>
</h2>
<p><code>dataset_permissive{.json/.parquet}</code> is a curated collection of pairs of pytorch programs and equivalent triton code (generated by torch inductor) which can be used to train models to translate pytorch code to triton code.</p>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="#dataset-creation" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-creation">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Creation
	</span>
</h2>
<p>The dataset was created through the following process:</p>
<ol>
<li><strong>Repository Collection</strong>: PyTorch repositories were collected from GitHub using repositories (and associated hashes) from the <a rel="nofollow" href="https://huggingface.co/datasets/bigcode/the-stack">Stack v1</a>.</li>
<li><strong>PyTorch Module Extraction</strong>: We extracted the pytorch code from the repositories, and seperated them into individual <code>torch.nn</code> modules with appropriate dependencies.</li>
<li><strong>Creating Unit Tests</strong>: We created unit tests for each module to ensure that the code was working as expected. Code in which could not create unit tests for was removed.</li>
<li><strong>Extracting Triton Code</strong>: We used torch.compile in order to produce triton code from the pytorch code.</li>
<li><strong>Transorming Triton Code</strong>: We transformed the triton code into one which resembled the format seen in <a rel="nofollow" href="https://github.com/ScalingIntelligence/KernelBench">KernelBench</a>.</li>
<li><strong>Metadata Enrichment</strong>: Each repository entry was enriched with metadata such as license information, star count, and commit SHA.</li>
</ol>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="#data-structure" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="data-structure">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Data Structure
	</span>
</h2>
<p>Each entry in the dataset contains the following fields:</p>
<div class="max-w-full overflow-auto">
	<table>
		<thead><tr>
<th>Field</th>
<th>Description</th>
</tr>

		</thead><tbody><tr>
<td><code>repo_name</code></td>
<td>The name of the repository in the format <code>username/repository</code></td>
</tr>
<tr>
<td><code>licenses</code></td>
<td>List of licenses associated with the repository</td>
</tr>
<tr>
<td><code>stars</code></td>
<td>Number of GitHub stars the repository has</td>
</tr>
<tr>
<td><code>sha</code></td>
<td>The commit SHA hash used for version reference</td>
</tr>
<tr>
<td><code>repo_link</code></td>
<td>Direct link to the repository at the specific commit (GitHub URL)</td>
</tr>
<tr>
<td><em>Additional fields</em></td>
<td>The dataset may contain other repository-specific information</td>
</tr>
</tbody>
	</table>
</div>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="#file-formats" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="file-formats">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		File Formats
	</span>
</h2>
<p>The dataset is available in two formats:</p>
<ol>
<li><strong>JSON</strong>: <code>dataset_permissive.json</code> - A human-readable format that can be easily parsed by most programming languages.</li>
<li><strong>Parquet</strong>: <code>dataset_permissive.parquet</code> - A columnar storage format optimized for analytics and big data processing.</li>
</ol>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="#usage-examples" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="usage-examples">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Usage Examples
	</span>
</h2>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="#loading-the-dataset-in-python" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="loading-the-dataset-in-python">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Loading the Dataset in Python
	</span>
</h3>
<h4 class="relative group flex items-center">
	<a rel="nofollow" href="#using-json" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="using-json">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Using JSON:
	</span>
</h4>
<pre><code class="language-python"><span class="hljs-keyword">import</span> json

<span class="hljs-comment"># Load the JSON version</span>
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'dataset_permissive.json'</span>, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:
    repos = json.load(f)

<span class="hljs-comment"># Example: Print the first 5 repository names</span>
<span class="hljs-keyword">for</span> repo <span class="hljs-keyword">in</span> repos[:<span class="hljs-number">5</span>]:
    <span class="hljs-built_in">print</span>(repo[<span class="hljs-string">'repo_name'</span>])
</code></pre>
<h4 class="relative group flex items-center">
	<a rel="nofollow" href="#using-parquet" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="using-parquet">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Using Parquet:
	</span>
</h4>
<pre><code class="language-python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

<span class="hljs-comment"># Load the Parquet version</span>
df = pd.read_parquet(<span class="hljs-string">'dataset_permissive.parquet'</span>)

<span class="hljs-comment"># Example: Get repositories with more than 1000 stars</span>
popular_repos = df[df[<span class="hljs-string">'stars'</span>] &gt; <span class="hljs-number">1000</span>]
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Number of popular repositories: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(popular_repos)}</span>"</span>)
</code></pre>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="#license-information" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="license-information">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		License Information
	</span>
</h2>
<p>The <code>dataset_permissive</code> contains only repositories with permissive licenses, including but not limited to:</p>
<ul>
<li>MIT License</li>
<li>Apache License 2.0</li>
<li>BSD Licenses (various)</li>
<li>Mozilla Public License</li>
<li>Unlicense</li>
<li>zlib License</li>
</ul>
<p>The dataset itself is provided for research and development purposes. Users should still verify the license of individual repositories before using their code in production or commercial settings.</p>
<!-- HTML_TAG_END --></div>
</div></section>
			
			<section class="pt-6 border-gray-100 md:pb-24 md:pl-6 md:w-64 lg:w-80 xl:w-96 flex-none order-first md:order-none md:border-l pt-3! md:pt-6!"><dl class="flex items-baseline justify-between"><dt class="text-sm text-gray-500">Downloads last month</dt><div class="mx-4 flex-1 border-b border-dotted"></div><dd class="font-semibold">39</dd></dl>
				
				<div class="divider-column-vertical"></div>
				<div class="grid grid-cols-2 gap-x-2 md:flex md:flex-row md:flex-wrap"><div class="SVELTE_HYDRATER contents" data-target="DatasetAndModelActionsDropdown" data-props="{&quot;classNames&quot;:&quot;order-last&quot;,&quot;discussionsDisabled&quot;:false,&quot;repo&quot;:{&quot;type&quot;:&quot;dataset&quot;,&quot;name&quot;:&quot;GPUMODE/Inductor_Created_Data_Permissive&quot;},&quot;canWrite&quot;:false,&quot;canDisable&quot;:false,&quot;repoIsPrivate&quot;:false,&quot;repoIsGated&quot;:false,&quot;repoIsDisabled&quot;:false,&quot;repoIsAdminFlaggedNFAA&quot;:false,&quot;repoHasBlockedOids&quot;:false}"><div class="order-last"><div class="relative ">
	<button class="btn px-1.5 py-1.5 " type="button">
		
			<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="p-0.5" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><circle cx="16" cy="7" r="3" fill="currentColor"></circle><circle cx="16" cy="16" r="3" fill="currentColor"></circle><circle cx="16" cy="25" r="3" fill="currentColor"></circle></svg>
		
		</button>
	
	
	</div></div>













</div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetLibrary" data-props="{&quot;classNames&quot;:&quot;md:w-full xl:w-auto xl:flex-none&quot;,&quot;libraries&quot;:[{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;datasets&quot;,&quot;function&quot;:&quot;load_dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{},&quot;code&quot;:&quot;from datasets import load_dataset\n\n# Login using e.g. `huggingface-cli login` to access this dataset\nds = load_dataset(\&quot;GPUMODE/Inductor_Created_Data_Permissive\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;pandas&quot;,&quot;function&quot;:&quot;pd.read_parquet&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;dataset_permissive.parquet&quot;}},&quot;code&quot;:&quot;import pandas as pd\n\n# Login using e.g. `huggingface-cli login` to access this dataset\ndf = pd.read_parquet(\&quot;hf://datasets/GPUMODE/Inductor_Created_Data_Permissive/dataset_permissive.parquet\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;mlcroissant&quot;,&quot;function&quot;:&quot;Dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;record_set&quot;:&quot;default&quot;,&quot;partial&quot;:false},&quot;code&quot;:&quot;import requests\nfrom huggingface_hub.file_download import build_hf_headers\nfrom mlcroissant import Dataset\n\n# Login using e.g. `huggingface-cli login` to access this dataset\nheaders = build_hf_headers()  # handles authentication\njsonld = requests.get(\&quot;https://huggingface.co/api/datasets/GPUMODE/Inductor_Created_Data_Permissive/croissant\&quot;, headers=headers).json()\nds = Dataset(jsonld=jsonld)\nrecords = ds.records(\&quot;default\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;polars&quot;,&quot;function&quot;:&quot;pl.read_parquet&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;dataset_permissive.parquet&quot;}},&quot;code&quot;:&quot;import polars as pl\n\n# Login using e.g. `huggingface-cli login` to access this dataset\ndf = pl.read_parquet('hf://datasets/GPUMODE/Inductor_Created_Data_Permissive/dataset_permissive.parquet')\n&quot;}]}]}"><div class="relative md:w-full xl:w-auto xl:flex-none">
	<button class="from-gray-800! to-black! max-xl:mb-2 text-white! gap-1! border-gray-800! dark:border-gray-900!  btn w-full cursor-pointer text-sm" type="button">
		<svg class="mr-1.5 mr-0.5! " xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>
			Use this dataset
		<svg class="-mr-1 text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path d="M16.293 9.293L12 13.586L7.707 9.293l-1.414 1.414L12 16.414l5.707-5.707z" fill="currentColor"></path></svg></button>
	
	
	</div>

</div>
					
					</div>
				<div class="divider-column-vertical"></div>
					<div class="flex flex-col flex-wrap xl:flex-row"><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 " href="/datasets/GPUMODE/Inductor_Created_Data_Permissive/tree/refs%2Fconvert%2Fparquet/" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of the auto-converted Parquet files:</div>
		<div class="truncate text-sm group-hover:underline">
			<!-- HTML_TAG_START -->87.7 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Number of rows:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->18,162<!-- HTML_TAG_END --></div></a></div>
				
				
				
				

				
				<div class="divider-column-vertical md:hidden"></div></section></div></main>

	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container relative flex flex-col justify-between space-y-2 py-6 text-gray-500 max-md:*:self-start md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="SVELTE_HYDRATER contents" data-target="ThemeSwitcher" data-props="{&quot;theme&quot;:&quot;system&quot;,&quot;isLoggedIn&quot;:false,&quot;menuClassNames&quot;:&quot;md:-top-24&quot;,&quot;classNames&quot;:&quot;max-md:mb-5 max-md:*:self-start&quot;}">
<div class="relative inline-block max-md:mb-5 max-md:*:self-start">
	<button class="rounded-full border border-gray-100 pl-2 py-1 pr-2.5  flex items-center text-sm text-gray-500 bg-white hover:bg-purple-50 hover:border-purple-200 dark:hover:bg-gray-800 dark:hover:border-gray-950 dark:border-gray-800 " type="button">
		<svg class="mr-1.5 text-gray-500" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M6 22.5h20a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2H6a2 2 0 0 0-2 2v13.5a2 2 0 0 0 2 2ZM7 7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h18a1 1 0 0 0 1-1V8a1 1 0 0 0-1-1H7Z" fill="currentColor"></path><path d="M6 8a1 1 0 0 1 1-1h18a1 1 0 0 1 1 1v11a1 1 0 0 1-1 1H7a1 1 0 0 1-1-1V8Z" fill="currentColor" fill-opacity=".4"></path><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path></svg>
			System theme
		</button>
	
	
	</div></div>
		<div class="font-semibold text-black md:hidden">Company</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="max-md:mb-4! max-md:mt-8! group flex-none max-md:order-last"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="max-md:mt-8! font-semibold text-black md:hidden">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("\/front\/build\/kube-68d7aa0\/index.js");
			window.moonSha = "kube-68d7aa0\/";
			window.__hf_deferred = {};
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>
	</body>
</html>
