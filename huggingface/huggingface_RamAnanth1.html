<!doctype html>
<html class="">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
		<meta name="description" content="Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science." />
		<meta property="fb:app_id" content="1321688464574422" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@huggingface" />
		<meta name="twitter:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/RamAnanth1/talkrl-podcast.png" />
		<meta property="og:title" content="RamAnanth1/talkrl-podcast Â· Datasets at Hugging Face" />
		<meta property="og:type" content="website" />
		<meta property="og:url" content="https://huggingface.co/datasets/RamAnanth1/talkrl-podcast" />
		<meta property="og:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/RamAnanth1/talkrl-podcast.png" />

		<link rel="stylesheet" href="/front/build/kube-68d7aa0/style.css" />

		<link rel="preconnect" href="https://fonts.gstatic.com" />
		<link
			href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap"
			rel="stylesheet"
		/>

		<link
			rel="preload"
			href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css"
			as="style"
			onload="this.onload=null;this.rel='stylesheet'"
		/>
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		<script>const guestTheme = document.cookie.match(/theme=(\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches));</script>
<link rel="canonical" href="https://huggingface.co/datasets/RamAnanth1/talkrl-podcast"> <script type="application/ld+json">{
  "@context": {
    "@language": "en",
    "@vocab": "https:\/\/schema.org\/",
    "citeAs": "cr:citeAs",
    "column": "cr:column",
    "conformsTo": "dct:conformsTo",
    "cr": "http:\/\/mlcommons.org\/croissant\/",
    "data": {
      "@id": "cr:data",
      "@type": "@json"
    },
    "dataBiases": "cr:dataBiases",
    "dataCollection": "cr:dataCollection",
    "dataType": {
      "@id": "cr:dataType",
      "@type": "@vocab"
    },
    "dct": "http:\/\/purl.org\/dc\/terms\/",
    "extract": "cr:extract",
    "field": "cr:field",
    "fileProperty": "cr:fileProperty",
    "fileObject": "cr:fileObject",
    "fileSet": "cr:fileSet",
    "format": "cr:format",
    "includes": "cr:includes",
    "isLiveDataset": "cr:isLiveDataset",
    "jsonPath": "cr:jsonPath",
    "key": "cr:key",
    "md5": "cr:md5",
    "parentField": "cr:parentField",
    "path": "cr:path",
    "personalSensitiveInformation": "cr:personalSensitiveInformation",
    "recordSet": "cr:recordSet",
    "references": "cr:references",
    "regex": "cr:regex",
    "repeated": "cr:repeated",
    "replace": "cr:replace",
    "sc": "https:\/\/schema.org\/",
    "separator": "cr:separator",
    "source": "cr:source",
    "subField": "cr:subField",
    "transform": "cr:transform"
  },
  "@type": "sc:Dataset",
  "distribution": [
    {
      "@type": "cr:FileObject",
      "@id": "repo",
      "name": "repo",
      "description": "The Hugging Face git repository.",
      "contentUrl": "https:\/\/huggingface.co\/datasets\/RamAnanth1\/talkrl-podcast\/tree\/refs%2Fconvert%2Fparquet",
      "encodingFormat": "git+https",
      "sha256": "https:\/\/github.com\/mlcommons\/croissant\/issues\/80"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-default",
      "name": "parquet-files-for-config-default",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "default\/*\/*.parquet"
    }
  ],
  "recordSet": [
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "default_splits\/split_name"
      },
      "@id": "default_splits",
      "name": "default_splits",
      "description": "Splits for the default config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "default_splits\/split_name": "train"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "default",
      "name": "default",
      "description": "RamAnanth1\/talkrl-podcast - 'default' subset",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default\/split",
          "name": "default\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "default\/(?:partial-)?(train)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "default_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/title",
          "name": "default\/title",
          "description": "Column 'title' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "title"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/summary",
          "name": "default\/summary",
          "description": "Column 'summary' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "summary"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/link",
          "name": "default\/link",
          "description": "Column 'link' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "link"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/transcript",
          "name": "default\/transcript",
          "description": "Column 'transcript' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "transcript"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/segments",
          "name": "default\/segments",
          "description": "Column 'segments' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "default\/segments\/end",
              "name": "default\/segments\/end",
              "description": "Column 'segments' from the Hugging Face parquet file.",
              "dataType": "sc:Float",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-default"
                },
                "extract": {
                  "column": "segments"
                },
                "transform": {
                  "jsonPath": "end"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "default\/segments\/start",
              "name": "default\/segments\/start",
              "description": "Column 'segments' from the Hugging Face parquet file.",
              "dataType": "sc:Float",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-default"
                },
                "extract": {
                  "column": "segments"
                },
                "transform": {
                  "jsonPath": "start"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "default\/segments\/text",
              "name": "default\/segments\/text",
              "description": "Column 'segments' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-default"
                },
                "extract": {
                  "column": "segments"
                },
                "transform": {
                  "jsonPath": "text"
                }
              }
            }
          ],
          "repeated": true
        }
      ]
    }
  ],
  "conformsTo": "http:\/\/mlcommons.org\/croissant\/1.0",
  "name": "talkrl-podcast",
  "description": "\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for \"talkrl-podcast\"\n\t\n\nThis dataset is sourced from the TalkRL Podcast website and contains English transcripts of wonderful TalkRL podcast episodes. The transcripts were generated using OpenAI's base Whisper model\n",
  "alternateName": [
    "RamAnanth1\/talkrl-podcast",
    "TalkRL Podcast"
  ],
  "creator": {
    "@type": "Person",
    "name": "Ram Ananth",
    "url": "https:\/\/huggingface.co\/RamAnanth1"
  },
  "keywords": [
    "text-classification",
    "text-generation",
    "summarization",
    "English",
    "\u003c 1K",
    "parquet",
    "Text",
    "Datasets",
    "pandas",
    "Croissant",
    "Polars",
    "ðŸ‡ºðŸ‡¸ Region: US"
  ],
  "url": "https:\/\/huggingface.co\/datasets\/RamAnanth1\/talkrl-podcast"
}</script> 

		<title>RamAnanth1/talkrl-podcast Â· Datasets at Hugging Face</title>

		<script
			defer
			data-domain="huggingface.co"
			event-loggedIn="false"
			src="/js/script.pageview-props.js"
		></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script>
			window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer></script>
	</head>
	<body class="flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DatasetPage">
		<div class="flex min-h-dvh flex-col"><div class="SVELTE_HYDRATER contents" data-target="SystemThemeMonitor" data-props="{&quot;isLoggedIn&quot;:false}"></div>

	<div class="SVELTE_HYDRATER contents" data-target="MainHeader" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false,&quot;isPro&quot;:false}"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg">
				<span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a>
			<div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..."   spellcheck="false" type="text" value="">
	<svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
	</div>
			<div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg>
		</button>

	</div></div>
		<nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 2xl:space-x-2"><li class="hover:text-indigo-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
					Models</a>
			</li><li class="hover:text-red-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					Datasets</a>
			</li><li class="hover:text-blue-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg>
					Spaces</a>
			</li><li class="hover:text-yellow-700 max-xl:hidden"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 text-yellow-500!" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg>
					Posts</a>
			</li><li class="hover:text-yellow-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 16 16"><path d="m2.28 3.7-.3.16a.67.67 0 0 0-.34.58v8.73l.01.04.02.07.01.04.03.06.02.04.02.03.04.06.05.05.04.04.06.04.06.04.08.04.08.02h.05l.07.02h.11l.04-.01.07-.02.03-.01.07-.03.22-.12a5.33 5.33 0 0 1 5.15.1.67.67 0 0 0 .66 0 5.33 5.33 0 0 1 5.33 0 .67.67 0 0 0 1-.58V4.36a.67.67 0 0 0-.34-.5l-.3-.17v7.78a.63.63 0 0 1-.87.59 4.9 4.9 0 0 0-4.35.35l-.65.39a.29.29 0 0 1-.15.04.29.29 0 0 1-.16-.04l-.65-.4a4.9 4.9 0 0 0-4.34-.34.63.63 0 0 1-.87-.59V3.7Z" fill="currentColor" class="dark:opacity-40"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M8 3.1a5.99 5.99 0 0 0-5.3-.43.66.66 0 0 0-.42.62v8.18c0 .45.46.76.87.59a4.9 4.9 0 0 1 4.34.35l.65.39c.05.03.1.04.16.04.05 0 .1-.01.15-.04l.65-.4a4.9 4.9 0 0 1 4.35-.34.63.63 0 0 0 .86-.59V3.3a.67.67 0 0 0-.41-.62 5.99 5.99 0 0 0-5.3.43l-.3.17L8 3.1Zm.73 1.87a.43.43 0 1 0-.86 0v5.48a.43.43 0 0 0 .86 0V4.97Z" fill="currentColor" class="opacity-40 dark:opacity-100"></path><path d="M8.73 4.97a.43.43 0 1 0-.86 0v5.48a.43.43 0 1 0 .86 0V4.96Z" fill="currentColor" class="dark:opacity-40"></path></svg>
					Docs</a>
			</li><li class="hover:text-black dark:hover:text-white"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/enterprise"><svg class="mr-1.5 text-gray-400 group-hover:text-black dark:group-hover:text-white" xmlns="http://www.w3.org/2000/svg" fill="none" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 33 27"><path fill="currentColor" fill-rule="evenodd" d="M13.5.7a8.7 8.7 0 0 0-7.7 5.7L1 20.6c-1 3.1.9 5.7 4.1 5.7h15c3.3 0 6.8-2.6 7.8-5.7l4.6-14.2c1-3.1-.8-5.7-4-5.7h-15Zm1.1 5.7L9.8 20.3h9.8l1-3.1h-5.8l.8-2.5h4.8l1.1-3h-4.8l.8-2.3H23l1-3h-9.5Z" clip-rule="evenodd"></path></svg>
					Enterprise</a>
			</li>

		<li><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/pricing">Pricing
			</a></li>

		<li><div class="relative group">
	<button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button">
		<svg class=" text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>
			
		</button>
	
	
	</div></li>
		<li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li>
		<li><a class="block cursor-pointer whitespace-nowrap px-2 py-0.5 hover:text-gray-500 dark:text-gray-300 dark:hover:text-gray-100" href="/login">Log In
				</a></li>
			<li><a class="whitespace-nowrap rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up
					</a></li></ul></nav></div></header></div>
	
	
	
	<div class="SVELTE_HYDRATER contents" data-target="SSOBanner" data-props="{}"></div>
	
	

	<main class="flex flex-1 flex-col">
	<div class="SVELTE_HYDRATER contents" data-target="DatasetHeader" data-props="{&quot;activeTab&quot;:&quot;datasetCard&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;62aeb75c4b5d61650842dbf4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1671346465329-62aeb75c4b5d61650842dbf4.png&quot;,&quot;fullname&quot;:&quot;Ram Ananth&quot;,&quot;name&quot;:&quot;RamAnanth1&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:27},&quot;canReadRepoSettings&quot;:false,&quot;dataset&quot;:{&quot;author&quot;:&quot;RamAnanth1&quot;,&quot;cardData&quot;:{&quot;dataset_info&quot;:{&quot;features&quot;:[{&quot;name&quot;:&quot;title&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;summary&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;link&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;transcript&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;segments&quot;,&quot;list&quot;:[{&quot;name&quot;:&quot;end&quot;,&quot;dtype&quot;:&quot;float64&quot;},{&quot;name&quot;:&quot;start&quot;,&quot;dtype&quot;:&quot;float64&quot;},{&quot;name&quot;:&quot;text&quot;,&quot;dtype&quot;:&quot;string&quot;}]}],&quot;splits&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;num_bytes&quot;:4845076,&quot;num_examples&quot;:39}],&quot;download_size&quot;:2633561,&quot;dataset_size&quot;:4845076},&quot;task_categories&quot;:[&quot;text-classification&quot;,&quot;text-generation&quot;,&quot;summarization&quot;],&quot;language&quot;:[&quot;en&quot;],&quot;size_categories&quot;:[&quot;n<1K&quot;],&quot;pretty_name&quot;:&quot;TalkRL Podcast&quot;},&quot;cardExists&quot;:true,&quot;createdAt&quot;:&quot;2023-01-10T23:09:01.000Z&quot;,&quot;description&quot;:&quot;\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for \&quot;talkrl-podcast\&quot;\n\t\n\nThis dataset is sourced from the TalkRL Podcast website and contains English transcripts of wonderful TalkRL podcast episodes. The transcripts were generated using OpenAI's base Whisper model\n&quot;,&quot;downloads&quot;:106,&quot;downloadsAllTime&quot;:596,&quot;id&quot;:&quot;RamAnanth1/talkrl-podcast&quot;,&quot;isLikedByUser&quot;:false,&quot;lastModified&quot;:&quot;2023-01-12T20:46:26.000Z&quot;,&quot;likes&quot;:2,&quot;datasetsServerInfo&quot;:{&quot;viewer&quot;:&quot;viewer&quot;,&quot;numRows&quot;:39,&quot;libraries&quot;:[&quot;datasets&quot;,&quot;pandas&quot;,&quot;mlcroissant&quot;,&quot;polars&quot;],&quot;formats&quot;:[&quot;parquet&quot;],&quot;modalities&quot;:[&quot;text&quot;]},&quot;discussionsDisabled&quot;:false,&quot;repoType&quot;:&quot;dataset&quot;,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;tags&quot;:[&quot;task_categories:text-classification&quot;,&quot;task_categories:text-generation&quot;,&quot;task_categories:summarization&quot;,&quot;language:en&quot;,&quot;size_categories:n<1K&quot;,&quot;format:parquet&quot;,&quot;modality:text&quot;,&quot;library:datasets&quot;,&quot;library:pandas&quot;,&quot;library:mlcroissant&quot;,&quot;library:polars&quot;,&quot;region:us&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;task_categories:text-classification&quot;,&quot;label&quot;:&quot;text-classification&quot;,&quot;type&quot;:&quot;task_categories&quot;,&quot;subType&quot;:&quot;nlp&quot;},{&quot;id&quot;:&quot;task_categories:text-generation&quot;,&quot;label&quot;:&quot;text-generation&quot;,&quot;type&quot;:&quot;task_categories&quot;,&quot;subType&quot;:&quot;nlp&quot;},{&quot;id&quot;:&quot;task_categories:summarization&quot;,&quot;label&quot;:&quot;summarization&quot;,&quot;type&quot;:&quot;task_categories&quot;,&quot;subType&quot;:&quot;nlp&quot;},{&quot;id&quot;:&quot;language:en&quot;,&quot;label&quot;:&quot;English&quot;,&quot;type&quot;:&quot;language&quot;},{&quot;id&quot;:&quot;size_categories:n<1K&quot;,&quot;label&quot;:&quot;< 1K&quot;,&quot;type&quot;:&quot;size_categories&quot;},{&quot;id&quot;:&quot;format:parquet&quot;,&quot;label&quot;:&quot;parquet&quot;,&quot;type&quot;:&quot;format&quot;},{&quot;id&quot;:&quot;modality:text&quot;,&quot;label&quot;:&quot;Text&quot;,&quot;type&quot;:&quot;modality&quot;},{&quot;id&quot;:&quot;library:datasets&quot;,&quot;label&quot;:&quot;Datasets&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:pandas&quot;,&quot;label&quot;:&quot;pandas&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:mlcroissant&quot;,&quot;label&quot;:&quot;Croissant&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:polars&quot;,&quot;label&quot;:&quot;Polars&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;ðŸ‡ºðŸ‡¸ Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;}],&quot;hasBlockedOids&quot;:false,&quot;region&quot;:&quot;us&quot;,&quot;xetEnabled&quot;:false},&quot;discussionsStats&quot;:{&quot;closed&quot;:0,&quot;open&quot;:0,&quot;total&quot;:0}}"><header class="bg-linear-to-t border-b border-gray-100 pt-6 sm:pt-9 from-gray-50-to-white via-white dark:via-gray-950"><div class="container relative "><h1 class="flex flex-wrap items-center max-md:leading-tight mb-3 text-lg max-sm:gap-y-1.5 md:text-xl"><a href="/datasets" class="group flex items-center"><svg class="sm:mr-1.5 -mr-1 text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					<span class="mr-2.5 font-semibold text-gray-400 group-hover:text-gray-500 max-sm:hidden">Datasets:</span></a>
				<hr class="mx-1.5 h-2 translate-y-px rounded-sm border-r dark:border-gray-600 sm:hidden">
			<div class="group flex flex-none items-center"><div class="relative mr-1 flex items-center">

			

<span class="inline-block "><span class="contents"><a href="/RamAnanth1" class="text-gray-400 hover:text-blue-600"><img alt="" class="w-3.5 h-3.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1671346465329-62aeb75c4b5d61650842dbf4.png" crossorigin="anonymous"></a></span>
	</span></div>
		

<span class="inline-block "><span class="contents"><a href="/RamAnanth1" class="text-gray-400 hover:text-blue-600">RamAnanth1</a></span>
	</span>
		<div class="mx-0.5 text-gray-300">/</div></div>

<div class="max-w-full "><a class="break-words font-mono font-semibold hover:text-blue-600 " href="/datasets/RamAnanth1/talkrl-podcast">talkrl-podcast</a>
	<button class="relative text-sm mr-4 focus:outline-hidden inline-flex cursor-pointer items-center text-sm  mx-0.5   text-gray-600 " title="Copy dataset name to clipboard" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg>
	
	</button></div>
			<div class="inline-flex items-center overflow-hidden whitespace-nowrap rounded-md border bg-white text-sm leading-none text-gray-500  mr-2"><button class="relative flex items-center overflow-hidden from-red-50 to-transparent dark:from-red-900 px-1.5 py-1 hover:bg-linear-to-t focus:outline-hidden"  title="Like"><svg class="left-1.5 absolute" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>

		
		<span class="ml-4 pl-0.5 ">like</span></button>
	<button class="focus:outline-hidden flex items-center border-l px-1.5 py-1 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="See users who liked this repository">2</button></div>




			
			
	</h1>
		<div class="mb-3 flex flex-wrap md:mb-4"><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Tasks:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?task_categories=task_categories%3Atext-classification"><div class="tag tag-white   "><div class="tag-ico -ml-2 tag-ico-orange"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><circle cx="10" cy="20" r="2" fill="currentColor"></circle><circle cx="10" cy="28" r="2" fill="currentColor"></circle><circle cx="10" cy="14" r="2" fill="currentColor"></circle><circle cx="28" cy="4" r="2" fill="currentColor"></circle><circle cx="22" cy="6" r="2" fill="currentColor"></circle><circle cx="28" cy="10" r="2" fill="currentColor"></circle><circle cx="20" cy="12" r="2" fill="currentColor"></circle><circle cx="28" cy="22" r="2" fill="currentColor"></circle><circle cx="26" cy="28" r="2" fill="currentColor"></circle><circle cx="20" cy="26" r="2" fill="currentColor"></circle><circle cx="22" cy="20" r="2" fill="currentColor"></circle><circle cx="16" cy="4" r="2" fill="currentColor"></circle><circle cx="4" cy="24" r="2" fill="currentColor"></circle><circle cx="4" cy="16" r="2" fill="currentColor"></circle></svg></div>

	

	<span>Text Classification</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?task_categories=task_categories%3Atext-generation"><div class="tag tag-white   "><div class="tag-ico -ml-2 tag-ico-indigo"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 18 18"><path d="M16.2607 8.08202L14.468 6.28928C14.3063 6.12804 14.0873 6.03749 13.859 6.03749C13.6307 6.03749 13.4117 6.12804 13.25 6.28928L5.6375 13.904V16.9125H8.64607L16.2607 9.30002C16.422 9.13836 16.5125 8.91935 16.5125 8.69102C16.5125 8.4627 16.422 8.24369 16.2607 8.08202V8.08202ZM8.1953 15.825H6.725V14.3547L11.858 9.22118L13.3288 10.6915L8.1953 15.825ZM14.0982 9.92262L12.6279 8.45232L13.8606 7.21964L15.3309 8.68994L14.0982 9.92262Z"></path><path d="M6.18125 9.84373H7.26875V6.03748H8.9V4.94998H4.55V6.03748H6.18125V9.84373Z"></path><path d="M4.55 11.475H2.375V2.775H11.075V4.95H12.1625V2.775C12.1625 2.48658 12.0479 2.20997 11.844 2.00602C11.64 1.80208 11.3634 1.6875 11.075 1.6875H2.375C2.08658 1.6875 1.80997 1.80208 1.60602 2.00602C1.40207 2.20997 1.2875 2.48658 1.2875 2.775V11.475C1.2875 11.7634 1.40207 12.04 1.60602 12.244C1.80997 12.4479 2.08658 12.5625 2.375 12.5625H4.55V11.475Z"></path></svg></div>

	

	<span>Text Generation</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?task_categories=task_categories%3Asummarization"><div class="tag tag-white   "><div class="tag-ico -ml-2 tag-ico-indigo"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 18 19"><path d="M15.4988 8.79309L12.1819 5.47621C12.0188 5.25871 11.7469 5.14996 11.475 5.14996H7.12501C6.52688 5.14996 6.03751 5.63934 6.03751 6.23746V16.025C6.03751 16.6231 6.52688 17.1125 7.12501 17.1125H14.7375C15.3356 17.1125 15.825 16.6231 15.825 16.025V9.55434C15.825 9.28246 15.7163 9.01059 15.4988 8.79309V8.79309ZM11.475 6.23746L14.6831 9.49996H11.475V6.23746ZM7.12501 16.025V6.23746H10.3875V9.49996C10.3875 10.0981 10.8769 10.5875 11.475 10.5875H14.7375V16.025H7.12501Z"></path><path d="M3.8625 10.5875H2.775V2.97498C2.775 2.37686 3.26438 1.88748 3.8625 1.88748H11.475V2.97498H3.8625V10.5875Z"></path></svg></div>

	

	<span>Summarization</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Modalities:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?modality=modality%3Atext"><div class="tag tag-white   ">
		<svg class="text-red-700 dark:text-red-600" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.619 4.619C2.667 6.573 2.667 9.715 2.667 16s0 9.428 1.952 11.38C6.573 29.333 9.715 29.333 16 29.333s9.428 0 11.38-1.953c1.953-1.95 1.953-5.095 1.953-11.38s0-9.428-1.953-11.381C25.43 2.667 22.285 2.667 16 2.667s-9.428 0-11.381 1.952m8.65 3.714c-.573 0-1.109 0-1.546.066-.495.073-1.003.248-1.41.7-.392.436-.53.956-.59 1.452-.056.464-.056 1.04-.056 1.689V13a1 1 0 1 0 2 0v-.704c0-.724.001-1.176.041-1.505q.015-.15.061-.294a.2.2 0 0 1 .031-.061q0-.003.016-.01a.8.8 0 0 1 .203-.05c.272-.04.654-.043 1.314-.043H15v11.334h-2.333a1 1 0 1 0 0 2H20a1 1 0 0 0 0-2h-3V10.333h1.667c.66 0 1.042.003 1.314.043.123.019.18.04.203.05l.015.009a.2.2 0 0 1 .032.061c.018.05.042.14.061.295.04.329.041.781.041 1.506V13a1 1 0 1 0 2 0v-.76c0-.65 0-1.225-.056-1.69-.06-.495-.198-1.015-.59-1.453-.407-.45-.915-.625-1.41-.698-.437-.067-.973-.067-1.546-.066z" fill="currentColor"></path></svg>

	

	<span>Text</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Formats:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?format=format%3Aparquet"><div class="tag tag-white   ">
		<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 34 34"><path fill-rule="evenodd" clip-rule="evenodd" d="m17.97 18.44-3.98-2.3-7.1 3.78 3.98 2.3 7.1-3.78Zm15.1-1.4-3.99-2.3-16.22 8.63 3.98 2.3 16.22-8.63Zm-5.98-3.45-3.97-2.3-7.1 3.78 3.98 2.3 7.1-3.78Zm-9.94-5.74 3.98 2.3-11.16 5.93L6 13.78l11.16-5.93Zm-13.19 7 3.98 2.3-3.04 1.62-3.98-2.3 3.04-1.61Z" fill="currentColor"></path></svg>

	

	<span>parquet</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Languages:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?language=language%3Aen"><div class="tag tag-white   ">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="text-green-600/80" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 10 10"><path fill-rule="evenodd" clip-rule="evenodd" d="M0.625 5C0.625 6.16032 1.08594 7.27312 1.90641 8.09359C2.72688 8.91406 3.83968 9.375 5 9.375C6.16032 9.375 7.27312 8.91406 8.09359 8.09359C8.91406 7.27312 9.375 6.16032 9.375 5C9.375 3.83968 8.91406 2.72688 8.09359 1.90641C7.27312 1.08594 6.16032 0.625 5 0.625C3.83968 0.625 2.72688 1.08594 1.90641 1.90641C1.08594 2.72688 0.625 3.83968 0.625 5ZM7.64365 7.48027C7.61734 7.50832 7.59054 7.53598 7.56326 7.56326C7.13828 7.98824 6.61864 8.2968 6.0539 8.46842C6.29802 8.11949 6.49498 7.64804 6.63475 7.09483C7.00845 7.18834 7.35014 7.3187 7.64365 7.48027ZM8.10076 6.87776C8.37677 6.42196 8.55005 5.90894 8.60556 5.37499H6.86808C6.85542 5.71597 6.82551 6.04557 6.77971 6.35841C7.25309 6.47355 7.68808 6.6414 8.062 6.85549C8.07497 6.86283 8.08789 6.87025 8.10076 6.87776ZM6.03795 6.22536C6.07708 5.95737 6.1044 5.67232 6.11705 5.37499H3.88295C3.89666 5.69742 3.92764 6.00542 3.9722 6.29287C4.37075 6.21726 4.79213 6.17749 5.224 6.17749C5.50054 6.17749 5.77294 6.19376 6.03795 6.22536ZM4.1261 7.02673C4.34894 7.84835 4.68681 8.375 5 8.375C5.32122 8.375 5.66839 7.82101 5.8908 6.963C5.67389 6.93928 5.45082 6.92699 5.224 6.92699C4.84316 6.92699 4.47332 6.96176 4.1261 7.02673ZM3.39783 7.21853C3.53498 7.71842 3.72038 8.14579 3.9461 8.46842C3.42141 8.30898 2.93566 8.03132 2.52857 7.65192C2.77253 7.48017 3.06711 7.33382 3.39783 7.21853ZM3.23916 6.48077C3.18263 6.13193 3.14625 5.76074 3.13192 5.37499H1.39444C1.4585 5.99112 1.67936 6.57938 2.03393 7.08403C2.3706 6.83531 2.78055 6.63162 3.23916 6.48077ZM1.39444 4.62499H3.13192C3.14615 4.24204 3.18211 3.87344 3.23794 3.52681C2.77814 3.37545 2.36731 3.17096 2.03024 2.92123C1.67783 3.42469 1.45828 4.011 1.39444 4.62499ZM2.5237 2.35262C2.76812 2.52552 3.06373 2.67281 3.39584 2.78875C3.53318 2.28573 3.71928 1.85578 3.9461 1.53158C3.41932 1.69166 2.93178 1.97089 2.5237 2.35262ZM3.97101 3.71489C3.92709 4.00012 3.89654 4.30547 3.88295 4.62499H6.11705C6.10453 4.33057 6.07761 4.04818 6.03909 3.78248C5.77372 3.81417 5.50093 3.83049 5.224 3.83049C4.79169 3.83049 4.3699 3.79065 3.97101 3.71489ZM5.8928 3.04476C5.67527 3.06863 5.45151 3.08099 5.224 3.08099C4.84241 3.08099 4.47186 3.04609 4.12405 2.98086C4.34686 2.1549 4.68584 1.625 5 1.625C5.32218 1.625 5.67048 2.18233 5.8928 3.04476ZM6.78083 3.6493C6.826 3.95984 6.85552 4.28682 6.86808 4.62499H8.60556C8.55029 4.09337 8.37827 3.58251 8.10436 3.1282C8.0903 3.1364 8.07618 3.14449 8.062 3.15249C7.68838 3.36641 7.25378 3.53417 6.78083 3.6493ZM7.64858 2.52499C7.35446 2.68754 7.0117 2.81868 6.63664 2.91268C6.49676 2.35623 6.29913 1.88209 6.0539 1.53158C6.61864 1.7032 7.13828 2.01176 7.56326 2.43674C7.59224 2.46572 7.62068 2.49514 7.64858 2.52499Z" fill="currentColor"></path></svg>

	

	<span>English</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Size:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?size_categories=size_categories%3An%3C1K"><div class="tag tag-white   ">

	

	<span>&lt; 1K</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Libraries:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Adatasets"><div class="tag tag-white   "><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 95 88"><path fill="#fff" d="M94.25 70.08a8.28 8.28 0 0 1-.43 6.46 10.57 10.57 0 0 1-3 3.6 25.18 25.18 0 0 1-5.7 3.2 65.74 65.74 0 0 1-7.56 2.65 46.67 46.67 0 0 1-11.42 1.68c-5.42.05-10.09-1.23-13.4-4.5a40.4 40.4 0 0 1-10.14.03c-3.34 3.25-7.99 4.52-13.39 4.47a46.82 46.82 0 0 1-11.43-1.68 66.37 66.37 0 0 1-7.55-2.65c-2.28-.98-4.17-2-5.68-3.2a10.5 10.5 0 0 1-3.02-3.6c-.99-2-1.18-4.3-.42-6.46a8.54 8.54 0 0 1-.33-5.63c.25-.95.66-1.83 1.18-2.61a8.67 8.67 0 0 1 2.1-8.47 8.23 8.23 0 0 1 2.82-2.07 41.75 41.75 0 1 1 81.3-.12 8.27 8.27 0 0 1 3.11 2.19 8.7 8.7 0 0 1 2.1 8.47c.52.78.93 1.66 1.18 2.61a8.61 8.61 0 0 1-.32 5.63Z"></path><path fill="#FFD21E" d="M47.21 76.5a34.75 34.75 0 1 0 0-69.5 34.75 34.75 0 0 0 0 69.5Z"></path><path fill="#FF9D0B" d="M81.96 41.75a34.75 34.75 0 1 0-69.5 0 34.75 34.75 0 0 0 69.5 0Zm-73.5 0a38.75 38.75 0 1 1 77.5 0 38.75 38.75 0 0 1-77.5 0Z"></path><path fill="#3A3B45" d="M58.5 32.3c1.28.44 1.78 3.06 3.07 2.38a5 5 0 1 0-6.76-2.07c.61 1.15 2.55-.72 3.7-.32ZM34.95 32.3c-1.28.44-1.79 3.06-3.07 2.38a5 5 0 1 1 6.76-2.07c-.61 1.15-2.56-.72-3.7-.32Z"></path><path fill="#FF323D" d="M46.96 56.29c9.83 0 13-8.76 13-13.26 0-2.34-1.57-1.6-4.09-.36-2.33 1.15-5.46 2.74-8.9 2.74-7.19 0-13-6.88-13-2.38s3.16 13.26 13 13.26Z"></path><path fill="#3A3B45" fill-rule="evenodd" d="M39.43 54a8.7 8.7 0 0 1 5.3-4.49c.4-.12.81.57 1.24 1.28.4.68.82 1.37 1.24 1.37.45 0 .9-.68 1.33-1.35.45-.7.89-1.38 1.32-1.25a8.61 8.61 0 0 1 5 4.17c3.73-2.94 5.1-7.74 5.1-10.7 0-2.34-1.57-1.6-4.09-.36l-.14.07c-2.31 1.15-5.39 2.67-8.77 2.67s-6.45-1.52-8.77-2.67c-2.6-1.29-4.23-2.1-4.23.29 0 3.05 1.46 8.06 5.47 10.97Z" clip-rule="evenodd"></path><path fill="#FF9D0B" d="M70.71 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM24.21 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM17.52 48c-1.62 0-3.06.66-4.07 1.87a5.97 5.97 0 0 0-1.33 3.76 7.1 7.1 0 0 0-1.94-.3c-1.55 0-2.95.59-3.94 1.66a5.8 5.8 0 0 0-.8 7 5.3 5.3 0 0 0-1.79 2.82c-.24.9-.48 2.8.8 4.74a5.22 5.22 0 0 0-.37 5.02c1.02 2.32 3.57 4.14 8.52 6.1 3.07 1.22 5.89 2 5.91 2.01a44.33 44.33 0 0 0 10.93 1.6c5.86 0 10.05-1.8 12.46-5.34 3.88-5.69 3.33-10.9-1.7-15.92-2.77-2.78-4.62-6.87-5-7.77-.78-2.66-2.84-5.62-6.25-5.62a5.7 5.7 0 0 0-4.6 2.46c-1-1.26-1.98-2.25-2.86-2.82A7.4 7.4 0 0 0 17.52 48Zm0 4c.51 0 1.14.22 1.82.65 2.14 1.36 6.25 8.43 7.76 11.18.5.92 1.37 1.31 2.14 1.31 1.55 0 2.75-1.53.15-3.48-3.92-2.93-2.55-7.72-.68-8.01.08-.02.17-.02.24-.02 1.7 0 2.45 2.93 2.45 2.93s2.2 5.52 5.98 9.3c3.77 3.77 3.97 6.8 1.22 10.83-1.88 2.75-5.47 3.58-9.16 3.58-3.81 0-7.73-.9-9.92-1.46-.11-.03-13.45-3.8-11.76-7 .28-.54.75-.76 1.34-.76 2.38 0 6.7 3.54 8.57 3.54.41 0 .7-.17.83-.6.79-2.85-12.06-4.05-10.98-8.17.2-.73.71-1.02 1.44-1.02 3.14 0 10.2 5.53 11.68 5.53.11 0 .2-.03.24-.1.74-1.2.33-2.04-4.9-5.2-5.21-3.16-8.88-5.06-6.8-7.33.24-.26.58-.38 1-.38 3.17 0 10.66 6.82 10.66 6.82s2.02 2.1 3.25 2.1c.28 0 .52-.1.68-.38.86-1.46-8.06-8.22-8.56-11.01-.34-1.9.24-2.85 1.31-2.85Z"></path><path fill="#FFD21E" d="M38.6 76.69c2.75-4.04 2.55-7.07-1.22-10.84-3.78-3.77-5.98-9.3-5.98-9.3s-.82-3.2-2.69-2.9c-1.87.3-3.24 5.08.68 8.01 3.91 2.93-.78 4.92-2.29 2.17-1.5-2.75-5.62-9.82-7.76-11.18-2.13-1.35-3.63-.6-3.13 2.2.5 2.79 9.43 9.55 8.56 11-.87 1.47-3.93-1.71-3.93-1.71s-9.57-8.71-11.66-6.44c-2.08 2.27 1.59 4.17 6.8 7.33 5.23 3.16 5.64 4 4.9 5.2-.75 1.2-12.28-8.53-13.36-4.4-1.08 4.11 11.77 5.3 10.98 8.15-.8 2.85-9.06-5.38-10.74-2.18-1.7 3.21 11.65 6.98 11.76 7.01 4.3 1.12 15.25 3.49 19.08-2.12Z"></path><path fill="#FF9D0B" d="M77.4 48c1.62 0 3.07.66 4.07 1.87a5.97 5.97 0 0 1 1.33 3.76 7.1 7.1 0 0 1 1.95-.3c1.55 0 2.95.59 3.94 1.66a5.8 5.8 0 0 1 .8 7 5.3 5.3 0 0 1 1.78 2.82c.24.9.48 2.8-.8 4.74a5.22 5.22 0 0 1 .37 5.02c-1.02 2.32-3.57 4.14-8.51 6.1-3.08 1.22-5.9 2-5.92 2.01a44.33 44.33 0 0 1-10.93 1.6c-5.86 0-10.05-1.8-12.46-5.34-3.88-5.69-3.33-10.9 1.7-15.92 2.78-2.78 4.63-6.87 5.01-7.77.78-2.66 2.83-5.62 6.24-5.62a5.7 5.7 0 0 1 4.6 2.46c1-1.26 1.98-2.25 2.87-2.82A7.4 7.4 0 0 1 77.4 48Zm0 4c-.51 0-1.13.22-1.82.65-2.13 1.36-6.25 8.43-7.76 11.18a2.43 2.43 0 0 1-2.14 1.31c-1.54 0-2.75-1.53-.14-3.48 3.91-2.93 2.54-7.72.67-8.01a1.54 1.54 0 0 0-.24-.02c-1.7 0-2.45 2.93-2.45 2.93s-2.2 5.52-5.97 9.3c-3.78 3.77-3.98 6.8-1.22 10.83 1.87 2.75 5.47 3.58 9.15 3.58 3.82 0 7.73-.9 9.93-1.46.1-.03 13.45-3.8 11.76-7-.29-.54-.75-.76-1.34-.76-2.38 0-6.71 3.54-8.57 3.54-.42 0-.71-.17-.83-.6-.8-2.85 12.05-4.05 10.97-8.17-.19-.73-.7-1.02-1.44-1.02-3.14 0-10.2 5.53-11.68 5.53-.1 0-.19-.03-.23-.1-.74-1.2-.34-2.04 4.88-5.2 5.23-3.16 8.9-5.06 6.8-7.33-.23-.26-.57-.38-.98-.38-3.18 0-10.67 6.82-10.67 6.82s-2.02 2.1-3.24 2.1a.74.74 0 0 1-.68-.38c-.87-1.46 8.05-8.22 8.55-11.01.34-1.9-.24-2.85-1.31-2.85Z"></path><path fill="#FFD21E" d="M56.33 76.69c-2.75-4.04-2.56-7.07 1.22-10.84 3.77-3.77 5.97-9.3 5.97-9.3s.82-3.2 2.7-2.9c1.86.3 3.23 5.08-.68 8.01-3.92 2.93.78 4.92 2.28 2.17 1.51-2.75 5.63-9.82 7.76-11.18 2.13-1.35 3.64-.6 3.13 2.2-.5 2.79-9.42 9.55-8.55 11 .86 1.47 3.92-1.71 3.92-1.71s9.58-8.71 11.66-6.44c2.08 2.27-1.58 4.17-6.8 7.33-5.23 3.16-5.63 4-4.9 5.2.75 1.2 12.28-8.53 13.36-4.4 1.08 4.11-11.76 5.3-10.97 8.15.8 2.85 9.05-5.38 10.74-2.18 1.69 3.21-11.65 6.98-11.76 7.01-4.31 1.12-15.26 3.49-19.08-2.12Z"></path></svg>

	

	<span>Datasets</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Apandas"><div class="tag tag-white   "><svg class="text-black inline-block text-sm text-[#130754] dark:text-gray-200" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid slice" viewBox="0 0 210.21 280.43"><rect fill="currentColor" x="74.51" y="43.03" width="24.09" height="50.02"></rect><rect fill="currentColor" x="74.51" y="145.78" width="24.09" height="50.02"></rect><rect fill="#ffca00" x="74.51" y="107.65" width="24.09" height="23.6"></rect><rect fill="currentColor" x="35.81" y="84.15" width="24.09" height="166.27"></rect><rect fill="currentColor" x="112.41" y="187.05" width="24.09" height="50.02"></rect><rect fill="currentColor" x="112.41" y="84.21" width="24.09" height="50.02"></rect><rect fill="#e70488" x="112.41" y="148.84" width="24.09" height="23.6"></rect><rect fill="currentColor" x="150.3" y="30" width="24.09" height="166.27"></rect></svg>

	

	<span>pandas</span>
	

	</div></a><div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-lg rounded-br-none " type="button">
		<div class="tag tag-white   relative rounded-br-none pr-2.5"><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="none" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="#F5AB6A"></path><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="url(#paint0_radial_18_31665)"></path><g filter="url(#filter0_f_18_31665)"><path d="M22.2849 12.1817L23.4375 13.2656L24.4375 4.70312C23.5121 4.1242 23.0198 3.96369 22.6563 3.89062L22.2849 12.1817Z" fill="url(#paint1_linear_18_31665)"></path></g><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint2_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint3_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint4_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint5_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint6_linear_18_31665)"></path><g filter="url(#filter1_f_18_31665)"><path d="M13.1016 2.72656C11.862 3.06924 11.5298 3.40016 11.5298 3.40016C10.9102 3.52335 10.936 4.11525 11.2408 4.6799C13.1487 6.95202 15.1361 13.2496 18.007 14.0958C18.2707 14.1633 18.6953 14.2107 19.1797 14.2344L13.1016 2.72656Z" fill="url(#paint7_linear_18_31665)"></path></g><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint8_radial_18_31665)"></path><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint9_radial_18_31665)"></path><g filter="url(#filter2_f_18_31665)"><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint10_linear_18_31665)"></path><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint11_radial_18_31665)"></path></g><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="#EC9F6A"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint12_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint13_linear_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint14_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint15_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint16_radial_18_31665)"></path><g filter="url(#filter3_f_18_31665)"><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint17_linear_18_31665)"></path><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint18_linear_18_31665)"></path></g><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="#D79453"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint19_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint20_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint21_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint22_linear_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint23_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint24_radial_18_31665)"></path><defs><filter id="filter0_f_18_31665" x="22.0349" y="3.64062" width="2.65265" height="9.875" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter1_f_18_31665" x="10.7815" y="2.47656" width="8.64819" height="12.0078" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter2_f_18_31665" x="3.89062" y="22.5416" width="9.57812" height="2.2804" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter3_f_18_31665" x="2.45312" y="11.2538" width="12.1255" height="8.50903" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><radialGradient id="paint0_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.8125 12.9375) rotate(42.7741) scale(12.5164 7.08839)"><stop offset="0.0937591" stop-color="#C05159"></stop><stop offset="0.553697" stop-color="#F6AC6A"></stop><stop offset="0.832916" stop-color="#FFD186"></stop><stop offset="0.916927" stop-color="#FFDC87"></stop></radialGradient><linearGradient id="paint1_linear_18_31665" x1="24.7344" y1="4.67187" x2="20.8594" y2="12.8906" gradientUnits="userSpaceOnUse"><stop stop-color="#EBD67C"></stop><stop offset="0.0655686" stop-color="#FFFFA6"></stop><stop offset="0.530552" stop-color="#F8C281"></stop><stop offset="0.937338" stop-color="#E99E6B"></stop></linearGradient><radialGradient id="paint2_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.9009 13.1847) rotate(-127.648) scale(14.3438 11.7966)"><stop stop-color="#FFBE66"></stop><stop offset="1" stop-color="#E2AE5B"></stop></radialGradient><radialGradient id="paint3_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(18 11.4375) rotate(53.9726) scale(11.9013 4.84018)"><stop stop-color="#D67C63"></stop><stop offset="1" stop-color="#D97D67" stop-opacity="0"></stop></radialGradient><radialGradient id="paint4_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(23 4.1875) rotate(45.7639) scale(3.31486 5.75622)"><stop stop-color="#FFE4A6"></stop><stop offset="0.711285" stop-color="#F8B76F"></stop><stop offset="1" stop-color="#F9B870" stop-opacity="0"></stop></radialGradient><radialGradient id="paint5_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.875 12.4375) rotate(88.9391) scale(3.37558 1.29066)"><stop stop-color="#FFBC67"></stop><stop offset="1" stop-color="#FFBC67" stop-opacity="0"></stop></radialGradient><linearGradient id="paint6_linear_18_31665" x1="20.375" y1="15.6875" x2="20.125" y2="12.7813" gradientUnits="userSpaceOnUse"><stop offset="0.461609" stop-color="#B45077"></stop><stop offset="0.855389" stop-color="#B75077" stop-opacity="0"></stop></linearGradient><linearGradient id="paint7_linear_18_31665" x1="12.9375" y1="2.57056" x2="18.5625" y2="14.3891" gradientUnits="userSpaceOnUse"><stop stop-color="#DDC173"></stop><stop offset="0.485173" stop-color="#D59F65"></stop><stop offset="1" stop-color="#E49966"></stop></linearGradient><radialGradient id="paint8_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.5625 23.5) rotate(109.113) scale(6.68078 10.2578)"><stop offset="0.165756" stop-color="#FFBF7E"></stop><stop offset="0.827674" stop-color="#DF8C6D"></stop><stop offset="1" stop-color="#B05A66"></stop></radialGradient><radialGradient id="paint9_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.1875 26) rotate(41.0652) scale(8.37243 2.03649)"><stop stop-color="#FFD483"></stop><stop offset="1" stop-color="#FFD688" stop-opacity="0"></stop></radialGradient><linearGradient id="paint10_linear_18_31665" x1="3.96063" y1="23.794" x2="13.3748" y2="23.5143" gradientUnits="userSpaceOnUse"><stop stop-color="#A8716F"></stop><stop offset="0.103615" stop-color="#B37173"></stop><stop offset="0.225484" stop-color="#DB9F84"></stop><stop offset="0.799889" stop-color="#F1BB8A"></stop><stop offset="1" stop-color="#FFD780"></stop></linearGradient><radialGradient id="paint11_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4219 23.1719) rotate(-178.616) scale(3.23532 0.569081)"><stop offset="0.621498" stop-color="#AF5A3E"></stop><stop offset="1" stop-color="#B35445" stop-opacity="0"></stop></radialGradient><radialGradient id="paint12_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.625 19.125) rotate(-171.737) scale(15.2205 15.0767)"><stop offset="0.138435" stop-color="#FFB974"></stop><stop offset="0.403618" stop-color="#F2A56D"></stop><stop offset="0.925938" stop-color="#A16948"></stop></radialGradient><linearGradient id="paint13_linear_18_31665" x1="8.22184" y1="13.125" x2="6.81191" y2="15.4996" gradientUnits="userSpaceOnUse"><stop offset="0.610751" stop-color="#984847"></stop><stop offset="0.850075" stop-color="#9A4947" stop-opacity="0"></stop></linearGradient><radialGradient id="paint14_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.25 23.7461) scale(11.25 5.68361)"><stop stop-color="#C66364"></stop><stop offset="1" stop-color="#D4766B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint15_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.875 23.5313) scale(10.0937 1.29657)"><stop stop-color="#B64B4B"></stop><stop offset="1" stop-color="#C56158" stop-opacity="0"></stop></radialGradient><radialGradient id="paint16_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4375 19.875) rotate(-46.8882) scale(4.02385 7.51767)"><stop stop-color="#FFC083"></stop><stop offset="0.620218" stop-color="#FFBD7D" stop-opacity="0"></stop></radialGradient><linearGradient id="paint17_linear_18_31665" x1="2.8125" y1="13.0312" x2="14.5582" y2="18.9404" gradientUnits="userSpaceOnUse"><stop stop-color="#B89367"></stop><stop offset="1" stop-color="#C5835E"></stop></linearGradient><linearGradient id="paint18_linear_18_31665" x1="8.21875" y1="14.6406" x2="7.59349" y2="15.6717" gradientUnits="userSpaceOnUse"><stop offset="0.351552" stop-color="#A74746"></stop><stop offset="0.845198" stop-color="#A04346" stop-opacity="0"></stop></linearGradient><radialGradient id="paint19_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.5625 14.5625) rotate(140.244) scale(18.3733 13.7403)"><stop stop-color="#FDAE69"></stop><stop offset="0.729021" stop-color="#CE8C4F"></stop><stop offset="0.921546" stop-color="#AD7B45"></stop><stop offset="1" stop-color="#8B6B4A"></stop></radialGradient><radialGradient id="paint20_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.0625 7) rotate(65.3152) scale(11.0745 3.16547)"><stop offset="0.233237" stop-color="#FFD47C"></stop><stop offset="0.853648" stop-color="#FFD98B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint21_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.3125 8.875) rotate(100.886) scale(6.6191 5.57808)"><stop offset="0.128419" stop-color="#FFD88C"></stop><stop offset="0.924134" stop-color="#FFBE7B" stop-opacity="0"></stop></radialGradient><linearGradient id="paint22_linear_18_31665" x1="7.25" y1="15.1875" x2="10.7588" y2="10.3142" gradientUnits="userSpaceOnUse"><stop offset="0.142353" stop-color="#C15F4D"></stop><stop offset="1" stop-color="#D58366" stop-opacity="0"></stop></linearGradient><radialGradient id="paint23_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(8.15625 15.7813) rotate(28.5422) scale(12.5574 1.96589)"><stop offset="0.149989" stop-color="#E4745D"></stop><stop offset="0.453292" stop-color="#C8604C"></stop><stop offset="0.632597" stop-color="#C0605F"></stop><stop offset="1" stop-color="#C0605F" stop-opacity="0"></stop></radialGradient><radialGradient id="paint24_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(1.40625 2.69067) rotate(46.0943) scale(22.3963)"><stop offset="0.935802" stop-color="#C17C61" stop-opacity="0"></stop><stop offset="0.982109" stop-color="#C17C61"></stop></radialGradient></defs></svg>

	

	<span>Croissant</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	<button class="tag tag-ghost px-1! -ml-0.5 mb-1 md:mb-1.5" type="button">+ 1</button></div></div>

		<div class="flex flex-col-reverse lg:flex-row lg:items-center lg:justify-between"><div class="-mb-px flex h-12 items-center overflow-x-auto overflow-y-hidden ">
	<a class="tab-alternate active" href="/datasets/RamAnanth1/talkrl-podcast"><svg class="mr-1.5 text-gray-400 flex-none" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
	Dataset card
	

	
		</a><a class="tab-alternate" href="/datasets/RamAnanth1/talkrl-podcast/viewer/"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	Data Studio
	

	
		</a><a class="tab-alternate" href="/datasets/RamAnanth1/talkrl-podcast/tree/main"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M21 19h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-8h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0 4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M9 19a1 1 0 0 1-1-1V6a1 1 0 0 1 2 0v12a1 1 0 0 1-1 1zm-6-4.333a1 1 0 0 1-.64-1.769L3.438 12l-1.078-.898a1 1 0 0 1 1.28-1.538l2 1.667a1 1 0 0 1 0 1.538l-2 1.667a.999.999 0 0 1-.64.231z" fill="currentColor"></path></svg>
	<span class="xl:hidden">Files</span>
		<span class="hidden xl:inline">Files and versions</span>
	

	
		</a><a class="tab-alternate" href="/datasets/RamAnanth1/talkrl-podcast/discussions"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M20.6081 3C21.7684 3 22.8053 3.49196 23.5284 4.38415C23.9756 4.93678 24.4428 5.82749 24.4808 7.16133C24.9674 7.01707 25.4353 6.93643 25.8725 6.93643C26.9833 6.93643 27.9865 7.37587 28.696 8.17411C29.6075 9.19872 30.0124 10.4579 29.8361 11.7177C29.7523 12.3177 29.5581 12.8555 29.2678 13.3534C29.8798 13.8646 30.3306 14.5763 30.5485 15.4322C30.719 16.1032 30.8939 17.5006 29.9808 18.9403C30.0389 19.0342 30.0934 19.1319 30.1442 19.2318C30.6932 20.3074 30.7283 21.5229 30.2439 22.6548C29.5093 24.3704 27.6841 25.7219 24.1397 27.1727C21.9347 28.0753 19.9174 28.6523 19.8994 28.6575C16.9842 29.4379 14.3477 29.8345 12.0653 29.8345C7.87017 29.8345 4.8668 28.508 3.13831 25.8921C0.356375 21.6797 0.754104 17.8269 4.35369 14.1131C6.34591 12.058 7.67023 9.02782 7.94613 8.36275C8.50224 6.39343 9.97271 4.20438 12.4172 4.20438H12.4179C12.6236 4.20438 12.8314 4.2214 13.0364 4.25468C14.107 4.42854 15.0428 5.06476 15.7115 6.02205C16.4331 5.09583 17.134 4.359 17.7682 3.94323C18.7242 3.31737 19.6794 3 20.6081 3ZM20.6081 5.95917C20.2427 5.95917 19.7963 6.1197 19.3039 6.44225C17.7754 7.44319 14.8258 12.6772 13.7458 14.7131C13.3839 15.3952 12.7655 15.6837 12.2086 15.6837C11.1036 15.6837 10.2408 14.5497 12.1076 13.1085C14.9146 10.9402 13.9299 7.39584 12.5898 7.1776C12.5311 7.16799 12.4731 7.16355 12.4172 7.16355C11.1989 7.16355 10.6615 9.33114 10.6615 9.33114C10.6615 9.33114 9.0863 13.4148 6.38031 16.206C3.67434 18.998 3.5346 21.2388 5.50675 24.2246C6.85185 26.2606 9.42666 26.8753 12.0653 26.8753C14.8021 26.8753 17.6077 26.2139 19.1799 25.793C19.2574 25.7723 28.8193 22.984 27.6081 20.6107C27.4046 20.212 27.0693 20.0522 26.6471 20.0522C24.9416 20.0522 21.8393 22.6726 20.5057 22.6726C20.2076 22.6726 19.9976 22.5416 19.9116 22.222C19.3433 20.1173 28.552 19.2325 27.7758 16.1839C27.639 15.6445 27.2677 15.4256 26.746 15.4263C24.4923 15.4263 19.4358 19.5181 18.3759 19.5181C18.2949 19.5181 18.2368 19.4937 18.2053 19.4419C17.6743 18.557 17.9653 17.9394 21.7082 15.6009C25.4511 13.2617 28.0783 11.8545 26.5841 10.1752C26.4121 9.98141 26.1684 9.8956 25.8725 9.8956C23.6001 9.89634 18.2311 14.9403 18.2311 14.9403C18.2311 14.9403 16.7821 16.496 15.9057 16.496C15.7043 16.496 15.533 16.4139 15.4169 16.2112C14.7956 15.1296 21.1879 10.1286 21.5484 8.06535C21.7928 6.66715 21.3771 5.95917 20.6081 5.95917Z" fill="#FF9D00"></path><path d="M5.50686 24.2246C3.53472 21.2387 3.67446 18.9979 6.38043 16.206C9.08641 13.4147 10.6615 9.33111 10.6615 9.33111C10.6615 9.33111 11.2499 6.95933 12.59 7.17757C13.93 7.39581 14.9139 10.9401 12.1069 13.1084C9.29997 15.276 12.6659 16.7489 13.7459 14.713C14.8258 12.6772 17.7747 7.44316 19.304 6.44221C20.8326 5.44128 21.9089 6.00204 21.5484 8.06532C21.188 10.1286 14.795 15.1295 15.4171 16.2118C16.0391 17.2934 18.2312 14.9402 18.2312 14.9402C18.2312 14.9402 25.0907 8.49588 26.5842 10.1752C28.0776 11.8545 25.4512 13.2616 21.7082 15.6008C17.9646 17.9393 17.6744 18.557 18.2054 19.4418C18.7372 20.3266 26.9998 13.1351 27.7759 16.1838C28.5513 19.2324 19.3434 20.1173 19.9117 22.2219C20.48 24.3274 26.3979 18.2382 27.6082 20.6107C28.8193 22.9839 19.2574 25.7722 19.18 25.7929C16.0914 26.62 8.24723 28.3726 5.50686 24.2246Z" fill="#FFD21E"></path></svg>
	Community
	

	
		</a></div>
	
			</div></div></header>
</div>
	
<div class="container relative flex flex-col md:grid md:space-y-0 w-full md:grid-cols-12 md:flex-1 md:grid-rows-full space-y-4 md:gap-6 ">
		<section class="pt-6 border-gray-100 md:col-span-8 pb-24 relative break-words copiable-code-container">
				<div class="SVELTE_HYDRATER contents" data-target="UnsafeBanner" data-props="{&quot;classNames&quot;:&quot;mb-4&quot;,&quot;repoId&quot;:&quot;RamAnanth1/talkrl-podcast&quot;,&quot;repoType&quot;:&quot;dataset&quot;,&quot;minLevel&quot;:&quot;unsafe&quot;}"></div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetViewer" data-props="{&quot;data&quot;:{&quot;kind&quot;:&quot;DatasetAndSampleData&quot;,&quot;datasetInfo&quot;:[{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Size of downloaded dataset files:&quot;,&quot;value&quot;:&quot;2.63 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;/datasets/RamAnanth1/talkrl-podcast/tree/refs%2Fconvert%2Fparquet/&quot;,&quot;label&quot;:&quot;Size of the auto-converted Parquet files:&quot;,&quot;value&quot;:&quot;2.63 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Number of rows:&quot;,&quot;value&quot;:&quot;39&quot;}],&quot;partial&quot;:false,&quot;configsData&quot;:{&quot;configInfos&quot;:[{&quot;name&quot;:&quot;default&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:39}],&quot;selectedConfig&quot;:&quot;default&quot;,&quot;hasSelectedConfigParquet&quot;:true},&quot;splitsData&quot;:{&quot;splitInfos&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;numRows&quot;:39}],&quot;selectedSplit&quot;:&quot;train&quot;},&quot;sampleData&quot;:{&quot;dataset&quot;:&quot;RamAnanth1/talkrl-podcast&quot;,&quot;config&quot;:&quot;default&quot;,&quot;split&quot;:&quot;train&quot;,&quot;capabilities&quot;:{&quot;rows&quot;:true,&quot;search&quot;:true,&quot;filter&quot;:true,&quot;statistics&quot;:true},&quot;navigation&quot;:{&quot;p&quot;:0},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzA1OSwic3ViIjoiL2RhdGFzZXRzL1JhbUFuYW50aDEvdGFsa3JsLXBvZGNhc3QiLCJleHAiOjE3NDI5MjY2NTksImlzcyI6Imh0dHBzOi8vaHVnZ2luZ2ZhY2UuY28ifQ.qk4NaB36N3Brq9_dV8owI_37C6sYrNZh_lx20i1_PeOOYyDxjBJkP4O-5Brcin7_1HMlxA856PC1q_VPFF4YBA&quot;,&quot;sampleData&quot;:{&quot;columns&quot;:[{&quot;name&quot;:&quot;title&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;title&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:9,&quot;max&quot;:81,&quot;mean&quot;:18.64103,&quot;median&quot;:15,&quot;std&quot;:13.54231,&quot;histogram&quot;:{&quot;hist&quot;:[24,10,3,0,0,0,1,0,0,1],&quot;bin_edges&quot;:[9,17,25,33,41,49,57,65,73,81,81]}}}},{&quot;name&quot;:&quot;summary&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;summary&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:69,&quot;max&quot;:170,&quot;mean&quot;:142.69231,&quot;median&quot;:147,&quot;std&quot;:27.68065,&quot;histogram&quot;:{&quot;hist&quot;:[1,3,1,0,1,4,8,6,6,9],&quot;bin_edges&quot;:[69,80,91,102,113,124,135,146,157,168,170]}}}},{&quot;name&quot;:&quot;link&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;link&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:58,&quot;max&quot;:58,&quot;mean&quot;:58,&quot;median&quot;:58,&quot;std&quot;:0,&quot;histogram&quot;:{&quot;hist&quot;:[39],&quot;bin_edges&quot;:[58,58]}}}},{&quot;name&quot;:&quot;transcript&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;transcript&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:1663,&quot;max&quot;:94178,&quot;mean&quot;:56503.25641,&quot;median&quot;:55690,&quot;std&quot;:20365.81197,&quot;histogram&quot;:{&quot;hist&quot;:[1,0,2,5,3,9,8,6,0,5],&quot;bin_edges&quot;:[1663,10915,20167,29419,38671,47923,57175,66427,75679,84931,94178]}}}},{&quot;name&quot;:&quot;segments&quot;,&quot;align&quot;:&quot;left&quot;,&quot;type&quot;:&quot;list&quot;}],&quot;rows&quot;:[{&quot;rowIdx&quot;:0,&quot;cells&quot;:{&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;John Schulman&quot;},&quot;summary&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;John Schulman, OpenAI cofounder and researcher, inventor of PPO/TRPO talks RL from human feedback, tuning GPT-3 to follow instructions (InstructGPT) and answer long-fo...&quot;},&quot;link&quot;:{&quot;kind&quot;:&quot;audio&quot;,&quot;value&quot;:[{&quot;type&quot;:&quot;audio/mpeg&quot;,&quot;src&quot;:&quot;https://media.transistor.fm/2bfa4dc4/b29b0c00.mp3?src=site&quot;}],&quot;inferredFromUrl&quot;:true},&quot;transcript&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot; The answer was affirmative. We can get an agent to basically use a set of tools that we give it. In this case, the browsing commands like searchings. I would say I expect AI to be able to do better, a better job than humans at most jobs that humans do now. Five years or so. TalkAulRO podcast is all reinforcing learning all the time, featuring brilliant guests, both research and applied. Join the conversation on Twitter at TalkRL podcast. I'm your host, Robin Chohan. John Schulman is a co-founder of OpenAI and a researcher and engineer at OpenAI. He is well known for major contributions to the field of reinforcement learning, including the TRPO algorithm that's trust region policy optimization, GAE, generalized advanced estimation. Those are from his UC Berkeley dissertation and TRPO's descendant proximal policy optimization, or PPO. His current focus at OpenAI is on RL from human feedback. John, welcome to the show and thanks so much for being here. Thanks a lot for having me. You were literally one of the first people I thought of when I started the show three years back. Thanks, I'm honored. It means a lot to me to have you here today. I definitely remember you were nuts and bolts of deep RL video back in the day and watching that multiple times and gaining a lot from that. You helped a generation of RL practitioners back then. By the way, there's going to be a reboot of the nuts and bolts presentation. I got invited to give a talk at NERPS this year on it. I'll have to revamp the guidelines and everything. That'll be fun. Oh, that's awesome. Can't wait for that. You were clearly one of the earlier pioneers in deep RL. How did you choose to move your focus to RL from human feedback? Why is that an important problem? Why is that important to you? After GB3 was trained, I was blown away by how smart it was and I realized the next frontier was figuring out how to make language models actually useful. I'm still really interested in RL but solving RL benchmarks isn't the end of the story. To use your RL algorithm you need a reward function. Whereas the reward function come from in RL benchmarks, you usually just code up the reward function. But if you're not in a simulator environment, that doesn't work. What we have to do in any kind of real-world use case is have humans look at what the AI did and decide if it was good or bad. How exactly do you define this reward becomes a really challenging and important problem, especially as the tasks get harder to evaluate? Another angle on this is that language models are very smart but it's hard to get them to do anything useful. A big part of that is they're not necessarily trying to do what you want. They're just trying to imitate the training corpus. That means there's a big opportunity to improve them a lot by just giving them the right objective. That's what we can do by applying RL to these language models using human feedback to define the reward. Is human feedback harder or very different in some way than using a synthetic reward? There are a lot of new complications. You have to collect a data set dynamically. You're always in the business of building data sets of human preferences. Often the data quality there matters more than various algorithmic details. You also have to think a lot about exactly how you're giving the task to the human trainers and various other things that you wouldn't have thought about if you just had a programmatic reward function. Does the difference between human-raders or the noisiness of the reward signal cost any problems? I would say the noise definitely you need to be below some threshold of noise to learn anything. I think in general if you have a large noisy data set that can be as good as a smaller clean data set. Actually, noise isn't the thing that worries me the most. It's more that there are sometimes consistent biases that people have. For example, in settings like question answering or settings where you have a model writing some text, often people prefer longer answers. You end up with these very verbose answers. If you're not careful with the instructions that is. You can also instruct people the raiders to reward brevity. But without yet, if you're not careful you can incentivize the wrong kinds of behaviors. So let's move to some of your recent work. First up is WebGPT. Browser assisted question answering with human feedback. That's a Nekano at all with yourself as a co-author in 2021. Can you tell us what is the main idea of this paper? What is WebGPT? In WebGPT, we basically took our language models and we hooked them up to a web browser so they could retrieve information from the web. They can write an answer by summarizing the relevant pages from the web. That way if you're asking a question about current events or a question that requires some detailed scientific or technical knowledge, this AI can go out and look up the answer and with detailed citations to its sources. I would say there's two interesting points to this. One is we were exploring whether you could turn language models into a kind of agent. There's a lot of data on the web of different texts that people have written. But there's not a lot of data that shows how to actually do some multi-step process. So it's not that clear, uprearry whether you can get a language model to actually carry out some iterative process. We just have a lot of data like writing essays and having chats and so forth. So that was one thing we were exploring here and I think the answer was affirmative. We can get an agent to basically use a set of tools that we give it. In this case the browsing commands like searchings, scroll link, click on links. The second theme of this paper was around truthfulness. I mean a big issue with language models is I mean they're not very reliable at giving you true information. They know a vastly superhuman amount. But if you prompt them in the wrong way they'll just output lots of plausible sounding nonsense. So how to fix that is a big research question or one of the biggest research questions in the world of language models. I think it's going to be challenging to fully fix it but I think a big part of the story involves retrieval and having models write answers that contain citations. Citations to try trusted sources. So a person who's checking over the answer doesn't have to go and try to figure out where the model might have gotten this idea. They can go and directly look at the source and see if it supports the AI statement. With WebGBT we just wanted to see if we do give the language model a really flexible interface to the web. Can we have it answer hard questions truthfully using like with the help of all these citations. And it's actually really non-trivial because if you look at the data that we use the Reddit explain it like on five. The questions are really varied like some of them are about science, history, current events. Like our Raiders didn't necessarily know anything about these topics but still they had to judge the answers written detailed answers. So it would have been really hard to do it without the supporting citations. So we kind of validated that we could get good feedback in a hard domain like this with the help of citations. Can you talk about where the idea for WebGBT came from? Is that an idea you've had kicking around for a while or was it something that came up recently before the paper? How did that play out? Some of the ideas had been floating around like we thought that we actually had a project at OpenAI very early on a world called World of Bits. We were looking at controlling web browsers or doing tasks that involve tasks on the internet with the web browser but it was way too early at the time. So we kind of abandoned it for a few years. Actually we were trying to back then we were trying to do it with full visual input. So we thought yeah we could give some instructions to the agent like go and figure out figure out the address of this building or something. The agent would go and search the web or use Google Maps or whatever to figure out the answer. And we were trying to do this all in pixels that obviously didn't work very well. But now we have these great language models on the work on text data. We can also extract the text out of web pages to get most of the information. We can't really interact with a lot of dynamic websites. Yeah, where there's a lot of JavaScript and images and so forth. But as long as it's just browsing and reading text we're fine. So yeah we had good enough models and that made it kind of feasible to revisit this idea of using the internet as an environment. So I would say that was one of the sources of inspiration that long-stinted, that long kind of thread about like using the internet as an environment. Another motivation was just after we got after we started playing with GPD3 we noticed that it had all these problems with factual accuracy and the reliability of the information it was giving us. So that kind of motivated doing more research on how to make language models more truthful. We were kind of brainstorming what to do there and we went through some docs and eventually decided that we wanted to try some question answering like using the web, looking up knowledge on the web to help answer questions. So actually the original version of the project used trivia questions. So there's another, there's this well-known data set trivia QA that has some basic trivia questions. So we first worked a little bit on that data set and tried to see if we could boost the model's accuracy by giving it web search and yeah that actually works quite straight, that worked pretty easily. So then we decided to move on to long-form question answering and so that gave us the, that was the project we ended up working on for a while. It seems like you use a few different data sets here and a number of different training methods. I'll just mention the last behavior cloning, reward modeling, reinforcement learning, and rejection sampling. So we were using a fairly standard methodology which was actually adapted from previous work on RL from Human Preferences. So the pipeline is you first train a model with supervised learning where you you have human demonstrators show how to do the task, like show how to map from observations to actions. Yeah so that's the supervised learning or behavior cloning step then we train a reward model or preference model. It looks at two actions or two out trajectories and decides which one is better. In this case like in a question answering setting you're looking at two answers and deciding which answer is better and we use that to train a reward model that assigns higher score to the good answers than the bad ones. Then you do reinforcement learning against that reward function and of course you can iterate these last two steps. After you do a little RL now you're, you sort of exploited some of the flaws of the reward model like or some of the noise in the reward model and it's not necessarily accurate on your new distribution of data. You recollect more pairs of samples and refit this preference model and then you do another iteration of RL. So that's like that's the whole RL from Human Feedback Pipeline and there's this other idea called rejection sampling or best event sampling and in general you can do other kinds of search too where instead of doing RL once you have your reward model you can just search against that reward model so you can take a bunch of collect a bunch of samples and re-rank them with the reward model and take the best one as your action. Kind of like NPC. Yeah exactly. Yeah kind of depends exactly what setting you're in what you can do. If you're in a setting where there's some environment you're interacting with then you would have to simulate your, you would have to simulate the dynamics of your environment which yeah so that would look kind of like NPC. In our case we were the only thing we had to learn a model of was the human preference so like we're it's a question answering setting so it's really like a contextual banded problem so it's kind of straightforward to take a bunch of sample a bunch of actions where each action is a full answer and re-rank them or search against the search over answers. So in terms of the action space was it the action space just a list of commands or is it still generating tokens like a regular generative mode? We were generating tokens. We had two phases of like in each episode of the RL task so there is first a browsing phase where where the model goes and it issues searches and clicks on things and quotes relevant information like if it sees something useful on the page it'll it'll quote it using this quote commands and then once it's browse it's done browsing it'll issue another command called end browsing and it'll write its answer that's also expressed in tokens but really we rolled this all into one big RL task where your episode involves browsing and writing out the answer and it's all one big RL episode. Did you think this is going to work well or were you kind of surprised? At the very beginning of the project we didn't know if it was going to work or not. Like after we did the initial experiments with Trivia QA which actually didn't take that long to get running then it became pretty clear that it would work that the browsing part worked at least and we already know that we can get these models to write pretty good long form text with a bunch of if you give them a bunch of snippets of text that they they can cite. So I noticed the the the human raiders task was quite complicated as it was a long guide and there was many types of feedback that they were giving but in the end the paper said that only the final rating was used so I was just curious if you hadn't commented about that like why do you think maybe the model couldn't use that extra feedback whereas it was maybe just too much or not enough samples. Yeah that's been one frustrating finding so far in in that project and also some other projects we've had the same finding but you have your raiders go through this long process for each for each comparison they do where they're comparing a pair of answers and then you only use one bit of information from the whole from this whole process which might have taken like half an hour. It seems like it would be better if we if we were able to extract more information more about the process they went through in arriving at the answer. So we did collect all sorts of other information like we had them provide ratings along several different axes like coherence and factual accuracy and so forth but in the end we didn't really get much of a boost out of using any of this this other information so I'd say it seems like there's it should be possible to do better but unfortunately this methodology which seems kind of dumb so far it's hard to be and people have tried various other ideas for like how to use human feedback instead of you getting these preference scores there various other things you can do like you can have them right critiques and edit or maybe edit the responses. Yeah I think some of these things are are also promising but yeah this methodology of collecting preference data works well. Yeah I think it's it's still an open area of research. Oh yeah regarding the really long instructions. Yeah I think for any of these tasks there is a lot of subtlety in how to do the task properly and so we ended up adding more and more details of like what do you do in this situation and what do you do in that situation. I think it's starting to get pretty unwieldy with these really long instruction manuals so there's some promising ideas for how to address this like there's a paper from DeepMind recently Sparrow that used basically broke down the task and they trained they basically had people look at one aspect of the one aspect of the response at a time and and then they had a way of combining these different rule specific they would train a bunch of rule specific reward models and then combine them at the end. Yeah I think there's some other interesting ideas for how to how to make this process better. So I gather that from your answer about WebGPT and the whole idea of WebGPT is that you want the the language model type access to external knowledge but I wonder where you think the line should really be in terms of what a language model should know and what the language model should look up and maybe what the language model should not know or not purport to know. Do you have opinions about that? Yeah let's see like some people are advocating for very small language models that have like no external knowledge aside from language I guess would be the extreme position and then other people other people talked about language models that just know everything as opposed to having an external knowledge source. There's some interesting questions there so I think it is a little hard to separate knowledge factual knowledge from understanding. So as humans we get by like not memorizing all sorts of facts and just knowing that we can look them up if needed. For working on a specific domain it is useful to like have a lot of facts internalized so that you can recall them very quickly and kind of combine them combine them in your head. So I wouldn't take an extreme position on either side I would say I think retrieval is going to be really useful just at the very least for current events but also I don't think we want to try to pack all human knowledge into the weights of a neural net. On the other hand I think people have had a lot of luck just scaling up models and like as they soak up more factual knowledge they also get better at reasoning and other things and I think I haven't seen any demonstrations of tiny models that just do lots of retrieval and save all their weights for reasoning. Yeah I just haven't seen any evidence of this or that or I haven't seen any successful attempts at making this. Let's move on to training language models to follow instructions with human feedback that was uyang et al and that was 2022 with yourself as a co-author. Can you tell us the main idea with this paper? This is the instruct GPT paper. What does instruct GPT and what's going on here? Instruct GPT is a language model that's fine tuned to follow instructions and it's in fact the one that you can play with if you go to the open AI website you get a big text box and you can write some text and then press the button to generate a completion. So the idea here was I mean language models are pretty useful and you can sometimes get them to do what you want by prompting them just right. This idea of few shot prompting has been become pretty popular where you give a few examples like a few question answer examples and then if you ask another question it'll hopefully provide an answer in the same style. So the idea yeah so if you can get language models to do great things with prompting but prompting is itself an arg and it's tricky to get right and it's also kind of not necessarily getting the best possible performance out of the model. If you just take a raw language model and you try to you try to talk to it like you ask it a question it probably it doesn't know that it should actually answer that question as well as possible. For all it knows you want it to give a joke answer or a riddle or something. Yeah so the idea of instruct GPT was let's make a kind of small change for our language models so that they're much easier to use. In particular we're going to train them to if you have a piece of text where there's an instruction the model will try to follow that instruction to the best of its abilities and pretty much anything can be an instruction like you can have a the instruction can be to continue a chat or it can be to like summarize like summarize this text or give me a list of names for my company that sells widgets. Yeah instructions can be anything and that makes that makes this kind of model very powerful. So that was kind of that's the idea of an instruction following model it's like a model that can do anything that you specify with an instruction and by the way I wasn't a core contributor to this work I was more involved with like getting the RL infrastructure and some of the RL training details like helping out with that that stuff. But anyway yeah what we did in this project was we ran this this whole methodology that I just described of RL from even preferences in this instruction following setting. So we did supervised fine tuning, collected preference data, trained a reward model and then did RL against that reward model and one interesting detail is actually whereas the original initial data was just collected using contractors. At a certain point we had the the API and it's got this I mean we have this playground on the website where this is where you the big text box where you can use the model. So we we took prompts that people that users had put into the into the playground and use those for training like both to collect preference data and to do RL. So and this is like this is disclosed to users pretty prominently like when when people are using the playgrounds you get notified that your prompts might be used for the training and we're also careful to train in such a way that we don't memorize any information that was in in the prompts. Like it and it explicit like we have a pretty like elaborate process for making sure there's no like private information being leaked into the model. But anyway yeah that's that's basically the experimental setup and the result was that it works like this methodology works quite well and you get a model that's vastly preferred to the base model on this distribution of of realistic prompts that people are giving the model often which contain instructions. So the raw like the the raw language models generally do a really bad job following instructions but this RL trained instruction following model is is a lot better and it's something like if you just calculate how much better it's something like it's as good as a model that's a hundred times bigger. That's a lot. Yeah. You wanted the model to be truthful is that is that one of the criteria you wanted? Oh yeah truthfulness was one of the criteria. That seems amazing to me that truthfulness is something that I could learn by example like does that mean that truthfulness is somehow represented inside the network or because there's no external way for the model to confirm whether something is true or false. So how how might it know what is what is true without any external reference? I think to some extent there is some internal representation of truthfulness. So I would say like one way to think about what language models do is they're trained to imitate the whole internet and the internet is written by lots of different people and has lots of different types of content from fiction to nonfiction to like like technical like detailed technical literature to like jokes and like forum posts whatever. So what the model is basically an ensemble of all these people who wrote stuff on the internet the raw pre-trained model. When you feed it a prompt what it's doing internally has to be something like figuring out who wrote the first wrote this prompt and then trying to continue in that style. So if it thinks it's reading just reading something on the Wall Street Betts Reddit it's going to continue on that style but if it thinks it's in the New York Times it's going to write in a very different way. So effectively the model must be like calculating somewhere like what style is this or what ensemble what's the like narrower ensemble of styles that I'm trying to imitate now. At the very least when you do some kind of when you do training like either supervised fine tuning or are all from human feedback you can at least like narrow down the set of styles the model is producing and try to imitate like the best or the best person in the training set or the best style in the training set and obviously best will differ a lot. So what we'll end up with will depend on our instructions. So if we if we tell I don't know we'll end up with something that has kind of safe like not too not too controversial but a bit corporate will end up with something like that depending on what our instructions are. So at the very least like we can kind of narrow in on one style instead of having the whole distribution of styles on the internet. I think probably there's more to it than that like we're not just learning about style but the model probably is like internally trying to determine if things are if statements are true or not like if the prompt contains incorrect information because that probably would be useful for determining a likely completion. I'm just talking about the raw pre-trained model so I think yeah I think just the objective of predicting next tokens probably gives you a lot it forces the model to like the determine if things are true or not. I think for our alfine tuning there's a lot more potential for the model to actually like try to output something truthful as opposed to trying to imitate a certain style though it's hard to I guess it would be hard to like determine if that's what the model is actually trying to do. So it's almost like the the prompt is guiding the model it's like what corner of the internet do we want to do we want to imitate here and maybe we want to instruct GPG wants to to focus more on the most more truthful corners of the internet something similar to that. Yeah I would hope so at least I think that's a pretty good though maybe a little simplistic picture of what's going on. At the very least we should be able to imitate the most truthful corner of the internet. So can you talk about a generalization and how does this type of model perform out of distribution? Like I guess if it seems questions that are a bit different than what it was trained on. What happens if we get a little bit away from the training data with the reward models? I mean language models in general generalize surprisingly well and I would say overall like these pre-trained models that are trained on super diverse data sets from the internet. They tend to generalize quite well or surprisingly well at least it's surprising to those of us who were around for the earlier days of machine learning when everything was trained from scratch and very fragile. For example if you ask if you provide an instruction in some other language even a even a fairly rare language it'll often do a decent job following the instruction even if there's zero data in the whole instruction following the training process that's in that language and that's just to carry over from the pre-training. So I think generalization yeah I think language models generalize quite well. So you asked about reward models I think one of the tricky pieces about RL from human feedback is how so you have this reward model and you're actually training against it meaning you're training your policy to have high reward and it's going to exploit the errors in the reward model so it's going to eventually find adversarial examples to the reward model. This is worse than kind of normal out of distribution behavior it's like targeted out of distribution examples so so there are definitely some challenges around getting reward models to generalize well or generalize as far as possible from the training set. Can these types of agents tell us when they don't know something or is that a hard problem? I'd say sort of if you ask a question that's kind of in the core of the model's knowledge it will know know the answer and it'll know that it knows. By the way I'm talking about models like the for the instruct model if you ask it about something that's like very simple at the core of its knowledge it'll know if you there are certain things that it knows that it doesn't know like current events where it's been trained to know that it doesn't know certain things in real time but if you ask it about something that's kind of on the edge of its knowledge it's it's going to have a hard time it's it's necessarily going to be inaccurate. I mean there have been a couple papers about this question so there is in paper from Anthropic recently called language models mostly know what they know and there is also a paper from FHI and OpenAI called getting language models to express their uncertainty and words. These language models as well as a lot of other models in machine learning are training to maximize likelihood so maximize log-prob of data. You're already training them to always predict a distribution of outputs. So for language models given a prefix it's predicting a distribution over the next token. These predictions for the next token like generally are pretty well calibrated but 80% if it puts 80% probability on something and you look at all the times when it puts 80% probability on something like it's right 80% of the time. Like that's just a result of the training objective. The training objective like strongly incentivizes the model to be calibrated meaning it has a reasonable estimate of its uncertainty. So at the single token level models definitely are calibrated. The question is whether they're calibrated on whether this calibration extends to settings where they are generating multi-token outputs or whether they can judge the correctness of some multi-token statement. So I would say since models are calibrated at the single token level they I think they definitely have the information to be calibrated in these other settings. So that's why I think the problem of models knowing what they know isn't actually that hard or at least getting a model to express its uncertainty pretty much as well as a human does doesn't feel like an insurmountable problem but there's some practical difficulties to getting getting there. People use the phrase AI alignment in different ways. Can you talk about how you see alignment in your work on Aral from human feedback? I think of alignment mostly as the problem of getting the model to try to do the right thing so we can kind of make a distinction between what the model is capable of doing. Like if you just take a raw language model and you ask it a question like I said before it doesn't know that you actually wanted to give the correct answer as opposed to. It might think someone who is not very knowledgeable is answering. By doing some extra training we can get the model to actually try to do the right thing and so I would say that that's the main goal of alignment. So there was an open AI blog post recently that talked about the sequence in alignment. One was training AI systems using human feedback to use it training AI systems to assist human evaluation and three training AI systems to do alignment research. So is your current work mostly about this first item and when and how do you see us getting to these other stages? I'm doing some work now on number two training AI systems to assist human feedback. I think that's sort of becomes increasingly necessary as you start trying to get the systems to solve harder and harder problems. When you have models that are kind of very below human level or maybe at human level at a certain task it's pretty straightforward to supervise them. But once they're doing things that are very hard or doing things that require a lot of diverse technical knowledge it becomes pretty hard to provide a useful supervision signal. So we have to start doing things like one model writes an answer to do a question and then another model provides a critique of that answer points out some flaws and then the human only has to judge the first answer after looking at the critique meaning basically the critique helps the human assess the answer. So I think like that kind of idea is starting to become pretty relevant. A colleague's an I are exploring that kind of idea now. As for assisting alignment research there's some other work at open AI that's starting to explore this. It's also that sort of the for this down the road. So I saw Stuart Russell was on your PhD committee and I really enjoyed his book Human Compatible. I wonder if you share the idea mentioned in the book that the standard RL framing with this fixed reward signal is problematic and that agents powerful agents should try to do what we want and maintain some uncertainty about what it is we want and the agents that are too certain will be problematic. What do you have any thoughts on that idea? I totally agree with that idea. So I think first it's really hard to write down a simple reward function that actually captures what we want or what any any particular person wants. I can say I want a little more of this or a little more of that but you wouldn't want to take that to the extreme. If we build agents that try to cater to our to our wishes we should make sure they're like they have a lot of they have uncertainty about what we want or what we value and that that'll also cause them to be a little more cautious and say not disturb anything that might be important to us. So yeah I agree with that like Stuart Russell gave a very good like problem definition of what we want AI to do like we want it to basically we want to jointly like play this game where AI is the AI is trying to figure out what we want and then trying to do that but simultaneously maintaining some uncertainty about what we want. I would say if you you start to look at how to get that in practice it actually looks quite a bit like the kind of RL from human feedback that we're working on at OpenAI and others are working on other places. I think yeah I think I see what we're doing as a practical implementation of getting towards this behavior that Russell have described. Do you think of a AGI as an abstract goal or are we going to see a model come out one day and people are going to say oh that's the first AGI model like what does it have to do for people to say that? I think people will say that many times then realize that it doesn't quite do everything that you want. I think we're going to have a lot of like a long series of models that are that are superhuman at most things or at a certain class of things but they also have some failure modes and weaknesses. Like I expect us to like see multiple models that are proclaimed as AGI and then only after interacting with it a while you do realize it's not quite there. What would you say is the relationship between AGI and RL and AGI and these large language models? How do those concepts fit together? I would say that RL is a useful like component of training AGI or an almost essential component. The thing RL lets you do is it lets you optimize any objective for the agents. Any objective that is a function of the agents behavior. So with pre-training like what we do for language models you're kind of choosing an objective that lets us do something with all the training day we have which is all this internet text. So we choose this maximum likelihood objective which is basically the only or not the only thing but it's like a sensible way to absorb all this knowledge. But then if we really want to optimize the agents behavior for a specific objective RL is kind of the only framework that lets you do that. Okay John we have a few questions from the audience and I'm just going to pick the two that have the highest score in terms of Twitter likes. So the first is from Eric Chang VP of AI at a Hello Di Robotics. He asked RL distributions are non-stationary making it hard to reason about PPO losses and how that relates to return or generalization. Are there any intermediate plots and visualizations you like to generate to debug or incrementally build up a large scale RL system? Yeah there are definitely some stats that I look at so I will be I'll talk about this in the nuts and bolts like reboot waited a year but I'd say things like you're looking at the explained variants of the value function and looking at the like how many samples are getting clipped in PPO and what the KL between the what what the KL divergence is between the policy before and after the update is yeah things like that. And then Ethan the calibar from Miele asks what is your median estimate for the arrival date of AGI? I think not too far away but I like I said I expect there to be a lot of fall starts I would say expect like like AI to be able to do better a better job than humans at most jobs that humans do now five years or so that's not all jobs but most jobs for a while we're going to discover things that AI isn't very good at and then where we want to keep humans in control so I think there'll be some kind of gradual process over the next 10 or 15 years. I've been curious about this I see that some RL work is patented but I could not find a TRPO or PPO in I could not find patents on these are those protected patent protected at all or how do you how do you think of intellectual property protection for that kind of work? I haven't ever looked looked into patenting anything and open AI hasn't either as far as I know I think the trend over time has been for people to take a patent scene machine like a machine learning algorithms last seriously there is this algorithm in computer vision called sift which is like this key point to detector and this was patented I think the the guy who patented it he probably made his university some money from the patent but in the end all it did was cause people a lot of annoyance because like the people people had to come up with alternative algorithms that like had a different acronym and weren't patented so like the open CV open source library would have like had to be careful about putting this algorithm in their library because of the patent risks so I think like these patents aren't the patent rights aren't exercise that much and I think big companies like Google will patent a lot of stuff for defensive reasons so if they get in some big legal dispute with another company it can be used as like one of the bargaining chips but I think I don't think anyone's going to like get sued for royalties for not yeah for not providing royalties for the use of some algorithm okay and then there's been a ton of work in RL of course since you first published TRPO and BBO but from your point of view if you had to pick a few highlights in terms of a few important milestones in in RL algorithms since PPO came out and by the way it's amazing that in 2022 we're still using PPO I think quite similar into it's original form is that right yeah pretty much yeah so so what would you say are the the biggest highlights for you in terms of our algorithm since since you did PPO yeah there's definitely been some interesting stuff so I think like a little after PPO there is TD3 and SAC and those are seem like pretty solid value-based methods that was one development that was interesting I think like yeah I thought museiro and it's and it's like elaborations we're also like efficient zero we're also pretty impressive that you can get that good sample efficiency both of the things I just mentioned were kind of well I don't want to say mostly on toy tasks or benchmarks because yeah I'm sure people are doing some real things with these algorithms yeah so I think that's that stuff was interesting I think like the whole recent interest in search of interest in the offline RL was also notable I would say the like the stuff we're doing with RL from human feedback is the kind of offline RL because we're like we have a fixed dataset and we have a fixed reward modeling dataset and we're training against that this is like offline RL but you're doing it in a different way you're using an on-policy algorithm with a reward model as opposed to maybe a more typical way to do offline RL would be use off-policy algorithm would that work here or would that not work here well we're doing here is kind of like model-based RL because the reward model is like a model of the like the unknown part of the system so like the unknown part of the system here is the is the human radar or the human it's not the outputting appending to your list of tokens so this is kind of like the work that's like takes a dynamics model at the environment and does some kind of just runs a policy grading algorithm against it so it's not like so the idea of running an online algorithm against a model that's kind of a well-established idea so I would say the papers that previously did this they were in a pretty different regime were in this regime of doing fairly small updates to the policy because we have this these awesome pre-trained models and we don't need to actually change them that much so yeah we use these online algorithms I'd say part of the reason why we can get away with using just an like an online algorithm is because we've been just looking at a band a contextual banded problem yeah because we only have like one time step like you get a query and you output a response and then that response gets a reward so if we had a like a multi-step process such as a conversation where you can't assign a reward until the very end of the conversation and or you had some I don't know some interaction with like some real-world system that's hard to simulate you wouldn't then it wouldn't be S-ray forward to you wouldn't be able to use exactly exactly the same methodology you would probably have to use a you would have to probably train a Q function or or something like that if you want if you want your method to be sample efficient you would probably have to do something slightly different I think we'll we'll have to we'll have to start exploring this at some point soon but so far we haven't at least I haven't seen any cases in like in the domain I'm looking at that require this but I expect it to to be relevant at some point so we had Arvind Shrinivas talking about decision transformer on the show recently that was a great episode and I see that you were also a co-author on the the 2016 RL squared paper I want to ask you what your thoughts about meta RL Arvind had some interesting things to say about maybe the idea that a transformer could kind of supersede the need for an RL algorithm altogether what do you expect from meta RL do expect will will still be using human authored RL algorithms in the future yeah that's a pretty bold statement that we don't need we won't need any RL algorithms anymore yeah since the RL squared paper people have been talking less about meta learning as far as I can tell actually because of sequence modeling has gotten so good like transformer let sequence models so that it's kind of queer the meta learning is just a special case of learning like it's it's just it's just like a certain kind of long context learning learning involving long episodes and maybe it shouldn't be treated that differently or are addressed with special algorithms I would say yeah the ideas like decision transformer are pretty interesting where you try to reduce RL to supervise learning it's still not like certain exactly how these compare and performance to RL like people have started to analyze that empirically and theoretically and I would say in practice sometimes sometimes it's better sometimes it's worse in my experience like it's been worse on the problems that I've that I've my colleagues and I have where we've tested it but yeah it's definitely an interesting direction Dr. John Schillman thank you so much for sharing your time in your insight with the talk our audience today thanks so much thank you&quot;},&quot;segments&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;end&quot;:6.24,&quot;start&quot;:0,&quot;text&quot;:&quot; The answer was affirmative. We can get an agent to basically use a set of tools that we give it.&quot;},{&quot;end&quot;:12.48,&quot;start&quot;:6.24,&quot;text&quot;:&quot; In this case, the browsing commands like searchings. I would say I expect AI to be able to do better,&quot;},{&quot;end&quot;:17.84,&quot;start&quot;:12.48,&quot;text&quot;:&quot; a better job than humans at most jobs that humans do now. Five years or so.&quot;},{&quot;end&quot;:27.92,&quot;start&quot;:22.56,&quot;text&quot;:&quot; TalkAulRO podcast is all reinforcing learning all the time, featuring brilliant guests,&quot;},{&quot;end&quot;:34.08,&quot;start&quot;:27.92,&quot;text&quot;:&quot; both research and applied. Join the conversation on Twitter at TalkRL podcast. I'm your host,&quot;},{&quot;end&quot;:44.32,&quot;start&quot;:34.08,&quot;text&quot;:&quot; Robin Chohan. John Schulman is a co-founder of OpenAI and a researcher and engineer at OpenAI.&quot;},{&quot;end&quot;:48.32000000000001,&quot;start&quot;:44.32,&quot;text&quot;:&quot; He is well known for major contributions to the field of reinforcement learning,&quot;},{&quot;end&quot;:54.400000000000006,&quot;start&quot;:48.32000000000001,&quot;text&quot;:&quot; including the TRPO algorithm that's trust region policy optimization, GAE, generalized&quot;},{&quot;end&quot;:59.12,&quot;start&quot;:54.4,&quot;text&quot;:&quot; advanced estimation. Those are from his UC Berkeley dissertation and TRPO's&quot;},{&quot;end&quot;:65.03999999999999,&quot;start&quot;:59.12,&quot;text&quot;:&quot; descendant proximal policy optimization, or PPO. His current focus at OpenAI is on RL from&quot;},{&quot;end&quot;:68.16,&quot;start&quot;:65.03999999999999,&quot;text&quot;:&quot; human feedback. John, welcome to the show and thanks so much for being here.&quot;},{&quot;end&quot;:71.75999999999999,&quot;start&quot;:68.16,&quot;text&quot;:&quot; Thanks a lot for having me. You were literally one of the first people I thought of when I started&quot;},{&quot;end&quot;:77.6,&quot;start&quot;:71.75999999999999,&quot;text&quot;:&quot; the show three years back. Thanks, I'm honored. It means a lot to me to have you here today. I definitely&quot;},{&quot;end&quot;:83.12,&quot;start&quot;:77.6,&quot;text&quot;:&quot; remember you were nuts and bolts of deep RL video back in the day and watching that multiple times&quot;},{&quot;end&quot;:88.88000000000001,&quot;start&quot;:83.12,&quot;text&quot;:&quot; and gaining a lot from that. You helped a generation of RL practitioners back then. By the way,&quot;},{&quot;end&quot;:95.52000000000001,&quot;start&quot;:88.88000000000001,&quot;text&quot;:&quot; there's going to be a reboot of the nuts and bolts presentation. I got invited to give a talk&quot;},{&quot;end&quot;:101.92,&quot;start&quot;:95.52000000000001,&quot;text&quot;:&quot; at NERPS this year on it. I'll have to revamp the guidelines and everything. That'll be fun.&quot;},{&quot;end&quot;:107.12,&quot;start&quot;:101.92,&quot;text&quot;:&quot; Oh, that's awesome. Can't wait for that. You were clearly one of the earlier pioneers in deep RL.&quot;},{&quot;end&quot;:112.4,&quot;start&quot;:107.12,&quot;text&quot;:&quot; How did you choose to move your focus to RL from human feedback? Why is that an important problem?&quot;},{&quot;end&quot;:117.84,&quot;start&quot;:112.4,&quot;text&quot;:&quot; Why is that important to you? After GB3 was trained, I was blown away by how smart it was and I&quot;},{&quot;end&quot;:122.32000000000001,&quot;start&quot;:117.84,&quot;text&quot;:&quot; realized the next frontier was figuring out how to make language models actually useful. I'm still&quot;},{&quot;end&quot;:128.4,&quot;start&quot;:122.32000000000001,&quot;text&quot;:&quot; really interested in RL but solving RL benchmarks isn't the end of the story. To use your RL&quot;},{&quot;end&quot;:134.08,&quot;start&quot;:128.4,&quot;text&quot;:&quot; algorithm you need a reward function. Whereas the reward function come from in RL benchmarks,&quot;},{&quot;end&quot;:138.16,&quot;start&quot;:134.08,&quot;text&quot;:&quot; you usually just code up the reward function. But if you're not in a simulator environment,&quot;},{&quot;end&quot;:144.07999999999998,&quot;start&quot;:138.16,&quot;text&quot;:&quot; that doesn't work. What we have to do in any kind of real-world use case is have humans look at&quot;},{&quot;end&quot;:149.04,&quot;start&quot;:144.07999999999998,&quot;text&quot;:&quot; what the AI did and decide if it was good or bad. How exactly do you define this reward&quot;},{&quot;end&quot;:154,&quot;start&quot;:149.04,&quot;text&quot;:&quot; becomes a really challenging and important problem, especially as the tasks get harder to evaluate?&quot;},{&quot;end&quot;:159.2,&quot;start&quot;:154,&quot;text&quot;:&quot; Another angle on this is that language models are very smart but it's hard to get them to do&quot;},{&quot;end&quot;:164.24,&quot;start&quot;:159.2,&quot;text&quot;:&quot; anything useful. A big part of that is they're not necessarily trying to do what you want. They're&quot;},{&quot;end&quot;:168.88,&quot;start&quot;:164.24,&quot;text&quot;:&quot; just trying to imitate the training corpus. That means there's a big opportunity to improve&quot;},{&quot;end&quot;:173.84,&quot;start&quot;:168.88,&quot;text&quot;:&quot; them a lot by just giving them the right objective. That's what we can do by applying RL to these&quot;},{&quot;end&quot;:181.12,&quot;start&quot;:174.64000000000001,&quot;text&quot;:&quot; language models using human feedback to define the reward. Is human feedback harder or&quot;},{&quot;end&quot;:185.92000000000002,&quot;start&quot;:181.12,&quot;text&quot;:&quot; very different in some way than using a synthetic reward? There are a lot of new complications.&quot;},{&quot;end&quot;:192.56,&quot;start&quot;:187.36,&quot;text&quot;:&quot; You have to collect a data set dynamically. You're always in the business of building data sets of&quot;},{&quot;end&quot;:199.12,&quot;start&quot;:192.56,&quot;text&quot;:&quot; human preferences. Often the data quality there matters more than various algorithmic details.&quot;},{&quot;end&quot;:204.32,&quot;start&quot;:199.12,&quot;text&quot;:&quot; You also have to think a lot about exactly how you're giving the task to the human trainers&quot;},{&quot;end&quot;:208.32,&quot;start&quot;:204.32,&quot;text&quot;:&quot; and various other things that you wouldn't have thought about if you just had a programmatic reward&quot;},{&quot;end&quot;:213.44,&quot;start&quot;:208.32,&quot;text&quot;:&quot; function. Does the difference between human-raders or the noisiness of the reward signal cost any&quot;},{&quot;end&quot;:220.56,&quot;start&quot;:213.44,&quot;text&quot;:&quot; problems? I would say the noise definitely you need to be below some threshold of noise to learn&quot;},{&quot;end&quot;:226.64000000000001,&quot;start&quot;:220.56,&quot;text&quot;:&quot; anything. I think in general if you have a large noisy data set that can be as good as a smaller&quot;},{&quot;end&quot;:231.6,&quot;start&quot;:226.64000000000001,&quot;text&quot;:&quot; clean data set. Actually, noise isn't the thing that worries me the most. It's more that there are&quot;},{&quot;end&quot;:238,&quot;start&quot;:231.6,&quot;text&quot;:&quot; sometimes consistent biases that people have. For example, in settings like question answering&quot;},{&quot;end&quot;:244.4,&quot;start&quot;:238,&quot;text&quot;:&quot; or settings where you have a model writing some text, often people prefer longer answers. You end&quot;},{&quot;end&quot;:249.36,&quot;start&quot;:244.4,&quot;text&quot;:&quot; up with these very verbose answers. If you're not careful with the instructions that is. You can&quot;},{&quot;end&quot;:256.40000000000003,&quot;start&quot;:249.36,&quot;text&quot;:&quot; also instruct people the raiders to reward brevity. But without yet, if you're not careful you can&quot;},{&quot;end&quot;:262,&quot;start&quot;:257.04,&quot;text&quot;:&quot; incentivize the wrong kinds of behaviors. So let's move to some of your recent work. First up is&quot;},{&quot;end&quot;:268.40000000000003,&quot;start&quot;:262,&quot;text&quot;:&quot; WebGPT. Browser assisted question answering with human feedback. That's a Nekano at all with yourself&quot;},{&quot;end&quot;:273.84000000000003,&quot;start&quot;:268.40000000000003,&quot;text&quot;:&quot; as a co-author in 2021. Can you tell us what is the main idea of this paper? What is WebGPT?&quot;},{&quot;end&quot;:280.23999999999995,&quot;start&quot;:273.84,&quot;text&quot;:&quot; In WebGPT, we basically took our language models and we hooked them up to a web browser so they&quot;},{&quot;end&quot;:285.35999999999996,&quot;start&quot;:280.23999999999995,&quot;text&quot;:&quot; could retrieve information from the web. They can write an answer by summarizing the relevant pages&quot;},{&quot;end&quot;:290.08,&quot;start&quot;:285.35999999999996,&quot;text&quot;:&quot; from the web. That way if you're asking a question about current events or a question that requires&quot;},{&quot;end&quot;:295.35999999999996,&quot;start&quot;:290.08,&quot;text&quot;:&quot; some detailed scientific or technical knowledge, this AI can go out and look up the answer and&quot;},{&quot;end&quot;:301.67999999999995,&quot;start&quot;:295.35999999999996,&quot;text&quot;:&quot; with detailed citations to its sources. I would say there's two interesting points to this. One is&quot;},{&quot;end&quot;:306.24,&quot;start&quot;:301.68,&quot;text&quot;:&quot; we were exploring whether you could turn language models into a kind of agent. There's a lot of data&quot;},{&quot;end&quot;:310.32,&quot;start&quot;:306.24,&quot;text&quot;:&quot; on the web of different texts that people have written. But there's not a lot of data that shows&quot;},{&quot;end&quot;:316.24,&quot;start&quot;:310.32,&quot;text&quot;:&quot; how to actually do some multi-step process. So it's not that clear, uprearry whether you can get a&quot;},{&quot;end&quot;:321.68,&quot;start&quot;:316.24,&quot;text&quot;:&quot; language model to actually carry out some iterative process. We just have a lot of data like writing&quot;},{&quot;end&quot;:326.16,&quot;start&quot;:321.68,&quot;text&quot;:&quot; essays and having chats and so forth. So that was one thing we were exploring here and I think&quot;},{&quot;end&quot;:332.8,&quot;start&quot;:326.16,&quot;text&quot;:&quot; the answer was affirmative. We can get an agent to basically use a set of tools that we give it.&quot;},{&quot;end&quot;:338.16,&quot;start&quot;:332.8,&quot;text&quot;:&quot; In this case the browsing commands like searchings, scroll link, click on links. The second&quot;},{&quot;end&quot;:344.24,&quot;start&quot;:338.16,&quot;text&quot;:&quot; theme of this paper was around truthfulness. I mean a big issue with language models is I mean&quot;},{&quot;end&quot;:349.76000000000005,&quot;start&quot;:344.24,&quot;text&quot;:&quot; they're not very reliable at giving you true information. They know a vastly superhuman amount. But&quot;},{&quot;end&quot;:354.64000000000004,&quot;start&quot;:349.76000000000005,&quot;text&quot;:&quot; if you prompt them in the wrong way they'll just output lots of plausible sounding nonsense. So&quot;},{&quot;end&quot;:359.84,&quot;start&quot;:354.64,&quot;text&quot;:&quot; how to fix that is a big research question or one of the biggest research questions in the&quot;},{&quot;end&quot;:364.32,&quot;start&quot;:359.84,&quot;text&quot;:&quot; world of language models. I think it's going to be challenging to fully fix it but I think a big&quot;},{&quot;end&quot;:370.32,&quot;start&quot;:364.32,&quot;text&quot;:&quot; part of the story involves retrieval and having models write answers that contain citations.&quot;},{&quot;end&quot;:375.28,&quot;start&quot;:370.32,&quot;text&quot;:&quot; Citations to try trusted sources. So a person who's checking over the answer doesn't have to go and&quot;},{&quot;end&quot;:379.91999999999996,&quot;start&quot;:375.28,&quot;text&quot;:&quot; try to figure out where the model might have gotten this idea. They can go and directly look at&quot;},{&quot;end&quot;:387.6,&quot;start&quot;:379.92,&quot;text&quot;:&quot; the source and see if it supports the AI statement. With WebGBT we just wanted to see if we do give&quot;},{&quot;end&quot;:392.40000000000003,&quot;start&quot;:387.6,&quot;text&quot;:&quot; the language model a really flexible interface to the web. Can we have it answer hard questions&quot;},{&quot;end&quot;:398.32,&quot;start&quot;:392.40000000000003,&quot;text&quot;:&quot; truthfully using like with the help of all these citations. And it's actually really non-trivial&quot;},{&quot;end&quot;:403.76,&quot;start&quot;:398.32,&quot;text&quot;:&quot; because if you look at the data that we use the Reddit explain it like on five. The questions&quot;},{&quot;end&quot;:408.08000000000004,&quot;start&quot;:403.76,&quot;text&quot;:&quot; are really varied like some of them are about science, history, current events. Like our&quot;},{&quot;end&quot;:413.84,&quot;start&quot;:408.08,&quot;text&quot;:&quot; Raiders didn't necessarily know anything about these topics but still they had to judge the answers&quot;},{&quot;end&quot;:418.88,&quot;start&quot;:413.84,&quot;text&quot;:&quot; written detailed answers. So it would have been really hard to do it without the supporting&quot;},{&quot;end&quot;:425.12,&quot;start&quot;:418.88,&quot;text&quot;:&quot; citations. So we kind of validated that we could get good feedback in a hard domain like this&quot;},{&quot;end&quot;:431.12,&quot;start&quot;:425.12,&quot;text&quot;:&quot; with the help of citations. Can you talk about where the idea for WebGBT came from? Is that an idea&quot;},{&quot;end&quot;:435.12,&quot;start&quot;:431.12,&quot;text&quot;:&quot; you've had kicking around for a while or was it something that came up recently before the&quot;},{&quot;end&quot;:441.36,&quot;start&quot;:435.12,&quot;text&quot;:&quot; paper? How did that play out? Some of the ideas had been floating around like we thought that we&quot;},{&quot;end&quot;:447.12,&quot;start&quot;:441.36,&quot;text&quot;:&quot; actually had a project at OpenAI very early on a world called World of Bits. We were looking at&quot;},{&quot;end&quot;:452.16,&quot;start&quot;:447.12,&quot;text&quot;:&quot; controlling web browsers or doing tasks that involve tasks on the internet with the web browser&quot;},{&quot;end&quot;:458.4,&quot;start&quot;:452.16,&quot;text&quot;:&quot; but it was way too early at the time. So we kind of abandoned it for a few years. Actually we&quot;},{&quot;end&quot;:462.8,&quot;start&quot;:458.4,&quot;text&quot;:&quot; were trying to back then we were trying to do it with full visual input. So we thought yeah we could&quot;},{&quot;end&quot;:469.12,&quot;start&quot;:462.8,&quot;text&quot;:&quot; give some instructions to the agent like go and figure out figure out the address of this&quot;},{&quot;end&quot;:475.68,&quot;start&quot;:469.84000000000003,&quot;text&quot;:&quot; building or something. The agent would go and search the web or use Google Maps or whatever&quot;},{&quot;end&quot;:479.92,&quot;start&quot;:475.68,&quot;text&quot;:&quot; to figure out the answer. And we were trying to do this all in pixels that obviously didn't work&quot;},{&quot;end&quot;:486.16,&quot;start&quot;:479.92,&quot;text&quot;:&quot; very well. But now we have these great language models on the work on text data. We can also&quot;},{&quot;end&quot;:493.12,&quot;start&quot;:486.16,&quot;text&quot;:&quot; extract the text out of web pages to get most of the information. We can't really interact with&quot;},{&quot;end&quot;:498.16,&quot;start&quot;:493.12,&quot;text&quot;:&quot; a lot of dynamic websites. Yeah, where there's a lot of JavaScript and images and so forth. But&quot;},{&quot;end&quot;:504.64000000000004,&quot;start&quot;:498.16,&quot;text&quot;:&quot; as long as it's just browsing and reading text we're fine. So yeah we had good enough models and&quot;},{&quot;end&quot;:510.8,&quot;start&quot;:504.64000000000004,&quot;text&quot;:&quot; that made it kind of feasible to revisit this idea of using the internet as an environment.&quot;},{&quot;end&quot;:516.32,&quot;start&quot;:510.8,&quot;text&quot;:&quot; So I would say that was one of the sources of inspiration that long-stinted, that long kind of&quot;},{&quot;end&quot;:522.4,&quot;start&quot;:516.32,&quot;text&quot;:&quot; thread about like using the internet as an environment. Another motivation was just after we got&quot;},{&quot;end&quot;:529.12,&quot;start&quot;:523.2,&quot;text&quot;:&quot; after we started playing with GPD3 we noticed that it had all these problems with factual&quot;},{&quot;end&quot;:535.52,&quot;start&quot;:529.12,&quot;text&quot;:&quot; accuracy and the reliability of the information it was giving us. So that kind of motivated doing&quot;},{&quot;end&quot;:540.4,&quot;start&quot;:535.52,&quot;text&quot;:&quot; more research on how to make language models more truthful. We were kind of brainstorming what to&quot;},{&quot;end&quot;:547.04,&quot;start&quot;:540.4,&quot;text&quot;:&quot; do there and we went through some docs and eventually decided that we wanted to try some question&quot;},{&quot;end&quot;:551.92,&quot;start&quot;:547.04,&quot;text&quot;:&quot; answering like using the web, looking up knowledge on the web to help answer questions. So actually&quot;},{&quot;end&quot;:556.24,&quot;start&quot;:551.92,&quot;text&quot;:&quot; the original version of the project used trivia questions. So there's another, there's this&quot;},{&quot;end&quot;:562.16,&quot;start&quot;:556.24,&quot;text&quot;:&quot; well-known data set trivia QA that has some basic trivia questions. So we first worked a little&quot;},{&quot;end&quot;:569.12,&quot;start&quot;:562.16,&quot;text&quot;:&quot; bit on that data set and tried to see if we could boost the model's accuracy by giving it web search&quot;},{&quot;end&quot;:576,&quot;start&quot;:569.12,&quot;text&quot;:&quot; and yeah that actually works quite straight, that worked pretty easily. So then we decided to move on&quot;},{&quot;end&quot;:582.72,&quot;start&quot;:576,&quot;text&quot;:&quot; to long-form question answering and so that gave us the, that was the project we ended up working on&quot;},{&quot;end&quot;:589.12,&quot;start&quot;:582.72,&quot;text&quot;:&quot; for a while. It seems like you use a few different data sets here and a number of different training&quot;},{&quot;end&quot;:594.96,&quot;start&quot;:589.12,&quot;text&quot;:&quot; methods. I'll just mention the last behavior cloning, reward modeling, reinforcement learning,&quot;},{&quot;end&quot;:601.76,&quot;start&quot;:594.96,&quot;text&quot;:&quot; and rejection sampling. So we were using a fairly standard methodology which was actually adapted&quot;},{&quot;end&quot;:609.2,&quot;start&quot;:601.76,&quot;text&quot;:&quot; from previous work on RL from Human Preferences. So the pipeline is you first train a model with&quot;},{&quot;end&quot;:615.44,&quot;start&quot;:609.2,&quot;text&quot;:&quot; supervised learning where you you have human demonstrators show how to do the task, like show how to map&quot;},{&quot;end&quot;:620.8000000000001,&quot;start&quot;:615.44,&quot;text&quot;:&quot; from observations to actions. Yeah so that's the supervised learning or behavior cloning step then we&quot;},{&quot;end&quot;:628.7199999999999,&quot;start&quot;:620.8,&quot;text&quot;:&quot; train a reward model or preference model. It looks at two actions or two out trajectories and decides&quot;},{&quot;end&quot;:633.76,&quot;start&quot;:628.7199999999999,&quot;text&quot;:&quot; which one is better. In this case like in a question answering setting you're looking at two answers&quot;},{&quot;end&quot;:638.56,&quot;start&quot;:633.76,&quot;text&quot;:&quot; and deciding which answer is better and we use that to train a reward model that assigns higher score&quot;},{&quot;end&quot;:643.04,&quot;start&quot;:638.56,&quot;text&quot;:&quot; to the good answers than the bad ones. Then you do reinforcement learning against that reward function&quot;},{&quot;end&quot;:648.16,&quot;start&quot;:643.04,&quot;text&quot;:&quot; and of course you can iterate these last two steps. After you do a little RL now you're, you sort of&quot;},{&quot;end&quot;:653.4399999999999,&quot;start&quot;:648.16,&quot;text&quot;:&quot; exploited some of the flaws of the reward model like or some of the noise in the reward model and&quot;},{&quot;end&quot;:658.9599999999999,&quot;start&quot;:653.4399999999999,&quot;text&quot;:&quot; it's not necessarily accurate on your new distribution of data. You recollect more pairs of samples&quot;},{&quot;end&quot;:665.28,&quot;start&quot;:658.9599999999999,&quot;text&quot;:&quot; and refit this preference model and then you do another iteration of RL. So that's like that's&quot;},{&quot;end&quot;:670.9599999999999,&quot;start&quot;:665.28,&quot;text&quot;:&quot; the whole RL from Human Feedback Pipeline and there's this other idea called rejection sampling&quot;},{&quot;end&quot;:676.48,&quot;start&quot;:670.9599999999999,&quot;text&quot;:&quot; or best event sampling and in general you can do other kinds of search too where instead of doing&quot;},{&quot;end&quot;:681.52,&quot;start&quot;:676.48,&quot;text&quot;:&quot; RL once you have your reward model you can just search against that reward model so you can take&quot;},{&quot;end&quot;:687.6,&quot;start&quot;:681.52,&quot;text&quot;:&quot; a bunch of collect a bunch of samples and re-rank them with the reward model and take the best one&quot;},{&quot;end&quot;:694.08,&quot;start&quot;:687.6,&quot;text&quot;:&quot; as your action. Kind of like NPC. Yeah exactly. Yeah kind of depends exactly what setting you're in&quot;},{&quot;end&quot;:699.76,&quot;start&quot;:694.64,&quot;text&quot;:&quot; what you can do. If you're in a setting where there's some environment you're interacting with then&quot;},{&quot;end&quot;:705.44,&quot;start&quot;:699.76,&quot;text&quot;:&quot; you would have to simulate your, you would have to simulate the dynamics of your environment which&quot;},{&quot;end&quot;:711.84,&quot;start&quot;:705.44,&quot;text&quot;:&quot; yeah so that would look kind of like NPC. In our case we were the only thing we had to learn a model of&quot;},{&quot;end&quot;:718.24,&quot;start&quot;:711.84,&quot;text&quot;:&quot; was the human preference so like we're it's a question answering setting so it's really like a&quot;},{&quot;end&quot;:723.2,&quot;start&quot;:718.24,&quot;text&quot;:&quot; contextual banded problem so it's kind of straightforward to take a bunch of sample a bunch of&quot;},{&quot;end&quot;:730.8000000000001,&quot;start&quot;:723.2,&quot;text&quot;:&quot; actions where each action is a full answer and re-rank them or search against the search over answers.&quot;},{&quot;end&quot;:736.4,&quot;start&quot;:730.8,&quot;text&quot;:&quot; So in terms of the action space was it the action space just a list of commands or is it still&quot;},{&quot;end&quot;:743.76,&quot;start&quot;:736.4,&quot;text&quot;:&quot; generating tokens like a regular generative mode? We were generating tokens. We had two phases of&quot;},{&quot;end&quot;:751.12,&quot;start&quot;:743.76,&quot;text&quot;:&quot; like in each episode of the RL task so there is first a browsing phase where where the model goes&quot;},{&quot;end&quot;:757.04,&quot;start&quot;:751.12,&quot;text&quot;:&quot; and it issues searches and clicks on things and quotes relevant information like if it sees&quot;},{&quot;end&quot;:761.92,&quot;start&quot;:757.04,&quot;text&quot;:&quot; something useful on the page it'll it'll quote it using this quote commands and then once it's&quot;},{&quot;end&quot;:769.28,&quot;start&quot;:762.8,&quot;text&quot;:&quot; browse it's done browsing it'll issue another command called end browsing and it'll write its&quot;},{&quot;end&quot;:775.68,&quot;start&quot;:769.28,&quot;text&quot;:&quot; answer that's also expressed in tokens but really we rolled this all into one big RL task where&quot;},{&quot;end&quot;:781.28,&quot;start&quot;:775.68,&quot;text&quot;:&quot; your episode involves browsing and writing out the answer and it's all one big RL episode.&quot;},{&quot;end&quot;:785.28,&quot;start&quot;:781.28,&quot;text&quot;:&quot; Did you think this is going to work well or were you kind of surprised? At the very beginning of the&quot;},{&quot;end&quot;:790.72,&quot;start&quot;:785.28,&quot;text&quot;:&quot; project we didn't know if it was going to work or not. Like after we did the initial experiments&quot;},{&quot;end&quot;:797.68,&quot;start&quot;:790.72,&quot;text&quot;:&quot; with Trivia QA which actually didn't take that long to get running then it became pretty clear&quot;},{&quot;end&quot;:802.24,&quot;start&quot;:797.68,&quot;text&quot;:&quot; that it would work that the browsing part worked at least and we already know that we can get&quot;},{&quot;end&quot;:807.8399999999999,&quot;start&quot;:802.24,&quot;text&quot;:&quot; these models to write pretty good long form text with a bunch of if you give them a bunch of&quot;},{&quot;end&quot;:814.16,&quot;start&quot;:807.8399999999999,&quot;text&quot;:&quot; snippets of text that they they can cite. So I noticed the the the human raiders task was&quot;},{&quot;end&quot;:818.88,&quot;start&quot;:814.16,&quot;text&quot;:&quot; quite complicated as it was a long guide and there was many types of feedback that they were giving&quot;},{&quot;end&quot;:823.52,&quot;start&quot;:818.88,&quot;text&quot;:&quot; but in the end the paper said that only the final rating was used so I was just curious if you&quot;},{&quot;end&quot;:827.28,&quot;start&quot;:823.52,&quot;text&quot;:&quot; hadn't commented about that like why do you think maybe the model couldn't use that extra feedback&quot;},{&quot;end&quot;:833.12,&quot;start&quot;:827.28,&quot;text&quot;:&quot; whereas it was maybe just too much or not enough samples. Yeah that's been one frustrating&quot;},{&quot;end&quot;:840.0799999999999,&quot;start&quot;:833.8399999999999,&quot;text&quot;:&quot; finding so far in in that project and also some other projects we've had the same finding but&quot;},{&quot;end&quot;:845.76,&quot;start&quot;:840.08,&quot;text&quot;:&quot; you have your raiders go through this long process for each for each comparison they do where&quot;},{&quot;end&quot;:851.12,&quot;start&quot;:845.76,&quot;text&quot;:&quot; they're comparing a pair of answers and then you only use one bit of information from the whole&quot;},{&quot;end&quot;:855.84,&quot;start&quot;:851.12,&quot;text&quot;:&quot; from this whole process which might have taken like half an hour. It seems like it would be better if&quot;},{&quot;end&quot;:862.08,&quot;start&quot;:855.84,&quot;text&quot;:&quot; we if we were able to extract more information more about the process they went through in arriving&quot;},{&quot;end&quot;:867.0400000000001,&quot;start&quot;:862.08,&quot;text&quot;:&quot; at the answer. So we did collect all sorts of other information like we had them provide ratings&quot;},{&quot;end&quot;:873.4399999999999,&quot;start&quot;:867.04,&quot;text&quot;:&quot; along several different axes like coherence and factual accuracy and so forth but in the end&quot;},{&quot;end&quot;:880.24,&quot;start&quot;:874.3199999999999,&quot;text&quot;:&quot; we didn't really get much of a boost out of using any of this this other information so I'd say&quot;},{&quot;end&quot;:886.56,&quot;start&quot;:881.12,&quot;text&quot;:&quot; it seems like there's it should be possible to do better but unfortunately this methodology which&quot;},{&quot;end&quot;:893.68,&quot;start&quot;:886.56,&quot;text&quot;:&quot; seems kind of dumb so far it's hard to be and people have tried various other ideas for like how&quot;},{&quot;end&quot;:898,&quot;start&quot;:893.68,&quot;text&quot;:&quot; to use human feedback instead of you getting these preference scores there various other things you&quot;},{&quot;end&quot;:903.68,&quot;start&quot;:898,&quot;text&quot;:&quot; can do like you can have them right critiques and edit or maybe edit the responses. Yeah I think&quot;},{&quot;end&quot;:911.12,&quot;start&quot;:903.68,&quot;text&quot;:&quot; some of these things are are also promising but yeah this methodology of collecting preference data&quot;},{&quot;end&quot;:917.04,&quot;start&quot;:911.12,&quot;text&quot;:&quot; works well. Yeah I think it's it's still an open area of research. Oh yeah regarding the really&quot;},{&quot;end&quot;:922.64,&quot;start&quot;:917.04,&quot;text&quot;:&quot; long instructions. Yeah I think for any of these tasks there is a lot of subtlety in how to do the&quot;},{&quot;end&quot;:929.4399999999999,&quot;start&quot;:922.64,&quot;text&quot;:&quot; task properly and so we ended up adding more and more details of like what do you do in this situation&quot;},{&quot;end&quot;:933.76,&quot;start&quot;:929.4399999999999,&quot;text&quot;:&quot; and what do you do in that situation. I think it's starting to get pretty unwieldy with these really&quot;},{&quot;end&quot;:940.8,&quot;start&quot;:933.76,&quot;text&quot;:&quot; long instruction manuals so there's some promising ideas for how to address this like there's a&quot;},{&quot;end&quot;:946.4,&quot;start&quot;:940.8,&quot;text&quot;:&quot; paper from DeepMind recently Sparrow that used basically broke down the task and they trained&quot;},{&quot;end&quot;:952.3199999999999,&quot;start&quot;:947.04,&quot;text&quot;:&quot; they basically had people look at one aspect of the one aspect of the response at a time&quot;},{&quot;end&quot;:957.0400000000001,&quot;start&quot;:952.32,&quot;text&quot;:&quot; and and then they had a way of combining these different rule specific they would train a bunch&quot;},{&quot;end&quot;:961.6800000000001,&quot;start&quot;:957.0400000000001,&quot;text&quot;:&quot; of rule specific reward models and then combine them at the end. Yeah I think there's some other&quot;},{&quot;end&quot;:967.5200000000001,&quot;start&quot;:961.6800000000001,&quot;text&quot;:&quot; interesting ideas for how to how to make this process better. So I gather that from your answer&quot;},{&quot;end&quot;:972.6400000000001,&quot;start&quot;:967.5200000000001,&quot;text&quot;:&quot; about WebGPT and the whole idea of WebGPT is that you want the the language model type access to&quot;},{&quot;end&quot;:978.48,&quot;start&quot;:972.6400000000001,&quot;text&quot;:&quot; external knowledge but I wonder where you think the line should really be in terms of what a&quot;},{&quot;end&quot;:982.88,&quot;start&quot;:978.48,&quot;text&quot;:&quot; language model should know and what the language model should look up and maybe what the language&quot;},{&quot;end&quot;:987.6800000000001,&quot;start&quot;:982.88,&quot;text&quot;:&quot; model should not know or not purport to know. Do you have opinions about that? Yeah let's see&quot;},{&quot;end&quot;:994.16,&quot;start&quot;:988.4,&quot;text&quot;:&quot; like some people are advocating for very small language models that have like no external knowledge&quot;},{&quot;end&quot;:998.5600000000001,&quot;start&quot;:994.16,&quot;text&quot;:&quot; aside from language I guess would be the extreme position and then other people other people&quot;},{&quot;end&quot;:1002.5600000000001,&quot;start&quot;:998.5600000000001,&quot;text&quot;:&quot; talked about language models that just know everything as opposed to having an external knowledge&quot;},{&quot;end&quot;:1008.24,&quot;start&quot;:1002.5600000000001,&quot;text&quot;:&quot; source. There's some interesting questions there so I think it is a little hard to separate knowledge&quot;},{&quot;end&quot;:1015.6,&quot;start&quot;:1008.24,&quot;text&quot;:&quot; factual knowledge from understanding. So as humans we get by like not memorizing all sorts of&quot;},{&quot;end&quot;:1021.6800000000001,&quot;start&quot;:1016.4,&quot;text&quot;:&quot; facts and just knowing that we can look them up if needed. For working on a specific domain it is&quot;},{&quot;end&quot;:1028.88,&quot;start&quot;:1021.6800000000001,&quot;text&quot;:&quot; useful to like have a lot of facts internalized so that you can recall them very quickly and&quot;},{&quot;end&quot;:1034.24,&quot;start&quot;:1028.88,&quot;text&quot;:&quot; kind of combine them combine them in your head. So I wouldn't take an extreme position on either&quot;},{&quot;end&quot;:1041.44,&quot;start&quot;:1034.24,&quot;text&quot;:&quot; side I would say I think retrieval is going to be really useful just at the very least for&quot;},{&quot;end&quot;:1048.88,&quot;start&quot;:1041.44,&quot;text&quot;:&quot; current events but also I don't think we want to try to pack all human knowledge into the weights&quot;},{&quot;end&quot;:1054.72,&quot;start&quot;:1048.88,&quot;text&quot;:&quot; of a neural net. On the other hand I think people have had a lot of luck just scaling up models and&quot;},{&quot;end&quot;:1061.44,&quot;start&quot;:1055.68,&quot;text&quot;:&quot; like as they soak up more factual knowledge they also get better at reasoning and other things&quot;},{&quot;end&quot;:1068,&quot;start&quot;:1061.44,&quot;text&quot;:&quot; and I think I haven't seen any demonstrations of tiny models that just do lots of retrieval&quot;},{&quot;end&quot;:1073.68,&quot;start&quot;:1068,&quot;text&quot;:&quot; and save all their weights for reasoning. Yeah I just haven't seen any evidence of this&quot;},{&quot;end&quot;:1078.8,&quot;start&quot;:1073.68,&quot;text&quot;:&quot; or that or I haven't seen any successful attempts at making this. Let's move on to training&quot;},{&quot;end&quot;:1084.16,&quot;start&quot;:1078.8,&quot;text&quot;:&quot; language models to follow instructions with human feedback that was uyang et al and that was 2022&quot;},{&quot;end&quot;:1088.72,&quot;start&quot;:1084.16,&quot;text&quot;:&quot; with yourself as a co-author. Can you tell us the main idea with this paper? This is the instruct&quot;},{&quot;end&quot;:1094.64,&quot;start&quot;:1088.72,&quot;text&quot;:&quot; GPT paper. What does instruct GPT and what's going on here? Instruct GPT is a language model that's&quot;},{&quot;end&quot;:1099.44,&quot;start&quot;:1094.64,&quot;text&quot;:&quot; fine tuned to follow instructions and it's in fact the one that you can play with if you go to&quot;},{&quot;end&quot;:1105.84,&quot;start&quot;:1100.08,&quot;text&quot;:&quot; the open AI website you get a big text box and you can write some text and then press the button&quot;},{&quot;end&quot;:1112.24,&quot;start&quot;:1105.84,&quot;text&quot;:&quot; to generate a completion. So the idea here was I mean language models are pretty useful and you can&quot;},{&quot;end&quot;:1117.84,&quot;start&quot;:1112.96,&quot;text&quot;:&quot; sometimes get them to do what you want by prompting them just right. This idea of few shot&quot;},{&quot;end&quot;:1123.52,&quot;start&quot;:1117.84,&quot;text&quot;:&quot; prompting has been become pretty popular where you give a few examples like a few question answer&quot;},{&quot;end&quot;:1128.3999999999999,&quot;start&quot;:1123.52,&quot;text&quot;:&quot; examples and then if you ask another question it'll hopefully provide an answer in the same style.&quot;},{&quot;end&quot;:1133.84,&quot;start&quot;:1128.3999999999999,&quot;text&quot;:&quot; So the idea yeah so if you can get language models to do great things with prompting but prompting&quot;},{&quot;end&quot;:1139.04,&quot;start&quot;:1133.84,&quot;text&quot;:&quot; is itself an arg and it's tricky to get right and it's also kind of not necessarily getting the&quot;},{&quot;end&quot;:1143.6799999999998,&quot;start&quot;:1139.04,&quot;text&quot;:&quot; best possible performance out of the model. If you just take a raw language model and you try to&quot;},{&quot;end&quot;:1148.4,&quot;start&quot;:1143.68,&quot;text&quot;:&quot; you try to talk to it like you ask it a question it probably it doesn't know that it should actually&quot;},{&quot;end&quot;:1154.0800000000002,&quot;start&quot;:1148.4,&quot;text&quot;:&quot; answer that question as well as possible. For all it knows you want it to give a joke answer or&quot;},{&quot;end&quot;:1160,&quot;start&quot;:1154.0800000000002,&quot;text&quot;:&quot; a riddle or something. Yeah so the idea of instruct GPT was let's make a kind of small change&quot;},{&quot;end&quot;:1164.4,&quot;start&quot;:1160,&quot;text&quot;:&quot; for our language models so that they're much easier to use. In particular we're going to train them&quot;},{&quot;end&quot;:1171.3600000000001,&quot;start&quot;:1164.4,&quot;text&quot;:&quot; to if you have a piece of text where there's an instruction the model will try to follow that&quot;},{&quot;end&quot;:1176.6399999999999,&quot;start&quot;:1171.36,&quot;text&quot;:&quot; instruction to the best of its abilities and pretty much anything can be an instruction like&quot;},{&quot;end&quot;:1183.04,&quot;start&quot;:1176.6399999999999,&quot;text&quot;:&quot; you can have a the instruction can be to continue a chat or it can be to like summarize like&quot;},{&quot;end&quot;:1190.8,&quot;start&quot;:1183.04,&quot;text&quot;:&quot; summarize this text or give me a list of names for my company that sells widgets. Yeah instructions&quot;},{&quot;end&quot;:1195.84,&quot;start&quot;:1190.8,&quot;text&quot;:&quot; can be anything and that makes that makes this kind of model very powerful. So that was kind of&quot;},{&quot;end&quot;:1199.76,&quot;start&quot;:1195.84,&quot;text&quot;:&quot; that's the idea of an instruction following model it's like a model that can do anything that&quot;},{&quot;end&quot;:1204.16,&quot;start&quot;:1199.76,&quot;text&quot;:&quot; you specify with an instruction and by the way I wasn't a core contributor to this work I was&quot;},{&quot;end&quot;:1212.16,&quot;start&quot;:1204.8,&quot;text&quot;:&quot; more involved with like getting the RL infrastructure and some of the RL training details&quot;},{&quot;end&quot;:1218.24,&quot;start&quot;:1212.16,&quot;text&quot;:&quot; like helping out with that that stuff. But anyway yeah what we did in this project was we ran this&quot;},{&quot;end&quot;:1224,&quot;start&quot;:1218.24,&quot;text&quot;:&quot; this whole methodology that I just described of RL from even preferences in this instruction&quot;},{&quot;end&quot;:1230.32,&quot;start&quot;:1224,&quot;text&quot;:&quot; following setting. So we did supervised fine tuning, collected preference data, trained a reward&quot;},{&quot;end&quot;:1236.72,&quot;start&quot;:1230.32,&quot;text&quot;:&quot; model and then did RL against that reward model and one interesting detail is actually whereas the&quot;},{&quot;end&quot;:1244.88,&quot;start&quot;:1236.72,&quot;text&quot;:&quot; original initial data was just collected using contractors. At a certain point we had the the API&quot;},{&quot;end&quot;:1252,&quot;start&quot;:1244.88,&quot;text&quot;:&quot; and it's got this I mean we have this playground on the website where this is where you the big&quot;},{&quot;end&quot;:1258.24,&quot;start&quot;:1252,&quot;text&quot;:&quot; text box where you can use the model. So we we took prompts that people that users had put into&quot;},{&quot;end&quot;:1264.56,&quot;start&quot;:1258.24,&quot;text&quot;:&quot; the into the playground and use those for training like both to collect preference data and to do RL.&quot;},{&quot;end&quot;:1271.92,&quot;start&quot;:1264.56,&quot;text&quot;:&quot; So and this is like this is disclosed to users pretty prominently like when when people are using&quot;},{&quot;end&quot;:1276.88,&quot;start&quot;:1271.92,&quot;text&quot;:&quot; the playgrounds you get notified that your prompts might be used for the training and we're also&quot;},{&quot;end&quot;:1282.88,&quot;start&quot;:1276.88,&quot;text&quot;:&quot; careful to train in such a way that we don't memorize any information that was in in the prompts.&quot;},{&quot;end&quot;:1288.5600000000002,&quot;start&quot;:1282.88,&quot;text&quot;:&quot; Like it and it explicit like we have a pretty like elaborate process for making sure there's no&quot;},{&quot;end&quot;:1295.0400000000002,&quot;start&quot;:1289.2800000000002,&quot;text&quot;:&quot; like private information being leaked into the model. But anyway yeah that's that's basically the&quot;},{&quot;end&quot;:1302.16,&quot;start&quot;:1295.7600000000002,&quot;text&quot;:&quot; experimental setup and the result was that it works like this methodology works quite well and you&quot;},{&quot;end&quot;:1308.64,&quot;start&quot;:1302.16,&quot;text&quot;:&quot; get a model that's vastly preferred to the base model on this distribution of of realistic prompts&quot;},{&quot;end&quot;:1314.4,&quot;start&quot;:1308.64,&quot;text&quot;:&quot; that people are giving the model often which contain instructions. So the raw like the the raw&quot;},{&quot;end&quot;:1321.68,&quot;start&quot;:1314.4,&quot;text&quot;:&quot; language models generally do a really bad job following instructions but this RL trained instruction&quot;},{&quot;end&quot;:1328.0800000000002,&quot;start&quot;:1321.68,&quot;text&quot;:&quot; following model is is a lot better and it's something like if you just calculate how much better&quot;},{&quot;end&quot;:1333.6,&quot;start&quot;:1328.08,&quot;text&quot;:&quot; it's something like it's as good as a model that's a hundred times bigger. That's a lot. Yeah.&quot;},{&quot;end&quot;:1337.36,&quot;start&quot;:1333.6,&quot;text&quot;:&quot; You wanted the model to be truthful is that is that one of the criteria you wanted?&quot;},{&quot;end&quot;:1342.1599999999999,&quot;start&quot;:1337.36,&quot;text&quot;:&quot; Oh yeah truthfulness was one of the criteria. That seems amazing to me that truthfulness is&quot;},{&quot;end&quot;:1346.32,&quot;start&quot;:1342.1599999999999,&quot;text&quot;:&quot; something that I could learn by example like does that mean that truthfulness is somehow&quot;},{&quot;end&quot;:1351.04,&quot;start&quot;:1346.32,&quot;text&quot;:&quot; represented inside the network or because there's no external way for the model to confirm&quot;},{&quot;end&quot;:1355.76,&quot;start&quot;:1351.04,&quot;text&quot;:&quot; whether something is true or false. So how how might it know what is what is true without any&quot;},{&quot;end&quot;:1362.24,&quot;start&quot;:1355.76,&quot;text&quot;:&quot; external reference? I think to some extent there is some internal representation of truthfulness.&quot;},{&quot;end&quot;:1367.12,&quot;start&quot;:1362.24,&quot;text&quot;:&quot; So I would say like one way to think about what language models do is they're trained to imitate&quot;},{&quot;end&quot;:1371.52,&quot;start&quot;:1367.12,&quot;text&quot;:&quot; the whole internet and the internet is written by lots of different people and has lots of different&quot;},{&quot;end&quot;:1379.04,&quot;start&quot;:1371.52,&quot;text&quot;:&quot; types of content from fiction to nonfiction to like like technical like detailed technical literature&quot;},{&quot;end&quot;:1386.3999999999999,&quot;start&quot;:1379.04,&quot;text&quot;:&quot; to like jokes and like forum posts whatever. So what the model is basically an ensemble of all&quot;},{&quot;end&quot;:1392.8799999999999,&quot;start&quot;:1386.3999999999999,&quot;text&quot;:&quot; these people who wrote stuff on the internet the raw pre-trained model. When you feed it a prompt&quot;},{&quot;end&quot;:1398.08,&quot;start&quot;:1392.8799999999999,&quot;text&quot;:&quot; what it's doing internally has to be something like figuring out who wrote the first wrote this prompt&quot;},{&quot;end&quot;:1403.04,&quot;start&quot;:1398.08,&quot;text&quot;:&quot; and then trying to continue in that style. So if it thinks it's reading just reading something on the&quot;},{&quot;end&quot;:1409.36,&quot;start&quot;:1403.04,&quot;text&quot;:&quot; Wall Street Betts Reddit it's going to continue on that style but if it thinks it's in the New&quot;},{&quot;end&quot;:1417.6,&quot;start&quot;:1409.36,&quot;text&quot;:&quot; York Times it's going to write in a very different way. So effectively the model must be like calculating&quot;},{&quot;end&quot;:1423.92,&quot;start&quot;:1417.6,&quot;text&quot;:&quot; somewhere like what style is this or what ensemble what's the like narrower ensemble of styles that&quot;},{&quot;end&quot;:1429.76,&quot;start&quot;:1423.92,&quot;text&quot;:&quot; I'm trying to imitate now. At the very least when you do some kind of when you do training like either&quot;},{&quot;end&quot;:1435.12,&quot;start&quot;:1429.76,&quot;text&quot;:&quot; supervised fine tuning or are all from human feedback you can at least like narrow down the set of&quot;},{&quot;end&quot;:1442.56,&quot;start&quot;:1435.12,&quot;text&quot;:&quot; styles the model is producing and try to imitate like the best or the best person in the training set&quot;},{&quot;end&quot;:1448,&quot;start&quot;:1442.56,&quot;text&quot;:&quot; or the best style in the training set and obviously best will differ a lot. So what we'll end up with&quot;},{&quot;end&quot;:1453.68,&quot;start&quot;:1448,&quot;text&quot;:&quot; will depend on our instructions. So if we if we tell I don't know we'll end up with something that&quot;},{&quot;end&quot;:1462.16,&quot;start&quot;:1453.68,&quot;text&quot;:&quot; has kind of safe like not too not too controversial but a bit corporate will end up with something&quot;},{&quot;end&quot;:1468.8,&quot;start&quot;:1462.16,&quot;text&quot;:&quot; like that depending on what our instructions are. So at the very least like we can kind of narrow&quot;},{&quot;end&quot;:1474,&quot;start&quot;:1468.8,&quot;text&quot;:&quot; in on one style instead of having the whole distribution of styles on the internet. I think probably&quot;},{&quot;end&quot;:1479.52,&quot;start&quot;:1474,&quot;text&quot;:&quot; there's more to it than that like we're not just learning about style but the model probably is&quot;},{&quot;end&quot;:1485.04,&quot;start&quot;:1479.52,&quot;text&quot;:&quot; like internally trying to determine if things are if statements are true or not like if the prompt&quot;},{&quot;end&quot;:1490.6399999999999,&quot;start&quot;:1485.04,&quot;text&quot;:&quot; contains incorrect information because that probably would be useful for determining a likely&quot;},{&quot;end&quot;:1495.6,&quot;start&quot;:1490.6399999999999,&quot;text&quot;:&quot; completion. I'm just talking about the raw pre-trained model so I think yeah I think just the&quot;},{&quot;end&quot;:1501.52,&quot;start&quot;:1495.6,&quot;text&quot;:&quot; objective of predicting next tokens probably gives you a lot it forces the model to like the&quot;},{&quot;end&quot;:1506.8799999999999,&quot;start&quot;:1501.52,&quot;text&quot;:&quot; determine if things are true or not. I think for our alfine tuning there's a lot more potential&quot;},{&quot;end&quot;:1513.1200000000001,&quot;start&quot;:1506.88,&quot;text&quot;:&quot; for the model to actually like try to output something truthful as opposed to trying to imitate&quot;},{&quot;end&quot;:1519.1200000000001,&quot;start&quot;:1513.1200000000001,&quot;text&quot;:&quot; a certain style though it's hard to I guess it would be hard to like determine if that's what the&quot;},{&quot;end&quot;:1524.4,&quot;start&quot;:1519.1200000000001,&quot;text&quot;:&quot; model is actually trying to do. So it's almost like the the prompt is guiding the model it's like&quot;},{&quot;end&quot;:1529.5200000000002,&quot;start&quot;:1524.4,&quot;text&quot;:&quot; what corner of the internet do we want to do we want to imitate here and maybe we want to&quot;},{&quot;end&quot;:1534.0800000000002,&quot;start&quot;:1529.5200000000002,&quot;text&quot;:&quot; instruct GPG wants to to focus more on the most more truthful corners of the internet&quot;},{&quot;end&quot;:1539.1999999999998,&quot;start&quot;:1534.08,&quot;text&quot;:&quot; something similar to that. Yeah I would hope so at least I think that's a pretty good though maybe&quot;},{&quot;end&quot;:1543.1999999999998,&quot;start&quot;:1539.1999999999998,&quot;text&quot;:&quot; a little simplistic picture of what's going on. At the very least we should be able to imitate&quot;},{&quot;end&quot;:1549.36,&quot;start&quot;:1543.1999999999998,&quot;text&quot;:&quot; the most truthful corner of the internet. So can you talk about a generalization and how does&quot;},{&quot;end&quot;:1554.56,&quot;start&quot;:1549.36,&quot;text&quot;:&quot; this type of model perform out of distribution? Like I guess if it seems questions that are a bit&quot;},{&quot;end&quot;:1558.3999999999999,&quot;start&quot;:1554.56,&quot;text&quot;:&quot; different than what it was trained on. What happens if we get a little bit away from the training&quot;},{&quot;end&quot;:1563.84,&quot;start&quot;:1558.3999999999999,&quot;text&quot;:&quot; data with the reward models? I mean language models in general generalize surprisingly well and&quot;},{&quot;end&quot;:1568.8,&quot;start&quot;:1563.84,&quot;text&quot;:&quot; I would say overall like these pre-trained models that are trained on super diverse data sets&quot;},{&quot;end&quot;:1573.84,&quot;start&quot;:1568.8,&quot;text&quot;:&quot; from the internet. They tend to generalize quite well or surprisingly well at least it's surprising&quot;},{&quot;end&quot;:1580.72,&quot;start&quot;:1573.84,&quot;text&quot;:&quot; to those of us who were around for the earlier days of machine learning when everything was&quot;},{&quot;end&quot;:1586.08,&quot;start&quot;:1580.72,&quot;text&quot;:&quot; trained from scratch and very fragile. For example if you ask if you provide an instruction in some&quot;},{&quot;end&quot;:1591.28,&quot;start&quot;:1586.08,&quot;text&quot;:&quot; other language even a even a fairly rare language it'll often do a decent job following the&quot;},{&quot;end&quot;:1597.36,&quot;start&quot;:1591.28,&quot;text&quot;:&quot; instruction even if there's zero data in the whole instruction following the training process&quot;},{&quot;end&quot;:1603.84,&quot;start&quot;:1597.92,&quot;text&quot;:&quot; that's in that language and that's just to carry over from the pre-training. So I think generalization&quot;},{&quot;end&quot;:1608.16,&quot;start&quot;:1603.84,&quot;text&quot;:&quot; yeah I think language models generalize quite well. So you asked about reward models I think one&quot;},{&quot;end&quot;:1614.08,&quot;start&quot;:1608.16,&quot;text&quot;:&quot; of the tricky pieces about RL from human feedback is how so you have this reward model and you're&quot;},{&quot;end&quot;:1618.6399999999999,&quot;start&quot;:1614.08,&quot;text&quot;:&quot; actually training against it meaning you're training your policy to have high reward and it's&quot;},{&quot;end&quot;:1623.6000000000001,&quot;start&quot;:1618.64,&quot;text&quot;:&quot; going to exploit the errors in the reward model so it's going to eventually find adversarial&quot;},{&quot;end&quot;:1628.72,&quot;start&quot;:1623.6000000000001,&quot;text&quot;:&quot; examples to the reward model. This is worse than kind of normal out of distribution behavior it's&quot;},{&quot;end&quot;:1634.0800000000002,&quot;start&quot;:1628.72,&quot;text&quot;:&quot; like targeted out of distribution examples so so there are definitely some challenges around&quot;},{&quot;end&quot;:1640.8000000000002,&quot;start&quot;:1634.8000000000002,&quot;text&quot;:&quot; getting reward models to generalize well or generalize as far as possible from the training set.&quot;},{&quot;end&quot;:1645.6000000000001,&quot;start&quot;:1640.8000000000002,&quot;text&quot;:&quot; Can these types of agents tell us when they don't know something or is that a hard problem?&quot;},{&quot;end&quot;:1651.9199999999998,&quot;start&quot;:1645.6,&quot;text&quot;:&quot; I'd say sort of if you ask a question that's kind of in the core of the model's knowledge it will&quot;},{&quot;end&quot;:1656,&quot;start&quot;:1651.9199999999998,&quot;text&quot;:&quot; know know the answer and it'll know that it knows. By the way I'm talking about models like the&quot;},{&quot;end&quot;:1661.6,&quot;start&quot;:1656,&quot;text&quot;:&quot; for the instruct model if you ask it about something that's like very simple at the core of its&quot;},{&quot;end&quot;:1666.08,&quot;start&quot;:1661.6,&quot;text&quot;:&quot; knowledge it'll know if you there are certain things that it knows that it doesn't know like&quot;},{&quot;end&quot;:1672.8799999999999,&quot;start&quot;:1666.7199999999998,&quot;text&quot;:&quot; current events where it's been trained to know that it doesn't know certain things in real time but&quot;},{&quot;end&quot;:1678.16,&quot;start&quot;:1672.88,&quot;text&quot;:&quot; if you ask it about something that's kind of on the edge of its knowledge it's it's going to have a&quot;},{&quot;end&quot;:1682.96,&quot;start&quot;:1678.16,&quot;text&quot;:&quot; hard time it's it's necessarily going to be inaccurate. I mean there have been a couple papers&quot;},{&quot;end&quot;:1689.2800000000002,&quot;start&quot;:1683.7600000000002,&quot;text&quot;:&quot; about this question so there is in paper from Anthropic recently called language models&quot;},{&quot;end&quot;:1695.2800000000002,&quot;start&quot;:1689.2800000000002,&quot;text&quot;:&quot; mostly know what they know and there is also a paper from FHI and OpenAI called&quot;},{&quot;end&quot;:1700.48,&quot;start&quot;:1696.5600000000002,&quot;text&quot;:&quot; getting language models to express their uncertainty and words. These language&quot;},{&quot;end&quot;:1706.32,&quot;start&quot;:1700.48,&quot;text&quot;:&quot; models as well as a lot of other models in machine learning are training to maximize likelihood&quot;},{&quot;end&quot;:1711.6,&quot;start&quot;:1706.32,&quot;text&quot;:&quot; so maximize log-prob of data. You're already training them to always predict a distribution of&quot;},{&quot;end&quot;:1718.8,&quot;start&quot;:1711.6,&quot;text&quot;:&quot; outputs. So for language models given a prefix it's predicting a distribution over the next token.&quot;},{&quot;end&quot;:1725.76,&quot;start&quot;:1718.8,&quot;text&quot;:&quot; These predictions for the next token like generally are pretty well calibrated but 80% if it puts 80%&quot;},{&quot;end&quot;:1731.6,&quot;start&quot;:1725.76,&quot;text&quot;:&quot; probability on something and you look at all the times when it puts 80% probability on something&quot;},{&quot;end&quot;:1736.72,&quot;start&quot;:1731.6,&quot;text&quot;:&quot; like it's right 80% of the time. Like that's just a result of the training objective. The training&quot;},{&quot;end&quot;:1742.96,&quot;start&quot;:1736.72,&quot;text&quot;:&quot; objective like strongly incentivizes the model to be calibrated meaning it has a reasonable&quot;},{&quot;end&quot;:1748.8,&quot;start&quot;:1742.96,&quot;text&quot;:&quot; estimate of its uncertainty. So at the single token level models definitely are calibrated.&quot;},{&quot;end&quot;:1754.8,&quot;start&quot;:1748.8,&quot;text&quot;:&quot; The question is whether they're calibrated on whether this calibration extends to settings where&quot;},{&quot;end&quot;:1760.6399999999999,&quot;start&quot;:1754.8,&quot;text&quot;:&quot; they are generating multi-token outputs or whether they can judge the correctness of some&quot;},{&quot;end&quot;:1766,&quot;start&quot;:1760.6399999999999,&quot;text&quot;:&quot; multi-token statement. So I would say since models are calibrated at the single token level&quot;},{&quot;end&quot;:1772.6399999999999,&quot;start&quot;:1766.56,&quot;text&quot;:&quot; they I think they definitely have the information to be calibrated in these other settings.&quot;},{&quot;end&quot;:1778.48,&quot;start&quot;:1772.6399999999999,&quot;text&quot;:&quot; So that's why I think the problem of models knowing what they know isn't actually that hard&quot;},{&quot;end&quot;:1783.9199999999998,&quot;start&quot;:1778.48,&quot;text&quot;:&quot; or at least getting a model to express its uncertainty pretty much as well as a human does&quot;},{&quot;end&quot;:1788.88,&quot;start&quot;:1783.92,&quot;text&quot;:&quot; doesn't feel like an insurmountable problem but there's some practical difficulties to getting&quot;},{&quot;end&quot;:1793.92,&quot;start&quot;:1788.88,&quot;text&quot;:&quot; getting there. People use the phrase AI alignment in different ways. Can you talk about how you see&quot;},{&quot;end&quot;:1800.0800000000002,&quot;start&quot;:1793.92,&quot;text&quot;:&quot; alignment in your work on Aral from human feedback? I think of alignment mostly as the problem of&quot;},{&quot;end&quot;:1805.28,&quot;start&quot;:1800.0800000000002,&quot;text&quot;:&quot; getting the model to try to do the right thing so we can kind of make a distinction between&quot;},{&quot;end&quot;:1811.28,&quot;start&quot;:1805.92,&quot;text&quot;:&quot; what the model is capable of doing. Like if you just take a raw language model and you ask&quot;},{&quot;end&quot;:1815.76,&quot;start&quot;:1811.28,&quot;text&quot;:&quot; it a question like I said before it doesn't know that you actually wanted to give the correct answer&quot;},{&quot;end&quot;:1821.68,&quot;start&quot;:1815.76,&quot;text&quot;:&quot; as opposed to. It might think someone who is not very knowledgeable is answering. By doing some&quot;},{&quot;end&quot;:1826,&quot;start&quot;:1821.68,&quot;text&quot;:&quot; extra training we can get the model to actually try to do the right thing and so I would say that&quot;},{&quot;end&quot;:1832.16,&quot;start&quot;:1826,&quot;text&quot;:&quot; that's the main goal of alignment. So there was an open AI blog post recently that talked about&quot;},{&quot;end&quot;:1840.24,&quot;start&quot;:1832.16,&quot;text&quot;:&quot; the sequence in alignment. One was training AI systems using human feedback to use it training AI&quot;},{&quot;end&quot;:1846,&quot;start&quot;:1840.24,&quot;text&quot;:&quot; systems to assist human evaluation and three training AI systems to do alignment research.&quot;},{&quot;end&quot;:1852,&quot;start&quot;:1846,&quot;text&quot;:&quot; So is your current work mostly about this first item and when and how do you see us getting to&quot;},{&quot;end&quot;:1858.4,&quot;start&quot;:1852,&quot;text&quot;:&quot; these other stages? I'm doing some work now on number two training AI systems to assist human feedback.&quot;},{&quot;end&quot;:1865.04,&quot;start&quot;:1858.4,&quot;text&quot;:&quot; I think that's sort of becomes increasingly necessary as you start trying to get the systems&quot;},{&quot;end&quot;:1869.44,&quot;start&quot;:1865.04,&quot;text&quot;:&quot; to solve harder and harder problems. When you have models that are kind of very below human level&quot;},{&quot;end&quot;:1875.6000000000001,&quot;start&quot;:1869.44,&quot;text&quot;:&quot; or maybe at human level at a certain task it's pretty straightforward to supervise them. But&quot;},{&quot;end&quot;:1880.4,&quot;start&quot;:1875.6000000000001,&quot;text&quot;:&quot; once they're doing things that are very hard or doing things that require a lot of diverse&quot;},{&quot;end&quot;:1886.96,&quot;start&quot;:1880.4,&quot;text&quot;:&quot; technical knowledge it becomes pretty hard to provide a useful supervision signal. So we have to&quot;},{&quot;end&quot;:1893.3600000000001,&quot;start&quot;:1886.96,&quot;text&quot;:&quot; start doing things like one model writes an answer to do a question and then another model provides&quot;},{&quot;end&quot;:1900.9599999999998,&quot;start&quot;:1893.36,&quot;text&quot;:&quot; a critique of that answer points out some flaws and then the human only has to judge the first answer&quot;},{&quot;end&quot;:1906.7199999999998,&quot;start&quot;:1900.9599999999998,&quot;text&quot;:&quot; after looking at the critique meaning basically the critique helps the human assess the answer. So I&quot;},{&quot;end&quot;:1912.1599999999999,&quot;start&quot;:1906.7199999999998,&quot;text&quot;:&quot; think like that kind of idea is starting to become pretty relevant. A colleague's an I are exploring&quot;},{&quot;end&quot;:1917.28,&quot;start&quot;:1912.1599999999999,&quot;text&quot;:&quot; that kind of idea now. As for assisting alignment research there's some other work at open AI that's&quot;},{&quot;end&quot;:1923.1999999999998,&quot;start&quot;:1917.28,&quot;text&quot;:&quot; starting to explore this. It's also that sort of the for this down the road. So I saw Stuart Russell&quot;},{&quot;end&quot;:1928.96,&quot;start&quot;:1923.2,&quot;text&quot;:&quot; was on your PhD committee and I really enjoyed his book Human Compatible. I wonder if you share&quot;},{&quot;end&quot;:1933.04,&quot;start&quot;:1928.96,&quot;text&quot;:&quot; the idea mentioned in the book that the standard RL framing with this fixed reward signal&quot;},{&quot;end&quot;:1939.8400000000001,&quot;start&quot;:1933.8400000000001,&quot;text&quot;:&quot; is problematic and that agents powerful agents should try to do what we want and maintain some&quot;},{&quot;end&quot;:1945.68,&quot;start&quot;:1939.8400000000001,&quot;text&quot;:&quot; uncertainty about what it is we want and the agents that are too certain will be problematic.&quot;},{&quot;end&quot;:1952.56,&quot;start&quot;:1945.68,&quot;text&quot;:&quot; What do you have any thoughts on that idea? I totally agree with that idea. So I think first it's&quot;},{&quot;end&quot;:1959.36,&quot;start&quot;:1952.56,&quot;text&quot;:&quot; really hard to write down a simple reward function that actually captures what we want or what any&quot;},{&quot;end&quot;:1964.96,&quot;start&quot;:1959.36,&quot;text&quot;:&quot; any particular person wants. I can say I want a little more of this or a little more of that but&quot;},{&quot;end&quot;:1971.2,&quot;start&quot;:1965.6,&quot;text&quot;:&quot; you wouldn't want to take that to the extreme. If we build agents that try to cater to our to our&quot;},{&quot;end&quot;:1978.6399999999999,&quot;start&quot;:1971.2,&quot;text&quot;:&quot; wishes we should make sure they're like they have a lot of they have uncertainty about what we&quot;},{&quot;end&quot;:1984.64,&quot;start&quot;:1978.64,&quot;text&quot;:&quot; want or what we value and that that'll also cause them to be a little more cautious and say&quot;},{&quot;end&quot;:1991.92,&quot;start&quot;:1984.64,&quot;text&quot;:&quot; not disturb anything that might be important to us. So yeah I agree with that like Stuart Russell&quot;},{&quot;end&quot;:1998.3200000000002,&quot;start&quot;:1991.92,&quot;text&quot;:&quot; gave a very good like problem definition of what we want AI to do like we want it to basically&quot;},{&quot;end&quot;:2003.6000000000001,&quot;start&quot;:1998.3200000000002,&quot;text&quot;:&quot; we want to jointly like play this game where AI is the AI is trying to figure out what we want&quot;},{&quot;end&quot;:2008.4,&quot;start&quot;:2003.6000000000001,&quot;text&quot;:&quot; and then trying to do that but simultaneously maintaining some uncertainty about what we want.&quot;},{&quot;end&quot;:2013.0400000000002,&quot;start&quot;:2008.4,&quot;text&quot;:&quot; I would say if you you start to look at how to get that in practice it actually looks quite a bit&quot;},{&quot;end&quot;:2019.76,&quot;start&quot;:2013.0400000000002,&quot;text&quot;:&quot; like the kind of RL from human feedback that we're working on at OpenAI and others are working on&quot;},{&quot;end&quot;:2027.8400000000001,&quot;start&quot;:2019.76,&quot;text&quot;:&quot; other places. I think yeah I think I see what we're doing as a practical implementation of getting&quot;},{&quot;end&quot;:2033.2,&quot;start&quot;:2027.8400000000001,&quot;text&quot;:&quot; towards this behavior that Russell have described. Do you think of a AGI as an abstract goal or&quot;},{&quot;end&quot;:2037.44,&quot;start&quot;:2033.2,&quot;text&quot;:&quot; are we going to see a model come out one day and people are going to say oh that's the first AGI&quot;},{&quot;end&quot;:2044,&quot;start&quot;:2037.44,&quot;text&quot;:&quot; model like what does it have to do for people to say that? I think people will say that many times&quot;},{&quot;end&quot;:2049.52,&quot;start&quot;:2044.72,&quot;text&quot;:&quot; then realize that it doesn't quite do everything that you want. I think we're going to have a lot of&quot;},{&quot;end&quot;:2055.44,&quot;start&quot;:2049.52,&quot;text&quot;:&quot; like a long series of models that are that are superhuman at most things or at a certain class of&quot;},{&quot;end&quot;:2064.08,&quot;start&quot;:2055.44,&quot;text&quot;:&quot; things but they also have some failure modes and weaknesses. Like I expect us to like see multiple&quot;},{&quot;end&quot;:2070.96,&quot;start&quot;:2064.08,&quot;text&quot;:&quot; models that are proclaimed as AGI and then only after interacting with it a while you do realize&quot;},{&quot;end&quot;:2078,&quot;start&quot;:2070.96,&quot;text&quot;:&quot; it's not quite there. What would you say is the relationship between AGI and RL and AGI and&quot;},{&quot;end&quot;:2084.96,&quot;start&quot;:2078,&quot;text&quot;:&quot; these large language models? How do those concepts fit together? I would say that RL is a useful&quot;},{&quot;end&quot;:2090.96,&quot;start&quot;:2084.96,&quot;text&quot;:&quot; like component of training AGI or an almost essential component. The thing RL lets you do is it&quot;},{&quot;end&quot;:2098.48,&quot;start&quot;:2090.96,&quot;text&quot;:&quot; lets you optimize any objective for the agents. Any objective that is a function of the agents&quot;},{&quot;end&quot;:2105.04,&quot;start&quot;:2098.48,&quot;text&quot;:&quot; behavior. So with pre-training like what we do for language models you're kind of choosing an&quot;},{&quot;end&quot;:2111.04,&quot;start&quot;:2105.04,&quot;text&quot;:&quot; objective that lets us do something with all the training day we have which is all this internet&quot;},{&quot;end&quot;:2116.4,&quot;start&quot;:2111.04,&quot;text&quot;:&quot; text. So we choose this maximum likelihood objective which is basically the only or not the&quot;},{&quot;end&quot;:2122.4,&quot;start&quot;:2116.4,&quot;text&quot;:&quot; only thing but it's like a sensible way to absorb all this knowledge. But then if we really want to&quot;},{&quot;end&quot;:2128.32,&quot;start&quot;:2122.4,&quot;text&quot;:&quot; optimize the agents behavior for a specific objective RL is kind of the only framework that lets you&quot;},{&quot;end&quot;:2133.12,&quot;start&quot;:2128.32,&quot;text&quot;:&quot; do that. Okay John we have a few questions from the audience and I'm just going to pick the two&quot;},{&quot;end&quot;:2139.36,&quot;start&quot;:2133.12,&quot;text&quot;:&quot; that have the highest score in terms of Twitter likes. So the first is from Eric Chang VP of AI&quot;},{&quot;end&quot;:2144.48,&quot;start&quot;:2139.36,&quot;text&quot;:&quot; at a Hello Di Robotics. He asked RL distributions are non-stationary making it hard to reason about&quot;},{&quot;end&quot;:2149.92,&quot;start&quot;:2144.48,&quot;text&quot;:&quot; PPO losses and how that relates to return or generalization. Are there any intermediate plots&quot;},{&quot;end&quot;:2156,&quot;start&quot;:2149.92,&quot;text&quot;:&quot; and visualizations you like to generate to debug or incrementally build up a large scale RL system?&quot;},{&quot;end&quot;:2163.2,&quot;start&quot;:2156,&quot;text&quot;:&quot; Yeah there are definitely some stats that I look at so I will be I'll talk about this in the nuts&quot;},{&quot;end&quot;:2172.2400000000002,&quot;start&quot;:2163.2,&quot;text&quot;:&quot; and bolts like reboot waited a year but I'd say things like you're looking at the explained&quot;},{&quot;end&quot;:2177.3599999999997,&quot;start&quot;:2172.24,&quot;text&quot;:&quot; variants of the value function and looking at the like how many samples are getting clipped in&quot;},{&quot;end&quot;:2183.3599999999997,&quot;start&quot;:2177.3599999999997,&quot;text&quot;:&quot; PPO and what the KL between the what what the KL divergence is between the policy before and after&quot;},{&quot;end&quot;:2191.3599999999997,&quot;start&quot;:2183.3599999999997,&quot;text&quot;:&quot; the update is yeah things like that. And then Ethan the calibar from Miele asks what is your median&quot;},{&quot;end&quot;:2198,&quot;start&quot;:2191.3599999999997,&quot;text&quot;:&quot; estimate for the arrival date of AGI? I think not too far away but I like I said I expect there to&quot;},{&quot;end&quot;:2205.12,&quot;start&quot;:2198,&quot;text&quot;:&quot; be a lot of fall starts I would say expect like like AI to be able to do better a better job than&quot;},{&quot;end&quot;:2211.36,&quot;start&quot;:2205.12,&quot;text&quot;:&quot; humans at most jobs that humans do now five years or so that's not all jobs but most jobs for a while&quot;},{&quot;end&quot;:2215.84,&quot;start&quot;:2211.36,&quot;text&quot;:&quot; we're going to discover things that AI isn't very good at and then where we want to keep humans in&quot;},{&quot;end&quot;:2221.12,&quot;start&quot;:2215.84,&quot;text&quot;:&quot; control so I think there'll be some kind of gradual process over the next 10 or 15 years.&quot;},{&quot;end&quot;:2227.2,&quot;start&quot;:2221.12,&quot;text&quot;:&quot; I've been curious about this I see that some RL work is patented but I could not find a TRPO or&quot;},{&quot;end&quot;:2234.3999999999996,&quot;start&quot;:2227.2,&quot;text&quot;:&quot; PPO in I could not find patents on these are those protected patent protected at all or how do you&quot;},{&quot;end&quot;:2240.56,&quot;start&quot;:2234.3999999999996,&quot;text&quot;:&quot; how do you think of intellectual property protection for that kind of work? I haven't ever looked&quot;},{&quot;end&quot;:2246.16,&quot;start&quot;:2240.56,&quot;text&quot;:&quot; looked into patenting anything and open AI hasn't either as far as I know I think the trend over time&quot;},{&quot;end&quot;:2251.12,&quot;start&quot;:2246.16,&quot;text&quot;:&quot; has been for people to take a patent scene machine like a machine learning algorithms last&quot;},{&quot;end&quot;:2256,&quot;start&quot;:2251.12,&quot;text&quot;:&quot; seriously there is this algorithm in computer vision called sift which is like this key point&quot;},{&quot;end&quot;:2262.88,&quot;start&quot;:2256,&quot;text&quot;:&quot; to detector and this was patented I think the the guy who patented it he probably made his&quot;},{&quot;end&quot;:2268.88,&quot;start&quot;:2262.88,&quot;text&quot;:&quot; university some money from the patent but in the end all it did was cause people a lot of annoyance&quot;},{&quot;end&quot;:2275.28,&quot;start&quot;:2268.88,&quot;text&quot;:&quot; because like the people people had to come up with alternative algorithms that like had a&quot;},{&quot;end&quot;:2282.72,&quot;start&quot;:2275.28,&quot;text&quot;:&quot; different acronym and weren't patented so like the open CV open source library would have like&quot;},{&quot;end&quot;:2288.08,&quot;start&quot;:2282.72,&quot;text&quot;:&quot; had to be careful about putting this algorithm in their library because of the patent risks so&quot;},{&quot;end&quot;:2294.7999999999997,&quot;start&quot;:2288.64,&quot;text&quot;:&quot; I think like these patents aren't the patent rights aren't exercise that much and I think big&quot;},{&quot;end&quot;:2301.2,&quot;start&quot;:2294.7999999999997,&quot;text&quot;:&quot; companies like Google will patent a lot of stuff for defensive reasons so if they get in some big&quot;},{&quot;end&quot;:2307.52,&quot;start&quot;:2301.2,&quot;text&quot;:&quot; legal dispute with another company it can be used as like one of the bargaining chips but I think&quot;},{&quot;end&quot;:2315.12,&quot;start&quot;:2307.52,&quot;text&quot;:&quot; I don't think anyone's going to like get sued for royalties for not yeah for not providing royalties&quot;},{&quot;end&quot;:2320.24,&quot;start&quot;:2315.12,&quot;text&quot;:&quot; for the use of some algorithm okay and then there's been a ton of work in RL of course since you&quot;},{&quot;end&quot;:2326.4,&quot;start&quot;:2320.24,&quot;text&quot;:&quot; first published TRPO and BBO but from your point of view if you had to pick a few highlights in&quot;},{&quot;end&quot;:2333.52,&quot;start&quot;:2326.4,&quot;text&quot;:&quot; terms of a few important milestones in in RL algorithms since PPO came out and by the way it's&quot;},{&quot;end&quot;:2341.2,&quot;start&quot;:2333.52,&quot;text&quot;:&quot; amazing that in 2022 we're still using PPO I think quite similar into it's original form is that&quot;},{&quot;end&quot;:2347.44,&quot;start&quot;:2341.2,&quot;text&quot;:&quot; right yeah pretty much yeah so so what would you say are the the biggest highlights for you&quot;},{&quot;end&quot;:2352.96,&quot;start&quot;:2348.4,&quot;text&quot;:&quot; in terms of our algorithm since since you did PPO yeah there's definitely been some interesting&quot;},{&quot;end&quot;:2361.6,&quot;start&quot;:2352.96,&quot;text&quot;:&quot; stuff so I think like a little after PPO there is TD3 and SAC and those are seem like pretty solid&quot;},{&quot;end&quot;:2366.96,&quot;start&quot;:2361.6,&quot;text&quot;:&quot; value-based methods that was one development that was interesting I think like yeah I thought&quot;},{&quot;end&quot;:2375.36,&quot;start&quot;:2366.96,&quot;text&quot;:&quot; museiro and it's and it's like elaborations we're also like efficient zero we're also pretty&quot;},{&quot;end&quot;:2380.72,&quot;start&quot;:2375.36,&quot;text&quot;:&quot; impressive that you can get that good sample efficiency both of the things I just mentioned were&quot;},{&quot;end&quot;:2386.7999999999997,&quot;start&quot;:2380.72,&quot;text&quot;:&quot; kind of well I don't want to say mostly on toy tasks or benchmarks because yeah I'm sure people&quot;},{&quot;end&quot;:2391.92,&quot;start&quot;:2386.8,&quot;text&quot;:&quot; are doing some real things with these algorithms yeah so I think that's that stuff was interesting&quot;},{&quot;end&quot;:2400.2400000000002,&quot;start&quot;:2391.92,&quot;text&quot;:&quot; I think like the whole recent interest in search of interest in the offline RL was also notable&quot;},{&quot;end&quot;:2405.28,&quot;start&quot;:2400.2400000000002,&quot;text&quot;:&quot; I would say the like the stuff we're doing with RL from human feedback is the kind of offline RL&quot;},{&quot;end&quot;:2411.76,&quot;start&quot;:2405.92,&quot;text&quot;:&quot; because we're like we have a fixed dataset and we have a fixed reward modeling dataset and we're&quot;},{&quot;end&quot;:2416.2400000000002,&quot;start&quot;:2411.76,&quot;text&quot;:&quot; training against that this is like offline RL but you're doing it in a different way you're using&quot;},{&quot;end&quot;:2423.2,&quot;start&quot;:2416.24,&quot;text&quot;:&quot; an on-policy algorithm with a reward model as opposed to maybe a more typical way to do offline RL&quot;},{&quot;end&quot;:2427.9199999999996,&quot;start&quot;:2423.2,&quot;text&quot;:&quot; would be use off-policy algorithm would that work here or would that not work here well we're&quot;},{&quot;end&quot;:2434.3999999999996,&quot;start&quot;:2427.9199999999996,&quot;text&quot;:&quot; doing here is kind of like model-based RL because the reward model is like a model of the like the&quot;},{&quot;end&quot;:2440.56,&quot;start&quot;:2434.3999999999996,&quot;text&quot;:&quot; unknown part of the system so like the unknown part of the system here is the is the human&quot;},{&quot;end&quot;:2448.08,&quot;start&quot;:2440.56,&quot;text&quot;:&quot; radar or the human it's not the outputting appending to your list of tokens so this is kind of like&quot;},{&quot;end&quot;:2454.48,&quot;start&quot;:2448.08,&quot;text&quot;:&quot; the work that's like takes a dynamics model at the environment and does some kind of just runs a&quot;},{&quot;end&quot;:2459.44,&quot;start&quot;:2454.48,&quot;text&quot;:&quot; policy grading algorithm against it so it's not like so the idea of running an online algorithm&quot;},{&quot;end&quot;:2465.52,&quot;start&quot;:2460.08,&quot;text&quot;:&quot; against a model that's kind of a well-established idea so I would say the papers that previously&quot;},{&quot;end&quot;:2470.72,&quot;start&quot;:2465.52,&quot;text&quot;:&quot; did this they were in a pretty different regime were in this regime of doing fairly small&quot;},{&quot;end&quot;:2476.24,&quot;start&quot;:2470.72,&quot;text&quot;:&quot; updates to the policy because we have this these awesome pre-trained models and we don't need to&quot;},{&quot;end&quot;:2482.56,&quot;start&quot;:2476.24,&quot;text&quot;:&quot; actually change them that much so yeah we use these online algorithms I'd say part of the reason&quot;},{&quot;end&quot;:2490.4,&quot;start&quot;:2482.56,&quot;text&quot;:&quot; why we can get away with using just an like an online algorithm is because we've been just looking&quot;},{&quot;end&quot;:2495.52,&quot;start&quot;:2490.4,&quot;text&quot;:&quot; at a band a contextual banded problem yeah because we only have like one time step like you get&quot;},{&quot;end&quot;:2501.52,&quot;start&quot;:2495.52,&quot;text&quot;:&quot; a query and you output a response and then that response gets a reward so if we had a like a&quot;},{&quot;end&quot;:2509.04,&quot;start&quot;:2501.52,&quot;text&quot;:&quot; multi-step process such as a conversation where you can't assign a reward until the very end of&quot;},{&quot;end&quot;:2516,&quot;start&quot;:2509.04,&quot;text&quot;:&quot; the conversation and or you had some I don't know some interaction with like some real-world&quot;},{&quot;end&quot;:2520.64,&quot;start&quot;:2516,&quot;text&quot;:&quot; system that's hard to simulate you wouldn't then it wouldn't be S-ray forward to you wouldn't&quot;},{&quot;end&quot;:2526.08,&quot;start&quot;:2520.64,&quot;text&quot;:&quot; be able to use exactly exactly the same methodology you would probably have to use a you would have&quot;},{&quot;end&quot;:2532.24,&quot;start&quot;:2526.08,&quot;text&quot;:&quot; to probably train a Q function or or something like that if you want if you want your method to be&quot;},{&quot;end&quot;:2536.4,&quot;start&quot;:2532.24,&quot;text&quot;:&quot; sample efficient you would probably have to do something slightly different I think we'll we'll&quot;},{&quot;end&quot;:2542.88,&quot;start&quot;:2536.4,&quot;text&quot;:&quot; have to we'll have to start exploring this at some point soon but so far we haven't at least&quot;},{&quot;end&quot;:2550.48,&quot;start&quot;:2542.88,&quot;text&quot;:&quot; I haven't seen any cases in like in the domain I'm looking at that require this but I expect it to&quot;},{&quot;end&quot;:2556.96,&quot;start&quot;:2551.44,&quot;text&quot;:&quot; to be relevant at some point so we had Arvind Shrinivas talking about decision transformer&quot;},{&quot;end&quot;:2561.76,&quot;start&quot;:2556.96,&quot;text&quot;:&quot; on the show recently that was a great episode and I see that you were also a co-author on the&quot;},{&quot;end&quot;:2565.92,&quot;start&quot;:2561.76,&quot;text&quot;:&quot; the 2016 RL squared paper I want to ask you what your thoughts about meta RL&quot;},{&quot;end&quot;:2571.28,&quot;start&quot;:2566.6400000000003,&quot;text&quot;:&quot; Arvind had some interesting things to say about maybe the idea that a transformer could kind of&quot;},{&quot;end&quot;:2575.92,&quot;start&quot;:2571.28,&quot;text&quot;:&quot; supersede the need for an RL algorithm altogether what do you expect from meta RL&quot;},{&quot;end&quot;:2581.36,&quot;start&quot;:2575.92,&quot;text&quot;:&quot; do expect will will still be using human authored RL algorithms in the future yeah that's a pretty&quot;},{&quot;end&quot;:2586.6400000000003,&quot;start&quot;:2581.36,&quot;text&quot;:&quot; bold statement that we don't need we won't need any RL algorithms anymore yeah since the RL squared&quot;},{&quot;end&quot;:2593.0400000000004,&quot;start&quot;:2586.6400000000003,&quot;text&quot;:&quot; paper people have been talking less about meta learning as far as I can tell actually because&quot;},{&quot;end&quot;:2599.28,&quot;start&quot;:2593.0400000000004,&quot;text&quot;:&quot; of sequence modeling has gotten so good like transformer let sequence models so that it's kind&quot;},{&quot;end&quot;:2604.2400000000002,&quot;start&quot;:2599.28,&quot;text&quot;:&quot; of queer the meta learning is just a special case of learning like it's it's just it's just like&quot;},{&quot;end&quot;:2610.0800000000004,&quot;start&quot;:2604.2400000000002,&quot;text&quot;:&quot; a certain kind of long context learning learning involving long episodes and maybe it shouldn't be&quot;},{&quot;end&quot;:2615.36,&quot;start&quot;:2610.0800000000004,&quot;text&quot;:&quot; treated that differently or are addressed with special algorithms I would say yeah the ideas like&quot;},{&quot;end&quot;:2620.6400000000003,&quot;start&quot;:2615.36,&quot;text&quot;:&quot; decision transformer are pretty interesting where you try to reduce RL to supervise learning it's&quot;},{&quot;end&quot;:2626.0800000000004,&quot;start&quot;:2620.6400000000003,&quot;text&quot;:&quot; still not like certain exactly how these compare and performance to RL like people have started to&quot;},{&quot;end&quot;:2633.04,&quot;start&quot;:2626.08,&quot;text&quot;:&quot; analyze that empirically and theoretically and I would say in practice sometimes sometimes it's&quot;},{&quot;end&quot;:2638.48,&quot;start&quot;:2633.04,&quot;text&quot;:&quot; better sometimes it's worse in my experience like it's been worse on the problems that I've&quot;},{&quot;end&quot;:2644.56,&quot;start&quot;:2638.48,&quot;text&quot;:&quot; that I've my colleagues and I have where we've tested it but yeah it's definitely an interesting&quot;},{&quot;end&quot;:2649.12,&quot;start&quot;:2644.56,&quot;text&quot;:&quot; direction Dr. John Schillman thank you so much for sharing your time in your insight with the&quot;},{&quot;end&quot;:2660.08,&quot;start&quot;:2649.12,&quot;text&quot;:&quot; talk our audience today thanks so much thank you&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;end\&quot;: 6.24,\n    \&quot;start\&quot;: 0,\n    \&quot;text\&quot;: \&quot; The answer was affirmative. We can get an agent to basically use a set of tools that we give it.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 12.48,\n    \&quot;start\&quot;: 6.24,\n    \&quot;text\&quot;: \&quot; In this case, the browsing commands like searchings. I would say I expect AI to be able to do better,\&quot;\n  },\n  {\n    \&quot;end\&quot;: 17.84,\n    \&quot;start\&quot;: 12.48,\n    \&quot;text\&quot;: \&quot; a better job than humans at most jobs that humans do now. Five years or so.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 27.92,\n    \&quot;start\&quot;: 22.56,\n    \&quot;text\&quot;: \&quot; TalkAulRO podcast is all reinforcing learning all the time, featuring brilliant guests,\&quot;\n  },\n  {\n    \&quot;end\&quot;: 34.08,\n    \&quot;start\&quot;: 27.92,\n    \&quot;text\&quot;: \&quot; both research and applied. Join the conversation on Twitter at TalkRL podcast. I'm your host,\&quot;\n  },\n  {\n    \&quot;end\&quot;: 44.32,\n    \&quot;start\&quot;: 34.08,\n    \&quot;text\&quot;: \&quot; Robin Chohan. John Schulman is a co-founder of OpenAI and a researcher and engineer at OpenAI.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 48.32000000000001,\n    \&quot;start\&quot;: 44.32,\n    \&quot;text\&quot;: \&quot; He is well known for major contributions to the field of reinforcement learning,\&quot;\n  },\n  {\n    \&quot;end\&quot;: 54.400000000000006,\n    \&quot;start\&quot;: 48.32000000000001,\n    \&quot;text\&quot;: \&quot; including the TRPO algorithm that's trust region policy optimization, GAE, generalized\&quot;\n  },\n  {\n    \&quot;end\&quot;: 59.12,\n    \&quot;start\&quot;: 54.4,\n    \&quot;text\&quot;: \&quot; advanced estimation. Those are from his UC Berkeley dissertation and TRPO's\&quot;\n  },\n  {\n    \&quot;end\&quot;: 65.03999999999999,\n    \&quot;start\&quot;: 59.12,\n    \&quot;text\&quot;: \&quot; descendant proximal policy optimization, or PPO. His current focus at OpenAI is on RL from\&quot;\n  },\n  {\n    \&quot;end\&quot;: 68.16,\n    \&quot;start\&quot;: 65.03999999999999,\n    \&quot;text\&quot;: \&quot; human feedback. John, welcome to the show and thanks so much for being here.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 71.75999999999999,\n    \&quot;start\&quot;: 68.16,\n    \&quot;text\&quot;: \&quot; Thanks a lot for having me. You were literally one of the first people I thought of when I started\&quot;\n  },\n  {\n    \&quot;end\&quot;: 77.6,\n    \&quot;start\&quot;: 71.75999999999999,\n    \&quot;text\&quot;: \&quot; the show three years back. Thanks, I'm honored. It means a lot to me to have you here today. I definitely\&quot;\n  },\n  {\n    \&quot;end\&quot;: 83.12,\n    \&quot;start\&quot;: 77.6,\n    \&quot;text\&quot;: \&quot; remember you were nuts and bolts of deep RL video back in the day and watching that multiple times\&quot;\n  },\n  {\n    \&quot;end\&quot;: 88.88000000000001,\n    \&quot;start\&quot;: 83.12,\n    \&quot;text\&quot;: \&quot; and gaining a lot from that. You helped a generation of RL practitioners back then. By the way,\&quot;\n  },\n  {\n    \&quot;end\&quot;: 95.52000000000001,\n    \&quot;start\&quot;: 88.88000000000001,\n    \&quot;text\&quot;: \&quot; there's going to be a reboot of the nuts and bolts presentation. I got invited to give a talk\&quot;\n  },\n  {\n    \&quot;end\&quot;: 101.92,\n    \&quot;start\&quot;: 95.52000000000001,\n    \&quot;text\&quot;: \&quot; at NERPS this year on it. I'll have to revamp the guidelines and everything. That'll be fun.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 107.12,\n    \&quot;start\&quot;: 101.92,\n    \&quot;text\&quot;: \&quot; Oh, that's awesome. Can't wait for that. You were clearly one of the earlier pioneers in deep RL.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 112.4,\n    \&quot;start\&quot;: 107.12,\n    \&quot;text\&quot;: \&quot; How did you choose to move your focus to RL from human feedback? Why is that an important problem?\&quot;\n  },\n  {\n    \&quot;end\&quot;: 117.84,\n    \&quot;start\&quot;: 112.4,\n    \&quot;text\&quot;: \&quot; Why is that important to you? After GB3 was trained, I was blown away by how smart it was and I\&quot;\n  },\n  {\n    \&quot;end\&quot;: 122.32000000000001,\n    \&quot;start\&quot;: 117.84,\n    \&quot;text\&quot;: \&quot; realized the next frontier was figuring out how to make language models actually useful. I'm still\&quot;\n  },\n  {\n    \&quot;end\&quot;: 128.4,\n    \&quot;start\&quot;: 122.32000000000001,\n    \&quot;text\&quot;: \&quot; really interested in RL but solving RL benchmarks isn't the end of the story. To use your RL\&quot;\n  },\n  {\n    \&quot;end\&quot;: 134.08,\n    \&quot;start\&quot;: 128.4,\n    \&quot;text\&quot;: \&quot; algorithm you need a reward function. Whereas the reward function come from in RL benchmarks,\&quot;\n  },\n  {\n    \&quot;end\&quot;: 138.16,\n    \&quot;start\&quot;: 134.08,\n    \&quot;text\&quot;: \&quot; you usually just code up the reward function. But if you're not in a simulator environment,\&quot;\n  },\n  {\n    \&quot;end\&quot;: 144.07999999999998,\n    \&quot;start\&quot;: 138.16,\n    \&quot;text\&quot;: \&quot; that doesn't work. What we have to do in any kind of real-world use case is have humans look at\&quot;\n  },\n  {\n    \&quot;end\&quot;: 149.04,\n    \&quot;start\&quot;: 144.07999999999998,\n    \&quot;text\&quot;: \&quot; what the AI did and decide if it was good or bad. How exactly do you define this reward\&quot;\n  },\n  {\n    \&quot;end\&quot;: 154,\n    \&quot;start\&quot;: 149.04,\n    \&quot;text\&quot;: \&quot; becomes a really challenging and important problem, especially as the tasks get harder to evaluate?\&quot;\n  },\n  {\n    \&quot;end\&quot;: 159.2,\n    \&quot;start\&quot;: 154,\n    \&quot;text\&quot;: \&quot; Another angle on this is that language models are very smart but it's hard to get them to do\&quot;\n  },\n  {\n    \&quot;end\&quot;: 164.24,\n    \&quot;start\&quot;: 159.2,\n    \&quot;text\&quot;: \&quot; anything useful. A big part of that is they're not necessarily trying to do what you want. They're\&quot;\n  },\n  {\n    \&quot;end\&quot;: 168.88,\n    \&quot;start\&quot;: 164.24,\n    \&quot;text\&quot;: \&quot; just trying to imitate the training corpus. That means there's a big opportunity to improve\&quot;\n  },\n  {\n    \&quot;end\&quot;: 173.84,\n    \&quot;start\&quot;: 168.88,\n    \&quot;text\&quot;: \&quot; them a lot by just giving them the right objective. That's what we can do by applying RL to these\&quot;\n  },\n  {\n    \&quot;end\&quot;: 181.12,\n    \&quot;start\&quot;: 174.64000000000001,\n    \&quot;text\&quot;: \&quot; language models using human feedback to define the reward. Is human feedback harder or\&quot;\n  },\n  {\n    \&quot;end\&quot;: 185.92000000000002,\n    \&quot;start\&quot;: 181.12,\n    \&quot;text\&quot;: \&quot; very different in some way than using a synthetic reward? There are a lot of new complications.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 192.56,\n    \&quot;start\&quot;: 187.36,\n    \&quot;text\&quot;: \&quot; You have to collect a data set dynamically. You're always in the business of building data sets of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 199.12,\n    \&quot;start\&quot;: 192.56,\n    \&quot;text\&quot;: \&quot; human preferences. Often the data quality there matters more than various algorithmic details.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 204.32,\n    \&quot;start\&quot;: 199.12,\n    \&quot;text\&quot;: \&quot; You also have to think a lot about exactly how you're giving the task to the human trainers\&quot;\n  },\n  {\n    \&quot;end\&quot;: 208.32,\n    \&quot;start\&quot;: 204.32,\n    \&quot;text\&quot;: \&quot; and various other things that you wouldn't have thought about if you just had a programmatic reward\&quot;\n  },\n  {\n    \&quot;end\&quot;: 213.44,\n    \&quot;start\&quot;: 208.32,\n    \&quot;text\&quot;: \&quot; function. Does the difference between human-raders or the noisiness of the reward signal cost any\&quot;\n  },\n  {\n    \&quot;end\&quot;: 220.56,\n    \&quot;start\&quot;: 213.44,\n    \&quot;text\&quot;: \&quot; problems? I would say the noise definitely you need to be below some threshold of noise to learn\&quot;\n  },\n  {\n    \&quot;end\&quot;: 226.64000000000001,\n    \&quot;start\&quot;: 220.56,\n    \&quot;text\&quot;: \&quot; anything. I think in general if you have a large noisy data set that can be as good as a smaller\&quot;\n  },\n  {\n    \&quot;end\&quot;: 231.6,\n    \&quot;start\&quot;: 226.64000000000001,\n    \&quot;text\&quot;: \&quot; clean data set. Actually, noise isn't the thing that worries me the most. It's more that there are\&quot;\n  },\n  {\n    \&quot;end\&quot;: 238,\n    \&quot;start\&quot;: 231.6,\n    \&quot;text\&quot;: \&quot; sometimes consistent biases that people have. For example, in settings like question answering\&quot;\n  },\n  {\n    \&quot;end\&quot;: 244.4,\n    \&quot;start\&quot;: 238,\n    \&quot;text\&quot;: \&quot; or settings where you have a model writing some text, often people prefer longer answers. You end\&quot;\n  },\n  {\n    \&quot;end\&quot;: 249.36,\n    \&quot;start\&quot;: 244.4,\n    \&quot;text\&quot;: \&quot; up with these very verbose answers. If you're not careful with the instructions that is. You can\&quot;\n  },\n  {\n    \&quot;end\&quot;: 256.40000000000003,\n    \&quot;start\&quot;: 249.36,\n    \&quot;text\&quot;: \&quot; also instruct people the raiders to reward brevity. But without yet, if you're not careful you can\&quot;\n  },\n  {\n    \&quot;end\&quot;: 262,\n    \&quot;start\&quot;: 257.04,\n    \&quot;text\&quot;: \&quot; incentivize the wrong kinds of behaviors. So let's move to some of your recent work. First up is\&quot;\n  },\n  {\n    \&quot;end\&quot;: 268.40000000000003,\n    \&quot;start\&quot;: 262,\n    \&quot;text\&quot;: \&quot; WebGPT. Browser assisted question answering with human feedback. That's a Nekano at all with yourself\&quot;\n  },\n  {\n    \&quot;end\&quot;: 273.84000000000003,\n    \&quot;start\&quot;: 268.40000000000003,\n    \&quot;text\&quot;: \&quot; as a co-author in 2021. Can you tell us what is the main idea of this paper? What is WebGPT?\&quot;\n  },\n  {\n    \&quot;end\&quot;: 280.23999999999995,\n    \&quot;start\&quot;: 273.84,\n    \&quot;text\&quot;: \&quot; In WebGPT, we basically took our language models and we hooked them up to a web browser so they\&quot;\n  },\n  {\n    \&quot;end\&quot;: 285.35999999999996,\n    \&quot;start\&quot;: 280.23999999999995,\n    \&quot;text\&quot;: \&quot; could retrieve information from the web. They can write an answer by summarizing the relevant pages\&quot;\n  },\n  {\n    \&quot;end\&quot;: 290.08,\n    \&quot;start\&quot;: 285.35999999999996,\n    \&quot;text\&quot;: \&quot; from the web. That way if you're asking a question about current events or a question that requires\&quot;\n  },\n  {\n    \&quot;end\&quot;: 295.35999999999996,\n    \&quot;start\&quot;: 290.08,\n    \&quot;text\&quot;: \&quot; some detailed scientific or technical knowledge, this AI can go out and look up the answer and\&quot;\n  },\n  {\n    \&quot;end\&quot;: 301.67999999999995,\n    \&quot;start\&quot;: 295.35999999999996,\n    \&quot;text\&quot;: \&quot; with detailed citations to its sources. I would say there's two interesting points to this. One is\&quot;\n  },\n  {\n    \&quot;end\&quot;: 306.24,\n    \&quot;start\&quot;: 301.68,\n    \&quot;text\&quot;: \&quot; we were exploring whether you could turn language models into a kind of agent. There's a lot of data\&quot;\n  },\n  {\n    \&quot;end\&quot;: 310.32,\n    \&quot;start\&quot;: 306.24,\n    \&quot;text\&quot;: \&quot; on the web of different texts that people have written. But there's not a lot of data that shows\&quot;\n  },\n  {\n    \&quot;end\&quot;: 316.24,\n    \&quot;start\&quot;: 310.32,\n    \&quot;text\&quot;: \&quot; how to actually do some multi-step process. So it's not that clear, uprearry whether you can get a\&quot;\n  },\n  {\n    \&quot;end\&quot;: 321.68,\n    \&quot;start\&quot;: 316.24,\n    \&quot;text\&quot;: \&quot; language model to actually carry out some iterative process. We just have a lot of data like writing\&quot;\n  },\n  {\n    \&quot;end\&quot;: 326.16,\n    \&quot;start\&quot;: 321.68,\n    \&quot;text\&quot;: \&quot; essays and having chats and so forth. So that was one thing we were exploring here and I think\&quot;\n  },\n  {\n    \&quot;end\&quot;: 332.8,\n    \&quot;start\&quot;: 326.16,\n    \&quot;text\&quot;: \&quot; the answer was affirmative. We can get an agent to basically use a set of tools that we give it.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 338.16,\n    \&quot;start\&quot;: 332.8,\n    \&quot;text\&quot;: \&quot; In this case the browsing commands like searchings, scroll link, click on links. The second\&quot;\n  },\n  {\n    \&quot;end\&quot;: 344.24,\n    \&quot;start\&quot;: 338.16,\n    \&quot;text\&quot;: \&quot; theme of this paper was around truthfulness. I mean a big issue with language models is I mean\&quot;\n  },\n  {\n    \&quot;end\&quot;: 349.76000000000005,\n    \&quot;start\&quot;: 344.24,\n    \&quot;text\&quot;: \&quot; they're not very reliable at giving you true information. They know a vastly superhuman amount. But\&quot;\n  },\n  {\n    \&quot;end\&quot;: 354.64000000000004,\n    \&quot;start\&quot;: 349.76000000000005,\n    \&quot;text\&quot;: \&quot; if you prompt them in the wrong way they'll just output lots of plausible sounding nonsense. So\&quot;\n  },\n  {\n    \&quot;end\&quot;: 359.84,\n    \&quot;start\&quot;: 354.64,\n    \&quot;text\&quot;: \&quot; how to fix that is a big research question or one of the biggest research questions in the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 364.32,\n    \&quot;start\&quot;: 359.84,\n    \&quot;text\&quot;: \&quot; world of language models. I think it's going to be challenging to fully fix it but I think a big\&quot;\n  },\n  {\n    \&quot;end\&quot;: 370.32,\n    \&quot;start\&quot;: 364.32,\n    \&quot;text\&quot;: \&quot; part of the story involves retrieval and having models write answers that contain citations.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 375.28,\n    \&quot;start\&quot;: 370.32,\n    \&quot;text\&quot;: \&quot; Citations to try trusted sources. So a person who's checking over the answer doesn't have to go and\&quot;\n  },\n  {\n    \&quot;end\&quot;: 379.91999999999996,\n    \&quot;start\&quot;: 375.28,\n    \&quot;text\&quot;: \&quot; try to figure out where the model might have gotten this idea. They can go and directly look at\&quot;\n  },\n  {\n    \&quot;end\&quot;: 387.6,\n    \&quot;start\&quot;: 379.92,\n    \&quot;text\&quot;: \&quot; the source and see if it supports the AI statement. With WebGBT we just wanted to see if we do give\&quot;\n  },\n  {\n    \&quot;end\&quot;: 392.40000000000003,\n    \&quot;start\&quot;: 387.6,\n    \&quot;text\&quot;: \&quot; the language model a really flexible interface to the web. Can we have it answer hard questions\&quot;\n  },\n  {\n    \&quot;end\&quot;: 398.32,\n    \&quot;start\&quot;: 392.40000000000003,\n    \&quot;text\&quot;: \&quot; truthfully using like with the help of all these citations. And it's actually really non-trivial\&quot;\n  },\n  {\n    \&quot;end\&quot;: 403.76,\n    \&quot;start\&quot;: 398.32,\n    \&quot;text\&quot;: \&quot; because if you look at the data that we use the Reddit explain it like on five. The questions\&quot;\n  },\n  {\n    \&quot;end\&quot;: 408.08000000000004,\n    \&quot;start\&quot;: 403.76,\n    \&quot;text\&quot;: \&quot; are really varied like some of them are about science, history, current events. Like our\&quot;\n  },\n  {\n    \&quot;end\&quot;: 413.84,\n    \&quot;start\&quot;: 408.08,\n    \&quot;text\&quot;: \&quot; Raiders didn't necessarily know anything about these topics but still they had to judge the answers\&quot;\n  },\n  {\n    \&quot;end\&quot;: 418.88,\n    \&quot;start\&quot;: 413.84,\n    \&quot;text\&quot;: \&quot; written detailed answers. So it would have been really hard to do it without the supporting\&quot;\n  },\n  {\n    \&quot;end\&quot;: 425.12,\n    \&quot;start\&quot;: 418.88,\n    \&quot;text\&quot;: \&quot; citations. So we kind of validated that we could get good feedback in a hard domain like this\&quot;\n  },\n  {\n    \&quot;end\&quot;: 431.12,\n    \&quot;start\&quot;: 425.12,\n    \&quot;text\&quot;: \&quot; with the help of citations. Can you talk about where the idea for WebGBT came from? Is that an idea\&quot;\n  },\n  {\n    \&quot;end\&quot;: 435.12,\n    \&quot;start\&quot;: 431.12,\n    \&quot;text\&quot;: \&quot; you've had kicking around for a while or was it something that came up recently before the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 441.36,\n    \&quot;start\&quot;: 435.12,\n    \&quot;text\&quot;: \&quot; paper? How did that play out? Some of the ideas had been floating around like we thought that we\&quot;\n  },\n  {\n    \&quot;end\&quot;: 447.12,\n    \&quot;start\&quot;: 441.36,\n    \&quot;text\&quot;: \&quot; actually had a project at OpenAI very early on a world called World of Bits. We were looking at\&quot;\n  },\n  {\n    \&quot;end\&quot;: 452.16,\n    \&quot;start\&quot;: 447.12,\n    \&quot;text\&quot;: \&quot; controlling web browsers or doing tasks that involve tasks on the internet with the web browser\&quot;\n  },\n  {\n    \&quot;end\&quot;: 458.4,\n    \&quot;start\&quot;: 452.16,\n    \&quot;text\&quot;: \&quot; but it was way too early at the time. So we kind of abandoned it for a few years. Actually we\&quot;\n  },\n  {\n    \&quot;end\&quot;: 462.8,\n    \&quot;start\&quot;: 458.4,\n    \&quot;text\&quot;: \&quot; were trying to back then we were trying to do it with full visual input. So we thought yeah we could\&quot;\n  },\n  {\n    \&quot;end\&quot;: 469.12,\n    \&quot;start\&quot;: 462.8,\n    \&quot;text\&quot;: \&quot; give some instructions to the agent like go and figure out figure out the address of this\&quot;\n  },\n  {\n    \&quot;end\&quot;: 475.68,\n    \&quot;start\&quot;: 469.84000000000003,\n    \&quot;text\&quot;: \&quot; building or something. The agent would go and search the web or use Google Maps or whatever\&quot;\n  },\n  {\n    \&quot;end\&quot;: 479.92,\n    \&quot;start\&quot;: 475.68,\n    \&quot;text\&quot;: \&quot; to figure out the answer. And we were trying to do this all in pixels that obviously didn't work\&quot;\n  },\n  {\n    \&quot;end\&quot;: 486.16,\n    \&quot;start\&quot;: 479.92,\n    \&quot;text\&quot;: \&quot; very well. But now we have these great language models on the work on text data. We can also\&quot;\n  },\n  {\n    \&quot;end\&quot;: 493.12,\n    \&quot;start\&quot;: 486.16,\n    \&quot;text\&quot;: \&quot; extract the text out of web pages to get most of the information. We can't really interact with\&quot;\n  },\n  {\n    \&quot;end\&quot;: 498.16,\n    \&quot;start\&quot;: 493.12,\n    \&quot;text\&quot;: \&quot; a lot of dynamic websites. Yeah, where there's a lot of JavaScript and images and so forth. But\&quot;\n  },\n  {\n    \&quot;end\&quot;: 504.64000000000004,\n    \&quot;start\&quot;: 498.16,\n    \&quot;text\&quot;: \&quot; as long as it's just browsing and reading text we're fine. So yeah we had good enough models and\&quot;\n  },\n  {\n    \&quot;end\&quot;: 510.8,\n    \&quot;start\&quot;: 504.64000000000004,\n    \&quot;text\&quot;: \&quot; that made it kind of feasible to revisit this idea of using the internet as an environment.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 516.32,\n    \&quot;start\&quot;: 510.8,\n    \&quot;text\&quot;: \&quot; So I would say that was one of the sources of inspiration that long-stinted, that long kind of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 522.4,\n    \&quot;start\&quot;: 516.32,\n    \&quot;text\&quot;: \&quot; thread about like using the internet as an environment. Another motivation was just after we got\&quot;\n  },\n  {\n    \&quot;end\&quot;: 529.12,\n    \&quot;start\&quot;: 523.2,\n    \&quot;text\&quot;: \&quot; after we started playing with GPD3 we noticed that it had all these problems with factual\&quot;\n  },\n  {\n    \&quot;end\&quot;: 535.52,\n    \&quot;start\&quot;: 529.12,\n    \&quot;text\&quot;: \&quot; accuracy and the reliability of the information it was giving us. So that kind of motivated doing\&quot;\n  },\n  {\n    \&quot;end\&quot;: 540.4,\n    \&quot;start\&quot;: 535.52,\n    \&quot;text\&quot;: \&quot; more research on how to make language models more truthful. We were kind of brainstorming what to\&quot;\n  },\n  {\n    \&quot;end\&quot;: 547.04,\n    \&quot;start\&quot;: 540.4,\n    \&quot;text\&quot;: \&quot; do there and we went through some docs and eventually decided that we wanted to try some question\&quot;\n  },\n  {\n    \&quot;end\&quot;: 551.92,\n    \&quot;start\&quot;: 547.04,\n    \&quot;text\&quot;: \&quot; answering like using the web, looking up knowledge on the web to help answer questions. So actually\&quot;\n  },\n  {\n    \&quot;end\&quot;: 556.24,\n    \&quot;start\&quot;: 551.92,\n    \&quot;text\&quot;: \&quot; the original version of the project used trivia questions. So there's another, there's this\&quot;\n  },\n  {\n    \&quot;end\&quot;: 562.16,\n    \&quot;start\&quot;: 556.24,\n    \&quot;text\&quot;: \&quot; well-known data set trivia QA that has some basic trivia questions. So we first worked a little\&quot;\n  },\n  {\n    \&quot;end\&quot;: 569.12,\n    \&quot;start\&quot;: 562.16,\n    \&quot;text\&quot;: \&quot; bit on that data set and tried to see if we could boost the model's accuracy by giving it web search\&quot;\n  },\n  {\n    \&quot;end\&quot;: 576,\n    \&quot;start\&quot;: 569.12,\n    \&quot;text\&quot;: \&quot; and yeah that actually works quite straight, that worked pretty easily. So then we decided to move on\&quot;\n  },\n  {\n    \&quot;end\&quot;: 582.72,\n    \&quot;start\&quot;: 576,\n    \&quot;text\&quot;: \&quot; to long-form question answering and so that gave us the, that was the project we ended up working on\&quot;\n  },\n  {\n    \&quot;end\&quot;: 589.12,\n    \&quot;start\&quot;: 582.72,\n    \&quot;text\&quot;: \&quot; for a while. It seems like you use a few different data sets here and a number of different training\&quot;\n  },\n  {\n    \&quot;end\&quot;: 594.96,\n    \&quot;start\&quot;: 589.12,\n    \&quot;text\&quot;: \&quot; methods. I'll just mention the last behavior cloning, reward modeling, reinforcement learning,\&quot;\n  },\n  {\n    \&quot;end\&quot;: 601.76,\n    \&quot;start\&quot;: 594.96,\n    \&quot;text\&quot;: \&quot; and rejection sampling. So we were using a fairly standard methodology which was actually adapted\&quot;\n  },\n  {\n    \&quot;end\&quot;: 609.2,\n    \&quot;start\&quot;: 601.76,\n    \&quot;text\&quot;: \&quot; from previous work on RL from Human Preferences. So the pipeline is you first train a model with\&quot;\n  },\n  {\n    \&quot;end\&quot;: 615.44,\n    \&quot;start\&quot;: 609.2,\n    \&quot;text\&quot;: \&quot; supervised learning where you you have human demonstrators show how to do the task, like show how to map\&quot;\n  },\n  {\n    \&quot;end\&quot;: 620.8000000000001,\n    \&quot;start\&quot;: 615.44,\n    \&quot;text\&quot;: \&quot; from observations to actions. Yeah so that's the supervised learning or behavior cloning step then we\&quot;\n  },\n  {\n    \&quot;end\&quot;: 628.7199999999999,\n    \&quot;start\&quot;: 620.8,\n    \&quot;text\&quot;: \&quot; train a reward model or preference model. It looks at two actions or two out trajectories and decides\&quot;\n  },\n  {\n    \&quot;end\&quot;: 633.76,\n    \&quot;start\&quot;: 628.7199999999999,\n    \&quot;text\&quot;: \&quot; which one is better. In this case like in a question answering setting you're looking at two answers\&quot;\n  },\n  {\n    \&quot;end\&quot;: 638.56,\n    \&quot;start\&quot;: 633.76,\n    \&quot;text\&quot;: \&quot; and deciding which answer is better and we use that to train a reward model that assigns higher score\&quot;\n  },\n  {\n    \&quot;end\&quot;: 643.04,\n    \&quot;start\&quot;: 638.56,\n    \&quot;text\&quot;: \&quot; to the good answers than the bad ones. Then you do reinforcement learning against that reward function\&quot;\n  },\n  {\n    \&quot;end\&quot;: 648.16,\n    \&quot;start\&quot;: 643.04,\n    \&quot;text\&quot;: \&quot; and of course you can iterate these last two steps. After you do a little RL now you're, you sort of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 653.4399999999999,\n    \&quot;start\&quot;: 648.16,\n    \&quot;text\&quot;: \&quot; exploited some of the flaws of the reward model like or some of the noise in the reward model and\&quot;\n  },\n  {\n    \&quot;end\&quot;: 658.9599999999999,\n    \&quot;start\&quot;: 653.4399999999999,\n    \&quot;text\&quot;: \&quot; it's not necessarily accurate on your new distribution of data. You recollect more pairs of samples\&quot;\n  },\n  {\n    \&quot;end\&quot;: 665.28,\n    \&quot;start\&quot;: 658.9599999999999,\n    \&quot;text\&quot;: \&quot; and refit this preference model and then you do another iteration of RL. So that's like that's\&quot;\n  },\n  {\n    \&quot;end\&quot;: 670.9599999999999,\n    \&quot;start\&quot;: 665.28,\n    \&quot;text\&quot;: \&quot; the whole RL from Human Feedback Pipeline and there's this other idea called rejection sampling\&quot;\n  },\n  {\n    \&quot;end\&quot;: 676.48,\n    \&quot;start\&quot;: 670.9599999999999,\n    \&quot;text\&quot;: \&quot; or best event sampling and in general you can do other kinds of search too where instead of doing\&quot;\n  },\n  {\n    \&quot;end\&quot;: 681.52,\n    \&quot;start\&quot;: 676.48,\n    \&quot;text\&quot;: \&quot; RL once you have your reward model you can just search against that reward model so you can take\&quot;\n  },\n  {\n    \&quot;end\&quot;: 687.6,\n    \&quot;start\&quot;: 681.52,\n    \&quot;text\&quot;: \&quot; a bunch of collect a bunch of samples and re-rank them with the reward model and take the best one\&quot;\n  },\n  {\n    \&quot;end\&quot;: 694.08,\n    \&quot;start\&quot;: 687.6,\n    \&quot;text\&quot;: \&quot; as your action. Kind of like NPC. Yeah exactly. Yeah kind of depends exactly what setting you're in\&quot;\n  },\n  {\n    \&quot;end\&quot;: 699.76,\n    \&quot;start\&quot;: 694.64,\n    \&quot;text\&quot;: \&quot; what you can do. If you're in a setting where there's some environment you're interacting with then\&quot;\n  },\n  {\n    \&quot;end\&quot;: 705.44,\n    \&quot;start\&quot;: 699.76,\n    \&quot;text\&quot;: \&quot; you would have to simulate your, you would have to simulate the dynamics of your environment which\&quot;\n  },\n  {\n    \&quot;end\&quot;: 711.84,\n    \&quot;start\&quot;: 705.44,\n    \&quot;text\&quot;: \&quot; yeah so that would look kind of like NPC. In our case we were the only thing we had to learn a model of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 718.24,\n    \&quot;start\&quot;: 711.84,\n    \&quot;text\&quot;: \&quot; was the human preference so like we're it's a question answering setting so it's really like a\&quot;\n  },\n  {\n    \&quot;end\&quot;: 723.2,\n    \&quot;start\&quot;: 718.24,\n    \&quot;text\&quot;: \&quot; contextual banded problem so it's kind of straightforward to take a bunch of sample a bunch of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 730.8000000000001,\n    \&quot;start\&quot;: 723.2,\n    \&quot;text\&quot;: \&quot; actions where each action is a full answer and re-rank them or search against the search over answers.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 736.4,\n    \&quot;start\&quot;: 730.8,\n    \&quot;text\&quot;: \&quot; So in terms of the action space was it the action space just a list of commands or is it still\&quot;\n  },\n  {\n    \&quot;end\&quot;: 743.76,\n    \&quot;start\&quot;: 736.4,\n    \&quot;text\&quot;: \&quot; generating tokens like a regular generative mode? We were generating tokens. We had two phases of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 751.12,\n    \&quot;start\&quot;: 743.76,\n    \&quot;text\&quot;: \&quot; like in each episode of the RL task so there is first a browsing phase where where the model goes\&quot;\n  },\n  {\n    \&quot;end\&quot;: 757.04,\n    \&quot;start\&quot;: 751.12,\n    \&quot;text\&quot;: \&quot; and it issues searches and clicks on things and quotes relevant information like if it sees\&quot;\n  },\n  {\n    \&quot;end\&quot;: 761.92,\n    \&quot;start\&quot;: 757.04,\n    \&quot;text\&quot;: \&quot; something useful on the page it'll it'll quote it using this quote commands and then once it's\&quot;\n  },\n  {\n    \&quot;end\&quot;: 769.28,\n    \&quot;start\&quot;: 762.8,\n    \&quot;text\&quot;: \&quot; browse it's done browsing it'll issue another command called end browsing and it'll write its\&quot;\n  },\n  {\n    \&quot;end\&quot;: 775.68,\n    \&quot;start\&quot;: 769.28,\n    \&quot;text\&quot;: \&quot; answer that's also expressed in tokens but really we rolled this all into one big RL task where\&quot;\n  },\n  {\n    \&quot;end\&quot;: 781.28,\n    \&quot;start\&quot;: 775.68,\n    \&quot;text\&quot;: \&quot; your episode involves browsing and writing out the answer and it's all one big RL episode.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 785.28,\n    \&quot;start\&quot;: 781.28,\n    \&quot;text\&quot;: \&quot; Did you think this is going to work well or were you kind of surprised? At the very beginning of the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 790.72,\n    \&quot;start\&quot;: 785.28,\n    \&quot;text\&quot;: \&quot; project we didn't know if it was going to work or not. Like after we did the initial experiments\&quot;\n  },\n  {\n    \&quot;end\&quot;: 797.68,\n    \&quot;start\&quot;: 790.72,\n    \&quot;text\&quot;: \&quot; with Trivia QA which actually didn't take that long to get running then it became pretty clear\&quot;\n  },\n  {\n    \&quot;end\&quot;: 802.24,\n    \&quot;start\&quot;: 797.68,\n    \&quot;text\&quot;: \&quot; that it would work that the browsing part worked at least and we already know that we can get\&quot;\n  },\n  {\n    \&quot;end\&quot;: 807.8399999999999,\n    \&quot;start\&quot;: 802.24,\n    \&quot;text\&quot;: \&quot; these models to write pretty good long form text with a bunch of if you give them a bunch of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 814.16,\n    \&quot;start\&quot;: 807.8399999999999,\n    \&quot;text\&quot;: \&quot; snippets of text that they they can cite. So I noticed the the the human raiders task was\&quot;\n  },\n  {\n    \&quot;end\&quot;: 818.88,\n    \&quot;start\&quot;: 814.16,\n    \&quot;text\&quot;: \&quot; quite complicated as it was a long guide and there was many types of feedback that they were giving\&quot;\n  },\n  {\n    \&quot;end\&quot;: 823.52,\n    \&quot;start\&quot;: 818.88,\n    \&quot;text\&quot;: \&quot; but in the end the paper said that only the final rating was used so I was just curious if you\&quot;\n  },\n  {\n    \&quot;end\&quot;: 827.28,\n    \&quot;start\&quot;: 823.52,\n    \&quot;text\&quot;: \&quot; hadn't commented about that like why do you think maybe the model couldn't use that extra feedback\&quot;\n  },\n  {\n    \&quot;end\&quot;: 833.12,\n    \&quot;start\&quot;: 827.28,\n    \&quot;text\&quot;: \&quot; whereas it was maybe just too much or not enough samples. Yeah that's been one frustrating\&quot;\n  },\n  {\n    \&quot;end\&quot;: 840.0799999999999,\n    \&quot;start\&quot;: 833.8399999999999,\n    \&quot;text\&quot;: \&quot; finding so far in in that project and also some other projects we've had the same finding but\&quot;\n  },\n  {\n    \&quot;end\&quot;: 845.76,\n    \&quot;start\&quot;: 840.08,\n    \&quot;text\&quot;: \&quot; you have your raiders go through this long process for each for each comparison they do where\&quot;\n  },\n  {\n    \&quot;end\&quot;: 851.12,\n    \&quot;start\&quot;: 845.76,\n    \&quot;text\&quot;: \&quot; they're comparing a pair of answers and then you only use one bit of information from the whole\&quot;\n  },\n  {\n    \&quot;end\&quot;: 855.84,\n    \&quot;start\&quot;: 851.12,\n    \&quot;text\&quot;: \&quot; from this whole process which might have taken like half an hour. It seems like it would be better if\&quot;\n  },\n  {\n    \&quot;end\&quot;: 862.08,\n    \&quot;start\&quot;: 855.84,\n    \&quot;text\&quot;: \&quot; we if we were able to extract more information more about the process they went through in arriving\&quot;\n  },\n  {\n    \&quot;end\&quot;: 867.0400000000001,\n    \&quot;start\&quot;: 862.08,\n    \&quot;text\&quot;: \&quot; at the answer. So we did collect all sorts of other information like we had them provide ratings\&quot;\n  },\n  {\n    \&quot;end\&quot;: 873.4399999999999,\n    \&quot;start\&quot;: 867.04,\n    \&quot;text\&quot;: \&quot; along several different axes like coherence and factual accuracy and so forth but in the end\&quot;\n  },\n  {\n    \&quot;end\&quot;: 880.24,\n    \&quot;start\&quot;: 874.3199999999999,\n    \&quot;text\&quot;: \&quot; we didn't really get much of a boost out of using any of this this other information so I'd say\&quot;\n  },\n  {\n    \&quot;end\&quot;: 886.56,\n    \&quot;start\&quot;: 881.12,\n    \&quot;text\&quot;: \&quot; it seems like there's it should be possible to do better but unfortunately this methodology which\&quot;\n  },\n  {\n    \&quot;end\&quot;: 893.68,\n    \&quot;start\&quot;: 886.56,\n    \&quot;text\&quot;: \&quot; seems kind of dumb so far it's hard to be and people have tried various other ideas for like how\&quot;\n  },\n  {\n    \&quot;end\&quot;: 898,\n    \&quot;start\&quot;: 893.68,\n    \&quot;text\&quot;: \&quot; to use human feedback instead of you getting these preference scores there various other things you\&quot;\n  },\n  {\n    \&quot;end\&quot;: 903.68,\n    \&quot;start\&quot;: 898,\n    \&quot;text\&quot;: \&quot; can do like you can have them right critiques and edit or maybe edit the responses. Yeah I think\&quot;\n  },\n  {\n    \&quot;end\&quot;: 911.12,\n    \&quot;start\&quot;: 903.68,\n    \&quot;text\&quot;: \&quot; some of these things are are also promising but yeah this methodology of collecting preference data\&quot;\n  },\n  {\n    \&quot;end\&quot;: 917.04,\n    \&quot;start\&quot;: 911.12,\n    \&quot;text\&quot;: \&quot; works well. Yeah I think it's it's still an open area of research. Oh yeah regarding the really\&quot;\n  },\n  {\n    \&quot;end\&quot;: 922.64,\n    \&quot;start\&quot;: 917.04,\n    \&quot;text\&quot;: \&quot; long instructions. Yeah I think for any of these tasks there is a lot of subtlety in how to do the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 929.4399999999999,\n    \&quot;start\&quot;: 922.64,\n    \&quot;text\&quot;: \&quot; task properly and so we ended up adding more and more details of like what do you do in this situation\&quot;\n  },\n  {\n    \&quot;end\&quot;: 933.76,\n    \&quot;start\&quot;: 929.4399999999999,\n    \&quot;text\&quot;: \&quot; and what do you do in that situation. I think it's starting to get pretty unwieldy with these really\&quot;\n  },\n  {\n    \&quot;end\&quot;: 940.8,\n    \&quot;start\&quot;: 933.76,\n    \&quot;text\&quot;: \&quot; long instruction manuals so there's some promising ideas for how to address this like there's a\&quot;\n  },\n  {\n    \&quot;end\&quot;: 946.4,\n    \&quot;start\&quot;: 940.8,\n    \&quot;text\&quot;: \&quot; paper from DeepMind recently Sparrow that used basically broke down the task and they trained\&quot;\n  },\n  {\n    \&quot;end\&quot;: 952.3199999999999,\n    \&quot;start\&quot;: 947.04,\n    \&quot;text\&quot;: \&quot; they basically had people look at one aspect of the one aspect of the response at a time\&quot;\n  },\n  {\n    \&quot;end\&quot;: 957.0400000000001,\n    \&quot;start\&quot;: 952.32,\n    \&quot;text\&quot;: \&quot; and and then they had a way of combining these different rule specific they would train a bunch\&quot;\n  },\n  {\n    \&quot;end\&quot;: 961.6800000000001,\n    \&quot;start\&quot;: 957.0400000000001,\n    \&quot;text\&quot;: \&quot; of rule specific reward models and then combine them at the end. Yeah I think there's some other\&quot;\n  },\n  {\n    \&quot;end\&quot;: 967.5200000000001,\n    \&quot;start\&quot;: 961.6800000000001,\n    \&quot;text\&quot;: \&quot; interesting ideas for how to how to make this process better. So I gather that from your answer\&quot;\n  },\n  {\n    \&quot;end\&quot;: 972.6400000000001,\n    \&quot;start\&quot;: 967.5200000000001,\n    \&quot;text\&quot;: \&quot; about WebGPT and the whole idea of WebGPT is that you want the the language model type access to\&quot;\n  },\n  {\n    \&quot;end\&quot;: 978.48,\n    \&quot;start\&quot;: 972.6400000000001,\n    \&quot;text\&quot;: \&quot; external knowledge but I wonder where you think the line should really be in terms of what a\&quot;\n  },\n  {\n    \&quot;end\&quot;: 982.88,\n    \&quot;start\&quot;: 978.48,\n    \&quot;text\&quot;: \&quot; language model should know and what the language model should look up and maybe what the language\&quot;\n  },\n  {\n    \&quot;end\&quot;: 987.6800000000001,\n    \&quot;start\&quot;: 982.88,\n    \&quot;text\&quot;: \&quot; model should not know or not purport to know. Do you have opinions about that? Yeah let's see\&quot;\n  },\n  {\n    \&quot;end\&quot;: 994.16,\n    \&quot;start\&quot;: 988.4,\n    \&quot;text\&quot;: \&quot; like some people are advocating for very small language models that have like no external knowledge\&quot;\n  },\n  {\n    \&quot;end\&quot;: 998.5600000000001,\n    \&quot;start\&quot;: 994.16,\n    \&quot;text\&quot;: \&quot; aside from language I guess would be the extreme position and then other people other people\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1002.5600000000001,\n    \&quot;start\&quot;: 998.5600000000001,\n    \&quot;text\&quot;: \&quot; talked about language models that just know everything as opposed to having an external knowledge\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1008.24,\n    \&quot;start\&quot;: 1002.5600000000001,\n    \&quot;text\&quot;: \&quot; source. There's some interesting questions there so I think it is a little hard to separate knowledge\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1015.6,\n    \&quot;start\&quot;: 1008.24,\n    \&quot;text\&quot;: \&quot; factual knowledge from understanding. So as humans we get by like not memorizing all sorts of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1021.6800000000001,\n    \&quot;start\&quot;: 1016.4,\n    \&quot;text\&quot;: \&quot; facts and just knowing that we can look them up if needed. For working on a specific domain it is\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1028.88,\n    \&quot;start\&quot;: 1021.6800000000001,\n    \&quot;text\&quot;: \&quot; useful to like have a lot of facts internalized so that you can recall them very quickly and\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1034.24,\n    \&quot;start\&quot;: 1028.88,\n    \&quot;text\&quot;: \&quot; kind of combine them combine them in your head. So I wouldn't take an extreme position on either\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1041.44,\n    \&quot;start\&quot;: 1034.24,\n    \&quot;text\&quot;: \&quot; side I would say I think retrieval is going to be really useful just at the very least for\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1048.88,\n    \&quot;start\&quot;: 1041.44,\n    \&quot;text\&quot;: \&quot; current events but also I don't think we want to try to pack all human knowledge into the weights\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1054.72,\n    \&quot;start\&quot;: 1048.88,\n    \&quot;text\&quot;: \&quot; of a neural net. On the other hand I think people have had a lot of luck just scaling up models and\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1061.44,\n    \&quot;start\&quot;: 1055.68,\n    \&quot;text\&quot;: \&quot; like as they soak up more factual knowledge they also get better at reasoning and other things\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1068,\n    \&quot;start\&quot;: 1061.44,\n    \&quot;text\&quot;: \&quot; and I think I haven't seen any demonstrations of tiny models that just do lots of retrieval\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1073.68,\n    \&quot;start\&quot;: 1068,\n    \&quot;text\&quot;: \&quot; and save all their weights for reasoning. Yeah I just haven't seen any evidence of this\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1078.8,\n    \&quot;start\&quot;: 1073.68,\n    \&quot;text\&quot;: \&quot; or that or I haven't seen any successful attempts at making this. Let's move on to training\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1084.16,\n    \&quot;start\&quot;: 1078.8,\n    \&quot;text\&quot;: \&quot; language models to follow instructions with human feedback that was uyang et al and that was 2022\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1088.72,\n    \&quot;start\&quot;: 1084.16,\n    \&quot;text\&quot;: \&quot; with yourself as a co-author. Can you tell us the main idea with this paper? This is the instruct\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1094.64,\n    \&quot;start\&quot;: 1088.72,\n    \&quot;text\&quot;: \&quot; GPT paper. What does instruct GPT and what's going on here? Instruct GPT is a language model that's\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1099.44,\n    \&quot;start\&quot;: 1094.64,\n    \&quot;text\&quot;: \&quot; fine tuned to follow instructions and it's in fact the one that you can play with if you go to\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1105.84,\n    \&quot;start\&quot;: 1100.08,\n    \&quot;text\&quot;: \&quot; the open AI website you get a big text box and you can write some text and then press the button\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1112.24,\n    \&quot;start\&quot;: 1105.84,\n    \&quot;text\&quot;: \&quot; to generate a completion. So the idea here was I mean language models are pretty useful and you can\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1117.84,\n    \&quot;start\&quot;: 1112.96,\n    \&quot;text\&quot;: \&quot; sometimes get them to do what you want by prompting them just right. This idea of few shot\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1123.52,\n    \&quot;start\&quot;: 1117.84,\n    \&quot;text\&quot;: \&quot; prompting has been become pretty popular where you give a few examples like a few question answer\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1128.3999999999999,\n    \&quot;start\&quot;: 1123.52,\n    \&quot;text\&quot;: \&quot; examples and then if you ask another question it'll hopefully provide an answer in the same style.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1133.84,\n    \&quot;start\&quot;: 1128.3999999999999,\n    \&quot;text\&quot;: \&quot; So the idea yeah so if you can get language models to do great things with prompting but prompting\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1139.04,\n    \&quot;start\&quot;: 1133.84,\n    \&quot;text\&quot;: \&quot; is itself an arg and it's tricky to get right and it's also kind of not necessarily getting the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1143.6799999999998,\n    \&quot;start\&quot;: 1139.04,\n    \&quot;text\&quot;: \&quot; best possible performance out of the model. If you just take a raw language model and you try to\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1148.4,\n    \&quot;start\&quot;: 1143.68,\n    \&quot;text\&quot;: \&quot; you try to talk to it like you ask it a question it probably it doesn't know that it should actually\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1154.0800000000002,\n    \&quot;start\&quot;: 1148.4,\n    \&quot;text\&quot;: \&quot; answer that question as well as possible. For all it knows you want it to give a joke answer or\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1160,\n    \&quot;start\&quot;: 1154.0800000000002,\n    \&quot;text\&quot;: \&quot; a riddle or something. Yeah so the idea of instruct GPT was let's make a kind of small change\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1164.4,\n    \&quot;start\&quot;: 1160,\n    \&quot;text\&quot;: \&quot; for our language models so that they're much easier to use. In particular we're going to train them\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1171.3600000000001,\n    \&quot;start\&quot;: 1164.4,\n    \&quot;text\&quot;: \&quot; to if you have a piece of text where there's an instruction the model will try to follow that\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1176.6399999999999,\n    \&quot;start\&quot;: 1171.36,\n    \&quot;text\&quot;: \&quot; instruction to the best of its abilities and pretty much anything can be an instruction like\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1183.04,\n    \&quot;start\&quot;: 1176.6399999999999,\n    \&quot;text\&quot;: \&quot; you can have a the instruction can be to continue a chat or it can be to like summarize like\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1190.8,\n    \&quot;start\&quot;: 1183.04,\n    \&quot;text\&quot;: \&quot; summarize this text or give me a list of names for my company that sells widgets. Yeah instructions\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1195.84,\n    \&quot;start\&quot;: 1190.8,\n    \&quot;text\&quot;: \&quot; can be anything and that makes that makes this kind of model very powerful. So that was kind of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1199.76,\n    \&quot;start\&quot;: 1195.84,\n    \&quot;text\&quot;: \&quot; that's the idea of an instruction following model it's like a model that can do anything that\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1204.16,\n    \&quot;start\&quot;: 1199.76,\n    \&quot;text\&quot;: \&quot; you specify with an instruction and by the way I wasn't a core contributor to this work I was\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1212.16,\n    \&quot;start\&quot;: 1204.8,\n    \&quot;text\&quot;: \&quot; more involved with like getting the RL infrastructure and some of the RL training details\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1218.24,\n    \&quot;start\&quot;: 1212.16,\n    \&quot;text\&quot;: \&quot; like helping out with that that stuff. But anyway yeah what we did in this project was we ran this\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1224,\n    \&quot;start\&quot;: 1218.24,\n    \&quot;text\&quot;: \&quot; this whole methodology that I just described of RL from even preferences in this instruction\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1230.32,\n    \&quot;start\&quot;: 1224,\n    \&quot;text\&quot;: \&quot; following setting. So we did supervised fine tuning, collected preference data, trained a reward\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1236.72,\n    \&quot;start\&quot;: 1230.32,\n    \&quot;text\&quot;: \&quot; model and then did RL against that reward model and one interesting detail is actually whereas the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1244.88,\n    \&quot;start\&quot;: 1236.72,\n    \&quot;text\&quot;: \&quot; original initial data was just collected using contractors. At a certain point we had the the API\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1252,\n    \&quot;start\&quot;: 1244.88,\n    \&quot;text\&quot;: \&quot; and it's got this I mean we have this playground on the website where this is where you the big\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1258.24,\n    \&quot;start\&quot;: 1252,\n    \&quot;text\&quot;: \&quot; text box where you can use the model. So we we took prompts that people that users had put into\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1264.56,\n    \&quot;start\&quot;: 1258.24,\n    \&quot;text\&quot;: \&quot; the into the playground and use those for training like both to collect preference data and to do RL.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1271.92,\n    \&quot;start\&quot;: 1264.56,\n    \&quot;text\&quot;: \&quot; So and this is like this is disclosed to users pretty prominently like when when people are using\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1276.88,\n    \&quot;start\&quot;: 1271.92,\n    \&quot;text\&quot;: \&quot; the playgrounds you get notified that your prompts might be used for the training and we're also\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1282.88,\n    \&quot;start\&quot;: 1276.88,\n    \&quot;text\&quot;: \&quot; careful to train in such a way that we don't memorize any information that was in in the prompts.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1288.5600000000002,\n    \&quot;start\&quot;: 1282.88,\n    \&quot;text\&quot;: \&quot; Like it and it explicit like we have a pretty like elaborate process for making sure there's no\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1295.0400000000002,\n    \&quot;start\&quot;: 1289.2800000000002,\n    \&quot;text\&quot;: \&quot; like private information being leaked into the model. But anyway yeah that's that's basically the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1302.16,\n    \&quot;start\&quot;: 1295.7600000000002,\n    \&quot;text\&quot;: \&quot; experimental setup and the result was that it works like this methodology works quite well and you\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1308.64,\n    \&quot;start\&quot;: 1302.16,\n    \&quot;text\&quot;: \&quot; get a model that's vastly preferred to the base model on this distribution of of realistic prompts\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1314.4,\n    \&quot;start\&quot;: 1308.64,\n    \&quot;text\&quot;: \&quot; that people are giving the model often which contain instructions. So the raw like the the raw\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1321.68,\n    \&quot;start\&quot;: 1314.4,\n    \&quot;text\&quot;: \&quot; language models generally do a really bad job following instructions but this RL trained instruction\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1328.0800000000002,\n    \&quot;start\&quot;: 1321.68,\n    \&quot;text\&quot;: \&quot; following model is is a lot better and it's something like if you just calculate how much better\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1333.6,\n    \&quot;start\&quot;: 1328.08,\n    \&quot;text\&quot;: \&quot; it's something like it's as good as a model that's a hundred times bigger. That's a lot. Yeah.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1337.36,\n    \&quot;start\&quot;: 1333.6,\n    \&quot;text\&quot;: \&quot; You wanted the model to be truthful is that is that one of the criteria you wanted?\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1342.1599999999999,\n    \&quot;start\&quot;: 1337.36,\n    \&quot;text\&quot;: \&quot; Oh yeah truthfulness was one of the criteria. That seems amazing to me that truthfulness is\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1346.32,\n    \&quot;start\&quot;: 1342.1599999999999,\n    \&quot;text\&quot;: \&quot; something that I could learn by example like does that mean that truthfulness is somehow\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1351.04,\n    \&quot;start\&quot;: 1346.32,\n    \&quot;text\&quot;: \&quot; represented inside the network or because there's no external way for the model to confirm\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1355.76,\n    \&quot;start\&quot;: 1351.04,\n    \&quot;text\&quot;: \&quot; whether something is true or false. So how how might it know what is what is true without any\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1362.24,\n    \&quot;start\&quot;: 1355.76,\n    \&quot;text\&quot;: \&quot; external reference? I think to some extent there is some internal representation of truthfulness.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1367.12,\n    \&quot;start\&quot;: 1362.24,\n    \&quot;text\&quot;: \&quot; So I would say like one way to think about what language models do is they're trained to imitate\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1371.52,\n    \&quot;start\&quot;: 1367.12,\n    \&quot;text\&quot;: \&quot; the whole internet and the internet is written by lots of different people and has lots of different\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1379.04,\n    \&quot;start\&quot;: 1371.52,\n    \&quot;text\&quot;: \&quot; types of content from fiction to nonfiction to like like technical like detailed technical literature\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1386.3999999999999,\n    \&quot;start\&quot;: 1379.04,\n    \&quot;text\&quot;: \&quot; to like jokes and like forum posts whatever. So what the model is basically an ensemble of all\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1392.8799999999999,\n    \&quot;start\&quot;: 1386.3999999999999,\n    \&quot;text\&quot;: \&quot; these people who wrote stuff on the internet the raw pre-trained model. When you feed it a prompt\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1398.08,\n    \&quot;start\&quot;: 1392.8799999999999,\n    \&quot;text\&quot;: \&quot; what it's doing internally has to be something like figuring out who wrote the first wrote this prompt\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1403.04,\n    \&quot;start\&quot;: 1398.08,\n    \&quot;text\&quot;: \&quot; and then trying to continue in that style. So if it thinks it's reading just reading something on the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1409.36,\n    \&quot;start\&quot;: 1403.04,\n    \&quot;text\&quot;: \&quot; Wall Street Betts Reddit it's going to continue on that style but if it thinks it's in the New\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1417.6,\n    \&quot;start\&quot;: 1409.36,\n    \&quot;text\&quot;: \&quot; York Times it's going to write in a very different way. So effectively the model must be like calculating\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1423.92,\n    \&quot;start\&quot;: 1417.6,\n    \&quot;text\&quot;: \&quot; somewhere like what style is this or what ensemble what's the like narrower ensemble of styles that\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1429.76,\n    \&quot;start\&quot;: 1423.92,\n    \&quot;text\&quot;: \&quot; I'm trying to imitate now. At the very least when you do some kind of when you do training like either\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1435.12,\n    \&quot;start\&quot;: 1429.76,\n    \&quot;text\&quot;: \&quot; supervised fine tuning or are all from human feedback you can at least like narrow down the set of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1442.56,\n    \&quot;start\&quot;: 1435.12,\n    \&quot;text\&quot;: \&quot; styles the model is producing and try to imitate like the best or the best person in the training set\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1448,\n    \&quot;start\&quot;: 1442.56,\n    \&quot;text\&quot;: \&quot; or the best style in the training set and obviously best will differ a lot. So what we'll end up with\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1453.68,\n    \&quot;start\&quot;: 1448,\n    \&quot;text\&quot;: \&quot; will depend on our instructions. So if we if we tell I don't know we'll end up with something that\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1462.16,\n    \&quot;start\&quot;: 1453.68,\n    \&quot;text\&quot;: \&quot; has kind of safe like not too not too controversial but a bit corporate will end up with something\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1468.8,\n    \&quot;start\&quot;: 1462.16,\n    \&quot;text\&quot;: \&quot; like that depending on what our instructions are. So at the very least like we can kind of narrow\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1474,\n    \&quot;start\&quot;: 1468.8,\n    \&quot;text\&quot;: \&quot; in on one style instead of having the whole distribution of styles on the internet. I think probably\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1479.52,\n    \&quot;start\&quot;: 1474,\n    \&quot;text\&quot;: \&quot; there's more to it than that like we're not just learning about style but the model probably is\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1485.04,\n    \&quot;start\&quot;: 1479.52,\n    \&quot;text\&quot;: \&quot; like internally trying to determine if things are if statements are true or not like if the prompt\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1490.6399999999999,\n    \&quot;start\&quot;: 1485.04,\n    \&quot;text\&quot;: \&quot; contains incorrect information because that probably would be useful for determining a likely\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1495.6,\n    \&quot;start\&quot;: 1490.6399999999999,\n    \&quot;text\&quot;: \&quot; completion. I'm just talking about the raw pre-trained model so I think yeah I think just the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1501.52,\n    \&quot;start\&quot;: 1495.6,\n    \&quot;text\&quot;: \&quot; objective of predicting next tokens probably gives you a lot it forces the model to like the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1506.8799999999999,\n    \&quot;start\&quot;: 1501.52,\n    \&quot;text\&quot;: \&quot; determine if things are true or not. I think for our alfine tuning there's a lot more potential\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1513.1200000000001,\n    \&quot;start\&quot;: 1506.88,\n    \&quot;text\&quot;: \&quot; for the model to actually like try to output something truthful as opposed to trying to imitate\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1519.1200000000001,\n    \&quot;start\&quot;: 1513.1200000000001,\n    \&quot;text\&quot;: \&quot; a certain style though it's hard to I guess it would be hard to like determine if that's what the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1524.4,\n    \&quot;start\&quot;: 1519.1200000000001,\n    \&quot;text\&quot;: \&quot; model is actually trying to do. So it's almost like the the prompt is guiding the model it's like\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1529.5200000000002,\n    \&quot;start\&quot;: 1524.4,\n    \&quot;text\&quot;: \&quot; what corner of the internet do we want to do we want to imitate here and maybe we want to\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1534.0800000000002,\n    \&quot;start\&quot;: 1529.5200000000002,\n    \&quot;text\&quot;: \&quot; instruct GPG wants to to focus more on the most more truthful corners of the internet\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1539.1999999999998,\n    \&quot;start\&quot;: 1534.08,\n    \&quot;text\&quot;: \&quot; something similar to that. Yeah I would hope so at least I think that's a pretty good though maybe\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1543.1999999999998,\n    \&quot;start\&quot;: 1539.1999999999998,\n    \&quot;text\&quot;: \&quot; a little simplistic picture of what's going on. At the very least we should be able to imitate\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1549.36,\n    \&quot;start\&quot;: 1543.1999999999998,\n    \&quot;text\&quot;: \&quot; the most truthful corner of the internet. So can you talk about a generalization and how does\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1554.56,\n    \&quot;start\&quot;: 1549.36,\n    \&quot;text\&quot;: \&quot; this type of model perform out of distribution? Like I guess if it seems questions that are a bit\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1558.3999999999999,\n    \&quot;start\&quot;: 1554.56,\n    \&quot;text\&quot;: \&quot; different than what it was trained on. What happens if we get a little bit away from the training\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1563.84,\n    \&quot;start\&quot;: 1558.3999999999999,\n    \&quot;text\&quot;: \&quot; data with the reward models? I mean language models in general generalize surprisingly well and\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1568.8,\n    \&quot;start\&quot;: 1563.84,\n    \&quot;text\&quot;: \&quot; I would say overall like these pre-trained models that are trained on super diverse data sets\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1573.84,\n    \&quot;start\&quot;: 1568.8,\n    \&quot;text\&quot;: \&quot; from the internet. They tend to generalize quite well or surprisingly well at least it's surprising\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1580.72,\n    \&quot;start\&quot;: 1573.84,\n    \&quot;text\&quot;: \&quot; to those of us who were around for the earlier days of machine learning when everything was\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1586.08,\n    \&quot;start\&quot;: 1580.72,\n    \&quot;text\&quot;: \&quot; trained from scratch and very fragile. For example if you ask if you provide an instruction in some\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1591.28,\n    \&quot;start\&quot;: 1586.08,\n    \&quot;text\&quot;: \&quot; other language even a even a fairly rare language it'll often do a decent job following the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1597.36,\n    \&quot;start\&quot;: 1591.28,\n    \&quot;text\&quot;: \&quot; instruction even if there's zero data in the whole instruction following the training process\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1603.84,\n    \&quot;start\&quot;: 1597.92,\n    \&quot;text\&quot;: \&quot; that's in that language and that's just to carry over from the pre-training. So I think generalization\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1608.16,\n    \&quot;start\&quot;: 1603.84,\n    \&quot;text\&quot;: \&quot; yeah I think language models generalize quite well. So you asked about reward models I think one\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1614.08,\n    \&quot;start\&quot;: 1608.16,\n    \&quot;text\&quot;: \&quot; of the tricky pieces about RL from human feedback is how so you have this reward model and you're\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1618.6399999999999,\n    \&quot;start\&quot;: 1614.08,\n    \&quot;text\&quot;: \&quot; actually training against it meaning you're training your policy to have high reward and it's\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1623.6000000000001,\n    \&quot;start\&quot;: 1618.64,\n    \&quot;text\&quot;: \&quot; going to exploit the errors in the reward model so it's going to eventually find adversarial\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1628.72,\n    \&quot;start\&quot;: 1623.6000000000001,\n    \&quot;text\&quot;: \&quot; examples to the reward model. This is worse than kind of normal out of distribution behavior it's\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1634.0800000000002,\n    \&quot;start\&quot;: 1628.72,\n    \&quot;text\&quot;: \&quot; like targeted out of distribution examples so so there are definitely some challenges around\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1640.8000000000002,\n    \&quot;start\&quot;: 1634.8000000000002,\n    \&quot;text\&quot;: \&quot; getting reward models to generalize well or generalize as far as possible from the training set.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1645.6000000000001,\n    \&quot;start\&quot;: 1640.8000000000002,\n    \&quot;text\&quot;: \&quot; Can these types of agents tell us when they don't know something or is that a hard problem?\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1651.9199999999998,\n    \&quot;start\&quot;: 1645.6,\n    \&quot;text\&quot;: \&quot; I'd say sort of if you ask a question that's kind of in the core of the model's knowledge it will\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1656,\n    \&quot;start\&quot;: 1651.9199999999998,\n    \&quot;text\&quot;: \&quot; know know the answer and it'll know that it knows. By the way I'm talking about models like the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1661.6,\n    \&quot;start\&quot;: 1656,\n    \&quot;text\&quot;: \&quot; for the instruct model if you ask it about something that's like very simple at the core of its\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1666.08,\n    \&quot;start\&quot;: 1661.6,\n    \&quot;text\&quot;: \&quot; knowledge it'll know if you there are certain things that it knows that it doesn't know like\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1672.8799999999999,\n    \&quot;start\&quot;: 1666.7199999999998,\n    \&quot;text\&quot;: \&quot; current events where it's been trained to know that it doesn't know certain things in real time but\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1678.16,\n    \&quot;start\&quot;: 1672.88,\n    \&quot;text\&quot;: \&quot; if you ask it about something that's kind of on the edge of its knowledge it's it's going to have a\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1682.96,\n    \&quot;start\&quot;: 1678.16,\n    \&quot;text\&quot;: \&quot; hard time it's it's necessarily going to be inaccurate. I mean there have been a couple papers\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1689.2800000000002,\n    \&quot;start\&quot;: 1683.7600000000002,\n    \&quot;text\&quot;: \&quot; about this question so there is in paper from Anthropic recently called language models\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1695.2800000000002,\n    \&quot;start\&quot;: 1689.2800000000002,\n    \&quot;text\&quot;: \&quot; mostly know what they know and there is also a paper from FHI and OpenAI called\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1700.48,\n    \&quot;start\&quot;: 1696.5600000000002,\n    \&quot;text\&quot;: \&quot; getting language models to express their uncertainty and words. These language\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1706.32,\n    \&quot;start\&quot;: 1700.48,\n    \&quot;text\&quot;: \&quot; models as well as a lot of other models in machine learning are training to maximize likelihood\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1711.6,\n    \&quot;start\&quot;: 1706.32,\n    \&quot;text\&quot;: \&quot; so maximize log-prob of data. You're already training them to always predict a distribution of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1718.8,\n    \&quot;start\&quot;: 1711.6,\n    \&quot;text\&quot;: \&quot; outputs. So for language models given a prefix it's predicting a distribution over the next token.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1725.76,\n    \&quot;start\&quot;: 1718.8,\n    \&quot;text\&quot;: \&quot; These predictions for the next token like generally are pretty well calibrated but 80% if it puts 80%\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1731.6,\n    \&quot;start\&quot;: 1725.76,\n    \&quot;text\&quot;: \&quot; probability on something and you look at all the times when it puts 80% probability on something\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1736.72,\n    \&quot;start\&quot;: 1731.6,\n    \&quot;text\&quot;: \&quot; like it's right 80% of the time. Like that's just a result of the training objective. The training\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1742.96,\n    \&quot;start\&quot;: 1736.72,\n    \&quot;text\&quot;: \&quot; objective like strongly incentivizes the model to be calibrated meaning it has a reasonable\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1748.8,\n    \&quot;start\&quot;: 1742.96,\n    \&quot;text\&quot;: \&quot; estimate of its uncertainty. So at the single token level models definitely are calibrated.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1754.8,\n    \&quot;start\&quot;: 1748.8,\n    \&quot;text\&quot;: \&quot; The question is whether they're calibrated on whether this calibration extends to settings where\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1760.6399999999999,\n    \&quot;start\&quot;: 1754.8,\n    \&quot;text\&quot;: \&quot; they are generating multi-token outputs or whether they can judge the correctness of some\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1766,\n    \&quot;start\&quot;: 1760.6399999999999,\n    \&quot;text\&quot;: \&quot; multi-token statement. So I would say since models are calibrated at the single token level\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1772.6399999999999,\n    \&quot;start\&quot;: 1766.56,\n    \&quot;text\&quot;: \&quot; they I think they definitely have the information to be calibrated in these other settings.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1778.48,\n    \&quot;start\&quot;: 1772.6399999999999,\n    \&quot;text\&quot;: \&quot; So that's why I think the problem of models knowing what they know isn't actually that hard\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1783.9199999999998,\n    \&quot;start\&quot;: 1778.48,\n    \&quot;text\&quot;: \&quot; or at least getting a model to express its uncertainty pretty much as well as a human does\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1788.88,\n    \&quot;start\&quot;: 1783.92,\n    \&quot;text\&quot;: \&quot; doesn't feel like an insurmountable problem but there's some practical difficulties to getting\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1793.92,\n    \&quot;start\&quot;: 1788.88,\n    \&quot;text\&quot;: \&quot; getting there. People use the phrase AI alignment in different ways. Can you talk about how you see\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1800.0800000000002,\n    \&quot;start\&quot;: 1793.92,\n    \&quot;text\&quot;: \&quot; alignment in your work on Aral from human feedback? I think of alignment mostly as the problem of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1805.28,\n    \&quot;start\&quot;: 1800.0800000000002,\n    \&quot;text\&quot;: \&quot; getting the model to try to do the right thing so we can kind of make a distinction between\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1811.28,\n    \&quot;start\&quot;: 1805.92,\n    \&quot;text\&quot;: \&quot; what the model is capable of doing. Like if you just take a raw language model and you ask\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1815.76,\n    \&quot;start\&quot;: 1811.28,\n    \&quot;text\&quot;: \&quot; it a question like I said before it doesn't know that you actually wanted to give the correct answer\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1821.68,\n    \&quot;start\&quot;: 1815.76,\n    \&quot;text\&quot;: \&quot; as opposed to. It might think someone who is not very knowledgeable is answering. By doing some\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1826,\n    \&quot;start\&quot;: 1821.68,\n    \&quot;text\&quot;: \&quot; extra training we can get the model to actually try to do the right thing and so I would say that\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1832.16,\n    \&quot;start\&quot;: 1826,\n    \&quot;text\&quot;: \&quot; that's the main goal of alignment. So there was an open AI blog post recently that talked about\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1840.24,\n    \&quot;start\&quot;: 1832.16,\n    \&quot;text\&quot;: \&quot; the sequence in alignment. One was training AI systems using human feedback to use it training AI\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1846,\n    \&quot;start\&quot;: 1840.24,\n    \&quot;text\&quot;: \&quot; systems to assist human evaluation and three training AI systems to do alignment research.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1852,\n    \&quot;start\&quot;: 1846,\n    \&quot;text\&quot;: \&quot; So is your current work mostly about this first item and when and how do you see us getting to\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1858.4,\n    \&quot;start\&quot;: 1852,\n    \&quot;text\&quot;: \&quot; these other stages? I'm doing some work now on number two training AI systems to assist human feedback.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1865.04,\n    \&quot;start\&quot;: 1858.4,\n    \&quot;text\&quot;: \&quot; I think that's sort of becomes increasingly necessary as you start trying to get the systems\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1869.44,\n    \&quot;start\&quot;: 1865.04,\n    \&quot;text\&quot;: \&quot; to solve harder and harder problems. When you have models that are kind of very below human level\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1875.6000000000001,\n    \&quot;start\&quot;: 1869.44,\n    \&quot;text\&quot;: \&quot; or maybe at human level at a certain task it's pretty straightforward to supervise them. But\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1880.4,\n    \&quot;start\&quot;: 1875.6000000000001,\n    \&quot;text\&quot;: \&quot; once they're doing things that are very hard or doing things that require a lot of diverse\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1886.96,\n    \&quot;start\&quot;: 1880.4,\n    \&quot;text\&quot;: \&quot; technical knowledge it becomes pretty hard to provide a useful supervision signal. So we have to\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1893.3600000000001,\n    \&quot;start\&quot;: 1886.96,\n    \&quot;text\&quot;: \&quot; start doing things like one model writes an answer to do a question and then another model provides\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1900.9599999999998,\n    \&quot;start\&quot;: 1893.36,\n    \&quot;text\&quot;: \&quot; a critique of that answer points out some flaws and then the human only has to judge the first answer\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1906.7199999999998,\n    \&quot;start\&quot;: 1900.9599999999998,\n    \&quot;text\&quot;: \&quot; after looking at the critique meaning basically the critique helps the human assess the answer. So I\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1912.1599999999999,\n    \&quot;start\&quot;: 1906.7199999999998,\n    \&quot;text\&quot;: \&quot; think like that kind of idea is starting to become pretty relevant. A colleague's an I are exploring\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1917.28,\n    \&quot;start\&quot;: 1912.1599999999999,\n    \&quot;text\&quot;: \&quot; that kind of idea now. As for assisting alignment research there's some other work at open AI that's\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1923.1999999999998,\n    \&quot;start\&quot;: 1917.28,\n    \&quot;text\&quot;: \&quot; starting to explore this. It's also that sort of the for this down the road. So I saw Stuart Russell\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1928.96,\n    \&quot;start\&quot;: 1923.2,\n    \&quot;text\&quot;: \&quot; was on your PhD committee and I really enjoyed his book Human Compatible. I wonder if you share\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1933.04,\n    \&quot;start\&quot;: 1928.96,\n    \&quot;text\&quot;: \&quot; the idea mentioned in the book that the standard RL framing with this fixed reward signal\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1939.8400000000001,\n    \&quot;start\&quot;: 1933.8400000000001,\n    \&quot;text\&quot;: \&quot; is problematic and that agents powerful agents should try to do what we want and maintain some\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1945.68,\n    \&quot;start\&quot;: 1939.8400000000001,\n    \&quot;text\&quot;: \&quot; uncertainty about what it is we want and the agents that are too certain will be problematic.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1952.56,\n    \&quot;start\&quot;: 1945.68,\n    \&quot;text\&quot;: \&quot; What do you have any thoughts on that idea? I totally agree with that idea. So I think first it's\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1959.36,\n    \&quot;start\&quot;: 1952.56,\n    \&quot;text\&quot;: \&quot; really hard to write down a simple reward function that actually captures what we want or what any\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1964.96,\n    \&quot;start\&quot;: 1959.36,\n    \&quot;text\&quot;: \&quot; any particular person wants. I can say I want a little more of this or a little more of that but\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1971.2,\n    \&quot;start\&quot;: 1965.6,\n    \&quot;text\&quot;: \&quot; you wouldn't want to take that to the extreme. If we build agents that try to cater to our to our\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1978.6399999999999,\n    \&quot;start\&quot;: 1971.2,\n    \&quot;text\&quot;: \&quot; wishes we should make sure they're like they have a lot of they have uncertainty about what we\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1984.64,\n    \&quot;start\&quot;: 1978.64,\n    \&quot;text\&quot;: \&quot; want or what we value and that that'll also cause them to be a little more cautious and say\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1991.92,\n    \&quot;start\&quot;: 1984.64,\n    \&quot;text\&quot;: \&quot; not disturb anything that might be important to us. So yeah I agree with that like Stuart Russell\&quot;\n  },\n  {\n    \&quot;end\&quot;: 1998.3200000000002,\n    \&quot;start\&quot;: 1991.92,\n    \&quot;text\&quot;: \&quot; gave a very good like problem definition of what we want AI to do like we want it to basically\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2003.6000000000001,\n    \&quot;start\&quot;: 1998.3200000000002,\n    \&quot;text\&quot;: \&quot; we want to jointly like play this game where AI is the AI is trying to figure out what we want\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2008.4,\n    \&quot;start\&quot;: 2003.6000000000001,\n    \&quot;text\&quot;: \&quot; and then trying to do that but simultaneously maintaining some uncertainty about what we want.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2013.0400000000002,\n    \&quot;start\&quot;: 2008.4,\n    \&quot;text\&quot;: \&quot; I would say if you you start to look at how to get that in practice it actually looks quite a bit\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2019.76,\n    \&quot;start\&quot;: 2013.0400000000002,\n    \&quot;text\&quot;: \&quot; like the kind of RL from human feedback that we're working on at OpenAI and others are working on\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2027.8400000000001,\n    \&quot;start\&quot;: 2019.76,\n    \&quot;text\&quot;: \&quot; other places. I think yeah I think I see what we're doing as a practical implementation of getting\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2033.2,\n    \&quot;start\&quot;: 2027.8400000000001,\n    \&quot;text\&quot;: \&quot; towards this behavior that Russell have described. Do you think of a AGI as an abstract goal or\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2037.44,\n    \&quot;start\&quot;: 2033.2,\n    \&quot;text\&quot;: \&quot; are we going to see a model come out one day and people are going to say oh that's the first AGI\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2044,\n    \&quot;start\&quot;: 2037.44,\n    \&quot;text\&quot;: \&quot; model like what does it have to do for people to say that? I think people will say that many times\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2049.52,\n    \&quot;start\&quot;: 2044.72,\n    \&quot;text\&quot;: \&quot; then realize that it doesn't quite do everything that you want. I think we're going to have a lot of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2055.44,\n    \&quot;start\&quot;: 2049.52,\n    \&quot;text\&quot;: \&quot; like a long series of models that are that are superhuman at most things or at a certain class of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2064.08,\n    \&quot;start\&quot;: 2055.44,\n    \&quot;text\&quot;: \&quot; things but they also have some failure modes and weaknesses. Like I expect us to like see multiple\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2070.96,\n    \&quot;start\&quot;: 2064.08,\n    \&quot;text\&quot;: \&quot; models that are proclaimed as AGI and then only after interacting with it a while you do realize\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2078,\n    \&quot;start\&quot;: 2070.96,\n    \&quot;text\&quot;: \&quot; it's not quite there. What would you say is the relationship between AGI and RL and AGI and\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2084.96,\n    \&quot;start\&quot;: 2078,\n    \&quot;text\&quot;: \&quot; these large language models? How do those concepts fit together? I would say that RL is a useful\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2090.96,\n    \&quot;start\&quot;: 2084.96,\n    \&quot;text\&quot;: \&quot; like component of training AGI or an almost essential component. The thing RL lets you do is it\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2098.48,\n    \&quot;start\&quot;: 2090.96,\n    \&quot;text\&quot;: \&quot; lets you optimize any objective for the agents. Any objective that is a function of the agents\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2105.04,\n    \&quot;start\&quot;: 2098.48,\n    \&quot;text\&quot;: \&quot; behavior. So with pre-training like what we do for language models you're kind of choosing an\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2111.04,\n    \&quot;start\&quot;: 2105.04,\n    \&quot;text\&quot;: \&quot; objective that lets us do something with all the training day we have which is all this internet\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2116.4,\n    \&quot;start\&quot;: 2111.04,\n    \&quot;text\&quot;: \&quot; text. So we choose this maximum likelihood objective which is basically the only or not the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2122.4,\n    \&quot;start\&quot;: 2116.4,\n    \&quot;text\&quot;: \&quot; only thing but it's like a sensible way to absorb all this knowledge. But then if we really want to\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2128.32,\n    \&quot;start\&quot;: 2122.4,\n    \&quot;text\&quot;: \&quot; optimize the agents behavior for a specific objective RL is kind of the only framework that lets you\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2133.12,\n    \&quot;start\&quot;: 2128.32,\n    \&quot;text\&quot;: \&quot; do that. Okay John we have a few questions from the audience and I'm just going to pick the two\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2139.36,\n    \&quot;start\&quot;: 2133.12,\n    \&quot;text\&quot;: \&quot; that have the highest score in terms of Twitter likes. So the first is from Eric Chang VP of AI\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2144.48,\n    \&quot;start\&quot;: 2139.36,\n    \&quot;text\&quot;: \&quot; at a Hello Di Robotics. He asked RL distributions are non-stationary making it hard to reason about\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2149.92,\n    \&quot;start\&quot;: 2144.48,\n    \&quot;text\&quot;: \&quot; PPO losses and how that relates to return or generalization. Are there any intermediate plots\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2156,\n    \&quot;start\&quot;: 2149.92,\n    \&quot;text\&quot;: \&quot; and visualizations you like to generate to debug or incrementally build up a large scale RL system?\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2163.2,\n    \&quot;start\&quot;: 2156,\n    \&quot;text\&quot;: \&quot; Yeah there are definitely some stats that I look at so I will be I'll talk about this in the nuts\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2172.2400000000002,\n    \&quot;start\&quot;: 2163.2,\n    \&quot;text\&quot;: \&quot; and bolts like reboot waited a year but I'd say things like you're looking at the explained\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2177.3599999999997,\n    \&quot;start\&quot;: 2172.24,\n    \&quot;text\&quot;: \&quot; variants of the value function and looking at the like how many samples are getting clipped in\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2183.3599999999997,\n    \&quot;start\&quot;: 2177.3599999999997,\n    \&quot;text\&quot;: \&quot; PPO and what the KL between the what what the KL divergence is between the policy before and after\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2191.3599999999997,\n    \&quot;start\&quot;: 2183.3599999999997,\n    \&quot;text\&quot;: \&quot; the update is yeah things like that. And then Ethan the calibar from Miele asks what is your median\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2198,\n    \&quot;start\&quot;: 2191.3599999999997,\n    \&quot;text\&quot;: \&quot; estimate for the arrival date of AGI? I think not too far away but I like I said I expect there to\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2205.12,\n    \&quot;start\&quot;: 2198,\n    \&quot;text\&quot;: \&quot; be a lot of fall starts I would say expect like like AI to be able to do better a better job than\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2211.36,\n    \&quot;start\&quot;: 2205.12,\n    \&quot;text\&quot;: \&quot; humans at most jobs that humans do now five years or so that's not all jobs but most jobs for a while\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2215.84,\n    \&quot;start\&quot;: 2211.36,\n    \&quot;text\&quot;: \&quot; we're going to discover things that AI isn't very good at and then where we want to keep humans in\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2221.12,\n    \&quot;start\&quot;: 2215.84,\n    \&quot;text\&quot;: \&quot; control so I think there'll be some kind of gradual process over the next 10 or 15 years.\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2227.2,\n    \&quot;start\&quot;: 2221.12,\n    \&quot;text\&quot;: \&quot; I've been curious about this I see that some RL work is patented but I could not find a TRPO or\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2234.3999999999996,\n    \&quot;start\&quot;: 2227.2,\n    \&quot;text\&quot;: \&quot; PPO in I could not find patents on these are those protected patent protected at all or how do you\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2240.56,\n    \&quot;start\&quot;: 2234.3999999999996,\n    \&quot;text\&quot;: \&quot; how do you think of intellectual property protection for that kind of work? I haven't ever looked\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2246.16,\n    \&quot;start\&quot;: 2240.56,\n    \&quot;text\&quot;: \&quot; looked into patenting anything and open AI hasn't either as far as I know I think the trend over time\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2251.12,\n    \&quot;start\&quot;: 2246.16,\n    \&quot;text\&quot;: \&quot; has been for people to take a patent scene machine like a machine learning algorithms last\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2256,\n    \&quot;start\&quot;: 2251.12,\n    \&quot;text\&quot;: \&quot; seriously there is this algorithm in computer vision called sift which is like this key point\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2262.88,\n    \&quot;start\&quot;: 2256,\n    \&quot;text\&quot;: \&quot; to detector and this was patented I think the the guy who patented it he probably made his\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2268.88,\n    \&quot;start\&quot;: 2262.88,\n    \&quot;text\&quot;: \&quot; university some money from the patent but in the end all it did was cause people a lot of annoyance\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2275.28,\n    \&quot;start\&quot;: 2268.88,\n    \&quot;text\&quot;: \&quot; because like the people people had to come up with alternative algorithms that like had a\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2282.72,\n    \&quot;start\&quot;: 2275.28,\n    \&quot;text\&quot;: \&quot; different acronym and weren't patented so like the open CV open source library would have like\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2288.08,\n    \&quot;start\&quot;: 2282.72,\n    \&quot;text\&quot;: \&quot; had to be careful about putting this algorithm in their library because of the patent risks so\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2294.7999999999997,\n    \&quot;start\&quot;: 2288.64,\n    \&quot;text\&quot;: \&quot; I think like these patents aren't the patent rights aren't exercise that much and I think big\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2301.2,\n    \&quot;start\&quot;: 2294.7999999999997,\n    \&quot;text\&quot;: \&quot; companies like Google will patent a lot of stuff for defensive reasons so if they get in some big\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2307.52,\n    \&quot;start\&quot;: 2301.2,\n    \&quot;text\&quot;: \&quot; legal dispute with another company it can be used as like one of the bargaining chips but I think\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2315.12,\n    \&quot;start\&quot;: 2307.52,\n    \&quot;text\&quot;: \&quot; I don't think anyone's going to like get sued for royalties for not yeah for not providing royalties\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2320.24,\n    \&quot;start\&quot;: 2315.12,\n    \&quot;text\&quot;: \&quot; for the use of some algorithm okay and then there's been a ton of work in RL of course since you\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2326.4,\n    \&quot;start\&quot;: 2320.24,\n    \&quot;text\&quot;: \&quot; first published TRPO and BBO but from your point of view if you had to pick a few highlights in\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2333.52,\n    \&quot;start\&quot;: 2326.4,\n    \&quot;text\&quot;: \&quot; terms of a few important milestones in in RL algorithms since PPO came out and by the way it's\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2341.2,\n    \&quot;start\&quot;: 2333.52,\n    \&quot;text\&quot;: \&quot; amazing that in 2022 we're still using PPO I think quite similar into it's original form is that\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2347.44,\n    \&quot;start\&quot;: 2341.2,\n    \&quot;text\&quot;: \&quot; right yeah pretty much yeah so so what would you say are the the biggest highlights for you\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2352.96,\n    \&quot;start\&quot;: 2348.4,\n    \&quot;text\&quot;: \&quot; in terms of our algorithm since since you did PPO yeah there's definitely been some interesting\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2361.6,\n    \&quot;start\&quot;: 2352.96,\n    \&quot;text\&quot;: \&quot; stuff so I think like a little after PPO there is TD3 and SAC and those are seem like pretty solid\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2366.96,\n    \&quot;start\&quot;: 2361.6,\n    \&quot;text\&quot;: \&quot; value-based methods that was one development that was interesting I think like yeah I thought\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2375.36,\n    \&quot;start\&quot;: 2366.96,\n    \&quot;text\&quot;: \&quot; museiro and it's and it's like elaborations we're also like efficient zero we're also pretty\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2380.72,\n    \&quot;start\&quot;: 2375.36,\n    \&quot;text\&quot;: \&quot; impressive that you can get that good sample efficiency both of the things I just mentioned were\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2386.7999999999997,\n    \&quot;start\&quot;: 2380.72,\n    \&quot;text\&quot;: \&quot; kind of well I don't want to say mostly on toy tasks or benchmarks because yeah I'm sure people\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2391.92,\n    \&quot;start\&quot;: 2386.8,\n    \&quot;text\&quot;: \&quot; are doing some real things with these algorithms yeah so I think that's that stuff was interesting\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2400.2400000000002,\n    \&quot;start\&quot;: 2391.92,\n    \&quot;text\&quot;: \&quot; I think like the whole recent interest in search of interest in the offline RL was also notable\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2405.28,\n    \&quot;start\&quot;: 2400.2400000000002,\n    \&quot;text\&quot;: \&quot; I would say the like the stuff we're doing with RL from human feedback is the kind of offline RL\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2411.76,\n    \&quot;start\&quot;: 2405.92,\n    \&quot;text\&quot;: \&quot; because we're like we have a fixed dataset and we have a fixed reward modeling dataset and we're\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2416.2400000000002,\n    \&quot;start\&quot;: 2411.76,\n    \&quot;text\&quot;: \&quot; training against that this is like offline RL but you're doing it in a different way you're using\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2423.2,\n    \&quot;start\&quot;: 2416.24,\n    \&quot;text\&quot;: \&quot; an on-policy algorithm with a reward model as opposed to maybe a more typical way to do offline RL\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2427.9199999999996,\n    \&quot;start\&quot;: 2423.2,\n    \&quot;text\&quot;: \&quot; would be use off-policy algorithm would that work here or would that not work here well we're\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2434.3999999999996,\n    \&quot;start\&quot;: 2427.9199999999996,\n    \&quot;text\&quot;: \&quot; doing here is kind of like model-based RL because the reward model is like a model of the like the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2440.56,\n    \&quot;start\&quot;: 2434.3999999999996,\n    \&quot;text\&quot;: \&quot; unknown part of the system so like the unknown part of the system here is the is the human\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2448.08,\n    \&quot;start\&quot;: 2440.56,\n    \&quot;text\&quot;: \&quot; radar or the human it's not the outputting appending to your list of tokens so this is kind of like\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2454.48,\n    \&quot;start\&quot;: 2448.08,\n    \&quot;text\&quot;: \&quot; the work that's like takes a dynamics model at the environment and does some kind of just runs a\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2459.44,\n    \&quot;start\&quot;: 2454.48,\n    \&quot;text\&quot;: \&quot; policy grading algorithm against it so it's not like so the idea of running an online algorithm\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2465.52,\n    \&quot;start\&quot;: 2460.08,\n    \&quot;text\&quot;: \&quot; against a model that's kind of a well-established idea so I would say the papers that previously\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2470.72,\n    \&quot;start\&quot;: 2465.52,\n    \&quot;text\&quot;: \&quot; did this they were in a pretty different regime were in this regime of doing fairly small\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2476.24,\n    \&quot;start\&quot;: 2470.72,\n    \&quot;text\&quot;: \&quot; updates to the policy because we have this these awesome pre-trained models and we don't need to\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2482.56,\n    \&quot;start\&quot;: 2476.24,\n    \&quot;text\&quot;: \&quot; actually change them that much so yeah we use these online algorithms I'd say part of the reason\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2490.4,\n    \&quot;start\&quot;: 2482.56,\n    \&quot;text\&quot;: \&quot; why we can get away with using just an like an online algorithm is because we've been just looking\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2495.52,\n    \&quot;start\&quot;: 2490.4,\n    \&quot;text\&quot;: \&quot; at a band a contextual banded problem yeah because we only have like one time step like you get\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2501.52,\n    \&quot;start\&quot;: 2495.52,\n    \&quot;text\&quot;: \&quot; a query and you output a response and then that response gets a reward so if we had a like a\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2509.04,\n    \&quot;start\&quot;: 2501.52,\n    \&quot;text\&quot;: \&quot; multi-step process such as a conversation where you can't assign a reward until the very end of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2516,\n    \&quot;start\&quot;: 2509.04,\n    \&quot;text\&quot;: \&quot; the conversation and or you had some I don't know some interaction with like some real-world\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2520.64,\n    \&quot;start\&quot;: 2516,\n    \&quot;text\&quot;: \&quot; system that's hard to simulate you wouldn't then it wouldn't be S-ray forward to you wouldn't\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2526.08,\n    \&quot;start\&quot;: 2520.64,\n    \&quot;text\&quot;: \&quot; be able to use exactly exactly the same methodology you would probably have to use a you would have\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2532.24,\n    \&quot;start\&quot;: 2526.08,\n    \&quot;text\&quot;: \&quot; to probably train a Q function or or something like that if you want if you want your method to be\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2536.4,\n    \&quot;start\&quot;: 2532.24,\n    \&quot;text\&quot;: \&quot; sample efficient you would probably have to do something slightly different I think we'll we'll\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2542.88,\n    \&quot;start\&quot;: 2536.4,\n    \&quot;text\&quot;: \&quot; have to we'll have to start exploring this at some point soon but so far we haven't at least\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2550.48,\n    \&quot;start\&quot;: 2542.88,\n    \&quot;text\&quot;: \&quot; I haven't seen any cases in like in the domain I'm looking at that require this but I expect it to\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2556.96,\n    \&quot;start\&quot;: 2551.44,\n    \&quot;text\&quot;: \&quot; to be relevant at some point so we had Arvind Shrinivas talking about decision transformer\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2561.76,\n    \&quot;start\&quot;: 2556.96,\n    \&quot;text\&quot;: \&quot; on the show recently that was a great episode and I see that you were also a co-author on the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2565.92,\n    \&quot;start\&quot;: 2561.76,\n    \&quot;text\&quot;: \&quot; the 2016 RL squared paper I want to ask you what your thoughts about meta RL\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2571.28,\n    \&quot;start\&quot;: 2566.6400000000003,\n    \&quot;text\&quot;: \&quot; Arvind had some interesting things to say about maybe the idea that a transformer could kind of\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2575.92,\n    \&quot;start\&quot;: 2571.28,\n    \&quot;text\&quot;: \&quot; supersede the need for an RL algorithm altogether what do you expect from meta RL\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2581.36,\n    \&quot;start\&quot;: 2575.92,\n    \&quot;text\&quot;: \&quot; do expect will will still be using human authored RL algorithms in the future yeah that's a pretty\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2586.6400000000003,\n    \&quot;start\&quot;: 2581.36,\n    \&quot;text\&quot;: \&quot; bold statement that we don't need we won't need any RL algorithms anymore yeah since the RL squared\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2593.0400000000004,\n    \&quot;start\&quot;: 2586.6400000000003,\n    \&quot;text\&quot;: \&quot; paper people have been talking less about meta learning as far as I can tell actually because\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2599.28,\n    \&quot;start\&quot;: 2593.0400000000004,\n    \&quot;text\&quot;: \&quot; of sequence modeling has gotten so good like transformer let sequence models so that it's kind\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2604.2400000000002,\n    \&quot;start\&quot;: 2599.28,\n    \&quot;text\&quot;: \&quot; of queer the meta learning is just a special case of learning like it's it's just it's just like\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2610.0800000000004,\n    \&quot;start\&quot;: 2604.2400000000002,\n    \&quot;text\&quot;: \&quot; a certain kind of long context learning learning involving long episodes and maybe it shouldn't be\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2615.36,\n    \&quot;start\&quot;: 2610.0800000000004,\n    \&quot;text\&quot;: \&quot; treated that differently or are addressed with special algorithms I would say yeah the ideas like\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2620.6400000000003,\n    \&quot;start\&quot;: 2615.36,\n    \&quot;text\&quot;: \&quot; decision transformer are pretty interesting where you try to reduce RL to supervise learning it's\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2626.0800000000004,\n    \&quot;start\&quot;: 2620.6400000000003,\n    \&quot;text\&quot;: \&quot; still not like certain exactly how these compare and performance to RL like people have started to\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2633.04,\n    \&quot;start\&quot;: 2626.08,\n    \&quot;text\&quot;: \&quot; analyze that empirically and theoretically and I would say in practice sometimes sometimes it's\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2638.48,\n    \&quot;start\&quot;: 2633.04,\n    \&quot;text\&quot;: \&quot; better sometimes it's worse in my experience like it's been worse on the problems that I've\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2644.56,\n    \&quot;start\&quot;: 2638.48,\n    \&quot;text\&quot;: \&quot; that I've my colleagues and I have where we've tested it but yeah it's definitely an interesting\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2649.12,\n    \&quot;start\&quot;: 2644.56,\n    \&quot;text\&quot;: \&quot; direction Dr. John Schillman thank you so much for sharing your time in your insight with the\&quot;\n  },\n  {\n    \&quot;end\&quot;: 2660.08,\n    \&quot;start\&quot;: 2649.12,\n    \&quot;text\&quot;: \&quot; talk our audience today thanks so much thank you\&quot;\n  }\n]&quot;}}},{&quot;rowIdx&quot;:1,&quot;cells&quot;:{&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Sven Mika&quot;},&quot;summary&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;Sven Mika of Anyscale on RLlib present and future, Ray and Ray Summit 2022, applied RL in Games / F&quot;},&quot;link&quot;:{&quot;kind&quot;:&quot;audio&quot;,&quot;value&quot;:[{&quot;type&quot;:&quot;audio/mpeg&quot;,&quot;src&quot;:&quot;https://media.transistor.fm/367e37c3/0b59214b.mp3?src=site&quot;}],&quot;inferredFromUrl&quot;:true},&quot;transcript&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot; There's a rise in interest in our finance. We have JPM for example, as well as other companies tha&quot;},&quot;segments&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[{\&quot;end\&quot;:5.72,\&quot;start\&quot;:0.0,\&quot;text\&quot;:\&quot; There's a rise in interest in our finance. We have JPM for example&quot;}}},{&quot;rowIdx&quot;:2,&quot;cells&quot;:{&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Karol Hausman and Fei Xia&quot;},&quot;summary&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;Karol Hausman and Fei Xia of Google Research on newly updated (PaLM-)SayCan, Inner Monologue, robot&quot;},&quot;link&quot;:{&quot;kind&quot;:&quot;audio&quot;,&quot;value&quot;:[{&quot;type&quot;:&quot;audio/mpeg&quot;,&quot;src&quot;:&quot;https://media.transistor.fm/a911824e/a83f417f.mp3?src=site&quot;}],&quot;inferredFromUrl&quot;:true},&quot;transcript&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot; This type of emergent capability is super interesting for us to see and super exciting for us by u&quot;},&quot;segments&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[{\&quot;end\&quot;:7.12,\&quot;start\&quot;:0.0,\&quot;text\&quot;:\&quot; This type of emergent capability is super interesting for us to se&quot;}}},{&quot;rowIdx&quot;:3,&quot;cells&quot;:{&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Sai Krishna Gottipati&quot;},&quot;summary&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;Sai Krishna Gottipati of AI Redefined on RL for synthesizable drug discovery, Multi-Teacher Self-Pl&quot;},&quot;link&quot;:{&quot;kind&quot;:&quot;audio&quot;,&quot;value&quot;:[{&quot;type&quot;:&quot;audio/mpeg&quot;,&quot;src&quot;:&quot;https://media.transistor.fm/b803a301/db38180a.mp3?src=site&quot;}],&quot;inferredFromUrl&quot;:true},&quot;transcript&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot; TalkRL podcast is all reinforced in learning all the time, featuring brilliant guests both researc&quot;},&quot;segments&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[{\&quot;end\&quot;:10.32,\&quot;start\&quot;:0.0,\&quot;text\&quot;:\&quot; TalkRL podcast is all reinforced in learning all the time, featur&quot;}}},{&quot;rowIdx&quot;:4,&quot;cells&quot;:{&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Aravind Srinivas 2&quot;},&quot;summary&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;Aravind Srinivas, Research Scientist at OpenAI, returns to talk Decision Transformer, VideoGPT, cho&quot;},&quot;link&quot;:{&quot;kind&quot;:&quot;audio&quot;,&quot;value&quot;:[{&quot;type&quot;:&quot;audio/mpeg&quot;,&quot;src&quot;:&quot;https://media.transistor.fm/cb13a30d/98e58583.mp3?src=site&quot;}],&quot;inferredFromUrl&quot;:true},&quot;transcript&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot; TalkRL podcast is all reinforced in learning all the time, featuring brilliant guests both researc&quot;},&quot;segments&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[{\&quot;end\&quot;:11.46,\&quot;start\&quot;:0.0,\&quot;text\&quot;:\&quot; TalkRL podcast is all reinforced in learning all the time, featur&quot;}}},{&quot;rowIdx&quot;:5,&quot;cells&quot;:{&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Rohin Shah&quot;},&quot;summary&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;DeepMind Research Scientist Dr. Rohin Shah on Value Alignment, Learning from Human feedback, Assist&quot;},&quot;link&quot;:{&quot;kind&quot;:&quot;audio&quot;,&quot;value&quot;:[{&quot;type&quot;:&quot;audio/mpeg&quot;,&quot;src&quot;:&quot;https://media.transistor.fm/5ba5d6af/41bb0ae3.mp3?src=site&quot;}],&quot;inferredFromUrl&quot;:true},&quot;transcript&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot; TalkRL podcast is all reinforcing learning all the time, featuring brilliant guests, both research&quot;},&quot;segments&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[{\&quot;end\&quot;:11.0,\&quot;start\&quot;:0.0,\&quot;text\&quot;:\&quot; TalkRL podcast is all reinforcing learning all the time, featuring&quot;}}},{&quot;rowIdx&quot;:6,&quot;cells&quot;:{&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Jordan Terry&quot;},&quot;summary&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;Jordan Terry on maintaining Gym and PettingZoo, hardware accelerated environments and the future of&quot;},&quot;link&quot;:{&quot;kind&quot;:&quot;audio&quot;,&quot;value&quot;:[{&quot;type&quot;:&quot;audio/mpeg&quot;,&quot;src&quot;:&quot;https://media.transistor.fm/fe014c06/e475c615.mp3?src=site&quot;}],&quot;inferredFromUrl&quot;:true},&quot;transcript&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot; TalkRL podcast is all reinforced in learning all the time featuring brilliant guests both research&quot;},&quot;segments&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[{\&quot;end\&quot;:10.72,\&quot;start\&quot;:0.0,\&quot;text\&quot;:\&quot; TalkRL podcast is all reinforced in learning all the time featuri&quot;}}},{&quot;rowIdx&quot;:7,&quot;cells&quot;:{&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Robert Lange&quot;},&quot;summary&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;Robert Lange on learning vs hard-coding, meta-RL, Lottery Tickets and Minimal Task Representations,&quot;},&quot;link&quot;:{&quot;kind&quot;:&quot;audio&quot;,&quot;value&quot;:[{&quot;type&quot;:&quot;audio/mpeg&quot;,&quot;src&quot;:&quot;https://media.transistor.fm/935a12e6/8a440dc1.mp3?src=site&quot;}],&quot;inferredFromUrl&quot;:true},&quot;transcript&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot; Robert Tiacolange is a PhD student working at the Technical University of Berlin. Thanks so much f&quot;},&quot;segments&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[{\&quot;end\&quot;:8.0,\&quot;start\&quot;:0.0,\&quot;text\&quot;:\&quot; Robert Tiacolange is a PhD student working at the Technical Univers&quot;}}},{&quot;rowIdx&quot;:8,&quot;cells&quot;:{&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;NeurIPS 2021 Political Economy of Reinforcement Learning Systems (PERLS) Workshop&quot;},&quot;summary&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Dr. Thomas Gilbert and Dr. Mark Nitzberg on the upcoming PERLS Workshop @ NeurIPS 2021&quot;},&quot;link&quot;:{&quot;kind&quot;:&quot;audio&quot;,&quot;value&quot;:[{&quot;type&quot;:&quot;audio/mpeg&quot;,&quot;src&quot;:&quot;https://media.transistor.fm/3d58a0b7/dc66961c.mp3?src=site&quot;}],&quot;inferredFromUrl&quot;:true},&quot;transcript&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot; Hi listeners, today we're going to hear about the upcoming Pearls Workshop. That is the political &quot;},&quot;segments&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[{\&quot;end\&quot;:5.5600000000000005,\&quot;start\&quot;:0.0,\&quot;text\&quot;:\&quot; Hi listeners, today we're going to hear about the up&quot;}}},{&quot;rowIdx&quot;:9,&quot;cells&quot;:{&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Amy Zhang&quot;},&quot;summary&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;Amy Zhang shares her work on Invariant Causal Prediction for Block MDPs, Multi-Task Reinforcement L&quot;},&quot;link&quot;:{&quot;kind&quot;:&quot;audio&quot;,&quot;value&quot;:[{&quot;type&quot;:&quot;audio/mpeg&quot;,&quot;src&quot;:&quot;https://media.transistor.fm/069ca161/19719c6c.mp3?src=site&quot;}],&quot;inferredFromUrl&quot;:true},&quot;transcript&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot; This is TalkArail Podcast. All reinforcement learning, all the time. Interviews of brilliant folks&quot;},&quot;segments&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[{\&quot;end\&quot;:11.0,\&quot;start\&quot;:0.0,\&quot;text\&quot;:\&quot; This is TalkArail Podcast.\&quot;},{\&quot;end\&quot;:13.84,\&quot;start\&quot;:11.0,\&quot;text\&quot;:\&quot; Al&quot;}}}],&quot;truncated&quot;:true},&quot;paginationData&quot;:{&quot;pageIndex&quot;:0,&quot;numItemsPerPage&quot;:100,&quot;numTotalItems&quot;:39,&quot;offset&quot;:0,&quot;length&quot;:100}},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzA1OSwic3ViIjoiL2RhdGFzZXRzL1JhbUFuYW50aDEvdGFsa3JsLXBvZGNhc3QiLCJleHAiOjE3NDI5MjY2NTksImlzcyI6Imh0dHBzOi8vaHVnZ2luZ2ZhY2UuY28ifQ.qk4NaB36N3Brq9_dV8owI_37C6sYrNZh_lx20i1_PeOOYyDxjBJkP4O-5Brcin7_1HMlxA856PC1q_VPFF4YBA&quot;,&quot;displayUrls&quot;:true},&quot;dataset&quot;:&quot;RamAnanth1/talkrl-podcast&quot;,&quot;isGated&quot;:false,&quot;isPrivate&quot;:false,&quot;hasParquetFormat&quot;:true,&quot;author&quot;:{&quot;_id&quot;:&quot;62aeb75c4b5d61650842dbf4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1671346465329-62aeb75c4b5d61650842dbf4.png&quot;,&quot;fullname&quot;:&quot;Ram Ananth&quot;,&quot;name&quot;:&quot;RamAnanth1&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:27},&quot;compact&quot;:true}"><div class="flex flex-col overflow-hidden shadow-xs mx-auto mb-10 rounded-lg border pt-2  px-2.5"><div class="mb-2 flex flex-wrap items-center gap-2"><div class="mr-auto flex items-center"><svg class="mr-1 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	<div class="whitespace-nowrap font-semibold">Dataset Viewer</div>
	</div>
				<a href="/datasets/RamAnanth1/talkrl-podcast/tree/refs%2Fconvert%2Fparquet/default" class="group mr-1 text-xs text-gray-400 max-sm:hidden"><svg class="text-[.6rem] mr-1 inline -translate-y-px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill="currentColor" d="M12 10H6.78A11 11 0 0 1 27 16h2A13 13 0 0 0 6 7.68V4H4v8h8zm8 12h5.22A11 11 0 0 1 5 16H3a13 13 0 0 0 23 8.32V28h2v-8h-8z"></path></svg>
						<span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Auto-converted</span> to Parquet
					</a>
				<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>API</button>
					<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path d="M9.80603 2.86737H3.56107C3.37704 2.86737 3.20055 2.94048 3.07042 3.0706C2.94029 3.20073 2.86719 3.37723 2.86719 3.56126V9.80622C2.86719 9.99025 2.94029 10.1667 3.07042 10.2969C3.20055 10.427 3.37704 10.5001 3.56107 10.5001H9.80603C9.99006 10.5001 10.1666 10.427 10.2967 10.2969C10.4268 10.1667 10.4999 9.99025 10.4999 9.80622V3.56126C10.4999 3.37723 10.4268 3.20073 10.2967 3.0706C10.1666 2.94048 9.99006 2.86737 9.80603 2.86737Z" fill="currentColor" fill-opacity="0.3"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M2.40942 1.66191C2.05175 1.66191 1.7618 1.95186 1.7618 2.30953V6.76191H1.43799V2.30953C1.43799 1.77303 1.87291 1.3381 2.40942 1.3381H6.45704V1.66191H2.40942Z" fill="currentColor"></path></svg>Embed</button>

					<button class="bg-linear-to-b shadow-xs flex items-center gap-1.5 rounded-full border from-white to-red-100/90 px-2 py-0.5 text-xs font-medium text-[#2D3648] transition-shadow hover:shadow-inner dark:from-gray-900 dark:to-red-800/30 dark:text-gray-100 dark:hover:shadow-inner dark:hover:shadow-red-800/30" ><svg class="h-3.5 w-3.5 text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
						<span>Data Studio</span></button></div>
		<div class="flex flex-1 flex-col overflow-hidden -mx-2.5"><div class="flex flex-1 flex-col overflow-hidden"><div class="flex min-h-0 flex-1"><div class="flex flex-1 flex-col overflow-hidden"><div class="md:-mx-2.5 flex min-w-0 flex-wrap border-t"><div class="flex min-w-0 flex-1 flex-wrap"><div class="grid flex-1 grid-cols-1 overflow-hidden text-sm md:grid-cols-2 md:place-content-center sm:mx-2.5"><label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-950 md:border-r md:border-r-0 hidden" title="default"><span class="text-gray-500">Subset (1)</span>
			<div class="flex items-center whitespace-nowrap"><span class="truncate">default</span>
				<span class="mx-2 text-gray-500">Â·</span>
					<span class="text-gray-500">39 rows</span>
				<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
			<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Subset (1)"><option value="default" selected>default (39 rows)</option></optgroup></select></label>
		<label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-900 md:border-r md:border-r" title="train"><div class="text-gray-500">Split (1)</div>
				<div class="flex items-center overflow-hidden whitespace-nowrap"><span class="truncate">train</span>
					<span class="mx-2 text-gray-500">Â·</span>
						<span class="text-gray-500">39 rows</span>
					<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
				<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Split (1)"><option value="train" selected>train (39 rows)</option></optgroup></select></label></div></div>
								</div>

							<div class="flex min-h-0 flex-1 flex-col ">
	<div class="bg-linear-to-r text-smd relative flex items-center dark:border-gray-900 dark:bg-gray-950 false border-t [&amp;:has(:focus)]:from-gray-50 [&amp;:has(:focus)]:to-transparent [&amp;:has(:focus)]:to-20% dark:[&amp;:has(:focus)]:from-gray-900"><form class="flex-1"><svg class="absolute left-3 top-1/2 transform -translate-y-1/2 pointer-events-none text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				<input disabled class="outline-hidden h-9 w-full border-none bg-transparent px-1 pl-9 pr-3 placeholder:text-gray-400 " placeholder="Search this dataset" dir="auto"></form>
			<div class="flex items-center gap-2 px-2 py-1"><button type="button" class="hover:bg-yellow-200/70 flex items-center gap-1 rounded-md border border-yellow-200 bg-yellow-100 pl-0.5 pr-1 text-[.8rem] leading-normal text-gray-700 dark:border-orange-500/25 dark:bg-orange-500/20 dark:text-gray-300 dark:hover:brightness-110 hidden"><div class="rounded-sm bg-yellow-300 px-1 font-mono text-[.7rem] font-bold text-black dark:bg-yellow-700 dark:text-gray-200">SQL
	</div>
	Console
</button></div></div>


<div class="flex flex-1 flex-col overflow-hidden min-h-64 border-t">
		

<div class="max-h-96 relative overflow-auto"><table class="w-full table-auto rounded-lg font-mono text-xs text-gray-900"><thead class="shadow-xs sticky left-0 right-0 top-0 z-1 bg-white align-top"><tr class="space-y-54 h-full min-w-fit divide-x border-b text-left"><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">title
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['Â·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="15.166666666666666" width="11.2" height="14.833333333333334" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="22.75" width="11.2" height="7.25" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="24.916666666666668" width="11.2" height="5.083333333333333" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="24.916666666666668" width="11.2" height="5.083333333333333" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">9</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">81</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">summary
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['Â·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="23.11111111111111" width="11.2" height="6.888888888888889" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="17.333333333333336" width="11.2" height="12.666666666666666" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="23.11111111111111" width="11.2" height="6.888888888888889" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="23.11111111111111" width="11.2" height="6.888888888888889" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="14.444444444444445" width="11.2" height="15.555555555555555" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="2.8888888888888893" width="11.2" height="27.11111111111111" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="8.666666666666668" width="11.2" height="21.333333333333332" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="8.666666666666668" width="11.2" height="21.333333333333332" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="0" width="11.2" height="30" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">69</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">170</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">link
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['Â·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="130" height="30" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="132" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">58</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">58</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">transcript
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['Â·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="23.11111111111111" width="11.2" height="6.888888888888889" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="20.22222222222222" width="11.2" height="9.777777777777779" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="11.555555555555557" width="11.2" height="18.444444444444443" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="17.333333333333336" width="11.2" height="12.666666666666666" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="2.8888888888888893" width="11.2" height="27.11111111111111" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="8.666666666666668" width="11.2" height="21.333333333333332" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="11.555555555555557" width="11.2" height="18.444444444444443" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">1.66k</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">94.2k</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">segments
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>list</span></div></div>

		<div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th></tr></thead>
			<tbody class="h-16 overflow-scroll"><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="0"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">John Schulman</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">John Schulman, OpenAI cofounder and researcher, inventor of PPO/TRPO talks RL from human feedback, tuning GPT-3 to follow instructions (InstructGPT) and answer long-fo...</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="false "><div class="" dir="auto"><div style="content-visibility: auto">
	<audio controls controlslist="nodownload" preload="none" class="h-dvh max-h-[2.25rem] w-full min-w-[300px] max-w-xs"><source src="https://media.transistor.fm/2bfa4dc4/b29b0c00.mp3?src=site" type="audio/mpeg"></audio></div>
	<div class="mt-1 text-[.7rem] leading-tight text-gray-400">https://media.transistorâ€¦c00.mp3?src=site
	<a href="https://media.transistor.fm/2bfa4dc4/b29b0c00.mp3?src=site" target="_blank" class="hover:text-gray-700 dark:hover:text-gray-100"><svg class="inline" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path fill="currentColor" d="M10 6v2h12.59L6 24.59L7.41 26L24 9.41V22h2V6H10z"></path></svg></a></div></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class=""> The answer was affirmative. We can get an agent to basically use a set of tools that we give it. In this case, the browsing commands like searchings. I would say I expect AI to be able to do better, a better job than humans at most jobs that humans do now. Five years or so. TalkAulRO podcast is all reinforcing learning all the time, featuring brilliant guests, both research and applied. Join the conversation on Twitter at TalkRL podcast. I'm your host, Robin Chohan. John Schulman is a co-founder of OpenAI and a researcher and engineer at OpenAI. He is well known for major contributions to the field of reinforcement learning, including the TRPO algorithm that's trust region policy optimization, GAE, generalized advanced estimation. Those are from his UC Berkeley dissertation and TRPO's descendant proximal policy optimization, or PPO. His current focus at OpenAI is on RL from human feedback. John, welcome to the show and thanks so much for being here. Thanks a lot for having me. You were literally one of the first people I thought of when I started the show three years back. Thanks, I'm honored. It means a lot to me to have you here today. I definitely remember you were nuts and bolts of deep RL video back in the day and watching that multiple times and gaining a lot from that. You helped a generation of RL practitioners back then. By the way, there's going to be a reboot of the nuts and bolts presentation. I got invited to give a talk at NERPS this year on it. I'll have to revamp the guidelines and everything. That'll be fun. Oh, that's awesome. Can't wait for that. You were clearly one of the earlier pioneers in deep RL. How did you choose to move your focus to RL from human feedback? Why is that an important problem? Why is that important to you? After GB3 was trained, I was blown away by how smart it was and I realized the next frontier was figuring out how to make language models actually useful. I'm still really interested in RL but solving RL benchmarks isn't the end of the story. To use your RL algorithm you need a reward function. Whereas the reward function come from in RL benchmarks, you usually just code up the reward function. But if you're not in a simulator environment, that doesn't work. What we have to do in any kind of real-world use case is have humans look at what the AI did and decide if it was good or bad. How exactly do you define this reward becomes a really challenging and important problem, especially as the tasks get harder to evaluate? Another angle on this is that language models are very smart but it's hard to get them to do anything useful. A big part of that is they're not necessarily trying to do what you want. They're just trying to imitate the training corpus. That means there's a big opportunity to improve them a lot by just giving them the right objective. That's what we can do by applying RL to these language models using human feedback to define the reward. Is human feedback harder or very different in some way than using a synthetic reward? There are a lot of new complications. You have to collect a data set dynamically. You're always in the business of building data sets of human preferences. Often the data quality there matters more than various algorithmic details. You also have to think a lot about exactly how you're giving the task to the human trainers and various other things that you wouldn't have thought about if you just had a programmatic reward function. Does the difference between human-raders or the noisiness of the reward signal cost any problems? I would say the noise definitely you need to be below some threshold of noise to learn anything. I think in general if you have a large noisy data set that can be as good as a smaller clean data set. Actually, noise isn't the thing that worries me the most. It's more that there are sometimes consistent biases that people have. For example, in settings like question answering or settings where you have a model writing some text, often people prefer longer answers. You end up with these very verbose answers. If you're not careful with the instructions that is. You can also instruct people the raiders to reward brevity. But without yet, if you're not careful you can incentivize the wrong kinds of behaviors. So let's move to some of your recent work. First up is WebGPT. Browser assisted question answering with human feedback. That's a Nekano at all with yourself as a co-author in 2021. Can you tell us what is the main idea of this paper? What is WebGPT? In WebGPT, we basically took our language models and we hooked them up to a web browser so they could retrieve information from the web. They can write an answer by summarizing the relevant pages from the web. That way if you're asking a question about current events or a question that requires some detailed scientific or technical knowledge, this AI can go out and look up the answer and with detailed citations to its sources. I would say there's two interesting points to this. One is we were exploring whether you could turn language models into a kind of agent. There's a lot of data on the web of different texts that people have written. But there's not a lot of data that shows how to actually do some multi-step process. So it's not that clear, uprearry whether you can get a language model to actually carry out some iterative process. We just have a lot of data like writing essays and having chats and so forth. So that was one thing we were exploring here and I think the answer was affirmative. We can get an agent to basically use a set of tools that we give it. In this case the browsing commands like searchings, scroll link, click on links. The second theme of this paper was around truthfulness. I mean a big issue with language models is I mean they're not very reliable at giving you true information. They know a vastly superhuman amount. But if you prompt them in the wrong way they'll just output lots of plausible sounding nonsense. So how to fix that is a big research question or one of the biggest research questions in the world of language models. I think it's going to be challenging to fully fix it but I think a big part of the story involves retrieval and having models write answers that contain citations. Citations to try trusted sources. So a person who's checking over the answer doesn't have to go and try to figure out where the model might have gotten this idea. They can go and directly look at the source and see if it supports the AI statement. With WebGBT we just wanted to see if we do give the language model a really flexible interface to the web. Can we have it answer hard questions truthfully using like with the help of all these citations. And it's actually really non-trivial because if you look at the data that we use the Reddit explain it like on five. The questions are really varied like some of them are about science, history, current events. Like our Raiders didn't necessarily know anything about these topics but still they had to judge the answers written detailed answers. So it would have been really hard to do it without the supporting citations. So we kind of validated that we could get good feedback in a hard domain like this with the help of citations. Can you talk about where the idea for WebGBT came from? Is that an idea you've had kicking around for a while or was it something that came up recently before the paper? How did that play out? Some of the ideas had been floating around like we thought that we actually had a project at OpenAI very early on a world called World of Bits. We were looking at controlling web browsers or doing tasks that involve tasks on the internet with the web browser but it was way too early at the time. So we kind of abandoned it for a few years. Actually we were trying to back then we were trying to do it with full visual input. So we thought yeah we could give some instructions to the agent like go and figure out figure out the address of this building or something. The agent would go and search the web or use Google Maps or whatever to figure out the answer. And we were trying to do this all in pixels that obviously didn't work very well. But now we have these great language models on the work on text data. We can also extract the text out of web pages to get most of the information. We can't really interact with a lot of dynamic websites. Yeah, where there's a lot of JavaScript and images and so forth. But as long as it's just browsing and reading text we're fine. So yeah we had good enough models and that made it kind of feasible to revisit this idea of using the internet as an environment. So I would say that was one of the sources of inspiration that long-stinted, that long kind of thread about like using the internet as an environment. Another motivation was just after we got after we started playing with GPD3 we noticed that it had all these problems with factual accuracy and the reliability of the information it was giving us. So that kind of motivated doing more research on how to make language models more truthful. We were kind of brainstorming what to do there and we went through some docs and eventually decided that we wanted to try some question answering like using the web, looking up knowledge on the web to help answer questions. So actually the original version of the project used trivia questions. So there's another, there's this well-known data set trivia QA that has some basic trivia questions. So we first worked a little bit on that data set and tried to see if we could boost the model's accuracy by giving it web search and yeah that actually works quite straight, that worked pretty easily. So then we decided to move on to long-form question answering and so that gave us the, that was the project we ended up working on for a while. It seems like you use a few different data sets here and a number of different training methods. I'll just mention the last behavior cloning, reward modeling, reinforcement learning, and rejection sampling. So we were using a fairly standard methodology which was actually adapted from previous work on RL from Human Preferences. So the pipeline is you first train a model with supervised learning where you you have human demonstrators show how to do the task, like show how to map from observations to actions. Yeah so that's the supervised learning or behavior cloning step then we train a reward model or preference model. It looks at two actions or two out trajectories and decides which one is better. In this case like in a question answering setting you're looking at two answers and deciding which answer is better and we use that to train a reward model that assigns higher score to the good answers than the bad ones. Then you do reinforcement learning against that reward function and of course you can iterate these last two steps. After you do a little RL now you're, you sort of exploited some of the flaws of the reward model like or some of the noise in the reward model and it's not necessarily accurate on your new distribution of data. You recollect more pairs of samples and refit this preference model and then you do another iteration of RL. So that's like that's the whole RL from Human Feedback Pipeline and there's this other idea called rejection sampling or best event sampling and in general you can do other kinds of search too where instead of doing RL once you have your reward model you can just search against that reward model so you can take a bunch of collect a bunch of samples and re-rank them with the reward model and take the best one as your action. Kind of like NPC. Yeah exactly. Yeah kind of depends exactly what setting you're in what you can do. If you're in a setting where there's some environment you're interacting with then you would have to simulate your, you would have to simulate the dynamics of your environment which yeah so that would look kind of like NPC. In our case we were the only thing we had to learn a model of was the human preference so like we're it's a question answering setting so it's really like a contextual banded problem so it's kind of straightforward to take a bunch of sample a bunch of actions where each action is a full answer and re-rank them or search against the search over answers. So in terms of the action space was it the action space just a list of commands or is it still generating tokens like a regular generative mode? We were generating tokens. We had two phases of like in each episode of the RL task so there is first a browsing phase where where the model goes and it issues searches and clicks on things and quotes relevant information like if it sees something useful on the page it'll it'll quote it using this quote commands and then once it's browse it's done browsing it'll issue another command called end browsing and it'll write its answer that's also expressed in tokens but really we rolled this all into one big RL task where your episode involves browsing and writing out the answer and it's all one big RL episode. Did you think this is going to work well or were you kind of surprised? At the very beginning of the project we didn't know if it was going to work or not. Like after we did the initial experiments with Trivia QA which actually didn't take that long to get running then it became pretty clear that it would work that the browsing part worked at least and we already know that we can get these models to write pretty good long form text with a bunch of if you give them a bunch of snippets of text that they they can cite. So I noticed the the the human raiders task was quite complicated as it was a long guide and there was many types of feedback that they were giving but in the end the paper said that only the final rating was used so I was just curious if you hadn't commented about that like why do you think maybe the model couldn't use that extra feedback whereas it was maybe just too much or not enough samples. Yeah that's been one frustrating finding so far in in that project and also some other projects we've had the same finding but you have your raiders go through this long process for each for each comparison they do where they're comparing a pair of answers and then you only use one bit of information from the whole from this whole process which might have taken like half an hour. It seems like it would be better if we if we were able to extract more information more about the process they went through in arriving at the answer. So we did collect all sorts of other information like we had them provide ratings along several different axes like coherence and factual accuracy and so forth but in the end we didn't really get much of a boost out of using any of this this other information so I'd say it seems like there's it should be possible to do better but unfortunately this methodology which seems kind of dumb so far it's hard to be and people have tried various other ideas for like how to use human feedback instead of you getting these preference scores there various other things you can do like you can have them right critiques and edit or maybe edit the responses. Yeah I think some of these things are are also promising but yeah this methodology of collecting preference data works well. Yeah I think it's it's still an open area of research. Oh yeah regarding the really long instructions. Yeah I think for any of these tasks there is a lot of subtlety in how to do the task properly and so we ended up adding more and more details of like what do you do in this situation and what do you do in that situation. I think it's starting to get pretty unwieldy with these really long instruction manuals so there's some promising ideas for how to address this like there's a paper from DeepMind recently Sparrow that used basically broke down the task and they trained they basically had people look at one aspect of the one aspect of the response at a time and and then they had a way of combining these different rule specific they would train a bunch of rule specific reward models and then combine them at the end. Yeah I think there's some other interesting ideas for how to how to make this process better. So I gather that from your answer about WebGPT and the whole idea of WebGPT is that you want the the language model type access to external knowledge but I wonder where you think the line should really be in terms of what a language model should know and what the language model should look up and maybe what the language model should not know or not purport to know. Do you have opinions about that? Yeah let's see like some people are advocating for very small language models that have like no external knowledge aside from language I guess would be the extreme position and then other people other people talked about language models that just know everything as opposed to having an external knowledge source. There's some interesting questions there so I think it is a little hard to separate knowledge factual knowledge from understanding. So as humans we get by like not memorizing all sorts of facts and just knowing that we can look them up if needed. For working on a specific domain it is useful to like have a lot of facts internalized so that you can recall them very quickly and kind of combine them combine them in your head. So I wouldn't take an extreme position on either side I would say I think retrieval is going to be really useful just at the very least for current events but also I don't think we want to try to pack all human knowledge into the weights of a neural net. On the other hand I think people have had a lot of luck just scaling up models and like as they soak up more factual knowledge they also get better at reasoning and other things and I think I haven't seen any demonstrations of tiny models that just do lots of retrieval and save all their weights for reasoning. Yeah I just haven't seen any evidence of this or that or I haven't seen any successful attempts at making this. Let's move on to training language models to follow instructions with human feedback that was uyang et al and that was 2022 with yourself as a co-author. Can you tell us the main idea with this paper? This is the instruct GPT paper. What does instruct GPT and what's going on here? Instruct GPT is a language model that's fine tuned to follow instructions and it's in fact the one that you can play with if you go to the open AI website you get a big text box and you can write some text and then press the button to generate a completion. So the idea here was I mean language models are pretty useful and you can sometimes get them to do what you want by prompting them just right. This idea of few shot prompting has been become pretty popular where you give a few examples like a few question answer examples and then if you ask another question it'll hopefully provide an answer in the same style. So the idea yeah so if you can get language models to do great things with prompting but prompting is itself an arg and it's tricky to get right and it's also kind of not necessarily getting the best possible performance out of the model. If you just take a raw language model and you try to you try to talk to it like you ask it a question it probably it doesn't know that it should actually answer that question as well as possible. For all it knows you want it to give a joke answer or a riddle or something. Yeah so the idea of instruct GPT was let's make a kind of small change for our language models so that they're much easier to use. In particular we're going to train them to if you have a piece of text where there's an instruction the model will try to follow that instruction to the best of its abilities and pretty much anything can be an instruction like you can have a the instruction can be to continue a chat or it can be to like summarize like summarize this text or give me a list of names for my company that sells widgets. Yeah instructions can be anything and that makes that makes this kind of model very powerful. So that was kind of that's the idea of an instruction following model it's like a model that can do anything that you specify with an instruction and by the way I wasn't a core contributor to this work I was more involved with like getting the RL infrastructure and some of the RL training details like helping out with that that stuff. But anyway yeah what we did in this project was we ran this this whole methodology that I just described of RL from even preferences in this instruction following setting. So we did supervised fine tuning, collected preference data, trained a reward model and then did RL against that reward model and one interesting detail is actually whereas the original initial data was just collected using contractors. At a certain point we had the the API and it's got this I mean we have this playground on the website where this is where you the big text box where you can use the model. So we we took prompts that people that users had put into the into the playground and use those for training like both to collect preference data and to do RL. So and this is like this is disclosed to users pretty prominently like when when people are using the playgrounds you get notified that your prompts might be used for the training and we're also careful to train in such a way that we don't memorize any information that was in in the prompts. Like it and it explicit like we have a pretty like elaborate process for making sure there's no like private information being leaked into the model. But anyway yeah that's that's basically the experimental setup and the result was that it works like this methodology works quite well and you get a model that's vastly preferred to the base model on this distribution of of realistic prompts that people are giving the model often which contain instructions. So the raw like the the raw language models generally do a really bad job following instructions but this RL trained instruction following model is is a lot better and it's something like if you just calculate how much better it's something like it's as good as a model that's a hundred times bigger. That's a lot. Yeah. You wanted the model to be truthful is that is that one of the criteria you wanted? Oh yeah truthfulness was one of the criteria. That seems amazing to me that truthfulness is something that I could learn by example like does that mean that truthfulness is somehow represented inside the network or because there's no external way for the model to confirm whether something is true or false. So how how might it know what is what is true without any external reference? I think to some extent there is some internal representation of truthfulness. So I would say like one way to think about what language models do is they're trained to imitate the whole internet and the internet is written by lots of different people and has lots of different types of content from fiction to nonfiction to like like technical like detailed technical literature to like jokes and like forum posts whatever. So what the model is basically an ensemble of all these people who wrote stuff on the internet the raw pre-trained model. When you feed it a prompt what it's doing internally has to be something like figuring out who wrote the first wrote this prompt and then trying to continue in that style. So if it thinks it's reading just reading something on the Wall Street Betts Reddit it's going to continue on that style but if it thinks it's in the New York Times it's going to write in a very different way. So effectively the model must be like calculating somewhere like what style is this or what ensemble what's the like narrower ensemble of styles that I'm trying to imitate now. At the very least when you do some kind of when you do training like either supervised fine tuning or are all from human feedback you can at least like narrow down the set of styles the model is producing and try to imitate like the best or the best person in the training set or the best style in the training set and obviously best will differ a lot. So what we'll end up with will depend on our instructions. So if we if we tell I don't know we'll end up with something that has kind of safe like not too not too controversial but a bit corporate will end up with something like that depending on what our instructions are. So at the very least like we can kind of narrow in on one style instead of having the whole distribution of styles on the internet. I think probably there's more to it than that like we're not just learning about style but the model probably is like internally trying to determine if things are if statements are true or not like if the prompt contains incorrect information because that probably would be useful for determining a likely completion. I'm just talking about the raw pre-trained model so I think yeah I think just the objective of predicting next tokens probably gives you a lot it forces the model to like the determine if things are true or not. I think for our alfine tuning there's a lot more potential for the model to actually like try to output something truthful as opposed to trying to imitate a certain style though it's hard to I guess it would be hard to like determine if that's what the model is actually trying to do. So it's almost like the the prompt is guiding the model it's like what corner of the internet do we want to do we want to imitate here and maybe we want to instruct GPG wants to to focus more on the most more truthful corners of the internet something similar to that. Yeah I would hope so at least I think that's a pretty good though maybe a little simplistic picture of what's going on. At the very least we should be able to imitate the most truthful corner of the internet. So can you talk about a generalization and how does this type of model perform out of distribution? Like I guess if it seems questions that are a bit different than what it was trained on. What happens if we get a little bit away from the training data with the reward models? I mean language models in general generalize surprisingly well and I would say overall like these pre-trained models that are trained on super diverse data sets from the internet. They tend to generalize quite well or surprisingly well at least it's surprising to those of us who were around for the earlier days of machine learning when everything was trained from scratch and very fragile. For example if you ask if you provide an instruction in some other language even a even a fairly rare language it'll often do a decent job following the instruction even if there's zero data in the whole instruction following the training process that's in that language and that's just to carry over from the pre-training. So I think generalization yeah I think language models generalize quite well. So you asked about reward models I think one of the tricky pieces about RL from human feedback is how so you have this reward model and you're actually training against it meaning you're training your policy to have high reward and it's going to exploit the errors in the reward model so it's going to eventually find adversarial examples to the reward model. This is worse than kind of normal out of distribution behavior it's like targeted out of distribution examples so so there are definitely some challenges around getting reward models to generalize well or generalize as far as possible from the training set. Can these types of agents tell us when they don't know something or is that a hard problem? I'd say sort of if you ask a question that's kind of in the core of the model's knowledge it will know know the answer and it'll know that it knows. By the way I'm talking about models like the for the instruct model if you ask it about something that's like very simple at the core of its knowledge it'll know if you there are certain things that it knows that it doesn't know like current events where it's been trained to know that it doesn't know certain things in real time but if you ask it about something that's kind of on the edge of its knowledge it's it's going to have a hard time it's it's necessarily going to be inaccurate. I mean there have been a couple papers about this question so there is in paper from Anthropic recently called language models mostly know what they know and there is also a paper from FHI and OpenAI called getting language models to express their uncertainty and words. These language models as well as a lot of other models in machine learning are training to maximize likelihood so maximize log-prob of data. You're already training them to always predict a distribution of outputs. So for language models given a prefix it's predicting a distribution over the next token. These predictions for the next token like generally are pretty well calibrated but 80% if it puts 80% probability on something and you look at all the times when it puts 80% probability on something like it's right 80% of the time. Like that's just a result of the training objective. The training objective like strongly incentivizes the model to be calibrated meaning it has a reasonable estimate of its uncertainty. So at the single token level models definitely are calibrated. The question is whether they're calibrated on whether this calibration extends to settings where they are generating multi-token outputs or whether they can judge the correctness of some multi-token statement. So I would say since models are calibrated at the single token level they I think they definitely have the information to be calibrated in these other settings. So that's why I think the problem of models knowing what they know isn't actually that hard or at least getting a model to express its uncertainty pretty much as well as a human does doesn't feel like an insurmountable problem but there's some practical difficulties to getting getting there. People use the phrase AI alignment in different ways. Can you talk about how you see alignment in your work on Aral from human feedback? I think of alignment mostly as the problem of getting the model to try to do the right thing so we can kind of make a distinction between what the model is capable of doing. Like if you just take a raw language model and you ask it a question like I said before it doesn't know that you actually wanted to give the correct answer as opposed to. It might think someone who is not very knowledgeable is answering. By doing some extra training we can get the model to actually try to do the right thing and so I would say that that's the main goal of alignment. So there was an open AI blog post recently that talked about the sequence in alignment. One was training AI systems using human feedback to use it training AI systems to assist human evaluation and three training AI systems to do alignment research. So is your current work mostly about this first item and when and how do you see us getting to these other stages? I'm doing some work now on number two training AI systems to assist human feedback. I think that's sort of becomes increasingly necessary as you start trying to get the systems to solve harder and harder problems. When you have models that are kind of very below human level or maybe at human level at a certain task it's pretty straightforward to supervise them. But once they're doing things that are very hard or doing things that require a lot of diverse technical knowledge it becomes pretty hard to provide a useful supervision signal. So we have to start doing things like one model writes an answer to do a question and then another model provides a critique of that answer points out some flaws and then the human only has to judge the first answer after looking at the critique meaning basically the critique helps the human assess the answer. So I think like that kind of idea is starting to become pretty relevant. A colleague's an I are exploring that kind of idea now. As for assisting alignment research there's some other work at open AI that's starting to explore this. It's also that sort of the for this down the road. So I saw Stuart Russell was on your PhD committee and I really enjoyed his book Human Compatible. I wonder if you share the idea mentioned in the book that the standard RL framing with this fixed reward signal is problematic and that agents powerful agents should try to do what we want and maintain some uncertainty about what it is we want and the agents that are too certain will be problematic. What do you have any thoughts on that idea? I totally agree with that idea. So I think first it's really hard to write down a simple reward function that actually captures what we want or what any any particular person wants. I can say I want a little more of this or a little more of that but you wouldn't want to take that to the extreme. If we build agents that try to cater to our to our wishes we should make sure they're like they have a lot of they have uncertainty about what we want or what we value and that that'll also cause them to be a little more cautious and say not disturb anything that might be important to us. So yeah I agree with that like Stuart Russell gave a very good like problem definition of what we want AI to do like we want it to basically we want to jointly like play this game where AI is the AI is trying to figure out what we want and then trying to do that but simultaneously maintaining some uncertainty about what we want. I would say if you you start to look at how to get that in practice it actually looks quite a bit like the kind of RL from human feedback that we're working on at OpenAI and others are working on other places. I think yeah I think I see what we're doing as a practical implementation of getting towards this behavior that Russell have described. Do you think of a AGI as an abstract goal or are we going to see a model come out one day and people are going to say oh that's the first AGI model like what does it have to do for people to say that? I think people will say that many times then realize that it doesn't quite do everything that you want. I think we're going to have a lot of like a long series of models that are that are superhuman at most things or at a certain class of things but they also have some failure modes and weaknesses. Like I expect us to like see multiple models that are proclaimed as AGI and then only after interacting with it a while you do realize it's not quite there. What would you say is the relationship between AGI and RL and AGI and these large language models? How do those concepts fit together? I would say that RL is a useful like component of training AGI or an almost essential component. The thing RL lets you do is it lets you optimize any objective for the agents. Any objective that is a function of the agents behavior. So with pre-training like what we do for language models you're kind of choosing an objective that lets us do something with all the training day we have which is all this internet text. So we choose this maximum likelihood objective which is basically the only or not the only thing but it's like a sensible way to absorb all this knowledge. But then if we really want to optimize the agents behavior for a specific objective RL is kind of the only framework that lets you do that. Okay John we have a few questions from the audience and I'm just going to pick the two that have the highest score in terms of Twitter likes. So the first is from Eric Chang VP of AI at a Hello Di Robotics. He asked RL distributions are non-stationary making it hard to reason about PPO losses and how that relates to return or generalization. Are there any intermediate plots and visualizations you like to generate to debug or incrementally build up a large scale RL system? Yeah there are definitely some stats that I look at so I will be I'll talk about this in the nuts and bolts like reboot waited a year but I'd say things like you're looking at the explained variants of the value function and looking at the like how many samples are getting clipped in PPO and what the KL between the what what the KL divergence is between the policy before and after the update is yeah things like that. And then Ethan the calibar from Miele asks what is your median estimate for the arrival date of AGI? I think not too far away but I like I said I expect there to be a lot of fall starts I would say expect like like AI to be able to do better a better job than humans at most jobs that humans do now five years or so that's not all jobs but most jobs for a while we're going to discover things that AI isn't very good at and then where we want to keep humans in control so I think there'll be some kind of gradual process over the next 10 or 15 years. I've been curious about this I see that some RL work is patented but I could not find a TRPO or PPO in I could not find patents on these are those protected patent protected at all or how do you how do you think of intellectual property protection for that kind of work? I haven't ever looked looked into patenting anything and open AI hasn't either as far as I know I think the trend over time has been for people to take a patent scene machine like a machine learning algorithms last seriously there is this algorithm in computer vision called sift which is like this key point to detector and this was patented I think the the guy who patented it he probably made his university some money from the patent but in the end all it did was cause people a lot of annoyance because like the people people had to come up with alternative algorithms that like had a different acronym and weren't patented so like the open CV open source library would have like had to be careful about putting this algorithm in their library because of the patent risks so I think like these patents aren't the patent rights aren't exercise that much and I think big companies like Google will patent a lot of stuff for defensive reasons so if they get in some big legal dispute with another company it can be used as like one of the bargaining chips but I think I don't think anyone's going to like get sued for royalties for not yeah for not providing royalties for the use of some algorithm okay and then there's been a ton of work in RL of course since you first published TRPO and BBO but from your point of view if you had to pick a few highlights in terms of a few important milestones in in RL algorithms since PPO came out and by the way it's amazing that in 2022 we're still using PPO I think quite similar into it's original form is that right yeah pretty much yeah so so what would you say are the the biggest highlights for you in terms of our algorithm since since you did PPO yeah there's definitely been some interesting stuff so I think like a little after PPO there is TD3 and SAC and those are seem like pretty solid value-based methods that was one development that was interesting I think like yeah I thought museiro and it's and it's like elaborations we're also like efficient zero we're also pretty impressive that you can get that good sample efficiency both of the things I just mentioned were kind of well I don't want to say mostly on toy tasks or benchmarks because yeah I'm sure people are doing some real things with these algorithms yeah so I think that's that stuff was interesting I think like the whole recent interest in search of interest in the offline RL was also notable I would say the like the stuff we're doing with RL from human feedback is the kind of offline RL because we're like we have a fixed dataset and we have a fixed reward modeling dataset and we're training against that this is like offline RL but you're doing it in a different way you're using an on-policy algorithm with a reward model as opposed to maybe a more typical way to do offline RL would be use off-policy algorithm would that work here or would that not work here well we're doing here is kind of like model-based RL because the reward model is like a model of the like the unknown part of the system so like the unknown part of the system here is the is the human radar or the human it's not the outputting appending to your list of tokens so this is kind of like the work that's like takes a dynamics model at the environment and does some kind of just runs a policy grading algorithm against it so it's not like so the idea of running an online algorithm against a model that's kind of a well-established idea so I would say the papers that previously did this they were in a pretty different regime were in this regime of doing fairly small updates to the policy because we have this these awesome pre-trained models and we don't need to actually change them that much so yeah we use these online algorithms I'd say part of the reason why we can get away with using just an like an online algorithm is because we've been just looking at a band a contextual banded problem yeah because we only have like one time step like you get a query and you output a response and then that response gets a reward so if we had a like a multi-step process such as a conversation where you can't assign a reward until the very end of the conversation and or you had some I don't know some interaction with like some real-world system that's hard to simulate you wouldn't then it wouldn't be S-ray forward to you wouldn't be able to use exactly exactly the same methodology you would probably have to use a you would have to probably train a Q function or or something like that if you want if you want your method to be sample efficient you would probably have to do something slightly different I think we'll we'll have to we'll have to start exploring this at some point soon but so far we haven't at least I haven't seen any cases in like in the domain I'm looking at that require this but I expect it to to be relevant at some point so we had Arvind Shrinivas talking about decision transformer on the show recently that was a great episode and I see that you were also a co-author on the the 2016 RL squared paper I want to ask you what your thoughts about meta RL Arvind had some interesting things to say about maybe the idea that a transformer could kind of supersede the need for an RL algorithm altogether what do you expect from meta RL do expect will will still be using human authored RL algorithms in the future yeah that's a pretty bold statement that we don't need we won't need any RL algorithms anymore yeah since the RL squared paper people have been talking less about meta learning as far as I can tell actually because of sequence modeling has gotten so good like transformer let sequence models so that it's kind of queer the meta learning is just a special case of learning like it's it's just it's just like a certain kind of long context learning learning involving long episodes and maybe it shouldn't be treated that differently or are addressed with special algorithms I would say yeah the ideas like decision transformer are pretty interesting where you try to reduce RL to supervise learning it's still not like certain exactly how these compare and performance to RL like people have started to analyze that empirically and theoretically and I would say in practice sometimes sometimes it's better sometimes it's worse in my experience like it's been worse on the problems that I've that I've my colleagues and I have where we've tested it but yeah it's definitely an interesting direction Dr. John Schillman thank you so much for sharing your time in your insight with the talk our audience today thanks so much thank you</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "end": 6.24,
    "start": 0,
    "text": " The answer was affirmative. We can get an agent to basically use a set of tools that we give it."
  },
  {
    "end": 12.48,
    "start": 6.24,
    "text": " In this case, the browsing commands like searchings. I would say I expect AI to be able to do better,"
  },
  {
    "end": 17.84,
    "start": 12.48,
    "text": " a better job than humans at most jobs that humans do now. Five years or so."
  },
  {
    "end": 27.92,
    "start": 22.56,
    "text": " TalkAulRO podcast is all reinforcing learning all the time, featuring brilliant guests,"
  },
  {
    "end": 34.08,
    "start": 27.92,
    "text": " both research and applied. Join the conversation on Twitter at TalkRL podcast. I'm your host,"
  },
  {
    "end": 44.32,
    "start": 34.08,
    "text": " Robin Chohan. John Schulman is a co-founder of OpenAI and a researcher and engineer at OpenAI."
  },
  {
    "end": 48.32000000000001,
    "start": 44.32,
    "text": " He is well known for major contributions to the field of reinforcement learning,"
  },
  {
    "end": 54.400000000000006,
    "start": 48.32000000000001,
    "text": " including the TRPO algorithm that's trust region policy optimization, GAE, generalized"
  },
  {
    "end": 59.12,
    "start": 54.4,
    "text": " advanced estimation. Those are from his UC Berkeley dissertation and TRPO's"
  },
  {
    "end": 65.03999999999999,
    "start": 59.12,
    "text": " descendant proximal policy optimization, or PPO. His current focus at OpenAI is on RL from"
  },
  {
    "end": 68.16,
    "start": 65.03999999999999,
    "text": " human feedback. John, welcome to the show and thanks so much for being here."
  },
  {
    "end": 71.75999999999999,
    "start": 68.16,
    "text": " Thanks a lot for having me. You were literally one of the first people I thought of when I started"
  },
  {
    "end": 77.6,
    "start": 71.75999999999999,
    "text": " the show three years back. Thanks, I'm honored. It means a lot to me to have you here today. I definitely"
  },
  {
    "end": 83.12,
    "start": 77.6,
    "text": " remember you were nuts and bolts of deep RL video back in the day and watching that multiple times"
  },
  {
    "end": 88.88000000000001,
    "start": 83.12,
    "text": " and gaining a lot from that. You helped a generation of RL practitioners back then. By the way,"
  },
  {
    "end": 95.52000000000001,
    "start": 88.88000000000001,
    "text": " there's going to be a reboot of the nuts and bolts presentation. I got invited to give a talk"
  },
  {
    "end": 101.92,
    "start": 95.52000000000001,
    "text": " at NERPS this year on it. I'll have to revamp the guidelines and everything. That'll be fun."
  },
  {
    "end": 107.12,
    "start": 101.92,
    "text": " Oh, that's awesome. Can't wait for that. You were clearly one of the earlier pioneers in deep RL."
  },
  {
    "end": 112.4,
    "start": 107.12,
    "text": " How did you choose to move your focus to RL from human feedback? Why is that an important problem?"
  },
  {
    "end": 117.84,
    "start": 112.4,
    "text": " Why is that important to you? After GB3 was trained, I was blown away by how smart it was and I"
  },
  {
    "end": 122.32000000000001,
    "start": 117.84,
    "text": " realized the next frontier was figuring out how to make language models actually useful. I'm still"
  },
  {
    "end": 128.4,
    "start": 122.32000000000001,
    "text": " really interested in RL but solving RL benchmarks isn't the end of the story. To use your RL"
  },
  {
    "end": 134.08,
    "start": 128.4,
    "text": " algorithm you need a reward function. Whereas the reward function come from in RL benchmarks,"
  },
  {
    "end": 138.16,
    "start": 134.08,
    "text": " you usually just code up the reward function. But if you're not in a simulator environment,"
  },
  {
    "end": 144.07999999999998,
    "start": 138.16,
    "text": " that doesn't work. What we have to do in any kind of real-world use case is have humans look at"
  },
  {
    "end": 149.04,
    "start": 144.07999999999998,
    "text": " what the AI did and decide if it was good or bad. How exactly do you define this reward"
  },
  {
    "end": 154,
    "start": 149.04,
    "text": " becomes a really challenging and important problem, especially as the tasks get harder to evaluate?"
  },
  {
    "end": 159.2,
    "start": 154,
    "text": " Another angle on this is that language models are very smart but it's hard to get them to do"
  },
  {
    "end": 164.24,
    "start": 159.2,
    "text": " anything useful. A big part of that is they're not necessarily trying to do what you want. They're"
  },
  {
    "end": 168.88,
    "start": 164.24,
    "text": " just trying to imitate the training corpus. That means there's a big opportunity to improve"
  },
  {
    "end": 173.84,
    "start": 168.88,
    "text": " them a lot by just giving them the right objective. That's what we can do by applying RL to these"
  },
  {
    "end": 181.12,
    "start": 174.64000000000001,
    "text": " language models using human feedback to define the reward. Is human feedback harder or"
  },
  {
    "end": 185.92000000000002,
    "start": 181.12,
    "text": " very different in some way than using a synthetic reward? There are a lot of new complications."
  },
  {
    "end": 192.56,
    "start": 187.36,
    "text": " You have to collect a data set dynamically. You're always in the business of building data sets of"
  },
  {
    "end": 199.12,
    "start": 192.56,
    "text": " human preferences. Often the data quality there matters more than various algorithmic details."
  },
  {
    "end": 204.32,
    "start": 199.12,
    "text": " You also have to think a lot about exactly how you're giving the task to the human trainers"
  },
  {
    "end": 208.32,
    "start": 204.32,
    "text": " and various other things that you wouldn't have thought about if you just had a programmatic reward"
  },
  {
    "end": 213.44,
    "start": 208.32,
    "text": " function. Does the difference between human-raders or the noisiness of the reward signal cost any"
  },
  {
    "end": 220.56,
    "start": 213.44,
    "text": " problems? I would say the noise definitely you need to be below some threshold of noise to learn"
  },
  {
    "end": 226.64000000000001,
    "start": 220.56,
    "text": " anything. I think in general if you have a large noisy data set that can be as good as a smaller"
  },
  {
    "end": 231.6,
    "start": 226.64000000000001,
    "text": " clean data set. Actually, noise isn't the thing that worries me the most. It's more that there are"
  },
  {
    "end": 238,
    "start": 231.6,
    "text": " sometimes consistent biases that people have. For example, in settings like question answering"
  },
  {
    "end": 244.4,
    "start": 238,
    "text": " or settings where you have a model writing some text, often people prefer longer answers. You end"
  },
  {
    "end": 249.36,
    "start": 244.4,
    "text": " up with these very verbose answers. If you're not careful with the instructions that is. You can"
  },
  {
    "end": 256.40000000000003,
    "start": 249.36,
    "text": " also instruct people the raiders to reward brevity. But without yet, if you're not careful you can"
  },
  {
    "end": 262,
    "start": 257.04,
    "text": " incentivize the wrong kinds of behaviors. So let's move to some of your recent work. First up is"
  },
  {
    "end": 268.40000000000003,
    "start": 262,
    "text": " WebGPT. Browser assisted question answering with human feedback. That's a Nekano at all with yourself"
  },
  {
    "end": 273.84000000000003,
    "start": 268.40000000000003,
    "text": " as a co-author in 2021. Can you tell us what is the main idea of this paper? What is WebGPT?"
  },
  {
    "end": 280.23999999999995,
    "start": 273.84,
    "text": " In WebGPT, we basically took our language models and we hooked them up to a web browser so they"
  },
  {
    "end": 285.35999999999996,
    "start": 280.23999999999995,
    "text": " could retrieve information from the web. They can write an answer by summarizing the relevant pages"
  },
  {
    "end": 290.08,
    "start": 285.35999999999996,
    "text": " from the web. That way if you're asking a question about current events or a question that requires"
  },
  {
    "end": 295.35999999999996,
    "start": 290.08,
    "text": " some detailed scientific or technical knowledge, this AI can go out and look up the answer and"
  },
  {
    "end": 301.67999999999995,
    "start": 295.35999999999996,
    "text": " with detailed citations to its sources. I would say there's two interesting points to this. One is"
  },
  {
    "end": 306.24,
    "start": 301.68,
    "text": " we were exploring whether you could turn language models into a kind of agent. There's a lot of data"
  },
  {
    "end": 310.32,
    "start": 306.24,
    "text": " on the web of different texts that people have written. But there's not a lot of data that shows"
  },
  {
    "end": 316.24,
    "start": 310.32,
    "text": " how to actually do some multi-step process. So it's not that clear, uprearry whether you can get a"
  },
  {
    "end": 321.68,
    "start": 316.24,
    "text": " language model to actually carry out some iterative process. We just have a lot of data like writing"
  },
  {
    "end": 326.16,
    "start": 321.68,
    "text": " essays and having chats and so forth. So that was one thing we were exploring here and I think"
  },
  {
    "end": 332.8,
    "start": 326.16,
    "text": " the answer was affirmative. We can get an agent to basically use a set of tools that we give it."
  },
  {
    "end": 338.16,
    "start": 332.8,
    "text": " In this case the browsing commands like searchings, scroll link, click on links. The second"
  },
  {
    "end": 344.24,
    "start": 338.16,
    "text": " theme of this paper was around truthfulness. I mean a big issue with language models is I mean"
  },
  {
    "end": 349.76000000000005,
    "start": 344.24,
    "text": " they're not very reliable at giving you true information. They know a vastly superhuman amount. But"
  },
  {
    "end": 354.64000000000004,
    "start": 349.76000000000005,
    "text": " if you prompt them in the wrong way they'll just output lots of plausible sounding nonsense. So"
  },
  {
    "end": 359.84,
    "start": 354.64,
    "text": " how to fix that is a big research question or one of the biggest research questions in the"
  },
  {
    "end": 364.32,
    "start": 359.84,
    "text": " world of language models. I think it's going to be challenging to fully fix it but I think a big"
  },
  {
    "end": 370.32,
    "start": 364.32,
    "text": " part of the story involves retrieval and having models write answers that contain citations."
  },
  {
    "end": 375.28,
    "start": 370.32,
    "text": " Citations to try trusted sources. So a person who's checking over the answer doesn't have to go and"
  },
  {
    "end": 379.91999999999996,
    "start": 375.28,
    "text": " try to figure out where the model might have gotten this idea. They can go and directly look at"
  },
  {
    "end": 387.6,
    "start": 379.92,
    "text": " the source and see if it supports the AI statement. With WebGBT we just wanted to see if we do give"
  },
  {
    "end": 392.40000000000003,
    "start": 387.6,
    "text": " the language model a really flexible interface to the web. Can we have it answer hard questions"
  },
  {
    "end": 398.32,
    "start": 392.40000000000003,
    "text": " truthfully using like with the help of all these citations. And it's actually really non-trivial"
  },
  {
    "end": 403.76,
    "start": 398.32,
    "text": " because if you look at the data that we use the Reddit explain it like on five. The questions"
  },
  {
    "end": 408.08000000000004,
    "start": 403.76,
    "text": " are really varied like some of them are about science, history, current events. Like our"
  },
  {
    "end": 413.84,
    "start": 408.08,
    "text": " Raiders didn't necessarily know anything about these topics but still they had to judge the answers"
  },
  {
    "end": 418.88,
    "start": 413.84,
    "text": " written detailed answers. So it would have been really hard to do it without the supporting"
  },
  {
    "end": 425.12,
    "start": 418.88,
    "text": " citations. So we kind of validated that we could get good feedback in a hard domain like this"
  },
  {
    "end": 431.12,
    "start": 425.12,
    "text": " with the help of citations. Can you talk about where the idea for WebGBT came from? Is that an idea"
  },
  {
    "end": 435.12,
    "start": 431.12,
    "text": " you've had kicking around for a while or was it something that came up recently before the"
  },
  {
    "end": 441.36,
    "start": 435.12,
    "text": " paper? How did that play out? Some of the ideas had been floating around like we thought that we"
  },
  {
    "end": 447.12,
    "start": 441.36,
    "text": " actually had a project at OpenAI very early on a world called World of Bits. We were looking at"
  },
  {
    "end": 452.16,
    "start": 447.12,
    "text": " controlling web browsers or doing tasks that involve tasks on the internet with the web browser"
  },
  {
    "end": 458.4,
    "start": 452.16,
    "text": " but it was way too early at the time. So we kind of abandoned it for a few years. Actually we"
  },
  {
    "end": 462.8,
    "start": 458.4,
    "text": " were trying to back then we were trying to do it with full visual input. So we thought yeah we could"
  },
  {
    "end": 469.12,
    "start": 462.8,
    "text": " give some instructions to the agent like go and figure out figure out the address of this"
  },
  {
    "end": 475.68,
    "start": 469.84000000000003,
    "text": " building or something. The agent would go and search the web or use Google Maps or whatever"
  },
  {
    "end": 479.92,
    "start": 475.68,
    "text": " to figure out the answer. And we were trying to do this all in pixels that obviously didn't work"
  },
  {
    "end": 486.16,
    "start": 479.92,
    "text": " very well. But now we have these great language models on the work on text data. We can also"
  },
  {
    "end": 493.12,
    "start": 486.16,
    "text": " extract the text out of web pages to get most of the information. We can't really interact with"
  },
  {
    "end": 498.16,
    "start": 493.12,
    "text": " a lot of dynamic websites. Yeah, where there's a lot of JavaScript and images and so forth. But"
  },
  {
    "end": 504.64000000000004,
    "start": 498.16,
    "text": " as long as it's just browsing and reading text we're fine. So yeah we had good enough models and"
  },
  {
    "end": 510.8,
    "start": 504.64000000000004,
    "text": " that made it kind of feasible to revisit this idea of using the internet as an environment."
  },
  {
    "end": 516.32,
    "start": 510.8,
    "text": " So I would say that was one of the sources of inspiration that long-stinted, that long kind of"
  },
  {
    "end": 522.4,
    "start": 516.32,
    "text": " thread about like using the internet as an environment. Another motivation was just after we got"
  },
  {
    "end": 529.12,
    "start": 523.2,
    "text": " after we started playing with GPD3 we noticed that it had all these problems with factual"
  },
  {
    "end": 535.52,
    "start": 529.12,
    "text": " accuracy and the reliability of the information it was giving us. So that kind of motivated doing"
  },
  {
    "end": 540.4,
    "start": 535.52,
    "text": " more research on how to make language models more truthful. We were kind of brainstorming what to"
  },
  {
    "end": 547.04,
    "start": 540.4,
    "text": " do there and we went through some docs and eventually decided that we wanted to try some question"
  },
  {
    "end": 551.92,
    "start": 547.04,
    "text": " answering like using the web, looking up knowledge on the web to help answer questions. So actually"
  },
  {
    "end": 556.24,
    "start": 551.92,
    "text": " the original version of the project used trivia questions. So there's another, there's this"
  },
  {
    "end": 562.16,
    "start": 556.24,
    "text": " well-known data set trivia QA that has some basic trivia questions. So we first worked a little"
  },
  {
    "end": 569.12,
    "start": 562.16,
    "text": " bit on that data set and tried to see if we could boost the model's accuracy by giving it web search"
  },
  {
    "end": 576,
    "start": 569.12,
    "text": " and yeah that actually works quite straight, that worked pretty easily. So then we decided to move on"
  },
  {
    "end": 582.72,
    "start": 576,
    "text": " to long-form question answering and so that gave us the, that was the project we ended up working on"
  },
  {
    "end": 589.12,
    "start": 582.72,
    "text": " for a while. It seems like you use a few different data sets here and a number of different training"
  },
  {
    "end": 594.96,
    "start": 589.12,
    "text": " methods. I'll just mention the last behavior cloning, reward modeling, reinforcement learning,"
  },
  {
    "end": 601.76,
    "start": 594.96,
    "text": " and rejection sampling. So we were using a fairly standard methodology which was actually adapted"
  },
  {
    "end": 609.2,
    "start": 601.76,
    "text": " from previous work on RL from Human Preferences. So the pipeline is you first train a model with"
  },
  {
    "end": 615.44,
    "start": 609.2,
    "text": " supervised learning where you you have human demonstrators show how to do the task, like show how to map"
  },
  {
    "end": 620.8000000000001,
    "start": 615.44,
    "text": " from observations to actions. Yeah so that's the supervised learning or behavior cloning step then we"
  },
  {
    "end": 628.7199999999999,
    "start": 620.8,
    "text": " train a reward model or preference model. It looks at two actions or two out trajectories and decides"
  },
  {
    "end": 633.76,
    "start": 628.7199999999999,
    "text": " which one is better. In this case like in a question answering setting you're looking at two answers"
  },
  {
    "end": 638.56,
    "start": 633.76,
    "text": " and deciding which answer is better and we use that to train a reward model that assigns higher score"
  },
  {
    "end": 643.04,
    "start": 638.56,
    "text": " to the good answers than the bad ones. Then you do reinforcement learning against that reward function"
  },
  {
    "end": 648.16,
    "start": 643.04,
    "text": " and of course you can iterate these last two steps. After you do a little RL now you're, you sort of"
  },
  {
    "end": 653.4399999999999,
    "start": 648.16,
    "text": " exploited some of the flaws of the reward model like or some of the noise in the reward model and"
  },
  {
    "end": 658.9599999999999,
    "start": 653.4399999999999,
    "text": " it's not necessarily accurate on your new distribution of data. You recollect more pairs of samples"
  },
  {
    "end": 665.28,
    "start": 658.9599999999999,
    "text": " and refit this preference model and then you do another iteration of RL. So that's like that's"
  },
  {
    "end": 670.9599999999999,
    "start": 665.28,
    "text": " the whole RL from Human Feedback Pipeline and there's this other idea called rejection sampling"
  },
  {
    "end": 676.48,
    "start": 670.9599999999999,
    "text": " or best event sampling and in general you can do other kinds of search too where instead of doing"
  },
  {
    "end": 681.52,
    "start": 676.48,
    "text": " RL once you have your reward model you can just search against that reward model so you can take"
  },
  {
    "end": 687.6,
    "start": 681.52,
    "text": " a bunch of collect a bunch of samples and re-rank them with the reward model and take the best one"
  },
  {
    "end": 694.08,
    "start": 687.6,
    "text": " as your action. Kind of like NPC. Yeah exactly. Yeah kind of depends exactly what setting you're in"
  },
  {
    "end": 699.76,
    "start": 694.64,
    "text": " what you can do. If you're in a setting where there's some environment you're interacting with then"
  },
  {
    "end": 705.44,
    "start": 699.76,
    "text": " you would have to simulate your, you would have to simulate the dynamics of your environment which"
  },
  {
    "end": 711.84,
    "start": 705.44,
    "text": " yeah so that would look kind of like NPC. In our case we were the only thing we had to learn a model of"
  },
  {
    "end": 718.24,
    "start": 711.84,
    "text": " was the human preference so like we're it's a question answering setting so it's really like a"
  },
  {
    "end": 723.2,
    "start": 718.24,
    "text": " contextual banded problem so it's kind of straightforward to take a bunch of sample a bunch of"
  },
  {
    "end": 730.8000000000001,
    "start": 723.2,
    "text": " actions where each action is a full answer and re-rank them or search against the search over answers."
  },
  {
    "end": 736.4,
    "start": 730.8,
    "text": " So in terms of the action space was it the action space just a list of commands or is it still"
  },
  {
    "end": 743.76,
    "start": 736.4,
    "text": " generating tokens like a regular generative mode? We were generating tokens. We had two phases of"
  },
  {
    "end": 751.12,
    "start": 743.76,
    "text": " like in each episode of the RL task so there is first a browsing phase where where the model goes"
  },
  {
    "end": 757.04,
    "start": 751.12,
    "text": " and it issues searches and clicks on things and quotes relevant information like if it sees"
  },
  {
    "end": 761.92,
    "start": 757.04,
    "text": " something useful on the page it'll it'll quote it using this quote commands and then once it's"
  },
  {
    "end": 769.28,
    "start": 762.8,
    "text": " browse it's done browsing it'll issue another command called end browsing and it'll write its"
  },
  {
    "end": 775.68,
    "start": 769.28,
    "text": " answer that's also expressed in tokens but really we rolled this all into one big RL task where"
  },
  {
    "end": 781.28,
    "start": 775.68,
    "text": " your episode involves browsing and writing out the answer and it's all one big RL episode."
  },
  {
    "end": 785.28,
    "start": 781.28,
    "text": " Did you think this is going to work well or were you kind of surprised? At the very beginning of the"
  },
  {
    "end": 790.72,
    "start": 785.28,
    "text": " project we didn't know if it was going to work or not. Like after we did the initial experiments"
  },
  {
    "end": 797.68,
    "start": 790.72,
    "text": " with Trivia QA which actually didn't take that long to get running then it became pretty clear"
  },
  {
    "end": 802.24,
    "start": 797.68,
    "text": " that it would work that the browsing part worked at least and we already know that we can get"
  },
  {
    "end": 807.8399999999999,
    "start": 802.24,
    "text": " these models to write pretty good long form text with a bunch of if you give them a bunch of"
  },
  {
    "end": 814.16,
    "start": 807.8399999999999,
    "text": " snippets of text that they they can cite. So I noticed the the the human raiders task was"
  },
  {
    "end": 818.88,
    "start": 814.16,
    "text": " quite complicated as it was a long guide and there was many types of feedback that they were giving"
  },
  {
    "end": 823.52,
    "start": 818.88,
    "text": " but in the end the paper said that only the final rating was used so I was just curious if you"
  },
  {
    "end": 827.28,
    "start": 823.52,
    "text": " hadn't commented about that like why do you think maybe the model couldn't use that extra feedback"
  },
  {
    "end": 833.12,
    "start": 827.28,
    "text": " whereas it was maybe just too much or not enough samples. Yeah that's been one frustrating"
  },
  {
    "end": 840.0799999999999,
    "start": 833.8399999999999,
    "text": " finding so far in in that project and also some other projects we've had the same finding but"
  },
  {
    "end": 845.76,
    "start": 840.08,
    "text": " you have your raiders go through this long process for each for each comparison they do where"
  },
  {
    "end": 851.12,
    "start": 845.76,
    "text": " they're comparing a pair of answers and then you only use one bit of information from the whole"
  },
  {
    "end": 855.84,
    "start": 851.12,
    "text": " from this whole process which might have taken like half an hour. It seems like it would be better if"
  },
  {
    "end": 862.08,
    "start": 855.84,
    "text": " we if we were able to extract more information more about the process they went through in arriving"
  },
  {
    "end": 867.0400000000001,
    "start": 862.08,
    "text": " at the answer. So we did collect all sorts of other information like we had them provide ratings"
  },
  {
    "end": 873.4399999999999,
    "start": 867.04,
    "text": " along several different axes like coherence and factual accuracy and so forth but in the end"
  },
  {
    "end": 880.24,
    "start": 874.3199999999999,
    "text": " we didn't really get much of a boost out of using any of this this other information so I'd say"
  },
  {
    "end": 886.56,
    "start": 881.12,
    "text": " it seems like there's it should be possible to do better but unfortunately this methodology which"
  },
  {
    "end": 893.68,
    "start": 886.56,
    "text": " seems kind of dumb so far it's hard to be and people have tried various other ideas for like how"
  },
  {
    "end": 898,
    "start": 893.68,
    "text": " to use human feedback instead of you getting these preference scores there various other things you"
  },
  {
    "end": 903.68,
    "start": 898,
    "text": " can do like you can have them right critiques and edit or maybe edit the responses. Yeah I think"
  },
  {
    "end": 911.12,
    "start": 903.68,
    "text": " some of these things are are also promising but yeah this methodology of collecting preference data"
  },
  {
    "end": 917.04,
    "start": 911.12,
    "text": " works well. Yeah I think it's it's still an open area of research. Oh yeah regarding the really"
  },
  {
    "end": 922.64,
    "start": 917.04,
    "text": " long instructions. Yeah I think for any of these tasks there is a lot of subtlety in how to do the"
  },
  {
    "end": 929.4399999999999,
    "start": 922.64,
    "text": " task properly and so we ended up adding more and more details of like what do you do in this situation"
  },
  {
    "end": 933.76,
    "start": 929.4399999999999,
    "text": " and what do you do in that situation. I think it's starting to get pretty unwieldy with these really"
  },
  {
    "end": 940.8,
    "start": 933.76,
    "text": " long instruction manuals so there's some promising ideas for how to address this like there's a"
  },
  {
    "end": 946.4,
    "start": 940.8,
    "text": " paper from DeepMind recently Sparrow that used basically broke down the task and they trained"
  },
  {
    "end": 952.3199999999999,
    "start": 947.04,
    "text": " they basically had people look at one aspect of the one aspect of the response at a time"
  },
  {
    "end": 957.0400000000001,
    "start": 952.32,
    "text": " and and then they had a way of combining these different rule specific they would train a bunch"
  },
  {
    "end": 961.6800000000001,
    "start": 957.0400000000001,
    "text": " of rule specific reward models and then combine them at the end. Yeah I think there's some other"
  },
  {
    "end": 967.5200000000001,
    "start": 961.6800000000001,
    "text": " interesting ideas for how to how to make this process better. So I gather that from your answer"
  },
  {
    "end": 972.6400000000001,
    "start": 967.5200000000001,
    "text": " about WebGPT and the whole idea of WebGPT is that you want the the language model type access to"
  },
  {
    "end": 978.48,
    "start": 972.6400000000001,
    "text": " external knowledge but I wonder where you think the line should really be in terms of what a"
  },
  {
    "end": 982.88,
    "start": 978.48,
    "text": " language model should know and what the language model should look up and maybe what the language"
  },
  {
    "end": 987.6800000000001,
    "start": 982.88,
    "text": " model should not know or not purport to know. Do you have opinions about that? Yeah let's see"
  },
  {
    "end": 994.16,
    "start": 988.4,
    "text": " like some people are advocating for very small language models that have like no external knowledge"
  },
  {
    "end": 998.5600000000001,
    "start": 994.16,
    "text": " aside from language I guess would be the extreme position and then other people other people"
  },
  {
    "end": 1002.5600000000001,
    "start": 998.5600000000001,
    "text": " talked about language models that just know everything as opposed to having an external knowledge"
  },
  {
    "end": 1008.24,
    "start": 1002.5600000000001,
    "text": " source. There's some interesting questions there so I think it is a little hard to separate knowledge"
  },
  {
    "end": 1015.6,
    "start": 1008.24,
    "text": " factual knowledge from understanding. So as humans we get by like not memorizing all sorts of"
  },
  {
    "end": 1021.6800000000001,
    "start": 1016.4,
    "text": " facts and just knowing that we can look them up if needed. For working on a specific domain it is"
  },
  {
    "end": 1028.88,
    "start": 1021.6800000000001,
    "text": " useful to like have a lot of facts internalized so that you can recall them very quickly and"
  },
  {
    "end": 1034.24,
    "start": 1028.88,
    "text": " kind of combine them combine them in your head. So I wouldn't take an extreme position on either"
  },
  {
    "end": 1041.44,
    "start": 1034.24,
    "text": " side I would say I think retrieval is going to be really useful just at the very least for"
  },
  {
    "end": 1048.88,
    "start": 1041.44,
    "text": " current events but also I don't think we want to try to pack all human knowledge into the weights"
  },
  {
    "end": 1054.72,
    "start": 1048.88,
    "text": " of a neural net. On the other hand I think people have had a lot of luck just scaling up models and"
  },
  {
    "end": 1061.44,
    "start": 1055.68,
    "text": " like as they soak up more factual knowledge they also get better at reasoning and other things"
  },
  {
    "end": 1068,
    "start": 1061.44,
    "text": " and I think I haven't seen any demonstrations of tiny models that just do lots of retrieval"
  },
  {
    "end": 1073.68,
    "start": 1068,
    "text": " and save all their weights for reasoning. Yeah I just haven't seen any evidence of this"
  },
  {
    "end": 1078.8,
    "start": 1073.68,
    "text": " or that or I haven't seen any successful attempts at making this. Let's move on to training"
  },
  {
    "end": 1084.16,
    "start": 1078.8,
    "text": " language models to follow instructions with human feedback that was uyang et al and that was 2022"
  },
  {
    "end": 1088.72,
    "start": 1084.16,
    "text": " with yourself as a co-author. Can you tell us the main idea with this paper? This is the instruct"
  },
  {
    "end": 1094.64,
    "start": 1088.72,
    "text": " GPT paper. What does instruct GPT and what's going on here? Instruct GPT is a language model that's"
  },
  {
    "end": 1099.44,
    "start": 1094.64,
    "text": " fine tuned to follow instructions and it's in fact the one that you can play with if you go to"
  },
  {
    "end": 1105.84,
    "start": 1100.08,
    "text": " the open AI website you get a big text box and you can write some text and then press the button"
  },
  {
    "end": 1112.24,
    "start": 1105.84,
    "text": " to generate a completion. So the idea here was I mean language models are pretty useful and you can"
  },
  {
    "end": 1117.84,
    "start": 1112.96,
    "text": " sometimes get them to do what you want by prompting them just right. This idea of few shot"
  },
  {
    "end": 1123.52,
    "start": 1117.84,
    "text": " prompting has been become pretty popular where you give a few examples like a few question answer"
  },
  {
    "end": 1128.3999999999999,
    "start": 1123.52,
    "text": " examples and then if you ask another question it'll hopefully provide an answer in the same style."
  },
  {
    "end": 1133.84,
    "start": 1128.3999999999999,
    "text": " So the idea yeah so if you can get language models to do great things with prompting but prompting"
  },
  {
    "end": 1139.04,
    "start": 1133.84,
    "text": " is itself an arg and it's tricky to get right and it's also kind of not necessarily getting the"
  },
  {
    "end": 1143.6799999999998,
    "start": 1139.04,
    "text": " best possible performance out of the model. If you just take a raw language model and you try to"
  },
  {
    "end": 1148.4,
    "start": 1143.68,
    "text": " you try to talk to it like you ask it a question it probably it doesn't know that it should actually"
  },
  {
    "end": 1154.0800000000002,
    "start": 1148.4,
    "text": " answer that question as well as possible. For all it knows you want it to give a joke answer or"
  },
  {
    "end": 1160,
    "start": 1154.0800000000002,
    "text": " a riddle or something. Yeah so the idea of instruct GPT was let's make a kind of small change"
  },
  {
    "end": 1164.4,
    "start": 1160,
    "text": " for our language models so that they're much easier to use. In particular we're going to train them"
  },
  {
    "end": 1171.3600000000001,
    "start": 1164.4,
    "text": " to if you have a piece of text where there's an instruction the model will try to follow that"
  },
  {
    "end": 1176.6399999999999,
    "start": 1171.36,
    "text": " instruction to the best of its abilities and pretty much anything can be an instruction like"
  },
  {
    "end": 1183.04,
    "start": 1176.6399999999999,
    "text": " you can have a the instruction can be to continue a chat or it can be to like summarize like"
  },
  {
    "end": 1190.8,
    "start": 1183.04,
    "text": " summarize this text or give me a list of names for my company that sells widgets. Yeah instructions"
  },
  {
    "end": 1195.84,
    "start": 1190.8,
    "text": " can be anything and that makes that makes this kind of model very powerful. So that was kind of"
  },
  {
    "end": 1199.76,
    "start": 1195.84,
    "text": " that's the idea of an instruction following model it's like a model that can do anything that"
  },
  {
    "end": 1204.16,
    "start": 1199.76,
    "text": " you specify with an instruction and by the way I wasn't a core contributor to this work I was"
  },
  {
    "end": 1212.16,
    "start": 1204.8,
    "text": " more involved with like getting the RL infrastructure and some of the RL training details"
  },
  {
    "end": 1218.24,
    "start": 1212.16,
    "text": " like helping out with that that stuff. But anyway yeah what we did in this project was we ran this"
  },
  {
    "end": 1224,
    "start": 1218.24,
    "text": " this whole methodology that I just described of RL from even preferences in this instruction"
  },
  {
    "end": 1230.32,
    "start": 1224,
    "text": " following setting. So we did supervised fine tuning, collected preference data, trained a reward"
  },
  {
    "end": 1236.72,
    "start": 1230.32,
    "text": " model and then did RL against that reward model and one interesting detail is actually whereas the"
  },
  {
    "end": 1244.88,
    "start": 1236.72,
    "text": " original initial data was just collected using contractors. At a certain point we had the the API"
  },
  {
    "end": 1252,
    "start": 1244.88,
    "text": " and it's got this I mean we have this playground on the website where this is where you the big"
  },
  {
    "end": 1258.24,
    "start": 1252,
    "text": " text box where you can use the model. So we we took prompts that people that users had put into"
  },
  {
    "end": 1264.56,
    "start": 1258.24,
    "text": " the into the playground and use those for training like both to collect preference data and to do RL."
  },
  {
    "end": 1271.92,
    "start": 1264.56,
    "text": " So and this is like this is disclosed to users pretty prominently like when when people are using"
  },
  {
    "end": 1276.88,
    "start": 1271.92,
    "text": " the playgrounds you get notified that your prompts might be used for the training and we're also"
  },
  {
    "end": 1282.88,
    "start": 1276.88,
    "text": " careful to train in such a way that we don't memorize any information that was in in the prompts."
  },
  {
    "end": 1288.5600000000002,
    "start": 1282.88,
    "text": " Like it and it explicit like we have a pretty like elaborate process for making sure there's no"
  },
  {
    "end": 1295.0400000000002,
    "start": 1289.2800000000002,
    "text": " like private information being leaked into the model. But anyway yeah that's that's basically the"
  },
  {
    "end": 1302.16,
    "start": 1295.7600000000002,
    "text": " experimental setup and the result was that it works like this methodology works quite well and you"
  },
  {
    "end": 1308.64,
    "start": 1302.16,
    "text": " get a model that's vastly preferred to the base model on this distribution of of realistic prompts"
  },
  {
    "end": 1314.4,
    "start": 1308.64,
    "text": " that people are giving the model often which contain instructions. So the raw like the the raw"
  },
  {
    "end": 1321.68,
    "start": 1314.4,
    "text": " language models generally do a really bad job following instructions but this RL trained instruction"
  },
  {
    "end": 1328.0800000000002,
    "start": 1321.68,
    "text": " following model is is a lot better and it's something like if you just calculate how much better"
  },
  {
    "end": 1333.6,
    "start": 1328.08,
    "text": " it's something like it's as good as a model that's a hundred times bigger. That's a lot. Yeah."
  },
  {
    "end": 1337.36,
    "start": 1333.6,
    "text": " You wanted the model to be truthful is that is that one of the criteria you wanted?"
  },
  {
    "end": 1342.1599999999999,
    "start": 1337.36,
    "text": " Oh yeah truthfulness was one of the criteria. That seems amazing to me that truthfulness is"
  },
  {
    "end": 1346.32,
    "start": 1342.1599999999999,
    "text": " something that I could learn by example like does that mean that truthfulness is somehow"
  },
  {
    "end": 1351.04,
    "start": 1346.32,
    "text": " represented inside the network or because there's no external way for the model to confirm"
  },
  {
    "end": 1355.76,
    "start": 1351.04,
    "text": " whether something is true or false. So how how might it know what is what is true without any"
  },
  {
    "end": 1362.24,
    "start": 1355.76,
    "text": " external reference? I think to some extent there is some internal representation of truthfulness."
  },
  {
    "end": 1367.12,
    "start": 1362.24,
    "text": " So I would say like one way to think about what language models do is they're trained to imitate"
  },
  {
    "end": 1371.52,
    "start": 1367.12,
    "text": " the whole internet and the internet is written by lots of different people and has lots of different"
  },
  {
    "end": 1379.04,
    "start": 1371.52,
    "text": " types of content from fiction to nonfiction to like like technical like detailed technical literature"
  },
  {
    "end": 1386.3999999999999,
    "start": 1379.04,
    "text": " to like jokes and like forum posts whatever. So what the model is basically an ensemble of all"
  },
  {
    "end": 1392.8799999999999,
    "start": 1386.3999999999999,
    "text": " these people who wrote stuff on the internet the raw pre-trained model. When you feed it a prompt"
  },
  {
    "end": 1398.08,
    "start": 1392.8799999999999,
    "text": " what it's doing internally has to be something like figuring out who wrote the first wrote this prompt"
  },
  {
    "end": 1403.04,
    "start": 1398.08,
    "text": " and then trying to continue in that style. So if it thinks it's reading just reading something on the"
  },
  {
    "end": 1409.36,
    "start": 1403.04,
    "text": " Wall Street Betts Reddit it's going to continue on that style but if it thinks it's in the New"
  },
  {
    "end": 1417.6,
    "start": 1409.36,
    "text": " York Times it's going to write in a very different way. So effectively the model must be like calculating"
  },
  {
    "end": 1423.92,
    "start": 1417.6,
    "text": " somewhere like what style is this or what ensemble what's the like narrower ensemble of styles that"
  },
  {
    "end": 1429.76,
    "start": 1423.92,
    "text": " I'm trying to imitate now. At the very least when you do some kind of when you do training like either"
  },
  {
    "end": 1435.12,
    "start": 1429.76,
    "text": " supervised fine tuning or are all from human feedback you can at least like narrow down the set of"
  },
  {
    "end": 1442.56,
    "start": 1435.12,
    "text": " styles the model is producing and try to imitate like the best or the best person in the training set"
  },
  {
    "end": 1448,
    "start": 1442.56,
    "text": " or the best style in the training set and obviously best will differ a lot. So what we'll end up with"
  },
  {
    "end": 1453.68,
    "start": 1448,
    "text": " will depend on our instructions. So if we if we tell I don't know we'll end up with something that"
  },
  {
    "end": 1462.16,
    "start": 1453.68,
    "text": " has kind of safe like not too not too controversial but a bit corporate will end up with something"
  },
  {
    "end": 1468.8,
    "start": 1462.16,
    "text": " like that depending on what our instructions are. So at the very least like we can kind of narrow"
  },
  {
    "end": 1474,
    "start": 1468.8,
    "text": " in on one style instead of having the whole distribution of styles on the internet. I think probably"
  },
  {
    "end": 1479.52,
    "start": 1474,
    "text": " there's more to it than that like we're not just learning about style but the model probably is"
  },
  {
    "end": 1485.04,
    "start": 1479.52,
    "text": " like internally trying to determine if things are if statements are true or not like if the prompt"
  },
  {
    "end": 1490.6399999999999,
    "start": 1485.04,
    "text": " contains incorrect information because that probably would be useful for determining a likely"
  },
  {
    "end": 1495.6,
    "start": 1490.6399999999999,
    "text": " completion. I'm just talking about the raw pre-trained model so I think yeah I think just the"
  },
  {
    "end": 1501.52,
    "start": 1495.6,
    "text": " objective of predicting next tokens probably gives you a lot it forces the model to like the"
  },
  {
    "end": 1506.8799999999999,
    "start": 1501.52,
    "text": " determine if things are true or not. I think for our alfine tuning there's a lot more potential"
  },
  {
    "end": 1513.1200000000001,
    "start": 1506.88,
    "text": " for the model to actually like try to output something truthful as opposed to trying to imitate"
  },
  {
    "end": 1519.1200000000001,
    "start": 1513.1200000000001,
    "text": " a certain style though it's hard to I guess it would be hard to like determine if that's what the"
  },
  {
    "end": 1524.4,
    "start": 1519.1200000000001,
    "text": " model is actually trying to do. So it's almost like the the prompt is guiding the model it's like"
  },
  {
    "end": 1529.5200000000002,
    "start": 1524.4,
    "text": " what corner of the internet do we want to do we want to imitate here and maybe we want to"
  },
  {
    "end": 1534.0800000000002,
    "start": 1529.5200000000002,
    "text": " instruct GPG wants to to focus more on the most more truthful corners of the internet"
  },
  {
    "end": 1539.1999999999998,
    "start": 1534.08,
    "text": " something similar to that. Yeah I would hope so at least I think that's a pretty good though maybe"
  },
  {
    "end": 1543.1999999999998,
    "start": 1539.1999999999998,
    "text": " a little simplistic picture of what's going on. At the very least we should be able to imitate"
  },
  {
    "end": 1549.36,
    "start": 1543.1999999999998,
    "text": " the most truthful corner of the internet. So can you talk about a generalization and how does"
  },
  {
    "end": 1554.56,
    "start": 1549.36,
    "text": " this type of model perform out of distribution? Like I guess if it seems questions that are a bit"
  },
  {
    "end": 1558.3999999999999,
    "start": 1554.56,
    "text": " different than what it was trained on. What happens if we get a little bit away from the training"
  },
  {
    "end": 1563.84,
    "start": 1558.3999999999999,
    "text": " data with the reward models? I mean language models in general generalize surprisingly well and"
  },
  {
    "end": 1568.8,
    "start": 1563.84,
    "text": " I would say overall like these pre-trained models that are trained on super diverse data sets"
  },
  {
    "end": 1573.84,
    "start": 1568.8,
    "text": " from the internet. They tend to generalize quite well or surprisingly well at least it's surprising"
  },
  {
    "end": 1580.72,
    "start": 1573.84,
    "text": " to those of us who were around for the earlier days of machine learning when everything was"
  },
  {
    "end": 1586.08,
    "start": 1580.72,
    "text": " trained from scratch and very fragile. For example if you ask if you provide an instruction in some"
  },
  {
    "end": 1591.28,
    "start": 1586.08,
    "text": " other language even a even a fairly rare language it'll often do a decent job following the"
  },
  {
    "end": 1597.36,
    "start": 1591.28,
    "text": " instruction even if there's zero data in the whole instruction following the training process"
  },
  {
    "end": 1603.84,
    "start": 1597.92,
    "text": " that's in that language and that's just to carry over from the pre-training. So I think generalization"
  },
  {
    "end": 1608.16,
    "start": 1603.84,
    "text": " yeah I think language models generalize quite well. So you asked about reward models I think one"
  },
  {
    "end": 1614.08,
    "start": 1608.16,
    "text": " of the tricky pieces about RL from human feedback is how so you have this reward model and you're"
  },
  {
    "end": 1618.6399999999999,
    "start": 1614.08,
    "text": " actually training against it meaning you're training your policy to have high reward and it's"
  },
  {
    "end": 1623.6000000000001,
    "start": 1618.64,
    "text": " going to exploit the errors in the reward model so it's going to eventually find adversarial"
  },
  {
    "end": 1628.72,
    "start": 1623.6000000000001,
    "text": " examples to the reward model. This is worse than kind of normal out of distribution behavior it's"
  },
  {
    "end": 1634.0800000000002,
    "start": 1628.72,
    "text": " like targeted out of distribution examples so so there are definitely some challenges around"
  },
  {
    "end": 1640.8000000000002,
    "start": 1634.8000000000002,
    "text": " getting reward models to generalize well or generalize as far as possible from the training set."
  },
  {
    "end": 1645.6000000000001,
    "start": 1640.8000000000002,
    "text": " Can these types of agents tell us when they don't know something or is that a hard problem?"
  },
  {
    "end": 1651.9199999999998,
    "start": 1645.6,
    "text": " I'd say sort of if you ask a question that's kind of in the core of the model's knowledge it will"
  },
  {
    "end": 1656,
    "start": 1651.9199999999998,
    "text": " know know the answer and it'll know that it knows. By the way I'm talking about models like the"
  },
  {
    "end": 1661.6,
    "start": 1656,
    "text": " for the instruct model if you ask it about something that's like very simple at the core of its"
  },
  {
    "end": 1666.08,
    "start": 1661.6,
    "text": " knowledge it'll know if you there are certain things that it knows that it doesn't know like"
  },
  {
    "end": 1672.8799999999999,
    "start": 1666.7199999999998,
    "text": " current events where it's been trained to know that it doesn't know certain things in real time but"
  },
  {
    "end": 1678.16,
    "start": 1672.88,
    "text": " if you ask it about something that's kind of on the edge of its knowledge it's it's going to have a"
  },
  {
    "end": 1682.96,
    "start": 1678.16,
    "text": " hard time it's it's necessarily going to be inaccurate. I mean there have been a couple papers"
  },
  {
    "end": 1689.2800000000002,
    "start": 1683.7600000000002,
    "text": " about this question so there is in paper from Anthropic recently called language models"
  },
  {
    "end": 1695.2800000000002,
    "start": 1689.2800000000002,
    "text": " mostly know what they know and there is also a paper from FHI and OpenAI called"
  },
  {
    "end": 1700.48,
    "start": 1696.5600000000002,
    "text": " getting language models to express their uncertainty and words. These language"
  },
  {
    "end": 1706.32,
    "start": 1700.48,
    "text": " models as well as a lot of other models in machine learning are training to maximize likelihood"
  },
  {
    "end": 1711.6,
    "start": 1706.32,
    "text": " so maximize log-prob of data. You're already training them to always predict a distribution of"
  },
  {
    "end": 1718.8,
    "start": 1711.6,
    "text": " outputs. So for language models given a prefix it's predicting a distribution over the next token."
  },
  {
    "end": 1725.76,
    "start": 1718.8,
    "text": " These predictions for the next token like generally are pretty well calibrated but 80% if it puts 80%"
  },
  {
    "end": 1731.6,
    "start": 1725.76,
    "text": " probability on something and you look at all the times when it puts 80% probability on something"
  },
  {
    "end": 1736.72,
    "start": 1731.6,
    "text": " like it's right 80% of the time. Like that's just a result of the training objective. The training"
  },
  {
    "end": 1742.96,
    "start": 1736.72,
    "text": " objective like strongly incentivizes the model to be calibrated meaning it has a reasonable"
  },
  {
    "end": 1748.8,
    "start": 1742.96,
    "text": " estimate of its uncertainty. So at the single token level models definitely are calibrated."
  },
  {
    "end": 1754.8,
    "start": 1748.8,
    "text": " The question is whether they're calibrated on whether this calibration extends to settings where"
  },
  {
    "end": 1760.6399999999999,
    "start": 1754.8,
    "text": " they are generating multi-token outputs or whether they can judge the correctness of some"
  },
  {
    "end": 1766,
    "start": 1760.6399999999999,
    "text": " multi-token statement. So I would say since models are calibrated at the single token level"
  },
  {
    "end": 1772.6399999999999,
    "start": 1766.56,
    "text": " they I think they definitely have the information to be calibrated in these other settings."
  },
  {
    "end": 1778.48,
    "start": 1772.6399999999999,
    "text": " So that's why I think the problem of models knowing what they know isn't actually that hard"
  },
  {
    "end": 1783.9199999999998,
    "start": 1778.48,
    "text": " or at least getting a model to express its uncertainty pretty much as well as a human does"
  },
  {
    "end": 1788.88,
    "start": 1783.92,
    "text": " doesn't feel like an insurmountable problem but there's some practical difficulties to getting"
  },
  {
    "end": 1793.92,
    "start": 1788.88,
    "text": " getting there. People use the phrase AI alignment in different ways. Can you talk about how you see"
  },
  {
    "end": 1800.0800000000002,
    "start": 1793.92,
    "text": " alignment in your work on Aral from human feedback? I think of alignment mostly as the problem of"
  },
  {
    "end": 1805.28,
    "start": 1800.0800000000002,
    "text": " getting the model to try to do the right thing so we can kind of make a distinction between"
  },
  {
    "end": 1811.28,
    "start": 1805.92,
    "text": " what the model is capable of doing. Like if you just take a raw language model and you ask"
  },
  {
    "end": 1815.76,
    "start": 1811.28,
    "text": " it a question like I said before it doesn't know that you actually wanted to give the correct answer"
  },
  {
    "end": 1821.68,
    "start": 1815.76,
    "text": " as opposed to. It might think someone who is not very knowledgeable is answering. By doing some"
  },
  {
    "end": 1826,
    "start": 1821.68,
    "text": " extra training we can get the model to actually try to do the right thing and so I would say that"
  },
  {
    "end": 1832.16,
    "start": 1826,
    "text": " that's the main goal of alignment. So there was an open AI blog post recently that talked about"
  },
  {
    "end": 1840.24,
    "start": 1832.16,
    "text": " the sequence in alignment. One was training AI systems using human feedback to use it training AI"
  },
  {
    "end": 1846,
    "start": 1840.24,
    "text": " systems to assist human evaluation and three training AI systems to do alignment research."
  },
  {
    "end": 1852,
    "start": 1846,
    "text": " So is your current work mostly about this first item and when and how do you see us getting to"
  },
  {
    "end": 1858.4,
    "start": 1852,
    "text": " these other stages? I'm doing some work now on number two training AI systems to assist human feedback."
  },
  {
    "end": 1865.04,
    "start": 1858.4,
    "text": " I think that's sort of becomes increasingly necessary as you start trying to get the systems"
  },
  {
    "end": 1869.44,
    "start": 1865.04,
    "text": " to solve harder and harder problems. When you have models that are kind of very below human level"
  },
  {
    "end": 1875.6000000000001,
    "start": 1869.44,
    "text": " or maybe at human level at a certain task it's pretty straightforward to supervise them. But"
  },
  {
    "end": 1880.4,
    "start": 1875.6000000000001,
    "text": " once they're doing things that are very hard or doing things that require a lot of diverse"
  },
  {
    "end": 1886.96,
    "start": 1880.4,
    "text": " technical knowledge it becomes pretty hard to provide a useful supervision signal. So we have to"
  },
  {
    "end": 1893.3600000000001,
    "start": 1886.96,
    "text": " start doing things like one model writes an answer to do a question and then another model provides"
  },
  {
    "end": 1900.9599999999998,
    "start": 1893.36,
    "text": " a critique of that answer points out some flaws and then the human only has to judge the first answer"
  },
  {
    "end": 1906.7199999999998,
    "start": 1900.9599999999998,
    "text": " after looking at the critique meaning basically the critique helps the human assess the answer. So I"
  },
  {
    "end": 1912.1599999999999,
    "start": 1906.7199999999998,
    "text": " think like that kind of idea is starting to become pretty relevant. A colleague's an I are exploring"
  },
  {
    "end": 1917.28,
    "start": 1912.1599999999999,
    "text": " that kind of idea now. As for assisting alignment research there's some other work at open AI that's"
  },
  {
    "end": 1923.1999999999998,
    "start": 1917.28,
    "text": " starting to explore this. It's also that sort of the for this down the road. So I saw Stuart Russell"
  },
  {
    "end": 1928.96,
    "start": 1923.2,
    "text": " was on your PhD committee and I really enjoyed his book Human Compatible. I wonder if you share"
  },
  {
    "end": 1933.04,
    "start": 1928.96,
    "text": " the idea mentioned in the book that the standard RL framing with this fixed reward signal"
  },
  {
    "end": 1939.8400000000001,
    "start": 1933.8400000000001,
    "text": " is problematic and that agents powerful agents should try to do what we want and maintain some"
  },
  {
    "end": 1945.68,
    "start": 1939.8400000000001,
    "text": " uncertainty about what it is we want and the agents that are too certain will be problematic."
  },
  {
    "end": 1952.56,
    "start": 1945.68,
    "text": " What do you have any thoughts on that idea? I totally agree with that idea. So I think first it's"
  },
  {
    "end": 1959.36,
    "start": 1952.56,
    "text": " really hard to write down a simple reward function that actually captures what we want or what any"
  },
  {
    "end": 1964.96,
    "start": 1959.36,
    "text": " any particular person wants. I can say I want a little more of this or a little more of that but"
  },
  {
    "end": 1971.2,
    "start": 1965.6,
    "text": " you wouldn't want to take that to the extreme. If we build agents that try to cater to our to our"
  },
  {
    "end": 1978.6399999999999,
    "start": 1971.2,
    "text": " wishes we should make sure they're like they have a lot of they have uncertainty about what we"
  },
  {
    "end": 1984.64,
    "start": 1978.64,
    "text": " want or what we value and that that'll also cause them to be a little more cautious and say"
  },
  {
    "end": 1991.92,
    "start": 1984.64,
    "text": " not disturb anything that might be important to us. So yeah I agree with that like Stuart Russell"
  },
  {
    "end": 1998.3200000000002,
    "start": 1991.92,
    "text": " gave a very good like problem definition of what we want AI to do like we want it to basically"
  },
  {
    "end": 2003.6000000000001,
    "start": 1998.3200000000002,
    "text": " we want to jointly like play this game where AI is the AI is trying to figure out what we want"
  },
  {
    "end": 2008.4,
    "start": 2003.6000000000001,
    "text": " and then trying to do that but simultaneously maintaining some uncertainty about what we want."
  },
  {
    "end": 2013.0400000000002,
    "start": 2008.4,
    "text": " I would say if you you start to look at how to get that in practice it actually looks quite a bit"
  },
  {
    "end": 2019.76,
    "start": 2013.0400000000002,
    "text": " like the kind of RL from human feedback that we're working on at OpenAI and others are working on"
  },
  {
    "end": 2027.8400000000001,
    "start": 2019.76,
    "text": " other places. I think yeah I think I see what we're doing as a practical implementation of getting"
  },
  {
    "end": 2033.2,
    "start": 2027.8400000000001,
    "text": " towards this behavior that Russell have described. Do you think of a AGI as an abstract goal or"
  },
  {
    "end": 2037.44,
    "start": 2033.2,
    "text": " are we going to see a model come out one day and people are going to say oh that's the first AGI"
  },
  {
    "end": 2044,
    "start": 2037.44,
    "text": " model like what does it have to do for people to say that? I think people will say that many times"
  },
  {
    "end": 2049.52,
    "start": 2044.72,
    "text": " then realize that it doesn't quite do everything that you want. I think we're going to have a lot of"
  },
  {
    "end": 2055.44,
    "start": 2049.52,
    "text": " like a long series of models that are that are superhuman at most things or at a certain class of"
  },
  {
    "end": 2064.08,
    "start": 2055.44,
    "text": " things but they also have some failure modes and weaknesses. Like I expect us to like see multiple"
  },
  {
    "end": 2070.96,
    "start": 2064.08,
    "text": " models that are proclaimed as AGI and then only after interacting with it a while you do realize"
  },
  {
    "end": 2078,
    "start": 2070.96,
    "text": " it's not quite there. What would you say is the relationship between AGI and RL and AGI and"
  },
  {
    "end": 2084.96,
    "start": 2078,
    "text": " these large language models? How do those concepts fit together? I would say that RL is a useful"
  },
  {
    "end": 2090.96,
    "start": 2084.96,
    "text": " like component of training AGI or an almost essential component. The thing RL lets you do is it"
  },
  {
    "end": 2098.48,
    "start": 2090.96,
    "text": " lets you optimize any objective for the agents. Any objective that is a function of the agents"
  },
  {
    "end": 2105.04,
    "start": 2098.48,
    "text": " behavior. So with pre-training like what we do for language models you're kind of choosing an"
  },
  {
    "end": 2111.04,
    "start": 2105.04,
    "text": " objective that lets us do something with all the training day we have which is all this internet"
  },
  {
    "end": 2116.4,
    "start": 2111.04,
    "text": " text. So we choose this maximum likelihood objective which is basically the only or not the"
  },
  {
    "end": 2122.4,
    "start": 2116.4,
    "text": " only thing but it's like a sensible way to absorb all this knowledge. But then if we really want to"
  },
  {
    "end": 2128.32,
    "start": 2122.4,
    "text": " optimize the agents behavior for a specific objective RL is kind of the only framework that lets you"
  },
  {
    "end": 2133.12,
    "start": 2128.32,
    "text": " do that. Okay John we have a few questions from the audience and I'm just going to pick the two"
  },
  {
    "end": 2139.36,
    "start": 2133.12,
    "text": " that have the highest score in terms of Twitter likes. So the first is from Eric Chang VP of AI"
  },
  {
    "end": 2144.48,
    "start": 2139.36,
    "text": " at a Hello Di Robotics. He asked RL distributions are non-stationary making it hard to reason about"
  },
  {
    "end": 2149.92,
    "start": 2144.48,
    "text": " PPO losses and how that relates to return or generalization. Are there any intermediate plots"
  },
  {
    "end": 2156,
    "start": 2149.92,
    "text": " and visualizations you like to generate to debug or incrementally build up a large scale RL system?"
  },
  {
    "end": 2163.2,
    "start": 2156,
    "text": " Yeah there are definitely some stats that I look at so I will be I'll talk about this in the nuts"
  },
  {
    "end": 2172.2400000000002,
    "start": 2163.2,
    "text": " and bolts like reboot waited a year but I'd say things like you're looking at the explained"
  },
  {
    "end": 2177.3599999999997,
    "start": 2172.24,
    "text": " variants of the value function and looking at the like how many samples are getting clipped in"
  },
  {
    "end": 2183.3599999999997,
    "start": 2177.3599999999997,
    "text": " PPO and what the KL between the what what the KL divergence is between the policy before and after"
  },
  {
    "end": 2191.3599999999997,
    "start": 2183.3599999999997,
    "text": " the update is yeah things like that. And then Ethan the calibar from Miele asks what is your median"
  },
  {
    "end": 2198,
    "start": 2191.3599999999997,
    "text": " estimate for the arrival date of AGI? I think not too far away but I like I said I expect there to"
  },
  {
    "end": 2205.12,
    "start": 2198,
    "text": " be a lot of fall starts I would say expect like like AI to be able to do better a better job than"
  },
  {
    "end": 2211.36,
    "start": 2205.12,
    "text": " humans at most jobs that humans do now five years or so that's not all jobs but most jobs for a while"
  },
  {
    "end": 2215.84,
    "start": 2211.36,
    "text": " we're going to discover things that AI isn't very good at and then where we want to keep humans in"
  },
  {
    "end": 2221.12,
    "start": 2215.84,
    "text": " control so I think there'll be some kind of gradual process over the next 10 or 15 years."
  },
  {
    "end": 2227.2,
    "start": 2221.12,
    "text": " I've been curious about this I see that some RL work is patented but I could not find a TRPO or"
  },
  {
    "end": 2234.3999999999996,
    "start": 2227.2,
    "text": " PPO in I could not find patents on these are those protected patent protected at all or how do you"
  },
  {
    "end": 2240.56,
    "start": 2234.3999999999996,
    "text": " how do you think of intellectual property protection for that kind of work? I haven't ever looked"
  },
  {
    "end": 2246.16,
    "start": 2240.56,
    "text": " looked into patenting anything and open AI hasn't either as far as I know I think the trend over time"
  },
  {
    "end": 2251.12,
    "start": 2246.16,
    "text": " has been for people to take a patent scene machine like a machine learning algorithms last"
  },
  {
    "end": 2256,
    "start": 2251.12,
    "text": " seriously there is this algorithm in computer vision called sift which is like this key point"
  },
  {
    "end": 2262.88,
    "start": 2256,
    "text": " to detector and this was patented I think the the guy who patented it he probably made his"
  },
  {
    "end": 2268.88,
    "start": 2262.88,
    "text": " university some money from the patent but in the end all it did was cause people a lot of annoyance"
  },
  {
    "end": 2275.28,
    "start": 2268.88,
    "text": " because like the people people had to come up with alternative algorithms that like had a"
  },
  {
    "end": 2282.72,
    "start": 2275.28,
    "text": " different acronym and weren't patented so like the open CV open source library would have like"
  },
  {
    "end": 2288.08,
    "start": 2282.72,
    "text": " had to be careful about putting this algorithm in their library because of the patent risks so"
  },
  {
    "end": 2294.7999999999997,
    "start": 2288.64,
    "text": " I think like these patents aren't the patent rights aren't exercise that much and I think big"
  },
  {
    "end": 2301.2,
    "start": 2294.7999999999997,
    "text": " companies like Google will patent a lot of stuff for defensive reasons so if they get in some big"
  },
  {
    "end": 2307.52,
    "start": 2301.2,
    "text": " legal dispute with another company it can be used as like one of the bargaining chips but I think"
  },
  {
    "end": 2315.12,
    "start": 2307.52,
    "text": " I don't think anyone's going to like get sued for royalties for not yeah for not providing royalties"
  },
  {
    "end": 2320.24,
    "start": 2315.12,
    "text": " for the use of some algorithm okay and then there's been a ton of work in RL of course since you"
  },
  {
    "end": 2326.4,
    "start": 2320.24,
    "text": " first published TRPO and BBO but from your point of view if you had to pick a few highlights in"
  },
  {
    "end": 2333.52,
    "start": 2326.4,
    "text": " terms of a few important milestones in in RL algorithms since PPO came out and by the way it's"
  },
  {
    "end": 2341.2,
    "start": 2333.52,
    "text": " amazing that in 2022 we're still using PPO I think quite similar into it's original form is that"
  },
  {
    "end": 2347.44,
    "start": 2341.2,
    "text": " right yeah pretty much yeah so so what would you say are the the biggest highlights for you"
  },
  {
    "end": 2352.96,
    "start": 2348.4,
    "text": " in terms of our algorithm since since you did PPO yeah there's definitely been some interesting"
  },
  {
    "end": 2361.6,
    "start": 2352.96,
    "text": " stuff so I think like a little after PPO there is TD3 and SAC and those are seem like pretty solid"
  },
  {
    "end": 2366.96,
    "start": 2361.6,
    "text": " value-based methods that was one development that was interesting I think like yeah I thought"
  },
  {
    "end": 2375.36,
    "start": 2366.96,
    "text": " museiro and it's and it's like elaborations we're also like efficient zero we're also pretty"
  },
  {
    "end": 2380.72,
    "start": 2375.36,
    "text": " impressive that you can get that good sample efficiency both of the things I just mentioned were"
  },
  {
    "end": 2386.7999999999997,
    "start": 2380.72,
    "text": " kind of well I don't want to say mostly on toy tasks or benchmarks because yeah I'm sure people"
  },
  {
    "end": 2391.92,
    "start": 2386.8,
    "text": " are doing some real things with these algorithms yeah so I think that's that stuff was interesting"
  },
  {
    "end": 2400.2400000000002,
    "start": 2391.92,
    "text": " I think like the whole recent interest in search of interest in the offline RL was also notable"
  },
  {
    "end": 2405.28,
    "start": 2400.2400000000002,
    "text": " I would say the like the stuff we're doing with RL from human feedback is the kind of offline RL"
  },
  {
    "end": 2411.76,
    "start": 2405.92,
    "text": " because we're like we have a fixed dataset and we have a fixed reward modeling dataset and we're"
  },
  {
    "end": 2416.2400000000002,
    "start": 2411.76,
    "text": " training against that this is like offline RL but you're doing it in a different way you're using"
  },
  {
    "end": 2423.2,
    "start": 2416.24,
    "text": " an on-policy algorithm with a reward model as opposed to maybe a more typical way to do offline RL"
  },
  {
    "end": 2427.9199999999996,
    "start": 2423.2,
    "text": " would be use off-policy algorithm would that work here or would that not work here well we're"
  },
  {
    "end": 2434.3999999999996,
    "start": 2427.9199999999996,
    "text": " doing here is kind of like model-based RL because the reward model is like a model of the like the"
  },
  {
    "end": 2440.56,
    "start": 2434.3999999999996,
    "text": " unknown part of the system so like the unknown part of the system here is the is the human"
  },
  {
    "end": 2448.08,
    "start": 2440.56,
    "text": " radar or the human it's not the outputting appending to your list of tokens so this is kind of like"
  },
  {
    "end": 2454.48,
    "start": 2448.08,
    "text": " the work that's like takes a dynamics model at the environment and does some kind of just runs a"
  },
  {
    "end": 2459.44,
    "start": 2454.48,
    "text": " policy grading algorithm against it so it's not like so the idea of running an online algorithm"
  },
  {
    "end": 2465.52,
    "start": 2460.08,
    "text": " against a model that's kind of a well-established idea so I would say the papers that previously"
  },
  {
    "end": 2470.72,
    "start": 2465.52,
    "text": " did this they were in a pretty different regime were in this regime of doing fairly small"
  },
  {
    "end": 2476.24,
    "start": 2470.72,
    "text": " updates to the policy because we have this these awesome pre-trained models and we don't need to"
  },
  {
    "end": 2482.56,
    "start": 2476.24,
    "text": " actually change them that much so yeah we use these online algorithms I'd say part of the reason"
  },
  {
    "end": 2490.4,
    "start": 2482.56,
    "text": " why we can get away with using just an like an online algorithm is because we've been just looking"
  },
  {
    "end": 2495.52,
    "start": 2490.4,
    "text": " at a band a contextual banded problem yeah because we only have like one time step like you get"
  },
  {
    "end": 2501.52,
    "start": 2495.52,
    "text": " a query and you output a response and then that response gets a reward so if we had a like a"
  },
  {
    "end": 2509.04,
    "start": 2501.52,
    "text": " multi-step process such as a conversation where you can't assign a reward until the very end of"
  },
  {
    "end": 2516,
    "start": 2509.04,
    "text": " the conversation and or you had some I don't know some interaction with like some real-world"
  },
  {
    "end": 2520.64,
    "start": 2516,
    "text": " system that's hard to simulate you wouldn't then it wouldn't be S-ray forward to you wouldn't"
  },
  {
    "end": 2526.08,
    "start": 2520.64,
    "text": " be able to use exactly exactly the same methodology you would probably have to use a you would have"
  },
  {
    "end": 2532.24,
    "start": 2526.08,
    "text": " to probably train a Q function or or something like that if you want if you want your method to be"
  },
  {
    "end": 2536.4,
    "start": 2532.24,
    "text": " sample efficient you would probably have to do something slightly different I think we'll we'll"
  },
  {
    "end": 2542.88,
    "start": 2536.4,
    "text": " have to we'll have to start exploring this at some point soon but so far we haven't at least"
  },
  {
    "end": 2550.48,
    "start": 2542.88,
    "text": " I haven't seen any cases in like in the domain I'm looking at that require this but I expect it to"
  },
  {
    "end": 2556.96,
    "start": 2551.44,
    "text": " to be relevant at some point so we had Arvind Shrinivas talking about decision transformer"
  },
  {
    "end": 2561.76,
    "start": 2556.96,
    "text": " on the show recently that was a great episode and I see that you were also a co-author on the"
  },
  {
    "end": 2565.92,
    "start": 2561.76,
    "text": " the 2016 RL squared paper I want to ask you what your thoughts about meta RL"
  },
  {
    "end": 2571.28,
    "start": 2566.6400000000003,
    "text": " Arvind had some interesting things to say about maybe the idea that a transformer could kind of"
  },
  {
    "end": 2575.92,
    "start": 2571.28,
    "text": " supersede the need for an RL algorithm altogether what do you expect from meta RL"
  },
  {
    "end": 2581.36,
    "start": 2575.92,
    "text": " do expect will will still be using human authored RL algorithms in the future yeah that's a pretty"
  },
  {
    "end": 2586.6400000000003,
    "start": 2581.36,
    "text": " bold statement that we don't need we won't need any RL algorithms anymore yeah since the RL squared"
  },
  {
    "end": 2593.0400000000004,
    "start": 2586.6400000000003,
    "text": " paper people have been talking less about meta learning as far as I can tell actually because"
  },
  {
    "end": 2599.28,
    "start": 2593.0400000000004,
    "text": " of sequence modeling has gotten so good like transformer let sequence models so that it's kind"
  },
  {
    "end": 2604.2400000000002,
    "start": 2599.28,
    "text": " of queer the meta learning is just a special case of learning like it's it's just it's just like"
  },
  {
    "end": 2610.0800000000004,
    "start": 2604.2400000000002,
    "text": " a certain kind of long context learning learning involving long episodes and maybe it shouldn't be"
  },
  {
    "end": 2615.36,
    "start": 2610.0800000000004,
    "text": " treated that differently or are addressed with special algorithms I would say yeah the ideas like"
  },
  {
    "end": 2620.6400000000003,
    "start": 2615.36,
    "text": " decision transformer are pretty interesting where you try to reduce RL to supervise learning it's"
  },
  {
    "end": 2626.0800000000004,
    "start": 2620.6400000000003,
    "text": " still not like certain exactly how these compare and performance to RL like people have started to"
  },
  {
    "end": 2633.04,
    "start": 2626.08,
    "text": " analyze that empirically and theoretically and I would say in practice sometimes sometimes it's"
  },
  {
    "end": 2638.48,
    "start": 2633.04,
    "text": " better sometimes it's worse in my experience like it's been worse on the problems that I've"
  },
  {
    "end": 2644.56,
    "start": 2638.48,
    "text": " that I've my colleagues and I have where we've tested it but yeah it's definitely an interesting"
  },
  {
    "end": 2649.12,
    "start": 2644.56,
    "text": " direction Dr. John Schillman thank you so much for sharing your time in your insight with the"
  },
  {
    "end": 2660.08,
    "start": 2649.12,
    "text": " talk our audience today thanks so much thank you"
  }
]</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="1"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Sven Mika</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"Sven Mika of Anyscale on RLlib present and future, Ray and Ray Summit 2022, applied RL in Games / F<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="false "><div class="" dir="auto"><div style="content-visibility: auto">
	<audio controls controlslist="nodownload" preload="none" class="h-dvh max-h-[2.25rem] w-full min-w-[300px] max-w-xs"><source src="https://media.transistor.fm/367e37c3/0b59214b.mp3?src=site" type="audio/mpeg"></audio></div>
	<div class="mt-1 text-[.7rem] leading-tight text-gray-400">https://media.transistorâ€¦14b.mp3?src=site
	<a href="https://media.transistor.fm/367e37c3/0b59214b.mp3?src=site" target="_blank" class="hover:text-gray-700 dark:hover:text-gray-100"><svg class="inline" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path fill="currentColor" d="M10 6v2h12.59L6 24.59L7.41 26L24 9.41V22h2V6H10z"></path></svg></a></div></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">" There's a rise in interest in our finance. We have JPM for example, as well as other companies tha<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[{"end":5.72,"start":0.0,"text":" There's a rise in interest in our finance. We have JPM for example<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="2"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Karol Hausman and Fei Xia</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"Karol Hausman and Fei Xia of Google Research on newly updated (PaLM-)SayCan, Inner Monologue, robot<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="false "><div class="" dir="auto"><div style="content-visibility: auto">
	<audio controls controlslist="nodownload" preload="none" class="h-dvh max-h-[2.25rem] w-full min-w-[300px] max-w-xs"><source src="https://media.transistor.fm/a911824e/a83f417f.mp3?src=site" type="audio/mpeg"></audio></div>
	<div class="mt-1 text-[.7rem] leading-tight text-gray-400">https://media.transistorâ€¦17f.mp3?src=site
	<a href="https://media.transistor.fm/a911824e/a83f417f.mp3?src=site" target="_blank" class="hover:text-gray-700 dark:hover:text-gray-100"><svg class="inline" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path fill="currentColor" d="M10 6v2h12.59L6 24.59L7.41 26L24 9.41V22h2V6H10z"></path></svg></a></div></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">" This type of emergent capability is super interesting for us to see and super exciting for us by u<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[{"end":7.12,"start":0.0,"text":" This type of emergent capability is super interesting for us to se<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="3"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Sai Krishna Gottipati</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"Sai Krishna Gottipati of AI Redefined on RL for synthesizable drug discovery, Multi-Teacher Self-Pl<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="false "><div class="" dir="auto"><div style="content-visibility: auto">
	<audio controls controlslist="nodownload" preload="none" class="h-dvh max-h-[2.25rem] w-full min-w-[300px] max-w-xs"><source src="https://media.transistor.fm/b803a301/db38180a.mp3?src=site" type="audio/mpeg"></audio></div>
	<div class="mt-1 text-[.7rem] leading-tight text-gray-400">https://media.transistorâ€¦80a.mp3?src=site
	<a href="https://media.transistor.fm/b803a301/db38180a.mp3?src=site" target="_blank" class="hover:text-gray-700 dark:hover:text-gray-100"><svg class="inline" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path fill="currentColor" d="M10 6v2h12.59L6 24.59L7.41 26L24 9.41V22h2V6H10z"></path></svg></a></div></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">" TalkRL podcast is all reinforced in learning all the time, featuring brilliant guests both researc<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[{"end":10.32,"start":0.0,"text":" TalkRL podcast is all reinforced in learning all the time, featur<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="4"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Aravind Srinivas 2</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"Aravind Srinivas, Research Scientist at OpenAI, returns to talk Decision Transformer, VideoGPT, cho<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="false "><div class="" dir="auto"><div style="content-visibility: auto">
	<audio controls controlslist="nodownload" preload="none" class="h-dvh max-h-[2.25rem] w-full min-w-[300px] max-w-xs"><source src="https://media.transistor.fm/cb13a30d/98e58583.mp3?src=site" type="audio/mpeg"></audio></div>
	<div class="mt-1 text-[.7rem] leading-tight text-gray-400">https://media.transistorâ€¦583.mp3?src=site
	<a href="https://media.transistor.fm/cb13a30d/98e58583.mp3?src=site" target="_blank" class="hover:text-gray-700 dark:hover:text-gray-100"><svg class="inline" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path fill="currentColor" d="M10 6v2h12.59L6 24.59L7.41 26L24 9.41V22h2V6H10z"></path></svg></a></div></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">" TalkRL podcast is all reinforced in learning all the time, featuring brilliant guests both researc<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[{"end":11.46,"start":0.0,"text":" TalkRL podcast is all reinforced in learning all the time, featur<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="5"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Rohin Shah</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"DeepMind Research Scientist Dr. Rohin Shah on Value Alignment, Learning from Human feedback, Assist<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="false "><div class="" dir="auto"><div style="content-visibility: auto">
	<audio controls controlslist="nodownload" preload="none" class="h-dvh max-h-[2.25rem] w-full min-w-[300px] max-w-xs"><source src="https://media.transistor.fm/5ba5d6af/41bb0ae3.mp3?src=site" type="audio/mpeg"></audio></div>
	<div class="mt-1 text-[.7rem] leading-tight text-gray-400">https://media.transistorâ€¦ae3.mp3?src=site
	<a href="https://media.transistor.fm/5ba5d6af/41bb0ae3.mp3?src=site" target="_blank" class="hover:text-gray-700 dark:hover:text-gray-100"><svg class="inline" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path fill="currentColor" d="M10 6v2h12.59L6 24.59L7.41 26L24 9.41V22h2V6H10z"></path></svg></a></div></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">" TalkRL podcast is all reinforcing learning all the time, featuring brilliant guests, both research<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[{"end":11.0,"start":0.0,"text":" TalkRL podcast is all reinforcing learning all the time, featuring<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="6"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Jordan Terry</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"Jordan Terry on maintaining Gym and PettingZoo, hardware accelerated environments and the future of<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="false "><div class="" dir="auto"><div style="content-visibility: auto">
	<audio controls controlslist="nodownload" preload="none" class="h-dvh max-h-[2.25rem] w-full min-w-[300px] max-w-xs"><source src="https://media.transistor.fm/fe014c06/e475c615.mp3?src=site" type="audio/mpeg"></audio></div>
	<div class="mt-1 text-[.7rem] leading-tight text-gray-400">https://media.transistorâ€¦615.mp3?src=site
	<a href="https://media.transistor.fm/fe014c06/e475c615.mp3?src=site" target="_blank" class="hover:text-gray-700 dark:hover:text-gray-100"><svg class="inline" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path fill="currentColor" d="M10 6v2h12.59L6 24.59L7.41 26L24 9.41V22h2V6H10z"></path></svg></a></div></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">" TalkRL podcast is all reinforced in learning all the time featuring brilliant guests both research<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[{"end":10.72,"start":0.0,"text":" TalkRL podcast is all reinforced in learning all the time featuri<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="7"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Robert Lange</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"Robert Lange on learning vs hard-coding, meta-RL, Lottery Tickets and Minimal Task Representations,<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="false "><div class="" dir="auto"><div style="content-visibility: auto">
	<audio controls controlslist="nodownload" preload="none" class="h-dvh max-h-[2.25rem] w-full min-w-[300px] max-w-xs"><source src="https://media.transistor.fm/935a12e6/8a440dc1.mp3?src=site" type="audio/mpeg"></audio></div>
	<div class="mt-1 text-[.7rem] leading-tight text-gray-400">https://media.transistorâ€¦dc1.mp3?src=site
	<a href="https://media.transistor.fm/935a12e6/8a440dc1.mp3?src=site" target="_blank" class="hover:text-gray-700 dark:hover:text-gray-100"><svg class="inline" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path fill="currentColor" d="M10 6v2h12.59L6 24.59L7.41 26L24 9.41V22h2V6H10z"></path></svg></a></div></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">" Robert Tiacolange is a PhD student working at the Technical University of Berlin. Thanks so much f<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[{"end":8.0,"start":0.0,"text":" Robert Tiacolange is a PhD student working at the Technical Univers<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="8"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">NeurIPS 2021 Political Economy of Reinforcement Learning Systems (PERLS) Workshop</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Dr. Thomas Gilbert and Dr. Mark Nitzberg on the upcoming PERLS Workshop @ NeurIPS 2021</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="false "><div class="" dir="auto"><div style="content-visibility: auto">
	<audio controls controlslist="nodownload" preload="none" class="h-dvh max-h-[2.25rem] w-full min-w-[300px] max-w-xs"><source src="https://media.transistor.fm/3d58a0b7/dc66961c.mp3?src=site" type="audio/mpeg"></audio></div>
	<div class="mt-1 text-[.7rem] leading-tight text-gray-400">https://media.transistorâ€¦61c.mp3?src=site
	<a href="https://media.transistor.fm/3d58a0b7/dc66961c.mp3?src=site" target="_blank" class="hover:text-gray-700 dark:hover:text-gray-100"><svg class="inline" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path fill="currentColor" d="M10 6v2h12.59L6 24.59L7.41 26L24 9.41V22h2V6H10z"></path></svg></a></div></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">" Hi listeners, today we're going to hear about the upcoming Pearls Workshop. That is the political <span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[{"end":5.5600000000000005,"start":0.0,"text":" Hi listeners, today we're going to hear about the up<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="9"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Amy Zhang</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"Amy Zhang shares her work on Invariant Causal Prediction for Block MDPs, Multi-Task Reinforcement L<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="false "><div class="" dir="auto"><div style="content-visibility: auto">
	<audio controls controlslist="nodownload" preload="none" class="h-dvh max-h-[2.25rem] w-full min-w-[300px] max-w-xs"><source src="https://media.transistor.fm/069ca161/19719c6c.mp3?src=site" type="audio/mpeg"></audio></div>
	<div class="mt-1 text-[.7rem] leading-tight text-gray-400">https://media.transistorâ€¦c6c.mp3?src=site
	<a href="https://media.transistor.fm/069ca161/19719c6c.mp3?src=site" target="_blank" class="hover:text-gray-700 dark:hover:text-gray-100"><svg class="inline" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path fill="currentColor" d="M10 6v2h12.59L6 24.59L7.41 26L24 9.41V22h2V6H10z"></path></svg></a></div></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">" This is TalkArail Podcast. All reinforcement learning, all the time. Interviews of brilliant folks<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[{"end":11.0,"start":0.0,"text":" This is TalkArail Podcast."},{"end":13.84,"start":11.0,"text":" Al<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td>
					</tr></tbody></table>
		<div class="bg-linear-to-b sticky left-0 border-t border-dashed border-gray-300 from-gray-100 to-white py-3 text-center font-mono text-xs dark:border-gray-700 dark:from-gray-950 dark:to-gray-900">End of preview. <a href="/datasets/RamAnanth1/talkrl-podcast/viewer/default/train" class="group"><span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Expand</span>
						in <svg class="text-lg mr-0.5 inline -translate-y-px text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>Data Studio
					</a></div></div>




			</div>






</div></div></div></div></div></div></div>

				
					<div class="SVELTE_HYDRATER contents" data-target="RepoCodeCopy" data-props="{}"><div></div></div>
					

					<div class="SVELTE_HYDRATER contents" data-target="SideNavigation" data-props="{&quot;titleTree&quot;:[],&quot;classNames&quot;:&quot;top-6&quot;}">

</div>
					<div class="2xl:pr-6"><div class="prose pl-6 -ml-6 hf-sanitized hf-sanitized-jL_0OFaVzvjZX2BCxV82m">
	<!-- HTML_TAG_START --><h1 class="relative group flex items-center">
	<a rel="nofollow" href="#dataset-card-for-talkrl-podcast" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-card-for-talkrl-podcast">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Card for "talkrl-podcast"
	</span>
</h1>
<p>This dataset is sourced from the <a rel="nofollow" href="https://www.talkrl.com/">TalkRL Podcast website</a> and contains English transcripts of wonderful TalkRL podcast episodes. The transcripts were generated using OpenAI's base Whisper model</p>
<!-- HTML_TAG_END --></div>
</div></section>
			
			<section class="pt-6 border-gray-100 md:pb-24 md:pl-6 md:w-64 lg:w-80 xl:w-96 flex-none order-first md:order-none md:border-l pt-3! md:pt-6!"><dl class="flex items-baseline justify-between"><dt class="text-sm text-gray-500">Downloads last month</dt><div class="mx-4 flex-1 border-b border-dotted"></div><dd class="font-semibold">106</dd></dl>
				
				<div class="divider-column-vertical"></div>
				<div class="grid grid-cols-2 gap-x-2 md:flex md:flex-row md:flex-wrap"><div class="SVELTE_HYDRATER contents" data-target="DatasetAndModelActionsDropdown" data-props="{&quot;classNames&quot;:&quot;order-last&quot;,&quot;discussionsDisabled&quot;:false,&quot;repo&quot;:{&quot;type&quot;:&quot;dataset&quot;,&quot;name&quot;:&quot;RamAnanth1/talkrl-podcast&quot;},&quot;canWrite&quot;:false,&quot;canDisable&quot;:false,&quot;repoIsPrivate&quot;:false,&quot;repoIsGated&quot;:false,&quot;repoIsDisabled&quot;:false,&quot;repoIsAdminFlaggedNFAA&quot;:false,&quot;repoHasBlockedOids&quot;:false}"><div class="order-last"><div class="relative ">
	<button class="btn px-1.5 py-1.5 " type="button">
		
			<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="p-0.5" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><circle cx="16" cy="7" r="3" fill="currentColor"></circle><circle cx="16" cy="16" r="3" fill="currentColor"></circle><circle cx="16" cy="25" r="3" fill="currentColor"></circle></svg>
		
		</button>
	
	
	</div></div>













</div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetLibrary" data-props="{&quot;classNames&quot;:&quot;md:w-full xl:w-auto xl:flex-none&quot;,&quot;libraries&quot;:[{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;datasets&quot;,&quot;function&quot;:&quot;load_dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{},&quot;code&quot;:&quot;from datasets import load_dataset\n\nds = load_dataset(\&quot;RamAnanth1/talkrl-podcast\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;pandas&quot;,&quot;function&quot;:&quot;pd.read_parquet&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;data/train-00000-of-00001-99190af4fdecda9c.parquet&quot;}},&quot;code&quot;:&quot;import pandas as pd\n\ndf = pd.read_parquet(\&quot;hf://datasets/RamAnanth1/talkrl-podcast/data/train-00000-of-00001-99190af4fdecda9c.parquet\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;mlcroissant&quot;,&quot;function&quot;:&quot;Dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;record_set&quot;:&quot;default&quot;,&quot;partial&quot;:false},&quot;code&quot;:&quot;from mlcroissant import Dataset\n\nds = Dataset(jsonld=\&quot;https://huggingface.co/api/datasets/RamAnanth1/talkrl-podcast/croissant\&quot;)\nrecords = ds.records(\&quot;default\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;polars&quot;,&quot;function&quot;:&quot;pl.read_parquet&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;data/train-00000-of-00001-99190af4fdecda9c.parquet&quot;}},&quot;code&quot;:&quot;import polars as pl\n\ndf = pl.read_parquet('hf://datasets/RamAnanth1/talkrl-podcast/data/train-00000-of-00001-99190af4fdecda9c.parquet')\n&quot;}]}]}"><div class="relative md:w-full xl:w-auto xl:flex-none">
	<button class="from-gray-800! to-black! max-xl:mb-2 text-white! gap-1! border-gray-800! dark:border-gray-900!  btn w-full cursor-pointer text-sm" type="button">
		<svg class="mr-1.5 mr-0.5! " xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>
			Use this dataset
		<svg class="-mr-1 text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path d="M16.293 9.293L12 13.586L7.707 9.293l-1.414 1.414L12 16.414l5.707-5.707z" fill="currentColor"></path></svg></button>
	
	
	</div>

</div>
					
					</div>
				<div class="divider-column-vertical"></div>
					<div class="flex flex-col flex-wrap xl:flex-row"><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of downloaded dataset files:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->2.63 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 " href="/datasets/RamAnanth1/talkrl-podcast/tree/refs%2Fconvert%2Fparquet/" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of the auto-converted Parquet files:</div>
		<div class="truncate text-sm group-hover:underline">
			<!-- HTML_TAG_START -->2.63 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Number of rows:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->39<!-- HTML_TAG_END --></div></a></div>
				
				
				<div class="divider-column-vertical"></div>
					<h2 class="text-smd mb-5 flex items-baseline overflow-hidden whitespace-nowrap font-semibold text-gray-800"><svg class="mr-1.5 text-sm inline self-center flex-none text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
						Models trained or fine-tuned on
						<span class="ml-1 truncate font-mono text-[0.87rem] font-medium">RamAnanth1/talkrl-podcast</span></h2>

					<div class="space-y-3"><div class=""><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/davidschulte/ESM_RamAnanth1__talkrl-podcast_default"><div class="w-full truncate"><header class="flex items-center mb-1" title="davidschulte/ESM_RamAnanth1__talkrl-podcast_default"><img alt="" class="w-3 h-3 rounded-full mr-1.5 flex-none flex-none" src="/avatars/ab7fb2af582f38cbc33debb349fb67f2.svg" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">davidschulte/ESM_RamAnanth1__talkrl-podcast_default</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400">
	
				<span class="truncate">Updated
					<time datetime="2025-03-25T11:37:53" title="Tue, 25 Mar 2025 11:37:53 GMT">about 6 hours ago</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					12
				
	
				

				</div></div>
		
	</a></article>
							</div>
						</div>
				

				
				<div class="divider-column-vertical md:hidden"></div></section></div></main>

	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container relative flex flex-col justify-between space-y-2 py-6 text-gray-500 max-md:*:self-start md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="SVELTE_HYDRATER contents" data-target="ThemeSwitcher" data-props="{&quot;theme&quot;:&quot;system&quot;,&quot;isLoggedIn&quot;:false,&quot;menuClassNames&quot;:&quot;md:-top-24&quot;,&quot;classNames&quot;:&quot;max-md:mb-5 max-md:*:self-start&quot;}">
<div class="relative inline-block max-md:mb-5 max-md:*:self-start">
	<button class="rounded-full border border-gray-100 pl-2 py-1 pr-2.5  flex items-center text-sm text-gray-500 bg-white hover:bg-purple-50 hover:border-purple-200 dark:hover:bg-gray-800 dark:hover:border-gray-950 dark:border-gray-800 " type="button">
		<svg class="mr-1.5 text-gray-500" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M6 22.5h20a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2H6a2 2 0 0 0-2 2v13.5a2 2 0 0 0 2 2ZM7 7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h18a1 1 0 0 0 1-1V8a1 1 0 0 0-1-1H7Z" fill="currentColor"></path><path d="M6 8a1 1 0 0 1 1-1h18a1 1 0 0 1 1 1v11a1 1 0 0 1-1 1H7a1 1 0 0 1-1-1V8Z" fill="currentColor" fill-opacity=".4"></path><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path></svg>
			System theme
		</button>
	
	
	</div></div>
		<div class="font-semibold text-black md:hidden">Company</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="max-md:mb-4! max-md:mt-8! group flex-none max-md:order-last"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="max-md:mt-8! font-semibold text-black md:hidden">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("\/front\/build\/kube-68d7aa0\/index.js");
			window.moonSha = "kube-68d7aa0\/";
			window.__hf_deferred = {};
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>
	</body>
</html>
