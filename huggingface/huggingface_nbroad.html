<!doctype html>
<html class="">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
		<meta name="description" content="We’re on a journey to advance and democratize artificial intelligence through open source and open science." />
		<meta property="fb:app_id" content="1321688464574422" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@huggingface" />
		<meta name="twitter:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/nbroad/small_arxiv_classification.png" />
		<meta property="og:title" content="nbroad/small_arxiv_classification · Datasets at Hugging Face" />
		<meta property="og:type" content="website" />
		<meta property="og:url" content="https://huggingface.co/datasets/nbroad/small_arxiv_classification" />
		<meta property="og:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/nbroad/small_arxiv_classification.png" />

		<link rel="stylesheet" href="/front/build/kube-68d7aa0/style.css" />

		<link rel="preconnect" href="https://fonts.gstatic.com" />
		<link
			href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap"
			rel="stylesheet"
		/>

		<link
			rel="preload"
			href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css"
			as="style"
			onload="this.onload=null;this.rel='stylesheet'"
		/>
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		<script>const guestTheme = document.cookie.match(/theme=(\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches));</script>
<link rel="canonical" href="https://huggingface.co/datasets/nbroad/small_arxiv_classification"> <script type="application/ld+json">{
  "@context": {
    "@language": "en",
    "@vocab": "https:\/\/schema.org\/",
    "citeAs": "cr:citeAs",
    "column": "cr:column",
    "conformsTo": "dct:conformsTo",
    "cr": "http:\/\/mlcommons.org\/croissant\/",
    "data": {
      "@id": "cr:data",
      "@type": "@json"
    },
    "dataBiases": "cr:dataBiases",
    "dataCollection": "cr:dataCollection",
    "dataType": {
      "@id": "cr:dataType",
      "@type": "@vocab"
    },
    "dct": "http:\/\/purl.org\/dc\/terms\/",
    "extract": "cr:extract",
    "field": "cr:field",
    "fileProperty": "cr:fileProperty",
    "fileObject": "cr:fileObject",
    "fileSet": "cr:fileSet",
    "format": "cr:format",
    "includes": "cr:includes",
    "isLiveDataset": "cr:isLiveDataset",
    "jsonPath": "cr:jsonPath",
    "key": "cr:key",
    "md5": "cr:md5",
    "parentField": "cr:parentField",
    "path": "cr:path",
    "personalSensitiveInformation": "cr:personalSensitiveInformation",
    "recordSet": "cr:recordSet",
    "references": "cr:references",
    "regex": "cr:regex",
    "repeated": "cr:repeated",
    "replace": "cr:replace",
    "sc": "https:\/\/schema.org\/",
    "separator": "cr:separator",
    "source": "cr:source",
    "subField": "cr:subField",
    "transform": "cr:transform"
  },
  "@type": "sc:Dataset",
  "distribution": [
    {
      "@type": "cr:FileObject",
      "@id": "repo",
      "name": "repo",
      "description": "The Hugging Face git repository.",
      "contentUrl": "https:\/\/huggingface.co\/datasets\/nbroad\/small_arxiv_classification\/tree\/refs%2Fconvert%2Fparquet",
      "encodingFormat": "git+https",
      "sha256": "https:\/\/github.com\/mlcommons\/croissant\/issues\/80"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-default",
      "name": "parquet-files-for-config-default",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "default\/*\/*.parquet"
    }
  ],
  "recordSet": [
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "default_splits\/split_name"
      },
      "@id": "default_splits",
      "name": "default_splits",
      "description": "Splits for the default config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "default_splits\/split_name": "train"
        },
        {
          "default_splits\/split_name": "validation"
        },
        {
          "default_splits\/split_name": "test"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "default",
      "name": "default",
      "description": "nbroad\/small_arxiv_classification - 'default' subset\n\nAdditional information:\n- 3 splits: train, validation, test",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default\/split",
          "name": "default\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "default\/(?:partial-)?(train|validation|test)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "default_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/text",
          "name": "default\/text",
          "description": "Column 'text' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "text"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/label",
          "name": "default\/label",
          "description": "Column 'label' from the Hugging Face parquet file.",
          "dataType": "sc:Integer",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "label"
            }
          }
        }
      ]
    }
  ],
  "conformsTo": "http:\/\/mlcommons.org\/croissant\/1.0",
  "name": "small_arxiv_classification",
  "description": "nbroad\/small_arxiv_classification dataset hosted on Hugging Face and contributed by the HF Datasets community",
  "alternateName": [
    "nbroad\/small_arxiv_classification"
  ],
  "creator": {
    "@type": "Person",
    "name": "Nicholas Broad",
    "url": "https:\/\/huggingface.co\/nbroad"
  },
  "keywords": [
    "1K - 10K",
    "parquet",
    "Text",
    "Datasets",
    "pandas",
    "Croissant",
    "Polars",
    "🇺🇸 Region: US"
  ],
  "url": "https:\/\/huggingface.co\/datasets\/nbroad\/small_arxiv_classification"
}</script> 

		<title>nbroad/small_arxiv_classification · Datasets at Hugging Face</title>

		<script
			defer
			data-domain="huggingface.co"
			event-loggedIn="false"
			src="/js/script.pageview-props.js"
		></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script>
			window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer></script>
	</head>
	<body class="flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DatasetPage">
		<div class="flex min-h-dvh flex-col"><div class="SVELTE_HYDRATER contents" data-target="SystemThemeMonitor" data-props="{&quot;isLoggedIn&quot;:false}"></div>

	<div class="SVELTE_HYDRATER contents" data-target="MainHeader" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false,&quot;isPro&quot;:false}"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg">
				<span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a>
			<div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..."   spellcheck="false" type="text" value="">
	<svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
	</div>
			<div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg>
		</button>

	</div></div>
		<nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 2xl:space-x-2"><li class="hover:text-indigo-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
					Models</a>
			</li><li class="hover:text-red-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					Datasets</a>
			</li><li class="hover:text-blue-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg>
					Spaces</a>
			</li><li class="hover:text-yellow-700 max-xl:hidden"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 text-yellow-500!" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg>
					Posts</a>
			</li><li class="hover:text-yellow-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 16 16"><path d="m2.28 3.7-.3.16a.67.67 0 0 0-.34.58v8.73l.01.04.02.07.01.04.03.06.02.04.02.03.04.06.05.05.04.04.06.04.06.04.08.04.08.02h.05l.07.02h.11l.04-.01.07-.02.03-.01.07-.03.22-.12a5.33 5.33 0 0 1 5.15.1.67.67 0 0 0 .66 0 5.33 5.33 0 0 1 5.33 0 .67.67 0 0 0 1-.58V4.36a.67.67 0 0 0-.34-.5l-.3-.17v7.78a.63.63 0 0 1-.87.59 4.9 4.9 0 0 0-4.35.35l-.65.39a.29.29 0 0 1-.15.04.29.29 0 0 1-.16-.04l-.65-.4a4.9 4.9 0 0 0-4.34-.34.63.63 0 0 1-.87-.59V3.7Z" fill="currentColor" class="dark:opacity-40"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M8 3.1a5.99 5.99 0 0 0-5.3-.43.66.66 0 0 0-.42.62v8.18c0 .45.46.76.87.59a4.9 4.9 0 0 1 4.34.35l.65.39c.05.03.1.04.16.04.05 0 .1-.01.15-.04l.65-.4a4.9 4.9 0 0 1 4.35-.34.63.63 0 0 0 .86-.59V3.3a.67.67 0 0 0-.41-.62 5.99 5.99 0 0 0-5.3.43l-.3.17L8 3.1Zm.73 1.87a.43.43 0 1 0-.86 0v5.48a.43.43 0 0 0 .86 0V4.97Z" fill="currentColor" class="opacity-40 dark:opacity-100"></path><path d="M8.73 4.97a.43.43 0 1 0-.86 0v5.48a.43.43 0 1 0 .86 0V4.96Z" fill="currentColor" class="dark:opacity-40"></path></svg>
					Docs</a>
			</li><li class="hover:text-black dark:hover:text-white"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/enterprise"><svg class="mr-1.5 text-gray-400 group-hover:text-black dark:group-hover:text-white" xmlns="http://www.w3.org/2000/svg" fill="none" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 33 27"><path fill="currentColor" fill-rule="evenodd" d="M13.5.7a8.7 8.7 0 0 0-7.7 5.7L1 20.6c-1 3.1.9 5.7 4.1 5.7h15c3.3 0 6.8-2.6 7.8-5.7l4.6-14.2c1-3.1-.8-5.7-4-5.7h-15Zm1.1 5.7L9.8 20.3h9.8l1-3.1h-5.8l.8-2.5h4.8l1.1-3h-4.8l.8-2.3H23l1-3h-9.5Z" clip-rule="evenodd"></path></svg>
					Enterprise</a>
			</li>

		<li><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/pricing">Pricing
			</a></li>

		<li><div class="relative group">
	<button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button">
		<svg class=" text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>
			
		</button>
	
	
	</div></li>
		<li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li>
		<li><a class="block cursor-pointer whitespace-nowrap px-2 py-0.5 hover:text-gray-500 dark:text-gray-300 dark:hover:text-gray-100" href="/login">Log In
				</a></li>
			<li><a class="whitespace-nowrap rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up
					</a></li></ul></nav></div></header></div>
	
	
	
	<div class="SVELTE_HYDRATER contents" data-target="SSOBanner" data-props="{}"></div>
	
	

	<main class="flex flex-1 flex-col">
	<div class="SVELTE_HYDRATER contents" data-target="DatasetHeader" data-props="{&quot;activeTab&quot;:&quot;datasetCard&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;5f353bb37e58354338621655&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1639773384591-5f353bb37e58354338621655.jpeg&quot;,&quot;fullname&quot;:&quot;Nicholas Broad&quot;,&quot;name&quot;:&quot;nbroad&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:101},&quot;canReadRepoSettings&quot;:false,&quot;dataset&quot;:{&quot;author&quot;:&quot;nbroad&quot;,&quot;createdAt&quot;:&quot;2022-10-18T23:26:49.000Z&quot;,&quot;downloads&quot;:127,&quot;downloadsAllTime&quot;:390,&quot;id&quot;:&quot;nbroad/small_arxiv_classification&quot;,&quot;isLikedByUser&quot;:false,&quot;lastModified&quot;:&quot;2022-10-18T23:29:38.000Z&quot;,&quot;likes&quot;:2,&quot;datasetsServerInfo&quot;:{&quot;viewer&quot;:&quot;viewer&quot;,&quot;numRows&quot;:2000,&quot;libraries&quot;:[&quot;datasets&quot;,&quot;pandas&quot;,&quot;mlcroissant&quot;,&quot;polars&quot;],&quot;formats&quot;:[&quot;parquet&quot;],&quot;modalities&quot;:[&quot;text&quot;]},&quot;discussionsDisabled&quot;:false,&quot;repoType&quot;:&quot;dataset&quot;,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;tags&quot;:[&quot;size_categories:1K<n<10K&quot;,&quot;format:parquet&quot;,&quot;modality:text&quot;,&quot;library:datasets&quot;,&quot;library:pandas&quot;,&quot;library:mlcroissant&quot;,&quot;library:polars&quot;,&quot;region:us&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;size_categories:1K<n<10K&quot;,&quot;label&quot;:&quot;1K - 10K&quot;,&quot;type&quot;:&quot;size_categories&quot;},{&quot;id&quot;:&quot;format:parquet&quot;,&quot;label&quot;:&quot;parquet&quot;,&quot;type&quot;:&quot;format&quot;},{&quot;id&quot;:&quot;modality:text&quot;,&quot;label&quot;:&quot;Text&quot;,&quot;type&quot;:&quot;modality&quot;},{&quot;id&quot;:&quot;library:datasets&quot;,&quot;label&quot;:&quot;Datasets&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:pandas&quot;,&quot;label&quot;:&quot;pandas&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:mlcroissant&quot;,&quot;label&quot;:&quot;Croissant&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:polars&quot;,&quot;label&quot;:&quot;Polars&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;🇺🇸 Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;}],&quot;hasBlockedOids&quot;:false,&quot;region&quot;:&quot;us&quot;,&quot;xetEnabled&quot;:false},&quot;discussionsStats&quot;:{&quot;closed&quot;:0,&quot;open&quot;:0,&quot;total&quot;:0}}"><header class="bg-linear-to-t border-b border-gray-100 pt-6 sm:pt-9 from-gray-50-to-white via-white dark:via-gray-950"><div class="container relative "><h1 class="flex flex-wrap items-center max-md:leading-tight mb-3 text-lg max-sm:gap-y-1.5 md:text-xl"><a href="/datasets" class="group flex items-center"><svg class="sm:mr-1.5 -mr-1 text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					<span class="mr-2.5 font-semibold text-gray-400 group-hover:text-gray-500 max-sm:hidden">Datasets:</span></a>
				<hr class="mx-1.5 h-2 translate-y-px rounded-sm border-r dark:border-gray-600 sm:hidden">
			<div class="group flex flex-none items-center"><div class="relative mr-1 flex items-center">

			

<span class="inline-block "><span class="contents"><a href="/nbroad" class="text-gray-400 hover:text-blue-600"><img alt="" class="w-3.5 h-3.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1639773384591-5f353bb37e58354338621655.jpeg" crossorigin="anonymous"></a></span>
	</span></div>
		

<span class="inline-block "><span class="contents"><a href="/nbroad" class="text-gray-400 hover:text-blue-600">nbroad</a></span>
	</span>
		<div class="mx-0.5 text-gray-300">/</div></div>

<div class="max-w-full "><a class="break-words font-mono font-semibold hover:text-blue-600 " href="/datasets/nbroad/small_arxiv_classification">small_arxiv_classification</a>
	<button class="relative text-sm mr-4 focus:outline-hidden inline-flex cursor-pointer items-center text-sm  mx-0.5   text-gray-600 " title="Copy dataset name to clipboard" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg>
	
	</button></div>
			<div class="inline-flex items-center overflow-hidden whitespace-nowrap rounded-md border bg-white text-sm leading-none text-gray-500  mr-2"><button class="relative flex items-center overflow-hidden from-red-50 to-transparent dark:from-red-900 px-1.5 py-1 hover:bg-linear-to-t focus:outline-hidden"  title="Like"><svg class="left-1.5 absolute" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>

		
		<span class="ml-4 pl-0.5 ">like</span></button>
	<button class="focus:outline-hidden flex items-center border-l px-1.5 py-1 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="See users who liked this repository">2</button></div>




			
			
	</h1>
		<div class="mb-3 flex flex-wrap md:mb-4"><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Modalities:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?modality=modality%3Atext"><div class="tag tag-white   ">
		<svg class="text-red-700 dark:text-red-600" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.619 4.619C2.667 6.573 2.667 9.715 2.667 16s0 9.428 1.952 11.38C6.573 29.333 9.715 29.333 16 29.333s9.428 0 11.38-1.953c1.953-1.95 1.953-5.095 1.953-11.38s0-9.428-1.953-11.381C25.43 2.667 22.285 2.667 16 2.667s-9.428 0-11.381 1.952m8.65 3.714c-.573 0-1.109 0-1.546.066-.495.073-1.003.248-1.41.7-.392.436-.53.956-.59 1.452-.056.464-.056 1.04-.056 1.689V13a1 1 0 1 0 2 0v-.704c0-.724.001-1.176.041-1.505q.015-.15.061-.294a.2.2 0 0 1 .031-.061q0-.003.016-.01a.8.8 0 0 1 .203-.05c.272-.04.654-.043 1.314-.043H15v11.334h-2.333a1 1 0 1 0 0 2H20a1 1 0 0 0 0-2h-3V10.333h1.667c.66 0 1.042.003 1.314.043.123.019.18.04.203.05l.015.009a.2.2 0 0 1 .032.061c.018.05.042.14.061.295.04.329.041.781.041 1.506V13a1 1 0 1 0 2 0v-.76c0-.65 0-1.225-.056-1.69-.06-.495-.198-1.015-.59-1.453-.407-.45-.915-.625-1.41-.698-.437-.067-.973-.067-1.546-.066z" fill="currentColor"></path></svg>

	

	<span>Text</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Formats:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?format=format%3Aparquet"><div class="tag tag-white   ">
		<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 34 34"><path fill-rule="evenodd" clip-rule="evenodd" d="m17.97 18.44-3.98-2.3-7.1 3.78 3.98 2.3 7.1-3.78Zm15.1-1.4-3.99-2.3-16.22 8.63 3.98 2.3 16.22-8.63Zm-5.98-3.45-3.97-2.3-7.1 3.78 3.98 2.3 7.1-3.78Zm-9.94-5.74 3.98 2.3-11.16 5.93L6 13.78l11.16-5.93Zm-13.19 7 3.98 2.3-3.04 1.62-3.98-2.3 3.04-1.61Z" fill="currentColor"></path></svg>

	

	<span>parquet</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Size:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?size_categories=size_categories%3A1K%3Cn%3C10K"><div class="tag tag-white   ">

	

	<span>1K - 10K</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Libraries:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Adatasets"><div class="tag tag-white   "><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 95 88"><path fill="#fff" d="M94.25 70.08a8.28 8.28 0 0 1-.43 6.46 10.57 10.57 0 0 1-3 3.6 25.18 25.18 0 0 1-5.7 3.2 65.74 65.74 0 0 1-7.56 2.65 46.67 46.67 0 0 1-11.42 1.68c-5.42.05-10.09-1.23-13.4-4.5a40.4 40.4 0 0 1-10.14.03c-3.34 3.25-7.99 4.52-13.39 4.47a46.82 46.82 0 0 1-11.43-1.68 66.37 66.37 0 0 1-7.55-2.65c-2.28-.98-4.17-2-5.68-3.2a10.5 10.5 0 0 1-3.02-3.6c-.99-2-1.18-4.3-.42-6.46a8.54 8.54 0 0 1-.33-5.63c.25-.95.66-1.83 1.18-2.61a8.67 8.67 0 0 1 2.1-8.47 8.23 8.23 0 0 1 2.82-2.07 41.75 41.75 0 1 1 81.3-.12 8.27 8.27 0 0 1 3.11 2.19 8.7 8.7 0 0 1 2.1 8.47c.52.78.93 1.66 1.18 2.61a8.61 8.61 0 0 1-.32 5.63Z"></path><path fill="#FFD21E" d="M47.21 76.5a34.75 34.75 0 1 0 0-69.5 34.75 34.75 0 0 0 0 69.5Z"></path><path fill="#FF9D0B" d="M81.96 41.75a34.75 34.75 0 1 0-69.5 0 34.75 34.75 0 0 0 69.5 0Zm-73.5 0a38.75 38.75 0 1 1 77.5 0 38.75 38.75 0 0 1-77.5 0Z"></path><path fill="#3A3B45" d="M58.5 32.3c1.28.44 1.78 3.06 3.07 2.38a5 5 0 1 0-6.76-2.07c.61 1.15 2.55-.72 3.7-.32ZM34.95 32.3c-1.28.44-1.79 3.06-3.07 2.38a5 5 0 1 1 6.76-2.07c-.61 1.15-2.56-.72-3.7-.32Z"></path><path fill="#FF323D" d="M46.96 56.29c9.83 0 13-8.76 13-13.26 0-2.34-1.57-1.6-4.09-.36-2.33 1.15-5.46 2.74-8.9 2.74-7.19 0-13-6.88-13-2.38s3.16 13.26 13 13.26Z"></path><path fill="#3A3B45" fill-rule="evenodd" d="M39.43 54a8.7 8.7 0 0 1 5.3-4.49c.4-.12.81.57 1.24 1.28.4.68.82 1.37 1.24 1.37.45 0 .9-.68 1.33-1.35.45-.7.89-1.38 1.32-1.25a8.61 8.61 0 0 1 5 4.17c3.73-2.94 5.1-7.74 5.1-10.7 0-2.34-1.57-1.6-4.09-.36l-.14.07c-2.31 1.15-5.39 2.67-8.77 2.67s-6.45-1.52-8.77-2.67c-2.6-1.29-4.23-2.1-4.23.29 0 3.05 1.46 8.06 5.47 10.97Z" clip-rule="evenodd"></path><path fill="#FF9D0B" d="M70.71 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM24.21 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM17.52 48c-1.62 0-3.06.66-4.07 1.87a5.97 5.97 0 0 0-1.33 3.76 7.1 7.1 0 0 0-1.94-.3c-1.55 0-2.95.59-3.94 1.66a5.8 5.8 0 0 0-.8 7 5.3 5.3 0 0 0-1.79 2.82c-.24.9-.48 2.8.8 4.74a5.22 5.22 0 0 0-.37 5.02c1.02 2.32 3.57 4.14 8.52 6.1 3.07 1.22 5.89 2 5.91 2.01a44.33 44.33 0 0 0 10.93 1.6c5.86 0 10.05-1.8 12.46-5.34 3.88-5.69 3.33-10.9-1.7-15.92-2.77-2.78-4.62-6.87-5-7.77-.78-2.66-2.84-5.62-6.25-5.62a5.7 5.7 0 0 0-4.6 2.46c-1-1.26-1.98-2.25-2.86-2.82A7.4 7.4 0 0 0 17.52 48Zm0 4c.51 0 1.14.22 1.82.65 2.14 1.36 6.25 8.43 7.76 11.18.5.92 1.37 1.31 2.14 1.31 1.55 0 2.75-1.53.15-3.48-3.92-2.93-2.55-7.72-.68-8.01.08-.02.17-.02.24-.02 1.7 0 2.45 2.93 2.45 2.93s2.2 5.52 5.98 9.3c3.77 3.77 3.97 6.8 1.22 10.83-1.88 2.75-5.47 3.58-9.16 3.58-3.81 0-7.73-.9-9.92-1.46-.11-.03-13.45-3.8-11.76-7 .28-.54.75-.76 1.34-.76 2.38 0 6.7 3.54 8.57 3.54.41 0 .7-.17.83-.6.79-2.85-12.06-4.05-10.98-8.17.2-.73.71-1.02 1.44-1.02 3.14 0 10.2 5.53 11.68 5.53.11 0 .2-.03.24-.1.74-1.2.33-2.04-4.9-5.2-5.21-3.16-8.88-5.06-6.8-7.33.24-.26.58-.38 1-.38 3.17 0 10.66 6.82 10.66 6.82s2.02 2.1 3.25 2.1c.28 0 .52-.1.68-.38.86-1.46-8.06-8.22-8.56-11.01-.34-1.9.24-2.85 1.31-2.85Z"></path><path fill="#FFD21E" d="M38.6 76.69c2.75-4.04 2.55-7.07-1.22-10.84-3.78-3.77-5.98-9.3-5.98-9.3s-.82-3.2-2.69-2.9c-1.87.3-3.24 5.08.68 8.01 3.91 2.93-.78 4.92-2.29 2.17-1.5-2.75-5.62-9.82-7.76-11.18-2.13-1.35-3.63-.6-3.13 2.2.5 2.79 9.43 9.55 8.56 11-.87 1.47-3.93-1.71-3.93-1.71s-9.57-8.71-11.66-6.44c-2.08 2.27 1.59 4.17 6.8 7.33 5.23 3.16 5.64 4 4.9 5.2-.75 1.2-12.28-8.53-13.36-4.4-1.08 4.11 11.77 5.3 10.98 8.15-.8 2.85-9.06-5.38-10.74-2.18-1.7 3.21 11.65 6.98 11.76 7.01 4.3 1.12 15.25 3.49 19.08-2.12Z"></path><path fill="#FF9D0B" d="M77.4 48c1.62 0 3.07.66 4.07 1.87a5.97 5.97 0 0 1 1.33 3.76 7.1 7.1 0 0 1 1.95-.3c1.55 0 2.95.59 3.94 1.66a5.8 5.8 0 0 1 .8 7 5.3 5.3 0 0 1 1.78 2.82c.24.9.48 2.8-.8 4.74a5.22 5.22 0 0 1 .37 5.02c-1.02 2.32-3.57 4.14-8.51 6.1-3.08 1.22-5.9 2-5.92 2.01a44.33 44.33 0 0 1-10.93 1.6c-5.86 0-10.05-1.8-12.46-5.34-3.88-5.69-3.33-10.9 1.7-15.92 2.78-2.78 4.63-6.87 5.01-7.77.78-2.66 2.83-5.62 6.24-5.62a5.7 5.7 0 0 1 4.6 2.46c1-1.26 1.98-2.25 2.87-2.82A7.4 7.4 0 0 1 77.4 48Zm0 4c-.51 0-1.13.22-1.82.65-2.13 1.36-6.25 8.43-7.76 11.18a2.43 2.43 0 0 1-2.14 1.31c-1.54 0-2.75-1.53-.14-3.48 3.91-2.93 2.54-7.72.67-8.01a1.54 1.54 0 0 0-.24-.02c-1.7 0-2.45 2.93-2.45 2.93s-2.2 5.52-5.97 9.3c-3.78 3.77-3.98 6.8-1.22 10.83 1.87 2.75 5.47 3.58 9.15 3.58 3.82 0 7.73-.9 9.93-1.46.1-.03 13.45-3.8 11.76-7-.29-.54-.75-.76-1.34-.76-2.38 0-6.71 3.54-8.57 3.54-.42 0-.71-.17-.83-.6-.8-2.85 12.05-4.05 10.97-8.17-.19-.73-.7-1.02-1.44-1.02-3.14 0-10.2 5.53-11.68 5.53-.1 0-.19-.03-.23-.1-.74-1.2-.34-2.04 4.88-5.2 5.23-3.16 8.9-5.06 6.8-7.33-.23-.26-.57-.38-.98-.38-3.18 0-10.67 6.82-10.67 6.82s-2.02 2.1-3.24 2.1a.74.74 0 0 1-.68-.38c-.87-1.46 8.05-8.22 8.55-11.01.34-1.9-.24-2.85-1.31-2.85Z"></path><path fill="#FFD21E" d="M56.33 76.69c-2.75-4.04-2.56-7.07 1.22-10.84 3.77-3.77 5.97-9.3 5.97-9.3s.82-3.2 2.7-2.9c1.86.3 3.23 5.08-.68 8.01-3.92 2.93.78 4.92 2.28 2.17 1.51-2.75 5.63-9.82 7.76-11.18 2.13-1.35 3.64-.6 3.13 2.2-.5 2.79-9.42 9.55-8.55 11 .86 1.47 3.92-1.71 3.92-1.71s9.58-8.71 11.66-6.44c2.08 2.27-1.58 4.17-6.8 7.33-5.23 3.16-5.63 4-4.9 5.2.75 1.2 12.28-8.53 13.36-4.4 1.08 4.11-11.76 5.3-10.97 8.15.8 2.85 9.05-5.38 10.74-2.18 1.69 3.21-11.65 6.98-11.76 7.01-4.31 1.12-15.26 3.49-19.08-2.12Z"></path></svg>

	

	<span>Datasets</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Apandas"><div class="tag tag-white   "><svg class="text-black inline-block text-sm text-[#130754] dark:text-gray-200" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid slice" viewBox="0 0 210.21 280.43"><rect fill="currentColor" x="74.51" y="43.03" width="24.09" height="50.02"></rect><rect fill="currentColor" x="74.51" y="145.78" width="24.09" height="50.02"></rect><rect fill="#ffca00" x="74.51" y="107.65" width="24.09" height="23.6"></rect><rect fill="currentColor" x="35.81" y="84.15" width="24.09" height="166.27"></rect><rect fill="currentColor" x="112.41" y="187.05" width="24.09" height="50.02"></rect><rect fill="currentColor" x="112.41" y="84.21" width="24.09" height="50.02"></rect><rect fill="#e70488" x="112.41" y="148.84" width="24.09" height="23.6"></rect><rect fill="currentColor" x="150.3" y="30" width="24.09" height="166.27"></rect></svg>

	

	<span>pandas</span>
	

	</div></a><div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-lg rounded-br-none " type="button">
		<div class="tag tag-white   relative rounded-br-none pr-2.5"><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="none" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="#F5AB6A"></path><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="url(#paint0_radial_18_31665)"></path><g filter="url(#filter0_f_18_31665)"><path d="M22.2849 12.1817L23.4375 13.2656L24.4375 4.70312C23.5121 4.1242 23.0198 3.96369 22.6563 3.89062L22.2849 12.1817Z" fill="url(#paint1_linear_18_31665)"></path></g><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint2_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint3_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint4_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint5_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint6_linear_18_31665)"></path><g filter="url(#filter1_f_18_31665)"><path d="M13.1016 2.72656C11.862 3.06924 11.5298 3.40016 11.5298 3.40016C10.9102 3.52335 10.936 4.11525 11.2408 4.6799C13.1487 6.95202 15.1361 13.2496 18.007 14.0958C18.2707 14.1633 18.6953 14.2107 19.1797 14.2344L13.1016 2.72656Z" fill="url(#paint7_linear_18_31665)"></path></g><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint8_radial_18_31665)"></path><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint9_radial_18_31665)"></path><g filter="url(#filter2_f_18_31665)"><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint10_linear_18_31665)"></path><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint11_radial_18_31665)"></path></g><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="#EC9F6A"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint12_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint13_linear_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint14_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint15_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint16_radial_18_31665)"></path><g filter="url(#filter3_f_18_31665)"><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint17_linear_18_31665)"></path><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint18_linear_18_31665)"></path></g><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="#D79453"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint19_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint20_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint21_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint22_linear_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint23_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint24_radial_18_31665)"></path><defs><filter id="filter0_f_18_31665" x="22.0349" y="3.64062" width="2.65265" height="9.875" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter1_f_18_31665" x="10.7815" y="2.47656" width="8.64819" height="12.0078" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter2_f_18_31665" x="3.89062" y="22.5416" width="9.57812" height="2.2804" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter3_f_18_31665" x="2.45312" y="11.2538" width="12.1255" height="8.50903" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><radialGradient id="paint0_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.8125 12.9375) rotate(42.7741) scale(12.5164 7.08839)"><stop offset="0.0937591" stop-color="#C05159"></stop><stop offset="0.553697" stop-color="#F6AC6A"></stop><stop offset="0.832916" stop-color="#FFD186"></stop><stop offset="0.916927" stop-color="#FFDC87"></stop></radialGradient><linearGradient id="paint1_linear_18_31665" x1="24.7344" y1="4.67187" x2="20.8594" y2="12.8906" gradientUnits="userSpaceOnUse"><stop stop-color="#EBD67C"></stop><stop offset="0.0655686" stop-color="#FFFFA6"></stop><stop offset="0.530552" stop-color="#F8C281"></stop><stop offset="0.937338" stop-color="#E99E6B"></stop></linearGradient><radialGradient id="paint2_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.9009 13.1847) rotate(-127.648) scale(14.3438 11.7966)"><stop stop-color="#FFBE66"></stop><stop offset="1" stop-color="#E2AE5B"></stop></radialGradient><radialGradient id="paint3_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(18 11.4375) rotate(53.9726) scale(11.9013 4.84018)"><stop stop-color="#D67C63"></stop><stop offset="1" stop-color="#D97D67" stop-opacity="0"></stop></radialGradient><radialGradient id="paint4_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(23 4.1875) rotate(45.7639) scale(3.31486 5.75622)"><stop stop-color="#FFE4A6"></stop><stop offset="0.711285" stop-color="#F8B76F"></stop><stop offset="1" stop-color="#F9B870" stop-opacity="0"></stop></radialGradient><radialGradient id="paint5_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.875 12.4375) rotate(88.9391) scale(3.37558 1.29066)"><stop stop-color="#FFBC67"></stop><stop offset="1" stop-color="#FFBC67" stop-opacity="0"></stop></radialGradient><linearGradient id="paint6_linear_18_31665" x1="20.375" y1="15.6875" x2="20.125" y2="12.7813" gradientUnits="userSpaceOnUse"><stop offset="0.461609" stop-color="#B45077"></stop><stop offset="0.855389" stop-color="#B75077" stop-opacity="0"></stop></linearGradient><linearGradient id="paint7_linear_18_31665" x1="12.9375" y1="2.57056" x2="18.5625" y2="14.3891" gradientUnits="userSpaceOnUse"><stop stop-color="#DDC173"></stop><stop offset="0.485173" stop-color="#D59F65"></stop><stop offset="1" stop-color="#E49966"></stop></linearGradient><radialGradient id="paint8_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.5625 23.5) rotate(109.113) scale(6.68078 10.2578)"><stop offset="0.165756" stop-color="#FFBF7E"></stop><stop offset="0.827674" stop-color="#DF8C6D"></stop><stop offset="1" stop-color="#B05A66"></stop></radialGradient><radialGradient id="paint9_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.1875 26) rotate(41.0652) scale(8.37243 2.03649)"><stop stop-color="#FFD483"></stop><stop offset="1" stop-color="#FFD688" stop-opacity="0"></stop></radialGradient><linearGradient id="paint10_linear_18_31665" x1="3.96063" y1="23.794" x2="13.3748" y2="23.5143" gradientUnits="userSpaceOnUse"><stop stop-color="#A8716F"></stop><stop offset="0.103615" stop-color="#B37173"></stop><stop offset="0.225484" stop-color="#DB9F84"></stop><stop offset="0.799889" stop-color="#F1BB8A"></stop><stop offset="1" stop-color="#FFD780"></stop></linearGradient><radialGradient id="paint11_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4219 23.1719) rotate(-178.616) scale(3.23532 0.569081)"><stop offset="0.621498" stop-color="#AF5A3E"></stop><stop offset="1" stop-color="#B35445" stop-opacity="0"></stop></radialGradient><radialGradient id="paint12_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.625 19.125) rotate(-171.737) scale(15.2205 15.0767)"><stop offset="0.138435" stop-color="#FFB974"></stop><stop offset="0.403618" stop-color="#F2A56D"></stop><stop offset="0.925938" stop-color="#A16948"></stop></radialGradient><linearGradient id="paint13_linear_18_31665" x1="8.22184" y1="13.125" x2="6.81191" y2="15.4996" gradientUnits="userSpaceOnUse"><stop offset="0.610751" stop-color="#984847"></stop><stop offset="0.850075" stop-color="#9A4947" stop-opacity="0"></stop></linearGradient><radialGradient id="paint14_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.25 23.7461) scale(11.25 5.68361)"><stop stop-color="#C66364"></stop><stop offset="1" stop-color="#D4766B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint15_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.875 23.5313) scale(10.0937 1.29657)"><stop stop-color="#B64B4B"></stop><stop offset="1" stop-color="#C56158" stop-opacity="0"></stop></radialGradient><radialGradient id="paint16_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4375 19.875) rotate(-46.8882) scale(4.02385 7.51767)"><stop stop-color="#FFC083"></stop><stop offset="0.620218" stop-color="#FFBD7D" stop-opacity="0"></stop></radialGradient><linearGradient id="paint17_linear_18_31665" x1="2.8125" y1="13.0312" x2="14.5582" y2="18.9404" gradientUnits="userSpaceOnUse"><stop stop-color="#B89367"></stop><stop offset="1" stop-color="#C5835E"></stop></linearGradient><linearGradient id="paint18_linear_18_31665" x1="8.21875" y1="14.6406" x2="7.59349" y2="15.6717" gradientUnits="userSpaceOnUse"><stop offset="0.351552" stop-color="#A74746"></stop><stop offset="0.845198" stop-color="#A04346" stop-opacity="0"></stop></linearGradient><radialGradient id="paint19_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.5625 14.5625) rotate(140.244) scale(18.3733 13.7403)"><stop stop-color="#FDAE69"></stop><stop offset="0.729021" stop-color="#CE8C4F"></stop><stop offset="0.921546" stop-color="#AD7B45"></stop><stop offset="1" stop-color="#8B6B4A"></stop></radialGradient><radialGradient id="paint20_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.0625 7) rotate(65.3152) scale(11.0745 3.16547)"><stop offset="0.233237" stop-color="#FFD47C"></stop><stop offset="0.853648" stop-color="#FFD98B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint21_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.3125 8.875) rotate(100.886) scale(6.6191 5.57808)"><stop offset="0.128419" stop-color="#FFD88C"></stop><stop offset="0.924134" stop-color="#FFBE7B" stop-opacity="0"></stop></radialGradient><linearGradient id="paint22_linear_18_31665" x1="7.25" y1="15.1875" x2="10.7588" y2="10.3142" gradientUnits="userSpaceOnUse"><stop offset="0.142353" stop-color="#C15F4D"></stop><stop offset="1" stop-color="#D58366" stop-opacity="0"></stop></linearGradient><radialGradient id="paint23_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(8.15625 15.7813) rotate(28.5422) scale(12.5574 1.96589)"><stop offset="0.149989" stop-color="#E4745D"></stop><stop offset="0.453292" stop-color="#C8604C"></stop><stop offset="0.632597" stop-color="#C0605F"></stop><stop offset="1" stop-color="#C0605F" stop-opacity="0"></stop></radialGradient><radialGradient id="paint24_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(1.40625 2.69067) rotate(46.0943) scale(22.3963)"><stop offset="0.935802" stop-color="#C17C61" stop-opacity="0"></stop><stop offset="0.982109" stop-color="#C17C61"></stop></radialGradient></defs></svg>

	

	<span>Croissant</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	<button class="tag tag-ghost px-1! -ml-0.5 mb-1 md:mb-1.5" type="button">+ 1</button></div></div>

		<div class="flex flex-col-reverse lg:flex-row lg:items-center lg:justify-between"><div class="-mb-px flex h-12 items-center overflow-x-auto overflow-y-hidden ">
	<a class="tab-alternate active" href="/datasets/nbroad/small_arxiv_classification"><svg class="mr-1.5 text-gray-400 flex-none" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
	Dataset card
	

	
		</a><a class="tab-alternate" href="/datasets/nbroad/small_arxiv_classification/viewer/"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	Data Studio
	

	
		</a><a class="tab-alternate" href="/datasets/nbroad/small_arxiv_classification/tree/main"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M21 19h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-8h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0 4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M9 19a1 1 0 0 1-1-1V6a1 1 0 0 1 2 0v12a1 1 0 0 1-1 1zm-6-4.333a1 1 0 0 1-.64-1.769L3.438 12l-1.078-.898a1 1 0 0 1 1.28-1.538l2 1.667a1 1 0 0 1 0 1.538l-2 1.667a.999.999 0 0 1-.64.231z" fill="currentColor"></path></svg>
	<span class="xl:hidden">Files</span>
		<span class="hidden xl:inline">Files and versions</span>
	

	
		</a><a class="tab-alternate" href="/datasets/nbroad/small_arxiv_classification/discussions"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M20.6081 3C21.7684 3 22.8053 3.49196 23.5284 4.38415C23.9756 4.93678 24.4428 5.82749 24.4808 7.16133C24.9674 7.01707 25.4353 6.93643 25.8725 6.93643C26.9833 6.93643 27.9865 7.37587 28.696 8.17411C29.6075 9.19872 30.0124 10.4579 29.8361 11.7177C29.7523 12.3177 29.5581 12.8555 29.2678 13.3534C29.8798 13.8646 30.3306 14.5763 30.5485 15.4322C30.719 16.1032 30.8939 17.5006 29.9808 18.9403C30.0389 19.0342 30.0934 19.1319 30.1442 19.2318C30.6932 20.3074 30.7283 21.5229 30.2439 22.6548C29.5093 24.3704 27.6841 25.7219 24.1397 27.1727C21.9347 28.0753 19.9174 28.6523 19.8994 28.6575C16.9842 29.4379 14.3477 29.8345 12.0653 29.8345C7.87017 29.8345 4.8668 28.508 3.13831 25.8921C0.356375 21.6797 0.754104 17.8269 4.35369 14.1131C6.34591 12.058 7.67023 9.02782 7.94613 8.36275C8.50224 6.39343 9.97271 4.20438 12.4172 4.20438H12.4179C12.6236 4.20438 12.8314 4.2214 13.0364 4.25468C14.107 4.42854 15.0428 5.06476 15.7115 6.02205C16.4331 5.09583 17.134 4.359 17.7682 3.94323C18.7242 3.31737 19.6794 3 20.6081 3ZM20.6081 5.95917C20.2427 5.95917 19.7963 6.1197 19.3039 6.44225C17.7754 7.44319 14.8258 12.6772 13.7458 14.7131C13.3839 15.3952 12.7655 15.6837 12.2086 15.6837C11.1036 15.6837 10.2408 14.5497 12.1076 13.1085C14.9146 10.9402 13.9299 7.39584 12.5898 7.1776C12.5311 7.16799 12.4731 7.16355 12.4172 7.16355C11.1989 7.16355 10.6615 9.33114 10.6615 9.33114C10.6615 9.33114 9.0863 13.4148 6.38031 16.206C3.67434 18.998 3.5346 21.2388 5.50675 24.2246C6.85185 26.2606 9.42666 26.8753 12.0653 26.8753C14.8021 26.8753 17.6077 26.2139 19.1799 25.793C19.2574 25.7723 28.8193 22.984 27.6081 20.6107C27.4046 20.212 27.0693 20.0522 26.6471 20.0522C24.9416 20.0522 21.8393 22.6726 20.5057 22.6726C20.2076 22.6726 19.9976 22.5416 19.9116 22.222C19.3433 20.1173 28.552 19.2325 27.7758 16.1839C27.639 15.6445 27.2677 15.4256 26.746 15.4263C24.4923 15.4263 19.4358 19.5181 18.3759 19.5181C18.2949 19.5181 18.2368 19.4937 18.2053 19.4419C17.6743 18.557 17.9653 17.9394 21.7082 15.6009C25.4511 13.2617 28.0783 11.8545 26.5841 10.1752C26.4121 9.98141 26.1684 9.8956 25.8725 9.8956C23.6001 9.89634 18.2311 14.9403 18.2311 14.9403C18.2311 14.9403 16.7821 16.496 15.9057 16.496C15.7043 16.496 15.533 16.4139 15.4169 16.2112C14.7956 15.1296 21.1879 10.1286 21.5484 8.06535C21.7928 6.66715 21.3771 5.95917 20.6081 5.95917Z" fill="#FF9D00"></path><path d="M5.50686 24.2246C3.53472 21.2387 3.67446 18.9979 6.38043 16.206C9.08641 13.4147 10.6615 9.33111 10.6615 9.33111C10.6615 9.33111 11.2499 6.95933 12.59 7.17757C13.93 7.39581 14.9139 10.9401 12.1069 13.1084C9.29997 15.276 12.6659 16.7489 13.7459 14.713C14.8258 12.6772 17.7747 7.44316 19.304 6.44221C20.8326 5.44128 21.9089 6.00204 21.5484 8.06532C21.188 10.1286 14.795 15.1295 15.4171 16.2118C16.0391 17.2934 18.2312 14.9402 18.2312 14.9402C18.2312 14.9402 25.0907 8.49588 26.5842 10.1752C28.0776 11.8545 25.4512 13.2616 21.7082 15.6008C17.9646 17.9393 17.6744 18.557 18.2054 19.4418C18.7372 20.3266 26.9998 13.1351 27.7759 16.1838C28.5513 19.2324 19.3434 20.1173 19.9117 22.2219C20.48 24.3274 26.3979 18.2382 27.6082 20.6107C28.8193 22.9839 19.2574 25.7722 19.18 25.7929C16.0914 26.62 8.24723 28.3726 5.50686 24.2246Z" fill="#FFD21E"></path></svg>
	Community
	

	
		</a></div>
	
			</div></div></header>
</div>
	
<div class="container relative flex flex-col md:grid md:space-y-0 w-full md:grid-cols-12 md:flex-1 md:grid-rows-full space-y-4 md:gap-6 ">
		<section class="pt-6 border-gray-100 md:col-span-8 pb-24 relative break-words copiable-code-container">
				<div class="SVELTE_HYDRATER contents" data-target="UnsafeBanner" data-props="{&quot;classNames&quot;:&quot;mb-4&quot;,&quot;repoId&quot;:&quot;nbroad/small_arxiv_classification&quot;,&quot;repoType&quot;:&quot;dataset&quot;,&quot;minLevel&quot;:&quot;unsafe&quot;}"></div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetViewer" data-props="{&quot;data&quot;:{&quot;kind&quot;:&quot;DatasetAndSampleData&quot;,&quot;datasetInfo&quot;:[{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Size of downloaded dataset files:&quot;,&quot;value&quot;:&quot;60.5 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;/datasets/nbroad/small_arxiv_classification/tree/refs%2Fconvert%2Fparquet/&quot;,&quot;label&quot;:&quot;Size of the auto-converted Parquet files:&quot;,&quot;value&quot;:&quot;60.5 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Number of rows:&quot;,&quot;value&quot;:&quot;2,000&quot;}],&quot;partial&quot;:false,&quot;configsData&quot;:{&quot;configInfos&quot;:[{&quot;name&quot;:&quot;default&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:2000}],&quot;selectedConfig&quot;:&quot;default&quot;,&quot;hasSelectedConfigParquet&quot;:true},&quot;splitsData&quot;:{&quot;splitInfos&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;numRows&quot;:1000},{&quot;name&quot;:&quot;validation&quot;,&quot;numRows&quot;:500},{&quot;name&quot;:&quot;test&quot;,&quot;numRows&quot;:500}],&quot;selectedSplit&quot;:&quot;train&quot;},&quot;sampleData&quot;:{&quot;dataset&quot;:&quot;nbroad/small_arxiv_classification&quot;,&quot;config&quot;:&quot;default&quot;,&quot;split&quot;:&quot;train&quot;,&quot;capabilities&quot;:{&quot;rows&quot;:true,&quot;search&quot;:true,&quot;filter&quot;:true,&quot;statistics&quot;:true},&quot;navigation&quot;:{&quot;p&quot;:0},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzA0Niwic3ViIjoiL2RhdGFzZXRzL25icm9hZC9zbWFsbF9hcnhpdl9jbGFzc2lmaWNhdGlvbiIsImV4cCI6MTc0MjkyNjY0NiwiaXNzIjoiaHR0cHM6Ly9odWdnaW5nZmFjZS5jbyJ9.rkoJDh6PrNEFN7WCjOUIlMNig3bbgTv7Q82W4aH46CMTqGaa7d6Yz7lOWxew8gDy3x9foWOpNXWXYhDffmYoAQ&quot;,&quot;sampleData&quot;:{&quot;columns&quot;:[{&quot;name&quot;:&quot;text&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;text&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:7269,&quot;max&quot;:649740,&quot;mean&quot;:56405.845,&quot;median&quot;:47738,&quot;std&quot;:42692.59368,&quot;histogram&quot;:{&quot;hist&quot;:[764,208,21,2,2,0,1,0,1,1],&quot;bin_edges&quot;:[7269,71517,135765,200013,264261,328509,392757,457005,521253,585501,649740]}}}},{&quot;name&quot;:&quot;label&quot;,&quot;align&quot;:&quot;right&quot;,&quot;type&quot;:&quot;int64&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;label&quot;,&quot;column_type&quot;:&quot;int&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:0,&quot;max&quot;:10,&quot;mean&quot;:5.202,&quot;median&quot;:6,&quot;std&quot;:3.09302,&quot;histogram&quot;:{&quot;hist&quot;:[151,183,162,212,209,83],&quot;bin_edges&quot;:[0,2,4,6,8,10,10]}}}}],&quot;rows&quot;:[{&quot;rowIdx&quot;:0,&quot;cells&quot;:{&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;arXiv:1608.03703v2 [] 26 Apr 2017\n\nTemplate estimation in computational anatomy:\nFréchet means in top and quotient spaces are\nnot consistent\nLoïc Devilliers∗, Stéphanie Allassonnière†, Alain Trouvé‡,\nand Xavier Pennec§\nApril 27, 2017\n\nAbstract\nIn this article, we study the consistency of the template estimation\nwith the Fréchet mean in quotient spaces. The Fréchet mean in quotient\nspaces is often used when the observations are deformed or transformed\nby a group action. We show that in most cases this estimator is actually\ninconsistent. We exhibit a sufficient condition for this inconsistency, which\namounts to the folding of the distribution of the noisy template when it\nis projected to the quotient space. This condition appears to be fulfilled\nas soon as the support of the noise is large enough. To quantify this\ninconsistency we provide lower and upper bounds of the bias as a function\nof the variability (the noise level). This shows that the consistency bias\ncannot be neglected when the variability increases.\n\nKeyword : Template, Fréchet mean, group action, quotient space, inconsistency,\nconsistency bias, empirical Fréchet mean, Hilbert space, manifold\n\n∗ Université\n\nCôte d’Azur, Inria, France, loic.devilliers@inria.fr\nEcole polytechnique, CNRS, Université Paris-Saclay, 91128, Palaiseau, France\n‡ CMLA, ENS Cachan, CNRS, Université Paris-Saclay, 94235 Cachan, France\n§ Université Côte d’Azur, Inria, France\n† CMAP,\n\n1\n\n\fContents\n1 Introduction\n\n3\n\n2 Definitions, notations and generative model\n\n5\n\n3 Inconsistency for finite group when the template is\npoint\n3.1 Presence of inconsistency . . . . . . . . . . . . . . . .\n3.2 Upper bound of the consistency bias . . . . . . . . . .\n3.3 Study of the consistency bias in a simple example . . .\n\n. . . . . .\n. . . . . .\n. . . . . .\n\n4 Inconsistency for any group when the template is\npoint\n4.1 Presence of an inconsistency . . . . . . . . . . . . . .\n4.2 Analysis of the condition in theorem 4.1 . . . . . . .\n4.3 Lower bound of the consistency bias . . . . . . . . .\n4.4 Upper bound of the consistency bias . . . . . . . . .\n4.5 Empirical Fréchet mean . . . . . . . . . . . . . . . .\n4.6 Examples . . . . . . . . . . . . . . . . . . . . . . . .\n4.6.1 Action of translation on L2 (R/Z) . . . . . . .\n4.6.2 Action of discrete translation on RZ/NZ . . . .\n4.6.3 Action of rotations on Rn . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n5 Fréchet means top and quotient spaces\nthe template is a fixed point\n5.1 Result . . . . . . . . . . . . . . . . . .\n5.2 Proofs of these theorems . . . . . . . .\n5.2.1 Proof of theorem 5.1 . . . . . .\n5.2.2 Proof of theorem 5.2 . . . . . .\n6 Conclusion and discussion\n\na regular\n8\n9\n12\n13\n\nnot a fixed\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n14\n14\n15\n18\n20\n22\n22\n23\n23\n23\n\nare not consistent when\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n25\n25\n26\n26\n28\n28\n\nA Proof of theorems for finite groups’ setting\n29\nA.1 Proof of theorem 3.2: differentiation of the variance in the quotient space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nA.2 Proof of theorem 3.1: the gradient is not zero at the template . . 32\nA.3 Proof of theorem 3.3: upper bound of the consistency bias . . . . 32\nA.4 Proof of proposition 3.2: inconsistency in R2 for the action of\ntranslation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\nB Proof of lemma 5.1: differentiation of the variance in the top\nspace\n35\n\n2\n\n\f1\n\nIntroduction\n\nIn Kendall’s shape space theory [Ken89], in computational anatomy [GM98],\nin statistics on signals, or in image analysis, one often aims at estimating a\ntemplate. A template stands for a prototype of the data. The data can be the\nshape of an organ studied in a population [DPC+ 14] or an aircraft [LAJ+ 12],\nan electrical signal of the human body, a MR image etc. To analyse the observations, one assumes that these data follow a statistical model. One often\nmodels observations as random deformations of the template with additional\nnoise. This deformable template model proposed in [GM98] is commonly used\nin computational anatomy. The concept of deformation introduces the notion of\ngroup action: the deformations we consider are elements of a group which acts\non the space of observations, called here the top space. Since the deformations\nare unknown, one usually considers equivalent classes of observations under the\ngroup action. In other words, one considers the quotient space of the top space\n(or ambient space) by the group. In this particular setting, the template estimation is most of the time based on the minimisation of the empirical variance\nin the quotient space (for instance [KSW11, JDJG04, SBG08] among many others). The points that minimise the empirical variance are called the empirical\nFréchet mean. The Fréchet means introduced in [Fré48] is comprised of the\nelements minimising the variance. This generalises the notion of expected value\nin non linear spaces. Note that the existence or uniqueness of Fréchet mean is\nnot ensured. But sufficient conditions may be given in order to reach existence\nand uniqueness (for instance [Kar77] and [Ken90]).\nSeveral group actions are used in practice: some signals can be shifted in\ntime compared to other signals (action of translations [HCG+ 13]), landmarks\ncan be transformed rigidly [Ken89], shapes can be deformed by diffeomorphisms [DPC+ 14], etc. In this paper we restrict to transformation which leads\nthe norm unchanged. Rotations for instance leave the norm unchanged, but it\nmay seem restrictive. In fact, the square root trick detailed in section 5, allows\nto build norms which are unchanged, for instance by reparametrization of curves\nwith a diffeomorphism, where our work can be applied.\nWe raise several issues concerning the estimation of the template.\n1. Is the Fréchet mean in the quotient space equal to the original template\nprojected in the quotient space? In other words, is the template estimation\nwith the Fréchet mean in quotient space consistent?\n2. If there is an inconsistency, how large is the consistency bias? Indeed,\nwe may expect the consistency bias to be negligible in many practicable\ncases.\n3. If one gets only a finite sample, one can only estimate the empirical Fréchet\nmean. How far is the empirical Fréchet mean from the original template?\nThese issues originated from an example exhibited by Allassonnière, Amit and\nTrouvé [AAT07]: they took a step function as a template and they added some\n\n3\n\n\fnoise and shifted in time this function. By repeating this process they created a\ndata sample from this template. With this data sample, they tried to estimate\nthe template with the empirical Fréchet mean in the quotient space. In this\nexample, minimising the empirical variance did not succeed in estimating well\nthe template when the noise added to the template increases, even with a large\nsample size.\nOne solution to ensure convergence to the template is to replace this estimation method with a Bayesian paradigm ([AKT10, BG14] or [ZSF13]). But there\nis a need to have a better understanding of the failure of the template estimation with the Fréchet mean. One can studied the inconsistency of the template\nestimation. Bigot and Charlier [BC11] first studied the question of the template\nestimation with a finite sample in the case of translated signals or images by\nproviding a lower bound of the consistency bias. This lower bound was unfortunately not so informative as it is converging to zero asymptotically when the\ndimension of the space tends to infinity. Miolane et al. [MP15, MHP16] later\nprovided a more general explanation of why the template is badly estimated\nfor a general group action thanks to a geometric interpretation. They showed\nthat the external curvature of the orbits is responsible for the inconsistency.\nThis result was further quantified with Gaussian noise. In this article, we provide sufficient conditions on the noise for which inconsistency appears and we\nquantify the consistency bias in the general (non necessarily Gaussian) case.\nMoreover, we mostly consider a vector space (possibly infinite dimensional) as\nthe top space while the article of Miolane et al. is restricted to finite dimensional manifolds. In a preliminary unpublished version of this work [ADP15],\nwe proved the inconsistency when the transformations come from a finite group\nacting by translation. The current article extends these results by generalizing\nto any isometric action of finite and non-finite groups.\nThis article is organised as follows. Section 2 details the mathematical terms\nthat we use and the generative model. In sections 3 and 4, we exhibit sufficient\ncondition that lead to an inconsistency when the template is not a fixed point\nunder the group action. This sufficient condition can be roughly understand as\nfollows: with a non zero probability, the projection of the random variable on\nthe orbit of the template is different from the template itself. This condition is\nactually quite general. In particular, this condition it is always fulfilled with the\nGaussian noise or with any noise whose support is the whole space. Moreover\nwe quantify the consistency bias with lower and upper bounds. We restrict\nour study to Hilbert spaces and isometric actions. This means that the space\nis linear, the group acts linearly and leaves the norm (or the dot product)\nunchanged. Section 3 is dedicated to finite groups. Then we generalise our\nresult in section 4 to non-finite groups. To complete this study, we extend in\nsection 5 the result when the template is a fixed point under the group action\nand when the top space is a manifold. As a result we show that the inconsistency\nexists for almost all noises. Although the bias can be neglected when the noise\nlevel is sufficiently small, its linear asymptotic behaviour with respect to the\nnoise level show that it becomes unavoidable for large noises.\n\n4\n\n\f2\n\nDefinitions, notations and generative model\n\nWe denote by M the top space, which is the image/shape space, and G the\ngroup acting on M . The action is a map:\nG×M\n(g, m)\n\n→\nM\n7→ g · m\n\nsatisfying the following properties: for all g, g 0 ∈ G, m ∈ M (gg 0 )·m = g ·(g 0 ·m)\nand eG · m = m where eG is the neutral element of G. For m ∈ M we note by\n[m] the orbit of m (or the class of m). This is the set of points reachable from\nm under the group action: [m] = {g · m, g ∈ G}. Note that if we take two orbits\n[m] and [n] there are two possibilities:\n1. The orbits are equal: [m] = [n] i.e. ∃g ∈ G s.t. n = g · m.\n2. The orbits have an empty intersection: [m] ∩ [n] = ∅.\nWe call quotient of M by the group G the set all orbits. This quotient is noted\nby:\nQ = M/G = {[m], m ∈ M }.\nThe orbit of an element m ∈ M can be seen as the subset of M of all elements\ng · m for g ∈ G or as a point in the quotient space. In this article we use these\ntwo ways. We project an element m of the top space M into the quotient by\ntaking [m].\nNow we are interested in adding a structure on the quotient from an existing\nstructure in the top space: take M a metric space, with dM its distance. Suppose\nthat dM is invariant under the group action which means that ∀g ∈ G, ∀a, b ∈\nM dM (a, b) = dM (g · a, g · b). Then we obtain a pseudo-distance on Q defined\nby:\ndQ ([a], [b]) = inf dM (g · a, b).\n(1)\ng∈G\n\nWe remind that a distance on M is a map dM : M × M 7→ R+ such that for all\nm, n, p ∈ M :\n1. dM (m, n) = dM (n, m) (symmetry).\n2. dM (m, n) ≤ dM (m, p) + dM (p, n) (triangular inequality).\n3. dM (m, m) = 0.\n4. dM (m, n) = 0 ⇐⇒ m = n.\nA pseudo-distance satisfies only the first three conditions. If we suppose that\nall the orbits are closed sets of M , then one can show that dQ is a distance. In\nthis article, we assume that dQ is always a distance, even if a pseudo-distance\nwould be sufficient. dQ ([a], [b]) can be interpreted as the distance between the\nshapes a and b, once one has removed the parametrisation by the group G. In\nother words, a and b have been registered. In this article, except in section 5, we\n5\n\n\fsuppose that the the group acts isometrically on an Hilbert space, this means\nthat the map x 7→ g ·x is linear, and that the norm associated to the dot product\nis conserved: kg · xk = kxk. Then dM (a, b) = ka − bk is a particular case of\ninvariant distance.\nWe now introduce the generative model used in this article for M a\nvector space. Let us take a template t0 ∈ M to which we add a unbiased noise\n\u000f: X = t0 + \u000f. Finally we transform X with a random shift S of G. We assume\nthat this variable S is independent of X and the only observed variable is:\nY = S · X = S · (t0 + \u000f), with E(\u000f) = 0,\n\n(2)\n\nwhile S, X and \u000f are hidden variables.\nNote that it is not the generative model defined by Grenander and often\nused in computational anatomy. Where the observed variable is rather Y 0 =\nS · t0 + \u000f0 . But when the noise is isotropic and the action is isometric, one can\nshow that the two models have the same law, since S · \u000f and \u000f have the same\nprobability distribution. As a consequence, the inconsistency of the template\nestimation with the Fréchet mean in quotient space with one model implies the\ninconsistency with the other model. Because the former model (2) leads to\nsimpler computation we consider only this model.\nWe can now set the inverse problem: given the observation Y , how to estimate the template t0 in M ? This is an ill-posed problem. Indeed for some\nelement group g ∈ G, the template t0 can be replaced by the translated g ·t0 , the\nshift S by Sg −1 and the noise \u000f by g\u000f, which leads to the same observation Y . So\ninstead of estimating the template t0 , we estimate its orbit [t0 ]. By projecting\nthe observation Y in the quotient space we obtain [Y ]. Although the observation\nY = S · X and the noisy template X are different random variables in the top\nspace, their projections on the quotient space lead to the same random orbit\n[Y ] = [X]. That is why we consider the generative model (2): the projection\nin the quotient space remove the transformation of the group G. From now on,\nwe use the random orbit [X] in lieu of the random orbit of the observation [Y ].\nThe variance of the random orbit [X] (sometimes called the Fréchet functional or the energy function) at the quotient point [m] ∈ Q is the expected\nvalue of the square distance between [m] and the random orbit [X], namely:\nQ 3 [m] 7→ E(dQ ([m], [X])2 )\n\n(3)\n\nAn orbit [m] ∈ Q which minimises this map is called a Fréchet mean of [X].\nIf we have an i.i.d sample of observations Y1 , . . . , Yn we can write the empirical quotient variance:\nn\n\nQ 3 [m] 7→\n\nn\n\n1X\n1X\ndQ ([m], [Yi ])2 =\ninf km − gi · Yi k2 .\nn i=1\nn i=1 gi ∈G\n\n(4)\n\nThanks to the equality of the quotient variables [X] and [Y ], an element which\nminimises this map is an empirical Fréchet mean of [X].\n\n6\n\n\fIn order to minimise the empirical quotient variance (4), the max-max algon\nP\nrithm1 alternatively minimises the function J(m, (gi )i ) = n1 km−gi ·Yi k2 over\ni=1\n\na point m of the orbit [m] and over the hidden transformation (gi )1≤i≤n ∈ Gn .\nWith these notations we can reformulate our questions as:\n1. Is the orbit of the template [t0 ] a minimiser of the quotient variance defined\nin (3)? If not, the Fréchet mean in quotient space is an inconsistent\nestimator of [t0 ].\n2. In this last case, can we quantify the quotient distance between [t0 ] and a\nFréchet mean of [X]?\n3. Can we quantify the distance between [t0 ] and an empirical Fréchet mean\nof a n-sample?\nThis article shows that the answer to the first question is usually \&quot;no\&quot; in the\nframework of an Hilbert space M on which a group G acts linearly and isometrically. The only exception is theorem 5.1 where the top space M is a manifold.\nIn order to prove inconsistency, an important notion in this framework is the\nisotropy group of a point m in the top space. This is the subgroup which leaves\nthis point unchanged:\nIso(m) = {g ∈ G, g · m = m}.\nWe start in section 3 with the simple example where the group is finite and the\nisotropy group of the template is reduced to the identity element (Iso(t0 ) =\n{eG }, in this case t0 is called a regular point). We turn in section 4 to the case\nof a general group and an isotropy group of the template which does not cover\nthe whole group (Iso(t0 ) 6= G) i.e t0 is not a fixed point under the group action.\nTo complete the analysis, we assume in section 5 that the template t0 is a fixed\npoint which means that Iso(t0 ) = G.\nIn sections 3 and 4 we show lower and upper bounds of the consistency bias\nwhich we define as the quotient distance between the template orbit and the\nFréchet mean in quotient space. These results give an answer to the second\nquestion. In section 4, we show a lower bound for the case of the empirical\nFréchet mean which answers to the third question.\nAs we deal with different notions whose name or definition may seem similar,\nwe use the following vocabulary:\n1. The variance of the noisy template X in the top space is the function\nE : m ∈ M 7→ E(km − Xk2 ). The unique element which minimises this\nfunction is the Fréchet mean of X in the top space. With our assumptions\nit is the template t0 itself.\n2. We call variability (or noise level) of the template the value of the variance\nat this minimum: σ 2 = E(kt0 − Xk2 ) = E(t0 ).\n1 The term max-max algorithm is used for instance in [AAT07], and we prefer to keep the\nsame name, even if it is a minimisation.\n\n7\n\n\f3. The variance of the random orbit [X] in the quotient space is the function\nF : m 7→ E(dQ ([m], [X])2 ). Notice that we define this function from the\ntop space and not from the quotient space. With this definition, an orbit\n[m? ] is a Fréchet mean of [X] if the point m? is a global minimiser of F .\nIn sections 3 and 4, we exhibit a sufficient condition for the inconsistency,\nwhich is: the noisy template X takes value with a non zero probability in\nthe set of points which are strictly closer to g · t0 for some g ∈ G than the\ntemplate t0 itself. This is linked to the folding of the distribution of the noisy\ntemplate when it is projected to the quotient space. The points for which the\ndistance to the template orbit in the quotient space is equal to the distance to\nthe template in the top space are projected without being folded. If the support\nof the distribution of the noisy template contains folded points (we only assume\nthat the probability measure of X, noted P, is a regular measure), then there\nis inconsistency. The support of the noisy template X is defined by the set of\npoints x such that P(X ∈ B(x, r)) > 0 for all r > 0. For different geometries of\nthe orbit of the template, we show that this condition is fulfilled as soon as the\nsupport of the noise is large enough.\nThe recent article of Cleveland et al. [CWS16] may seem contradictory with\nour current work. Indeed the consistency of the template estimation with the\nFréchet mean in quotient space is proved under hypotheses which seem to satisfy\nour framework: the norm is unchanged under their group action (isometric\naction) and a noise is present in their generative model. However we believe\nthat the noise they consider might actually not be measurable. Indeed, their\ntop space is:\n\u001a\n\u001b\nZ 1\nL2 ([0, 1]) = f : [0, 1] → R such that f is measurable and\nf 2 (t)dt < +∞ .\n0\n\nThe noise e is supposed to be in L2 ([0, 1]) such that for all t, s ∈ [0, 1], E(e(t)) = 0\nand E(e(t)e(s)) = σ 2 1s=t , for σ > 0. This means that e(t) and e(s) are chosen\nwithout correlation as soon as s 6= t. In this case, it is not clear for us that the\nresulting function e is measurable, and thus that its Lebesgue integration makes\nsense. Thus, the existence of such a random process should be established before\nwe can fairly compare the results of both works.\n\n3\n\nInconsistency for finite group when the template is a regular point\n\nIn this Section, we consider a finite group G acting isometrically and effectively\non M = Rn a finite dimensional space equipped with the euclidean norm k k,\nassociated to the dot product h , i.\nWe say that the action is effective if x 7→ g · x is the identity map if and only\nif g = eG . Note that if the action is not effective, we can define a new effective\naction by simply quotienting G by the subgroup of the element g ∈ G such that\nx 7→ g · x is the identity map.\n8\n\n\fThe template is assumed to be a regular point which means that the isotropy\ngroup of the template is reduced to the neutral element of G. Note that the\nmeasure of singular points (the points which are not regular) is a null set for\nthe Lebesgue measure (see item 1 in appendix A.1).\nExample 3.1. The action of translation on coordinates: this action is a simplified setting for image registration, where images can be obtained by the translation of one scan to another due to different poses. More precisely, we take\nthe vector space M = RT where G = T = (Z/N Z)D is the finite torus in Ddimension. An element of RT is seen as a function m : T → R, where m(τ ) is\nthe grey value at pixel τ . When D = 1, m can be seen like a discretised signal\nwith N points, when D = 2, we can see m like an image with N × N pixels etc.\nWe then define the group action of T on RT by:\nτ ∈ T, m ∈ RT\n\nτ · m : σ 7→ m(σ + τ ).\n\nThis group acts isometrically and effectively on M = RT .\nIn this setting, if E(kXk2 ) < +∞ then the variance of [X] is well defined:\nF : m ∈ M 7→ E(dQ ([X], [m])2 ).\nIn this framework, F is non-negative and continuous.\nSchwarz inequality we have:\nlim F (m) ≥\n\nkmk→∞\n\n(5)\nThanks to Cauchy-\n\nlim kmk2 − 2kmkE(kXk) + E(kXk2 ) = +∞.\n\nkmk→∞\n\nThus for some R > 0 we have: for all m ∈ M if kmk > R then F (m) ≥ F (0) + 1.\nThe closed ball B(0, R) is a compact set (because M is a finite vector space)\nthen F restricted to this ball reached its minimum m? . Then for all m ∈ M ,\nif m ∈ B(0, R), F (m? ) ≤ F (m), if kmk > R then F (m) ≥ F (0) + 1 > F (0) ≥\nF (m? ). Therefore [m? ] is a Fréchet mean of [X] in the quotient Q = M/G.\nNote that this ensure the existence but not the uniqueness.\nIn this Section, we show that as soon as the support of the distribution of\nX is big enough, the orbit of the template is not a Fréchet mean of [X]. We\nprovide a upper bound of the consistency bias depending on the variability of\nX and an example of computation of this consistency bias.\n\n3.1\n\nPresence of inconsistency\n\nThe following theorem gives a sufficient condition on the random variable X for\nan inconsistency:\nTheorem 3.1. Let G be a finite group acting on M = Rn isometrically and\neffectively. Assume that the random variable X is absolutely continuous with\nrespect to the Lebesgue’s measure, with E(kXk2 ) < +∞. We assume that t0 =\nE(X) is a regular point.\n\n9\n\n\fg · t0\n\n0\n\nCone(t0 )\n\nt0\n\ng 0 · t0\nFigure 1: Planar representation of a part of the orbit of the template t0 . The\nlines are the hyperplanes whose points are equally distant of two distinct elements of the orbit of t0 , Cone(t0 ) represented in points is the set of points closer\nfrom t0 than any other points in the orbit of t0 . Theorem 3.1 states that if the\nsupport (the dotted disk) of the random variable X is not included in this cone,\nthen there is an inconsistency.\nWe define Cone(t0 ) as the set of points closer from t0 than any other points\nof the orbit [t0 ], see fig. 1 or item 6 in appendix A.1 for a formal definition. In\nother words, Cone(t0 ) is defined as the set of points already registered with t0 .\nSuppose that:\nP (X ∈\n/ Cone(t0 )) > 0,\n(6)\nthen [t0 ] is not a Fréchet mean of [X].\nThe proof of theorem 3.1 is based on two steps: first, differentiating the\nvariance F of [X]. Second, showing that the gradient at the template is not\nzero, therefore the template can not be a minimum of F . Theorem 3.2 makes\nthe first step.\nTheorem 3.2. The variance F of [X] is differentiable at any regular points. For\nm0 a regular point, we define g(x, m0 ) as the almost unique g ∈ G minimising\nkm0 − g · xk (in other words, g(x, m0 ) · x ∈ Cone(m0 )). This allows us to\ncompute the gradient of F at m0 :\n∇F (m0 ) = 2(m0 − E(g(X, m0 ) · X)).\n\n(7)\n\nThis Theorem is proved in appendix A.1. Then we show that the gradient\nof F at t0 is not zero. To ensure that F is differentiable at t0 we suppose in\nthe assumptions of theorem 3.1 that t0 = E(X) is a regular point. Thanks\nto theorem 3.2 we have:\n∇F (t0 ) = 2(t0 − E(g(X, t0 ) · X)).\nTherefore ∇F (t0 )/2 is the difference between two terms, which are represented on fig. 2: on fig. 2a there is a mass under the two hyperplanes outside\n\n10\n\n\fg · t0\n\n0\n\ng · t0\n\nCone(t0 )\n\nt0\n\n0\n\ng 0 · t0\n\nCone(t0 )\n\nt0\n\nZ\n\ng 0 · t0\n\n(a) Graphic representation of the\ntemplate t0 = E(X) mean of\npoints of the support of X.\n\n(b) Graphic representation of\nZ = E(g(X, t0 ) · X). The points\nX which were outside Cone(t0 )\nare now in Cone(t0 ) (thanks to\ng(X, t0 )). This part, in grid-line,\nrepresents the points which have\nbeen folded.\n\nFigure 2: Z is the mean of points in Cone(t0 ) where Cone(t0 ) is the set of points\ncloser from t0 than g · t0 for g ∈ G \\ eG . Therefore it seems that Z is higher that\nt0 , therefore ∇F (t0 ) = 2(t0 − Z) 6= 0.\nCone(t0 ), so this mass is nearer from gt0 for some g ∈ G than from t0 . In the following expression Z = E(g(X, t0 ) · X), for X ∈\n/ Cone(t0 ), g(X, t0 )X ∈ Cone(t0 )\nsuch points are represented in grid-line on fig. 2. This suggests that the point\nZ = E(g(X, t0 ) · X) which is the mean of points in Cone(t0 ) is further away\nfrom 0 than t0 . Then ∇F (t0 )/2 = t0 − Z should be not zero, and t0 = E(X) is\nnot a critical point of the variance of [X]. As a conclusion [t0 ] is not a Fréchet\nmean of [X]. This is turned into a rigorous proof in appendix A.2.\nIn the proof of theorem 3.1, we took M an Euclidean space and we work with\nthe Lebesgue’s measure in order to have P(X ∈ H) = 0 for every hyperplane\nH. Therefore the proof of theorem 3.1 can be extended immediately to any\nHilbert space M , if we make now the assumption that P(X ∈ H) = 0 for\nevery hyperplane H, as long as we keep a finite group acting isometrically and\neffectively on M .\nFigure 2 illustrates the condition of theorem 3.1: if there is no mass beyond\nthe hyperplanes, then the two terms in ∇F (t0 ) are equal (because almost surely\ng(X, t0 ) · X = X). Therefore in this case we have ∇F (t0 ) = 0. This do not\nprove necessarily that there is no inconsistency, just that the template t0 is a\ncritical point of F . Moreover this figure can give us an intuition on what the\nconsistency bias (the distance between [t0 ] and the set of all Fréchet mean in\nthe quotient space) depends: for t0 a fixed regular point, when the variability\nof X (defined by E(kX − t0 k2 )) increases the mass beyond the hyperplanes\non fig. 2 also increases, the distance between E(g(X, t0 ) · X) and t0 (i.e. the\nnorm of ∇F (t0 )) augments. Therefore q the Fréchet mean should be further\nfrom t0 , (because at this point one should have ∇F (q) = 0 or q is a singular\n11\n\n\fpoint). Therefore the consistency bias appears to increase with the variability\nof X. By establishing a lower and upper bound of the consistency bias and\nby computing the consistency bias in a very simple case, sections 3.2, 3.3, 4.3\nand 4.4 investigate how far this hypothesis is true.\nWe can also wonder if the converse of theorem 3.1 is true: if the support is\nincluded in Cone(t0 ), is there consistency? We do not have a general answer\nto that. In the simple example section 3.3 it happens that condition (6) is\nnecessary and sufficient. More generally the following proposition provides a\npartial converse:\nCone(y)\n\ng · t0\n\ny\nt0\nO\nCone(t0 )\ng 0 · t0\nFigure 3: y 7→ Cone(y) is continuous. When the support of the X is bounded\nand included in the interior of Cone(t0 ) the hatched cone. For y sufficiently\nclose to the template t0 , the support of the X (the ball in red) is still included\nin Cone(y) (in grey), then F (y) = (E(kX − yk2 ). Therefore in this case, [t0 ] is\nat least a Karcher mean of [X].\nProposition 3.1. If the support of X is a compact set included in the interior\nof Cone(t0 ), then the orbit of the template [t0 ] is at least a Karcher mean of [X]\n(a Karcher mean is a local minimum of the variance).\nProof. If the support of X is a compact set included in the interior of Cone(t0 )\nthen we know that X-almost surely: dQ ([X], [t0 ]) = kX −t0 k. Thus the variance\nat t0 in the quotient space is equal to the variance at t0 in the top space. Now\nby continuity of the distance map (see fig. 3) for y in a small neighbourhood\nof t0 , the support of X is still included in the interior of Cone(y). We still\nhave dQ ([X], [y]) = kX − yk X-almost surely. In other words, locally around\nt0 , the variance in the quotient space is equal to the variance in the top space.\nMoreover we know that t0 = E(X) is the only global minimiser of the variance\nof X: m 7→ E(km − Xk2 ) = E(m). Therefore t0 is a local minimum of F\nthe variance in the quotient space (since the two variances are locally equal).\nTherefore [t0 ] is at least a Karcher mean of [X] in this case.\n\n3.2\n\nUpper bound of the consistency bias\n\nIn this Subsection we show an explicit upper bound of the consistency bias.\n12\n\n\fTheorem 3.3. When G is a finite group acting isometrically on M = Rn ,\nwe denote |G| the cardinal of the group G. If X is Gaussian vector: X ∼\nN (t0 , s2 IdRn ), and m? ∈ argmin F , then we have the upper bound of the consistency bias:\np\n(8)\ndQ ([t0 ], [m? ]) ≤ s 8 log(|G|).\nThe proof is postponed in appendix A.3. When X ∼ N (t0 , s2 Idn ) the\nvariability of X is σ 2 = E(||X − t0 ||2 )p= ns2 and we can write the upper\nbound of the bias: dQ ([t0 ], [m? ]) ≤ √σn 8 log |G|. This Theorem shows that\nthe consistency bias is low when the variability of X is small, which tends to\nconfirm our hypothesis in section 3.1. It is important to notice that this upper\nbound explodes when the cardinal of the group tends to infinity.\n\n3.3\n\nStudy of the consistency bias in a simple example\n\nIn this Subsection, we take a particular case of example 3.1: the action of\ntranslation with T = Z/2Z. We identify RT with R2 and we note by (u, v)T an\nelement of RT . In this setting, one can completely describe the action of T on\nRT : 0 · (u, v)T = (u, v)T and 1 · (u, v)T = (v, u)T . The set of singularities is the\nline L = {(u, u)T , u ∈ R}. We note HPA = {(u, v)T , v > u} the half-plane\nabove L and HPB the half-plane below L. This simple example will allow us\nto provide necessary and sufficient condition for an inconsistency at regular and\nsingular points. Moreover we can compute exactly the consistency bias, and\nexhibit which parameters govern the bias. We can then find an equivalent of\nthe consistency bias when the noise tends to zero or infinity. More precisely, we\nhave the following theorem proved in appendix A.4:\nProposition 3.2. Let X be a random variable such that E(kXk2 ) < +∞ and\nt0 = E(X).\n1. If t0 ∈ L, there is no inconsistency if and only if the support of X is\nincluded in the line L = {(u, u), u ∈ R}. If t0 ∈ HPA (respectively in\nHPB ), there is no inconsistency if and only if the support of X is included\nin HPA ∪ L (respectively in HPB ∪ L).\n2. If X is Gaussian: X ∼ N (t0 , s2 Id2 ), then the Fréchet mean of [X] exists\nand is unique. This Fréchet mean [m? ] is on the line passing through E(X)\nand perpendicular to L and the consistency bias ρ̃ = dQ ([t0 ], [m? ]) is the\nfunction of s and d = dist(t0 , L) given by:\n\u0012 2\u0013 \u0012 \u0013\nZ\n2 +∞ 2\nr\nd\nρ̃(d, s) = s\nr exp −\ng\ndr,\n(9)\nπ ds\n2\nrs\nwhere g is a non-negative function on [0, 1] defined by g(x) = sin(arccos(x))−\nx arccos(x).\n(a) If d > 0 then s 7→ ρ̃(d, s) has an asymptotic linear expansion:\n\u0012 2\u0013\nZ\n2 +∞ 2\nr\nρ̃(d, s) ∼ s\nr exp −\ndr.\ns→∞ π 0\n2\n13\n\n(10)\n\n\f(b) If d > 0, then ρ̃(d, s) = o(sk ) when s → 0, for all k ∈ N.\n(c) s →\n7 ρ̃(0, s) is linear with respect to s (for d = 0 the template is a\nfixed point).\nRemark 3.1. Here, contrarily to the case of the action of rotation in [MHP16],\nit is not the ratio kE(X)k over the noise which matters to estimate the consistency bias. Rather the ratio dist(E(X), L) over the noise. However in both cases\nwe measure the distance between the signal and the singularities which was {0}\nin [MHP16] for the action of rotations, L in this case.\n\n4\n\nInconsistency for any group when the template\nis not a fixed point\n\nIn section 3 we exhibited sufficient condition to have an inconsistency, restricted\nto the case of finite group acting on an Euclidean space. We now generalize this\nanalysis to Hilbert spaces of any dimension included infinite. Let M be such\nan Hilbert space with its dot product noted by h , i and its associated norm\nk k. In this section, we do not anymore suppose that the group G is finite.\nIn the following, we prove that there is an inconsistency in a large number of\nsituations, and we quantify the consistency bias with lower and upper bounds.\nExample 4.1. The action of continuous translation: We take G = (R/Z)D\nacting on M = L2 ((R/Z)D , R) with:\n∀τ ∈ G\n\n∀f ∈ M\n\n(τ · f ) : t 7→ f (t + τ )\n\nThis isometric action is the continuous version of the example 3.1: the elements\nof M are now continuous images in dimension D.\n\n4.1\n\nPresence of an inconsistency\n\nWe state here a generalization of theorem 3.1:\nTheorem 4.1. Let G be a group acting isometrically on M an Hilbert space,\nand X a random variable in M , E(kXk2 ) < +∞ and E(X) = t0 6= 0. If:\nP (dQ ([t0 ], [X]) < kt0 − Xk) > 0,\n\n(11)\n\n\u0012\n\u0013\nP sup hg · X, t0 i > hX, t0 i > 0.\n\n(12)\n\nor equivalently:\ng∈G\n\nThen [t0 ] is not a Fréchet mean of [X] in Q = M/G.\nThe condition of this Theorem is the same condition of theorem 3.1: the\nsupport of the law of X contains points closer from gt0 for some g than t0 .\nThus the condition (12) is equivalent to E(dQ ([X], [t0 ])2 ) < E(kX − t0 k2 ). In\nother words, the variance in the quotient space at t0 is strictly smaller than the\nvariance in the top space at t0 .\n14\n\n\fProof. First the two conditions are equivalent by definition of the quotient distance and by expansion of the square norm of kt0 − Xk and of kt0 − gXk for\ng ∈ G.\nAs above, we define the variance of [X] by:\n\u0012\n\u0013\n2\nF (m) = E inf kg · X − mk .\ng∈G\n\nIn order to prove this Theorem, we find a point m such that F (m) < F (t0 ),\nwhich directly implies that [t0 ] is not be a Fréchet mean of [X].\nIn the proof of theorem 3.1, we showed that under condition (6) we had\nh∇F (t0 ), t0 i < 0. This leads us to study F restricted to R+ t0 : we define for\na ∈ R+ f (a) = F (at0 ) = E(inf g∈G kg · X − ak2 ). Thanks to the isometric action\nwe can expand f (a) by:\n\u0012\n\u0013\nf (a) = a2 kt0 k2 − 2aE sup hg · X, t0 i + E(kXk2 ),\n(13)\ng∈G\n\nand explicit the unique element of R+ which minimises f :\n\u0012\n\u0013\nE sup hg · X, t0 i\ng∈G\n.\na? =\nkt0 k2\n\n(14)\n\nFor all x ∈ M , we have sup hg · x, t0 i ≥ hx, t0 i and thanks to condition (12) we\ng∈G\n\nget:\nE(sup hg · X, t0 i) > E(hX, t0 i) = hE(X), t0 i = kt0 k2 ,\n\n(15)\n\ng∈G\n\nwhich implies a? > 1. Then F (a? t0 ) < F (t0 ).\n\u0001\nNote that kt0 k2 (a? − 1) = E supg∈G hg · X, t0 i − E(hX, t0 i) (which is positive) is exactly − h∇F (t0 ), t0 i /2 in the case of finite group, see Equation (44).\nHere we find the same expression without having to differentiate the variance\nF , which may be not possible in the current setting.\n\n4.2\n\nAnalysis of the condition in theorem 4.1\n\nWe now look for general cases when we are sure that Equation (12) holds which\nimplies the presence of inconsistency. We saw in section 3 that when the group\nwas finite, it is possible to have no inconsistency only if the support of the\nlaw is included in a cone delimited by some hyperplanes. The hyperplanes were\ndefined as the set of points equally distant of the template t0 and g ·t0 for g ∈ G.\nTherefore if the cardinal of the group becomes more and more important, one\ncould think that in order to have no inconsistency the space where X should\ntakes value becomes smaller and smaller. At the limit it leaves only at most an\nhyperplane. In the following, we formalise this idea to make it rigorous. We\nshow that the cases where theorem 4.1 cannot be applied are not generic cases.\n15\n\n\fFirst we can notice that it is not possible to have the condition (12) if t0 is a\nfixed point under the action of G. Indeed in this case hg · X, t0 i = X, g −1 t0 =\nhX, t0 i). So from now, we suppose that t0 is not a fixed point. Now let us see\nsome settings when we have the condition (11) and thus condition (12).\nProposition 4.1. Let G be a group acting isometrically on an Hilbert space M ,\nand X a random variable in M , with E(kXk2 ) < +∞ and E(X) = t0 6= 0. If:\n1. [t0 ] \\ {t0 } is a dense set in [t0 ].\n2. There exists η > 0 such that the support of X contains a ball B(t0 , η).\nThen condition (12) holds, and the estimator is inconsistent according to theorem 4.1.\n\nB(t0 , η)\nO\n\nt0\ng · t0\n\n[t0 ]\nFigure 4: The smallest disk is included in the support of X and the points in\nthat disk is closer from g · t0 than from t0 . According to theorem 4.1 there is an\ninconsistency.\nProof. By density, one takes g · t0 ∈ B(t0 , η) \\ {t0 } for some g ∈ G, now if we\ntake r < min(kg ·t0 −t0 k/2, η −kg ·t0 −t0 k) then B(g ·t0 , r) ⊂ B(t0 , \u000f). Therefore\nby the assumption we made on the support one has P(X ∈ B(g · t0 , r)) > 0.\nFor y ∈ B(g · t0 , r) we have that kgt0 − yk < kt0 − yk (see fig. 4). Then we\nhave: P (dQ ([X], [t0 ]) < kX − t0 k) ≥ P(X ∈ B(g · t0 , r)) > 0. Then we verify\ncondition (12), and we can apply theorem 4.1.\nProposition 4.1 proves that there is a large number of cases where we can\nensure the presence of an inconsistency. For instance when M is a finite dimensional vector space and the random variable X has a continuous positive\ndensity (for the Lebesgue’s measure) at t0 , condition 2 of Proposition 4.1 is\nfulfilled. Unfortunately this proposition do not cover the case where there is no\nmass at the expected value t0 = E(X). This situation could appear if X has\ntwo modes for instance. The following proposition deals with this situation:\n16\n\n\fProposition 4.2. Let G be a group acting isometrically on M . Let X be a\nrandom variable in M , such that E(kXk2 ) < +∞ and E(X) = t0 6= 0. If:\n1. ∃ϕ s.t. ϕ : (−a, a) → [t0 ] is C 1 with ϕ(0) = t0 , ϕ0 (0) = v 6= 0.\n2. The support of X is not included in the hyperplane v ⊥ : P(X ∈\n/ v ⊥ ) > 0.\nThen condition (12) is fulfilled, which leads to an inconsistency thanks to Theorem 4.1.\nProof. Thanks to the isometric action: ht0 , vi = 0. We choose y ∈\n/ v ⊥ in the\nsupport of X and make a Taylor expansion of the following square distance (see\nalso Figure 5) at 0:\nkϕ(x) − yk2 = kt0 + xv + o(x) − yk2 = kt0 − yk2 − 2x hy, vi + o(x).\nThen: ∃x? ∈ (−a, a) s.t. kx? k < a, x hy, vi > 0 and kϕ(x? ) − yk < kt0 − yk. For\nsome g ∈ G, ϕ(x? ) = g · t0 . By continuity of the norm we have:\n∃r > 0 s.t. ∀z ∈ B(y, r) kg · t0 − zk < kt0 − zk.\nThen P(kg·t0 −Xk < kt0 −Xk) ≥ P(X ∈ B(y, r)) > 0. Theorem 4.1 applies.\nProposition 4.2 was a sufficient condition on inconsistency in the case of an\norbit which contains a curve. This brings us to extend this result for orbits\nwhich are manifolds:\nProposition 4.3. Let G be a group acting isometrically on an Hilbert space M ,\nX a random variable in M , with E(kXk2 ) < +∞. Assume X = t0 + σ\u000f, where\nt0 6= 0 and E(\u000f) = 0, and E(k\u000fk) = 1. We suppose that [t0 ] is a sub-manifold of\nM and write Tt0 [t0 ] the linear tangent space of [t0 ] at t0 . If:\nP(X ∈\n/ Tt0 [t0 ]⊥ ) > 0,\n\n(16)\n\nP(\u000f ∈\n/ Tt0 [t0 ]⊥ ) > 0,\n\n(17)\n\nwhich is equivalent to:\nthen there is an inconsistency.\nProof. First t0 ⊥ Tt0 [t0 ] (because the action is isometric), Tt0 [t0 ]⊥ = t0 +\nTt0 [t0 ]⊥ , then the event {X ∈ Tt0 [t0 ]⊥ } is equal to {\u000f ∈ Tt0 [t0 ]⊥ }. This proves\nthat equations (16) and (17) are equivalent. Thanks to assumption (16), we can\nchoose y in the support of X such that y ∈\n/ Tt0 [t0 ]⊥ . Let us take v ∈ Tt0 [t0 ]\n1\nsuch that hy, vi =\n6 0 and choose ϕ a C curve in [t0 ], such that ϕ(0) = t0 and\nϕ0 (0) = v. Applying proposition 4.2 we get the inconsistency.\nNote that Condition (16) is very weak, because Tt0 [t0 ] is a strict linear\nsubspace of M .\n\n17\n\n\f[t0 ]\n\nTt0 [t0 ]\n\ny\n\ng · t0\nO\nt0\n\nTt0 [t0 ]⊥\n\nFigure 5: y ∈\n/ Tt0 [t0 ]⊥ therefore y is closer from g · t0 for some g ∈ G than t0\nitself. In conclusion, if y is in the support of X, there is an inconsistency.\n\n4.3\n\nLower bound of the consistency bias\n\nUnder the assumption of Theorem 4.1, we have an element a? t0 such that\nF (a? t0 ) < F (t0 ) where F is the variance of [X]. From this element, we deduce lower bounds of the consistency bias:\nTheorem 4.2. Let δ be the unique positive solution of the following equation:\nδ 2 + 2δ (kt0 k + EkXk) − kt0 k2 (a? − 1)2 = 0.\nLet δ? be the unique positive solution of the following equation:\n\u0010\n\u0011\np\nδ 2 + 2δkt0 k 1 + 1 + σ 2 /kt0 k2 − kt0 k2 (a? − 1)2 = 0,\n\n(18)\n\n(19)\n\nwhere σ 2 = E(kX − t0 k2 ) is the variability of X. Then δ and δ? are two lower\nbounds of the consistency bias.\nProof. In order to prove this Theorem, we exhibit a ball around t0 such that the\npoints on this ball have a variance bigger than the variance at the point a? t0 ,\nwhere a? was defined in Equation (14): thanks to the expansion of the function\nf we did in (13) we get :\nF (t0 ) − F (a? t0 ) = kt0 k2 (a? − 1)2 > 0,\n\n(20)\n\nMoreover we can show (exactly like equation (43)) that for all x ∈ M :\n\u0012\n\u0013\n2\n2\n|F (t0 ) − F (x)| ≤ E inf kg · X − t0 k − inf kg · X − xk\ng∈G\n\ng∈G\n\n≤ kx − t0 k (2kt0 k + kx − t0 k + E(k2Xk)) .\n\n(21)\n\nWith Equations (20) and (21), for all x ∈ B(t0 , δ) we have F (x) > F (a? t0 ).\nNo point in that ball mapped in the quotient space is a Fréchet mean of [X]. So\n18\n\n\fδ is a lower bound of the consistency bias. Now by using\u0010the fact that E(kXk)\u0011 ≤\np\np\nkt0 k2 + σ 2 , we get: 2|F (t0 )−F (x)| ≤ 2kx−t0 k×kt0 k 1 + 1 + σ 2 /kt0 k2 +\nkx − t0 k2 . This proves that δ? is also a lower bound of the consistency bias.\nδ? is smaller than δ, but the variability of X intervenes in δ? . Therefore we\npropose to study the asymptotic behaviour of δ? when the variability tends to\ninfinity. We have the following proposition:\nProposition 4.4. Under the hypotheses of Theorem 4.2, we write X = t0 + σ\u000f,\nwith E(\u000f) = 0, and E(k\u000fk2 ) = 1 and note ν = E(supg∈G hg\u000f, t0 /kt0 ki) ∈ (0, 1],\nwe have that:\np\nδ? ∼ σ( 1 + ν 2 − 1),\nσ→+∞\n\nIn particular, the consistency bias explodes when the variability of X tends\nto infinity.\nProof. First, let us prove that that ν ∈ (0, 1] under the condition (12). We\nhave ν ≥ E(h\u000f, t0 /kt0 ki = 0. By a reductio ad absurdum: if ν = 0, then\nsup hg\u000f, t0 i = h\u000f, t0 i almost surely. We have then almost surely: hX, t0 i ≤\ng∈G\n\nsupg∈G hgX, t0 i ≤ kt0 k2 + supg∈G σ hg\u000f, t0 i = kt0 k2 + σ h\u000f, t0 i ≤ hX, t0 i , which\np\nis in contradiction with (12). Besides ν ≤ E(k\u000fk) ≤ Ek\u000fk2 = 1\nSecond, we exhibit equivalent of the terms in equation (19) when σ → +∞:\n\u0010\n\u0011\np\n2kt0 k 1 + 1 + σ 2 /kt0 k2 ∼ 2σ.\n(22)\nNow by definition of a? in Equation (14) and the decomposition of X = t0 + σ\u000f\nwe get:\n\u0012\n\u0013\n1\nE sup (hg · t0 , t0 i + hg · σ\u000f, t0 i) − kt0 k\nkt0 k(a? − 1) =\nkt0 k\ng∈G\n\u0012\n\u0013\n1\nkt0 k(a? − 1) ≤\nE sup hg · σ\u000f, t0 i = σν\n(23)\nkt0 k\ng∈G\n\u0012\n\u0013\n1\nkt0 k(a? − 1) ≥\nE sup hg · σ\u000f, t0 i − 2kt0 k = σν − 2kt0 k,\n(24)\nkt0 k\ng∈G\nThe lower bound and the upper bound of kt0 k(a? −1) found in (23) and (24) are\nboth equivalent to σν, when σ → +∞. Then the constant term of the quadratic\nEquation (19) has an equivalent:\n− kt0 k2 (a? − 1)2 ∼ −σ 2 ν 2 .\n\n(25)\n\nFinallye if we solve the quadratic Equation (19), we write δ? as a function of\nthe coefficients of the quadratic equation (19). We use the equivalent of each of\nthese terms thanks to equation (22) and (25), this proves proposition 4.4.\n\n19\n\n\fRemark 4.1. Thanks to inequality (24), if ktσ0 k < ν2 , then kt0 k2 (1 − a? )2 ≥\n(σν −2kt0 k)2 , then we write δ? as a function of the coefficients of Equation (19),\nwe obtain a lower bound of the inconsistency bias as a function of kt0 k, σ and\nν for σ > 2kt0 k/ν:\nq\np\np\nδ?\n2\n2\n≥ −(1 + 1 + σ /kt0 k ) + (1 + 1 + σ 2 /kt0 k2 )2 + (σν/kt0 k − 2)2 .\nkt0 k\nAlthough the constant ν intervenes in this lower bound, it is not an explicit\nterm. We now explicit its behaviour depending on t0 . We remind that:\n\u0012\n\u0013\n1\nν=\nE sup hg\u000f, t0 i .\nkt0 k\ng∈G\nTo this end, we first note that the set of fixed points under the action of G is a\nclosed linear space, (because we can write it as an intersection of the kernel of\nthe continuous and linear functions: x 7→ g · x − x for all g ∈ G). We denote by\np the orthogonal projection on the set of fixed points Fix(M ). Then for x ∈ M ,\nwe have: dist(x, Fix(M )) = kx − p(x)k. Which yields:\nhg\u000f, t0 i = hg\u000f, t0 − p(t0 )i + h\u000f, p(t0 )i .\n\n(26)\n\nThe right hand side of Equation (26) does not depend on g as p(t0 ) ∈ Fix(M ).\nThen:\n\u0012\n\u0013\nkt0 kν = E sup hg\u000f, t0 − p(t0 )i + hE(\u000f), p(t0 )i .\ng∈G\n\nApplying the Cauchy-Schwarz inequality and using E(\u000f) = 0, we can conclude\nthat:\nν≤\n\n1\ndist(t0 , Fix(M ))E(k\u000fk) = dist(t0 /kt0 k, Fix(M ))E(k\u000fk).\nkt0 k\n\n(27)\n\nThis leads to the following comment: our lower bound of the consistency bias is\nsmaller when our normalized template t0 /kt0 k is closer to the set of fixed points.\n\n4.4\n\nUpper bound of the consistency bias\n\nIn this Section, we find a upper bound of the consistency bias. More precisely\nwe have the following Theorem:\nProposition 4.5. Let X be a random variable in M , such that X = t0 + σ\u000f\nwhere σ > 0, E(\u000f) = 0 and E(||\u000f||2 ) = 1. We suppose that [m? ] is a Fréchet\nmean of [X]. Then we have the following upper bound of the quotient distance\nbetween the orbit of the template t0 and the Fréchet mean of [X]:\np\ndQ ([m? ], [t0 ]) ≤ σν(m∗ −m0 )+ σ 2 ν(m∗ − m0 )2 + 2dist(t0 , Fix(M ))σν(m∗ − m0 ),\n(28)\nwhere we have noted ν(m) = E(supg hg\u000f, m/kmki) ∈ [0, 1] if m 6= 0 and\nν(0) = 0, and m0 the orthogonal projection of t0 on F ix(M ).\n20\n\n\fNote that we made no hypothesis on the template\npin this proposition. We\ndeduce from Equation (28) that √\ndQ ([m? ], [t0 ]) ≤ σ + σ 2 + 2σdist(t0 , Fix(M ))\nis a O(σ) when σ → ∞, but a O( σ) when σ → 0, in particular the consistency\nbias can be neglected when σ is small.\nProof. First we have:\nF (m? ) ≤ F (t0 ) = E(inf ||t0 − g(t0 + σ\u000f)||2 ) ≤ E(||σ\u000f||2 ) = σ 2 .\ng\n\n(29)\n\nSecondly we have for all m ∈ M , (in particular for m? ):\nF (m) =\n\nE(inf (km − gt0 k2 + σ 2 k\u000fk2 − 2hgσ\u000f, m − gt0 i))\n\n≥\n\ndQ ([m], [t0 ])2 + σ 2 − 2E(suphσ\u000f, gmi).\n\ng\n\n(30)\n\ng\n\nWith Inequalities (29) and (30) one gets:\ndQ ([m∗ ], [t0 ])2 ≤ 2E(sup hσ\u000f, gm? i) = 2σν(m? )||m? ||,\ng\n\nnote that at this point, if m? = 0 then E(supg hσ\u000f, gm? i) = 0 and ν(m? ) = 0\nalthough Equation (4.4) is still true even if m? = 0. Moreover with the triangular\ninequality applied at [m? ], [0] and [t0 ], one gets: km? k ≤ kt0 k + dQ ([m? ], [t0 ])\nand then:\ndQ ([m∗ ], [t0 ])2 ≤ 2σν(m? )(dQ ([m∗ ], [t0 ]) + kt0 k).\n(31)\nWe can solve inequality (31) and we get:\np\ndQ ([m? ], [t0 ]) ≤ σν(m? ) + σ 2 ν(m? )2 + 2kt0 kσν(m? ),\n\n(32)\n\nWe note by FX instead of F the variance in the quotient space of [X], and we\nwant to apply inequality (32) to X − m0 . As m0 is a fixed point:\n\u0012\n\u0013\n2\nFX (m) = E inf kX − m0 − g · (m − m0 )k = FX−m0 (m − m0 )\ng∈G\n\nThen m? minimises FX if and only if m? − m0 minimises FX−m0 . We apply\nEquation (32) to X − m0 , with E(X − m0 ) = t0 − m0 and [m? − m0 ] a Fréchet\nmean of [X − m0 ]. We get:\np\ndQ ([m? −m0 ], [t0 −m0 ]) ≤ σν(m∗ −m0 )+ σ 2 ν(m∗ − m0 )2 + 2kt0 − m0 kσν(m∗ − m0 ).\nMoreover dQ ([m? ], [t0 ]) = dQ ([m? − m0 ], [t0 − m0 ]), which concludes the proof.\n\n21\n\n\f4.5\n\nEmpirical Fréchet mean\n\nIn practice, we never compute the Fréchet mean in quotient space, only the\nempirical Fréchet mean in quotient space when the size of a sample is supposed\nto be large enough. If the empirical Fréchet in the quotient space means converges to the Fréchet mean in the quotient space then we can not use these\nempirical Fréchet mean in order to estimate the template. In [BB08], it has\nbeen proved that the empirical Fréchet mean converges to the Fréchet mean\nwith a √1n convergence speed, however the law of the random variable is supposed to be included in a ball whose radius depends on the geometry on the\nmanifold. Here we are not in a manifold, indeed the quotient space contains\nsingularities, moreover we do not suppose that the law is necessarily bounded.\nHowever in [Zie77] the empirical Fréchet means is proved to converge to the\nFréchet means but no convergence rate is provided.\nWe propose now to prove that the quotient distance between the template\nand the empirical Fréchet mean in quotient space have an lower bound which\nis the asymptotic of the one lower bound of the consistency bias found in (18).\nTake X, X1 , . . . , Xn independent and identically distributed (with t0 = E(X)\nnot a fixed point). We define the empirical variance of [X] by:\nn\n\nm ∈ M 7→ Fn (m) =\n\nn\n\n1X\n1X\ndQ ([m], [Xi ])2 =\ninf km − g · Xi k2 ,\nn i=1\nn i=1 g∈G\n\nand we say that [mn? ] is a empirical Fréchet mean of [X] if mn? is a global\nminimiser of Fn .\nProposition 4.6. Let X, X1 , . . . , Xn independent and identically distributed\nrandom variables, with t0 = E(X). Let be [mn? ] be an empirical Fréchet mean\nof [X]. Then δn is a lower bound of the quotient distance between the orbit of\nthe template and [mn? ], where δn is the unique positive solution of:\n!\nn\n1X\n2\nkXi k δ − kt0 k2 (an? − 1)2 = 0.\nδ + 2 ||t0 || +\nn i=1\nan? is defined like a? in section 4.1 by:\nn\nP\n1\nsup hg · Xi , t0 i\nn\ni=1g∈G\nan? =\n.\nkt0 k2\nWe have that δn → δ by the law of large numbers.\nThe proof is a direct application of theorem 4.2, but applied to the empirical\nlaw of X given by the realization of X1 , . . . , Xn .\n\n4.6\n\nExamples\n\nIn this Subsection, we discuss, in some examples, the application of theorem 4.1\nand see the behaviour of the constant ν. This constant intervened in lower\nbound of the consistency bias.\n22\n\n\f4.6.1\n\nAction of translation on L2 (R/Z)\n\nWe take an orbit O = [f0 ], where f0 ∈ C 2 (R/Z), non constant. We show\neasily that O is a manifold of dimension 1 and the tangent space at f0 is2\nRf00 . Therefore a sufficient condition on X such that E(X) = f0 to have an\ninconsistency is: P(X ∈\n/ f00⊥ ) > 0 according to proposition 4.3. Now if we\ndenote by 1 the constant function on R/Z equal to 1. We have in this setting:\nthat the set of fixed points under the action of G is the set of constant functions:\nFix(M ) = R1 and:\ns\n\u00132\nZ 1\u0012\nZ 1\nf0 (t) −\nf0 (s)ds dt.\ndist(f0 , Fix(M )) = kf0 − hf0 , 1i 1k =\n0\n\n0\n\nThis distance to the fixed points is used in the upper bound of the constant ν in\nEquation (27). Note that if f0 is not differentiable, then [f0 ] is not necessarily\na manifold, and (4.3) does not apply. However proposition 4.1 does: if f0 is not\na constant function, then [f0 ] \\ {f0 } is dense in [f0 ]. Therefore as soon as the\nsupport of X contains a ball around f0 , there is an inconsistency.\n4.6.2\n\nAction of discrete translation on RZ/NZ\n\nWe come back on example 3.1, with D = 1 (discretised signals). For some signal\nt0 , ν previously defined is:\n\u0012\n\u0013\n1\nν=\nE max h\u000f, τ · t0 i .\nkt0 k\nτ ∈Z/NZ\nTherefore if we have a sample of size I of \u000f iid, then:\nν=\n\nI\n1X\n1\nlim\nmax h\u000fi , τi · t0 i ,\nkt0 k I→+∞ I i=1 τi ∈Z/N Z\n\nBy an exhaustive research, we can find the τi ’s which maximise the dot product, then with this sample and t0 we can approximate ν. We have done this\napproximation for several signals t0 on fig. 6. According the previous results,\nthe bigger ν is, the more important the lower bound of the consistency bias is.\nWe remark that the ν estimated is small, ν \u001c 1 for different signals.\n4.6.3\n\nAction of rotations on Rn\n\nNow we consider the action of rotations on Rn with a Gaussian noise. Take\nX ∼ N (t0 , s2 Idn ) then the variability of X is ns2 , then X has a decomposition:\n] − 21 , 12 [ →\nO\nis a local parametrisation of O: f0 = ϕ(0), and we\nt\n7→ f0 (. − t)\n0\ncheck that: lim kϕ(x) − ϕ(0) − xf0 kL2 = 0 with Taylor-Lagrange inequality at the order\n2 Indeed\n\nϕ :\n\nx→0\n\n2. As a conclusion ϕ is differentiable at 0, and it is an immersion (since f00 6= 0), and\nD0 ϕ : x 7→ xf00 , then O is a manifold of dimension 1 and the tangent space of O at f0 is:\nTf0 O = D0 ϕ(R) = Rf00 .\n\n23\n\n\fnu value for each signal\n0.4\n0.14456\n0.082143\n0.24981\n\n0.3\n\n0.2\n\n0.1\n\n0\n\n-0.1\n\n-0.2\n\n-0.3\n\n-0.4\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nFigure 6: Different signals and their ν approximated with a sample of size 103\nin RZ/100Z . \u000f is here a Gaussian noise in RZ/100Z , such that E(\u000f) = 0 and\nE(k\u000fk2 ) = 1. For instance the blue signal is a signal defined randomly, and\nwhen we approximate the ν which corresponds to that t0 we find ' 0.25.\n√\nX = t0 + ns\u000f with E(\u000f) = 0 and E(k\u000fk2 ) = 1. According to proposition 4.4\nwe have by noting δ? the lower bound of the consistency bias when s → ∞:\np\n√\nδ?\n→ n(−1 + 1 + ν 2 ).\ns\nNow ν = E(supg∈G hg\u000f, t0 )i /kt0 k = E(k\u000fk) → 1 when n tends to infinity (expected value of the Chi distribution) we have that for n large enough:\n√ √\nδ?\n' n( 2 − 1).\ns→∞ s\nWe compare this result with the exact computation of the consistency bias\n(noted here CB) made by Miolane et al. [MHP16], which writes with our current\nnotations:\nCB √ Γ((n + 1)/2)\nlim\n= 2\n.\ns→∞ s\nΓ(n/2)\nlim\n\nUsing a standard Taylor expansion on the Gamma function, we have that for n\nlarge enough:\nCB √\nlim\n' n.\ns→∞ s\nAs a conclusion, when the dimension of the space is large enough our lower\nbound and the exact computation of the\n√ bias have the same asymptotic behaviour. It differs only by the constant 2 − 1 ' 0.4 in our lower bound, 1 in\nthe work of Miolane et al. [MP15].\n\n24\n\n\f5\n\nFréchet means top and quotient spaces are not\nconsistent when the template is a fixed point\n\nIn this Section, we do not assume that the top space M is a vector space, but\nrather a manifold. We need then to rewrite the generative model likewise: let\nt0 ∈ M , and X any random variable of M such as t0 is a Fréchet mean of X.\nThen Y = S · X is the observed variable where S is a random variable whose\nvalue are in G. In this Section we make the assumption that the template t0 is\na fixed point under the action of G.\n\n5.1\n\nResult\n\nLet X be a random variable on M and define the variance of X as:\nE(m) = E(dM (m, X)2 ).\nWe say that t0 is a Fréchet mean of X if t0 is a global minimiser of the variance\nE. We prove the following result:\nTheorem 5.1. Assume that M is a complete finite dimensional Riemannian\nmanifold and that dM is the geodesic distance on M . Let X be a random variable\non M , with E(d(x, X)2 ) < +∞ for some x ∈ M . We assume that t0 is a fixed\npoint and a Fréchet mean of X and that P(X ∈ C(t0 )) = 0 where C(t0 ) is the\ncut locus of t0 . Suppose that there exists a point in the support of X which is\nnot a fixed point nor in the cut locus of t0 . Then [t0 ] is not a Fréchet mean of\n[X].\nThe previous result is finite dimensional and does not cover interesting infinite dimensional setting concerning curves for instance. However, a simple\nextension to the previous result can be stated when M is a Hilbert vector space\nsince then the space is flat and some technical problems like the presence of cut\nlocus point do not occur.\nTheorem 5.2. Assume that M is a Hilbert space and that dM is given by the\nHilbert norm on M . Let X be a random variable on M , with E(kXk2 ) < +∞.\nWe assume that t0 = E(X). Suppose that there exists a point in the support of\nthe law of X that is not a fixed point for the action of G. Then [t0 ] is not a\nFréchet mean of [X].\nNote that the reciprocal is true: if all the points in the support of the law\nof X are fixed points, then almost surely, for all m ∈ M and for all g ∈ G we\nhave:\ndM (X, m) = dM (g · X, m) = dQ ([X], [m]).\nUp to the projection on the quotient, we have that the variance of X is equal to\nthe variance of [X] in M/G, therefore [t0 ] is a Fréchet mean of [X] if and only\nif t0 is a Fréchet mean of X. There is no inconsistency in that case.\n\n25\n\n\fExample 5.1. Theorem 5.2 covers the interesting case of the Fisher Rao metric\non functions:\nF = {f : [0, 1] → R\n\n|\n\nf is absolutely continuous}.\n\nThen considering for G the group of smooth diffeomorphisms γ on [0, 1] such\nthat γ(0) = 0 and γ(1) = 1, we have a right group action G × F → F given\nby γ · f = f ◦ γ. The Fisher Rao metric is built as a pull back metric\nq of the\n2\n2\n˙\nL ([0, 1], R) space through the map Q : F → L given by: Q(f ) = f / |f˙|. This\nsquare root trick is often used, see for instance [KSW11]. Note that in this case,\nRt\nQ is a bijective mapping with inverse given by q 7→ f with √\nf (t) = 0 q(s)|q(s)|ds.\nWe can define a group action on M = L2 as: γ · q = q ◦ γ γ̇, for which one can\ncheck easily by a change of variable that:\np\np\nkγ · q − γ · q 0 k2 = kq ◦ γ γ̇ − q 0 ◦ γ γ̇k2 = kq − q 0 k2 .\nSo up to the mapping Q, the Fisher Rao metric on curve corresponds to the\nsituation M where theorem 5.2 applies. Note that in this case the set of fixed\npoints under the action of G corresponds in the space F to constant functions.\nWe can also provide an computation of the consistency bias in this setting:\nProposition 5.1. Under the assumptions of theorem 5.2, we write X = t0 + σ\u000f\nwhere t0 is a fixed point, σ > 0, E(\u000f) = 0 and E(k\u000fk2 ) = 1, if there is a Fréchet\nmean of [X], then the consistency bias is linear with respect to σ and it is equal\nto:\nσ sup E(sup hv, g · \u000fi).\nkvk=1\n\ng∈G\n\nProof. For λ > 0 and kvk = 1, we compute the variance F in the quotient space\nof [X] at the point t0 + λv. Since t0 is a fixed point we get:\nF (t0 +λv) = E( inf kt0 +λv−gXk2 ) = E(kXk2 )−kt0 k2 −2λE(sup hv, g(X − t0 )i)+λ2 .\ng∈G\n\ng\n\nThen we minimise F with respect to λ, and after we minimise with respect to\nv (with kvk = 1). Which concludes.\n\n5.2\n5.2.1\n\nProofs of these theorems\nProof of theorem 5.1\n\nWe start with the following simple result, which aims to differentiate the variance\nof X. This classical result (see [Pen06] for instance) is proved in appendix B in\norder to be the more self-contained as possible:\nLemma 5.1. Let X a random variable on M such that E(d(x, X)2 ) < +∞ for\nsome x ∈ M . Then the variance m 7→ E(m) = E(dM (m, X)2 ) is a continuous\n\n26\n\n\ffunction which is differentiable at any point m ∈ M such that P(X ∈ C(m)) = 0\nwhere C(m) is the cut locus of m. Moreover at such point one has:\n∇E(m) = −2E(logm (X)),\nwhere logm : M \\ C(m) → Tm M is defined for any x ∈ M \\ C(m) as the unique\nu ∈ Tm M such that expm (u) = x and kukm = dM (x, m).\nWe are now ready to prove theorem 5.1.\nProof. (of theorem 5.1) Let m0 be a point in the support of M which is not a\nfixed point and not in the cut locus of t0 . Then there exists g0 ∈ G such that\nm1 = g0 m0 6= m0 . Note that since x 7→ g0 x is a symmetry (the distance is\nequivariant under the action of G) have that m1 = g0 m0 ∈\n/ C(g0 t0 ) = C(t0 ) (t0\nis a fixed point under the action of G). Let v0 = logt0 (m0 ) and v1 = logt0 (m1 ).\nWe have v0 6= v1 and since C(t0 ) is closed and the logt0 is continuous application\non M \\ C(t0 ) we have:\nlim\n\n\u000f→0 P(X\n\n1\nE(1X∈B(m0 ,\u000f) logt0 (X)) = v0 .\n∈ B(m0 , \u000f))\n\n(we use here the fact that since m0 is in the support of the law of X, P(X ∈\nB(m0 , \u000f)) > 0 for any \u000f > 0 so that the denominator does not vanish and the\nfact that since M is a complete manifold, it is a locally compact space (the\nclosed balls are compacts) and logt0 is locally bounded). Similarly:\nlim\n\n\u000f→0 P(X\n\n1\nE(1X∈B(m0 ,\u000f) logt0 (g0 X)) = v1 .\n∈ B(m0 , \u000f))\n\nThus for sufficiently small \u000f > 0 we have (since v0 6= v1 ):\nE(logt0 (X)1X∈B(m0 ,\u000f) ) 6= E(logt0 (g0 X)1X∈B(m0 ,\u000f) ).\n\n(33)\n\nBy using using a reductio ad absurdum, we suppose that [t0 ] is a Fréchet mean\nof [X] and we want to find a contradiction with (33). In order to do that we\nintroduce simple functions as the function x 7→ 1x∈B(m0 ,\u000f) which intervenes in\nEquation (33). Let s : M → G be a simple function (i.e. a measurable function\nwith finite number of values in G). Then x 7→ h(x) = s(x)x is a measurable\nfunction3 . Now, let Es (x) = E(d(x, s(X)X)2 ) be the variance of the variable\ns(X)X. Note that (and this is the main point):\n∀g ∈ G\n3 Indeed\n\nif: s =\n\ndM (t0 , x) = dM (gt0 , gx) = dM (t0 , gx) = dQ ([t0 ], [x]),\nn\nP\n\ngi 1Ai where (Ai )1≤i≤n is a partition of M (such that the sum is always\n\ni=1\n\ndefined). Then for any Borel set B ⊂ M we have: h−1 (B) =\n\nn\nS\n\ngi−1 (B) ∩ Ai is a measurable\n\ni=1\n\nset since x 7→ gi x is a measurable function.\n\n27\n\n\fwe have: Es (t0 ) = E(t0 ). Assume now that [t0 ] a Fréchet mean for [X] on the\nquotient space and let us show that Es has a global minimum at t0 . Indeed for\nany m, we have:\nEs (m) = E(dM (m, s(X)X)2 ) ≥ E(dQ ([m], [X])2 ) ≥ E(dQ ([t0 ], [X])2 ) = Es (t0 ).\nNow, we want to apply lemma 5.1 to the random variables s(X)X and X at the\npoint t0 . Since we assume that X ∈\n/ C(t0 ) almost surely and X ∈\n/ C(t0 ) implies\ns(X)X ∈\n/ C(t0 ) we get P(s(X)X ∈ C(t0 )) = 0 and the lemma 5.1 applies. As\nt0 is a minimum, we already know that the differential of Es (respectively E)\nat t0 should be zero. We get:\nE(logt0 (X)) = E(logt0 (s(X)X)) = 0.\n\n(34)\n\nNow we apply Equation (34) to a particular simple function defined by s(x) =\ng0 1x∈B(m0 ,\u000f) + eG 1x∈B(m\n. We split the two expected values in (34) into two\n/\n0 ,\u000f)\nparts:\nE(logt0 (X)1X∈B(m0 ,\u000f) ) + E(logt0 (X)1X ∈B(m\n) = 0,\n(35)\n/\n0 ,\u000f)\n) = 0.\nE(logt0 (g0 X)1X∈B(m0 ,\u000f) ) + E(logt0 (X)1X ∈B(m\n/\n0 ,\u000f)\n\n(36)\n\nBy substrating (35) from (36), one gets:\nE(logt0 (X)1X∈B(m0 ,\u000f) ) = E(logt0 (g0 X)1X∈B(m0 ,\u000f) ),\nwhich is a contradiction with (33). Which concludes.\n5.2.2\n\nProof of theorem 5.2\n\nProof. The extension to theorem 5.2 is quite straightforward. In this setting\nmany things are now explicit since d(x, y) = kx − yk , ∇x d(x, y)2 = 2(x − y),\nlogx (y) = y − x and the cut locus is always empty. It is then sufficient to go\nalong the previous proof and to change the quantity accordingly. Note that the\nlocal compactness of the space is not true in infinite dimension. However this\nwas only used to prove that the log was locally bounded but this last result is\ntrivial in this setting.\n\n6\n\nConclusion and discussion\n\nIn this article, we exhibit conditions which imply that the template estimation\nwith the Fréchet mean in quotient space is inconsistent. These conditions are\nrather generic. As a result, without any more information, a priori there is\ninconsistency. The behaviour of the consistency bias is summarized in table 1.\nSurely future works could improve these lower and upper bounds.\nIn a more general case: when we take an infinite-dimensional vector space\nquotiented by a non isometric group action, is there always an inconsistency?\nAn important example of such action is the action of diffeomorphisms. Can we\nestimate the consistency bias? In this setting, one estimates the template (or\n28\n\n\fTable 1: Behaviour of the consistency bias with respect to σ 2 the variability of\nX = t0 + σ\u000f. The constants Ki ’s depend on the kind of noise, on the template\nt0 and on the group action.\nConsistency bias : CB\nG is any group\nSupplementary properties for\nG a finite group\n√\nUpper bound of CB\nCB ≤ σ + 2 σ 2 + K1 σ CB ≤ K2 σ (theorem 3.3)\n(proposition 4.5)\nLower bound of CB for σ → ∞\nCB ≥ L ∼ K3 σ (proposition 4.4)\nσ→∞\nwhen the template is not a fixed\npoint\n√\nBehavior of CB for σ → 0 when CB ≤ U ∼ K4 σ\nCB = o(σ k ), ∀k ∈ N in the\nσ→0\n0\nthe template is not a fixed point\nsection 3.3, can we extend this\nresult for finite group?\nCB = σ sup E(supg∈G hv, g\u000fi) (proposition 5.1)\n\nCB when the template is a fixed\npoint\n\nkvk=1\n\nan atlas), but does not exactly compute the Fréchet mean in quotient space,\nbecause a regularization term is added. In this setting, can we ensure that the\nconsistency bias will be small enough to estimate the original template? Otherwise, one has to reconsider the template estimation with stochastic algorithms\nas in [AKT10] or develop new methods.\n\nA\n\nProof of theorems for finite groups’ setting\n\nA.1\n\nProof of theorem 3.2: differentiation of the variance\nin the quotient space\n\nIn order to show theorem 3.2 we proceed in three steps. First we see some\nfollowing properties and definitions which will be used. Most of these properties\nare the consequences of the fact that the group G is finite. Then we show that\nthe integrand of F is differentiable. Finally we show that we can permute\ngradient and integral signs.\n1. The set of singular points in Rn , is a null set (for the Lebesgue’s measure),\nsince it is equal to:\n[\nker(x 7→ g · x − x),\ng6=eG\n\na finite union of strict linear subspaces of Rn thanks to the linearity and\neffectively of the action and to the finite group.\n2. If m is regular, then for g, g 0 two different elements of G, we pose:\nH(g · m, g 0 · m) = {x ∈ Rn , kx − g · mk = kx − g 0 · mk}.\nMoreover H(g · m, g 0 · m) = (g · m − g 0 · m)⊥ is an hyperplane.\n29\n\n\f3. For m a regular point we define the set of points which are equally distant\nfrom two different points of the orbit of m:\n[\nH(g · m, g 0 · m).\nAm =\ng6=g 0\n\nThen Am is a null set. For m regular and x ∈\n/ Am the minimum in the\ndefinition of the quotient distance :\ndQ ([m], [x]) = minkm − g · xk,\ng∈G\n\n(37)\n\nis reached at a unique g ∈ G, we call g(x, m) this unique element.\n4. By expansion of the squared norm: g minimises km − g · xk if and only if\ng maximises hm, g · xi.\n5. If m is regular and x ∈\n/ Am then:\n∀g ∈ G \\ {g(x, m)}, km − g(x, m) · xk < km − g · xk,\nby continuity of the norm and by the fact that G is a finite group, we can\nfind α > 0, such that for µ ∈ B(m, α) and y ∈ B(x, α):\n∀g ∈ G \\ {g(x, m)} kµ − g(x, m) · yk < kµ − g · yk.\n\n(38)\n\nTherefore for such y and µ we have:\ng(x, m) = g(y, µ).\n6. For m a regular point, we define Cone(m) the convex cone of Rn :\nCone(m) = {x ∈ Rn / ∀g ∈ G kx − mk ≤ kx − g · mk}\n\n(39)\n\nn\n\n= {x ∈ R / ∀g ∈ G hm, xi ≥ hgm, xi}.\nThis is the intersection of |G| − 1 half-spaces: each half space is delimited\nby H(m, gm) for g 6= eG (see fig. 1). Cone(m) is the set of points whose\nprojection on [m] is m, (where the projection of one point p on [m] is one\npoint g · m which minimises the set {kp − g · mk, g ∈ G}).\n7. Taking a regular T\npoint m allows us to see the\nT quotient. For every point x ∈\nRn we have: [x] Cone(m) 6= ∅, card([x] Cone(m)) ≥ 2 if and only if\nx ∈ Am . The borders of the cone is Cone(m)\\Int(Cone(m)) = Cone(m)∩\nAm (we denote by Int(A) the interior of a part A). Therefore Q = Rn /G\ncan be seen like Cone(m) whose border have been glued together.\nThe proof of theorem 3.2 is the consequence of the following lemmas. The\nfirst lemma studies the differentiability of the integrand, and the second allows\nus to permute gradient and integral sign. Let us denote by f the integrand of\nF:\n30\n\n\f∀ m, x ∈ M\n\nf (x, m) = minkm − g · xk2 .\n\n(40)\n\ng∈G\n\nThus we have: F (m) = E(f (X, m)). The min of differentiable functions is not\nnecessarily differentiable, however we prove the following result:\nLemma A.1. Let m0 be a regular point, if x ∈\n/ Am0 then m 7→ f (x, m) is\ndifferentiable at m0 , besides we have:\n∂f\n(x, m0 ) = 2(m0 − g(x, m0 ) · x)\n∂m\n\n(41)\n\nProof. If m0 is regular and x ∈\n/ Am0 then we know from the item 5 of the\nappendix A.1 that g(x, m0 ) is locally constant. Therefore around m0 , we have:\nf (x, m) = km − g(x, m0 ) · xk2 ,\nwhich can differentiate with respect to m at m0 . This proves the lemma A.1.\nNow we want to prove that we can permute the integral and the gradient\nsign. The following lemma provides us a sufficient condition to permute integral\nand differentiation signs thanks to the dominated convergence theorem:\nLemma A.2. For every m0 ∈ M we have the existence of an integrable function\nΦ : M → R+ such that:\n∀m ∈ B(m0 , 1), ∀x ∈ M\n\n|f (x, m0 ) − f (x, m)| ≤ km − m0 kΦ(x).\n\n(42)\n\nProof. For all g ∈ G, m ∈ M we have:\nkg · x − m0 k2 − kg · x − mk2 = hm − m0 , 2g · x − (m0 + m)i\n≤ km − m0 k × (km0 + mk + k2xk)\n2\n\nminkg · x − m0 k ≤ km − m0 k (km0 + mk + k2xk) + kg · x − mk2\ng∈G\n\nminkg · x − m0 k2 ≤ km − m0 k (km0 + mk + k2xk) + minkg · x − mk2\ng∈G\n\n2\n\ng∈G\n\n2\n\nminkg · x − m0 k − minkg · x − mk ≤ km − m0 k (2km0 k + km − m0 k + k2xk)\ng∈G\n\ng∈G\n\nBy symmetry we get also the same control of f (x, m) − f (x, m0 ), then:\n|f (x, m0 ) − f (x, m)| ≤ km0 − mk (2km0 k + km − m0 k + k2xk)\n\n(43)\n\nThe function Φ should depend on x or m0 , but not on m. That is why we take\nonly m ∈ B(m0 , 1), then we replace km−m0 k by 1 in (43), which concludes.\n\n31\n\n\fA.2\n\nProof of theorem 3.1: the gradient is not zero at the\ntemplate\n\nTo prove it, we suppose that ∇F (t0 ) = 0, and we take the dot product with t0 :\nh∇F (t0 ), t0 i = 2E(hX, t0 i − hg(X, t0 ) · X, t0 i) = 0.\n\n(44)\n\nThe item 4 of (x, m) 7→ g(x, m) seen at appendix A.1 leads to:\nhX, t0 i − hg(X, t0 ) · X, t0 i ≤ 0 almost surely.\nSo the expected value of a non-positive random variable is null. Then\nhX, t0 i − hg(X, t0 ) · X, t0 i = 0 almost surely hX, t0 i = hg(X, t0 ) · X, t0 i almost surely.\nThen g = eG maximizes the dot product almost surely. Therefore (as we know\nthat g(X, t0 ) is unique almost surely, since t0 is regular):\ng(X, t0 ) = eG almost surely,\nwhich is a contradiction with Equation (6).\n\nA.3\n\nProof of theorem 3.3: upper bound of the consistency\nbias\n\nIn order to show this Theorem, we use the following lemma:\nLemma A.3. We write X = t0 +\u000f where E(\u000f) = 0 and we make the assumption\nthat the noise \u000f is a subgaussian random variable. This means that it exists c > 0\nsuch that:\n\u0013\n\u0012 2\ns kmk2\n.\n(45)\n∀m ∈ M = Rn , E(exp(h\u000f, mi)) ≤ c exp\n2\nIf for m ∈ M we have:\np\nρ̃ := dQ ([m], [t0 ]) ≥ s 2 log(c|G|),\n\n(46)\n\np\nρ̃2 − ρ̃s 8 log(c|G|) ≤ F (m) − E(k\u000fk2 ).\n\n(47)\n\nthen we have:\nProof. (of lemma A.3) First we expand the right member of the inequality (47):\n\u0012\n\u0013\nE(k\u000fk2 ) − F (m) = E max(kX − t0 k2 − kX − gmk2 )\ng∈G\n\nWe use the formula kAk2 − kA + Bk2 = −2 hA, Bi − kBk2 with A = X − t0 and\nB = t0 − gm:\n\u0014\n\u0015\n\u0001\nE(k\u000fk2 ) − F (m) = E max −2 hX − t0 , t0 − gmi − kt0 − gmk2 = E(max ηg ),\ng∈G\n\ng∈G\n\n(48)\n32\n\n\fwith ηg = −kt0 − gmk2 + 2 h\u000f, gm − t0 i. Our goal is to find a lower bound of\nF (m) − E(k\u000fk2 ), that is why we search an upper bound of E(maxηg ) with the\ng∈G\n\nJensen’s inequality. We take x > 0 and we get by using the assumption (45):\n\n\nX\nexp(xE(max ηg )) ≤ E(exp(max xηg )) ≤ E \nexp(xηg )\ng∈G\n\ng∈G\n\n≤\n\nX\n\ng∈G\n2\n\nexp(−xkt0 − gmk )E(exp(h\u000f, 2x(gm − t0 )i)\n\ng\n\nX\n≤c\nexp(−xkt0 − gmk2 ) exp(2s2 x2 kgm − t0 k2 )\ng\n\nX\n≤c\nexp(kgm − t0 k2 (−x + 2x2 s2 ))\n\n(49)\n\ng\n\nNow if (−x + 2t2 x2 ) < 0, we can take an upper bound of the sum sign in (49)\nby taking the smallest value in the sum sign, which is reached when g minimizes\nkg · m − t0 k multiplied by the number of elements summed. Moreover (−x +\n2x2 s) < 0 ⇐⇒ 0 < x < 2s12 . Then we have:\nexp(xE(max ηg )) ≤ c|G| exp(ρ̃2 (−x + 2x2 s2 )) as soon as 0 < x <\ng∈G\n\n1\n.\n2s2\n\nThen by taking the log:\nE(maxηg ) ≤\ng∈G\n\nlog c|G|\n+ (2xs2 − 1)ρ̃2 .\nx\n\n(50)\n\nNow we find the x which optimizes inequality (50).p By differentiation, the\nright member of inequality (50) is minimal for x? = log c|G|/2/(sρ̃) which is\na valid choice because x? ∈ (0, 2s12 ) by using the assumption (46). With the\nequations (48) and (50) and x? we get the result.\nProof. (of theorem 3.3) We take m? ∈ argmin F , ρ̃ = dQ ([m? ], [t0 ]), and \u000f =\n2\n2\nX − tp\n0 . We have: F (m? ) ≤ F (t0 ) ≤ E(k\u000fk ) then F (m? ) − E(k\u000fk ) ≤ 0. If\nρ̃ > s 2 log(|G|) then we can apply lemma A.3 with c = 1. Thus:\np\nρ̃2 − ρ̃s 8 log(|G|) ≤ 2F (m? ) − E(k\u000fk2 ) ≤ 0,\np\np\nwhich yields to ρ̃ ≤ s 8 log(|G|). If ρ̃ ≤ s 2 log(|G|), we have nothing to\nprove.\nNote that the proof of this upper bound does not use the fact that the action\nis isometric, therefore this upper bound is true for every finite group action.\n\n33\n\n\fA.4\n\nProof of proposition 3.2: inconsistency in R2 for the\naction of translation\n\nProof. We suppose that E(X) ∈ HPA ∪ L. In this setting we call τ (x, m) one\nof element of the group G = T which minimises kτ · x − mk see (37) instead of\ng(x, m). The variance in the quotient space at the point m is:\n\u0012\n\u0013\nF (m) = E min kτ · X − mk2 = E(kτ (X, m) · X − mk2 ).\nτ ∈Z/2Z\n\nAs we want to minimize F and F (1 · m) = F (m), we can suppose that m ∈\nHPA ∪ L. We can completely write what take τ (x, m) for x ∈ M :\n• If x ∈ HPA ∪ L we can set τ (x, m) = 0 (because in this case x, m are on\nthe same half plane delimited by L the perpendicular bisector of m and\n−m).\n• If x ∈ HPB then we can set τ (x, m) = 1 (because in this case x, m are\nnot on the same half plane delimited by L the perpendicular bisector of\nm and −m).\nThis allows use to write the variance at the point m ∈ HPA :\n\u0001\u0001\n\u0001\nF (m) = E kX − mk2 1{X∈HPA ∪L} + E k1 · X − mk2 1{X∈HPB }\nThen we define the random variable Z by: Z = X1X∈HPA ∪L + 1 · X1X∈HPB ,\nsuch that for m ∈ HPA we have: F (m) = E(kZ − mk2 ) and F (m) = F (1 · m).\nThus if m? is a global minimiser of F , then m? = E(Z) or m? = 1 · E(Z). So the\nFréchet mean of [X] is [E(Z)]. Here instead of using theorem 3.1, we can work\nexplicitly: Indeed there is no inconsistency if and only if E(Z) = E(X), (E(Z) =\n1 · E(X) would be another possibility, but by assumption E(Z), E(X) ∈ HPA ),\nby writing X = X1X∈HPA + X1X∈HPB ∪L , we have:\nE(Z) = E(X) ⇐⇒ E(1 · X1X∈HPB ∪L ) = E(X1X∈HPB ∪L )\n⇐⇒ 1 · E(X1X∈HPB ∪L ) = E(X1X∈HPB ∪L )\n⇐⇒ E(X1X∈HPB ∪L ) ∈ L\n⇐⇒ P(X ∈ HPB ) = 0,\nTherefore there is an inconsistency if and only if P(X ∈ HPB ) > 0 (we remind\nthat we made the assumption that E(X) ∈ HPA ∪ L). If E(X) is regular (i.e.\nE(X) ∈\n/ L), then there is an inconsistency if and only if X takes values in HPB ,\n(this is exactly the condition of theorem 3.1, but in this particular case, this is\na necessarily and sufficient condition). This proves point 1. Now we make the\nassumption that X follows a Gaussian noise in order compute E(Z) (note that\nwe could take another noise, as long as we are able to compute E(Z)). For that\nwe convert to polar coordinates: (u, v)T = E(X) + (r cos θ, r sin θ)T where r > 0\net θ ∈ [0, 2π]. We also define: d = dist(E(X), L), E(X) is a regular point if\n\n34\n\n\fand only if d > 0. We still suppose that E(X) = (α, β)T ∈ HPA ∪ L. First we\nparametrise in function of (r, θ) the points which are in HPB :\nv < u ⇐⇒ β + r sin θ < α + r cos θ ⇐⇒\n\nβ−α √\nπ\n< 2 cos(θ + )\nr\n4\n\nd\nπ\n< cos(θ + )\nr h\n4\ni\nπ\nπ\n⇐⇒ θ ∈ − − arccos(d/r), − + arccos(d/r) and d < r\n4\n4\n⇐⇒\n\nThen we compute E(Z):\nE(Z) =E(X1X∈HPA ) + E(1 · X1X∈HPB )\n\u0010\n\u0011\n\u0013 exp − r2\nZ d Z 2π \u0012\n2s2\nα + r cos θ\nrdθdr\nE(Z) =\n2\nβ\n+\nr\nsin\nθ\n2πs\n0\n0\n\u0010\n\u0011\n\u0013 exp − r2\nZ +∞ Z 2π− π4 −arccos( dr ) \u0012\n2\n2s\nα + r cos θ\nrdrdθ\n+\n2\nβ\n+\nr\nsin\nθ\nd\nπ\n2πs\narccos( r )− 4\nd\n\u0010 2\u0011\nr\n\u0013\nZ +∞ Z − π4 +arccos( dr ) \u0012\nβ + r sin θ exp − 2s2\n+\nrdrdθ\nα + r cos θ\nd\n2πs2\nd\n−π\n4 −arccos( r )\n\u0012 \u0013\nZ +∞ 2\nr2 √\nr exp(− 2s\nd\n2)\n=E(X) +\n2g\ndr × (−1, 1)T ,\n2\nπs\nr\nd\nWe compute ρ̃ = dQ ([E(X)], [E(Z)]) where dQ is the distance in the quotient\nspace defined in (1). As we know that E(X), E(Z) are in the same half-plane\ndelimited by L, we have: ρ̃ = dQ ([E(Z)], [E(X)]) = kE(Z) − E(X)k. This proves\neq. (9), note that items 2a to 2c are the direct consequence of eq. (9) and basic\nanalysis.\n\nB\n\nProof of lemma 5.1: differentiation of the variance in the top space\n\nProof. By triangle inequality it is easy to show that E is finite and continuous\neverywhere. Moreover, it is a well known fact that x 7→ dM (x, z)2 is differentiable at any m ∈ M \\ C(z) (i.e. z ∈\n/ C(m)) with derivative −2 logm (z). Now\nsince:\n|dM (x, z)2 − dM (y, z)2 | = |dM (x, z) − dM (y, z)kdM (x, z) + dM (y, z)|\n≤ dM (x, y)(2dM (x, z) + dM (y, x)),\nwe get in a local chart φ : U → V ⊂ Rn at t = φ(m) we have locally around t\nthat:\nh 7→ dM (φ−1 (t), φ−1 (t + h)),\n35\n\n\fis smooth and |dM (φ−1 (t), φ−1 (t+h))| ≤ C|h| for a C > 0. Hence for sufficiently\nsmall h, |dM (φ−1 (t), z)2 − dM (φ−1 (t + h), z)2 | ≤ C|h|(2dM (m, z) + 1). We get\nthe result from dominated convergence Lebesgue theorem with E(dM (m, X)) ≤\nE(dM (m, X)2 + 1) < +∞.\n\nReferences\n[AAT07]\n\nStéphanie Allassonnière, Yali Amit, and Alain Trouvé. Towards a\ncoherent statistical framework for dense deformable template estimation. Journal of the Royal Statistical Society: Series B (Statistical\nMethodology), 69(1):3–29, 2007.\n\n[ADP15]\n\nStéphanie Allassonnière, Loïc Devilliers, and Xavier Pennec. Estimating the template in the total space with the fréchet mean on\nquotient spaces may have a bias: a case study on vector spaces quotiented by the group of translations. In Mathematical Foundations\nof Computational Anatomy (MFCA’15), 2015.\n\n[AKT10]\n\nStéphanie Allassonnière, Estelle Kuhn, and Alain Trouvé. Construction of bayesian deformable models via a stochastic approximation\nalgorithm: a convergence study. Bernoulli, 16(3):641–678, 2010.\n\n[BB08]\n\nAbhishek Bhattacharya and Rabi Bhattacharya. Statistics on riemannian manifolds: asymptotic distribution and curvature. Proceedings of the American Mathematical Society, 136(8):2959–2967,\n2008.\n\n[BC11]\n\nJérémie Bigot and Benjamin Charlier. On the consistency of fréchet\nmeans in deformable models for curve and image analysis. Electronic\nJournal of Statistics, 5:1054–1089, 2011.\n\n[BG14]\n\nDominique Bontemps and Sébastien Gadat. Bayesian methods\nfor the shape invariant model. Electronic Journal of Statistics,\n8(1):1522–1568, 2014.\n\n[CWS16]\n\nJason Cleveland, Wei Wu, and Anuj Srivastava. Norm-preserving\nconstraint in the fisher–rao registration and its application in signal estimation. Journal of Nonparametric Statistics, 28(2):338–359,\n2016.\n\n[DPC+ 14] Stanley Durrleman, Marcel Prastawa, Nicolas Charon, Julie R Korenberg, Sarang Joshi, Guido Gerig, and Alain Trouvé. Morphometry of anatomical shape complexes with dense deformations and\nsparse parameters. NeuroImage, 101:35–49, 2014.\n[Fré48]\n\nMaurice Fréchet. Les elements aléatoires de nature quelconque dans\nun espace distancié. In Annales de l’institut Henri Poincaré, volume 10, pages 215–310, 1948.\n36\n\n\f[GM98]\n\nUlf Grenander and Michael I. Miller. Computational anatomy: An\nemerging discipline. Q. Appl. Math., LVI(4):617–694, December\n1998.\n\n[HCG+ 13] Sebastian Hitziger, Maureen Clerc, Alexandre Gramfort, Sandrine\nSaillet, Christian Bénar, and Théodore Papadopoulo. Jitter-adaptive\ndictionary learning-application to multi-trial neuroelectric signals.\narXiv preprint arXiv:1301.3611, 2013.\n[JDJG04] Sarang Joshi, Brad Davis, Mathieu Jomier, and Guido Gerig. Unbiased diffeomorphic atlas construction for computational anatomy.\nNeuroimage, 23:S151–S160, 2004.\n[Kar77]\n\nHermann Karcher. Riemannian center of mass and mollifier smoothing. Communications on pure and applied mathematics, 30(5):509–\n541, 1977.\n\n[Ken89]\n\nDavid G Kendall. A survey of the statistical theory of shape. Statistical Science, pages 87–99, 1989.\n\n[Ken90]\n\nWilfrid S Kendall. Probability, convexity, and harmonic maps with\nsmall image i: uniqueness and fine existence. Proceedings of the\nLondon Mathematical Society, 3(2):371–406, 1990.\n\n[KSW11]\n\nSebastian A. Kurtek, Anuj Srivastava, and Wei Wu. Signal estimation under random time-warpings and nonlinear signal alignment. In J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and\nK.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 675–683. Curran Associates, Inc., 2011.\n\n[LAJ+ 12] Sidonie Lefebvre, Stéphanie Allassonnière, Jérémie Jakubowicz,\nThomas Lasne, and Eric Moulines. Aircraft classification with a\nlow resolution infrared sensor. Machine Vision and Applications,\n24(1):175–186, 2012.\n[MHP16]\n\nNina Miolane, Susan Holmes, and Xavier Pennec. Template\nshape estimation: correcting an asymptotic bias. arXiv preprint\narXiv:1610.01502, 2016.\n\n[MP15]\n\nNina Miolane and Xavier Pennec. Biased estimators on quotient\nspaces. In Geometric Science of Information. Second International\nConference, GSI 2015, Palaiseau, France, October 28-30, 2015, Proceedings, volume 9389. Springer, 2015.\n\n[Pen06]\n\nXavier Pennec. Intrinsic statistics on riemannian manifolds: Basic\ntools for geometric measurements. Journal of Mathematical Imaging\nand Vision, 25(1):127–154, 2006.\n\n37\n\n\f[SBG08]\n\nMert Sabuncu, Serdar K. Balci, and Polina Golland. Discovering\nmodes of an image population through mixture modeling. Proceeding\nof the MICCAI conference, LNCS(5242):381–389, 2008.\n\n[Zie77]\n\nHerbert Ziezold. On expected figures and a strong law of large numbers for random elements in quasi-metric spaces. In Transactions of\nthe Seventh Prague Conference on Information Theory, Statistical\nDecision Functions, Random Processes and of the 1974 European\nMeeting of Statisticians, pages 591–602. Springer, 1977.\n\n[ZSF13]\n\nMiaomiao Zhang, Nikhil Singh, and P.Thomas Fletcher. Bayesian\nestimation of regularization and atlas building in diffeomorphic image registration. In JamesC. Gee, Sarang Joshi, KilianM. Pohl,\nWilliamM. Wells, and Lilla Zöllei, editors, Information Processing\nin Medical Imaging, volume 7917 of Lecture Notes in Computer Science, pages 37–48. Springer Berlin Heidelberg, 2013.\n\n38\n\n\f&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:10,&quot;string&quot;:&quot;10&quot;}}},{&quot;rowIdx&quot;:1,&quot;cells&quot;:{&quot;text&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for\nReal-time Embedded Object Detection\n\narXiv:1802.06488v1 [] 19 Feb 2018\n\nAlexander Wong, Mohammad Javad Shafiee, Francis Li, Brendan Chwyl\nDept. of Systems Design Engineering\nUniversity of Waterloo, DarwinAI\n{a28wong, mjshafiee}@uwaterloo.ca, {francis, brendan}@darwinai.ca\n\nAbstract—Object detection is a major challenge in computer\nvision, involving both object classification and object localization within a scene. While deep neural networks have been\nshown in recent years to yield very powerful techniques for\ntackling the challenge of object detection, one of the biggest\nchallenges with enabling such object detection networks for\nwidespread deployment on embedded devices is high computational and memory requirements. Recently, there has been\nan increasing focus in exploring small deep neural network\narchitectures for object detection that are more suitable for embedded devices, such as Tiny YOLO and SqueezeDet. Inspired\nby the efficiency of the Fire microarchitecture introduced in\nSqueezeNet and the object detection performance of the singleshot detection macroarchitecture introduced in SSD, this paper\nintroduces Tiny SSD, a single-shot detection deep convolutional\nneural network for real-time embedded object detection that\nis composed of a highly optimized, non-uniform Fire subnetwork stack and a non-uniform sub-network stack of highly\noptimized SSD-based auxiliary convolutional feature layers\ndesigned specifically to minimize model size while maintaining\nobject detection performance. The resulting Tiny SSD possess\na model size of 2.3MB (∼26X smaller than Tiny YOLO) while\nstill achieving an mAP of 61.3% on VOC 2007 (∼4.2% higher\nthan Tiny YOLO). These experimental results show that very\nsmall deep neural network architectures can be designed for\nreal-time object detection that are well-suited for embedded\nscenarios.\nKeywords-object detection; deep neural network; embedded;\nreal-time; single-shot\n\nI. I NTRODUCTION\nObject detection can be considered a major challenge in\ncomputer vision, as it involves a combination of object classification and object localization within a scene (see Figure 1).\nThe advent of modern advances in deep learning [7], [6]\nhas led to significant advances in object detection, with the\nmajority of research focuses on designing increasingly more\ncomplex object detection networks for improved accuracy\nsuch as SSD [9], R-CNN [1], Mask R-CNN [2], and other\nextended variants of these networks [4], [8], [15]. Despite the\nfact that such object detection networks have showed stateof-the-art object detection accuracies beyond what can be\nachieved by previous state-of-the-art methods, such networks\nare often intractable for use for embedded applications due\nto computational and memory constraints. In fact, even faster\nvariants of these networks such as Faster R-CNN [13] are only\n\nFigure 1. Tiny SSD results on the VOC test set. The bounding boxes,\ncategories, and confidences are shown.\n\ncapable of single-digit frame rates on a high-end graphics\nprocessing unit (GPU). As such, more efficient deep neural\nnetworks for real-time embedded object detection is highly\ndesired given the large number of operational scenarios that\nsuch networks would enable, ranging from smartphones to\naerial drones.\nRecently, there has been an increasing focus in exploring\nsmall deep neural network architectures for object detection\nthat are more suitable for embedded devices. For example,\nRedmon et al. introduced YOLO [11] and YOLOv2 [12],\nwhich were designed with speed in mind and was able to\nachieve real-time object detection performance on a high-end\nNvidia Titan X desktop GPU. However, the model size of\nYOLO and YOLOv2 remains very large in size (753 MB and\n193 MB, respectively), making them too large from a memory\nperspective for most embedded devices. Furthermore, their\nobject detection speed drops considerably when running on\nembedded chips [14]. To address this issue, Tiny YOLO [10]\nwas introduced where the network architecture was reduced\nconsiderably to greatly reduce model size (60.5 MB) as well\nas greatly reduce the number of floating point operations\nrequired (just 6.97 billion operations) at a cost of object\ndetection accuracy (57.1% on the twenty-category VOC 2017\ntest set). Similarly, Wu et al. introduced SqueezeDet [16], a\nfully convolutional neural network that leveraged the efficient\nFire microarchitecture introduced in SqueezeNet [5] within\nan end-to-end object detection network architecture. Given\nthat the Fire microarchitecture is highly efficient, the resulting\nSqueezeDet had a reduced model size specifically for the\n\n\fpurpose of autonomous driving. However, SqueezeDet has\nonly been demonstrated for objection detection with limited\nobject categories (only three) and thus its ability to handle\nlarger number of categories have not been demonstrated.\nAs such, the design of highly efficient deep neural network\narchitectures that are well-suited for real-time embedded\nobject detection while achieving improved object detection\naccuracy on a variety of object categories is still a challenge\nworth tackling.\nIn an effort to achieve a fine balance between object\ndetection accuracy and real-time embedded requirements\n(i.e., small model size and real-time embedded inference\nspeed), we take inspiration by both the incredible efficiency\nof the Fire microarchitecture introduced in SqueezeNet [5]\nand the powerful object detection performance demonstrated\nby the single-shot detection macroarchitecture introduced\nin SSD [9]. The resulting network architecture achieved\nin this paper is Tiny SSD, a single-shot detection deep\nconvolutional neural network designed specifically for realtime embedded object detection. Tiny SSD is composed\nof a non-uniform highly optimized Fire sub-network stack,\nwhich feeds into a non-uniform sub-network stack of highly\noptimized SSD-based auxiliary convolutional feature layers,\ndesigned specifically to minimize model size while retaining\nobject detection performance.\nThis paper is organized as follows. Section 2 describes the\nhighly optimized Fire sub-network stack leveraged in the Tiny\nSSD network architecture. Section 3 describes the highly\noptimized sub-network stack of SSD-based convolutional\nfeature layers used in the Tiny SSD network architecture.\nSection 4 presents experimental results that evaluate the\nefficacy of Tiny SSD for real-time embedded object detection.\nFinally, conclusions are drawn in Section 5.\nII. O PTIMIZED F IRE S UB - NETWORK S TACK\nThe overall network architecture of the Tiny SSD network\nfor real-time embedded object detection is composed of two\nmain sub-network stacks: i) a non-uniform Fire sub-network\nstack, and ii) a non-uniform sub-network stack of highly\noptimized SSD-based auxiliary convolutional feature layers,\nwith the first sub-network stack feeding into the second subnetwork stack. In this section, let us first discuss in detail\nthe design philosophy behind the first sub-network stack\nof the Tiny SSD network architecture: the optimized fire\nsub-network stack.\nA powerful approach to designing smaller deep neural\nnetwork architectures for embedded inference is to take a\nmore principled approach and leverage architectural design\nstrategies to achieve more efficient deep neural network\nmicroarchitectures [3], [5]. A very illustrative example of\nsuch a principled approach is the SqueezeNet [5] network architecture, where three key design strategies were leveraged:\n1) reduce the number of 3 × 3 filters as much as possible,\n\nFigure 2. An illustration of the Fire microarchitecture. The output of\nprevious layer is squeezed by a squeeze convolutional layer of 1 × 1 filters,\nwhich reduces the number of input channels to 3 × 3 filters. The result of\nthe squeeze convolutional layers is passed into the expand convolutional\nlayer which consists of both 1 × 1 and 3 × 3 filters.\n\n2) reduce the number of input channels to 3 × 3 filters\nwhere possible, and\n3) perform downsampling at a later stage in the network.\nThis principled designed strategy led to the design of what\nthe authors referred to as the Fire module, which consists of\na squeeze convolutional layer of 1x1 filters (which realizes\nthe second design strategy of effectively reduces the number\nof input channels to 3 × 3 filters) that feeds into an expand\nconvolutional layer comprised of both 1 × 1 filters and 3 × 3\nfilters (which realizes the first design strategy of effectively\nreducing the number of 3 × 3 filters). An illustration of the\nFire microarchitecture is shown in Figure 2.\nInspired by the elegance and simplicity of the Fire\nmicroarchitecture design, we design the first sub-network\nstack of the Tiny SSD network architecture as a standard\nconvolutional layer followed by a set of highly optimized\nFire modules. One of the key challenges to designing this\nsub-network stack is to determine the ideal number of Fire\nmodules as well as the ideal microarchitecture of each of\nthe Fire modules to achieve a fine balance between object\ndetection performance and model size as well as inference\nspeed. First, it was determined empirically that 10 Fire\nmodules in the optimized Fire sub-network stack provided\nstrong object detection performance. In terms of the ideal\nmicroarchitecture, the key design parameters of the Fire\nmicroarchitecture are the number of filters of each size\n(1 × 1 or 3 × 3) that form this microarchitecture. In the\nSqueezeNet network architecture that first introduced the\nFire microarchitecture [5], the microarchitectures of the Fire\nmodules are largely uniform, with many of the modules\nsharing the same microarchitecture configuration. In an effort\nto achieve more optimized Fire microarchitectures on a permodule basis, the number of filters of each size in each Fire\n\n\fTable I\nT HE OPTIMIZED F IRE SUB - NETWORK STACK OF THE T INY SSD\nNETWORK ARCHITECTURE . T HE NUMBER OF FILTERS AND INPUT SIZE TO\nEACH LAYER ARE REPORTED FOR THE CONVOLUTIONAL LAYERS AND\n\nF IRE MODULES . E ACH F IRE MODULE IS REPORTED IN ONE ROW FOR A\nBETTER REPRESENTATION . ”x@S – y@E1 – z@E3\&quot; STANDS FOR x\nNUMBERS OF 1 × 1 FILTERS IN THE SQUEEZE CONVOLUTIONAL LAYER , y\nNUMBERS OF 1 × 1 FILTERS AND z NUMBERS OF 3 × 3 FILTERS IN THE\nEXPAND CONVOLUTIONAL LAYER .\n\nType / Stride\nConv1 / s2\nPool1 / s2\nFire1\nFire2\n\nFigure 3.\nAn illustration of the network architecture of the second\nsub-network stack of Tiny SSD. The output of three Fire modules and\ntwo auxiliary convolutional feature layers, all with highly optimized\nmicroarchitecture configurations, are combined together for object detection.\n\nmodule is optimized to have as few parameters as possible\nwhile still maintaining the overall object detection accuracy.\nAs a result, the optimized Fire sub-network stack in the Tiny\nSSD network architecture is highly non-uniform in nature for\nan optimal sub-network architecture configuration. Table I\nshows the overall architecture of the highly optimized Fire\nsub-network stack in Tiny SSD, and the number of parameters\nin each layer of the sub-network stack.\nIII. O PTIMIZED S UB - NETWORK S TACK OF SSD- BASED\nC ONVOLUTIONAL F EATURE L AYERS\nIn this section, let us first discuss in detail the design\nphilosophy behind the second sub-network stack of the Tiny\nSSD network architecture: the sub-network stack of highly\noptimized SSD-based auxiliary convolutional feature layers.\nOne of the most widely-used and effective object detection\nnetwork macroarchitectures in recent years has been the\nsingle-shot multibox detection (SSD) macroarchitecture [9].\nThe SSD macroarchitecture augments a base feature extraction network architecture with a set of auxiliary convolutional\nfeature layers and convolutional predictors. The auxiliary\nconvolutional feature layers are designed such that they\ndecrease in size in a progressive manner, thus enabling the\nflexibility of detecting objects within a scene across different\nscales. Each of the auxiliary convolutional feature layers\ncan then be leveraged to obtain either: i) a confidence score\nfor a object category, or ii) a shape offset relative to default\nbounding box coordinates [9]. As a result, a number of object\ndetections can be obtained per object category in this manner\nin a powerful, end-to-end single-shot manner.\nInspired by the powerful object detection performance\nand multi-scale flexibility of the SSD macroarchitecture [9],\nthe second sub-network stack of Tiny SSD is comprised of\na set of auxiliary convolutional feature layers and convo-\n\nPool3 / s2\nFire3\nFire4\nPool5 / s2\nFire5\nFire6\nFire7\nFire8\nPool9 / s2\nFire 9\nPool10 / s2\nFire10\n\nFilter Shapes\n3 × 3 × 57\n3×3\n15@S – 49@E1 – 53@E3\nConcat1\n15@S – 54@E1 – 52@E3\nConcat2\n3×3\n29@S – 92@E1 – 94@E3\nConcat3\n29@S – 90@E1 – 83@E3\nConcat4\n3×3\n44@S – 166@E1 – 161@E3\nConcat5\n45@S – 155@E1 – 146@E3\nConcat6\n49@S – 163@E1 – 171@E3\nConcat7\n25@S – 29@E1 – 54@E3\nConcat8\n3×3\n37@S – 45@E1 – 56@E3\nConcat9\n3×3\n38@S – 41@E1 – 44@E3\nConcat10\n\nInput Size\n300 × 300\n149 × 149\n74 × 74\n74 × 74\n74 × 74\n37 × 37\n37 × 37\n37 × 37\n18 × 18\n18 × 18\n18 × 18\n18 × 18\n18 × 18\n9×9\n\n4×4\n\nlutional predictors with highly optimized microarchitecture\nconfigurations (see Figure 3).\nAs with the Fire microarchitecture, a key challenge to\ndesigning this sub-network stack is to determine the ideal\nmicroarchitecture of each of the auxiliary convolutional\nfeature layers and convolutional predictors to achieve a fine\nbalance between object detection performance and model\nsize as well as inference speed. The key design parameters\nof the auxiliary convolutional feature layer microarchitecture\nare the number of filters that form this microarchitecture.\nAs such, similar to the strategy taken for constructing\nthe highly optimized Fire sub-network stack, the number\nof filters in each auxiliary convolutional feature layer is\noptimized to minimize the number of parameters while\npreserving overall object detection accuracy of the full Tiny\nSSD network. As a result, the optimized sub-network stack\nof auxiliary convolutional feature layers in the Tiny SSD\nnetwork architecture is highly non-uniform in nature for\nan optimal sub-network architecture configuration. Table II\nshows the overall architecture of the optimized sub-network\nstack of the auxiliary convolutional feature layers within the\nTiny SSD network architecture, along with the number of\n\n\fTable II\nT HE OPTIMIZED SUB - NETWORK STACK OF THE AUXILIARY\nCONVOLUTIONAL FEATURE LAYERS WITHIN THE T INY SSD NETWORK\nARCHITECTURE . T HE INPUT SIZES TO EACH CONVOLUTIONAL LAYER\nAND KERNEL SIZES ARE REPORTED .\n\nType / Stride\nConv12-1 / s2\nConv12-2\nConv13-1\nConv13-2\nFire5-mbox-loc\nFire5-mbox-conf\nFire9-mbox-loc\nFire9-mbox-conf\nFire10-mbox-loc\nFire10-mbox-conf\nFire11-mbox-loc\nFire11-mbox-conf\nConv12-2-mbox-loc\nConv12-2-mbox-conf\nConv13-2-mbox-loc\nConv13-2-mbox-conf\n\nFilter Shape\n3 × 3 × 51\n3 × 3 × 46\n3 × 3 × 55\n3 × 3 × 85\n3 × 3 × 16\n3 × 3 × 84\n3 × 3 × 24\n3 × 3 × 126\n3 × 3 × 24\n3 × 3 × 126\n3 × 3 × 24\n3 × 3 × 126\n3 × 3 × 24\n3 × 3 × 126\n3 × 3 × 16\n3 × 3 × 84\n\nInput Size\n4×4\n4×4\n2×2\n2×2\n37 × 37\n37 × 37\n18 × 18\n18 × 18\n9×9\n9×9\n4×4\n4×4\n2×2\n2×2\n1×1\n1×1\n\nparameters in each layer.\n\nModel\nsize\n60.5MB\n2.3MB\n\nmAP\n(VOC 2007)\n57.1%\n61.3%\n\nTable IV\nR ESOURCE USAGE OF T INY SSD.\n\nModel\nName\nTiny SSD\n\nV. E XPERIMENTAL R ESULTS AND D ISCUSSION\nTo study the utility of Tiny SSD for real-time embedded object detection, we examine the model size, object\ndetection accuracies, and computational operations on the\nVOC2007/2012 datasets. For evaluation purposes, the Tiny\nYOLO network [10] was used as a baseline reference comparison given its popularity for embedded object detection,\nand was also demonstrated to possess one of the smallest\nmodel sizes in literature for object detection on the VOC\n2007/2012 datasets (only 60.5MB in size and requiring\njust 6.97 billion operations). The VOC2007/2012 datasets\nconsist of natural images that have been annotated with 20\ndifferent types of objects, with illustrative examples shown\nin Figure 4. The tested deep neural networks were trained\nusing the VOC2007/2012 training datasets, and the mean\naverage precision (mAP) was computed on the VOC2007\ntest dataset to evaluate the object detection accuracy of the\ndeep neural networks.\nA. Training Setup\n\nTable III\nO BJECT DETECTION ACCURACY RESULTS OF T INY SSD ON VOC 2007\nTEST SET. T INY YOLO RESULTS ARE PROVIDED AS A BASELINE\nCOMPARISON .\n\nModel\nName\nTiny YOLO [10]\nTiny SSD\n\nreductions while having a negligible effect on object detection\naccuracy.\n\nTotal number\nof Parameters\n1.13M\n\nTotal number\nof MACs\n571.09M\n\nIV. PARAMETER P RECISION O PTIMIZATION\nIn this section, let us discuss the parameter precision\noptimization strategy for Tiny SSD. For embedded scenarios\nwhere the computational requirements and memory requirements are more strict, an effective strategy for reducing\ncomputational and memory footprint of deep neural networks\nis reducing the data precision of parameters in a deep neural\nnetwork. In particular, modern CPUs and GPUs have moved\ntowards accelerated mixed precision operations as well as\nbetter handling of reduced parameter precision, and thus the\nability to take advantage of these factors can yield noticeable\nimprovements for embedded scenarios. For Tiny SSD, the\nparameters are represented in half precision floating-point,\nthus leading to further deep neural network model size\n\nThe proposed Tiny SSD network was trained for 220,000\niterations in the Caffe framework with training batch size of\n24. RMSProp was utilized as the training policy with base\nlearning rate set to 0.00001 and γ = 0.5.\nB. Discussion\nTable III shows the model size and the object detection\naccuracy of the proposed Tiny SSD network on the VOC\n2007 test dataset, along with the model size and the object\ndetection accuracy of Tiny YOLO. A number of interesting\nobservations can be made. First, the resulting Tiny SSD\npossesses a model size of 2.3MB, which is ∼26X smaller\nthan Tiny YOLO. The significantly smaller model size of\nTiny SSD compared to Tiny YOLO illustrates its efficacy\nfor greatly reducing the memory requirements for leveraging\nTiny SSD for real-time embedded object detection purposes.\nSecond, it can be observed that the resulting Tiny SSD\nwas still able to achieve an mAP of 61.3% on the VOC\n2007 test dataset, which is ∼4.2% higher than that achieved\nusing Tiny YOLO. Figure 5 demonstrates several example\nobject detection results produced by the proposed Tiny SSD\ncompared to Tiny YOLO. It can be observed that Tiny SSD\nhas comparable object detection results as Tiny YOLO in\nsome cases, while in some cases outperforms Tiny YOLO in\nassigning more accurate category labels to detected objects.\nFor example, in the first image case, Tiny SSD is able to\ndetect the chair in the scene, while Tiny YOLO misses the\nchair. In the third image case, Tiny SSD is able to identify\nthe dog in the scene while Tiny YOLO detects two bounding\nboxes around the dog, with one of the bounding boxes\nincorrectly labeling it as cat. This significant improvement\n\n\fFigure 4.\n\nExample images from the Pascal VOC dataset. The ground-truth bounding boxes and object categories are shown for each image.\n\nin object detection accuracy when compared to Tiny YOLO\nillustrates the efficacy of Tiny SSD for providing more\nreliable embedded object detection performance. Furthermore,\nas seen in Table IV, Tiny SSD requires just 571.09 million\nMAC operations to perform inference, making it well-suited\nfor real-time embedded object detection. These experimental\nresults show that very small deep neural network architectures\ncan be designed for real-time object detection that are wellsuited for embedded scenarios.\nVI. C ONCLUSIONS\nIn this paper, a single-shot detection deep convolutional\nneural network called Tiny SSD is introduced for real-time\nembedded object detection. Composed of a highly optimized,\nnon-uniform Fire sub-network stack and a non-uniform subnetwork stack of highly optimized SSD-based auxiliary\nconvolutional feature layers designed specifically to minimize\nmodel size while maintaining object detection performance,\nTiny SSD possesses a model size that is ∼26X smaller than\nTiny YOLO, requires just 571.09 million MAC operations,\nwhile still achieving an mAP of that is ∼4.2% higher than\nTiny YOLO on the VOC 2007 test dataset. These results\ndemonstrates the efficacy of designing very small deep neural\nnetwork architectures such as Tiny SSD for real-time object\ndetection in embedded scenarios.\nACKNOWLEDGMENT\nThe authors thank Natural Sciences and Engineering Research Council of Canada, Canada Research Chairs Program,\nDarwinAI, and Nvidia for hardware support.\nR EFERENCES\n[1] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n580–587, 2014.\n[2] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask r-cnn.\nICCV, 2017.\n[3] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017.\n\n[4] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu,\nAnoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna,\nYang Song, Sergio Guadarrama, et al. Speed/accuracy tradeoffs for modern convolutional object detectors. In IEEE CVPR,\n2017.\n[5] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid\nAshraf, William J Dally, and Kurt Keutzer. Squeezenet:\nAlexnet-level accuracy with 50x fewer parameters and< 0.5\nmb model size. arXiv preprint arXiv:1602.07360, 2016.\n[6] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks.\nIn NIPS, 2012.\n[7] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep\nlearning. Nature, 2015.\n[8] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In CVPR, volume 1, page 4,\n2017.\n[9] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\nSzegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg.\nSSD: Single shot multibox detector. In European conference\non computer vision, pages 21–37. Springer, 2016.\n[10] J. Redmon.\nYOLO: Real-time object\nhttps://pjreddie.com/darknet/yolo/, 2016.\n\ndetection.\n\n[11] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\nFarhadi. You only look once: Unified, real-time object\ndetection. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 779–788, 2016.\n[12] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\nstronger. arXiv preprint, 1612, 2016.\n[13] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R-CNN: Towards real-time object detection with region\nproposal networks. In Advances in neural information\nprocessing systems, pages 91–99, 2015.\n[14] Mohammad Javad Shafiee, Brendan Chywl, Francis Li, and\nAlexander Wong. Fast YOLO: A fast you only look once\nsystem for real-time embedded object detection in video. arXiv\npreprint arXiv:1709.05943, 2017.\n\n\fInput Image\n\nTiny YOLO\n\nTiny SSD\n\nFigure 5. Example object detection results produced by the proposed Tiny SSD compared to Tiny YOLO. It can be observed that Tiny SSD has comparable\nobject detection results as Tiny YOLO in some cases, while in some cases outperforms Tiny YOLO in assigning more accurate category labels to detected\nobjects. This significant improvement in object detection accuracy when compared to Tiny YOLO illustrates the efficacy of Tiny SSD for providing more\nreliable embedded object detection performance.\n\n\f[15] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick.\nTraining region-based object detectors with online hard\nexample mining. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 761–769,\n2016.\n[16] Bichen Wu, Forrest Iandola, Peter H Jin, and Kurt Keutzer.\nSqueezedet: Unified, small, low power fully convolutional\nneural networks for real-time object detection for autonomous\ndriving. arXiv preprint arXiv:1612.01051, 2016.\n\n\f&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1,&quot;string&quot;:&quot;1&quot;}}},{&quot;rowIdx&quot;:2,&quot;cells&quot;:{&quot;text&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;1\\n\\nAn overview of deep learning based methods for\\nunsupervised and semi-supervised anomaly\\ndete&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1,&quot;string&quot;:&quot;1&quot;}}},{&quot;rowIdx&quot;:3,&quot;cells&quot;:{&quot;text&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;Energy Clustering\\nGuilherme França∗ and Joshua T. Vogelstein†\\nJohns Hopkins University\\n\\nA&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1,&quot;string&quot;:&quot;1&quot;}}},{&quot;rowIdx&quot;:4,&quot;cells&quot;:{&quot;text&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;Bayesian Probabilistic Numerical Methods\\nJon Cockayne∗\\n\\nChris Oates†\\n\\nTim Sullivan‡\\n\\nM&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:10,&quot;string&quot;:&quot;10&quot;}}},{&quot;rowIdx&quot;:5,&quot;cells&quot;:{&quot;text&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;Inverse Stability Problem and Applications to\\nRenewables Integration\\n\\narXiv:1703.04491v5 [] 14 O&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:3,&quot;string&quot;:&quot;3&quot;}}},{&quot;rowIdx&quot;:6,&quot;cells&quot;:{&quot;text&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;arXiv:1201.3325v2 [] 14 Aug 2012\\n\\nSIMPLICIAL COMPLEXES WITH RIGID DEPTH\\nADNAN ASLAM AND VIVIANA &quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;}}},{&quot;rowIdx&quot;:7,&quot;cells&quot;:{&quot;text&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;Orthogonal Series Density Estimation for Complex Surveys\\nShangyuan Ye, Ye Liang and Ibrahim A. Ahm&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:10,&quot;string&quot;:&quot;10&quot;}}},{&quot;rowIdx&quot;:8,&quot;cells&quot;:{&quot;text&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;A Parametric MPC Approach to Balancing the Cost of Abstraction for\\nDifferential-Drive Mobile Robot&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:3,&quot;string&quot;:&quot;3&quot;}}},{&quot;rowIdx&quot;:9,&quot;cells&quot;:{&quot;text&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;\&quot;Online Model Estimation for Predictive Thermal Control of Buildings\\nPeter Radecki, Member, IEEE, a&quot;},&quot;label&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:3,&quot;string&quot;:&quot;3&quot;}}}],&quot;truncated&quot;:true},&quot;paginationData&quot;:{&quot;pageIndex&quot;:0,&quot;numItemsPerPage&quot;:100,&quot;numTotalItems&quot;:1000,&quot;offset&quot;:0,&quot;length&quot;:100}},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzA0Niwic3ViIjoiL2RhdGFzZXRzL25icm9hZC9zbWFsbF9hcnhpdl9jbGFzc2lmaWNhdGlvbiIsImV4cCI6MTc0MjkyNjY0NiwiaXNzIjoiaHR0cHM6Ly9odWdnaW5nZmFjZS5jbyJ9.rkoJDh6PrNEFN7WCjOUIlMNig3bbgTv7Q82W4aH46CMTqGaa7d6Yz7lOWxew8gDy3x9foWOpNXWXYhDffmYoAQ&quot;,&quot;displayUrls&quot;:true},&quot;dataset&quot;:&quot;nbroad/small_arxiv_classification&quot;,&quot;isGated&quot;:false,&quot;isPrivate&quot;:false,&quot;hasParquetFormat&quot;:true,&quot;author&quot;:{&quot;_id&quot;:&quot;5f353bb37e58354338621655&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1639773384591-5f353bb37e58354338621655.jpeg&quot;,&quot;fullname&quot;:&quot;Nicholas Broad&quot;,&quot;name&quot;:&quot;nbroad&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:101},&quot;compact&quot;:true}"><div class="flex flex-col overflow-hidden shadow-xs mx-auto mb-10 rounded-lg border pt-2  px-2.5"><div class="mb-2 flex flex-wrap items-center gap-2"><div class="mr-auto flex items-center"><svg class="mr-1 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	<div class="whitespace-nowrap font-semibold">Dataset Viewer</div>
	</div>
				<a href="/datasets/nbroad/small_arxiv_classification/tree/refs%2Fconvert%2Fparquet/default" class="group mr-1 text-xs text-gray-400 max-sm:hidden"><svg class="text-[.6rem] mr-1 inline -translate-y-px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill="currentColor" d="M12 10H6.78A11 11 0 0 1 27 16h2A13 13 0 0 0 6 7.68V4H4v8h8zm8 12h5.22A11 11 0 0 1 5 16H3a13 13 0 0 0 23 8.32V28h2v-8h-8z"></path></svg>
						<span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Auto-converted</span> to Parquet
					</a>
				<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>API</button>
					<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path d="M9.80603 2.86737H3.56107C3.37704 2.86737 3.20055 2.94048 3.07042 3.0706C2.94029 3.20073 2.86719 3.37723 2.86719 3.56126V9.80622C2.86719 9.99025 2.94029 10.1667 3.07042 10.2969C3.20055 10.427 3.37704 10.5001 3.56107 10.5001H9.80603C9.99006 10.5001 10.1666 10.427 10.2967 10.2969C10.4268 10.1667 10.4999 9.99025 10.4999 9.80622V3.56126C10.4999 3.37723 10.4268 3.20073 10.2967 3.0706C10.1666 2.94048 9.99006 2.86737 9.80603 2.86737Z" fill="currentColor" fill-opacity="0.3"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M2.40942 1.66191C2.05175 1.66191 1.7618 1.95186 1.7618 2.30953V6.76191H1.43799V2.30953C1.43799 1.77303 1.87291 1.3381 2.40942 1.3381H6.45704V1.66191H2.40942Z" fill="currentColor"></path></svg>Embed</button>

					<button class="bg-linear-to-b shadow-xs flex items-center gap-1.5 rounded-full border from-white to-red-100/90 px-2 py-0.5 text-xs font-medium text-[#2D3648] transition-shadow hover:shadow-inner dark:from-gray-900 dark:to-red-800/30 dark:text-gray-100 dark:hover:shadow-inner dark:hover:shadow-red-800/30" ><svg class="h-3.5 w-3.5 text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
						<span>Data Studio</span></button></div>
		<div class="flex flex-1 flex-col overflow-hidden -mx-2.5"><div class="flex flex-1 flex-col overflow-hidden"><div class="flex min-h-0 flex-1"><div class="flex flex-1 flex-col overflow-hidden"><div class="md:-mx-2.5 flex min-w-0 flex-wrap border-t"><div class="flex min-w-0 flex-1 flex-wrap"><div class="grid flex-1 grid-cols-1 overflow-hidden text-sm md:grid-cols-2 md:place-content-center sm:mx-2.5"><label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-950 md:border-r md:border-r-0 hidden" title="default"><span class="text-gray-500">Subset (1)</span>
			<div class="flex items-center whitespace-nowrap"><span class="truncate">default</span>
				<span class="mx-2 text-gray-500">·</span>
					<span class="text-gray-500">2k rows</span>
				<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
			<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Subset (1)"><option value="default" selected>default (2k rows)</option></optgroup></select></label>
		<label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-900 md:border-r md:border-r" title="train"><div class="text-gray-500">Split (3)</div>
				<div class="flex items-center overflow-hidden whitespace-nowrap"><span class="truncate">train</span>
					<span class="mx-2 text-gray-500">·</span>
						<span class="text-gray-500">1k rows</span>
					<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
				<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Split (3)"><option value="train" selected>train (1k rows)</option><option value="validation" >validation (500 rows)</option><option value="test" >test (500 rows)</option></optgroup></select></label></div></div>
								</div>

							<div class="flex min-h-0 flex-1 flex-col ">
	<div class="bg-linear-to-r text-smd relative flex items-center dark:border-gray-900 dark:bg-gray-950 false border-t [&amp;:has(:focus)]:from-gray-50 [&amp;:has(:focus)]:to-transparent [&amp;:has(:focus)]:to-20% dark:[&amp;:has(:focus)]:from-gray-900"><form class="flex-1"><svg class="absolute left-3 top-1/2 transform -translate-y-1/2 pointer-events-none text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				<input disabled class="outline-hidden h-9 w-full border-none bg-transparent px-1 pl-9 pr-3 placeholder:text-gray-400 " placeholder="Search this dataset" dir="auto"></form>
			<div class="flex items-center gap-2 px-2 py-1"><button type="button" class="hover:bg-yellow-200/70 flex items-center gap-1 rounded-md border border-yellow-200 bg-yellow-100 pl-0.5 pr-1 text-[.8rem] leading-normal text-gray-700 dark:border-orange-500/25 dark:bg-orange-500/20 dark:text-gray-300 dark:hover:brightness-110 hidden"><div class="rounded-sm bg-yellow-300 px-1 font-mono text-[.7rem] font-bold text-black dark:bg-yellow-700 dark:text-gray-200">SQL
	</div>
	Console
</button></div></div>


<div class="flex flex-1 flex-col overflow-hidden min-h-64 border-t">
		

<div class="max-h-96 relative overflow-auto"><table class="w-full table-auto rounded-lg font-mono text-xs text-gray-900"><thead class="shadow-xs sticky left-0 right-0 top-0 z-1 bg-white align-top"><tr class="space-y-54 h-full min-w-fit divide-x border-b text-left"><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">text
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="18.92146596858639" width="11.2" height="11.078534031413612" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">7.27k</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">650k</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">label
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>int64</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="0" y="7.481132075471699" width="20" height="22.5188679245283" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="22" y="3.556603773584907" width="20" height="26.443396226415093" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="44" y="6.132075471698112" width="20" height="23.867924528301888" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="66" y="0" width="20" height="30" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="88" y="0.36792452830188793" width="20" height="29.632075471698112" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="110" y="15.820754716981131" width="20" height="14.179245283018869" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="22" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="21" y="0" width="22" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="43" y="0" width="22" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="22" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="87" y="0" width="22" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="109" y="0" width="22" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">0</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">10</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th></tr></thead>
			<tbody class="h-16 overflow-scroll"><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="0"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">arXiv:1608.03703v2 [] 26 Apr 2017

Template estimation in computational anatomy:
Fréchet means in top and quotient spaces are
not consistent
Loïc Devilliers∗, Stéphanie Allassonnière†, Alain Trouvé‡,
and Xavier Pennec§
April 27, 2017

Abstract
In this article, we study the consistency of the template estimation
with the Fréchet mean in quotient spaces. The Fréchet mean in quotient
spaces is often used when the observations are deformed or transformed
by a group action. We show that in most cases this estimator is actually
inconsistent. We exhibit a sufficient condition for this inconsistency, which
amounts to the folding of the distribution of the noisy template when it
is projected to the quotient space. This condition appears to be fulfilled
as soon as the support of the noise is large enough. To quantify this
inconsistency we provide lower and upper bounds of the bias as a function
of the variability (the noise level). This shows that the consistency bias
cannot be neglected when the variability increases.

Keyword : Template, Fréchet mean, group action, quotient space, inconsistency,
consistency bias, empirical Fréchet mean, Hilbert space, manifold

∗ Université

Côte d’Azur, Inria, France, loic.devilliers@inria.fr
Ecole polytechnique, CNRS, Université Paris-Saclay, 91128, Palaiseau, France
‡ CMLA, ENS Cachan, CNRS, Université Paris-Saclay, 94235 Cachan, France
§ Université Côte d’Azur, Inria, France
† CMAP,

1

Contents
1 Introduction

3

2 Definitions, notations and generative model

5

3 Inconsistency for finite group when the template is
point
3.1 Presence of inconsistency . . . . . . . . . . . . . . . .
3.2 Upper bound of the consistency bias . . . . . . . . . .
3.3 Study of the consistency bias in a simple example . . .

. . . . . .
. . . . . .
. . . . . .

4 Inconsistency for any group when the template is
point
4.1 Presence of an inconsistency . . . . . . . . . . . . . .
4.2 Analysis of the condition in theorem 4.1 . . . . . . .
4.3 Lower bound of the consistency bias . . . . . . . . .
4.4 Upper bound of the consistency bias . . . . . . . . .
4.5 Empirical Fréchet mean . . . . . . . . . . . . . . . .
4.6 Examples . . . . . . . . . . . . . . . . . . . . . . . .
4.6.1 Action of translation on L2 (R/Z) . . . . . . .
4.6.2 Action of discrete translation on RZ/NZ . . . .
4.6.3 Action of rotations on Rn . . . . . . . . . . .

.
.
.
.
.
.
.
.
.

5 Fréchet means top and quotient spaces
the template is a fixed point
5.1 Result . . . . . . . . . . . . . . . . . .
5.2 Proofs of these theorems . . . . . . . .
5.2.1 Proof of theorem 5.1 . . . . . .
5.2.2 Proof of theorem 5.2 . . . . . .
6 Conclusion and discussion

a regular
8
9
12
13

not a fixed
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

14
14
15
18
20
22
22
23
23
23

are not consistent when
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

25
25
26
26
28
28

A Proof of theorems for finite groups’ setting
29
A.1 Proof of theorem 3.2: differentiation of the variance in the quotient space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
A.2 Proof of theorem 3.1: the gradient is not zero at the template . . 32
A.3 Proof of theorem 3.3: upper bound of the consistency bias . . . . 32
A.4 Proof of proposition 3.2: inconsistency in R2 for the action of
translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
B Proof of lemma 5.1: differentiation of the variance in the top
space
35

2

1

Introduction

In Kendall’s shape space theory [Ken89], in computational anatomy [GM98],
in statistics on signals, or in image analysis, one often aims at estimating a
template. A template stands for a prototype of the data. The data can be the
shape of an organ studied in a population [DPC+ 14] or an aircraft [LAJ+ 12],
an electrical signal of the human body, a MR image etc. To analyse the observations, one assumes that these data follow a statistical model. One often
models observations as random deformations of the template with additional
noise. This deformable template model proposed in [GM98] is commonly used
in computational anatomy. The concept of deformation introduces the notion of
group action: the deformations we consider are elements of a group which acts
on the space of observations, called here the top space. Since the deformations
are unknown, one usually considers equivalent classes of observations under the
group action. In other words, one considers the quotient space of the top space
(or ambient space) by the group. In this particular setting, the template estimation is most of the time based on the minimisation of the empirical variance
in the quotient space (for instance [KSW11, JDJG04, SBG08] among many others). The points that minimise the empirical variance are called the empirical
Fréchet mean. The Fréchet means introduced in [Fré48] is comprised of the
elements minimising the variance. This generalises the notion of expected value
in non linear spaces. Note that the existence or uniqueness of Fréchet mean is
not ensured. But sufficient conditions may be given in order to reach existence
and uniqueness (for instance [Kar77] and [Ken90]).
Several group actions are used in practice: some signals can be shifted in
time compared to other signals (action of translations [HCG+ 13]), landmarks
can be transformed rigidly [Ken89], shapes can be deformed by diffeomorphisms [DPC+ 14], etc. In this paper we restrict to transformation which leads
the norm unchanged. Rotations for instance leave the norm unchanged, but it
may seem restrictive. In fact, the square root trick detailed in section 5, allows
to build norms which are unchanged, for instance by reparametrization of curves
with a diffeomorphism, where our work can be applied.
We raise several issues concerning the estimation of the template.
1. Is the Fréchet mean in the quotient space equal to the original template
projected in the quotient space? In other words, is the template estimation
with the Fréchet mean in quotient space consistent?
2. If there is an inconsistency, how large is the consistency bias? Indeed,
we may expect the consistency bias to be negligible in many practicable
cases.
3. If one gets only a finite sample, one can only estimate the empirical Fréchet
mean. How far is the empirical Fréchet mean from the original template?
These issues originated from an example exhibited by Allassonnière, Amit and
Trouvé [AAT07]: they took a step function as a template and they added some

3

noise and shifted in time this function. By repeating this process they created a
data sample from this template. With this data sample, they tried to estimate
the template with the empirical Fréchet mean in the quotient space. In this
example, minimising the empirical variance did not succeed in estimating well
the template when the noise added to the template increases, even with a large
sample size.
One solution to ensure convergence to the template is to replace this estimation method with a Bayesian paradigm ([AKT10, BG14] or [ZSF13]). But there
is a need to have a better understanding of the failure of the template estimation with the Fréchet mean. One can studied the inconsistency of the template
estimation. Bigot and Charlier [BC11] first studied the question of the template
estimation with a finite sample in the case of translated signals or images by
providing a lower bound of the consistency bias. This lower bound was unfortunately not so informative as it is converging to zero asymptotically when the
dimension of the space tends to infinity. Miolane et al. [MP15, MHP16] later
provided a more general explanation of why the template is badly estimated
for a general group action thanks to a geometric interpretation. They showed
that the external curvature of the orbits is responsible for the inconsistency.
This result was further quantified with Gaussian noise. In this article, we provide sufficient conditions on the noise for which inconsistency appears and we
quantify the consistency bias in the general (non necessarily Gaussian) case.
Moreover, we mostly consider a vector space (possibly infinite dimensional) as
the top space while the article of Miolane et al. is restricted to finite dimensional manifolds. In a preliminary unpublished version of this work [ADP15],
we proved the inconsistency when the transformations come from a finite group
acting by translation. The current article extends these results by generalizing
to any isometric action of finite and non-finite groups.
This article is organised as follows. Section 2 details the mathematical terms
that we use and the generative model. In sections 3 and 4, we exhibit sufficient
condition that lead to an inconsistency when the template is not a fixed point
under the group action. This sufficient condition can be roughly understand as
follows: with a non zero probability, the projection of the random variable on
the orbit of the template is different from the template itself. This condition is
actually quite general. In particular, this condition it is always fulfilled with the
Gaussian noise or with any noise whose support is the whole space. Moreover
we quantify the consistency bias with lower and upper bounds. We restrict
our study to Hilbert spaces and isometric actions. This means that the space
is linear, the group acts linearly and leaves the norm (or the dot product)
unchanged. Section 3 is dedicated to finite groups. Then we generalise our
result in section 4 to non-finite groups. To complete this study, we extend in
section 5 the result when the template is a fixed point under the group action
and when the top space is a manifold. As a result we show that the inconsistency
exists for almost all noises. Although the bias can be neglected when the noise
level is sufficiently small, its linear asymptotic behaviour with respect to the
noise level show that it becomes unavoidable for large noises.

4

2

Definitions, notations and generative model

We denote by M the top space, which is the image/shape space, and G the
group acting on M . The action is a map:
G×M
(g, m)

→
M
7→ g · m

satisfying the following properties: for all g, g 0 ∈ G, m ∈ M (gg 0 )·m = g ·(g 0 ·m)
and eG · m = m where eG is the neutral element of G. For m ∈ M we note by
[m] the orbit of m (or the class of m). This is the set of points reachable from
m under the group action: [m] = {g · m, g ∈ G}. Note that if we take two orbits
[m] and [n] there are two possibilities:
1. The orbits are equal: [m] = [n] i.e. ∃g ∈ G s.t. n = g · m.
2. The orbits have an empty intersection: [m] ∩ [n] = ∅.
We call quotient of M by the group G the set all orbits. This quotient is noted
by:
Q = M/G = {[m], m ∈ M }.
The orbit of an element m ∈ M can be seen as the subset of M of all elements
g · m for g ∈ G or as a point in the quotient space. In this article we use these
two ways. We project an element m of the top space M into the quotient by
taking [m].
Now we are interested in adding a structure on the quotient from an existing
structure in the top space: take M a metric space, with dM its distance. Suppose
that dM is invariant under the group action which means that ∀g ∈ G, ∀a, b ∈
M dM (a, b) = dM (g · a, g · b). Then we obtain a pseudo-distance on Q defined
by:
dQ ([a], [b]) = inf dM (g · a, b).
(1)
g∈G

We remind that a distance on M is a map dM : M × M 7→ R+ such that for all
m, n, p ∈ M :
1. dM (m, n) = dM (n, m) (symmetry).
2. dM (m, n) ≤ dM (m, p) + dM (p, n) (triangular inequality).
3. dM (m, m) = 0.
4. dM (m, n) = 0 ⇐⇒ m = n.
A pseudo-distance satisfies only the first three conditions. If we suppose that
all the orbits are closed sets of M , then one can show that dQ is a distance. In
this article, we assume that dQ is always a distance, even if a pseudo-distance
would be sufficient. dQ ([a], [b]) can be interpreted as the distance between the
shapes a and b, once one has removed the parametrisation by the group G. In
other words, a and b have been registered. In this article, except in section 5, we
5

suppose that the the group acts isometrically on an Hilbert space, this means
that the map x 7→ g ·x is linear, and that the norm associated to the dot product
is conserved: kg · xk = kxk. Then dM (a, b) = ka − bk is a particular case of
invariant distance.
We now introduce the generative model used in this article for M a
vector space. Let us take a template t0 ∈ M to which we add a unbiased noise
: X = t0 + . Finally we transform X with a random shift S of G. We assume
that this variable S is independent of X and the only observed variable is:
Y = S · X = S · (t0 + ), with E() = 0,

(2)

while S, X and  are hidden variables.
Note that it is not the generative model defined by Grenander and often
used in computational anatomy. Where the observed variable is rather Y 0 =
S · t0 + 0 . But when the noise is isotropic and the action is isometric, one can
show that the two models have the same law, since S ·  and  have the same
probability distribution. As a consequence, the inconsistency of the template
estimation with the Fréchet mean in quotient space with one model implies the
inconsistency with the other model. Because the former model (2) leads to
simpler computation we consider only this model.
We can now set the inverse problem: given the observation Y , how to estimate the template t0 in M ? This is an ill-posed problem. Indeed for some
element group g ∈ G, the template t0 can be replaced by the translated g ·t0 , the
shift S by Sg −1 and the noise  by g, which leads to the same observation Y . So
instead of estimating the template t0 , we estimate its orbit [t0 ]. By projecting
the observation Y in the quotient space we obtain [Y ]. Although the observation
Y = S · X and the noisy template X are different random variables in the top
space, their projections on the quotient space lead to the same random orbit
[Y ] = [X]. That is why we consider the generative model (2): the projection
in the quotient space remove the transformation of the group G. From now on,
we use the random orbit [X] in lieu of the random orbit of the observation [Y ].
The variance of the random orbit [X] (sometimes called the Fréchet functional or the energy function) at the quotient point [m] ∈ Q is the expected
value of the square distance between [m] and the random orbit [X], namely:
Q 3 [m] 7→ E(dQ ([m], [X])2 )

(3)

An orbit [m] ∈ Q which minimises this map is called a Fréchet mean of [X].
If we have an i.i.d sample of observations Y1 , . . . , Yn we can write the empirical quotient variance:
n

Q 3 [m] 7→

n

1X
1X
dQ ([m], [Yi ])2 =
inf km − gi · Yi k2 .
n i=1
n i=1 gi ∈G

(4)

Thanks to the equality of the quotient variables [X] and [Y ], an element which
minimises this map is an empirical Fréchet mean of [X].

6

In order to minimise the empirical quotient variance (4), the max-max algon
P
rithm1 alternatively minimises the function J(m, (gi )i ) = n1 km−gi ·Yi k2 over
i=1

a point m of the orbit [m] and over the hidden transformation (gi )1≤i≤n ∈ Gn .
With these notations we can reformulate our questions as:
1. Is the orbit of the template [t0 ] a minimiser of the quotient variance defined
in (3)? If not, the Fréchet mean in quotient space is an inconsistent
estimator of [t0 ].
2. In this last case, can we quantify the quotient distance between [t0 ] and a
Fréchet mean of [X]?
3. Can we quantify the distance between [t0 ] and an empirical Fréchet mean
of a n-sample?
This article shows that the answer to the first question is usually "no" in the
framework of an Hilbert space M on which a group G acts linearly and isometrically. The only exception is theorem 5.1 where the top space M is a manifold.
In order to prove inconsistency, an important notion in this framework is the
isotropy group of a point m in the top space. This is the subgroup which leaves
this point unchanged:
Iso(m) = {g ∈ G, g · m = m}.
We start in section 3 with the simple example where the group is finite and the
isotropy group of the template is reduced to the identity element (Iso(t0 ) =
{eG }, in this case t0 is called a regular point). We turn in section 4 to the case
of a general group and an isotropy group of the template which does not cover
the whole group (Iso(t0 ) 6= G) i.e t0 is not a fixed point under the group action.
To complete the analysis, we assume in section 5 that the template t0 is a fixed
point which means that Iso(t0 ) = G.
In sections 3 and 4 we show lower and upper bounds of the consistency bias
which we define as the quotient distance between the template orbit and the
Fréchet mean in quotient space. These results give an answer to the second
question. In section 4, we show a lower bound for the case of the empirical
Fréchet mean which answers to the third question.
As we deal with different notions whose name or definition may seem similar,
we use the following vocabulary:
1. The variance of the noisy template X in the top space is the function
E : m ∈ M 7→ E(km − Xk2 ). The unique element which minimises this
function is the Fréchet mean of X in the top space. With our assumptions
it is the template t0 itself.
2. We call variability (or noise level) of the template the value of the variance
at this minimum: σ 2 = E(kt0 − Xk2 ) = E(t0 ).
1 The term max-max algorithm is used for instance in [AAT07], and we prefer to keep the
same name, even if it is a minimisation.

7

3. The variance of the random orbit [X] in the quotient space is the function
F : m 7→ E(dQ ([m], [X])2 ). Notice that we define this function from the
top space and not from the quotient space. With this definition, an orbit
[m? ] is a Fréchet mean of [X] if the point m? is a global minimiser of F .
In sections 3 and 4, we exhibit a sufficient condition for the inconsistency,
which is: the noisy template X takes value with a non zero probability in
the set of points which are strictly closer to g · t0 for some g ∈ G than the
template t0 itself. This is linked to the folding of the distribution of the noisy
template when it is projected to the quotient space. The points for which the
distance to the template orbit in the quotient space is equal to the distance to
the template in the top space are projected without being folded. If the support
of the distribution of the noisy template contains folded points (we only assume
that the probability measure of X, noted P, is a regular measure), then there
is inconsistency. The support of the noisy template X is defined by the set of
points x such that P(X ∈ B(x, r)) > 0 for all r > 0. For different geometries of
the orbit of the template, we show that this condition is fulfilled as soon as the
support of the noise is large enough.
The recent article of Cleveland et al. [CWS16] may seem contradictory with
our current work. Indeed the consistency of the template estimation with the
Fréchet mean in quotient space is proved under hypotheses which seem to satisfy
our framework: the norm is unchanged under their group action (isometric
action) and a noise is present in their generative model. However we believe
that the noise they consider might actually not be measurable. Indeed, their
top space is:


Z 1
L2 ([0, 1]) = f : [0, 1] → R such that f is measurable and
f 2 (t)dt &lt; +∞ .
0

The noise e is supposed to be in L2 ([0, 1]) such that for all t, s ∈ [0, 1], E(e(t)) = 0
and E(e(t)e(s)) = σ 2 1s=t , for σ > 0. This means that e(t) and e(s) are chosen
without correlation as soon as s 6= t. In this case, it is not clear for us that the
resulting function e is measurable, and thus that its Lebesgue integration makes
sense. Thus, the existence of such a random process should be established before
we can fairly compare the results of both works.

3

Inconsistency for finite group when the template is a regular point

In this Section, we consider a finite group G acting isometrically and effectively
on M = Rn a finite dimensional space equipped with the euclidean norm k k,
associated to the dot product h , i.
We say that the action is effective if x 7→ g · x is the identity map if and only
if g = eG . Note that if the action is not effective, we can define a new effective
action by simply quotienting G by the subgroup of the element g ∈ G such that
x 7→ g · x is the identity map.
8

The template is assumed to be a regular point which means that the isotropy
group of the template is reduced to the neutral element of G. Note that the
measure of singular points (the points which are not regular) is a null set for
the Lebesgue measure (see item 1 in appendix A.1).
Example 3.1. The action of translation on coordinates: this action is a simplified setting for image registration, where images can be obtained by the translation of one scan to another due to different poses. More precisely, we take
the vector space M = RT where G = T = (Z/N Z)D is the finite torus in Ddimension. An element of RT is seen as a function m : T → R, where m(τ ) is
the grey value at pixel τ . When D = 1, m can be seen like a discretised signal
with N points, when D = 2, we can see m like an image with N × N pixels etc.
We then define the group action of T on RT by:
τ ∈ T, m ∈ RT

τ · m : σ 7→ m(σ + τ ).

This group acts isometrically and effectively on M = RT .
In this setting, if E(kXk2 ) &lt; +∞ then the variance of [X] is well defined:
F : m ∈ M 7→ E(dQ ([X], [m])2 ).
In this framework, F is non-negative and continuous.
Schwarz inequality we have:
lim F (m) ≥

kmk→∞

(5)
Thanks to Cauchy-

lim kmk2 − 2kmkE(kXk) + E(kXk2 ) = +∞.

kmk→∞

Thus for some R > 0 we have: for all m ∈ M if kmk > R then F (m) ≥ F (0) + 1.
The closed ball B(0, R) is a compact set (because M is a finite vector space)
then F restricted to this ball reached its minimum m? . Then for all m ∈ M ,
if m ∈ B(0, R), F (m? ) ≤ F (m), if kmk > R then F (m) ≥ F (0) + 1 > F (0) ≥
F (m? ). Therefore [m? ] is a Fréchet mean of [X] in the quotient Q = M/G.
Note that this ensure the existence but not the uniqueness.
In this Section, we show that as soon as the support of the distribution of
X is big enough, the orbit of the template is not a Fréchet mean of [X]. We
provide a upper bound of the consistency bias depending on the variability of
X and an example of computation of this consistency bias.

3.1

Presence of inconsistency

The following theorem gives a sufficient condition on the random variable X for
an inconsistency:
Theorem 3.1. Let G be a finite group acting on M = Rn isometrically and
effectively. Assume that the random variable X is absolutely continuous with
respect to the Lebesgue’s measure, with E(kXk2 ) &lt; +∞. We assume that t0 =
E(X) is a regular point.

9

g · t0

0

Cone(t0 )

t0

g 0 · t0
Figure 1: Planar representation of a part of the orbit of the template t0 . The
lines are the hyperplanes whose points are equally distant of two distinct elements of the orbit of t0 , Cone(t0 ) represented in points is the set of points closer
from t0 than any other points in the orbit of t0 . Theorem 3.1 states that if the
support (the dotted disk) of the random variable X is not included in this cone,
then there is an inconsistency.
We define Cone(t0 ) as the set of points closer from t0 than any other points
of the orbit [t0 ], see fig. 1 or item 6 in appendix A.1 for a formal definition. In
other words, Cone(t0 ) is defined as the set of points already registered with t0 .
Suppose that:
P (X ∈
/ Cone(t0 )) > 0,
(6)
then [t0 ] is not a Fréchet mean of [X].
The proof of theorem 3.1 is based on two steps: first, differentiating the
variance F of [X]. Second, showing that the gradient at the template is not
zero, therefore the template can not be a minimum of F . Theorem 3.2 makes
the first step.
Theorem 3.2. The variance F of [X] is differentiable at any regular points. For
m0 a regular point, we define g(x, m0 ) as the almost unique g ∈ G minimising
km0 − g · xk (in other words, g(x, m0 ) · x ∈ Cone(m0 )). This allows us to
compute the gradient of F at m0 :
∇F (m0 ) = 2(m0 − E(g(X, m0 ) · X)).

(7)

This Theorem is proved in appendix A.1. Then we show that the gradient
of F at t0 is not zero. To ensure that F is differentiable at t0 we suppose in
the assumptions of theorem 3.1 that t0 = E(X) is a regular point. Thanks
to theorem 3.2 we have:
∇F (t0 ) = 2(t0 − E(g(X, t0 ) · X)).
Therefore ∇F (t0 )/2 is the difference between two terms, which are represented on fig. 2: on fig. 2a there is a mass under the two hyperplanes outside

10

g · t0

0

g · t0

Cone(t0 )

t0

0

g 0 · t0

Cone(t0 )

t0

Z

g 0 · t0

(a) Graphic representation of the
template t0 = E(X) mean of
points of the support of X.

(b) Graphic representation of
Z = E(g(X, t0 ) · X). The points
X which were outside Cone(t0 )
are now in Cone(t0 ) (thanks to
g(X, t0 )). This part, in grid-line,
represents the points which have
been folded.

Figure 2: Z is the mean of points in Cone(t0 ) where Cone(t0 ) is the set of points
closer from t0 than g · t0 for g ∈ G \ eG . Therefore it seems that Z is higher that
t0 , therefore ∇F (t0 ) = 2(t0 − Z) 6= 0.
Cone(t0 ), so this mass is nearer from gt0 for some g ∈ G than from t0 . In the following expression Z = E(g(X, t0 ) · X), for X ∈
/ Cone(t0 ), g(X, t0 )X ∈ Cone(t0 )
such points are represented in grid-line on fig. 2. This suggests that the point
Z = E(g(X, t0 ) · X) which is the mean of points in Cone(t0 ) is further away
from 0 than t0 . Then ∇F (t0 )/2 = t0 − Z should be not zero, and t0 = E(X) is
not a critical point of the variance of [X]. As a conclusion [t0 ] is not a Fréchet
mean of [X]. This is turned into a rigorous proof in appendix A.2.
In the proof of theorem 3.1, we took M an Euclidean space and we work with
the Lebesgue’s measure in order to have P(X ∈ H) = 0 for every hyperplane
H. Therefore the proof of theorem 3.1 can be extended immediately to any
Hilbert space M , if we make now the assumption that P(X ∈ H) = 0 for
every hyperplane H, as long as we keep a finite group acting isometrically and
effectively on M .
Figure 2 illustrates the condition of theorem 3.1: if there is no mass beyond
the hyperplanes, then the two terms in ∇F (t0 ) are equal (because almost surely
g(X, t0 ) · X = X). Therefore in this case we have ∇F (t0 ) = 0. This do not
prove necessarily that there is no inconsistency, just that the template t0 is a
critical point of F . Moreover this figure can give us an intuition on what the
consistency bias (the distance between [t0 ] and the set of all Fréchet mean in
the quotient space) depends: for t0 a fixed regular point, when the variability
of X (defined by E(kX − t0 k2 )) increases the mass beyond the hyperplanes
on fig. 2 also increases, the distance between E(g(X, t0 ) · X) and t0 (i.e. the
norm of ∇F (t0 )) augments. Therefore q the Fréchet mean should be further
from t0 , (because at this point one should have ∇F (q) = 0 or q is a singular
11

point). Therefore the consistency bias appears to increase with the variability
of X. By establishing a lower and upper bound of the consistency bias and
by computing the consistency bias in a very simple case, sections 3.2, 3.3, 4.3
and 4.4 investigate how far this hypothesis is true.
We can also wonder if the converse of theorem 3.1 is true: if the support is
included in Cone(t0 ), is there consistency? We do not have a general answer
to that. In the simple example section 3.3 it happens that condition (6) is
necessary and sufficient. More generally the following proposition provides a
partial converse:
Cone(y)

g · t0

y
t0
O
Cone(t0 )
g 0 · t0
Figure 3: y 7→ Cone(y) is continuous. When the support of the X is bounded
and included in the interior of Cone(t0 ) the hatched cone. For y sufficiently
close to the template t0 , the support of the X (the ball in red) is still included
in Cone(y) (in grey), then F (y) = (E(kX − yk2 ). Therefore in this case, [t0 ] is
at least a Karcher mean of [X].
Proposition 3.1. If the support of X is a compact set included in the interior
of Cone(t0 ), then the orbit of the template [t0 ] is at least a Karcher mean of [X]
(a Karcher mean is a local minimum of the variance).
Proof. If the support of X is a compact set included in the interior of Cone(t0 )
then we know that X-almost surely: dQ ([X], [t0 ]) = kX −t0 k. Thus the variance
at t0 in the quotient space is equal to the variance at t0 in the top space. Now
by continuity of the distance map (see fig. 3) for y in a small neighbourhood
of t0 , the support of X is still included in the interior of Cone(y). We still
have dQ ([X], [y]) = kX − yk X-almost surely. In other words, locally around
t0 , the variance in the quotient space is equal to the variance in the top space.
Moreover we know that t0 = E(X) is the only global minimiser of the variance
of X: m 7→ E(km − Xk2 ) = E(m). Therefore t0 is a local minimum of F
the variance in the quotient space (since the two variances are locally equal).
Therefore [t0 ] is at least a Karcher mean of [X] in this case.

3.2

Upper bound of the consistency bias

In this Subsection we show an explicit upper bound of the consistency bias.
12

Theorem 3.3. When G is a finite group acting isometrically on M = Rn ,
we denote |G| the cardinal of the group G. If X is Gaussian vector: X ∼
N (t0 , s2 IdRn ), and m? ∈ argmin F , then we have the upper bound of the consistency bias:
p
(8)
dQ ([t0 ], [m? ]) ≤ s 8 log(|G|).
The proof is postponed in appendix A.3. When X ∼ N (t0 , s2 Idn ) the
variability of X is σ 2 = E(||X − t0 ||2 )p= ns2 and we can write the upper
bound of the bias: dQ ([t0 ], [m? ]) ≤ √σn 8 log |G|. This Theorem shows that
the consistency bias is low when the variability of X is small, which tends to
confirm our hypothesis in section 3.1. It is important to notice that this upper
bound explodes when the cardinal of the group tends to infinity.

3.3

Study of the consistency bias in a simple example

In this Subsection, we take a particular case of example 3.1: the action of
translation with T = Z/2Z. We identify RT with R2 and we note by (u, v)T an
element of RT . In this setting, one can completely describe the action of T on
RT : 0 · (u, v)T = (u, v)T and 1 · (u, v)T = (v, u)T . The set of singularities is the
line L = {(u, u)T , u ∈ R}. We note HPA = {(u, v)T , v > u} the half-plane
above L and HPB the half-plane below L. This simple example will allow us
to provide necessary and sufficient condition for an inconsistency at regular and
singular points. Moreover we can compute exactly the consistency bias, and
exhibit which parameters govern the bias. We can then find an equivalent of
the consistency bias when the noise tends to zero or infinity. More precisely, we
have the following theorem proved in appendix A.4:
Proposition 3.2. Let X be a random variable such that E(kXk2 ) &lt; +∞ and
t0 = E(X).
1. If t0 ∈ L, there is no inconsistency if and only if the support of X is
included in the line L = {(u, u), u ∈ R}. If t0 ∈ HPA (respectively in
HPB ), there is no inconsistency if and only if the support of X is included
in HPA ∪ L (respectively in HPB ∪ L).
2. If X is Gaussian: X ∼ N (t0 , s2 Id2 ), then the Fréchet mean of [X] exists
and is unique. This Fréchet mean [m? ] is on the line passing through E(X)
and perpendicular to L and the consistency bias ρ̃ = dQ ([t0 ], [m? ]) is the
function of s and d = dist(t0 , L) given by:
 2  
Z
2 +∞ 2
r
d
ρ̃(d, s) = s
r exp −
g
dr,
(9)
π ds
2
rs
where g is a non-negative function on [0, 1] defined by g(x) = sin(arccos(x))−
x arccos(x).
(a) If d > 0 then s 7→ ρ̃(d, s) has an asymptotic linear expansion:
 2
Z
2 +∞ 2
r
ρ̃(d, s) ∼ s
r exp −
dr.
s→∞ π 0
2
13

(10)

(b) If d > 0, then ρ̃(d, s) = o(sk ) when s → 0, for all k ∈ N.
(c) s →
7 ρ̃(0, s) is linear with respect to s (for d = 0 the template is a
fixed point).
Remark 3.1. Here, contrarily to the case of the action of rotation in [MHP16],
it is not the ratio kE(X)k over the noise which matters to estimate the consistency bias. Rather the ratio dist(E(X), L) over the noise. However in both cases
we measure the distance between the signal and the singularities which was {0}
in [MHP16] for the action of rotations, L in this case.

4

Inconsistency for any group when the template
is not a fixed point

In section 3 we exhibited sufficient condition to have an inconsistency, restricted
to the case of finite group acting on an Euclidean space. We now generalize this
analysis to Hilbert spaces of any dimension included infinite. Let M be such
an Hilbert space with its dot product noted by h , i and its associated norm
k k. In this section, we do not anymore suppose that the group G is finite.
In the following, we prove that there is an inconsistency in a large number of
situations, and we quantify the consistency bias with lower and upper bounds.
Example 4.1. The action of continuous translation: We take G = (R/Z)D
acting on M = L2 ((R/Z)D , R) with:
∀τ ∈ G

∀f ∈ M

(τ · f ) : t 7→ f (t + τ )

This isometric action is the continuous version of the example 3.1: the elements
of M are now continuous images in dimension D.

4.1

Presence of an inconsistency

We state here a generalization of theorem 3.1:
Theorem 4.1. Let G be a group acting isometrically on M an Hilbert space,
and X a random variable in M , E(kXk2 ) &lt; +∞ and E(X) = t0 6= 0. If:
P (dQ ([t0 ], [X]) &lt; kt0 − Xk) > 0,

(11)



P sup hg · X, t0 i > hX, t0 i > 0.

(12)

or equivalently:
g∈G

Then [t0 ] is not a Fréchet mean of [X] in Q = M/G.
The condition of this Theorem is the same condition of theorem 3.1: the
support of the law of X contains points closer from gt0 for some g than t0 .
Thus the condition (12) is equivalent to E(dQ ([X], [t0 ])2 ) &lt; E(kX − t0 k2 ). In
other words, the variance in the quotient space at t0 is strictly smaller than the
variance in the top space at t0 .
14

Proof. First the two conditions are equivalent by definition of the quotient distance and by expansion of the square norm of kt0 − Xk and of kt0 − gXk for
g ∈ G.
As above, we define the variance of [X] by:


2
F (m) = E inf kg · X − mk .
g∈G

In order to prove this Theorem, we find a point m such that F (m) &lt; F (t0 ),
which directly implies that [t0 ] is not be a Fréchet mean of [X].
In the proof of theorem 3.1, we showed that under condition (6) we had
h∇F (t0 ), t0 i &lt; 0. This leads us to study F restricted to R+ t0 : we define for
a ∈ R+ f (a) = F (at0 ) = E(inf g∈G kg · X − ak2 ). Thanks to the isometric action
we can expand f (a) by:


f (a) = a2 kt0 k2 − 2aE sup hg · X, t0 i + E(kXk2 ),
(13)
g∈G

and explicit the unique element of R+ which minimises f :


E sup hg · X, t0 i
g∈G
.
a? =
kt0 k2

(14)

For all x ∈ M , we have sup hg · x, t0 i ≥ hx, t0 i and thanks to condition (12) we
g∈G

get:
E(sup hg · X, t0 i) > E(hX, t0 i) = hE(X), t0 i = kt0 k2 ,

(15)

g∈G

which implies a? > 1. Then F (a? t0 ) &lt; F (t0 ).

Note that kt0 k2 (a? − 1) = E supg∈G hg · X, t0 i − E(hX, t0 i) (which is positive) is exactly − h∇F (t0 ), t0 i /2 in the case of finite group, see Equation (44).
Here we find the same expression without having to differentiate the variance
F , which may be not possible in the current setting.

4.2

Analysis of the condition in theorem 4.1

We now look for general cases when we are sure that Equation (12) holds which
implies the presence of inconsistency. We saw in section 3 that when the group
was finite, it is possible to have no inconsistency only if the support of the
law is included in a cone delimited by some hyperplanes. The hyperplanes were
defined as the set of points equally distant of the template t0 and g ·t0 for g ∈ G.
Therefore if the cardinal of the group becomes more and more important, one
could think that in order to have no inconsistency the space where X should
takes value becomes smaller and smaller. At the limit it leaves only at most an
hyperplane. In the following, we formalise this idea to make it rigorous. We
show that the cases where theorem 4.1 cannot be applied are not generic cases.
15

First we can notice that it is not possible to have the condition (12) if t0 is a
fixed point under the action of G. Indeed in this case hg · X, t0 i = X, g −1 t0 =
hX, t0 i). So from now, we suppose that t0 is not a fixed point. Now let us see
some settings when we have the condition (11) and thus condition (12).
Proposition 4.1. Let G be a group acting isometrically on an Hilbert space M ,
and X a random variable in M , with E(kXk2 ) &lt; +∞ and E(X) = t0 6= 0. If:
1. [t0 ] \ {t0 } is a dense set in [t0 ].
2. There exists η > 0 such that the support of X contains a ball B(t0 , η).
Then condition (12) holds, and the estimator is inconsistent according to theorem 4.1.

B(t0 , η)
O

t0
g · t0

[t0 ]
Figure 4: The smallest disk is included in the support of X and the points in
that disk is closer from g · t0 than from t0 . According to theorem 4.1 there is an
inconsistency.
Proof. By density, one takes g · t0 ∈ B(t0 , η) \ {t0 } for some g ∈ G, now if we
take r &lt; min(kg ·t0 −t0 k/2, η −kg ·t0 −t0 k) then B(g ·t0 , r) ⊂ B(t0 , ). Therefore
by the assumption we made on the support one has P(X ∈ B(g · t0 , r)) > 0.
For y ∈ B(g · t0 , r) we have that kgt0 − yk &lt; kt0 − yk (see fig. 4). Then we
have: P (dQ ([X], [t0 ]) &lt; kX − t0 k) ≥ P(X ∈ B(g · t0 , r)) > 0. Then we verify
condition (12), and we can apply theorem 4.1.
Proposition 4.1 proves that there is a large number of cases where we can
ensure the presence of an inconsistency. For instance when M is a finite dimensional vector space and the random variable X has a continuous positive
density (for the Lebesgue’s measure) at t0 , condition 2 of Proposition 4.1 is
fulfilled. Unfortunately this proposition do not cover the case where there is no
mass at the expected value t0 = E(X). This situation could appear if X has
two modes for instance. The following proposition deals with this situation:
16

Proposition 4.2. Let G be a group acting isometrically on M . Let X be a
random variable in M , such that E(kXk2 ) &lt; +∞ and E(X) = t0 6= 0. If:
1. ∃ϕ s.t. ϕ : (−a, a) → [t0 ] is C 1 with ϕ(0) = t0 , ϕ0 (0) = v 6= 0.
2. The support of X is not included in the hyperplane v ⊥ : P(X ∈
/ v ⊥ ) > 0.
Then condition (12) is fulfilled, which leads to an inconsistency thanks to Theorem 4.1.
Proof. Thanks to the isometric action: ht0 , vi = 0. We choose y ∈
/ v ⊥ in the
support of X and make a Taylor expansion of the following square distance (see
also Figure 5) at 0:
kϕ(x) − yk2 = kt0 + xv + o(x) − yk2 = kt0 − yk2 − 2x hy, vi + o(x).
Then: ∃x? ∈ (−a, a) s.t. kx? k &lt; a, x hy, vi > 0 and kϕ(x? ) − yk &lt; kt0 − yk. For
some g ∈ G, ϕ(x? ) = g · t0 . By continuity of the norm we have:
∃r > 0 s.t. ∀z ∈ B(y, r) kg · t0 − zk &lt; kt0 − zk.
Then P(kg·t0 −Xk &lt; kt0 −Xk) ≥ P(X ∈ B(y, r)) > 0. Theorem 4.1 applies.
Proposition 4.2 was a sufficient condition on inconsistency in the case of an
orbit which contains a curve. This brings us to extend this result for orbits
which are manifolds:
Proposition 4.3. Let G be a group acting isometrically on an Hilbert space M ,
X a random variable in M , with E(kXk2 ) &lt; +∞. Assume X = t0 + σ, where
t0 6= 0 and E() = 0, and E(kk) = 1. We suppose that [t0 ] is a sub-manifold of
M and write Tt0 [t0 ] the linear tangent space of [t0 ] at t0 . If:
P(X ∈
/ Tt0 [t0 ]⊥ ) > 0,

(16)

P( ∈
/ Tt0 [t0 ]⊥ ) > 0,

(17)

which is equivalent to:
then there is an inconsistency.
Proof. First t0 ⊥ Tt0 [t0 ] (because the action is isometric), Tt0 [t0 ]⊥ = t0 +
Tt0 [t0 ]⊥ , then the event {X ∈ Tt0 [t0 ]⊥ } is equal to { ∈ Tt0 [t0 ]⊥ }. This proves
that equations (16) and (17) are equivalent. Thanks to assumption (16), we can
choose y in the support of X such that y ∈
/ Tt0 [t0 ]⊥ . Let us take v ∈ Tt0 [t0 ]
1
such that hy, vi =
6 0 and choose ϕ a C curve in [t0 ], such that ϕ(0) = t0 and
ϕ0 (0) = v. Applying proposition 4.2 we get the inconsistency.
Note that Condition (16) is very weak, because Tt0 [t0 ] is a strict linear
subspace of M .

17

[t0 ]

Tt0 [t0 ]

y

g · t0
O
t0

Tt0 [t0 ]⊥

Figure 5: y ∈
/ Tt0 [t0 ]⊥ therefore y is closer from g · t0 for some g ∈ G than t0
itself. In conclusion, if y is in the support of X, there is an inconsistency.

4.3

Lower bound of the consistency bias

Under the assumption of Theorem 4.1, we have an element a? t0 such that
F (a? t0 ) &lt; F (t0 ) where F is the variance of [X]. From this element, we deduce lower bounds of the consistency bias:
Theorem 4.2. Let δ be the unique positive solution of the following equation:
δ 2 + 2δ (kt0 k + EkXk) − kt0 k2 (a? − 1)2 = 0.
Let δ? be the unique positive solution of the following equation:


p
δ 2 + 2δkt0 k 1 + 1 + σ 2 /kt0 k2 − kt0 k2 (a? − 1)2 = 0,

(18)

(19)

where σ 2 = E(kX − t0 k2 ) is the variability of X. Then δ and δ? are two lower
bounds of the consistency bias.
Proof. In order to prove this Theorem, we exhibit a ball around t0 such that the
points on this ball have a variance bigger than the variance at the point a? t0 ,
where a? was defined in Equation (14): thanks to the expansion of the function
f we did in (13) we get :
F (t0 ) − F (a? t0 ) = kt0 k2 (a? − 1)2 > 0,

(20)

Moreover we can show (exactly like equation (43)) that for all x ∈ M :


2
2
|F (t0 ) − F (x)| ≤ E inf kg · X − t0 k − inf kg · X − xk
g∈G

g∈G

≤ kx − t0 k (2kt0 k + kx − t0 k + E(k2Xk)) .

(21)

With Equations (20) and (21), for all x ∈ B(t0 , δ) we have F (x) > F (a? t0 ).
No point in that ball mapped in the quotient space is a Fréchet mean of [X]. So
18

δ is a lower bound of the consistency bias. Now by usingthe fact that E(kXk) ≤
p
p
kt0 k2 + σ 2 , we get: 2|F (t0 )−F (x)| ≤ 2kx−t0 k×kt0 k 1 + 1 + σ 2 /kt0 k2 +
kx − t0 k2 . This proves that δ? is also a lower bound of the consistency bias.
δ? is smaller than δ, but the variability of X intervenes in δ? . Therefore we
propose to study the asymptotic behaviour of δ? when the variability tends to
infinity. We have the following proposition:
Proposition 4.4. Under the hypotheses of Theorem 4.2, we write X = t0 + σ,
with E() = 0, and E(kk2 ) = 1 and note ν = E(supg∈G hg, t0 /kt0 ki) ∈ (0, 1],
we have that:
p
δ? ∼ σ( 1 + ν 2 − 1),
σ→+∞

In particular, the consistency bias explodes when the variability of X tends
to infinity.
Proof. First, let us prove that that ν ∈ (0, 1] under the condition (12). We
have ν ≥ E(h, t0 /kt0 ki = 0. By a reductio ad absurdum: if ν = 0, then
sup hg, t0 i = h, t0 i almost surely. We have then almost surely: hX, t0 i ≤
g∈G

supg∈G hgX, t0 i ≤ kt0 k2 + supg∈G σ hg, t0 i = kt0 k2 + σ h, t0 i ≤ hX, t0 i , which
p
is in contradiction with (12). Besides ν ≤ E(kk) ≤ Ekk2 = 1
Second, we exhibit equivalent of the terms in equation (19) when σ → +∞:


p
2kt0 k 1 + 1 + σ 2 /kt0 k2 ∼ 2σ.
(22)
Now by definition of a? in Equation (14) and the decomposition of X = t0 + σ
we get:


1
E sup (hg · t0 , t0 i + hg · σ, t0 i) − kt0 k
kt0 k(a? − 1) =
kt0 k
g∈G


1
kt0 k(a? − 1) ≤
E sup hg · σ, t0 i = σν
(23)
kt0 k
g∈G


1
kt0 k(a? − 1) ≥
E sup hg · σ, t0 i − 2kt0 k = σν − 2kt0 k,
(24)
kt0 k
g∈G
The lower bound and the upper bound of kt0 k(a? −1) found in (23) and (24) are
both equivalent to σν, when σ → +∞. Then the constant term of the quadratic
Equation (19) has an equivalent:
− kt0 k2 (a? − 1)2 ∼ −σ 2 ν 2 .

(25)

Finallye if we solve the quadratic Equation (19), we write δ? as a function of
the coefficients of the quadratic equation (19). We use the equivalent of each of
these terms thanks to equation (22) and (25), this proves proposition 4.4.

19

Remark 4.1. Thanks to inequality (24), if ktσ0 k &lt; ν2 , then kt0 k2 (1 − a? )2 ≥
(σν −2kt0 k)2 , then we write δ? as a function of the coefficients of Equation (19),
we obtain a lower bound of the inconsistency bias as a function of kt0 k, σ and
ν for σ > 2kt0 k/ν:
q
p
p
δ?
2
2
≥ −(1 + 1 + σ /kt0 k ) + (1 + 1 + σ 2 /kt0 k2 )2 + (σν/kt0 k − 2)2 .
kt0 k
Although the constant ν intervenes in this lower bound, it is not an explicit
term. We now explicit its behaviour depending on t0 . We remind that:


1
ν=
E sup hg, t0 i .
kt0 k
g∈G
To this end, we first note that the set of fixed points under the action of G is a
closed linear space, (because we can write it as an intersection of the kernel of
the continuous and linear functions: x 7→ g · x − x for all g ∈ G). We denote by
p the orthogonal projection on the set of fixed points Fix(M ). Then for x ∈ M ,
we have: dist(x, Fix(M )) = kx − p(x)k. Which yields:
hg, t0 i = hg, t0 − p(t0 )i + h, p(t0 )i .

(26)

The right hand side of Equation (26) does not depend on g as p(t0 ) ∈ Fix(M ).
Then:


kt0 kν = E sup hg, t0 − p(t0 )i + hE(), p(t0 )i .
g∈G

Applying the Cauchy-Schwarz inequality and using E() = 0, we can conclude
that:
ν≤

1
dist(t0 , Fix(M ))E(kk) = dist(t0 /kt0 k, Fix(M ))E(kk).
kt0 k

(27)

This leads to the following comment: our lower bound of the consistency bias is
smaller when our normalized template t0 /kt0 k is closer to the set of fixed points.

4.4

Upper bound of the consistency bias

In this Section, we find a upper bound of the consistency bias. More precisely
we have the following Theorem:
Proposition 4.5. Let X be a random variable in M , such that X = t0 + σ
where σ > 0, E() = 0 and E(||||2 ) = 1. We suppose that [m? ] is a Fréchet
mean of [X]. Then we have the following upper bound of the quotient distance
between the orbit of the template t0 and the Fréchet mean of [X]:
p
dQ ([m? ], [t0 ]) ≤ σν(m∗ −m0 )+ σ 2 ν(m∗ − m0 )2 + 2dist(t0 , Fix(M ))σν(m∗ − m0 ),
(28)
where we have noted ν(m) = E(supg hg, m/kmki) ∈ [0, 1] if m 6= 0 and
ν(0) = 0, and m0 the orthogonal projection of t0 on F ix(M ).
20

Note that we made no hypothesis on the template
pin this proposition. We
deduce from Equation (28) that √
dQ ([m? ], [t0 ]) ≤ σ + σ 2 + 2σdist(t0 , Fix(M ))
is a O(σ) when σ → ∞, but a O( σ) when σ → 0, in particular the consistency
bias can be neglected when σ is small.
Proof. First we have:
F (m? ) ≤ F (t0 ) = E(inf ||t0 − g(t0 + σ)||2 ) ≤ E(||σ||2 ) = σ 2 .
g

(29)

Secondly we have for all m ∈ M , (in particular for m? ):
F (m) =

E(inf (km − gt0 k2 + σ 2 kk2 − 2hgσ, m − gt0 i))

≥

dQ ([m], [t0 ])2 + σ 2 − 2E(suphσ, gmi).

g

(30)

g

With Inequalities (29) and (30) one gets:
dQ ([m∗ ], [t0 ])2 ≤ 2E(sup hσ, gm? i) = 2σν(m? )||m? ||,
g

note that at this point, if m? = 0 then E(supg hσ, gm? i) = 0 and ν(m? ) = 0
although Equation (4.4) is still true even if m? = 0. Moreover with the triangular
inequality applied at [m? ], [0] and [t0 ], one gets: km? k ≤ kt0 k + dQ ([m? ], [t0 ])
and then:
dQ ([m∗ ], [t0 ])2 ≤ 2σν(m? )(dQ ([m∗ ], [t0 ]) + kt0 k).
(31)
We can solve inequality (31) and we get:
p
dQ ([m? ], [t0 ]) ≤ σν(m? ) + σ 2 ν(m? )2 + 2kt0 kσν(m? ),

(32)

We note by FX instead of F the variance in the quotient space of [X], and we
want to apply inequality (32) to X − m0 . As m0 is a fixed point:


2
FX (m) = E inf kX − m0 − g · (m − m0 )k = FX−m0 (m − m0 )
g∈G

Then m? minimises FX if and only if m? − m0 minimises FX−m0 . We apply
Equation (32) to X − m0 , with E(X − m0 ) = t0 − m0 and [m? − m0 ] a Fréchet
mean of [X − m0 ]. We get:
p
dQ ([m? −m0 ], [t0 −m0 ]) ≤ σν(m∗ −m0 )+ σ 2 ν(m∗ − m0 )2 + 2kt0 − m0 kσν(m∗ − m0 ).
Moreover dQ ([m? ], [t0 ]) = dQ ([m? − m0 ], [t0 − m0 ]), which concludes the proof.

21

4.5

Empirical Fréchet mean

In practice, we never compute the Fréchet mean in quotient space, only the
empirical Fréchet mean in quotient space when the size of a sample is supposed
to be large enough. If the empirical Fréchet in the quotient space means converges to the Fréchet mean in the quotient space then we can not use these
empirical Fréchet mean in order to estimate the template. In [BB08], it has
been proved that the empirical Fréchet mean converges to the Fréchet mean
with a √1n convergence speed, however the law of the random variable is supposed to be included in a ball whose radius depends on the geometry on the
manifold. Here we are not in a manifold, indeed the quotient space contains
singularities, moreover we do not suppose that the law is necessarily bounded.
However in [Zie77] the empirical Fréchet means is proved to converge to the
Fréchet means but no convergence rate is provided.
We propose now to prove that the quotient distance between the template
and the empirical Fréchet mean in quotient space have an lower bound which
is the asymptotic of the one lower bound of the consistency bias found in (18).
Take X, X1 , . . . , Xn independent and identically distributed (with t0 = E(X)
not a fixed point). We define the empirical variance of [X] by:
n

m ∈ M 7→ Fn (m) =

n

1X
1X
dQ ([m], [Xi ])2 =
inf km − g · Xi k2 ,
n i=1
n i=1 g∈G

and we say that [mn? ] is a empirical Fréchet mean of [X] if mn? is a global
minimiser of Fn .
Proposition 4.6. Let X, X1 , . . . , Xn independent and identically distributed
random variables, with t0 = E(X). Let be [mn? ] be an empirical Fréchet mean
of [X]. Then δn is a lower bound of the quotient distance between the orbit of
the template and [mn? ], where δn is the unique positive solution of:
!
n
1X
2
kXi k δ − kt0 k2 (an? − 1)2 = 0.
δ + 2 ||t0 || +
n i=1
an? is defined like a? in section 4.1 by:
n
P
1
sup hg · Xi , t0 i
n
i=1g∈G
an? =
.
kt0 k2
We have that δn → δ by the law of large numbers.
The proof is a direct application of theorem 4.2, but applied to the empirical
law of X given by the realization of X1 , . . . , Xn .

4.6

Examples

In this Subsection, we discuss, in some examples, the application of theorem 4.1
and see the behaviour of the constant ν. This constant intervened in lower
bound of the consistency bias.
22

4.6.1

Action of translation on L2 (R/Z)

We take an orbit O = [f0 ], where f0 ∈ C 2 (R/Z), non constant. We show
easily that O is a manifold of dimension 1 and the tangent space at f0 is2
Rf00 . Therefore a sufficient condition on X such that E(X) = f0 to have an
inconsistency is: P(X ∈
/ f00⊥ ) > 0 according to proposition 4.3. Now if we
denote by 1 the constant function on R/Z equal to 1. We have in this setting:
that the set of fixed points under the action of G is the set of constant functions:
Fix(M ) = R1 and:
s
2
Z 1
Z 1
f0 (t) −
f0 (s)ds dt.
dist(f0 , Fix(M )) = kf0 − hf0 , 1i 1k =
0

0

This distance to the fixed points is used in the upper bound of the constant ν in
Equation (27). Note that if f0 is not differentiable, then [f0 ] is not necessarily
a manifold, and (4.3) does not apply. However proposition 4.1 does: if f0 is not
a constant function, then [f0 ] \ {f0 } is dense in [f0 ]. Therefore as soon as the
support of X contains a ball around f0 , there is an inconsistency.
4.6.2

Action of discrete translation on RZ/NZ

We come back on example 3.1, with D = 1 (discretised signals). For some signal
t0 , ν previously defined is:


1
ν=
E max h, τ · t0 i .
kt0 k
τ ∈Z/NZ
Therefore if we have a sample of size I of  iid, then:
ν=

I
1X
1
lim
max hi , τi · t0 i ,
kt0 k I→+∞ I i=1 τi ∈Z/N Z

By an exhaustive research, we can find the τi ’s which maximise the dot product, then with this sample and t0 we can approximate ν. We have done this
approximation for several signals t0 on fig. 6. According the previous results,
the bigger ν is, the more important the lower bound of the consistency bias is.
We remark that the ν estimated is small, ν  1 for different signals.
4.6.3

Action of rotations on Rn

Now we consider the action of rotations on Rn with a Gaussian noise. Take
X ∼ N (t0 , s2 Idn ) then the variability of X is ns2 , then X has a decomposition:
] − 21 , 12 [ →
O
is a local parametrisation of O: f0 = ϕ(0), and we
t
7→ f0 (. − t)
0
check that: lim kϕ(x) − ϕ(0) − xf0 kL2 = 0 with Taylor-Lagrange inequality at the order
2 Indeed

ϕ :

x→0

2. As a conclusion ϕ is differentiable at 0, and it is an immersion (since f00 6= 0), and
D0 ϕ : x 7→ xf00 , then O is a manifold of dimension 1 and the tangent space of O at f0 is:
Tf0 O = D0 ϕ(R) = Rf00 .

23

nu value for each signal
0.4
0.14456
0.082143
0.24981

0.3

0.2

0.1

0

-0.1

-0.2

-0.3

-0.4
0

0.2

0.4

0.6

0.8

1

Figure 6: Different signals and their ν approximated with a sample of size 103
in RZ/100Z .  is here a Gaussian noise in RZ/100Z , such that E() = 0 and
E(kk2 ) = 1. For instance the blue signal is a signal defined randomly, and
when we approximate the ν which corresponds to that t0 we find ' 0.25.
√
X = t0 + ns with E() = 0 and E(kk2 ) = 1. According to proposition 4.4
we have by noting δ? the lower bound of the consistency bias when s → ∞:
p
√
δ?
→ n(−1 + 1 + ν 2 ).
s
Now ν = E(supg∈G hg, t0 )i /kt0 k = E(kk) → 1 when n tends to infinity (expected value of the Chi distribution) we have that for n large enough:
√ √
δ?
' n( 2 − 1).
s→∞ s
We compare this result with the exact computation of the consistency bias
(noted here CB) made by Miolane et al. [MHP16], which writes with our current
notations:
CB √ Γ((n + 1)/2)
lim
= 2
.
s→∞ s
Γ(n/2)
lim

Using a standard Taylor expansion on the Gamma function, we have that for n
large enough:
CB √
lim
' n.
s→∞ s
As a conclusion, when the dimension of the space is large enough our lower
bound and the exact computation of the
√ bias have the same asymptotic behaviour. It differs only by the constant 2 − 1 ' 0.4 in our lower bound, 1 in
the work of Miolane et al. [MP15].

24

5

Fréchet means top and quotient spaces are not
consistent when the template is a fixed point

In this Section, we do not assume that the top space M is a vector space, but
rather a manifold. We need then to rewrite the generative model likewise: let
t0 ∈ M , and X any random variable of M such as t0 is a Fréchet mean of X.
Then Y = S · X is the observed variable where S is a random variable whose
value are in G. In this Section we make the assumption that the template t0 is
a fixed point under the action of G.

5.1

Result

Let X be a random variable on M and define the variance of X as:
E(m) = E(dM (m, X)2 ).
We say that t0 is a Fréchet mean of X if t0 is a global minimiser of the variance
E. We prove the following result:
Theorem 5.1. Assume that M is a complete finite dimensional Riemannian
manifold and that dM is the geodesic distance on M . Let X be a random variable
on M , with E(d(x, X)2 ) &lt; +∞ for some x ∈ M . We assume that t0 is a fixed
point and a Fréchet mean of X and that P(X ∈ C(t0 )) = 0 where C(t0 ) is the
cut locus of t0 . Suppose that there exists a point in the support of X which is
not a fixed point nor in the cut locus of t0 . Then [t0 ] is not a Fréchet mean of
[X].
The previous result is finite dimensional and does not cover interesting infinite dimensional setting concerning curves for instance. However, a simple
extension to the previous result can be stated when M is a Hilbert vector space
since then the space is flat and some technical problems like the presence of cut
locus point do not occur.
Theorem 5.2. Assume that M is a Hilbert space and that dM is given by the
Hilbert norm on M . Let X be a random variable on M , with E(kXk2 ) &lt; +∞.
We assume that t0 = E(X). Suppose that there exists a point in the support of
the law of X that is not a fixed point for the action of G. Then [t0 ] is not a
Fréchet mean of [X].
Note that the reciprocal is true: if all the points in the support of the law
of X are fixed points, then almost surely, for all m ∈ M and for all g ∈ G we
have:
dM (X, m) = dM (g · X, m) = dQ ([X], [m]).
Up to the projection on the quotient, we have that the variance of X is equal to
the variance of [X] in M/G, therefore [t0 ] is a Fréchet mean of [X] if and only
if t0 is a Fréchet mean of X. There is no inconsistency in that case.

25

Example 5.1. Theorem 5.2 covers the interesting case of the Fisher Rao metric
on functions:
F = {f : [0, 1] → R

|

f is absolutely continuous}.

Then considering for G the group of smooth diffeomorphisms γ on [0, 1] such
that γ(0) = 0 and γ(1) = 1, we have a right group action G × F → F given
by γ · f = f ◦ γ. The Fisher Rao metric is built as a pull back metric
q of the
2
2
˙
L ([0, 1], R) space through the map Q : F → L given by: Q(f ) = f / |f˙|. This
square root trick is often used, see for instance [KSW11]. Note that in this case,
Rt
Q is a bijective mapping with inverse given by q 7→ f with √
f (t) = 0 q(s)|q(s)|ds.
We can define a group action on M = L2 as: γ · q = q ◦ γ γ̇, for which one can
check easily by a change of variable that:
p
p
kγ · q − γ · q 0 k2 = kq ◦ γ γ̇ − q 0 ◦ γ γ̇k2 = kq − q 0 k2 .
So up to the mapping Q, the Fisher Rao metric on curve corresponds to the
situation M where theorem 5.2 applies. Note that in this case the set of fixed
points under the action of G corresponds in the space F to constant functions.
We can also provide an computation of the consistency bias in this setting:
Proposition 5.1. Under the assumptions of theorem 5.2, we write X = t0 + σ
where t0 is a fixed point, σ > 0, E() = 0 and E(kk2 ) = 1, if there is a Fréchet
mean of [X], then the consistency bias is linear with respect to σ and it is equal
to:
σ sup E(sup hv, g · i).
kvk=1

g∈G

Proof. For λ > 0 and kvk = 1, we compute the variance F in the quotient space
of [X] at the point t0 + λv. Since t0 is a fixed point we get:
F (t0 +λv) = E( inf kt0 +λv−gXk2 ) = E(kXk2 )−kt0 k2 −2λE(sup hv, g(X − t0 )i)+λ2 .
g∈G

g

Then we minimise F with respect to λ, and after we minimise with respect to
v (with kvk = 1). Which concludes.

5.2
5.2.1

Proofs of these theorems
Proof of theorem 5.1

We start with the following simple result, which aims to differentiate the variance
of X. This classical result (see [Pen06] for instance) is proved in appendix B in
order to be the more self-contained as possible:
Lemma 5.1. Let X a random variable on M such that E(d(x, X)2 ) &lt; +∞ for
some x ∈ M . Then the variance m 7→ E(m) = E(dM (m, X)2 ) is a continuous

26

function which is differentiable at any point m ∈ M such that P(X ∈ C(m)) = 0
where C(m) is the cut locus of m. Moreover at such point one has:
∇E(m) = −2E(logm (X)),
where logm : M \ C(m) → Tm M is defined for any x ∈ M \ C(m) as the unique
u ∈ Tm M such that expm (u) = x and kukm = dM (x, m).
We are now ready to prove theorem 5.1.
Proof. (of theorem 5.1) Let m0 be a point in the support of M which is not a
fixed point and not in the cut locus of t0 . Then there exists g0 ∈ G such that
m1 = g0 m0 6= m0 . Note that since x 7→ g0 x is a symmetry (the distance is
equivariant under the action of G) have that m1 = g0 m0 ∈
/ C(g0 t0 ) = C(t0 ) (t0
is a fixed point under the action of G). Let v0 = logt0 (m0 ) and v1 = logt0 (m1 ).
We have v0 6= v1 and since C(t0 ) is closed and the logt0 is continuous application
on M \ C(t0 ) we have:
lim

→0 P(X

1
E(1X∈B(m0 ,) logt0 (X)) = v0 .
∈ B(m0 , ))

(we use here the fact that since m0 is in the support of the law of X, P(X ∈
B(m0 , )) > 0 for any  > 0 so that the denominator does not vanish and the
fact that since M is a complete manifold, it is a locally compact space (the
closed balls are compacts) and logt0 is locally bounded). Similarly:
lim

→0 P(X

1
E(1X∈B(m0 ,) logt0 (g0 X)) = v1 .
∈ B(m0 , ))

Thus for sufficiently small  > 0 we have (since v0 6= v1 ):
E(logt0 (X)1X∈B(m0 ,) ) 6= E(logt0 (g0 X)1X∈B(m0 ,) ).

(33)

By using using a reductio ad absurdum, we suppose that [t0 ] is a Fréchet mean
of [X] and we want to find a contradiction with (33). In order to do that we
introduce simple functions as the function x 7→ 1x∈B(m0 ,) which intervenes in
Equation (33). Let s : M → G be a simple function (i.e. a measurable function
with finite number of values in G). Then x 7→ h(x) = s(x)x is a measurable
function3 . Now, let Es (x) = E(d(x, s(X)X)2 ) be the variance of the variable
s(X)X. Note that (and this is the main point):
∀g ∈ G
3 Indeed

if: s =

dM (t0 , x) = dM (gt0 , gx) = dM (t0 , gx) = dQ ([t0 ], [x]),
n
P

gi 1Ai where (Ai )1≤i≤n is a partition of M (such that the sum is always

i=1

defined). Then for any Borel set B ⊂ M we have: h−1 (B) =

n
S

gi−1 (B) ∩ Ai is a measurable

i=1

set since x 7→ gi x is a measurable function.

27

we have: Es (t0 ) = E(t0 ). Assume now that [t0 ] a Fréchet mean for [X] on the
quotient space and let us show that Es has a global minimum at t0 . Indeed for
any m, we have:
Es (m) = E(dM (m, s(X)X)2 ) ≥ E(dQ ([m], [X])2 ) ≥ E(dQ ([t0 ], [X])2 ) = Es (t0 ).
Now, we want to apply lemma 5.1 to the random variables s(X)X and X at the
point t0 . Since we assume that X ∈
/ C(t0 ) almost surely and X ∈
/ C(t0 ) implies
s(X)X ∈
/ C(t0 ) we get P(s(X)X ∈ C(t0 )) = 0 and the lemma 5.1 applies. As
t0 is a minimum, we already know that the differential of Es (respectively E)
at t0 should be zero. We get:
E(logt0 (X)) = E(logt0 (s(X)X)) = 0.

(34)

Now we apply Equation (34) to a particular simple function defined by s(x) =
g0 1x∈B(m0 ,) + eG 1x∈B(m
. We split the two expected values in (34) into two
/
0 ,)
parts:
E(logt0 (X)1X∈B(m0 ,) ) + E(logt0 (X)1X ∈B(m
) = 0,
(35)
/
0 ,)
) = 0.
E(logt0 (g0 X)1X∈B(m0 ,) ) + E(logt0 (X)1X ∈B(m
/
0 ,)

(36)

By substrating (35) from (36), one gets:
E(logt0 (X)1X∈B(m0 ,) ) = E(logt0 (g0 X)1X∈B(m0 ,) ),
which is a contradiction with (33). Which concludes.
5.2.2

Proof of theorem 5.2

Proof. The extension to theorem 5.2 is quite straightforward. In this setting
many things are now explicit since d(x, y) = kx − yk , ∇x d(x, y)2 = 2(x − y),
logx (y) = y − x and the cut locus is always empty. It is then sufficient to go
along the previous proof and to change the quantity accordingly. Note that the
local compactness of the space is not true in infinite dimension. However this
was only used to prove that the log was locally bounded but this last result is
trivial in this setting.

6

Conclusion and discussion

In this article, we exhibit conditions which imply that the template estimation
with the Fréchet mean in quotient space is inconsistent. These conditions are
rather generic. As a result, without any more information, a priori there is
inconsistency. The behaviour of the consistency bias is summarized in table 1.
Surely future works could improve these lower and upper bounds.
In a more general case: when we take an infinite-dimensional vector space
quotiented by a non isometric group action, is there always an inconsistency?
An important example of such action is the action of diffeomorphisms. Can we
estimate the consistency bias? In this setting, one estimates the template (or
28

Table 1: Behaviour of the consistency bias with respect to σ 2 the variability of
X = t0 + σ. The constants Ki ’s depend on the kind of noise, on the template
t0 and on the group action.
Consistency bias : CB
G is any group
Supplementary properties for
G a finite group
√
Upper bound of CB
CB ≤ σ + 2 σ 2 + K1 σ CB ≤ K2 σ (theorem 3.3)
(proposition 4.5)
Lower bound of CB for σ → ∞
CB ≥ L ∼ K3 σ (proposition 4.4)
σ→∞
when the template is not a fixed
point
√
Behavior of CB for σ → 0 when CB ≤ U ∼ K4 σ
CB = o(σ k ), ∀k ∈ N in the
σ→0
0
the template is not a fixed point
section 3.3, can we extend this
result for finite group?
CB = σ sup E(supg∈G hv, gi) (proposition 5.1)

CB when the template is a fixed
point

kvk=1

an atlas), but does not exactly compute the Fréchet mean in quotient space,
because a regularization term is added. In this setting, can we ensure that the
consistency bias will be small enough to estimate the original template? Otherwise, one has to reconsider the template estimation with stochastic algorithms
as in [AKT10] or develop new methods.

A

Proof of theorems for finite groups’ setting

A.1

Proof of theorem 3.2: differentiation of the variance
in the quotient space

In order to show theorem 3.2 we proceed in three steps. First we see some
following properties and definitions which will be used. Most of these properties
are the consequences of the fact that the group G is finite. Then we show that
the integrand of F is differentiable. Finally we show that we can permute
gradient and integral signs.
1. The set of singular points in Rn , is a null set (for the Lebesgue’s measure),
since it is equal to:
[
ker(x 7→ g · x − x),
g6=eG

a finite union of strict linear subspaces of Rn thanks to the linearity and
effectively of the action and to the finite group.
2. If m is regular, then for g, g 0 two different elements of G, we pose:
H(g · m, g 0 · m) = {x ∈ Rn , kx − g · mk = kx − g 0 · mk}.
Moreover H(g · m, g 0 · m) = (g · m − g 0 · m)⊥ is an hyperplane.
29

3. For m a regular point we define the set of points which are equally distant
from two different points of the orbit of m:
[
H(g · m, g 0 · m).
Am =
g6=g 0

Then Am is a null set. For m regular and x ∈
/ Am the minimum in the
definition of the quotient distance :
dQ ([m], [x]) = minkm − g · xk,
g∈G

(37)

is reached at a unique g ∈ G, we call g(x, m) this unique element.
4. By expansion of the squared norm: g minimises km − g · xk if and only if
g maximises hm, g · xi.
5. If m is regular and x ∈
/ Am then:
∀g ∈ G \ {g(x, m)}, km − g(x, m) · xk &lt; km − g · xk,
by continuity of the norm and by the fact that G is a finite group, we can
find α > 0, such that for µ ∈ B(m, α) and y ∈ B(x, α):
∀g ∈ G \ {g(x, m)} kµ − g(x, m) · yk &lt; kµ − g · yk.

(38)

Therefore for such y and µ we have:
g(x, m) = g(y, µ).
6. For m a regular point, we define Cone(m) the convex cone of Rn :
Cone(m) = {x ∈ Rn / ∀g ∈ G kx − mk ≤ kx − g · mk}

(39)

n

= {x ∈ R / ∀g ∈ G hm, xi ≥ hgm, xi}.
This is the intersection of |G| − 1 half-spaces: each half space is delimited
by H(m, gm) for g 6= eG (see fig. 1). Cone(m) is the set of points whose
projection on [m] is m, (where the projection of one point p on [m] is one
point g · m which minimises the set {kp − g · mk, g ∈ G}).
7. Taking a regular T
point m allows us to see the
T quotient. For every point x ∈
Rn we have: [x] Cone(m) 6= ∅, card([x] Cone(m)) ≥ 2 if and only if
x ∈ Am . The borders of the cone is Cone(m)\Int(Cone(m)) = Cone(m)∩
Am (we denote by Int(A) the interior of a part A). Therefore Q = Rn /G
can be seen like Cone(m) whose border have been glued together.
The proof of theorem 3.2 is the consequence of the following lemmas. The
first lemma studies the differentiability of the integrand, and the second allows
us to permute gradient and integral sign. Let us denote by f the integrand of
F:
30

∀ m, x ∈ M

f (x, m) = minkm − g · xk2 .

(40)

g∈G

Thus we have: F (m) = E(f (X, m)). The min of differentiable functions is not
necessarily differentiable, however we prove the following result:
Lemma A.1. Let m0 be a regular point, if x ∈
/ Am0 then m 7→ f (x, m) is
differentiable at m0 , besides we have:
∂f
(x, m0 ) = 2(m0 − g(x, m0 ) · x)
∂m

(41)

Proof. If m0 is regular and x ∈
/ Am0 then we know from the item 5 of the
appendix A.1 that g(x, m0 ) is locally constant. Therefore around m0 , we have:
f (x, m) = km − g(x, m0 ) · xk2 ,
which can differentiate with respect to m at m0 . This proves the lemma A.1.
Now we want to prove that we can permute the integral and the gradient
sign. The following lemma provides us a sufficient condition to permute integral
and differentiation signs thanks to the dominated convergence theorem:
Lemma A.2. For every m0 ∈ M we have the existence of an integrable function
Φ : M → R+ such that:
∀m ∈ B(m0 , 1), ∀x ∈ M

|f (x, m0 ) − f (x, m)| ≤ km − m0 kΦ(x).

(42)

Proof. For all g ∈ G, m ∈ M we have:
kg · x − m0 k2 − kg · x − mk2 = hm − m0 , 2g · x − (m0 + m)i
≤ km − m0 k × (km0 + mk + k2xk)
2

minkg · x − m0 k ≤ km − m0 k (km0 + mk + k2xk) + kg · x − mk2
g∈G

minkg · x − m0 k2 ≤ km − m0 k (km0 + mk + k2xk) + minkg · x − mk2
g∈G

2

g∈G

2

minkg · x − m0 k − minkg · x − mk ≤ km − m0 k (2km0 k + km − m0 k + k2xk)
g∈G

g∈G

By symmetry we get also the same control of f (x, m) − f (x, m0 ), then:
|f (x, m0 ) − f (x, m)| ≤ km0 − mk (2km0 k + km − m0 k + k2xk)

(43)

The function Φ should depend on x or m0 , but not on m. That is why we take
only m ∈ B(m0 , 1), then we replace km−m0 k by 1 in (43), which concludes.

31

A.2

Proof of theorem 3.1: the gradient is not zero at the
template

To prove it, we suppose that ∇F (t0 ) = 0, and we take the dot product with t0 :
h∇F (t0 ), t0 i = 2E(hX, t0 i − hg(X, t0 ) · X, t0 i) = 0.

(44)

The item 4 of (x, m) 7→ g(x, m) seen at appendix A.1 leads to:
hX, t0 i − hg(X, t0 ) · X, t0 i ≤ 0 almost surely.
So the expected value of a non-positive random variable is null. Then
hX, t0 i − hg(X, t0 ) · X, t0 i = 0 almost surely hX, t0 i = hg(X, t0 ) · X, t0 i almost surely.
Then g = eG maximizes the dot product almost surely. Therefore (as we know
that g(X, t0 ) is unique almost surely, since t0 is regular):
g(X, t0 ) = eG almost surely,
which is a contradiction with Equation (6).

A.3

Proof of theorem 3.3: upper bound of the consistency
bias

In order to show this Theorem, we use the following lemma:
Lemma A.3. We write X = t0 + where E() = 0 and we make the assumption
that the noise  is a subgaussian random variable. This means that it exists c > 0
such that:

 2
s kmk2
.
(45)
∀m ∈ M = Rn , E(exp(h, mi)) ≤ c exp
2
If for m ∈ M we have:
p
ρ̃ := dQ ([m], [t0 ]) ≥ s 2 log(c|G|),

(46)

p
ρ̃2 − ρ̃s 8 log(c|G|) ≤ F (m) − E(kk2 ).

(47)

then we have:
Proof. (of lemma A.3) First we expand the right member of the inequality (47):


E(kk2 ) − F (m) = E max(kX − t0 k2 − kX − gmk2 )
g∈G

We use the formula kAk2 − kA + Bk2 = −2 hA, Bi − kBk2 with A = X − t0 and
B = t0 − gm:



E(kk2 ) − F (m) = E max −2 hX − t0 , t0 − gmi − kt0 − gmk2 = E(max ηg ),
g∈G

g∈G

(48)
32

with ηg = −kt0 − gmk2 + 2 h, gm − t0 i. Our goal is to find a lower bound of
F (m) − E(kk2 ), that is why we search an upper bound of E(maxηg ) with the
g∈G

Jensen’s inequality. We take x > 0 and we get by using the assumption (45):


X
exp(xE(max ηg )) ≤ E(exp(max xηg )) ≤ E 
exp(xηg )
g∈G

g∈G

≤

X

g∈G
2

exp(−xkt0 − gmk )E(exp(h, 2x(gm − t0 )i)

g

X
≤c
exp(−xkt0 − gmk2 ) exp(2s2 x2 kgm − t0 k2 )
g

X
≤c
exp(kgm − t0 k2 (−x + 2x2 s2 ))

(49)

g

Now if (−x + 2t2 x2 ) &lt; 0, we can take an upper bound of the sum sign in (49)
by taking the smallest value in the sum sign, which is reached when g minimizes
kg · m − t0 k multiplied by the number of elements summed. Moreover (−x +
2x2 s) &lt; 0 ⇐⇒ 0 &lt; x &lt; 2s12 . Then we have:
exp(xE(max ηg )) ≤ c|G| exp(ρ̃2 (−x + 2x2 s2 )) as soon as 0 &lt; x &lt;
g∈G

1
.
2s2

Then by taking the log:
E(maxηg ) ≤
g∈G

log c|G|
+ (2xs2 − 1)ρ̃2 .
x

(50)

Now we find the x which optimizes inequality (50).p By differentiation, the
right member of inequality (50) is minimal for x? = log c|G|/2/(sρ̃) which is
a valid choice because x? ∈ (0, 2s12 ) by using the assumption (46). With the
equations (48) and (50) and x? we get the result.
Proof. (of theorem 3.3) We take m? ∈ argmin F , ρ̃ = dQ ([m? ], [t0 ]), and  =
2
2
X − tp
0 . We have: F (m? ) ≤ F (t0 ) ≤ E(kk ) then F (m? ) − E(kk ) ≤ 0. If
ρ̃ > s 2 log(|G|) then we can apply lemma A.3 with c = 1. Thus:
p
ρ̃2 − ρ̃s 8 log(|G|) ≤ 2F (m? ) − E(kk2 ) ≤ 0,
p
p
which yields to ρ̃ ≤ s 8 log(|G|). If ρ̃ ≤ s 2 log(|G|), we have nothing to
prove.
Note that the proof of this upper bound does not use the fact that the action
is isometric, therefore this upper bound is true for every finite group action.

33

A.4

Proof of proposition 3.2: inconsistency in R2 for the
action of translation

Proof. We suppose that E(X) ∈ HPA ∪ L. In this setting we call τ (x, m) one
of element of the group G = T which minimises kτ · x − mk see (37) instead of
g(x, m). The variance in the quotient space at the point m is:


F (m) = E min kτ · X − mk2 = E(kτ (X, m) · X − mk2 ).
τ ∈Z/2Z

As we want to minimize F and F (1 · m) = F (m), we can suppose that m ∈
HPA ∪ L. We can completely write what take τ (x, m) for x ∈ M :
• If x ∈ HPA ∪ L we can set τ (x, m) = 0 (because in this case x, m are on
the same half plane delimited by L the perpendicular bisector of m and
−m).
• If x ∈ HPB then we can set τ (x, m) = 1 (because in this case x, m are
not on the same half plane delimited by L the perpendicular bisector of
m and −m).
This allows use to write the variance at the point m ∈ HPA :


F (m) = E kX − mk2 1{X∈HPA ∪L} + E k1 · X − mk2 1{X∈HPB }
Then we define the random variable Z by: Z = X1X∈HPA ∪L + 1 · X1X∈HPB ,
such that for m ∈ HPA we have: F (m) = E(kZ − mk2 ) and F (m) = F (1 · m).
Thus if m? is a global minimiser of F , then m? = E(Z) or m? = 1 · E(Z). So the
Fréchet mean of [X] is [E(Z)]. Here instead of using theorem 3.1, we can work
explicitly: Indeed there is no inconsistency if and only if E(Z) = E(X), (E(Z) =
1 · E(X) would be another possibility, but by assumption E(Z), E(X) ∈ HPA ),
by writing X = X1X∈HPA + X1X∈HPB ∪L , we have:
E(Z) = E(X) ⇐⇒ E(1 · X1X∈HPB ∪L ) = E(X1X∈HPB ∪L )
⇐⇒ 1 · E(X1X∈HPB ∪L ) = E(X1X∈HPB ∪L )
⇐⇒ E(X1X∈HPB ∪L ) ∈ L
⇐⇒ P(X ∈ HPB ) = 0,
Therefore there is an inconsistency if and only if P(X ∈ HPB ) > 0 (we remind
that we made the assumption that E(X) ∈ HPA ∪ L). If E(X) is regular (i.e.
E(X) ∈
/ L), then there is an inconsistency if and only if X takes values in HPB ,
(this is exactly the condition of theorem 3.1, but in this particular case, this is
a necessarily and sufficient condition). This proves point 1. Now we make the
assumption that X follows a Gaussian noise in order compute E(Z) (note that
we could take another noise, as long as we are able to compute E(Z)). For that
we convert to polar coordinates: (u, v)T = E(X) + (r cos θ, r sin θ)T where r > 0
et θ ∈ [0, 2π]. We also define: d = dist(E(X), L), E(X) is a regular point if

34

and only if d > 0. We still suppose that E(X) = (α, β)T ∈ HPA ∪ L. First we
parametrise in function of (r, θ) the points which are in HPB :
v &lt; u ⇐⇒ β + r sin θ &lt; α + r cos θ ⇐⇒

β−α √
π
&lt; 2 cos(θ + )
r
4

d
π
&lt; cos(θ + )
r h
4
i
π
π
⇐⇒ θ ∈ − − arccos(d/r), − + arccos(d/r) and d &lt; r
4
4
⇐⇒

Then we compute E(Z):
E(Z) =E(X1X∈HPA ) + E(1 · X1X∈HPB )


 exp − r2
Z d Z 2π 
2s2
α + r cos θ
rdθdr
E(Z) =
2
β
+
r
sin
θ
2πs
0
0


 exp − r2
Z +∞ Z 2π− π4 −arccos( dr ) 
2
2s
α + r cos θ
rdrdθ
+
2
β
+
r
sin
θ
d
π
2πs
arccos( r )− 4
d
 2
r

Z +∞ Z − π4 +arccos( dr ) 
β + r sin θ exp − 2s2
+
rdrdθ
α + r cos θ
d
2πs2
d
−π
4 −arccos( r )
 
Z +∞ 2
r2 √
r exp(− 2s
d
2)
=E(X) +
2g
dr × (−1, 1)T ,
2
πs
r
d
We compute ρ̃ = dQ ([E(X)], [E(Z)]) where dQ is the distance in the quotient
space defined in (1). As we know that E(X), E(Z) are in the same half-plane
delimited by L, we have: ρ̃ = dQ ([E(Z)], [E(X)]) = kE(Z) − E(X)k. This proves
eq. (9), note that items 2a to 2c are the direct consequence of eq. (9) and basic
analysis.

B

Proof of lemma 5.1: differentiation of the variance in the top space

Proof. By triangle inequality it is easy to show that E is finite and continuous
everywhere. Moreover, it is a well known fact that x 7→ dM (x, z)2 is differentiable at any m ∈ M \ C(z) (i.e. z ∈
/ C(m)) with derivative −2 logm (z). Now
since:
|dM (x, z)2 − dM (y, z)2 | = |dM (x, z) − dM (y, z)kdM (x, z) + dM (y, z)|
≤ dM (x, y)(2dM (x, z) + dM (y, x)),
we get in a local chart φ : U → V ⊂ Rn at t = φ(m) we have locally around t
that:
h 7→ dM (φ−1 (t), φ−1 (t + h)),
35

is smooth and |dM (φ−1 (t), φ−1 (t+h))| ≤ C|h| for a C > 0. Hence for sufficiently
small h, |dM (φ−1 (t), z)2 − dM (φ−1 (t + h), z)2 | ≤ C|h|(2dM (m, z) + 1). We get
the result from dominated convergence Lebesgue theorem with E(dM (m, X)) ≤
E(dM (m, X)2 + 1) &lt; +∞.

References
[AAT07]

Stéphanie Allassonnière, Yali Amit, and Alain Trouvé. Towards a
coherent statistical framework for dense deformable template estimation. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 69(1):3–29, 2007.

[ADP15]

Stéphanie Allassonnière, Loïc Devilliers, and Xavier Pennec. Estimating the template in the total space with the fréchet mean on
quotient spaces may have a bias: a case study on vector spaces quotiented by the group of translations. In Mathematical Foundations
of Computational Anatomy (MFCA’15), 2015.

[AKT10]

Stéphanie Allassonnière, Estelle Kuhn, and Alain Trouvé. Construction of bayesian deformable models via a stochastic approximation
algorithm: a convergence study. Bernoulli, 16(3):641–678, 2010.

[BB08]

Abhishek Bhattacharya and Rabi Bhattacharya. Statistics on riemannian manifolds: asymptotic distribution and curvature. Proceedings of the American Mathematical Society, 136(8):2959–2967,
2008.

[BC11]

Jérémie Bigot and Benjamin Charlier. On the consistency of fréchet
means in deformable models for curve and image analysis. Electronic
Journal of Statistics, 5:1054–1089, 2011.

[BG14]

Dominique Bontemps and Sébastien Gadat. Bayesian methods
for the shape invariant model. Electronic Journal of Statistics,
8(1):1522–1568, 2014.

[CWS16]

Jason Cleveland, Wei Wu, and Anuj Srivastava. Norm-preserving
constraint in the fisher–rao registration and its application in signal estimation. Journal of Nonparametric Statistics, 28(2):338–359,
2016.

[DPC+ 14] Stanley Durrleman, Marcel Prastawa, Nicolas Charon, Julie R Korenberg, Sarang Joshi, Guido Gerig, and Alain Trouvé. Morphometry of anatomical shape complexes with dense deformations and
sparse parameters. NeuroImage, 101:35–49, 2014.
[Fré48]

Maurice Fréchet. Les elements aléatoires de nature quelconque dans
un espace distancié. In Annales de l’institut Henri Poincaré, volume 10, pages 215–310, 1948.
36

[GM98]

Ulf Grenander and Michael I. Miller. Computational anatomy: An
emerging discipline. Q. Appl. Math., LVI(4):617–694, December
1998.

[HCG+ 13] Sebastian Hitziger, Maureen Clerc, Alexandre Gramfort, Sandrine
Saillet, Christian Bénar, and Théodore Papadopoulo. Jitter-adaptive
dictionary learning-application to multi-trial neuroelectric signals.
arXiv preprint arXiv:1301.3611, 2013.
[JDJG04] Sarang Joshi, Brad Davis, Mathieu Jomier, and Guido Gerig. Unbiased diffeomorphic atlas construction for computational anatomy.
Neuroimage, 23:S151–S160, 2004.
[Kar77]

Hermann Karcher. Riemannian center of mass and mollifier smoothing. Communications on pure and applied mathematics, 30(5):509–
541, 1977.

[Ken89]

David G Kendall. A survey of the statistical theory of shape. Statistical Science, pages 87–99, 1989.

[Ken90]

Wilfrid S Kendall. Probability, convexity, and harmonic maps with
small image i: uniqueness and fine existence. Proceedings of the
London Mathematical Society, 3(2):371–406, 1990.

[KSW11]

Sebastian A. Kurtek, Anuj Srivastava, and Wei Wu. Signal estimation under random time-warpings and nonlinear signal alignment. In J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and
K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 675–683. Curran Associates, Inc., 2011.

[LAJ+ 12] Sidonie Lefebvre, Stéphanie Allassonnière, Jérémie Jakubowicz,
Thomas Lasne, and Eric Moulines. Aircraft classification with a
low resolution infrared sensor. Machine Vision and Applications,
24(1):175–186, 2012.
[MHP16]

Nina Miolane, Susan Holmes, and Xavier Pennec. Template
shape estimation: correcting an asymptotic bias. arXiv preprint
arXiv:1610.01502, 2016.

[MP15]

Nina Miolane and Xavier Pennec. Biased estimators on quotient
spaces. In Geometric Science of Information. Second International
Conference, GSI 2015, Palaiseau, France, October 28-30, 2015, Proceedings, volume 9389. Springer, 2015.

[Pen06]

Xavier Pennec. Intrinsic statistics on riemannian manifolds: Basic
tools for geometric measurements. Journal of Mathematical Imaging
and Vision, 25(1):127–154, 2006.

37

[SBG08]

Mert Sabuncu, Serdar K. Balci, and Polina Golland. Discovering
modes of an image population through mixture modeling. Proceeding
of the MICCAI conference, LNCS(5242):381–389, 2008.

[Zie77]

Herbert Ziezold. On expected figures and a strong law of large numbers for random elements in quasi-metric spaces. In Transactions of
the Seventh Prague Conference on Information Theory, Statistical
Decision Functions, Random Processes and of the 1974 European
Meeting of Statisticians, pages 591–602. Springer, 1977.

[ZSF13]

Miaomiao Zhang, Nikhil Singh, and P.Thomas Fletcher. Bayesian
estimation of regularization and atlas building in diffeomorphic image registration. In JamesC. Gee, Sarang Joshi, KilianM. Pohl,
WilliamM. Wells, and Lilla Zöllei, editors, Information Processing
in Medical Imaging, volume 7917 of Lecture Notes in Computer Science, pages 37–48. Springer Berlin Heidelberg, 2013.

38

</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">10</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="1"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for
Real-time Embedded Object Detection

arXiv:1802.06488v1 [] 19 Feb 2018

Alexander Wong, Mohammad Javad Shafiee, Francis Li, Brendan Chwyl
Dept. of Systems Design Engineering
University of Waterloo, DarwinAI
{a28wong, mjshafiee}@uwaterloo.ca, {francis, brendan}@darwinai.ca

Abstract—Object detection is a major challenge in computer
vision, involving both object classification and object localization within a scene. While deep neural networks have been
shown in recent years to yield very powerful techniques for
tackling the challenge of object detection, one of the biggest
challenges with enabling such object detection networks for
widespread deployment on embedded devices is high computational and memory requirements. Recently, there has been
an increasing focus in exploring small deep neural network
architectures for object detection that are more suitable for embedded devices, such as Tiny YOLO and SqueezeDet. Inspired
by the efficiency of the Fire microarchitecture introduced in
SqueezeNet and the object detection performance of the singleshot detection macroarchitecture introduced in SSD, this paper
introduces Tiny SSD, a single-shot detection deep convolutional
neural network for real-time embedded object detection that
is composed of a highly optimized, non-uniform Fire subnetwork stack and a non-uniform sub-network stack of highly
optimized SSD-based auxiliary convolutional feature layers
designed specifically to minimize model size while maintaining
object detection performance. The resulting Tiny SSD possess
a model size of 2.3MB (∼26X smaller than Tiny YOLO) while
still achieving an mAP of 61.3% on VOC 2007 (∼4.2% higher
than Tiny YOLO). These experimental results show that very
small deep neural network architectures can be designed for
real-time object detection that are well-suited for embedded
scenarios.
Keywords-object detection; deep neural network; embedded;
real-time; single-shot

I. I NTRODUCTION
Object detection can be considered a major challenge in
computer vision, as it involves a combination of object classification and object localization within a scene (see Figure 1).
The advent of modern advances in deep learning [7], [6]
has led to significant advances in object detection, with the
majority of research focuses on designing increasingly more
complex object detection networks for improved accuracy
such as SSD [9], R-CNN [1], Mask R-CNN [2], and other
extended variants of these networks [4], [8], [15]. Despite the
fact that such object detection networks have showed stateof-the-art object detection accuracies beyond what can be
achieved by previous state-of-the-art methods, such networks
are often intractable for use for embedded applications due
to computational and memory constraints. In fact, even faster
variants of these networks such as Faster R-CNN [13] are only

Figure 1. Tiny SSD results on the VOC test set. The bounding boxes,
categories, and confidences are shown.

capable of single-digit frame rates on a high-end graphics
processing unit (GPU). As such, more efficient deep neural
networks for real-time embedded object detection is highly
desired given the large number of operational scenarios that
such networks would enable, ranging from smartphones to
aerial drones.
Recently, there has been an increasing focus in exploring
small deep neural network architectures for object detection
that are more suitable for embedded devices. For example,
Redmon et al. introduced YOLO [11] and YOLOv2 [12],
which were designed with speed in mind and was able to
achieve real-time object detection performance on a high-end
Nvidia Titan X desktop GPU. However, the model size of
YOLO and YOLOv2 remains very large in size (753 MB and
193 MB, respectively), making them too large from a memory
perspective for most embedded devices. Furthermore, their
object detection speed drops considerably when running on
embedded chips [14]. To address this issue, Tiny YOLO [10]
was introduced where the network architecture was reduced
considerably to greatly reduce model size (60.5 MB) as well
as greatly reduce the number of floating point operations
required (just 6.97 billion operations) at a cost of object
detection accuracy (57.1% on the twenty-category VOC 2017
test set). Similarly, Wu et al. introduced SqueezeDet [16], a
fully convolutional neural network that leveraged the efficient
Fire microarchitecture introduced in SqueezeNet [5] within
an end-to-end object detection network architecture. Given
that the Fire microarchitecture is highly efficient, the resulting
SqueezeDet had a reduced model size specifically for the

purpose of autonomous driving. However, SqueezeDet has
only been demonstrated for objection detection with limited
object categories (only three) and thus its ability to handle
larger number of categories have not been demonstrated.
As such, the design of highly efficient deep neural network
architectures that are well-suited for real-time embedded
object detection while achieving improved object detection
accuracy on a variety of object categories is still a challenge
worth tackling.
In an effort to achieve a fine balance between object
detection accuracy and real-time embedded requirements
(i.e., small model size and real-time embedded inference
speed), we take inspiration by both the incredible efficiency
of the Fire microarchitecture introduced in SqueezeNet [5]
and the powerful object detection performance demonstrated
by the single-shot detection macroarchitecture introduced
in SSD [9]. The resulting network architecture achieved
in this paper is Tiny SSD, a single-shot detection deep
convolutional neural network designed specifically for realtime embedded object detection. Tiny SSD is composed
of a non-uniform highly optimized Fire sub-network stack,
which feeds into a non-uniform sub-network stack of highly
optimized SSD-based auxiliary convolutional feature layers,
designed specifically to minimize model size while retaining
object detection performance.
This paper is organized as follows. Section 2 describes the
highly optimized Fire sub-network stack leveraged in the Tiny
SSD network architecture. Section 3 describes the highly
optimized sub-network stack of SSD-based convolutional
feature layers used in the Tiny SSD network architecture.
Section 4 presents experimental results that evaluate the
efficacy of Tiny SSD for real-time embedded object detection.
Finally, conclusions are drawn in Section 5.
II. O PTIMIZED F IRE S UB - NETWORK S TACK
The overall network architecture of the Tiny SSD network
for real-time embedded object detection is composed of two
main sub-network stacks: i) a non-uniform Fire sub-network
stack, and ii) a non-uniform sub-network stack of highly
optimized SSD-based auxiliary convolutional feature layers,
with the first sub-network stack feeding into the second subnetwork stack. In this section, let us first discuss in detail
the design philosophy behind the first sub-network stack
of the Tiny SSD network architecture: the optimized fire
sub-network stack.
A powerful approach to designing smaller deep neural
network architectures for embedded inference is to take a
more principled approach and leverage architectural design
strategies to achieve more efficient deep neural network
microarchitectures [3], [5]. A very illustrative example of
such a principled approach is the SqueezeNet [5] network architecture, where three key design strategies were leveraged:
1) reduce the number of 3 × 3 filters as much as possible,

Figure 2. An illustration of the Fire microarchitecture. The output of
previous layer is squeezed by a squeeze convolutional layer of 1 × 1 filters,
which reduces the number of input channels to 3 × 3 filters. The result of
the squeeze convolutional layers is passed into the expand convolutional
layer which consists of both 1 × 1 and 3 × 3 filters.

2) reduce the number of input channels to 3 × 3 filters
where possible, and
3) perform downsampling at a later stage in the network.
This principled designed strategy led to the design of what
the authors referred to as the Fire module, which consists of
a squeeze convolutional layer of 1x1 filters (which realizes
the second design strategy of effectively reduces the number
of input channels to 3 × 3 filters) that feeds into an expand
convolutional layer comprised of both 1 × 1 filters and 3 × 3
filters (which realizes the first design strategy of effectively
reducing the number of 3 × 3 filters). An illustration of the
Fire microarchitecture is shown in Figure 2.
Inspired by the elegance and simplicity of the Fire
microarchitecture design, we design the first sub-network
stack of the Tiny SSD network architecture as a standard
convolutional layer followed by a set of highly optimized
Fire modules. One of the key challenges to designing this
sub-network stack is to determine the ideal number of Fire
modules as well as the ideal microarchitecture of each of
the Fire modules to achieve a fine balance between object
detection performance and model size as well as inference
speed. First, it was determined empirically that 10 Fire
modules in the optimized Fire sub-network stack provided
strong object detection performance. In terms of the ideal
microarchitecture, the key design parameters of the Fire
microarchitecture are the number of filters of each size
(1 × 1 or 3 × 3) that form this microarchitecture. In the
SqueezeNet network architecture that first introduced the
Fire microarchitecture [5], the microarchitectures of the Fire
modules are largely uniform, with many of the modules
sharing the same microarchitecture configuration. In an effort
to achieve more optimized Fire microarchitectures on a permodule basis, the number of filters of each size in each Fire

Table I
T HE OPTIMIZED F IRE SUB - NETWORK STACK OF THE T INY SSD
NETWORK ARCHITECTURE . T HE NUMBER OF FILTERS AND INPUT SIZE TO
EACH LAYER ARE REPORTED FOR THE CONVOLUTIONAL LAYERS AND

F IRE MODULES . E ACH F IRE MODULE IS REPORTED IN ONE ROW FOR A
BETTER REPRESENTATION . ”x@S – y@E1 – z@E3" STANDS FOR x
NUMBERS OF 1 × 1 FILTERS IN THE SQUEEZE CONVOLUTIONAL LAYER , y
NUMBERS OF 1 × 1 FILTERS AND z NUMBERS OF 3 × 3 FILTERS IN THE
EXPAND CONVOLUTIONAL LAYER .

Type / Stride
Conv1 / s2
Pool1 / s2
Fire1
Fire2

Figure 3.
An illustration of the network architecture of the second
sub-network stack of Tiny SSD. The output of three Fire modules and
two auxiliary convolutional feature layers, all with highly optimized
microarchitecture configurations, are combined together for object detection.

module is optimized to have as few parameters as possible
while still maintaining the overall object detection accuracy.
As a result, the optimized Fire sub-network stack in the Tiny
SSD network architecture is highly non-uniform in nature for
an optimal sub-network architecture configuration. Table I
shows the overall architecture of the highly optimized Fire
sub-network stack in Tiny SSD, and the number of parameters
in each layer of the sub-network stack.
III. O PTIMIZED S UB - NETWORK S TACK OF SSD- BASED
C ONVOLUTIONAL F EATURE L AYERS
In this section, let us first discuss in detail the design
philosophy behind the second sub-network stack of the Tiny
SSD network architecture: the sub-network stack of highly
optimized SSD-based auxiliary convolutional feature layers.
One of the most widely-used and effective object detection
network macroarchitectures in recent years has been the
single-shot multibox detection (SSD) macroarchitecture [9].
The SSD macroarchitecture augments a base feature extraction network architecture with a set of auxiliary convolutional
feature layers and convolutional predictors. The auxiliary
convolutional feature layers are designed such that they
decrease in size in a progressive manner, thus enabling the
flexibility of detecting objects within a scene across different
scales. Each of the auxiliary convolutional feature layers
can then be leveraged to obtain either: i) a confidence score
for a object category, or ii) a shape offset relative to default
bounding box coordinates [9]. As a result, a number of object
detections can be obtained per object category in this manner
in a powerful, end-to-end single-shot manner.
Inspired by the powerful object detection performance
and multi-scale flexibility of the SSD macroarchitecture [9],
the second sub-network stack of Tiny SSD is comprised of
a set of auxiliary convolutional feature layers and convo-

Pool3 / s2
Fire3
Fire4
Pool5 / s2
Fire5
Fire6
Fire7
Fire8
Pool9 / s2
Fire 9
Pool10 / s2
Fire10

Filter Shapes
3 × 3 × 57
3×3
15@S – 49@E1 – 53@E3
Concat1
15@S – 54@E1 – 52@E3
Concat2
3×3
29@S – 92@E1 – 94@E3
Concat3
29@S – 90@E1 – 83@E3
Concat4
3×3
44@S – 166@E1 – 161@E3
Concat5
45@S – 155@E1 – 146@E3
Concat6
49@S – 163@E1 – 171@E3
Concat7
25@S – 29@E1 – 54@E3
Concat8
3×3
37@S – 45@E1 – 56@E3
Concat9
3×3
38@S – 41@E1 – 44@E3
Concat10

Input Size
300 × 300
149 × 149
74 × 74
74 × 74
74 × 74
37 × 37
37 × 37
37 × 37
18 × 18
18 × 18
18 × 18
18 × 18
18 × 18
9×9

4×4

lutional predictors with highly optimized microarchitecture
configurations (see Figure 3).
As with the Fire microarchitecture, a key challenge to
designing this sub-network stack is to determine the ideal
microarchitecture of each of the auxiliary convolutional
feature layers and convolutional predictors to achieve a fine
balance between object detection performance and model
size as well as inference speed. The key design parameters
of the auxiliary convolutional feature layer microarchitecture
are the number of filters that form this microarchitecture.
As such, similar to the strategy taken for constructing
the highly optimized Fire sub-network stack, the number
of filters in each auxiliary convolutional feature layer is
optimized to minimize the number of parameters while
preserving overall object detection accuracy of the full Tiny
SSD network. As a result, the optimized sub-network stack
of auxiliary convolutional feature layers in the Tiny SSD
network architecture is highly non-uniform in nature for
an optimal sub-network architecture configuration. Table II
shows the overall architecture of the optimized sub-network
stack of the auxiliary convolutional feature layers within the
Tiny SSD network architecture, along with the number of

Table II
T HE OPTIMIZED SUB - NETWORK STACK OF THE AUXILIARY
CONVOLUTIONAL FEATURE LAYERS WITHIN THE T INY SSD NETWORK
ARCHITECTURE . T HE INPUT SIZES TO EACH CONVOLUTIONAL LAYER
AND KERNEL SIZES ARE REPORTED .

Type / Stride
Conv12-1 / s2
Conv12-2
Conv13-1
Conv13-2
Fire5-mbox-loc
Fire5-mbox-conf
Fire9-mbox-loc
Fire9-mbox-conf
Fire10-mbox-loc
Fire10-mbox-conf
Fire11-mbox-loc
Fire11-mbox-conf
Conv12-2-mbox-loc
Conv12-2-mbox-conf
Conv13-2-mbox-loc
Conv13-2-mbox-conf

Filter Shape
3 × 3 × 51
3 × 3 × 46
3 × 3 × 55
3 × 3 × 85
3 × 3 × 16
3 × 3 × 84
3 × 3 × 24
3 × 3 × 126
3 × 3 × 24
3 × 3 × 126
3 × 3 × 24
3 × 3 × 126
3 × 3 × 24
3 × 3 × 126
3 × 3 × 16
3 × 3 × 84

Input Size
4×4
4×4
2×2
2×2
37 × 37
37 × 37
18 × 18
18 × 18
9×9
9×9
4×4
4×4
2×2
2×2
1×1
1×1

parameters in each layer.

Model
size
60.5MB
2.3MB

mAP
(VOC 2007)
57.1%
61.3%

Table IV
R ESOURCE USAGE OF T INY SSD.

Model
Name
Tiny SSD

V. E XPERIMENTAL R ESULTS AND D ISCUSSION
To study the utility of Tiny SSD for real-time embedded object detection, we examine the model size, object
detection accuracies, and computational operations on the
VOC2007/2012 datasets. For evaluation purposes, the Tiny
YOLO network [10] was used as a baseline reference comparison given its popularity for embedded object detection,
and was also demonstrated to possess one of the smallest
model sizes in literature for object detection on the VOC
2007/2012 datasets (only 60.5MB in size and requiring
just 6.97 billion operations). The VOC2007/2012 datasets
consist of natural images that have been annotated with 20
different types of objects, with illustrative examples shown
in Figure 4. The tested deep neural networks were trained
using the VOC2007/2012 training datasets, and the mean
average precision (mAP) was computed on the VOC2007
test dataset to evaluate the object detection accuracy of the
deep neural networks.
A. Training Setup

Table III
O BJECT DETECTION ACCURACY RESULTS OF T INY SSD ON VOC 2007
TEST SET. T INY YOLO RESULTS ARE PROVIDED AS A BASELINE
COMPARISON .

Model
Name
Tiny YOLO [10]
Tiny SSD

reductions while having a negligible effect on object detection
accuracy.

Total number
of Parameters
1.13M

Total number
of MACs
571.09M

IV. PARAMETER P RECISION O PTIMIZATION
In this section, let us discuss the parameter precision
optimization strategy for Tiny SSD. For embedded scenarios
where the computational requirements and memory requirements are more strict, an effective strategy for reducing
computational and memory footprint of deep neural networks
is reducing the data precision of parameters in a deep neural
network. In particular, modern CPUs and GPUs have moved
towards accelerated mixed precision operations as well as
better handling of reduced parameter precision, and thus the
ability to take advantage of these factors can yield noticeable
improvements for embedded scenarios. For Tiny SSD, the
parameters are represented in half precision floating-point,
thus leading to further deep neural network model size

The proposed Tiny SSD network was trained for 220,000
iterations in the Caffe framework with training batch size of
24. RMSProp was utilized as the training policy with base
learning rate set to 0.00001 and γ = 0.5.
B. Discussion
Table III shows the model size and the object detection
accuracy of the proposed Tiny SSD network on the VOC
2007 test dataset, along with the model size and the object
detection accuracy of Tiny YOLO. A number of interesting
observations can be made. First, the resulting Tiny SSD
possesses a model size of 2.3MB, which is ∼26X smaller
than Tiny YOLO. The significantly smaller model size of
Tiny SSD compared to Tiny YOLO illustrates its efficacy
for greatly reducing the memory requirements for leveraging
Tiny SSD for real-time embedded object detection purposes.
Second, it can be observed that the resulting Tiny SSD
was still able to achieve an mAP of 61.3% on the VOC
2007 test dataset, which is ∼4.2% higher than that achieved
using Tiny YOLO. Figure 5 demonstrates several example
object detection results produced by the proposed Tiny SSD
compared to Tiny YOLO. It can be observed that Tiny SSD
has comparable object detection results as Tiny YOLO in
some cases, while in some cases outperforms Tiny YOLO in
assigning more accurate category labels to detected objects.
For example, in the first image case, Tiny SSD is able to
detect the chair in the scene, while Tiny YOLO misses the
chair. In the third image case, Tiny SSD is able to identify
the dog in the scene while Tiny YOLO detects two bounding
boxes around the dog, with one of the bounding boxes
incorrectly labeling it as cat. This significant improvement

Figure 4.

Example images from the Pascal VOC dataset. The ground-truth bounding boxes and object categories are shown for each image.

in object detection accuracy when compared to Tiny YOLO
illustrates the efficacy of Tiny SSD for providing more
reliable embedded object detection performance. Furthermore,
as seen in Table IV, Tiny SSD requires just 571.09 million
MAC operations to perform inference, making it well-suited
for real-time embedded object detection. These experimental
results show that very small deep neural network architectures
can be designed for real-time object detection that are wellsuited for embedded scenarios.
VI. C ONCLUSIONS
In this paper, a single-shot detection deep convolutional
neural network called Tiny SSD is introduced for real-time
embedded object detection. Composed of a highly optimized,
non-uniform Fire sub-network stack and a non-uniform subnetwork stack of highly optimized SSD-based auxiliary
convolutional feature layers designed specifically to minimize
model size while maintaining object detection performance,
Tiny SSD possesses a model size that is ∼26X smaller than
Tiny YOLO, requires just 571.09 million MAC operations,
while still achieving an mAP of that is ∼4.2% higher than
Tiny YOLO on the VOC 2007 test dataset. These results
demonstrates the efficacy of designing very small deep neural
network architectures such as Tiny SSD for real-time object
detection in embedded scenarios.
ACKNOWLEDGMENT
The authors thank Natural Sciences and Engineering Research Council of Canada, Canada Research Chairs Program,
DarwinAI, and Nvidia for hardware support.
R EFERENCES
[1] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages
580–587, 2014.
[2] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask r-cnn.
ICCV, 2017.
[3] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017.

[4] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu,
Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna,
Yang Song, Sergio Guadarrama, et al. Speed/accuracy tradeoffs for modern convolutional object detectors. In IEEE CVPR,
2017.
[5] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid
Ashraf, William J Dally, and Kurt Keutzer. Squeezenet:
Alexnet-level accuracy with 50x fewer parameters and&lt; 0.5
mb model size. arXiv preprint arXiv:1602.07360, 2016.
[6] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks.
In NIPS, 2012.
[7] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep
learning. Nature, 2015.
[8] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramid
networks for object detection. In CVPR, volume 1, page 4,
2017.
[9] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg.
SSD: Single shot multibox detector. In European conference
on computer vision, pages 21–37. Springer, 2016.
[10] J. Redmon.
YOLO: Real-time object
https://pjreddie.com/darknet/yolo/, 2016.

detection.

[11] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Unified, real-time object
detection. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 779–788, 2016.
[12] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,
stronger. arXiv preprint, 1612, 2016.
[13] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with region
proposal networks. In Advances in neural information
processing systems, pages 91–99, 2015.
[14] Mohammad Javad Shafiee, Brendan Chywl, Francis Li, and
Alexander Wong. Fast YOLO: A fast you only look once
system for real-time embedded object detection in video. arXiv
preprint arXiv:1709.05943, 2017.

Input Image

Tiny YOLO

Tiny SSD

Figure 5. Example object detection results produced by the proposed Tiny SSD compared to Tiny YOLO. It can be observed that Tiny SSD has comparable
object detection results as Tiny YOLO in some cases, while in some cases outperforms Tiny YOLO in assigning more accurate category labels to detected
objects. This significant improvement in object detection accuracy when compared to Tiny YOLO illustrates the efficacy of Tiny SSD for providing more
reliable embedded object detection performance.

[15] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick.
Training region-based object detectors with online hard
example mining. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 761–769,
2016.
[16] Bichen Wu, Forrest Iandola, Peter H Jin, and Kurt Keutzer.
Squeezedet: Unified, small, low power fully convolutional
neural networks for real-time object detection for autonomous
driving. arXiv preprint arXiv:1612.01051, 2016.

</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="2"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"1\n\nAn overview of deep learning based methods for\nunsupervised and semi-supervised anomaly\ndete<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="3"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"Energy Clustering\nGuilherme França∗ and Joshua T. Vogelstein†\nJohns Hopkins University\n\nA<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="4"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"Bayesian Probabilistic Numerical Methods\nJon Cockayne∗\n\nChris Oates†\n\nTim Sullivan‡\n\nM<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">10</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="5"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"Inverse Stability Problem and Applications to\nRenewables Integration\n\narXiv:1703.04491v5 [] 14 O<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">3</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="6"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"arXiv:1201.3325v2 [] 14 Aug 2012\n\nSIMPLICIAL COMPLEXES WITH RIGID DEPTH\nADNAN ASLAM AND VIVIANA <span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="7"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"Orthogonal Series Density Estimation for Complex Surveys\nShangyuan Ye, Ye Liang and Ibrahim A. Ahm<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">10</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="8"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"A Parametric MPC Approach to Balancing the Cost of Abstraction for\nDifferential-Drive Mobile Robot<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">3</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="9"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">"Online Model Estimation for Predictive Thermal Control of Buildings\nPeter Radecki, Member, IEEE, a<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">3</div></div>
							</td>
					</tr></tbody></table>
		<div class="bg-linear-to-b sticky left-0 border-t border-dashed border-gray-300 from-gray-100 to-white py-3 text-center font-mono text-xs dark:border-gray-700 dark:from-gray-950 dark:to-gray-900">End of preview. <a href="/datasets/nbroad/small_arxiv_classification/viewer/default/train" class="group"><span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Expand</span>
						in <svg class="text-lg mr-0.5 inline -translate-y-px text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>Data Studio
					</a></div></div>




			<div class="bg-linear-to-b from-gray-100 to-white dark:from-gray-950 dark:to-gray-900 "><hr class="flex-none -translate-y-px border-t border-dashed border-gray-300 bg-white dark:border-gray-700 dark:bg-gray-950">
					<nav><ul class="flex select-none items-center justify-between space-x-2 text-gray-700 sm:justify-center py-1 text-center font-mono text-xs rounded-b-lg"><li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 pointer-events-none cursor-default text-gray-400 hover:text-gray-700" href=""><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg>
		Previous</a></li>
			<li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1 bg-gray-50 font-semibold ring-1 ring-inset ring-gray-200 dark:bg-gray-900 dark:text-yellow-500 dark:ring-gray-900 hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/nbroad/small_arxiv_classification/viewer/default/train?p=0">1</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/nbroad/small_arxiv_classification/viewer/default/train?p=1">2</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/nbroad/small_arxiv_classification/viewer/default/train?p=2">3</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  pointer-events-none cursor-default" href="#">...</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/nbroad/small_arxiv_classification/viewer/default/train?p=9">10</a>
				</li>
			<li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 " href="/datasets/nbroad/small_arxiv_classification/viewer/default/train?p=1">Next
		<svg class="ml-1.5 transform rotate-180" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg></a></li></ul></nav></div></div>






</div></div></div></div></div></div></div>

				<div class="from-gray-50-to-white bg-linear-to-t rounded-lg border border-dotted border-gray-200 py-24 text-center md:px-6"><p class="mb-1 mt-2">No dataset card yet</p>
						</div></section>
			
			<section class="pt-6 border-gray-100 md:pb-24 md:pl-6 md:w-64 lg:w-80 xl:w-96 flex-none order-first md:order-none md:border-l pt-3! md:pt-6!"><dl class="flex items-baseline justify-between"><dt class="text-sm text-gray-500">Downloads last month</dt><div class="mx-4 flex-1 border-b border-dotted"></div><dd class="font-semibold">127</dd></dl>
				
				<div class="divider-column-vertical"></div>
				<div class="grid grid-cols-2 gap-x-2 md:flex md:flex-row md:flex-wrap"><div class="SVELTE_HYDRATER contents" data-target="DatasetAndModelActionsDropdown" data-props="{&quot;classNames&quot;:&quot;order-last&quot;,&quot;discussionsDisabled&quot;:false,&quot;repo&quot;:{&quot;type&quot;:&quot;dataset&quot;,&quot;name&quot;:&quot;nbroad/small_arxiv_classification&quot;},&quot;canWrite&quot;:false,&quot;canDisable&quot;:false,&quot;repoIsPrivate&quot;:false,&quot;repoIsGated&quot;:false,&quot;repoIsDisabled&quot;:false,&quot;repoIsAdminFlaggedNFAA&quot;:false,&quot;repoHasBlockedOids&quot;:false}"><div class="order-last"><div class="relative ">
	<button class="btn px-1.5 py-1.5 " type="button">
		
			<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="p-0.5" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><circle cx="16" cy="7" r="3" fill="currentColor"></circle><circle cx="16" cy="16" r="3" fill="currentColor"></circle><circle cx="16" cy="25" r="3" fill="currentColor"></circle></svg>
		
		</button>
	
	
	</div></div>













</div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetLibrary" data-props="{&quot;classNames&quot;:&quot;md:w-full xl:w-auto xl:flex-none&quot;,&quot;libraries&quot;:[{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;datasets&quot;,&quot;function&quot;:&quot;load_dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{},&quot;code&quot;:&quot;from datasets import load_dataset\n\nds = load_dataset(\&quot;nbroad/small_arxiv_classification\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;pandas&quot;,&quot;function&quot;:&quot;pd.read_parquet&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;train.parquet&quot;,&quot;validation&quot;:&quot;validation.parquet&quot;,&quot;test&quot;:&quot;test.parquet&quot;}},&quot;code&quot;:&quot;import pandas as pd\n\nsplits = {'train': 'train.parquet', 'validation': 'validation.parquet', 'test': 'test.parquet'}\ndf = pd.read_parquet(\&quot;hf://datasets/nbroad/small_arxiv_classification/\&quot; + splits[\&quot;train\&quot;])&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;mlcroissant&quot;,&quot;function&quot;:&quot;Dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;record_set&quot;:&quot;default&quot;,&quot;partial&quot;:false},&quot;code&quot;:&quot;from mlcroissant import Dataset\n\nds = Dataset(jsonld=\&quot;https://huggingface.co/api/datasets/nbroad/small_arxiv_classification/croissant\&quot;)\nrecords = ds.records(\&quot;default\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;polars&quot;,&quot;function&quot;:&quot;pl.read_parquet&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;train.parquet&quot;,&quot;validation&quot;:&quot;validation.parquet&quot;,&quot;test&quot;:&quot;test.parquet&quot;}},&quot;code&quot;:&quot;import polars as pl\n\nsplits = {'train': 'train.parquet', 'validation': 'validation.parquet', 'test': 'test.parquet'}\ndf = pl.read_parquet('hf://datasets/nbroad/small_arxiv_classification/' + splits['train'])\n&quot;}]}]}"><div class="relative md:w-full xl:w-auto xl:flex-none">
	<button class="from-gray-800! to-black! max-xl:mb-2 text-white! gap-1! border-gray-800! dark:border-gray-900!  btn w-full cursor-pointer text-sm" type="button">
		<svg class="mr-1.5 mr-0.5! " xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>
			Use this dataset
		<svg class="-mr-1 text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path d="M16.293 9.293L12 13.586L7.707 9.293l-1.414 1.414L12 16.414l5.707-5.707z" fill="currentColor"></path></svg></button>
	
	
	</div>

</div>
					
					</div>
				<div class="divider-column-vertical"></div>
					<div class="flex flex-col flex-wrap xl:flex-row"><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of downloaded dataset files:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->60.5 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 " href="/datasets/nbroad/small_arxiv_classification/tree/refs%2Fconvert%2Fparquet/" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of the auto-converted Parquet files:</div>
		<div class="truncate text-sm group-hover:underline">
			<!-- HTML_TAG_START -->60.5 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Number of rows:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->2,000<!-- HTML_TAG_END --></div></a></div>
				
				
				
				

				
				<div class="divider-column-vertical md:hidden"></div></section></div></main>

	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container relative flex flex-col justify-between space-y-2 py-6 text-gray-500 max-md:*:self-start md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="SVELTE_HYDRATER contents" data-target="ThemeSwitcher" data-props="{&quot;theme&quot;:&quot;system&quot;,&quot;isLoggedIn&quot;:false,&quot;menuClassNames&quot;:&quot;md:-top-24&quot;,&quot;classNames&quot;:&quot;max-md:mb-5 max-md:*:self-start&quot;}">
<div class="relative inline-block max-md:mb-5 max-md:*:self-start">
	<button class="rounded-full border border-gray-100 pl-2 py-1 pr-2.5  flex items-center text-sm text-gray-500 bg-white hover:bg-purple-50 hover:border-purple-200 dark:hover:bg-gray-800 dark:hover:border-gray-950 dark:border-gray-800 " type="button">
		<svg class="mr-1.5 text-gray-500" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M6 22.5h20a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2H6a2 2 0 0 0-2 2v13.5a2 2 0 0 0 2 2ZM7 7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h18a1 1 0 0 0 1-1V8a1 1 0 0 0-1-1H7Z" fill="currentColor"></path><path d="M6 8a1 1 0 0 1 1-1h18a1 1 0 0 1 1 1v11a1 1 0 0 1-1 1H7a1 1 0 0 1-1-1V8Z" fill="currentColor" fill-opacity=".4"></path><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path></svg>
			System theme
		</button>
	
	
	</div></div>
		<div class="font-semibold text-black md:hidden">Company</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="max-md:mb-4! max-md:mt-8! group flex-none max-md:order-last"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="max-md:mt-8! font-semibold text-black md:hidden">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("\/front\/build\/kube-68d7aa0\/index.js");
			window.moonSha = "kube-68d7aa0\/";
			window.__hf_deferred = {};
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>
	</body>
</html>
