<!doctype html>
<html class="">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
		<meta name="description" content="Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science." />
		<meta property="fb:app_id" content="1321688464574422" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@huggingface" />
		<meta name="twitter:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/CShorten/ML-ArXiv-Papers.png" />
		<meta property="og:title" content="CShorten/ML-ArXiv-Papers Â· Datasets at Hugging Face" />
		<meta property="og:type" content="website" />
		<meta property="og:url" content="https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers" />
		<meta property="og:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/CShorten/ML-ArXiv-Papers.png" />

		<link rel="stylesheet" href="/front/build/kube-68d7aa0/style.css" />

		<link rel="preconnect" href="https://fonts.gstatic.com" />
		<link
			href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap"
			rel="stylesheet"
		/>

		<link
			rel="preload"
			href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css"
			as="style"
			onload="this.onload=null;this.rel='stylesheet'"
		/>
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		<script>const guestTheme = document.cookie.match(/theme=(\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches));</script>
<link rel="canonical" href="https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers"> <script type="application/ld+json">{
  "@context": {
    "@language": "en",
    "@vocab": "https:\/\/schema.org\/",
    "citeAs": "cr:citeAs",
    "column": "cr:column",
    "conformsTo": "dct:conformsTo",
    "cr": "http:\/\/mlcommons.org\/croissant\/",
    "data": {
      "@id": "cr:data",
      "@type": "@json"
    },
    "dataBiases": "cr:dataBiases",
    "dataCollection": "cr:dataCollection",
    "dataType": {
      "@id": "cr:dataType",
      "@type": "@vocab"
    },
    "dct": "http:\/\/purl.org\/dc\/terms\/",
    "extract": "cr:extract",
    "field": "cr:field",
    "fileProperty": "cr:fileProperty",
    "fileObject": "cr:fileObject",
    "fileSet": "cr:fileSet",
    "format": "cr:format",
    "includes": "cr:includes",
    "isLiveDataset": "cr:isLiveDataset",
    "jsonPath": "cr:jsonPath",
    "key": "cr:key",
    "md5": "cr:md5",
    "parentField": "cr:parentField",
    "path": "cr:path",
    "personalSensitiveInformation": "cr:personalSensitiveInformation",
    "recordSet": "cr:recordSet",
    "references": "cr:references",
    "regex": "cr:regex",
    "repeated": "cr:repeated",
    "replace": "cr:replace",
    "sc": "https:\/\/schema.org\/",
    "separator": "cr:separator",
    "source": "cr:source",
    "subField": "cr:subField",
    "transform": "cr:transform"
  },
  "@type": "sc:Dataset",
  "distribution": [
    {
      "@type": "cr:FileObject",
      "@id": "repo",
      "name": "repo",
      "description": "The Hugging Face git repository.",
      "contentUrl": "https:\/\/huggingface.co\/datasets\/CShorten\/ML-ArXiv-Papers\/tree\/refs%2Fconvert%2Fparquet",
      "encodingFormat": "git+https",
      "sha256": "https:\/\/github.com\/mlcommons\/croissant\/issues\/80"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-default",
      "name": "parquet-files-for-config-default",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "default\/*\/*.parquet"
    }
  ],
  "recordSet": [
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "default_splits\/split_name"
      },
      "@id": "default_splits",
      "name": "default_splits",
      "description": "Splits for the default config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "default_splits\/split_name": "train"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "default",
      "name": "default",
      "description": "CShorten\/ML-ArXiv-Papers - 'default' subset",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default\/split",
          "name": "default\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "default\/(?:partial-)?(train)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "default_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/Unnamed__0.1",
          "name": "default\/Unnamed__0.1",
          "description": "Column 'Unnamed: 0.1' from the Hugging Face parquet file.",
          "dataType": "sc:Integer",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "Unnamed: 0.1"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/Unnamed__0",
          "name": "default\/Unnamed__0",
          "description": "Column 'Unnamed: 0' from the Hugging Face parquet file.",
          "dataType": "sc:Float",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "Unnamed: 0"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/title",
          "name": "default\/title",
          "description": "Column 'title' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "title"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/abstract",
          "name": "default\/abstract",
          "description": "Column 'abstract' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "abstract"
            }
          }
        }
      ]
    }
  ],
  "conformsTo": "http:\/\/mlcommons.org\/croissant\/1.0",
  "name": "ML-ArXiv-Papers",
  "description": "This dataset contains the subset of ArXiv papers with the \"cs.LG\" tag to indicate the paper is about Machine Learning.\nThe core dataset is filtered from the full ArXiv dataset hosted on Kaggle: https:\/\/www.kaggle.com\/datasets\/Cornell-University\/arxiv. The original dataset contains roughly 2 million papers. This dataset contains roughly 100,000 papers following the category filtering.\nThe dataset is maintained by with requests to the ArXiv API.\nThe current iteration of the dataset only containsâ€¦ See the full description on the dataset page: https:\/\/huggingface.co\/datasets\/CShorten\/ML-ArXiv-Papers.",
  "alternateName": [
    "CShorten\/ML-ArXiv-Papers"
  ],
  "creator": {
    "@type": "Person",
    "name": "Connor Shorten",
    "url": "https:\/\/huggingface.co\/CShorten"
  },
  "keywords": [
    "afl-3.0",
    "100K - 1M",
    "csv",
    "Tabular",
    "Text",
    "Datasets",
    "pandas",
    "Croissant",
    "Polars",
    "ðŸ‡ºðŸ‡¸ Region: US"
  ],
  "license": "https:\/\/choosealicense.com\/licenses\/afl-3.0\/",
  "url": "https:\/\/huggingface.co\/datasets\/CShorten\/ML-ArXiv-Papers"
}</script> 

		<title>CShorten/ML-ArXiv-Papers Â· Datasets at Hugging Face</title>

		<script
			defer
			data-domain="huggingface.co"
			event-loggedIn="false"
			src="/js/script.pageview-props.js"
		></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script>
			window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer></script>
	</head>
	<body class="flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DatasetPage">
		<div class="flex min-h-dvh flex-col"><div class="SVELTE_HYDRATER contents" data-target="SystemThemeMonitor" data-props="{&quot;isLoggedIn&quot;:false}"></div>

	<div class="SVELTE_HYDRATER contents" data-target="MainHeader" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false,&quot;isPro&quot;:false}"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg">
				<span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a>
			<div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..."   spellcheck="false" type="text" value="">
	<svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
	</div>
			<div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg>
		</button>

	</div></div>
		<nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 2xl:space-x-2"><li class="hover:text-indigo-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
					Models</a>
			</li><li class="hover:text-red-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					Datasets</a>
			</li><li class="hover:text-blue-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg>
					Spaces</a>
			</li><li class="hover:text-yellow-700 max-xl:hidden"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 text-yellow-500!" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg>
					Posts</a>
			</li><li class="hover:text-yellow-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 16 16"><path d="m2.28 3.7-.3.16a.67.67 0 0 0-.34.58v8.73l.01.04.02.07.01.04.03.06.02.04.02.03.04.06.05.05.04.04.06.04.06.04.08.04.08.02h.05l.07.02h.11l.04-.01.07-.02.03-.01.07-.03.22-.12a5.33 5.33 0 0 1 5.15.1.67.67 0 0 0 .66 0 5.33 5.33 0 0 1 5.33 0 .67.67 0 0 0 1-.58V4.36a.67.67 0 0 0-.34-.5l-.3-.17v7.78a.63.63 0 0 1-.87.59 4.9 4.9 0 0 0-4.35.35l-.65.39a.29.29 0 0 1-.15.04.29.29 0 0 1-.16-.04l-.65-.4a4.9 4.9 0 0 0-4.34-.34.63.63 0 0 1-.87-.59V3.7Z" fill="currentColor" class="dark:opacity-40"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M8 3.1a5.99 5.99 0 0 0-5.3-.43.66.66 0 0 0-.42.62v8.18c0 .45.46.76.87.59a4.9 4.9 0 0 1 4.34.35l.65.39c.05.03.1.04.16.04.05 0 .1-.01.15-.04l.65-.4a4.9 4.9 0 0 1 4.35-.34.63.63 0 0 0 .86-.59V3.3a.67.67 0 0 0-.41-.62 5.99 5.99 0 0 0-5.3.43l-.3.17L8 3.1Zm.73 1.87a.43.43 0 1 0-.86 0v5.48a.43.43 0 0 0 .86 0V4.97Z" fill="currentColor" class="opacity-40 dark:opacity-100"></path><path d="M8.73 4.97a.43.43 0 1 0-.86 0v5.48a.43.43 0 1 0 .86 0V4.96Z" fill="currentColor" class="dark:opacity-40"></path></svg>
					Docs</a>
			</li><li class="hover:text-black dark:hover:text-white"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/enterprise"><svg class="mr-1.5 text-gray-400 group-hover:text-black dark:group-hover:text-white" xmlns="http://www.w3.org/2000/svg" fill="none" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 33 27"><path fill="currentColor" fill-rule="evenodd" d="M13.5.7a8.7 8.7 0 0 0-7.7 5.7L1 20.6c-1 3.1.9 5.7 4.1 5.7h15c3.3 0 6.8-2.6 7.8-5.7l4.6-14.2c1-3.1-.8-5.7-4-5.7h-15Zm1.1 5.7L9.8 20.3h9.8l1-3.1h-5.8l.8-2.5h4.8l1.1-3h-4.8l.8-2.3H23l1-3h-9.5Z" clip-rule="evenodd"></path></svg>
					Enterprise</a>
			</li>

		<li><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/pricing">Pricing
			</a></li>

		<li><div class="relative group">
	<button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button">
		<svg class=" text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>
			
		</button>
	
	
	</div></li>
		<li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li>
		<li><a class="block cursor-pointer whitespace-nowrap px-2 py-0.5 hover:text-gray-500 dark:text-gray-300 dark:hover:text-gray-100" href="/login">Log In
				</a></li>
			<li><a class="whitespace-nowrap rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up
					</a></li></ul></nav></div></header></div>
	
	
	
	<div class="SVELTE_HYDRATER contents" data-target="SSOBanner" data-props="{}"></div>
	
	

	<main class="flex flex-1 flex-col">
	<div class="SVELTE_HYDRATER contents" data-target="DatasetHeader" data-props="{&quot;activeTab&quot;:&quot;datasetCard&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;5fda30110e761cf3183cfd5c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1654000939422-5fda30110e761cf3183cfd5c.png&quot;,&quot;fullname&quot;:&quot;Connor Shorten&quot;,&quot;name&quot;:&quot;CShorten&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:6},&quot;canReadRepoSettings&quot;:false,&quot;dataset&quot;:{&quot;author&quot;:&quot;CShorten&quot;,&quot;cardData&quot;:{&quot;license&quot;:&quot;afl-3.0&quot;},&quot;cardExists&quot;:true,&quot;createdAt&quot;:&quot;2022-06-23T14:31:39.000Z&quot;,&quot;description&quot;:&quot;This dataset contains the subset of ArXiv papers with the \&quot;cs.LG\&quot; tag to indicate the paper is about Machine Learning.\nThe core dataset is filtered from the full ArXiv dataset hosted on Kaggle: https://www.kaggle.com/datasets/Cornell-University/arxiv. The original dataset contains roughly 2 million papers. This dataset contains roughly 100,000 papers following the category filtering.\nThe dataset is maintained by with requests to the ArXiv API.\nThe current iteration of the dataset only containsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers.&quot;,&quot;downloads&quot;:1580,&quot;downloadsAllTime&quot;:45731,&quot;id&quot;:&quot;CShorten/ML-ArXiv-Papers&quot;,&quot;isLikedByUser&quot;:false,&quot;lastModified&quot;:&quot;2022-06-27T12:15:11.000Z&quot;,&quot;likes&quot;:53,&quot;datasetsServerInfo&quot;:{&quot;viewer&quot;:&quot;viewer&quot;,&quot;numRows&quot;:117592,&quot;libraries&quot;:[&quot;datasets&quot;,&quot;pandas&quot;,&quot;mlcroissant&quot;,&quot;polars&quot;],&quot;formats&quot;:[&quot;csv&quot;],&quot;modalities&quot;:[&quot;tabular&quot;,&quot;text&quot;]},&quot;discussionsDisabled&quot;:false,&quot;repoType&quot;:&quot;dataset&quot;,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;tags&quot;:[&quot;license:afl-3.0&quot;,&quot;size_categories:100K<n<1M&quot;,&quot;format:csv&quot;,&quot;modality:tabular&quot;,&quot;modality:text&quot;,&quot;library:datasets&quot;,&quot;library:pandas&quot;,&quot;library:mlcroissant&quot;,&quot;library:polars&quot;,&quot;region:us&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;license:afl-3.0&quot;,&quot;label&quot;:&quot;afl-3.0&quot;,&quot;type&quot;:&quot;license&quot;},{&quot;id&quot;:&quot;size_categories:100K<n<1M&quot;,&quot;label&quot;:&quot;100K - 1M&quot;,&quot;type&quot;:&quot;size_categories&quot;},{&quot;id&quot;:&quot;format:csv&quot;,&quot;label&quot;:&quot;csv&quot;,&quot;type&quot;:&quot;format&quot;},{&quot;id&quot;:&quot;modality:tabular&quot;,&quot;label&quot;:&quot;Tabular&quot;,&quot;type&quot;:&quot;modality&quot;},{&quot;id&quot;:&quot;modality:text&quot;,&quot;label&quot;:&quot;Text&quot;,&quot;type&quot;:&quot;modality&quot;},{&quot;id&quot;:&quot;library:datasets&quot;,&quot;label&quot;:&quot;Datasets&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:pandas&quot;,&quot;label&quot;:&quot;pandas&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:mlcroissant&quot;,&quot;label&quot;:&quot;Croissant&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:polars&quot;,&quot;label&quot;:&quot;Polars&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;ðŸ‡ºðŸ‡¸ Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;}],&quot;hasBlockedOids&quot;:false,&quot;region&quot;:&quot;us&quot;,&quot;xetEnabled&quot;:false},&quot;discussionsStats&quot;:{&quot;closed&quot;:0,&quot;open&quot;:0,&quot;total&quot;:0}}"><header class="bg-linear-to-t border-b border-gray-100 pt-6 sm:pt-9 from-gray-50-to-white via-white dark:via-gray-950"><div class="container relative "><h1 class="flex flex-wrap items-center max-md:leading-tight mb-3 text-lg max-sm:gap-y-1.5 md:text-xl"><a href="/datasets" class="group flex items-center"><svg class="sm:mr-1.5 -mr-1 text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					<span class="mr-2.5 font-semibold text-gray-400 group-hover:text-gray-500 max-sm:hidden">Datasets:</span></a>
				<hr class="mx-1.5 h-2 translate-y-px rounded-sm border-r dark:border-gray-600 sm:hidden">
			<div class="group flex flex-none items-center"><div class="relative mr-1 flex items-center">

			

<span class="inline-block "><span class="contents"><a href="/CShorten" class="text-gray-400 hover:text-blue-600"><img alt="" class="w-3.5 h-3.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1654000939422-5fda30110e761cf3183cfd5c.png" crossorigin="anonymous"></a></span>
	</span></div>
		

<span class="inline-block "><span class="contents"><a href="/CShorten" class="text-gray-400 hover:text-blue-600">CShorten</a></span>
	</span>
		<div class="mx-0.5 text-gray-300">/</div></div>

<div class="max-w-full "><a class="break-words font-mono font-semibold hover:text-blue-600 " href="/datasets/CShorten/ML-ArXiv-Papers">ML-ArXiv-Papers</a>
	<button class="relative text-sm mr-4 focus:outline-hidden inline-flex cursor-pointer items-center text-sm  mx-0.5   text-gray-600 " title="Copy dataset name to clipboard" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg>
	
	</button></div>
			<div class="inline-flex items-center overflow-hidden whitespace-nowrap rounded-md border bg-white text-sm leading-none text-gray-500  mr-2"><button class="relative flex items-center overflow-hidden from-red-50 to-transparent dark:from-red-900 px-1.5 py-1 hover:bg-linear-to-t focus:outline-hidden"  title="Like"><svg class="left-1.5 absolute" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>

		
		<span class="ml-4 pl-0.5 ">like</span></button>
	<button class="focus:outline-hidden flex items-center border-l px-1.5 py-1 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="See users who liked this repository">53</button></div>




			
			
	</h1>
		<div class="mb-3 flex flex-wrap md:mb-4"><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Modalities:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?modality=modality%3Atabular"><div class="tag tag-white   ">
		<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.619 4.619C2.667 6.572 2.667 9.715 2.667 16s0 9.43 1.952 11.38C6.572 29.333 9.715 29.333 16 29.333s9.43 0 11.38-1.953c1.953-1.952 1.953-5.095 1.953-11.38 0-6.285 0-9.427-1.953-11.381C25.428 2.667 22.285 2.667 16 2.667c-6.285 0-9.427 0-11.381 1.952ZM16.615 10a1 1 0 1 1 0 2h-1.23a1 1 0 1 1 0-2h1.23Zm0 5a1 1 0 1 1 0 2h-1.23a1 1 0 1 1 0-2h1.23Zm3.154 1a1 1 0 0 0 1 1H22a1 1 0 1 0 0-2h-1.23a1 1 0 0 0-1 1Zm-3.154 4a1 1 0 1 1 0 2h-1.23a1 1 0 1 1 0-2h1.23Zm3.154 1a1 1 0 0 0 1 1H22a1 1 0 1 0 0-2h-1.23a1 1 0 0 0-1 1Zm0-10a1 1 0 0 0 1 1H22a1 1 0 1 0 0-2h-1.23a1 1 0 0 0-1 1Zm-8.538-1H10a1 1 0 1 0 0 2h1.23a1 1 0 1 0 0-2Zm0 5H10a1 1 0 1 0 0 2h1.23a1 1 0 1 0 0-2Zm0 5H10a1 1 0 1 0 0 2h1.23a1 1 0 1 0 0-2Z" fill="currentColor"></path></svg>

	

	<span>Tabular</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?modality=modality%3Atext"><div class="tag tag-white   ">
		<svg class="text-red-700 dark:text-red-600" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.619 4.619C2.667 6.573 2.667 9.715 2.667 16s0 9.428 1.952 11.38C6.573 29.333 9.715 29.333 16 29.333s9.428 0 11.38-1.953c1.953-1.95 1.953-5.095 1.953-11.38s0-9.428-1.953-11.381C25.43 2.667 22.285 2.667 16 2.667s-9.428 0-11.381 1.952m8.65 3.714c-.573 0-1.109 0-1.546.066-.495.073-1.003.248-1.41.7-.392.436-.53.956-.59 1.452-.056.464-.056 1.04-.056 1.689V13a1 1 0 1 0 2 0v-.704c0-.724.001-1.176.041-1.505q.015-.15.061-.294a.2.2 0 0 1 .031-.061q0-.003.016-.01a.8.8 0 0 1 .203-.05c.272-.04.654-.043 1.314-.043H15v11.334h-2.333a1 1 0 1 0 0 2H20a1 1 0 0 0 0-2h-3V10.333h1.667c.66 0 1.042.003 1.314.043.123.019.18.04.203.05l.015.009a.2.2 0 0 1 .032.061c.018.05.042.14.061.295.04.329.041.781.041 1.506V13a1 1 0 1 0 2 0v-.76c0-.65 0-1.225-.056-1.69-.06-.495-.198-1.015-.59-1.453-.407-.45-.915-.625-1.41-.698-.437-.067-.973-.067-1.546-.066z" fill="currentColor"></path></svg>

	

	<span>Text</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Formats:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?format=format%3Acsv"><div class="tag tag-white   ">
		<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.508 4.486h3.98v-1h-3.98v1Zm5.004 0h3.98v-1h-3.98v1ZM5.488 6.5h-3.98v-1h3.98v1Zm1.024 0h3.98v-1h-3.98v1ZM5.488 8.514h-3.98v-1h3.98v1Zm1.024 0h3.98v-1h-3.98v1Z" fill="currentColor"></path></svg>

	

	<span>csv</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Size:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?size_categories=size_categories%3A100K%3Cn%3C1M"><div class="tag tag-white   ">

	

	<span>100K - 1M</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Libraries:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Adatasets"><div class="tag tag-white   "><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 95 88"><path fill="#fff" d="M94.25 70.08a8.28 8.28 0 0 1-.43 6.46 10.57 10.57 0 0 1-3 3.6 25.18 25.18 0 0 1-5.7 3.2 65.74 65.74 0 0 1-7.56 2.65 46.67 46.67 0 0 1-11.42 1.68c-5.42.05-10.09-1.23-13.4-4.5a40.4 40.4 0 0 1-10.14.03c-3.34 3.25-7.99 4.52-13.39 4.47a46.82 46.82 0 0 1-11.43-1.68 66.37 66.37 0 0 1-7.55-2.65c-2.28-.98-4.17-2-5.68-3.2a10.5 10.5 0 0 1-3.02-3.6c-.99-2-1.18-4.3-.42-6.46a8.54 8.54 0 0 1-.33-5.63c.25-.95.66-1.83 1.18-2.61a8.67 8.67 0 0 1 2.1-8.47 8.23 8.23 0 0 1 2.82-2.07 41.75 41.75 0 1 1 81.3-.12 8.27 8.27 0 0 1 3.11 2.19 8.7 8.7 0 0 1 2.1 8.47c.52.78.93 1.66 1.18 2.61a8.61 8.61 0 0 1-.32 5.63Z"></path><path fill="#FFD21E" d="M47.21 76.5a34.75 34.75 0 1 0 0-69.5 34.75 34.75 0 0 0 0 69.5Z"></path><path fill="#FF9D0B" d="M81.96 41.75a34.75 34.75 0 1 0-69.5 0 34.75 34.75 0 0 0 69.5 0Zm-73.5 0a38.75 38.75 0 1 1 77.5 0 38.75 38.75 0 0 1-77.5 0Z"></path><path fill="#3A3B45" d="M58.5 32.3c1.28.44 1.78 3.06 3.07 2.38a5 5 0 1 0-6.76-2.07c.61 1.15 2.55-.72 3.7-.32ZM34.95 32.3c-1.28.44-1.79 3.06-3.07 2.38a5 5 0 1 1 6.76-2.07c-.61 1.15-2.56-.72-3.7-.32Z"></path><path fill="#FF323D" d="M46.96 56.29c9.83 0 13-8.76 13-13.26 0-2.34-1.57-1.6-4.09-.36-2.33 1.15-5.46 2.74-8.9 2.74-7.19 0-13-6.88-13-2.38s3.16 13.26 13 13.26Z"></path><path fill="#3A3B45" fill-rule="evenodd" d="M39.43 54a8.7 8.7 0 0 1 5.3-4.49c.4-.12.81.57 1.24 1.28.4.68.82 1.37 1.24 1.37.45 0 .9-.68 1.33-1.35.45-.7.89-1.38 1.32-1.25a8.61 8.61 0 0 1 5 4.17c3.73-2.94 5.1-7.74 5.1-10.7 0-2.34-1.57-1.6-4.09-.36l-.14.07c-2.31 1.15-5.39 2.67-8.77 2.67s-6.45-1.52-8.77-2.67c-2.6-1.29-4.23-2.1-4.23.29 0 3.05 1.46 8.06 5.47 10.97Z" clip-rule="evenodd"></path><path fill="#FF9D0B" d="M70.71 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM24.21 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM17.52 48c-1.62 0-3.06.66-4.07 1.87a5.97 5.97 0 0 0-1.33 3.76 7.1 7.1 0 0 0-1.94-.3c-1.55 0-2.95.59-3.94 1.66a5.8 5.8 0 0 0-.8 7 5.3 5.3 0 0 0-1.79 2.82c-.24.9-.48 2.8.8 4.74a5.22 5.22 0 0 0-.37 5.02c1.02 2.32 3.57 4.14 8.52 6.1 3.07 1.22 5.89 2 5.91 2.01a44.33 44.33 0 0 0 10.93 1.6c5.86 0 10.05-1.8 12.46-5.34 3.88-5.69 3.33-10.9-1.7-15.92-2.77-2.78-4.62-6.87-5-7.77-.78-2.66-2.84-5.62-6.25-5.62a5.7 5.7 0 0 0-4.6 2.46c-1-1.26-1.98-2.25-2.86-2.82A7.4 7.4 0 0 0 17.52 48Zm0 4c.51 0 1.14.22 1.82.65 2.14 1.36 6.25 8.43 7.76 11.18.5.92 1.37 1.31 2.14 1.31 1.55 0 2.75-1.53.15-3.48-3.92-2.93-2.55-7.72-.68-8.01.08-.02.17-.02.24-.02 1.7 0 2.45 2.93 2.45 2.93s2.2 5.52 5.98 9.3c3.77 3.77 3.97 6.8 1.22 10.83-1.88 2.75-5.47 3.58-9.16 3.58-3.81 0-7.73-.9-9.92-1.46-.11-.03-13.45-3.8-11.76-7 .28-.54.75-.76 1.34-.76 2.38 0 6.7 3.54 8.57 3.54.41 0 .7-.17.83-.6.79-2.85-12.06-4.05-10.98-8.17.2-.73.71-1.02 1.44-1.02 3.14 0 10.2 5.53 11.68 5.53.11 0 .2-.03.24-.1.74-1.2.33-2.04-4.9-5.2-5.21-3.16-8.88-5.06-6.8-7.33.24-.26.58-.38 1-.38 3.17 0 10.66 6.82 10.66 6.82s2.02 2.1 3.25 2.1c.28 0 .52-.1.68-.38.86-1.46-8.06-8.22-8.56-11.01-.34-1.9.24-2.85 1.31-2.85Z"></path><path fill="#FFD21E" d="M38.6 76.69c2.75-4.04 2.55-7.07-1.22-10.84-3.78-3.77-5.98-9.3-5.98-9.3s-.82-3.2-2.69-2.9c-1.87.3-3.24 5.08.68 8.01 3.91 2.93-.78 4.92-2.29 2.17-1.5-2.75-5.62-9.82-7.76-11.18-2.13-1.35-3.63-.6-3.13 2.2.5 2.79 9.43 9.55 8.56 11-.87 1.47-3.93-1.71-3.93-1.71s-9.57-8.71-11.66-6.44c-2.08 2.27 1.59 4.17 6.8 7.33 5.23 3.16 5.64 4 4.9 5.2-.75 1.2-12.28-8.53-13.36-4.4-1.08 4.11 11.77 5.3 10.98 8.15-.8 2.85-9.06-5.38-10.74-2.18-1.7 3.21 11.65 6.98 11.76 7.01 4.3 1.12 15.25 3.49 19.08-2.12Z"></path><path fill="#FF9D0B" d="M77.4 48c1.62 0 3.07.66 4.07 1.87a5.97 5.97 0 0 1 1.33 3.76 7.1 7.1 0 0 1 1.95-.3c1.55 0 2.95.59 3.94 1.66a5.8 5.8 0 0 1 .8 7 5.3 5.3 0 0 1 1.78 2.82c.24.9.48 2.8-.8 4.74a5.22 5.22 0 0 1 .37 5.02c-1.02 2.32-3.57 4.14-8.51 6.1-3.08 1.22-5.9 2-5.92 2.01a44.33 44.33 0 0 1-10.93 1.6c-5.86 0-10.05-1.8-12.46-5.34-3.88-5.69-3.33-10.9 1.7-15.92 2.78-2.78 4.63-6.87 5.01-7.77.78-2.66 2.83-5.62 6.24-5.62a5.7 5.7 0 0 1 4.6 2.46c1-1.26 1.98-2.25 2.87-2.82A7.4 7.4 0 0 1 77.4 48Zm0 4c-.51 0-1.13.22-1.82.65-2.13 1.36-6.25 8.43-7.76 11.18a2.43 2.43 0 0 1-2.14 1.31c-1.54 0-2.75-1.53-.14-3.48 3.91-2.93 2.54-7.72.67-8.01a1.54 1.54 0 0 0-.24-.02c-1.7 0-2.45 2.93-2.45 2.93s-2.2 5.52-5.97 9.3c-3.78 3.77-3.98 6.8-1.22 10.83 1.87 2.75 5.47 3.58 9.15 3.58 3.82 0 7.73-.9 9.93-1.46.1-.03 13.45-3.8 11.76-7-.29-.54-.75-.76-1.34-.76-2.38 0-6.71 3.54-8.57 3.54-.42 0-.71-.17-.83-.6-.8-2.85 12.05-4.05 10.97-8.17-.19-.73-.7-1.02-1.44-1.02-3.14 0-10.2 5.53-11.68 5.53-.1 0-.19-.03-.23-.1-.74-1.2-.34-2.04 4.88-5.2 5.23-3.16 8.9-5.06 6.8-7.33-.23-.26-.57-.38-.98-.38-3.18 0-10.67 6.82-10.67 6.82s-2.02 2.1-3.24 2.1a.74.74 0 0 1-.68-.38c-.87-1.46 8.05-8.22 8.55-11.01.34-1.9-.24-2.85-1.31-2.85Z"></path><path fill="#FFD21E" d="M56.33 76.69c-2.75-4.04-2.56-7.07 1.22-10.84 3.77-3.77 5.97-9.3 5.97-9.3s.82-3.2 2.7-2.9c1.86.3 3.23 5.08-.68 8.01-3.92 2.93.78 4.92 2.28 2.17 1.51-2.75 5.63-9.82 7.76-11.18 2.13-1.35 3.64-.6 3.13 2.2-.5 2.79-9.42 9.55-8.55 11 .86 1.47 3.92-1.71 3.92-1.71s9.58-8.71 11.66-6.44c2.08 2.27-1.58 4.17-6.8 7.33-5.23 3.16-5.63 4-4.9 5.2.75 1.2 12.28-8.53 13.36-4.4 1.08 4.11-11.76 5.3-10.97 8.15.8 2.85 9.05-5.38 10.74-2.18 1.69 3.21-11.65 6.98-11.76 7.01-4.31 1.12-15.26 3.49-19.08-2.12Z"></path></svg>

	

	<span>Datasets</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Apandas"><div class="tag tag-white   "><svg class="text-black inline-block text-sm text-[#130754] dark:text-gray-200" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid slice" viewBox="0 0 210.21 280.43"><rect fill="currentColor" x="74.51" y="43.03" width="24.09" height="50.02"></rect><rect fill="currentColor" x="74.51" y="145.78" width="24.09" height="50.02"></rect><rect fill="#ffca00" x="74.51" y="107.65" width="24.09" height="23.6"></rect><rect fill="currentColor" x="35.81" y="84.15" width="24.09" height="166.27"></rect><rect fill="currentColor" x="112.41" y="187.05" width="24.09" height="50.02"></rect><rect fill="currentColor" x="112.41" y="84.21" width="24.09" height="50.02"></rect><rect fill="#e70488" x="112.41" y="148.84" width="24.09" height="23.6"></rect><rect fill="currentColor" x="150.3" y="30" width="24.09" height="166.27"></rect></svg>

	

	<span>pandas</span>
	

	</div></a><div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-lg rounded-br-none " type="button">
		<div class="tag tag-white   relative rounded-br-none pr-2.5"><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="none" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="#F5AB6A"></path><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="url(#paint0_radial_18_31665)"></path><g filter="url(#filter0_f_18_31665)"><path d="M22.2849 12.1817L23.4375 13.2656L24.4375 4.70312C23.5121 4.1242 23.0198 3.96369 22.6563 3.89062L22.2849 12.1817Z" fill="url(#paint1_linear_18_31665)"></path></g><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint2_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint3_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint4_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint5_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint6_linear_18_31665)"></path><g filter="url(#filter1_f_18_31665)"><path d="M13.1016 2.72656C11.862 3.06924 11.5298 3.40016 11.5298 3.40016C10.9102 3.52335 10.936 4.11525 11.2408 4.6799C13.1487 6.95202 15.1361 13.2496 18.007 14.0958C18.2707 14.1633 18.6953 14.2107 19.1797 14.2344L13.1016 2.72656Z" fill="url(#paint7_linear_18_31665)"></path></g><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint8_radial_18_31665)"></path><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint9_radial_18_31665)"></path><g filter="url(#filter2_f_18_31665)"><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint10_linear_18_31665)"></path><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint11_radial_18_31665)"></path></g><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="#EC9F6A"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint12_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint13_linear_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint14_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint15_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint16_radial_18_31665)"></path><g filter="url(#filter3_f_18_31665)"><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint17_linear_18_31665)"></path><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint18_linear_18_31665)"></path></g><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="#D79453"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint19_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint20_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint21_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint22_linear_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint23_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint24_radial_18_31665)"></path><defs><filter id="filter0_f_18_31665" x="22.0349" y="3.64062" width="2.65265" height="9.875" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter1_f_18_31665" x="10.7815" y="2.47656" width="8.64819" height="12.0078" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter2_f_18_31665" x="3.89062" y="22.5416" width="9.57812" height="2.2804" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter3_f_18_31665" x="2.45312" y="11.2538" width="12.1255" height="8.50903" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><radialGradient id="paint0_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.8125 12.9375) rotate(42.7741) scale(12.5164 7.08839)"><stop offset="0.0937591" stop-color="#C05159"></stop><stop offset="0.553697" stop-color="#F6AC6A"></stop><stop offset="0.832916" stop-color="#FFD186"></stop><stop offset="0.916927" stop-color="#FFDC87"></stop></radialGradient><linearGradient id="paint1_linear_18_31665" x1="24.7344" y1="4.67187" x2="20.8594" y2="12.8906" gradientUnits="userSpaceOnUse"><stop stop-color="#EBD67C"></stop><stop offset="0.0655686" stop-color="#FFFFA6"></stop><stop offset="0.530552" stop-color="#F8C281"></stop><stop offset="0.937338" stop-color="#E99E6B"></stop></linearGradient><radialGradient id="paint2_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.9009 13.1847) rotate(-127.648) scale(14.3438 11.7966)"><stop stop-color="#FFBE66"></stop><stop offset="1" stop-color="#E2AE5B"></stop></radialGradient><radialGradient id="paint3_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(18 11.4375) rotate(53.9726) scale(11.9013 4.84018)"><stop stop-color="#D67C63"></stop><stop offset="1" stop-color="#D97D67" stop-opacity="0"></stop></radialGradient><radialGradient id="paint4_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(23 4.1875) rotate(45.7639) scale(3.31486 5.75622)"><stop stop-color="#FFE4A6"></stop><stop offset="0.711285" stop-color="#F8B76F"></stop><stop offset="1" stop-color="#F9B870" stop-opacity="0"></stop></radialGradient><radialGradient id="paint5_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.875 12.4375) rotate(88.9391) scale(3.37558 1.29066)"><stop stop-color="#FFBC67"></stop><stop offset="1" stop-color="#FFBC67" stop-opacity="0"></stop></radialGradient><linearGradient id="paint6_linear_18_31665" x1="20.375" y1="15.6875" x2="20.125" y2="12.7813" gradientUnits="userSpaceOnUse"><stop offset="0.461609" stop-color="#B45077"></stop><stop offset="0.855389" stop-color="#B75077" stop-opacity="0"></stop></linearGradient><linearGradient id="paint7_linear_18_31665" x1="12.9375" y1="2.57056" x2="18.5625" y2="14.3891" gradientUnits="userSpaceOnUse"><stop stop-color="#DDC173"></stop><stop offset="0.485173" stop-color="#D59F65"></stop><stop offset="1" stop-color="#E49966"></stop></linearGradient><radialGradient id="paint8_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.5625 23.5) rotate(109.113) scale(6.68078 10.2578)"><stop offset="0.165756" stop-color="#FFBF7E"></stop><stop offset="0.827674" stop-color="#DF8C6D"></stop><stop offset="1" stop-color="#B05A66"></stop></radialGradient><radialGradient id="paint9_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.1875 26) rotate(41.0652) scale(8.37243 2.03649)"><stop stop-color="#FFD483"></stop><stop offset="1" stop-color="#FFD688" stop-opacity="0"></stop></radialGradient><linearGradient id="paint10_linear_18_31665" x1="3.96063" y1="23.794" x2="13.3748" y2="23.5143" gradientUnits="userSpaceOnUse"><stop stop-color="#A8716F"></stop><stop offset="0.103615" stop-color="#B37173"></stop><stop offset="0.225484" stop-color="#DB9F84"></stop><stop offset="0.799889" stop-color="#F1BB8A"></stop><stop offset="1" stop-color="#FFD780"></stop></linearGradient><radialGradient id="paint11_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4219 23.1719) rotate(-178.616) scale(3.23532 0.569081)"><stop offset="0.621498" stop-color="#AF5A3E"></stop><stop offset="1" stop-color="#B35445" stop-opacity="0"></stop></radialGradient><radialGradient id="paint12_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.625 19.125) rotate(-171.737) scale(15.2205 15.0767)"><stop offset="0.138435" stop-color="#FFB974"></stop><stop offset="0.403618" stop-color="#F2A56D"></stop><stop offset="0.925938" stop-color="#A16948"></stop></radialGradient><linearGradient id="paint13_linear_18_31665" x1="8.22184" y1="13.125" x2="6.81191" y2="15.4996" gradientUnits="userSpaceOnUse"><stop offset="0.610751" stop-color="#984847"></stop><stop offset="0.850075" stop-color="#9A4947" stop-opacity="0"></stop></linearGradient><radialGradient id="paint14_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.25 23.7461) scale(11.25 5.68361)"><stop stop-color="#C66364"></stop><stop offset="1" stop-color="#D4766B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint15_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.875 23.5313) scale(10.0937 1.29657)"><stop stop-color="#B64B4B"></stop><stop offset="1" stop-color="#C56158" stop-opacity="0"></stop></radialGradient><radialGradient id="paint16_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4375 19.875) rotate(-46.8882) scale(4.02385 7.51767)"><stop stop-color="#FFC083"></stop><stop offset="0.620218" stop-color="#FFBD7D" stop-opacity="0"></stop></radialGradient><linearGradient id="paint17_linear_18_31665" x1="2.8125" y1="13.0312" x2="14.5582" y2="18.9404" gradientUnits="userSpaceOnUse"><stop stop-color="#B89367"></stop><stop offset="1" stop-color="#C5835E"></stop></linearGradient><linearGradient id="paint18_linear_18_31665" x1="8.21875" y1="14.6406" x2="7.59349" y2="15.6717" gradientUnits="userSpaceOnUse"><stop offset="0.351552" stop-color="#A74746"></stop><stop offset="0.845198" stop-color="#A04346" stop-opacity="0"></stop></linearGradient><radialGradient id="paint19_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.5625 14.5625) rotate(140.244) scale(18.3733 13.7403)"><stop stop-color="#FDAE69"></stop><stop offset="0.729021" stop-color="#CE8C4F"></stop><stop offset="0.921546" stop-color="#AD7B45"></stop><stop offset="1" stop-color="#8B6B4A"></stop></radialGradient><radialGradient id="paint20_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.0625 7) rotate(65.3152) scale(11.0745 3.16547)"><stop offset="0.233237" stop-color="#FFD47C"></stop><stop offset="0.853648" stop-color="#FFD98B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint21_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.3125 8.875) rotate(100.886) scale(6.6191 5.57808)"><stop offset="0.128419" stop-color="#FFD88C"></stop><stop offset="0.924134" stop-color="#FFBE7B" stop-opacity="0"></stop></radialGradient><linearGradient id="paint22_linear_18_31665" x1="7.25" y1="15.1875" x2="10.7588" y2="10.3142" gradientUnits="userSpaceOnUse"><stop offset="0.142353" stop-color="#C15F4D"></stop><stop offset="1" stop-color="#D58366" stop-opacity="0"></stop></linearGradient><radialGradient id="paint23_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(8.15625 15.7813) rotate(28.5422) scale(12.5574 1.96589)"><stop offset="0.149989" stop-color="#E4745D"></stop><stop offset="0.453292" stop-color="#C8604C"></stop><stop offset="0.632597" stop-color="#C0605F"></stop><stop offset="1" stop-color="#C0605F" stop-opacity="0"></stop></radialGradient><radialGradient id="paint24_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(1.40625 2.69067) rotate(46.0943) scale(22.3963)"><stop offset="0.935802" stop-color="#C17C61" stop-opacity="0"></stop><stop offset="0.982109" stop-color="#C17C61"></stop></radialGradient></defs></svg>

	

	<span>Croissant</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	<button class="tag tag-ghost px-1! -ml-0.5 mb-1 md:mb-1.5" type="button">+ 1</button></div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">License:
	</span>
	<div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-full rounded-br-none " type="button">
		<div class="tag tag-white rounded-full  relative rounded-br-none pr-2.5">
		<svg class="text-xs text-gray-900" width="1em" height="1em" viewBox="0 0 10 10" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.46009 5.0945V6.88125C1.46009 7.25201 1.75937 7.55129 2.13012 7.55129C2.50087 7.55129 2.80016 7.25201 2.80016 6.88125V5.0945C2.80016 4.72375 2.50087 4.42446 2.13012 4.42446C1.75937 4.42446 1.46009 4.72375 1.46009 5.0945ZM4.14022 5.0945V6.88125C4.14022 7.25201 4.4395 7.55129 4.81026 7.55129C5.18101 7.55129 5.48029 7.25201 5.48029 6.88125V5.0945C5.48029 4.72375 5.18101 4.42446 4.81026 4.42446C4.4395 4.42446 4.14022 4.72375 4.14022 5.0945ZM1.23674 9.78473H8.38377C8.75452 9.78473 9.0538 9.48545 9.0538 9.1147C9.0538 8.74395 8.75452 8.44466 8.38377 8.44466H1.23674C0.865993 8.44466 0.566711 8.74395 0.566711 9.1147C0.566711 9.48545 0.865993 9.78473 1.23674 9.78473ZM6.82036 5.0945V6.88125C6.82036 7.25201 7.11964 7.55129 7.49039 7.55129C7.86114 7.55129 8.16042 7.25201 8.16042 6.88125V5.0945C8.16042 4.72375 7.86114 4.42446 7.49039 4.42446C7.11964 4.42446 6.82036 4.72375 6.82036 5.0945ZM4.39484 0.623142L0.865993 2.48137C0.682851 2.57517 0.566711 2.76725 0.566711 2.97273C0.566711 3.28094 0.816857 3.53109 1.12507 3.53109H8.49991C8.80365 3.53109 9.0538 3.28094 9.0538 2.97273C9.0538 2.76725 8.93766 2.57517 8.75452 2.48137L5.22568 0.623142C4.9666 0.484669 4.65391 0.484669 4.39484 0.623142V0.623142Z" fill="currentColor"></path></svg>

	

	<span>afl-3.0</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	</div></div>

		<div class="flex flex-col-reverse lg:flex-row lg:items-center lg:justify-between"><div class="-mb-px flex h-12 items-center overflow-x-auto overflow-y-hidden ">
	<a class="tab-alternate active" href="/datasets/CShorten/ML-ArXiv-Papers"><svg class="mr-1.5 text-gray-400 flex-none" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
	Dataset card
	

	
		</a><a class="tab-alternate" href="/datasets/CShorten/ML-ArXiv-Papers/viewer/"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	Data Studio
	

	
		</a><a class="tab-alternate" href="/datasets/CShorten/ML-ArXiv-Papers/tree/main"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M21 19h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-8h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0 4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M9 19a1 1 0 0 1-1-1V6a1 1 0 0 1 2 0v12a1 1 0 0 1-1 1zm-6-4.333a1 1 0 0 1-.64-1.769L3.438 12l-1.078-.898a1 1 0 0 1 1.28-1.538l2 1.667a1 1 0 0 1 0 1.538l-2 1.667a.999.999 0 0 1-.64.231z" fill="currentColor"></path></svg>
	<span class="xl:hidden">Files</span>
		<span class="hidden xl:inline">Files and versions</span>
	

	
		</a><a class="tab-alternate" href="/datasets/CShorten/ML-ArXiv-Papers/discussions"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M20.6081 3C21.7684 3 22.8053 3.49196 23.5284 4.38415C23.9756 4.93678 24.4428 5.82749 24.4808 7.16133C24.9674 7.01707 25.4353 6.93643 25.8725 6.93643C26.9833 6.93643 27.9865 7.37587 28.696 8.17411C29.6075 9.19872 30.0124 10.4579 29.8361 11.7177C29.7523 12.3177 29.5581 12.8555 29.2678 13.3534C29.8798 13.8646 30.3306 14.5763 30.5485 15.4322C30.719 16.1032 30.8939 17.5006 29.9808 18.9403C30.0389 19.0342 30.0934 19.1319 30.1442 19.2318C30.6932 20.3074 30.7283 21.5229 30.2439 22.6548C29.5093 24.3704 27.6841 25.7219 24.1397 27.1727C21.9347 28.0753 19.9174 28.6523 19.8994 28.6575C16.9842 29.4379 14.3477 29.8345 12.0653 29.8345C7.87017 29.8345 4.8668 28.508 3.13831 25.8921C0.356375 21.6797 0.754104 17.8269 4.35369 14.1131C6.34591 12.058 7.67023 9.02782 7.94613 8.36275C8.50224 6.39343 9.97271 4.20438 12.4172 4.20438H12.4179C12.6236 4.20438 12.8314 4.2214 13.0364 4.25468C14.107 4.42854 15.0428 5.06476 15.7115 6.02205C16.4331 5.09583 17.134 4.359 17.7682 3.94323C18.7242 3.31737 19.6794 3 20.6081 3ZM20.6081 5.95917C20.2427 5.95917 19.7963 6.1197 19.3039 6.44225C17.7754 7.44319 14.8258 12.6772 13.7458 14.7131C13.3839 15.3952 12.7655 15.6837 12.2086 15.6837C11.1036 15.6837 10.2408 14.5497 12.1076 13.1085C14.9146 10.9402 13.9299 7.39584 12.5898 7.1776C12.5311 7.16799 12.4731 7.16355 12.4172 7.16355C11.1989 7.16355 10.6615 9.33114 10.6615 9.33114C10.6615 9.33114 9.0863 13.4148 6.38031 16.206C3.67434 18.998 3.5346 21.2388 5.50675 24.2246C6.85185 26.2606 9.42666 26.8753 12.0653 26.8753C14.8021 26.8753 17.6077 26.2139 19.1799 25.793C19.2574 25.7723 28.8193 22.984 27.6081 20.6107C27.4046 20.212 27.0693 20.0522 26.6471 20.0522C24.9416 20.0522 21.8393 22.6726 20.5057 22.6726C20.2076 22.6726 19.9976 22.5416 19.9116 22.222C19.3433 20.1173 28.552 19.2325 27.7758 16.1839C27.639 15.6445 27.2677 15.4256 26.746 15.4263C24.4923 15.4263 19.4358 19.5181 18.3759 19.5181C18.2949 19.5181 18.2368 19.4937 18.2053 19.4419C17.6743 18.557 17.9653 17.9394 21.7082 15.6009C25.4511 13.2617 28.0783 11.8545 26.5841 10.1752C26.4121 9.98141 26.1684 9.8956 25.8725 9.8956C23.6001 9.89634 18.2311 14.9403 18.2311 14.9403C18.2311 14.9403 16.7821 16.496 15.9057 16.496C15.7043 16.496 15.533 16.4139 15.4169 16.2112C14.7956 15.1296 21.1879 10.1286 21.5484 8.06535C21.7928 6.66715 21.3771 5.95917 20.6081 5.95917Z" fill="#FF9D00"></path><path d="M5.50686 24.2246C3.53472 21.2387 3.67446 18.9979 6.38043 16.206C9.08641 13.4147 10.6615 9.33111 10.6615 9.33111C10.6615 9.33111 11.2499 6.95933 12.59 7.17757C13.93 7.39581 14.9139 10.9401 12.1069 13.1084C9.29997 15.276 12.6659 16.7489 13.7459 14.713C14.8258 12.6772 17.7747 7.44316 19.304 6.44221C20.8326 5.44128 21.9089 6.00204 21.5484 8.06532C21.188 10.1286 14.795 15.1295 15.4171 16.2118C16.0391 17.2934 18.2312 14.9402 18.2312 14.9402C18.2312 14.9402 25.0907 8.49588 26.5842 10.1752C28.0776 11.8545 25.4512 13.2616 21.7082 15.6008C17.9646 17.9393 17.6744 18.557 18.2054 19.4418C18.7372 20.3266 26.9998 13.1351 27.7759 16.1838C28.5513 19.2324 19.3434 20.1173 19.9117 22.2219C20.48 24.3274 26.3979 18.2382 27.6082 20.6107C28.8193 22.9839 19.2574 25.7722 19.18 25.7929C16.0914 26.62 8.24723 28.3726 5.50686 24.2246Z" fill="#FFD21E"></path></svg>
	Community
	

	
		</a></div>
	
			</div></div></header>
</div>
	
<div class="container relative flex flex-col md:grid md:space-y-0 w-full md:grid-cols-12 md:flex-1 md:grid-rows-full space-y-4 md:gap-6 ">
		<section class="pt-6 border-gray-100 md:col-span-8 pb-24 relative break-words copiable-code-container">
				<div class="SVELTE_HYDRATER contents" data-target="UnsafeBanner" data-props="{&quot;classNames&quot;:&quot;mb-4&quot;,&quot;repoId&quot;:&quot;CShorten/ML-ArXiv-Papers&quot;,&quot;repoType&quot;:&quot;dataset&quot;,&quot;minLevel&quot;:&quot;unsafe&quot;}"></div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetViewer" data-props="{&quot;data&quot;:{&quot;kind&quot;:&quot;DatasetAndSampleData&quot;,&quot;datasetInfo&quot;:[{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Size of downloaded dataset files:&quot;,&quot;value&quot;:&quot;147 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;/datasets/CShorten/ML-ArXiv-Papers/tree/refs%2Fconvert%2Fparquet/&quot;,&quot;label&quot;:&quot;Size of the auto-converted Parquet files:&quot;,&quot;value&quot;:&quot;85 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Number of rows:&quot;,&quot;value&quot;:&quot;117,592&quot;}],&quot;partial&quot;:false,&quot;configsData&quot;:{&quot;configInfos&quot;:[{&quot;name&quot;:&quot;default&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:117592}],&quot;selectedConfig&quot;:&quot;default&quot;,&quot;hasSelectedConfigParquet&quot;:true},&quot;splitsData&quot;:{&quot;splitInfos&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;numRows&quot;:117592}],&quot;selectedSplit&quot;:&quot;train&quot;},&quot;sampleData&quot;:{&quot;dataset&quot;:&quot;CShorten/ML-ArXiv-Papers&quot;,&quot;config&quot;:&quot;default&quot;,&quot;split&quot;:&quot;train&quot;,&quot;capabilities&quot;:{&quot;rows&quot;:true,&quot;search&quot;:true,&quot;filter&quot;:true,&quot;statistics&quot;:true},&quot;navigation&quot;:{&quot;p&quot;:0},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzAzMCwic3ViIjoiL2RhdGFzZXRzL0NTaG9ydGVuL01MLUFyWGl2LVBhcGVycyIsImV4cCI6MTc0MjkyNjYzMCwiaXNzIjoiaHR0cHM6Ly9odWdnaW5nZmFjZS5jbyJ9.GYk9NFULOBdWR2CIwQ6be5UnPXot795i0T4Dggf4MEuLGqgcgGkfBSQcoqdcRUFaAsOyA7akJutVCP0KY-GMAQ&quot;,&quot;sampleData&quot;:{&quot;columns&quot;:[{&quot;name&quot;:&quot;Unnamed: 0.1&quot;,&quot;align&quot;:&quot;right&quot;,&quot;type&quot;:&quot;int64&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;Unnamed: 0.1&quot;,&quot;column_type&quot;:&quot;int&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:0,&quot;max&quot;:112591,&quot;mean&quot;:54008.0995,&quot;median&quot;:53795.5,&quot;std&quot;:33606.74941,&quot;histogram&quot;:{&quot;hist&quot;:[16260,11260,11260,11260,11260,11260,11260,11260,11260,11252],&quot;bin_edges&quot;:[0,11260,22520,33780,45040,56300,67560,78820,90080,101340,112591]}}}},{&quot;name&quot;:&quot;Unnamed: 0&quot;,&quot;align&quot;:&quot;right&quot;,&quot;type&quot;:&quot;float64&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;Unnamed: 0&quot;,&quot;column_type&quot;:&quot;float&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:5000,&quot;nan_proportion&quot;:0.04252,&quot;min&quot;:0,&quot;max&quot;:112591,&quot;mean&quot;:56295.5,&quot;median&quot;:56295.5,&quot;std&quot;:32502.65509,&quot;histogram&quot;:{&quot;hist&quot;:[11260,11259,11259,11259,11259,11259,11259,11259,11259,11260],&quot;bin_edges&quot;:[0,11259.1,22518.2,33777.3,45036.4,56295.5,67554.6,78813.7,90072.8,101331.9,112591]}}}},{&quot;name&quot;:&quot;title&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;title&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:7,&quot;max&quot;:246,&quot;mean&quot;:73.48347,&quot;median&quot;:71,&quot;std&quot;:24.62741,&quot;histogram&quot;:{&quot;hist&quot;:[2885,23907,42950,34495,10441,2313,468,93,29,11],&quot;bin_edges&quot;:[7,31,55,79,103,127,151,175,199,223,246]}}}},{&quot;name&quot;:&quot;abstract&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;abstract&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:6,&quot;max&quot;:3312,&quot;mean&quot;:1157.39331,&quot;median&quot;:1141,&quot;std&quot;:342.0902,&quot;histogram&quot;:{&quot;hist&quot;:[539,7869,31092,42431,25247,10397,10,5,1,1],&quot;bin_edges&quot;:[6,337,668,999,1330,1661,1992,2323,2654,2985,3312]}}}}],&quot;rows&quot;:[{&quot;rowIdx&quot;:0,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:0,&quot;string&quot;:&quot;0&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Learning from compressed observations&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The problem of statistical learning is to construct a predictor of a random\nvariable $Y$ as a function of a related random variable $X$ on the basis of an\ni.i.d. training sample from the joint distribution of $(X,Y)$. Allowable\npredictors are drawn from some specified class, and the goal is to approach\nasymptotically the performance (expected loss) of the best predictor in the\nclass. We consider the setting in which one has perfect observation of the\n$X$-part of the sample, while the $Y$-part has to be communicated at some\nfinite bit rate. The encoding of the $Y$-values is allowed to depend on the\n$X$-values. Under suitable regularity conditions on the admissible predictors,\nthe underlying family of probability distributions and the loss function, we\ngive an information-theoretic characterization of achievable predictor\nperformance in terms of conditional distortion-rate functions. The ideas are\nillustrated on the example of nonparametric regression in Gaussian noise.\n&quot;}}},{&quot;rowIdx&quot;:1,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1,&quot;string&quot;:&quot;1&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1,&quot;string&quot;:&quot;1&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Sensor Networks with Random Links: Topology Design for Distributed\n  Consensus&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In a sensor network, in practice, the communication among sensors is subject\nto:(1) errors or failures at random times; (3) costs; and(2) constraints since\nsensors and networks operate under scarce resources, such as power, data rate,\nor communication. The signal-to-noise ratio (SNR) is usually a main factor in\ndetermining the probability of error (or of communication failure) in a link.\nThese probabilities are then a proxy for the SNR under which the links operate.\nThe paper studies the problem of designing the topology, i.e., assigning the\nprobabilities of reliable communication among sensors (or of link failures) to\nmaximize the rate of convergence of average consensus, when the link\ncommunication costs are taken into account, and there is an overall\ncommunication budget constraint. To consider this problem, we address a number\nof preliminary issues: (1) model the network as a random topology; (2)\nestablish necessary and sufficient conditions for mean square sense (mss) and\nalmost sure (a.s.) convergence of average consensus when network links fail;\nand, in particular, (3) show that a necessary and sufficient condition for both\nmss and a.s. convergence is for the algebraic connectivity of the mean graph\ndescribing the network topology to be strictly positive. With these results, we\nformulate topology design, subject to random link failures and to a\ncommunication cost constraint, as a constrained convex optimization problem to\nwhich we apply semidefinite programming techniques. We show by an extensive\nnumerical study that the optimal design improves significantly the convergence\nspeed of the consensus algorithm and can achieve the asymptotic performance of\na non-random network at a fraction of the communication cost.\n&quot;}}},{&quot;rowIdx&quot;:2,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2,&quot;string&quot;:&quot;2&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:2,&quot;string&quot;:&quot;2&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The on-line shortest path problem under partial monitoring&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The on-line shortest path problem is considered under various models of\npartial monitoring. Given a weighted directed acyclic graph whose edge weights\ncan change in an arbitrary (adversarial) way, a decision maker has to choose in\neach round of a game a path between two distinguished vertices such that the\nloss of the chosen path (defined as the sum of the weights of its composing\nedges) be as small as possible. In a setting generalizing the multi-armed\nbandit problem, after choosing a path, the decision maker learns only the\nweights of those edges that belong to the chosen path. For this problem, an\nalgorithm is given whose average cumulative loss in n rounds exceeds that of\nthe best path, matched off-line to the entire sequence of the edge weights, by\na quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on\nthe number of edges of the graph. The algorithm can be implemented with linear\ncomplexity in the number of rounds n and in the number of edges. An extension\nto the so-called label efficient setting is also given, in which the decision\nmaker is informed about the weights of the edges corresponding to the chosen\npath at a total of m << n time instances. Another extension is shown where the\ndecision maker competes against a time-varying path, a generalization of the\nproblem of tracking the best expert. A version of the multi-armed bandit\nsetting for shortest path is also discussed where the decision maker learns\nonly the total weight of the chosen path but not the weights of the individual\nedges on the path. Applications to routing in packet switched networks along\nwith simulation results are also presented.\n&quot;}}},{&quot;rowIdx&quot;:3,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:3,&quot;string&quot;:&quot;3&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:3,&quot;string&quot;:&quot;3&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A neural network approach to ordinal regression&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Ordinal regression is an important type of learning, which has properties of\nboth classification and regression. Here we describe a simple and effective\napproach to adapt a traditional neural network to learn ordinal categories. Our\napproach is a generalization of the perceptron method for ordinal regression.\nOn several benchmark datasets, our method (NNRank) outperforms a neural network\nclassification method. Compared with the ordinal regression methods using\nGaussian processes and support vector machines, NNRank achieves comparable\nperformance. Moreover, NNRank has the advantages of traditional neural\nnetworks: learning in both online and batch modes, handling very large training\ndatasets, and making rapid predictions. These features make NNRank a useful and\ncomplementary tool for large-scale data processing tasks such as information\nretrieval, web page ranking, collaborative filtering, and protein ranking in\nBioinformatics.\n&quot;}}},{&quot;rowIdx&quot;:4,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:4,&quot;string&quot;:&quot;4&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:4,&quot;string&quot;:&quot;4&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Parametric Learning and Monte Carlo Optimization&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This paper uncovers and explores the close relationship between Monte Carlo\nOptimization of a parametrized integral (MCO), Parametric machine-Learning\n(PL), and `blackbox' or `oracle'-based optimization (BO). We make four\ncontributions. First, we prove that MCO is mathematically identical to a broad\nclass of PL problems. This identity potentially provides a new application\ndomain for all broadly applicable PL techniques: MCO. Second, we introduce\nimmediate sampling, a new version of the Probability Collectives (PC) algorithm\nfor blackbox optimization. Immediate sampling transforms the original BO\nproblem into an MCO problem. Accordingly, by combining these first two\ncontributions, we can apply all PL techniques to BO. In our third contribution\nwe validate this way of improving BO by demonstrating that cross-validation and\nbagging improve immediate sampling. Finally, conventional MC and MCO procedures\nignore the relationship between the sample point locations and the associated\nvalues of the integrand; only the values of the integrand at those locations\nare considered. We demonstrate that one can exploit the sample location\ninformation using PL techniques, for example by forming a fit of the sample\nlocations to the associated values of the integrand. This provides an\nadditional way to apply PL techniques to improve MCO.\n&quot;}}},{&quot;rowIdx&quot;:5,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:5,&quot;string&quot;:&quot;5&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:5,&quot;string&quot;:&quot;5&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Preconditioned Temporal Difference Learning&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This paper has been withdrawn by the author. This draft is withdrawn for its\npoor quality in english, unfortunately produced by the author when he was just\nstarting his science route. Look at the ICML version instead:\nhttp://icml2008.cs.helsinki.fi/papers/111.pdf\n&quot;}}},{&quot;rowIdx&quot;:6,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:6,&quot;string&quot;:&quot;6&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:6,&quot;string&quot;:&quot;6&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Note on the Inapproximability of Correlation Clustering&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We consider inapproximability of the correlation clustering problem defined\nas follows: Given a graph $G = (V,E)$ where each edge is labeled either \&quot;+\&quot;\n(similar) or \&quot;-\&quot; (dissimilar), correlation clustering seeks to partition the\nvertices into clusters so that the number of pairs correctly (resp.\nincorrectly) classified with respect to the labels is maximized (resp.\nminimized). The two complementary problems are called MaxAgree and MinDisagree,\nrespectively, and have been studied on complete graphs, where every edge is\nlabeled, and general graphs, where some edge might not have been labeled.\nNatural edge-weighted versions of both problems have been studied as well. Let\nS-MaxAgree denote the weighted problem where all weights are taken from set S,\nwe show that S-MaxAgree with weights bounded by $O(|V|^{1/2-\\delta})$\nessentially belongs to the same hardness class in the following sense: if there\nis a polynomial time algorithm that approximates S-MaxAgree within a factor of\n$\\lambda = O(\\log{|V|})$ with high probability, then for any choice of S',\nS'-MaxAgree can be approximated in polynomial time within a factor of $(\\lambda\n+ \\epsilon)$, where $\\epsilon > 0$ can be arbitrarily small, with high\nprobability. A similar statement also holds for $S-MinDisagree. This result\nimplies it is hard (assuming $NP \\neq RP$) to approximate unweighted MaxAgree\nwithin a factor of $80/79-\\epsilon$, improving upon a previous known factor of\n$116/115-\\epsilon$ by Charikar et. al. \\cite{Chari05}.\n&quot;}}},{&quot;rowIdx&quot;:7,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:7,&quot;string&quot;:&quot;7&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:7,&quot;string&quot;:&quot;7&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Joint universal lossy coding and identification of stationary mixing\n  sources&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The problem of joint universal source coding and modeling, treated in the\ncontext of lossless codes by Rissanen, was recently generalized to fixed-rate\nlossy coding of finitely parametrized continuous-alphabet i.i.d. sources. We\nextend these results to variable-rate lossy block coding of stationary ergodic\nsources and show that, for bounded metric distortion measures, any finitely\nparametrized family of stationary sources satisfying suitable mixing,\nsmoothness and Vapnik-Chervonenkis learnability conditions admits universal\nschemes for joint lossy source coding and identification. We also give several\nexplicit examples of parametric sources satisfying the regularity conditions.\n&quot;}}},{&quot;rowIdx&quot;:8,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:8,&quot;string&quot;:&quot;8&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:8,&quot;string&quot;:&quot;8&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Supervised Feature Selection via Dependence Estimation&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We introduce a framework for filtering features that employs the\nHilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence\nbetween the features and the labels. The key idea is that good features should\nmaximise such dependence. Feature selection for various supervised learning\nproblems (including classification and regression) is unified under this\nframework, and the solutions can be approximated using a backward-elimination\nalgorithm. We demonstrate the usefulness of our method on both artificial and\nreal world datasets.\n&quot;}}},{&quot;rowIdx&quot;:9,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:9,&quot;string&quot;:&quot;9&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:9,&quot;string&quot;:&quot;9&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Equivalence of LP Relaxation and Max-Product for Weighted Matching in\n  General Graphs&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Max-product belief propagation is a local, iterative algorithm to find the\nmode/MAP estimate of a probability distribution. While it has been successfully\nemployed in a wide variety of applications, there are relatively few\ntheoretical guarantees of convergence and correctness for general loopy graphs\nthat may have many short cycles. Of these, even fewer provide exact ``necessary\nand sufficient'' characterizations.\n  In this paper we investigate the problem of using max-product to find the\nmaximum weight matching in an arbitrary graph with edge weights. This is done\nby first constructing a probability distribution whose mode corresponds to the\noptimal matching, and then running max-product. Weighted matching can also be\nposed as an integer program, for which there is an LP relaxation. This\nrelaxation is not always tight. In this paper we show that \\begin{enumerate}\n\\item If the LP relaxation is tight, then max-product always converges, and\nthat too to the correct answer. \\item If the LP relaxation is loose, then\nmax-product does not converge. \\end{enumerate} This provides an exact,\ndata-dependent characterization of max-product performance, and a precise\nconnection to LP relaxation, which is a well-studied optimization technique.\nAlso, since LP relaxation is known to be tight for bipartite graphs, our\nresults generalize other recent results on using max-product to find weighted\nmatchings in bipartite graphs.\n&quot;}}},{&quot;rowIdx&quot;:10,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:10,&quot;string&quot;:&quot;10&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:10,&quot;string&quot;:&quot;10&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;HMM Speaker Identification Using Linear and Non-linear Merging\n  Techniques&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Speaker identification is a powerful, non-invasive and in-expensive biometric\ntechnique. The recognition accuracy, however, deteriorates when noise levels\naffect a specific band of frequency. In this paper, we present a sub-band based\nspeaker identification that intends to improve the live testing performance.\nEach frequency sub-band is processed and classified independently. We also\ncompare the linear and non-linear merging techniques for the sub-bands\nrecognizer. Support vector machines and Gaussian Mixture models are the\nnon-linear merging techniques that are investigated. Results showed that the\nsub-band based method used with linear merging techniques enormously improved\nthe performance of the speaker identification over the performance of wide-band\nrecognizers when tested live. A live testing improvement of 9.78% was achieved\n&quot;}}},{&quot;rowIdx&quot;:11,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:11,&quot;string&quot;:&quot;11&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:11,&quot;string&quot;:&quot;11&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Statistical Mechanics of Nonlinear On-line Learning for Ensemble\n  Teachers&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We analyze the generalization performance of a student in a model composed of\nnonlinear perceptrons: a true teacher, ensemble teachers, and the student. We\ncalculate the generalization error of the student analytically or numerically\nusing statistical mechanics in the framework of on-line learning. We treat two\nwell-known learning rules: Hebbian learning and perceptron learning. As a\nresult, it is proven that the nonlinear model shows qualitatively different\nbehaviors from the linear model. Moreover, it is clarified that Hebbian\nlearning and perceptron learning show qualitatively different behaviors from\neach other. In Hebbian learning, we can analytically obtain the solutions. In\nthis case, the generalization error monotonically decreases. The steady value\nof the generalization error is independent of the learning rate. The larger the\nnumber of teachers is and the more variety the ensemble teachers have, the\nsmaller the generalization error is. In perceptron learning, we have to\nnumerically obtain the solutions. In this case, the dynamical behaviors of the\ngeneralization error are non-monotonic. The smaller the learning rate is, the\nlarger the number of teachers is; and the more variety the ensemble teachers\nhave, the smaller the minimum value of the generalization error is.\n&quot;}}},{&quot;rowIdx&quot;:12,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:12,&quot;string&quot;:&quot;12&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:12,&quot;string&quot;:&quot;12&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;On the monotonization of the training set&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We consider the problem of minimal correction of the training set to make it\nconsistent with monotonic constraints. This problem arises during analysis of\ndata sets via techniques that require monotone data. We show that this problem\nis NP-hard in general and is equivalent to finding a maximal independent set in\nspecial orgraphs. Practically important cases of that problem considered in\ndetail. These are the cases when a partial order given on the replies set is a\ntotal order or has a dimension 2. We show that the second case can be reduced\nto maximization of a quadratic convex function on a convex set. For this case\nwe construct an approximate polynomial algorithm based on convex optimization.\n&quot;}}},{&quot;rowIdx&quot;:13,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:13,&quot;string&quot;:&quot;13&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:13,&quot;string&quot;:&quot;13&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Mixed membership stochastic blockmodels&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Observations consisting of measurements on relationships for pairs of objects\narise in many settings, such as protein interaction and gene regulatory\nnetworks, collections of author-recipient email, and social networks. Analyzing\nsuch data with probabilisic models can be delicate because the simple\nexchangeability assumptions underlying many boilerplate models no longer hold.\nIn this paper, we describe a latent variable model of such data called the\nmixed membership stochastic blockmodel. This model extends blockmodels for\nrelational data to ones which capture mixed membership latent relational\nstructure, thus providing an object-specific low-dimensional representation. We\ndevelop a general variational inference algorithm for fast approximate\nposterior inference. We explore applications to social and protein interaction\nnetworks.\n&quot;}}},{&quot;rowIdx&quot;:14,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:14,&quot;string&quot;:&quot;14&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:14,&quot;string&quot;:&quot;14&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Loop corrections for message passing algorithms in continuous variable\n  models&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper we derive the equations for Loop Corrected Belief Propagation\non a continuous variable Gaussian model. Using the exactness of the averages\nfor belief propagation for Gaussian models, a different way of obtaining the\ncovariances is found, based on Belief Propagation on cavity graphs. We discuss\nthe relation of this loop correction algorithm to Expectation Propagation\nalgorithms for the case in which the model is no longer Gaussian, but slightly\nperturbed by nonlinear terms.\n&quot;}}},{&quot;rowIdx&quot;:15,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:15,&quot;string&quot;:&quot;15&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:15,&quot;string&quot;:&quot;15&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Novel Model of Working Set Selection for SMO Decomposition Methods&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In the process of training Support Vector Machines (SVMs) by decomposition\nmethods, working set selection is an important technique, and some exciting\nschemes were employed into this field. To improve working set selection, we\npropose a new model for working set selection in sequential minimal\noptimization (SMO) decomposition methods. In this model, it selects B as\nworking set without reselection. Some properties are given by simple proof, and\nexperiments demonstrate that the proposed method is in general faster than\nexisting methods.\n&quot;}}},{&quot;rowIdx&quot;:16,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:16,&quot;string&quot;:&quot;16&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:16,&quot;string&quot;:&quot;16&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Getting started in probabilistic graphical models&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Probabilistic graphical models (PGMs) have become a popular tool for\ncomputational analysis of biological data in a variety of domains. But, what\nexactly are they and how do they work? How can we use PGMs to discover patterns\nthat are biologically relevant? And to what extent can PGMs help us formulate\nnew hypotheses that are testable at the bench? This note sketches out some\nanswers and illustrates the main ideas behind the statistical approach to\nbiological pattern discovery.\n&quot;}}},{&quot;rowIdx&quot;:17,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:17,&quot;string&quot;:&quot;17&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:17,&quot;string&quot;:&quot;17&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A tutorial on conformal prediction&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Conformal prediction uses past experience to determine precise levels of\nconfidence in new predictions. Given an error probability $\\epsilon$, together\nwith a method that makes a prediction $\\hat{y}$ of a label $y$, it produces a\nset of labels, typically containing $\\hat{y}$, that also contains $y$ with\nprobability $1-\\epsilon$. Conformal prediction can be applied to any method for\nproducing $\\hat{y}$: a nearest-neighbor method, a support-vector machine, ridge\nregression, etc.\n  Conformal prediction is designed for an on-line setting in which labels are\npredicted successively, each one being revealed before the next is predicted.\nThe most novel and valuable feature of conformal prediction is that if the\nsuccessive examples are sampled independently from the same distribution, then\nthe successive predictions will be right $1-\\epsilon$ of the time, even though\nthey are based on an accumulating dataset rather than on independent datasets.\n  In addition to the model under which successive examples are sampled\nindependently, other on-line compression models can also use conformal\nprediction. The widely used Gaussian linear model is one of these.\n  This tutorial presents a self-contained account of the theory of conformal\nprediction and works through several numerical examples. A more comprehensive\ntreatment of the topic is provided in \&quot;Algorithmic Learning in a Random World\&quot;,\nby Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).\n&quot;}}},{&quot;rowIdx&quot;:18,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:18,&quot;string&quot;:&quot;18&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:18,&quot;string&quot;:&quot;18&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Scale-sensitive Psi-dimensions: the Capacity Measures for Classifiers\n  Taking Values in R^Q&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Bounds on the risk play a crucial role in statistical learning theory. They\nusually involve as capacity measure of the model studied the VC dimension or\none of its extensions. In classification, such \&quot;VC dimensions\&quot; exist for models\ntaking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations\nappropriate for the missing case, the one of models with values in R^Q. This\nprovides us with a new guaranteed risk for M-SVMs which appears superior to the\nexisting one.\n&quot;}}},{&quot;rowIdx&quot;:19,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:19,&quot;string&quot;:&quot;19&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:19,&quot;string&quot;:&quot;19&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The Role of Time in the Creation of Knowledge&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This paper I assume that in humans the creation of knowledge depends on a\ndiscrete time, or stage, sequential decision-making process subjected to a\nstochastic, information transmitting environment. For each time-stage, this\nenvironment randomly transmits Shannon type information-packets to the\ndecision-maker, who examines each of them for relevancy and then determines his\noptimal choices. Using this set of relevant information-packets, the\ndecision-maker adapts, over time, to the stochastic nature of his environment,\nand optimizes the subjective expected rate-of-growth of knowledge. The\ndecision-maker's optimal actions, lead to a decision function that involves,\nover time, his view of the subjective entropy of the environmental process and\nother important parameters at each time-stage of the process. Using this model\nof human behavior, one could create psychometric experiments using computer\nsimulation and real decision-makers, to play programmed games to measure the\nresulting human performance.\n&quot;}}},{&quot;rowIdx&quot;:20,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:20,&quot;string&quot;:&quot;20&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:20,&quot;string&quot;:&quot;20&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Clustering and Feature Selection using Sparse Principal Component\n  Analysis&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper, we study the application of sparse principal component\nanalysis (PCA) to clustering and feature selection problems. Sparse PCA seeks\nsparse factors, or linear combinations of the data variables, explaining a\nmaximum amount of variance in the data while having only a limited number of\nnonzero coefficients. PCA is often used as a simple clustering technique and\nsparse factors allow us here to interpret the clusters in terms of a reduced\nset of variables. We begin with a brief introduction and motivation on sparse\nPCA and detail our implementation of the algorithm in d'Aspremont et al.\n(2005). We then apply these results to some classic clustering and feature\nselection problems arising in biology.\n&quot;}}},{&quot;rowIdx&quot;:21,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:21,&quot;string&quot;:&quot;21&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:21,&quot;string&quot;:&quot;21&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Model Selection Through Sparse Maximum Likelihood Estimation&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We consider the problem of estimating the parameters of a Gaussian or binary\ndistribution in such a way that the resulting undirected graphical model is\nsparse. Our approach is to solve a maximum likelihood problem with an added\nl_1-norm penalty term. The problem as formulated is convex but the memory\nrequirements and complexity of existing interior point methods are prohibitive\nfor problems with more than tens of nodes. We present two new algorithms for\nsolving problems with at least a thousand nodes in the Gaussian case. Our first\nalgorithm uses block coordinate descent, and can be interpreted as recursive\nl_1-norm penalized regression. Our second algorithm, based on Nesterov's first\norder method, yields a complexity estimate with a better dependence on problem\nsize than existing interior point methods. Using a log determinant relaxation\nof the log partition function (Wainwright &amp; Jordan (2006)), we show that these\nsame algorithms can be used to solve an approximate sparse maximum likelihood\nproblem for the binary case. We test our algorithms on synthetic data, as well\nas on gene expression and senate voting records data.\n&quot;}}},{&quot;rowIdx&quot;:22,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:22,&quot;string&quot;:&quot;22&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:22,&quot;string&quot;:&quot;22&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Optimal Solutions for Sparse Principal Component Analysis&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Given a sample covariance matrix, we examine the problem of maximizing the\nvariance explained by a linear combination of the input variables while\nconstraining the number of nonzero coefficients in this combination. This is\nknown as sparse principal component analysis and has a wide array of\napplications in machine learning and engineering. We formulate a new\nsemidefinite relaxation to this problem and derive a greedy algorithm that\ncomputes a full set of good solutions for all target numbers of non zero\ncoefficients, with total complexity O(n^3), where n is the number of variables.\nWe then use the same relaxation to derive sufficient conditions for global\noptimality of a solution, which can be tested in O(n^3) per pattern. We discuss\napplications in subset selection and sparse recovery and show on artificial\nexamples and biological data that our algorithm does provide globally optimal\nsolutions in many cases.\n&quot;}}},{&quot;rowIdx&quot;:23,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:23,&quot;string&quot;:&quot;23&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:23,&quot;string&quot;:&quot;23&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A New Generalization of Chebyshev Inequality for Random Vectors&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this article, we derive a new generalization of Chebyshev inequality for\nrandom vectors. We demonstrate that the new generalization is much less\nconservative than the classical generalization.\n&quot;}}},{&quot;rowIdx&quot;:24,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:24,&quot;string&quot;:&quot;24&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:24,&quot;string&quot;:&quot;24&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Clusters, Graphs, and Networks for Analysing Internet-Web-Supported\n  Communication within a Virtual Community&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The proposal is to use clusters, graphs and networks as models in order to\nanalyse the Web structure. Clusters, graphs and networks provide knowledge\nrepresentation and organization. Clusters were generated by co-site analysis.\nThe sample is a set of academic Web sites from the countries belonging to the\nEuropean Union. These clusters are here revisited from the point of view of\ngraph theory and social network analysis. This is a quantitative and structural\nanalysis. In fact, the Internet is a computer network that connects people and\norganizations. Thus we may consider it to be a social network. The set of Web\nacademic sites represents an empirical social network, and is viewed as a\nvirtual community. The network structural properties are here analysed applying\ntogether cluster analysis, graph theory and social network analysis.\n&quot;}}},{&quot;rowIdx&quot;:25,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:25,&quot;string&quot;:&quot;25&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:25,&quot;string&quot;:&quot;25&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Universal Reinforcement Learning&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We consider an agent interacting with an unmodeled environment. At each time,\nthe agent makes an observation, takes an action, and incurs a cost. Its actions\ncan influence future observations and costs. The goal is to minimize the\nlong-term average cost. We propose a novel algorithm, known as the active LZ\nalgorithm, for optimal control based on ideas from the Lempel-Ziv scheme for\nuniversal data compression and prediction. We establish that, under the active\nLZ algorithm, if there exists an integer $K$ such that the future is\nconditionally independent of the past given a window of $K$ consecutive actions\nand observations, then the average cost converges to the optimum. Experimental\nresults involving the game of Rock-Paper-Scissors illustrate merits of the\nalgorithm.\n&quot;}}},{&quot;rowIdx&quot;:26,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:26,&quot;string&quot;:&quot;26&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:26,&quot;string&quot;:&quot;26&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Consistency of the group Lasso and multiple kernel learning&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We consider the least-square regression problem with regularization by a\nblock 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger\nthan one. This problem, referred to as the group Lasso, extends the usual\nregularization by the 1-norm where all spaces have dimension one, where it is\ncommonly referred to as the Lasso. In this paper, we study the asymptotic model\nconsistency of the group Lasso. We derive necessary and sufficient conditions\nfor the consistency of group Lasso under practical assumptions, such as model\nmisspecification. When the linear predictors and Euclidean norms are replaced\nby functions and reproducing kernel Hilbert norms, the problem is usually\nreferred to as multiple kernel learning and is commonly used for learning from\nheterogeneous data sources and for non linear variable selection. Using tools\nfrom functional analysis, and in particular covariance operators, we extend the\nconsistency results to this infinite dimensional case and also propose an\nadaptive scheme to obtain a consistent model estimate, even when the necessary\ncondition required for the non adaptive scheme is not satisfied.\n&quot;}}},{&quot;rowIdx&quot;:27,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:27,&quot;string&quot;:&quot;27&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:27,&quot;string&quot;:&quot;27&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Quantum Algorithms for Learning and Testing Juntas&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this article we develop quantum algorithms for learning and testing\njuntas, i.e. Boolean functions which depend only on an unknown set of k out of\nn input variables. Our aim is to develop efficient algorithms:\n  - whose sample complexity has no dependence on n, the dimension of the domain\nthe Boolean functions are defined over;\n  - with no access to any classical or quantum membership (\&quot;black-box\&quot;)\nqueries. Instead, our algorithms use only classical examples generated\nuniformly at random and fixed quantum superpositions of such classical\nexamples;\n  - which require only a few quantum examples but possibly many classical\nrandom examples (which are considered quite \&quot;cheap\&quot; relative to quantum\nexamples).\n  Our quantum algorithms are based on a subroutine FS which enables sampling\naccording to the Fourier spectrum of f; the FS subroutine was used in earlier\nwork of Bshouty and Jackson on quantum learning. Our results are as follows:\n  - We give an algorithm for testing k-juntas to accuracy $\\epsilon$ that uses\n$O(k/\\epsilon)$ quantum examples. This improves on the number of examples used\nby the best known classical algorithm.\n  - We establish the following lower bound: any FS-based k-junta testing\nalgorithm requires $\\Omega(\\sqrt{k})$ queries.\n  - We give an algorithm for learning $k$-juntas to accuracy $\\epsilon$ that\nuses $O(\\epsilon^{-1} k\\log k)$ quantum examples and $O(2^k \\log(1/\\epsilon))$\nrandom examples. We show that this learning algorithms is close to optimal by\ngiving a related lower bound.\n&quot;}}},{&quot;rowIdx&quot;:28,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:28,&quot;string&quot;:&quot;28&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:28,&quot;string&quot;:&quot;28&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Virtual screening with support vector machines and structure kernels&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Support vector machines and kernel methods have recently gained considerable\nattention in chemoinformatics. They offer generally good performance for\nproblems of supervised classification or regression, and provide a flexible and\ncomputationally efficient framework to include relevant information and prior\nknowledge about the data and problems to be handled. In particular, with kernel\nmethods molecules do not need to be represented and stored explicitly as\nvectors or fingerprints, but only to be compared to each other through a\ncomparison function technically called a kernel. While classical kernels can be\nused to compare vector or fingerprint representations of molecules, completely\nnew kernels were developed in the recent years to directly compare the 2D or 3D\nstructures of molecules, without the need for an explicit vectorization step\nthrough the extraction of molecular descriptors. While still in their infancy,\nthese approaches have already demonstrated their relevance on several toxicity\nprediction and structure-activity relationship problems.\n&quot;}}},{&quot;rowIdx&quot;:29,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:29,&quot;string&quot;:&quot;29&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:29,&quot;string&quot;:&quot;29&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Structure or Noise?&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We show how rate-distortion theory provides a mechanism for automated theory\nbuilding by naturally distinguishing between regularity and randomness. We\nstart from the simple principle that model variables should, as much as\npossible, render the future and past conditionally independent. From this, we\nconstruct an objective function for model making whose extrema embody the\ntrade-off between a model's structural complexity and its predictive power. The\nsolutions correspond to a hierarchy of models that, at each level of\ncomplexity, achieve optimal predictive power at minimal cost. In the limit of\nmaximal prediction the resulting optimal model identifies a process's intrinsic\norganization by extracting the underlying causal states. In this limit, the\nmodel's complexity is given by the statistical complexity, which is known to be\nminimal for achieving maximum prediction. Examples show how theory building can\nprofit from analyzing a process's causal compressibility, which is reflected in\nthe optimal models' rate-distortion curve--the process's characteristic for\noptimally balancing structure and noise at different levels of representation.\n&quot;}}},{&quot;rowIdx&quot;:30,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:30,&quot;string&quot;:&quot;30&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:30,&quot;string&quot;:&quot;30&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Cost-minimising strategies for data labelling : optimal stopping and\n  active learning&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Supervised learning deals with the inference of a distribution over an output\nor label space $\\CY$ conditioned on points in an observation space $\\CX$, given\na training dataset $D$ of pairs in $\\CX \\times \\CY$. However, in a lot of\napplications of interest, acquisition of large amounts of observations is easy,\nwhile the process of generating labels is time-consuming or costly. One way to\ndeal with this problem is {\\em active} learning, where points to be labelled\nare selected with the aim of creating a model with better performance than that\nof an model trained on an equal number of randomly sampled points. In this\npaper, we instead propose to deal with the labelling cost directly: The\nlearning goal is defined as the minimisation of a cost which is a function of\nthe expected model performance and the total cost of the labels used. This\nallows the development of general strategies and specific algorithms for (a)\noptimal stopping, where the expected cost dictates whether label acquisition\nshould continue (b) empirical evaluation, where the cost is used as a\nperformance metric for a given combination of inference, stopping and sampling\nmethods. Though the main focus of the paper is optimal stopping, we also aim to\nprovide the background for further developments and discussion in the related\nfield of active learning.\n&quot;}}},{&quot;rowIdx&quot;:31,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:31,&quot;string&quot;:&quot;31&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:31,&quot;string&quot;:&quot;31&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Defensive forecasting for optimal prediction with expert advice&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The method of defensive forecasting is applied to the problem of prediction\nwith expert advice for binary outcomes. It turns out that defensive forecasting\nis not only competitive with the Aggregating Algorithm but also handles the\ncase of \&quot;second-guessing\&quot; experts, whose advice depends on the learner's\nprediction; this paper assumes that the dependence on the learner's prediction\nis continuous.\n&quot;}}},{&quot;rowIdx&quot;:32,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:32,&quot;string&quot;:&quot;32&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:32,&quot;string&quot;:&quot;32&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Optimal Causal Inference: Estimating Stored Information and\n  Approximating Causal Architecture&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We introduce an approach to inferring the causal architecture of stochastic\ndynamical systems that extends rate distortion theory to use causal\nshielding---a natural principle of learning. We study two distinct cases of\ncausal inference: optimal causal filtering and optimal causal estimation.\n  Filtering corresponds to the ideal case in which the probability distribution\nof measurement sequences is known, giving a principled method to approximate a\nsystem's causal structure at a desired level of representation. We show that,\nin the limit in which a model complexity constraint is relaxed, filtering finds\nthe exact causal architecture of a stochastic dynamical system, known as the\ncausal-state partition. From this, one can estimate the amount of historical\ninformation the process stores. More generally, causal filtering finds a graded\nmodel-complexity hierarchy of approximations to the causal architecture. Abrupt\nchanges in the hierarchy, as a function of approximation, capture distinct\nscales of structural organization.\n  For nonideal cases with finite data, we show how the correct number of\nunderlying causal states can be found by optimal causal estimation. A\npreviously derived model complexity control term allows us to correct for the\neffect of statistical fluctuations in probability estimates and thereby avoid\nover-fitting.\n&quot;}}},{&quot;rowIdx&quot;:33,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:33,&quot;string&quot;:&quot;33&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:33,&quot;string&quot;:&quot;33&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;On Semimeasures Predicting Martin-Loef Random Sequences&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Solomonoff's central result on induction is that the posterior of a universal\nsemimeasure M converges rapidly and with probability 1 to the true sequence\ngenerating posterior mu, if the latter is computable. Hence, M is eligible as a\nuniversal sequence predictor in case of unknown mu. Despite some nearby results\nand proofs in the literature, the stronger result of convergence for all\n(Martin-Loef) random sequences remained open. Such a convergence result would\nbe particularly interesting and natural, since randomness can be defined in\nterms of M itself. We show that there are universal semimeasures M which do not\nconverge for all random sequences, i.e. we give a partial negative answer to\nthe open problem. We also provide a positive answer for some non-universal\nsemimeasures. We define the incomputable measure D as a mixture over all\ncomputable measures and the enumerable semimeasure W as a mixture over all\nenumerable nearly-measures. We show that W converges to D and D to mu on all\nrandom sequences. The Hellinger distance measuring closeness of two\ndistributions plays a central role.\n&quot;}}},{&quot;rowIdx&quot;:34,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:34,&quot;string&quot;:&quot;34&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:34,&quot;string&quot;:&quot;34&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Continuous and randomized defensive forecasting: unified view&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Defensive forecasting is a method of transforming laws of probability (stated\nin game-theoretic terms as strategies for Sceptic) into forecasting algorithms.\nThere are two known varieties of defensive forecasting: \&quot;continuous\&quot;, in which\nSceptic's moves are assumed to depend on the forecasts in a (semi)continuous\nmanner and which produces deterministic forecasts, and \&quot;randomized\&quot;, in which\nthe dependence of Sceptic's moves on the forecasts is arbitrary and\nForecaster's moves are allowed to be randomized. This note shows that the\nrandomized variety can be obtained from the continuous variety by smearing\nSceptic's moves to make them continuous.\n&quot;}}},{&quot;rowIdx&quot;:35,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:35,&quot;string&quot;:&quot;35&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:35,&quot;string&quot;:&quot;35&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Dichotomy Theorem for General Minimum Cost Homomorphism Problem&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In the constraint satisfaction problem ($CSP$), the aim is to find an\nassignment of values to a set of variables subject to specified constraints. In\nthe minimum cost homomorphism problem ($MinHom$), one is additionally given\nweights $c_{va}$ for every variable $v$ and value $a$, and the aim is to find\nan assignment $f$ to the variables that minimizes $\\sum_{v} c_{vf(v)}$. Let\n$MinHom(\\Gamma)$ denote the $MinHom$ problem parameterized by the set of\npredicates allowed for constraints. $MinHom(\\Gamma)$ is related to many\nwell-studied combinatorial optimization problems, and concrete applications can\nbe found in, for instance, defence logistics and machine learning. We show that\n$MinHom(\\Gamma)$ can be studied by using algebraic methods similar to those\nused for CSPs. With the aid of algebraic techniques, we classify the\ncomputational complexity of $MinHom(\\Gamma)$ for all choices of $\\Gamma$. Our\nresult settles a general dichotomy conjecture previously resolved only for\ncertain classes of directed graphs, [Gutin, Hell, Rafiey, Yeo, European J. of\nCombinatorics, 2008].\n&quot;}}},{&quot;rowIdx&quot;:36,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:36,&quot;string&quot;:&quot;36&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:36,&quot;string&quot;:&quot;36&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Filtering Additive Measurement Noise with Maximum Entropy in the Mean&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The purpose of this note is to show how the method of maximum entropy in the\nmean (MEM) may be used to improve parametric estimation when the measurements\nare corrupted by large level of noise. The method is developed in the context\non a concrete example: that of estimation of the parameter in an exponential\ndistribution. We compare the performance of our method with the bayesian and\nmaximum likelihood approaches.\n&quot;}}},{&quot;rowIdx&quot;:37,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:37,&quot;string&quot;:&quot;37&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:37,&quot;string&quot;:&quot;37&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;On Universal Prediction and Bayesian Confirmation&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The Bayesian framework is a well-studied and successful framework for\ninductive reasoning, which includes hypothesis testing and confirmation,\nparameter estimation, sequence prediction, classification, and regression. But\nstandard statistical guidelines for choosing the model class and prior are not\nalways available or fail, in particular in complex situations. Solomonoff\ncompleted the Bayesian framework by providing a rigorous, unique, formal, and\nuniversal choice for the model class and the prior. We discuss in breadth how\nand in which sense universal (non-i.i.d.) sequence prediction solves various\n(philosophical) problems of traditional Bayesian sequence prediction. We show\nthat Solomonoff's model possesses many desirable properties: Strong total and\nweak instantaneous bounds, and in contrast to most classical continuous prior\ndensities has no zero p(oste)rior problem, i.e. can confirm universal\nhypotheses, is reparametrization and regrouping invariant, and avoids the\nold-evidence and updating problem. It even performs well (actually better) in\nnon-computable environments.\n&quot;}}},{&quot;rowIdx&quot;:38,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:38,&quot;string&quot;:&quot;38&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:38,&quot;string&quot;:&quot;38&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Learning for Dynamic Bidding in Cognitive Radio Resources&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper, we model the various wireless users in a cognitive radio\nnetwork as a collection of selfish, autonomous agents that strategically\ninteract in order to acquire the dynamically available spectrum opportunities.\nOur main focus is on developing solutions for wireless users to successfully\ncompete with each other for the limited and time-varying spectrum\nopportunities, given the experienced dynamics in the wireless network. We\ncategorize these dynamics into two types: one is the disturbance due to the\nenvironment (e.g. wireless channel conditions, source traffic characteristics,\netc.) and the other is the impact caused by competing users. To analyze the\ninteractions among users given the environment disturbance, we propose a\ngeneral stochastic framework for modeling how the competition among users for\nspectrum opportunities evolves over time. At each stage of the dynamic resource\nallocation, a central spectrum moderator auctions the available resources and\nthe users strategically bid for the required resources. The joint bid actions\naffect the resource allocation and hence, the rewards and future strategies of\nall users. Based on the observed resource allocation and corresponding rewards\nfrom previous allocations, we propose a best response learning algorithm that\ncan be deployed by wireless users to improve their bidding policy at each\nstage. The simulation results show that by deploying the proposed best response\nlearning algorithm, the wireless users can significantly improve their own\nperformance in terms of both the packet loss rate and the incurred cost for the\nused resources.\n&quot;}}},{&quot;rowIdx&quot;:39,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:39,&quot;string&quot;:&quot;39&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:39,&quot;string&quot;:&quot;39&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Mutual information for the selection of relevant variables in\n  spectrometric nonlinear modelling&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Data from spectrophotometers form vectors of a large number of exploitable\nvariables. Building quantitative models using these variables most often\nrequires using a smaller set of variables than the initial one. Indeed, a too\nlarge number of input variables to a model results in a too large number of\nparameters, leading to overfitting and poor generalization abilities. In this\npaper, we suggest the use of the mutual information measure to select variables\nfrom the initial set. The mutual information measures the information content\nin input variables with respect to the model output, without making any\nassumption on the model that will be used; it is thus suitable for nonlinear\nmodelling. In addition, it leads to the selection of variables among the\ninitial set, and not to linear or nonlinear combinations of them. Without\ndecreasing the model performances compared to other variable projection\nmethods, it allows therefore a greater interpretability of the results.\n&quot;}}},{&quot;rowIdx&quot;:40,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:40,&quot;string&quot;:&quot;40&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:40,&quot;string&quot;:&quot;40&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Fast Algorithm and Implementation of Dissimilarity Self-Organizing Maps&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In many real world applications, data cannot be accurately represented by\nvectors. In those situations, one possible solution is to rely on dissimilarity\nmeasures that enable sensible comparison between observations. Kohonen's\nSelf-Organizing Map (SOM) has been adapted to data described only through their\ndissimilarity matrix. This algorithm provides both non linear projection and\nclustering of non vector data. Unfortunately, the algorithm suffers from a high\ncost that makes it quite difficult to use with voluminous data sets. In this\npaper, we propose a new algorithm that provides an important reduction of the\ntheoretical cost of the dissimilarity SOM without changing its outcome (the\nresults are exactly the same as the ones obtained with the original algorithm).\nMoreover, we introduce implementation methods that result in very short running\ntimes. Improvements deduced from the theoretical cost model are validated on\nsimulated and real world data (a word list clustering problem). We also\ndemonstrate that the proposed implementation methods reduce by a factor up to 3\nthe running time of the fast algorithm over a standard implementation.\n&quot;}}},{&quot;rowIdx&quot;:41,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:41,&quot;string&quot;:&quot;41&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:41,&quot;string&quot;:&quot;41&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Une adaptation des cartes auto-organisatrices pour des donn\\'ees\n  d\\'ecrites par un tableau de dissimilarit\\'es&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Many data analysis methods cannot be applied to data that are not represented\nby a fixed number of real values, whereas most of real world observations are\nnot readily available in such a format. Vector based data analysis methods have\ntherefore to be adapted in order to be used with non standard complex data. A\nflexible and general solution for this adaptation is to use a (dis)similarity\nmeasure. Indeed, thanks to expert knowledge on the studied data, it is\ngenerally possible to define a measure that can be used to make pairwise\ncomparison between observations. General data analysis methods are then\nobtained by adapting existing methods to (dis)similarity matrices. In this\narticle, we propose an adaptation of Kohonen's Self Organizing Map (SOM) to\n(dis)similarity data. The proposed algorithm is an adapted version of the\nvector based batch SOM. The method is validated on real world data: we provide\nan analysis of the usage patterns of the web site of the Institut National de\nRecherche en Informatique et Automatique, constructed thanks to web log mining\nmethod.\n&quot;}}},{&quot;rowIdx&quot;:42,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:42,&quot;string&quot;:&quot;42&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:42,&quot;string&quot;:&quot;42&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Self-organizing maps and symbolic data&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In data analysis new forms of complex data have to be considered like for\nexample (symbolic data, functional data, web data, trees, SQL query and\nmultimedia data, ...). In this context classical data analysis for knowledge\ndiscovery based on calculating the center of gravity can not be used because\ninput are not $\\mathbb{R}^p$ vectors. In this paper, we present an application\non real world symbolic data using the self-organizing map. To this end, we\npropose an extension of the self-organizing map that can handle symbolic data.\n&quot;}}},{&quot;rowIdx&quot;:43,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:43,&quot;string&quot;:&quot;43&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:43,&quot;string&quot;:&quot;43&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Fast Selection of Spectral Variables with B-Spline Compression&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The large number of spectral variables in most data sets encountered in\nspectral chemometrics often renders the prediction of a dependent variable\nuneasy. The number of variables hopefully can be reduced, by using either\nprojection techniques or selection methods; the latter allow for the\ninterpretation of the selected variables. Since the optimal approach of testing\nall possible subsets of variables with the prediction model is intractable, an\nincremental selection approach using a nonparametric statistics is a good\noption, as it avoids the computationally intensive use of the model itself. It\nhas two drawbacks however: the number of groups of variables to test is still\nhuge, and colinearities can make the results unstable. To overcome these\nlimitations, this paper presents a method to select groups of spectral\nvariables. It consists in a forward-backward procedure applied to the\ncoefficients of a B-Spline representation of the spectra. The criterion used in\nthe forward-backward procedure is the mutual information, allowing to find\nnonlinear dependencies between variables, on the contrary of the generally used\ncorrelation. The spline representation is used to get interpretability of the\nresults, as groups of consecutive spectral variables will be selected. The\nexperiments conducted on NIR spectra from fescue grass and diesel fuels show\nthat the method provides clearly identified groups of selected variables,\nmaking interpretation easy, while keeping a low computational load. The\nprediction performances obtained using the selected coefficients are higher\nthan those obtained by the same method applied directly to the original\nvariables and similar to those obtained using traditional models, although\nusing significantly less spectral variables.\n&quot;}}},{&quot;rowIdx&quot;:44,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:44,&quot;string&quot;:&quot;44&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:44,&quot;string&quot;:&quot;44&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Resampling methods for parameter-free and robust feature selection with\n  mutual information&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Combining the mutual information criterion with a forward feature selection\nstrategy offers a good trade-off between optimality of the selected feature\nsubset and computation time. However, it requires to set the parameter(s) of\nthe mutual information estimator and to determine when to halt the forward\nprocedure. These two choices are difficult to make because, as the\ndimensionality of the subset increases, the estimation of the mutual\ninformation becomes less and less reliable. This paper proposes to use\nresampling methods, a K-fold cross-validation and the permutation test, to\naddress both issues. The resampling methods bring information about the\nvariance of the estimator, information which can then be used to automatically\nset the parameter and to calculate a threshold to stop the forward procedure.\nThe procedure is illustrated on a synthetic dataset as well as on real-world\nexamples.\n&quot;}}},{&quot;rowIdx&quot;:45,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:45,&quot;string&quot;:&quot;45&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:45,&quot;string&quot;:&quot;45&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Evolving Classifiers: Methods for Incremental Learning&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The ability of a classifier to take on new information and classes by\nevolving the classifier without it having to be fully retrained is known as\nincremental learning. Incremental learning has been successfully applied to\nmany classification problems, where the data is changing and is not all\navailable at once. In this paper there is a comparison between Learn++, which\nis one of the most recent incremental learning algorithms, and the new proposed\nmethod of Incremental Learning Using Genetic Algorithm (ILUGA). Learn++ has\nshown good incremental learning capabilities on benchmark datasets on which the\nnew ILUGA method has been tested. ILUGA has also shown good incremental\nlearning ability using only a few classifiers and does not suffer from\ncatastrophic forgetting. The results obtained for ILUGA on the Optical\nCharacter Recognition (OCR) and Wine datasets are good, with an overall\naccuracy of 93% and 94% respectively showing a 4% improvement over Learn++.MT\nfor the difficult multi-class OCR dataset.\n&quot;}}},{&quot;rowIdx&quot;:46,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:46,&quot;string&quot;:&quot;46&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:46,&quot;string&quot;:&quot;46&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Classification of Images Using Support Vector Machines&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Support Vector Machines (SVMs) are a relatively new supervised classification\ntechnique to the land cover mapping community. They have their roots in\nStatistical Learning Theory and have gained prominence because they are robust,\naccurate and are effective even when using a small training sample. By their\nnature SVMs are essentially binary classifiers, however, they can be adopted to\nhandle the multiple classification tasks common in remote sensing studies. The\ntwo approaches commonly used are the One-Against-One (1A1) and One-Against-All\n(1AA) techniques. In this paper, these approaches are evaluated in as far as\ntheir impact and implication for land cover mapping. The main finding from this\nresearch is that whereas the 1AA technique is more predisposed to yielding\nunclassified and mixed pixels, the resulting classification accuracy is not\nsignificantly different from 1A1 approach. It is the authors conclusions that\nultimately the choice of technique adopted boils down to personal preference\nand the uniqueness of the dataset at hand.\n&quot;}}},{&quot;rowIdx&quot;:47,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:47,&quot;string&quot;:&quot;47&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:47,&quot;string&quot;:&quot;47&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Prediction with expert advice for the Brier game&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We show that the Brier game of prediction is mixable and find the optimal\nlearning rate and substitution function for it. The resulting prediction\nalgorithm is applied to predict results of football and tennis matches. The\ntheoretical performance guarantee turns out to be rather tight on these data\nsets, especially in the case of the more extensive tennis data.\n&quot;}}},{&quot;rowIdx&quot;:48,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:48,&quot;string&quot;:&quot;48&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:48,&quot;string&quot;:&quot;48&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Association Rules in the Relational Calculus&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  One of the most utilized data mining tasks is the search for association\nrules. Association rules represent significant relationships between items in\ntransactions. We extend the concept of association rule to represent a much\nbroader class of associations, which we refer to as \\emph{entity-relationship\nrules.} Semantically, entity-relationship rules express associations between\nproperties of related objects. Syntactically, these rules are based on a broad\nsubclass of safe domain relational calculus queries. We propose a new\ndefinition of support and confidence for entity-relationship rules and for the\nfrequency of entity-relationship queries. We prove that the definition of\nfrequency satisfies standard probability axioms and the Apriori property.\n&quot;}}},{&quot;rowIdx&quot;:49,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:49,&quot;string&quot;:&quot;49&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:49,&quot;string&quot;:&quot;49&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The structure of verbal sequences analyzed with unsupervised learning\n  techniques&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Data mining allows the exploration of sequences of phenomena, whereas one\nusually tends to focus on isolated phenomena or on the relation between two\nphenomena. It offers invaluable tools for theoretical analyses and exploration\nof the structure of sentences, texts, dialogues, and speech. We report here the\nresults of an attempt at using it for inspecting sequences of verbs from French\naccounts of road accidents. This analysis comes from an original approach of\nunsupervised training allowing the discovery of the structure of sequential\ndata. The entries of the analyzer were only made of the verbs appearing in the\nsentences. It provided a classification of the links between two successive\nverbs into four distinct clusters, allowing thus text segmentation. We give\nhere an interpretation of these clusters by applying a statistical analysis to\nindependent semantic annotations.\n&quot;}}},{&quot;rowIdx&quot;:50,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:50,&quot;string&quot;:&quot;50&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:50,&quot;string&quot;:&quot;50&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Consistency of trace norm minimization&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Regularization by the sum of singular values, also referred to as the trace\nnorm, is a popular technique for estimating low rank rectangular matrices. In\nthis paper, we extend some of the consistency results of the Lasso to provide\nnecessary and sufficient conditions for rank consistency of trace norm\nminimization with the square loss. We also provide an adaptive version that is\nrank consistent even when the necessary condition for the non adaptive version\nis not fulfilled.\n&quot;}}},{&quot;rowIdx&quot;:51,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:51,&quot;string&quot;:&quot;51&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:51,&quot;string&quot;:&quot;51&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;An efficient reduction of ranking to classification&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This paper describes an efficient reduction of the learning problem of\nranking to binary classification. The reduction guarantees an average pairwise\nmisranking regret of at most that of the binary classifier regret, improving a\nrecent result of Balcan et al which only guarantees a factor of 2. Moreover,\nour reduction applies to a broader class of ranking loss functions, admits a\nsimpler proof, and the expected running time complexity of our algorithm in\nterms of number of calls to a classifier or preference function is improved\nfrom $\\Omega(n^2)$ to $O(n \\log n)$. In addition, when the top $k$ ranked\nelements only are required ($k \\ll n$), as in many applications in information\nextraction or search engines, the time complexity of our algorithm can be\nfurther reduced to $O(k \\log k + n)$. Our reduction and algorithm are thus\npractical for realistic applications where the number of points to rank exceeds\nseveral thousands. Much of our results also extend beyond the bipartite case\npreviously studied.\n  Our rediction is a randomized one. To complement our result, we also derive\nlower bounds on any deterministic reduction from binary (preference)\nclassification to ranking, implying that our use of a randomized reduction is\nessentially necessary for the guarantees we provide.\n&quot;}}},{&quot;rowIdx&quot;:52,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:52,&quot;string&quot;:&quot;52&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:52,&quot;string&quot;:&quot;52&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Combining haplotypers&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Statistically resolving the underlying haplotype pair for a genotype\nmeasurement is an important intermediate step in gene mapping studies, and has\nreceived much attention recently. Consequently, a variety of methods for this\nproblem have been developed. Different methods employ different statistical\nmodels, and thus implicitly encode different assumptions about the nature of\nthe underlying haplotype structure. Depending on the population sample in\nquestion, their relative performance can vary greatly, and it is unclear which\nmethod to choose for a particular sample. Instead of choosing a single method,\nwe explore combining predictions returned by different methods in a principled\nway, and thereby circumvent the problem of method selection.\n  We propose several techniques for combining haplotype reconstructions and\nanalyze their computational properties. In an experimental study on real-world\nhaplotype data we show that such techniques can provide more accurate and\nrobust reconstructions, and are useful for outlier detection. Typically, the\ncombined prediction is at least as accurate as or even more accurate than the\nbest individual method, effectively circumventing the method selection problem.\n&quot;}}},{&quot;rowIdx&quot;:53,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:53,&quot;string&quot;:&quot;53&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:53,&quot;string&quot;:&quot;53&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Tutorial on Spectral Clustering&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In recent years, spectral clustering has become one of the most popular\nmodern clustering algorithms. It is simple to implement, can be solved\nefficiently by standard linear algebra software, and very often outperforms\ntraditional clustering algorithms such as the k-means algorithm. On the first\nglance spectral clustering appears slightly mysterious, and it is not obvious\nto see why it works at all and what it really does. The goal of this tutorial\nis to give some intuition on those questions. We describe different graph\nLaplacians and their basic properties, present the most common spectral\nclustering algorithms, and derive those algorithms from scratch by several\ndifferent approaches. Advantages and disadvantages of the different spectral\nclustering algorithms are discussed.\n&quot;}}},{&quot;rowIdx&quot;:54,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:54,&quot;string&quot;:&quot;54&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:54,&quot;string&quot;:&quot;54&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Building Rules on Top of Ontologies for the Semantic Web with Inductive\n  Logic Programming&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Building rules on top of ontologies is the ultimate goal of the logical layer\nof the Semantic Web. To this aim an ad-hoc mark-up language for this layer is\ncurrently under discussion. It is intended to follow the tradition of hybrid\nknowledge representation and reasoning systems such as $\\mathcal{AL}$-log that\nintegrates the description logic $\\mathcal{ALC}$ and the function-free Horn\nclausal language \\textsc{Datalog}. In this paper we consider the problem of\nautomating the acquisition of these rules for the Semantic Web. We propose a\ngeneral framework for rule induction that adopts the methodological apparatus\nof Inductive Logic Programming and relies on the expressive and deductive power\nof $\\mathcal{AL}$-log. The framework is valid whatever the scope of induction\n(description vs. prediction) is. Yet, for illustrative purposes, we also\ndiscuss an instantiation of the framework which aims at description and turns\nout to be useful in Ontology Refinement.\n  Keywords: Inductive Logic Programming, Hybrid Knowledge Representation and\nReasoning Systems, Ontologies, Semantic Web.\n  Note: To appear in Theory and Practice of Logic Programming (TPLP)\n&quot;}}},{&quot;rowIdx&quot;:55,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:55,&quot;string&quot;:&quot;55&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:55,&quot;string&quot;:&quot;55&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Empirical Evaluation of Four Tensor Decomposition Algorithms&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Higher-order tensor decompositions are analogous to the familiar Singular\nValue Decomposition (SVD), but they transcend the limitations of matrices\n(second-order tensors). SVD is a powerful tool that has achieved impressive\nresults in information retrieval, collaborative filtering, computational\nlinguistics, computational vision, and other fields. However, SVD is limited to\ntwo-dimensional arrays of data (two modes), and many potential applications\nhave three or more modes, which require higher-order tensor decompositions.\nThis paper evaluates four algorithms for higher-order tensor decomposition:\nHigher-Order Singular Value Decomposition (HO-SVD), Higher-Order Orthogonal\nIteration (HOOI), Slice Projection (SP), and Multislice Projection (MP). We\nmeasure the time (elapsed run time), space (RAM and disk space requirements),\nand fit (tensor reconstruction accuracy) of the four algorithms, under a\nvariety of conditions. We find that standard implementations of HO-SVD and HOOI\ndo not scale up to larger tensors, due to increasing RAM requirements. We\nrecommend HOOI for tensors that are small enough for the available RAM and MP\nfor larger tensors.\n&quot;}}},{&quot;rowIdx&quot;:56,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:56,&quot;string&quot;:&quot;56&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:56,&quot;string&quot;:&quot;56&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Inverse Sampling for Nonasymptotic Sequential Estimation of Bounded\n  Variable Means&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper, we consider the nonasymptotic sequential estimation of means\nof random variables bounded in between zero and one. We have rigorously\ndemonstrated that, in order to guarantee prescribed relative precision and\nconfidence level, it suffices to continue sampling until the sample sum is no\nless than a certain bound and then take the average of samples as an estimate\nfor the mean of the bounded random variable. We have developed an explicit\nformula and a bisection search method for the determination of such bound of\nsample sum, without any knowledge of the bounded variable. Moreover, we have\nderived bounds for the distribution of sample size. In the special case of\nBernoulli random variables, we have established analytical and numerical\nmethods to further reduce the bound of sample sum and thus improve the\nefficiency of sampling. Furthermore, the fallacy of existing results are\ndetected and analyzed.\n&quot;}}},{&quot;rowIdx&quot;:57,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:57,&quot;string&quot;:&quot;57&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:57,&quot;string&quot;:&quot;57&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Image Classification Using SVMs: One-against-One Vs One-against-All&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Support Vector Machines (SVMs) are a relatively new supervised classification\ntechnique to the land cover mapping community. They have their roots in\nStatistical Learning Theory and have gained prominence because they are robust,\naccurate and are effective even when using a small training sample. By their\nnature SVMs are essentially binary classifiers, however, they can be adopted to\nhandle the multiple classification tasks common in remote sensing studies. The\ntwo approaches commonly used are the One-Against-One (1A1) and One-Against-All\n(1AA) techniques. In this paper, these approaches are evaluated in as far as\ntheir impact and implication for land cover mapping. The main finding from this\nresearch is that whereas the 1AA technique is more predisposed to yielding\nunclassified and mixed pixels, the resulting classification accuracy is not\nsignificantly different from 1A1 approach. It is the authors conclusion\ntherefore that ultimately the choice of technique adopted boils down to\npersonal preference and the uniqueness of the dataset at hand.\n&quot;}}},{&quot;rowIdx&quot;:58,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:58,&quot;string&quot;:&quot;58&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:58,&quot;string&quot;:&quot;58&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Clustering with Transitive Distance and K-Means Duality&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Recent spectral clustering methods are a propular and powerful technique for\ndata clustering. These methods need to solve the eigenproblem whose\ncomputational complexity is $O(n^3)$, where $n$ is the number of data samples.\nIn this paper, a non-eigenproblem based clustering method is proposed to deal\nwith the clustering problem. Its performance is comparable to the spectral\nclustering algorithms but it is more efficient with computational complexity\n$O(n^2)$. We show that with a transitive distance and an observed property,\ncalled K-means duality, our algorithm can be used to handle data sets with\ncomplex cluster shapes, multi-scale clusters, and noise. Moreover, no\nparameters except the number of clusters need to be set in our algorithm.\n&quot;}}},{&quot;rowIdx&quot;:59,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:59,&quot;string&quot;:&quot;59&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:59,&quot;string&quot;:&quot;59&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Derivations of Normalized Mutual Information in Binary Classifications&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This correspondence studies the basic problem of classifications - how to\nevaluate different classifiers. Although the conventional performance indexes,\nsuch as accuracy, are commonly used in classifier selection or evaluation,\ninformation-based criteria, such as mutual information, are becoming popular in\nfeature/model selections. In this work, we propose to assess classifiers in\nterms of normalized mutual information (NI), which is novel and well defined in\na compact range for classifier evaluation. We derive close-form relations of\nnormalized mutual information with respect to accuracy, precision, and recall\nin binary classifications. By exploring the relations among them, we reveal\nthat NI is actually a set of nonlinear functions, with a concordant\npower-exponent form, to each performance index. The relations can also be\nexpressed with respect to precision and recall, or to false alarm and hitting\nrate (recall).\n&quot;}}},{&quot;rowIdx&quot;:60,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:60,&quot;string&quot;:&quot;60&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:60,&quot;string&quot;:&quot;60&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Covariance and PCA for Categorical Variables&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Covariances from categorical variables are defined using a regular simplex\nexpression for categories. The method follows the variance definition by Gini,\nand it gives the covariance as a solution of simultaneous equations. The\ncalculated results give reasonable values for test data. A method of principal\ncomponent analysis (RS-PCA) is also proposed using regular simplex expressions,\nwhich allows easy interpretation of the principal components. The proposed\nmethods apply to variable selection problem of categorical data USCensus1990\ndata. The proposed methods give appropriate criterion for the variable\nselection problem of categorical\n&quot;}}},{&quot;rowIdx&quot;:61,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:61,&quot;string&quot;:&quot;61&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:61,&quot;string&quot;:&quot;61&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;On the Relationship between the Posterior and Optimal Similarity&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  For a classification problem described by the joint density $P(\\omega,x)$,\nmodels of $P(\\omega\\eq\\omega'|x,x')$ (the ``Bayesian similarity measure'') have\nbeen shown to be an optimal similarity measure for nearest neighbor\nclassification. This paper analyzes demonstrates several additional properties\nof that conditional distribution. The paper first shows that we can\nreconstruct, up to class labels, the class posterior distribution $P(\\omega|x)$\ngiven $P(\\omega\\eq\\omega'|x,x')$, gives a procedure for recovering the class\nlabels, and gives an asymptotically Bayes-optimal classification procedure. It\nalso shows, given such an optimal similarity measure, how to construct a\nclassifier that outperforms the nearest neighbor classifier and achieves\nBayes-optimal classification rates. The paper then analyzes Bayesian similarity\nin a framework where a classifier faces a number of related classification\ntasks (multitask learning) and illustrates that reconstruction of the class\nposterior distribution is not possible in general. Finally, the paper\nidentifies a distinct class of classification problems using\n$P(\\omega\\eq\\omega'|x,x')$ and shows that using $P(\\omega\\eq\\omega'|x,x')$ to\nsolve those problems is the Bayes optimal solution.\n&quot;}}},{&quot;rowIdx&quot;:62,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:62,&quot;string&quot;:&quot;62&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:62,&quot;string&quot;:&quot;62&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Reactive Tabu Search Algorithm for Stimuli Generation in\n  Psycholinguistics&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The generation of meaningless \&quot;words\&quot; matching certain statistical and/or\nlinguistic criteria is frequently needed for experimental purposes in\nPsycholinguistics. Such stimuli receive the name of pseudowords or nonwords in\nthe Cognitive Neuroscience literatue. The process for building nonwords\nsometimes has to be based on linguistic units such as syllables or morphemes,\nresulting in a numerical explosion of combinations when the size of the\nnonwords is increased. In this paper, a reactive tabu search scheme is proposed\nto generate nonwords of variables size. The approach builds pseudowords by\nusing a modified Metaheuristic algorithm based on a local search procedure\nenhanced by a feedback-based scheme. Experimental results show that the new\nalgorithm is a practical and effective tool for nonword generation.\n&quot;}}},{&quot;rowIdx&quot;:63,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:63,&quot;string&quot;:&quot;63&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:63,&quot;string&quot;:&quot;63&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Equations of States in Singular Statistical Estimation&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Learning machines which have hierarchical structures or hidden variables are\nsingular statistical models because they are nonidentifiable and their Fisher\ninformation matrices are singular. In singular statistical models, neither the\nBayes a posteriori distribution converges to the normal distribution nor the\nmaximum likelihood estimator satisfies asymptotic normality. This is the main\nreason why it has been difficult to predict their generalization performances\nfrom trained states. In this paper, we study four errors, (1) Bayes\ngeneralization error, (2) Bayes training error, (3) Gibbs generalization error,\nand (4) Gibbs training error, and prove that there are mathematical relations\namong these errors. The formulas proved in this paper are equations of states\nin statistical estimation because they hold for any true distribution, any\nparametric model, and any a priori distribution. Also we show that Bayes and\nGibbs generalization errors are estimated by Bayes and Gibbs training errors,\nand propose widely applicable information criteria which can be applied to both\nregular and singular statistical models.\n&quot;}}},{&quot;rowIdx&quot;:64,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:64,&quot;string&quot;:&quot;64&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:64,&quot;string&quot;:&quot;64&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Universal Kernel for Learning Regular Languages&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We give a universal kernel that renders all the regular languages linearly\nseparable. We are not able to compute this kernel efficiently and conjecture\nthat it is intractable, but we do have an efficient $\\eps$-approximation.\n&quot;}}},{&quot;rowIdx&quot;:65,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:65,&quot;string&quot;:&quot;65&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:65,&quot;string&quot;:&quot;65&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Automatic Pattern Classification by Unsupervised Learning Using\n  Dimensionality Reduction of Data with Mirroring Neural Networks&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This paper proposes an unsupervised learning technique by using Multi-layer\nMirroring Neural Network and Forgy's clustering algorithm. Multi-layer\nMirroring Neural Network is a neural network that can be trained with\ngeneralized data inputs (different categories of image patterns) to perform\nnon-linear dimensionality reduction and the resultant low-dimensional code is\nused for unsupervised pattern classification using Forgy's algorithm. By\nadapting the non-linear activation function (modified sigmoidal function) and\ninitializing the weights and bias terms to small random values, mirroring of\nthe input pattern is initiated. In training, the weights and bias terms are\nchanged in such a way that the input presented is reproduced at the output by\nback propagating the error. The mirroring neural network is capable of reducing\nthe input vector to a great degree (approximately 1/30th the original size) and\nalso able to reconstruct the input pattern at the output layer from this\nreduced code units. The feature set (output of central hidden layer) extracted\nfrom this network is fed to Forgy's algorithm, which classify input data\npatterns into distinguishable classes. In the implementation of Forgy's\nalgorithm, initial seed points are selected in such a way that they are distant\nenough to be perfectly grouped into different categories. Thus a new method of\nunsupervised learning is formulated and demonstrated in this paper. This method\ngave impressive results when applied to classification of different image\npatterns.\n&quot;}}},{&quot;rowIdx&quot;:66,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:66,&quot;string&quot;:&quot;66&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:66,&quot;string&quot;:&quot;66&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Reconstruction of Markov Random Fields from Samples: Some Easy\n  Observations and Algorithms&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Markov random fields are used to model high dimensional distributions in a\nnumber of applied areas. Much recent interest has been devoted to the\nreconstruction of the dependency structure from independent samples from the\nMarkov random fields. We analyze a simple algorithm for reconstructing the\nunderlying graph defining a Markov random field on $n$ nodes and maximum degree\n$d$ given observations. We show that under mild non-degeneracy conditions it\nreconstructs the generating graph with high probability using $\\Theta(d\n\\epsilon^{-2}\\delta^{-4} \\log n)$ samples where $\\epsilon,\\delta$ depend on the\nlocal interactions. For most local interaction $\\eps,\\delta$ are of order\n$\\exp(-O(d))$.\n  Our results are optimal as a function of $n$ up to a multiplicative constant\ndepending on $d$ and the strength of the local interactions. Our results seem\nto be the first results for general models that guarantee that {\\em the}\ngenerating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2}\n\\epsilon^{-2}\\delta^{-4} \\log n)$ running time bound. In cases where the\nmeasure on the graph has correlation decay, the running time is $O(n^2 \\log n)$\nfor all fixed $d$. We also discuss the effect of observing noisy samples and\nshow that as long as the noise level is low, our algorithm is effective. On the\nother hand, we construct an example where large noise implies\nnon-identifiability even for generic noise and interactions. Finally, we\nbriefly show that in some simple cases, models with hidden nodes can also be\nrecovered.\n&quot;}}},{&quot;rowIdx&quot;:67,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:67,&quot;string&quot;:&quot;67&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:67,&quot;string&quot;:&quot;67&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A New Theoretic Foundation for Cross-Layer Optimization&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Cross-layer optimization solutions have been proposed in recent years to\nimprove the performance of network users operating in a time-varying,\nerror-prone wireless environment. However, these solutions often rely on ad-hoc\noptimization approaches, which ignore the different environmental dynamics\nexperienced at various layers by a user and violate the layered network\narchitecture of the protocol stack by requiring layers to provide access to\ntheir internal protocol parameters to other layers. This paper presents a new\ntheoretic foundation for cross-layer optimization, which allows each layer to\nmake autonomous decisions individually, while maximizing the utility of the\nwireless user by optimally determining what information needs to be exchanged\namong layers. Hence, this cross-layer framework does not change the current\nlayered architecture. Specifically, because the wireless user interacts with\nthe environment at various layers of the protocol stack, the cross-layer\noptimization problem is formulated as a layered Markov decision process (MDP)\nin which each layer adapts its own protocol parameters and exchanges\ninformation (messages) with other layers in order to cooperatively maximize the\nperformance of the wireless user. The message exchange mechanism for\ndetermining the optimal cross-layer transmission strategies has been designed\nfor both off-line optimization and on-line dynamic adaptation. We also show\nthat many existing cross-layer optimization algorithms can be formulated as\nsimplified, sub-optimal, versions of our layered MDP framework.\n&quot;}}},{&quot;rowIdx&quot;:68,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:68,&quot;string&quot;:&quot;68&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:68,&quot;string&quot;:&quot;68&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Density estimation in linear time&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We consider the problem of choosing a density estimate from a set of\ndistributions F, minimizing the L1-distance to an unknown distribution\n(Devroye, Lugosi 2001). Devroye and Lugosi analyze two algorithms for the\nproblem: Scheffe tournament winner and minimum distance estimate. The Scheffe\ntournament estimate requires fewer computations than the minimum distance\nestimate, but has strictly weaker guarantees than the latter.\n  We focus on the computational aspect of density estimation. We present two\nalgorithms, both with the same guarantee as the minimum distance estimate. The\nfirst one, a modification of the minimum distance estimate, uses the same\nnumber (quadratic in |F|) of computations as the Scheffe tournament. The second\none, called ``efficient minimum loss-weight estimate,'' uses only a linear\nnumber of computations, assuming that F is preprocessed.\n  We also give examples showing that the guarantees of the algorithms cannot be\nimproved and explore randomized algorithms for density estimation.\n&quot;}}},{&quot;rowIdx&quot;:69,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:69,&quot;string&quot;:&quot;69&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:69,&quot;string&quot;:&quot;69&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Graph kernels between point clouds&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Point clouds are sets of points in two or three dimensions. Most kernel\nmethods for learning on sets of points have not yet dealt with the specific\ngeometrical invariances and practical constraints associated with point clouds\nin computer vision and graphics. In this paper, we present extensions of graph\nkernels for point clouds, which allow to use kernel methods for such ob jects\nas shapes, line drawings, or any three-dimensional point clouds. In order to\ndesign rich and numerically efficient kernels with as few free parameters as\npossible, we use kernels between covariance matrices and their factorizations\non graphical models. We derive polynomial time dynamic programming recursions\nand present applications to recognition of handwritten digits and Chinese\ncharacters from few training examples.\n&quot;}}},{&quot;rowIdx&quot;:70,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:70,&quot;string&quot;:&quot;70&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:70,&quot;string&quot;:&quot;70&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Improving the Performance of PieceWise Linear Separation Incremental\n  Algorithms for Practical Hardware Implementations&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper we shall review the common problems associated with Piecewise\nLinear Separation incremental algorithms. This kind of neural models yield poor\nperformances when dealing with some classification problems, due to the\nevolving schemes used to construct the resulting networks. So as to avoid this\nundesirable behavior we shall propose a modification criterion. It is based\nupon the definition of a function which will provide information about the\nquality of the network growth process during the learning phase. This function\nis evaluated periodically as the network structure evolves, and will permit, as\nwe shall show through exhaustive benchmarks, to considerably improve the\nperformance(measured in terms of network complexity and generalization\ncapabilities) offered by the networks generated by these incremental models.\n&quot;}}},{&quot;rowIdx&quot;:71,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:71,&quot;string&quot;:&quot;71&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:71,&quot;string&quot;:&quot;71&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Improved Collaborative Filtering Algorithm via Information\n  Transformation&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper, we propose a spreading activation approach for collaborative\nfiltering (SA-CF). By using the opinion spreading process, the similarity\nbetween any users can be obtained. The algorithm has remarkably higher accuracy\nthan the standard collaborative filtering (CF) using Pearson correlation.\nFurthermore, we introduce a free parameter $\\beta$ to regulate the\ncontributions of objects to user-user correlations. The numerical results\nindicate that decreasing the influence of popular objects can further improve\nthe algorithmic accuracy and personality. We argue that a better algorithm\nshould simultaneously require less computation and generate higher accuracy.\nAccordingly, we further propose an algorithm involving only the top-$N$ similar\nneighbors for each target user, which has both less computational complexity\nand higher algorithmic accuracy.\n&quot;}}},{&quot;rowIdx&quot;:72,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:72,&quot;string&quot;:&quot;72&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:72,&quot;string&quot;:&quot;72&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Online EM Algorithm for Latent Data Models&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this contribution, we propose a generic online (also sometimes called\nadaptive or recursive) version of the Expectation-Maximisation (EM) algorithm\napplicable to latent variable models of independent observations. Compared to\nthe algorithm of Titterington (1984), this approach is more directly connected\nto the usual EM algorithm and does not rely on integration with respect to the\ncomplete data distribution. The resulting algorithm is usually simpler and is\nshown to achieve convergence to the stationary points of the Kullback-Leibler\ndivergence between the marginal distribution of the observation and the model\ndistribution at the optimal rate, i.e., that of the maximum likelihood\nestimator. In addition, the proposed approach is also suitable for conditional\n(or regression) models, as illustrated in the case of the mixture of linear\nregressions model.\n&quot;}}},{&quot;rowIdx&quot;:73,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:73,&quot;string&quot;:&quot;73&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:73,&quot;string&quot;:&quot;73&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Staring at Economic Aggregators through Information Lenses&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  It is hard to exaggerate the role of economic aggregators -- functions that\nsummarize numerous and / or heterogeneous data -- in economic models since the\nearly XX$^{th}$ century. In many cases, as witnessed by the pioneering works of\nCobb and Douglas, these functions were information quantities tailored to\neconomic theories, i.e. they were built to fit economic phenomena. In this\npaper, we look at these functions from the complementary side: information. We\nuse a recent toolbox built on top of a vast class of distortions coined by\nBregman, whose application field rivals metrics' in various subfields of\nmathematics. This toolbox makes it possible to find the quality of an\naggregator (for consumptions, prices, labor, capital, wages, etc.), from the\nstandpoint of the information it carries. We prove a rather striking result.\n  From the informational standpoint, well-known economic aggregators do belong\nto the \\textit{optimal} set. As common economic assumptions enter the analysis,\nthis large set shrinks, and it essentially ends up \\textit{exactly fitting}\neither CES, or Cobb-Douglas, or both. To summarize, in the relevant economic\ncontexts, one could not have crafted better some aggregator from the\ninformation standpoint. We also discuss global economic behaviors of optimal\ninformation aggregators in general, and present a brief panorama of the links\nbetween economic and information aggregators.\n  Keywords: Economic Aggregators, CES, Cobb-Douglas, Bregman divergences\n&quot;}}},{&quot;rowIdx&quot;:74,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:74,&quot;string&quot;:&quot;74&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:74,&quot;string&quot;:&quot;74&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Online variants of the cross-entropy method&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The cross-entropy method is a simple but efficient method for global\noptimization. In this paper we provide two online variants of the basic CEM,\ntogether with a proof of convergence.\n&quot;}}},{&quot;rowIdx&quot;:75,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:75,&quot;string&quot;:&quot;75&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:75,&quot;string&quot;:&quot;75&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Factored Value Iteration Converges&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this paper we propose a novel algorithm, factored value iteration (FVI),\nfor the approximate solution of factored Markov decision processes (fMDPs). The\ntraditional approximate value iteration algorithm is modified in two ways. For\none, the least-squares projection operator is modified so that it does not\nincrease max-norm, and thus preserves convergence. The other modification is\nthat we uniformly sample polynomially many samples from the (exponentially\nlarge) state space. This way, the complexity of our algorithm becomes\npolynomial in the size of the fMDP description length. We prove that the\nalgorithm is convergent. We also derive an upper bound on the difference\nbetween our approximate solution and the optimal one, and also on the error\nintroduced by sampling. We analyze various projection operators with respect to\ntheir computation complexity and their convergence when combined with\napproximate value iteration.\n&quot;}}},{&quot;rowIdx&quot;:76,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:76,&quot;string&quot;:&quot;76&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:76,&quot;string&quot;:&quot;76&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;The optimal assignment kernel is not positive definite&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We prove that the optimal assignment kernel, proposed recently as an attempt\nto embed labeled graphs and more generally tuples of basic data to a Hilbert\nspace, is in fact not always positive definite.\n&quot;}}},{&quot;rowIdx&quot;:77,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:77,&quot;string&quot;:&quot;77&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:77,&quot;string&quot;:&quot;77&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Information Width&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Kolmogorov argued that the concept of information exists also in problems\nwith no underlying stochastic model (as Shannon's information representation)\nfor instance, the information contained in an algorithm or in the genome. He\nintroduced a combinatorial notion of entropy and information $I(x:\\sy)$\nconveyed by a binary string $x$ about the unknown value of a variable $\\sy$.\nThe current paper poses the following questions: what is the relationship\nbetween the information conveyed by $x$ about $\\sy$ to the description\ncomplexity of $x$ ? is there a notion of cost of information ? are there limits\non how efficient $x$ conveys information ?\n  To answer these questions Kolmogorov's definition is extended and a new\nconcept termed {\\em information width} which is similar to $n$-widths in\napproximation theory is introduced. Information of any input source, e.g.,\nsample-based, general side-information or a hybrid of both can be evaluated by\na single common formula. An application to the space of binary functions is\nconsidered.\n&quot;}}},{&quot;rowIdx&quot;:78,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:78,&quot;string&quot;:&quot;78&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:78,&quot;string&quot;:&quot;78&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;On the Complexity of Binary Samples&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Consider a class $\\mH$ of binary functions $h: X\\to\\{-1, +1\\}$ on a finite\ninterval $X=[0, B]\\subset \\Real$. Define the {\\em sample width} of $h$ on a\nfinite subset (a sample) $S\\subset X$ as $\\w_S(h) \\equiv \\min_{x\\in S}\n|\\w_h(x)|$, where $\\w_h(x) = h(x) \\max\\{a\\geq 0: h(z)=h(x), x-a\\leq z\\leq\nx+a\\}$. Let $\\mathbb{S}_\\ell$ be the space of all samples in $X$ of cardinality\n$\\ell$ and consider sets of wide samples, i.e., {\\em hypersets} which are\ndefined as $A_{\\beta, h} = \\{S\\in \\mathbb{S}_\\ell: \\w_{S}(h) \\geq \\beta\\}$.\nThrough an application of the Sauer-Shelah result on the density of sets an\nupper estimate is obtained on the growth function (or trace) of the class\n$\\{A_{\\beta, h}: h\\in\\mH\\}$, $\\beta>0$, i.e., on the number of possible\ndichotomies obtained by intersecting all hypersets with a fixed collection of\nsamples $S\\in\\mathbb{S}_\\ell$ of cardinality $m$. The estimate is\n$2\\sum_{i=0}^{2\\lfloor B/(2\\beta)\\rfloor}{m-\\ell\\choose i}$.\n&quot;}}},{&quot;rowIdx&quot;:79,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:79,&quot;string&quot;:&quot;79&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:79,&quot;string&quot;:&quot;79&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;New Estimation Procedures for PLS Path Modelling&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Given R groups of numerical variables X1, ... XR, we assume that each group\nis the result of one underlying latent variable, and that all latent variables\nare bound together through a linear equation system. Moreover, we assume that\nsome explanatory latent variables may interact pairwise in one or more\nequations. We basically consider PLS Path Modelling's algorithm to estimate\nboth latent variables and the model's coefficients. New \&quot;external\&quot; estimation\nschemes are proposed that draw latent variables towards strong group structures\nin a more flexible way. New \&quot;internal\&quot; estimation schemes are proposed to\nenable PLSPM to make good use of variable group complementarity and to deal\nwith interactions. Application examples are given.\n&quot;}}},{&quot;rowIdx&quot;:80,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:80,&quot;string&quot;:&quot;80&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:80,&quot;string&quot;:&quot;80&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Learning Balanced Mixtures of Discrete Distributions with Small Sample&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We study the problem of partitioning a small sample of $n$ individuals from a\nmixture of $k$ product distributions over a Boolean cube $\\{0, 1\\}^K$ according\nto their distributions. Each distribution is described by a vector of allele\nfrequencies in $\\R^K$. Given two distributions, we use $\\gamma$ to denote the\naverage $\\ell_2^2$ distance in frequencies across $K$ dimensions, which\nmeasures the statistical divergence between them. We study the case assuming\nthat bits are independently distributed across $K$ dimensions. This work\ndemonstrates that, for a balanced input instance for $k = 2$, a certain\ngraph-based optimization function returns the correct partition with high\nprobability, where a weighted graph $G$ is formed over $n$ individuals, whose\npairwise hamming distances between their corresponding bit vectors define the\nedge weights, so long as $K = \\Omega(\\ln n/\\gamma)$ and $Kn = \\tilde\\Omega(\\ln\nn/\\gamma^2)$. The function computes a maximum-weight balanced cut of $G$, where\nthe weight of a cut is the sum of the weights across all edges in the cut. This\nresult demonstrates a nice property in the high-dimensional feature space: one\ncan trade off the number of features that are required with the size of the\nsample to accomplish certain tasks like clustering.\n&quot;}}},{&quot;rowIdx&quot;:81,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:81,&quot;string&quot;:&quot;81&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:81,&quot;string&quot;:&quot;81&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Bayesian Nonlinear Principal Component Analysis Using Random Fields&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We propose a novel model for nonlinear dimension reduction motivated by the\nprobabilistic formulation of principal component analysis. Nonlinearity is\nachieved by specifying different transformation matrices at different locations\nof the latent space and smoothing the transformation using a Markov random\nfield type prior. The computation is made feasible by the recent advances in\nsampling from von Mises-Fisher distributions.\n&quot;}}},{&quot;rowIdx&quot;:82,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:82,&quot;string&quot;:&quot;82&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:82,&quot;string&quot;:&quot;82&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A New Approach to Collaborative Filtering: Operator Estimation with\n  Spectral Regularization&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We present a general approach for collaborative filtering (CF) using spectral\nregularization to learn linear operators from \&quot;users\&quot; to the \&quot;objects\&quot; they\nrate. Recent low-rank type matrix completion approaches to CF are shown to be\nspecial cases. However, unlike existing regularization based CF methods, our\napproach can be used to also incorporate information such as attributes of the\nusers or the objects -- a limitation of existing regularization based CF\nmethods. We then provide novel representer theorems that we use to develop new\nestimation methods. We provide learning algorithms based on low-rank\ndecompositions, and test them on a standard CF dataset. The experiments\nindicate the advantages of generalizing the existing regularization based CF\nmethods to incorporate related information about users and objects. Finally, we\nshow that certain multi-task learning methods can be also seen as special cases\nof our proposed approach.\n&quot;}}},{&quot;rowIdx&quot;:83,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:83,&quot;string&quot;:&quot;83&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:83,&quot;string&quot;:&quot;83&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Combining Expert Advice Efficiently&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We show how models for prediction with expert advice can be defined concisely\nand clearly using hidden Markov models (HMMs); standard HMM algorithms can then\nbe used to efficiently calculate, among other things, how the expert\npredictions should be weighted according to the model. We cast many existing\nmodels as HMMs and recover the best known running times in each case. We also\ndescribe two new models: the switch distribution, which was recently developed\nto improve Bayesian/Minimum Description Length model selection, and a new\ngeneralisation of the fixed share algorithm based on run-length coding. We give\nloss bounds for all models and shed new light on their relationships.\n&quot;}}},{&quot;rowIdx&quot;:84,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:84,&quot;string&quot;:&quot;84&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:84,&quot;string&quot;:&quot;84&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Radar-Shaped Statistic for Testing and Visualizing Uniformity\n  Properties in Computer Experiments&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In the study of computer codes, filling space as uniformly as possible is\nimportant to describe the complexity of the investigated phenomenon. However,\nthis property is not conserved by reducing the dimension. Some numeric\nexperiment designs are conceived in this sense as Latin hypercubes or\northogonal arrays, but they consider only the projections onto the axes or the\ncoordinate planes. In this article we introduce a statistic which allows\nstudying the good distribution of points according to all 1-dimensional\nprojections. By angularly scanning the domain, we obtain a radar type\nrepresentation, allowing the uniformity defects of a design to be identified\nwith respect to its projections onto straight lines. The advantages of this new\ntool are demonstrated on usual examples of space-filling designs (SFD) and a\nglobal statistic independent of the angle of rotation is studied.\n&quot;}}},{&quot;rowIdx&quot;:85,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:85,&quot;string&quot;:&quot;85&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:85,&quot;string&quot;:&quot;85&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Compressed Counting&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Counting is among the most fundamental operations in computing. For example,\ncounting the pth frequency moment has been a very active area of research, in\ntheoretical computer science, databases, and data mining. When p=1, the task\n(i.e., counting the sum) can be accomplished using a simple counter.\n  Compressed Counting (CC) is proposed for efficiently computing the pth\nfrequency moment of a data stream signal A_t, where 0<p<=2. CC is applicable if\nthe streaming data follow the Turnstile model, with the restriction that at the\ntime t for the evaluation, A_t[i]>= 0, which includes the strict Turnstile\nmodel as a special case. For natural data streams encountered in practice, this\nrestriction is minor.\n  The underly technique for CC is what we call skewed stable random\nprojections, which captures the intuition that, when p=1 a simple counter\nsuffices, and when p = 1+/\\Delta with small \\Delta, the sample complexity of a\ncounter system should be low (continuously as a function of \\Delta). We show at\nsmall \\Delta the sample complexity (number of projections) k = O(1/\\epsilon)\ninstead of O(1/\\epsilon^2).\n  Compressed Counting can serve a basic building block for other tasks in\nstatistics and computing, for example, estimation entropies of data streams,\nparameter estimations using the method of moments and maximum likelihood.\n  Finally, another contribution is an algorithm for approximating the\nlogarithmic norm, \\sum_{i=1}^D\\log A_t[i], and logarithmic distance. The\nlogarithmic distance is useful in machine learning practice with heavy-tailed\ndata.\n&quot;}}},{&quot;rowIdx&quot;:86,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:86,&quot;string&quot;:&quot;86&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:86,&quot;string&quot;:&quot;86&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Sign Language Tutoring Tool&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  In this project, we have developed a sign language tutor that lets users\nlearn isolated signs by watching recorded videos and by trying the same signs.\nThe system records the user's video and analyses it. If the sign is recognized,\nboth verbal and animated feedback is given to the user. The system is able to\nrecognize complex signs that involve both hand gestures and head movements and\nexpressions. Our performance tests yield a 99% recognition rate on signs\ninvolving only manual gestures and 85% recognition rate on signs that involve\nboth manual and non manual components, such as head movement and facial\nexpressions.\n&quot;}}},{&quot;rowIdx&quot;:87,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:87,&quot;string&quot;:&quot;87&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:87,&quot;string&quot;:&quot;87&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Pure Exploration for Multi-Armed Bandit Problems&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We consider the framework of stochastic multi-armed bandit problems and study\nthe possibilities and limitations of forecasters that perform an on-line\nexploration of the arms. These forecasters are assessed in terms of their\nsimple regret, a regret notion that captures the fact that exploration is only\nconstrained by the number of available rounds (not necessarily known in\nadvance), in contrast to the case when the cumulative regret is considered and\nwhen exploitation needs to be performed at the same time. We believe that this\nperformance criterion is suited to situations when the cost of pulling an arm\nis expressed in terms of resources rather than rewards. We discuss the links\nbetween the simple and the cumulative regret. One of the main results in the\ncase of a finite number of arms is a general lower bound on the simple regret\nof a forecaster in terms of its cumulative regret: the smaller the latter, the\nlarger the former. Keeping this result in mind, we then exhibit upper bounds on\nthe simple regret of some forecasters. The paper ends with a study devoted to\ncontinuous-armed bandit problems; we show that the simple regret can be\nminimized with respect to a family of probability distributions if and only if\nthe cumulative regret can be minimized for it. Based on this equivalence, we\nare able to prove that the separable metric spaces are exactly the metric\nspaces on which these regrets can be minimized with respect to the family of\nall probability distributions with continuous mean-payoff functions.\n&quot;}}},{&quot;rowIdx&quot;:88,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:88,&quot;string&quot;:&quot;88&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:88,&quot;string&quot;:&quot;88&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Knowledge Technologies&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Several technologies are emerging that provide new ways to capture, store,\npresent and use knowledge. This book is the first to provide a comprehensive\nintroduction to five of the most important of these technologies: Knowledge\nEngineering, Knowledge Based Engineering, Knowledge Webs, Ontologies and\nSemantic Webs. For each of these, answers are given to a number of key\nquestions (What is it? How does it operate? How is a system developed? What can\nit be used for? What tools are available? What are the main issues?). The book\nis aimed at students, researchers and practitioners interested in Knowledge\nManagement, Artificial Intelligence, Design Engineering and Web Technologies.\n  During the 1990s, Nick worked at the University of Nottingham on the\napplication of AI techniques to knowledge management and on various knowledge\nacquisition projects to develop expert systems for military applications. In\n1999, he joined Epistemics where he worked on numerous knowledge projects and\nhelped establish knowledge management programmes at large organisations in the\nengineering, technology and legal sectors. He is author of the book \&quot;Knowledge\nAcquisition in Practice\&quot;, which describes a step-by-step procedure for\nacquiring and implementing expertise. He maintains strong links with leading\nresearch organisations working on knowledge technologies, such as\nknowledge-based engineering, ontologies and semantic technologies.\n&quot;}}},{&quot;rowIdx&quot;:89,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:89,&quot;string&quot;:&quot;89&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:89,&quot;string&quot;:&quot;89&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;What Can We Learn Privately?&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Learning problems form an important category of computational tasks that\ngeneralizes many of the computations researchers apply to large real-life data\nsets. We ask: what concept classes can be learned privately, namely, by an\nalgorithm whose output does not depend too heavily on any one input or specific\ntraining example? More precisely, we investigate learning algorithms that\nsatisfy differential privacy, a notion that provides strong confidentiality\nguarantees in contexts where aggregate information is released about a database\ncontaining sensitive information about individuals. We demonstrate that,\nignoring computational constraints, it is possible to privately agnostically\nlearn any concept class using a sample size approximately logarithmic in the\ncardinality of the concept class. Therefore, almost anything learnable is\nlearnable privately: specifically, if a concept class is learnable by a\n(non-private) algorithm with polynomial sample complexity and output size, then\nit can be learned privately using a polynomial number of samples. We also\npresent a computationally efficient private PAC learner for the class of parity\nfunctions. Local (or randomized response) algorithms are a practical class of\nprivate algorithms that have received extensive investigation. We provide a\nprecise characterization of local private learning algorithms. We show that a\nconcept class is learnable by a local algorithm if and only if it is learnable\nin the statistical query (SQ) model. Finally, we present a separation between\nthe power of interactive and noninteractive local learning algorithms.\n&quot;}}},{&quot;rowIdx&quot;:90,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:90,&quot;string&quot;:&quot;90&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:90,&quot;string&quot;:&quot;90&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Privacy Preserving ID3 over Horizontally, Vertically and Grid\n  Partitioned Data&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We consider privacy preserving decision tree induction via ID3 in the case\nwhere the training data is horizontally or vertically distributed. Furthermore,\nwe consider the same problem in the case where the data is both horizontally\nand vertically distributed, a situation we refer to as grid partitioned data.\nWe give an algorithm for privacy preserving ID3 over horizontally partitioned\ndata involving more than two parties. For grid partitioned data, we discuss two\ndifferent evaluation methods for preserving privacy ID3, namely, first merging\nhorizontally and developing vertically or first merging vertically and next\ndeveloping horizontally. Next to introducing privacy preserving data mining\nover grid-partitioned data, the main contribution of this paper is that we\nshow, by means of a complexity analysis that the former evaluation method is\nthe more efficient.\n&quot;}}},{&quot;rowIdx&quot;:91,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:91,&quot;string&quot;:&quot;91&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:91,&quot;string&quot;:&quot;91&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Figuring out Actors in Text Streams: Using Collocations to establish\n  Incremental Mind-maps&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  The recognition, involvement, and description of main actors influences the\nstory line of the whole text. This is of higher importance as the text per se\nrepresents a flow of words and expressions that once it is read it is lost. In\nthis respect, the understanding of a text and moreover on how the actor exactly\nbehaves is not only a major concern: as human beings try to store a given input\non short-term memory while associating diverse aspects and actors with\nincidents, the following approach represents a virtual architecture, where\ncollocations are concerned and taken as the associative completion of the\nactors' acting. Once that collocations are discovered, they become managed in\nseparated memory blocks broken down by the actors. As for human beings, the\nmemory blocks refer to associative mind-maps. We then present several priority\nfunctions to represent the actual temporal situation inside a mind-map to\nenable the user to reconstruct the recent events from the discovered temporal\nresults.\n&quot;}}},{&quot;rowIdx&quot;:92,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:92,&quot;string&quot;:&quot;92&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:92,&quot;string&quot;:&quot;92&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Robustness and Regularization of Support Vector Machines&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We consider regularized support vector machines (SVMs) and show that they are\nprecisely equivalent to a new robust optimization formulation. We show that\nthis equivalence of robust optimization and regularization has implications for\nboth algorithms, and analysis. In terms of algorithms, the equivalence suggests\nmore general SVM-like algorithms for classification that explicitly build in\nprotection to noise, and at the same time control overfitting. On the analysis\nfront, the equivalence of robustness and regularization, provides a robust\noptimization interpretation for the success of regularized SVMs. We use the\nthis new robustness interpretation of SVMs to give a new proof of consistency\nof (kernelized) SVMs, thus establishing robustness as the reason regularized\nSVMs generalize well.\n&quot;}}},{&quot;rowIdx&quot;:93,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:93,&quot;string&quot;:&quot;93&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:93,&quot;string&quot;:&quot;93&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Recorded Step Directional Mutation for Faster Convergence&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  Two meta-evolutionary optimization strategies described in this paper\naccelerate the convergence of evolutionary programming algorithms while still\nretaining much of their ability to deal with multi-modal problems. The\nstrategies, called directional mutation and recorded step in this paper, can\noperate independently but together they greatly enhance the ability of\nevolutionary programming algorithms to deal with fitness landscapes\ncharacterized by long narrow valleys. The directional mutation aspect of this\ncombined method uses correlated meta-mutation but does not introduce a full\ncovariance matrix. These new methods are thus much more economical in terms of\nstorage for problems with high dimensionality. Additionally, directional\nmutation is rotationally invariant which is a substantial advantage over\nself-adaptive methods which use a single variance per coordinate for problems\nwhere the natural orientation of the problem is not oriented along the axes.\n&quot;}}},{&quot;rowIdx&quot;:94,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:94,&quot;string&quot;:&quot;94&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:94,&quot;string&quot;:&quot;94&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Support Vector Machine Classification with Indefinite Kernels&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We propose a method for support vector machine classification using\nindefinite kernels. Instead of directly minimizing or stabilizing a nonconvex\nloss function, our algorithm simultaneously computes support vectors and a\nproxy kernel matrix used in forming the loss. This can be interpreted as a\npenalized kernel learning problem where indefinite kernel matrices are treated\nas a noisy observations of a true Mercer kernel. Our formulation keeps the\nproblem convex and relatively large problems can be solved efficiently using\nthe projected gradient or analytic center cutting plane methods. We compare the\nperformance of our technique with other methods on several classic data sets.\n&quot;}}},{&quot;rowIdx&quot;:95,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:95,&quot;string&quot;:&quot;95&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:95,&quot;string&quot;:&quot;95&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A Unified Semi-Supervised Dimensionality Reduction Framework for\n  Manifold Learning&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We present a general framework of semi-supervised dimensionality reduction\nfor manifold learning which naturally generalizes existing supervised and\nunsupervised learning frameworks which apply the spectral decomposition.\nAlgorithms derived under our framework are able to employ both labeled and\nunlabeled examples and are able to handle complex problems where data form\nseparate clusters of manifolds. Our framework offers simple views, explains\nrelationships among existing frameworks and provides further extensions which\ncan improve existing algorithms. Furthermore, a new semi-supervised\nkernelization framework called ``KPCA trick'' is proposed to handle non-linear\nproblems.\n&quot;}}},{&quot;rowIdx&quot;:96,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:96,&quot;string&quot;:&quot;96&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:96,&quot;string&quot;:&quot;96&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Bolasso: model consistent Lasso estimation through the bootstrap&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We consider the least-square linear regression problem with regularization by\nthe l1-norm, a problem usually referred to as the Lasso. In this paper, we\npresent a detailed asymptotic analysis of model consistency of the Lasso. For\nvarious decays of the regularization parameter, we compute asymptotic\nequivalents of the probability of correct model selection (i.e., variable\nselection). For a specific rate decay, we show that the Lasso selects all the\nvariables that should enter the model with probability tending to one\nexponentially fast, while it selects all other variables with strictly positive\nprobability. We show that this property implies that if we run the Lasso for\nseveral bootstrapped replications of a given sample, then intersecting the\nsupports of the Lasso bootstrap estimates leads to consistent model selection.\nThis novel variable selection algorithm, referred to as the Bolasso, is\ncompared favorably to other linear regression methods on synthetic data and\ndatasets from the UCI machine learning repository.\n&quot;}}},{&quot;rowIdx&quot;:97,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:97,&quot;string&quot;:&quot;97&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:97,&quot;string&quot;:&quot;97&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;On Kernelization of Supervised Mahalanobis Distance Learners&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  This paper focuses on the problem of kernelizing an existing supervised\nMahalanobis distance learner. The following features are included in the paper.\nFirstly, three popular learners, namely, \&quot;neighborhood component analysis\&quot;,\n\&quot;large margin nearest neighbors\&quot; and \&quot;discriminant neighborhood embedding\&quot;,\nwhich do not have kernel versions are kernelized in order to improve their\nclassification performances. Secondly, an alternative kernelization framework\ncalled \&quot;KPCA trick\&quot; is presented. Implementing a learner in the new framework\ngains several advantages over the standard framework, e.g. no mathematical\nformulas and no reprogramming are required for a kernel implementation, the\nframework avoids troublesome problems such as singularity, etc. Thirdly, while\nthe truths of representer theorems are just assumptions in previous papers\nrelated to ours, here, representer theorems are formally proven. The proofs\nvalidate both the kernel trick and the KPCA trick in the context of Mahalanobis\ndistance learning. Fourthly, unlike previous works which always apply brute\nforce methods to select a kernel, we investigate two approaches which can be\nefficiently adopted to construct an appropriate kernel for a given dataset.\nFinally, numerical results on various real-world datasets are presented.\n&quot;}}},{&quot;rowIdx&quot;:98,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:98,&quot;string&quot;:&quot;98&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:98,&quot;string&quot;:&quot;98&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Isotropic PCA and Affine-Invariant Clustering&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We present a new algorithm for clustering points in R^n. The key property of\nthe algorithm is that it is affine-invariant, i.e., it produces the same\npartition for any affine transformation of the input. It has strong guarantees\nwhen the input is drawn from a mixture model. For a mixture of two arbitrary\nGaussians, the algorithm correctly classifies the sample assuming only that the\ntwo components are separable by a hyperplane, i.e., there exists a halfspace\nthat contains most of one Gaussian and almost none of the other in probability\nmass. This is nearly the best possible, improving known results substantially.\nFor k > 2 components, the algorithm requires only that there be some\n(k-1)-dimensional subspace in which the emoverlap in every direction is small.\nHere we define overlap to be the ratio of the following two quantities: 1) the\naverage squared distance between a point and the mean of its component, and 2)\nthe average squared distance between a point and the mean of the mixture. The\nmain result may also be stated in the language of linear discriminant analysis:\nif the standard Fisher discriminant is small enough, labels are not needed to\nestimate the optimal subspace for projection. Our main tools are isotropic\ntransformation, spectral projection and a simple reweighting technique. We call\nthis combination isotropic PCA.\n&quot;}}},{&quot;rowIdx&quot;:99,&quot;cells&quot;:{&quot;Unnamed: 0.1&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:99,&quot;string&quot;:&quot;99&quot;},&quot;Unnamed: 0&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:99,&quot;string&quot;:&quot;99&quot;},&quot;title&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Multiple Random Oracles Are Better Than One&quot;},&quot;abstract&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;  We study the problem of learning k-juntas given access to examples drawn from\na number of different product distributions. Thus we wish to learn a function f\n: {-1,1}^n -> {-1,1} that depends on k (unknown) coordinates. While the best\nknown algorithms for the general problem of learning a k-junta require running\ntime of n^k * poly(n,2^k), we show that given access to k different product\ndistributions with biases separated by \\gamma>0, the functions may be learned\nin time poly(n,2^k,\\gamma^{-k}). More generally, given access to t <= k\ndifferent product distributions, the functions may be learned in time n^{k/t} *\npoly(n,2^k,\\gamma^{-k}). Our techniques involve novel results in Fourier\nanalysis relating Fourier expansions with respect to different biases and a\ngeneralization of Russo's formula.\n&quot;}}}],&quot;truncated&quot;:true},&quot;paginationData&quot;:{&quot;pageIndex&quot;:0,&quot;numItemsPerPage&quot;:100,&quot;numTotalItems&quot;:117592,&quot;offset&quot;:0,&quot;length&quot;:100}},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzAzMCwic3ViIjoiL2RhdGFzZXRzL0NTaG9ydGVuL01MLUFyWGl2LVBhcGVycyIsImV4cCI6MTc0MjkyNjYzMCwiaXNzIjoiaHR0cHM6Ly9odWdnaW5nZmFjZS5jbyJ9.GYk9NFULOBdWR2CIwQ6be5UnPXot795i0T4Dggf4MEuLGqgcgGkfBSQcoqdcRUFaAsOyA7akJutVCP0KY-GMAQ&quot;,&quot;displayUrls&quot;:true},&quot;dataset&quot;:&quot;CShorten/ML-ArXiv-Papers&quot;,&quot;isGated&quot;:false,&quot;isPrivate&quot;:false,&quot;hasParquetFormat&quot;:false,&quot;author&quot;:{&quot;_id&quot;:&quot;5fda30110e761cf3183cfd5c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1654000939422-5fda30110e761cf3183cfd5c.png&quot;,&quot;fullname&quot;:&quot;Connor Shorten&quot;,&quot;name&quot;:&quot;CShorten&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:6},&quot;compact&quot;:true}"><div class="flex flex-col overflow-hidden shadow-xs mx-auto mb-10 rounded-lg border pt-2  px-2.5"><div class="mb-2 flex flex-wrap items-center gap-2"><div class="mr-auto flex items-center"><svg class="mr-1 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	<div class="whitespace-nowrap font-semibold">Dataset Viewer</div>
	</div>
				<a href="/datasets/CShorten/ML-ArXiv-Papers/tree/refs%2Fconvert%2Fparquet/default" class="group mr-1 text-xs text-gray-400 max-sm:hidden"><svg class="text-[.6rem] mr-1 inline -translate-y-px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill="currentColor" d="M12 10H6.78A11 11 0 0 1 27 16h2A13 13 0 0 0 6 7.68V4H4v8h8zm8 12h5.22A11 11 0 0 1 5 16H3a13 13 0 0 0 23 8.32V28h2v-8h-8z"></path></svg>
						<span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Auto-converted</span> to Parquet
					</a>
				<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>API</button>
					<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path d="M9.80603 2.86737H3.56107C3.37704 2.86737 3.20055 2.94048 3.07042 3.0706C2.94029 3.20073 2.86719 3.37723 2.86719 3.56126V9.80622C2.86719 9.99025 2.94029 10.1667 3.07042 10.2969C3.20055 10.427 3.37704 10.5001 3.56107 10.5001H9.80603C9.99006 10.5001 10.1666 10.427 10.2967 10.2969C10.4268 10.1667 10.4999 9.99025 10.4999 9.80622V3.56126C10.4999 3.37723 10.4268 3.20073 10.2967 3.0706C10.1666 2.94048 9.99006 2.86737 9.80603 2.86737Z" fill="currentColor" fill-opacity="0.3"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M2.40942 1.66191C2.05175 1.66191 1.7618 1.95186 1.7618 2.30953V6.76191H1.43799V2.30953C1.43799 1.77303 1.87291 1.3381 2.40942 1.3381H6.45704V1.66191H2.40942Z" fill="currentColor"></path></svg>Embed</button>

					<button class="bg-linear-to-b shadow-xs flex items-center gap-1.5 rounded-full border from-white to-red-100/90 px-2 py-0.5 text-xs font-medium text-[#2D3648] transition-shadow hover:shadow-inner dark:from-gray-900 dark:to-red-800/30 dark:text-gray-100 dark:hover:shadow-inner dark:hover:shadow-red-800/30" ><svg class="h-3.5 w-3.5 text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
						<span>Data Studio</span></button></div>
		<div class="flex flex-1 flex-col overflow-hidden -mx-2.5"><div class="flex flex-1 flex-col overflow-hidden"><div class="flex min-h-0 flex-1"><div class="flex flex-1 flex-col overflow-hidden"><div class="md:-mx-2.5 flex min-w-0 flex-wrap border-t"><div class="flex min-w-0 flex-1 flex-wrap"><div class="grid flex-1 grid-cols-1 overflow-hidden text-sm md:grid-cols-2 md:place-content-center sm:mx-2.5"><label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-950 md:border-r md:border-r-0 hidden" title="default"><span class="text-gray-500">Subset (1)</span>
			<div class="flex items-center whitespace-nowrap"><span class="truncate">default</span>
				<span class="mx-2 text-gray-500">Â·</span>
					<span class="text-gray-500">118k rows</span>
				<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
			<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Subset (1)"><option value="default" selected>default (118k rows)</option></optgroup></select></label>
		<label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-900 md:border-r md:border-r" title="train"><div class="text-gray-500">Split (1)</div>
				<div class="flex items-center overflow-hidden whitespace-nowrap"><span class="truncate">train</span>
					<span class="mx-2 text-gray-500">Â·</span>
						<span class="text-gray-500">118k rows</span>
					<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
				<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Split (1)"><option value="train" selected>train (118k rows)</option></optgroup></select></label></div></div>
								</div>

							<div class="flex min-h-0 flex-1 flex-col ">
	<div class="bg-linear-to-r text-smd relative flex items-center dark:border-gray-900 dark:bg-gray-950 false border-t [&amp;:has(:focus)]:from-gray-50 [&amp;:has(:focus)]:to-transparent [&amp;:has(:focus)]:to-20% dark:[&amp;:has(:focus)]:from-gray-900"><form class="flex-1"><svg class="absolute left-3 top-1/2 transform -translate-y-1/2 pointer-events-none text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				<input disabled class="outline-hidden h-9 w-full border-none bg-transparent px-1 pl-9 pr-3 placeholder:text-gray-400 " placeholder="Search this dataset" dir="auto"></form>
			<div class="flex items-center gap-2 px-2 py-1"><button type="button" class="hover:bg-yellow-200/70 flex items-center gap-1 rounded-md border border-yellow-200 bg-yellow-100 pl-0.5 pr-1 text-[.8rem] leading-normal text-gray-700 dark:border-orange-500/25 dark:bg-orange-500/20 dark:text-gray-300 dark:hover:brightness-110 hidden"><div class="rounded-sm bg-yellow-300 px-1 font-mono text-[.7rem] font-bold text-black dark:bg-yellow-700 dark:text-gray-200">SQL
	</div>
	Console
</button></div></div>


<div class="flex flex-1 flex-col overflow-hidden min-h-64 border-t">
		

<div class="max-h-96 relative overflow-auto"><table class="w-full table-auto rounded-lg font-mono text-xs text-gray-900"><thead class="shadow-xs sticky left-0 right-0 top-0 z-1 bg-white align-top"><tr class="space-y-54 h-full min-w-fit divide-x border-b text-left"><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">Unnamed: 0.1
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>int64</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="0" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="13.2" y="7.995079950799507" width="11.2" height="22.004920049200493" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="26.4" y="7.995079950799507" width="11.2" height="22.004920049200493" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="39.599999999999994" y="7.995079950799507" width="11.2" height="22.004920049200493" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="52.8" y="7.995079950799507" width="11.2" height="22.004920049200493" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="66" y="7.995079950799507" width="11.2" height="22.004920049200493" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="79.19999999999999" y="7.995079950799507" width="11.2" height="22.004920049200493" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="92.39999999999999" y="7.995079950799507" width="11.2" height="22.004920049200493" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="105.6" y="7.995079950799507" width="11.2" height="22.004920049200493" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="118.8" y="8.007872078720787" width="11.2" height="21.992127921279213" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">0</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">113k</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">Unnamed: 0
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>float64</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="0" y="0" width="10" height="30" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="12" y="0.002309058614564208" width="10" height="29.997690941385436" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="24" y="0.002309058614564208" width="10" height="29.997690941385436" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="36" y="0.002309058614564208" width="10" height="29.997690941385436" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="48" y="0.002309058614564208" width="10" height="29.997690941385436" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="60" y="0.002309058614564208" width="10" height="29.997690941385436" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="72" y="0.002309058614564208" width="10" height="29.997690941385436" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="84" y="0.002309058614564208" width="10" height="29.997690941385436" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="96" y="0.002309058614564208" width="10" height="29.997690941385436" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="108" y="0" width="10" height="30" fill-opacity="1"></rect><rect class=" fill-gray-200 dark:fill-gray-500/80" rx="2" x="120" y="14.454706927175843" width="10" height="15.545293072824157" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="11" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="23" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="35" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="47" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="59" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="71" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="83" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="95" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="107" y="0" width="12" height="30" fill-opacity="0"></rect><rect class="fill-gray-200 cursor-pointer" x="119" y="0" width="12" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 54px">0</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 12px; max-width: 54px">113k</div>
			<div class="absolute -translate-x-1/2" style="left: 125px">âŒ€</div></div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">title
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['Â·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="24.253550640279393" width="11.2" height="5.746449359720605" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="11.52777648428405" width="11.2" height="18.47222351571595" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="5.118277066356228" width="11.2" height="24.881722933643772" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="19.679487776484283" width="11.2" height="10.320512223515717" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="24.599813736903375" width="11.2" height="5.400186263096624" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">7</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">246</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">abstract
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['Â·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="21.178195187480853" width="11.2" height="8.821804812519149" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="6.948080412905657" width="11.2" height="23.051919587094343" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="10.529659918455845" width="11.2" height="19.470340081544155" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="19.629139072847682" width="11.2" height="10.370860927152318" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">6</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">3.31k</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th></tr></thead>
			<tbody class="h-16 overflow-scroll"><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="0"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">0</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Learning from compressed observations</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The problem of statistical learning is to construct a predictor of a random
variable $Y$ as a function of a related random variable $X$ on the basis of an
i.i.d. training sample from the joint distribution of $(X,Y)$. Allowable
predictors are drawn from some specified class, and the goal is to approach
asymptotically the performance (expected loss) of the best predictor in the
class. We consider the setting in which one has perfect observation of the
$X$-part of the sample, while the $Y$-part has to be communicated at some
finite bit rate. The encoding of the $Y$-values is allowed to depend on the
$X$-values. Under suitable regularity conditions on the admissible predictors,
the underlying family of probability distributions and the loss function, we
give an information-theoretic characterization of achievable predictor
performance in terms of conditional distortion-rate functions. The ideas are
illustrated on the example of nonparametric regression in Gaussian noise.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="1"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Sensor Networks with Random Links: Topology Design for Distributed
  Consensus</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In a sensor network, in practice, the communication among sensors is subject
to:(1) errors or failures at random times; (3) costs; and(2) constraints since
sensors and networks operate under scarce resources, such as power, data rate,
or communication. The signal-to-noise ratio (SNR) is usually a main factor in
determining the probability of error (or of communication failure) in a link.
These probabilities are then a proxy for the SNR under which the links operate.
The paper studies the problem of designing the topology, i.e., assigning the
probabilities of reliable communication among sensors (or of link failures) to
maximize the rate of convergence of average consensus, when the link
communication costs are taken into account, and there is an overall
communication budget constraint. To consider this problem, we address a number
of preliminary issues: (1) model the network as a random topology; (2)
establish necessary and sufficient conditions for mean square sense (mss) and
almost sure (a.s.) convergence of average consensus when network links fail;
and, in particular, (3) show that a necessary and sufficient condition for both
mss and a.s. convergence is for the algebraic connectivity of the mean graph
describing the network topology to be strictly positive. With these results, we
formulate topology design, subject to random link failures and to a
communication cost constraint, as a constrained convex optimization problem to
which we apply semidefinite programming techniques. We show by an extensive
numerical study that the optimal design improves significantly the convergence
speed of the consensus algorithm and can achieve the asymptotic performance of
a non-random network at a fraction of the communication cost.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="2"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">2</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The on-line shortest path problem under partial monitoring</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The on-line shortest path problem is considered under various models of
partial monitoring. Given a weighted directed acyclic graph whose edge weights
can change in an arbitrary (adversarial) way, a decision maker has to choose in
each round of a game a path between two distinguished vertices such that the
loss of the chosen path (defined as the sum of the weights of its composing
edges) be as small as possible. In a setting generalizing the multi-armed
bandit problem, after choosing a path, the decision maker learns only the
weights of those edges that belong to the chosen path. For this problem, an
algorithm is given whose average cumulative loss in n rounds exceeds that of
the best path, matched off-line to the entire sequence of the edge weights, by
a quantity that is proportional to 1/\sqrt{n} and depends only polynomially on
the number of edges of the graph. The algorithm can be implemented with linear
complexity in the number of rounds n and in the number of edges. An extension
to the so-called label efficient setting is also given, in which the decision
maker is informed about the weights of the edges corresponding to the chosen
path at a total of m &lt;&lt; n time instances. Another extension is shown where the
decision maker competes against a time-varying path, a generalization of the
problem of tracking the best expert. A version of the multi-armed bandit
setting for shortest path is also discussed where the decision maker learns
only the total weight of the chosen path but not the weights of the individual
edges on the path. Applications to routing in packet switched networks along
with simulation results are also presented.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="3"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">3</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">3</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A neural network approach to ordinal regression</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Ordinal regression is an important type of learning, which has properties of
both classification and regression. Here we describe a simple and effective
approach to adapt a traditional neural network to learn ordinal categories. Our
approach is a generalization of the perceptron method for ordinal regression.
On several benchmark datasets, our method (NNRank) outperforms a neural network
classification method. Compared with the ordinal regression methods using
Gaussian processes and support vector machines, NNRank achieves comparable
performance. Moreover, NNRank has the advantages of traditional neural
networks: learning in both online and batch modes, handling very large training
datasets, and making rapid predictions. These features make NNRank a useful and
complementary tool for large-scale data processing tasks such as information
retrieval, web page ranking, collaborative filtering, and protein ranking in
Bioinformatics.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="4"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">4</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">4</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Parametric Learning and Monte Carlo Optimization</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This paper uncovers and explores the close relationship between Monte Carlo
Optimization of a parametrized integral (MCO), Parametric machine-Learning
(PL), and `blackbox' or `oracle'-based optimization (BO). We make four
contributions. First, we prove that MCO is mathematically identical to a broad
class of PL problems. This identity potentially provides a new application
domain for all broadly applicable PL techniques: MCO. Second, we introduce
immediate sampling, a new version of the Probability Collectives (PC) algorithm
for blackbox optimization. Immediate sampling transforms the original BO
problem into an MCO problem. Accordingly, by combining these first two
contributions, we can apply all PL techniques to BO. In our third contribution
we validate this way of improving BO by demonstrating that cross-validation and
bagging improve immediate sampling. Finally, conventional MC and MCO procedures
ignore the relationship between the sample point locations and the associated
values of the integrand; only the values of the integrand at those locations
are considered. We demonstrate that one can exploit the sample location
information using PL techniques, for example by forming a fit of the sample
locations to the associated values of the integrand. This provides an
additional way to apply PL techniques to improve MCO.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="5"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">5</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">5</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Preconditioned Temporal Difference Learning</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This paper has been withdrawn by the author. This draft is withdrawn for its
poor quality in english, unfortunately produced by the author when he was just
starting his science route. Look at the ICML version instead:
http://icml2008.cs.helsinki.fi/papers/111.pdf
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="6"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">6</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">6</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Note on the Inapproximability of Correlation Clustering</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We consider inapproximability of the correlation clustering problem defined
as follows: Given a graph $G = (V,E)$ where each edge is labeled either "+"
(similar) or "-" (dissimilar), correlation clustering seeks to partition the
vertices into clusters so that the number of pairs correctly (resp.
incorrectly) classified with respect to the labels is maximized (resp.
minimized). The two complementary problems are called MaxAgree and MinDisagree,
respectively, and have been studied on complete graphs, where every edge is
labeled, and general graphs, where some edge might not have been labeled.
Natural edge-weighted versions of both problems have been studied as well. Let
S-MaxAgree denote the weighted problem where all weights are taken from set S,
we show that S-MaxAgree with weights bounded by $O(|V|^{1/2-\delta})$
essentially belongs to the same hardness class in the following sense: if there
is a polynomial time algorithm that approximates S-MaxAgree within a factor of
$\lambda = O(\log{|V|})$ with high probability, then for any choice of S',
S'-MaxAgree can be approximated in polynomial time within a factor of $(\lambda
+ \epsilon)$, where $\epsilon > 0$ can be arbitrarily small, with high
probability. A similar statement also holds for $S-MinDisagree. This result
implies it is hard (assuming $NP \neq RP$) to approximate unweighted MaxAgree
within a factor of $80/79-\epsilon$, improving upon a previous known factor of
$116/115-\epsilon$ by Charikar et. al. \cite{Chari05}.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="7"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">7</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">7</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Joint universal lossy coding and identification of stationary mixing
  sources</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The problem of joint universal source coding and modeling, treated in the
context of lossless codes by Rissanen, was recently generalized to fixed-rate
lossy coding of finitely parametrized continuous-alphabet i.i.d. sources. We
extend these results to variable-rate lossy block coding of stationary ergodic
sources and show that, for bounded metric distortion measures, any finitely
parametrized family of stationary sources satisfying suitable mixing,
smoothness and Vapnik-Chervonenkis learnability conditions admits universal
schemes for joint lossy source coding and identification. We also give several
explicit examples of parametric sources satisfying the regularity conditions.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="8"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">8</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">8</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Supervised Feature Selection via Dependence Estimation</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We introduce a framework for filtering features that employs the
Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence
between the features and the labels. The key idea is that good features should
maximise such dependence. Feature selection for various supervised learning
problems (including classification and regression) is unified under this
framework, and the solutions can be approximated using a backward-elimination
algorithm. We demonstrate the usefulness of our method on both artificial and
real world datasets.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="9"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">9</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">9</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Equivalence of LP Relaxation and Max-Product for Weighted Matching in
  General Graphs</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Max-product belief propagation is a local, iterative algorithm to find the
mode/MAP estimate of a probability distribution. While it has been successfully
employed in a wide variety of applications, there are relatively few
theoretical guarantees of convergence and correctness for general loopy graphs
that may have many short cycles. Of these, even fewer provide exact ``necessary
and sufficient'' characterizations.
  In this paper we investigate the problem of using max-product to find the
maximum weight matching in an arbitrary graph with edge weights. This is done
by first constructing a probability distribution whose mode corresponds to the
optimal matching, and then running max-product. Weighted matching can also be
posed as an integer program, for which there is an LP relaxation. This
relaxation is not always tight. In this paper we show that \begin{enumerate}
\item If the LP relaxation is tight, then max-product always converges, and
that too to the correct answer. \item If the LP relaxation is loose, then
max-product does not converge. \end{enumerate} This provides an exact,
data-dependent characterization of max-product performance, and a precise
connection to LP relaxation, which is a well-studied optimization technique.
Also, since LP relaxation is known to be tight for bipartite graphs, our
results generalize other recent results on using max-product to find weighted
matchings in bipartite graphs.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="10"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">10</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">10</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">HMM Speaker Identification Using Linear and Non-linear Merging
  Techniques</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Speaker identification is a powerful, non-invasive and in-expensive biometric
technique. The recognition accuracy, however, deteriorates when noise levels
affect a specific band of frequency. In this paper, we present a sub-band based
speaker identification that intends to improve the live testing performance.
Each frequency sub-band is processed and classified independently. We also
compare the linear and non-linear merging techniques for the sub-bands
recognizer. Support vector machines and Gaussian Mixture models are the
non-linear merging techniques that are investigated. Results showed that the
sub-band based method used with linear merging techniques enormously improved
the performance of the speaker identification over the performance of wide-band
recognizers when tested live. A live testing improvement of 9.78% was achieved
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="11"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">11</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">11</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Statistical Mechanics of Nonlinear On-line Learning for Ensemble
  Teachers</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We analyze the generalization performance of a student in a model composed of
nonlinear perceptrons: a true teacher, ensemble teachers, and the student. We
calculate the generalization error of the student analytically or numerically
using statistical mechanics in the framework of on-line learning. We treat two
well-known learning rules: Hebbian learning and perceptron learning. As a
result, it is proven that the nonlinear model shows qualitatively different
behaviors from the linear model. Moreover, it is clarified that Hebbian
learning and perceptron learning show qualitatively different behaviors from
each other. In Hebbian learning, we can analytically obtain the solutions. In
this case, the generalization error monotonically decreases. The steady value
of the generalization error is independent of the learning rate. The larger the
number of teachers is and the more variety the ensemble teachers have, the
smaller the generalization error is. In perceptron learning, we have to
numerically obtain the solutions. In this case, the dynamical behaviors of the
generalization error are non-monotonic. The smaller the learning rate is, the
larger the number of teachers is; and the more variety the ensemble teachers
have, the smaller the minimum value of the generalization error is.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="12"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">12</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">12</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">On the monotonization of the training set</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We consider the problem of minimal correction of the training set to make it
consistent with monotonic constraints. This problem arises during analysis of
data sets via techniques that require monotone data. We show that this problem
is NP-hard in general and is equivalent to finding a maximal independent set in
special orgraphs. Practically important cases of that problem considered in
detail. These are the cases when a partial order given on the replies set is a
total order or has a dimension 2. We show that the second case can be reduced
to maximization of a quadratic convex function on a convex set. For this case
we construct an approximate polynomial algorithm based on convex optimization.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="13"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">13</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">13</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Mixed membership stochastic blockmodels</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Observations consisting of measurements on relationships for pairs of objects
arise in many settings, such as protein interaction and gene regulatory
networks, collections of author-recipient email, and social networks. Analyzing
such data with probabilisic models can be delicate because the simple
exchangeability assumptions underlying many boilerplate models no longer hold.
In this paper, we describe a latent variable model of such data called the
mixed membership stochastic blockmodel. This model extends blockmodels for
relational data to ones which capture mixed membership latent relational
structure, thus providing an object-specific low-dimensional representation. We
develop a general variational inference algorithm for fast approximate
posterior inference. We explore applications to social and protein interaction
networks.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="14"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">14</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">14</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Loop corrections for message passing algorithms in continuous variable
  models</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper we derive the equations for Loop Corrected Belief Propagation
on a continuous variable Gaussian model. Using the exactness of the averages
for belief propagation for Gaussian models, a different way of obtaining the
covariances is found, based on Belief Propagation on cavity graphs. We discuss
the relation of this loop correction algorithm to Expectation Propagation
algorithms for the case in which the model is no longer Gaussian, but slightly
perturbed by nonlinear terms.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="15"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">15</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">15</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Novel Model of Working Set Selection for SMO Decomposition Methods</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In the process of training Support Vector Machines (SVMs) by decomposition
methods, working set selection is an important technique, and some exciting
schemes were employed into this field. To improve working set selection, we
propose a new model for working set selection in sequential minimal
optimization (SMO) decomposition methods. In this model, it selects B as
working set without reselection. Some properties are given by simple proof, and
experiments demonstrate that the proposed method is in general faster than
existing methods.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="16"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">16</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">16</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Getting started in probabilistic graphical models</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Probabilistic graphical models (PGMs) have become a popular tool for
computational analysis of biological data in a variety of domains. But, what
exactly are they and how do they work? How can we use PGMs to discover patterns
that are biologically relevant? And to what extent can PGMs help us formulate
new hypotheses that are testable at the bench? This note sketches out some
answers and illustrates the main ideas behind the statistical approach to
biological pattern discovery.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="17"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">17</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">17</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A tutorial on conformal prediction</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Conformal prediction uses past experience to determine precise levels of
confidence in new predictions. Given an error probability $\epsilon$, together
with a method that makes a prediction $\hat{y}$ of a label $y$, it produces a
set of labels, typically containing $\hat{y}$, that also contains $y$ with
probability $1-\epsilon$. Conformal prediction can be applied to any method for
producing $\hat{y}$: a nearest-neighbor method, a support-vector machine, ridge
regression, etc.
  Conformal prediction is designed for an on-line setting in which labels are
predicted successively, each one being revealed before the next is predicted.
The most novel and valuable feature of conformal prediction is that if the
successive examples are sampled independently from the same distribution, then
the successive predictions will be right $1-\epsilon$ of the time, even though
they are based on an accumulating dataset rather than on independent datasets.
  In addition to the model under which successive examples are sampled
independently, other on-line compression models can also use conformal
prediction. The widely used Gaussian linear model is one of these.
  This tutorial presents a self-contained account of the theory of conformal
prediction and works through several numerical examples. A more comprehensive
treatment of the topic is provided in "Algorithmic Learning in a Random World",
by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="18"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">18</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">18</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Scale-sensitive Psi-dimensions: the Capacity Measures for Classifiers
  Taking Values in R^Q</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Bounds on the risk play a crucial role in statistical learning theory. They
usually involve as capacity measure of the model studied the VC dimension or
one of its extensions. In classification, such "VC dimensions" exist for models
taking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations
appropriate for the missing case, the one of models with values in R^Q. This
provides us with a new guaranteed risk for M-SVMs which appears superior to the
existing one.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="19"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">19</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">19</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The Role of Time in the Creation of Knowledge</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This paper I assume that in humans the creation of knowledge depends on a
discrete time, or stage, sequential decision-making process subjected to a
stochastic, information transmitting environment. For each time-stage, this
environment randomly transmits Shannon type information-packets to the
decision-maker, who examines each of them for relevancy and then determines his
optimal choices. Using this set of relevant information-packets, the
decision-maker adapts, over time, to the stochastic nature of his environment,
and optimizes the subjective expected rate-of-growth of knowledge. The
decision-maker's optimal actions, lead to a decision function that involves,
over time, his view of the subjective entropy of the environmental process and
other important parameters at each time-stage of the process. Using this model
of human behavior, one could create psychometric experiments using computer
simulation and real decision-makers, to play programmed games to measure the
resulting human performance.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="20"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">20</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">20</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Clustering and Feature Selection using Sparse Principal Component
  Analysis</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper, we study the application of sparse principal component
analysis (PCA) to clustering and feature selection problems. Sparse PCA seeks
sparse factors, or linear combinations of the data variables, explaining a
maximum amount of variance in the data while having only a limited number of
nonzero coefficients. PCA is often used as a simple clustering technique and
sparse factors allow us here to interpret the clusters in terms of a reduced
set of variables. We begin with a brief introduction and motivation on sparse
PCA and detail our implementation of the algorithm in d'Aspremont et al.
(2005). We then apply these results to some classic clustering and feature
selection problems arising in biology.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="21"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">21</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">21</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Model Selection Through Sparse Maximum Likelihood Estimation</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We consider the problem of estimating the parameters of a Gaussian or binary
distribution in such a way that the resulting undirected graphical model is
sparse. Our approach is to solve a maximum likelihood problem with an added
l_1-norm penalty term. The problem as formulated is convex but the memory
requirements and complexity of existing interior point methods are prohibitive
for problems with more than tens of nodes. We present two new algorithms for
solving problems with at least a thousand nodes in the Gaussian case. Our first
algorithm uses block coordinate descent, and can be interpreted as recursive
l_1-norm penalized regression. Our second algorithm, based on Nesterov's first
order method, yields a complexity estimate with a better dependence on problem
size than existing interior point methods. Using a log determinant relaxation
of the log partition function (Wainwright &amp; Jordan (2006)), we show that these
same algorithms can be used to solve an approximate sparse maximum likelihood
problem for the binary case. We test our algorithms on synthetic data, as well
as on gene expression and senate voting records data.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="22"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">22</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">22</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Optimal Solutions for Sparse Principal Component Analysis</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Given a sample covariance matrix, we examine the problem of maximizing the
variance explained by a linear combination of the input variables while
constraining the number of nonzero coefficients in this combination. This is
known as sparse principal component analysis and has a wide array of
applications in machine learning and engineering. We formulate a new
semidefinite relaxation to this problem and derive a greedy algorithm that
computes a full set of good solutions for all target numbers of non zero
coefficients, with total complexity O(n^3), where n is the number of variables.
We then use the same relaxation to derive sufficient conditions for global
optimality of a solution, which can be tested in O(n^3) per pattern. We discuss
applications in subset selection and sparse recovery and show on artificial
examples and biological data that our algorithm does provide globally optimal
solutions in many cases.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="23"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">23</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">23</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A New Generalization of Chebyshev Inequality for Random Vectors</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this article, we derive a new generalization of Chebyshev inequality for
random vectors. We demonstrate that the new generalization is much less
conservative than the classical generalization.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="24"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">24</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">24</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Clusters, Graphs, and Networks for Analysing Internet-Web-Supported
  Communication within a Virtual Community</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The proposal is to use clusters, graphs and networks as models in order to
analyse the Web structure. Clusters, graphs and networks provide knowledge
representation and organization. Clusters were generated by co-site analysis.
The sample is a set of academic Web sites from the countries belonging to the
European Union. These clusters are here revisited from the point of view of
graph theory and social network analysis. This is a quantitative and structural
analysis. In fact, the Internet is a computer network that connects people and
organizations. Thus we may consider it to be a social network. The set of Web
academic sites represents an empirical social network, and is viewed as a
virtual community. The network structural properties are here analysed applying
together cluster analysis, graph theory and social network analysis.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="25"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">25</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">25</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Universal Reinforcement Learning</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We consider an agent interacting with an unmodeled environment. At each time,
the agent makes an observation, takes an action, and incurs a cost. Its actions
can influence future observations and costs. The goal is to minimize the
long-term average cost. We propose a novel algorithm, known as the active LZ
algorithm, for optimal control based on ideas from the Lempel-Ziv scheme for
universal data compression and prediction. We establish that, under the active
LZ algorithm, if there exists an integer $K$ such that the future is
conditionally independent of the past given a window of $K$ consecutive actions
and observations, then the average cost converges to the optimum. Experimental
results involving the game of Rock-Paper-Scissors illustrate merits of the
algorithm.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="26"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">26</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">26</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Consistency of the group Lasso and multiple kernel learning</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We consider the least-square regression problem with regularization by a
block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger
than one. This problem, referred to as the group Lasso, extends the usual
regularization by the 1-norm where all spaces have dimension one, where it is
commonly referred to as the Lasso. In this paper, we study the asymptotic model
consistency of the group Lasso. We derive necessary and sufficient conditions
for the consistency of group Lasso under practical assumptions, such as model
misspecification. When the linear predictors and Euclidean norms are replaced
by functions and reproducing kernel Hilbert norms, the problem is usually
referred to as multiple kernel learning and is commonly used for learning from
heterogeneous data sources and for non linear variable selection. Using tools
from functional analysis, and in particular covariance operators, we extend the
consistency results to this infinite dimensional case and also propose an
adaptive scheme to obtain a consistent model estimate, even when the necessary
condition required for the non adaptive scheme is not satisfied.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="27"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">27</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">27</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Quantum Algorithms for Learning and Testing Juntas</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this article we develop quantum algorithms for learning and testing
juntas, i.e. Boolean functions which depend only on an unknown set of k out of
n input variables. Our aim is to develop efficient algorithms:
  - whose sample complexity has no dependence on n, the dimension of the domain
the Boolean functions are defined over;
  - with no access to any classical or quantum membership ("black-box")
queries. Instead, our algorithms use only classical examples generated
uniformly at random and fixed quantum superpositions of such classical
examples;
  - which require only a few quantum examples but possibly many classical
random examples (which are considered quite "cheap" relative to quantum
examples).
  Our quantum algorithms are based on a subroutine FS which enables sampling
according to the Fourier spectrum of f; the FS subroutine was used in earlier
work of Bshouty and Jackson on quantum learning. Our results are as follows:
  - We give an algorithm for testing k-juntas to accuracy $\epsilon$ that uses
$O(k/\epsilon)$ quantum examples. This improves on the number of examples used
by the best known classical algorithm.
  - We establish the following lower bound: any FS-based k-junta testing
algorithm requires $\Omega(\sqrt{k})$ queries.
  - We give an algorithm for learning $k$-juntas to accuracy $\epsilon$ that
uses $O(\epsilon^{-1} k\log k)$ quantum examples and $O(2^k \log(1/\epsilon))$
random examples. We show that this learning algorithms is close to optimal by
giving a related lower bound.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="28"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">28</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">28</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Virtual screening with support vector machines and structure kernels</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Support vector machines and kernel methods have recently gained considerable
attention in chemoinformatics. They offer generally good performance for
problems of supervised classification or regression, and provide a flexible and
computationally efficient framework to include relevant information and prior
knowledge about the data and problems to be handled. In particular, with kernel
methods molecules do not need to be represented and stored explicitly as
vectors or fingerprints, but only to be compared to each other through a
comparison function technically called a kernel. While classical kernels can be
used to compare vector or fingerprint representations of molecules, completely
new kernels were developed in the recent years to directly compare the 2D or 3D
structures of molecules, without the need for an explicit vectorization step
through the extraction of molecular descriptors. While still in their infancy,
these approaches have already demonstrated their relevance on several toxicity
prediction and structure-activity relationship problems.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="29"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">29</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">29</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Structure or Noise?</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We show how rate-distortion theory provides a mechanism for automated theory
building by naturally distinguishing between regularity and randomness. We
start from the simple principle that model variables should, as much as
possible, render the future and past conditionally independent. From this, we
construct an objective function for model making whose extrema embody the
trade-off between a model's structural complexity and its predictive power. The
solutions correspond to a hierarchy of models that, at each level of
complexity, achieve optimal predictive power at minimal cost. In the limit of
maximal prediction the resulting optimal model identifies a process's intrinsic
organization by extracting the underlying causal states. In this limit, the
model's complexity is given by the statistical complexity, which is known to be
minimal for achieving maximum prediction. Examples show how theory building can
profit from analyzing a process's causal compressibility, which is reflected in
the optimal models' rate-distortion curve--the process's characteristic for
optimally balancing structure and noise at different levels of representation.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="30"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">30</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">30</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Cost-minimising strategies for data labelling : optimal stopping and
  active learning</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Supervised learning deals with the inference of a distribution over an output
or label space $\CY$ conditioned on points in an observation space $\CX$, given
a training dataset $D$ of pairs in $\CX \times \CY$. However, in a lot of
applications of interest, acquisition of large amounts of observations is easy,
while the process of generating labels is time-consuming or costly. One way to
deal with this problem is {\em active} learning, where points to be labelled
are selected with the aim of creating a model with better performance than that
of an model trained on an equal number of randomly sampled points. In this
paper, we instead propose to deal with the labelling cost directly: The
learning goal is defined as the minimisation of a cost which is a function of
the expected model performance and the total cost of the labels used. This
allows the development of general strategies and specific algorithms for (a)
optimal stopping, where the expected cost dictates whether label acquisition
should continue (b) empirical evaluation, where the cost is used as a
performance metric for a given combination of inference, stopping and sampling
methods. Though the main focus of the paper is optimal stopping, we also aim to
provide the background for further developments and discussion in the related
field of active learning.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="31"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">31</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">31</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Defensive forecasting for optimal prediction with expert advice</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The method of defensive forecasting is applied to the problem of prediction
with expert advice for binary outcomes. It turns out that defensive forecasting
is not only competitive with the Aggregating Algorithm but also handles the
case of "second-guessing" experts, whose advice depends on the learner's
prediction; this paper assumes that the dependence on the learner's prediction
is continuous.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="32"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">32</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">32</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Optimal Causal Inference: Estimating Stored Information and
  Approximating Causal Architecture</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We introduce an approach to inferring the causal architecture of stochastic
dynamical systems that extends rate distortion theory to use causal
shielding---a natural principle of learning. We study two distinct cases of
causal inference: optimal causal filtering and optimal causal estimation.
  Filtering corresponds to the ideal case in which the probability distribution
of measurement sequences is known, giving a principled method to approximate a
system's causal structure at a desired level of representation. We show that,
in the limit in which a model complexity constraint is relaxed, filtering finds
the exact causal architecture of a stochastic dynamical system, known as the
causal-state partition. From this, one can estimate the amount of historical
information the process stores. More generally, causal filtering finds a graded
model-complexity hierarchy of approximations to the causal architecture. Abrupt
changes in the hierarchy, as a function of approximation, capture distinct
scales of structural organization.
  For nonideal cases with finite data, we show how the correct number of
underlying causal states can be found by optimal causal estimation. A
previously derived model complexity control term allows us to correct for the
effect of statistical fluctuations in probability estimates and thereby avoid
over-fitting.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="33"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">33</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">33</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">On Semimeasures Predicting Martin-Loef Random Sequences</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Solomonoff's central result on induction is that the posterior of a universal
semimeasure M converges rapidly and with probability 1 to the true sequence
generating posterior mu, if the latter is computable. Hence, M is eligible as a
universal sequence predictor in case of unknown mu. Despite some nearby results
and proofs in the literature, the stronger result of convergence for all
(Martin-Loef) random sequences remained open. Such a convergence result would
be particularly interesting and natural, since randomness can be defined in
terms of M itself. We show that there are universal semimeasures M which do not
converge for all random sequences, i.e. we give a partial negative answer to
the open problem. We also provide a positive answer for some non-universal
semimeasures. We define the incomputable measure D as a mixture over all
computable measures and the enumerable semimeasure W as a mixture over all
enumerable nearly-measures. We show that W converges to D and D to mu on all
random sequences. The Hellinger distance measuring closeness of two
distributions plays a central role.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="34"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">34</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">34</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Continuous and randomized defensive forecasting: unified view</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Defensive forecasting is a method of transforming laws of probability (stated
in game-theoretic terms as strategies for Sceptic) into forecasting algorithms.
There are two known varieties of defensive forecasting: "continuous", in which
Sceptic's moves are assumed to depend on the forecasts in a (semi)continuous
manner and which produces deterministic forecasts, and "randomized", in which
the dependence of Sceptic's moves on the forecasts is arbitrary and
Forecaster's moves are allowed to be randomized. This note shows that the
randomized variety can be obtained from the continuous variety by smearing
Sceptic's moves to make them continuous.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="35"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">35</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">35</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Dichotomy Theorem for General Minimum Cost Homomorphism Problem</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In the constraint satisfaction problem ($CSP$), the aim is to find an
assignment of values to a set of variables subject to specified constraints. In
the minimum cost homomorphism problem ($MinHom$), one is additionally given
weights $c_{va}$ for every variable $v$ and value $a$, and the aim is to find
an assignment $f$ to the variables that minimizes $\sum_{v} c_{vf(v)}$. Let
$MinHom(\Gamma)$ denote the $MinHom$ problem parameterized by the set of
predicates allowed for constraints. $MinHom(\Gamma)$ is related to many
well-studied combinatorial optimization problems, and concrete applications can
be found in, for instance, defence logistics and machine learning. We show that
$MinHom(\Gamma)$ can be studied by using algebraic methods similar to those
used for CSPs. With the aid of algebraic techniques, we classify the
computational complexity of $MinHom(\Gamma)$ for all choices of $\Gamma$. Our
result settles a general dichotomy conjecture previously resolved only for
certain classes of directed graphs, [Gutin, Hell, Rafiey, Yeo, European J. of
Combinatorics, 2008].
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="36"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">36</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">36</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Filtering Additive Measurement Noise with Maximum Entropy in the Mean</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The purpose of this note is to show how the method of maximum entropy in the
mean (MEM) may be used to improve parametric estimation when the measurements
are corrupted by large level of noise. The method is developed in the context
on a concrete example: that of estimation of the parameter in an exponential
distribution. We compare the performance of our method with the bayesian and
maximum likelihood approaches.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="37"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">37</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">37</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">On Universal Prediction and Bayesian Confirmation</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The Bayesian framework is a well-studied and successful framework for
inductive reasoning, which includes hypothesis testing and confirmation,
parameter estimation, sequence prediction, classification, and regression. But
standard statistical guidelines for choosing the model class and prior are not
always available or fail, in particular in complex situations. Solomonoff
completed the Bayesian framework by providing a rigorous, unique, formal, and
universal choice for the model class and the prior. We discuss in breadth how
and in which sense universal (non-i.i.d.) sequence prediction solves various
(philosophical) problems of traditional Bayesian sequence prediction. We show
that Solomonoff's model possesses many desirable properties: Strong total and
weak instantaneous bounds, and in contrast to most classical continuous prior
densities has no zero p(oste)rior problem, i.e. can confirm universal
hypotheses, is reparametrization and regrouping invariant, and avoids the
old-evidence and updating problem. It even performs well (actually better) in
non-computable environments.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="38"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">38</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">38</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Learning for Dynamic Bidding in Cognitive Radio Resources</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper, we model the various wireless users in a cognitive radio
network as a collection of selfish, autonomous agents that strategically
interact in order to acquire the dynamically available spectrum opportunities.
Our main focus is on developing solutions for wireless users to successfully
compete with each other for the limited and time-varying spectrum
opportunities, given the experienced dynamics in the wireless network. We
categorize these dynamics into two types: one is the disturbance due to the
environment (e.g. wireless channel conditions, source traffic characteristics,
etc.) and the other is the impact caused by competing users. To analyze the
interactions among users given the environment disturbance, we propose a
general stochastic framework for modeling how the competition among users for
spectrum opportunities evolves over time. At each stage of the dynamic resource
allocation, a central spectrum moderator auctions the available resources and
the users strategically bid for the required resources. The joint bid actions
affect the resource allocation and hence, the rewards and future strategies of
all users. Based on the observed resource allocation and corresponding rewards
from previous allocations, we propose a best response learning algorithm that
can be deployed by wireless users to improve their bidding policy at each
stage. The simulation results show that by deploying the proposed best response
learning algorithm, the wireless users can significantly improve their own
performance in terms of both the packet loss rate and the incurred cost for the
used resources.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="39"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">39</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">39</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Mutual information for the selection of relevant variables in
  spectrometric nonlinear modelling</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Data from spectrophotometers form vectors of a large number of exploitable
variables. Building quantitative models using these variables most often
requires using a smaller set of variables than the initial one. Indeed, a too
large number of input variables to a model results in a too large number of
parameters, leading to overfitting and poor generalization abilities. In this
paper, we suggest the use of the mutual information measure to select variables
from the initial set. The mutual information measures the information content
in input variables with respect to the model output, without making any
assumption on the model that will be used; it is thus suitable for nonlinear
modelling. In addition, it leads to the selection of variables among the
initial set, and not to linear or nonlinear combinations of them. Without
decreasing the model performances compared to other variable projection
methods, it allows therefore a greater interpretability of the results.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="40"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">40</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">40</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Fast Algorithm and Implementation of Dissimilarity Self-Organizing Maps</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In many real world applications, data cannot be accurately represented by
vectors. In those situations, one possible solution is to rely on dissimilarity
measures that enable sensible comparison between observations. Kohonen's
Self-Organizing Map (SOM) has been adapted to data described only through their
dissimilarity matrix. This algorithm provides both non linear projection and
clustering of non vector data. Unfortunately, the algorithm suffers from a high
cost that makes it quite difficult to use with voluminous data sets. In this
paper, we propose a new algorithm that provides an important reduction of the
theoretical cost of the dissimilarity SOM without changing its outcome (the
results are exactly the same as the ones obtained with the original algorithm).
Moreover, we introduce implementation methods that result in very short running
times. Improvements deduced from the theoretical cost model are validated on
simulated and real world data (a word list clustering problem). We also
demonstrate that the proposed implementation methods reduce by a factor up to 3
the running time of the fast algorithm over a standard implementation.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="41"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">41</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">41</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Une adaptation des cartes auto-organisatrices pour des donn\'ees
  d\'ecrites par un tableau de dissimilarit\'es</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Many data analysis methods cannot be applied to data that are not represented
by a fixed number of real values, whereas most of real world observations are
not readily available in such a format. Vector based data analysis methods have
therefore to be adapted in order to be used with non standard complex data. A
flexible and general solution for this adaptation is to use a (dis)similarity
measure. Indeed, thanks to expert knowledge on the studied data, it is
generally possible to define a measure that can be used to make pairwise
comparison between observations. General data analysis methods are then
obtained by adapting existing methods to (dis)similarity matrices. In this
article, we propose an adaptation of Kohonen's Self Organizing Map (SOM) to
(dis)similarity data. The proposed algorithm is an adapted version of the
vector based batch SOM. The method is validated on real world data: we provide
an analysis of the usage patterns of the web site of the Institut National de
Recherche en Informatique et Automatique, constructed thanks to web log mining
method.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="42"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">42</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">42</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Self-organizing maps and symbolic data</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In data analysis new forms of complex data have to be considered like for
example (symbolic data, functional data, web data, trees, SQL query and
multimedia data, ...). In this context classical data analysis for knowledge
discovery based on calculating the center of gravity can not be used because
input are not $\mathbb{R}^p$ vectors. In this paper, we present an application
on real world symbolic data using the self-organizing map. To this end, we
propose an extension of the self-organizing map that can handle symbolic data.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="43"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">43</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">43</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Fast Selection of Spectral Variables with B-Spline Compression</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The large number of spectral variables in most data sets encountered in
spectral chemometrics often renders the prediction of a dependent variable
uneasy. The number of variables hopefully can be reduced, by using either
projection techniques or selection methods; the latter allow for the
interpretation of the selected variables. Since the optimal approach of testing
all possible subsets of variables with the prediction model is intractable, an
incremental selection approach using a nonparametric statistics is a good
option, as it avoids the computationally intensive use of the model itself. It
has two drawbacks however: the number of groups of variables to test is still
huge, and colinearities can make the results unstable. To overcome these
limitations, this paper presents a method to select groups of spectral
variables. It consists in a forward-backward procedure applied to the
coefficients of a B-Spline representation of the spectra. The criterion used in
the forward-backward procedure is the mutual information, allowing to find
nonlinear dependencies between variables, on the contrary of the generally used
correlation. The spline representation is used to get interpretability of the
results, as groups of consecutive spectral variables will be selected. The
experiments conducted on NIR spectra from fescue grass and diesel fuels show
that the method provides clearly identified groups of selected variables,
making interpretation easy, while keeping a low computational load. The
prediction performances obtained using the selected coefficients are higher
than those obtained by the same method applied directly to the original
variables and similar to those obtained using traditional models, although
using significantly less spectral variables.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="44"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">44</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">44</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Resampling methods for parameter-free and robust feature selection with
  mutual information</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Combining the mutual information criterion with a forward feature selection
strategy offers a good trade-off between optimality of the selected feature
subset and computation time. However, it requires to set the parameter(s) of
the mutual information estimator and to determine when to halt the forward
procedure. These two choices are difficult to make because, as the
dimensionality of the subset increases, the estimation of the mutual
information becomes less and less reliable. This paper proposes to use
resampling methods, a K-fold cross-validation and the permutation test, to
address both issues. The resampling methods bring information about the
variance of the estimator, information which can then be used to automatically
set the parameter and to calculate a threshold to stop the forward procedure.
The procedure is illustrated on a synthetic dataset as well as on real-world
examples.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="45"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">45</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">45</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Evolving Classifiers: Methods for Incremental Learning</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The ability of a classifier to take on new information and classes by
evolving the classifier without it having to be fully retrained is known as
incremental learning. Incremental learning has been successfully applied to
many classification problems, where the data is changing and is not all
available at once. In this paper there is a comparison between Learn++, which
is one of the most recent incremental learning algorithms, and the new proposed
method of Incremental Learning Using Genetic Algorithm (ILUGA). Learn++ has
shown good incremental learning capabilities on benchmark datasets on which the
new ILUGA method has been tested. ILUGA has also shown good incremental
learning ability using only a few classifiers and does not suffer from
catastrophic forgetting. The results obtained for ILUGA on the Optical
Character Recognition (OCR) and Wine datasets are good, with an overall
accuracy of 93% and 94% respectively showing a 4% improvement over Learn++.MT
for the difficult multi-class OCR dataset.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="46"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">46</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">46</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Classification of Images Using Support Vector Machines</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Support Vector Machines (SVMs) are a relatively new supervised classification
technique to the land cover mapping community. They have their roots in
Statistical Learning Theory and have gained prominence because they are robust,
accurate and are effective even when using a small training sample. By their
nature SVMs are essentially binary classifiers, however, they can be adopted to
handle the multiple classification tasks common in remote sensing studies. The
two approaches commonly used are the One-Against-One (1A1) and One-Against-All
(1AA) techniques. In this paper, these approaches are evaluated in as far as
their impact and implication for land cover mapping. The main finding from this
research is that whereas the 1AA technique is more predisposed to yielding
unclassified and mixed pixels, the resulting classification accuracy is not
significantly different from 1A1 approach. It is the authors conclusions that
ultimately the choice of technique adopted boils down to personal preference
and the uniqueness of the dataset at hand.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="47"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">47</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">47</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Prediction with expert advice for the Brier game</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We show that the Brier game of prediction is mixable and find the optimal
learning rate and substitution function for it. The resulting prediction
algorithm is applied to predict results of football and tennis matches. The
theoretical performance guarantee turns out to be rather tight on these data
sets, especially in the case of the more extensive tennis data.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="48"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">48</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">48</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Association Rules in the Relational Calculus</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  One of the most utilized data mining tasks is the search for association
rules. Association rules represent significant relationships between items in
transactions. We extend the concept of association rule to represent a much
broader class of associations, which we refer to as \emph{entity-relationship
rules.} Semantically, entity-relationship rules express associations between
properties of related objects. Syntactically, these rules are based on a broad
subclass of safe domain relational calculus queries. We propose a new
definition of support and confidence for entity-relationship rules and for the
frequency of entity-relationship queries. We prove that the definition of
frequency satisfies standard probability axioms and the Apriori property.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="49"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">49</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">49</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The structure of verbal sequences analyzed with unsupervised learning
  techniques</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Data mining allows the exploration of sequences of phenomena, whereas one
usually tends to focus on isolated phenomena or on the relation between two
phenomena. It offers invaluable tools for theoretical analyses and exploration
of the structure of sentences, texts, dialogues, and speech. We report here the
results of an attempt at using it for inspecting sequences of verbs from French
accounts of road accidents. This analysis comes from an original approach of
unsupervised training allowing the discovery of the structure of sequential
data. The entries of the analyzer were only made of the verbs appearing in the
sentences. It provided a classification of the links between two successive
verbs into four distinct clusters, allowing thus text segmentation. We give
here an interpretation of these clusters by applying a statistical analysis to
independent semantic annotations.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="50"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">50</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">50</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Consistency of trace norm minimization</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Regularization by the sum of singular values, also referred to as the trace
norm, is a popular technique for estimating low rank rectangular matrices. In
this paper, we extend some of the consistency results of the Lasso to provide
necessary and sufficient conditions for rank consistency of trace norm
minimization with the square loss. We also provide an adaptive version that is
rank consistent even when the necessary condition for the non adaptive version
is not fulfilled.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="51"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">51</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">51</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">An efficient reduction of ranking to classification</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This paper describes an efficient reduction of the learning problem of
ranking to binary classification. The reduction guarantees an average pairwise
misranking regret of at most that of the binary classifier regret, improving a
recent result of Balcan et al which only guarantees a factor of 2. Moreover,
our reduction applies to a broader class of ranking loss functions, admits a
simpler proof, and the expected running time complexity of our algorithm in
terms of number of calls to a classifier or preference function is improved
from $\Omega(n^2)$ to $O(n \log n)$. In addition, when the top $k$ ranked
elements only are required ($k \ll n$), as in many applications in information
extraction or search engines, the time complexity of our algorithm can be
further reduced to $O(k \log k + n)$. Our reduction and algorithm are thus
practical for realistic applications where the number of points to rank exceeds
several thousands. Much of our results also extend beyond the bipartite case
previously studied.
  Our rediction is a randomized one. To complement our result, we also derive
lower bounds on any deterministic reduction from binary (preference)
classification to ranking, implying that our use of a randomized reduction is
essentially necessary for the guarantees we provide.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="52"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">52</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">52</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Combining haplotypers</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Statistically resolving the underlying haplotype pair for a genotype
measurement is an important intermediate step in gene mapping studies, and has
received much attention recently. Consequently, a variety of methods for this
problem have been developed. Different methods employ different statistical
models, and thus implicitly encode different assumptions about the nature of
the underlying haplotype structure. Depending on the population sample in
question, their relative performance can vary greatly, and it is unclear which
method to choose for a particular sample. Instead of choosing a single method,
we explore combining predictions returned by different methods in a principled
way, and thereby circumvent the problem of method selection.
  We propose several techniques for combining haplotype reconstructions and
analyze their computational properties. In an experimental study on real-world
haplotype data we show that such techniques can provide more accurate and
robust reconstructions, and are useful for outlier detection. Typically, the
combined prediction is at least as accurate as or even more accurate than the
best individual method, effectively circumventing the method selection problem.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="53"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">53</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">53</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Tutorial on Spectral Clustering</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In recent years, spectral clustering has become one of the most popular
modern clustering algorithms. It is simple to implement, can be solved
efficiently by standard linear algebra software, and very often outperforms
traditional clustering algorithms such as the k-means algorithm. On the first
glance spectral clustering appears slightly mysterious, and it is not obvious
to see why it works at all and what it really does. The goal of this tutorial
is to give some intuition on those questions. We describe different graph
Laplacians and their basic properties, present the most common spectral
clustering algorithms, and derive those algorithms from scratch by several
different approaches. Advantages and disadvantages of the different spectral
clustering algorithms are discussed.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="54"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">54</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">54</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Building Rules on Top of Ontologies for the Semantic Web with Inductive
  Logic Programming</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Building rules on top of ontologies is the ultimate goal of the logical layer
of the Semantic Web. To this aim an ad-hoc mark-up language for this layer is
currently under discussion. It is intended to follow the tradition of hybrid
knowledge representation and reasoning systems such as $\mathcal{AL}$-log that
integrates the description logic $\mathcal{ALC}$ and the function-free Horn
clausal language \textsc{Datalog}. In this paper we consider the problem of
automating the acquisition of these rules for the Semantic Web. We propose a
general framework for rule induction that adopts the methodological apparatus
of Inductive Logic Programming and relies on the expressive and deductive power
of $\mathcal{AL}$-log. The framework is valid whatever the scope of induction
(description vs. prediction) is. Yet, for illustrative purposes, we also
discuss an instantiation of the framework which aims at description and turns
out to be useful in Ontology Refinement.
  Keywords: Inductive Logic Programming, Hybrid Knowledge Representation and
Reasoning Systems, Ontologies, Semantic Web.
  Note: To appear in Theory and Practice of Logic Programming (TPLP)
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="55"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">55</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">55</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Empirical Evaluation of Four Tensor Decomposition Algorithms</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Higher-order tensor decompositions are analogous to the familiar Singular
Value Decomposition (SVD), but they transcend the limitations of matrices
(second-order tensors). SVD is a powerful tool that has achieved impressive
results in information retrieval, collaborative filtering, computational
linguistics, computational vision, and other fields. However, SVD is limited to
two-dimensional arrays of data (two modes), and many potential applications
have three or more modes, which require higher-order tensor decompositions.
This paper evaluates four algorithms for higher-order tensor decomposition:
Higher-Order Singular Value Decomposition (HO-SVD), Higher-Order Orthogonal
Iteration (HOOI), Slice Projection (SP), and Multislice Projection (MP). We
measure the time (elapsed run time), space (RAM and disk space requirements),
and fit (tensor reconstruction accuracy) of the four algorithms, under a
variety of conditions. We find that standard implementations of HO-SVD and HOOI
do not scale up to larger tensors, due to increasing RAM requirements. We
recommend HOOI for tensors that are small enough for the available RAM and MP
for larger tensors.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="56"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">56</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">56</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Inverse Sampling for Nonasymptotic Sequential Estimation of Bounded
  Variable Means</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper, we consider the nonasymptotic sequential estimation of means
of random variables bounded in between zero and one. We have rigorously
demonstrated that, in order to guarantee prescribed relative precision and
confidence level, it suffices to continue sampling until the sample sum is no
less than a certain bound and then take the average of samples as an estimate
for the mean of the bounded random variable. We have developed an explicit
formula and a bisection search method for the determination of such bound of
sample sum, without any knowledge of the bounded variable. Moreover, we have
derived bounds for the distribution of sample size. In the special case of
Bernoulli random variables, we have established analytical and numerical
methods to further reduce the bound of sample sum and thus improve the
efficiency of sampling. Furthermore, the fallacy of existing results are
detected and analyzed.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="57"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">57</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">57</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Image Classification Using SVMs: One-against-One Vs One-against-All</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Support Vector Machines (SVMs) are a relatively new supervised classification
technique to the land cover mapping community. They have their roots in
Statistical Learning Theory and have gained prominence because they are robust,
accurate and are effective even when using a small training sample. By their
nature SVMs are essentially binary classifiers, however, they can be adopted to
handle the multiple classification tasks common in remote sensing studies. The
two approaches commonly used are the One-Against-One (1A1) and One-Against-All
(1AA) techniques. In this paper, these approaches are evaluated in as far as
their impact and implication for land cover mapping. The main finding from this
research is that whereas the 1AA technique is more predisposed to yielding
unclassified and mixed pixels, the resulting classification accuracy is not
significantly different from 1A1 approach. It is the authors conclusion
therefore that ultimately the choice of technique adopted boils down to
personal preference and the uniqueness of the dataset at hand.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="58"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">58</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">58</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Clustering with Transitive Distance and K-Means Duality</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Recent spectral clustering methods are a propular and powerful technique for
data clustering. These methods need to solve the eigenproblem whose
computational complexity is $O(n^3)$, where $n$ is the number of data samples.
In this paper, a non-eigenproblem based clustering method is proposed to deal
with the clustering problem. Its performance is comparable to the spectral
clustering algorithms but it is more efficient with computational complexity
$O(n^2)$. We show that with a transitive distance and an observed property,
called K-means duality, our algorithm can be used to handle data sets with
complex cluster shapes, multi-scale clusters, and noise. Moreover, no
parameters except the number of clusters need to be set in our algorithm.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="59"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">59</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">59</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Derivations of Normalized Mutual Information in Binary Classifications</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This correspondence studies the basic problem of classifications - how to
evaluate different classifiers. Although the conventional performance indexes,
such as accuracy, are commonly used in classifier selection or evaluation,
information-based criteria, such as mutual information, are becoming popular in
feature/model selections. In this work, we propose to assess classifiers in
terms of normalized mutual information (NI), which is novel and well defined in
a compact range for classifier evaluation. We derive close-form relations of
normalized mutual information with respect to accuracy, precision, and recall
in binary classifications. By exploring the relations among them, we reveal
that NI is actually a set of nonlinear functions, with a concordant
power-exponent form, to each performance index. The relations can also be
expressed with respect to precision and recall, or to false alarm and hitting
rate (recall).
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="60"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">60</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">60</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Covariance and PCA for Categorical Variables</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Covariances from categorical variables are defined using a regular simplex
expression for categories. The method follows the variance definition by Gini,
and it gives the covariance as a solution of simultaneous equations. The
calculated results give reasonable values for test data. A method of principal
component analysis (RS-PCA) is also proposed using regular simplex expressions,
which allows easy interpretation of the principal components. The proposed
methods apply to variable selection problem of categorical data USCensus1990
data. The proposed methods give appropriate criterion for the variable
selection problem of categorical
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="61"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">61</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">61</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">On the Relationship between the Posterior and Optimal Similarity</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  For a classification problem described by the joint density $P(\omega,x)$,
models of $P(\omega\eq\omega'|x,x')$ (the ``Bayesian similarity measure'') have
been shown to be an optimal similarity measure for nearest neighbor
classification. This paper analyzes demonstrates several additional properties
of that conditional distribution. The paper first shows that we can
reconstruct, up to class labels, the class posterior distribution $P(\omega|x)$
given $P(\omega\eq\omega'|x,x')$, gives a procedure for recovering the class
labels, and gives an asymptotically Bayes-optimal classification procedure. It
also shows, given such an optimal similarity measure, how to construct a
classifier that outperforms the nearest neighbor classifier and achieves
Bayes-optimal classification rates. The paper then analyzes Bayesian similarity
in a framework where a classifier faces a number of related classification
tasks (multitask learning) and illustrates that reconstruction of the class
posterior distribution is not possible in general. Finally, the paper
identifies a distinct class of classification problems using
$P(\omega\eq\omega'|x,x')$ and shows that using $P(\omega\eq\omega'|x,x')$ to
solve those problems is the Bayes optimal solution.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="62"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">62</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">62</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Reactive Tabu Search Algorithm for Stimuli Generation in
  Psycholinguistics</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The generation of meaningless "words" matching certain statistical and/or
linguistic criteria is frequently needed for experimental purposes in
Psycholinguistics. Such stimuli receive the name of pseudowords or nonwords in
the Cognitive Neuroscience literatue. The process for building nonwords
sometimes has to be based on linguistic units such as syllables or morphemes,
resulting in a numerical explosion of combinations when the size of the
nonwords is increased. In this paper, a reactive tabu search scheme is proposed
to generate nonwords of variables size. The approach builds pseudowords by
using a modified Metaheuristic algorithm based on a local search procedure
enhanced by a feedback-based scheme. Experimental results show that the new
algorithm is a practical and effective tool for nonword generation.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="63"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">63</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">63</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Equations of States in Singular Statistical Estimation</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Learning machines which have hierarchical structures or hidden variables are
singular statistical models because they are nonidentifiable and their Fisher
information matrices are singular. In singular statistical models, neither the
Bayes a posteriori distribution converges to the normal distribution nor the
maximum likelihood estimator satisfies asymptotic normality. This is the main
reason why it has been difficult to predict their generalization performances
from trained states. In this paper, we study four errors, (1) Bayes
generalization error, (2) Bayes training error, (3) Gibbs generalization error,
and (4) Gibbs training error, and prove that there are mathematical relations
among these errors. The formulas proved in this paper are equations of states
in statistical estimation because they hold for any true distribution, any
parametric model, and any a priori distribution. Also we show that Bayes and
Gibbs generalization errors are estimated by Bayes and Gibbs training errors,
and propose widely applicable information criteria which can be applied to both
regular and singular statistical models.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="64"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">64</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">64</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Universal Kernel for Learning Regular Languages</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We give a universal kernel that renders all the regular languages linearly
separable. We are not able to compute this kernel efficiently and conjecture
that it is intractable, but we do have an efficient $\eps$-approximation.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="65"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">65</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">65</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Automatic Pattern Classification by Unsupervised Learning Using
  Dimensionality Reduction of Data with Mirroring Neural Networks</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This paper proposes an unsupervised learning technique by using Multi-layer
Mirroring Neural Network and Forgy's clustering algorithm. Multi-layer
Mirroring Neural Network is a neural network that can be trained with
generalized data inputs (different categories of image patterns) to perform
non-linear dimensionality reduction and the resultant low-dimensional code is
used for unsupervised pattern classification using Forgy's algorithm. By
adapting the non-linear activation function (modified sigmoidal function) and
initializing the weights and bias terms to small random values, mirroring of
the input pattern is initiated. In training, the weights and bias terms are
changed in such a way that the input presented is reproduced at the output by
back propagating the error. The mirroring neural network is capable of reducing
the input vector to a great degree (approximately 1/30th the original size) and
also able to reconstruct the input pattern at the output layer from this
reduced code units. The feature set (output of central hidden layer) extracted
from this network is fed to Forgy's algorithm, which classify input data
patterns into distinguishable classes. In the implementation of Forgy's
algorithm, initial seed points are selected in such a way that they are distant
enough to be perfectly grouped into different categories. Thus a new method of
unsupervised learning is formulated and demonstrated in this paper. This method
gave impressive results when applied to classification of different image
patterns.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="66"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">66</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">66</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Reconstruction of Markov Random Fields from Samples: Some Easy
  Observations and Algorithms</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Markov random fields are used to model high dimensional distributions in a
number of applied areas. Much recent interest has been devoted to the
reconstruction of the dependency structure from independent samples from the
Markov random fields. We analyze a simple algorithm for reconstructing the
underlying graph defining a Markov random field on $n$ nodes and maximum degree
$d$ given observations. We show that under mild non-degeneracy conditions it
reconstructs the generating graph with high probability using $\Theta(d
\epsilon^{-2}\delta^{-4} \log n)$ samples where $\epsilon,\delta$ depend on the
local interactions. For most local interaction $\eps,\delta$ are of order
$\exp(-O(d))$.
  Our results are optimal as a function of $n$ up to a multiplicative constant
depending on $d$ and the strength of the local interactions. Our results seem
to be the first results for general models that guarantee that {\em the}
generating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2}
\epsilon^{-2}\delta^{-4} \log n)$ running time bound. In cases where the
measure on the graph has correlation decay, the running time is $O(n^2 \log n)$
for all fixed $d$. We also discuss the effect of observing noisy samples and
show that as long as the noise level is low, our algorithm is effective. On the
other hand, we construct an example where large noise implies
non-identifiability even for generic noise and interactions. Finally, we
briefly show that in some simple cases, models with hidden nodes can also be
recovered.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="67"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">67</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">67</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A New Theoretic Foundation for Cross-Layer Optimization</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Cross-layer optimization solutions have been proposed in recent years to
improve the performance of network users operating in a time-varying,
error-prone wireless environment. However, these solutions often rely on ad-hoc
optimization approaches, which ignore the different environmental dynamics
experienced at various layers by a user and violate the layered network
architecture of the protocol stack by requiring layers to provide access to
their internal protocol parameters to other layers. This paper presents a new
theoretic foundation for cross-layer optimization, which allows each layer to
make autonomous decisions individually, while maximizing the utility of the
wireless user by optimally determining what information needs to be exchanged
among layers. Hence, this cross-layer framework does not change the current
layered architecture. Specifically, because the wireless user interacts with
the environment at various layers of the protocol stack, the cross-layer
optimization problem is formulated as a layered Markov decision process (MDP)
in which each layer adapts its own protocol parameters and exchanges
information (messages) with other layers in order to cooperatively maximize the
performance of the wireless user. The message exchange mechanism for
determining the optimal cross-layer transmission strategies has been designed
for both off-line optimization and on-line dynamic adaptation. We also show
that many existing cross-layer optimization algorithms can be formulated as
simplified, sub-optimal, versions of our layered MDP framework.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="68"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">68</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">68</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Density estimation in linear time</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We consider the problem of choosing a density estimate from a set of
distributions F, minimizing the L1-distance to an unknown distribution
(Devroye, Lugosi 2001). Devroye and Lugosi analyze two algorithms for the
problem: Scheffe tournament winner and minimum distance estimate. The Scheffe
tournament estimate requires fewer computations than the minimum distance
estimate, but has strictly weaker guarantees than the latter.
  We focus on the computational aspect of density estimation. We present two
algorithms, both with the same guarantee as the minimum distance estimate. The
first one, a modification of the minimum distance estimate, uses the same
number (quadratic in |F|) of computations as the Scheffe tournament. The second
one, called ``efficient minimum loss-weight estimate,'' uses only a linear
number of computations, assuming that F is preprocessed.
  We also give examples showing that the guarantees of the algorithms cannot be
improved and explore randomized algorithms for density estimation.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="69"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">69</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">69</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Graph kernels between point clouds</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Point clouds are sets of points in two or three dimensions. Most kernel
methods for learning on sets of points have not yet dealt with the specific
geometrical invariances and practical constraints associated with point clouds
in computer vision and graphics. In this paper, we present extensions of graph
kernels for point clouds, which allow to use kernel methods for such ob jects
as shapes, line drawings, or any three-dimensional point clouds. In order to
design rich and numerically efficient kernels with as few free parameters as
possible, we use kernels between covariance matrices and their factorizations
on graphical models. We derive polynomial time dynamic programming recursions
and present applications to recognition of handwritten digits and Chinese
characters from few training examples.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="70"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">70</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">70</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Improving the Performance of PieceWise Linear Separation Incremental
  Algorithms for Practical Hardware Implementations</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper we shall review the common problems associated with Piecewise
Linear Separation incremental algorithms. This kind of neural models yield poor
performances when dealing with some classification problems, due to the
evolving schemes used to construct the resulting networks. So as to avoid this
undesirable behavior we shall propose a modification criterion. It is based
upon the definition of a function which will provide information about the
quality of the network growth process during the learning phase. This function
is evaluated periodically as the network structure evolves, and will permit, as
we shall show through exhaustive benchmarks, to considerably improve the
performance(measured in terms of network complexity and generalization
capabilities) offered by the networks generated by these incremental models.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="71"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">71</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">71</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Improved Collaborative Filtering Algorithm via Information
  Transformation</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper, we propose a spreading activation approach for collaborative
filtering (SA-CF). By using the opinion spreading process, the similarity
between any users can be obtained. The algorithm has remarkably higher accuracy
than the standard collaborative filtering (CF) using Pearson correlation.
Furthermore, we introduce a free parameter $\beta$ to regulate the
contributions of objects to user-user correlations. The numerical results
indicate that decreasing the influence of popular objects can further improve
the algorithmic accuracy and personality. We argue that a better algorithm
should simultaneously require less computation and generate higher accuracy.
Accordingly, we further propose an algorithm involving only the top-$N$ similar
neighbors for each target user, which has both less computational complexity
and higher algorithmic accuracy.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="72"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">72</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">72</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Online EM Algorithm for Latent Data Models</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this contribution, we propose a generic online (also sometimes called
adaptive or recursive) version of the Expectation-Maximisation (EM) algorithm
applicable to latent variable models of independent observations. Compared to
the algorithm of Titterington (1984), this approach is more directly connected
to the usual EM algorithm and does not rely on integration with respect to the
complete data distribution. The resulting algorithm is usually simpler and is
shown to achieve convergence to the stationary points of the Kullback-Leibler
divergence between the marginal distribution of the observation and the model
distribution at the optimal rate, i.e., that of the maximum likelihood
estimator. In addition, the proposed approach is also suitable for conditional
(or regression) models, as illustrated in the case of the mixture of linear
regressions model.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="73"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">73</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">73</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Staring at Economic Aggregators through Information Lenses</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  It is hard to exaggerate the role of economic aggregators -- functions that
summarize numerous and / or heterogeneous data -- in economic models since the
early XX$^{th}$ century. In many cases, as witnessed by the pioneering works of
Cobb and Douglas, these functions were information quantities tailored to
economic theories, i.e. they were built to fit economic phenomena. In this
paper, we look at these functions from the complementary side: information. We
use a recent toolbox built on top of a vast class of distortions coined by
Bregman, whose application field rivals metrics' in various subfields of
mathematics. This toolbox makes it possible to find the quality of an
aggregator (for consumptions, prices, labor, capital, wages, etc.), from the
standpoint of the information it carries. We prove a rather striking result.
  From the informational standpoint, well-known economic aggregators do belong
to the \textit{optimal} set. As common economic assumptions enter the analysis,
this large set shrinks, and it essentially ends up \textit{exactly fitting}
either CES, or Cobb-Douglas, or both. To summarize, in the relevant economic
contexts, one could not have crafted better some aggregator from the
information standpoint. We also discuss global economic behaviors of optimal
information aggregators in general, and present a brief panorama of the links
between economic and information aggregators.
  Keywords: Economic Aggregators, CES, Cobb-Douglas, Bregman divergences
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="74"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">74</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">74</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Online variants of the cross-entropy method</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The cross-entropy method is a simple but efficient method for global
optimization. In this paper we provide two online variants of the basic CEM,
together with a proof of convergence.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="75"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">75</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">75</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Factored Value Iteration Converges</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this paper we propose a novel algorithm, factored value iteration (FVI),
for the approximate solution of factored Markov decision processes (fMDPs). The
traditional approximate value iteration algorithm is modified in two ways. For
one, the least-squares projection operator is modified so that it does not
increase max-norm, and thus preserves convergence. The other modification is
that we uniformly sample polynomially many samples from the (exponentially
large) state space. This way, the complexity of our algorithm becomes
polynomial in the size of the fMDP description length. We prove that the
algorithm is convergent. We also derive an upper bound on the difference
between our approximate solution and the optimal one, and also on the error
introduced by sampling. We analyze various projection operators with respect to
their computation complexity and their convergence when combined with
approximate value iteration.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="76"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">76</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">76</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">The optimal assignment kernel is not positive definite</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We prove that the optimal assignment kernel, proposed recently as an attempt
to embed labeled graphs and more generally tuples of basic data to a Hilbert
space, is in fact not always positive definite.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="77"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">77</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">77</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Information Width</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Kolmogorov argued that the concept of information exists also in problems
with no underlying stochastic model (as Shannon's information representation)
for instance, the information contained in an algorithm or in the genome. He
introduced a combinatorial notion of entropy and information $I(x:\sy)$
conveyed by a binary string $x$ about the unknown value of a variable $\sy$.
The current paper poses the following questions: what is the relationship
between the information conveyed by $x$ about $\sy$ to the description
complexity of $x$ ? is there a notion of cost of information ? are there limits
on how efficient $x$ conveys information ?
  To answer these questions Kolmogorov's definition is extended and a new
concept termed {\em information width} which is similar to $n$-widths in
approximation theory is introduced. Information of any input source, e.g.,
sample-based, general side-information or a hybrid of both can be evaluated by
a single common formula. An application to the space of binary functions is
considered.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="78"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">78</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">78</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">On the Complexity of Binary Samples</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Consider a class $\mH$ of binary functions $h: X\to\{-1, +1\}$ on a finite
interval $X=[0, B]\subset \Real$. Define the {\em sample width} of $h$ on a
finite subset (a sample) $S\subset X$ as $\w_S(h) \equiv \min_{x\in S}
|\w_h(x)|$, where $\w_h(x) = h(x) \max\{a\geq 0: h(z)=h(x), x-a\leq z\leq
x+a\}$. Let $\mathbb{S}_\ell$ be the space of all samples in $X$ of cardinality
$\ell$ and consider sets of wide samples, i.e., {\em hypersets} which are
defined as $A_{\beta, h} = \{S\in \mathbb{S}_\ell: \w_{S}(h) \geq \beta\}$.
Through an application of the Sauer-Shelah result on the density of sets an
upper estimate is obtained on the growth function (or trace) of the class
$\{A_{\beta, h}: h\in\mH\}$, $\beta>0$, i.e., on the number of possible
dichotomies obtained by intersecting all hypersets with a fixed collection of
samples $S\in\mathbb{S}_\ell$ of cardinality $m$. The estimate is
$2\sum_{i=0}^{2\lfloor B/(2\beta)\rfloor}{m-\ell\choose i}$.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="79"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">79</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">79</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">New Estimation Procedures for PLS Path Modelling</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Given R groups of numerical variables X1, ... XR, we assume that each group
is the result of one underlying latent variable, and that all latent variables
are bound together through a linear equation system. Moreover, we assume that
some explanatory latent variables may interact pairwise in one or more
equations. We basically consider PLS Path Modelling's algorithm to estimate
both latent variables and the model's coefficients. New "external" estimation
schemes are proposed that draw latent variables towards strong group structures
in a more flexible way. New "internal" estimation schemes are proposed to
enable PLSPM to make good use of variable group complementarity and to deal
with interactions. Application examples are given.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="80"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">80</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">80</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Learning Balanced Mixtures of Discrete Distributions with Small Sample</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We study the problem of partitioning a small sample of $n$ individuals from a
mixture of $k$ product distributions over a Boolean cube $\{0, 1\}^K$ according
to their distributions. Each distribution is described by a vector of allele
frequencies in $\R^K$. Given two distributions, we use $\gamma$ to denote the
average $\ell_2^2$ distance in frequencies across $K$ dimensions, which
measures the statistical divergence between them. We study the case assuming
that bits are independently distributed across $K$ dimensions. This work
demonstrates that, for a balanced input instance for $k = 2$, a certain
graph-based optimization function returns the correct partition with high
probability, where a weighted graph $G$ is formed over $n$ individuals, whose
pairwise hamming distances between their corresponding bit vectors define the
edge weights, so long as $K = \Omega(\ln n/\gamma)$ and $Kn = \tilde\Omega(\ln
n/\gamma^2)$. The function computes a maximum-weight balanced cut of $G$, where
the weight of a cut is the sum of the weights across all edges in the cut. This
result demonstrates a nice property in the high-dimensional feature space: one
can trade off the number of features that are required with the size of the
sample to accomplish certain tasks like clustering.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="81"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">81</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">81</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Bayesian Nonlinear Principal Component Analysis Using Random Fields</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We propose a novel model for nonlinear dimension reduction motivated by the
probabilistic formulation of principal component analysis. Nonlinearity is
achieved by specifying different transformation matrices at different locations
of the latent space and smoothing the transformation using a Markov random
field type prior. The computation is made feasible by the recent advances in
sampling from von Mises-Fisher distributions.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="82"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">82</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">82</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A New Approach to Collaborative Filtering: Operator Estimation with
  Spectral Regularization</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We present a general approach for collaborative filtering (CF) using spectral
regularization to learn linear operators from "users" to the "objects" they
rate. Recent low-rank type matrix completion approaches to CF are shown to be
special cases. However, unlike existing regularization based CF methods, our
approach can be used to also incorporate information such as attributes of the
users or the objects -- a limitation of existing regularization based CF
methods. We then provide novel representer theorems that we use to develop new
estimation methods. We provide learning algorithms based on low-rank
decompositions, and test them on a standard CF dataset. The experiments
indicate the advantages of generalizing the existing regularization based CF
methods to incorporate related information about users and objects. Finally, we
show that certain multi-task learning methods can be also seen as special cases
of our proposed approach.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="83"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">83</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">83</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Combining Expert Advice Efficiently</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We show how models for prediction with expert advice can be defined concisely
and clearly using hidden Markov models (HMMs); standard HMM algorithms can then
be used to efficiently calculate, among other things, how the expert
predictions should be weighted according to the model. We cast many existing
models as HMMs and recover the best known running times in each case. We also
describe two new models: the switch distribution, which was recently developed
to improve Bayesian/Minimum Description Length model selection, and a new
generalisation of the fixed share algorithm based on run-length coding. We give
loss bounds for all models and shed new light on their relationships.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="84"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">84</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">84</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Radar-Shaped Statistic for Testing and Visualizing Uniformity
  Properties in Computer Experiments</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In the study of computer codes, filling space as uniformly as possible is
important to describe the complexity of the investigated phenomenon. However,
this property is not conserved by reducing the dimension. Some numeric
experiment designs are conceived in this sense as Latin hypercubes or
orthogonal arrays, but they consider only the projections onto the axes or the
coordinate planes. In this article we introduce a statistic which allows
studying the good distribution of points according to all 1-dimensional
projections. By angularly scanning the domain, we obtain a radar type
representation, allowing the uniformity defects of a design to be identified
with respect to its projections onto straight lines. The advantages of this new
tool are demonstrated on usual examples of space-filling designs (SFD) and a
global statistic independent of the angle of rotation is studied.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="85"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">85</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">85</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Compressed Counting</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Counting is among the most fundamental operations in computing. For example,
counting the pth frequency moment has been a very active area of research, in
theoretical computer science, databases, and data mining. When p=1, the task
(i.e., counting the sum) can be accomplished using a simple counter.
  Compressed Counting (CC) is proposed for efficiently computing the pth
frequency moment of a data stream signal A_t, where 0&lt;p&lt;=2. CC is applicable if
the streaming data follow the Turnstile model, with the restriction that at the
time t for the evaluation, A_t[i]>= 0, which includes the strict Turnstile
model as a special case. For natural data streams encountered in practice, this
restriction is minor.
  The underly technique for CC is what we call skewed stable random
projections, which captures the intuition that, when p=1 a simple counter
suffices, and when p = 1+/\Delta with small \Delta, the sample complexity of a
counter system should be low (continuously as a function of \Delta). We show at
small \Delta the sample complexity (number of projections) k = O(1/\epsilon)
instead of O(1/\epsilon^2).
  Compressed Counting can serve a basic building block for other tasks in
statistics and computing, for example, estimation entropies of data streams,
parameter estimations using the method of moments and maximum likelihood.
  Finally, another contribution is an algorithm for approximating the
logarithmic norm, \sum_{i=1}^D\log A_t[i], and logarithmic distance. The
logarithmic distance is useful in machine learning practice with heavy-tailed
data.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="86"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">86</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">86</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Sign Language Tutoring Tool</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  In this project, we have developed a sign language tutor that lets users
learn isolated signs by watching recorded videos and by trying the same signs.
The system records the user's video and analyses it. If the sign is recognized,
both verbal and animated feedback is given to the user. The system is able to
recognize complex signs that involve both hand gestures and head movements and
expressions. Our performance tests yield a 99% recognition rate on signs
involving only manual gestures and 85% recognition rate on signs that involve
both manual and non manual components, such as head movement and facial
expressions.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="87"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">87</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">87</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Pure Exploration for Multi-Armed Bandit Problems</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We consider the framework of stochastic multi-armed bandit problems and study
the possibilities and limitations of forecasters that perform an on-line
exploration of the arms. These forecasters are assessed in terms of their
simple regret, a regret notion that captures the fact that exploration is only
constrained by the number of available rounds (not necessarily known in
advance), in contrast to the case when the cumulative regret is considered and
when exploitation needs to be performed at the same time. We believe that this
performance criterion is suited to situations when the cost of pulling an arm
is expressed in terms of resources rather than rewards. We discuss the links
between the simple and the cumulative regret. One of the main results in the
case of a finite number of arms is a general lower bound on the simple regret
of a forecaster in terms of its cumulative regret: the smaller the latter, the
larger the former. Keeping this result in mind, we then exhibit upper bounds on
the simple regret of some forecasters. The paper ends with a study devoted to
continuous-armed bandit problems; we show that the simple regret can be
minimized with respect to a family of probability distributions if and only if
the cumulative regret can be minimized for it. Based on this equivalence, we
are able to prove that the separable metric spaces are exactly the metric
spaces on which these regrets can be minimized with respect to the family of
all probability distributions with continuous mean-payoff functions.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="88"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">88</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">88</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Knowledge Technologies</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Several technologies are emerging that provide new ways to capture, store,
present and use knowledge. This book is the first to provide a comprehensive
introduction to five of the most important of these technologies: Knowledge
Engineering, Knowledge Based Engineering, Knowledge Webs, Ontologies and
Semantic Webs. For each of these, answers are given to a number of key
questions (What is it? How does it operate? How is a system developed? What can
it be used for? What tools are available? What are the main issues?). The book
is aimed at students, researchers and practitioners interested in Knowledge
Management, Artificial Intelligence, Design Engineering and Web Technologies.
  During the 1990s, Nick worked at the University of Nottingham on the
application of AI techniques to knowledge management and on various knowledge
acquisition projects to develop expert systems for military applications. In
1999, he joined Epistemics where he worked on numerous knowledge projects and
helped establish knowledge management programmes at large organisations in the
engineering, technology and legal sectors. He is author of the book "Knowledge
Acquisition in Practice", which describes a step-by-step procedure for
acquiring and implementing expertise. He maintains strong links with leading
research organisations working on knowledge technologies, such as
knowledge-based engineering, ontologies and semantic technologies.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="89"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">89</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">89</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">What Can We Learn Privately?</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Learning problems form an important category of computational tasks that
generalizes many of the computations researchers apply to large real-life data
sets. We ask: what concept classes can be learned privately, namely, by an
algorithm whose output does not depend too heavily on any one input or specific
training example? More precisely, we investigate learning algorithms that
satisfy differential privacy, a notion that provides strong confidentiality
guarantees in contexts where aggregate information is released about a database
containing sensitive information about individuals. We demonstrate that,
ignoring computational constraints, it is possible to privately agnostically
learn any concept class using a sample size approximately logarithmic in the
cardinality of the concept class. Therefore, almost anything learnable is
learnable privately: specifically, if a concept class is learnable by a
(non-private) algorithm with polynomial sample complexity and output size, then
it can be learned privately using a polynomial number of samples. We also
present a computationally efficient private PAC learner for the class of parity
functions. Local (or randomized response) algorithms are a practical class of
private algorithms that have received extensive investigation. We provide a
precise characterization of local private learning algorithms. We show that a
concept class is learnable by a local algorithm if and only if it is learnable
in the statistical query (SQ) model. Finally, we present a separation between
the power of interactive and noninteractive local learning algorithms.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="90"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">90</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">90</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Privacy Preserving ID3 over Horizontally, Vertically and Grid
  Partitioned Data</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We consider privacy preserving decision tree induction via ID3 in the case
where the training data is horizontally or vertically distributed. Furthermore,
we consider the same problem in the case where the data is both horizontally
and vertically distributed, a situation we refer to as grid partitioned data.
We give an algorithm for privacy preserving ID3 over horizontally partitioned
data involving more than two parties. For grid partitioned data, we discuss two
different evaluation methods for preserving privacy ID3, namely, first merging
horizontally and developing vertically or first merging vertically and next
developing horizontally. Next to introducing privacy preserving data mining
over grid-partitioned data, the main contribution of this paper is that we
show, by means of a complexity analysis that the former evaluation method is
the more efficient.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="91"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">91</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">91</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Figuring out Actors in Text Streams: Using Collocations to establish
  Incremental Mind-maps</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  The recognition, involvement, and description of main actors influences the
story line of the whole text. This is of higher importance as the text per se
represents a flow of words and expressions that once it is read it is lost. In
this respect, the understanding of a text and moreover on how the actor exactly
behaves is not only a major concern: as human beings try to store a given input
on short-term memory while associating diverse aspects and actors with
incidents, the following approach represents a virtual architecture, where
collocations are concerned and taken as the associative completion of the
actors' acting. Once that collocations are discovered, they become managed in
separated memory blocks broken down by the actors. As for human beings, the
memory blocks refer to associative mind-maps. We then present several priority
functions to represent the actual temporal situation inside a mind-map to
enable the user to reconstruct the recent events from the discovered temporal
results.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="92"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">92</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">92</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Robustness and Regularization of Support Vector Machines</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We consider regularized support vector machines (SVMs) and show that they are
precisely equivalent to a new robust optimization formulation. We show that
this equivalence of robust optimization and regularization has implications for
both algorithms, and analysis. In terms of algorithms, the equivalence suggests
more general SVM-like algorithms for classification that explicitly build in
protection to noise, and at the same time control overfitting. On the analysis
front, the equivalence of robustness and regularization, provides a robust
optimization interpretation for the success of regularized SVMs. We use the
this new robustness interpretation of SVMs to give a new proof of consistency
of (kernelized) SVMs, thus establishing robustness as the reason regularized
SVMs generalize well.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="93"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">93</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">93</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Recorded Step Directional Mutation for Faster Convergence</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  Two meta-evolutionary optimization strategies described in this paper
accelerate the convergence of evolutionary programming algorithms while still
retaining much of their ability to deal with multi-modal problems. The
strategies, called directional mutation and recorded step in this paper, can
operate independently but together they greatly enhance the ability of
evolutionary programming algorithms to deal with fitness landscapes
characterized by long narrow valleys. The directional mutation aspect of this
combined method uses correlated meta-mutation but does not introduce a full
covariance matrix. These new methods are thus much more economical in terms of
storage for problems with high dimensionality. Additionally, directional
mutation is rotationally invariant which is a substantial advantage over
self-adaptive methods which use a single variance per coordinate for problems
where the natural orientation of the problem is not oriented along the axes.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="94"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">94</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">94</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Support Vector Machine Classification with Indefinite Kernels</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We propose a method for support vector machine classification using
indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex
loss function, our algorithm simultaneously computes support vectors and a
proxy kernel matrix used in forming the loss. This can be interpreted as a
penalized kernel learning problem where indefinite kernel matrices are treated
as a noisy observations of a true Mercer kernel. Our formulation keeps the
problem convex and relatively large problems can be solved efficiently using
the projected gradient or analytic center cutting plane methods. We compare the
performance of our technique with other methods on several classic data sets.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="95"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">95</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">95</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A Unified Semi-Supervised Dimensionality Reduction Framework for
  Manifold Learning</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We present a general framework of semi-supervised dimensionality reduction
for manifold learning which naturally generalizes existing supervised and
unsupervised learning frameworks which apply the spectral decomposition.
Algorithms derived under our framework are able to employ both labeled and
unlabeled examples and are able to handle complex problems where data form
separate clusters of manifolds. Our framework offers simple views, explains
relationships among existing frameworks and provides further extensions which
can improve existing algorithms. Furthermore, a new semi-supervised
kernelization framework called ``KPCA trick'' is proposed to handle non-linear
problems.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="96"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">96</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">96</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Bolasso: model consistent Lasso estimation through the bootstrap</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We consider the least-square linear regression problem with regularization by
the l1-norm, a problem usually referred to as the Lasso. In this paper, we
present a detailed asymptotic analysis of model consistency of the Lasso. For
various decays of the regularization parameter, we compute asymptotic
equivalents of the probability of correct model selection (i.e., variable
selection). For a specific rate decay, we show that the Lasso selects all the
variables that should enter the model with probability tending to one
exponentially fast, while it selects all other variables with strictly positive
probability. We show that this property implies that if we run the Lasso for
several bootstrapped replications of a given sample, then intersecting the
supports of the Lasso bootstrap estimates leads to consistent model selection.
This novel variable selection algorithm, referred to as the Bolasso, is
compared favorably to other linear regression methods on synthetic data and
datasets from the UCI machine learning repository.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="97"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">97</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">97</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">On Kernelization of Supervised Mahalanobis Distance Learners</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  This paper focuses on the problem of kernelizing an existing supervised
Mahalanobis distance learner. The following features are included in the paper.
Firstly, three popular learners, namely, "neighborhood component analysis",
"large margin nearest neighbors" and "discriminant neighborhood embedding",
which do not have kernel versions are kernelized in order to improve their
classification performances. Secondly, an alternative kernelization framework
called "KPCA trick" is presented. Implementing a learner in the new framework
gains several advantages over the standard framework, e.g. no mathematical
formulas and no reprogramming are required for a kernel implementation, the
framework avoids troublesome problems such as singularity, etc. Thirdly, while
the truths of representer theorems are just assumptions in previous papers
related to ours, here, representer theorems are formally proven. The proofs
validate both the kernel trick and the KPCA trick in the context of Mahalanobis
distance learning. Fourthly, unlike previous works which always apply brute
force methods to select a kernel, we investigate two approaches which can be
efficiently adopted to construct an appropriate kernel for a given dataset.
Finally, numerical results on various real-world datasets are presented.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="98"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">98</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">98</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Isotropic PCA and Affine-Invariant Clustering</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We present a new algorithm for clustering points in R^n. The key property of
the algorithm is that it is affine-invariant, i.e., it produces the same
partition for any affine transformation of the input. It has strong guarantees
when the input is drawn from a mixture model. For a mixture of two arbitrary
Gaussians, the algorithm correctly classifies the sample assuming only that the
two components are separable by a hyperplane, i.e., there exists a halfspace
that contains most of one Gaussian and almost none of the other in probability
mass. This is nearly the best possible, improving known results substantially.
For k > 2 components, the algorithm requires only that there be some
(k-1)-dimensional subspace in which the emoverlap in every direction is small.
Here we define overlap to be the ratio of the following two quantities: 1) the
average squared distance between a point and the mean of its component, and 2)
the average squared distance between a point and the mean of the mixture. The
main result may also be stated in the language of linear discriminant analysis:
if the standard Fisher discriminant is small enough, labels are not needed to
estimate the optimal subspace for projection. Our main tools are isotropic
transformation, spectral projection and a simple reweighting technique. We call
this combination isotropic PCA.
</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="99"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">99</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">99</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">Multiple Random Oracles Are Better Than One</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">  We study the problem of learning k-juntas given access to examples drawn from
a number of different product distributions. Thus we wish to learn a function f
: {-1,1}^n -> {-1,1} that depends on k (unknown) coordinates. While the best
known algorithms for the general problem of learning a k-junta require running
time of n^k * poly(n,2^k), we show that given access to k different product
distributions with biases separated by \gamma>0, the functions may be learned
in time poly(n,2^k,\gamma^{-k}). More generally, given access to t &lt;= k
different product distributions, the functions may be learned in time n^{k/t} *
poly(n,2^k,\gamma^{-k}). Our techniques involve novel results in Fourier
analysis relating Fourier expansions with respect to different biases and a
generalization of Russo's formula.
</span></div></div>
							</td>
					</tr></tbody></table>
		<div class="bg-linear-to-b sticky left-0 border-t border-dashed border-gray-300 from-gray-100 to-white py-3 text-center font-mono text-xs dark:border-gray-700 dark:from-gray-950 dark:to-gray-900">End of preview. <a href="/datasets/CShorten/ML-ArXiv-Papers/viewer/default/train" class="group"><span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Expand</span>
						in <svg class="text-lg mr-0.5 inline -translate-y-px text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>Data Studio
					</a></div></div>




			<div class="bg-linear-to-b from-gray-100 to-white dark:from-gray-950 dark:to-gray-900 "><hr class="flex-none -translate-y-px border-t border-dashed border-gray-300 bg-white dark:border-gray-700 dark:bg-gray-950">
					<nav><ul class="flex select-none items-center justify-between space-x-2 text-gray-700 sm:justify-center py-1 text-center font-mono text-xs rounded-b-lg"><li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 pointer-events-none cursor-default text-gray-400 hover:text-gray-700" href=""><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg>
		Previous</a></li>
			<li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1 bg-gray-50 font-semibold ring-1 ring-inset ring-gray-200 dark:bg-gray-900 dark:text-yellow-500 dark:ring-gray-900 hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/CShorten/ML-ArXiv-Papers/viewer/default/train?p=0">1</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/CShorten/ML-ArXiv-Papers/viewer/default/train?p=1">2</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/CShorten/ML-ArXiv-Papers/viewer/default/train?p=2">3</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  pointer-events-none cursor-default" href="#">...</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/CShorten/ML-ArXiv-Papers/viewer/default/train?p=1175">1,176</a>
				</li>
			<li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 " href="/datasets/CShorten/ML-ArXiv-Papers/viewer/default/train?p=1">Next
		<svg class="ml-1.5 transform rotate-180" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg></a></li></ul></nav></div></div>






</div></div></div></div></div></div></div>

				
					<div class="SVELTE_HYDRATER contents" data-target="RepoCodeCopy" data-props="{}"><div></div></div>
					

					<div class="SVELTE_HYDRATER contents" data-target="SideNavigation" data-props="{&quot;titleTree&quot;:[],&quot;classNames&quot;:&quot;top-6&quot;}">

</div>
					<div class="2xl:pr-6"><div class="prose pl-6 -ml-6 hf-sanitized hf-sanitized-Wg5ZviwP5JuKoPjliOriV">
	<!-- HTML_TAG_START --><p>This dataset contains the subset of ArXiv papers with the "cs.LG" tag to indicate the paper is about Machine Learning.</p>
<p>The core dataset is filtered from the full ArXiv dataset hosted on Kaggle: <a rel="nofollow" href="https://www.kaggle.com/datasets/Cornell-University/arxiv">https://www.kaggle.com/datasets/Cornell-University/arxiv</a>. The original dataset contains roughly 2 million papers. This dataset contains roughly 100,000 papers following the category filtering.</p>
<p>The dataset is maintained by with requests to the ArXiv API.</p>
<p>The current iteration of the dataset only contains the title and abstract of the paper.</p>
<p>The ArXiv dataset contains additional features that we may look to include in future releases. We have highlighted the top two features on the roadmap for integration:</p>
<ul>
<li> <b>authors</b> </li>
<li> <b>update_date</b> </li>
<li> Submitter </li>
<li> Comments </li>
<li> Journal-ref </li>
<li> doi </li>
<li> report-no </li>
<li> categories </li>
<li> license </li>
<li> versions </li>
<li> authors_parsed </li>
</ul><!-- HTML_TAG_END --></div>
</div></section>
			
			<section class="pt-6 border-gray-100 md:pb-24 md:pl-6 md:w-64 lg:w-80 xl:w-96 flex-none order-first md:order-none md:border-l pt-3! md:pt-6!"><dl class="flex items-baseline justify-between"><dt class="text-sm text-gray-500">Downloads last month</dt><div class="mx-4 flex-1 border-b border-dotted"></div><dd class="font-semibold">1,580</dd></dl>
				
				<div class="divider-column-vertical"></div>
				<div class="grid grid-cols-2 gap-x-2 md:flex md:flex-row md:flex-wrap"><div class="SVELTE_HYDRATER contents" data-target="DatasetAndModelActionsDropdown" data-props="{&quot;classNames&quot;:&quot;order-last&quot;,&quot;discussionsDisabled&quot;:false,&quot;repo&quot;:{&quot;type&quot;:&quot;dataset&quot;,&quot;name&quot;:&quot;CShorten/ML-ArXiv-Papers&quot;},&quot;canWrite&quot;:false,&quot;canDisable&quot;:false,&quot;repoIsPrivate&quot;:false,&quot;repoIsGated&quot;:false,&quot;repoIsDisabled&quot;:false,&quot;repoIsAdminFlaggedNFAA&quot;:false,&quot;repoHasBlockedOids&quot;:false}"><div class="order-last"><div class="relative ">
	<button class="btn px-1.5 py-1.5 " type="button">
		
			<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="p-0.5" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><circle cx="16" cy="7" r="3" fill="currentColor"></circle><circle cx="16" cy="16" r="3" fill="currentColor"></circle><circle cx="16" cy="25" r="3" fill="currentColor"></circle></svg>
		
		</button>
	
	
	</div></div>













</div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetLibrary" data-props="{&quot;classNames&quot;:&quot;md:w-full xl:w-auto xl:flex-none&quot;,&quot;libraries&quot;:[{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;datasets&quot;,&quot;function&quot;:&quot;load_dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{},&quot;code&quot;:&quot;from datasets import load_dataset\n\nds = load_dataset(\&quot;CShorten/ML-ArXiv-Papers\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;pandas&quot;,&quot;function&quot;:&quot;pd.read_csv&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;ML-Arxiv-Papers.csv&quot;}},&quot;code&quot;:&quot;import pandas as pd\n\ndf = pd.read_csv(\&quot;hf://datasets/CShorten/ML-ArXiv-Papers/ML-Arxiv-Papers.csv\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;mlcroissant&quot;,&quot;function&quot;:&quot;Dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;record_set&quot;:&quot;default&quot;,&quot;partial&quot;:false},&quot;code&quot;:&quot;from mlcroissant import Dataset\n\nds = Dataset(jsonld=\&quot;https://huggingface.co/api/datasets/CShorten/ML-ArXiv-Papers/croissant\&quot;)\nrecords = ds.records(\&quot;default\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;polars&quot;,&quot;function&quot;:&quot;pl.read_csv&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;ML-Arxiv-Papers.csv&quot;}},&quot;code&quot;:&quot;import polars as pl\n\ndf = pl.read_csv('hf://datasets/CShorten/ML-ArXiv-Papers/ML-Arxiv-Papers.csv')\n&quot;}]}]}"><div class="relative md:w-full xl:w-auto xl:flex-none">
	<button class="from-gray-800! to-black! max-xl:mb-2 text-white! gap-1! border-gray-800! dark:border-gray-900!  btn w-full cursor-pointer text-sm" type="button">
		<svg class="mr-1.5 mr-0.5! " xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>
			Use this dataset
		<svg class="-mr-1 text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path d="M16.293 9.293L12 13.586L7.707 9.293l-1.414 1.414L12 16.414l5.707-5.707z" fill="currentColor"></path></svg></button>
	
	
	</div>

</div>
					
					</div>
				<div class="divider-column-vertical"></div>
					<div class="flex flex-col flex-wrap xl:flex-row"><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of downloaded dataset files:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->147 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 " href="/datasets/CShorten/ML-ArXiv-Papers/tree/refs%2Fconvert%2Fparquet/" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of the auto-converted Parquet files:</div>
		<div class="truncate text-sm group-hover:underline">
			<!-- HTML_TAG_START -->85 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Number of rows:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->117,592<!-- HTML_TAG_END --></div></a></div>
				
				
				<div class="divider-column-vertical"></div>
					<h2 class="text-smd mb-5 flex items-baseline overflow-hidden whitespace-nowrap font-semibold text-gray-800"><svg class="mr-1.5 text-sm inline self-center flex-none text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
						Models trained or fine-tuned on
						<span class="ml-1 truncate font-mono text-[0.87rem] font-medium">CShorten/ML-ArXiv-Papers</span></h2>

					<div class="space-y-3"><div class=""><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/or4cl3ai/Aiden_t5"><div class="w-full truncate"><header class="flex items-center mb-1" title="or4cl3ai/Aiden_t5"><img alt="" class="w-3 h-3 rounded-full mr-1.5 flex-none flex-none" src="/avatars/fa989f8f648b94c70e52a8bcb797d14c.svg" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">or4cl3ai/Aiden_t5</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400"><svg class="mr-1.5 text-[.8rem] flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 18 18"><path d="M16.2607 8.08202L14.468 6.28928C14.3063 6.12804 14.0873 6.03749 13.859 6.03749C13.6307 6.03749 13.4117 6.12804 13.25 6.28928L5.6375 13.904V16.9125H8.64607L16.2607 9.30002C16.422 9.13836 16.5125 8.91935 16.5125 8.69102C16.5125 8.4627 16.422 8.24369 16.2607 8.08202V8.08202ZM8.1953 15.825H6.725V14.3547L11.858 9.22118L13.3288 10.6915L8.1953 15.825ZM14.0982 9.92262L12.6279 8.45232L13.8606 7.21964L15.3309 8.68994L14.0982 9.92262Z"></path><path d="M6.18125 9.84373H7.26875V6.03748H8.9V4.94998H4.55V6.03748H6.18125V9.84373Z"></path><path d="M4.55 11.475H2.375V2.775H11.075V4.95H12.1625V2.775C12.1625 2.48658 12.0479 2.20997 11.844 2.00602C11.64 1.80208 11.3634 1.6875 11.075 1.6875H2.375C2.08658 1.6875 1.80997 1.80208 1.60602 2.00602C1.40207 2.20997 1.2875 2.48658 1.2875 2.775V11.475C1.2875 11.7634 1.40207 12.04 1.60602 12.244C1.80997 12.4479 2.08658 12.5625 2.375 12.5625H4.55V11.475Z"></path></svg>
			Text Generation
			<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
	
				<span class="truncate">Updated
					<time datetime="2023-10-06T01:02:28" title="Fri, 06 Oct 2023 01:02:28 GMT">Oct 6, 2023</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					2.21k
				
	
				<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
					<svg class="flex-none w-3 text-gray-400 mr-1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>
					16

				</div></div>
		
	</a></article>
							</div><div class=""><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/acecalisto3/PhiCo-D-Instruck"><div class="w-full truncate"><header class="flex items-center mb-1" title="acecalisto3/PhiCo-D-Instruck"><img alt="" class="w-3 h-3 rounded-full mr-1.5 flex-none flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/64bf6a4a979949d2e25473ae/d-Zk9ZKLYbzoCv9hpr_DT.jpeg" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">acecalisto3/PhiCo-D-Instruck</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400"><svg class="mr-1.5 text-[.8rem] flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 18 18"><path d="M4.00626 16.5125C3.46854 16.5125 2.9429 16.353 2.4958 16.0543C2.0487 15.7556 1.70024 15.3309 1.49446 14.8342C1.28868 14.3374 1.23484 13.7907 1.33975 13.2633C1.44465 12.7359 1.70359 12.2515 2.08381 11.8713C2.46403 11.4911 2.94847 11.2321 3.47586 11.1272C4.00324 11.0223 4.54989 11.0762 5.04668 11.2819C5.54346 11.4877 5.96807 11.8362 6.26681 12.2833C6.56555 12.7304 6.72501 13.256 6.72501 13.7937C6.72414 14.5145 6.43743 15.2055 5.92775 15.7152C5.41807 16.2249 4.72705 16.5116 4.00626 16.5125V16.5125ZM4.00626 12.1625C3.68363 12.1625 3.36824 12.2582 3.09998 12.4374C2.83173 12.6166 2.62264 12.8714 2.49918 13.1695C2.37571 13.4676 2.34341 13.7955 2.40635 14.112C2.46929 14.4284 2.62465 14.7191 2.85279 14.9472C3.08092 15.1753 3.37158 15.3307 3.68802 15.3936C4.00445 15.4566 4.33244 15.4243 4.63051 15.3008C4.92858 15.1773 5.18335 14.9683 5.36259 14.7C5.54184 14.4317 5.63751 14.1164 5.63751 13.7937C5.63708 13.3612 5.46507 12.9466 5.15925 12.6407C4.85342 12.3349 4.43876 12.1629 4.00626 12.1625Z"></path><path d="M13.25 14.3375H7.81251V13.25H13.25V9.44371H4.55001C4.26167 9.44343 3.98523 9.32876 3.78135 9.12487C3.57747 8.92099 3.4628 8.64455 3.46251 8.35621V4.54996C3.4628 4.26163 3.57747 3.98519 3.78135 3.7813C3.98523 3.57742 4.26167 3.46275 4.55001 3.46246H9.98751V4.54996H4.55001V8.35621H13.25C13.5383 8.3565 13.8148 8.47117 14.0187 8.67505C14.2226 8.87894 14.3372 9.15538 14.3375 9.44371V13.25C14.3372 13.5383 14.2226 13.8147 14.0187 14.0186C13.8148 14.2225 13.5383 14.3372 13.25 14.3375V14.3375Z"></path><path d="M15.425 6.72504H12.1625C11.8742 6.72475 11.5977 6.61008 11.3939 6.4062C11.19 6.20231 11.0753 5.92587 11.075 5.63754V2.37504C11.0753 2.0867 11.19 1.81026 11.3939 1.60638C11.5977 1.40249 11.8742 1.28782 12.1625 1.28754H15.425C15.7133 1.28782 15.9898 1.40249 16.1937 1.60638C16.3976 1.81026 16.5122 2.0867 16.5125 2.37504V5.63754C16.5122 5.92587 16.3976 6.20231 16.1937 6.4062C15.9898 6.61008 15.7133 6.72475 15.425 6.72504V6.72504ZM12.1625 2.37504V5.63754H15.425V2.37504H12.1625Z"></path></svg>
			Text2Text Generation
			<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
	
				<span class="truncate">Updated
					<time datetime="2024-07-05T16:13:26" title="Fri, 05 Jul 2024 16:13:26 GMT">Jul 5, 2024</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					24
				
	
				<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
					<svg class="flex-none w-3 text-gray-400 mr-1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>
					2

				</div></div>
		
	</a></article>
							</div><div class=""><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/aalksii/albert-base-v2-ml-arxiv-papers"><div class="w-full truncate"><header class="flex items-center mb-1" title="aalksii/albert-base-v2-ml-arxiv-papers"><img alt="" class="w-3 h-3 rounded-full mr-1.5 flex-none flex-none" src="/avatars/e4684102e9ec24b684ca3b63b2cdef71.svg" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">aalksii/albert-base-v2-ml-arxiv-papers</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400"><svg class="mr-1.5 text-[.8rem] flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 18 19"><path d="M12.3625 13.85H10.1875V12.7625H12.3625V10.5875H13.45V12.7625C13.4497 13.0508 13.335 13.3272 13.1312 13.5311C12.9273 13.735 12.6508 13.8497 12.3625 13.85V13.85Z"></path><path d="M5.8375 8.41246H4.75V6.23746C4.75029 5.94913 4.86496 5.67269 5.06884 5.4688C5.27272 5.26492 5.54917 5.15025 5.8375 5.14996H8.0125V6.23746H5.8375V8.41246Z"></path><path d="M15.625 5.14998H13.45V2.97498C13.4497 2.68665 13.335 2.4102 13.1312 2.20632C12.9273 2.00244 12.6508 1.88777 12.3625 1.88748H2.575C2.28666 1.88777 2.01022 2.00244 1.80633 2.20632C1.60245 2.4102 1.48778 2.68665 1.4875 2.97498V12.7625C1.48778 13.0508 1.60245 13.3273 1.80633 13.5311C2.01022 13.735 2.28666 13.8497 2.575 13.85H4.75V16.025C4.75028 16.3133 4.86495 16.5898 5.06883 16.7936C5.27272 16.9975 5.54916 17.1122 5.8375 17.1125H15.625C15.9133 17.1122 16.1898 16.9975 16.3937 16.7936C16.5975 16.5898 16.7122 16.3133 16.7125 16.025V6.23748C16.7122 5.94915 16.5975 5.6727 16.3937 5.46882C16.1898 5.26494 15.9133 5.15027 15.625 5.14998V5.14998ZM15.625 16.025H5.8375V13.85H8.0125V12.7625H5.8375V10.5875H4.75V12.7625H2.575V2.97498H12.3625V5.14998H10.1875V6.23748H12.3625V8.41248H13.45V6.23748H15.625V16.025Z"></path></svg>
			Fill-Mask
			<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
	
				<span class="truncate">Updated
					<time datetime="2023-06-01T12:45:05" title="Thu, 01 Jun 2023 12:45:05 GMT">Jun 1, 2023</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					21
				
	
				

				</div></div>
		
	</a></article>
							</div><div class=""><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/aalksii/distilbert-base-uncased-ml-arxiv-papers"><div class="w-full truncate"><header class="flex items-center mb-1" title="aalksii/distilbert-base-uncased-ml-arxiv-papers"><img alt="" class="w-3 h-3 rounded-full mr-1.5 flex-none flex-none" src="/avatars/e4684102e9ec24b684ca3b63b2cdef71.svg" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">aalksii/distilbert-base-uncased-ml-arxiv-papers</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400"><svg class="mr-1.5 text-[.8rem] flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 18 19"><path d="M12.3625 13.85H10.1875V12.7625H12.3625V10.5875H13.45V12.7625C13.4497 13.0508 13.335 13.3272 13.1312 13.5311C12.9273 13.735 12.6508 13.8497 12.3625 13.85V13.85Z"></path><path d="M5.8375 8.41246H4.75V6.23746C4.75029 5.94913 4.86496 5.67269 5.06884 5.4688C5.27272 5.26492 5.54917 5.15025 5.8375 5.14996H8.0125V6.23746H5.8375V8.41246Z"></path><path d="M15.625 5.14998H13.45V2.97498C13.4497 2.68665 13.335 2.4102 13.1312 2.20632C12.9273 2.00244 12.6508 1.88777 12.3625 1.88748H2.575C2.28666 1.88777 2.01022 2.00244 1.80633 2.20632C1.60245 2.4102 1.48778 2.68665 1.4875 2.97498V12.7625C1.48778 13.0508 1.60245 13.3273 1.80633 13.5311C2.01022 13.735 2.28666 13.8497 2.575 13.85H4.75V16.025C4.75028 16.3133 4.86495 16.5898 5.06883 16.7936C5.27272 16.9975 5.54916 17.1122 5.8375 17.1125H15.625C15.9133 17.1122 16.1898 16.9975 16.3937 16.7936C16.5975 16.5898 16.7122 16.3133 16.7125 16.025V6.23748C16.7122 5.94915 16.5975 5.6727 16.3937 5.46882C16.1898 5.26494 15.9133 5.15027 15.625 5.14998V5.14998ZM15.625 16.025H5.8375V13.85H8.0125V12.7625H5.8375V10.5875H4.75V12.7625H2.575V2.97498H12.3625V5.14998H10.1875V6.23748H12.3625V8.41248H13.45V6.23748H15.625V16.025Z"></path></svg>
			Fill-Mask
			<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
	
				<span class="truncate">Updated
					<time datetime="2023-06-01T12:44:33" title="Thu, 01 Jun 2023 12:44:33 GMT">Jun 1, 2023</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					18
				
	
				

				</div></div>
		
	</a></article>
							</div><div class="hidden md:block"><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/AlexWortega/taskGPT2-xl-v0.2a"><div class="w-full truncate"><header class="flex items-center mb-1" title="AlexWortega/taskGPT2-xl-v0.2a"><img alt="" class="w-3 h-3 rounded-full mr-1.5 flex-none flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/60b23b069c978cce68723b25/rFi5BkB8XepyiQ-j4ipdO.jpeg" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">AlexWortega/taskGPT2-xl-v0.2a</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400"><svg class="mr-1.5 text-[.8rem] flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 18 18"><path d="M16.2607 8.08202L14.468 6.28928C14.3063 6.12804 14.0873 6.03749 13.859 6.03749C13.6307 6.03749 13.4117 6.12804 13.25 6.28928L5.6375 13.904V16.9125H8.64607L16.2607 9.30002C16.422 9.13836 16.5125 8.91935 16.5125 8.69102C16.5125 8.4627 16.422 8.24369 16.2607 8.08202V8.08202ZM8.1953 15.825H6.725V14.3547L11.858 9.22118L13.3288 10.6915L8.1953 15.825ZM14.0982 9.92262L12.6279 8.45232L13.8606 7.21964L15.3309 8.68994L14.0982 9.92262Z"></path><path d="M6.18125 9.84373H7.26875V6.03748H8.9V4.94998H4.55V6.03748H6.18125V9.84373Z"></path><path d="M4.55 11.475H2.375V2.775H11.075V4.95H12.1625V2.775C12.1625 2.48658 12.0479 2.20997 11.844 2.00602C11.64 1.80208 11.3634 1.6875 11.075 1.6875H2.375C2.08658 1.6875 1.80997 1.80208 1.60602 2.00602C1.40207 2.20997 1.2875 2.48658 1.2875 2.775V11.475C1.2875 11.7634 1.40207 12.04 1.60602 12.244C1.80997 12.4479 2.08658 12.5625 2.375 12.5625H4.55V11.475Z"></path></svg>
			Text Generation
			<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
	
				<span class="truncate">Updated
					<time datetime="2023-11-21T08:31:24" title="Tue, 21 Nov 2023 08:31:24 GMT">Nov 21, 2023</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					12
				
	
				<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
					<svg class="flex-none w-3 text-gray-400 mr-1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>
					5

				</div></div>
		
	</a></article>
							</div><div class="hidden md:block"><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/ZMC2019/Llama-3-330M"><div class="w-full truncate"><header class="flex items-center mb-1" title="ZMC2019/Llama-3-330M"><img alt="" class="w-3 h-3 rounded-full mr-1.5 flex-none flex-none" src="/avatars/2c6300471cd09a067d56aa09bc06ad78.svg" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">ZMC2019/Llama-3-330M</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400"><svg class="mr-1.5 text-[.8rem] flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 18 18"><path d="M16.2607 8.08202L14.468 6.28928C14.3063 6.12804 14.0873 6.03749 13.859 6.03749C13.6307 6.03749 13.4117 6.12804 13.25 6.28928L5.6375 13.904V16.9125H8.64607L16.2607 9.30002C16.422 9.13836 16.5125 8.91935 16.5125 8.69102C16.5125 8.4627 16.422 8.24369 16.2607 8.08202V8.08202ZM8.1953 15.825H6.725V14.3547L11.858 9.22118L13.3288 10.6915L8.1953 15.825ZM14.0982 9.92262L12.6279 8.45232L13.8606 7.21964L15.3309 8.68994L14.0982 9.92262Z"></path><path d="M6.18125 9.84373H7.26875V6.03748H8.9V4.94998H4.55V6.03748H6.18125V9.84373Z"></path><path d="M4.55 11.475H2.375V2.775H11.075V4.95H12.1625V2.775C12.1625 2.48658 12.0479 2.20997 11.844 2.00602C11.64 1.80208 11.3634 1.6875 11.075 1.6875H2.375C2.08658 1.6875 1.80997 1.80208 1.60602 2.00602C1.40207 2.20997 1.2875 2.48658 1.2875 2.775V11.475C1.2875 11.7634 1.40207 12.04 1.60602 12.244C1.80997 12.4479 2.08658 12.5625 2.375 12.5625H4.55V11.475Z"></path></svg>
			Text Generation
			<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
	
				<span class="truncate">Updated
					<time datetime="2024-11-08T20:53:33" title="Fri, 08 Nov 2024 20:53:33 GMT">Nov 8, 2024</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					12
				
	
				<span class="px-1.5 text-gray-300 dark:text-gray-500">â€¢ </span>
					<svg class="flex-none w-3 text-gray-400 mr-1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>
					1

				</div></div>
		
	</a></article>
							</div>
						<a class="flex items-center pt-1 text-sm text-gray-500 underline hover:text-gray-700" href="/models?dataset=dataset:CShorten/ML-ArXiv-Papers">Browse 7 models trained on this dataset
							</a></div>
				

				
				<div class="divider-column-vertical md:hidden"></div></section></div></main>

	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container relative flex flex-col justify-between space-y-2 py-6 text-gray-500 max-md:*:self-start md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="SVELTE_HYDRATER contents" data-target="ThemeSwitcher" data-props="{&quot;theme&quot;:&quot;system&quot;,&quot;isLoggedIn&quot;:false,&quot;menuClassNames&quot;:&quot;md:-top-24&quot;,&quot;classNames&quot;:&quot;max-md:mb-5 max-md:*:self-start&quot;}">
<div class="relative inline-block max-md:mb-5 max-md:*:self-start">
	<button class="rounded-full border border-gray-100 pl-2 py-1 pr-2.5  flex items-center text-sm text-gray-500 bg-white hover:bg-purple-50 hover:border-purple-200 dark:hover:bg-gray-800 dark:hover:border-gray-950 dark:border-gray-800 " type="button">
		<svg class="mr-1.5 text-gray-500" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M6 22.5h20a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2H6a2 2 0 0 0-2 2v13.5a2 2 0 0 0 2 2ZM7 7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h18a1 1 0 0 0 1-1V8a1 1 0 0 0-1-1H7Z" fill="currentColor"></path><path d="M6 8a1 1 0 0 1 1-1h18a1 1 0 0 1 1 1v11a1 1 0 0 1-1 1H7a1 1 0 0 1-1-1V8Z" fill="currentColor" fill-opacity=".4"></path><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path></svg>
			System theme
		</button>
	
	
	</div></div>
		<div class="font-semibold text-black md:hidden">Company</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="max-md:mb-4! max-md:mt-8! group flex-none max-md:order-last"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="max-md:mt-8! font-semibold text-black md:hidden">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("\/front\/build\/kube-68d7aa0\/index.js");
			window.moonSha = "kube-68d7aa0\/";
			window.__hf_deferred = {};
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>
	</body>
</html>
