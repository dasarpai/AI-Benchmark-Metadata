<!doctype html>
<html class="">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
		<meta name="description" content="We’re on a journey to advance and democratize artificial intelligence through open source and open science." />
		<meta property="fb:app_id" content="1321688464574422" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@huggingface" />
		<meta name="twitter:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/lmms-lab/LLaVA-Video-178K.png" />
		<meta property="og:title" content="lmms-lab/LLaVA-Video-178K · Datasets at Hugging Face" />
		<meta property="og:type" content="website" />
		<meta property="og:url" content="https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K" />
		<meta property="og:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/lmms-lab/LLaVA-Video-178K.png" />

		<link rel="stylesheet" href="/front/build/kube-68d7aa0/style.css" />

		<link rel="preconnect" href="https://fonts.gstatic.com" />
		<link
			href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap"
			rel="stylesheet"
		/>

		<link
			rel="preload"
			href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css"
			as="style"
			onload="this.onload=null;this.rel='stylesheet'"
		/>
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		<script>const guestTheme = document.cookie.match(/theme=(\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches));</script>
<link rel="canonical" href="https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K"> <script type="application/ld+json">{
  "@context": {
    "@language": "en",
    "@vocab": "https:\/\/schema.org\/",
    "citeAs": "cr:citeAs",
    "column": "cr:column",
    "conformsTo": "dct:conformsTo",
    "cr": "http:\/\/mlcommons.org\/croissant\/",
    "data": {
      "@id": "cr:data",
      "@type": "@json"
    },
    "dataBiases": "cr:dataBiases",
    "dataCollection": "cr:dataCollection",
    "dataType": {
      "@id": "cr:dataType",
      "@type": "@vocab"
    },
    "dct": "http:\/\/purl.org\/dc\/terms\/",
    "extract": "cr:extract",
    "field": "cr:field",
    "fileProperty": "cr:fileProperty",
    "fileObject": "cr:fileObject",
    "fileSet": "cr:fileSet",
    "format": "cr:format",
    "includes": "cr:includes",
    "isLiveDataset": "cr:isLiveDataset",
    "jsonPath": "cr:jsonPath",
    "key": "cr:key",
    "md5": "cr:md5",
    "parentField": "cr:parentField",
    "path": "cr:path",
    "personalSensitiveInformation": "cr:personalSensitiveInformation",
    "recordSet": "cr:recordSet",
    "references": "cr:references",
    "regex": "cr:regex",
    "repeated": "cr:repeated",
    "replace": "cr:replace",
    "sc": "https:\/\/schema.org\/",
    "separator": "cr:separator",
    "source": "cr:source",
    "subField": "cr:subField",
    "transform": "cr:transform"
  },
  "@type": "sc:Dataset",
  "distribution": [
    {
      "@type": "cr:FileObject",
      "@id": "repo",
      "name": "repo",
      "description": "The Hugging Face git repository.",
      "contentUrl": "https:\/\/huggingface.co\/datasets\/lmms-lab\/LLaVA-Video-178K\/tree\/refs%2Fconvert%2Fparquet",
      "encodingFormat": "git+https",
      "sha256": "https:\/\/github.com\/mlcommons\/croissant\/issues\/80"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-0_30_s_academic_v0_1",
      "name": "parquet-files-for-config-0_30_s_academic_v0_1",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "0_30_s_academic_v0_1\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-0_30_s_nextqa",
      "name": "parquet-files-for-config-0_30_s_nextqa",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "0_30_s_nextqa\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-0_30_s_perceptiontest",
      "name": "parquet-files-for-config-0_30_s_perceptiontest",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "0_30_s_perceptiontest\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-0_30_s_youtube_v0_1",
      "name": "parquet-files-for-config-0_30_s_youtube_v0_1",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "0_30_s_youtube_v0_1\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-1_2_m_academic_v0_1",
      "name": "parquet-files-for-config-1_2_m_academic_v0_1",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "1_2_m_academic_v0_1\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-1_2_m_nextqa",
      "name": "parquet-files-for-config-1_2_m_nextqa",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "1_2_m_nextqa\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-1_2_m_youtube_v0_1",
      "name": "parquet-files-for-config-1_2_m_youtube_v0_1",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "1_2_m_youtube_v0_1\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-2_3_m_academic_v0_1",
      "name": "parquet-files-for-config-2_3_m_academic_v0_1",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "2_3_m_academic_v0_1\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-2_3_m_nextqa",
      "name": "parquet-files-for-config-2_3_m_nextqa",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "2_3_m_nextqa\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-2_3_m_youtube_v0_1",
      "name": "parquet-files-for-config-2_3_m_youtube_v0_1",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "2_3_m_youtube_v0_1\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-30_60_s_academic_v0_1",
      "name": "parquet-files-for-config-30_60_s_academic_v0_1",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "30_60_s_academic_v0_1\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-30_60_s_nextqa",
      "name": "parquet-files-for-config-30_60_s_nextqa",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "30_60_s_nextqa\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-30_60_s_perceptiontest",
      "name": "parquet-files-for-config-30_60_s_perceptiontest",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "30_60_s_perceptiontest\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-30_60_s_youtube_v0_1",
      "name": "parquet-files-for-config-30_60_s_youtube_v0_1",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "30_60_s_youtube_v0_1\/*\/*.parquet"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-llava_hound",
      "name": "parquet-files-for-config-llava_hound",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "llava_hound\/*\/*.parquet"
    }
  ],
  "recordSet": [
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "0_30_s_academic_v0_1_splits\/split_name"
      },
      "@id": "0_30_s_academic_v0_1_splits",
      "name": "0_30_s_academic_v0_1_splits",
      "description": "Splits for the 0_30_s_academic_v0_1 config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "0_30_s_academic_v0_1_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "0_30_s_academic_v0_1_splits\/split_name": "caption"
        },
        {
          "0_30_s_academic_v0_1_splits\/split_name": "open_ended"
        },
        {
          "0_30_s_academic_v0_1_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "0_30_s_academic_v0_1",
      "name": "0_30_s_academic_v0_1",
      "description": "lmms-lab\/LLaVA-Video-178K - '0_30_s_academic_v0_1' subset\n\nAdditional information:\n- 3 splits: caption, open_ended, multi_choice",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "0_30_s_academic_v0_1\/split",
          "name": "0_30_s_academic_v0_1\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_academic_v0_1"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "0_30_s_academic_v0_1\/(?:partial-)?(caption|open_ended|multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "0_30_s_academic_v0_1_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_academic_v0_1\/id",
          "name": "0_30_s_academic_v0_1\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_academic_v0_1"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_academic_v0_1\/conversations",
          "name": "0_30_s_academic_v0_1\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "0_30_s_academic_v0_1\/conversations\/from",
              "name": "0_30_s_academic_v0_1\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-0_30_s_academic_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "0_30_s_academic_v0_1\/conversations\/value",
              "name": "0_30_s_academic_v0_1\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-0_30_s_academic_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_academic_v0_1\/data_source",
          "name": "0_30_s_academic_v0_1\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_academic_v0_1"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_academic_v0_1\/video",
          "name": "0_30_s_academic_v0_1\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_academic_v0_1"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "0_30_s_nextqa_splits\/split_name"
      },
      "@id": "0_30_s_nextqa_splits",
      "name": "0_30_s_nextqa_splits",
      "description": "Splits for the 0_30_s_nextqa config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "0_30_s_nextqa_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "0_30_s_nextqa_splits\/split_name": "open_ended"
        },
        {
          "0_30_s_nextqa_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "0_30_s_nextqa",
      "name": "0_30_s_nextqa",
      "description": "lmms-lab\/LLaVA-Video-178K - '0_30_s_nextqa' subset\n\nAdditional information:\n- 2 splits: open_ended, multi_choice",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "0_30_s_nextqa\/split",
          "name": "0_30_s_nextqa\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_nextqa"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "0_30_s_nextqa\/(?:partial-)?(open_ended|multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "0_30_s_nextqa_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_nextqa\/id",
          "name": "0_30_s_nextqa\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_nextqa"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_nextqa\/conversations",
          "name": "0_30_s_nextqa\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "0_30_s_nextqa\/conversations\/from",
              "name": "0_30_s_nextqa\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-0_30_s_nextqa"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "0_30_s_nextqa\/conversations\/value",
              "name": "0_30_s_nextqa\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-0_30_s_nextqa"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_nextqa\/data_source",
          "name": "0_30_s_nextqa\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_nextqa"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_nextqa\/video",
          "name": "0_30_s_nextqa\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_nextqa"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "0_30_s_perceptiontest_splits\/split_name"
      },
      "@id": "0_30_s_perceptiontest_splits",
      "name": "0_30_s_perceptiontest_splits",
      "description": "Splits for the 0_30_s_perceptiontest config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "0_30_s_perceptiontest_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "0_30_s_perceptiontest_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "0_30_s_perceptiontest",
      "name": "0_30_s_perceptiontest",
      "description": "lmms-lab\/LLaVA-Video-178K - '0_30_s_perceptiontest' subset",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "0_30_s_perceptiontest\/split",
          "name": "0_30_s_perceptiontest\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_perceptiontest"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "0_30_s_perceptiontest\/(?:partial-)?(multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "0_30_s_perceptiontest_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_perceptiontest\/id",
          "name": "0_30_s_perceptiontest\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_perceptiontest"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_perceptiontest\/conversations",
          "name": "0_30_s_perceptiontest\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "0_30_s_perceptiontest\/conversations\/from",
              "name": "0_30_s_perceptiontest\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-0_30_s_perceptiontest"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "0_30_s_perceptiontest\/conversations\/value",
              "name": "0_30_s_perceptiontest\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-0_30_s_perceptiontest"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_perceptiontest\/data_source",
          "name": "0_30_s_perceptiontest\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_perceptiontest"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_perceptiontest\/video",
          "name": "0_30_s_perceptiontest\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_perceptiontest"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "0_30_s_youtube_v0_1_splits\/split_name"
      },
      "@id": "0_30_s_youtube_v0_1_splits",
      "name": "0_30_s_youtube_v0_1_splits",
      "description": "Splits for the 0_30_s_youtube_v0_1 config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "0_30_s_youtube_v0_1_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "0_30_s_youtube_v0_1_splits\/split_name": "caption"
        },
        {
          "0_30_s_youtube_v0_1_splits\/split_name": "open_ended"
        },
        {
          "0_30_s_youtube_v0_1_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "0_30_s_youtube_v0_1",
      "name": "0_30_s_youtube_v0_1",
      "description": "lmms-lab\/LLaVA-Video-178K - '0_30_s_youtube_v0_1' subset\n\nAdditional information:\n- 3 splits: caption, open_ended, multi_choice",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "0_30_s_youtube_v0_1\/split",
          "name": "0_30_s_youtube_v0_1\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_youtube_v0_1"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "0_30_s_youtube_v0_1\/(?:partial-)?(caption|open_ended|multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "0_30_s_youtube_v0_1_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_youtube_v0_1\/id",
          "name": "0_30_s_youtube_v0_1\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_youtube_v0_1"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_youtube_v0_1\/conversations",
          "name": "0_30_s_youtube_v0_1\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "0_30_s_youtube_v0_1\/conversations\/from",
              "name": "0_30_s_youtube_v0_1\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-0_30_s_youtube_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "0_30_s_youtube_v0_1\/conversations\/value",
              "name": "0_30_s_youtube_v0_1\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-0_30_s_youtube_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_youtube_v0_1\/data_source",
          "name": "0_30_s_youtube_v0_1\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_youtube_v0_1"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "0_30_s_youtube_v0_1\/video",
          "name": "0_30_s_youtube_v0_1\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-0_30_s_youtube_v0_1"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "1_2_m_academic_v0_1_splits\/split_name"
      },
      "@id": "1_2_m_academic_v0_1_splits",
      "name": "1_2_m_academic_v0_1_splits",
      "description": "Splits for the 1_2_m_academic_v0_1 config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "1_2_m_academic_v0_1_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "1_2_m_academic_v0_1_splits\/split_name": "caption"
        },
        {
          "1_2_m_academic_v0_1_splits\/split_name": "open_ended"
        },
        {
          "1_2_m_academic_v0_1_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "1_2_m_academic_v0_1",
      "name": "1_2_m_academic_v0_1",
      "description": "lmms-lab\/LLaVA-Video-178K - '1_2_m_academic_v0_1' subset\n\nAdditional information:\n- 3 splits: caption, open_ended, multi_choice",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "1_2_m_academic_v0_1\/split",
          "name": "1_2_m_academic_v0_1\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-1_2_m_academic_v0_1"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "1_2_m_academic_v0_1\/(?:partial-)?(caption|open_ended|multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "1_2_m_academic_v0_1_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "1_2_m_academic_v0_1\/id",
          "name": "1_2_m_academic_v0_1\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-1_2_m_academic_v0_1"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "1_2_m_academic_v0_1\/conversations",
          "name": "1_2_m_academic_v0_1\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "1_2_m_academic_v0_1\/conversations\/from",
              "name": "1_2_m_academic_v0_1\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-1_2_m_academic_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "1_2_m_academic_v0_1\/conversations\/value",
              "name": "1_2_m_academic_v0_1\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-1_2_m_academic_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "1_2_m_academic_v0_1\/data_source",
          "name": "1_2_m_academic_v0_1\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-1_2_m_academic_v0_1"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "1_2_m_academic_v0_1\/video",
          "name": "1_2_m_academic_v0_1\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-1_2_m_academic_v0_1"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "1_2_m_nextqa_splits\/split_name"
      },
      "@id": "1_2_m_nextqa_splits",
      "name": "1_2_m_nextqa_splits",
      "description": "Splits for the 1_2_m_nextqa config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "1_2_m_nextqa_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "1_2_m_nextqa_splits\/split_name": "open_ended"
        },
        {
          "1_2_m_nextqa_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "1_2_m_nextqa",
      "name": "1_2_m_nextqa",
      "description": "lmms-lab\/LLaVA-Video-178K - '1_2_m_nextqa' subset\n\nAdditional information:\n- 2 splits: open_ended, multi_choice",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "1_2_m_nextqa\/split",
          "name": "1_2_m_nextqa\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-1_2_m_nextqa"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "1_2_m_nextqa\/(?:partial-)?(open_ended|multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "1_2_m_nextqa_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "1_2_m_nextqa\/id",
          "name": "1_2_m_nextqa\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-1_2_m_nextqa"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "1_2_m_nextqa\/conversations",
          "name": "1_2_m_nextqa\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "1_2_m_nextqa\/conversations\/from",
              "name": "1_2_m_nextqa\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-1_2_m_nextqa"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "1_2_m_nextqa\/conversations\/value",
              "name": "1_2_m_nextqa\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-1_2_m_nextqa"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "1_2_m_nextqa\/data_source",
          "name": "1_2_m_nextqa\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-1_2_m_nextqa"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "1_2_m_nextqa\/video",
          "name": "1_2_m_nextqa\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-1_2_m_nextqa"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "1_2_m_youtube_v0_1_splits\/split_name"
      },
      "@id": "1_2_m_youtube_v0_1_splits",
      "name": "1_2_m_youtube_v0_1_splits",
      "description": "Splits for the 1_2_m_youtube_v0_1 config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "1_2_m_youtube_v0_1_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "1_2_m_youtube_v0_1_splits\/split_name": "caption"
        },
        {
          "1_2_m_youtube_v0_1_splits\/split_name": "open_ended"
        },
        {
          "1_2_m_youtube_v0_1_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "1_2_m_youtube_v0_1",
      "name": "1_2_m_youtube_v0_1",
      "description": "lmms-lab\/LLaVA-Video-178K - '1_2_m_youtube_v0_1' subset\n\nAdditional information:\n- 3 splits: caption, open_ended, multi_choice",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "1_2_m_youtube_v0_1\/split",
          "name": "1_2_m_youtube_v0_1\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-1_2_m_youtube_v0_1"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "1_2_m_youtube_v0_1\/(?:partial-)?(caption|open_ended|multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "1_2_m_youtube_v0_1_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "1_2_m_youtube_v0_1\/id",
          "name": "1_2_m_youtube_v0_1\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-1_2_m_youtube_v0_1"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "1_2_m_youtube_v0_1\/conversations",
          "name": "1_2_m_youtube_v0_1\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "1_2_m_youtube_v0_1\/conversations\/from",
              "name": "1_2_m_youtube_v0_1\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-1_2_m_youtube_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "1_2_m_youtube_v0_1\/conversations\/value",
              "name": "1_2_m_youtube_v0_1\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-1_2_m_youtube_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "1_2_m_youtube_v0_1\/data_source",
          "name": "1_2_m_youtube_v0_1\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-1_2_m_youtube_v0_1"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "1_2_m_youtube_v0_1\/video",
          "name": "1_2_m_youtube_v0_1\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-1_2_m_youtube_v0_1"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "2_3_m_academic_v0_1_splits\/split_name"
      },
      "@id": "2_3_m_academic_v0_1_splits",
      "name": "2_3_m_academic_v0_1_splits",
      "description": "Splits for the 2_3_m_academic_v0_1 config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "2_3_m_academic_v0_1_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "2_3_m_academic_v0_1_splits\/split_name": "caption"
        },
        {
          "2_3_m_academic_v0_1_splits\/split_name": "open_ended"
        },
        {
          "2_3_m_academic_v0_1_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "2_3_m_academic_v0_1",
      "name": "2_3_m_academic_v0_1",
      "description": "lmms-lab\/LLaVA-Video-178K - '2_3_m_academic_v0_1' subset\n\nAdditional information:\n- 3 splits: caption, open_ended, multi_choice",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "2_3_m_academic_v0_1\/split",
          "name": "2_3_m_academic_v0_1\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-2_3_m_academic_v0_1"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "2_3_m_academic_v0_1\/(?:partial-)?(caption|open_ended|multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "2_3_m_academic_v0_1_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "2_3_m_academic_v0_1\/id",
          "name": "2_3_m_academic_v0_1\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-2_3_m_academic_v0_1"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "2_3_m_academic_v0_1\/conversations",
          "name": "2_3_m_academic_v0_1\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "2_3_m_academic_v0_1\/conversations\/from",
              "name": "2_3_m_academic_v0_1\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-2_3_m_academic_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "2_3_m_academic_v0_1\/conversations\/value",
              "name": "2_3_m_academic_v0_1\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-2_3_m_academic_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "2_3_m_academic_v0_1\/data_source",
          "name": "2_3_m_academic_v0_1\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-2_3_m_academic_v0_1"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "2_3_m_academic_v0_1\/video",
          "name": "2_3_m_academic_v0_1\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-2_3_m_academic_v0_1"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "2_3_m_nextqa_splits\/split_name"
      },
      "@id": "2_3_m_nextqa_splits",
      "name": "2_3_m_nextqa_splits",
      "description": "Splits for the 2_3_m_nextqa config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "2_3_m_nextqa_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "2_3_m_nextqa_splits\/split_name": "open_ended"
        },
        {
          "2_3_m_nextqa_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "2_3_m_nextqa",
      "name": "2_3_m_nextqa",
      "description": "lmms-lab\/LLaVA-Video-178K - '2_3_m_nextqa' subset\n\nAdditional information:\n- 2 splits: open_ended, multi_choice",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "2_3_m_nextqa\/split",
          "name": "2_3_m_nextqa\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-2_3_m_nextqa"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "2_3_m_nextqa\/(?:partial-)?(open_ended|multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "2_3_m_nextqa_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "2_3_m_nextqa\/id",
          "name": "2_3_m_nextqa\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-2_3_m_nextqa"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "2_3_m_nextqa\/conversations",
          "name": "2_3_m_nextqa\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "2_3_m_nextqa\/conversations\/from",
              "name": "2_3_m_nextqa\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-2_3_m_nextqa"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "2_3_m_nextqa\/conversations\/value",
              "name": "2_3_m_nextqa\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-2_3_m_nextqa"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "2_3_m_nextqa\/data_source",
          "name": "2_3_m_nextqa\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-2_3_m_nextqa"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "2_3_m_nextqa\/video",
          "name": "2_3_m_nextqa\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-2_3_m_nextqa"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "2_3_m_youtube_v0_1_splits\/split_name"
      },
      "@id": "2_3_m_youtube_v0_1_splits",
      "name": "2_3_m_youtube_v0_1_splits",
      "description": "Splits for the 2_3_m_youtube_v0_1 config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "2_3_m_youtube_v0_1_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "2_3_m_youtube_v0_1_splits\/split_name": "caption"
        },
        {
          "2_3_m_youtube_v0_1_splits\/split_name": "open_ended"
        },
        {
          "2_3_m_youtube_v0_1_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "2_3_m_youtube_v0_1",
      "name": "2_3_m_youtube_v0_1",
      "description": "lmms-lab\/LLaVA-Video-178K - '2_3_m_youtube_v0_1' subset\n\nAdditional information:\n- 3 splits: caption, open_ended, multi_choice",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "2_3_m_youtube_v0_1\/split",
          "name": "2_3_m_youtube_v0_1\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-2_3_m_youtube_v0_1"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "2_3_m_youtube_v0_1\/(?:partial-)?(caption|open_ended|multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "2_3_m_youtube_v0_1_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "2_3_m_youtube_v0_1\/id",
          "name": "2_3_m_youtube_v0_1\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-2_3_m_youtube_v0_1"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "2_3_m_youtube_v0_1\/conversations",
          "name": "2_3_m_youtube_v0_1\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "2_3_m_youtube_v0_1\/conversations\/from",
              "name": "2_3_m_youtube_v0_1\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-2_3_m_youtube_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "2_3_m_youtube_v0_1\/conversations\/value",
              "name": "2_3_m_youtube_v0_1\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-2_3_m_youtube_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "2_3_m_youtube_v0_1\/data_source",
          "name": "2_3_m_youtube_v0_1\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-2_3_m_youtube_v0_1"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "2_3_m_youtube_v0_1\/video",
          "name": "2_3_m_youtube_v0_1\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-2_3_m_youtube_v0_1"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "30_60_s_academic_v0_1_splits\/split_name"
      },
      "@id": "30_60_s_academic_v0_1_splits",
      "name": "30_60_s_academic_v0_1_splits",
      "description": "Splits for the 30_60_s_academic_v0_1 config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "30_60_s_academic_v0_1_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "30_60_s_academic_v0_1_splits\/split_name": "caption"
        },
        {
          "30_60_s_academic_v0_1_splits\/split_name": "open_ended"
        },
        {
          "30_60_s_academic_v0_1_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "30_60_s_academic_v0_1",
      "name": "30_60_s_academic_v0_1",
      "description": "lmms-lab\/LLaVA-Video-178K - '30_60_s_academic_v0_1' subset\n\nAdditional information:\n- 3 splits: caption, open_ended, multi_choice",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "30_60_s_academic_v0_1\/split",
          "name": "30_60_s_academic_v0_1\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_academic_v0_1"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "30_60_s_academic_v0_1\/(?:partial-)?(caption|open_ended|multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "30_60_s_academic_v0_1_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_academic_v0_1\/id",
          "name": "30_60_s_academic_v0_1\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_academic_v0_1"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_academic_v0_1\/conversations",
          "name": "30_60_s_academic_v0_1\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "30_60_s_academic_v0_1\/conversations\/from",
              "name": "30_60_s_academic_v0_1\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-30_60_s_academic_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "30_60_s_academic_v0_1\/conversations\/value",
              "name": "30_60_s_academic_v0_1\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-30_60_s_academic_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_academic_v0_1\/data_source",
          "name": "30_60_s_academic_v0_1\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_academic_v0_1"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_academic_v0_1\/video",
          "name": "30_60_s_academic_v0_1\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_academic_v0_1"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "30_60_s_nextqa_splits\/split_name"
      },
      "@id": "30_60_s_nextqa_splits",
      "name": "30_60_s_nextqa_splits",
      "description": "Splits for the 30_60_s_nextqa config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "30_60_s_nextqa_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "30_60_s_nextqa_splits\/split_name": "open_ended"
        },
        {
          "30_60_s_nextqa_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "30_60_s_nextqa",
      "name": "30_60_s_nextqa",
      "description": "lmms-lab\/LLaVA-Video-178K - '30_60_s_nextqa' subset\n\nAdditional information:\n- 2 splits: open_ended, multi_choice",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "30_60_s_nextqa\/split",
          "name": "30_60_s_nextqa\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_nextqa"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "30_60_s_nextqa\/(?:partial-)?(open_ended|multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "30_60_s_nextqa_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_nextqa\/id",
          "name": "30_60_s_nextqa\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_nextqa"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_nextqa\/conversations",
          "name": "30_60_s_nextqa\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "30_60_s_nextqa\/conversations\/from",
              "name": "30_60_s_nextqa\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-30_60_s_nextqa"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "30_60_s_nextqa\/conversations\/value",
              "name": "30_60_s_nextqa\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-30_60_s_nextqa"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_nextqa\/data_source",
          "name": "30_60_s_nextqa\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_nextqa"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_nextqa\/video",
          "name": "30_60_s_nextqa\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_nextqa"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "30_60_s_perceptiontest_splits\/split_name"
      },
      "@id": "30_60_s_perceptiontest_splits",
      "name": "30_60_s_perceptiontest_splits",
      "description": "Splits for the 30_60_s_perceptiontest config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "30_60_s_perceptiontest_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "30_60_s_perceptiontest_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "30_60_s_perceptiontest",
      "name": "30_60_s_perceptiontest",
      "description": "lmms-lab\/LLaVA-Video-178K - '30_60_s_perceptiontest' subset",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "30_60_s_perceptiontest\/split",
          "name": "30_60_s_perceptiontest\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_perceptiontest"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "30_60_s_perceptiontest\/(?:partial-)?(multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "30_60_s_perceptiontest_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_perceptiontest\/id",
          "name": "30_60_s_perceptiontest\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_perceptiontest"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_perceptiontest\/conversations",
          "name": "30_60_s_perceptiontest\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "30_60_s_perceptiontest\/conversations\/from",
              "name": "30_60_s_perceptiontest\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-30_60_s_perceptiontest"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "30_60_s_perceptiontest\/conversations\/value",
              "name": "30_60_s_perceptiontest\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-30_60_s_perceptiontest"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_perceptiontest\/data_source",
          "name": "30_60_s_perceptiontest\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_perceptiontest"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_perceptiontest\/video",
          "name": "30_60_s_perceptiontest\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_perceptiontest"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "30_60_s_youtube_v0_1_splits\/split_name"
      },
      "@id": "30_60_s_youtube_v0_1_splits",
      "name": "30_60_s_youtube_v0_1_splits",
      "description": "Splits for the 30_60_s_youtube_v0_1 config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "30_60_s_youtube_v0_1_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "30_60_s_youtube_v0_1_splits\/split_name": "caption"
        },
        {
          "30_60_s_youtube_v0_1_splits\/split_name": "open_ended"
        },
        {
          "30_60_s_youtube_v0_1_splits\/split_name": "multi_choice"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "30_60_s_youtube_v0_1",
      "name": "30_60_s_youtube_v0_1",
      "description": "lmms-lab\/LLaVA-Video-178K - '30_60_s_youtube_v0_1' subset\n\nAdditional information:\n- 3 splits: caption, open_ended, multi_choice",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "30_60_s_youtube_v0_1\/split",
          "name": "30_60_s_youtube_v0_1\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_youtube_v0_1"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "30_60_s_youtube_v0_1\/(?:partial-)?(caption|open_ended|multi_choice)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "30_60_s_youtube_v0_1_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_youtube_v0_1\/id",
          "name": "30_60_s_youtube_v0_1\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_youtube_v0_1"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_youtube_v0_1\/conversations",
          "name": "30_60_s_youtube_v0_1\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "30_60_s_youtube_v0_1\/conversations\/from",
              "name": "30_60_s_youtube_v0_1\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-30_60_s_youtube_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "30_60_s_youtube_v0_1\/conversations\/value",
              "name": "30_60_s_youtube_v0_1\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-30_60_s_youtube_v0_1"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_youtube_v0_1\/data_source",
          "name": "30_60_s_youtube_v0_1\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_youtube_v0_1"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "30_60_s_youtube_v0_1\/video",
          "name": "30_60_s_youtube_v0_1\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-30_60_s_youtube_v0_1"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "llava_hound_splits\/split_name"
      },
      "@id": "llava_hound_splits",
      "name": "llava_hound_splits",
      "description": "Splits for the llava_hound config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "llava_hound_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "llava_hound_splits\/split_name": "open_ended"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "llava_hound",
      "name": "llava_hound",
      "description": "lmms-lab\/LLaVA-Video-178K - 'llava_hound' subset",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "llava_hound\/split",
          "name": "llava_hound\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-llava_hound"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "llava_hound\/(?:partial-)?(open_ended)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "llava_hound_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "llava_hound\/id",
          "name": "llava_hound\/id",
          "description": "Column 'id' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-llava_hound"
            },
            "extract": {
              "column": "id"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "llava_hound\/conversations",
          "name": "llava_hound\/conversations",
          "description": "Column 'conversations' from the Hugging Face parquet file.",
          "subField": [
            {
              "@type": "cr:Field",
              "@id": "llava_hound\/conversations\/from",
              "name": "llava_hound\/conversations\/from",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-llava_hound"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "from"
                }
              }
            },
            {
              "@type": "cr:Field",
              "@id": "llava_hound\/conversations\/value",
              "name": "llava_hound\/conversations\/value",
              "description": "Column 'conversations' from the Hugging Face parquet file.",
              "dataType": "sc:Text",
              "source": {
                "fileSet": {
                  "@id": "parquet-files-for-config-llava_hound"
                },
                "extract": {
                  "column": "conversations"
                },
                "transform": {
                  "jsonPath": "value"
                }
              }
            }
          ],
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "llava_hound\/data_source",
          "name": "llava_hound\/data_source",
          "description": "Column 'data_source' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-llava_hound"
            },
            "extract": {
              "column": "data_source"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "llava_hound\/video",
          "name": "llava_hound\/video",
          "description": "Column 'video' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-llava_hound"
            },
            "extract": {
              "column": "video"
            }
          }
        }
      ]
    }
  ],
  "conformsTo": "http:\/\/mlcommons.org\/croissant\/1.0",
  "name": "LLaVA-Video-178K",
  "description": "\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for LLaVA-Video-178K\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tUses\n\t\n\nThis dataset is used for the training of the LLaVA-Video model. We only allow the use of this dataset for academic research and education purpose. For OpenAI GPT-4 generated data, we recommend the users to check the OpenAI Usage Policy.\n\n\t\n\t\t\n\t\n\t\n\t\tData Sources\n\t\n\nFor the training of LLaVA-Video, we utilized video-language data from five primary sources:\n\nLLaVA-Video-178K: This dataset includes 178,510 caption entries, 960… See the full description on the dataset page: https:\/\/huggingface.co\/datasets\/lmms-lab\/LLaVA-Video-178K.",
  "alternateName": [
    "lmms-lab\/LLaVA-Video-178K"
  ],
  "creator": {
    "@type": "Organization",
    "name": "LMMs-Lab",
    "url": "https:\/\/huggingface.co\/lmms-lab"
  },
  "keywords": [
    "visual-question-answering",
    "video-text-to-text",
    "English",
    "1M - 10M",
    "Text",
    "Video",
    "arxiv:2410.02713",
    "🇺🇸 Region: US",
    "video"
  ],
  "url": "https:\/\/huggingface.co\/datasets\/lmms-lab\/LLaVA-Video-178K"
}</script> 

		<title>lmms-lab/LLaVA-Video-178K · Datasets at Hugging Face</title>

		<script
			defer
			data-domain="huggingface.co"
			event-loggedIn="false"
			src="/js/script.pageview-props.js"
		></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script>
			window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer></script>
	</head>
	<body class="flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DatasetPage">
		<div class="flex min-h-dvh flex-col"><div class="SVELTE_HYDRATER contents" data-target="SystemThemeMonitor" data-props="{&quot;isLoggedIn&quot;:false}"></div>

	<div class="SVELTE_HYDRATER contents" data-target="MainHeader" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false,&quot;isPro&quot;:false}"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg">
				<span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a>
			<div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..."   spellcheck="false" type="text" value="">
	<svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
	</div>
			<div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg>
		</button>

	</div></div>
		<nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 2xl:space-x-2"><li class="hover:text-indigo-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
					Models</a>
			</li><li class="hover:text-red-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					Datasets</a>
			</li><li class="hover:text-blue-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg>
					Spaces</a>
			</li><li class="hover:text-yellow-700 max-xl:hidden"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 text-yellow-500!" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg>
					Posts</a>
			</li><li class="hover:text-yellow-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 16 16"><path d="m2.28 3.7-.3.16a.67.67 0 0 0-.34.58v8.73l.01.04.02.07.01.04.03.06.02.04.02.03.04.06.05.05.04.04.06.04.06.04.08.04.08.02h.05l.07.02h.11l.04-.01.07-.02.03-.01.07-.03.22-.12a5.33 5.33 0 0 1 5.15.1.67.67 0 0 0 .66 0 5.33 5.33 0 0 1 5.33 0 .67.67 0 0 0 1-.58V4.36a.67.67 0 0 0-.34-.5l-.3-.17v7.78a.63.63 0 0 1-.87.59 4.9 4.9 0 0 0-4.35.35l-.65.39a.29.29 0 0 1-.15.04.29.29 0 0 1-.16-.04l-.65-.4a4.9 4.9 0 0 0-4.34-.34.63.63 0 0 1-.87-.59V3.7Z" fill="currentColor" class="dark:opacity-40"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M8 3.1a5.99 5.99 0 0 0-5.3-.43.66.66 0 0 0-.42.62v8.18c0 .45.46.76.87.59a4.9 4.9 0 0 1 4.34.35l.65.39c.05.03.1.04.16.04.05 0 .1-.01.15-.04l.65-.4a4.9 4.9 0 0 1 4.35-.34.63.63 0 0 0 .86-.59V3.3a.67.67 0 0 0-.41-.62 5.99 5.99 0 0 0-5.3.43l-.3.17L8 3.1Zm.73 1.87a.43.43 0 1 0-.86 0v5.48a.43.43 0 0 0 .86 0V4.97Z" fill="currentColor" class="opacity-40 dark:opacity-100"></path><path d="M8.73 4.97a.43.43 0 1 0-.86 0v5.48a.43.43 0 1 0 .86 0V4.96Z" fill="currentColor" class="dark:opacity-40"></path></svg>
					Docs</a>
			</li><li class="hover:text-black dark:hover:text-white"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/enterprise"><svg class="mr-1.5 text-gray-400 group-hover:text-black dark:group-hover:text-white" xmlns="http://www.w3.org/2000/svg" fill="none" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 33 27"><path fill="currentColor" fill-rule="evenodd" d="M13.5.7a8.7 8.7 0 0 0-7.7 5.7L1 20.6c-1 3.1.9 5.7 4.1 5.7h15c3.3 0 6.8-2.6 7.8-5.7l4.6-14.2c1-3.1-.8-5.7-4-5.7h-15Zm1.1 5.7L9.8 20.3h9.8l1-3.1h-5.8l.8-2.5h4.8l1.1-3h-4.8l.8-2.3H23l1-3h-9.5Z" clip-rule="evenodd"></path></svg>
					Enterprise</a>
			</li>

		<li><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/pricing">Pricing
			</a></li>

		<li><div class="relative group">
	<button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button">
		<svg class=" text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>
			
		</button>
	
	
	</div></li>
		<li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li>
		<li><a class="block cursor-pointer whitespace-nowrap px-2 py-0.5 hover:text-gray-500 dark:text-gray-300 dark:hover:text-gray-100" href="/login">Log In
				</a></li>
			<li><a class="whitespace-nowrap rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up
					</a></li></ul></nav></div></header></div>
	
	
	
	<div class="SVELTE_HYDRATER contents" data-target="SSOBanner" data-props="{}"></div>
	
	

	<main class="flex flex-1 flex-col">
	<div class="SVELTE_HYDRATER contents" data-target="DatasetHeader" data-props="{&quot;activeTab&quot;:&quot;datasetCard&quot;,&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62d3f7d84b0933c48f3cdd9c/mCBOS7IodEEgBweo19hU9.png&quot;,&quot;fullname&quot;:&quot;LMMs-Lab&quot;,&quot;name&quot;:&quot;lmms-lab&quot;,&quot;type&quot;:&quot;org&quot;,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;isEnterprise&quot;:false,&quot;followerCount&quot;:371},&quot;canReadRepoSettings&quot;:false,&quot;dataset&quot;:{&quot;author&quot;:&quot;lmms-lab&quot;,&quot;cardData&quot;:{&quot;configs&quot;:[{&quot;config_name&quot;:&quot;0_30_s_academic_v0_1&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;caption&quot;,&quot;path&quot;:&quot;0_30_s_academic_v0_1/*cap*.json&quot;},{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;0_30_s_academic_v0_1/*oe*.json&quot;},{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;0_30_s_academic_v0_1/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;0_30_s_youtube_v0_1&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;caption&quot;,&quot;path&quot;:&quot;0_30_s_youtube_v0_1/*cap*.json&quot;},{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;0_30_s_youtube_v0_1/*oe*.json&quot;},{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;0_30_s_youtube_v0_1/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;0_30_s_activitynet&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;0_30_s_activitynet/*oe*.json&quot;}]},{&quot;config_name&quot;:&quot;0_30_s_perceptiontest&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;0_30_s_perceptiontest/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;0_30_s_nextqa&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;0_30_s_nextqa/*oe*.json&quot;},{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;0_30_s_nextqa/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;30_60_s_academic_v0_1&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;caption&quot;,&quot;path&quot;:&quot;30_60_s_academic_v0_1/*cap*.json&quot;},{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;30_60_s_academic_v0_1/*oe*.json&quot;},{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;30_60_s_academic_v0_1/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;30_60_s_youtube_v0_1&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;caption&quot;,&quot;path&quot;:&quot;30_60_s_youtube_v0_1/*cap*.json&quot;},{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;30_60_s_youtube_v0_1/*oe*.json&quot;},{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;30_60_s_youtube_v0_1/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;30_60_s_activitynet&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;30_60_s_activitynet/*oe*.json&quot;}]},{&quot;config_name&quot;:&quot;30_60_s_perceptiontest&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;30_60_s_perceptiontest/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;30_60_s_nextqa&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;30_60_s_nextqa/*oe*.json&quot;},{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;30_60_s_nextqa/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;1_2_m_youtube_v0_1&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;caption&quot;,&quot;path&quot;:&quot;1_2_m_youtube_v0_1/*cap*.json&quot;},{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;1_2_m_youtube_v0_1/*oe*.json&quot;},{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;1_2_m_youtube_v0_1/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;1_2_m_academic_v0_1&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;caption&quot;,&quot;path&quot;:&quot;1_2_m_academic_v0_1/*cap*.json&quot;},{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;1_2_m_academic_v0_1/*oe*.json&quot;},{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;1_2_m_academic_v0_1/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;1_2_m_activitynet&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;1_2_m_activitynet/*oe*.json&quot;}]},{&quot;config_name&quot;:&quot;1_2_m_nextqa&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;1_2_m_nextqa/*oe*.json&quot;},{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;1_2_m_nextqa/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;2_3_m_youtube_v0_1&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;caption&quot;,&quot;path&quot;:&quot;2_3_m_youtube_v0_1/*cap*.json&quot;},{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;2_3_m_youtube_v0_1/*oe*.json&quot;},{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;2_3_m_youtube_v0_1/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;2_3_m_academic_v0_1&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;caption&quot;,&quot;path&quot;:&quot;2_3_m_academic_v0_1/*cap*.json&quot;},{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;2_3_m_academic_v0_1/*oe*.json&quot;},{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;2_3_m_academic_v0_1/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;2_3_m_activitynet&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;2_3_m_activitynet/*oe*.json&quot;}]},{&quot;config_name&quot;:&quot;2_3_m_nextqa&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;2_3_m_nextqa/*oe*.json&quot;},{&quot;split&quot;:&quot;multi_choice&quot;,&quot;path&quot;:&quot;2_3_m_nextqa/*mc*.json&quot;}]},{&quot;config_name&quot;:&quot;llava_hound&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;open_ended&quot;,&quot;path&quot;:&quot;llava_hound/sharegptvideo_qa_255k_processed.json&quot;}]}],&quot;language&quot;:[&quot;en&quot;],&quot;task_categories&quot;:[&quot;visual-question-answering&quot;,&quot;video-text-to-text&quot;],&quot;tags&quot;:[&quot;video&quot;]},&quot;cardExists&quot;:true,&quot;createdAt&quot;:&quot;2024-08-27T07:09:50.000Z&quot;,&quot;description&quot;:&quot;\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for LLaVA-Video-178K\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tUses\n\t\n\nThis dataset is used for the training of the LLaVA-Video model. We only allow the use of this dataset for academic research and education purpose. For OpenAI GPT-4 generated data, we recommend the users to check the OpenAI Usage Policy.\n\n\t\n\t\t\n\t\n\t\n\t\tData Sources\n\t\n\nFor the training of LLaVA-Video, we utilized video-language data from five primary sources:\n\nLLaVA-Video-178K: This dataset includes 178,510 caption entries, 960… See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K.&quot;,&quot;downloads&quot;:19454,&quot;downloadsAllTime&quot;:130208,&quot;id&quot;:&quot;lmms-lab/LLaVA-Video-178K&quot;,&quot;isLikedByUser&quot;:false,&quot;lastModified&quot;:&quot;2024-10-11T04:59:25.000Z&quot;,&quot;likes&quot;:124,&quot;datasetsServerInfo&quot;:{&quot;viewer&quot;:&quot;viewer&quot;,&quot;numRows&quot;:1627017,&quot;libraries&quot;:[],&quot;formats&quot;:[],&quot;modalities&quot;:[&quot;text&quot;]},&quot;discussionsDisabled&quot;:false,&quot;repoType&quot;:&quot;dataset&quot;,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;tags&quot;:[&quot;task_categories:visual-question-answering&quot;,&quot;task_categories:video-text-to-text&quot;,&quot;language:en&quot;,&quot;size_categories:1M<n<10M&quot;,&quot;modality:text&quot;,&quot;modality:video&quot;,&quot;arxiv:2410.02713&quot;,&quot;region:us&quot;,&quot;video&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;task_categories:visual-question-answering&quot;,&quot;label&quot;:&quot;visual-question-answering&quot;,&quot;type&quot;:&quot;task_categories&quot;,&quot;subType&quot;:&quot;multimodal&quot;},{&quot;id&quot;:&quot;task_categories:video-text-to-text&quot;,&quot;label&quot;:&quot;video-text-to-text&quot;,&quot;type&quot;:&quot;task_categories&quot;,&quot;subType&quot;:&quot;multimodal&quot;},{&quot;id&quot;:&quot;language:en&quot;,&quot;label&quot;:&quot;English&quot;,&quot;type&quot;:&quot;language&quot;},{&quot;id&quot;:&quot;size_categories:1M<n<10M&quot;,&quot;label&quot;:&quot;1M - 10M&quot;,&quot;type&quot;:&quot;size_categories&quot;},{&quot;id&quot;:&quot;modality:text&quot;,&quot;label&quot;:&quot;Text&quot;,&quot;type&quot;:&quot;modality&quot;},{&quot;id&quot;:&quot;modality:video&quot;,&quot;label&quot;:&quot;Video&quot;,&quot;type&quot;:&quot;modality&quot;},{&quot;id&quot;:&quot;arxiv:2410.02713&quot;,&quot;label&quot;:&quot;arxiv:2410.02713&quot;,&quot;type&quot;:&quot;arxiv&quot;,&quot;extra&quot;:{&quot;paperTitle&quot;:&quot;Video Instruction Tuning With Synthetic Data&quot;}},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;🇺🇸 Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;},{&quot;id&quot;:&quot;video&quot;,&quot;label&quot;:&quot;video&quot;,&quot;type&quot;:&quot;other&quot;}],&quot;hasBlockedOids&quot;:false,&quot;region&quot;:&quot;us&quot;,&quot;xetEnabled&quot;:false},&quot;discussionsStats&quot;:{&quot;closed&quot;:2,&quot;open&quot;:6,&quot;total&quot;:8}}"><header class="bg-linear-to-t border-b border-gray-100 pt-6 sm:pt-9 from-gray-50-to-white via-white dark:via-gray-950"><div class="container relative "><h1 class="flex flex-wrap items-center max-md:leading-tight mb-3 text-lg max-sm:gap-y-1.5 md:text-xl"><a href="/datasets" class="group flex items-center"><svg class="sm:mr-1.5 -mr-1 text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					<span class="mr-2.5 font-semibold text-gray-400 group-hover:text-gray-500 max-sm:hidden">Datasets:</span></a>
				<hr class="mx-1.5 h-2 translate-y-px rounded-sm border-r dark:border-gray-600 sm:hidden">
			<div class="group flex flex-none items-center"><div class="relative mr-1 flex items-center">

			

<span class="inline-block "><span class="contents"><a href="/lmms-lab" class="text-gray-400 hover:text-blue-600"><img alt="" class="w-3.5 h-3.5 rounded-sm  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/62d3f7d84b0933c48f3cdd9c/mCBOS7IodEEgBweo19hU9.png" crossorigin="anonymous"></a></span>
	</span></div>
		

<span class="inline-block "><span class="contents"><a href="/lmms-lab" class="text-gray-400 hover:text-blue-600">lmms-lab</a></span>
	</span>
		<div class="mx-0.5 text-gray-300">/</div></div>

<div class="max-w-full "><a class="break-words font-mono font-semibold hover:text-blue-600 " href="/datasets/lmms-lab/LLaVA-Video-178K">LLaVA-Video-178K</a>
	<button class="relative text-sm mr-4 focus:outline-hidden inline-flex cursor-pointer items-center text-sm  mx-0.5   text-gray-600 " title="Copy dataset name to clipboard" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg>
	
	</button></div>
			<div class="inline-flex items-center overflow-hidden whitespace-nowrap rounded-md border bg-white text-sm leading-none text-gray-500  mr-2"><button class="relative flex items-center overflow-hidden from-red-50 to-transparent dark:from-red-900 px-1.5 py-1 hover:bg-linear-to-t focus:outline-hidden"  title="Like"><svg class="left-1.5 absolute" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>

		
		<span class="ml-4 pl-0.5 ">like</span></button>
	<button class="focus:outline-hidden flex items-center border-l px-1.5 py-1 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="See users who liked this repository">124</button></div>




			<div class="relative flex items-center gap-1.5  "><div class="mr-2 inline-flex h-6 items-center overflow-hidden whitespace-nowrap rounded-md border text-sm text-gray-500"><button class="focus:outline-hidden relative flex h-full max-w-56 items-center gap-1.5 overflow-hidden px-1.5 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" type="button" ><div class="flex h-full flex-1 items-center justify-center ">Follow</div>
		<img alt="" class="rounded-xs size-3 flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/62d3f7d84b0933c48f3cdd9c/mCBOS7IodEEgBweo19hU9.png">
		<span class="truncate">LMMs-Lab</span></button>
	<button class="focus:outline-hidden flex h-full items-center border-l pl-1.5 pr-1.5 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="Show LMMs-Lab's followers" type="button">371</button></div>

		</div>
			
	</h1>
		<div class="mb-3 flex flex-wrap md:mb-4"><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Tasks:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?task_categories=task_categories%3Avisual-question-answering"><div class="tag tag-white   "><div class="tag-ico -ml-2 tag-ico-red"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" width="1em" height="1em" viewBox="0 0 32 32" class=""><path fill="currentColor" d="M10 16a5.99 5.99 0 0 0 9.471 4.885L24.586 26L26 24.586l-5.115-5.115A5.997 5.997 0 1 0 10 16m2 0a4 4 0 1 1 4 4a4.005 4.005 0 0 1-4-4"></path><path fill="currentColor" d="M29 7h-6.46l-1.71-2.55A1 1 0 0 0 20 4h-8a1 1 0 0 0-.83.45L9.46 7H3a1.003 1.003 0 0 0-1 1v17a1.003 1.003 0 0 0 1 1h9v-2H4V9h6a1 1 0 0 0 .83-.45L12.54 6h6.92l1.71 2.55A1 1 0 0 0 22 9h6v12h2V8a1.003 1.003 0 0 0-1-1"></path></svg></div>

	

	<span>Visual Question Answering</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?task_categories=task_categories%3Avideo-text-to-text"><div class="tag tag-white   "><div class="tag-ico -ml-2 tag-ico-blue"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 32 32" aria-hidden="true" class=""><path fill="currentColor" d="m30 19l-4 3.2V20a2 2 0 0 0-2-2h-8a2 2 0 0 0-2 2v6a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2v-2.2l4 3.2Zm-14 7v-6h8l.002 6Z"></path><path fill="currentColor" d="M12 28H8V4h8v6a2.006 2.006 0 0 0 2 2h6v3h2v-5a.91.91 0 0 0-.3-.7l-7-7A.9.9 0 0 0 18 2H8a2.006 2.006 0 0 0-2 2v24a2.006 2.006 0 0 0 2 2h4Zm6-23.6l5.6 5.6H18Z"></path></svg></div>

	

	<span>Video-Text-to-Text</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Modalities:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?modality=modality%3Atext"><div class="tag tag-white   ">
		<svg class="text-red-700 dark:text-red-600" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.619 4.619C2.667 6.573 2.667 9.715 2.667 16s0 9.428 1.952 11.38C6.573 29.333 9.715 29.333 16 29.333s9.428 0 11.38-1.953c1.953-1.95 1.953-5.095 1.953-11.38s0-9.428-1.953-11.381C25.43 2.667 22.285 2.667 16 2.667s-9.428 0-11.381 1.952m8.65 3.714c-.573 0-1.109 0-1.546.066-.495.073-1.003.248-1.41.7-.392.436-.53.956-.59 1.452-.056.464-.056 1.04-.056 1.689V13a1 1 0 1 0 2 0v-.704c0-.724.001-1.176.041-1.505q.015-.15.061-.294a.2.2 0 0 1 .031-.061q0-.003.016-.01a.8.8 0 0 1 .203-.05c.272-.04.654-.043 1.314-.043H15v11.334h-2.333a1 1 0 1 0 0 2H20a1 1 0 0 0 0-2h-3V10.333h1.667c.66 0 1.042.003 1.314.043.123.019.18.04.203.05l.015.009a.2.2 0 0 1 .032.061c.018.05.042.14.061.295.04.329.041.781.041 1.506V13a1 1 0 1 0 2 0v-.76c0-.65 0-1.225-.056-1.69-.06-.495-.198-1.015-.59-1.453-.407-.45-.915-.625-1.41-.698-.437-.067-.973-.067-1.546-.066z" fill="currentColor"></path></svg>

	

	<span>Text</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?modality=modality%3Avideo"><div class="tag tag-white   ">
		<svg class="text-indigo-800" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M2.667 16c0-6.285 0-9.427 1.952-11.381C6.572 2.667 9.715 2.667 16 2.667s9.43 0 11.38 1.952C29.333 6.572 29.333 9.715 29.333 16s0 9.43-1.953 11.38c-1.952 1.953-5.095 1.953-11.38 1.953s-9.427 0-11.381-1.953C2.667 25.428 2.667 22.285 2.667 16M9 16.375v-.75c0-2.466 0-3.697.681-4.528a3 3 0 0 1 .416-.416C10.927 10 12.16 10 14.625 10c2.466 0 3.698 0 4.529.681a3 3 0 0 1 .415.416c.527.64.646 1.523.673 3.028l.502-.247h.002c1.457-.73 2.187-1.095 2.72-.765S24 14.26 24 15.891v.219c0 1.631 0 2.447-.533 2.777-.534.33-1.263-.035-2.719-.762l-.004-.003-.502-.247c-.027 1.505-.146 2.387-.673 3.029a3 3 0 0 1-.415.415c-.83.681-2.064.681-4.529.681-2.466 0-3.697 0-4.528-.681a3 3 0 0 1-.416-.415C9 20.074 9 18.84 9 16.375" fill="currentColor"></path></svg>

	

	<span>Video</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Languages:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?language=language%3Aen"><div class="tag tag-white   ">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="text-green-600/80" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 10 10"><path fill-rule="evenodd" clip-rule="evenodd" d="M0.625 5C0.625 6.16032 1.08594 7.27312 1.90641 8.09359C2.72688 8.91406 3.83968 9.375 5 9.375C6.16032 9.375 7.27312 8.91406 8.09359 8.09359C8.91406 7.27312 9.375 6.16032 9.375 5C9.375 3.83968 8.91406 2.72688 8.09359 1.90641C7.27312 1.08594 6.16032 0.625 5 0.625C3.83968 0.625 2.72688 1.08594 1.90641 1.90641C1.08594 2.72688 0.625 3.83968 0.625 5ZM7.64365 7.48027C7.61734 7.50832 7.59054 7.53598 7.56326 7.56326C7.13828 7.98824 6.61864 8.2968 6.0539 8.46842C6.29802 8.11949 6.49498 7.64804 6.63475 7.09483C7.00845 7.18834 7.35014 7.3187 7.64365 7.48027ZM8.10076 6.87776C8.37677 6.42196 8.55005 5.90894 8.60556 5.37499H6.86808C6.85542 5.71597 6.82551 6.04557 6.77971 6.35841C7.25309 6.47355 7.68808 6.6414 8.062 6.85549C8.07497 6.86283 8.08789 6.87025 8.10076 6.87776ZM6.03795 6.22536C6.07708 5.95737 6.1044 5.67232 6.11705 5.37499H3.88295C3.89666 5.69742 3.92764 6.00542 3.9722 6.29287C4.37075 6.21726 4.79213 6.17749 5.224 6.17749C5.50054 6.17749 5.77294 6.19376 6.03795 6.22536ZM4.1261 7.02673C4.34894 7.84835 4.68681 8.375 5 8.375C5.32122 8.375 5.66839 7.82101 5.8908 6.963C5.67389 6.93928 5.45082 6.92699 5.224 6.92699C4.84316 6.92699 4.47332 6.96176 4.1261 7.02673ZM3.39783 7.21853C3.53498 7.71842 3.72038 8.14579 3.9461 8.46842C3.42141 8.30898 2.93566 8.03132 2.52857 7.65192C2.77253 7.48017 3.06711 7.33382 3.39783 7.21853ZM3.23916 6.48077C3.18263 6.13193 3.14625 5.76074 3.13192 5.37499H1.39444C1.4585 5.99112 1.67936 6.57938 2.03393 7.08403C2.3706 6.83531 2.78055 6.63162 3.23916 6.48077ZM1.39444 4.62499H3.13192C3.14615 4.24204 3.18211 3.87344 3.23794 3.52681C2.77814 3.37545 2.36731 3.17096 2.03024 2.92123C1.67783 3.42469 1.45828 4.011 1.39444 4.62499ZM2.5237 2.35262C2.76812 2.52552 3.06373 2.67281 3.39584 2.78875C3.53318 2.28573 3.71928 1.85578 3.9461 1.53158C3.41932 1.69166 2.93178 1.97089 2.5237 2.35262ZM3.97101 3.71489C3.92709 4.00012 3.89654 4.30547 3.88295 4.62499H6.11705C6.10453 4.33057 6.07761 4.04818 6.03909 3.78248C5.77372 3.81417 5.50093 3.83049 5.224 3.83049C4.79169 3.83049 4.3699 3.79065 3.97101 3.71489ZM5.8928 3.04476C5.67527 3.06863 5.45151 3.08099 5.224 3.08099C4.84241 3.08099 4.47186 3.04609 4.12405 2.98086C4.34686 2.1549 4.68584 1.625 5 1.625C5.32218 1.625 5.67048 2.18233 5.8928 3.04476ZM6.78083 3.6493C6.826 3.95984 6.85552 4.28682 6.86808 4.62499H8.60556C8.55029 4.09337 8.37827 3.58251 8.10436 3.1282C8.0903 3.1364 8.07618 3.14449 8.062 3.15249C7.68838 3.36641 7.25378 3.53417 6.78083 3.6493ZM7.64858 2.52499C7.35446 2.68754 7.0117 2.81868 6.63664 2.91268C6.49676 2.35623 6.29913 1.88209 6.0539 1.53158C6.61864 1.7032 7.13828 2.01176 7.56326 2.43674C7.59224 2.46572 7.62068 2.49514 7.64858 2.52499Z" fill="currentColor"></path></svg>

	

	<span>English</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Size:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?size_categories=size_categories%3A1M%3Cn%3C10M"><div class="tag tag-white   ">

	

	<span>1M - 10M</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">ArXiv:
	</span>
	

	<div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-full rounded-br-none " type="button">
		<div class="tag tag-white rounded-full  relative rounded-br-none pr-2.5">
		<svg class="-mr-1 text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet" fill="none"><path fill="currentColor" fill-rule="evenodd" d="M8.007 1.814a1.176 1.176 0 0 0-.732-.266H3.088c-.64 0-1.153.512-1.153 1.152v6.803c0 .64.513 1.152 1.153 1.152h5.54c.632 0 1.144-.511 1.144-1.152V3.816c0-.338-.137-.658-.412-.887L8.007 1.814Zm-1.875 1.81c0 .695.55 1.253 1.244 1.253h.983a.567.567 0 0 1 .553.585v4.041c0 .165-.119.302-.283.302h-5.55c-.156 0-.275-.137-.275-.302V2.7a.284.284 0 0 1 .284-.301h2.468a.574.574 0 0 1 .434.19.567.567 0 0 1 .142.395v.64Z" clip-rule="evenodd"></path><path fill="currentColor" fill-opacity=".2" fill-rule="evenodd" d="M6.132 3.624c0 .695.55 1.253 1.244 1.253h.97a.567.567 0 0 1 .566.585v4.041c0 .165-.119.302-.283.302h-5.55c-.156 0-.275-.137-.275-.302V2.7a.284.284 0 0 1 .284-.301h2.468a.567.567 0 0 1 .576.585v.64Z" clip-rule="evenodd"></path></svg>

	

	<span class="-mr-2 text-gray-400">arxiv:</span>
		<span>2410.02713</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Tags:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?other=video"><div class="tag tag-white   ">

	

	<span>video</span>
	

	</div></a>

	</div></div>

		<div class="flex flex-col-reverse lg:flex-row lg:items-center lg:justify-between"><div class="-mb-px flex h-12 items-center overflow-x-auto overflow-y-hidden ">
	<a class="tab-alternate active" href="/datasets/lmms-lab/LLaVA-Video-178K"><svg class="mr-1.5 text-gray-400 flex-none" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
	Dataset card
	

	
		</a><a class="tab-alternate" href="/datasets/lmms-lab/LLaVA-Video-178K/viewer/"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	Data Studio
	

	
		</a><a class="tab-alternate" href="/datasets/lmms-lab/LLaVA-Video-178K/tree/main"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M21 19h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-8h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0 4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M9 19a1 1 0 0 1-1-1V6a1 1 0 0 1 2 0v12a1 1 0 0 1-1 1zm-6-4.333a1 1 0 0 1-.64-1.769L3.438 12l-1.078-.898a1 1 0 0 1 1.28-1.538l2 1.667a1 1 0 0 1 0 1.538l-2 1.667a.999.999 0 0 1-.64.231z" fill="currentColor"></path></svg>
	<span class="xl:hidden">Files</span>
		<span class="hidden xl:inline">Files and versions</span>
	

	
		</a><a class="tab-alternate" href="/datasets/lmms-lab/LLaVA-Video-178K/discussions"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M20.6081 3C21.7684 3 22.8053 3.49196 23.5284 4.38415C23.9756 4.93678 24.4428 5.82749 24.4808 7.16133C24.9674 7.01707 25.4353 6.93643 25.8725 6.93643C26.9833 6.93643 27.9865 7.37587 28.696 8.17411C29.6075 9.19872 30.0124 10.4579 29.8361 11.7177C29.7523 12.3177 29.5581 12.8555 29.2678 13.3534C29.8798 13.8646 30.3306 14.5763 30.5485 15.4322C30.719 16.1032 30.8939 17.5006 29.9808 18.9403C30.0389 19.0342 30.0934 19.1319 30.1442 19.2318C30.6932 20.3074 30.7283 21.5229 30.2439 22.6548C29.5093 24.3704 27.6841 25.7219 24.1397 27.1727C21.9347 28.0753 19.9174 28.6523 19.8994 28.6575C16.9842 29.4379 14.3477 29.8345 12.0653 29.8345C7.87017 29.8345 4.8668 28.508 3.13831 25.8921C0.356375 21.6797 0.754104 17.8269 4.35369 14.1131C6.34591 12.058 7.67023 9.02782 7.94613 8.36275C8.50224 6.39343 9.97271 4.20438 12.4172 4.20438H12.4179C12.6236 4.20438 12.8314 4.2214 13.0364 4.25468C14.107 4.42854 15.0428 5.06476 15.7115 6.02205C16.4331 5.09583 17.134 4.359 17.7682 3.94323C18.7242 3.31737 19.6794 3 20.6081 3ZM20.6081 5.95917C20.2427 5.95917 19.7963 6.1197 19.3039 6.44225C17.7754 7.44319 14.8258 12.6772 13.7458 14.7131C13.3839 15.3952 12.7655 15.6837 12.2086 15.6837C11.1036 15.6837 10.2408 14.5497 12.1076 13.1085C14.9146 10.9402 13.9299 7.39584 12.5898 7.1776C12.5311 7.16799 12.4731 7.16355 12.4172 7.16355C11.1989 7.16355 10.6615 9.33114 10.6615 9.33114C10.6615 9.33114 9.0863 13.4148 6.38031 16.206C3.67434 18.998 3.5346 21.2388 5.50675 24.2246C6.85185 26.2606 9.42666 26.8753 12.0653 26.8753C14.8021 26.8753 17.6077 26.2139 19.1799 25.793C19.2574 25.7723 28.8193 22.984 27.6081 20.6107C27.4046 20.212 27.0693 20.0522 26.6471 20.0522C24.9416 20.0522 21.8393 22.6726 20.5057 22.6726C20.2076 22.6726 19.9976 22.5416 19.9116 22.222C19.3433 20.1173 28.552 19.2325 27.7758 16.1839C27.639 15.6445 27.2677 15.4256 26.746 15.4263C24.4923 15.4263 19.4358 19.5181 18.3759 19.5181C18.2949 19.5181 18.2368 19.4937 18.2053 19.4419C17.6743 18.557 17.9653 17.9394 21.7082 15.6009C25.4511 13.2617 28.0783 11.8545 26.5841 10.1752C26.4121 9.98141 26.1684 9.8956 25.8725 9.8956C23.6001 9.89634 18.2311 14.9403 18.2311 14.9403C18.2311 14.9403 16.7821 16.496 15.9057 16.496C15.7043 16.496 15.533 16.4139 15.4169 16.2112C14.7956 15.1296 21.1879 10.1286 21.5484 8.06535C21.7928 6.66715 21.3771 5.95917 20.6081 5.95917Z" fill="#FF9D00"></path><path d="M5.50686 24.2246C3.53472 21.2387 3.67446 18.9979 6.38043 16.206C9.08641 13.4147 10.6615 9.33111 10.6615 9.33111C10.6615 9.33111 11.2499 6.95933 12.59 7.17757C13.93 7.39581 14.9139 10.9401 12.1069 13.1084C9.29997 15.276 12.6659 16.7489 13.7459 14.713C14.8258 12.6772 17.7747 7.44316 19.304 6.44221C20.8326 5.44128 21.9089 6.00204 21.5484 8.06532C21.188 10.1286 14.795 15.1295 15.4171 16.2118C16.0391 17.2934 18.2312 14.9402 18.2312 14.9402C18.2312 14.9402 25.0907 8.49588 26.5842 10.1752C28.0776 11.8545 25.4512 13.2616 21.7082 15.6008C17.9646 17.9393 17.6744 18.557 18.2054 19.4418C18.7372 20.3266 26.9998 13.1351 27.7759 16.1838C28.5513 19.2324 19.3434 20.1173 19.9117 22.2219C20.48 24.3274 26.3979 18.2382 27.6082 20.6107C28.8193 22.9839 19.2574 25.7722 19.18 25.7929C16.0914 26.62 8.24723 28.3726 5.50686 24.2246Z" fill="#FFD21E"></path></svg>
	Community
	<div class="ml-1.5 flex h-4 min-w-[1rem] items-center justify-center rounded px-1 text-xs leading-none shadow-sm bg-black text-white dark:bg-gray-800 dark:text-gray-200">8</div>

	
		</a></div>
	
			</div></div></header>


</div>
	
<div class="container relative flex flex-col md:grid md:space-y-0 w-full md:grid-cols-12 md:flex-1 md:grid-rows-full space-y-4 md:gap-6 ">
		<section class="pt-6 border-gray-100 md:col-span-8 pb-24 relative break-words copiable-code-container">
				<div class="SVELTE_HYDRATER contents" data-target="UnsafeBanner" data-props="{&quot;classNames&quot;:&quot;mb-4&quot;,&quot;repoId&quot;:&quot;lmms-lab/LLaVA-Video-178K&quot;,&quot;repoType&quot;:&quot;dataset&quot;,&quot;minLevel&quot;:&quot;unsafe&quot;}"></div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetViewer" data-props="{&quot;data&quot;:{&quot;kind&quot;:&quot;DatasetAndSampleData&quot;,&quot;datasetInfo&quot;:[{&quot;isValid&quot;:true,&quot;href&quot;:&quot;/datasets/lmms-lab/LLaVA-Video-178K/tree/refs%2Fconvert%2Fparquet/&quot;,&quot;label&quot;:&quot;Size of the auto-converted Parquet files:&quot;,&quot;value&quot;:&quot;645 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Number of rows:&quot;,&quot;value&quot;:&quot;1,627,017&quot;}],&quot;partial&quot;:false,&quot;configsData&quot;:{&quot;configInfos&quot;:[{&quot;name&quot;:&quot;0_30_s_academic_v0_1&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:66206},{&quot;name&quot;:&quot;0_30_s_activitynet&quot;,&quot;status&quot;:&quot;ok&quot;},{&quot;name&quot;:&quot;0_30_s_nextqa&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:10988},{&quot;name&quot;:&quot;0_30_s_perceptiontest&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:1785},{&quot;name&quot;:&quot;0_30_s_youtube_v0_1&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:538899},{&quot;name&quot;:&quot;1_2_m_academic_v0_1&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:35170},{&quot;name&quot;:&quot;1_2_m_activitynet&quot;,&quot;status&quot;:&quot;ok&quot;},{&quot;name&quot;:&quot;1_2_m_nextqa&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:9327},{&quot;name&quot;:&quot;1_2_m_youtube_v0_1&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:201589},{&quot;name&quot;:&quot;2_3_m_academic_v0_1&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:28159},{&quot;name&quot;:&quot;2_3_m_activitynet&quot;,&quot;status&quot;:&quot;ok&quot;},{&quot;name&quot;:&quot;2_3_m_nextqa&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:113},{&quot;name&quot;:&quot;2_3_m_youtube_v0_1&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:206147},{&quot;name&quot;:&quot;30_60_s_academic_v0_1&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:88784},{&quot;name&quot;:&quot;30_60_s_activitynet&quot;,&quot;status&quot;:&quot;ok&quot;},{&quot;name&quot;:&quot;30_60_s_nextqa&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:13686},{&quot;name&quot;:&quot;30_60_s_perceptiontest&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:618},{&quot;name&quot;:&quot;30_60_s_youtube_v0_1&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:170546},{&quot;name&quot;:&quot;llava_hound&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:255000}],&quot;selectedConfig&quot;:&quot;0_30_s_academic_v0_1&quot;,&quot;hasSelectedConfigParquet&quot;:true},&quot;splitsData&quot;:{&quot;splitInfos&quot;:[{&quot;name&quot;:&quot;caption&quot;,&quot;numRows&quot;:11985},{&quot;name&quot;:&quot;open_ended&quot;,&quot;numRows&quot;:48468},{&quot;name&quot;:&quot;multi_choice&quot;,&quot;numRows&quot;:5753}],&quot;selectedSplit&quot;:&quot;caption&quot;},&quot;sampleData&quot;:{&quot;dataset&quot;:&quot;lmms-lab/LLaVA-Video-178K&quot;,&quot;config&quot;:&quot;0_30_s_academic_v0_1&quot;,&quot;split&quot;:&quot;caption&quot;,&quot;capabilities&quot;:{&quot;rows&quot;:true,&quot;search&quot;:true,&quot;filter&quot;:true,&quot;statistics&quot;:true},&quot;navigation&quot;:{&quot;p&quot;:0},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkxODY2MCwic3ViIjoiL2RhdGFzZXRzL2xtbXMtbGFiL0xMYVZBLVZpZGVvLTE3OEsiLCJleHAiOjE3NDI5MjIyNjAsImlzcyI6Imh0dHBzOi8vaHVnZ2luZ2ZhY2UuY28ifQ.b80r_ySo2TXwJ6x9XODb4B4lZGa3DX4edigO9KPx9R6XneD3DNiXWYF0n9bN6t7Kl-Nm2eceA95Rq44JfyqADg&quot;,&quot;sampleData&quot;:{&quot;columns&quot;:[{&quot;name&quot;:&quot;id&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;id&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:5,&quot;max&quot;:36,&quot;mean&quot;:16.00676,&quot;median&quot;:23,&quot;std&quot;:8.19116,&quot;histogram&quot;:{&quot;hist&quot;:[3682,0,2263,0,5860,56,0,124],&quot;bin_edges&quot;:[5,9,13,17,21,25,29,33,36]}}}},{&quot;name&quot;:&quot;conversations&quot;,&quot;align&quot;:&quot;left&quot;,&quot;type&quot;:&quot;list&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;conversations&quot;,&quot;column_type&quot;:&quot;list&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:2,&quot;max&quot;:2,&quot;mean&quot;:2,&quot;median&quot;:2,&quot;std&quot;:0,&quot;histogram&quot;:{&quot;hist&quot;:[11985],&quot;bin_edges&quot;:[2,2]}}}},{&quot;name&quot;:&quot;data_source&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;data_source&quot;,&quot;column_type&quot;:&quot;string_label&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;no_label_count&quot;:0,&quot;no_label_proportion&quot;:0,&quot;n_unique&quot;:1,&quot;frequencies&quot;:{&quot;0_30_s_academic_v0_1&quot;:11985}}}},{&quot;name&quot;:&quot;video&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;video&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:34,&quot;max&quot;:62,&quot;mean&quot;:44.97547,&quot;median&quot;:52,&quot;std&quot;:8.15865,&quot;histogram&quot;:{&quot;hist&quot;:[3682,0,1284,979,0,0,5860,0,56,124],&quot;bin_edges&quot;:[34,37,40,43,46,49,52,55,58,61,62]}}}}],&quot;rows&quot;:[{&quot;rowIdx&quot;:0,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;028CE&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nAnalyze the video, focusing on its cinematography and narrative structure.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a view of a garage or basement, characterized by a white door in the background. On the floor, there is a vacuum cleaner and a small box with a decorative pattern. A person dressed in black clothing and black shoes enters the frame and approaches the vacuum cleaner. The person squats down and starts to interact with the vacuum cleaner, adjusting or inspecting it. The scene remains focused on this interaction. The narrative continues in the same setting, with the white door still visible in the background. The person, still dressed in black, is initially squatting and interacting with the vacuum cleaner. They then stand up and step onto the small decorative box. After a brief moment standing on the box, the person steps off and exits the frame, leaving the vacuum cleaner and the box in their original positions. The video concludes with the garage or basement empty, showcasing the white door, the vacuum cleaner, and the small box on the floor.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nAnalyze the video, focusing on its cinematography and narrative structure.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a view of a garage or basement, characterized by a white door in the background. On the floor, there is a vacuum cleaner and a small box with a decorative pattern. A person dressed in black clothing and black shoes enters the frame and approaches the vacuum cleaner. The person squats down and starts to interact with the vacuum cleaner, adjusting or inspecting it. The scene remains focused on this interaction. The narrative continues in the same setting, with the white door still visible in the background. The person, still dressed in black, is initially squatting and interacting with the vacuum cleaner. They then stand up and step onto the small decorative box. After a brief moment standing on the box, the person steps off and exits the frame, leaving the vacuum cleaner and the box in their original positions. The video concludes with the garage or basement empty, showcasing the white door, the vacuum cleaner, and the small box on the floor.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/028CE.mp4&quot;}}},{&quot;rowIdx&quot;:1,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;14YU9&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nElaborate on the visual and narrative elements of the video in detail.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video features a person sitting on a bench in a room with yellow walls. The individual is dressed in a gray t-shirt with the text 'ONE GOOD THING ABOUT MUSIC WHEN IT HITS YOU FEEL NO PAIN' printed on it, beige shorts, and a black cap. The t-shirt's text suggests a theme of the therapeutic power of music. The person engages in various activities, such as looking towards an open door that leads to a dark room, and interacting with objects on a wooden cabinet next to them. The cabinet holds several items, including a blue spray bottle, a set of weights, and a few other small objects. The person picks up and puts down items, including a small white object and a black rectangular object, which could be a phone or a remote control. Throughout the video, the person adjusts their position and looks around the room, displaying a sense of alertness or curiosity.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nElaborate on the visual and narrative elements of the video in detail.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video features a person sitting on a bench in a room with yellow walls. The individual is dressed in a gray t-shirt with the text 'ONE GOOD THING ABOUT MUSIC WHEN IT HITS YOU FEEL NO PAIN' printed on it, beige shorts, and a black cap. The t-shirt's text suggests a theme of the therapeutic power of music. The person engages in various activities, such as looking towards an open door that leads to a dark room, and interacting with objects on a wooden cabinet next to them. The cabinet holds several items, including a blue spray bottle, a set of weights, and a few other small objects. The person picks up and puts down items, including a small white object and a black rectangular object, which could be a phone or a remote control. Throughout the video, the person adjusts their position and looks around the room, displaying a sense of alertness or curiosity.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/14YU9.mp4&quot;}}},{&quot;rowIdx&quot;:2,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1KJI0&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nShare a detailed breakdown of the video's storyline and visuals.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a cozy kitchen with light-colored walls adorned with a subtle pattern. It begins with a person wearing a black hoodie and gray pants cooking at a stove that has multiple pots and pans on it. The person uses a spatula to stir or flip food in a pan, with a blue stool positioned nearby and a wooden door visible in the background. After some time, the person steps away from the stove and walks towards the door.\n\nNext, the person walks towards a white refrigerator in the same kitchen. They open the refrigerator door and bend down to look inside, eventually retrieving a blue container. Standing up, they examine the container, possibly preparing to use it. The background remains consistent with the light-colored walls, blue stool near the stove, and wooden door in the background. The stove with multiple pots and pans is still present.\n\nIn the final part of the video, the person stands near the refrigerator, holding the blue container. They open the container and pour or transfer its contents into another container or dish. The blue stool remains near the stove, and the wooden door is still visible in the background. After pouring the contents, the person walks towards the stove, places the container on the counter, and then returns to the refrigerator, closing its door. They then stand near the refrigerator, holding the blue container, and look around, possibly contemplating the next step in their cooking process.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nShare a detailed breakdown of the video's storyline and visuals.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a cozy kitchen with light-colored walls adorned with a subtle pattern. It begins with a person wearing a black hoodie and gray pants cooking at a stove that has multiple pots and pans on it. The person uses a spatula to stir or flip food in a pan, with a blue stool positioned nearby and a wooden door visible in the background. After some time, the person steps away from the stove and walks towards the door.\\n\\nNext, the person walks towards a white refrigerator in the same kitchen. They open the refrigerator door and bend down to look inside, eventually retrieving a blue container. Standing up, they examine the container, possibly preparing to use it. The background remains consistent with the light-colored walls, blue stool near the stove, and wooden door in the background. The stove with multiple pots and pans is still present.\\n\\nIn the final part of the video, the person stands near the refrigerator, holding the blue container. They open the container and pour or transfer its contents into another container or dish. The blue stool remains near the stove, and the wooden door is still visible in the background. After pouring the contents, the person walks towards the stove, places the container on the counter, and then returns to the refrigerator, closing its door. They then stand near the refrigerator, holding the blue container, and look around, possibly contemplating the next step in their cooking process.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/1KJI0.mp4&quot;}}},{&quot;rowIdx&quot;:3,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;41SB2&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nOffer a detailed interpretation of the video's message and imagery.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a view of a bathroom, focusing on a mirror with a light fixture above it. The light fixture has four bulbs, all of which are illuminated. To the left of the mirror, there is a set of plastic drawers with various items on top. A person, partially visible and wearing dark clothing, is seen moving in the reflection of the mirror. The person steps closer to the light fixture and begins to adjust or replace one of the light bulbs, ensuring they are properly secured. After completing the task with the light fixture, the person steps back.\n\nThe scene then shifts to a different angle, showing a wooden cabinet with a countertop cluttered with various items, including bottles and containers. The same person, still partially visible and wearing dark clothing, enters the frame holding a broom. They begin to sweep the floor, moving the broom back and forth, focusing on cleaning the area near the cabinet.\n\nThe video continues with the person sweeping the floor, maintaining their focus on cleaning the area near the cabinet. The bathroom setting remains consistent, with the illuminated light fixture above the mirror and the cluttered countertop in view. The person diligently sweeps the floor, ensuring the area is clean and tidy.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nOffer a detailed interpretation of the video's message and imagery.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a view of a bathroom, focusing on a mirror with a light fixture above it. The light fixture has four bulbs, all of which are illuminated. To the left of the mirror, there is a set of plastic drawers with various items on top. A person, partially visible and wearing dark clothing, is seen moving in the reflection of the mirror. The person steps closer to the light fixture and begins to adjust or replace one of the light bulbs, ensuring they are properly secured. After completing the task with the light fixture, the person steps back.\\n\\nThe scene then shifts to a different angle, showing a wooden cabinet with a countertop cluttered with various items, including bottles and containers. The same person, still partially visible and wearing dark clothing, enters the frame holding a broom. They begin to sweep the floor, moving the broom back and forth, focusing on cleaning the area near the cabinet.\\n\\nThe video continues with the person sweeping the floor, maintaining their focus on cleaning the area near the cabinet. The bathroom setting remains consistent, with the illuminated light fixture above the mirror and the cluttered countertop in view. The person diligently sweeps the floor, ensuring the area is clean and tidy.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/41SB2.mp4&quot;}}},{&quot;rowIdx&quot;:4,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;422BV&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDescribe every scene and its significance in the video.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a small room with a wooden door and a window covered by yellow curtains with floral patterns. A person wearing a green hoodie and blue jeans is seen interacting with a cluttered desk that has a black backpack, books, papers, and a box on it. Initially, the person stands near the desk, then bends down to pick up a pair of blue shoes from the floor. They proceed to put on the shoes, adjusting them several times to ensure they fit properly. The room also contains a few cricket bats leaning against the wall and a marble-patterned floor. After putting on the shoes, the person bends down again, possibly adjusting them, and then stands up to begin organizing or searching through the items on the desk. The room's details, including the cricket bats and the marble-patterned floor, remain consistent throughout the video.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDescribe every scene and its significance in the video.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a small room with a wooden door and a window covered by yellow curtains with floral patterns. A person wearing a green hoodie and blue jeans is seen interacting with a cluttered desk that has a black backpack, books, papers, and a box on it. Initially, the person stands near the desk, then bends down to pick up a pair of blue shoes from the floor. They proceed to put on the shoes, adjusting them several times to ensure they fit properly. The room also contains a few cricket bats leaning against the wall and a marble-patterned floor. After putting on the shoes, the person bends down again, possibly adjusting them, and then stands up to begin organizing or searching through the items on the desk. The room's details, including the cricket bats and the marble-patterned floor, remain consistent throughout the video.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/422BV.mp4&quot;}}},{&quot;rowIdx&quot;:5,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;4CSXJ&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nOffer a detailed interpretation of the video's message and imagery.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person sitting on a narrow staircase, leaning against the wall. They are dressed in a teal robe and clutching a white pillow to their chest, conveying a sense of contemplation or distress. The staircase, with its beige walls and handrail on the left side, leads down to a small table that holds a framed picture, a small decorative item, and a glass of water. A pair of dark plaid pants lies next to the person on the stairs. Soft lighting from a window at the top of the stairs creates a calm yet somber atmosphere. The person occasionally adjusts their position and the pillow, indicating unease or discomfort.\n\nThe narrative develops as the person continues to sit on the staircase, still leaning against the wall in their teal robe. They interact with the dark plaid pants, picking them up, examining them, and then setting them aside. The person then reaches for a white cloth or garment further down the stairs, inspects it, and holds it close to their chest, much like they did with the pillow. The small table at the bottom of the stairs remains unchanged, maintaining the calm yet somber atmosphere with the soft lighting from the window. The person's careful handling of the items suggests a continued state of contemplation or distress, with a sense of attachment to the objects.\n\nThe video wraps up with the person still sitting on the staircase, now holding the white cloth close to their chest. The staircase, with its beige walls and handrail, remains unchanged, and the small table at the bottom still holds the framed picture, small decorative item, and glass of water. The soft lighting from the window continues to create a calm yet somber atmosphere. The person appears deep in thought or distress, occasionally looking towards the window, indicating a sense of longing or reflection. The dark plaid pants are no longer visible, suggesting they have been moved or set aside, further emphasizing the person's introspective state.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nOffer a detailed interpretation of the video's message and imagery.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person sitting on a narrow staircase, leaning against the wall. They are dressed in a teal robe and clutching a white pillow to their chest, conveying a sense of contemplation or distress. The staircase, with its beige walls and handrail on the left side, leads down to a small table that holds a framed picture, a small decorative item, and a glass of water. A pair of dark plaid pants lies next to the person on the stairs. Soft lighting from a window at the top of the stairs creates a calm yet somber atmosphere. The person occasionally adjusts their position and the pillow, indicating unease or discomfort.\\n\\nThe narrative develops as the person continues to sit on the staircase, still leaning against the wall in their teal robe. They interact with the dark plaid pants, picking them up, examining them, and then setting them aside. The person then reaches for a white cloth or garment further down the stairs, inspects it, and holds it close to their chest, much like they did with the pillow. The small table at the bottom of the stairs remains unchanged, maintaining the calm yet somber atmosphere with the soft lighting from the window. The person's careful handling of the items suggests a continued state of contemplation or distress, with a sense of attachment to the objects.\\n\\nThe video wraps up with the person still sitting on the staircase, now holding the white cloth close to their chest. The staircase, with its beige walls and handrail, remains unchanged, and the small table at the bottom still holds the framed picture, small decorative item, and glass of water. The soft lighting from the window continues to create a calm yet somber atmosphere. The person appears deep in thought or distress, occasionally looking towards the window, indicating a sense of longing or reflection. The dark plaid pants are no longer visible, suggesting they have been moved or set aside, further emphasizing the person's introspective state.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/4CSXJ.mp4&quot;}}},{&quot;rowIdx&quot;:6,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;4EM25&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nNarrate the video's unfolding events in a descriptive manner.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a narrow hallway with white walls and a patterned rug on the floor. A person dressed in black clothing is standing in the hallway, holding a phone. A white dog with black spots is present, moving around the person's legs. The dog appears to be excited, wagging its tail and looking up at the person. The person occasionally looks down at the dog and makes gestures with their free hand. Towards the end of this segment, the person bends down to interact with the dog, possibly to pet it or pick something up from the floor. The scene continues in the same hallway. The person, still holding the phone, raises their free hand, possibly to gesture or interact with the dog. The dog remains attentive, looking up at the person and wagging its tail. The person then starts walking towards the camera, with the dog following closely. As the person approaches the camera, they continue to hold the phone and occasionally look at it. The video concludes with the person walking past the camera, leaving the hallway empty.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nNarrate the video's unfolding events in a descriptive manner.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a narrow hallway with white walls and a patterned rug on the floor. A person dressed in black clothing is standing in the hallway, holding a phone. A white dog with black spots is present, moving around the person's legs. The dog appears to be excited, wagging its tail and looking up at the person. The person occasionally looks down at the dog and makes gestures with their free hand. Towards the end of this segment, the person bends down to interact with the dog, possibly to pet it or pick something up from the floor. The scene continues in the same hallway. The person, still holding the phone, raises their free hand, possibly to gesture or interact with the dog. The dog remains attentive, looking up at the person and wagging its tail. The person then starts walking towards the camera, with the dog following closely. As the person approaches the camera, they continue to hold the phone and occasionally look at it. The video concludes with the person walking past the camera, leaving the hallway empty.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/4EM25.mp4&quot;}}},{&quot;rowIdx&quot;:7,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;4J1AP&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nCharacterize each scene of the video using a detailed description.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person sitting at a table in a cozy kitchen, wearing a dark shawl and engaged in eating or drinking. On the table, there is a glass of orange juice and a bowl, from which the person is eating using a spoon. The kitchen exudes a homey feel with a light-colored tablecloth, wooden chairs, and a cabinet with glass doors displaying various items. The background includes a window and shelves with jars and other kitchen items. The person alternates between looking at the bowl and taking bites, indicating they are having a meal. The scene develops as the person continues to sit at the table, still wearing the dark shawl. They are seen holding the bowl and eating, then placing the bowl down to pick up the glass of orange juice and take a sip. The background remains consistent, featuring the same light-colored tablecloth, wooden chairs, cabinet with glass doors, window, and shelves with jars and other kitchen items. The person alternates between eating from the bowl and drinking the orange juice, continuing their meal. The video wraps up with the person still sitting at the table, holding the glass of orange juice with both hands and taking a sip. The table still has the light-colored tablecloth, and the background remains unchanged, featuring wooden chairs, a cabinet with glass doors displaying various items, a window, and shelves with jars and other kitchen items. The person is focused on drinking the orange juice, indicating they are continuing their meal.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nCharacterize each scene of the video using a detailed description.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person sitting at a table in a cozy kitchen, wearing a dark shawl and engaged in eating or drinking. On the table, there is a glass of orange juice and a bowl, from which the person is eating using a spoon. The kitchen exudes a homey feel with a light-colored tablecloth, wooden chairs, and a cabinet with glass doors displaying various items. The background includes a window and shelves with jars and other kitchen items. The person alternates between looking at the bowl and taking bites, indicating they are having a meal. The scene develops as the person continues to sit at the table, still wearing the dark shawl. They are seen holding the bowl and eating, then placing the bowl down to pick up the glass of orange juice and take a sip. The background remains consistent, featuring the same light-colored tablecloth, wooden chairs, cabinet with glass doors, window, and shelves with jars and other kitchen items. The person alternates between eating from the bowl and drinking the orange juice, continuing their meal. The video wraps up with the person still sitting at the table, holding the glass of orange juice with both hands and taking a sip. The table still has the light-colored tablecloth, and the background remains unchanged, featuring wooden chairs, a cabinet with glass doors displaying various items, a window, and shelves with jars and other kitchen items. The person is focused on drinking the orange juice, indicating they are continuing their meal.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/4J1AP.mp4&quot;}}},{&quot;rowIdx&quot;:8,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;5U92L&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDetail the video's plot development, character arcs, and thematic elements.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a dimly lit living room, featuring a door with blinds and a window to the left, also with blinds. The ceiling fan and light fixture are visible at the top, and various items, including a pair of shoes and a green object, are scattered on the floor. A person wearing a gray t-shirt and dark shorts enters the frame from the right, walking towards the door. They stop, look around, and then begin to remove their t-shirt, revealing a bare torso. After stretching their arms upwards, they proceed to remove their shorts, standing in the room in their underwear. The person stands still, having completed the action of undressing.\n\nThe scene continues in the same living room setting. The person, now shirtless and wearing dark shorts, stands near the door with blinds. The room remains dimly lit, with the ceiling fan and light fixture still visible. Various items, including a pair of shoes and a green object, are scattered on the floor. The person appears to be engaged in an activity involving a small object, which they pick up from the floor and examine closely. They then stand still, holding the object in their hands, and look down at it, seemingly contemplating it.\n\nThe video wraps up with the same living room setting, featuring the door with blinds and the window to the left, also with blinds. The room remains dimly lit, with the ceiling fan and light fixture visible at the top. Various items, including a pair of shoes and a green object, are scattered on the floor. The person, still shirtless and wearing dark shorts, stands near the door. They move around the room, stepping over the scattered items on the floor, and then walk out of the frame to the left, leaving the room empty. The final scene shows the empty living room, with the door and window with blinds, the ceiling fan, and the scattered items on the floor.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDetail the video's plot development, character arcs, and thematic elements.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a dimly lit living room, featuring a door with blinds and a window to the left, also with blinds. The ceiling fan and light fixture are visible at the top, and various items, including a pair of shoes and a green object, are scattered on the floor. A person wearing a gray t-shirt and dark shorts enters the frame from the right, walking towards the door. They stop, look around, and then begin to remove their t-shirt, revealing a bare torso. After stretching their arms upwards, they proceed to remove their shorts, standing in the room in their underwear. The person stands still, having completed the action of undressing.\\n\\nThe scene continues in the same living room setting. The person, now shirtless and wearing dark shorts, stands near the door with blinds. The room remains dimly lit, with the ceiling fan and light fixture still visible. Various items, including a pair of shoes and a green object, are scattered on the floor. The person appears to be engaged in an activity involving a small object, which they pick up from the floor and examine closely. They then stand still, holding the object in their hands, and look down at it, seemingly contemplating it.\\n\\nThe video wraps up with the same living room setting, featuring the door with blinds and the window to the left, also with blinds. The room remains dimly lit, with the ceiling fan and light fixture visible at the top. Various items, including a pair of shoes and a green object, are scattered on the floor. The person, still shirtless and wearing dark shorts, stands near the door. They move around the room, stepping over the scattered items on the floor, and then walk out of the frame to the left, leaving the room empty. The final scene shows the empty living room, with the door and window with blinds, the ceiling fan, and the scattered items on the floor.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/5U92L.mp4&quot;}}},{&quot;rowIdx&quot;:9,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;759MY&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nExplain the video's components, including its characters, setting, and plot.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person lying on a bed in a room. The bed features a black and white patterned bedspread and a white cushioned headboard adorned with black and white decorative elements. The person is dressed in a dark hoodie and gray pants. To the left of the bed, there is a desk cluttered with books, a lamp, and various other items. A patterned rug is placed on the floor near the bed. The person starts to move, shifting their position on the bed, and eventually sits up, reaching towards the desk as if to grab something. The scene transitions to the person sitting cross-legged on the bed, still reaching towards the desk. The dark hoodie now displays yellow text. The person shifts their position again, moving from sitting cross-legged to sitting on the edge of the bed. They then put on a pair of orange slippers and stand up. The person walks towards the desk, appearing to look at or reach for something on it. The video concludes with the person standing near the desk, facing it.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nExplain the video's components, including its characters, setting, and plot.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person lying on a bed in a room. The bed features a black and white patterned bedspread and a white cushioned headboard adorned with black and white decorative elements. The person is dressed in a dark hoodie and gray pants. To the left of the bed, there is a desk cluttered with books, a lamp, and various other items. A patterned rug is placed on the floor near the bed. The person starts to move, shifting their position on the bed, and eventually sits up, reaching towards the desk as if to grab something. The scene transitions to the person sitting cross-legged on the bed, still reaching towards the desk. The dark hoodie now displays yellow text. The person shifts their position again, moving from sitting cross-legged to sitting on the edge of the bed. They then put on a pair of orange slippers and stand up. The person walks towards the desk, appearing to look at or reach for something on it. The video concludes with the person standing near the desk, facing it.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/759MY.mp4&quot;}}},{&quot;rowIdx&quot;:10,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;8UGNK&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide a comprehensive analysis of the video's content and themes.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with an empty kitchen featuring wooden walls and ceiling, a green stove, a wooden table with a vase of red flowers, a wooden chair, and a trash can. Natural light streams through a window, illuminating the room. A person wearing a green hoodie and blue plaid pants enters from the left, walks towards the window, and looks outside. They then turn around, walk towards the table, and pick up a cup, taking a sip while standing contemplatively near the window. The scene transitions to the same person standing near the window, still holding the cup. Another individual, dressed in a camouflage jacket and floral pants, enters from the left, carrying a gray bag. This person approaches the one near the window and hands them the gray bag. They stand together, with the person in the green hoodie sipping from the cup as the other talks. The person in the camouflage jacket then walks towards the camera, leaving the person in the green hoodie by the window, still drinking from the cup. The final scene shows the person in the green hoodie standing near the window, deep in thought and occasionally sipping from the cup. They then walk away from the window towards the right side of the kitchen, stopping near the stove. The video concludes with the person standing near the stove, seemingly lost in contemplation while holding the cup.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide a comprehensive analysis of the video's content and themes.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with an empty kitchen featuring wooden walls and ceiling, a green stove, a wooden table with a vase of red flowers, a wooden chair, and a trash can. Natural light streams through a window, illuminating the room. A person wearing a green hoodie and blue plaid pants enters from the left, walks towards the window, and looks outside. They then turn around, walk towards the table, and pick up a cup, taking a sip while standing contemplatively near the window. The scene transitions to the same person standing near the window, still holding the cup. Another individual, dressed in a camouflage jacket and floral pants, enters from the left, carrying a gray bag. This person approaches the one near the window and hands them the gray bag. They stand together, with the person in the green hoodie sipping from the cup as the other talks. The person in the camouflage jacket then walks towards the camera, leaving the person in the green hoodie by the window, still drinking from the cup. The final scene shows the person in the green hoodie standing near the window, deep in thought and occasionally sipping from the cup. They then walk away from the window towards the right side of the kitchen, stopping near the stove. The video concludes with the person standing near the stove, seemingly lost in contemplation while holding the cup.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/8UGNK.mp4&quot;}}},{&quot;rowIdx&quot;:11,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;914NC&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWalk through the video, detailing its key moments and features.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a view of a staircase inside a house, featuring white steps, a wooden handrail, and white balusters. A blue bucket is placed on the floor near the base of the stairs. A person wearing a blue jacket and jeans, carrying a large blue bag, enters the frame from the left and starts ascending the stairs. They are followed closely by another person wearing a blue jacket and colorful pants. The first person holds a small white object in their hand as they climb. Both individuals continue to ascend the staircase, with the second person staying close behind the first. The scene develops as the two individuals, maintaining their positions, continue their ascent. The first person still carries the large blue bag and the small white object, while the second person follows closely. As they reach the top of the staircase, they begin to move out of the frame. The video wraps up with both individuals nearing the top of the stairs, with the first person slightly ahead of the second.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWalk through the video, detailing its key moments and features.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a view of a staircase inside a house, featuring white steps, a wooden handrail, and white balusters. A blue bucket is placed on the floor near the base of the stairs. A person wearing a blue jacket and jeans, carrying a large blue bag, enters the frame from the left and starts ascending the stairs. They are followed closely by another person wearing a blue jacket and colorful pants. The first person holds a small white object in their hand as they climb. Both individuals continue to ascend the staircase, with the second person staying close behind the first. The scene develops as the two individuals, maintaining their positions, continue their ascent. The first person still carries the large blue bag and the small white object, while the second person follows closely. As they reach the top of the staircase, they begin to move out of the frame. The video wraps up with both individuals nearing the top of the stairs, with the first person slightly ahead of the second.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/914NC.mp4&quot;}}},{&quot;rowIdx&quot;:12,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;95M38&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nConvey the narrative and visual elements of the video in detail.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a cozy living room featuring a large bookshelf, a lamp, and a couch. The room is well-lit by natural light streaming through the glass doors in the background, creating a warm and inviting atmosphere. A person with long, curly hair, dressed in a brown shirt and dark pants, is seated on a cushioned ottoman. They are holding a small object, possibly a toy or a snack, and appear to be deeply engaged with it. Throughout the video, the person occasionally looks around the room and adjusts their position slightly, maintaining a calm and relaxed demeanor. The setting and mood of the room remain consistent, with the person showing a sense of engagement and focus as they move their hands and look down at the object they are holding. The overall atmosphere is one of tranquility and quiet concentration.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nConvey the narrative and visual elements of the video in detail.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a cozy living room featuring a large bookshelf, a lamp, and a couch. The room is well-lit by natural light streaming through the glass doors in the background, creating a warm and inviting atmosphere. A person with long, curly hair, dressed in a brown shirt and dark pants, is seated on a cushioned ottoman. They are holding a small object, possibly a toy or a snack, and appear to be deeply engaged with it. Throughout the video, the person occasionally looks around the room and adjusts their position slightly, maintaining a calm and relaxed demeanor. The setting and mood of the room remain consistent, with the person showing a sense of engagement and focus as they move their hands and look down at the object they are holding. The overall atmosphere is one of tranquility and quiet concentration.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/95M38.mp4&quot;}}},{&quot;rowIdx&quot;:13,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;9Z1A3&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDissect the video's content, explaining each element thoroughly.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a small kitchen area, beginning with a person standing near a table and shelves filled with various food items and kitchen supplies. The person, dressed in a dark jacket and a purple cap, is in the process of putting on the jacket, swinging it around their shoulders and adjusting it. They then start performing a series of dance-like movements, lifting their legs and moving their arms rhythmically. The background features a white refrigerator adorned with magnets and notes, a table with chairs, and a window with closed blinds. Various items such as cereal boxes, condiments, and kitchen utensils are visible on the shelves and table. The scene continues with the person, still wearing the dark jacket and purple cap, standing near the table and shelves, continuing their dance-like movements. They pick up a box from the table and move towards a door, still dancing. They approach a small table with a television on it, place the box down, and bend over to interact with something on the lower shelf of the table. The television is on, displaying various scenes. The person picks up a box from the lower shelf and examines it. The background includes a door, a table with a can on it, and a dining area with shelves filled with various food items and kitchen supplies. The person then stands up and turns towards the door, appearing to be ready to leave the area.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDissect the video's content, explaining each element thoroughly.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a small kitchen area, beginning with a person standing near a table and shelves filled with various food items and kitchen supplies. The person, dressed in a dark jacket and a purple cap, is in the process of putting on the jacket, swinging it around their shoulders and adjusting it. They then start performing a series of dance-like movements, lifting their legs and moving their arms rhythmically. The background features a white refrigerator adorned with magnets and notes, a table with chairs, and a window with closed blinds. Various items such as cereal boxes, condiments, and kitchen utensils are visible on the shelves and table. The scene continues with the person, still wearing the dark jacket and purple cap, standing near the table and shelves, continuing their dance-like movements. They pick up a box from the table and move towards a door, still dancing. They approach a small table with a television on it, place the box down, and bend over to interact with something on the lower shelf of the table. The television is on, displaying various scenes. The person picks up a box from the lower shelf and examines it. The background includes a door, a table with a can on it, and a dining area with shelves filled with various food items and kitchen supplies. The person then stands up and turns towards the door, appearing to be ready to leave the area.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/9Z1A3.mp4&quot;}}},{&quot;rowIdx&quot;:14,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;A3AV0&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nIllustrate the video's narrative journey, scene by scene, with attention to detail.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with two individuals sitting on the floor in a narrow hallway near a red door. One person is dressed in a green long-sleeve shirt and a pink apron, while the other wears a red hoodie and blue patterned pants. The person in the green shirt and pink apron is holding a small object and appears to be working on it with focused attention. The person in the red hoodie sits across from them, observing or waiting. The setting is a small, enclosed space with a wooden floor and a white door frame visible, creating an intimate atmosphere.\n\nThe scene continues with the same two individuals in the hallway. The person in the green long-sleeve shirt and pink apron remains engrossed in their task, concentrating on the small object in their hands. The person in the red hoodie and blue patterned pants continues to sit across from them, watching the activity. The setting remains unchanged, with the wooden floor and white door frame still visible. The interaction between the two is captured, with the person in the green shirt occasionally glancing down at their hands and the object. At one point, the person in the red hoodie raises their arm, possibly gesturing or reaching for something, adding a subtle dynamic to the quiet, focused atmosphere.\n\nThe video wraps up with the two individuals still in the narrow hallway near the red door. The person in the red hoodie and blue patterned pants begins to move, shifting their position and eventually standing up. Meanwhile, the person in the green long-sleeve shirt and pink apron remains seated, still focused on the small object in their hands. As the person in the red hoodie stands, they turn towards the door and appear to look outside through the window. The sequence captures this movement, with the person in the red hoodie standing and looking out the door, while the person in the green shirt continues their activity on the floor. The setting remains consistent, with the wooden floor and white door frame visible, maintaining the overall atmosphere of quiet focus and engagement in the small, enclosed space.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nIllustrate the video's narrative journey, scene by scene, with attention to detail.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with two individuals sitting on the floor in a narrow hallway near a red door. One person is dressed in a green long-sleeve shirt and a pink apron, while the other wears a red hoodie and blue patterned pants. The person in the green shirt and pink apron is holding a small object and appears to be working on it with focused attention. The person in the red hoodie sits across from them, observing or waiting. The setting is a small, enclosed space with a wooden floor and a white door frame visible, creating an intimate atmosphere.\\n\\nThe scene continues with the same two individuals in the hallway. The person in the green long-sleeve shirt and pink apron remains engrossed in their task, concentrating on the small object in their hands. The person in the red hoodie and blue patterned pants continues to sit across from them, watching the activity. The setting remains unchanged, with the wooden floor and white door frame still visible. The interaction between the two is captured, with the person in the green shirt occasionally glancing down at their hands and the object. At one point, the person in the red hoodie raises their arm, possibly gesturing or reaching for something, adding a subtle dynamic to the quiet, focused atmosphere.\\n\\nThe video wraps up with the two individuals still in the narrow hallway near the red door. The person in the red hoodie and blue patterned pants begins to move, shifting their position and eventually standing up. Meanwhile, the person in the green long-sleeve shirt and pink apron remains seated, still focused on the small object in their hands. As the person in the red hoodie stands, they turn towards the door and appear to look outside through the window. The sequence captures this movement, with the person in the red hoodie standing and looking out the door, while the person in the green shirt continues their activity on the floor. The setting remains consistent, with the wooden floor and white door frame visible, maintaining the overall atmosphere of quiet focus and engagement in the small, enclosed space.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/A3AV0.mp4&quot;}}},{&quot;rowIdx&quot;:15,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;AF8VD&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite an in-depth depiction of the video, covering all its aspects.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a cozy living room with a white tiled floor and a staircase in the background. The room is furnished with a blue bed covered with a pink and white blanket, a small table with a cup, and a television set on a stand. The television is on, displaying a scene with a person. A person, wrapped in a pink and white blanket and wearing a white sweater, is seen sitting on the bed. They adjust the blanket around them, occasionally looking towards the television. Various household items, including a blue bucket, a small stool, and other objects near the staircase, add to the domestic atmosphere. The person appears to be relaxing and watching television.\n\nThe scene continues with the person still seated on the blue bed, wrapped in the blanket. They adjust the blanket and their position while occasionally glancing at the television. The small table with a cup remains present, and the television continues to display a scene with a person. The person takes a sip from the cup, indicating they are drinking something. The calm and domestic atmosphere persists as the person relaxes and watches television.\n\nIn the final part of the video, the person remains seated on the blue bed, wrapped in the pink and white blanket. They are seen holding an orange cup and taking a sip from it. The small table with a cup is still present, and the television continues to display a scene with a person. The person stretches out their legs and continues to relax while watching television. The blue bucket, small stool, and other household items near the staircase remain visible. The overall atmosphere remains calm and domestic, with the person continuing to enjoy their time watching television.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite an in-depth depiction of the video, covering all its aspects.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a cozy living room with a white tiled floor and a staircase in the background. The room is furnished with a blue bed covered with a pink and white blanket, a small table with a cup, and a television set on a stand. The television is on, displaying a scene with a person. A person, wrapped in a pink and white blanket and wearing a white sweater, is seen sitting on the bed. They adjust the blanket around them, occasionally looking towards the television. Various household items, including a blue bucket, a small stool, and other objects near the staircase, add to the domestic atmosphere. The person appears to be relaxing and watching television.\\n\\nThe scene continues with the person still seated on the blue bed, wrapped in the blanket. They adjust the blanket and their position while occasionally glancing at the television. The small table with a cup remains present, and the television continues to display a scene with a person. The person takes a sip from the cup, indicating they are drinking something. The calm and domestic atmosphere persists as the person relaxes and watches television.\\n\\nIn the final part of the video, the person remains seated on the blue bed, wrapped in the pink and white blanket. They are seen holding an orange cup and taking a sip from it. The small table with a cup is still present, and the television continues to display a scene with a person. The person stretches out their legs and continues to relax while watching television. The blue bucket, small stool, and other household items near the staircase remain visible. The overall atmosphere remains calm and domestic, with the person continuing to enjoy their time watching television.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/AF8VD.mp4&quot;}}},{&quot;rowIdx&quot;:16,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;AX4EW&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite a comprehensive depiction of the entire video clip.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a cozy room featuring a round wooden table and a distinctive blue-arched doorway in the background. On the table, there is a laptop, a newspaper, a cup, and a water bottle, indicating a casual, everyday setting. A man in a striped shirt stands and holds a camera, seemingly capturing a photo or video of another man who is seated at the table and focused on his meal. The seated man, dressed in a white shirt with black stripes, is engrossed in eating. The room also includes a water dispenser on the right side and a framed picture on the wall, adding to the domestic atmosphere. The scene continues with the man in the striped shirt still standing and holding the camera, capturing the moment of the man at the table. The man at the table, still wearing the white shirt with black stripes, is now seen wiping his mouth with a napkin. The table's items remain unchanged, and the water dispenser and framed picture are still visible in the background. The video encapsulates a simple, everyday moment in a domestic setting, highlighting the interaction between the two men, one documenting the other's mealtime.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite a comprehensive depiction of the entire video clip.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a cozy room featuring a round wooden table and a distinctive blue-arched doorway in the background. On the table, there is a laptop, a newspaper, a cup, and a water bottle, indicating a casual, everyday setting. A man in a striped shirt stands and holds a camera, seemingly capturing a photo or video of another man who is seated at the table and focused on his meal. The seated man, dressed in a white shirt with black stripes, is engrossed in eating. The room also includes a water dispenser on the right side and a framed picture on the wall, adding to the domestic atmosphere. The scene continues with the man in the striped shirt still standing and holding the camera, capturing the moment of the man at the table. The man at the table, still wearing the white shirt with black stripes, is now seen wiping his mouth with a napkin. The table's items remain unchanged, and the water dispenser and framed picture are still visible in the background. The video encapsulates a simple, everyday moment in a domestic setting, highlighting the interaction between the two men, one documenting the other's mealtime.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/AX4EW.mp4&quot;}}},{&quot;rowIdx&quot;:17,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;D1UWI&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nExplain the video's components, including its characters, setting, and plot.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a small kitchen with beige walls and wooden cabinets, creating a cozy and warm atmosphere. A person dressed in a gray hoodie, orange shirt, dark pants, and red socks is the central figure throughout the video. The scene begins with the person standing near a white refrigerator and stove, focused on cooking or preparing something. The person is seen holding a glass or cup and gesturing towards the camera, indicating they might be explaining or sharing something. The person then moves to the middle of the kitchen, interacting with the counter and possibly reaching for an ingredient or utensil. The background features a microwave on the counter and a dishwasher below, with warm lighting from a ceiling light and a light above the stove enhancing the homely feel.\n\nAs the video progresses, the person continues their cooking activity, remaining near the stove. They are seen looking down at the stove, possibly stirring or checking on the food, and occasionally turning towards the camera while holding a utensil. The person's actions suggest they are deeply engaged in the cooking process, maintaining a consistent focus on the stove. The background elements, including the microwave and dishwasher, remain unchanged, and the warm lighting continues to illuminate the scene.\n\nThe video wraps up with the person still engrossed in their cooking task, maintaining their position near the stove. The repetitive actions of looking down at the stove, stirring, and occasionally turning towards the camera emphasize the person's dedication to their culinary activity. The consistent background and warm lighting throughout the video create a sense of continuity and comfort, making the viewer feel as though they are part of a familiar and inviting kitchen environment.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nExplain the video's components, including its characters, setting, and plot.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a small kitchen with beige walls and wooden cabinets, creating a cozy and warm atmosphere. A person dressed in a gray hoodie, orange shirt, dark pants, and red socks is the central figure throughout the video. The scene begins with the person standing near a white refrigerator and stove, focused on cooking or preparing something. The person is seen holding a glass or cup and gesturing towards the camera, indicating they might be explaining or sharing something. The person then moves to the middle of the kitchen, interacting with the counter and possibly reaching for an ingredient or utensil. The background features a microwave on the counter and a dishwasher below, with warm lighting from a ceiling light and a light above the stove enhancing the homely feel.\\n\\nAs the video progresses, the person continues their cooking activity, remaining near the stove. They are seen looking down at the stove, possibly stirring or checking on the food, and occasionally turning towards the camera while holding a utensil. The person's actions suggest they are deeply engaged in the cooking process, maintaining a consistent focus on the stove. The background elements, including the microwave and dishwasher, remain unchanged, and the warm lighting continues to illuminate the scene.\\n\\nThe video wraps up with the person still engrossed in their cooking task, maintaining their position near the stove. The repetitive actions of looking down at the stove, stirring, and occasionally turning towards the camera emphasize the person's dedication to their culinary activity. The consistent background and warm lighting throughout the video create a sense of continuity and comfort, making the viewer feel as though they are part of a familiar and inviting kitchen environment.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/D1UWI.mp4&quot;}}},{&quot;rowIdx&quot;:18,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;D2VU8&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide a comprehensive analysis of the video's content and themes.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person sitting on a beige couch in a living room, dressed in a gray tank top and gray sweatpants. They are using a laptop placed on a wooden coffee table, which is cluttered with various items, including a bowl of fruit, a smartphone, and a glass bowl. Behind the person, a colorful blanket is draped over the couch, and the wall features a large framed picture along with some smaller framed items. The person then stands up, holding a smartphone, and walks towards a side table with a lamp and some books. They pick up a book from the side table and return to the couch. After moving the blanket, they sit back down and appear to get comfortable, possibly preparing to read the book. The scene transitions to the person lying on the beige couch, now covered with the colorful blanket and holding the book. They seem to be engrossed in reading, occasionally adjusting the blanket for comfort. The coffee table in front of the couch remains cluttered with the same items. The person continues to read, sometimes looking up and around the room. The living room setting remains consistent with the same framed pictures on the wall and the side table with a lamp and books. The video wraps up with the person sitting up, still holding the book, and adjusting the blanket.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide a comprehensive analysis of the video's content and themes.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person sitting on a beige couch in a living room, dressed in a gray tank top and gray sweatpants. They are using a laptop placed on a wooden coffee table, which is cluttered with various items, including a bowl of fruit, a smartphone, and a glass bowl. Behind the person, a colorful blanket is draped over the couch, and the wall features a large framed picture along with some smaller framed items. The person then stands up, holding a smartphone, and walks towards a side table with a lamp and some books. They pick up a book from the side table and return to the couch. After moving the blanket, they sit back down and appear to get comfortable, possibly preparing to read the book. The scene transitions to the person lying on the beige couch, now covered with the colorful blanket and holding the book. They seem to be engrossed in reading, occasionally adjusting the blanket for comfort. The coffee table in front of the couch remains cluttered with the same items. The person continues to read, sometimes looking up and around the room. The living room setting remains consistent with the same framed pictures on the wall and the side table with a lamp and books. The video wraps up with the person sitting up, still holding the book, and adjusting the blanket.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/D2VU8.mp4&quot;}}},{&quot;rowIdx&quot;:19,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;DPB7X&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite a comprehensive depiction of the entire video clip.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video features a serene domestic scene set in a well-lit room with a large window on the right side. A person, dressed in a light-colored shirt, is seated at a table and is engaged in an activity involving a green bag and various items spread out on the table. The background includes a light blue cabinet with glass doors, showcasing an array of decorative items such as a red teapot and some pottery. The person appears to be unpacking or organizing the items from the green bag, occasionally glancing out the window. The natural light streaming in from the window bathes the room in a warm, inviting glow, enhancing the calm and peaceful atmosphere of the scene.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite a comprehensive depiction of the entire video clip.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video features a serene domestic scene set in a well-lit room with a large window on the right side. A person, dressed in a light-colored shirt, is seated at a table and is engaged in an activity involving a green bag and various items spread out on the table. The background includes a light blue cabinet with glass doors, showcasing an array of decorative items such as a red teapot and some pottery. The person appears to be unpacking or organizing the items from the green bag, occasionally glancing out the window. The natural light streaming in from the window bathes the room in a warm, inviting glow, enhancing the calm and peaceful atmosphere of the scene.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/DPB7X.mp4&quot;}}},{&quot;rowIdx&quot;:20,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;E6Q95&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nOffer a detailed interpretation of the video's message and imagery.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a cozy, rustic room with wooden walls and a wooden floor, furnished with a green armchair, a couch covered with blankets, and a small table with various items on it. A television sits on a wooden stand against the wall, and a door with a diamond-shaped window adds to the room's charm. Initially, a person wearing a camouflage jacket and plaid pants stands near the television. They move around the room, eventually walking towards the couch. After sitting down, they remove their jacket to reveal a gray shirt underneath, adjust the blankets, and appear to get comfortable, possibly preparing to rest or relax. The scene continues with the person, now in a gray shirt and plaid pants, seated on the couch and holding a blue and black patterned onesie. They lift the onesie and put it on, standing briefly to pull it over their head. Once dressed in the onesie, they sit back down and adjust the blankets around them, seemingly settling in for relaxation, perhaps to watch television or rest. The final part of the video shows the person, now in the blue and black patterned onesie, seated on the couch with blankets. They are holding a tablet or similar device, appearing engaged with it, possibly watching something or reading. The room's furnishings remain unchanged, and the television is on, displaying a scene. A box labeled 'MAGIC' is on the floor next to the television stand. The person seems comfortably settled in, continuing their relaxation.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nOffer a detailed interpretation of the video's message and imagery.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a cozy, rustic room with wooden walls and a wooden floor, furnished with a green armchair, a couch covered with blankets, and a small table with various items on it. A television sits on a wooden stand against the wall, and a door with a diamond-shaped window adds to the room's charm. Initially, a person wearing a camouflage jacket and plaid pants stands near the television. They move around the room, eventually walking towards the couch. After sitting down, they remove their jacket to reveal a gray shirt underneath, adjust the blankets, and appear to get comfortable, possibly preparing to rest or relax. The scene continues with the person, now in a gray shirt and plaid pants, seated on the couch and holding a blue and black patterned onesie. They lift the onesie and put it on, standing briefly to pull it over their head. Once dressed in the onesie, they sit back down and adjust the blankets around them, seemingly settling in for relaxation, perhaps to watch television or rest. The final part of the video shows the person, now in the blue and black patterned onesie, seated on the couch with blankets. They are holding a tablet or similar device, appearing engaged with it, possibly watching something or reading. The room's furnishings remain unchanged, and the television is on, displaying a scene. A box labeled 'MAGIC' is on the floor next to the television stand. The person seems comfortably settled in, continuing their relaxation.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/E6Q95.mp4&quot;}}},{&quot;rowIdx&quot;:21,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;ELFEW&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite a comprehensive depiction of the entire video clip.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person standing in front of an open closet or storage area, wearing a long-sleeved gray shirt. They hold a tool, possibly a wrench or pliers, in their right hand and a long object, perhaps a pipe or rod, in their left hand. The storage area contains a water heater, a white bucket on the floor, and some papers and a sign posted on the wall. The person appears to be inspecting or working on something near the water heater. As the video progresses, the person continues their inspection or work, focusing intently on the area near the water heater. They occasionally look at the tool in their right hand, suggesting they are making adjustments or repairs. The scene remains consistent with the storage area containing the same items. Towards the end of the video, the person turns slightly towards the storage area, possibly to get a closer look or to adjust their position for better access. Finally, the person faces the storage area more directly, with their back towards the camera, continuing their work near the water heater. The lighting in the scene casts a warm, orange hue on the wooden surfaces around the storage area, adding a cozy ambiance to the setting.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite a comprehensive depiction of the entire video clip.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person standing in front of an open closet or storage area, wearing a long-sleeved gray shirt. They hold a tool, possibly a wrench or pliers, in their right hand and a long object, perhaps a pipe or rod, in their left hand. The storage area contains a water heater, a white bucket on the floor, and some papers and a sign posted on the wall. The person appears to be inspecting or working on something near the water heater. As the video progresses, the person continues their inspection or work, focusing intently on the area near the water heater. They occasionally look at the tool in their right hand, suggesting they are making adjustments or repairs. The scene remains consistent with the storage area containing the same items. Towards the end of the video, the person turns slightly towards the storage area, possibly to get a closer look or to adjust their position for better access. Finally, the person faces the storage area more directly, with their back towards the camera, continuing their work near the water heater. The lighting in the scene casts a warm, orange hue on the wooden surfaces around the storage area, adding a cozy ambiance to the setting.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/ELFEW.mp4&quot;}}},{&quot;rowIdx&quot;:22,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;FB3ZG&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nConvey the narrative and visual elements of the video in detail.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person wearing a blue hoodie with white text and light-colored pants walking into a room characterized by light blue walls, a wooden door, and a bookshelf filled with books and other items. Several framed pictures adorn the walls, adding a personal touch to the space. The person briefly exits the frame, leaving the room empty. After a few moments, they return, approach the camera, and appear to adjust something. They then show a white mug to the camera before walking back towards the door, holding the mug and another object. The person exits the room through the door, leaving it empty once again. The video continues with the same room, now empty, maintaining its serene and unchanged setting. The light blue walls, wooden door, and bookshelf filled with books and other items remain in view, along with the framed pictures on the wall. The room stays empty for the remainder of the video, with no visible movement or changes, concluding with the same tranquil atmosphere.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nConvey the narrative and visual elements of the video in detail.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person wearing a blue hoodie with white text and light-colored pants walking into a room characterized by light blue walls, a wooden door, and a bookshelf filled with books and other items. Several framed pictures adorn the walls, adding a personal touch to the space. The person briefly exits the frame, leaving the room empty. After a few moments, they return, approach the camera, and appear to adjust something. They then show a white mug to the camera before walking back towards the door, holding the mug and another object. The person exits the room through the door, leaving it empty once again. The video continues with the same room, now empty, maintaining its serene and unchanged setting. The light blue walls, wooden door, and bookshelf filled with books and other items remain in view, along with the framed pictures on the wall. The room stays empty for the remainder of the video, with no visible movement or changes, concluding with the same tranquil atmosphere.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/FB3ZG.mp4&quot;}}},{&quot;rowIdx&quot;:23,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;FEKJQ&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide an exhaustive description of the video content.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a well-lit kitchen with light-colored cabinets, a black countertop, and various utensils, including a sink and a stove. The setting is enhanced by a tiled wall with decorative patterns, a shelf with kitchen items, and a window that allows natural light to flood the room. A man wearing a white sleeveless shirt and dark pants is the central figure throughout the video. Initially, he is seen adjusting a mirror placed on the countertop and proceeds to groom himself, possibly shaving or trimming his facial hair. The scene captures the intimate details of his grooming routine in this domestic kitchen setting. As the video progresses, the man continues his grooming routine, finishing up and then adjusting the mirror. He begins to clean the area, meticulously wiping the countertop, maintaining the cleanliness of the space. The background remains consistent, reinforcing the homely atmosphere. In the final part of the video, the man bends down to pick up a black laptop from the floor. He stands up, holding and inspecting the laptop, suggesting he might be preparing to use it or move it to another location. The video encapsulates a slice of everyday life, focusing on the man's personal grooming and subsequent activities in his kitchen.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide an exhaustive description of the video content.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a well-lit kitchen with light-colored cabinets, a black countertop, and various utensils, including a sink and a stove. The setting is enhanced by a tiled wall with decorative patterns, a shelf with kitchen items, and a window that allows natural light to flood the room. A man wearing a white sleeveless shirt and dark pants is the central figure throughout the video. Initially, he is seen adjusting a mirror placed on the countertop and proceeds to groom himself, possibly shaving or trimming his facial hair. The scene captures the intimate details of his grooming routine in this domestic kitchen setting. As the video progresses, the man continues his grooming routine, finishing up and then adjusting the mirror. He begins to clean the area, meticulously wiping the countertop, maintaining the cleanliness of the space. The background remains consistent, reinforcing the homely atmosphere. In the final part of the video, the man bends down to pick up a black laptop from the floor. He stands up, holding and inspecting the laptop, suggesting he might be preparing to use it or move it to another location. The video encapsulates a slice of everyday life, focusing on the man's personal grooming and subsequent activities in his kitchen.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/FEKJQ.mp4&quot;}}},{&quot;rowIdx&quot;:24,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;FLDHS&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nCharacterize each scene of the video using a detailed description.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person in a black hoodie and blue jeans standing in a room with a wooden door and a window covered by yellow floral curtains. The person is facing away from the camera and appears to be organizing or searching through items on a small table or shelf next to the door. The room has a tiled floor, and there are various items scattered around, including a green bag, a cricket bat, and some boxes. The person bends down to pick up or move items, including a pink box and a purple container. They continue to rearrange and handle different objects, occasionally bending down to pick up items from the floor or lower shelves. The scene develops as the person methodically organizes the area, moving items from one place to another.\n\nThe narrative progresses with the person still in the same room, now seen bending down and picking up a green bag from the floor. They lift the bag and begin to shake it out, possibly to empty its contents or to straighten it. The person then folds the green bag and places it on the table or shelf next to the door. The room remains the same with the tiled floor and various items scattered around, including a cricket bat and some boxes. The person continues to focus on organizing the area, now specifically dealing with the green bag.\n\nThe video wraps up with the person still in the same room, standing at the table or shelf next to the door, organizing items. They handle the green bag, placing it on the table, and then proceed to sort through various objects on the table, including a purple container and some books. The room remains unchanged with the tiled floor and various items scattered around, including a cricket bat and some boxes. The person continues to methodically organize the area, moving items from one place to another, focusing on the items on the table.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nCharacterize each scene of the video using a detailed description.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person in a black hoodie and blue jeans standing in a room with a wooden door and a window covered by yellow floral curtains. The person is facing away from the camera and appears to be organizing or searching through items on a small table or shelf next to the door. The room has a tiled floor, and there are various items scattered around, including a green bag, a cricket bat, and some boxes. The person bends down to pick up or move items, including a pink box and a purple container. They continue to rearrange and handle different objects, occasionally bending down to pick up items from the floor or lower shelves. The scene develops as the person methodically organizes the area, moving items from one place to another.\\n\\nThe narrative progresses with the person still in the same room, now seen bending down and picking up a green bag from the floor. They lift the bag and begin to shake it out, possibly to empty its contents or to straighten it. The person then folds the green bag and places it on the table or shelf next to the door. The room remains the same with the tiled floor and various items scattered around, including a cricket bat and some boxes. The person continues to focus on organizing the area, now specifically dealing with the green bag.\\n\\nThe video wraps up with the person still in the same room, standing at the table or shelf next to the door, organizing items. They handle the green bag, placing it on the table, and then proceed to sort through various objects on the table, including a purple container and some books. The room remains unchanged with the tiled floor and various items scattered around, including a cricket bat and some boxes. The person continues to methodically organize the area, moving items from one place to another, focusing on the items on the table.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/FLDHS.mp4&quot;}}},{&quot;rowIdx&quot;:25,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;FYR3P&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nOffer a detailed interpretation of the video's message and imagery.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a bedroom featuring a bed on the left side, covered with a black and white patterned bedspread and some white clothes on it. Behind the bed, there is a window with beige curtains, and on the right side, a wardrobe with a mirror. A person dressed in a yellow kurta with white pants and black shoes is initially seen standing. They begin to remove their yellow vest, turning around to face the wardrobe as they do so. The person successfully removes the vest and holds it in their hands. The scene continues with the same person standing near the bed. They bend down to pick up the white clothes from the bed and then walk towards the wardrobe with a mirror. The video concludes with the person standing in front of the wardrobe, holding the white clothes in their hands.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nOffer a detailed interpretation of the video's message and imagery.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a bedroom featuring a bed on the left side, covered with a black and white patterned bedspread and some white clothes on it. Behind the bed, there is a window with beige curtains, and on the right side, a wardrobe with a mirror. A person dressed in a yellow kurta with white pants and black shoes is initially seen standing. They begin to remove their yellow vest, turning around to face the wardrobe as they do so. The person successfully removes the vest and holds it in their hands. The scene continues with the same person standing near the bed. They bend down to pick up the white clothes from the bed and then walk towards the wardrobe with a mirror. The video concludes with the person standing in front of the wardrobe, holding the white clothes in their hands.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/FYR3P.mp4&quot;}}},{&quot;rowIdx&quot;:26,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;GC1Q1&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nNarrate the video's unfolding events in a descriptive manner.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a view of a partially open brown door in a hallway with light-colored walls and a tiled floor. A person wearing a red and white striped shirt and blue jeans opens the door wider and steps inside, carrying a stack of folded clothes in various colors. They walk further into the room, which contains a wooden chair and shelves filled with household items. The person places the stack of clothes on the chair and adjusts them, appearing to organize or sort through the items. The scene develops as the person continues to handle the clothes near the chair. The narrative progresses with the person in the red and white striped shirt and blue jeans standing near the wooden chair, which now has some clothes placed on it. They then bend down and sit on the chair, adjusting their position to get comfortable. Leaning back, they cross one leg over the other and rest their arms on the armrests of the chair. The background shows shelves filled with various household items, and the brown door is now closed. The video wraps up with the person appearing to relax or take a break, remaining seated in a relaxed posture for the remainder of the scene.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nNarrate the video's unfolding events in a descriptive manner.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a view of a partially open brown door in a hallway with light-colored walls and a tiled floor. A person wearing a red and white striped shirt and blue jeans opens the door wider and steps inside, carrying a stack of folded clothes in various colors. They walk further into the room, which contains a wooden chair and shelves filled with household items. The person places the stack of clothes on the chair and adjusts them, appearing to organize or sort through the items. The scene develops as the person continues to handle the clothes near the chair. The narrative progresses with the person in the red and white striped shirt and blue jeans standing near the wooden chair, which now has some clothes placed on it. They then bend down and sit on the chair, adjusting their position to get comfortable. Leaning back, they cross one leg over the other and rest their arms on the armrests of the chair. The background shows shelves filled with various household items, and the brown door is now closed. The video wraps up with the person appearing to relax or take a break, remaining seated in a relaxed posture for the remainder of the scene.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/GC1Q1.mp4&quot;}}},{&quot;rowIdx&quot;:27,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;GJC1G&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide an exhaustive description of the video content.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a view of a small room with a brown tiled floor and white walls. In the corner, there is a yellow container with a red lid and a beige pet carrier. A small red box is on the floor near the door. A person wearing a gray hoodie and a dark jacket enters the room from the left side. They walk towards the camera, then start to remove their jacket, revealing the gray hoodie underneath. Holding the jacket in one hand, they turn towards the door on the right side of the room and reach for the door handle. The scene transitions to the person in the gray hoodie standing near the door, still holding the dark jacket. They bend down to pick up a brown paper bag from the floor. From the bag, they retrieve a small yellow container and examine it closely before placing it back into the bag. Next, they take out a red package and inspect it intently. The final scene shows the person bending down near the door, holding the brown paper bag. They take a small item from the bag and place it on a beige cat tree located to the right of the frame. While still holding the paper bag, they inspect the cat tree, occasionally touching it. The video concludes with the person standing upright, holding the paper bag in one hand and touching their face with the other, as they look towards the cat tree.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide an exhaustive description of the video content.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a view of a small room with a brown tiled floor and white walls. In the corner, there is a yellow container with a red lid and a beige pet carrier. A small red box is on the floor near the door. A person wearing a gray hoodie and a dark jacket enters the room from the left side. They walk towards the camera, then start to remove their jacket, revealing the gray hoodie underneath. Holding the jacket in one hand, they turn towards the door on the right side of the room and reach for the door handle. The scene transitions to the person in the gray hoodie standing near the door, still holding the dark jacket. They bend down to pick up a brown paper bag from the floor. From the bag, they retrieve a small yellow container and examine it closely before placing it back into the bag. Next, they take out a red package and inspect it intently. The final scene shows the person bending down near the door, holding the brown paper bag. They take a small item from the bag and place it on a beige cat tree located to the right of the frame. While still holding the paper bag, they inspect the cat tree, occasionally touching it. The video concludes with the person standing upright, holding the paper bag in one hand and touching their face with the other, as they look towards the cat tree.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/GJC1G.mp4&quot;}}},{&quot;rowIdx&quot;:28,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;IKPR3&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nAnalyze the video, focusing on its cinematography and narrative structure.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person sitting on a staircase, dressed in a white t-shirt and dark pants. They are holding a smartphone and appear to be engaged with it, possibly texting or browsing. The staircase features a wooden handrail on the right side, and the walls are painted in a light color. The warm lighting suggests an indoor setting. The person occasionally looks up from the phone and makes gestures with their hands, indicating they might be talking to someone or reacting to something on the phone. The scene develops as the person, still sitting on the staircase, starts to stand up, using the wooden handrail for support. They then proceed to walk down the stairs, moving towards the camera. The lighting remains warm, and the walls continue to be painted in a light color. The person exits the frame, leaving the staircase empty. The video wraps up with a view of the empty staircase, showcasing the wooden handrail on the right side and the light-colored walls.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nAnalyze the video, focusing on its cinematography and narrative structure.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person sitting on a staircase, dressed in a white t-shirt and dark pants. They are holding a smartphone and appear to be engaged with it, possibly texting or browsing. The staircase features a wooden handrail on the right side, and the walls are painted in a light color. The warm lighting suggests an indoor setting. The person occasionally looks up from the phone and makes gestures with their hands, indicating they might be talking to someone or reacting to something on the phone. The scene develops as the person, still sitting on the staircase, starts to stand up, using the wooden handrail for support. They then proceed to walk down the stairs, moving towards the camera. The lighting remains warm, and the walls continue to be painted in a light color. The person exits the frame, leaving the staircase empty. The video wraps up with a view of the empty staircase, showcasing the wooden handrail on the right side and the light-colored walls.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/IKPR3.mp4&quot;}}},{&quot;rowIdx&quot;:29,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;IMLN5&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWalk through the video, detailing its key moments and features.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a cluttered garage or storage area, where a person wearing a dark purple long-sleeve shirt is seen standing next to a black car. The scene is filled with various items, including a white shelving unit stocked with bottles and containers, boxes, tools, and a red jacket hanging on the wall. The person is engaged in organizing or searching for something on the shelves. They pick up a red object from the shelf and then drink from a yellow bottle. The focus remains on their tasks in the garage. The video continues with the person still in the same garage, now primarily focused on drinking from the yellow bottle. The background remains unchanged, with the same cluttered space and items visible. The person’s actions are minimal, centered around the yellow bottle. The video concludes with the person examining the red object closely, maintaining their position next to the black car. The white shelving unit and the cluttered background remain consistent, with the person’s attention now on the red object. Throughout the video, the person’s activities are concentrated on handling and examining objects within the garage.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWalk through the video, detailing its key moments and features.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a cluttered garage or storage area, where a person wearing a dark purple long-sleeve shirt is seen standing next to a black car. The scene is filled with various items, including a white shelving unit stocked with bottles and containers, boxes, tools, and a red jacket hanging on the wall. The person is engaged in organizing or searching for something on the shelves. They pick up a red object from the shelf and then drink from a yellow bottle. The focus remains on their tasks in the garage. The video continues with the person still in the same garage, now primarily focused on drinking from the yellow bottle. The background remains unchanged, with the same cluttered space and items visible. The person’s actions are minimal, centered around the yellow bottle. The video concludes with the person examining the red object closely, maintaining their position next to the black car. The white shelving unit and the cluttered background remain consistent, with the person’s attention now on the red object. Throughout the video, the person’s activities are concentrated on handling and examining objects within the garage.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/IMLN5.mp4&quot;}}},{&quot;rowIdx&quot;:30,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;JNK11&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDissect the video's content, explaining each element thoroughly.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a young girl in a cluttered garage, wearing a pink shirt and blue jeans. She is holding a white cup and a small object in her hands. As she walks through the garage, sipping from the cup, she approaches a shelf, places the cup down, and starts examining the small object. The scene develops as the girl, now barefoot, continues to focus on the small object while the garage remains filled with various items, including a shelf with cleaning supplies and a door leading to another room. She walks towards the door, opens it, and reveals a laundry area with a washing machine and clothes hanging. The video wraps up with the girl walking through the doorway into the laundry area, still engrossed in the small object, and moving further into the room until she is partially out of view.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDissect the video's content, explaining each element thoroughly.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a young girl in a cluttered garage, wearing a pink shirt and blue jeans. She is holding a white cup and a small object in her hands. As she walks through the garage, sipping from the cup, she approaches a shelf, places the cup down, and starts examining the small object. The scene develops as the girl, now barefoot, continues to focus on the small object while the garage remains filled with various items, including a shelf with cleaning supplies and a door leading to another room. She walks towards the door, opens it, and reveals a laundry area with a washing machine and clothes hanging. The video wraps up with the girl walking through the doorway into the laundry area, still engrossed in the small object, and moving further into the room until she is partially out of view.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/JNK11.mp4&quot;}}},{&quot;rowIdx&quot;:31,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;K71S1&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide an exhaustive description of the video content.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person entering a room and approaching a box placed on the floor near the entrance door. The person is dressed in a dark beanie, a gray jacket, and khaki pants. The room features a wooden floor, beige walls, and a white door, with a small table holding a vase of flowers and a mirror on the wall to the right. As the person kneels down to open the box, a small white dog with a pink collar approaches, showing interest in the box. The person continues to open the box, revealing a black item inside, while the dog remains curious and stays close by, watching attentively.\n\nThe narrative develops as the person continues to interact with the box, pulling out and examining the black item. The small white dog with a pink collar remains engaged, closely observing the person's actions. The room's setting remains unchanged, with the beige walls, white door, small table with a vase of flowers, and mirror on the wall to the right. The person continues to explore the contents of the box, occasionally lifting items out and inspecting them, while the dog stays nearby, maintaining its interest.\n\nThe video wraps up with the person still kneeling on the wooden floor near the entrance door, interacting with the box. The small white dog with a pink collar remains close by, attentively watching the person's actions. The person stands up while holding an item from the box, fully examining it as the dog continues to watch. The room's setting remains consistent, with beige walls, a white door, a small table with a vase of flowers, and a mirror on the wall to the right. The video concludes with the person standing and looking at the item, with the dog still standing nearby, maintaining its interest.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide an exhaustive description of the video content.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person entering a room and approaching a box placed on the floor near the entrance door. The person is dressed in a dark beanie, a gray jacket, and khaki pants. The room features a wooden floor, beige walls, and a white door, with a small table holding a vase of flowers and a mirror on the wall to the right. As the person kneels down to open the box, a small white dog with a pink collar approaches, showing interest in the box. The person continues to open the box, revealing a black item inside, while the dog remains curious and stays close by, watching attentively.\\n\\nThe narrative develops as the person continues to interact with the box, pulling out and examining the black item. The small white dog with a pink collar remains engaged, closely observing the person's actions. The room's setting remains unchanged, with the beige walls, white door, small table with a vase of flowers, and mirror on the wall to the right. The person continues to explore the contents of the box, occasionally lifting items out and inspecting them, while the dog stays nearby, maintaining its interest.\\n\\nThe video wraps up with the person still kneeling on the wooden floor near the entrance door, interacting with the box. The small white dog with a pink collar remains close by, attentively watching the person's actions. The person stands up while holding an item from the box, fully examining it as the dog continues to watch. The room's setting remains consistent, with beige walls, a white door, a small table with a vase of flowers, and a mirror on the wall to the right. The video concludes with the person standing and looking at the item, with the dog still standing nearby, maintaining its interest.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/K71S1.mp4&quot;}}},{&quot;rowIdx&quot;:32,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;KD2H4&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nExplain the video's components, including its characters, setting, and plot.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person washing dishes in a beige sink. They use a green and red dish brush to scrub a spoon under running water from the faucet. The sink has some stains, and there are other dishes, including a bowl and a plate, stacked on the left side. The person is wearing a black sleeve, and the background includes a white dish rack and a green dish soap bottle on the counter. The person continues to wash the dishes, scrubbing the spoon under the running water. After finishing with the spoon, they turn off the faucet and place the cleaned spoon in the sink. They then wash their hands under the faucet, rubbing them together to clean them, and dry their hands before stepping away from the sink. As they walk away, more of their black clothing and the beige tiled floor are revealed. The video then transitions to the person standing in front of a white oven. They are still wearing black clothing, and the beige tiled floor is visible. The person opens the oven door, revealing the interior with metal racks. The oven appears to be empty and has some stains and grime. After inspecting the oven, the person closes the oven door and stands in front of it, with the beige tiled floor still visible, bringing the video to a close.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nExplain the video's components, including its characters, setting, and plot.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person washing dishes in a beige sink. They use a green and red dish brush to scrub a spoon under running water from the faucet. The sink has some stains, and there are other dishes, including a bowl and a plate, stacked on the left side. The person is wearing a black sleeve, and the background includes a white dish rack and a green dish soap bottle on the counter. The person continues to wash the dishes, scrubbing the spoon under the running water. After finishing with the spoon, they turn off the faucet and place the cleaned spoon in the sink. They then wash their hands under the faucet, rubbing them together to clean them, and dry their hands before stepping away from the sink. As they walk away, more of their black clothing and the beige tiled floor are revealed. The video then transitions to the person standing in front of a white oven. They are still wearing black clothing, and the beige tiled floor is visible. The person opens the oven door, revealing the interior with metal racks. The oven appears to be empty and has some stains and grime. After inspecting the oven, the person closes the oven door and stands in front of it, with the beige tiled floor still visible, bringing the video to a close.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/KD2H4.mp4&quot;}}},{&quot;rowIdx&quot;:33,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;KSIYE&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nIllustrate the video's narrative journey, scene by scene, with attention to detail.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person in an orange shirt and dark pants standing at the bottom of a cluttered staircase in a house. The person is holding a broom and is actively cleaning and organizing the area, which is filled with various household items, including a laundry basket. The background features wooden walls and a small window at the top of the stairs, allowing natural light to filter in. The person diligently sweeps and picks up items from the floor and stairs, making their way up the staircase. As the person progresses up the stairs, they continue to sweep and organize, focusing on the steps and the items scattered on them. The staircase remains cluttered, but the person is determined to tidy up the space. The background remains consistent with wooden walls and the small window providing natural light. The video develops through the person's continued efforts to clean the staircase, but it takes an unexpected turn when the person accidentally trips and falls on the stairs. This causes the laundry basket to tumble down, scattering clothes all over the steps. The video wraps up with the person lying on the stairs, recovering from the fall, surrounded by the scattered clothes and the laundry basket.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nIllustrate the video's narrative journey, scene by scene, with attention to detail.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person in an orange shirt and dark pants standing at the bottom of a cluttered staircase in a house. The person is holding a broom and is actively cleaning and organizing the area, which is filled with various household items, including a laundry basket. The background features wooden walls and a small window at the top of the stairs, allowing natural light to filter in. The person diligently sweeps and picks up items from the floor and stairs, making their way up the staircase. As the person progresses up the stairs, they continue to sweep and organize, focusing on the steps and the items scattered on them. The staircase remains cluttered, but the person is determined to tidy up the space. The background remains consistent with wooden walls and the small window providing natural light. The video develops through the person's continued efforts to clean the staircase, but it takes an unexpected turn when the person accidentally trips and falls on the stairs. This causes the laundry basket to tumble down, scattering clothes all over the steps. The video wraps up with the person lying on the stairs, recovering from the fall, surrounded by the scattered clothes and the laundry basket.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/KSIYE.mp4&quot;}}},{&quot;rowIdx&quot;:34,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;KX90W&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDissect the video's content, explaining each element thoroughly.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a view of a hallway leading to a kitchen. A person dressed in black clothing and black shoes walks into the hallway from the kitchen, followed closely by a white dog with black spots. The person opens a white door on the right side of the hallway and retrieves a pair of purple and white shoes from inside. The dog remains close to the person throughout this sequence. The person stands near the open door, holding the shoes, while the dog stays nearby. The scene develops as the person, still in the hallway, starts to put on the purple and white shoes, lifting one foot and then the other. The dog remains close, occasionally moving around the person's feet and showing interest in the shoes. The person bends down to adjust the shoes, and the dog continues to stay close. The person then stands upright, having put on both shoes, with the dog by their side. The video wraps up with the person bending down to adjust their shoes once more while the dog stands nearby. The person then stands upright, interacts with the dog, who sits down and looks up at them. The person turns and walks towards a door at the end of the hallway, with the dog following closely. The person opens the door, and both the person and the dog exit through it. The video concludes with the door closing behind them, leaving the hallway empty except for a pair of black shoes on the floor.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDissect the video's content, explaining each element thoroughly.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a view of a hallway leading to a kitchen. A person dressed in black clothing and black shoes walks into the hallway from the kitchen, followed closely by a white dog with black spots. The person opens a white door on the right side of the hallway and retrieves a pair of purple and white shoes from inside. The dog remains close to the person throughout this sequence. The person stands near the open door, holding the shoes, while the dog stays nearby. The scene develops as the person, still in the hallway, starts to put on the purple and white shoes, lifting one foot and then the other. The dog remains close, occasionally moving around the person's feet and showing interest in the shoes. The person bends down to adjust the shoes, and the dog continues to stay close. The person then stands upright, having put on both shoes, with the dog by their side. The video wraps up with the person bending down to adjust their shoes once more while the dog stands nearby. The person then stands upright, interacts with the dog, who sits down and looks up at them. The person turns and walks towards a door at the end of the hallway, with the dog following closely. The person opens the door, and both the person and the dog exit through it. The video concludes with the door closing behind them, leaving the hallway empty except for a pair of black shoes on the floor.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/KX90W.mp4&quot;}}},{&quot;rowIdx&quot;:35,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;LFBCQ&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite a comprehensive depiction of the entire video clip.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person wearing a black hoodie standing in a brightly lit workshop or storage area. The room is filled with various tools and equipment, including a white shovel hanging on the wall and a cabinet. The person is facing a wooden door with a frosted glass window, which is partially open. They open the cabinet door and start searching inside, moving their hands and shifting their position slightly. The scene develops as the person continues to look inside the cabinet, with the door to the room remaining ajar. The narrative progresses with the person still in the same workshop or storage area, now turning away from the cabinet and walking towards the wooden door with the frosted glass window. The cabinet door remains open, and the white shovel is still visible on the wall. The lighting continues to be bright, provided by a fluorescent fixture on the ceiling. The video wraps up with the person walking towards the door, leaving the cabinet door open and the room's door ajar.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite a comprehensive depiction of the entire video clip.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person wearing a black hoodie standing in a brightly lit workshop or storage area. The room is filled with various tools and equipment, including a white shovel hanging on the wall and a cabinet. The person is facing a wooden door with a frosted glass window, which is partially open. They open the cabinet door and start searching inside, moving their hands and shifting their position slightly. The scene develops as the person continues to look inside the cabinet, with the door to the room remaining ajar. The narrative progresses with the person still in the same workshop or storage area, now turning away from the cabinet and walking towards the wooden door with the frosted glass window. The cabinet door remains open, and the white shovel is still visible on the wall. The lighting continues to be bright, provided by a fluorescent fixture on the ceiling. The video wraps up with the person walking towards the door, leaving the cabinet door open and the room's door ajar.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/LFBCQ.mp4&quot;}}},{&quot;rowIdx&quot;:36,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;M1B7N&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDetail the video's plot development, character arcs, and thematic elements.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a cozy, well-organized kitchen with wooden cabinets and a window in the background. A person wearing a light gray sweatshirt and dark pants is seen standing at the stove, stirring something in a red pot. The atmosphere is calm and domestic, emphasizing the routine activity of cooking. The person remains focused on their task, occasionally adjusting their position slightly. Various kitchen utensils and a large green plant on the counter add to the homely feel of the scene. The person then moves to the right side of the kitchen, reaching up to open a cabinet above the counter, taking out a plate, and returning to the stove area. The scene continues to highlight the routine activity of cooking and preparing food. Finally, the person is seen at the counter near the stove, handling a white mug with black polka dots. They place the mug on the counter, pick it up again, and move towards the microwave. Opening the microwave, they place the mug inside, maintaining the calm and domestic atmosphere of the kitchen. The video focuses on the simple, everyday activities of cooking and preparing a hot beverage, creating a sense of routine and tranquility.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDetail the video's plot development, character arcs, and thematic elements.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a cozy, well-organized kitchen with wooden cabinets and a window in the background. A person wearing a light gray sweatshirt and dark pants is seen standing at the stove, stirring something in a red pot. The atmosphere is calm and domestic, emphasizing the routine activity of cooking. The person remains focused on their task, occasionally adjusting their position slightly. Various kitchen utensils and a large green plant on the counter add to the homely feel of the scene. The person then moves to the right side of the kitchen, reaching up to open a cabinet above the counter, taking out a plate, and returning to the stove area. The scene continues to highlight the routine activity of cooking and preparing food. Finally, the person is seen at the counter near the stove, handling a white mug with black polka dots. They place the mug on the counter, pick it up again, and move towards the microwave. Opening the microwave, they place the mug inside, maintaining the calm and domestic atmosphere of the kitchen. The video focuses on the simple, everyday activities of cooking and preparing a hot beverage, creating a sense of routine and tranquility.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/M1B7N.mp4&quot;}}},{&quot;rowIdx&quot;:37,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;MN1O2&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nCharacterize each scene of the video using a detailed description.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a room with dark red walls, creating a warm and cozy atmosphere. Prominently displayed on the wall is a framed poster featuring a turtle carrying a glass of Guinness on its back, with the text 'Thinking about a GUINNESS?' (¿Pensando en una GUINNESS?) above it, suggesting a playful and inviting message. Below the poster, a wooden table surrounded by four wooden chairs is situated, and to the left of the poster, a mirror with a decorative frame adds a touch of elegance to the room. A person wearing a blue jacket and dark pants enters the scene from the left and begins to interact with the chairs, moving them around the table, occasionally glancing at the mirror, perhaps to check their appearance or the arrangement of the room. The background includes a wooden shelf with various items on it, located on the right side of the scene, adding to the room's character.\n\nThe scene continues with the person still present, adjusting the wooden chairs around the table. They move one of the chairs closer to the table and then look into the decorative mirror, possibly to ensure everything is in order. The person then brings a vacuum cleaner into the scene and starts vacuuming the floor around the table and chairs, indicating a focus on tidiness and cleanliness. The wooden shelf with various items remains visible on the right side, maintaining the room's consistent decor. The video wraps up with the person continuing to vacuum the floor, emphasizing their dedication to keeping the space neat and orderly.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nCharacterize each scene of the video using a detailed description.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a room with dark red walls, creating a warm and cozy atmosphere. Prominently displayed on the wall is a framed poster featuring a turtle carrying a glass of Guinness on its back, with the text 'Thinking about a GUINNESS?' (¿Pensando en una GUINNESS?) above it, suggesting a playful and inviting message. Below the poster, a wooden table surrounded by four wooden chairs is situated, and to the left of the poster, a mirror with a decorative frame adds a touch of elegance to the room. A person wearing a blue jacket and dark pants enters the scene from the left and begins to interact with the chairs, moving them around the table, occasionally glancing at the mirror, perhaps to check their appearance or the arrangement of the room. The background includes a wooden shelf with various items on it, located on the right side of the scene, adding to the room's character.\\n\\nThe scene continues with the person still present, adjusting the wooden chairs around the table. They move one of the chairs closer to the table and then look into the decorative mirror, possibly to ensure everything is in order. The person then brings a vacuum cleaner into the scene and starts vacuuming the floor around the table and chairs, indicating a focus on tidiness and cleanliness. The wooden shelf with various items remains visible on the right side, maintaining the room's consistent decor. The video wraps up with the person continuing to vacuum the floor, emphasizing their dedication to keeping the space neat and orderly.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/MN1O2.mp4&quot;}}},{&quot;rowIdx&quot;:38,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;PMG3A&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite a comprehensive depiction of the entire video clip.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a dimly lit room with a single light source, a lamp on a small table. The room is cluttered with various items scattered on the floor, including a folded chair and some cables. The walls are plain, and a small window is covered with a white cloth. The scene involves two individuals: one wearing a red hoodie and a long blue skirt, and the other dressed in a green long-sleeve shirt and a pink apron. The person in the red hoodie is standing and interacting with the table, adjusting items and pouring liquid from a pitcher into a cup, while the person in the green shirt is seated on a metal chair, focused on a large book or possibly a laptop. The interaction between them is casual and domestic, with the standing person attending to various tasks and the seated person engaged in reading or working on the device. The video continues with the person in the red hoodie crouching near the table, interacting with items on the floor, and then standing up to place a bowl on the table next to the cup. The seated person remains focused on the book or laptop. The standing person continues to adjust items on the table, including pouring more liquid from the pitcher into the cup. The video concludes with the person in the red hoodie standing near the table, holding a white container, and moving around the table to interact with the items on it. The standing person then walks towards the right side of the room, possibly to attend to other tasks, while the seated person remains engaged in reading or working on the device. The overall interaction remains casual and domestic throughout the video.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite a comprehensive depiction of the entire video clip.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a dimly lit room with a single light source, a lamp on a small table. The room is cluttered with various items scattered on the floor, including a folded chair and some cables. The walls are plain, and a small window is covered with a white cloth. The scene involves two individuals: one wearing a red hoodie and a long blue skirt, and the other dressed in a green long-sleeve shirt and a pink apron. The person in the red hoodie is standing and interacting with the table, adjusting items and pouring liquid from a pitcher into a cup, while the person in the green shirt is seated on a metal chair, focused on a large book or possibly a laptop. The interaction between them is casual and domestic, with the standing person attending to various tasks and the seated person engaged in reading or working on the device. The video continues with the person in the red hoodie crouching near the table, interacting with items on the floor, and then standing up to place a bowl on the table next to the cup. The seated person remains focused on the book or laptop. The standing person continues to adjust items on the table, including pouring more liquid from the pitcher into the cup. The video concludes with the person in the red hoodie standing near the table, holding a white container, and moving around the table to interact with the items on it. The standing person then walks towards the right side of the room, possibly to attend to other tasks, while the seated person remains engaged in reading or working on the device. The overall interaction remains casual and domestic throughout the video.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/PMG3A.mp4&quot;}}},{&quot;rowIdx&quot;:39,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;QEXP4&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nShare a detailed breakdown of the video's storyline and visuals.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person sitting on a bed in a dimly lit room, wearing a light-colored shirt with rolled-up sleeves and blue jeans. They are engrossed in reading or examining a newspaper or a large sheet of paper. The room features a wardrobe with a mirror on the door, reflecting part of the room, including a bed with a pink bedspread. The background includes a door and some hanging clothes, creating a typical bedroom setting. The atmosphere is calm and quiet, with the person focused on the paper.\n\nThe scene develops as the person stands up from the bed and walks towards the wardrobe with the mirror. They open the wardrobe door and begin searching through the items inside, appearing to look for something specific. The room remains dimly lit, with the same background elements visible, including the bed with the pink bedspread and the door. The calm and quiet atmosphere persists as the person continues their focused task.\n\nThe video wraps up with the person still standing in front of the open wardrobe, actively searching through the shelves. They move items around methodically, occasionally pausing to examine things more closely. The wardrobe door with the mirror continues to reflect part of the room, including the bed with the pink bedspread. The room remains dimly lit, maintaining the calm and quiet atmosphere throughout. The background elements, such as the door and the hanging clothes, are still visible, reinforcing the typical bedroom setting.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nShare a detailed breakdown of the video's storyline and visuals.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person sitting on a bed in a dimly lit room, wearing a light-colored shirt with rolled-up sleeves and blue jeans. They are engrossed in reading or examining a newspaper or a large sheet of paper. The room features a wardrobe with a mirror on the door, reflecting part of the room, including a bed with a pink bedspread. The background includes a door and some hanging clothes, creating a typical bedroom setting. The atmosphere is calm and quiet, with the person focused on the paper.\\n\\nThe scene develops as the person stands up from the bed and walks towards the wardrobe with the mirror. They open the wardrobe door and begin searching through the items inside, appearing to look for something specific. The room remains dimly lit, with the same background elements visible, including the bed with the pink bedspread and the door. The calm and quiet atmosphere persists as the person continues their focused task.\\n\\nThe video wraps up with the person still standing in front of the open wardrobe, actively searching through the shelves. They move items around methodically, occasionally pausing to examine things more closely. The wardrobe door with the mirror continues to reflect part of the room, including the bed with the pink bedspread. The room remains dimly lit, maintaining the calm and quiet atmosphere throughout. The background elements, such as the door and the hanging clothes, are still visible, reinforcing the typical bedroom setting.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/QEXP4.mp4&quot;}}},{&quot;rowIdx&quot;:40,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;SFHY2&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite a comprehensive depiction of the entire video clip.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a view of a closed white door in a room with beige walls. To the left of the door, there is a window with closed blinds, and below the window is a small yellow table with a few items on it. To the right of the door, a dining area with a table and chairs is visible, and a broom is leaning against the wall. The door opens, and a person wearing a black t-shirt with a graphic design and text that reads 'I WANT TO BELIEVE,' dark pants, and a purple cap worn backward enters the room. The person walks in, holding the door open, and looks around the room, appearing to be searching for something or someone. They then close the door behind them and continue to move further into the room.\n\nThe scene continues in the same room with beige walls and a white door. The person is now standing in front of the closed door, still wearing the same black t-shirt with the text 'I WANT TO BELIEVE,' dark pants, and a purple cap worn backward. They have one hand on their hip and occasionally adjust their cap while looking around the room, possibly still searching for something or someone. The small yellow table with a few items on it remains visible to the left of the door, and the dining area with a table and chairs, as well as the broom leaning against the wall, are still present to the right of the door. The window with closed blinds continues to allow some light into the room.\n\nThe video wraps up with the person still in the same room, standing near the closed door. They continue to wear the black t-shirt with the text 'I WANT TO BELIEVE,' dark pants, and a purple cap worn backward. With one hand on their hip, they appear to be persistently looking around the room, possibly still in search of something or someone. The small yellow table with a few items on it is still visible to the left of the door, and the dining area with a table and chairs, as well as the broom leaning against the wall, remain to the right of the door. The window with closed blinds continues to let in some light. The person occasionally adjusts their cap while continuing to look around, maintaining the sense of searching throughout the video.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite a comprehensive depiction of the entire video clip.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a view of a closed white door in a room with beige walls. To the left of the door, there is a window with closed blinds, and below the window is a small yellow table with a few items on it. To the right of the door, a dining area with a table and chairs is visible, and a broom is leaning against the wall. The door opens, and a person wearing a black t-shirt with a graphic design and text that reads 'I WANT TO BELIEVE,' dark pants, and a purple cap worn backward enters the room. The person walks in, holding the door open, and looks around the room, appearing to be searching for something or someone. They then close the door behind them and continue to move further into the room.\\n\\nThe scene continues in the same room with beige walls and a white door. The person is now standing in front of the closed door, still wearing the same black t-shirt with the text 'I WANT TO BELIEVE,' dark pants, and a purple cap worn backward. They have one hand on their hip and occasionally adjust their cap while looking around the room, possibly still searching for something or someone. The small yellow table with a few items on it remains visible to the left of the door, and the dining area with a table and chairs, as well as the broom leaning against the wall, are still present to the right of the door. The window with closed blinds continues to allow some light into the room.\\n\\nThe video wraps up with the person still in the same room, standing near the closed door. They continue to wear the black t-shirt with the text 'I WANT TO BELIEVE,' dark pants, and a purple cap worn backward. With one hand on their hip, they appear to be persistently looking around the room, possibly still in search of something or someone. The small yellow table with a few items on it is still visible to the left of the door, and the dining area with a table and chairs, as well as the broom leaning against the wall, remain to the right of the door. The window with closed blinds continues to let in some light. The person occasionally adjusts their cap while continuing to look around, maintaining the sense of searching throughout the video.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/SFHY2.mp4&quot;}}},{&quot;rowIdx&quot;:41,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;T0LOH&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWalk through the video, detailing its key moments and features.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a closed white door adorned with multiple locks and a peephole, establishing a sense of security or secrecy. To the right of the door, a colorful painting leans against the wall, adding a touch of vibrancy to the otherwise stark setting. A person dressed in a dark blue shirt and black pants enters from the left, carrying a black bundle close to their chest. They approach the door, momentarily adjusting the bundle and glancing down at it, before turning slightly to the right and reaching towards the painting. This action suggests a possible connection or interaction between the bundle and the painting.\n\nThe narrative develops as the person, still holding the black bundle, moves a black chair in front of the white door. The colorful painting remains in its position, maintaining its role as a significant visual element. The person then sits on the chair, facing the camera, and continues to adjust and look down at the bundle, indicating its importance. After a brief moment, they stand up and exit the frame to the left, still clutching the bundle, leaving the chair in front of the door.\n\nThe video wraps up with a static scene of the white door, now with the black chair positioned in front of it. The colorful painting continues to lean against the wall, and the person along with the black bundle are no longer visible. The unchanged background, featuring the white door and the painting, emphasizes a sense of stillness and unresolved mystery, as no further movement or elements are introduced.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWalk through the video, detailing its key moments and features.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a closed white door adorned with multiple locks and a peephole, establishing a sense of security or secrecy. To the right of the door, a colorful painting leans against the wall, adding a touch of vibrancy to the otherwise stark setting. A person dressed in a dark blue shirt and black pants enters from the left, carrying a black bundle close to their chest. They approach the door, momentarily adjusting the bundle and glancing down at it, before turning slightly to the right and reaching towards the painting. This action suggests a possible connection or interaction between the bundle and the painting.\\n\\nThe narrative develops as the person, still holding the black bundle, moves a black chair in front of the white door. The colorful painting remains in its position, maintaining its role as a significant visual element. The person then sits on the chair, facing the camera, and continues to adjust and look down at the bundle, indicating its importance. After a brief moment, they stand up and exit the frame to the left, still clutching the bundle, leaving the chair in front of the door.\\n\\nThe video wraps up with a static scene of the white door, now with the black chair positioned in front of it. The colorful painting continues to lean against the wall, and the person along with the black bundle are no longer visible. The unchanged background, featuring the white door and the painting, emphasizes a sense of stillness and unresolved mystery, as no further movement or elements are introduced.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/T0LOH.mp4&quot;}}},{&quot;rowIdx&quot;:42,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;T3L2A&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWalk through the video, detailing its key moments and features.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a view of a dimly lit hallway with yellowish lighting. On the left side of the hallway, a framed picture depicting three people leans against the wall. The hallway leads to an open door, beyond which another room is visible. A person wearing dark clothing and a hat walks into the frame from the right side, holding a bowl and a spoon. They stop in the middle of the hallway, facing the camera, and begin eating from the bowl. The scene continues with the person still standing in the hallway, eating from the bowl. They raise their arm, possibly gesturing or pointing towards something on the wall. The framed picture of three people remains leaning against the wall. The video progresses with the same person in the same position, continuing to eat from the bowl and occasionally gesturing towards the wall. The hallway's dim lighting and yellowish hue remain consistent throughout. In the final part of the video, another framed picture is visible hanging on the wall above the leaning picture. The person continues to eat from the bowl while standing in the hallway, maintaining the same posture and actions as before.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWalk through the video, detailing its key moments and features.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a view of a dimly lit hallway with yellowish lighting. On the left side of the hallway, a framed picture depicting three people leans against the wall. The hallway leads to an open door, beyond which another room is visible. A person wearing dark clothing and a hat walks into the frame from the right side, holding a bowl and a spoon. They stop in the middle of the hallway, facing the camera, and begin eating from the bowl. The scene continues with the person still standing in the hallway, eating from the bowl. They raise their arm, possibly gesturing or pointing towards something on the wall. The framed picture of three people remains leaning against the wall. The video progresses with the same person in the same position, continuing to eat from the bowl and occasionally gesturing towards the wall. The hallway's dim lighting and yellowish hue remain consistent throughout. In the final part of the video, another framed picture is visible hanging on the wall above the leaning picture. The person continues to eat from the bowl while standing in the hallway, maintaining the same posture and actions as before.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/T3L2A.mp4&quot;}}},{&quot;rowIdx&quot;:43,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;TVS1P&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nNarrate the video's unfolding events in a descriptive manner.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a man sitting on a toilet in a well-lit bathroom, dressed in a gray shirt, dark pants, and black shoes. He appears to be in distress or deep thought, holding a white towel to his face and leaning forward with his elbow resting on his knee. The bathroom features white walls, a white vanity with a mirror above it, a soap dispenser, and a small object on the vanity. The man remains in this position, showing minimal movement. The scene continues with the man in the same posture, maintaining his expression of distress or deep thought. The bathroom setting remains unchanged. Finally, the man starts to stand up, still holding the towel. As he stands, he adjusts his clothing and begins to move away from the toilet. The lighting in the bathroom suddenly changes to a dim, orange hue, and then the scene transitions to complete darkness, indicating the end of the video.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nNarrate the video's unfolding events in a descriptive manner.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a man sitting on a toilet in a well-lit bathroom, dressed in a gray shirt, dark pants, and black shoes. He appears to be in distress or deep thought, holding a white towel to his face and leaning forward with his elbow resting on his knee. The bathroom features white walls, a white vanity with a mirror above it, a soap dispenser, and a small object on the vanity. The man remains in this position, showing minimal movement. The scene continues with the man in the same posture, maintaining his expression of distress or deep thought. The bathroom setting remains unchanged. Finally, the man starts to stand up, still holding the towel. As he stands, he adjusts his clothing and begins to move away from the toilet. The lighting in the bathroom suddenly changes to a dim, orange hue, and then the scene transitions to complete darkness, indicating the end of the video.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/TVS1P.mp4&quot;}}},{&quot;rowIdx&quot;:44,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;U8X15&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nIllustrate the video's narrative journey, scene by scene, with attention to detail.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a woman standing in a room with vertical blinds covering a sliding door, which is partially open, allowing bright light to enter. She is wearing a red top and dark pants, holding a phone in her right hand. The room features a wooden chair and a small table. The woman adjusts her hair with her left hand while looking towards the door. A man wearing a green shirt and gray shorts enters from the left side and approaches her. They engage in a conversation, with the man gesturing with his hands and the woman listening attentively. Both individuals stand near the door, continuing their conversation.\n\nThe scene develops with the woman still near the sliding door, talking and gesturing with her right hand near her face. The man, now holding a phone, raises it as if to take a photo or video of the woman. She continues to gesture and talk, occasionally looking towards the man and the door. The man remains focused on his phone, capturing her actions. The woman bends down and then stands up, adjusting her hair, while the man continues to hold his phone up, possibly recording or taking pictures.\n\nThe video wraps up with the woman standing near the sliding door, holding her phone in her right hand and looking towards the door. She adjusts her hair with her left hand and places her left hand on her hip. The camera angle changes to a closer view of the woman as she stands with her hands on her hips, looking towards the camera. The video concludes with the woman walking towards the camera, revealing her bare feet and the wooden floor of the room.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nIllustrate the video's narrative journey, scene by scene, with attention to detail.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a woman standing in a room with vertical blinds covering a sliding door, which is partially open, allowing bright light to enter. She is wearing a red top and dark pants, holding a phone in her right hand. The room features a wooden chair and a small table. The woman adjusts her hair with her left hand while looking towards the door. A man wearing a green shirt and gray shorts enters from the left side and approaches her. They engage in a conversation, with the man gesturing with his hands and the woman listening attentively. Both individuals stand near the door, continuing their conversation.\\n\\nThe scene develops with the woman still near the sliding door, talking and gesturing with her right hand near her face. The man, now holding a phone, raises it as if to take a photo or video of the woman. She continues to gesture and talk, occasionally looking towards the man and the door. The man remains focused on his phone, capturing her actions. The woman bends down and then stands up, adjusting her hair, while the man continues to hold his phone up, possibly recording or taking pictures.\\n\\nThe video wraps up with the woman standing near the sliding door, holding her phone in her right hand and looking towards the door. She adjusts her hair with her left hand and places her left hand on her hip. The camera angle changes to a closer view of the woman as she stands with her hands on her hips, looking towards the camera. The video concludes with the woman walking towards the camera, revealing her bare feet and the wooden floor of the room.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/U8X15.mp4&quot;}}},{&quot;rowIdx&quot;:45,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;UIPJB&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nNarrate the video's unfolding events in a descriptive manner.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person standing in a doorway, framed by a checkered curtain in the background. They are dressed in a maroon and black striped long-sleeve shirt and black pants. In their right hand, they hold a foil-covered dish, while their left hand carries a bundle of colorful towels. The setting appears to be a home, with a white door on the left and a light blue wall on the right. The person looks down at the dish and then to the side, seemingly inspecting or contemplating something. This moment of pause and consideration continues as the person maintains their posture and actions, reinforcing the sense of introspection. The scene remains unchanged, with the same background and setting. Finally, the person shifts their gaze from the dish to looking up and straight ahead, transitioning from a contemplative stance to a more neutral one. The consistent setting and subtle changes in the person's posture and gaze create a narrative of thoughtful reflection and eventual resolution.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nNarrate the video's unfolding events in a descriptive manner.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person standing in a doorway, framed by a checkered curtain in the background. They are dressed in a maroon and black striped long-sleeve shirt and black pants. In their right hand, they hold a foil-covered dish, while their left hand carries a bundle of colorful towels. The setting appears to be a home, with a white door on the left and a light blue wall on the right. The person looks down at the dish and then to the side, seemingly inspecting or contemplating something. This moment of pause and consideration continues as the person maintains their posture and actions, reinforcing the sense of introspection. The scene remains unchanged, with the same background and setting. Finally, the person shifts their gaze from the dish to looking up and straight ahead, transitioning from a contemplative stance to a more neutral one. The consistent setting and subtle changes in the person's posture and gaze create a narrative of thoughtful reflection and eventual resolution.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/UIPJB.mp4&quot;}}},{&quot;rowIdx&quot;:46,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;UL9LN&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite an in-depth depiction of the video, covering all its aspects.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a close-up view of a green bottle with a yellow cap, likely a detergent or cleaning product, placed on a white surface, possibly a washing machine. To the left, a blue box with floral graphics and some text is partially visible. In the background, a hallway with an open closet door reveals hanging clothes. A person wearing a dark shirt enters the frame from the right, opens the closet door wider, and starts to move items inside the closet, appearing to organize or search for something among the hanging clothes. The setting appears to be a laundry area or utility room. The focus remains on the green bottle in the foreground, while the background activity provides context to the setting. The person continues to bend down and move items inside the closet, suggesting they are still organizing or searching for something. The scene maintains its setting in the laundry area, with the green bottle prominently in the foreground. The person, wearing a dark shirt with a graphic of a blue donkey and the text 'I'M A SMART ASS' (indicating a humorous or self-deprecating message), is seen moving items inside the closet. They then step closer to the foreground, holding a dark piece of clothing, and begin to organize or inspect it. The video maintains its focus on the green bottle with the background activity providing context, creating a coherent narrative of someone engaged in laundry-related tasks in a utility room.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite an in-depth depiction of the video, covering all its aspects.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a close-up view of a green bottle with a yellow cap, likely a detergent or cleaning product, placed on a white surface, possibly a washing machine. To the left, a blue box with floral graphics and some text is partially visible. In the background, a hallway with an open closet door reveals hanging clothes. A person wearing a dark shirt enters the frame from the right, opens the closet door wider, and starts to move items inside the closet, appearing to organize or search for something among the hanging clothes. The setting appears to be a laundry area or utility room. The focus remains on the green bottle in the foreground, while the background activity provides context to the setting. The person continues to bend down and move items inside the closet, suggesting they are still organizing or searching for something. The scene maintains its setting in the laundry area, with the green bottle prominently in the foreground. The person, wearing a dark shirt with a graphic of a blue donkey and the text 'I'M A SMART ASS' (indicating a humorous or self-deprecating message), is seen moving items inside the closet. They then step closer to the foreground, holding a dark piece of clothing, and begin to organize or inspect it. The video maintains its focus on the green bottle with the background activity providing context, creating a coherent narrative of someone engaged in laundry-related tasks in a utility room.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/UL9LN.mp4&quot;}}},{&quot;rowIdx&quot;:47,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;UZ2AL&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nIllustrate the video's narrative journey, scene by scene, with attention to detail.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a cozy kitchen adorned with wooden cabinets and a tiled backsplash. A person dressed in a blue hoodie and black pants stands near the stove, seemingly engaged in an activity such as turning a knob or inspecting something on the stovetop. The kitchen counter is cluttered with everyday items, including a dish rack filled with dishes, a toaster, and a kettle, adding to the lived-in feel of the space. A black towel hangs from one of the cabinet handles, contributing to the homely atmosphere. Throughout the video, the person remains focused on the stove area, suggesting they are either cooking or preparing something.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nIllustrate the video's narrative journey, scene by scene, with attention to detail.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a cozy kitchen adorned with wooden cabinets and a tiled backsplash. A person dressed in a blue hoodie and black pants stands near the stove, seemingly engaged in an activity such as turning a knob or inspecting something on the stovetop. The kitchen counter is cluttered with everyday items, including a dish rack filled with dishes, a toaster, and a kettle, adding to the lived-in feel of the space. A black towel hangs from one of the cabinet handles, contributing to the homely atmosphere. Throughout the video, the person remains focused on the stove area, suggesting they are either cooking or preparing something.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/UZ2AL.mp4&quot;}}},{&quot;rowIdx&quot;:48,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;V1BX9&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDescribe every scene and its significance in the video.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a busy kitchen with black tiled walls and a marble floor. The kitchen counter is cluttered with various utensils, pots, and pans, creating a sense of everyday activity. A person wearing a green hoodie and jeans stands by the counter, eating from a bowl, while a blue folding chair is placed nearby. The scene develops as another person, dressed in a blue and white striped sweater, a purple top, and black pants, enters the kitchen carrying a tray with items. This person places the tray on the counter and adjusts the blue folding chair, adding to the sense of movement and activity in the kitchen. The first person continues eating while the second person moves around the kitchen, eventually standing near the counter. The video continues to focus on the same kitchen setting, with the person in the green hoodie still eating from the bowl. The blue folding chair is now positioned closer to the counter, and the person in the blue and white striped sweater is seen walking away from the camera towards the counter. The overall scene remains busy and cluttered, emphasizing the daily routine and interactions between the two individuals in the kitchen.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDescribe every scene and its significance in the video.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a busy kitchen with black tiled walls and a marble floor. The kitchen counter is cluttered with various utensils, pots, and pans, creating a sense of everyday activity. A person wearing a green hoodie and jeans stands by the counter, eating from a bowl, while a blue folding chair is placed nearby. The scene develops as another person, dressed in a blue and white striped sweater, a purple top, and black pants, enters the kitchen carrying a tray with items. This person places the tray on the counter and adjusts the blue folding chair, adding to the sense of movement and activity in the kitchen. The first person continues eating while the second person moves around the kitchen, eventually standing near the counter. The video continues to focus on the same kitchen setting, with the person in the green hoodie still eating from the bowl. The blue folding chair is now positioned closer to the counter, and the person in the blue and white striped sweater is seen walking away from the camera towards the counter. The overall scene remains busy and cluttered, emphasizing the daily routine and interactions between the two individuals in the kitchen.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/V1BX9.mp4&quot;}}},{&quot;rowIdx&quot;:49,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;V3RAX&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nCharacterize each scene of the video using a detailed description.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person lying on a bed covered with a patterned blanket in a dimly lit room with blue walls, a wooden window with a cloth hanging on it, and clothes hanging on the wall to the left. The person, initially still, starts to move slightly under the blanket, then sits up, revealing a purple shirt, and begins to put on a white sock. After adjusting the sock, the person lies back down, covering themselves with the blanket again. The scene transitions to the person lying on the bed once more, covered with the same patterned blanket. They sit up, adjust the blanket, and begin to put on the white sock again. Fully sitting up, the person stands and moves towards the hanging clothes, picking up a white garment. They walk towards a table covered with a patterned cloth, which holds a framed picture and other items, and start handling the white garment, seemingly preparing or folding it. The final scene shows the person, still in the purple shirt, standing at the same table, continuing to handle the white garment. They then pick up a small plate with food and start eating from it while standing. The consistent dim lighting throughout the video suggests it might be early morning or late evening, creating a coherent narrative of a person’s routine in a modest, dimly lit room.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nCharacterize each scene of the video using a detailed description.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person lying on a bed covered with a patterned blanket in a dimly lit room with blue walls, a wooden window with a cloth hanging on it, and clothes hanging on the wall to the left. The person, initially still, starts to move slightly under the blanket, then sits up, revealing a purple shirt, and begins to put on a white sock. After adjusting the sock, the person lies back down, covering themselves with the blanket again. The scene transitions to the person lying on the bed once more, covered with the same patterned blanket. They sit up, adjust the blanket, and begin to put on the white sock again. Fully sitting up, the person stands and moves towards the hanging clothes, picking up a white garment. They walk towards a table covered with a patterned cloth, which holds a framed picture and other items, and start handling the white garment, seemingly preparing or folding it. The final scene shows the person, still in the purple shirt, standing at the same table, continuing to handle the white garment. They then pick up a small plate with food and start eating from it while standing. The consistent dim lighting throughout the video suggests it might be early morning or late evening, creating a coherent narrative of a person’s routine in a modest, dimly lit room.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/V3RAX.mp4&quot;}}},{&quot;rowIdx&quot;:50,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;V4YCA&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nCharacterize each scene of the video using a detailed description.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person wearing a dark hoodie and light-colored pants standing in front of a refrigerator in a cluttered kitchen area. The person opens the refrigerator door, which has various items attached to it, including a piece of paper and some magnets. They bend down and look inside the refrigerator, which is filled with various bottles and containers on the door shelves, seemingly searching for something. The scene develops as the person continues to search through the refrigerator, moving some bottles around and eventually reaching for a plastic-wrapped item from the door shelf. They examine the item closely before deciding to take it. The person then starts to walk away from the refrigerator, closing the door behind them. The video wraps up with the person walking towards a trash can with a black trash bag inside. They lift the lid of the trash can, appearing to be about to dispose of something, and the video concludes with the person standing next to the trash can with the lid open.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nCharacterize each scene of the video using a detailed description.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person wearing a dark hoodie and light-colored pants standing in front of a refrigerator in a cluttered kitchen area. The person opens the refrigerator door, which has various items attached to it, including a piece of paper and some magnets. They bend down and look inside the refrigerator, which is filled with various bottles and containers on the door shelves, seemingly searching for something. The scene develops as the person continues to search through the refrigerator, moving some bottles around and eventually reaching for a plastic-wrapped item from the door shelf. They examine the item closely before deciding to take it. The person then starts to walk away from the refrigerator, closing the door behind them. The video wraps up with the person walking towards a trash can with a black trash bag inside. They lift the lid of the trash can, appearing to be about to dispose of something, and the video concludes with the person standing next to the trash can with the lid open.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/V4YCA.mp4&quot;}}},{&quot;rowIdx&quot;:51,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;VL20E&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDetail the video's plot development, character arcs, and thematic elements.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person sitting at a dining table in a well-lit room with large windows in the background. The table is covered with a green tablecloth and has a vase with dried flowers and a small lamp on it. The person is holding and examining a camera, appearing to be focused on it. The room has a calm and cozy atmosphere, with wooden chairs around the table and a carpeted floor. The scene develops as the person continues to examine the camera, then puts it down and appears to be deep in thought, with their hand on their chin. They lean forward, possibly to look at something on the table, and then start to scratch their shoulder. The person stands up from the chair, still scratching their shoulder, and begins to walk away from the table. The video wraps up with the person walking away from the table, still scratching their shoulder. The focus then shifts to the empty dining table and chairs, maintaining the calm and cozy atmosphere of the room. The video concludes with the room being empty, highlighting the dining table and its items.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDetail the video's plot development, character arcs, and thematic elements.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person sitting at a dining table in a well-lit room with large windows in the background. The table is covered with a green tablecloth and has a vase with dried flowers and a small lamp on it. The person is holding and examining a camera, appearing to be focused on it. The room has a calm and cozy atmosphere, with wooden chairs around the table and a carpeted floor. The scene develops as the person continues to examine the camera, then puts it down and appears to be deep in thought, with their hand on their chin. They lean forward, possibly to look at something on the table, and then start to scratch their shoulder. The person stands up from the chair, still scratching their shoulder, and begins to walk away from the table. The video wraps up with the person walking away from the table, still scratching their shoulder. The focus then shifts to the empty dining table and chairs, maintaining the calm and cozy atmosphere of the room. The video concludes with the room being empty, highlighting the dining table and its items.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/VL20E.mp4&quot;}}},{&quot;rowIdx&quot;:52,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;WBJC3&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDetail the video's plot development, character arcs, and thematic elements.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person in a kitchen setting, dressed in a white t-shirt and dark pants. They are holding a green cup and drinking from it. The kitchen is well-organized, featuring shelves with various items, a hanging fruit basket with bananas, a cabinet with food items, and a bottle of red liquid on the right side. After taking a sip from the cup, the person places it on top of the cabinet and then bends down slightly, possibly to look at or reach for something inside the cabinet. The scene transitions to the same person still in the kitchen, now standing in front of the cabinet and facing the shelves. The green cup remains on top of the cabinet, and the hanging fruit basket with bananas is still visible on the left side, with the bottle of red liquid on the right. The person is holding something small in their right hand, possibly a piece of food, and appears to be eating it while looking at the shelves. The scene remains static with the person standing and observing the shelves.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDetail the video's plot development, character arcs, and thematic elements.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person in a kitchen setting, dressed in a white t-shirt and dark pants. They are holding a green cup and drinking from it. The kitchen is well-organized, featuring shelves with various items, a hanging fruit basket with bananas, a cabinet with food items, and a bottle of red liquid on the right side. After taking a sip from the cup, the person places it on top of the cabinet and then bends down slightly, possibly to look at or reach for something inside the cabinet. The scene transitions to the same person still in the kitchen, now standing in front of the cabinet and facing the shelves. The green cup remains on top of the cabinet, and the hanging fruit basket with bananas is still visible on the left side, with the bottle of red liquid on the right. The person is holding something small in their right hand, possibly a piece of food, and appears to be eating it while looking at the shelves. The scene remains static with the person standing and observing the shelves.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/WBJC3.mp4&quot;}}},{&quot;rowIdx&quot;:53,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;WHGAQ&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDescribe every scene and its significance in the video.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person in a pink shirt and dark pants sweeping a black staircase with a red broom in a modern, well-lit interior space with white walls. A wall clock and a framed picture are visible on the wall beside the staircase. The person sweeps the stairs methodically. The scene transitions to another person in a blue shirt standing near a round glass table with red chairs, holding a transparent container in what appears to be a kitchen area with shelves and various items. The person in the blue shirt walks towards the person sweeping the stairs, who continues with the cleaning task. The narrative develops as the person in the blue shirt places the transparent container on the floor near the staircase and bends down to pick up something from the floor before walking away, leaving the container behind. The person in the pink shirt continues to sweep the stairs, focusing on the lower steps. The background remains consistent with white walls, a wall clock, and a framed picture beside the staircase. The video wraps up with the person in the pink shirt still methodically sweeping the lower steps of the staircase, occasionally pausing to adjust their sweeping technique, but remaining focused on ensuring the stairs are clean.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDescribe every scene and its significance in the video.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person in a pink shirt and dark pants sweeping a black staircase with a red broom in a modern, well-lit interior space with white walls. A wall clock and a framed picture are visible on the wall beside the staircase. The person sweeps the stairs methodically. The scene transitions to another person in a blue shirt standing near a round glass table with red chairs, holding a transparent container in what appears to be a kitchen area with shelves and various items. The person in the blue shirt walks towards the person sweeping the stairs, who continues with the cleaning task. The narrative develops as the person in the blue shirt places the transparent container on the floor near the staircase and bends down to pick up something from the floor before walking away, leaving the container behind. The person in the pink shirt continues to sweep the stairs, focusing on the lower steps. The background remains consistent with white walls, a wall clock, and a framed picture beside the staircase. The video wraps up with the person in the pink shirt still methodically sweeping the lower steps of the staircase, occasionally pausing to adjust their sweeping technique, but remaining focused on ensuring the stairs are clean.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/WHGAQ.mp4&quot;}}},{&quot;rowIdx&quot;:54,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;WJ6JM&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nExplore the thematic and visual aspects of the video comprehensively.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a cluttered room with beige walls, a green cabinet, and a desk with a computer monitor displaying a white screen. Various decorative items adorn the walls, including a large feathered piece and a framed picture. A person wearing a blue shirt and plaid pants is seen moving around the room, holding a red broom. They appear to be cleaning or organizing the space, moving items and sweeping the floor. Occasionally, they pause to look at or handle different objects. The person continues their cleaning activities, holding both the broom and another object, possibly a remote or a phone.\n\nThe scene maintains the same cluttered environment as the person, still holding the red broom, examines a small object that looks like a handheld mirror. They stand near the doorway, occasionally looking at the object and then back at the room. The person seems to be contemplating or inspecting the item while the room remains filled with scattered items, including the large feathered decorative piece and the framed picture.\n\nThe video progresses with the person continuing to inspect the handheld mirror while holding the broom, standing near the doorway. They occasionally glance around the room, which remains cluttered with various items. The video concludes with the person still engaged in examining the mirror, maintaining their position near the doorway, surrounded by the same scattered items and decorations.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nExplore the thematic and visual aspects of the video comprehensively.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a cluttered room with beige walls, a green cabinet, and a desk with a computer monitor displaying a white screen. Various decorative items adorn the walls, including a large feathered piece and a framed picture. A person wearing a blue shirt and plaid pants is seen moving around the room, holding a red broom. They appear to be cleaning or organizing the space, moving items and sweeping the floor. Occasionally, they pause to look at or handle different objects. The person continues their cleaning activities, holding both the broom and another object, possibly a remote or a phone.\\n\\nThe scene maintains the same cluttered environment as the person, still holding the red broom, examines a small object that looks like a handheld mirror. They stand near the doorway, occasionally looking at the object and then back at the room. The person seems to be contemplating or inspecting the item while the room remains filled with scattered items, including the large feathered decorative piece and the framed picture.\\n\\nThe video progresses with the person continuing to inspect the handheld mirror while holding the broom, standing near the doorway. They occasionally glance around the room, which remains cluttered with various items. The video concludes with the person still engaged in examining the mirror, maintaining their position near the doorway, surrounded by the same scattered items and decorations.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/WJ6JM.mp4&quot;}}},{&quot;rowIdx&quot;:55,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;WLE0F&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide a comprehensive analysis of the video's content and themes.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a view of a wooden hallway leading to a well-lit bathroom, with natural light streaming through a window. A cloth or towel lies on the wooden floor. A person wearing a dark jacket and plaid pants emerges from the bathroom, carrying a pink bag with a floral pattern. They stop in the hallway, adjust the bag, and appear to search for something inside it. The scene continues with the person still in the hallway, holding the bag and looking around. They then start walking towards the bathroom, briefly stopping to adjust their hair. The person stands near the bathroom door, still holding the pink bag. The narrative progresses as the person begins to walk away from the bathroom, down the hallway, while still holding the bag. They briefly stop, turn around to look back towards the bathroom, and then continue walking down the hallway. The video concludes with the person exiting the frame, leaving the hallway empty except for the cloth or towel still lying on the wooden floor.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide a comprehensive analysis of the video's content and themes.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a view of a wooden hallway leading to a well-lit bathroom, with natural light streaming through a window. A cloth or towel lies on the wooden floor. A person wearing a dark jacket and plaid pants emerges from the bathroom, carrying a pink bag with a floral pattern. They stop in the hallway, adjust the bag, and appear to search for something inside it. The scene continues with the person still in the hallway, holding the bag and looking around. They then start walking towards the bathroom, briefly stopping to adjust their hair. The person stands near the bathroom door, still holding the pink bag. The narrative progresses as the person begins to walk away from the bathroom, down the hallway, while still holding the bag. They briefly stop, turn around to look back towards the bathroom, and then continue walking down the hallway. The video concludes with the person exiting the frame, leaving the hallway empty except for the cloth or towel still lying on the wooden floor.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/WLE0F.mp4&quot;}}},{&quot;rowIdx&quot;:56,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;X4DG4&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide a comprehensive analysis of the video's content and themes.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person sitting on a well-lit kitchen counter, wearing a green shirt and gray pants, with a white towel draped over their shoulder. The kitchen features white tiled walls, a countertop with various items, and a window with a blue basket on the sill. The person then gets up, removes the towel from their shoulder, and starts working at the window area, possibly cleaning or organizing something. The scene continues with the person focused on their task at the window. The narrative develops as the person moves away from the window area, maintaining the same kitchen environment. They appear to be walking towards another part of the kitchen, and the video wraps up with the person still in motion, with the kitchen setting remaining unchanged throughout.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide a comprehensive analysis of the video's content and themes.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person sitting on a well-lit kitchen counter, wearing a green shirt and gray pants, with a white towel draped over their shoulder. The kitchen features white tiled walls, a countertop with various items, and a window with a blue basket on the sill. The person then gets up, removes the towel from their shoulder, and starts working at the window area, possibly cleaning or organizing something. The scene continues with the person focused on their task at the window. The narrative develops as the person moves away from the window area, maintaining the same kitchen environment. They appear to be walking towards another part of the kitchen, and the video wraps up with the person still in motion, with the kitchen setting remaining unchanged throughout.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/X4DG4.mp4&quot;}}},{&quot;rowIdx&quot;:57,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;XOIEY&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nShare a detailed breakdown of the video's storyline and visuals.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a bedroom with light purple walls and a marble floor, featuring a wooden wardrobe with a mirror and a small table with a framed photo on it. A person in a white sleeveless shirt and dark pants is seen bending over, meticulously packing items into a blue bag placed on the floor. Nearby, another person wearing a red t-shirt and dark pants stands, observing the packing process. The focus remains on the careful organization and placement of items into the bag. As the video progresses, the person in the white sleeveless shirt is seen handling a beige cloth, folding it carefully before placing it into the bag. The observer in the red t-shirt continues to watch attentively. The video concludes with the person in the white sleeveless shirt shaking out the beige cloth and placing it on the floor next to the blue bag, while the observer maintains their position, watching the entire process. The consistent theme throughout the video is the meticulous and organized packing activity, with a clear emphasis on the handling and placement of the beige cloth.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nShare a detailed breakdown of the video's storyline and visuals.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a bedroom with light purple walls and a marble floor, featuring a wooden wardrobe with a mirror and a small table with a framed photo on it. A person in a white sleeveless shirt and dark pants is seen bending over, meticulously packing items into a blue bag placed on the floor. Nearby, another person wearing a red t-shirt and dark pants stands, observing the packing process. The focus remains on the careful organization and placement of items into the bag. As the video progresses, the person in the white sleeveless shirt is seen handling a beige cloth, folding it carefully before placing it into the bag. The observer in the red t-shirt continues to watch attentively. The video concludes with the person in the white sleeveless shirt shaking out the beige cloth and placing it on the floor next to the blue bag, while the observer maintains their position, watching the entire process. The consistent theme throughout the video is the meticulous and organized packing activity, with a clear emphasis on the handling and placement of the beige cloth.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/XOIEY.mp4&quot;}}},{&quot;rowIdx&quot;:58,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;ZBBP9&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nIllustrate the video's narrative journey, scene by scene, with attention to detail.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a home setting, specifically focusing on a room with a doorway leading to a bathroom. A man wearing a gray long-sleeve shirt and dark pants is seen standing in the bathroom, holding a white cup. He appears to be pouring liquid from the cup into the sink, likely using mouthwash and rinsing his mouth. The bathroom features a wooden cabinet with a towel hanging on a rack above it. In the foreground, there is a white shelf with various bottles and a wine rack. A person in a blue shirt with a white pattern on the front is seen moving around in the room outside the bathroom. The man in the bathroom continues his actions of pouring liquid and rinsing his mouth, while the person in the blue shirt occasionally moves in and out of the frame.\n\nThe scene remains consistent as the focus stays on the bathroom doorway. The man in the gray long-sleeve shirt and dark pants continues to rinse his mouth. In the foreground, a child with blonde hair wearing a white shirt is present, looking towards the bathroom. The person in the blue shirt with a white pattern is seen moving around the room, now holding a purple toy vacuum cleaner. The white shelf with various bottles and a wine rack remains visible in the background. The man in the bathroom continues his actions, and the two individuals move around in the room outside.\n\nThe video maintains the same home setting, with the focus still on the bathroom doorway. The man in the gray long-sleeve shirt and dark pants continues to rinse his mouth, holding the white cup and pouring liquid into the sink. The child with blonde hair in a white shirt remains in the foreground, looking towards the bathroom. The person in the blue shirt with a white pattern continues to move around the room, using the purple toy vacuum cleaner. The white shelf with various bottles and a wine rack is still visible in the background. The man continues his routine of pouring liquid and rinsing his mouth, while the person in the blue shirt moves the toy vacuum cleaner around the room, creating a consistent and coherent scene throughout the video.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nIllustrate the video's narrative journey, scene by scene, with attention to detail.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a home setting, specifically focusing on a room with a doorway leading to a bathroom. A man wearing a gray long-sleeve shirt and dark pants is seen standing in the bathroom, holding a white cup. He appears to be pouring liquid from the cup into the sink, likely using mouthwash and rinsing his mouth. The bathroom features a wooden cabinet with a towel hanging on a rack above it. In the foreground, there is a white shelf with various bottles and a wine rack. A person in a blue shirt with a white pattern on the front is seen moving around in the room outside the bathroom. The man in the bathroom continues his actions of pouring liquid and rinsing his mouth, while the person in the blue shirt occasionally moves in and out of the frame.\\n\\nThe scene remains consistent as the focus stays on the bathroom doorway. The man in the gray long-sleeve shirt and dark pants continues to rinse his mouth. In the foreground, a child with blonde hair wearing a white shirt is present, looking towards the bathroom. The person in the blue shirt with a white pattern is seen moving around the room, now holding a purple toy vacuum cleaner. The white shelf with various bottles and a wine rack remains visible in the background. The man in the bathroom continues his actions, and the two individuals move around in the room outside.\\n\\nThe video maintains the same home setting, with the focus still on the bathroom doorway. The man in the gray long-sleeve shirt and dark pants continues to rinse his mouth, holding the white cup and pouring liquid into the sink. The child with blonde hair in a white shirt remains in the foreground, looking towards the bathroom. The person in the blue shirt with a white pattern continues to move around the room, using the purple toy vacuum cleaner. The white shelf with various bottles and a wine rack is still visible in the background. The man continues his routine of pouring liquid and rinsing his mouth, while the person in the blue shirt moves the toy vacuum cleaner around the room, creating a consistent and coherent scene throughout the video.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/Charades/ZBBP9.mp4&quot;}}},{&quot;rowIdx&quot;:59,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0016-3503206487&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide a comprehensive analysis of the video's content and themes.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a close-up view of a bicycle wheel and a person's hand on the handlebar, indicating that the person is riding a bicycle. The scene transitions to a wider view, revealing a person in a blue plaid shirt and shorts sitting on the bicycle seat. They are at a street intersection with traffic lights and a railway crossing, waiting for the light to change. The background features palm trees, buildings, and a clear sky with some clouds, capturing the calm and sunny atmosphere of the street scene. The traffic lights show a red signal, and the person looks around occasionally while waiting. The video then focuses on a close-up of the street intersection, where the traffic lights remain red, indicating that vehicles and bicycles must wait. A street sign reading 'Hollywood Blvd' is visible, providing context to the location. The person in the blue plaid shirt continues to wait at the intersection, maintaining the consistent, tranquil atmosphere of the scene.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide a comprehensive analysis of the video's content and themes.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a close-up view of a bicycle wheel and a person's hand on the handlebar, indicating that the person is riding a bicycle. The scene transitions to a wider view, revealing a person in a blue plaid shirt and shorts sitting on the bicycle seat. They are at a street intersection with traffic lights and a railway crossing, waiting for the light to change. The background features palm trees, buildings, and a clear sky with some clouds, capturing the calm and sunny atmosphere of the street scene. The traffic lights show a red signal, and the person looks around occasionally while waiting. The video then focuses on a close-up of the street intersection, where the traffic lights remain red, indicating that vehicles and bicycles must wait. A street sign reading 'Hollywood Blvd' is visible, providing context to the location. The person in the blue plaid shirt continues to wait at the intersection, maintaining the consistent, tranquil atmosphere of the scene.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/0016/3503206487.mp4&quot;}}},{&quot;rowIdx&quot;:60,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0030-4363130739&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nElaborate on all aspects of the video you are viewing.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video features a heartwarming scene of a young child riding a small white pony, accompanied by two attentive adults. The child, dressed in a white helmet, a pink floral long-sleeve shirt, pink pants, and pink sandals, is seated on the pony, which is being led by an adult in a green shirt and cap holding the reins. Another adult, wearing a brown dress, a light gray cardigan, and a wide-brimmed white hat, walks alongside the child, holding their hand for support. The setting is a picturesque grassy area with trees and a fence in the background, creating a serene and safe environment. As the video progresses, the child continues to ride the pony with the same two adults ensuring their safety. The child appears to be enjoying the ride, smiling and looking content. The scene remains consistent with the grassy area, trees, and fence in the background. Towards the end of the video, the background reveals more people and ponies, suggesting a lively outdoor event or gathering. The adults' careful attention and the child's evident joy create a touching and memorable moment.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nElaborate on all aspects of the video you are viewing.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video features a heartwarming scene of a young child riding a small white pony, accompanied by two attentive adults. The child, dressed in a white helmet, a pink floral long-sleeve shirt, pink pants, and pink sandals, is seated on the pony, which is being led by an adult in a green shirt and cap holding the reins. Another adult, wearing a brown dress, a light gray cardigan, and a wide-brimmed white hat, walks alongside the child, holding their hand for support. The setting is a picturesque grassy area with trees and a fence in the background, creating a serene and safe environment. As the video progresses, the child continues to ride the pony with the same two adults ensuring their safety. The child appears to be enjoying the ride, smiling and looking content. The scene remains consistent with the grassy area, trees, and fence in the background. Towards the end of the video, the background reveals more people and ponies, suggesting a lively outdoor event or gathering. The adults' careful attention and the child's evident joy create a touching and memorable moment.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/0030/4363130739.mp4&quot;}}},{&quot;rowIdx&quot;:61,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0068-2598449017&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nCharacterize each scene of the video using a detailed description.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video showcases a lively and festive scene under a large white tent, where a person is riding a mechanical bull. The rider is dressed in a white tank top, dark shorts, a colorful tie, wristbands, a party hat, and sunglasses, embodying a playful and celebratory spirit. Throughout the ride, the rider raises one arm in the air, a common technique for maintaining balance on the mechanical bull. Nearby, a person in a cowboy hat and beige shirt stands, likely supervising or assisting the rider. The background features the tent structure and some balloons, further emphasizing the festive atmosphere. The mechanical bull is dark brown with a black saddle, and a sign on it reads 'WARNING: You only ride at your own risk,' highlighting the adventurous nature of the activity. The video captures the rider's various positions and movements as they strive to stay balanced on the bull, with the supervising person occasionally glancing at the rider and the bull, ensuring safety and adding to the overall dynamic of the scene.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nCharacterize each scene of the video using a detailed description.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video showcases a lively and festive scene under a large white tent, where a person is riding a mechanical bull. The rider is dressed in a white tank top, dark shorts, a colorful tie, wristbands, a party hat, and sunglasses, embodying a playful and celebratory spirit. Throughout the ride, the rider raises one arm in the air, a common technique for maintaining balance on the mechanical bull. Nearby, a person in a cowboy hat and beige shirt stands, likely supervising or assisting the rider. The background features the tent structure and some balloons, further emphasizing the festive atmosphere. The mechanical bull is dark brown with a black saddle, and a sign on it reads 'WARNING: You only ride at your own risk,' highlighting the adventurous nature of the activity. The video captures the rider's various positions and movements as they strive to stay balanced on the bull, with the supervising person occasionally glancing at the rider and the bull, ensuring safety and adding to the overall dynamic of the scene.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/0068/2598449017.mp4&quot;}}},{&quot;rowIdx&quot;:62,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1002-2932897373&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nElaborate on the visual and narrative elements of the video in detail.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a view of a charming European town square, surrounded by colorful buildings in yellow and pastel shades. These buildings feature traditional architectural elements such as wooden shutters, balconies adorned with flower boxes, and tiled roofs. A central fountain is visible, along with benches and potted plants that enhance the square's inviting atmosphere. The camera pans to the right, revealing more of the town's picturesque buildings, including facades in yellow, red, and blue, some of which are covered in ivy. The scene transitions to a closer view of a narrow alleyway, where a person is seen walking towards a building with a sign that reads 'Gasthof Zauner' and another sign for 'Hallstätter Markt.' The serene and inviting essence of the small, scenic European town is captured beautifully.\n\nThe video continues with a view of the same narrow alleyway, where two people, one in a dark jacket and light pants and the other in a light outfit, are seen walking towards the ivy-covered building with the 'Gasthof Zauner' sign. The 'Hallstätter Markt' sign is also prominently displayed on a nearby building. The camera pans upwards, revealing more of the ivy-covered building and its wooden balconies adorned with flower boxes. The camera then shifts to the right, showcasing a series of picturesque buildings with colorful facades in shades of white, red, and yellow, some of which are also covered in ivy. The video concludes with a view of a church steeple in the background, adding to the quaint and serene atmosphere of the town.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nElaborate on the visual and narrative elements of the video in detail.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a view of a charming European town square, surrounded by colorful buildings in yellow and pastel shades. These buildings feature traditional architectural elements such as wooden shutters, balconies adorned with flower boxes, and tiled roofs. A central fountain is visible, along with benches and potted plants that enhance the square's inviting atmosphere. The camera pans to the right, revealing more of the town's picturesque buildings, including facades in yellow, red, and blue, some of which are covered in ivy. The scene transitions to a closer view of a narrow alleyway, where a person is seen walking towards a building with a sign that reads 'Gasthof Zauner' and another sign for 'Hallstätter Markt.' The serene and inviting essence of the small, scenic European town is captured beautifully.\\n\\nThe video continues with a view of the same narrow alleyway, where two people, one in a dark jacket and light pants and the other in a light outfit, are seen walking towards the ivy-covered building with the 'Gasthof Zauner' sign. The 'Hallstätter Markt' sign is also prominently displayed on a nearby building. The camera pans upwards, revealing more of the ivy-covered building and its wooden balconies adorned with flower boxes. The camera then shifts to the right, showcasing a series of picturesque buildings with colorful facades in shades of white, red, and yellow, some of which are also covered in ivy. The video concludes with a view of a church steeple in the background, adding to the quaint and serene atmosphere of the town.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1002/2932897373.mp4&quot;}}},{&quot;rowIdx&quot;:63,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1002-7592105758&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDetail the video's plot development, character arcs, and thematic elements.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a close-up of a black vinyl record with an orange label spinning on a turntable. A makeshift tonearm constructed from LEGO pieces is positioned over the record, and to the right of the turntable, a white plastic cup is secured with a rubber band. The background reveals a white tablecloth-covered table cluttered with various LEGO boxes and a white container filled with LEGO pieces. The camera angle shifts slightly, showing a person in a gray shirt holding a smartphone. The view then widens to reveal a cluttered table with a white laptop displaying a blue screen, a few scattered LEGO pieces, and another white plastic cup lying on its side. The scene transitions to a top-down view of the spinning record and the surrounding items on the table.\n\nThe focus then returns to a close-up view of the spinning vinyl record with the makeshift LEGO tonearm. The white plastic cup remains secured with a rubber band to the right of the turntable, and another white plastic cup lies on its side nearby. The background features a cluttered table with a white laptop displaying a blue screen with a graphical interface, possibly a software program. To the left of the laptop, there is a closed laptop adorned with various stickers. A white container filled with LEGO pieces is visible in the bottom left corner. The camera angle shifts slightly, providing a clearer view of the laptop screen, which shows a graphical interface with a character on a stage under spotlights. The video concludes with a close-up of the laptop screen, focusing on the character under the spotlights.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDetail the video's plot development, character arcs, and thematic elements.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a close-up of a black vinyl record with an orange label spinning on a turntable. A makeshift tonearm constructed from LEGO pieces is positioned over the record, and to the right of the turntable, a white plastic cup is secured with a rubber band. The background reveals a white tablecloth-covered table cluttered with various LEGO boxes and a white container filled with LEGO pieces. The camera angle shifts slightly, showing a person in a gray shirt holding a smartphone. The view then widens to reveal a cluttered table with a white laptop displaying a blue screen, a few scattered LEGO pieces, and another white plastic cup lying on its side. The scene transitions to a top-down view of the spinning record and the surrounding items on the table.\\n\\nThe focus then returns to a close-up view of the spinning vinyl record with the makeshift LEGO tonearm. The white plastic cup remains secured with a rubber band to the right of the turntable, and another white plastic cup lies on its side nearby. The background features a cluttered table with a white laptop displaying a blue screen with a graphical interface, possibly a software program. To the left of the laptop, there is a closed laptop adorned with various stickers. A white container filled with LEGO pieces is visible in the bottom left corner. The camera angle shifts slightly, providing a clearer view of the laptop screen, which shows a graphical interface with a character on a stage under spotlights. The video concludes with a close-up of the laptop screen, focusing on the character under the spotlights.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1002/7592105758.mp4&quot;}}},{&quot;rowIdx&quot;:64,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1005-3522105391&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nOffer a detailed interpretation of the video's message and imagery.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a baby dressed in a blue shirt, lying on a colorful patchwork quilt and reaching out towards a blue and purple ball. The baby, positioned on their stomach, makes several attempts to grab the ball, which moves slightly with each attempt. An adult, wearing shorts, is sitting nearby, attentively watching the baby. The background features a red carpet and some furniture, and a cat is visible in the background, observing the scene. The narrative develops as the baby continues to lie on the quilt, still focused on the blue and purple ball that has now rolled away. The cat, previously in the background, becomes more prominent, standing near a piece of furniture. A dog enters the scene, approaching the ball and sniffing it, capturing the baby's curiosity. The baby watches the dog and the ball intently, while the cat moves closer to the baby. In the final frames of this segment, the baby interacts with the adult, who is now wearing shorts and a striped shirt. The background remains consistent with the red carpet and some furniture. The video wraps up with the baby still on the colorful patchwork quilt, looking towards the adult sitting nearby, dressed in shorts and a striped shirt. The adult is seated cross-legged on the red carpet, with one hand resting on their knee and the other near their face. The background includes a black armchair, a white side table, and a brown chair with a cushion. A large dog enters the scene, moving closer to the baby and partially obscuring the view. The baby looks at the dog with curiosity as the dog stands near the baby, while the adult continues to observe the interaction from nearby.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nOffer a detailed interpretation of the video's message and imagery.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a baby dressed in a blue shirt, lying on a colorful patchwork quilt and reaching out towards a blue and purple ball. The baby, positioned on their stomach, makes several attempts to grab the ball, which moves slightly with each attempt. An adult, wearing shorts, is sitting nearby, attentively watching the baby. The background features a red carpet and some furniture, and a cat is visible in the background, observing the scene. The narrative develops as the baby continues to lie on the quilt, still focused on the blue and purple ball that has now rolled away. The cat, previously in the background, becomes more prominent, standing near a piece of furniture. A dog enters the scene, approaching the ball and sniffing it, capturing the baby's curiosity. The baby watches the dog and the ball intently, while the cat moves closer to the baby. In the final frames of this segment, the baby interacts with the adult, who is now wearing shorts and a striped shirt. The background remains consistent with the red carpet and some furniture. The video wraps up with the baby still on the colorful patchwork quilt, looking towards the adult sitting nearby, dressed in shorts and a striped shirt. The adult is seated cross-legged on the red carpet, with one hand resting on their knee and the other near their face. The background includes a black armchair, a white side table, and a brown chair with a cushion. A large dog enters the scene, moving closer to the baby and partially obscuring the view. The baby looks at the dog with curiosity as the dog stands near the baby, while the adult continues to observe the interaction from nearby.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1005/3522105391.mp4&quot;}}},{&quot;rowIdx&quot;:65,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1009-2962441847&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite an in-depth depiction of the video, covering all its aspects.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person in a red sweater and jeans standing in a room with a couch and a shelf in the background. They are holding a small object, possibly a remote or a phone, and appear to be engaged in a conversation or demonstration. The person raises their arm, possibly gesturing or emphasizing a point. The focus then shifts to a table with a bottle and other items, indicating a casual setting. The person bends down, possibly to pick something up from the floor, while another individual is visible seated on the couch in the background. The person in the red sweater then sits down on the couch, continuing to interact with the object in their hands. They stand up again, holding and examining the object closely, making animated gestures, suggesting they are explaining or demonstrating something. The scene transitions to the person in the red sweater standing in front of the couch, holding the object and making animated gestures, engaged in a lively conversation or demonstration. The person continues to gesture energetically. Another individual in a black shirt approaches the person in the red sweater, who is now seated on the couch. The individual in the black shirt appears to be assisting or interacting with the person in the red sweater. The focus shifts to a table with a laptop, a bottle, and other items, maintaining the casual setting. The person in the red sweater is seen sitting on the couch, holding the object, while the individual in the black shirt stands nearby. The video wraps up with the person in the red sweater standing up again, smiling, and holding the object, suggesting a return to the previous lively interaction. The setting remains consistent throughout, with a casual, indoor environment and a relaxed atmosphere.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite an in-depth depiction of the video, covering all its aspects.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person in a red sweater and jeans standing in a room with a couch and a shelf in the background. They are holding a small object, possibly a remote or a phone, and appear to be engaged in a conversation or demonstration. The person raises their arm, possibly gesturing or emphasizing a point. The focus then shifts to a table with a bottle and other items, indicating a casual setting. The person bends down, possibly to pick something up from the floor, while another individual is visible seated on the couch in the background. The person in the red sweater then sits down on the couch, continuing to interact with the object in their hands. They stand up again, holding and examining the object closely, making animated gestures, suggesting they are explaining or demonstrating something. The scene transitions to the person in the red sweater standing in front of the couch, holding the object and making animated gestures, engaged in a lively conversation or demonstration. The person continues to gesture energetically. Another individual in a black shirt approaches the person in the red sweater, who is now seated on the couch. The individual in the black shirt appears to be assisting or interacting with the person in the red sweater. The focus shifts to a table with a laptop, a bottle, and other items, maintaining the casual setting. The person in the red sweater is seen sitting on the couch, holding the object, while the individual in the black shirt stands nearby. The video wraps up with the person in the red sweater standing up again, smiling, and holding the object, suggesting a return to the previous lively interaction. The setting remains consistent throughout, with a casual, indoor environment and a relaxed atmosphere.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1009/2962441847.mp4&quot;}}},{&quot;rowIdx&quot;:66,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1009-7121723227&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDescribe every scene and its significance in the video.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a cozy indoor scene where a person is sitting on a dark green armchair, holding a baby. The person is dressed in a colorful, patterned shirt, while the baby is in a blue and white striped outfit with a white bib. Natural light streams through large windows with partially drawn blinds, illuminating the room. Several plants and a small table with electronic equipment are visible to the left of the armchair. The person interacts affectionately with the baby, holding them close and looking at them with warmth. The baby curiously looks around the room and at the person holding them, creating a nurturing and tender atmosphere. The scene continues with the person still seated in the armchair, maintaining their affectionate interaction with the baby. The natural light, plants, and electronic equipment remain consistent in the background, reinforcing the warm and nurturing ambiance throughout the video.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDescribe every scene and its significance in the video.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a cozy indoor scene where a person is sitting on a dark green armchair, holding a baby. The person is dressed in a colorful, patterned shirt, while the baby is in a blue and white striped outfit with a white bib. Natural light streams through large windows with partially drawn blinds, illuminating the room. Several plants and a small table with electronic equipment are visible to the left of the armchair. The person interacts affectionately with the baby, holding them close and looking at them with warmth. The baby curiously looks around the room and at the person holding them, creating a nurturing and tender atmosphere. The scene continues with the person still seated in the armchair, maintaining their affectionate interaction with the baby. The natural light, plants, and electronic equipment remain consistent in the background, reinforcing the warm and nurturing ambiance throughout the video.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1009/7121723227.mp4&quot;}}},{&quot;rowIdx&quot;:67,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1010-4968805923&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDescribe every scene and its significance in the video.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a serene park-like setting with a grassy field and tall trees under a clear blue sky, indicating a sunny day. Three individuals are present throughout the video. Initially, the person on the left, dressed in a white shirt and jeans, is holding a baseball bat and preparing to swing. The person in the middle, wearing a black shirt and jeans, stands with their hands on their hips, observing the batter. The person on the right, in a dark shirt and light-colored pants, holds a baseball glove, ready to catch the ball. This scene suggests a casual baseball practice or game among friends. As the video progresses, the person on the left is seen bending down, possibly picking something up from the ground. Meanwhile, the person in the middle continues to observe with hands on hips, and the person on the right, now also with hands on hips, appears to be in conversation with the person in the middle. The video maintains a casual and friendly atmosphere, capturing a moment of leisure and camaraderie in the park.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDescribe every scene and its significance in the video.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a serene park-like setting with a grassy field and tall trees under a clear blue sky, indicating a sunny day. Three individuals are present throughout the video. Initially, the person on the left, dressed in a white shirt and jeans, is holding a baseball bat and preparing to swing. The person in the middle, wearing a black shirt and jeans, stands with their hands on their hips, observing the batter. The person on the right, in a dark shirt and light-colored pants, holds a baseball glove, ready to catch the ball. This scene suggests a casual baseball practice or game among friends. As the video progresses, the person on the left is seen bending down, possibly picking something up from the ground. Meanwhile, the person in the middle continues to observe with hands on hips, and the person on the right, now also with hands on hips, appears to be in conversation with the person in the middle. The video maintains a casual and friendly atmosphere, capturing a moment of leisure and camaraderie in the park.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1010/4968805923.mp4&quot;}}},{&quot;rowIdx&quot;:68,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1011-2909898562&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nIllustrate the video's narrative journey, scene by scene, with attention to detail.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person in a gray shirt and blue jeans holding a child in a red outfit while sitting at the top of a green slide attached to a wooden play structure. The background includes a grassy yard with trees and a white building with blue trim. The person and child slide down together, with the person holding the child securely. The child appears to be enjoying the ride, and they reach the bottom of the slide where the person stands up while still holding the child. The scene transitions to a person in a light blue shirt holding the same child in a red outfit with white patterns, standing outside in front of a house with white siding and blue trim. Another person in a white tank top and dark shorts is visible in the background. The person in the blue shirt interacts with the happy and engaged child, eventually lowering the child to the ground. The child, now standing, points and looks around the grassy yard with trees in the background, exploring the surroundings while the adults watch. The video wraps up with the child in the red outfit standing on the grassy yard, looking around and pointing curiously. The legs of the adult in dark shorts and sneakers are visible in the background, suggesting the presence of the person in the white tank top from the previous scene. The green slide from the wooden play structure is partially visible on the left side of the frame. The child takes a few steps, exploring the area, creating a playful and relaxed atmosphere.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nIllustrate the video's narrative journey, scene by scene, with attention to detail.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person in a gray shirt and blue jeans holding a child in a red outfit while sitting at the top of a green slide attached to a wooden play structure. The background includes a grassy yard with trees and a white building with blue trim. The person and child slide down together, with the person holding the child securely. The child appears to be enjoying the ride, and they reach the bottom of the slide where the person stands up while still holding the child. The scene transitions to a person in a light blue shirt holding the same child in a red outfit with white patterns, standing outside in front of a house with white siding and blue trim. Another person in a white tank top and dark shorts is visible in the background. The person in the blue shirt interacts with the happy and engaged child, eventually lowering the child to the ground. The child, now standing, points and looks around the grassy yard with trees in the background, exploring the surroundings while the adults watch. The video wraps up with the child in the red outfit standing on the grassy yard, looking around and pointing curiously. The legs of the adult in dark shorts and sneakers are visible in the background, suggesting the presence of the person in the white tank top from the previous scene. The green slide from the wooden play structure is partially visible on the left side of the frame. The child takes a few steps, exploring the area, creating a playful and relaxed atmosphere.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1011/2909898562.mp4&quot;}}},{&quot;rowIdx&quot;:69,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1011-9170516863&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nConvey the narrative and visual elements of the video in detail.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video features a live performance by 'THE WILD ONION STRING BAND' under a blue canopy. The band consists of four musicians: a banjo player in a red jacket and blue cap on the left, an acoustic guitar player in a black jacket and blue jeans in the center, a double bass player in the background, and a violin player in a white shirt and white cap on the right. The backdrop includes a green banner with text and images of flowers, reading 'THE WILD ONION STRING BAND.' The musicians are focused on their instruments, creating a lively and engaging atmosphere. Throughout the video, the musicians are shown in various stages of their performance, with slight variations in their movements and positions. The acoustic guitar player strums the guitar, the banjo player plucks the banjo, the double bass player plays the double bass, and the violin player plays the violin. The overall scene remains consistent, showcasing the band's dynamic performance. In one of the frames, the banjo player is seen holding a drink, adding a casual and relaxed element to the performance.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nConvey the narrative and visual elements of the video in detail.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video features a live performance by 'THE WILD ONION STRING BAND' under a blue canopy. The band consists of four musicians: a banjo player in a red jacket and blue cap on the left, an acoustic guitar player in a black jacket and blue jeans in the center, a double bass player in the background, and a violin player in a white shirt and white cap on the right. The backdrop includes a green banner with text and images of flowers, reading 'THE WILD ONION STRING BAND.' The musicians are focused on their instruments, creating a lively and engaging atmosphere. Throughout the video, the musicians are shown in various stages of their performance, with slight variations in their movements and positions. The acoustic guitar player strums the guitar, the banjo player plucks the banjo, the double bass player plays the double bass, and the violin player plays the violin. The overall scene remains consistent, showcasing the band's dynamic performance. In one of the frames, the banjo player is seen holding a drink, adding a casual and relaxed element to the performance.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1011/9170516863.mp4&quot;}}},{&quot;rowIdx&quot;:70,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1015-5398268379&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite a comprehensive depiction of the entire video clip.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video captures a lively birthday party scene in a well-lit room adorned with colorful balloons and a pink tablecloth on the dining table. It begins with a woman in a light pink sweater and blue jeans taking photos of the children, who are dressed in various colorful outfits, some resembling princess dresses, and are seated around the table. The festive atmosphere is enhanced as a woman in a red striped shirt and blue jeans enters carrying a birthday cake adorned with candles and decorations. The children, visibly excited, engage in conversation and activities around the table filled with party decorations and snacks. The woman in the red striped shirt brings the beautifully decorated cake, featuring a crown-like decoration and a '6' candle on top, closer to the children, who eagerly look at it. The scene continues with the children enjoying their snacks, including a bowl of cheese puffs and other treats, and interacting with each other. A green gift bag on the table adds to the celebratory ambiance, maintaining the festive atmosphere throughout the video.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite a comprehensive depiction of the entire video clip.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video captures a lively birthday party scene in a well-lit room adorned with colorful balloons and a pink tablecloth on the dining table. It begins with a woman in a light pink sweater and blue jeans taking photos of the children, who are dressed in various colorful outfits, some resembling princess dresses, and are seated around the table. The festive atmosphere is enhanced as a woman in a red striped shirt and blue jeans enters carrying a birthday cake adorned with candles and decorations. The children, visibly excited, engage in conversation and activities around the table filled with party decorations and snacks. The woman in the red striped shirt brings the beautifully decorated cake, featuring a crown-like decoration and a '6' candle on top, closer to the children, who eagerly look at it. The scene continues with the children enjoying their snacks, including a bowl of cheese puffs and other treats, and interacting with each other. A green gift bag on the table adds to the celebratory ambiance, maintaining the festive atmosphere throughout the video.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1015/5398268379.mp4&quot;}}},{&quot;rowIdx&quot;:71,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1018-5974881839&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDetail the video's plot development, character arcs, and thematic elements.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a close-up of a person performing a medical procedure on a dog's leg. The person, dressed in blue pants and sneakers, is meticulously inserting a needle into the dog's leg, which shows a patch of pink, irritated skin. The dog's leg is held gently but firmly to ensure stability during the procedure. The background features a gray floor, and the text 'Nottingham Vet School' is visible in the lower right corner of each frame, indicating that the setting is a veterinary school. The sequence highlights the care and precision involved in the needle insertion. The video then develops through the continuation of the medical procedure, where the same person is seen carefully removing the needle from the dog's leg. The dog's leg, still showing the patch of pink, irritated skin, is held gently but firmly. The background remains a gray floor, and the text 'Nottingham Vet School' continues to be visible, reinforcing the veterinary school setting. The sequence captures the careful removal of the needle, emphasizing the precision and care involved. The video wraps up with the dog's leg being released and the person’s hands moving away, indicating the end of the procedure.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDetail the video's plot development, character arcs, and thematic elements.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a close-up of a person performing a medical procedure on a dog's leg. The person, dressed in blue pants and sneakers, is meticulously inserting a needle into the dog's leg, which shows a patch of pink, irritated skin. The dog's leg is held gently but firmly to ensure stability during the procedure. The background features a gray floor, and the text 'Nottingham Vet School' is visible in the lower right corner of each frame, indicating that the setting is a veterinary school. The sequence highlights the care and precision involved in the needle insertion. The video then develops through the continuation of the medical procedure, where the same person is seen carefully removing the needle from the dog's leg. The dog's leg, still showing the patch of pink, irritated skin, is held gently but firmly. The background remains a gray floor, and the text 'Nottingham Vet School' continues to be visible, reinforcing the veterinary school setting. The sequence captures the careful removal of the needle, emphasizing the precision and care involved. The video wraps up with the dog's leg being released and the person’s hands moving away, indicating the end of the procedure.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1018/5974881839.mp4&quot;}}},{&quot;rowIdx&quot;:72,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1020-4405101727&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite an in-depth depiction of the video, covering all its aspects.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with an outdoor setting where several people are sitting on benches. The background features a white van and a fence adorned with lights, suggesting a park or a public area. The scene is bright and sunny, indicating daytime. A person in a blue jacket with a purple bag is seated on a bench, looking down at something in their hands. Next to them, another person in a black suit with a white shirt is also looking down. The video develops through a continuation of the same setting, with the focus shifting slightly to the person in the black suit, who is now closer to the camera, with their face partially visible. The scene remains bright and sunny, reinforcing the daytime setting. The video wraps up with a close-up view of the person in the black suit, capturing their shoulder and part of their face, while the background becomes slightly blurred, emphasizing the proximity of the camera to the person. The consistent elements of the white van, the fence with lights, and the sunny atmosphere throughout the video reinforce the idea of a tranquil moment in a public park.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite an in-depth depiction of the video, covering all its aspects.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with an outdoor setting where several people are sitting on benches. The background features a white van and a fence adorned with lights, suggesting a park or a public area. The scene is bright and sunny, indicating daytime. A person in a blue jacket with a purple bag is seated on a bench, looking down at something in their hands. Next to them, another person in a black suit with a white shirt is also looking down. The video develops through a continuation of the same setting, with the focus shifting slightly to the person in the black suit, who is now closer to the camera, with their face partially visible. The scene remains bright and sunny, reinforcing the daytime setting. The video wraps up with a close-up view of the person in the black suit, capturing their shoulder and part of their face, while the background becomes slightly blurred, emphasizing the proximity of the camera to the person. The consistent elements of the white van, the fence with lights, and the sunny atmosphere throughout the video reinforce the idea of a tranquil moment in a public park.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1020/4405101727.mp4&quot;}}},{&quot;rowIdx&quot;:73,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1022-8171216955&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWalk through the video, detailing its key moments and features.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a young girl adorned with a green bow in her hair, dressed in a plaid dress and white shirt, playing at a table filled with puzzle pieces and a toy dog. She interacts with the toy dog, which seems to be part of a game, and her excitement is evident as she moves around and gestures animatedly. The setting includes a wooden cabinet and a hallway in the background. Another child, also in a white shirt, joins her at the table, adding to the playful atmosphere. The video develops as the young girl continues to engage with the toy dog, her animated movements and gestures highlighting her enthusiasm. The other child remains at the table, and the two children share a lively and engaging moment, interacting with the toy dog and each other. The background remains consistent, featuring the wooden cabinet and hallway, providing a warm and familiar setting for their playful activity. The video wraps up with the children deeply engrossed in their game, capturing a joyful and dynamic interaction between them.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWalk through the video, detailing its key moments and features.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a young girl adorned with a green bow in her hair, dressed in a plaid dress and white shirt, playing at a table filled with puzzle pieces and a toy dog. She interacts with the toy dog, which seems to be part of a game, and her excitement is evident as she moves around and gestures animatedly. The setting includes a wooden cabinet and a hallway in the background. Another child, also in a white shirt, joins her at the table, adding to the playful atmosphere. The video develops as the young girl continues to engage with the toy dog, her animated movements and gestures highlighting her enthusiasm. The other child remains at the table, and the two children share a lively and engaging moment, interacting with the toy dog and each other. The background remains consistent, featuring the wooden cabinet and hallway, providing a warm and familiar setting for their playful activity. The video wraps up with the children deeply engrossed in their game, capturing a joyful and dynamic interaction between them.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1022/8171216955.mp4&quot;}}},{&quot;rowIdx&quot;:74,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1023-6826504502&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDissect the video's content, explaining each element thoroughly.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a view inside a vehicle, showing the interior and passengers as it drives on a dirt road in a rural area. The camera provides a wider view of the vehicle's interior, revealing three passengers. The scene transitions to an exterior view of the white pickup truck driving on a red dirt road, flanked by fields on either side. The overcast sky and the road stretching into the distance suggest a long journey through the rural landscape. The video develops through various perspectives of the vehicle and the surrounding scenery, maintaining the rural setting and overcast sky. The camera captures the right side of the vehicle, including the side mirror and part of the window. A passenger's arm extends out of the window, first making a waving gesture and then a thumbs-up gesture, before retracting. The journey continues with the road stretching ahead and another vehicle visible in the distance. The video wraps up with a close-up of a passenger in the white pickup truck, their arm extended out of the window, wearing a white shirt. The consistent rural landscape and overcast sky persist throughout the video, emphasizing the continuity of the journey.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDissect the video's content, explaining each element thoroughly.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a view inside a vehicle, showing the interior and passengers as it drives on a dirt road in a rural area. The camera provides a wider view of the vehicle's interior, revealing three passengers. The scene transitions to an exterior view of the white pickup truck driving on a red dirt road, flanked by fields on either side. The overcast sky and the road stretching into the distance suggest a long journey through the rural landscape. The video develops through various perspectives of the vehicle and the surrounding scenery, maintaining the rural setting and overcast sky. The camera captures the right side of the vehicle, including the side mirror and part of the window. A passenger's arm extends out of the window, first making a waving gesture and then a thumbs-up gesture, before retracting. The journey continues with the road stretching ahead and another vehicle visible in the distance. The video wraps up with a close-up of a passenger in the white pickup truck, their arm extended out of the window, wearing a white shirt. The consistent rural landscape and overcast sky persist throughout the video, emphasizing the continuity of the journey.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1023/6826504502.mp4&quot;}}},{&quot;rowIdx&quot;:75,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1024-3843551232&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide an exhaustive description of the video content.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video features a serene indoor setting with two dogs. The larger dog, with a black and white coat, is lying on a carpeted floor near a chair with metal legs and a wooden table. The smaller dog, entirely black, is initially lying down a short distance away from the larger dog. The smaller dog gets up and moves around, exploring the area, while the larger dog remains mostly stationary, occasionally shifting its position slightly. The background consistently shows the carpeted floor and parts of the furniture, including a chair with metal legs and a wheeled office chair. As the video progresses, the smaller dog continues to explore the area, moving around before eventually lying down on the floor a short distance away from the larger dog. Throughout the video, the larger dog remains mostly stationary, providing a calm and steady presence in the scene.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide an exhaustive description of the video content.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video features a serene indoor setting with two dogs. The larger dog, with a black and white coat, is lying on a carpeted floor near a chair with metal legs and a wooden table. The smaller dog, entirely black, is initially lying down a short distance away from the larger dog. The smaller dog gets up and moves around, exploring the area, while the larger dog remains mostly stationary, occasionally shifting its position slightly. The background consistently shows the carpeted floor and parts of the furniture, including a chair with metal legs and a wheeled office chair. As the video progresses, the smaller dog continues to explore the area, moving around before eventually lying down on the floor a short distance away from the larger dog. Throughout the video, the larger dog remains mostly stationary, providing a calm and steady presence in the scene.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1024/3843551232.mp4&quot;}}},{&quot;rowIdx&quot;:76,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1025-7359783390&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite a comprehensive depiction of the entire video clip.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a person in a red jacket and dark pants walking through a snowy forest. They are wearing a backpack and navigating through the snow, which has visible footprints. The background consists of tall trees and a misty atmosphere, suggesting a cold and possibly early morning setting. As they walk, they occasionally look down at the snow and adjust their balance. The person moves closer to the camera, making the details of their red jacket and backpack more visible. The scene transitions to a close-up of the person's upper body and backpack as they turn slightly to the side, revealing more of the snowy forest in the background. The narrative continues with the person now seen from behind, still walking through the snowy forest with the same backpack. The snowy ground is uneven, and they carefully navigate the terrain, occasionally spreading their arms for balance. The background remains consistent with tall trees and a misty atmosphere, reinforcing the cold and possibly early morning setting. As they move forward, they encounter a fallen tree trunk partially covered in snow. The person pauses, seemingly assessing the best way to cross it, and the video concludes with them beginning to step over the fallen tree trunk, continuing their journey through the snowy forest.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite a comprehensive depiction of the entire video clip.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a person in a red jacket and dark pants walking through a snowy forest. They are wearing a backpack and navigating through the snow, which has visible footprints. The background consists of tall trees and a misty atmosphere, suggesting a cold and possibly early morning setting. As they walk, they occasionally look down at the snow and adjust their balance. The person moves closer to the camera, making the details of their red jacket and backpack more visible. The scene transitions to a close-up of the person's upper body and backpack as they turn slightly to the side, revealing more of the snowy forest in the background. The narrative continues with the person now seen from behind, still walking through the snowy forest with the same backpack. The snowy ground is uneven, and they carefully navigate the terrain, occasionally spreading their arms for balance. The background remains consistent with tall trees and a misty atmosphere, reinforcing the cold and possibly early morning setting. As they move forward, they encounter a fallen tree trunk partially covered in snow. The person pauses, seemingly assessing the best way to cross it, and the video concludes with them beginning to step over the fallen tree trunk, continuing their journey through the snowy forest.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1025/7359783390.mp4&quot;}}},{&quot;rowIdx&quot;:77,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1051-6301223138&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWalk through the video, detailing its key moments and features.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a serene beach scene where a woman dressed in a pink top, navy blue skirt, and white hat is interacting with a small child at the edge of the water. The child is sitting in the shallow water, and the woman is bending over, holding the child's hand, seemingly helping or playing with them. Gentle waves crash onto the shore in the background, creating a peaceful atmosphere. As they enjoy their time together, another person in a polka-dotted swimsuit briefly walks by, momentarily passing between the camera and the woman with the child. The focus remains on the woman and child, with the woman maintaining her supportive stance to ensure the child's safety and enjoyment.\n\nThe scene continues with the woman and child at the water's edge. The woman, still in her pink top, navy blue skirt, and white hat, bends over to hold the child's hand, ensuring the child remains safe as they sit in the shallow water. The waves continue to gently crash onto the shore, providing a soothing backdrop. Throughout this part of the video, the woman occasionally adjusts her position slightly but always keeps a firm hold on the child's hand. The child appears to be enjoying the water, occasionally looking up at the woman, who remains focused on their interaction. No other people pass by during this segment, keeping the attention on the woman and child.\n\nThe video wraps up with the woman helping the child stand up in the shallow water. She holds both of the child's hands to provide support as the child, dressed in a red outfit, takes tentative steps in the water. The woman closely guides the child, ensuring their safety while the waves continue to gently crash onto the shore in the background. The sandy beach is visible in the foreground, adding to the idyllic setting. The scene remains focused on their interaction, highlighting the woman's care and the child's exploration of the water, with no other people passing by in this final segment.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWalk through the video, detailing its key moments and features.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a serene beach scene where a woman dressed in a pink top, navy blue skirt, and white hat is interacting with a small child at the edge of the water. The child is sitting in the shallow water, and the woman is bending over, holding the child's hand, seemingly helping or playing with them. Gentle waves crash onto the shore in the background, creating a peaceful atmosphere. As they enjoy their time together, another person in a polka-dotted swimsuit briefly walks by, momentarily passing between the camera and the woman with the child. The focus remains on the woman and child, with the woman maintaining her supportive stance to ensure the child's safety and enjoyment.\\n\\nThe scene continues with the woman and child at the water's edge. The woman, still in her pink top, navy blue skirt, and white hat, bends over to hold the child's hand, ensuring the child remains safe as they sit in the shallow water. The waves continue to gently crash onto the shore, providing a soothing backdrop. Throughout this part of the video, the woman occasionally adjusts her position slightly but always keeps a firm hold on the child's hand. The child appears to be enjoying the water, occasionally looking up at the woman, who remains focused on their interaction. No other people pass by during this segment, keeping the attention on the woman and child.\\n\\nThe video wraps up with the woman helping the child stand up in the shallow water. She holds both of the child's hands to provide support as the child, dressed in a red outfit, takes tentative steps in the water. The woman closely guides the child, ensuring their safety while the waves continue to gently crash onto the shore in the background. The sandy beach is visible in the foreground, adding to the idyllic setting. The scene remains focused on their interaction, highlighting the woman's care and the child's exploration of the water, with no other people passing by in this final segment.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1051/6301223138.mp4&quot;}}},{&quot;rowIdx&quot;:78,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1104-8181417359&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDissect the video's content, explaining each element thoroughly.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video captures a tender and heartwarming moment between two children lying on a soft, light blue blanket adorned with colorful patterns, including a red dot and blue lines. The older child, dressed in a white shirt with colorful stripes, gently interacts with a baby who is wearing a white outfit with pink polka dots and a white headband. The older child holds the baby's hand, showing affection and care. The background is a neutral-colored carpet, and various toys are visible around them, adding to the cozy and playful atmosphere. Throughout the video, the sense of bonding and love between the two children is palpable, as the older child continues to show gentle affection towards the baby, creating a touching and serene scene.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDissect the video's content, explaining each element thoroughly.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video captures a tender and heartwarming moment between two children lying on a soft, light blue blanket adorned with colorful patterns, including a red dot and blue lines. The older child, dressed in a white shirt with colorful stripes, gently interacts with a baby who is wearing a white outfit with pink polka dots and a white headband. The older child holds the baby's hand, showing affection and care. The background is a neutral-colored carpet, and various toys are visible around them, adding to the cozy and playful atmosphere. Throughout the video, the sense of bonding and love between the two children is palpable, as the older child continues to show gentle affection towards the baby, creating a touching and serene scene.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1104/8181417359.mp4&quot;}}},{&quot;rowIdx&quot;:79,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1122-2573200479&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nElaborate on all aspects of the video you are viewing.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a child walking down a hallway towards the camera, dressed in a dark shirt with a colorful design and blue pants. The hallway features wooden floors and a bubble wrap mat. In the background, a person in a grey skirt and red shoes is seen in the kitchen area, engaged in some activity. The child reaches out to touch the wall and door as they walk, creating a sense of curiosity and exploration. The home environment is warm and lived-in, with wooden cabinetry and a washing machine visible in the kitchen. The child then moves out of the frame, heading back towards the kitchen area.\n\nThe scene transitions to the child walking down the hallway again, this time moving towards the kitchen area. They pass a door on the right and continue their journey. The person in the kitchen remains engaged in their activity, adding a sense of continuity and daily life. The child moves closer to the kitchen, where the washing machine and wooden cabinetry are more prominently visible. They then turn around and start walking back towards the hallway, passing a white object on the floor, which adds a touch of realism to the setting.\n\nIn the final scene, the child is once again walking down the hallway towards the camera, maintaining their curious behavior by touching the wall and door. The person in the kitchen, still wearing a grey skirt and red shoes, continues their activity, providing a consistent backdrop. The white object on the floor in the kitchen is also visible, reinforcing the lived-in feel of the home. As the child moves closer to the camera, they continue to interact with their surroundings before eventually moving out of the frame, leaving the viewer with a sense of warmth and familiarity in this home environment.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nElaborate on all aspects of the video you are viewing.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a child walking down a hallway towards the camera, dressed in a dark shirt with a colorful design and blue pants. The hallway features wooden floors and a bubble wrap mat. In the background, a person in a grey skirt and red shoes is seen in the kitchen area, engaged in some activity. The child reaches out to touch the wall and door as they walk, creating a sense of curiosity and exploration. The home environment is warm and lived-in, with wooden cabinetry and a washing machine visible in the kitchen. The child then moves out of the frame, heading back towards the kitchen area.\\n\\nThe scene transitions to the child walking down the hallway again, this time moving towards the kitchen area. They pass a door on the right and continue their journey. The person in the kitchen remains engaged in their activity, adding a sense of continuity and daily life. The child moves closer to the kitchen, where the washing machine and wooden cabinetry are more prominently visible. They then turn around and start walking back towards the hallway, passing a white object on the floor, which adds a touch of realism to the setting.\\n\\nIn the final scene, the child is once again walking down the hallway towards the camera, maintaining their curious behavior by touching the wall and door. The person in the kitchen, still wearing a grey skirt and red shoes, continues their activity, providing a consistent backdrop. The white object on the floor in the kitchen is also visible, reinforcing the lived-in feel of the home. As the child moves closer to the camera, they continue to interact with their surroundings before eventually moving out of the frame, leaving the viewer with a sense of warmth and familiarity in this home environment.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1122/2573200479.mp4&quot;}}},{&quot;rowIdx&quot;:80,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1150-4772810451&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite a comprehensive depiction of the entire video clip.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with three dogs interacting in an outdoor setting. The dogs include two black and white dogs and one small white dog. The ground is covered with dirt and leaves, and there are some rocks and a green plastic chair visible in the background. The black and white dogs are sniffing the small white dog, and they all appear to be exploring their surroundings. The dogs move around, with the black and white dogs showing interest in each other and the small white dog. Towards the end of this segment, the two black and white dogs focus their attention on the green plastic chair, sniffing around it, while the small white dog is no longer in the frame.\n\nContinuing from the previous scene, the two black and white dogs are still near the green plastic chair, sniffing around it with curiosity. The small white dog reappears, joining the other two dogs. The dogs move around the area, with the black and white dogs occasionally sniffing the ground and each other, while the small white dog also explores the surroundings. The background remains consistent, featuring dirt, leaves, and a tent in the distance. The dogs eventually move away from the chair, heading towards a different part of the outdoor setting, with the black and white dogs leading and the small white dog following.\n\nIn the final segment, the two black and white dogs are near a mesh tent structure, continuing from the previous scene. The background now features a camper, a picnic table, and a wooded area with dirt and leaves on the ground. The dogs are sniffing around a tree stump. The small white dog reappears and joins the other two dogs. The dogs move around the area, with the black and white dogs leading and the small white dog following. The black and white dogs approach the camera, and the small white dog follows closely behind. The video concludes with a close-up of the small white dog sniffing the ground near a pile of rocks, and then both the small white dog and one of the black and white dogs interact with a person, sniffing their legs and showing curiosity.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite a comprehensive depiction of the entire video clip.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with three dogs interacting in an outdoor setting. The dogs include two black and white dogs and one small white dog. The ground is covered with dirt and leaves, and there are some rocks and a green plastic chair visible in the background. The black and white dogs are sniffing the small white dog, and they all appear to be exploring their surroundings. The dogs move around, with the black and white dogs showing interest in each other and the small white dog. Towards the end of this segment, the two black and white dogs focus their attention on the green plastic chair, sniffing around it, while the small white dog is no longer in the frame.\\n\\nContinuing from the previous scene, the two black and white dogs are still near the green plastic chair, sniffing around it with curiosity. The small white dog reappears, joining the other two dogs. The dogs move around the area, with the black and white dogs occasionally sniffing the ground and each other, while the small white dog also explores the surroundings. The background remains consistent, featuring dirt, leaves, and a tent in the distance. The dogs eventually move away from the chair, heading towards a different part of the outdoor setting, with the black and white dogs leading and the small white dog following.\\n\\nIn the final segment, the two black and white dogs are near a mesh tent structure, continuing from the previous scene. The background now features a camper, a picnic table, and a wooded area with dirt and leaves on the ground. The dogs are sniffing around a tree stump. The small white dog reappears and joins the other two dogs. The dogs move around the area, with the black and white dogs leading and the small white dog following. The black and white dogs approach the camera, and the small white dog follows closely behind. The video concludes with a close-up of the small white dog sniffing the ground near a pile of rocks, and then both the small white dog and one of the black and white dogs interact with a person, sniffing their legs and showing curiosity.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1150/4772810451.mp4&quot;}}},{&quot;rowIdx&quot;:81,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1150-6888660962&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nElaborate on all aspects of the video you are viewing.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video features a heartwarming interaction between a baby and an adult in a cozy, lived-in room. The baby, dressed in a dark blue onesie adorned with stars, planets, and spaceships, is lying on the adult's chest. The adult, wearing a white shirt and dark pants, gently holds and moves the baby's hands. The background includes a green patterned couch and a wooden side table with various items, such as a sewing machine and a white container. The baby looks around curiously and interacts with the adult.\n\nThe scene transitions to the baby being held up by the adult, who supports the baby's hands and helps the baby stand. The baby stands on the adult's chest while the adult's legs are bent at the knees. The baby continues to move slightly, exploring the surroundings and engaging with the adult.\n\nThe final part of the video shows the baby still being held up by the adult, maintaining the same supportive stance. The background remains consistent with the green patterned couch and the wooden side table. Additionally, a shelf with books and other items is visible, enhancing the warm and homely atmosphere of the room. Throughout the video, the baby and the adult share a tender and playful moment, highlighting the bond between them.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nElaborate on all aspects of the video you are viewing.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video features a heartwarming interaction between a baby and an adult in a cozy, lived-in room. The baby, dressed in a dark blue onesie adorned with stars, planets, and spaceships, is lying on the adult's chest. The adult, wearing a white shirt and dark pants, gently holds and moves the baby's hands. The background includes a green patterned couch and a wooden side table with various items, such as a sewing machine and a white container. The baby looks around curiously and interacts with the adult.\\n\\nThe scene transitions to the baby being held up by the adult, who supports the baby's hands and helps the baby stand. The baby stands on the adult's chest while the adult's legs are bent at the knees. The baby continues to move slightly, exploring the surroundings and engaging with the adult.\\n\\nThe final part of the video shows the baby still being held up by the adult, maintaining the same supportive stance. The background remains consistent with the green patterned couch and the wooden side table. Additionally, a shelf with books and other items is visible, enhancing the warm and homely atmosphere of the room. Throughout the video, the baby and the adult share a tender and playful moment, highlighting the bond between them.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1150/6888660962.mp4&quot;}}},{&quot;rowIdx&quot;:82,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1161-2977504625&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide a comprehensive analysis of the video's content and themes.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a serene scene on a paved path surrounded by lush greenery and trees. A child with blonde hair is seated in a small, red toy car, holding onto the steering wheel. A person dressed in a white shirt, blue shorts, and black shoes is seen pushing the toy car from behind. The clear, sunny day enhances the vibrant green foliage in the background. As the video progresses, the person continues to push the toy car along the path, occasionally looking back, possibly to check their surroundings or interact with the child. The consistent background of greenery and the joyful interaction between the child and the person pushing the car create a warm and pleasant atmosphere throughout the video.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide a comprehensive analysis of the video's content and themes.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a serene scene on a paved path surrounded by lush greenery and trees. A child with blonde hair is seated in a small, red toy car, holding onto the steering wheel. A person dressed in a white shirt, blue shorts, and black shoes is seen pushing the toy car from behind. The clear, sunny day enhances the vibrant green foliage in the background. As the video progresses, the person continues to push the toy car along the path, occasionally looking back, possibly to check their surroundings or interact with the child. The consistent background of greenery and the joyful interaction between the child and the person pushing the car create a warm and pleasant atmosphere throughout the video.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1161/2977504625.mp4&quot;}}},{&quot;rowIdx&quot;:83,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1165-8521113411&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDissect the video's content, explaining each element thoroughly.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a classroom setting, where two children are standing at the front of the room. The child on the left is dressed in a blue shirt with a graphic design and black pants, while the child on the right is wearing a black shirt featuring a robot graphic and the words 'high five,' paired with blue jeans and white sneakers. The child on the right is holding a plunger above their head. The classroom environment is typical, with desks, chairs, and various educational posters and decorations adorning the walls. Other children are seated at their desks, engaged in different activities. The scene remains consistent throughout the video, maintaining the same background and activities, with the two children continuing to stand at the front of the room in the same attire and positions.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDissect the video's content, explaining each element thoroughly.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a classroom setting, where two children are standing at the front of the room. The child on the left is dressed in a blue shirt with a graphic design and black pants, while the child on the right is wearing a black shirt featuring a robot graphic and the words 'high five,' paired with blue jeans and white sneakers. The child on the right is holding a plunger above their head. The classroom environment is typical, with desks, chairs, and various educational posters and decorations adorning the walls. Other children are seated at their desks, engaged in different activities. The scene remains consistent throughout the video, maintaining the same background and activities, with the two children continuing to stand at the front of the room in the same attire and positions.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1165/8521113411.mp4&quot;}}},{&quot;rowIdx&quot;:84,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;1201-7967750522&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nOffer a detailed interpretation of the video's message and imagery.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a close-up of a wet tiled floor, showcasing a mix of beige, white, and red tiles. This scene transitions to a wider view of a plaza adorned with multiple water fountains shooting streams of water into the air. The fountains are arranged in a line, creating a reflective surface on the dark tiles of the plaza. In the background, buildings with large columns and glass windows are visible, along with potted plants featuring red flowers. The fountains continue to spray water, and a person dressed in black runs through the water jets, creating playful splashes and adding a dynamic element to the scene. The person runs across the wet plaza, emphasizing the lively atmosphere created by the fountains.\n\nThe scene then shifts to a plaza with light-colored tiles, where a man in a green t-shirt and beige shorts is walking. The background features a building with large glass windows and columns, similar to the previous scene, and potted plants with red flowers are also visible. Another person in a black outfit runs across the plaza, adding a sense of movement and activity. The man in the green t-shirt continues to walk, while another person in a dark outfit is seen in the background, also walking. The video wraps up with the man in the green t-shirt turning slightly, while the person in black continues to run, maintaining the lively and dynamic atmosphere of the scene.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nOffer a detailed interpretation of the video's message and imagery.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a close-up of a wet tiled floor, showcasing a mix of beige, white, and red tiles. This scene transitions to a wider view of a plaza adorned with multiple water fountains shooting streams of water into the air. The fountains are arranged in a line, creating a reflective surface on the dark tiles of the plaza. In the background, buildings with large columns and glass windows are visible, along with potted plants featuring red flowers. The fountains continue to spray water, and a person dressed in black runs through the water jets, creating playful splashes and adding a dynamic element to the scene. The person runs across the wet plaza, emphasizing the lively atmosphere created by the fountains.\\n\\nThe scene then shifts to a plaza with light-colored tiles, where a man in a green t-shirt and beige shorts is walking. The background features a building with large glass windows and columns, similar to the previous scene, and potted plants with red flowers are also visible. Another person in a black outfit runs across the plaza, adding a sense of movement and activity. The man in the green t-shirt continues to walk, while another person in a dark outfit is seen in the background, also walking. The video wraps up with the man in the green t-shirt turning slightly, while the person in black continues to run, maintaining the lively and dynamic atmosphere of the scene.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/NextQA/1201/7967750522.mp4&quot;}}},{&quot;rowIdx&quot;:85,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;v_B6W_0AvgGTY&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nOffer a detailed interpretation of the video's message and imagery.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a logo on a white background, featuring the text 'IQK9 TRAINING' in a bold, teal and black font. The 'IQK9' part is larger and more prominent, while 'TRAINING' is smaller and positioned below and to the right. To the left of the text is an illustration of a dog's head, outlined in black and filled with teal. The scene transitions to an outdoor park setting where a person in an orange shirt and black shorts is training a black and white dog. The person gives commands, and the dog responds by sitting and then running after a thrown object, likely a frisbee. The text 'www.IQK9Training.com' appears at the bottom of the screen, indicating the website for the training service. The dog runs across the grassy field, showcasing its agility and training.\n\nThe video continues in the outdoor park setting, capturing a wide view of the grassy field with houses and trees in the backdrop. The text 'www.IQK9Training.com' is prominently displayed at the bottom of the screen. The field is mostly empty, with a few scattered objects and people in the distance. A black and white dog is seen running across the field, holding a red frisbee in its mouth, demonstrating its training and agility.\n\nThe video progresses with a continuation of the outdoor park setting. A person in an orange shirt and black shorts walks alongside a black and white dog holding a red frisbee in its mouth. The text 'www.IQK9Training.com' is prominently displayed at the bottom of the screen. The background features trees with autumn foliage and a few scattered objects. The scene transitions back to a white background displaying the 'IQK9 TRAINING' logo, reinforcing the branding of the training service.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nOffer a detailed interpretation of the video's message and imagery.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a logo on a white background, featuring the text 'IQK9 TRAINING' in a bold, teal and black font. The 'IQK9' part is larger and more prominent, while 'TRAINING' is smaller and positioned below and to the right. To the left of the text is an illustration of a dog's head, outlined in black and filled with teal. The scene transitions to an outdoor park setting where a person in an orange shirt and black shorts is training a black and white dog. The person gives commands, and the dog responds by sitting and then running after a thrown object, likely a frisbee. The text 'www.IQK9Training.com' appears at the bottom of the screen, indicating the website for the training service. The dog runs across the grassy field, showcasing its agility and training.\\n\\nThe video continues in the outdoor park setting, capturing a wide view of the grassy field with houses and trees in the backdrop. The text 'www.IQK9Training.com' is prominently displayed at the bottom of the screen. The field is mostly empty, with a few scattered objects and people in the distance. A black and white dog is seen running across the field, holding a red frisbee in its mouth, demonstrating its training and agility.\\n\\nThe video progresses with a continuation of the outdoor park setting. A person in an orange shirt and black shorts walks alongside a black and white dog holding a red frisbee in its mouth. The text 'www.IQK9Training.com' is prominently displayed at the bottom of the screen. The background features trees with autumn foliage and a few scattered objects. The scene transitions back to a white background displaying the 'IQK9 TRAINING' logo, reinforcing the branding of the training service.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/activitynet/v_B6W_0AvgGTY.mp4&quot;}}},{&quot;rowIdx&quot;:86,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;v_H6hb3SIqdL4&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDetail the video's plot development, character arcs, and thematic elements.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a rustic, cluttered shed with wooden walls and various tools and objects scattered around, creating a casual and somewhat disorganized atmosphere. A man wearing a red, white, and blue striped polo shirt is seen playing darts. He throws several darts towards a black dartboard with white sections and numbers around the edge, which is mounted on a wooden door. The man appears focused and engaged in the activity. The camera then moves closer to the dartboard, showing the darts stuck in it, indicating the man's accuracy. Items like a folding chair, a radio, and coiled extension cords are visible in the background, adding to the shed's informal setting. The man then approaches the dartboard, and the camera focuses on the darts stuck in it, further highlighting his precision. He reaches out and begins to remove the darts from the dartboard, maintaining the consistent, cluttered backdrop of the shed filled with various tools and objects.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDetail the video's plot development, character arcs, and thematic elements.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a rustic, cluttered shed with wooden walls and various tools and objects scattered around, creating a casual and somewhat disorganized atmosphere. A man wearing a red, white, and blue striped polo shirt is seen playing darts. He throws several darts towards a black dartboard with white sections and numbers around the edge, which is mounted on a wooden door. The man appears focused and engaged in the activity. The camera then moves closer to the dartboard, showing the darts stuck in it, indicating the man's accuracy. Items like a folding chair, a radio, and coiled extension cords are visible in the background, adding to the shed's informal setting. The man then approaches the dartboard, and the camera focuses on the darts stuck in it, further highlighting his precision. He reaches out and begins to remove the darts from the dartboard, maintaining the consistent, cluttered backdrop of the shed filled with various tools and objects.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/activitynet/v_H6hb3SIqdL4.mp4&quot;}}},{&quot;rowIdx&quot;:87,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;v_IuntoXkEWPI&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nElaborate on all aspects of the video you are viewing.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a young child, dressed in a white outfit, attempting to climb up a bright yellow slide in a playground. The playground structure is made of wood, and there are trees and other playground equipment visible in the background. Some graffiti is visible on the wooden structure, and a few parked cars can be seen through the trees. The child makes several attempts to climb the slide, using both hands and feet to gain traction on the smooth surface, showing determination despite the challenge. The scene continues with the child persistently trying to ascend the slide, making incremental progress. The background elements, including the wooden playground structure, graffiti, and parked cars, remain consistent. The child's effort and persistence are evident as they continue to climb. As the video progresses, the child reaches a higher point on the slide but then starts to slide back down. Eventually, the child slides down the slide, sitting upright and appearing to enjoy the descent. The video concludes with the child at the bottom of the slide, looking up at the structure, possibly contemplating another attempt.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nElaborate on all aspects of the video you are viewing.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a young child, dressed in a white outfit, attempting to climb up a bright yellow slide in a playground. The playground structure is made of wood, and there are trees and other playground equipment visible in the background. Some graffiti is visible on the wooden structure, and a few parked cars can be seen through the trees. The child makes several attempts to climb the slide, using both hands and feet to gain traction on the smooth surface, showing determination despite the challenge. The scene continues with the child persistently trying to ascend the slide, making incremental progress. The background elements, including the wooden playground structure, graffiti, and parked cars, remain consistent. The child's effort and persistence are evident as they continue to climb. As the video progresses, the child reaches a higher point on the slide but then starts to slide back down. Eventually, the child slides down the slide, sitting upright and appearing to enjoy the descent. The video concludes with the child at the bottom of the slide, looking up at the structure, possibly contemplating another attempt.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/activitynet/v_IuntoXkEWPI.mp4&quot;}}},{&quot;rowIdx&quot;:88,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;v_Vyj5eIh3jh0&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nElaborate on all aspects of the video you are viewing.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a dark scene where a small fire is visible in the distance. As the fire grows larger and more intense, it illuminates the surrounding area, revealing a sandy surface. The flames are bright and dynamic, casting light and shadows. A white pickup truck is parked near the fire, adding to the scene's intrigue. The fire continues to grow, creating a long, continuous line of flames on the sand. The flames rise high into the air, reaching their peak intensity. The video then focuses on the intense fire on the sandy surface, with the flames casting dramatic light and shadows. The fire forms a long, continuous line, with large flames reaching their highest point. The background remains dark, emphasizing the brightness and intensity of the fire. The white pickup truck is no longer visible, allowing the viewer to fully appreciate the peak intensity of the flames.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nElaborate on all aspects of the video you are viewing.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a dark scene where a small fire is visible in the distance. As the fire grows larger and more intense, it illuminates the surrounding area, revealing a sandy surface. The flames are bright and dynamic, casting light and shadows. A white pickup truck is parked near the fire, adding to the scene's intrigue. The fire continues to grow, creating a long, continuous line of flames on the sand. The flames rise high into the air, reaching their peak intensity. The video then focuses on the intense fire on the sandy surface, with the flames casting dramatic light and shadows. The fire forms a long, continuous line, with large flames reaching their highest point. The background remains dark, emphasizing the brightness and intensity of the fire. The white pickup truck is no longer visible, allowing the viewer to fully appreciate the peak intensity of the flames.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/activitynet/v_Vyj5eIh3jh0.mp4&quot;}}},{&quot;rowIdx&quot;:89,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;v_WjEh7acrr5o&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide an exhaustive description of the video content.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video features a woman in a red shirt sitting and knitting in a cozy, craft-oriented environment, possibly at a fair or workshop. She is holding knitting needles and working with yarn, with a colorful knitted item, a water bottle, and various other items on a table covered with an orange cloth in the background. A sign in the background reads 'Bargain beauty' and 'Flippin' fab cookery' with an image of a dish, indicating a setting that combines elements of beauty and cooking. The scene continues with the woman knitting, maintaining the same background elements and reinforcing the warm, creative atmosphere of the setting.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide an exhaustive description of the video content.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video features a woman in a red shirt sitting and knitting in a cozy, craft-oriented environment, possibly at a fair or workshop. She is holding knitting needles and working with yarn, with a colorful knitted item, a water bottle, and various other items on a table covered with an orange cloth in the background. A sign in the background reads 'Bargain beauty' and 'Flippin' fab cookery' with an image of a dish, indicating a setting that combines elements of beauty and cooking. The scene continues with the woman knitting, maintaining the same background elements and reinforcing the warm, creative atmosphere of the setting.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/activitynet/v_WjEh7acrr5o.mp4&quot;}}},{&quot;rowIdx&quot;:90,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;v_Zrv8_-Y-jSM&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nDescribe every scene and its significance in the video.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a black screen that transitions to a blue background with a futuristic design, featuring a central screen with a downward arrow icon. Bright star-like flashes appear, followed by the text 'ROPE SKIPPING...' in large white letters. The scene then shifts to a gymnasium where five individuals are seen skipping rope. The gym has a yellow floor with green lines, a brick wall, and a basketball hoop in the background. The individuals continue their rope skipping routine throughout the frames. The video develops through the same gymnasium setting, with the five individuals now performing a synchronized rope skipping routine. They are positioned in a line, each wearing different colored outfits: one in red, two in black, one in pink, and one in purple. The individuals jump in unison, with their arms extended outward, maintaining their rhythm and coordination, which emphasizes the energetic atmosphere of the gymnasium. The video wraps up with the same gymnasium setting, where the five individuals continue their synchronized rope skipping routine. The text 'Klasse 7c / Fr. Leifels' appears on the screen, indicating the class and instructor. The individuals transition from jumping to performing a coordinated sequence of movements, including squatting and sitting on the floor, while still maintaining their rhythm and coordination. The video concludes with the individuals seated on the floor, with the text 'Klasse 7c / Fr. Leifels' still visible on the screen.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nDescribe every scene and its significance in the video.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a black screen that transitions to a blue background with a futuristic design, featuring a central screen with a downward arrow icon. Bright star-like flashes appear, followed by the text 'ROPE SKIPPING...' in large white letters. The scene then shifts to a gymnasium where five individuals are seen skipping rope. The gym has a yellow floor with green lines, a brick wall, and a basketball hoop in the background. The individuals continue their rope skipping routine throughout the frames. The video develops through the same gymnasium setting, with the five individuals now performing a synchronized rope skipping routine. They are positioned in a line, each wearing different colored outfits: one in red, two in black, one in pink, and one in purple. The individuals jump in unison, with their arms extended outward, maintaining their rhythm and coordination, which emphasizes the energetic atmosphere of the gymnasium. The video wraps up with the same gymnasium setting, where the five individuals continue their synchronized rope skipping routine. The text 'Klasse 7c / Fr. Leifels' appears on the screen, indicating the class and instructor. The individuals transition from jumping to performing a coordinated sequence of movements, including squatting and sitting on the floor, while still maintaining their rhythm and coordination. The video concludes with the individuals seated on the floor, with the text 'Klasse 7c / Fr. Leifels' still visible on the screen.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/activitynet/v_Zrv8_-Y-jSM.mp4&quot;}}},{&quot;rowIdx&quot;:91,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;v_gCx-ucvPhDY&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nConvey the narrative and visual elements of the video in detail.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video features a diver at the 2012 London Olympics, standing on a platform and preparing for a dive. The diver is dressed in a black swimsuit with red accents, and the 'London 2012' Olympic signage is visible in the background, setting the scene at this prestigious event. The diver walks towards the edge of the platform, building momentum before executing a series of impressive flips and twists in the air. The audience and judges are seen in the background, attentively watching the performance. The video concludes with the diver entering the water with a splash, showcasing their skill and precision.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nConvey the narrative and visual elements of the video in detail.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video features a diver at the 2012 London Olympics, standing on a platform and preparing for a dive. The diver is dressed in a black swimsuit with red accents, and the 'London 2012' Olympic signage is visible in the background, setting the scene at this prestigious event. The diver walks towards the edge of the platform, building momentum before executing a series of impressive flips and twists in the air. The audience and judges are seen in the background, attentively watching the performance. The video concludes with the diver entering the water with a splash, showcasing their skill and precision.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/activitynet/v_gCx-ucvPhDY.mp4&quot;}}},{&quot;rowIdx&quot;:92,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;v_y0Kio7VOk5o&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide a comprehensive analysis of the video's content and themes.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a professional indoor diving facility, characterized by its well-lit environment, industrial lighting, and prominent decorations, including a large American flag and several banners, one of which reads 'NCAA.' It begins with a person in a blue swimsuit standing on a high diving platform, which is beige with metal railings and part of a larger structure with multiple levels and stairs. The person adjusts their position and stance, preparing for the dive. The scene then progresses to the person standing at the edge of the platform, raising their arms above their head in preparation for the dive. After a moment of stillness, they begin to lean forward, initiating the dive. The video captures the person mid-dive, in an upside-down position with their body arched and arms extended towards the water. The sequence continues with the person entering the water, creating a splash, and then emerging from the water to swim towards the pool's edge. The video concludes with the person reaching the pool's edge and climbing out of the water, successfully completing the dive. Throughout the video, the professional atmosphere of the facility is consistently maintained.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide a comprehensive analysis of the video's content and themes.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a professional indoor diving facility, characterized by its well-lit environment, industrial lighting, and prominent decorations, including a large American flag and several banners, one of which reads 'NCAA.' It begins with a person in a blue swimsuit standing on a high diving platform, which is beige with metal railings and part of a larger structure with multiple levels and stairs. The person adjusts their position and stance, preparing for the dive. The scene then progresses to the person standing at the edge of the platform, raising their arms above their head in preparation for the dive. After a moment of stillness, they begin to lean forward, initiating the dive. The video captures the person mid-dive, in an upside-down position with their body arched and arms extended towards the water. The sequence continues with the person entering the water, creating a splash, and then emerging from the water to swim towards the pool's edge. The video concludes with the person reaching the pool's edge and climbing out of the water, successfully completing the dive. Throughout the video, the professional atmosphere of the facility is consistently maintained.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/activitynet/v_y0Kio7VOk5o.mp4&quot;}}},{&quot;rowIdx&quot;:93,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;143f43b6-3108-4977-ae62-d94e74e14ad6&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nConvey the narrative and visual elements of the video in detail.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a modern office setting and revolves around a demonstration or testing of technology, likely involving virtual reality or augmented reality equipment. It begins with a person in a white shirt standing near a cluttered table, holding a black object, possibly a VR headset. Another person in a dark blue shirt stands near a door, while a third person, seated and wearing a head-mounted device, raises their arms. The person in the dark blue shirt mimics the seated person's actions by also raising their arms. The person in the white shirt leans over the table, possibly interacting with a laptop, while the other two individuals maintain their arm positions.\n\nThe scene continues with the person in the white shirt standing and gesturing, possibly explaining the functionality of the device on the table. The person in the dark blue shirt remains near the door, observing, while the seated person with the head-mounted device extends their arms forward. The table is cluttered with various items, including a laptop, documents, a coffee mug, a blue Ethernet cable, a box of disposable masks, a bottle of hand sanitizer, a stapler, and a bottle of green tea. The large screen in the background still displays a webpage. The person in the white shirt points at a document on the table, possibly providing further explanation or instructions.\n\nThe video wraps up with the person in the white shirt standing near the table, holding the black object near their face. The person in the dark blue shirt continues to observe from near the door. Another individual, wearing a green vest over a white shirt, is also holding a similar black object near their face. The table remains cluttered with various items, and the large screen in the background still displays a webpage with images. Both the person in the white shirt and the person in the green vest appear to adjust the black objects near their faces, possibly preparing to use them. The overall scene suggests a continuation of the demonstration or testing of the technology.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nConvey the narrative and visual elements of the video in detail.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a modern office setting and revolves around a demonstration or testing of technology, likely involving virtual reality or augmented reality equipment. It begins with a person in a white shirt standing near a cluttered table, holding a black object, possibly a VR headset. Another person in a dark blue shirt stands near a door, while a third person, seated and wearing a head-mounted device, raises their arms. The person in the dark blue shirt mimics the seated person's actions by also raising their arms. The person in the white shirt leans over the table, possibly interacting with a laptop, while the other two individuals maintain their arm positions.\\n\\nThe scene continues with the person in the white shirt standing and gesturing, possibly explaining the functionality of the device on the table. The person in the dark blue shirt remains near the door, observing, while the seated person with the head-mounted device extends their arms forward. The table is cluttered with various items, including a laptop, documents, a coffee mug, a blue Ethernet cable, a box of disposable masks, a bottle of hand sanitizer, a stapler, and a bottle of green tea. The large screen in the background still displays a webpage. The person in the white shirt points at a document on the table, possibly providing further explanation or instructions.\\n\\nThe video wraps up with the person in the white shirt standing near the table, holding the black object near their face. The person in the dark blue shirt continues to observe from near the door. Another individual, wearing a green vest over a white shirt, is also holding a similar black object near their face. The table remains cluttered with various items, and the large screen in the background still displays a webpage with images. Both the person in the white shirt and the person in the green vest appear to adjust the black objects near their faces, possibly preparing to use them. The overall scene suggests a continuation of the demonstration or testing of the technology.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/ego4d/143f43b6-3108-4977-ae62-d94e74e14ad6.mp4&quot;}}},{&quot;rowIdx&quot;:94,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;101-0O4bxhpFX9o-split_0&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide an exhaustive description of the video content.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with two individuals sitting side by side in a kitchen setting, one in a white t-shirt and the other in a sleeveless top with a colorful floral pattern. The background features a brick wall and various kitchen items, creating a cozy and homely atmosphere. The scene transitions to a close-up of hands, belonging to the person in the white t-shirt who has a tattoo on their forearm, carefully placing strips of raw bacon onto a metal baking tray. The bacon is meticulously laid out in rows. The baking tray is then placed into an oven, with overlaid text reading '400°F 10-15 minutes,' providing the cooking instructions for the bacon. The video continues with a close-up of the same person placing the tray into the oven, emphasizing the cooking process. The oven door is open, and the text '400°F 10-15 minutes' remains visible, reinforcing the cooking instructions. The person closes the oven door, marking the end of the cooking preparation.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide an exhaustive description of the video content.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with two individuals sitting side by side in a kitchen setting, one in a white t-shirt and the other in a sleeveless top with a colorful floral pattern. The background features a brick wall and various kitchen items, creating a cozy and homely atmosphere. The scene transitions to a close-up of hands, belonging to the person in the white t-shirt who has a tattoo on their forearm, carefully placing strips of raw bacon onto a metal baking tray. The bacon is meticulously laid out in rows. The baking tray is then placed into an oven, with overlaid text reading '400°F 10-15 minutes,' providing the cooking instructions for the bacon. The video continues with a close-up of the same person placing the tray into the oven, emphasizing the cooking process. The oven door is open, and the text '400°F 10-15 minutes' remains visible, reinforcing the cooking instructions. The person closes the oven door, marking the end of the cooking preparation.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/youcook2/101/0O4bxhpFX9o/split_0.mp4&quot;}}},{&quot;rowIdx&quot;:95,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;101-vyDO2t9m0H8-split_8&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nShare a detailed breakdown of the video's storyline and visuals.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video showcases the meticulous preparation of a vegan sandwich, emphasizing a healthy and happy lifestyle. It starts with a close-up of a slice of whole grain bread being spread with a creamy vegan substance, followed by the careful arrangement of cherry tomatoes by a hand with pink-painted fingernails. Adjacent to this slice, another piece of bread is topped with fresh green lettuce leaves. In the background, a turquoise plate with additional food items can be seen. The text 'Healthy. Happy. Life. 100% vegan' and 'lunchboxbunch.com' appears in the top right corner, promoting a vegan lifestyle and the website Lunchbox Bunch. The hand then places grilled or cooked tempeh strips on top of the tomatoes, followed by a generous amount of shredded carrots. The video wraps up with the sandwich nearly complete, as the lettuce-topped slice is poised to be placed on top, finalizing the delicious and nutritious vegan creation.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nShare a detailed breakdown of the video's storyline and visuals.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video showcases the meticulous preparation of a vegan sandwich, emphasizing a healthy and happy lifestyle. It starts with a close-up of a slice of whole grain bread being spread with a creamy vegan substance, followed by the careful arrangement of cherry tomatoes by a hand with pink-painted fingernails. Adjacent to this slice, another piece of bread is topped with fresh green lettuce leaves. In the background, a turquoise plate with additional food items can be seen. The text 'Healthy. Happy. Life. 100% vegan' and 'lunchboxbunch.com' appears in the top right corner, promoting a vegan lifestyle and the website Lunchbox Bunch. The hand then places grilled or cooked tempeh strips on top of the tomatoes, followed by a generous amount of shredded carrots. The video wraps up with the sandwich nearly complete, as the lettuce-topped slice is poised to be placed on top, finalizing the delicious and nutritious vegan creation.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/youcook2/101/vyDO2t9m0H8/split_8.mp4&quot;}}},{&quot;rowIdx&quot;:96,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;102-5haTwcEIyE8-split_2&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nCharacterize each scene of the video using a detailed description.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a close-up of a wooden cutting board with sliced onions placed on it, while in the background, a person stirs something in a metal bowl. The scene transitions to a closer view of the person's hand holding a whisk, mixing a white batter in a large metal bowl set on a colorful, floral-patterned tablecloth. The person continues to whisk the batter until it is smooth and well-mixed. The next sequence shows the person dipping an onion ring into the batter, coating it thoroughly. The onion ring is then lifted out of the batter, ready for the next step. A plate with breadcrumbs is visible next to the bowl, indicating that the onion ring will likely be coated with breadcrumbs next.\n\nThe video develops through a close-up of a large metal bowl containing white batter and a bowl filled with breadcrumbs, both placed on the same colorful, floral-patterned tablecloth. The hand dips an onion ring into the breadcrumbs, ensuring it is well-coated, and then lifts the coated onion ring out of the breadcrumbs. The scene shifts to a side view of the person placing the coated onion ring onto a baking sheet lined with parchment paper. The final frame of this sequence shows a person in a light pink shirt with dark hair, standing in a room adorned with decorative plates on the wall and a framed picture in the background. The person appears to be explaining or demonstrating something related to the cooking process.\n\nThe video wraps up with a close-up of a large metal bowl containing white batter, with several onion rings submerged in it. A whisk is also visible in the bowl, indicating that the batter is being mixed. The bowl is placed on the same colorful, floral-patterned tablecloth, consistent with the previous scenes. Next to the bowl, a plate filled with breadcrumbs is visible. The whisk moves slightly, suggesting that the batter is being stirred gently. The onion rings remain in the batter throughout this sequence, ready to be coated with breadcrumbs in the next step of the cooking process.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nCharacterize each scene of the video using a detailed description.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a close-up of a wooden cutting board with sliced onions placed on it, while in the background, a person stirs something in a metal bowl. The scene transitions to a closer view of the person's hand holding a whisk, mixing a white batter in a large metal bowl set on a colorful, floral-patterned tablecloth. The person continues to whisk the batter until it is smooth and well-mixed. The next sequence shows the person dipping an onion ring into the batter, coating it thoroughly. The onion ring is then lifted out of the batter, ready for the next step. A plate with breadcrumbs is visible next to the bowl, indicating that the onion ring will likely be coated with breadcrumbs next.\\n\\nThe video develops through a close-up of a large metal bowl containing white batter and a bowl filled with breadcrumbs, both placed on the same colorful, floral-patterned tablecloth. The hand dips an onion ring into the breadcrumbs, ensuring it is well-coated, and then lifts the coated onion ring out of the breadcrumbs. The scene shifts to a side view of the person placing the coated onion ring onto a baking sheet lined with parchment paper. The final frame of this sequence shows a person in a light pink shirt with dark hair, standing in a room adorned with decorative plates on the wall and a framed picture in the background. The person appears to be explaining or demonstrating something related to the cooking process.\\n\\nThe video wraps up with a close-up of a large metal bowl containing white batter, with several onion rings submerged in it. A whisk is also visible in the bowl, indicating that the batter is being mixed. The bowl is placed on the same colorful, floral-patterned tablecloth, consistent with the previous scenes. Next to the bowl, a plate filled with breadcrumbs is visible. The whisk moves slightly, suggesting that the batter is being stirred gently. The onion rings remain in the batter throughout this sequence, ready to be coated with breadcrumbs in the next step of the cooking process.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/youcook2/102/5haTwcEIyE8/split_2.mp4&quot;}}},{&quot;rowIdx&quot;:97,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;102-Gs3OGfQbPjc-split_4&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite an in-depth depiction of the video, covering all its aspects.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a close-up of a black frying pan on a gas stove. A hand places a small piece of food into the hot oil, causing bubbles to form around it. The stove's control panel, branded 'Winghouse Silhouette,' shows a white knob set to a low heat setting. The scene develops as a larger, ring-shaped piece of food is added to the pan, also starting to fry with bubbles forming around it. Both pieces of food continue to fry in the bubbling oil. The narrative progresses with the same frying pan on the gas stove, where the small and larger ring-shaped pieces of food are still frying. The control panel remains visible, and a hand with black tongs turns the smaller piece to ensure even cooking. The larger ring-shaped piece begins to turn golden brown as it is also turned. The scene wraps up with both pieces of food frying, the oil bubbling around them, and the larger piece showing a more pronounced golden brown color. The final segment shows the larger ring-shaped piece being lifted out of the oil with black tongs, revealing its golden brown color. The control panel is still visible, and the hand places the ring-shaped piece back into the pan. The oil continues to bubble as the hand uses the tongs to turn the other pieces of food for even cooking. The text 'Tell me your thoughts. Leave me a comment :)' appears at the top of the screen, inviting viewers to share their opinions.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite an in-depth depiction of the video, covering all its aspects.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a close-up of a black frying pan on a gas stove. A hand places a small piece of food into the hot oil, causing bubbles to form around it. The stove's control panel, branded 'Winghouse Silhouette,' shows a white knob set to a low heat setting. The scene develops as a larger, ring-shaped piece of food is added to the pan, also starting to fry with bubbles forming around it. Both pieces of food continue to fry in the bubbling oil. The narrative progresses with the same frying pan on the gas stove, where the small and larger ring-shaped pieces of food are still frying. The control panel remains visible, and a hand with black tongs turns the smaller piece to ensure even cooking. The larger ring-shaped piece begins to turn golden brown as it is also turned. The scene wraps up with both pieces of food frying, the oil bubbling around them, and the larger piece showing a more pronounced golden brown color. The final segment shows the larger ring-shaped piece being lifted out of the oil with black tongs, revealing its golden brown color. The control panel is still visible, and the hand places the ring-shaped piece back into the pan. The oil continues to bubble as the hand uses the tongs to turn the other pieces of food for even cooking. The text 'Tell me your thoughts. Leave me a comment :)' appears at the top of the screen, inviting viewers to share their opinions.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/youcook2/102/Gs3OGfQbPjc/split_4.mp4&quot;}}},{&quot;rowIdx&quot;:98,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;102-dmJB7NHZ2cI-split_6&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nProvide a comprehensive analysis of the video's content and themes.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video begins with a close-up of a frying pan filled with oil, where a hand hovers over it, likely checking the temperature. The scene transitions to a plate of freshly fried onion rings, showcasing their golden brown and crispy texture. These onion rings are placed on a white paper towel to absorb any excess oil. The background features a wooden surface, maintaining focus on the onion rings throughout the sequence. The video wraps up with a close-up view of the onion rings, emphasizing their appealing texture and color.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nProvide a comprehensive analysis of the video's content and themes.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video begins with a close-up of a frying pan filled with oil, where a hand hovers over it, likely checking the temperature. The scene transitions to a plate of freshly fried onion rings, showcasing their golden brown and crispy texture. These onion rings are placed on a white paper towel to absorb any excess oil. The background features a wooden surface, maintaining focus on the onion rings throughout the sequence. The video wraps up with a close-up view of the onion rings, emphasizing their appealing texture and color.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/youcook2/102/dmJB7NHZ2cI/split_6.mp4&quot;}}},{&quot;rowIdx&quot;:99,&quot;cells&quot;:{&quot;id&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;103-H_9oM1Y60lU-split_1&quot;},&quot;conversations&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[{&quot;from&quot;:&quot;human&quot;,&quot;value&quot;:&quot;<image>\nWrite a comprehensive depiction of the entire video clip.&quot;},{&quot;from&quot;:&quot;gpt&quot;,&quot;value&quot;:&quot;The video takes place in a vibrant kitchen setting on a television show, featuring a large window with a cityscape view. The kitchen is well-equipped with various cooking utensils and ingredients, creating an inviting atmosphere for a cooking segment. The scene opens with two individuals standing behind a counter filled with cooking items, including a pan on a stove, a plate of buns, various condiments, and a cutting board with green chiles. One person, wearing a white apron with the name 'Harry' embroidered on it, stands next to another person in a maroon shirt who is actively preparing food. The text overlay on the screen reads 'BOBBY'S THROWDOWN' and 'FLAY COOKS GREEN CHILE CHEESEBURGERS,' indicating that the segment is focused on cooking green chile cheeseburgers. \n\nThe person in the maroon shirt is seen chopping green chiles on the cutting board and mixing them in a small bowl, with close-up shots capturing the meticulous preparation. The scene then shifts to a wider shot, showing both individuals as the person in the maroon shirt continues to chop the chiles. Various cooking items, such as a pepper grinder, a bottle of oil, and a jar of honey, are visible on the counter, enhancing the culinary ambiance. The background consistently features the large window with the cityscape view, adding a dynamic backdrop to the cooking process.\n\nAs the segment progresses, the focus remains on the person in the maroon shirt, who is diligently chopping green chiles while the person in the white apron observes. The counter is now shown with additional items, including a pan on the stove with a burger patty cooking, emphasizing the ongoing preparation of the green chile cheeseburgers. The video wraps up with close-up shots of the person in the maroon shirt continuing to chop the chiles, ensuring that all ingredients are perfectly prepared for the cheeseburgers.&quot;}],&quot;string&quot;:&quot;[\n  {\n    \&quot;from\&quot;: \&quot;human\&quot;,\n    \&quot;value\&quot;: \&quot;<image>\\nWrite a comprehensive depiction of the entire video clip.\&quot;\n  },\n  {\n    \&quot;from\&quot;: \&quot;gpt\&quot;,\n    \&quot;value\&quot;: \&quot;The video takes place in a vibrant kitchen setting on a television show, featuring a large window with a cityscape view. The kitchen is well-equipped with various cooking utensils and ingredients, creating an inviting atmosphere for a cooking segment. The scene opens with two individuals standing behind a counter filled with cooking items, including a pan on a stove, a plate of buns, various condiments, and a cutting board with green chiles. One person, wearing a white apron with the name 'Harry' embroidered on it, stands next to another person in a maroon shirt who is actively preparing food. The text overlay on the screen reads 'BOBBY'S THROWDOWN' and 'FLAY COOKS GREEN CHILE CHEESEBURGERS,' indicating that the segment is focused on cooking green chile cheeseburgers. \\n\\nThe person in the maroon shirt is seen chopping green chiles on the cutting board and mixing them in a small bowl, with close-up shots capturing the meticulous preparation. The scene then shifts to a wider shot, showing both individuals as the person in the maroon shirt continues to chop the chiles. Various cooking items, such as a pepper grinder, a bottle of oil, and a jar of honey, are visible on the counter, enhancing the culinary ambiance. The background consistently features the large window with the cityscape view, adding a dynamic backdrop to the cooking process.\\n\\nAs the segment progresses, the focus remains on the person in the maroon shirt, who is diligently chopping green chiles while the person in the white apron observes. The counter is now shown with additional items, including a pan on the stove with a burger patty cooking, emphasizing the ongoing preparation of the green chile cheeseburgers. The video wraps up with close-up shots of the person in the maroon shirt continuing to chop the chiles, ensuring that all ingredients are perfectly prepared for the cheeseburgers.\&quot;\n  }\n]&quot;},&quot;data_source&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;0_30_s_academic_v0_1&quot;},&quot;video&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;academic_source/youcook2/103/H_9oM1Y60lU/split_1.mp4&quot;}}}],&quot;truncated&quot;:true},&quot;paginationData&quot;:{&quot;pageIndex&quot;:0,&quot;numItemsPerPage&quot;:100,&quot;numTotalItems&quot;:11985,&quot;offset&quot;:0,&quot;length&quot;:100}},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkxODY2MCwic3ViIjoiL2RhdGFzZXRzL2xtbXMtbGFiL0xMYVZBLVZpZGVvLTE3OEsiLCJleHAiOjE3NDI5MjIyNjAsImlzcyI6Imh0dHBzOi8vaHVnZ2luZ2ZhY2UuY28ifQ.b80r_ySo2TXwJ6x9XODb4B4lZGa3DX4edigO9KPx9R6XneD3DNiXWYF0n9bN6t7Kl-Nm2eceA95Rq44JfyqADg&quot;,&quot;displayUrls&quot;:true},&quot;dataset&quot;:&quot;lmms-lab/LLaVA-Video-178K&quot;,&quot;isGated&quot;:false,&quot;isPrivate&quot;:false,&quot;hasParquetFormat&quot;:false,&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62d3f7d84b0933c48f3cdd9c/mCBOS7IodEEgBweo19hU9.png&quot;,&quot;fullname&quot;:&quot;LMMs-Lab&quot;,&quot;name&quot;:&quot;lmms-lab&quot;,&quot;type&quot;:&quot;org&quot;,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;isEnterprise&quot;:false,&quot;followerCount&quot;:371},&quot;compact&quot;:true}"><div class="flex flex-col overflow-hidden shadow-xs mx-auto mb-10 rounded-lg border pt-2  px-2.5"><div class="mb-2 flex flex-wrap items-center gap-2"><div class="mr-auto flex items-center"><svg class="mr-1 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	<div class="whitespace-nowrap font-semibold">Dataset Viewer</div>
	</div>
				<a href="/datasets/lmms-lab/LLaVA-Video-178K/tree/refs%2Fconvert%2Fparquet/0_30_s_academic_v0_1" class="group mr-1 text-xs text-gray-400 max-sm:hidden"><svg class="text-[.6rem] mr-1 inline -translate-y-px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill="currentColor" d="M12 10H6.78A11 11 0 0 1 27 16h2A13 13 0 0 0 6 7.68V4H4v8h8zm8 12h5.22A11 11 0 0 1 5 16H3a13 13 0 0 0 23 8.32V28h2v-8h-8z"></path></svg>
						<span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Auto-converted</span> to Parquet
					</a>
				<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>API</button>
					<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path d="M9.80603 2.86737H3.56107C3.37704 2.86737 3.20055 2.94048 3.07042 3.0706C2.94029 3.20073 2.86719 3.37723 2.86719 3.56126V9.80622C2.86719 9.99025 2.94029 10.1667 3.07042 10.2969C3.20055 10.427 3.37704 10.5001 3.56107 10.5001H9.80603C9.99006 10.5001 10.1666 10.427 10.2967 10.2969C10.4268 10.1667 10.4999 9.99025 10.4999 9.80622V3.56126C10.4999 3.37723 10.4268 3.20073 10.2967 3.0706C10.1666 2.94048 9.99006 2.86737 9.80603 2.86737Z" fill="currentColor" fill-opacity="0.3"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M2.40942 1.66191C2.05175 1.66191 1.7618 1.95186 1.7618 2.30953V6.76191H1.43799V2.30953C1.43799 1.77303 1.87291 1.3381 2.40942 1.3381H6.45704V1.66191H2.40942Z" fill="currentColor"></path></svg>Embed</button>

					<button class="bg-linear-to-b shadow-xs flex items-center gap-1.5 rounded-full border from-white to-red-100/90 px-2 py-0.5 text-xs font-medium text-[#2D3648] transition-shadow hover:shadow-inner dark:from-gray-900 dark:to-red-800/30 dark:text-gray-100 dark:hover:shadow-inner dark:hover:shadow-red-800/30" ><svg class="h-3.5 w-3.5 text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
						<span>Data Studio</span></button></div>
		<div class="flex flex-1 flex-col overflow-hidden -mx-2.5"><div class="flex flex-1 flex-col overflow-hidden"><div class="flex min-h-0 flex-1"><div class="flex flex-1 flex-col overflow-hidden"><div class="md:-mx-2.5 flex min-w-0 flex-wrap border-t"><div class="flex min-w-0 flex-1 flex-wrap"><div class="grid flex-1 grid-cols-1 overflow-hidden text-sm md:grid-cols-2 md:place-content-center sm:mx-2.5"><label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-950 md:border-r max-md:first:border-b md:border-r" title="0_30_s_academic_v0_1"><span class="text-gray-500">Subset (19)</span>
			<div class="flex items-center whitespace-nowrap"><span class="truncate">0_30_s_academic_v0_1</span>
				<span class="mx-2 text-gray-500">·</span>
					<span class="text-gray-500">66.2k rows</span>
				<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
			<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Subset (19)"><option value="0_30_s_academic_v0_1" selected>0_30_s_academic_v0_1 (66.2k rows)</option><option value="0_30_s_activitynet" >0_30_s_activitynet</option><option value="0_30_s_nextqa" >0_30_s_nextqa (11k rows)</option><option value="0_30_s_perceptiontest" >0_30_s_perceptiontest (1.79k rows)</option><option value="0_30_s_youtube_v0_1" >0_30_s_youtube_v0_1 (539k rows)</option><option value="1_2_m_academic_v0_1" >1_2_m_academic_v0_1 (35.2k rows)</option><option value="1_2_m_activitynet" >1_2_m_activitynet</option><option value="1_2_m_nextqa" >1_2_m_nextqa (9.33k rows)</option><option value="1_2_m_youtube_v0_1" >1_2_m_youtube_v0_1 (202k rows)</option><option value="2_3_m_academic_v0_1" >2_3_m_academic_v0_1 (28.2k rows)</option><option value="2_3_m_activitynet" >2_3_m_activitynet</option><option value="2_3_m_nextqa" >2_3_m_nextqa (113 rows)</option><option value="2_3_m_youtube_v0_1" >2_3_m_youtube_v0_1 (206k rows)</option><option value="30_60_s_academic_v0_1" >30_60_s_academic_v0_1 (88.8k rows)</option><option value="30_60_s_activitynet" >30_60_s_activitynet</option><option value="30_60_s_nextqa" >30_60_s_nextqa (13.7k rows)</option><option value="30_60_s_perceptiontest" >30_60_s_perceptiontest (618 rows)</option><option value="30_60_s_youtube_v0_1" >30_60_s_youtube_v0_1 (171k rows)</option><option value="llava_hound" >llava_hound (255k rows)</option></optgroup></select></label>
		<label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-900 md:border-r md:border-r-0" title="caption"><div class="text-gray-500">Split (3)</div>
				<div class="flex items-center overflow-hidden whitespace-nowrap"><span class="truncate">caption</span>
					<span class="mx-2 text-gray-500">·</span>
						<span class="text-gray-500">12k rows</span>
					<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
				<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Split (3)"><option value="caption" selected>caption (12k rows)</option><option value="open_ended" >open_ended (48.5k rows)</option><option value="multi_choice" >multi_choice (5.75k rows)</option></optgroup></select></label></div></div>
								</div>

							<div class="flex min-h-0 flex-1 flex-col ">
	<div class="bg-linear-to-r text-smd relative flex items-center dark:border-gray-900 dark:bg-gray-950 false border-t [&amp;:has(:focus)]:from-gray-50 [&amp;:has(:focus)]:to-transparent [&amp;:has(:focus)]:to-20% dark:[&amp;:has(:focus)]:from-gray-900"><form class="flex-1"><svg class="absolute left-3 top-1/2 transform -translate-y-1/2 pointer-events-none text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				<input disabled class="outline-hidden h-9 w-full border-none bg-transparent px-1 pl-9 pr-3 placeholder:text-gray-400 " placeholder="Search this dataset" dir="auto"></form>
			<div class="flex items-center gap-2 px-2 py-1"><button type="button" class="hover:bg-yellow-200/70 flex items-center gap-1 rounded-md border border-yellow-200 bg-yellow-100 pl-0.5 pr-1 text-[.8rem] leading-normal text-gray-700 dark:border-orange-500/25 dark:bg-orange-500/20 dark:text-gray-300 dark:hover:brightness-110 hidden"><div class="rounded-sm bg-yellow-300 px-1 font-mono text-[.7rem] font-bold text-black dark:bg-yellow-700 dark:text-gray-200">SQL
	</div>
	Console
</button></div></div>


<div class="flex flex-1 flex-col overflow-hidden min-h-64 border-t">
		

<div class="max-h-96 relative overflow-auto"><table class="w-full table-auto rounded-lg font-mono text-xs text-gray-900"><thead class="shadow-xs sticky left-0 right-0 top-0 z-1 bg-white align-top"><tr class="space-y-54 h-full min-w-fit divide-x border-b text-left"><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">id
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="9.663481228668942" width="14.5" height="20.336518771331058" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="16.5" y="26" width="14.5" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="33" y="15.95938566552901" width="14.5" height="14.04061433447099" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="49.5" y="26" width="14.5" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="0" width="14.5" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="82.5" y="25" width="14.5" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="99" y="26" width="14.5" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="115.5" y="25" width="14.5" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="16.5" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="15.5" y="0" width="16.5" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="32" y="0" width="16.5" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="48.5" y="0" width="16.5" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="16.5" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="81.5" y="0" width="16.5" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="98" y="0" width="16.5" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="114.5" y="0" width="16.5" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">5</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">36</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">conversations
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>list</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="130" height="30" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="132" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">2</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">2</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">data_source
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">classes</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><defs><clipPath id="rounded-bar"><rect x="0" y="0" width="130" height="8" rx="4"></rect></clipPath><pattern id="hatching" patternUnits="userSpaceOnUse" patternTransform="rotate(-45)" height="1" width="5"><line y1="0" class="stroke-gray-400 dark:stroke-gray-500/80" stroke-width="3" y2="1" x1="2" x2="2"></line></pattern><pattern id="hatching-faded" patternUnits="userSpaceOnUse" patternTransform="rotate(-45)" height="1" width="5"><line y1="0" class="stroke-gray-100 dark:stroke-gray-500/20" stroke-width="3" y2="1" x1="2" x2="2"></line></pattern></defs><g height="8" style="transform: translateY(20px)" clip-path="url(#rounded-bar)"><g style="transform: scaleX(1.0153846153846153) translateX(-1px)"><g><rect class="fill-indigo-500 dark:fill-indigo-600/80" x="1" y="0" width="128" height="8" fill-opacity="1"></rect></g></g></g><g style="transform: scaleX(1.0153846153846153) translateX(-1px)"><g><rect class="fill-white cursor-pointer" x="0" y="0" width="130" height="28" fill-opacity="0"></rect></g></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 max-w-full overflow-hidden text-ellipsis whitespace-nowrap">1
				value</div></div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">video
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-['·']">lengths</span></div></div>

		<div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="9.663481228668942" width="11.2" height="20.336518771331058" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="13.2" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="26.4" y="20.30307167235495" width="11.2" height="9.69692832764505" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="21.65631399317406" width="11.2" height="8.343686006825939" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="52.8" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="66" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="79.19999999999999" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="92.39999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg>
	<div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 60px">34</div>
			<div class="absolute overflow-hidden text-ellipsis whitespace-nowrap" style="right: 0px; max-width: 60px">62</div>
			</div></div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th></tr></thead>
			<tbody class="h-16 overflow-scroll"><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="0"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">028CE</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nAnalyze the video, focusing on its cinematography and narrative structure."
  },
  {
    "from": "gpt",
    "value": "The video begins with a view of a garage or basement, characterized by a white door in the background. On the floor, there is a vacuum cleaner and a small box with a decorative pattern. A person dressed in black clothing and black shoes enters the frame and approaches the vacuum cleaner. The person squats down and starts to interact with the vacuum cleaner, adjusting or inspecting it. The scene remains focused on this interaction. The narrative continues in the same setting, with the white door still visible in the background. The person, still dressed in black, is initially squatting and interacting with the vacuum cleaner. They then stand up and step onto the small decorative box. After a brief moment standing on the box, the person steps off and exits the frame, leaving the vacuum cleaner and the box in their original positions. The video concludes with the garage or basement empty, showcasing the white door, the vacuum cleaner, and the small box on the floor."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/028CE.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="1"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">14YU9</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nElaborate on the visual and narrative elements of the video in detail."
  },
  {
    "from": "gpt",
    "value": "The video features a person sitting on a bench in a room with yellow walls. The individual is dressed in a gray t-shirt with the text 'ONE GOOD THING ABOUT MUSIC WHEN IT HITS YOU FEEL NO PAIN' printed on it, beige shorts, and a black cap. The t-shirt's text suggests a theme of the therapeutic power of music. The person engages in various activities, such as looking towards an open door that leads to a dark room, and interacting with objects on a wooden cabinet next to them. The cabinet holds several items, including a blue spray bottle, a set of weights, and a few other small objects. The person picks up and puts down items, including a small white object and a black rectangular object, which could be a phone or a remote control. Throughout the video, the person adjusts their position and looks around the room, displaying a sense of alertness or curiosity."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/14YU9.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="2"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1KJI0</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nShare a detailed breakdown of the video's storyline and visuals."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a cozy kitchen with light-colored walls adorned with a subtle pattern. It begins with a person wearing a black hoodie and gray pants cooking at a stove that has multiple pots and pans on it. The person uses a spatula to stir or flip food in a pan, with a blue stool positioned nearby and a wooden door visible in the background. After some time, the person steps away from the stove and walks towards the door.\n\nNext, the person walks towards a white refrigerator in the same kitchen. They open the refrigerator door and bend down to look inside, eventually retrieving a blue container. Standing up, they examine the container, possibly preparing to use it. The background remains consistent with the light-colored walls, blue stool near the stove, and wooden door in the background. The stove with multiple pots and pans is still present.\n\nIn the final part of the video, the person stands near the refrigerator, holding the blue container. They open the container and pour or transfer its contents into another container or dish. The blue stool remains near the stove, and the wooden door is still visible in the background. After pouring the contents, the person walks towards the stove, places the container on the counter, and then returns to the refrigerator, closing its door. They then stand near the refrigerator, holding the blue container, and look around, possibly contemplating the next step in their cooking process."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/1KJI0.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="3"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">41SB2</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nOffer a detailed interpretation of the video's message and imagery."
  },
  {
    "from": "gpt",
    "value": "The video begins with a view of a bathroom, focusing on a mirror with a light fixture above it. The light fixture has four bulbs, all of which are illuminated. To the left of the mirror, there is a set of plastic drawers with various items on top. A person, partially visible and wearing dark clothing, is seen moving in the reflection of the mirror. The person steps closer to the light fixture and begins to adjust or replace one of the light bulbs, ensuring they are properly secured. After completing the task with the light fixture, the person steps back.\n\nThe scene then shifts to a different angle, showing a wooden cabinet with a countertop cluttered with various items, including bottles and containers. The same person, still partially visible and wearing dark clothing, enters the frame holding a broom. They begin to sweep the floor, moving the broom back and forth, focusing on cleaning the area near the cabinet.\n\nThe video continues with the person sweeping the floor, maintaining their focus on cleaning the area near the cabinet. The bathroom setting remains consistent, with the illuminated light fixture above the mirror and the cluttered countertop in view. The person diligently sweeps the floor, ensuring the area is clean and tidy."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/41SB2.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="4"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">422BV</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDescribe every scene and its significance in the video."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a small room with a wooden door and a window covered by yellow curtains with floral patterns. A person wearing a green hoodie and blue jeans is seen interacting with a cluttered desk that has a black backpack, books, papers, and a box on it. Initially, the person stands near the desk, then bends down to pick up a pair of blue shoes from the floor. They proceed to put on the shoes, adjusting them several times to ensure they fit properly. The room also contains a few cricket bats leaning against the wall and a marble-patterned floor. After putting on the shoes, the person bends down again, possibly adjusting them, and then stands up to begin organizing or searching through the items on the desk. The room's details, including the cricket bats and the marble-patterned floor, remain consistent throughout the video."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/422BV.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="5"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">4CSXJ</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nOffer a detailed interpretation of the video's message and imagery."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person sitting on a narrow staircase, leaning against the wall. They are dressed in a teal robe and clutching a white pillow to their chest, conveying a sense of contemplation or distress. The staircase, with its beige walls and handrail on the left side, leads down to a small table that holds a framed picture, a small decorative item, and a glass of water. A pair of dark plaid pants lies next to the person on the stairs. Soft lighting from a window at the top of the stairs creates a calm yet somber atmosphere. The person occasionally adjusts their position and the pillow, indicating unease or discomfort.\n\nThe narrative develops as the person continues to sit on the staircase, still leaning against the wall in their teal robe. They interact with the dark plaid pants, picking them up, examining them, and then setting them aside. The person then reaches for a white cloth or garment further down the stairs, inspects it, and holds it close to their chest, much like they did with the pillow. The small table at the bottom of the stairs remains unchanged, maintaining the calm yet somber atmosphere with the soft lighting from the window. The person's careful handling of the items suggests a continued state of contemplation or distress, with a sense of attachment to the objects.\n\nThe video wraps up with the person still sitting on the staircase, now holding the white cloth close to their chest. The staircase, with its beige walls and handrail, remains unchanged, and the small table at the bottom still holds the framed picture, small decorative item, and glass of water. The soft lighting from the window continues to create a calm yet somber atmosphere. The person appears deep in thought or distress, occasionally looking towards the window, indicating a sense of longing or reflection. The dark plaid pants are no longer visible, suggesting they have been moved or set aside, further emphasizing the person's introspective state."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/4CSXJ.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="6"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">4EM25</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nNarrate the video's unfolding events in a descriptive manner."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a narrow hallway with white walls and a patterned rug on the floor. A person dressed in black clothing is standing in the hallway, holding a phone. A white dog with black spots is present, moving around the person's legs. The dog appears to be excited, wagging its tail and looking up at the person. The person occasionally looks down at the dog and makes gestures with their free hand. Towards the end of this segment, the person bends down to interact with the dog, possibly to pet it or pick something up from the floor. The scene continues in the same hallway. The person, still holding the phone, raises their free hand, possibly to gesture or interact with the dog. The dog remains attentive, looking up at the person and wagging its tail. The person then starts walking towards the camera, with the dog following closely. As the person approaches the camera, they continue to hold the phone and occasionally look at it. The video concludes with the person walking past the camera, leaving the hallway empty."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/4EM25.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="7"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">4J1AP</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nCharacterize each scene of the video using a detailed description."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person sitting at a table in a cozy kitchen, wearing a dark shawl and engaged in eating or drinking. On the table, there is a glass of orange juice and a bowl, from which the person is eating using a spoon. The kitchen exudes a homey feel with a light-colored tablecloth, wooden chairs, and a cabinet with glass doors displaying various items. The background includes a window and shelves with jars and other kitchen items. The person alternates between looking at the bowl and taking bites, indicating they are having a meal. The scene develops as the person continues to sit at the table, still wearing the dark shawl. They are seen holding the bowl and eating, then placing the bowl down to pick up the glass of orange juice and take a sip. The background remains consistent, featuring the same light-colored tablecloth, wooden chairs, cabinet with glass doors, window, and shelves with jars and other kitchen items. The person alternates between eating from the bowl and drinking the orange juice, continuing their meal. The video wraps up with the person still sitting at the table, holding the glass of orange juice with both hands and taking a sip. The table still has the light-colored tablecloth, and the background remains unchanged, featuring wooden chairs, a cabinet with glass doors displaying various items, a window, and shelves with jars and other kitchen items. The person is focused on drinking the orange juice, indicating they are continuing their meal."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/4J1AP.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="8"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">5U92L</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDetail the video's plot development, character arcs, and thematic elements."
  },
  {
    "from": "gpt",
    "value": "The video begins with a dimly lit living room, featuring a door with blinds and a window to the left, also with blinds. The ceiling fan and light fixture are visible at the top, and various items, including a pair of shoes and a green object, are scattered on the floor. A person wearing a gray t-shirt and dark shorts enters the frame from the right, walking towards the door. They stop, look around, and then begin to remove their t-shirt, revealing a bare torso. After stretching their arms upwards, they proceed to remove their shorts, standing in the room in their underwear. The person stands still, having completed the action of undressing.\n\nThe scene continues in the same living room setting. The person, now shirtless and wearing dark shorts, stands near the door with blinds. The room remains dimly lit, with the ceiling fan and light fixture still visible. Various items, including a pair of shoes and a green object, are scattered on the floor. The person appears to be engaged in an activity involving a small object, which they pick up from the floor and examine closely. They then stand still, holding the object in their hands, and look down at it, seemingly contemplating it.\n\nThe video wraps up with the same living room setting, featuring the door with blinds and the window to the left, also with blinds. The room remains dimly lit, with the ceiling fan and light fixture visible at the top. Various items, including a pair of shoes and a green object, are scattered on the floor. The person, still shirtless and wearing dark shorts, stands near the door. They move around the room, stepping over the scattered items on the floor, and then walk out of the frame to the left, leaving the room empty. The final scene shows the empty living room, with the door and window with blinds, the ceiling fan, and the scattered items on the floor."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/5U92L.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="9"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">759MY</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nExplain the video's components, including its characters, setting, and plot."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person lying on a bed in a room. The bed features a black and white patterned bedspread and a white cushioned headboard adorned with black and white decorative elements. The person is dressed in a dark hoodie and gray pants. To the left of the bed, there is a desk cluttered with books, a lamp, and various other items. A patterned rug is placed on the floor near the bed. The person starts to move, shifting their position on the bed, and eventually sits up, reaching towards the desk as if to grab something. The scene transitions to the person sitting cross-legged on the bed, still reaching towards the desk. The dark hoodie now displays yellow text. The person shifts their position again, moving from sitting cross-legged to sitting on the edge of the bed. They then put on a pair of orange slippers and stand up. The person walks towards the desk, appearing to look at or reach for something on it. The video concludes with the person standing near the desk, facing it."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/759MY.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="10"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">8UGNK</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide a comprehensive analysis of the video's content and themes."
  },
  {
    "from": "gpt",
    "value": "The video begins with an empty kitchen featuring wooden walls and ceiling, a green stove, a wooden table with a vase of red flowers, a wooden chair, and a trash can. Natural light streams through a window, illuminating the room. A person wearing a green hoodie and blue plaid pants enters from the left, walks towards the window, and looks outside. They then turn around, walk towards the table, and pick up a cup, taking a sip while standing contemplatively near the window. The scene transitions to the same person standing near the window, still holding the cup. Another individual, dressed in a camouflage jacket and floral pants, enters from the left, carrying a gray bag. This person approaches the one near the window and hands them the gray bag. They stand together, with the person in the green hoodie sipping from the cup as the other talks. The person in the camouflage jacket then walks towards the camera, leaving the person in the green hoodie by the window, still drinking from the cup. The final scene shows the person in the green hoodie standing near the window, deep in thought and occasionally sipping from the cup. They then walk away from the window towards the right side of the kitchen, stopping near the stove. The video concludes with the person standing near the stove, seemingly lost in contemplation while holding the cup."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/8UGNK.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="11"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">914NC</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWalk through the video, detailing its key moments and features."
  },
  {
    "from": "gpt",
    "value": "The video begins with a view of a staircase inside a house, featuring white steps, a wooden handrail, and white balusters. A blue bucket is placed on the floor near the base of the stairs. A person wearing a blue jacket and jeans, carrying a large blue bag, enters the frame from the left and starts ascending the stairs. They are followed closely by another person wearing a blue jacket and colorful pants. The first person holds a small white object in their hand as they climb. Both individuals continue to ascend the staircase, with the second person staying close behind the first. The scene develops as the two individuals, maintaining their positions, continue their ascent. The first person still carries the large blue bag and the small white object, while the second person follows closely. As they reach the top of the staircase, they begin to move out of the frame. The video wraps up with both individuals nearing the top of the stairs, with the first person slightly ahead of the second."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/914NC.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="12"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">95M38</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nConvey the narrative and visual elements of the video in detail."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a cozy living room featuring a large bookshelf, a lamp, and a couch. The room is well-lit by natural light streaming through the glass doors in the background, creating a warm and inviting atmosphere. A person with long, curly hair, dressed in a brown shirt and dark pants, is seated on a cushioned ottoman. They are holding a small object, possibly a toy or a snack, and appear to be deeply engaged with it. Throughout the video, the person occasionally looks around the room and adjusts their position slightly, maintaining a calm and relaxed demeanor. The setting and mood of the room remain consistent, with the person showing a sense of engagement and focus as they move their hands and look down at the object they are holding. The overall atmosphere is one of tranquility and quiet concentration."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/95M38.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="13"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">9Z1A3</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDissect the video's content, explaining each element thoroughly."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a small kitchen area, beginning with a person standing near a table and shelves filled with various food items and kitchen supplies. The person, dressed in a dark jacket and a purple cap, is in the process of putting on the jacket, swinging it around their shoulders and adjusting it. They then start performing a series of dance-like movements, lifting their legs and moving their arms rhythmically. The background features a white refrigerator adorned with magnets and notes, a table with chairs, and a window with closed blinds. Various items such as cereal boxes, condiments, and kitchen utensils are visible on the shelves and table. The scene continues with the person, still wearing the dark jacket and purple cap, standing near the table and shelves, continuing their dance-like movements. They pick up a box from the table and move towards a door, still dancing. They approach a small table with a television on it, place the box down, and bend over to interact with something on the lower shelf of the table. The television is on, displaying various scenes. The person picks up a box from the lower shelf and examines it. The background includes a door, a table with a can on it, and a dining area with shelves filled with various food items and kitchen supplies. The person then stands up and turns towards the door, appearing to be ready to leave the area."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/9Z1A3.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="14"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">A3AV0</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nIllustrate the video's narrative journey, scene by scene, with attention to detail."
  },
  {
    "from": "gpt",
    "value": "The video begins with two individuals sitting on the floor in a narrow hallway near a red door. One person is dressed in a green long-sleeve shirt and a pink apron, while the other wears a red hoodie and blue patterned pants. The person in the green shirt and pink apron is holding a small object and appears to be working on it with focused attention. The person in the red hoodie sits across from them, observing or waiting. The setting is a small, enclosed space with a wooden floor and a white door frame visible, creating an intimate atmosphere.\n\nThe scene continues with the same two individuals in the hallway. The person in the green long-sleeve shirt and pink apron remains engrossed in their task, concentrating on the small object in their hands. The person in the red hoodie and blue patterned pants continues to sit across from them, watching the activity. The setting remains unchanged, with the wooden floor and white door frame still visible. The interaction between the two is captured, with the person in the green shirt occasionally glancing down at their hands and the object. At one point, the person in the red hoodie raises their arm, possibly gesturing or reaching for something, adding a subtle dynamic to the quiet, focused atmosphere.\n\nThe video wraps up with the two individuals still in the narrow hallway near the red door. The person in the red hoodie and blue patterned pants begins to move, shifting their position and eventually standing up. Meanwhile, the person in the green long-sleeve shirt and pink apron remains seated, still focused on the small object in their hands. As the person in the red hoodie stands, they turn towards the door and appear to look outside through the window. The sequence captures this movement, with the person in the red hoodie standing and looking out the door, while the person in the green shirt continues their activity on the floor. The setting remains consistent, with the wooden floor and white door frame visible, maintaining the overall atmosphere of quiet focus and engagement in the small, enclosed space."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/A3AV0.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="15"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">AF8VD</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite an in-depth depiction of the video, covering all its aspects."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a cozy living room with a white tiled floor and a staircase in the background. The room is furnished with a blue bed covered with a pink and white blanket, a small table with a cup, and a television set on a stand. The television is on, displaying a scene with a person. A person, wrapped in a pink and white blanket and wearing a white sweater, is seen sitting on the bed. They adjust the blanket around them, occasionally looking towards the television. Various household items, including a blue bucket, a small stool, and other objects near the staircase, add to the domestic atmosphere. The person appears to be relaxing and watching television.\n\nThe scene continues with the person still seated on the blue bed, wrapped in the blanket. They adjust the blanket and their position while occasionally glancing at the television. The small table with a cup remains present, and the television continues to display a scene with a person. The person takes a sip from the cup, indicating they are drinking something. The calm and domestic atmosphere persists as the person relaxes and watches television.\n\nIn the final part of the video, the person remains seated on the blue bed, wrapped in the pink and white blanket. They are seen holding an orange cup and taking a sip from it. The small table with a cup is still present, and the television continues to display a scene with a person. The person stretches out their legs and continues to relax while watching television. The blue bucket, small stool, and other household items near the staircase remain visible. The overall atmosphere remains calm and domestic, with the person continuing to enjoy their time watching television."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/AF8VD.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="16"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">AX4EW</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite a comprehensive depiction of the entire video clip."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a cozy room featuring a round wooden table and a distinctive blue-arched doorway in the background. On the table, there is a laptop, a newspaper, a cup, and a water bottle, indicating a casual, everyday setting. A man in a striped shirt stands and holds a camera, seemingly capturing a photo or video of another man who is seated at the table and focused on his meal. The seated man, dressed in a white shirt with black stripes, is engrossed in eating. The room also includes a water dispenser on the right side and a framed picture on the wall, adding to the domestic atmosphere. The scene continues with the man in the striped shirt still standing and holding the camera, capturing the moment of the man at the table. The man at the table, still wearing the white shirt with black stripes, is now seen wiping his mouth with a napkin. The table's items remain unchanged, and the water dispenser and framed picture are still visible in the background. The video encapsulates a simple, everyday moment in a domestic setting, highlighting the interaction between the two men, one documenting the other's mealtime."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/AX4EW.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="17"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">D1UWI</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nExplain the video's components, including its characters, setting, and plot."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a small kitchen with beige walls and wooden cabinets, creating a cozy and warm atmosphere. A person dressed in a gray hoodie, orange shirt, dark pants, and red socks is the central figure throughout the video. The scene begins with the person standing near a white refrigerator and stove, focused on cooking or preparing something. The person is seen holding a glass or cup and gesturing towards the camera, indicating they might be explaining or sharing something. The person then moves to the middle of the kitchen, interacting with the counter and possibly reaching for an ingredient or utensil. The background features a microwave on the counter and a dishwasher below, with warm lighting from a ceiling light and a light above the stove enhancing the homely feel.\n\nAs the video progresses, the person continues their cooking activity, remaining near the stove. They are seen looking down at the stove, possibly stirring or checking on the food, and occasionally turning towards the camera while holding a utensil. The person's actions suggest they are deeply engaged in the cooking process, maintaining a consistent focus on the stove. The background elements, including the microwave and dishwasher, remain unchanged, and the warm lighting continues to illuminate the scene.\n\nThe video wraps up with the person still engrossed in their cooking task, maintaining their position near the stove. The repetitive actions of looking down at the stove, stirring, and occasionally turning towards the camera emphasize the person's dedication to their culinary activity. The consistent background and warm lighting throughout the video create a sense of continuity and comfort, making the viewer feel as though they are part of a familiar and inviting kitchen environment."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/D1UWI.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="18"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">D2VU8</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide a comprehensive analysis of the video's content and themes."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person sitting on a beige couch in a living room, dressed in a gray tank top and gray sweatpants. They are using a laptop placed on a wooden coffee table, which is cluttered with various items, including a bowl of fruit, a smartphone, and a glass bowl. Behind the person, a colorful blanket is draped over the couch, and the wall features a large framed picture along with some smaller framed items. The person then stands up, holding a smartphone, and walks towards a side table with a lamp and some books. They pick up a book from the side table and return to the couch. After moving the blanket, they sit back down and appear to get comfortable, possibly preparing to read the book. The scene transitions to the person lying on the beige couch, now covered with the colorful blanket and holding the book. They seem to be engrossed in reading, occasionally adjusting the blanket for comfort. The coffee table in front of the couch remains cluttered with the same items. The person continues to read, sometimes looking up and around the room. The living room setting remains consistent with the same framed pictures on the wall and the side table with a lamp and books. The video wraps up with the person sitting up, still holding the book, and adjusting the blanket."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/D2VU8.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="19"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">DPB7X</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite a comprehensive depiction of the entire video clip."
  },
  {
    "from": "gpt",
    "value": "The video features a serene domestic scene set in a well-lit room with a large window on the right side. A person, dressed in a light-colored shirt, is seated at a table and is engaged in an activity involving a green bag and various items spread out on the table. The background includes a light blue cabinet with glass doors, showcasing an array of decorative items such as a red teapot and some pottery. The person appears to be unpacking or organizing the items from the green bag, occasionally glancing out the window. The natural light streaming in from the window bathes the room in a warm, inviting glow, enhancing the calm and peaceful atmosphere of the scene."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/DPB7X.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="20"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">E6Q95</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nOffer a detailed interpretation of the video's message and imagery."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a cozy, rustic room with wooden walls and a wooden floor, furnished with a green armchair, a couch covered with blankets, and a small table with various items on it. A television sits on a wooden stand against the wall, and a door with a diamond-shaped window adds to the room's charm. Initially, a person wearing a camouflage jacket and plaid pants stands near the television. They move around the room, eventually walking towards the couch. After sitting down, they remove their jacket to reveal a gray shirt underneath, adjust the blankets, and appear to get comfortable, possibly preparing to rest or relax. The scene continues with the person, now in a gray shirt and plaid pants, seated on the couch and holding a blue and black patterned onesie. They lift the onesie and put it on, standing briefly to pull it over their head. Once dressed in the onesie, they sit back down and adjust the blankets around them, seemingly settling in for relaxation, perhaps to watch television or rest. The final part of the video shows the person, now in the blue and black patterned onesie, seated on the couch with blankets. They are holding a tablet or similar device, appearing engaged with it, possibly watching something or reading. The room's furnishings remain unchanged, and the television is on, displaying a scene. A box labeled 'MAGIC' is on the floor next to the television stand. The person seems comfortably settled in, continuing their relaxation."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/E6Q95.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="21"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">ELFEW</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite a comprehensive depiction of the entire video clip."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person standing in front of an open closet or storage area, wearing a long-sleeved gray shirt. They hold a tool, possibly a wrench or pliers, in their right hand and a long object, perhaps a pipe or rod, in their left hand. The storage area contains a water heater, a white bucket on the floor, and some papers and a sign posted on the wall. The person appears to be inspecting or working on something near the water heater. As the video progresses, the person continues their inspection or work, focusing intently on the area near the water heater. They occasionally look at the tool in their right hand, suggesting they are making adjustments or repairs. The scene remains consistent with the storage area containing the same items. Towards the end of the video, the person turns slightly towards the storage area, possibly to get a closer look or to adjust their position for better access. Finally, the person faces the storage area more directly, with their back towards the camera, continuing their work near the water heater. The lighting in the scene casts a warm, orange hue on the wooden surfaces around the storage area, adding a cozy ambiance to the setting."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/ELFEW.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="22"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">FB3ZG</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nConvey the narrative and visual elements of the video in detail."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person wearing a blue hoodie with white text and light-colored pants walking into a room characterized by light blue walls, a wooden door, and a bookshelf filled with books and other items. Several framed pictures adorn the walls, adding a personal touch to the space. The person briefly exits the frame, leaving the room empty. After a few moments, they return, approach the camera, and appear to adjust something. They then show a white mug to the camera before walking back towards the door, holding the mug and another object. The person exits the room through the door, leaving it empty once again. The video continues with the same room, now empty, maintaining its serene and unchanged setting. The light blue walls, wooden door, and bookshelf filled with books and other items remain in view, along with the framed pictures on the wall. The room stays empty for the remainder of the video, with no visible movement or changes, concluding with the same tranquil atmosphere."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/FB3ZG.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="23"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">FEKJQ</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide an exhaustive description of the video content."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a well-lit kitchen with light-colored cabinets, a black countertop, and various utensils, including a sink and a stove. The setting is enhanced by a tiled wall with decorative patterns, a shelf with kitchen items, and a window that allows natural light to flood the room. A man wearing a white sleeveless shirt and dark pants is the central figure throughout the video. Initially, he is seen adjusting a mirror placed on the countertop and proceeds to groom himself, possibly shaving or trimming his facial hair. The scene captures the intimate details of his grooming routine in this domestic kitchen setting. As the video progresses, the man continues his grooming routine, finishing up and then adjusting the mirror. He begins to clean the area, meticulously wiping the countertop, maintaining the cleanliness of the space. The background remains consistent, reinforcing the homely atmosphere. In the final part of the video, the man bends down to pick up a black laptop from the floor. He stands up, holding and inspecting the laptop, suggesting he might be preparing to use it or move it to another location. The video encapsulates a slice of everyday life, focusing on the man's personal grooming and subsequent activities in his kitchen."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/FEKJQ.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="24"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">FLDHS</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nCharacterize each scene of the video using a detailed description."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person in a black hoodie and blue jeans standing in a room with a wooden door and a window covered by yellow floral curtains. The person is facing away from the camera and appears to be organizing or searching through items on a small table or shelf next to the door. The room has a tiled floor, and there are various items scattered around, including a green bag, a cricket bat, and some boxes. The person bends down to pick up or move items, including a pink box and a purple container. They continue to rearrange and handle different objects, occasionally bending down to pick up items from the floor or lower shelves. The scene develops as the person methodically organizes the area, moving items from one place to another.\n\nThe narrative progresses with the person still in the same room, now seen bending down and picking up a green bag from the floor. They lift the bag and begin to shake it out, possibly to empty its contents or to straighten it. The person then folds the green bag and places it on the table or shelf next to the door. The room remains the same with the tiled floor and various items scattered around, including a cricket bat and some boxes. The person continues to focus on organizing the area, now specifically dealing with the green bag.\n\nThe video wraps up with the person still in the same room, standing at the table or shelf next to the door, organizing items. They handle the green bag, placing it on the table, and then proceed to sort through various objects on the table, including a purple container and some books. The room remains unchanged with the tiled floor and various items scattered around, including a cricket bat and some boxes. The person continues to methodically organize the area, moving items from one place to another, focusing on the items on the table."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/FLDHS.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="25"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">FYR3P</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nOffer a detailed interpretation of the video's message and imagery."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a bedroom featuring a bed on the left side, covered with a black and white patterned bedspread and some white clothes on it. Behind the bed, there is a window with beige curtains, and on the right side, a wardrobe with a mirror. A person dressed in a yellow kurta with white pants and black shoes is initially seen standing. They begin to remove their yellow vest, turning around to face the wardrobe as they do so. The person successfully removes the vest and holds it in their hands. The scene continues with the same person standing near the bed. They bend down to pick up the white clothes from the bed and then walk towards the wardrobe with a mirror. The video concludes with the person standing in front of the wardrobe, holding the white clothes in their hands."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/FYR3P.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="26"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">GC1Q1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nNarrate the video's unfolding events in a descriptive manner."
  },
  {
    "from": "gpt",
    "value": "The video begins with a view of a partially open brown door in a hallway with light-colored walls and a tiled floor. A person wearing a red and white striped shirt and blue jeans opens the door wider and steps inside, carrying a stack of folded clothes in various colors. They walk further into the room, which contains a wooden chair and shelves filled with household items. The person places the stack of clothes on the chair and adjusts them, appearing to organize or sort through the items. The scene develops as the person continues to handle the clothes near the chair. The narrative progresses with the person in the red and white striped shirt and blue jeans standing near the wooden chair, which now has some clothes placed on it. They then bend down and sit on the chair, adjusting their position to get comfortable. Leaning back, they cross one leg over the other and rest their arms on the armrests of the chair. The background shows shelves filled with various household items, and the brown door is now closed. The video wraps up with the person appearing to relax or take a break, remaining seated in a relaxed posture for the remainder of the scene."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/GC1Q1.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="27"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">GJC1G</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide an exhaustive description of the video content."
  },
  {
    "from": "gpt",
    "value": "The video begins with a view of a small room with a brown tiled floor and white walls. In the corner, there is a yellow container with a red lid and a beige pet carrier. A small red box is on the floor near the door. A person wearing a gray hoodie and a dark jacket enters the room from the left side. They walk towards the camera, then start to remove their jacket, revealing the gray hoodie underneath. Holding the jacket in one hand, they turn towards the door on the right side of the room and reach for the door handle. The scene transitions to the person in the gray hoodie standing near the door, still holding the dark jacket. They bend down to pick up a brown paper bag from the floor. From the bag, they retrieve a small yellow container and examine it closely before placing it back into the bag. Next, they take out a red package and inspect it intently. The final scene shows the person bending down near the door, holding the brown paper bag. They take a small item from the bag and place it on a beige cat tree located to the right of the frame. While still holding the paper bag, they inspect the cat tree, occasionally touching it. The video concludes with the person standing upright, holding the paper bag in one hand and touching their face with the other, as they look towards the cat tree."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/GJC1G.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="28"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">IKPR3</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nAnalyze the video, focusing on its cinematography and narrative structure."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person sitting on a staircase, dressed in a white t-shirt and dark pants. They are holding a smartphone and appear to be engaged with it, possibly texting or browsing. The staircase features a wooden handrail on the right side, and the walls are painted in a light color. The warm lighting suggests an indoor setting. The person occasionally looks up from the phone and makes gestures with their hands, indicating they might be talking to someone or reacting to something on the phone. The scene develops as the person, still sitting on the staircase, starts to stand up, using the wooden handrail for support. They then proceed to walk down the stairs, moving towards the camera. The lighting remains warm, and the walls continue to be painted in a light color. The person exits the frame, leaving the staircase empty. The video wraps up with a view of the empty staircase, showcasing the wooden handrail on the right side and the light-colored walls."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/IKPR3.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="29"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">IMLN5</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWalk through the video, detailing its key moments and features."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a cluttered garage or storage area, where a person wearing a dark purple long-sleeve shirt is seen standing next to a black car. The scene is filled with various items, including a white shelving unit stocked with bottles and containers, boxes, tools, and a red jacket hanging on the wall. The person is engaged in organizing or searching for something on the shelves. They pick up a red object from the shelf and then drink from a yellow bottle. The focus remains on their tasks in the garage. The video continues with the person still in the same garage, now primarily focused on drinking from the yellow bottle. The background remains unchanged, with the same cluttered space and items visible. The person’s actions are minimal, centered around the yellow bottle. The video concludes with the person examining the red object closely, maintaining their position next to the black car. The white shelving unit and the cluttered background remain consistent, with the person’s attention now on the red object. Throughout the video, the person’s activities are concentrated on handling and examining objects within the garage."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/IMLN5.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="30"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">JNK11</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDissect the video's content, explaining each element thoroughly."
  },
  {
    "from": "gpt",
    "value": "The video begins with a young girl in a cluttered garage, wearing a pink shirt and blue jeans. She is holding a white cup and a small object in her hands. As she walks through the garage, sipping from the cup, she approaches a shelf, places the cup down, and starts examining the small object. The scene develops as the girl, now barefoot, continues to focus on the small object while the garage remains filled with various items, including a shelf with cleaning supplies and a door leading to another room. She walks towards the door, opens it, and reveals a laundry area with a washing machine and clothes hanging. The video wraps up with the girl walking through the doorway into the laundry area, still engrossed in the small object, and moving further into the room until she is partially out of view."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/JNK11.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="31"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">K71S1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide an exhaustive description of the video content."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person entering a room and approaching a box placed on the floor near the entrance door. The person is dressed in a dark beanie, a gray jacket, and khaki pants. The room features a wooden floor, beige walls, and a white door, with a small table holding a vase of flowers and a mirror on the wall to the right. As the person kneels down to open the box, a small white dog with a pink collar approaches, showing interest in the box. The person continues to open the box, revealing a black item inside, while the dog remains curious and stays close by, watching attentively.\n\nThe narrative develops as the person continues to interact with the box, pulling out and examining the black item. The small white dog with a pink collar remains engaged, closely observing the person's actions. The room's setting remains unchanged, with the beige walls, white door, small table with a vase of flowers, and mirror on the wall to the right. The person continues to explore the contents of the box, occasionally lifting items out and inspecting them, while the dog stays nearby, maintaining its interest.\n\nThe video wraps up with the person still kneeling on the wooden floor near the entrance door, interacting with the box. The small white dog with a pink collar remains close by, attentively watching the person's actions. The person stands up while holding an item from the box, fully examining it as the dog continues to watch. The room's setting remains consistent, with beige walls, a white door, a small table with a vase of flowers, and a mirror on the wall to the right. The video concludes with the person standing and looking at the item, with the dog still standing nearby, maintaining its interest."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/K71S1.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="32"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">KD2H4</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nExplain the video's components, including its characters, setting, and plot."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person washing dishes in a beige sink. They use a green and red dish brush to scrub a spoon under running water from the faucet. The sink has some stains, and there are other dishes, including a bowl and a plate, stacked on the left side. The person is wearing a black sleeve, and the background includes a white dish rack and a green dish soap bottle on the counter. The person continues to wash the dishes, scrubbing the spoon under the running water. After finishing with the spoon, they turn off the faucet and place the cleaned spoon in the sink. They then wash their hands under the faucet, rubbing them together to clean them, and dry their hands before stepping away from the sink. As they walk away, more of their black clothing and the beige tiled floor are revealed. The video then transitions to the person standing in front of a white oven. They are still wearing black clothing, and the beige tiled floor is visible. The person opens the oven door, revealing the interior with metal racks. The oven appears to be empty and has some stains and grime. After inspecting the oven, the person closes the oven door and stands in front of it, with the beige tiled floor still visible, bringing the video to a close."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/KD2H4.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="33"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">KSIYE</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nIllustrate the video's narrative journey, scene by scene, with attention to detail."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person in an orange shirt and dark pants standing at the bottom of a cluttered staircase in a house. The person is holding a broom and is actively cleaning and organizing the area, which is filled with various household items, including a laundry basket. The background features wooden walls and a small window at the top of the stairs, allowing natural light to filter in. The person diligently sweeps and picks up items from the floor and stairs, making their way up the staircase. As the person progresses up the stairs, they continue to sweep and organize, focusing on the steps and the items scattered on them. The staircase remains cluttered, but the person is determined to tidy up the space. The background remains consistent with wooden walls and the small window providing natural light. The video develops through the person's continued efforts to clean the staircase, but it takes an unexpected turn when the person accidentally trips and falls on the stairs. This causes the laundry basket to tumble down, scattering clothes all over the steps. The video wraps up with the person lying on the stairs, recovering from the fall, surrounded by the scattered clothes and the laundry basket."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/KSIYE.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="34"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">KX90W</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDissect the video's content, explaining each element thoroughly."
  },
  {
    "from": "gpt",
    "value": "The video begins with a view of a hallway leading to a kitchen. A person dressed in black clothing and black shoes walks into the hallway from the kitchen, followed closely by a white dog with black spots. The person opens a white door on the right side of the hallway and retrieves a pair of purple and white shoes from inside. The dog remains close to the person throughout this sequence. The person stands near the open door, holding the shoes, while the dog stays nearby. The scene develops as the person, still in the hallway, starts to put on the purple and white shoes, lifting one foot and then the other. The dog remains close, occasionally moving around the person's feet and showing interest in the shoes. The person bends down to adjust the shoes, and the dog continues to stay close. The person then stands upright, having put on both shoes, with the dog by their side. The video wraps up with the person bending down to adjust their shoes once more while the dog stands nearby. The person then stands upright, interacts with the dog, who sits down and looks up at them. The person turns and walks towards a door at the end of the hallway, with the dog following closely. The person opens the door, and both the person and the dog exit through it. The video concludes with the door closing behind them, leaving the hallway empty except for a pair of black shoes on the floor."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/KX90W.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="35"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">LFBCQ</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite a comprehensive depiction of the entire video clip."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person wearing a black hoodie standing in a brightly lit workshop or storage area. The room is filled with various tools and equipment, including a white shovel hanging on the wall and a cabinet. The person is facing a wooden door with a frosted glass window, which is partially open. They open the cabinet door and start searching inside, moving their hands and shifting their position slightly. The scene develops as the person continues to look inside the cabinet, with the door to the room remaining ajar. The narrative progresses with the person still in the same workshop or storage area, now turning away from the cabinet and walking towards the wooden door with the frosted glass window. The cabinet door remains open, and the white shovel is still visible on the wall. The lighting continues to be bright, provided by a fluorescent fixture on the ceiling. The video wraps up with the person walking towards the door, leaving the cabinet door open and the room's door ajar."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/LFBCQ.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="36"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">M1B7N</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDetail the video's plot development, character arcs, and thematic elements."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a cozy, well-organized kitchen with wooden cabinets and a window in the background. A person wearing a light gray sweatshirt and dark pants is seen standing at the stove, stirring something in a red pot. The atmosphere is calm and domestic, emphasizing the routine activity of cooking. The person remains focused on their task, occasionally adjusting their position slightly. Various kitchen utensils and a large green plant on the counter add to the homely feel of the scene. The person then moves to the right side of the kitchen, reaching up to open a cabinet above the counter, taking out a plate, and returning to the stove area. The scene continues to highlight the routine activity of cooking and preparing food. Finally, the person is seen at the counter near the stove, handling a white mug with black polka dots. They place the mug on the counter, pick it up again, and move towards the microwave. Opening the microwave, they place the mug inside, maintaining the calm and domestic atmosphere of the kitchen. The video focuses on the simple, everyday activities of cooking and preparing a hot beverage, creating a sense of routine and tranquility."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/M1B7N.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="37"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">MN1O2</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nCharacterize each scene of the video using a detailed description."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a room with dark red walls, creating a warm and cozy atmosphere. Prominently displayed on the wall is a framed poster featuring a turtle carrying a glass of Guinness on its back, with the text 'Thinking about a GUINNESS?' (¿Pensando en una GUINNESS?) above it, suggesting a playful and inviting message. Below the poster, a wooden table surrounded by four wooden chairs is situated, and to the left of the poster, a mirror with a decorative frame adds a touch of elegance to the room. A person wearing a blue jacket and dark pants enters the scene from the left and begins to interact with the chairs, moving them around the table, occasionally glancing at the mirror, perhaps to check their appearance or the arrangement of the room. The background includes a wooden shelf with various items on it, located on the right side of the scene, adding to the room's character.\n\nThe scene continues with the person still present, adjusting the wooden chairs around the table. They move one of the chairs closer to the table and then look into the decorative mirror, possibly to ensure everything is in order. The person then brings a vacuum cleaner into the scene and starts vacuuming the floor around the table and chairs, indicating a focus on tidiness and cleanliness. The wooden shelf with various items remains visible on the right side, maintaining the room's consistent decor. The video wraps up with the person continuing to vacuum the floor, emphasizing their dedication to keeping the space neat and orderly."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/MN1O2.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="38"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">PMG3A</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite a comprehensive depiction of the entire video clip."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a dimly lit room with a single light source, a lamp on a small table. The room is cluttered with various items scattered on the floor, including a folded chair and some cables. The walls are plain, and a small window is covered with a white cloth. The scene involves two individuals: one wearing a red hoodie and a long blue skirt, and the other dressed in a green long-sleeve shirt and a pink apron. The person in the red hoodie is standing and interacting with the table, adjusting items and pouring liquid from a pitcher into a cup, while the person in the green shirt is seated on a metal chair, focused on a large book or possibly a laptop. The interaction between them is casual and domestic, with the standing person attending to various tasks and the seated person engaged in reading or working on the device. The video continues with the person in the red hoodie crouching near the table, interacting with items on the floor, and then standing up to place a bowl on the table next to the cup. The seated person remains focused on the book or laptop. The standing person continues to adjust items on the table, including pouring more liquid from the pitcher into the cup. The video concludes with the person in the red hoodie standing near the table, holding a white container, and moving around the table to interact with the items on it. The standing person then walks towards the right side of the room, possibly to attend to other tasks, while the seated person remains engaged in reading or working on the device. The overall interaction remains casual and domestic throughout the video."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/PMG3A.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="39"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">QEXP4</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nShare a detailed breakdown of the video's storyline and visuals."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person sitting on a bed in a dimly lit room, wearing a light-colored shirt with rolled-up sleeves and blue jeans. They are engrossed in reading or examining a newspaper or a large sheet of paper. The room features a wardrobe with a mirror on the door, reflecting part of the room, including a bed with a pink bedspread. The background includes a door and some hanging clothes, creating a typical bedroom setting. The atmosphere is calm and quiet, with the person focused on the paper.\n\nThe scene develops as the person stands up from the bed and walks towards the wardrobe with the mirror. They open the wardrobe door and begin searching through the items inside, appearing to look for something specific. The room remains dimly lit, with the same background elements visible, including the bed with the pink bedspread and the door. The calm and quiet atmosphere persists as the person continues their focused task.\n\nThe video wraps up with the person still standing in front of the open wardrobe, actively searching through the shelves. They move items around methodically, occasionally pausing to examine things more closely. The wardrobe door with the mirror continues to reflect part of the room, including the bed with the pink bedspread. The room remains dimly lit, maintaining the calm and quiet atmosphere throughout. The background elements, such as the door and the hanging clothes, are still visible, reinforcing the typical bedroom setting."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/QEXP4.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="40"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">SFHY2</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite a comprehensive depiction of the entire video clip."
  },
  {
    "from": "gpt",
    "value": "The video begins with a view of a closed white door in a room with beige walls. To the left of the door, there is a window with closed blinds, and below the window is a small yellow table with a few items on it. To the right of the door, a dining area with a table and chairs is visible, and a broom is leaning against the wall. The door opens, and a person wearing a black t-shirt with a graphic design and text that reads 'I WANT TO BELIEVE,' dark pants, and a purple cap worn backward enters the room. The person walks in, holding the door open, and looks around the room, appearing to be searching for something or someone. They then close the door behind them and continue to move further into the room.\n\nThe scene continues in the same room with beige walls and a white door. The person is now standing in front of the closed door, still wearing the same black t-shirt with the text 'I WANT TO BELIEVE,' dark pants, and a purple cap worn backward. They have one hand on their hip and occasionally adjust their cap while looking around the room, possibly still searching for something or someone. The small yellow table with a few items on it remains visible to the left of the door, and the dining area with a table and chairs, as well as the broom leaning against the wall, are still present to the right of the door. The window with closed blinds continues to allow some light into the room.\n\nThe video wraps up with the person still in the same room, standing near the closed door. They continue to wear the black t-shirt with the text 'I WANT TO BELIEVE,' dark pants, and a purple cap worn backward. With one hand on their hip, they appear to be persistently looking around the room, possibly still in search of something or someone. The small yellow table with a few items on it is still visible to the left of the door, and the dining area with a table and chairs, as well as the broom leaning against the wall, remain to the right of the door. The window with closed blinds continues to let in some light. The person occasionally adjusts their cap while continuing to look around, maintaining the sense of searching throughout the video."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/SFHY2.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="41"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">T0LOH</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWalk through the video, detailing its key moments and features."
  },
  {
    "from": "gpt",
    "value": "The video begins with a closed white door adorned with multiple locks and a peephole, establishing a sense of security or secrecy. To the right of the door, a colorful painting leans against the wall, adding a touch of vibrancy to the otherwise stark setting. A person dressed in a dark blue shirt and black pants enters from the left, carrying a black bundle close to their chest. They approach the door, momentarily adjusting the bundle and glancing down at it, before turning slightly to the right and reaching towards the painting. This action suggests a possible connection or interaction between the bundle and the painting.\n\nThe narrative develops as the person, still holding the black bundle, moves a black chair in front of the white door. The colorful painting remains in its position, maintaining its role as a significant visual element. The person then sits on the chair, facing the camera, and continues to adjust and look down at the bundle, indicating its importance. After a brief moment, they stand up and exit the frame to the left, still clutching the bundle, leaving the chair in front of the door.\n\nThe video wraps up with a static scene of the white door, now with the black chair positioned in front of it. The colorful painting continues to lean against the wall, and the person along with the black bundle are no longer visible. The unchanged background, featuring the white door and the painting, emphasizes a sense of stillness and unresolved mystery, as no further movement or elements are introduced."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/T0LOH.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="42"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">T3L2A</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWalk through the video, detailing its key moments and features."
  },
  {
    "from": "gpt",
    "value": "The video begins with a view of a dimly lit hallway with yellowish lighting. On the left side of the hallway, a framed picture depicting three people leans against the wall. The hallway leads to an open door, beyond which another room is visible. A person wearing dark clothing and a hat walks into the frame from the right side, holding a bowl and a spoon. They stop in the middle of the hallway, facing the camera, and begin eating from the bowl. The scene continues with the person still standing in the hallway, eating from the bowl. They raise their arm, possibly gesturing or pointing towards something on the wall. The framed picture of three people remains leaning against the wall. The video progresses with the same person in the same position, continuing to eat from the bowl and occasionally gesturing towards the wall. The hallway's dim lighting and yellowish hue remain consistent throughout. In the final part of the video, another framed picture is visible hanging on the wall above the leaning picture. The person continues to eat from the bowl while standing in the hallway, maintaining the same posture and actions as before."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/T3L2A.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="43"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">TVS1P</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nNarrate the video's unfolding events in a descriptive manner."
  },
  {
    "from": "gpt",
    "value": "The video begins with a man sitting on a toilet in a well-lit bathroom, dressed in a gray shirt, dark pants, and black shoes. He appears to be in distress or deep thought, holding a white towel to his face and leaning forward with his elbow resting on his knee. The bathroom features white walls, a white vanity with a mirror above it, a soap dispenser, and a small object on the vanity. The man remains in this position, showing minimal movement. The scene continues with the man in the same posture, maintaining his expression of distress or deep thought. The bathroom setting remains unchanged. Finally, the man starts to stand up, still holding the towel. As he stands, he adjusts his clothing and begins to move away from the toilet. The lighting in the bathroom suddenly changes to a dim, orange hue, and then the scene transitions to complete darkness, indicating the end of the video."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/TVS1P.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="44"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">U8X15</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nIllustrate the video's narrative journey, scene by scene, with attention to detail."
  },
  {
    "from": "gpt",
    "value": "The video begins with a woman standing in a room with vertical blinds covering a sliding door, which is partially open, allowing bright light to enter. She is wearing a red top and dark pants, holding a phone in her right hand. The room features a wooden chair and a small table. The woman adjusts her hair with her left hand while looking towards the door. A man wearing a green shirt and gray shorts enters from the left side and approaches her. They engage in a conversation, with the man gesturing with his hands and the woman listening attentively. Both individuals stand near the door, continuing their conversation.\n\nThe scene develops with the woman still near the sliding door, talking and gesturing with her right hand near her face. The man, now holding a phone, raises it as if to take a photo or video of the woman. She continues to gesture and talk, occasionally looking towards the man and the door. The man remains focused on his phone, capturing her actions. The woman bends down and then stands up, adjusting her hair, while the man continues to hold his phone up, possibly recording or taking pictures.\n\nThe video wraps up with the woman standing near the sliding door, holding her phone in her right hand and looking towards the door. She adjusts her hair with her left hand and places her left hand on her hip. The camera angle changes to a closer view of the woman as she stands with her hands on her hips, looking towards the camera. The video concludes with the woman walking towards the camera, revealing her bare feet and the wooden floor of the room."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/U8X15.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="45"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">UIPJB</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nNarrate the video's unfolding events in a descriptive manner."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person standing in a doorway, framed by a checkered curtain in the background. They are dressed in a maroon and black striped long-sleeve shirt and black pants. In their right hand, they hold a foil-covered dish, while their left hand carries a bundle of colorful towels. The setting appears to be a home, with a white door on the left and a light blue wall on the right. The person looks down at the dish and then to the side, seemingly inspecting or contemplating something. This moment of pause and consideration continues as the person maintains their posture and actions, reinforcing the sense of introspection. The scene remains unchanged, with the same background and setting. Finally, the person shifts their gaze from the dish to looking up and straight ahead, transitioning from a contemplative stance to a more neutral one. The consistent setting and subtle changes in the person's posture and gaze create a narrative of thoughtful reflection and eventual resolution."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/UIPJB.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="46"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">UL9LN</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite an in-depth depiction of the video, covering all its aspects."
  },
  {
    "from": "gpt",
    "value": "The video begins with a close-up view of a green bottle with a yellow cap, likely a detergent or cleaning product, placed on a white surface, possibly a washing machine. To the left, a blue box with floral graphics and some text is partially visible. In the background, a hallway with an open closet door reveals hanging clothes. A person wearing a dark shirt enters the frame from the right, opens the closet door wider, and starts to move items inside the closet, appearing to organize or search for something among the hanging clothes. The setting appears to be a laundry area or utility room. The focus remains on the green bottle in the foreground, while the background activity provides context to the setting. The person continues to bend down and move items inside the closet, suggesting they are still organizing or searching for something. The scene maintains its setting in the laundry area, with the green bottle prominently in the foreground. The person, wearing a dark shirt with a graphic of a blue donkey and the text 'I'M A SMART ASS' (indicating a humorous or self-deprecating message), is seen moving items inside the closet. They then step closer to the foreground, holding a dark piece of clothing, and begin to organize or inspect it. The video maintains its focus on the green bottle with the background activity providing context, creating a coherent narrative of someone engaged in laundry-related tasks in a utility room."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/UL9LN.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="47"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">UZ2AL</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nIllustrate the video's narrative journey, scene by scene, with attention to detail."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a cozy kitchen adorned with wooden cabinets and a tiled backsplash. A person dressed in a blue hoodie and black pants stands near the stove, seemingly engaged in an activity such as turning a knob or inspecting something on the stovetop. The kitchen counter is cluttered with everyday items, including a dish rack filled with dishes, a toaster, and a kettle, adding to the lived-in feel of the space. A black towel hangs from one of the cabinet handles, contributing to the homely atmosphere. Throughout the video, the person remains focused on the stove area, suggesting they are either cooking or preparing something."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/UZ2AL.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="48"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">V1BX9</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDescribe every scene and its significance in the video."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a busy kitchen with black tiled walls and a marble floor. The kitchen counter is cluttered with various utensils, pots, and pans, creating a sense of everyday activity. A person wearing a green hoodie and jeans stands by the counter, eating from a bowl, while a blue folding chair is placed nearby. The scene develops as another person, dressed in a blue and white striped sweater, a purple top, and black pants, enters the kitchen carrying a tray with items. This person places the tray on the counter and adjusts the blue folding chair, adding to the sense of movement and activity in the kitchen. The first person continues eating while the second person moves around the kitchen, eventually standing near the counter. The video continues to focus on the same kitchen setting, with the person in the green hoodie still eating from the bowl. The blue folding chair is now positioned closer to the counter, and the person in the blue and white striped sweater is seen walking away from the camera towards the counter. The overall scene remains busy and cluttered, emphasizing the daily routine and interactions between the two individuals in the kitchen."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/V1BX9.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="49"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">V3RAX</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nCharacterize each scene of the video using a detailed description."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person lying on a bed covered with a patterned blanket in a dimly lit room with blue walls, a wooden window with a cloth hanging on it, and clothes hanging on the wall to the left. The person, initially still, starts to move slightly under the blanket, then sits up, revealing a purple shirt, and begins to put on a white sock. After adjusting the sock, the person lies back down, covering themselves with the blanket again. The scene transitions to the person lying on the bed once more, covered with the same patterned blanket. They sit up, adjust the blanket, and begin to put on the white sock again. Fully sitting up, the person stands and moves towards the hanging clothes, picking up a white garment. They walk towards a table covered with a patterned cloth, which holds a framed picture and other items, and start handling the white garment, seemingly preparing or folding it. The final scene shows the person, still in the purple shirt, standing at the same table, continuing to handle the white garment. They then pick up a small plate with food and start eating from it while standing. The consistent dim lighting throughout the video suggests it might be early morning or late evening, creating a coherent narrative of a person’s routine in a modest, dimly lit room."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/V3RAX.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="50"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">V4YCA</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nCharacterize each scene of the video using a detailed description."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person wearing a dark hoodie and light-colored pants standing in front of a refrigerator in a cluttered kitchen area. The person opens the refrigerator door, which has various items attached to it, including a piece of paper and some magnets. They bend down and look inside the refrigerator, which is filled with various bottles and containers on the door shelves, seemingly searching for something. The scene develops as the person continues to search through the refrigerator, moving some bottles around and eventually reaching for a plastic-wrapped item from the door shelf. They examine the item closely before deciding to take it. The person then starts to walk away from the refrigerator, closing the door behind them. The video wraps up with the person walking towards a trash can with a black trash bag inside. They lift the lid of the trash can, appearing to be about to dispose of something, and the video concludes with the person standing next to the trash can with the lid open."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/V4YCA.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="51"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">VL20E</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDetail the video's plot development, character arcs, and thematic elements."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person sitting at a dining table in a well-lit room with large windows in the background. The table is covered with a green tablecloth and has a vase with dried flowers and a small lamp on it. The person is holding and examining a camera, appearing to be focused on it. The room has a calm and cozy atmosphere, with wooden chairs around the table and a carpeted floor. The scene develops as the person continues to examine the camera, then puts it down and appears to be deep in thought, with their hand on their chin. They lean forward, possibly to look at something on the table, and then start to scratch their shoulder. The person stands up from the chair, still scratching their shoulder, and begins to walk away from the table. The video wraps up with the person walking away from the table, still scratching their shoulder. The focus then shifts to the empty dining table and chairs, maintaining the calm and cozy atmosphere of the room. The video concludes with the room being empty, highlighting the dining table and its items."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/VL20E.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="52"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">WBJC3</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDetail the video's plot development, character arcs, and thematic elements."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person in a kitchen setting, dressed in a white t-shirt and dark pants. They are holding a green cup and drinking from it. The kitchen is well-organized, featuring shelves with various items, a hanging fruit basket with bananas, a cabinet with food items, and a bottle of red liquid on the right side. After taking a sip from the cup, the person places it on top of the cabinet and then bends down slightly, possibly to look at or reach for something inside the cabinet. The scene transitions to the same person still in the kitchen, now standing in front of the cabinet and facing the shelves. The green cup remains on top of the cabinet, and the hanging fruit basket with bananas is still visible on the left side, with the bottle of red liquid on the right. The person is holding something small in their right hand, possibly a piece of food, and appears to be eating it while looking at the shelves. The scene remains static with the person standing and observing the shelves."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/WBJC3.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="53"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">WHGAQ</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDescribe every scene and its significance in the video."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person in a pink shirt and dark pants sweeping a black staircase with a red broom in a modern, well-lit interior space with white walls. A wall clock and a framed picture are visible on the wall beside the staircase. The person sweeps the stairs methodically. The scene transitions to another person in a blue shirt standing near a round glass table with red chairs, holding a transparent container in what appears to be a kitchen area with shelves and various items. The person in the blue shirt walks towards the person sweeping the stairs, who continues with the cleaning task. The narrative develops as the person in the blue shirt places the transparent container on the floor near the staircase and bends down to pick up something from the floor before walking away, leaving the container behind. The person in the pink shirt continues to sweep the stairs, focusing on the lower steps. The background remains consistent with white walls, a wall clock, and a framed picture beside the staircase. The video wraps up with the person in the pink shirt still methodically sweeping the lower steps of the staircase, occasionally pausing to adjust their sweeping technique, but remaining focused on ensuring the stairs are clean."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/WHGAQ.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="54"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">WJ6JM</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nExplore the thematic and visual aspects of the video comprehensively."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a cluttered room with beige walls, a green cabinet, and a desk with a computer monitor displaying a white screen. Various decorative items adorn the walls, including a large feathered piece and a framed picture. A person wearing a blue shirt and plaid pants is seen moving around the room, holding a red broom. They appear to be cleaning or organizing the space, moving items and sweeping the floor. Occasionally, they pause to look at or handle different objects. The person continues their cleaning activities, holding both the broom and another object, possibly a remote or a phone.\n\nThe scene maintains the same cluttered environment as the person, still holding the red broom, examines a small object that looks like a handheld mirror. They stand near the doorway, occasionally looking at the object and then back at the room. The person seems to be contemplating or inspecting the item while the room remains filled with scattered items, including the large feathered decorative piece and the framed picture.\n\nThe video progresses with the person continuing to inspect the handheld mirror while holding the broom, standing near the doorway. They occasionally glance around the room, which remains cluttered with various items. The video concludes with the person still engaged in examining the mirror, maintaining their position near the doorway, surrounded by the same scattered items and decorations."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/WJ6JM.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="55"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">WLE0F</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide a comprehensive analysis of the video's content and themes."
  },
  {
    "from": "gpt",
    "value": "The video begins with a view of a wooden hallway leading to a well-lit bathroom, with natural light streaming through a window. A cloth or towel lies on the wooden floor. A person wearing a dark jacket and plaid pants emerges from the bathroom, carrying a pink bag with a floral pattern. They stop in the hallway, adjust the bag, and appear to search for something inside it. The scene continues with the person still in the hallway, holding the bag and looking around. They then start walking towards the bathroom, briefly stopping to adjust their hair. The person stands near the bathroom door, still holding the pink bag. The narrative progresses as the person begins to walk away from the bathroom, down the hallway, while still holding the bag. They briefly stop, turn around to look back towards the bathroom, and then continue walking down the hallway. The video concludes with the person exiting the frame, leaving the hallway empty except for the cloth or towel still lying on the wooden floor."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/WLE0F.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="56"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">X4DG4</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide a comprehensive analysis of the video's content and themes."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person sitting on a well-lit kitchen counter, wearing a green shirt and gray pants, with a white towel draped over their shoulder. The kitchen features white tiled walls, a countertop with various items, and a window with a blue basket on the sill. The person then gets up, removes the towel from their shoulder, and starts working at the window area, possibly cleaning or organizing something. The scene continues with the person focused on their task at the window. The narrative develops as the person moves away from the window area, maintaining the same kitchen environment. They appear to be walking towards another part of the kitchen, and the video wraps up with the person still in motion, with the kitchen setting remaining unchanged throughout."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/X4DG4.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="57"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">XOIEY</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nShare a detailed breakdown of the video's storyline and visuals."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a bedroom with light purple walls and a marble floor, featuring a wooden wardrobe with a mirror and a small table with a framed photo on it. A person in a white sleeveless shirt and dark pants is seen bending over, meticulously packing items into a blue bag placed on the floor. Nearby, another person wearing a red t-shirt and dark pants stands, observing the packing process. The focus remains on the careful organization and placement of items into the bag. As the video progresses, the person in the white sleeveless shirt is seen handling a beige cloth, folding it carefully before placing it into the bag. The observer in the red t-shirt continues to watch attentively. The video concludes with the person in the white sleeveless shirt shaking out the beige cloth and placing it on the floor next to the blue bag, while the observer maintains their position, watching the entire process. The consistent theme throughout the video is the meticulous and organized packing activity, with a clear emphasis on the handling and placement of the beige cloth."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/XOIEY.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="58"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">ZBBP9</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nIllustrate the video's narrative journey, scene by scene, with attention to detail."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a home setting, specifically focusing on a room with a doorway leading to a bathroom. A man wearing a gray long-sleeve shirt and dark pants is seen standing in the bathroom, holding a white cup. He appears to be pouring liquid from the cup into the sink, likely using mouthwash and rinsing his mouth. The bathroom features a wooden cabinet with a towel hanging on a rack above it. In the foreground, there is a white shelf with various bottles and a wine rack. A person in a blue shirt with a white pattern on the front is seen moving around in the room outside the bathroom. The man in the bathroom continues his actions of pouring liquid and rinsing his mouth, while the person in the blue shirt occasionally moves in and out of the frame.\n\nThe scene remains consistent as the focus stays on the bathroom doorway. The man in the gray long-sleeve shirt and dark pants continues to rinse his mouth. In the foreground, a child with blonde hair wearing a white shirt is present, looking towards the bathroom. The person in the blue shirt with a white pattern is seen moving around the room, now holding a purple toy vacuum cleaner. The white shelf with various bottles and a wine rack remains visible in the background. The man in the bathroom continues his actions, and the two individuals move around in the room outside.\n\nThe video maintains the same home setting, with the focus still on the bathroom doorway. The man in the gray long-sleeve shirt and dark pants continues to rinse his mouth, holding the white cup and pouring liquid into the sink. The child with blonde hair in a white shirt remains in the foreground, looking towards the bathroom. The person in the blue shirt with a white pattern continues to move around the room, using the purple toy vacuum cleaner. The white shelf with various bottles and a wine rack is still visible in the background. The man continues his routine of pouring liquid and rinsing his mouth, while the person in the blue shirt moves the toy vacuum cleaner around the room, creating a consistent and coherent scene throughout the video."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/Charades/ZBBP9.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="59"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0016-3503206487</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide a comprehensive analysis of the video's content and themes."
  },
  {
    "from": "gpt",
    "value": "The video begins with a close-up view of a bicycle wheel and a person's hand on the handlebar, indicating that the person is riding a bicycle. The scene transitions to a wider view, revealing a person in a blue plaid shirt and shorts sitting on the bicycle seat. They are at a street intersection with traffic lights and a railway crossing, waiting for the light to change. The background features palm trees, buildings, and a clear sky with some clouds, capturing the calm and sunny atmosphere of the street scene. The traffic lights show a red signal, and the person looks around occasionally while waiting. The video then focuses on a close-up of the street intersection, where the traffic lights remain red, indicating that vehicles and bicycles must wait. A street sign reading 'Hollywood Blvd' is visible, providing context to the location. The person in the blue plaid shirt continues to wait at the intersection, maintaining the consistent, tranquil atmosphere of the scene."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/0016/3503206487.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="60"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0030-4363130739</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nElaborate on all aspects of the video you are viewing."
  },
  {
    "from": "gpt",
    "value": "The video features a heartwarming scene of a young child riding a small white pony, accompanied by two attentive adults. The child, dressed in a white helmet, a pink floral long-sleeve shirt, pink pants, and pink sandals, is seated on the pony, which is being led by an adult in a green shirt and cap holding the reins. Another adult, wearing a brown dress, a light gray cardigan, and a wide-brimmed white hat, walks alongside the child, holding their hand for support. The setting is a picturesque grassy area with trees and a fence in the background, creating a serene and safe environment. As the video progresses, the child continues to ride the pony with the same two adults ensuring their safety. The child appears to be enjoying the ride, smiling and looking content. The scene remains consistent with the grassy area, trees, and fence in the background. Towards the end of the video, the background reveals more people and ponies, suggesting a lively outdoor event or gathering. The adults' careful attention and the child's evident joy create a touching and memorable moment."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/0030/4363130739.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="61"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0068-2598449017</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nCharacterize each scene of the video using a detailed description."
  },
  {
    "from": "gpt",
    "value": "The video showcases a lively and festive scene under a large white tent, where a person is riding a mechanical bull. The rider is dressed in a white tank top, dark shorts, a colorful tie, wristbands, a party hat, and sunglasses, embodying a playful and celebratory spirit. Throughout the ride, the rider raises one arm in the air, a common technique for maintaining balance on the mechanical bull. Nearby, a person in a cowboy hat and beige shirt stands, likely supervising or assisting the rider. The background features the tent structure and some balloons, further emphasizing the festive atmosphere. The mechanical bull is dark brown with a black saddle, and a sign on it reads 'WARNING: You only ride at your own risk,' highlighting the adventurous nature of the activity. The video captures the rider's various positions and movements as they strive to stay balanced on the bull, with the supervising person occasionally glancing at the rider and the bull, ensuring safety and adding to the overall dynamic of the scene."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/0068/2598449017.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="62"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1002-2932897373</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nElaborate on the visual and narrative elements of the video in detail."
  },
  {
    "from": "gpt",
    "value": "The video begins with a view of a charming European town square, surrounded by colorful buildings in yellow and pastel shades. These buildings feature traditional architectural elements such as wooden shutters, balconies adorned with flower boxes, and tiled roofs. A central fountain is visible, along with benches and potted plants that enhance the square's inviting atmosphere. The camera pans to the right, revealing more of the town's picturesque buildings, including facades in yellow, red, and blue, some of which are covered in ivy. The scene transitions to a closer view of a narrow alleyway, where a person is seen walking towards a building with a sign that reads 'Gasthof Zauner' and another sign for 'Hallstätter Markt.' The serene and inviting essence of the small, scenic European town is captured beautifully.\n\nThe video continues with a view of the same narrow alleyway, where two people, one in a dark jacket and light pants and the other in a light outfit, are seen walking towards the ivy-covered building with the 'Gasthof Zauner' sign. The 'Hallstätter Markt' sign is also prominently displayed on a nearby building. The camera pans upwards, revealing more of the ivy-covered building and its wooden balconies adorned with flower boxes. The camera then shifts to the right, showcasing a series of picturesque buildings with colorful facades in shades of white, red, and yellow, some of which are also covered in ivy. The video concludes with a view of a church steeple in the background, adding to the quaint and serene atmosphere of the town."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1002/2932897373.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="63"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1002-7592105758</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDetail the video's plot development, character arcs, and thematic elements."
  },
  {
    "from": "gpt",
    "value": "The video begins with a close-up of a black vinyl record with an orange label spinning on a turntable. A makeshift tonearm constructed from LEGO pieces is positioned over the record, and to the right of the turntable, a white plastic cup is secured with a rubber band. The background reveals a white tablecloth-covered table cluttered with various LEGO boxes and a white container filled with LEGO pieces. The camera angle shifts slightly, showing a person in a gray shirt holding a smartphone. The view then widens to reveal a cluttered table with a white laptop displaying a blue screen, a few scattered LEGO pieces, and another white plastic cup lying on its side. The scene transitions to a top-down view of the spinning record and the surrounding items on the table.\n\nThe focus then returns to a close-up view of the spinning vinyl record with the makeshift LEGO tonearm. The white plastic cup remains secured with a rubber band to the right of the turntable, and another white plastic cup lies on its side nearby. The background features a cluttered table with a white laptop displaying a blue screen with a graphical interface, possibly a software program. To the left of the laptop, there is a closed laptop adorned with various stickers. A white container filled with LEGO pieces is visible in the bottom left corner. The camera angle shifts slightly, providing a clearer view of the laptop screen, which shows a graphical interface with a character on a stage under spotlights. The video concludes with a close-up of the laptop screen, focusing on the character under the spotlights."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1002/7592105758.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="64"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1005-3522105391</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nOffer a detailed interpretation of the video's message and imagery."
  },
  {
    "from": "gpt",
    "value": "The video begins with a baby dressed in a blue shirt, lying on a colorful patchwork quilt and reaching out towards a blue and purple ball. The baby, positioned on their stomach, makes several attempts to grab the ball, which moves slightly with each attempt. An adult, wearing shorts, is sitting nearby, attentively watching the baby. The background features a red carpet and some furniture, and a cat is visible in the background, observing the scene. The narrative develops as the baby continues to lie on the quilt, still focused on the blue and purple ball that has now rolled away. The cat, previously in the background, becomes more prominent, standing near a piece of furniture. A dog enters the scene, approaching the ball and sniffing it, capturing the baby's curiosity. The baby watches the dog and the ball intently, while the cat moves closer to the baby. In the final frames of this segment, the baby interacts with the adult, who is now wearing shorts and a striped shirt. The background remains consistent with the red carpet and some furniture. The video wraps up with the baby still on the colorful patchwork quilt, looking towards the adult sitting nearby, dressed in shorts and a striped shirt. The adult is seated cross-legged on the red carpet, with one hand resting on their knee and the other near their face. The background includes a black armchair, a white side table, and a brown chair with a cushion. A large dog enters the scene, moving closer to the baby and partially obscuring the view. The baby looks at the dog with curiosity as the dog stands near the baby, while the adult continues to observe the interaction from nearby."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1005/3522105391.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="65"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1009-2962441847</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite an in-depth depiction of the video, covering all its aspects."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person in a red sweater and jeans standing in a room with a couch and a shelf in the background. They are holding a small object, possibly a remote or a phone, and appear to be engaged in a conversation or demonstration. The person raises their arm, possibly gesturing or emphasizing a point. The focus then shifts to a table with a bottle and other items, indicating a casual setting. The person bends down, possibly to pick something up from the floor, while another individual is visible seated on the couch in the background. The person in the red sweater then sits down on the couch, continuing to interact with the object in their hands. They stand up again, holding and examining the object closely, making animated gestures, suggesting they are explaining or demonstrating something. The scene transitions to the person in the red sweater standing in front of the couch, holding the object and making animated gestures, engaged in a lively conversation or demonstration. The person continues to gesture energetically. Another individual in a black shirt approaches the person in the red sweater, who is now seated on the couch. The individual in the black shirt appears to be assisting or interacting with the person in the red sweater. The focus shifts to a table with a laptop, a bottle, and other items, maintaining the casual setting. The person in the red sweater is seen sitting on the couch, holding the object, while the individual in the black shirt stands nearby. The video wraps up with the person in the red sweater standing up again, smiling, and holding the object, suggesting a return to the previous lively interaction. The setting remains consistent throughout, with a casual, indoor environment and a relaxed atmosphere."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1009/2962441847.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="66"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1009-7121723227</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDescribe every scene and its significance in the video."
  },
  {
    "from": "gpt",
    "value": "The video begins with a cozy indoor scene where a person is sitting on a dark green armchair, holding a baby. The person is dressed in a colorful, patterned shirt, while the baby is in a blue and white striped outfit with a white bib. Natural light streams through large windows with partially drawn blinds, illuminating the room. Several plants and a small table with electronic equipment are visible to the left of the armchair. The person interacts affectionately with the baby, holding them close and looking at them with warmth. The baby curiously looks around the room and at the person holding them, creating a nurturing and tender atmosphere. The scene continues with the person still seated in the armchair, maintaining their affectionate interaction with the baby. The natural light, plants, and electronic equipment remain consistent in the background, reinforcing the warm and nurturing ambiance throughout the video."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1009/7121723227.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="67"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1010-4968805923</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDescribe every scene and its significance in the video."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a serene park-like setting with a grassy field and tall trees under a clear blue sky, indicating a sunny day. Three individuals are present throughout the video. Initially, the person on the left, dressed in a white shirt and jeans, is holding a baseball bat and preparing to swing. The person in the middle, wearing a black shirt and jeans, stands with their hands on their hips, observing the batter. The person on the right, in a dark shirt and light-colored pants, holds a baseball glove, ready to catch the ball. This scene suggests a casual baseball practice or game among friends. As the video progresses, the person on the left is seen bending down, possibly picking something up from the ground. Meanwhile, the person in the middle continues to observe with hands on hips, and the person on the right, now also with hands on hips, appears to be in conversation with the person in the middle. The video maintains a casual and friendly atmosphere, capturing a moment of leisure and camaraderie in the park."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1010/4968805923.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="68"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1011-2909898562</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nIllustrate the video's narrative journey, scene by scene, with attention to detail."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person in a gray shirt and blue jeans holding a child in a red outfit while sitting at the top of a green slide attached to a wooden play structure. The background includes a grassy yard with trees and a white building with blue trim. The person and child slide down together, with the person holding the child securely. The child appears to be enjoying the ride, and they reach the bottom of the slide where the person stands up while still holding the child. The scene transitions to a person in a light blue shirt holding the same child in a red outfit with white patterns, standing outside in front of a house with white siding and blue trim. Another person in a white tank top and dark shorts is visible in the background. The person in the blue shirt interacts with the happy and engaged child, eventually lowering the child to the ground. The child, now standing, points and looks around the grassy yard with trees in the background, exploring the surroundings while the adults watch. The video wraps up with the child in the red outfit standing on the grassy yard, looking around and pointing curiously. The legs of the adult in dark shorts and sneakers are visible in the background, suggesting the presence of the person in the white tank top from the previous scene. The green slide from the wooden play structure is partially visible on the left side of the frame. The child takes a few steps, exploring the area, creating a playful and relaxed atmosphere."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1011/2909898562.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="69"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1011-9170516863</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nConvey the narrative and visual elements of the video in detail."
  },
  {
    "from": "gpt",
    "value": "The video features a live performance by 'THE WILD ONION STRING BAND' under a blue canopy. The band consists of four musicians: a banjo player in a red jacket and blue cap on the left, an acoustic guitar player in a black jacket and blue jeans in the center, a double bass player in the background, and a violin player in a white shirt and white cap on the right. The backdrop includes a green banner with text and images of flowers, reading 'THE WILD ONION STRING BAND.' The musicians are focused on their instruments, creating a lively and engaging atmosphere. Throughout the video, the musicians are shown in various stages of their performance, with slight variations in their movements and positions. The acoustic guitar player strums the guitar, the banjo player plucks the banjo, the double bass player plays the double bass, and the violin player plays the violin. The overall scene remains consistent, showcasing the band's dynamic performance. In one of the frames, the banjo player is seen holding a drink, adding a casual and relaxed element to the performance."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1011/9170516863.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="70"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1015-5398268379</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite a comprehensive depiction of the entire video clip."
  },
  {
    "from": "gpt",
    "value": "The video captures a lively birthday party scene in a well-lit room adorned with colorful balloons and a pink tablecloth on the dining table. It begins with a woman in a light pink sweater and blue jeans taking photos of the children, who are dressed in various colorful outfits, some resembling princess dresses, and are seated around the table. The festive atmosphere is enhanced as a woman in a red striped shirt and blue jeans enters carrying a birthday cake adorned with candles and decorations. The children, visibly excited, engage in conversation and activities around the table filled with party decorations and snacks. The woman in the red striped shirt brings the beautifully decorated cake, featuring a crown-like decoration and a '6' candle on top, closer to the children, who eagerly look at it. The scene continues with the children enjoying their snacks, including a bowl of cheese puffs and other treats, and interacting with each other. A green gift bag on the table adds to the celebratory ambiance, maintaining the festive atmosphere throughout the video."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1015/5398268379.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="71"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1018-5974881839</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDetail the video's plot development, character arcs, and thematic elements."
  },
  {
    "from": "gpt",
    "value": "The video begins with a close-up of a person performing a medical procedure on a dog's leg. The person, dressed in blue pants and sneakers, is meticulously inserting a needle into the dog's leg, which shows a patch of pink, irritated skin. The dog's leg is held gently but firmly to ensure stability during the procedure. The background features a gray floor, and the text 'Nottingham Vet School' is visible in the lower right corner of each frame, indicating that the setting is a veterinary school. The sequence highlights the care and precision involved in the needle insertion. The video then develops through the continuation of the medical procedure, where the same person is seen carefully removing the needle from the dog's leg. The dog's leg, still showing the patch of pink, irritated skin, is held gently but firmly. The background remains a gray floor, and the text 'Nottingham Vet School' continues to be visible, reinforcing the veterinary school setting. The sequence captures the careful removal of the needle, emphasizing the precision and care involved. The video wraps up with the dog's leg being released and the person’s hands moving away, indicating the end of the procedure."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1018/5974881839.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="72"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1020-4405101727</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite an in-depth depiction of the video, covering all its aspects."
  },
  {
    "from": "gpt",
    "value": "The video begins with an outdoor setting where several people are sitting on benches. The background features a white van and a fence adorned with lights, suggesting a park or a public area. The scene is bright and sunny, indicating daytime. A person in a blue jacket with a purple bag is seated on a bench, looking down at something in their hands. Next to them, another person in a black suit with a white shirt is also looking down. The video develops through a continuation of the same setting, with the focus shifting slightly to the person in the black suit, who is now closer to the camera, with their face partially visible. The scene remains bright and sunny, reinforcing the daytime setting. The video wraps up with a close-up view of the person in the black suit, capturing their shoulder and part of their face, while the background becomes slightly blurred, emphasizing the proximity of the camera to the person. The consistent elements of the white van, the fence with lights, and the sunny atmosphere throughout the video reinforce the idea of a tranquil moment in a public park."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1020/4405101727.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="73"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1022-8171216955</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWalk through the video, detailing its key moments and features."
  },
  {
    "from": "gpt",
    "value": "The video begins with a young girl adorned with a green bow in her hair, dressed in a plaid dress and white shirt, playing at a table filled with puzzle pieces and a toy dog. She interacts with the toy dog, which seems to be part of a game, and her excitement is evident as she moves around and gestures animatedly. The setting includes a wooden cabinet and a hallway in the background. Another child, also in a white shirt, joins her at the table, adding to the playful atmosphere. The video develops as the young girl continues to engage with the toy dog, her animated movements and gestures highlighting her enthusiasm. The other child remains at the table, and the two children share a lively and engaging moment, interacting with the toy dog and each other. The background remains consistent, featuring the wooden cabinet and hallway, providing a warm and familiar setting for their playful activity. The video wraps up with the children deeply engrossed in their game, capturing a joyful and dynamic interaction between them."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1022/8171216955.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="74"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1023-6826504502</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDissect the video's content, explaining each element thoroughly."
  },
  {
    "from": "gpt",
    "value": "The video begins with a view inside a vehicle, showing the interior and passengers as it drives on a dirt road in a rural area. The camera provides a wider view of the vehicle's interior, revealing three passengers. The scene transitions to an exterior view of the white pickup truck driving on a red dirt road, flanked by fields on either side. The overcast sky and the road stretching into the distance suggest a long journey through the rural landscape. The video develops through various perspectives of the vehicle and the surrounding scenery, maintaining the rural setting and overcast sky. The camera captures the right side of the vehicle, including the side mirror and part of the window. A passenger's arm extends out of the window, first making a waving gesture and then a thumbs-up gesture, before retracting. The journey continues with the road stretching ahead and another vehicle visible in the distance. The video wraps up with a close-up of a passenger in the white pickup truck, their arm extended out of the window, wearing a white shirt. The consistent rural landscape and overcast sky persist throughout the video, emphasizing the continuity of the journey."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1023/6826504502.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="75"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1024-3843551232</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide an exhaustive description of the video content."
  },
  {
    "from": "gpt",
    "value": "The video features a serene indoor setting with two dogs. The larger dog, with a black and white coat, is lying on a carpeted floor near a chair with metal legs and a wooden table. The smaller dog, entirely black, is initially lying down a short distance away from the larger dog. The smaller dog gets up and moves around, exploring the area, while the larger dog remains mostly stationary, occasionally shifting its position slightly. The background consistently shows the carpeted floor and parts of the furniture, including a chair with metal legs and a wheeled office chair. As the video progresses, the smaller dog continues to explore the area, moving around before eventually lying down on the floor a short distance away from the larger dog. Throughout the video, the larger dog remains mostly stationary, providing a calm and steady presence in the scene."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1024/3843551232.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="76"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1025-7359783390</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite a comprehensive depiction of the entire video clip."
  },
  {
    "from": "gpt",
    "value": "The video begins with a person in a red jacket and dark pants walking through a snowy forest. They are wearing a backpack and navigating through the snow, which has visible footprints. The background consists of tall trees and a misty atmosphere, suggesting a cold and possibly early morning setting. As they walk, they occasionally look down at the snow and adjust their balance. The person moves closer to the camera, making the details of their red jacket and backpack more visible. The scene transitions to a close-up of the person's upper body and backpack as they turn slightly to the side, revealing more of the snowy forest in the background. The narrative continues with the person now seen from behind, still walking through the snowy forest with the same backpack. The snowy ground is uneven, and they carefully navigate the terrain, occasionally spreading their arms for balance. The background remains consistent with tall trees and a misty atmosphere, reinforcing the cold and possibly early morning setting. As they move forward, they encounter a fallen tree trunk partially covered in snow. The person pauses, seemingly assessing the best way to cross it, and the video concludes with them beginning to step over the fallen tree trunk, continuing their journey through the snowy forest."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1025/7359783390.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="77"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1051-6301223138</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWalk through the video, detailing its key moments and features."
  },
  {
    "from": "gpt",
    "value": "The video begins with a serene beach scene where a woman dressed in a pink top, navy blue skirt, and white hat is interacting with a small child at the edge of the water. The child is sitting in the shallow water, and the woman is bending over, holding the child's hand, seemingly helping or playing with them. Gentle waves crash onto the shore in the background, creating a peaceful atmosphere. As they enjoy their time together, another person in a polka-dotted swimsuit briefly walks by, momentarily passing between the camera and the woman with the child. The focus remains on the woman and child, with the woman maintaining her supportive stance to ensure the child's safety and enjoyment.\n\nThe scene continues with the woman and child at the water's edge. The woman, still in her pink top, navy blue skirt, and white hat, bends over to hold the child's hand, ensuring the child remains safe as they sit in the shallow water. The waves continue to gently crash onto the shore, providing a soothing backdrop. Throughout this part of the video, the woman occasionally adjusts her position slightly but always keeps a firm hold on the child's hand. The child appears to be enjoying the water, occasionally looking up at the woman, who remains focused on their interaction. No other people pass by during this segment, keeping the attention on the woman and child.\n\nThe video wraps up with the woman helping the child stand up in the shallow water. She holds both of the child's hands to provide support as the child, dressed in a red outfit, takes tentative steps in the water. The woman closely guides the child, ensuring their safety while the waves continue to gently crash onto the shore in the background. The sandy beach is visible in the foreground, adding to the idyllic setting. The scene remains focused on their interaction, highlighting the woman's care and the child's exploration of the water, with no other people passing by in this final segment."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1051/6301223138.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="78"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1104-8181417359</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDissect the video's content, explaining each element thoroughly."
  },
  {
    "from": "gpt",
    "value": "The video captures a tender and heartwarming moment between two children lying on a soft, light blue blanket adorned with colorful patterns, including a red dot and blue lines. The older child, dressed in a white shirt with colorful stripes, gently interacts with a baby who is wearing a white outfit with pink polka dots and a white headband. The older child holds the baby's hand, showing affection and care. The background is a neutral-colored carpet, and various toys are visible around them, adding to the cozy and playful atmosphere. Throughout the video, the sense of bonding and love between the two children is palpable, as the older child continues to show gentle affection towards the baby, creating a touching and serene scene."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1104/8181417359.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="79"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1122-2573200479</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nElaborate on all aspects of the video you are viewing."
  },
  {
    "from": "gpt",
    "value": "The video begins with a child walking down a hallway towards the camera, dressed in a dark shirt with a colorful design and blue pants. The hallway features wooden floors and a bubble wrap mat. In the background, a person in a grey skirt and red shoes is seen in the kitchen area, engaged in some activity. The child reaches out to touch the wall and door as they walk, creating a sense of curiosity and exploration. The home environment is warm and lived-in, with wooden cabinetry and a washing machine visible in the kitchen. The child then moves out of the frame, heading back towards the kitchen area.\n\nThe scene transitions to the child walking down the hallway again, this time moving towards the kitchen area. They pass a door on the right and continue their journey. The person in the kitchen remains engaged in their activity, adding a sense of continuity and daily life. The child moves closer to the kitchen, where the washing machine and wooden cabinetry are more prominently visible. They then turn around and start walking back towards the hallway, passing a white object on the floor, which adds a touch of realism to the setting.\n\nIn the final scene, the child is once again walking down the hallway towards the camera, maintaining their curious behavior by touching the wall and door. The person in the kitchen, still wearing a grey skirt and red shoes, continues their activity, providing a consistent backdrop. The white object on the floor in the kitchen is also visible, reinforcing the lived-in feel of the home. As the child moves closer to the camera, they continue to interact with their surroundings before eventually moving out of the frame, leaving the viewer with a sense of warmth and familiarity in this home environment."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1122/2573200479.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="80"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1150-4772810451</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite a comprehensive depiction of the entire video clip."
  },
  {
    "from": "gpt",
    "value": "The video begins with three dogs interacting in an outdoor setting. The dogs include two black and white dogs and one small white dog. The ground is covered with dirt and leaves, and there are some rocks and a green plastic chair visible in the background. The black and white dogs are sniffing the small white dog, and they all appear to be exploring their surroundings. The dogs move around, with the black and white dogs showing interest in each other and the small white dog. Towards the end of this segment, the two black and white dogs focus their attention on the green plastic chair, sniffing around it, while the small white dog is no longer in the frame.\n\nContinuing from the previous scene, the two black and white dogs are still near the green plastic chair, sniffing around it with curiosity. The small white dog reappears, joining the other two dogs. The dogs move around the area, with the black and white dogs occasionally sniffing the ground and each other, while the small white dog also explores the surroundings. The background remains consistent, featuring dirt, leaves, and a tent in the distance. The dogs eventually move away from the chair, heading towards a different part of the outdoor setting, with the black and white dogs leading and the small white dog following.\n\nIn the final segment, the two black and white dogs are near a mesh tent structure, continuing from the previous scene. The background now features a camper, a picnic table, and a wooded area with dirt and leaves on the ground. The dogs are sniffing around a tree stump. The small white dog reappears and joins the other two dogs. The dogs move around the area, with the black and white dogs leading and the small white dog following. The black and white dogs approach the camera, and the small white dog follows closely behind. The video concludes with a close-up of the small white dog sniffing the ground near a pile of rocks, and then both the small white dog and one of the black and white dogs interact with a person, sniffing their legs and showing curiosity."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1150/4772810451.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="81"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1150-6888660962</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nElaborate on all aspects of the video you are viewing."
  },
  {
    "from": "gpt",
    "value": "The video features a heartwarming interaction between a baby and an adult in a cozy, lived-in room. The baby, dressed in a dark blue onesie adorned with stars, planets, and spaceships, is lying on the adult's chest. The adult, wearing a white shirt and dark pants, gently holds and moves the baby's hands. The background includes a green patterned couch and a wooden side table with various items, such as a sewing machine and a white container. The baby looks around curiously and interacts with the adult.\n\nThe scene transitions to the baby being held up by the adult, who supports the baby's hands and helps the baby stand. The baby stands on the adult's chest while the adult's legs are bent at the knees. The baby continues to move slightly, exploring the surroundings and engaging with the adult.\n\nThe final part of the video shows the baby still being held up by the adult, maintaining the same supportive stance. The background remains consistent with the green patterned couch and the wooden side table. Additionally, a shelf with books and other items is visible, enhancing the warm and homely atmosphere of the room. Throughout the video, the baby and the adult share a tender and playful moment, highlighting the bond between them."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1150/6888660962.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="82"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1161-2977504625</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide a comprehensive analysis of the video's content and themes."
  },
  {
    "from": "gpt",
    "value": "The video begins with a serene scene on a paved path surrounded by lush greenery and trees. A child with blonde hair is seated in a small, red toy car, holding onto the steering wheel. A person dressed in a white shirt, blue shorts, and black shoes is seen pushing the toy car from behind. The clear, sunny day enhances the vibrant green foliage in the background. As the video progresses, the person continues to push the toy car along the path, occasionally looking back, possibly to check their surroundings or interact with the child. The consistent background of greenery and the joyful interaction between the child and the person pushing the car create a warm and pleasant atmosphere throughout the video."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1161/2977504625.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="83"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1165-8521113411</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDissect the video's content, explaining each element thoroughly."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a classroom setting, where two children are standing at the front of the room. The child on the left is dressed in a blue shirt with a graphic design and black pants, while the child on the right is wearing a black shirt featuring a robot graphic and the words 'high five,' paired with blue jeans and white sneakers. The child on the right is holding a plunger above their head. The classroom environment is typical, with desks, chairs, and various educational posters and decorations adorning the walls. Other children are seated at their desks, engaged in different activities. The scene remains consistent throughout the video, maintaining the same background and activities, with the two children continuing to stand at the front of the room in the same attire and positions."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1165/8521113411.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="84"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">1201-7967750522</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nOffer a detailed interpretation of the video's message and imagery."
  },
  {
    "from": "gpt",
    "value": "The video begins with a close-up of a wet tiled floor, showcasing a mix of beige, white, and red tiles. This scene transitions to a wider view of a plaza adorned with multiple water fountains shooting streams of water into the air. The fountains are arranged in a line, creating a reflective surface on the dark tiles of the plaza. In the background, buildings with large columns and glass windows are visible, along with potted plants featuring red flowers. The fountains continue to spray water, and a person dressed in black runs through the water jets, creating playful splashes and adding a dynamic element to the scene. The person runs across the wet plaza, emphasizing the lively atmosphere created by the fountains.\n\nThe scene then shifts to a plaza with light-colored tiles, where a man in a green t-shirt and beige shorts is walking. The background features a building with large glass windows and columns, similar to the previous scene, and potted plants with red flowers are also visible. Another person in a black outfit runs across the plaza, adding a sense of movement and activity. The man in the green t-shirt continues to walk, while another person in a dark outfit is seen in the background, also walking. The video wraps up with the man in the green t-shirt turning slightly, while the person in black continues to run, maintaining the lively and dynamic atmosphere of the scene."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/NextQA/1201/7967750522.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="85"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">v_B6W_0AvgGTY</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nOffer a detailed interpretation of the video's message and imagery."
  },
  {
    "from": "gpt",
    "value": "The video begins with a logo on a white background, featuring the text 'IQK9 TRAINING' in a bold, teal and black font. The 'IQK9' part is larger and more prominent, while 'TRAINING' is smaller and positioned below and to the right. To the left of the text is an illustration of a dog's head, outlined in black and filled with teal. The scene transitions to an outdoor park setting where a person in an orange shirt and black shorts is training a black and white dog. The person gives commands, and the dog responds by sitting and then running after a thrown object, likely a frisbee. The text 'www.IQK9Training.com' appears at the bottom of the screen, indicating the website for the training service. The dog runs across the grassy field, showcasing its agility and training.\n\nThe video continues in the outdoor park setting, capturing a wide view of the grassy field with houses and trees in the backdrop. The text 'www.IQK9Training.com' is prominently displayed at the bottom of the screen. The field is mostly empty, with a few scattered objects and people in the distance. A black and white dog is seen running across the field, holding a red frisbee in its mouth, demonstrating its training and agility.\n\nThe video progresses with a continuation of the outdoor park setting. A person in an orange shirt and black shorts walks alongside a black and white dog holding a red frisbee in its mouth. The text 'www.IQK9Training.com' is prominently displayed at the bottom of the screen. The background features trees with autumn foliage and a few scattered objects. The scene transitions back to a white background displaying the 'IQK9 TRAINING' logo, reinforcing the branding of the training service."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/activitynet/v_B6W_0AvgGTY.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="86"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">v_H6hb3SIqdL4</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDetail the video's plot development, character arcs, and thematic elements."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a rustic, cluttered shed with wooden walls and various tools and objects scattered around, creating a casual and somewhat disorganized atmosphere. A man wearing a red, white, and blue striped polo shirt is seen playing darts. He throws several darts towards a black dartboard with white sections and numbers around the edge, which is mounted on a wooden door. The man appears focused and engaged in the activity. The camera then moves closer to the dartboard, showing the darts stuck in it, indicating the man's accuracy. Items like a folding chair, a radio, and coiled extension cords are visible in the background, adding to the shed's informal setting. The man then approaches the dartboard, and the camera focuses on the darts stuck in it, further highlighting his precision. He reaches out and begins to remove the darts from the dartboard, maintaining the consistent, cluttered backdrop of the shed filled with various tools and objects."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/activitynet/v_H6hb3SIqdL4.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="87"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">v_IuntoXkEWPI</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nElaborate on all aspects of the video you are viewing."
  },
  {
    "from": "gpt",
    "value": "The video begins with a young child, dressed in a white outfit, attempting to climb up a bright yellow slide in a playground. The playground structure is made of wood, and there are trees and other playground equipment visible in the background. Some graffiti is visible on the wooden structure, and a few parked cars can be seen through the trees. The child makes several attempts to climb the slide, using both hands and feet to gain traction on the smooth surface, showing determination despite the challenge. The scene continues with the child persistently trying to ascend the slide, making incremental progress. The background elements, including the wooden playground structure, graffiti, and parked cars, remain consistent. The child's effort and persistence are evident as they continue to climb. As the video progresses, the child reaches a higher point on the slide but then starts to slide back down. Eventually, the child slides down the slide, sitting upright and appearing to enjoy the descent. The video concludes with the child at the bottom of the slide, looking up at the structure, possibly contemplating another attempt."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/activitynet/v_IuntoXkEWPI.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="88"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">v_Vyj5eIh3jh0</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nElaborate on all aspects of the video you are viewing."
  },
  {
    "from": "gpt",
    "value": "The video begins with a dark scene where a small fire is visible in the distance. As the fire grows larger and more intense, it illuminates the surrounding area, revealing a sandy surface. The flames are bright and dynamic, casting light and shadows. A white pickup truck is parked near the fire, adding to the scene's intrigue. The fire continues to grow, creating a long, continuous line of flames on the sand. The flames rise high into the air, reaching their peak intensity. The video then focuses on the intense fire on the sandy surface, with the flames casting dramatic light and shadows. The fire forms a long, continuous line, with large flames reaching their highest point. The background remains dark, emphasizing the brightness and intensity of the fire. The white pickup truck is no longer visible, allowing the viewer to fully appreciate the peak intensity of the flames."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/activitynet/v_Vyj5eIh3jh0.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="89"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">v_WjEh7acrr5o</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide an exhaustive description of the video content."
  },
  {
    "from": "gpt",
    "value": "The video features a woman in a red shirt sitting and knitting in a cozy, craft-oriented environment, possibly at a fair or workshop. She is holding knitting needles and working with yarn, with a colorful knitted item, a water bottle, and various other items on a table covered with an orange cloth in the background. A sign in the background reads 'Bargain beauty' and 'Flippin' fab cookery' with an image of a dish, indicating a setting that combines elements of beauty and cooking. The scene continues with the woman knitting, maintaining the same background elements and reinforcing the warm, creative atmosphere of the setting."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/activitynet/v_WjEh7acrr5o.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="90"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">v_Zrv8_-Y-jSM</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nDescribe every scene and its significance in the video."
  },
  {
    "from": "gpt",
    "value": "The video begins with a black screen that transitions to a blue background with a futuristic design, featuring a central screen with a downward arrow icon. Bright star-like flashes appear, followed by the text 'ROPE SKIPPING...' in large white letters. The scene then shifts to a gymnasium where five individuals are seen skipping rope. The gym has a yellow floor with green lines, a brick wall, and a basketball hoop in the background. The individuals continue their rope skipping routine throughout the frames. The video develops through the same gymnasium setting, with the five individuals now performing a synchronized rope skipping routine. They are positioned in a line, each wearing different colored outfits: one in red, two in black, one in pink, and one in purple. The individuals jump in unison, with their arms extended outward, maintaining their rhythm and coordination, which emphasizes the energetic atmosphere of the gymnasium. The video wraps up with the same gymnasium setting, where the five individuals continue their synchronized rope skipping routine. The text 'Klasse 7c / Fr. Leifels' appears on the screen, indicating the class and instructor. The individuals transition from jumping to performing a coordinated sequence of movements, including squatting and sitting on the floor, while still maintaining their rhythm and coordination. The video concludes with the individuals seated on the floor, with the text 'Klasse 7c / Fr. Leifels' still visible on the screen."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/activitynet/v_Zrv8_-Y-jSM.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="91"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">v_gCx-ucvPhDY</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nConvey the narrative and visual elements of the video in detail."
  },
  {
    "from": "gpt",
    "value": "The video features a diver at the 2012 London Olympics, standing on a platform and preparing for a dive. The diver is dressed in a black swimsuit with red accents, and the 'London 2012' Olympic signage is visible in the background, setting the scene at this prestigious event. The diver walks towards the edge of the platform, building momentum before executing a series of impressive flips and twists in the air. The audience and judges are seen in the background, attentively watching the performance. The video concludes with the diver entering the water with a splash, showcasing their skill and precision."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/activitynet/v_gCx-ucvPhDY.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="92"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">v_y0Kio7VOk5o</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide a comprehensive analysis of the video's content and themes."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a professional indoor diving facility, characterized by its well-lit environment, industrial lighting, and prominent decorations, including a large American flag and several banners, one of which reads 'NCAA.' It begins with a person in a blue swimsuit standing on a high diving platform, which is beige with metal railings and part of a larger structure with multiple levels and stairs. The person adjusts their position and stance, preparing for the dive. The scene then progresses to the person standing at the edge of the platform, raising their arms above their head in preparation for the dive. After a moment of stillness, they begin to lean forward, initiating the dive. The video captures the person mid-dive, in an upside-down position with their body arched and arms extended towards the water. The sequence continues with the person entering the water, creating a splash, and then emerging from the water to swim towards the pool's edge. The video concludes with the person reaching the pool's edge and climbing out of the water, successfully completing the dive. Throughout the video, the professional atmosphere of the facility is consistently maintained."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/activitynet/v_y0Kio7VOk5o.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="93"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">143f43b6-3108-4977-ae62-d94e74e14ad6</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nConvey the narrative and visual elements of the video in detail."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a modern office setting and revolves around a demonstration or testing of technology, likely involving virtual reality or augmented reality equipment. It begins with a person in a white shirt standing near a cluttered table, holding a black object, possibly a VR headset. Another person in a dark blue shirt stands near a door, while a third person, seated and wearing a head-mounted device, raises their arms. The person in the dark blue shirt mimics the seated person's actions by also raising their arms. The person in the white shirt leans over the table, possibly interacting with a laptop, while the other two individuals maintain their arm positions.\n\nThe scene continues with the person in the white shirt standing and gesturing, possibly explaining the functionality of the device on the table. The person in the dark blue shirt remains near the door, observing, while the seated person with the head-mounted device extends their arms forward. The table is cluttered with various items, including a laptop, documents, a coffee mug, a blue Ethernet cable, a box of disposable masks, a bottle of hand sanitizer, a stapler, and a bottle of green tea. The large screen in the background still displays a webpage. The person in the white shirt points at a document on the table, possibly providing further explanation or instructions.\n\nThe video wraps up with the person in the white shirt standing near the table, holding the black object near their face. The person in the dark blue shirt continues to observe from near the door. Another individual, wearing a green vest over a white shirt, is also holding a similar black object near their face. The table remains cluttered with various items, and the large screen in the background still displays a webpage with images. Both the person in the white shirt and the person in the green vest appear to adjust the black objects near their faces, possibly preparing to use them. The overall scene suggests a continuation of the demonstration or testing of the technology."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/ego4d/143f43b6-3108-4977-ae62-d94e74e14ad6.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="94"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">101-0O4bxhpFX9o-split_0</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide an exhaustive description of the video content."
  },
  {
    "from": "gpt",
    "value": "The video begins with two individuals sitting side by side in a kitchen setting, one in a white t-shirt and the other in a sleeveless top with a colorful floral pattern. The background features a brick wall and various kitchen items, creating a cozy and homely atmosphere. The scene transitions to a close-up of hands, belonging to the person in the white t-shirt who has a tattoo on their forearm, carefully placing strips of raw bacon onto a metal baking tray. The bacon is meticulously laid out in rows. The baking tray is then placed into an oven, with overlaid text reading '400°F 10-15 minutes,' providing the cooking instructions for the bacon. The video continues with a close-up of the same person placing the tray into the oven, emphasizing the cooking process. The oven door is open, and the text '400°F 10-15 minutes' remains visible, reinforcing the cooking instructions. The person closes the oven door, marking the end of the cooking preparation."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/youcook2/101/0O4bxhpFX9o/split_0.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="95"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">101-vyDO2t9m0H8-split_8</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nShare a detailed breakdown of the video's storyline and visuals."
  },
  {
    "from": "gpt",
    "value": "The video showcases the meticulous preparation of a vegan sandwich, emphasizing a healthy and happy lifestyle. It starts with a close-up of a slice of whole grain bread being spread with a creamy vegan substance, followed by the careful arrangement of cherry tomatoes by a hand with pink-painted fingernails. Adjacent to this slice, another piece of bread is topped with fresh green lettuce leaves. In the background, a turquoise plate with additional food items can be seen. The text 'Healthy. Happy. Life. 100% vegan' and 'lunchboxbunch.com' appears in the top right corner, promoting a vegan lifestyle and the website Lunchbox Bunch. The hand then places grilled or cooked tempeh strips on top of the tomatoes, followed by a generous amount of shredded carrots. The video wraps up with the sandwich nearly complete, as the lettuce-topped slice is poised to be placed on top, finalizing the delicious and nutritious vegan creation."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/youcook2/101/vyDO2t9m0H8/split_8.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="96"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">102-5haTwcEIyE8-split_2</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nCharacterize each scene of the video using a detailed description."
  },
  {
    "from": "gpt",
    "value": "The video begins with a close-up of a wooden cutting board with sliced onions placed on it, while in the background, a person stirs something in a metal bowl. The scene transitions to a closer view of the person's hand holding a whisk, mixing a white batter in a large metal bowl set on a colorful, floral-patterned tablecloth. The person continues to whisk the batter until it is smooth and well-mixed. The next sequence shows the person dipping an onion ring into the batter, coating it thoroughly. The onion ring is then lifted out of the batter, ready for the next step. A plate with breadcrumbs is visible next to the bowl, indicating that the onion ring will likely be coated with breadcrumbs next.\n\nThe video develops through a close-up of a large metal bowl containing white batter and a bowl filled with breadcrumbs, both placed on the same colorful, floral-patterned tablecloth. The hand dips an onion ring into the breadcrumbs, ensuring it is well-coated, and then lifts the coated onion ring out of the breadcrumbs. The scene shifts to a side view of the person placing the coated onion ring onto a baking sheet lined with parchment paper. The final frame of this sequence shows a person in a light pink shirt with dark hair, standing in a room adorned with decorative plates on the wall and a framed picture in the background. The person appears to be explaining or demonstrating something related to the cooking process.\n\nThe video wraps up with a close-up of a large metal bowl containing white batter, with several onion rings submerged in it. A whisk is also visible in the bowl, indicating that the batter is being mixed. The bowl is placed on the same colorful, floral-patterned tablecloth, consistent with the previous scenes. Next to the bowl, a plate filled with breadcrumbs is visible. The whisk moves slightly, suggesting that the batter is being stirred gently. The onion rings remain in the batter throughout this sequence, ready to be coated with breadcrumbs in the next step of the cooking process."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/youcook2/102/5haTwcEIyE8/split_2.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="97"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">102-Gs3OGfQbPjc-split_4</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite an in-depth depiction of the video, covering all its aspects."
  },
  {
    "from": "gpt",
    "value": "The video begins with a close-up of a black frying pan on a gas stove. A hand places a small piece of food into the hot oil, causing bubbles to form around it. The stove's control panel, branded 'Winghouse Silhouette,' shows a white knob set to a low heat setting. The scene develops as a larger, ring-shaped piece of food is added to the pan, also starting to fry with bubbles forming around it. Both pieces of food continue to fry in the bubbling oil. The narrative progresses with the same frying pan on the gas stove, where the small and larger ring-shaped pieces of food are still frying. The control panel remains visible, and a hand with black tongs turns the smaller piece to ensure even cooking. The larger ring-shaped piece begins to turn golden brown as it is also turned. The scene wraps up with both pieces of food frying, the oil bubbling around them, and the larger piece showing a more pronounced golden brown color. The final segment shows the larger ring-shaped piece being lifted out of the oil with black tongs, revealing its golden brown color. The control panel is still visible, and the hand places the ring-shaped piece back into the pan. The oil continues to bubble as the hand uses the tongs to turn the other pieces of food for even cooking. The text 'Tell me your thoughts. Leave me a comment :)' appears at the top of the screen, inviting viewers to share their opinions."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/youcook2/102/Gs3OGfQbPjc/split_4.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="98"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">102-dmJB7NHZ2cI-split_6</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nProvide a comprehensive analysis of the video's content and themes."
  },
  {
    "from": "gpt",
    "value": "The video begins with a close-up of a frying pan filled with oil, where a hand hovers over it, likely checking the temperature. The scene transitions to a plate of freshly fried onion rings, showcasing their golden brown and crispy texture. These onion rings are placed on a white paper towel to absorb any excess oil. The background features a wooden surface, maintaining focus on the onion rings throughout the sequence. The video wraps up with a close-up view of the onion rings, emphasizing their appealing texture and color."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/youcook2/102/dmJB7NHZ2cI/split_6.mp4</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="99"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">103-H_9oM1Y60lU-split_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  {
    "from": "human",
    "value": "&lt;image>\nWrite a comprehensive depiction of the entire video clip."
  },
  {
    "from": "gpt",
    "value": "The video takes place in a vibrant kitchen setting on a television show, featuring a large window with a cityscape view. The kitchen is well-equipped with various cooking utensils and ingredients, creating an inviting atmosphere for a cooking segment. The scene opens with two individuals standing behind a counter filled with cooking items, including a pan on a stove, a plate of buns, various condiments, and a cutting board with green chiles. One person, wearing a white apron with the name 'Harry' embroidered on it, stands next to another person in a maroon shirt who is actively preparing food. The text overlay on the screen reads 'BOBBY'S THROWDOWN' and 'FLAY COOKS GREEN CHILE CHEESEBURGERS,' indicating that the segment is focused on cooking green chile cheeseburgers. \n\nThe person in the maroon shirt is seen chopping green chiles on the cutting board and mixing them in a small bowl, with close-up shots capturing the meticulous preparation. The scene then shifts to a wider shot, showing both individuals as the person in the maroon shirt continues to chop the chiles. Various cooking items, such as a pepper grinder, a bottle of oil, and a jar of honey, are visible on the counter, enhancing the culinary ambiance. The background consistently features the large window with the cityscape view, adding a dynamic backdrop to the cooking process.\n\nAs the segment progresses, the focus remains on the person in the maroon shirt, who is diligently chopping green chiles while the person in the white apron observes. The counter is now shown with additional items, including a pan on the stove with a burger patty cooking, emphasizing the ongoing preparation of the green chile cheeseburgers. The video wraps up with close-up shots of the person in the maroon shirt continuing to chop the chiles, ensuring that all ingredients are perfectly prepared for the cheeseburgers."
  }
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">0_30_s_academic_v0_1</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><span class="">academic_source/youcook2/103/H_9oM1Y60lU/split_1.mp4</span></div></div>
							</td>
					</tr></tbody></table>
		<div class="bg-linear-to-b sticky left-0 border-t border-dashed border-gray-300 from-gray-100 to-white py-3 text-center font-mono text-xs dark:border-gray-700 dark:from-gray-950 dark:to-gray-900">End of preview. <a href="/datasets/lmms-lab/LLaVA-Video-178K/viewer/0_30_s_academic_v0_1/caption" class="group"><span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Expand</span>
						in <svg class="text-lg mr-0.5 inline -translate-y-px text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>Data Studio
					</a></div></div>




			<div class="bg-linear-to-b from-gray-100 to-white dark:from-gray-950 dark:to-gray-900 "><hr class="flex-none -translate-y-px border-t border-dashed border-gray-300 bg-white dark:border-gray-700 dark:bg-gray-950">
					<nav><ul class="flex select-none items-center justify-between space-x-2 text-gray-700 sm:justify-center py-1 text-center font-mono text-xs rounded-b-lg"><li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 pointer-events-none cursor-default text-gray-400 hover:text-gray-700" href=""><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg>
		Previous</a></li>
			<li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1 bg-gray-50 font-semibold ring-1 ring-inset ring-gray-200 dark:bg-gray-900 dark:text-yellow-500 dark:ring-gray-900 hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/lmms-lab/LLaVA-Video-178K/viewer/0_30_s_academic_v0_1/caption?p=0">1</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/lmms-lab/LLaVA-Video-178K/viewer/0_30_s_academic_v0_1/caption?p=1">2</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/lmms-lab/LLaVA-Video-178K/viewer/0_30_s_academic_v0_1/caption?p=2">3</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  pointer-events-none cursor-default" href="#">...</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/lmms-lab/LLaVA-Video-178K/viewer/0_30_s_academic_v0_1/caption?p=119">120</a>
				</li>
			<li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 " href="/datasets/lmms-lab/LLaVA-Video-178K/viewer/0_30_s_academic_v0_1/caption?p=1">Next
		<svg class="ml-1.5 transform rotate-180" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg></a></li></ul></nav></div></div>






</div></div></div></div></div></div></div>

				
					<div class="SVELTE_HYDRATER contents" data-target="RepoCodeCopy" data-props="{}"><div></div></div>
					

					<div class="SVELTE_HYDRATER contents" data-target="SideNavigation" data-props="{&quot;titleTree&quot;:[{&quot;id&quot;:&quot;uses&quot;,&quot;label&quot;:&quot;Uses&quot;,&quot;children&quot;:[{&quot;id&quot;:&quot;data-sources&quot;,&quot;label&quot;:&quot;Data Sources&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Data Sources&quot;},{&quot;id&quot;:&quot;annotation-pipeline&quot;,&quot;label&quot;:&quot;Annotation Pipeline&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Annotation Pipeline&quot;},{&quot;id&quot;:&quot;the-subset-used-in-the-llava-onevision&quot;,&quot;label&quot;:&quot;The subset used in the LLaVA-OneVision&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;The subset used in the LLaVA-OneVision&quot;}],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Uses&quot;},{&quot;id&quot;:&quot;citation&quot;,&quot;label&quot;:&quot;Citation&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Citation&quot;},{&quot;id&quot;:&quot;dataset-card-contact&quot;,&quot;label&quot;:&quot;Dataset Card Contact&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Dataset Card Contact&quot;}],&quot;classNames&quot;:&quot;top-6&quot;}">

<div class="absolute -left-12 bottom-0 top-0 z-10 top-6"><div class="sticky top-4 flex"><div class="h-7 pt-[0.175rem]">
				<span class="peer" tabindex="0"><button class="select-none hover:cursor-pointer"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-lg opacity-80 hover:opacity-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg></button></span>
				<div class="invisible w-0 -translate-x-24 -translate-y-6 overflow-hidden rounded-xl border bg-white transition-transform hover:visible hover:w-52 hover:translate-x-0 peer-focus-within:visible peer-focus-within:w-52 peer-focus-within:translate-x-0"><nav aria-label="Secondary" class="max-h-[550px] overflow-y-auto p-3"><ul><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 *:break-words hover:underline active:text-gray-900 dark:active:text-gray-200" href="#uses" title="Uses"><!-- HTML_TAG_START -->Uses<!-- HTML_TAG_END --></a>
									<ul class="pl-1"><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="#data-sources" title="Data Sources"><!-- HTML_TAG_START -->Data Sources<!-- HTML_TAG_END --></a>
												<ul class="pl-2"></ul>
											</li><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="#annotation-pipeline" title="Annotation Pipeline"><!-- HTML_TAG_START -->Annotation Pipeline<!-- HTML_TAG_END --></a>
												<ul class="pl-2"></ul>
											</li><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="#the-subset-used-in-the-llava-onevision" title="The subset used in the LLaVA-OneVision"><!-- HTML_TAG_START -->The subset used in the LLaVA-OneVision<!-- HTML_TAG_END --></a>
												<ul class="pl-2"></ul>
											</li></ul>
								</li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 *:break-words hover:underline active:text-gray-900 dark:active:text-gray-200" href="#citation" title="Citation"><!-- HTML_TAG_START -->Citation<!-- HTML_TAG_END --></a>
									<ul class="pl-1"></ul>
								</li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 *:break-words hover:underline active:text-gray-900 dark:active:text-gray-200" href="#dataset-card-contact" title="Dataset Card Contact"><!-- HTML_TAG_START -->Dataset Card Contact<!-- HTML_TAG_END --></a>
									<ul class="pl-1"></ul>
								</li></ul></nav></div></div></div></div></div>
					<div class="2xl:pr-6"><div class="prose pl-6 -ml-6 hf-sanitized hf-sanitized-UfqH6tFsNiBUniPeJUP4a">
	<!-- HTML_TAG_START --><h1 class="relative group flex items-center">
	<a rel="nofollow" href="#dataset-card-for-llava-video-178k" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-card-for-llava-video-178k">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Card for LLaVA-Video-178K
	</span>
</h1>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="#uses" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="uses">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Uses
	</span>
</h2>
<p>This dataset is used for the training of the LLaVA-Video model. We only allow the use of this dataset for academic research and education purpose. For OpenAI GPT-4 generated data, we recommend the users to check the <a rel="nofollow" href="https://openai.com/policies/usage-policies/">OpenAI Usage Policy</a>.</p>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="#data-sources" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="data-sources">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Data Sources
	</span>
</h3>
<p>For the training of LLaVA-Video, we utilized video-language data from five primary sources:</p>
<ul>
<li><strong>LLaVA-Video-178K</strong>: This dataset includes <strong>178,510</strong> caption entries, 960,792 open-ended QA (question and answer) items, and 196,198 multiple-choice QA items. These data were newly annotated for this project.<ul>
<li>We include this dataset in this repository: LLaVA-Video-178K/XXX_academic_v0_1 and LLaVA-Video-178K/XXX_youtube_v0_1.</li>
</ul>
</li>
<li><strong>NeXT-QA</strong>: Comprises 17,090 open-ended QA items and 17,024 multiple-choice QA items.<ul>
<li>We include this dataset in this repository: LLaVA-Video-178K/XXX_nextqa.</li>
</ul>
</li>
<li><strong>ActivityNetQA</strong>: Includes 23,530 open-ended QA items,<ul>
<li>We include this dataset in this repository: LLaVA-Video-178K/XXX_activitynetqa.</li>
</ul>
</li>
<li><strong>PerceptionTest</strong>: Includes 1,803 open-ended QA items.<ul>
<li>We include this dataset in this repository: LLaVA-Video-178K/XXX_perceptiontest.</li>
</ul>
</li>
<li><strong>LLaVA-Hound</strong>: Contains 240,000 open-ended QA items and 15,000 caption entries.<ul>
<li>The video data and annotations are available at the following URLs:</li>
<li>Video data: <a rel="nofollow" href="https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction/tree/main/train_300k">train_300k</a></li>
<li>Annotation data: LLaVA-Video-178K/llava_hound</li>
<li>loading function is specified here: <a rel="nofollow" href="https://github.com/LLaVA-VL/LLaVA-NeXT/blob/7125e3654d88063cb467ed242db76f1e2b184d4c/llava/train/train.py#L1162">function</a></li>
</ul>
</li>
</ul>
<p>The <strong>LLaVA-Video-178K</strong> dataset is the only contribution from this repository; we provide additional datasets for reproducing LLaVA-Video.</p>
<ul>
<li><strong>Project Page:</strong> <a rel="nofollow" href="https://llava-vl.github.io/blog/2024-09-30-llava-video/">Project Page</a>.</li>
<li><strong>Paper</strong>: For more details, please check our <a rel="nofollow" href="https://arxiv.org/abs/2410.02713">paper</a></li>
</ul>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="#annotation-pipeline" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="annotation-pipeline">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Annotation Pipeline
	</span>
</h3>
<p>The following directories are provided for generating captions and QA data:</p>
<ul>
<li><strong>Captions</strong>: <code>LLaVA-Video-178K/gpt4o_caption_prompt</code></li>
<li><strong>QA</strong>: <code>LLaVA-Video-178K/gpt4o_qa_prompt</code></li>
</ul>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="#the-subset-used-in-the-llava-onevision" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="the-subset-used-in-the-llava-onevision">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		The subset used in the LLaVA-OneVision
	</span>
</h3>
<p>We have included captions and open-ended questions in the <a rel="nofollow" href="https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K/tree/main/0_30_s_academic_v0_1">0_30_s_academic_v0_1 split</a>, along with 240,000 open-ended QA items and 15,000 caption entries, as part of the video data in LLaVA-Hound for LLaVA-OneVision.</p>
<ul>
<li><a rel="nofollow" href="https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K/blob/main/0_30_s_academic_v0_1/0_30_s_academic_v0_1_cap_processed.json"><strong>0_30_s_academic_v0_1 caption</strong></a></li>
<li><a rel="nofollow" href="https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K/blob/main/0_30_s_academic_v0_1/0_30_s_academic_v0_1_cap_processed.json"><strong>0_30_s_academic_v0_1 open-ended QA</strong></a></li>
<li><strong>LLaVA-Hound</strong>: Same as above.</li>
</ul>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="#citation" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="citation">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Citation
	</span>
</h2>
<pre><code class="language-bibtex">
@misc{zhang2024videoinstructiontuningsynthetic,
    title={Video Instruction Tuning With Synthetic Data}, 
    author={Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li},
    year={2024},
    eprint={2410.02713},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2410.02713}, 
}
</code></pre>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="#dataset-card-contact" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-card-contact">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Card Contact
	</span>
</h2>
<p><a rel="nofollow" href="https://zhangyuanhan-ai.github.io/">Yuanhan Zhang</a></p>
<p><a rel="nofollow" href="https://scholar.google.com/citations?user=eh-XJIoAAAAJ&amp;hl=zh-CN">Jinming Wu</a></p>
<p><a rel="nofollow" href="https://scholar.google.com/citations?user=q8ZrKVIAAAAJ&amp;hl=zh-CN">Wei Li</a></p>
<!-- HTML_TAG_END --></div>
</div></section>
			
			<section class="pt-6 border-gray-100 md:pb-24 md:pl-6 md:w-64 lg:w-80 xl:w-96 flex-none order-first md:order-none md:border-l pt-3! md:pt-6!"><dl class="flex items-baseline justify-between"><dt class="text-sm text-gray-500">Downloads last month</dt><div class="mx-4 flex-1 border-b border-dotted"></div><dd class="font-semibold">19,454</dd></dl>
				
				<div class="divider-column-vertical"></div>
				<div class="grid grid-cols-2 gap-x-2 md:flex md:flex-row md:flex-wrap"><div class="SVELTE_HYDRATER contents" data-target="DatasetAndModelActionsDropdown" data-props="{&quot;classNames&quot;:&quot;order-last&quot;,&quot;discussionsDisabled&quot;:false,&quot;repo&quot;:{&quot;type&quot;:&quot;dataset&quot;,&quot;name&quot;:&quot;lmms-lab/LLaVA-Video-178K&quot;},&quot;canWrite&quot;:false,&quot;canDisable&quot;:false,&quot;repoIsPrivate&quot;:false,&quot;repoIsGated&quot;:false,&quot;repoIsDisabled&quot;:false,&quot;repoIsAdminFlaggedNFAA&quot;:false,&quot;repoHasBlockedOids&quot;:false}"><div class="order-last"><div class="relative ">
	<button class="btn px-1.5 py-1.5 " type="button">
		
			<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="p-0.5" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><circle cx="16" cy="7" r="3" fill="currentColor"></circle><circle cx="16" cy="16" r="3" fill="currentColor"></circle><circle cx="16" cy="25" r="3" fill="currentColor"></circle></svg>
		
		</button>
	
	
	</div></div>













</div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetLibrary" data-props="{&quot;classNames&quot;:&quot;md:w-full xl:w-auto xl:flex-none&quot;}">

</div>
					
					</div>
				<div class="divider-column-vertical"></div>
					<div class="flex flex-col flex-wrap xl:flex-row"><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Curated by:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->Yuanhan Zhang, Jinming Wu, Wei Li<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Language(s) (NLP):</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->English, Chinese<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">License:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->Apache License 2.0<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 " href="/datasets/lmms-lab/LLaVA-Video-178K/tree/refs%2Fconvert%2Fparquet/" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of the auto-converted Parquet files:</div>
		<div class="truncate text-sm group-hover:underline">
			<!-- HTML_TAG_START -->645 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Number of rows:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->1,627,017<!-- HTML_TAG_END --></div></a></div>
				
				
				<div class="divider-column-vertical"></div>
					<h2 class="text-smd mb-5 flex items-baseline overflow-hidden whitespace-nowrap font-semibold text-gray-800"><svg class="mr-1.5 text-sm inline self-center flex-none text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
						Models trained or fine-tuned on
						<span class="ml-1 truncate font-mono text-[0.87rem] font-medium">lmms-lab/LLaVA-Video-178K</span></h2>

					<div class="space-y-3"><div class=""><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/HuggingFaceTB/SmolVLM2-2.2B-Instruct"><div class="w-full truncate"><header class="flex items-center mb-1" title="HuggingFaceTB/SmolVLM2-2.2B-Instruct"><img alt="" class="w-3 h-3 rounded-xs mr-1.5 flex-none flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/e4VK7uW5sTeCYupD0s_ob.png" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">HuggingFaceTB/SmolVLM2-2.2B-Instruct</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400"><svg class="mr-1.5 text-[.8rem] flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29.707 19.293l-3-3a1 1 0 0 0-1.414 0L16 25.586V30h4.414l9.293-9.293a1 1 0 0 0 0-1.414zM19.586 28H18v-1.586l5-5L24.586 23zM26 21.586L24.414 20L26 18.414L27.586 20z" fill="currentColor"></path><path d="M20 13v-2h-2.142a3.94 3.94 0 0 0-.425-1.019l1.517-1.517l-1.414-1.414l-1.517 1.517A3.944 3.944 0 0 0 15 8.142V6h-2v2.142a3.944 3.944 0 0 0-1.019.425L10.464 7.05L9.05 8.464l1.517 1.517A3.94 3.94 0 0 0 10.142 11H8v2h2.142a3.94 3.94 0 0 0 .425 1.019L9.05 15.536l1.414 1.414l1.517-1.517a3.944 3.944 0 0 0 1.019.425V18h2v-2.142a3.944 3.944 0 0 0 1.019-.425l1.517 1.517l1.414-1.414l-1.517-1.517A3.94 3.94 0 0 0 17.858 13zm-6 1a2 2 0 1 1 2-2a2.002 2.002 0 0 1-2 2z" fill="currentColor"></path><path d="M12 30H6a2.002 2.002 0 0 1-2-2V4a2.002 2.002 0 0 1 2-2h16a2.002 2.002 0 0 1 2 2v10h-2V4H6v24h6z" fill="currentColor"></path></svg>
			Image-Text-to-Text
			<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
	
				<span class="truncate">Updated
					<time datetime="2025-03-06T13:44:39" title="Thu, 06 Mar 2025 13:44:39 GMT">19 days ago</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					509k
				
	
				<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
					<svg class="flex-none w-3 text-gray-400 mr-1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>
					131

				</div></div>
		
	</a></article>
							</div><div class=""><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/lmms-lab/LLaVA-Video-7B-Qwen2"><div class="w-full truncate"><header class="flex items-center mb-1" title="lmms-lab/LLaVA-Video-7B-Qwen2"><img alt="" class="w-3 h-3 rounded-xs mr-1.5 flex-none flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/62d3f7d84b0933c48f3cdd9c/mCBOS7IodEEgBweo19hU9.png" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">lmms-lab/LLaVA-Video-7B-Qwen2</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 32 32" aria-hidden="true" class="mr-1.5 text-[.8rem] flex-none"><path fill="currentColor" d="m30 19l-4 3.2V20a2 2 0 0 0-2-2h-8a2 2 0 0 0-2 2v6a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2v-2.2l4 3.2Zm-14 7v-6h8l.002 6Z"></path><path fill="currentColor" d="M12 28H8V4h8v6a2.006 2.006 0 0 0 2 2h6v3h2v-5a.91.91 0 0 0-.3-.7l-7-7A.9.9 0 0 0 18 2H8a2.006 2.006 0 0 0-2 2v24a2.006 2.006 0 0 0 2 2h4Zm6-23.6l5.6 5.6H18Z"></path></svg>
			Video-Text-to-Text
			<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
	
				<span class="truncate">Updated
					<time datetime="2024-10-25T01:38:23" title="Fri, 25 Oct 2024 01:38:23 GMT">Oct 25, 2024</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					46.1k
				
	
				<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
					<svg class="flex-none w-3 text-gray-400 mr-1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>
					86

				</div></div>
		
	</a></article>
							</div><div class=""><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/DAMO-NLP-SG/VideoLLaMA3-7B"><div class="w-full truncate"><header class="flex items-center mb-1" title="DAMO-NLP-SG/VideoLLaMA3-7B"><img alt="" class="w-3 h-3 rounded-xs mr-1.5 flex-none flex-none" src="https://www.gravatar.com/avatar/5d4eecbbbcd85556d44099e1684219cf?d=retro&amp;size=100" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">DAMO-NLP-SG/VideoLLaMA3-7B</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" width="1em" height="1em" viewBox="0 0 32 32" class="mr-1.5 text-[.8rem] flex-none"><path fill="currentColor" d="M10 16a5.99 5.99 0 0 0 9.471 4.885L24.586 26L26 24.586l-5.115-5.115A5.997 5.997 0 1 0 10 16m2 0a4 4 0 1 1 4 4a4.005 4.005 0 0 1-4-4"></path><path fill="currentColor" d="M29 7h-6.46l-1.71-2.55A1 1 0 0 0 20 4h-8a1 1 0 0 0-.83.45L9.46 7H3a1.003 1.003 0 0 0-1 1v17a1.003 1.003 0 0 0 1 1h9v-2H4V9h6a1 1 0 0 0 .83-.45L12.54 6h6.92l1.71 2.55A1 1 0 0 0 22 9h6v12h2V8a1.003 1.003 0 0 0-1-1"></path></svg>
			Visual Question Answering
			<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
	
				<span class="truncate">Updated
					<time datetime="2025-03-20T04:07:45" title="Thu, 20 Mar 2025 04:07:45 GMT">5 days ago</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					25k
				
	
				<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
					<svg class="flex-none w-3 text-gray-400 mr-1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>
					40

				</div></div>
		
	</a></article>
							</div><div class=""><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/HuggingFaceTB/SmolVLM2-500M-Video-Instruct"><div class="w-full truncate"><header class="flex items-center mb-1" title="HuggingFaceTB/SmolVLM2-500M-Video-Instruct"><img alt="" class="w-3 h-3 rounded-xs mr-1.5 flex-none flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/e4VK7uW5sTeCYupD0s_ob.png" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">HuggingFaceTB/SmolVLM2-500M-Video-Instruct</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400"><svg class="mr-1.5 text-[.8rem] flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29.707 19.293l-3-3a1 1 0 0 0-1.414 0L16 25.586V30h4.414l9.293-9.293a1 1 0 0 0 0-1.414zM19.586 28H18v-1.586l5-5L24.586 23zM26 21.586L24.414 20L26 18.414L27.586 20z" fill="currentColor"></path><path d="M20 13v-2h-2.142a3.94 3.94 0 0 0-.425-1.019l1.517-1.517l-1.414-1.414l-1.517 1.517A3.944 3.944 0 0 0 15 8.142V6h-2v2.142a3.944 3.944 0 0 0-1.019.425L10.464 7.05L9.05 8.464l1.517 1.517A3.94 3.94 0 0 0 10.142 11H8v2h2.142a3.94 3.94 0 0 0 .425 1.019L9.05 15.536l1.414 1.414l1.517-1.517a3.944 3.944 0 0 0 1.019.425V18h2v-2.142a3.944 3.944 0 0 0 1.019-.425l1.517 1.517l1.414-1.414l-1.517-1.517A3.94 3.94 0 0 0 17.858 13zm-6 1a2 2 0 1 1 2-2a2.002 2.002 0 0 1-2 2z" fill="currentColor"></path><path d="M12 30H6a2.002 2.002 0 0 1-2-2V4a2.002 2.002 0 0 1 2-2h16a2.002 2.002 0 0 1 2 2v10h-2V4H6v24h6z" fill="currentColor"></path></svg>
			Image-Text-to-Text
			<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
	
				<span class="truncate">Updated
					<time datetime="2025-03-06T13:45:03" title="Thu, 06 Mar 2025 13:45:03 GMT">19 days ago</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					10.6k
				
	
				<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
					<svg class="flex-none w-3 text-gray-400 mr-1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>
					48

				</div></div>
		
	</a></article>
							</div><div class="hidden md:block"><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/DAMO-NLP-SG/VideoLLaMA3-7B-Image"><div class="w-full truncate"><header class="flex items-center mb-1" title="DAMO-NLP-SG/VideoLLaMA3-7B-Image"><img alt="" class="w-3 h-3 rounded-xs mr-1.5 flex-none flex-none" src="https://www.gravatar.com/avatar/5d4eecbbbcd85556d44099e1684219cf?d=retro&amp;size=100" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">DAMO-NLP-SG/VideoLLaMA3-7B-Image</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" width="1em" height="1em" viewBox="0 0 32 32" class="mr-1.5 text-[.8rem] flex-none"><path fill="currentColor" d="M10 16a5.99 5.99 0 0 0 9.471 4.885L24.586 26L26 24.586l-5.115-5.115A5.997 5.997 0 1 0 10 16m2 0a4 4 0 1 1 4 4a4.005 4.005 0 0 1-4-4"></path><path fill="currentColor" d="M29 7h-6.46l-1.71-2.55A1 1 0 0 0 20 4h-8a1 1 0 0 0-.83.45L9.46 7H3a1.003 1.003 0 0 0-1 1v17a1.003 1.003 0 0 0 1 1h9v-2H4V9h6a1 1 0 0 0 .83-.45L12.54 6h6.92l1.71 2.55A1 1 0 0 0 22 9h6v12h2V8a1.003 1.003 0 0 0-1-1"></path></svg>
			Visual Question Answering
			<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
	
				<span class="truncate">Updated
					<time datetime="2025-03-20T04:08:44" title="Thu, 20 Mar 2025 04:08:44 GMT">5 days ago</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					6.65k
				
	
				<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
					<svg class="flex-none w-3 text-gray-400 mr-1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>
					10

				</div></div>
		
	</a></article>
							</div><div class="hidden md:block"><article class="overview-card-wrapper group/repo  "><a class="flex items-center justify-between gap-4 p-2" href="/HuggingFaceTB/SmolVLM2-256M-Video-Instruct"><div class="w-full truncate"><header class="flex items-center mb-1" title="HuggingFaceTB/SmolVLM2-256M-Video-Instruct"><img alt="" class="w-3 h-3 rounded-xs mr-1.5 flex-none flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/e4VK7uW5sTeCYupD0s_ob.png" crossorigin="anonymous">
				<h4 class="text-md truncate font-mono text-black dark:group-hover/repo:text-yellow-500 group-hover/repo:text-indigo-600 text-sm">HuggingFaceTB/SmolVLM2-256M-Video-Instruct</h4>
				
				</header>
			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400"><svg class="mr-1.5 text-[.8rem] flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29.707 19.293l-3-3a1 1 0 0 0-1.414 0L16 25.586V30h4.414l9.293-9.293a1 1 0 0 0 0-1.414zM19.586 28H18v-1.586l5-5L24.586 23zM26 21.586L24.414 20L26 18.414L27.586 20z" fill="currentColor"></path><path d="M20 13v-2h-2.142a3.94 3.94 0 0 0-.425-1.019l1.517-1.517l-1.414-1.414l-1.517 1.517A3.944 3.944 0 0 0 15 8.142V6h-2v2.142a3.944 3.944 0 0 0-1.019.425L10.464 7.05L9.05 8.464l1.517 1.517A3.94 3.94 0 0 0 10.142 11H8v2h2.142a3.94 3.94 0 0 0 .425 1.019L9.05 15.536l1.414 1.414l1.517-1.517a3.944 3.944 0 0 0 1.019.425V18h2v-2.142a3.944 3.944 0 0 0 1.019-.425l1.517 1.517l1.414-1.414l-1.517-1.517A3.94 3.94 0 0 0 17.858 13zm-6 1a2 2 0 1 1 2-2a2.002 2.002 0 0 1-2 2z" fill="currentColor"></path><path d="M12 30H6a2.002 2.002 0 0 1-2-2V4a2.002 2.002 0 0 1 2-2h16a2.002 2.002 0 0 1 2 2v10h-2V4H6v24h6z" fill="currentColor"></path></svg>
			Image-Text-to-Text
			<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
	
				<span class="truncate">Updated
					<time datetime="2025-03-06T13:46:08" title="Thu, 06 Mar 2025 13:46:08 GMT">19 days ago</time></span>
				
				<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
					<svg class="flex-none w-3 text-gray-400 mr-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg>
					6.3k
				
	
				<span class="px-1.5 text-gray-300 dark:text-gray-500">• </span>
					<svg class="flex-none w-3 text-gray-400 mr-1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>
					43

				</div></div>
		
	</a></article>
							</div>
						<a class="flex items-center pt-1 text-sm text-gray-500 underline hover:text-gray-700" href="/models?dataset=dataset:lmms-lab/LLaVA-Video-178K">Browse 14 models trained on this dataset
							</a></div>
				

				<div class="divider-column-vertical"></div>
					<h2 class="text-smd mb-5 flex items-baseline overflow-hidden whitespace-nowrap font-semibold text-gray-800"><svg class="mr-1 inline self-center flex-none" width="1em" height="1em" aria-hidden="true" focusable="false" role="img" viewBox="0 0 12 13" fill="none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><rect x="2" y="2.49902" width="8" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.4"></rect><rect x="6.21875" y="6.7334" width="3.78055" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.7"></rect><rect x="2" y="6.73438" width="3.78055" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.5"></rect></svg>
						Collection including
						<span class="ml-1 truncate font-mono text-[0.86rem] font-medium">lmms-lab/LLaVA-Video-178K</span></h2>
					<div class="flex flex-col gap-3">

<div class="flex flex-col"><article class="overview-card-wrapper group/collection rounded-md! from-white! to-white! dark:from-gray-900! dark:to-gray-900! relative  peer"><a href="/collections/lmms-lab/llava-video-661e86f5e8dabc3ff793c944" class="block p-2"><header class="mb-0.5 flex items-center" title="LLaVA-Video"><h4 class="text-md text-smd truncate font-semibold group-hover/collection:underline">LLaVA-Video</h4>
				<div class="ml-2 flex items-center rounded-sm sm:ml-2.5 bg-blue-500/10 py-0.5 pl-1 pr-1.5 text-xs leading-none text-gray-700"><svg class="mr-0.5 flex-none text-blue-700" width="1em" height="1em" aria-hidden="true" focusable="false" role="img" viewBox="0 0 12 13" fill="none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><rect x="2" y="2.49902" width="8" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.4"></rect><rect x="6.21875" y="6.7334" width="3.78055" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.7"></rect><rect x="2" y="6.73438" width="3.78055" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.5"></rect></svg>
					Collection
				</div></header>

			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400"><span class="max-w-64 truncate">Models focus on video understanding (previously known as LLaVA-NeXT-Video).</span>
					<span class="px-1.5 text-gray-300">• </span>
				<span>8 items</span>
				<span class="px-1.5 text-gray-300">• </span>
				<span class="truncate">Updated
					<time datetime="2025-02-21T14:51:44" title="2025-02-21T14:51:44.965Z">Feb 21</time></span>
				<span class="px-1.5 text-gray-300">•</span>

					<svg class="flex-none w-3 mr-1 text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12" fill="transparent"><path d="M9.30013 9.29152H9.3H2.7H2.69987C2.62308 9.29154 2.54762 9.27146 2.481 9.23328C2.41437 9.1951 2.3589 9.14015 2.32009 9.07389C2.28128 9.00763 2.26048 8.93237 2.25977 8.85558C2.25907 8.7798 2.27796 8.70513 2.31458 8.63882L5.62238 2.9426L5.67518 2.85168C5.7059 2.81806 5.74178 2.78928 5.78164 2.76649C5.84813 2.72848 5.9234 2.70848 6 2.70848C6.0766 2.70848 6.15187 2.72848 6.21836 2.76649C6.28441 2.80425 6.33953 2.85848 6.37836 2.92389L9.68527 8.63855C9.72199 8.70493 9.74093 8.7797 9.74023 8.85558C9.73952 8.93237 9.71872 9.00763 9.67991 9.07389C9.6411 9.14015 9.58563 9.1951 9.519 9.23328C9.45238 9.27146 9.37692 9.29154 9.30013 9.29152Z" stroke="currentColor"></path></svg>

					61</div></a></article>
	<div class="overview-card-wrapper rounded-md! bg-linear-to-t! -z-1  from-gray-50! to-gray-50! shadow-none! dark:from-gray-925! dark:to-gray-925! -mt-[1.60rem] h-8 scale-x-[98%] transition-transform peer-hover:-translate-y-[2px] peer-hover:from-gray-100/80 dark:peer-hover:from-black"></div></div></div>
				<div class="divider-column-vertical md:hidden"></div></section></div></main>

	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container relative flex flex-col justify-between space-y-2 py-6 text-gray-500 max-md:*:self-start md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="SVELTE_HYDRATER contents" data-target="ThemeSwitcher" data-props="{&quot;theme&quot;:&quot;system&quot;,&quot;isLoggedIn&quot;:false,&quot;menuClassNames&quot;:&quot;md:-top-24&quot;,&quot;classNames&quot;:&quot;max-md:mb-5 max-md:*:self-start&quot;}">
<div class="relative inline-block max-md:mb-5 max-md:*:self-start">
	<button class="rounded-full border border-gray-100 pl-2 py-1 pr-2.5  flex items-center text-sm text-gray-500 bg-white hover:bg-purple-50 hover:border-purple-200 dark:hover:bg-gray-800 dark:hover:border-gray-950 dark:border-gray-800 " type="button">
		<svg class="mr-1.5 text-gray-500" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M6 22.5h20a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2H6a2 2 0 0 0-2 2v13.5a2 2 0 0 0 2 2ZM7 7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h18a1 1 0 0 0 1-1V8a1 1 0 0 0-1-1H7Z" fill="currentColor"></path><path d="M6 8a1 1 0 0 1 1-1h18a1 1 0 0 1 1 1v11a1 1 0 0 1-1 1H7a1 1 0 0 1-1-1V8Z" fill="currentColor" fill-opacity=".4"></path><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path></svg>
			System theme
		</button>
	
	
	</div></div>
		<div class="font-semibold text-black md:hidden">Company</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="max-md:mb-4! max-md:mt-8! group flex-none max-md:order-last"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="max-md:mt-8! font-semibold text-black md:hidden">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("\/front\/build\/kube-68d7aa0\/index.js");
			window.moonSha = "kube-68d7aa0\/";
			window.__hf_deferred = {};
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>
	</body>
</html>
