<!doctype html>
<html class="">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
		<meta name="description" content="Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science." />
		<meta property="fb:app_id" content="1321688464574422" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@huggingface" />
		<meta name="twitter:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/scim/naacl_data_prog.png" />
		<meta property="og:title" content="scim/naacl_data_prog Â· Datasets at Hugging Face" />
		<meta property="og:type" content="website" />
		<meta property="og:url" content="https://huggingface.co/datasets/scim/naacl_data_prog" />
		<meta property="og:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/scim/naacl_data_prog.png" />

		<link rel="stylesheet" href="/front/build/kube-68d7aa0/style.css" />

		<link rel="preconnect" href="https://fonts.gstatic.com" />
		<link
			href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap"
			rel="stylesheet"
		/>

		<link
			rel="preload"
			href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css"
			as="style"
			onload="this.onload=null;this.rel='stylesheet'"
		/>
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		<script>const guestTheme = document.cookie.match(/theme=(\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches));</script>
<link rel="canonical" href="https://huggingface.co/datasets/scim/naacl_data_prog"> <script type="application/ld+json">{
  "@context": {
    "@language": "en",
    "@vocab": "https:\/\/schema.org\/",
    "citeAs": "cr:citeAs",
    "column": "cr:column",
    "conformsTo": "dct:conformsTo",
    "cr": "http:\/\/mlcommons.org\/croissant\/",
    "data": {
      "@id": "cr:data",
      "@type": "@json"
    },
    "dataBiases": "cr:dataBiases",
    "dataCollection": "cr:dataCollection",
    "dataType": {
      "@id": "cr:dataType",
      "@type": "@vocab"
    },
    "dct": "http:\/\/purl.org\/dc\/terms\/",
    "extract": "cr:extract",
    "field": "cr:field",
    "fileProperty": "cr:fileProperty",
    "fileObject": "cr:fileObject",
    "fileSet": "cr:fileSet",
    "format": "cr:format",
    "includes": "cr:includes",
    "isLiveDataset": "cr:isLiveDataset",
    "jsonPath": "cr:jsonPath",
    "key": "cr:key",
    "md5": "cr:md5",
    "parentField": "cr:parentField",
    "path": "cr:path",
    "personalSensitiveInformation": "cr:personalSensitiveInformation",
    "recordSet": "cr:recordSet",
    "references": "cr:references",
    "regex": "cr:regex",
    "repeated": "cr:repeated",
    "replace": "cr:replace",
    "sc": "https:\/\/schema.org\/",
    "separator": "cr:separator",
    "source": "cr:source",
    "subField": "cr:subField",
    "transform": "cr:transform"
  },
  "@type": "sc:Dataset",
  "distribution": [
    {
      "@type": "cr:FileObject",
      "@id": "repo",
      "name": "repo",
      "description": "The Hugging Face git repository.",
      "contentUrl": "https:\/\/huggingface.co\/datasets\/scim\/naacl_data_prog\/tree\/refs%2Fconvert%2Fparquet",
      "encodingFormat": "git+https",
      "sha256": "https:\/\/github.com\/mlcommons\/croissant\/issues\/80"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-default",
      "name": "parquet-files-for-config-default",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https:\/\/huggingface.co\/docs\/dataset-viewer\/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application\/x-parquet",
      "includes": "default\/*\/*.parquet"
    }
  ],
  "recordSet": [
    {
      "@type": "cr:RecordSet",
      "dataType": "cr:Split",
      "key": {
        "@id": "default_splits\/split_name"
      },
      "@id": "default_splits",
      "name": "default_splits",
      "description": "Splits for the default config.",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default_splits\/split_name",
          "name": "split_name",
          "description": "The name of the split.",
          "dataType": "sc:Text"
        }
      ],
      "data": [
        {
          "default_splits\/split_name": "train"
        }
      ]
    },
    {
      "@type": "cr:RecordSet",
      "@id": "default",
      "name": "default",
      "description": "scim\/naacl_data_prog - 'default' subset",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default\/split",
          "name": "default\/split",
          "description": "Split to which the example belongs to.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "fileProperty": "fullpath"
            },
            "transform": {
              "regex": "default\/(?:partial-)?(train)\/.+parquet$"
            }
          },
          "references": {
            "field": {
              "@id": "default_splits\/split_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default\/sentences",
          "name": "default\/sentences",
          "description": "Column 'sentences' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "sentences"
            }
          },
          "repeated": true
        },
        {
          "@type": "cr:Field",
          "@id": "default\/labels",
          "name": "default\/labels",
          "description": "Column 'labels' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "labels"
            }
          },
          "repeated": true
        }
      ]
    }
  ],
  "conformsTo": "http:\/\/mlcommons.org\/croissant\/1.0",
  "name": "naacl_data_prog",
  "description": "scim\/naacl_data_prog dataset hosted on Hugging Face and contributed by the HF Datasets community",
  "alternateName": [
    "scim\/naacl_data_prog"
  ],
  "creator": {
    "@type": "Organization",
    "name": "SCIM Project",
    "url": "https:\/\/huggingface.co\/scim"
  },
  "keywords": [
    "1K - 10K",
    "parquet",
    "Text",
    "Datasets",
    "pandas",
    "Croissant",
    "Polars",
    "ðŸ‡ºðŸ‡¸ Region: US"
  ],
  "url": "https:\/\/huggingface.co\/datasets\/scim\/naacl_data_prog"
}</script> 

		<title>scim/naacl_data_prog Â· Datasets at Hugging Face</title>

		<script
			defer
			data-domain="huggingface.co"
			event-loggedIn="false"
			src="/js/script.pageview-props.js"
		></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script>
			window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer></script>
	</head>
	<body class="flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DatasetPage">
		<div class="flex min-h-dvh flex-col"><div class="SVELTE_HYDRATER contents" data-target="SystemThemeMonitor" data-props="{&quot;isLoggedIn&quot;:false}"></div>

	<div class="SVELTE_HYDRATER contents" data-target="MainHeader" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false,&quot;isPro&quot;:false}"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg">
				<span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a>
			<div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..."   spellcheck="false" type="text" value="">
	<svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
	</div>
			<div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg>
		</button>

	</div></div>
		<nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 2xl:space-x-2"><li class="hover:text-indigo-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
					Models</a>
			</li><li class="hover:text-red-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					Datasets</a>
			</li><li class="hover:text-blue-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg>
					Spaces</a>
			</li><li class="hover:text-yellow-700 max-xl:hidden"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 text-yellow-500!" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg>
					Posts</a>
			</li><li class="hover:text-yellow-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 16 16"><path d="m2.28 3.7-.3.16a.67.67 0 0 0-.34.58v8.73l.01.04.02.07.01.04.03.06.02.04.02.03.04.06.05.05.04.04.06.04.06.04.08.04.08.02h.05l.07.02h.11l.04-.01.07-.02.03-.01.07-.03.22-.12a5.33 5.33 0 0 1 5.15.1.67.67 0 0 0 .66 0 5.33 5.33 0 0 1 5.33 0 .67.67 0 0 0 1-.58V4.36a.67.67 0 0 0-.34-.5l-.3-.17v7.78a.63.63 0 0 1-.87.59 4.9 4.9 0 0 0-4.35.35l-.65.39a.29.29 0 0 1-.15.04.29.29 0 0 1-.16-.04l-.65-.4a4.9 4.9 0 0 0-4.34-.34.63.63 0 0 1-.87-.59V3.7Z" fill="currentColor" class="dark:opacity-40"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M8 3.1a5.99 5.99 0 0 0-5.3-.43.66.66 0 0 0-.42.62v8.18c0 .45.46.76.87.59a4.9 4.9 0 0 1 4.34.35l.65.39c.05.03.1.04.16.04.05 0 .1-.01.15-.04l.65-.4a4.9 4.9 0 0 1 4.35-.34.63.63 0 0 0 .86-.59V3.3a.67.67 0 0 0-.41-.62 5.99 5.99 0 0 0-5.3.43l-.3.17L8 3.1Zm.73 1.87a.43.43 0 1 0-.86 0v5.48a.43.43 0 0 0 .86 0V4.97Z" fill="currentColor" class="opacity-40 dark:opacity-100"></path><path d="M8.73 4.97a.43.43 0 1 0-.86 0v5.48a.43.43 0 1 0 .86 0V4.96Z" fill="currentColor" class="dark:opacity-40"></path></svg>
					Docs</a>
			</li><li class="hover:text-black dark:hover:text-white"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/enterprise"><svg class="mr-1.5 text-gray-400 group-hover:text-black dark:group-hover:text-white" xmlns="http://www.w3.org/2000/svg" fill="none" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 33 27"><path fill="currentColor" fill-rule="evenodd" d="M13.5.7a8.7 8.7 0 0 0-7.7 5.7L1 20.6c-1 3.1.9 5.7 4.1 5.7h15c3.3 0 6.8-2.6 7.8-5.7l4.6-14.2c1-3.1-.8-5.7-4-5.7h-15Zm1.1 5.7L9.8 20.3h9.8l1-3.1h-5.8l.8-2.5h4.8l1.1-3h-4.8l.8-2.3H23l1-3h-9.5Z" clip-rule="evenodd"></path></svg>
					Enterprise</a>
			</li>

		<li><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/pricing">Pricing
			</a></li>

		<li><div class="relative group">
	<button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button">
		<svg class=" text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>
			
		</button>
	
	
	</div></li>
		<li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li>
		<li><a class="block cursor-pointer whitespace-nowrap px-2 py-0.5 hover:text-gray-500 dark:text-gray-300 dark:hover:text-gray-100" href="/login">Log In
				</a></li>
			<li><a class="whitespace-nowrap rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up
					</a></li></ul></nav></div></header></div>
	
	
	
	<div class="SVELTE_HYDRATER contents" data-target="SSOBanner" data-props="{}"></div>
	
	

	<main class="flex flex-1 flex-col">
	<div class="SVELTE_HYDRATER contents" data-target="DatasetHeader" data-props="{&quot;activeTab&quot;:&quot;datasetCard&quot;,&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://www.gravatar.com/avatar/b044ed6f48fb9b7eb557ea402e28e44a?d=retro&amp;size=100&quot;,&quot;fullname&quot;:&quot;SCIM Project&quot;,&quot;name&quot;:&quot;scim&quot;,&quot;type&quot;:&quot;org&quot;,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;isEnterprise&quot;:false,&quot;followerCount&quot;:3},&quot;canReadRepoSettings&quot;:false,&quot;dataset&quot;:{&quot;author&quot;:&quot;scim&quot;,&quot;createdAt&quot;:&quot;2022-12-09T05:57:56.000Z&quot;,&quot;downloads&quot;:53,&quot;downloadsAllTime&quot;:198,&quot;id&quot;:&quot;scim/naacl_data_prog&quot;,&quot;isLikedByUser&quot;:false,&quot;lastModified&quot;:&quot;2022-12-09T05:59:46.000Z&quot;,&quot;likes&quot;:3,&quot;datasetsServerInfo&quot;:{&quot;viewer&quot;:&quot;viewer&quot;,&quot;numRows&quot;:3051,&quot;libraries&quot;:[&quot;datasets&quot;,&quot;pandas&quot;,&quot;mlcroissant&quot;,&quot;polars&quot;],&quot;formats&quot;:[&quot;parquet&quot;],&quot;modalities&quot;:[&quot;text&quot;]},&quot;discussionsDisabled&quot;:false,&quot;repoType&quot;:&quot;dataset&quot;,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;tags&quot;:[&quot;size_categories:1K<n<10K&quot;,&quot;format:parquet&quot;,&quot;modality:text&quot;,&quot;library:datasets&quot;,&quot;library:pandas&quot;,&quot;library:mlcroissant&quot;,&quot;library:polars&quot;,&quot;region:us&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;size_categories:1K<n<10K&quot;,&quot;label&quot;:&quot;1K - 10K&quot;,&quot;type&quot;:&quot;size_categories&quot;},{&quot;id&quot;:&quot;format:parquet&quot;,&quot;label&quot;:&quot;parquet&quot;,&quot;type&quot;:&quot;format&quot;},{&quot;id&quot;:&quot;modality:text&quot;,&quot;label&quot;:&quot;Text&quot;,&quot;type&quot;:&quot;modality&quot;},{&quot;id&quot;:&quot;library:datasets&quot;,&quot;label&quot;:&quot;Datasets&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:pandas&quot;,&quot;label&quot;:&quot;pandas&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:mlcroissant&quot;,&quot;label&quot;:&quot;Croissant&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;library:polars&quot;,&quot;label&quot;:&quot;Polars&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;ðŸ‡ºðŸ‡¸ Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;}],&quot;hasBlockedOids&quot;:false,&quot;region&quot;:&quot;us&quot;,&quot;xetEnabled&quot;:false},&quot;discussionsStats&quot;:{&quot;closed&quot;:0,&quot;open&quot;:0,&quot;total&quot;:0}}"><header class="bg-linear-to-t border-b border-gray-100 pt-6 sm:pt-9 from-gray-50-to-white via-white dark:via-gray-950"><div class="container relative "><h1 class="flex flex-wrap items-center max-md:leading-tight mb-3 text-lg max-sm:gap-y-1.5 md:text-xl"><a href="/datasets" class="group flex items-center"><svg class="sm:mr-1.5 -mr-1 text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					<span class="mr-2.5 font-semibold text-gray-400 group-hover:text-gray-500 max-sm:hidden">Datasets:</span></a>
				<hr class="mx-1.5 h-2 translate-y-px rounded-sm border-r dark:border-gray-600 sm:hidden">
			<div class="group flex flex-none items-center"><div class="relative mr-1 flex items-center">

			

<span class="inline-block "><span class="contents"><a href="/scim" class="text-gray-400 hover:text-blue-600"><img alt="" class="w-3.5 h-3.5 rounded-sm  flex-none" src="https://www.gravatar.com/avatar/b044ed6f48fb9b7eb557ea402e28e44a?d=retro&amp;size=100" crossorigin="anonymous"></a></span>
	</span></div>
		

<span class="inline-block "><span class="contents"><a href="/scim" class="text-gray-400 hover:text-blue-600">scim</a></span>
	</span>
		<div class="mx-0.5 text-gray-300">/</div></div>

<div class="max-w-full "><a class="break-words font-mono font-semibold hover:text-blue-600 " href="/datasets/scim/naacl_data_prog">naacl_data_prog</a>
	<button class="relative text-sm mr-4 focus:outline-hidden inline-flex cursor-pointer items-center text-sm  mx-0.5   text-gray-600 " title="Copy dataset name to clipboard" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg>
	
	</button></div>
			<div class="inline-flex items-center overflow-hidden whitespace-nowrap rounded-md border bg-white text-sm leading-none text-gray-500  mr-2"><button class="relative flex items-center overflow-hidden from-red-50 to-transparent dark:from-red-900 px-1.5 py-1 hover:bg-linear-to-t focus:outline-hidden"  title="Like"><svg class="left-1.5 absolute" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg>

		
		<span class="ml-4 pl-0.5 ">like</span></button>
	<button class="focus:outline-hidden flex items-center border-l px-1.5 py-1 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="See users who liked this repository">3</button></div>




			<div class="relative flex items-center gap-1.5  "><div class="mr-2 inline-flex h-6 items-center overflow-hidden whitespace-nowrap rounded-md border text-sm text-gray-500"><button class="focus:outline-hidden relative flex h-full max-w-56 items-center gap-1.5 overflow-hidden px-1.5 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" type="button" ><div class="flex h-full flex-1 items-center justify-center ">Follow</div>
		<img alt="" class="rounded-xs size-3 flex-none" src="https://www.gravatar.com/avatar/b044ed6f48fb9b7eb557ea402e28e44a?d=retro&amp;size=100">
		<span class="truncate">SCIM Project</span></button>
	<button class="focus:outline-hidden flex h-full items-center border-l pl-1.5 pr-1.5 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="Show SCIM Project's followers" type="button">3</button></div>

		</div>
			
	</h1>
		<div class="mb-3 flex flex-wrap md:mb-4"><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Modalities:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?modality=modality%3Atext"><div class="tag tag-white   ">
		<svg class="text-red-700 dark:text-red-600" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.619 4.619C2.667 6.573 2.667 9.715 2.667 16s0 9.428 1.952 11.38C6.573 29.333 9.715 29.333 16 29.333s9.428 0 11.38-1.953c1.953-1.95 1.953-5.095 1.953-11.38s0-9.428-1.953-11.381C25.43 2.667 22.285 2.667 16 2.667s-9.428 0-11.381 1.952m8.65 3.714c-.573 0-1.109 0-1.546.066-.495.073-1.003.248-1.41.7-.392.436-.53.956-.59 1.452-.056.464-.056 1.04-.056 1.689V13a1 1 0 1 0 2 0v-.704c0-.724.001-1.176.041-1.505q.015-.15.061-.294a.2.2 0 0 1 .031-.061q0-.003.016-.01a.8.8 0 0 1 .203-.05c.272-.04.654-.043 1.314-.043H15v11.334h-2.333a1 1 0 1 0 0 2H20a1 1 0 0 0 0-2h-3V10.333h1.667c.66 0 1.042.003 1.314.043.123.019.18.04.203.05l.015.009a.2.2 0 0 1 .032.061c.018.05.042.14.061.295.04.329.041.781.041 1.506V13a1 1 0 1 0 2 0v-.76c0-.65 0-1.225-.056-1.69-.06-.495-.198-1.015-.59-1.453-.407-.45-.915-.625-1.41-.698-.437-.067-.973-.067-1.546-.066z" fill="currentColor"></path></svg>

	

	<span>Text</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Formats:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?format=format%3Aparquet"><div class="tag tag-white   ">
		<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 34 34"><path fill-rule="evenodd" clip-rule="evenodd" d="m17.97 18.44-3.98-2.3-7.1 3.78 3.98 2.3 7.1-3.78Zm15.1-1.4-3.99-2.3-16.22 8.63 3.98 2.3 16.22-8.63Zm-5.98-3.45-3.97-2.3-7.1 3.78 3.98 2.3 7.1-3.78Zm-9.94-5.74 3.98 2.3-11.16 5.93L6 13.78l11.16-5.93Zm-13.19 7 3.98 2.3-3.04 1.62-3.98-2.3 3.04-1.61Z" fill="currentColor"></path></svg>

	

	<span>parquet</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Size:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?size_categories=size_categories%3A1K%3Cn%3C10K"><div class="tag tag-white   ">

	

	<span>1K - 10K</span>
	

	</div></a>

	</div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Libraries:
	</span>
	<a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Adatasets"><div class="tag tag-white   "><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" width="1em" height="1em" viewBox="0 0 95 88"><path fill="#fff" d="M94.25 70.08a8.28 8.28 0 0 1-.43 6.46 10.57 10.57 0 0 1-3 3.6 25.18 25.18 0 0 1-5.7 3.2 65.74 65.74 0 0 1-7.56 2.65 46.67 46.67 0 0 1-11.42 1.68c-5.42.05-10.09-1.23-13.4-4.5a40.4 40.4 0 0 1-10.14.03c-3.34 3.25-7.99 4.52-13.39 4.47a46.82 46.82 0 0 1-11.43-1.68 66.37 66.37 0 0 1-7.55-2.65c-2.28-.98-4.17-2-5.68-3.2a10.5 10.5 0 0 1-3.02-3.6c-.99-2-1.18-4.3-.42-6.46a8.54 8.54 0 0 1-.33-5.63c.25-.95.66-1.83 1.18-2.61a8.67 8.67 0 0 1 2.1-8.47 8.23 8.23 0 0 1 2.82-2.07 41.75 41.75 0 1 1 81.3-.12 8.27 8.27 0 0 1 3.11 2.19 8.7 8.7 0 0 1 2.1 8.47c.52.78.93 1.66 1.18 2.61a8.61 8.61 0 0 1-.32 5.63Z"></path><path fill="#FFD21E" d="M47.21 76.5a34.75 34.75 0 1 0 0-69.5 34.75 34.75 0 0 0 0 69.5Z"></path><path fill="#FF9D0B" d="M81.96 41.75a34.75 34.75 0 1 0-69.5 0 34.75 34.75 0 0 0 69.5 0Zm-73.5 0a38.75 38.75 0 1 1 77.5 0 38.75 38.75 0 0 1-77.5 0Z"></path><path fill="#3A3B45" d="M58.5 32.3c1.28.44 1.78 3.06 3.07 2.38a5 5 0 1 0-6.76-2.07c.61 1.15 2.55-.72 3.7-.32ZM34.95 32.3c-1.28.44-1.79 3.06-3.07 2.38a5 5 0 1 1 6.76-2.07c-.61 1.15-2.56-.72-3.7-.32Z"></path><path fill="#FF323D" d="M46.96 56.29c9.83 0 13-8.76 13-13.26 0-2.34-1.57-1.6-4.09-.36-2.33 1.15-5.46 2.74-8.9 2.74-7.19 0-13-6.88-13-2.38s3.16 13.26 13 13.26Z"></path><path fill="#3A3B45" fill-rule="evenodd" d="M39.43 54a8.7 8.7 0 0 1 5.3-4.49c.4-.12.81.57 1.24 1.28.4.68.82 1.37 1.24 1.37.45 0 .9-.68 1.33-1.35.45-.7.89-1.38 1.32-1.25a8.61 8.61 0 0 1 5 4.17c3.73-2.94 5.1-7.74 5.1-10.7 0-2.34-1.57-1.6-4.09-.36l-.14.07c-2.31 1.15-5.39 2.67-8.77 2.67s-6.45-1.52-8.77-2.67c-2.6-1.29-4.23-2.1-4.23.29 0 3.05 1.46 8.06 5.47 10.97Z" clip-rule="evenodd"></path><path fill="#FF9D0B" d="M70.71 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM24.21 37a3.25 3.25 0 1 0 0-6.5 3.25 3.25 0 0 0 0 6.5ZM17.52 48c-1.62 0-3.06.66-4.07 1.87a5.97 5.97 0 0 0-1.33 3.76 7.1 7.1 0 0 0-1.94-.3c-1.55 0-2.95.59-3.94 1.66a5.8 5.8 0 0 0-.8 7 5.3 5.3 0 0 0-1.79 2.82c-.24.9-.48 2.8.8 4.74a5.22 5.22 0 0 0-.37 5.02c1.02 2.32 3.57 4.14 8.52 6.1 3.07 1.22 5.89 2 5.91 2.01a44.33 44.33 0 0 0 10.93 1.6c5.86 0 10.05-1.8 12.46-5.34 3.88-5.69 3.33-10.9-1.7-15.92-2.77-2.78-4.62-6.87-5-7.77-.78-2.66-2.84-5.62-6.25-5.62a5.7 5.7 0 0 0-4.6 2.46c-1-1.26-1.98-2.25-2.86-2.82A7.4 7.4 0 0 0 17.52 48Zm0 4c.51 0 1.14.22 1.82.65 2.14 1.36 6.25 8.43 7.76 11.18.5.92 1.37 1.31 2.14 1.31 1.55 0 2.75-1.53.15-3.48-3.92-2.93-2.55-7.72-.68-8.01.08-.02.17-.02.24-.02 1.7 0 2.45 2.93 2.45 2.93s2.2 5.52 5.98 9.3c3.77 3.77 3.97 6.8 1.22 10.83-1.88 2.75-5.47 3.58-9.16 3.58-3.81 0-7.73-.9-9.92-1.46-.11-.03-13.45-3.8-11.76-7 .28-.54.75-.76 1.34-.76 2.38 0 6.7 3.54 8.57 3.54.41 0 .7-.17.83-.6.79-2.85-12.06-4.05-10.98-8.17.2-.73.71-1.02 1.44-1.02 3.14 0 10.2 5.53 11.68 5.53.11 0 .2-.03.24-.1.74-1.2.33-2.04-4.9-5.2-5.21-3.16-8.88-5.06-6.8-7.33.24-.26.58-.38 1-.38 3.17 0 10.66 6.82 10.66 6.82s2.02 2.1 3.25 2.1c.28 0 .52-.1.68-.38.86-1.46-8.06-8.22-8.56-11.01-.34-1.9.24-2.85 1.31-2.85Z"></path><path fill="#FFD21E" d="M38.6 76.69c2.75-4.04 2.55-7.07-1.22-10.84-3.78-3.77-5.98-9.3-5.98-9.3s-.82-3.2-2.69-2.9c-1.87.3-3.24 5.08.68 8.01 3.91 2.93-.78 4.92-2.29 2.17-1.5-2.75-5.62-9.82-7.76-11.18-2.13-1.35-3.63-.6-3.13 2.2.5 2.79 9.43 9.55 8.56 11-.87 1.47-3.93-1.71-3.93-1.71s-9.57-8.71-11.66-6.44c-2.08 2.27 1.59 4.17 6.8 7.33 5.23 3.16 5.64 4 4.9 5.2-.75 1.2-12.28-8.53-13.36-4.4-1.08 4.11 11.77 5.3 10.98 8.15-.8 2.85-9.06-5.38-10.74-2.18-1.7 3.21 11.65 6.98 11.76 7.01 4.3 1.12 15.25 3.49 19.08-2.12Z"></path><path fill="#FF9D0B" d="M77.4 48c1.62 0 3.07.66 4.07 1.87a5.97 5.97 0 0 1 1.33 3.76 7.1 7.1 0 0 1 1.95-.3c1.55 0 2.95.59 3.94 1.66a5.8 5.8 0 0 1 .8 7 5.3 5.3 0 0 1 1.78 2.82c.24.9.48 2.8-.8 4.74a5.22 5.22 0 0 1 .37 5.02c-1.02 2.32-3.57 4.14-8.51 6.1-3.08 1.22-5.9 2-5.92 2.01a44.33 44.33 0 0 1-10.93 1.6c-5.86 0-10.05-1.8-12.46-5.34-3.88-5.69-3.33-10.9 1.7-15.92 2.78-2.78 4.63-6.87 5.01-7.77.78-2.66 2.83-5.62 6.24-5.62a5.7 5.7 0 0 1 4.6 2.46c1-1.26 1.98-2.25 2.87-2.82A7.4 7.4 0 0 1 77.4 48Zm0 4c-.51 0-1.13.22-1.82.65-2.13 1.36-6.25 8.43-7.76 11.18a2.43 2.43 0 0 1-2.14 1.31c-1.54 0-2.75-1.53-.14-3.48 3.91-2.93 2.54-7.72.67-8.01a1.54 1.54 0 0 0-.24-.02c-1.7 0-2.45 2.93-2.45 2.93s-2.2 5.52-5.97 9.3c-3.78 3.77-3.98 6.8-1.22 10.83 1.87 2.75 5.47 3.58 9.15 3.58 3.82 0 7.73-.9 9.93-1.46.1-.03 13.45-3.8 11.76-7-.29-.54-.75-.76-1.34-.76-2.38 0-6.71 3.54-8.57 3.54-.42 0-.71-.17-.83-.6-.8-2.85 12.05-4.05 10.97-8.17-.19-.73-.7-1.02-1.44-1.02-3.14 0-10.2 5.53-11.68 5.53-.1 0-.19-.03-.23-.1-.74-1.2-.34-2.04 4.88-5.2 5.23-3.16 8.9-5.06 6.8-7.33-.23-.26-.57-.38-.98-.38-3.18 0-10.67 6.82-10.67 6.82s-2.02 2.1-3.24 2.1a.74.74 0 0 1-.68-.38c-.87-1.46 8.05-8.22 8.55-11.01.34-1.9-.24-2.85-1.31-2.85Z"></path><path fill="#FFD21E" d="M56.33 76.69c-2.75-4.04-2.56-7.07 1.22-10.84 3.77-3.77 5.97-9.3 5.97-9.3s.82-3.2 2.7-2.9c1.86.3 3.23 5.08-.68 8.01-3.92 2.93.78 4.92 2.28 2.17 1.51-2.75 5.63-9.82 7.76-11.18 2.13-1.35 3.64-.6 3.13 2.2-.5 2.79-9.42 9.55-8.55 11 .86 1.47 3.92-1.71 3.92-1.71s9.58-8.71 11.66-6.44c2.08 2.27-1.58 4.17-6.8 7.33-5.23 3.16-5.63 4-4.9 5.2.75 1.2 12.28-8.53 13.36-4.4 1.08 4.11-11.76 5.3-10.97 8.15.8 2.85 9.05-5.38 10.74-2.18 1.69 3.21-11.65 6.98-11.76 7.01-4.31 1.12-15.26 3.49-19.08-2.12Z"></path></svg>

	

	<span>Datasets</span>
	

	</div></a><a class="mb-1 mr-1 md:mb-1.5 md:mr-1.5 rounded-lg" href="/datasets?library=library%3Apandas"><div class="tag tag-white   "><svg class="text-black inline-block text-sm text-[#130754] dark:text-gray-200" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid slice" viewBox="0 0 210.21 280.43"><rect fill="currentColor" x="74.51" y="43.03" width="24.09" height="50.02"></rect><rect fill="currentColor" x="74.51" y="145.78" width="24.09" height="50.02"></rect><rect fill="#ffca00" x="74.51" y="107.65" width="24.09" height="23.6"></rect><rect fill="currentColor" x="35.81" y="84.15" width="24.09" height="166.27"></rect><rect fill="currentColor" x="112.41" y="187.05" width="24.09" height="50.02"></rect><rect fill="currentColor" x="112.41" y="84.21" width="24.09" height="50.02"></rect><rect fill="#e70488" x="112.41" y="148.84" width="24.09" height="23.6"></rect><rect fill="currentColor" x="150.3" y="30" width="24.09" height="166.27"></rect></svg>

	

	<span>pandas</span>
	

	</div></a><div class="relative inline-block ">
	<button class="group mr-1 mb-1 md:mr-1.5 md:mb-1.5  rounded-lg rounded-br-none " type="button">
		<div class="tag tag-white   relative rounded-br-none pr-2.5"><svg class="text-black inline-block text-sm" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="none" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="#F5AB6A"></path><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="url(#paint0_radial_18_31665)"></path><g filter="url(#filter0_f_18_31665)"><path d="M22.2849 12.1817L23.4375 13.2656L24.4375 4.70312C23.5121 4.1242 23.0198 3.96369 22.6563 3.89062L22.2849 12.1817Z" fill="url(#paint1_linear_18_31665)"></path></g><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint2_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint3_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint4_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint5_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint6_linear_18_31665)"></path><g filter="url(#filter1_f_18_31665)"><path d="M13.1016 2.72656C11.862 3.06924 11.5298 3.40016 11.5298 3.40016C10.9102 3.52335 10.936 4.11525 11.2408 4.6799C13.1487 6.95202 15.1361 13.2496 18.007 14.0958C18.2707 14.1633 18.6953 14.2107 19.1797 14.2344L13.1016 2.72656Z" fill="url(#paint7_linear_18_31665)"></path></g><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint8_radial_18_31665)"></path><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint9_radial_18_31665)"></path><g filter="url(#filter2_f_18_31665)"><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint10_linear_18_31665)"></path><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint11_radial_18_31665)"></path></g><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="#EC9F6A"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint12_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint13_linear_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint14_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint15_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint16_radial_18_31665)"></path><g filter="url(#filter3_f_18_31665)"><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint17_linear_18_31665)"></path><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint18_linear_18_31665)"></path></g><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="#D79453"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint19_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint20_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint21_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint22_linear_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint23_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint24_radial_18_31665)"></path><defs><filter id="filter0_f_18_31665" x="22.0349" y="3.64062" width="2.65265" height="9.875" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter1_f_18_31665" x="10.7815" y="2.47656" width="8.64819" height="12.0078" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter2_f_18_31665" x="3.89062" y="22.5416" width="9.57812" height="2.2804" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><filter id="filter3_f_18_31665" x="2.45312" y="11.2538" width="12.1255" height="8.50903" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"></feFlood><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feBlend><feGaussianBlur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></feGaussianBlur></filter><radialGradient id="paint0_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.8125 12.9375) rotate(42.7741) scale(12.5164 7.08839)"><stop offset="0.0937591" stop-color="#C05159"></stop><stop offset="0.553697" stop-color="#F6AC6A"></stop><stop offset="0.832916" stop-color="#FFD186"></stop><stop offset="0.916927" stop-color="#FFDC87"></stop></radialGradient><linearGradient id="paint1_linear_18_31665" x1="24.7344" y1="4.67187" x2="20.8594" y2="12.8906" gradientUnits="userSpaceOnUse"><stop stop-color="#EBD67C"></stop><stop offset="0.0655686" stop-color="#FFFFA6"></stop><stop offset="0.530552" stop-color="#F8C281"></stop><stop offset="0.937338" stop-color="#E99E6B"></stop></linearGradient><radialGradient id="paint2_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.9009 13.1847) rotate(-127.648) scale(14.3438 11.7966)"><stop stop-color="#FFBE66"></stop><stop offset="1" stop-color="#E2AE5B"></stop></radialGradient><radialGradient id="paint3_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(18 11.4375) rotate(53.9726) scale(11.9013 4.84018)"><stop stop-color="#D67C63"></stop><stop offset="1" stop-color="#D97D67" stop-opacity="0"></stop></radialGradient><radialGradient id="paint4_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(23 4.1875) rotate(45.7639) scale(3.31486 5.75622)"><stop stop-color="#FFE4A6"></stop><stop offset="0.711285" stop-color="#F8B76F"></stop><stop offset="1" stop-color="#F9B870" stop-opacity="0"></stop></radialGradient><radialGradient id="paint5_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.875 12.4375) rotate(88.9391) scale(3.37558 1.29066)"><stop stop-color="#FFBC67"></stop><stop offset="1" stop-color="#FFBC67" stop-opacity="0"></stop></radialGradient><linearGradient id="paint6_linear_18_31665" x1="20.375" y1="15.6875" x2="20.125" y2="12.7813" gradientUnits="userSpaceOnUse"><stop offset="0.461609" stop-color="#B45077"></stop><stop offset="0.855389" stop-color="#B75077" stop-opacity="0"></stop></linearGradient><linearGradient id="paint7_linear_18_31665" x1="12.9375" y1="2.57056" x2="18.5625" y2="14.3891" gradientUnits="userSpaceOnUse"><stop stop-color="#DDC173"></stop><stop offset="0.485173" stop-color="#D59F65"></stop><stop offset="1" stop-color="#E49966"></stop></linearGradient><radialGradient id="paint8_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.5625 23.5) rotate(109.113) scale(6.68078 10.2578)"><stop offset="0.165756" stop-color="#FFBF7E"></stop><stop offset="0.827674" stop-color="#DF8C6D"></stop><stop offset="1" stop-color="#B05A66"></stop></radialGradient><radialGradient id="paint9_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.1875 26) rotate(41.0652) scale(8.37243 2.03649)"><stop stop-color="#FFD483"></stop><stop offset="1" stop-color="#FFD688" stop-opacity="0"></stop></radialGradient><linearGradient id="paint10_linear_18_31665" x1="3.96063" y1="23.794" x2="13.3748" y2="23.5143" gradientUnits="userSpaceOnUse"><stop stop-color="#A8716F"></stop><stop offset="0.103615" stop-color="#B37173"></stop><stop offset="0.225484" stop-color="#DB9F84"></stop><stop offset="0.799889" stop-color="#F1BB8A"></stop><stop offset="1" stop-color="#FFD780"></stop></linearGradient><radialGradient id="paint11_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4219 23.1719) rotate(-178.616) scale(3.23532 0.569081)"><stop offset="0.621498" stop-color="#AF5A3E"></stop><stop offset="1" stop-color="#B35445" stop-opacity="0"></stop></radialGradient><radialGradient id="paint12_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.625 19.125) rotate(-171.737) scale(15.2205 15.0767)"><stop offset="0.138435" stop-color="#FFB974"></stop><stop offset="0.403618" stop-color="#F2A56D"></stop><stop offset="0.925938" stop-color="#A16948"></stop></radialGradient><linearGradient id="paint13_linear_18_31665" x1="8.22184" y1="13.125" x2="6.81191" y2="15.4996" gradientUnits="userSpaceOnUse"><stop offset="0.610751" stop-color="#984847"></stop><stop offset="0.850075" stop-color="#9A4947" stop-opacity="0"></stop></linearGradient><radialGradient id="paint14_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.25 23.7461) scale(11.25 5.68361)"><stop stop-color="#C66364"></stop><stop offset="1" stop-color="#D4766B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint15_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.875 23.5313) scale(10.0937 1.29657)"><stop stop-color="#B64B4B"></stop><stop offset="1" stop-color="#C56158" stop-opacity="0"></stop></radialGradient><radialGradient id="paint16_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4375 19.875) rotate(-46.8882) scale(4.02385 7.51767)"><stop stop-color="#FFC083"></stop><stop offset="0.620218" stop-color="#FFBD7D" stop-opacity="0"></stop></radialGradient><linearGradient id="paint17_linear_18_31665" x1="2.8125" y1="13.0312" x2="14.5582" y2="18.9404" gradientUnits="userSpaceOnUse"><stop stop-color="#B89367"></stop><stop offset="1" stop-color="#C5835E"></stop></linearGradient><linearGradient id="paint18_linear_18_31665" x1="8.21875" y1="14.6406" x2="7.59349" y2="15.6717" gradientUnits="userSpaceOnUse"><stop offset="0.351552" stop-color="#A74746"></stop><stop offset="0.845198" stop-color="#A04346" stop-opacity="0"></stop></linearGradient><radialGradient id="paint19_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.5625 14.5625) rotate(140.244) scale(18.3733 13.7403)"><stop stop-color="#FDAE69"></stop><stop offset="0.729021" stop-color="#CE8C4F"></stop><stop offset="0.921546" stop-color="#AD7B45"></stop><stop offset="1" stop-color="#8B6B4A"></stop></radialGradient><radialGradient id="paint20_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.0625 7) rotate(65.3152) scale(11.0745 3.16547)"><stop offset="0.233237" stop-color="#FFD47C"></stop><stop offset="0.853648" stop-color="#FFD98B" stop-opacity="0"></stop></radialGradient><radialGradient id="paint21_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.3125 8.875) rotate(100.886) scale(6.6191 5.57808)"><stop offset="0.128419" stop-color="#FFD88C"></stop><stop offset="0.924134" stop-color="#FFBE7B" stop-opacity="0"></stop></radialGradient><linearGradient id="paint22_linear_18_31665" x1="7.25" y1="15.1875" x2="10.7588" y2="10.3142" gradientUnits="userSpaceOnUse"><stop offset="0.142353" stop-color="#C15F4D"></stop><stop offset="1" stop-color="#D58366" stop-opacity="0"></stop></linearGradient><radialGradient id="paint23_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(8.15625 15.7813) rotate(28.5422) scale(12.5574 1.96589)"><stop offset="0.149989" stop-color="#E4745D"></stop><stop offset="0.453292" stop-color="#C8604C"></stop><stop offset="0.632597" stop-color="#C0605F"></stop><stop offset="1" stop-color="#C0605F" stop-opacity="0"></stop></radialGradient><radialGradient id="paint24_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(1.40625 2.69067) rotate(46.0943) scale(22.3963)"><stop offset="0.935802" stop-color="#C17C61" stop-opacity="0"></stop><stop offset="0.982109" stop-color="#C17C61"></stop></radialGradient></defs></svg>

	

	<span>Croissant</span>
	

	<div class="border-br-gray-200 absolute bottom-0.5 right-0.5 h-1 w-1 border-[3px] border-l-transparent border-t-transparent border-b-gray-200 border-r-gray-200 dark:border-b-gray-700 dark:border-r-gray-700"></div></div>
		
		</button>
	
	
	</div>

	<button class="tag tag-ghost px-1! -ml-0.5 mb-1 md:mb-1.5" type="button">+ 1</button></div></div>

		<div class="flex flex-col-reverse lg:flex-row lg:items-center lg:justify-between"><div class="-mb-px flex h-12 items-center overflow-x-auto overflow-y-hidden ">
	<a class="tab-alternate active" href="/datasets/scim/naacl_data_prog"><svg class="mr-1.5 text-gray-400 flex-none" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
	Dataset card
	

	
		</a><a class="tab-alternate" href="/datasets/scim/naacl_data_prog/viewer/"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	Data Studio
	

	
		</a><a class="tab-alternate" href="/datasets/scim/naacl_data_prog/tree/main"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M21 19h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-8h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0 4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M9 19a1 1 0 0 1-1-1V6a1 1 0 0 1 2 0v12a1 1 0 0 1-1 1zm-6-4.333a1 1 0 0 1-.64-1.769L3.438 12l-1.078-.898a1 1 0 0 1 1.28-1.538l2 1.667a1 1 0 0 1 0 1.538l-2 1.667a.999.999 0 0 1-.64.231z" fill="currentColor"></path></svg>
	<span class="xl:hidden">Files</span>
		<span class="hidden xl:inline">Files and versions</span>
	

	
		</a><a class="tab-alternate" href="/datasets/scim/naacl_data_prog/discussions"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M20.6081 3C21.7684 3 22.8053 3.49196 23.5284 4.38415C23.9756 4.93678 24.4428 5.82749 24.4808 7.16133C24.9674 7.01707 25.4353 6.93643 25.8725 6.93643C26.9833 6.93643 27.9865 7.37587 28.696 8.17411C29.6075 9.19872 30.0124 10.4579 29.8361 11.7177C29.7523 12.3177 29.5581 12.8555 29.2678 13.3534C29.8798 13.8646 30.3306 14.5763 30.5485 15.4322C30.719 16.1032 30.8939 17.5006 29.9808 18.9403C30.0389 19.0342 30.0934 19.1319 30.1442 19.2318C30.6932 20.3074 30.7283 21.5229 30.2439 22.6548C29.5093 24.3704 27.6841 25.7219 24.1397 27.1727C21.9347 28.0753 19.9174 28.6523 19.8994 28.6575C16.9842 29.4379 14.3477 29.8345 12.0653 29.8345C7.87017 29.8345 4.8668 28.508 3.13831 25.8921C0.356375 21.6797 0.754104 17.8269 4.35369 14.1131C6.34591 12.058 7.67023 9.02782 7.94613 8.36275C8.50224 6.39343 9.97271 4.20438 12.4172 4.20438H12.4179C12.6236 4.20438 12.8314 4.2214 13.0364 4.25468C14.107 4.42854 15.0428 5.06476 15.7115 6.02205C16.4331 5.09583 17.134 4.359 17.7682 3.94323C18.7242 3.31737 19.6794 3 20.6081 3ZM20.6081 5.95917C20.2427 5.95917 19.7963 6.1197 19.3039 6.44225C17.7754 7.44319 14.8258 12.6772 13.7458 14.7131C13.3839 15.3952 12.7655 15.6837 12.2086 15.6837C11.1036 15.6837 10.2408 14.5497 12.1076 13.1085C14.9146 10.9402 13.9299 7.39584 12.5898 7.1776C12.5311 7.16799 12.4731 7.16355 12.4172 7.16355C11.1989 7.16355 10.6615 9.33114 10.6615 9.33114C10.6615 9.33114 9.0863 13.4148 6.38031 16.206C3.67434 18.998 3.5346 21.2388 5.50675 24.2246C6.85185 26.2606 9.42666 26.8753 12.0653 26.8753C14.8021 26.8753 17.6077 26.2139 19.1799 25.793C19.2574 25.7723 28.8193 22.984 27.6081 20.6107C27.4046 20.212 27.0693 20.0522 26.6471 20.0522C24.9416 20.0522 21.8393 22.6726 20.5057 22.6726C20.2076 22.6726 19.9976 22.5416 19.9116 22.222C19.3433 20.1173 28.552 19.2325 27.7758 16.1839C27.639 15.6445 27.2677 15.4256 26.746 15.4263C24.4923 15.4263 19.4358 19.5181 18.3759 19.5181C18.2949 19.5181 18.2368 19.4937 18.2053 19.4419C17.6743 18.557 17.9653 17.9394 21.7082 15.6009C25.4511 13.2617 28.0783 11.8545 26.5841 10.1752C26.4121 9.98141 26.1684 9.8956 25.8725 9.8956C23.6001 9.89634 18.2311 14.9403 18.2311 14.9403C18.2311 14.9403 16.7821 16.496 15.9057 16.496C15.7043 16.496 15.533 16.4139 15.4169 16.2112C14.7956 15.1296 21.1879 10.1286 21.5484 8.06535C21.7928 6.66715 21.3771 5.95917 20.6081 5.95917Z" fill="#FF9D00"></path><path d="M5.50686 24.2246C3.53472 21.2387 3.67446 18.9979 6.38043 16.206C9.08641 13.4147 10.6615 9.33111 10.6615 9.33111C10.6615 9.33111 11.2499 6.95933 12.59 7.17757C13.93 7.39581 14.9139 10.9401 12.1069 13.1084C9.29997 15.276 12.6659 16.7489 13.7459 14.713C14.8258 12.6772 17.7747 7.44316 19.304 6.44221C20.8326 5.44128 21.9089 6.00204 21.5484 8.06532C21.188 10.1286 14.795 15.1295 15.4171 16.2118C16.0391 17.2934 18.2312 14.9402 18.2312 14.9402C18.2312 14.9402 25.0907 8.49588 26.5842 10.1752C28.0776 11.8545 25.4512 13.2616 21.7082 15.6008C17.9646 17.9393 17.6744 18.557 18.2054 19.4418C18.7372 20.3266 26.9998 13.1351 27.7759 16.1838C28.5513 19.2324 19.3434 20.1173 19.9117 22.2219C20.48 24.3274 26.3979 18.2382 27.6082 20.6107C28.8193 22.9839 19.2574 25.7722 19.18 25.7929C16.0914 26.62 8.24723 28.3726 5.50686 24.2246Z" fill="#FFD21E"></path></svg>
	Community
	

	
		</a></div>
	
			</div></div></header>


</div>
	
<div class="container relative flex flex-col md:grid md:space-y-0 w-full md:grid-cols-12 md:flex-1 md:grid-rows-full space-y-4 md:gap-6 ">
		<section class="pt-6 border-gray-100 md:col-span-8 pb-24 relative break-words copiable-code-container">
				<div class="SVELTE_HYDRATER contents" data-target="UnsafeBanner" data-props="{&quot;classNames&quot;:&quot;mb-4&quot;,&quot;repoId&quot;:&quot;scim/naacl_data_prog&quot;,&quot;repoType&quot;:&quot;dataset&quot;,&quot;minLevel&quot;:&quot;unsafe&quot;}"></div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetViewer" data-props="{&quot;data&quot;:{&quot;kind&quot;:&quot;DatasetAndSampleData&quot;,&quot;datasetInfo&quot;:[{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Size of downloaded dataset files:&quot;,&quot;value&quot;:&quot;47.7 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;/datasets/scim/naacl_data_prog/tree/refs%2Fconvert%2Fparquet/&quot;,&quot;label&quot;:&quot;Size of the auto-converted Parquet files:&quot;,&quot;value&quot;:&quot;47.7 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Number of rows:&quot;,&quot;value&quot;:&quot;3,051&quot;}],&quot;partial&quot;:false,&quot;configsData&quot;:{&quot;configInfos&quot;:[{&quot;name&quot;:&quot;default&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:3051}],&quot;selectedConfig&quot;:&quot;default&quot;,&quot;hasSelectedConfigParquet&quot;:true},&quot;splitsData&quot;:{&quot;splitInfos&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;numRows&quot;:3051}],&quot;selectedSplit&quot;:&quot;train&quot;},&quot;sampleData&quot;:{&quot;dataset&quot;:&quot;scim/naacl_data_prog&quot;,&quot;config&quot;:&quot;default&quot;,&quot;split&quot;:&quot;train&quot;,&quot;capabilities&quot;:{&quot;rows&quot;:true,&quot;search&quot;:true,&quot;filter&quot;:true,&quot;statistics&quot;:false},&quot;navigation&quot;:{&quot;p&quot;:0},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzA1NCwic3ViIjoiL2RhdGFzZXRzL3NjaW0vbmFhY2xfZGF0YV9wcm9nIiwiZXhwIjoxNzQyOTI2NjU0LCJpc3MiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvIn0.Ka0w5mBlccNOxNwOnqmdke4esP007em8DAD4dX6AgNvDP05Ua9EN6MTArplPAe3OFJa3-tJXC-JiaAJ6iBpWAA&quot;,&quot;sampleData&quot;:{&quot;columns&quot;:[{&quot;name&quot;:&quot;sentences&quot;,&quot;align&quot;:&quot;left&quot;,&quot;type&quot;:&quot;sequence&quot;},{&quot;name&quot;:&quot;labels&quot;,&quot;align&quot;:&quot;left&quot;,&quot;type&quot;:&quot;sequence&quot;}],&quot;rows&quot;:[{&quot;rowIdx&quot;:0,&quot;cells&quot;:{&quot;sentences&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios.&quot;,&quot;We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets.&quot;,&quot;The biases are specified in terms of one or more bias-only models , which learn to leverage the dataset biases.&quot;,&quot;During training, the bias-only models' predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing training on the hard examples.&quot;,&quot;We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data.&quot;,&quot;Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets.&quot;,&quot;Our code and data are publicly available in https: //github.com/rabeehk/robust-nli .&quot;,&quot;Recent neural models (Devlin et al., 2019; Radford et al., 2018; Chen et al., 2017) have achieved high and even near human-performance on several large-scale natural language understanding benchmarks.&quot;,&quot;However, it has been demonstrated that neural models tend to rely on existing idiosyncratic biases in the datasets, and leverage superficial correlations between the label and existing shortcuts in the training dataset to perform surprisingly well, 1 without learning the underlying task (Kaushik and Lipton, 2018; Gururangan et al., 2018; Poliak et al., 2018; Schuster et al., 2019; 1 We use biases, heuristics or shortcuts interchangeably. McCoy et al., 2019b).&quot;,&quot;For instance, natural language inference (NLI) is supposed to test the ability of a model to determine whether a hypothesis sentence ( There is no teacher in the room ) can be inferred from a premise sentence ( Kids work at computers with a teacher's help ) (Dagan et al., 2006).&quot;,&quot;2 However, recent work has demonstrated that large-scale NLI benchmarks contain annotation artifacts; certain words in the hypothesis that are highly indicative of inference class and allow models that do not consider the premise to perform unexpectedly well (Poliak et al., 2018; Gururangan et al., 2018).&quot;,&quot;As an example, in some NLI benchmarks, negation words such as nobody, no, and not in the hypothesis are often highly correlated with the contradiction label.&quot;,&quot;As a result of the existence of such biases, models exploiting statistical shortcuts during training often perform poorly on out-of-domain datasets, especially if the datasets are carefully designed to limit the spurious cues.&quot;,&quot;To allow proper evaluation, recent studies have tried to create new evaluation datasets that do not contain such biases (Gururangan et al., 2018; Schuster et al., 2019; McCoy et al., 2019b).&quot;,&quot;Unfortunately, it is hard to avoid spurious statistical cues in the construction of large-scale benchmarks, and collecting new datasets is costly (Sharma et al., 2018).&quot;,&quot;It is, therefore, crucial to develop techniques to reduce the reliance on biases during the training of the neural models.&quot;,&quot;We propose two end-to-end debiasing techniques that can be used when the existing bias patterns are identified.&quot;,&quot;These methods work by adjusting the cross-entropy loss to reduce the biases learned from the training dataset, down-weighting the biased examples so that the model focuses on learning the hard examples.&quot;,&quot;Figure 1 illustrates an example of applying our strategy to prevent an NLI model from predicting the labels using existing biases in the hypotheses, where the bias-only model only sees the hypothesis.&quot;,&quot;Our strat-2 The given sentences are in the contradictory relation, and the hypothesis cannot be inferred from the premise.&quot;,&quot;egy involves adding this bias-only branch f B on top of the base model f M during training.&quot;,&quot;We then compute the combination of the two models f C in a way that motivates the base model to learn different strategies than the ones used by the bias-only branch f B .&quot;,&quot;At the end of the training, we remove the bias-only classifier and use the predictions of the base model.&quot;,&quot;In our first proposed method, Product of Experts, the training loss is computed on an ensemble of the base model and the bias-only model, which reduces the base model's loss for the examples that the bias-only model classifies correctly.&quot;,&quot;For the second method, Debiased Focal Loss, the bias-only predictions are used to directly weight the loss of the base model, explicitly modulating the loss depending on the accuracy of the bias-only model.&quot;,&quot;We also extend these methods to be robust against multiple sources of bias by training multiple bias-only models.&quot;,&quot;Our approaches are simple and highly effective.&quot;,&quot;They require training only a simple model on top of the base model.&quot;,&quot;They are model agnostic and general enough to be applicable for addressing common biases seen in many datasets in different domains.&quot;,&quot;We evaluate our models on challenging benchmarks in textual entailment and fact verification, including HANS (Heuristic Analysis for NLI Systems) (McCoy et al., 2019b), hard NLI sets (Gururangan et al., 2018) of Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and MultiNLI (MNLI) (Williams et al., 2018), and FEVER Symmetric test set (Schuster et al., 2019).&quot;,&quot;The selected datasets are highly challenging and have been carefully designed to be unbiased to allow proper evaluation of the out-of-domain performance of the models.&quot;,&quot;We additionally construct hard MNLI datasets from MNLI development sets to facilitate the out-of-domain evaluation on this dataset.&quot;,&quot;3 We show that including our strategies on training baseline models, including BERT (Devlin et al., 2019), provides a substantial gain on out-of-domain performance in all the experiments.&quot;,&quot;In summary, we make the following contributions:&quot;,&quot;1) Proposing two debiasing strategies to train neural models robust to dataset bias.&quot;,&quot;2) An empirical evaluation of the methods on two large-scale NLI datasets and a fact verification benchmark; obtaining a substantial gain on their challenging out-of-domain data, including 7.4 points on HANS, 4.8 points on SNLI hard set, and 9.8 points on FEVER symmetric test set, setting a new state-of-the-art.&quot;,&quot;3) Proposing debiasing strategies capable of combating multiple sources of bias.&quot;,&quot;4) Evaluating the transfer performance of the debiased models on 12 NLI datasets and demonstrating improved transfer to other NLI benchmarks.&quot;,&quot;To facilitate future work, we release our datasets and code.&quot;,&quot;To address dataset biases, researchers have proposed to augment datasets by balancing the existing cues (Schuster et al., 2019) or to create an adversarial dataset (Jia and Liang, 2017).&quot;,&quot;However, collecting new datasets, especially at a large scale, is costly, and thus remains an unsatisfactory solution.&quot;,&quot;It is, therefore, crucial to develop strategies to allow models to be trained on the existing biased datasets.&quot;,&quot;3 Removing the need to submit to an online evaluation system for MNLI hard test sets.&quot;,&quot;Schuster et al. (2019) propose to first compute the n-grams in the dataset's claims that are the most associated with each fact-verification label.&quot;,&quot;They then solve an optimization problem to assign a balancing weight to each training sample to alleviate the biases.&quot;,&quot;In contrast, we propose several end-to-end debiasing strategies.&quot;,&quot;Additionally, Belinkov et al. (2019a) propose adversarial techniques to remove from the NLI sentence encoder the features that allow a hypothesis-only model to succeed.&quot;,&quot;However, we believe that in general, the features used by the hypothesis-only model can include some information necessary to perform the NLI task, and removing such information from the sentence representation can hurt the performance of the full model.&quot;,&quot;Their approach consequently degrades the performance on the hard SNLI set, which is expected to be less biased.&quot;,&quot;In contrast, we propose to train a bias-only model to use its predictions to dynamically adapt the classification loss to reduce the importance of the most biased examples.&quot;,&quot;Concurrently to our work, Clark et al. (2019) and He et al. (2019) have also proposed to use the product of experts (PoE) models for avoiding biases.&quot;,&quot;They train their models in two stages, first training a bias-only model and then using it to train a robust model.&quot;,&quot;In contrast, our methods are trained in an end-to-end manner, which is convenient in practice.&quot;,&quot;We additionally show that our proposed Debiased Focal Loss model is an effective method to reduce biases, sometimes superior to PoE.&quot;,&quot;We have evaluated on new domains of NLI hard sets and fact verification.&quot;,&quot;Moreover, we have included an analysis showing that our debiased models indeed have lower correlations with the bias-only models, and have extended our methods to guard against multiple bias patterns simultaneously.&quot;,&quot;We furthermore study transfer performance to other NLI datasets.&quot;,&quot;Problem formulation We consider a general multi-class classification problem.&quot;,&quot;Given a dataset D = { x i ,y i } Ni =1 consisting of the input data x i  X , and labels y i  Y , the goal of the base model is to learn a mapping f M parameterized by  M that computes the predictions over the label space given the input data, shown as f M : X  R |Y| .&quot;,&quot;Our goal is to optimize  M parameters such that we build a model that is more resistant to benchmark dataset biases, to improve its robustness to domain changes where the biases typically observed in the training data do not exist in the evaluation dataset.&quot;,&quot;The key idea of our approach, depicted in Figure 1, is first to identify the dataset biases that the base model is susceptible to relying on, and define a bias-only model to capture them.&quot;,&quot;We then propose two strategies to incorporate this bias-only knowledge into the training of the base model to make it robust against the biases.&quot;,&quot;After training, we remove the bias-only model and use the predictions of the base model.&quot;,&quot;We assume that we do not have access to any data from the out-of-domain dataset, so we need to know a priori about the possible types of shortcuts we would like the base model to avoid relying on.&quot;,&quot;Once these patterns are identified, we train a bias-only model designed to capture the identified shortcuts that only uses biased features .&quot;,&quot;For instance, a hypothesis-only model in the large-scale NLI datasets can correctly classify the majority of samples using annotation artifacts (Poliak et al., 2018; Gururangan et al., 2018).&quot;,&quot;Motivated by this work, our bias-only model for NLI only uses hypothesis sentences.&quot;,&quot;Note that the bias-only model can, in general, have any form, and is not limited to models using only a part of the input data.&quot;,&quot;For instance, on the HANS dataset, our bias-only model makes use of syntactic heuristics and similarity features (see Section 4.3).&quot;,&quot;Let x bi  X b be biased features of x i that are predictive of y i .&quot;,&quot;We then formalize this bias-only model as a mapping f B : X b  R |Y| , parameterized by  B and trained using cross-entropy (CE) loss LB : LB (  B )=  1 NN (cid:88) i =1 log(  ( f y i B ( x bi ;  B ))) , (1) where f jB ( x bi , B ) is the j th element of f B ( . ) , and  ( u j )= e u j / (cid:80) |Y| k =1 e u k is the softmax function.&quot;,&quot;We propose two strategies to incorporate the bias-only f B knowledge into the training of the base model f M .&quot;,&quot;In our strategies, the predictions of the bias-only model are combined with either the predictions of the base model or its error, to down-weight the loss for the examples that the bias-only model can predict correctly.&quot;,&quot;We then update parameters of the base model  M based on this modified loss LC .&quot;,&quot;Our learning strategies are end-to-end.&quot;,&quot;Therefore, to prevent the base model from learning the biases, the bias-only loss LB is not back-propagated to any shared parameters of the base model, such as a shared sentence encoder.&quot;,&quot;Our first approach is based on the product of experts (PoE) method (Hinton, 2002).&quot;,&quot;Here, we use this method to combine the bias-only and base model's predictions by computing the element-wise product (cid:12) between their predictions as  ( f B ( x bi )) (cid:12)  ( f M ( x i )) .&quot;,&quot;We compute this combination in the logarithmic space, making it appropriate for the normalized exponential below: f C ( x i , x bi )=log(  ( f B ( x bi )))+log(  ( f M ( x i ))) , The key intuition behind this model is to combine the probability distributions of the bias-only and the base model to allow them to make predictions based on different characteristics of the input; the bias-only branch covers prediction based on biases, and the base model focuses on learning the actual task.&quot;,&quot;Then the base model parameters  M are trained using the cross-entropy loss LC of the combined classifier f C : LC (  M ;  B )=  1 NN (cid:88) i =1 log(  ( f y i C ( x i , x bi ))) .&quot;,&quot;the updates for examples that it can accurately predict.&quot;,&quot;Justification: Probability of label y i for the example x i in the PoE model is computed as:  ( f y i C ( x i , x bi ))=  ( f y i B ( x bi ))  ( f y i M ( x i )) (cid:80) |Y| k =1  ( f kB ( x bi ))  ( f kM ( x i )) Then the gradient of cross-entropy loss of the combined classifier (2) w.r.t  M is (Hinton, 2002):   MLC (  M ;  B )=  1 NN (cid:88) i =1 |Y| (cid:88) k =1 (cid:20) (cid:16)  y i k   ( f kC ( x i , x bi )) (cid:17)   M log(  ( f kM ( x i ))) (cid:21) , where  y i k is 1 when k = y i and 0 otherwise.&quot;,&quot;Generally, the closer the ensemble's prediction  ( f kC ( . )) is to the target  y i k , the more the gradient is decreased through the modulating term, which only happens when the bias-only and base models are both capturing biases.&quot;,&quot;In the extreme case, when the bias-only model correctly classifies the sample,  ( f y i C ( x i , x bi )) = 1 and therefore   MLC (  M ;  B ) = 0 , the biased examples are ignored during training.&quot;,&quot;Conversely, when the example is fully unbiased, the bias-only classifier predicts the uniform distribution over all labels  ( f kB ( x bi )) = 1 |Y| for k  Y , therefore  ( f y i C ( x i , x bi )) =  ( f y i M ( x i )) and the gradient of ensemble classifier remains the same as the CE loss.&quot;,&quot;Focal loss was originally proposed in Lin et al. (2017) to improve a single classifier by down-weighting the well-classified points.&quot;,&quot;We propose a novel variant of this loss that leverages the bias-only branch's predictions to reduce the relative importance of the most biased examples and allows the model to focus on learning the hard examples.&quot;,&quot;We define Debiased Focal Loss (DFL) as: LC (  M ;  B )= (3)  1 NN (cid:88) i =1 (cid:16) 1   ( f y i B ( x bi )) (cid:17)  log(  ( f y i M ( x i ))) where  is the focusing parameter, which impacts the down-weighting rate.&quot;,&quot;When  is set to 0, DFL is equivalent to the cross-entropy loss.&quot;,&quot;For  > 0 , as the value of  is increased, the effect of down-weighting is increased.&quot;,&quot;We set  =2 through all experiments, which works well in practice, and avoid fine-tuning it further.&quot;,&quot;We note the properties of this loss: (1) When the example x i is unbiased, and the bias-only branch does not do well,  ( f y i B ( x bi )) is small, therefore the scaling factor is close to 1 , and the loss remains unaffected.&quot;,&quot;(2) As the sample is more biased and  ( f y i B ( x bi )) is closer to 1, the modulating factor approaches 0 and the loss for the most biased examples is down-weighted.&quot;,&quot;We compare our models to RUBi (Cadene et al., 2019), a recently proposed model to alleviate unimodal biases learned by Visual Question Answering (VQA) models.&quot;,&quot;Cadene et al. (2019)'s study is limited to VQA datasets.&quot;,&quot;We, however, evaluate the effectiveness of their formulation on multiple challenging NLU benchmarks.&quot;,&quot;RUBi consists in first applying a sigmoid function  to the bias-only model's predictions to obtain a mask containing an importance weight between 0 and 1 for each label.&quot;,&quot;It then computes the element-wise product between the obtained mask and the base model's predictions: f C ( x i , x bi )= f M ( x i ) (cid:12)  ( f B ( x bi )) , The main intuition is to dynamically adjust the predictions of the base model to prevent it from leveraging the shortcuts.&quot;,&quot;Then the parameters of the base model  M are updated by back-propagating the cross-entropy loss LC of the combined classifier.&quot;,&quot;Neural models can, in practice, be prone to multiple types of biases in the datasets.&quot;,&quot;We, therefore, propose methods for combining several bias-only models.&quot;,&quot;To avoid learning relations between biased features, we do not consider training a classifier on top of their concatenation.&quot;,&quot;Instead, let { x b j i } Kj =1 be different sets of biased features of x i that are predictive of y i , and let f B j be an individual bias-only model capturing x b j i .&quot;,&quot;Next, we extend our debiasing strategies to handle multiple bias patterns.&quot;,&quot;Method 1: Joint Product of Experts We extend our proposed PoE model to multiple bias-only models by computing the element-wise product between the predictions of bias-only models and the base model as:  ( f B 1 ( x b 1 i )) (cid:12)(cid:12)  ( f BK ( x b K i )) (cid:12)  ( f M ( x i )) , computed in the logarithmic space: f C ( x i , { x b j i } Kj =1 )= K (cid:88) j =1 log(  ( f B j ( x b j i ))) +log(  ( f M ( x i ))) .&quot;,&quot;Then the base model parameters  M are trained using the cross-entropy loss of the combined classifier f C .&quot;,&quot;Method 2: Joint Debiased Focal Loss To extend DFL to handle multiple bias patterns, we first compute the element-wise average of the predictions of the multiple bias-only models: f B ( { x b j i } Kj =1 ) = 1 K (cid:80) Kj =1 f B j ( x b j i ) , and then compute the DFL (3) using the computed joint bias-only model.&quot;,&quot;We provide experiments on a fact verification (FEVER) and two large-scale NLI datasets (SNLI and MNLI).&quot;,&quot;We evaluate the models' performance on recently-proposed challenging unbiased evaluation sets.&quot;,&quot;We use the BERT (Devlin et al., 2019) implementation of Wolf et al. (2019) as our main baseline, known to work well for these tasks.&quot;,&quot;In all the experiments, we use the default hyperparameters of the baselines.&quot;,&quot;Dataset: The FEVER dataset contains claim-evidence pairs generated from Wikipedia.&quot;,&quot;Schuster et al. (2019) collected a new evaluation set for the FEVER dataset to avoid the idiosyncrasies observed in the claims of this benchmark.&quot;,&quot;They made the original claim-evidence pairs of the FEVER evaluation dataset symmetric, by augmenting them and making each claim and evidence appear with each label.&quot;,&quot;Therefore, by balancing the artifacts, relying on statistical cues in claims to classify samples is equivalent to a random guess.&quot;,&quot;The collected dataset is challenging, and the performance of the models relying on biases evaluated on this dataset drops significantly.&quot;,&quot;Base models: We consider BERT as the base model, which works the best on this dataset (Schuster et al., 2019), and predicts the relations based on the concatenation of the claim and the evidence with a delimiter token (see Appendix A).&quot;,&quot;Results: Table 1 shows the results.&quot;,&quot;Our proposed debiasing methods, PoE and DFL, are highly effective, boosting the performance of the baseline by 9.8 and 7.5 points respectively, significantly surpassing the prior work of Schuster et al. (2019).&quot;,&quot;Datasets: We evaluate on hard datasets of SNLI and MNLI (Gururangan et al., 2018), which are the splits of these datasets where a hypothesis-only model cannot correctly predict the labels.&quot;,&quot;Gururangan et al. (2018) show that the success of the recent textual entailment models is attributed to the biased examples, and the performance of these models is substantially lower on the hard sets.&quot;,&quot;Base models: We consider BERT and InferSent (Conneau et al., 2017) as our base models.&quot;,&quot;We choose InferSent to be able to compare with the prior work of Belinkov et al. (2019b).&quot;,&quot;Bias-only model: The bias-only model predicts the labels using the hypothesis (Appendix B).&quot;,&quot;Results on SNLI: Table 2 shows the SNLI results.&quot;,&quot;With InferSent, DFL and PoE result in 4.1 and 4.8 points gain.&quot;,&quot;With BERT, DFL and PoE improve the results by 2.5 and 1.6 absolute points.&quot;,&quot;Compared to the prior work of Belinkov et al. (2019b) (AdvCls), our PoE model obtains a 7.4 points gain, setting a new state-of-the-art.&quot;,&quot;Results on MNLI: We construct hard sets from the validation sets of MNLI Matched and Mismatched (MNLI-M).&quot;,&quot;Following Gururangan et al. (2018), we train a fastText classifier (Joulin et al., 2017) that predicts the labels using only the hypothesis and consider the subset on which it fails as hard examples.&quot;,&quot;We report the results on MNLI mismatched sets in Table 3 (see Appendix B for similar results on MNLI matched).&quot;,&quot;With BERT, DFL and PoE obtain 1.4 and 1.7 points gain on the hard development set, while with InferSent, they improve the results by 2.5 and 2.6 points.&quot;,&quot;To comply with limited access to the MNLI submission system, we evaluate only the best result of the baselines and our models on the test sets.&quot;,&quot;Our PoE model improves the performance on the hard test set by 1.1 points while retaining in-domain accuracy.&quot;,&quot;Dataset: McCoy et al. (2019b) show that NLI models trained on MNLI can adopt superficial syntactic heuristics.&quot;,&quot;They introduce HANS, consisting of several examples on which the syntactic heuristics fail.&quot;,&quot;Base model: We use BERT as our base model and train it on the MNLI dataset.&quot;,&quot;Bias-only model: We consider the following features for the bias-only model.&quot;,&quot;The first four features are based on the syntactic heuristics proposed in McCoy et al. (2019b):&quot;,&quot;1) Whether all words in the hypothesis are included in the premise;&quot;,&quot;2) If the hypothesis is the contiguous subsequence of the premise;&quot;,&quot;3) If the hypothesis is a subtree in the premise's parse tree;&quot;,&quot;4) The number of tokens shared between premise and hypothesis normalized by the number of tokens in the premise.&quot;,&quot;We additionally include some similarity features:&quot;,&quot;5) The cosine similarity between premise and hypothesis's pooled token representations from BERT followed by min, mean, and max-pooling.&quot;,&quot;We consider the same weight for contradiction and neutral labels in the bias-only loss to allow the model to recognize entailment from not-entailment.&quot;,&quot;During the evaluation, we map the neutral and contradiction labels to not-entailment.&quot;,&quot;Results: McCoy et al. (2019a) observe large variability in the linguistic generalization of neural models.&quot;,&quot;We, therefore, report the averaged results across 4 runs with the standard deviation in Table 4.&quot;,&quot;PoE and DFL obtain 4.4 and 7.4 points gain (see Appendix C for accuracy on individual heuristics of HANS).&quot;,&quot;We compare our results with the concurrent work of Clark et al., who propose a PoE model similar to ours, which gets similar results.&quot;,&quot;The main difference is that our models are trained end-to-end, which is convenient in practice, while Clark et&quot;,&quot;al.'s method requires two steps, first training a bias-only model and then using this pre-trained model to train a robust model.&quot;,&quot;The Reweight baseline in Clark et al. is a special case of our DFL with  =1 and performs similarly to our DFL method (using default  =2 ).&quot;,&quot;Their Learned-Mixin+H method requires hyperparameter tuning.&quot;,&quot;Since the assumption is not having access to any out-of-domain test data, and there is no available dev set for HANS, it is challenging to perform hyper-parameter tuning.&quot;,&quot;Clark et al. follow prior work (Grand and Belinkov, 2019; Ramakrishnan et al., 2018) and perform model section on the test set.&quot;,&quot;To provide a fair comparison, we consequently also tuned  in DFL by sweeping over { 0 .&quot;,&quot;5 , 1 , 2 , 3 , 4 } .&quot;,&quot;DFL (cid:68) is the selected model, with  = 3 .&quot;,&quot;With this hyperparameter tuning, DFL is even more effective, and our best result performs 2.8 points better than Clark et al. (2019).&quot;,&quot;To evaluate combating multiple bias patterns, we jointly debias a base model on the hypothesis artifacts and syntactic biases.&quot;,&quot;Results: Table 5 shows the results.&quot;,&quot;Models trained to be robust to hypothesis biases ( (cid:168) ) do not generalize to HANS.&quot;,&quot;On the other hand, models trained to be robust on HANS ( (cid:170) ) use a powerful bias-only model resulting in a slight improvement on MNLI mismatched hard dev set.&quot;,&quot;We expect a slight degradation when debiasing for both biases since models need to select samples accommodating both debiasing needs.&quot;,&quot;The jointly debiased models successfully obtain improvements on both datasets, which are close to the improvements on each dataset by the individually debiased models.&quot;,&quot;To evaluate how well the baseline and proposed models generalize to solving textual entailment in domains that do not share the same annotation biases as the large NLI training sets, we take trained NLI models and test them on several NLI datasets.&quot;,&quot;Datasets: We consider a total of 12 different NLI datasets.&quot;,&quot;We use the 11 datasets studied by Poliak et al. (2018).&quot;,&quot;These datasets include MNLI, SNLI, SciTail (Khot et al., 2018), AddOneRTE (ADD1) (Pavlick and Callison-Burch, 2016), Johns Hopkins Ordinal Commonsense Inference (JOCI) (Zhang et al., 2017), Multiple Premise Entailment (MPE) (Lai et al., 2017), Sentences Involving Compositional Knowledge (SICK) (Marelli et al., 2014), and three datasets from White et al. (2017) which are automatically generated from existing datasets for other NLP tasks including: Semantic Proto-Roles (SPR) (Reisinger et al., 2015), Definite Pronoun Resolution (DPR) (Rahman and Ng, 2012), FrameNet Plus (FN+) (Pavlick et al., 2015), and the GLUE benchmark's diagnostic test (Wang et al., 2019).&quot;,&quot;We additionally consider the Quora Question Pairs (QQP) dataset, where the task is to determine whether two given questions are semantically matching (duplicate) or not.&quot;,&quot;As in Gong et al. (2017), we interpret duplicate question pairs as an entailment relation and neutral otherwise.&quot;,&quot;We use the same split ratio mentioned by Wang et al. (2017).&quot;,&quot;Since the datasets considered have different label spaces, when evaluating on each target dataset, we map the model's labels to the corresponding target dataset's space.&quot;,&quot;See Appendix D for more details.&quot;,&quot;We strictly refrained from using any out-of-domain data when evaluating on the unbiased split of the same benchmark in Section 4.&quot;,&quot;However, as shown by prior work (Belinkov et al., 2019a), since different NLI target datasets contain different amounts of the bias found in the large-scale NLI dataset, we need to adjust the amount of debiasing according to each target dataset.&quot;,&quot;We consequently introduce a hyperparameter  for PoE to modulate the strength of the bias-only model in ensembling.&quot;,&quot;We follow prior work (Belinkov et al., 2019a) and perform model selection on the dev set of each target dataset Data CE DFL  PoE  SICK 57.05 57.91 +0.9 57.28 +0.2 ADD1 87.34 88.89 +1.5 87.86 +0.5 DPR 49.50 50.68 +1.2 50.14 +0.6 SPR 59.85 61.41 +1.6 62.45 +2.6 FN+ 53.16 54.77 +1.6 53.51 +0.4 JOCI 50.06 51.13 +1.1 50.85 +0.8 MPE 69.50 70.2 +0.7 70.1 +0.6 SCITAIL 67.64 69.33 +1.7 71.40 +3.8 GLUE 54.08 54.80 +0.7 54.71 +0.6 QQP 67.78 69.28 +1.5 68.61 +0.8 MNLI 74.40 73.58 -0.8 73.61 -0.8 MNLI-M 73.98 74.0 0.0 73.49 -0.5 Table 6: Accuracy results of models with BERT transferring to new target datasets.&quot;,&quot;Results: Table 6 shows the results of the debiased models and baseline with BERT.&quot;,&quot;As shown in prior work (Belinkov et al., 2019a), the MNLI datasets have very similar biases to SNLI, which the models are trained on, so we do not expect any improvement in the relative performance of our models and the baseline for MNLI and MNLI-M.&quot;,&quot;On all the remaining datasets, our proposed models perform better than the baseline, showing a substantial improvement in generalization by using our debasing techniques.&quot;,&quot;We additionally compare with Belinkov et al. (2019a) in Appendix D and show that our methods substantially surpass their results.&quot;,&quot;4 Since the test sets are not available for MNLI, we tune on the matched dev set and evaluate on the mismatched dev set or vice versa.&quot;,&quot;For GLUE, we tune on MNLI mismatched dev set.&quot;,&quot;Analysis of Debiased Focal Loss: As expected, improving the out-of-domain performance could come at the expense of decreased in-domain performance since the removed biases are useful for performing the in-domain task.&quot;,&quot;This happens especially for DFL, in which there is a trade-off between in-domain and out-of-domain performance that depends on the parameter  , and when the baseline model is not very powerful like InferSent.&quot;,&quot;To understand the impact of  in DFL, we train an InferSent model using DFL for different values of  on the SNLI dataset and evaluate its performance on SNLI test and SNLI hard sets.&quot;,&quot;As illustrated in Figure 2, increasing  increases debiasing and thus hurts in-domain accuracy on SNLI, but out-of-domain accuracy on the SNLI hard set is increased within a wide range of values (see a similar plot for BERT in Appendix E).&quot;,&quot;Correlation Analysis: In contrast to Belinkov et al. (2019a), who encourage only the encoder to not capture the unwanted biases, our learning strategies influence the parameters of the full model to reduce the reliance on unwanted patterns more effectively.&quot;,&quot;To test this assumption, in Figure 3, we report the correlation between the element-wise loss of the debiased models and the loss of a bias-only model on the considered datasets.&quot;,&quot;The results show that compared to the baselines, our debiasing methods, DFL and PoE, reduce the correlation to the bias-only model, confirming that our models are effective at reducing biases.&quot;,&quot;Interestingly, on MNLI, PoE has less correlation with the bias-only model than DFL and also has better performance on the unbiased split of this dataset.&quot;,&quot;On the other hand, on the HANS dataset, DFL loss is less correlated with the bias-only model than PoE and also obtains higher performance on the HANS dataset.&quot;,&quot;We propose two novel techniques, product-of-experts and debiased focal loss, to reduce biases learned by neural models, which are applicable whenever one can specify the biases in the form of one or more bias-only models.&quot;,&quot;The bias-only models are designed to leverage biases and shortcuts in the datasets.&quot;,&quot;Our debiasing strategies then work by adjusting the cross-entropy loss based on the performance of these bias-only models, to focus learning on the hard examples and downweight the importance of the biased examples.&quot;,&quot;Additionally, we extend our methods to combat multiple bias patterns simultaneously.&quot;,&quot;Our proposed debiasing techniques are model agnostic, simple, and highly effective.&quot;,&quot;Extensive experiments show that our methods substantially improve the model robustness to domain-shift, including 9.8 points gain on FEVER symmetric test set, 7.4 on HANS dataset, and 4.8 points on SNLI hard set.&quot;,&quot;Furthermore, we show that our debiasing techniques result in better generalization to other NLI datasets.&quot;,&quot;Future work may include developing debiasing strategies that do not require prior knowledge of bias patterns and can automatically identify them.&quot;,&quot;We would like to thank Daniel Andor and Suraj Srinivas for their helpful comments.&quot;,&quot;We additionally would like to thank the authors of Schuster et al. (2019); Cadene et al. (2019); McCoy et al. (2019b); Belinkov et al. (2019a) for their support to reproduce their results.&quot;,&quot;This research was supported by the Swiss National Science Foundation under the project Learning Representations of Abstraction for Opinion Summarization (LAOS), grant number FNS-30216.&quot;,&quot;Y.B. was supported by the Harvard Mind, Brain, and Behavior Initiative.&quot;],&quot;string&quot;:&quot;[\n  \&quot;Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios.\&quot;,\n  \&quot;We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets.\&quot;,\n  \&quot;The biases are specified in terms of one or more bias-only models , which learn to leverage the dataset biases.\&quot;,\n  \&quot;During training, the bias-only models' predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing training on the hard examples.\&quot;,\n  \&quot;We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data.\&quot;,\n  \&quot;Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets.\&quot;,\n  \&quot;Our code and data are publicly available in https: //github.com/rabeehk/robust-nli .\&quot;,\n  \&quot;Recent neural models (Devlin et al., 2019; Radford et al., 2018; Chen et al., 2017) have achieved high and even near human-performance on several large-scale natural language understanding benchmarks.\&quot;,\n  \&quot;However, it has been demonstrated that neural models tend to rely on existing idiosyncratic biases in the datasets, and leverage superficial correlations between the label and existing shortcuts in the training dataset to perform surprisingly well, 1 without learning the underlying task (Kaushik and Lipton, 2018; Gururangan et al., 2018; Poliak et al., 2018; Schuster et al., 2019; 1 We use biases, heuristics or shortcuts interchangeably. McCoy et al., 2019b).\&quot;,\n  \&quot;For instance, natural language inference (NLI) is supposed to test the ability of a model to determine whether a hypothesis sentence ( There is no teacher in the room ) can be inferred from a premise sentence ( Kids work at computers with a teacher's help ) (Dagan et al., 2006).\&quot;,\n  \&quot;2 However, recent work has demonstrated that large-scale NLI benchmarks contain annotation artifacts; certain words in the hypothesis that are highly indicative of inference class and allow models that do not consider the premise to perform unexpectedly well (Poliak et al., 2018; Gururangan et al., 2018).\&quot;,\n  \&quot;As an example, in some NLI benchmarks, negation words such as nobody, no, and not in the hypothesis are often highly correlated with the contradiction label.\&quot;,\n  \&quot;As a result of the existence of such biases, models exploiting statistical shortcuts during training often perform poorly on out-of-domain datasets, especially if the datasets are carefully designed to limit the spurious cues.\&quot;,\n  \&quot;To allow proper evaluation, recent studies have tried to create new evaluation datasets that do not contain such biases (Gururangan et al., 2018; Schuster et al., 2019; McCoy et al., 2019b).\&quot;,\n  \&quot;Unfortunately, it is hard to avoid spurious statistical cues in the construction of large-scale benchmarks, and collecting new datasets is costly (Sharma et al., 2018).\&quot;,\n  \&quot;It is, therefore, crucial to develop techniques to reduce the reliance on biases during the training of the neural models.\&quot;,\n  \&quot;We propose two end-to-end debiasing techniques that can be used when the existing bias patterns are identified.\&quot;,\n  \&quot;These methods work by adjusting the cross-entropy loss to reduce the biases learned from the training dataset, down-weighting the biased examples so that the model focuses on learning the hard examples.\&quot;,\n  \&quot;Figure 1 illustrates an example of applying our strategy to prevent an NLI model from predicting the labels using existing biases in the hypotheses, where the bias-only model only sees the hypothesis.\&quot;,\n  \&quot;Our strat-2 The given sentences are in the contradictory relation, and the hypothesis cannot be inferred from the premise.\&quot;,\n  \&quot;egy involves adding this bias-only branch f B on top of the base model f M during training.\&quot;,\n  \&quot;We then compute the combination of the two models f C in a way that motivates the base model to learn different strategies than the ones used by the bias-only branch f B .\&quot;,\n  \&quot;At the end of the training, we remove the bias-only classifier and use the predictions of the base model.\&quot;,\n  \&quot;In our first proposed method, Product of Experts, the training loss is computed on an ensemble of the base model and the bias-only model, which reduces the base model's loss for the examples that the bias-only model classifies correctly.\&quot;,\n  \&quot;For the second method, Debiased Focal Loss, the bias-only predictions are used to directly weight the loss of the base model, explicitly modulating the loss depending on the accuracy of the bias-only model.\&quot;,\n  \&quot;We also extend these methods to be robust against multiple sources of bias by training multiple bias-only models.\&quot;,\n  \&quot;Our approaches are simple and highly effective.\&quot;,\n  \&quot;They require training only a simple model on top of the base model.\&quot;,\n  \&quot;They are model agnostic and general enough to be applicable for addressing common biases seen in many datasets in different domains.\&quot;,\n  \&quot;We evaluate our models on challenging benchmarks in textual entailment and fact verification, including HANS (Heuristic Analysis for NLI Systems) (McCoy et al., 2019b), hard NLI sets (Gururangan et al., 2018) of Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and MultiNLI (MNLI) (Williams et al., 2018), and FEVER Symmetric test set (Schuster et al., 2019).\&quot;,\n  \&quot;The selected datasets are highly challenging and have been carefully designed to be unbiased to allow proper evaluation of the out-of-domain performance of the models.\&quot;,\n  \&quot;We additionally construct hard MNLI datasets from MNLI development sets to facilitate the out-of-domain evaluation on this dataset.\&quot;,\n  \&quot;3 We show that including our strategies on training baseline models, including BERT (Devlin et al., 2019), provides a substantial gain on out-of-domain performance in all the experiments.\&quot;,\n  \&quot;In summary, we make the following contributions:\&quot;,\n  \&quot;1) Proposing two debiasing strategies to train neural models robust to dataset bias.\&quot;,\n  \&quot;2) An empirical evaluation of the methods on two large-scale NLI datasets and a fact verification benchmark; obtaining a substantial gain on their challenging out-of-domain data, including 7.4 points on HANS, 4.8 points on SNLI hard set, and 9.8 points on FEVER symmetric test set, setting a new state-of-the-art.\&quot;,\n  \&quot;3) Proposing debiasing strategies capable of combating multiple sources of bias.\&quot;,\n  \&quot;4) Evaluating the transfer performance of the debiased models on 12 NLI datasets and demonstrating improved transfer to other NLI benchmarks.\&quot;,\n  \&quot;To facilitate future work, we release our datasets and code.\&quot;,\n  \&quot;To address dataset biases, researchers have proposed to augment datasets by balancing the existing cues (Schuster et al., 2019) or to create an adversarial dataset (Jia and Liang, 2017).\&quot;,\n  \&quot;However, collecting new datasets, especially at a large scale, is costly, and thus remains an unsatisfactory solution.\&quot;,\n  \&quot;It is, therefore, crucial to develop strategies to allow models to be trained on the existing biased datasets.\&quot;,\n  \&quot;3 Removing the need to submit to an online evaluation system for MNLI hard test sets.\&quot;,\n  \&quot;Schuster et al. (2019) propose to first compute the n-grams in the dataset's claims that are the most associated with each fact-verification label.\&quot;,\n  \&quot;They then solve an optimization problem to assign a balancing weight to each training sample to alleviate the biases.\&quot;,\n  \&quot;In contrast, we propose several end-to-end debiasing strategies.\&quot;,\n  \&quot;Additionally, Belinkov et al. (2019a) propose adversarial techniques to remove from the NLI sentence encoder the features that allow a hypothesis-only model to succeed.\&quot;,\n  \&quot;However, we believe that in general, the features used by the hypothesis-only model can include some information necessary to perform the NLI task, and removing such information from the sentence representation can hurt the performance of the full model.\&quot;,\n  \&quot;Their approach consequently degrades the performance on the hard SNLI set, which is expected to be less biased.\&quot;,\n  \&quot;In contrast, we propose to train a bias-only model to use its predictions to dynamically adapt the classification loss to reduce the importance of the most biased examples.\&quot;,\n  \&quot;Concurrently to our work, Clark et al. (2019) and He et al. (2019) have also proposed to use the product of experts (PoE) models for avoiding biases.\&quot;,\n  \&quot;They train their models in two stages, first training a bias-only model and then using it to train a robust model.\&quot;,\n  \&quot;In contrast, our methods are trained in an end-to-end manner, which is convenient in practice.\&quot;,\n  \&quot;We additionally show that our proposed Debiased Focal Loss model is an effective method to reduce biases, sometimes superior to PoE.\&quot;,\n  \&quot;We have evaluated on new domains of NLI hard sets and fact verification.\&quot;,\n  \&quot;Moreover, we have included an analysis showing that our debiased models indeed have lower correlations with the bias-only models, and have extended our methods to guard against multiple bias patterns simultaneously.\&quot;,\n  \&quot;We furthermore study transfer performance to other NLI datasets.\&quot;,\n  \&quot;Problem formulation We consider a general multi-class classification problem.\&quot;,\n  \&quot;Given a dataset D = { x i ,y i } Ni =1 consisting of the input data x i  X , and labels y i  Y , the goal of the base model is to learn a mapping f M parameterized by  M that computes the predictions over the label space given the input data, shown as f M : X  R |Y| .\&quot;,\n  \&quot;Our goal is to optimize  M parameters such that we build a model that is more resistant to benchmark dataset biases, to improve its robustness to domain changes where the biases typically observed in the training data do not exist in the evaluation dataset.\&quot;,\n  \&quot;The key idea of our approach, depicted in Figure 1, is first to identify the dataset biases that the base model is susceptible to relying on, and define a bias-only model to capture them.\&quot;,\n  \&quot;We then propose two strategies to incorporate this bias-only knowledge into the training of the base model to make it robust against the biases.\&quot;,\n  \&quot;After training, we remove the bias-only model and use the predictions of the base model.\&quot;,\n  \&quot;We assume that we do not have access to any data from the out-of-domain dataset, so we need to know a priori about the possible types of shortcuts we would like the base model to avoid relying on.\&quot;,\n  \&quot;Once these patterns are identified, we train a bias-only model designed to capture the identified shortcuts that only uses biased features .\&quot;,\n  \&quot;For instance, a hypothesis-only model in the large-scale NLI datasets can correctly classify the majority of samples using annotation artifacts (Poliak et al., 2018; Gururangan et al., 2018).\&quot;,\n  \&quot;Motivated by this work, our bias-only model for NLI only uses hypothesis sentences.\&quot;,\n  \&quot;Note that the bias-only model can, in general, have any form, and is not limited to models using only a part of the input data.\&quot;,\n  \&quot;For instance, on the HANS dataset, our bias-only model makes use of syntactic heuristics and similarity features (see Section 4.3).\&quot;,\n  \&quot;Let x bi  X b be biased features of x i that are predictive of y i .\&quot;,\n  \&quot;We then formalize this bias-only model as a mapping f B : X b  R |Y| , parameterized by  B and trained using cross-entropy (CE) loss LB : LB (  B )=  1 NN (cid:88) i =1 log(  ( f y i B ( x bi ;  B ))) , (1) where f jB ( x bi , B ) is the j th element of f B ( . ) , and  ( u j )= e u j / (cid:80) |Y| k =1 e u k is the softmax function.\&quot;,\n  \&quot;We propose two strategies to incorporate the bias-only f B knowledge into the training of the base model f M .\&quot;,\n  \&quot;In our strategies, the predictions of the bias-only model are combined with either the predictions of the base model or its error, to down-weight the loss for the examples that the bias-only model can predict correctly.\&quot;,\n  \&quot;We then update parameters of the base model  M based on this modified loss LC .\&quot;,\n  \&quot;Our learning strategies are end-to-end.\&quot;,\n  \&quot;Therefore, to prevent the base model from learning the biases, the bias-only loss LB is not back-propagated to any shared parameters of the base model, such as a shared sentence encoder.\&quot;,\n  \&quot;Our first approach is based on the product of experts (PoE) method (Hinton, 2002).\&quot;,\n  \&quot;Here, we use this method to combine the bias-only and base model's predictions by computing the element-wise product (cid:12) between their predictions as  ( f B ( x bi )) (cid:12)  ( f M ( x i )) .\&quot;,\n  \&quot;We compute this combination in the logarithmic space, making it appropriate for the normalized exponential below: f C ( x i , x bi )=log(  ( f B ( x bi )))+log(  ( f M ( x i ))) , The key intuition behind this model is to combine the probability distributions of the bias-only and the base model to allow them to make predictions based on different characteristics of the input; the bias-only branch covers prediction based on biases, and the base model focuses on learning the actual task.\&quot;,\n  \&quot;Then the base model parameters  M are trained using the cross-entropy loss LC of the combined classifier f C : LC (  M ;  B )=  1 NN (cid:88) i =1 log(  ( f y i C ( x i , x bi ))) .\&quot;,\n  \&quot;the updates for examples that it can accurately predict.\&quot;,\n  \&quot;Justification: Probability of label y i for the example x i in the PoE model is computed as:  ( f y i C ( x i , x bi ))=  ( f y i B ( x bi ))  ( f y i M ( x i )) (cid:80) |Y| k =1  ( f kB ( x bi ))  ( f kM ( x i )) Then the gradient of cross-entropy loss of the combined classifier (2) w.r.t  M is (Hinton, 2002):   MLC (  M ;  B )=  1 NN (cid:88) i =1 |Y| (cid:88) k =1 (cid:20) (cid:16)  y i k   ( f kC ( x i , x bi )) (cid:17)   M log(  ( f kM ( x i ))) (cid:21) , where  y i k is 1 when k = y i and 0 otherwise.\&quot;,\n  \&quot;Generally, the closer the ensemble's prediction  ( f kC ( . )) is to the target  y i k , the more the gradient is decreased through the modulating term, which only happens when the bias-only and base models are both capturing biases.\&quot;,\n  \&quot;In the extreme case, when the bias-only model correctly classifies the sample,  ( f y i C ( x i , x bi )) = 1 and therefore   MLC (  M ;  B ) = 0 , the biased examples are ignored during training.\&quot;,\n  \&quot;Conversely, when the example is fully unbiased, the bias-only classifier predicts the uniform distribution over all labels  ( f kB ( x bi )) = 1 |Y| for k  Y , therefore  ( f y i C ( x i , x bi )) =  ( f y i M ( x i )) and the gradient of ensemble classifier remains the same as the CE loss.\&quot;,\n  \&quot;Focal loss was originally proposed in Lin et al. (2017) to improve a single classifier by down-weighting the well-classified points.\&quot;,\n  \&quot;We propose a novel variant of this loss that leverages the bias-only branch's predictions to reduce the relative importance of the most biased examples and allows the model to focus on learning the hard examples.\&quot;,\n  \&quot;We define Debiased Focal Loss (DFL) as: LC (  M ;  B )= (3)  1 NN (cid:88) i =1 (cid:16) 1   ( f y i B ( x bi )) (cid:17)  log(  ( f y i M ( x i ))) where  is the focusing parameter, which impacts the down-weighting rate.\&quot;,\n  \&quot;When  is set to 0, DFL is equivalent to the cross-entropy loss.\&quot;,\n  \&quot;For  > 0 , as the value of  is increased, the effect of down-weighting is increased.\&quot;,\n  \&quot;We set  =2 through all experiments, which works well in practice, and avoid fine-tuning it further.\&quot;,\n  \&quot;We note the properties of this loss: (1) When the example x i is unbiased, and the bias-only branch does not do well,  ( f y i B ( x bi )) is small, therefore the scaling factor is close to 1 , and the loss remains unaffected.\&quot;,\n  \&quot;(2) As the sample is more biased and  ( f y i B ( x bi )) is closer to 1, the modulating factor approaches 0 and the loss for the most biased examples is down-weighted.\&quot;,\n  \&quot;We compare our models to RUBi (Cadene et al., 2019), a recently proposed model to alleviate unimodal biases learned by Visual Question Answering (VQA) models.\&quot;,\n  \&quot;Cadene et al. (2019)'s study is limited to VQA datasets.\&quot;,\n  \&quot;We, however, evaluate the effectiveness of their formulation on multiple challenging NLU benchmarks.\&quot;,\n  \&quot;RUBi consists in first applying a sigmoid function  to the bias-only model's predictions to obtain a mask containing an importance weight between 0 and 1 for each label.\&quot;,\n  \&quot;It then computes the element-wise product between the obtained mask and the base model's predictions: f C ( x i , x bi )= f M ( x i ) (cid:12)  ( f B ( x bi )) , The main intuition is to dynamically adjust the predictions of the base model to prevent it from leveraging the shortcuts.\&quot;,\n  \&quot;Then the parameters of the base model  M are updated by back-propagating the cross-entropy loss LC of the combined classifier.\&quot;,\n  \&quot;Neural models can, in practice, be prone to multiple types of biases in the datasets.\&quot;,\n  \&quot;We, therefore, propose methods for combining several bias-only models.\&quot;,\n  \&quot;To avoid learning relations between biased features, we do not consider training a classifier on top of their concatenation.\&quot;,\n  \&quot;Instead, let { x b j i } Kj =1 be different sets of biased features of x i that are predictive of y i , and let f B j be an individual bias-only model capturing x b j i .\&quot;,\n  \&quot;Next, we extend our debiasing strategies to handle multiple bias patterns.\&quot;,\n  \&quot;Method 1: Joint Product of Experts We extend our proposed PoE model to multiple bias-only models by computing the element-wise product between the predictions of bias-only models and the base model as:  ( f B 1 ( x b 1 i )) (cid:12)(cid:12)  ( f BK ( x b K i )) (cid:12)  ( f M ( x i )) , computed in the logarithmic space: f C ( x i , { x b j i } Kj =1 )= K (cid:88) j =1 log(  ( f B j ( x b j i ))) +log(  ( f M ( x i ))) .\&quot;,\n  \&quot;Then the base model parameters  M are trained using the cross-entropy loss of the combined classifier f C .\&quot;,\n  \&quot;Method 2: Joint Debiased Focal Loss To extend DFL to handle multiple bias patterns, we first compute the element-wise average of the predictions of the multiple bias-only models: f B ( { x b j i } Kj =1 ) = 1 K (cid:80) Kj =1 f B j ( x b j i ) , and then compute the DFL (3) using the computed joint bias-only model.\&quot;,\n  \&quot;We provide experiments on a fact verification (FEVER) and two large-scale NLI datasets (SNLI and MNLI).\&quot;,\n  \&quot;We evaluate the models' performance on recently-proposed challenging unbiased evaluation sets.\&quot;,\n  \&quot;We use the BERT (Devlin et al., 2019) implementation of Wolf et al. (2019) as our main baseline, known to work well for these tasks.\&quot;,\n  \&quot;In all the experiments, we use the default hyperparameters of the baselines.\&quot;,\n  \&quot;Dataset: The FEVER dataset contains claim-evidence pairs generated from Wikipedia.\&quot;,\n  \&quot;Schuster et al. (2019) collected a new evaluation set for the FEVER dataset to avoid the idiosyncrasies observed in the claims of this benchmark.\&quot;,\n  \&quot;They made the original claim-evidence pairs of the FEVER evaluation dataset symmetric, by augmenting them and making each claim and evidence appear with each label.\&quot;,\n  \&quot;Therefore, by balancing the artifacts, relying on statistical cues in claims to classify samples is equivalent to a random guess.\&quot;,\n  \&quot;The collected dataset is challenging, and the performance of the models relying on biases evaluated on this dataset drops significantly.\&quot;,\n  \&quot;Base models: We consider BERT as the base model, which works the best on this dataset (Schuster et al., 2019), and predicts the relations based on the concatenation of the claim and the evidence with a delimiter token (see Appendix A).\&quot;,\n  \&quot;Results: Table 1 shows the results.\&quot;,\n  \&quot;Our proposed debiasing methods, PoE and DFL, are highly effective, boosting the performance of the baseline by 9.8 and 7.5 points respectively, significantly surpassing the prior work of Schuster et al. (2019).\&quot;,\n  \&quot;Datasets: We evaluate on hard datasets of SNLI and MNLI (Gururangan et al., 2018), which are the splits of these datasets where a hypothesis-only model cannot correctly predict the labels.\&quot;,\n  \&quot;Gururangan et al. (2018) show that the success of the recent textual entailment models is attributed to the biased examples, and the performance of these models is substantially lower on the hard sets.\&quot;,\n  \&quot;Base models: We consider BERT and InferSent (Conneau et al., 2017) as our base models.\&quot;,\n  \&quot;We choose InferSent to be able to compare with the prior work of Belinkov et al. (2019b).\&quot;,\n  \&quot;Bias-only model: The bias-only model predicts the labels using the hypothesis (Appendix B).\&quot;,\n  \&quot;Results on SNLI: Table 2 shows the SNLI results.\&quot;,\n  \&quot;With InferSent, DFL and PoE result in 4.1 and 4.8 points gain.\&quot;,\n  \&quot;With BERT, DFL and PoE improve the results by 2.5 and 1.6 absolute points.\&quot;,\n  \&quot;Compared to the prior work of Belinkov et al. (2019b) (AdvCls), our PoE model obtains a 7.4 points gain, setting a new state-of-the-art.\&quot;,\n  \&quot;Results on MNLI: We construct hard sets from the validation sets of MNLI Matched and Mismatched (MNLI-M).\&quot;,\n  \&quot;Following Gururangan et al. (2018), we train a fastText classifier (Joulin et al., 2017) that predicts the labels using only the hypothesis and consider the subset on which it fails as hard examples.\&quot;,\n  \&quot;We report the results on MNLI mismatched sets in Table 3 (see Appendix B for similar results on MNLI matched).\&quot;,\n  \&quot;With BERT, DFL and PoE obtain 1.4 and 1.7 points gain on the hard development set, while with InferSent, they improve the results by 2.5 and 2.6 points.\&quot;,\n  \&quot;To comply with limited access to the MNLI submission system, we evaluate only the best result of the baselines and our models on the test sets.\&quot;,\n  \&quot;Our PoE model improves the performance on the hard test set by 1.1 points while retaining in-domain accuracy.\&quot;,\n  \&quot;Dataset: McCoy et al. (2019b) show that NLI models trained on MNLI can adopt superficial syntactic heuristics.\&quot;,\n  \&quot;They introduce HANS, consisting of several examples on which the syntactic heuristics fail.\&quot;,\n  \&quot;Base model: We use BERT as our base model and train it on the MNLI dataset.\&quot;,\n  \&quot;Bias-only model: We consider the following features for the bias-only model.\&quot;,\n  \&quot;The first four features are based on the syntactic heuristics proposed in McCoy et al. (2019b):\&quot;,\n  \&quot;1) Whether all words in the hypothesis are included in the premise;\&quot;,\n  \&quot;2) If the hypothesis is the contiguous subsequence of the premise;\&quot;,\n  \&quot;3) If the hypothesis is a subtree in the premise's parse tree;\&quot;,\n  \&quot;4) The number of tokens shared between premise and hypothesis normalized by the number of tokens in the premise.\&quot;,\n  \&quot;We additionally include some similarity features:\&quot;,\n  \&quot;5) The cosine similarity between premise and hypothesis's pooled token representations from BERT followed by min, mean, and max-pooling.\&quot;,\n  \&quot;We consider the same weight for contradiction and neutral labels in the bias-only loss to allow the model to recognize entailment from not-entailment.\&quot;,\n  \&quot;During the evaluation, we map the neutral and contradiction labels to not-entailment.\&quot;,\n  \&quot;Results: McCoy et al. (2019a) observe large variability in the linguistic generalization of neural models.\&quot;,\n  \&quot;We, therefore, report the averaged results across 4 runs with the standard deviation in Table 4.\&quot;,\n  \&quot;PoE and DFL obtain 4.4 and 7.4 points gain (see Appendix C for accuracy on individual heuristics of HANS).\&quot;,\n  \&quot;We compare our results with the concurrent work of Clark et al., who propose a PoE model similar to ours, which gets similar results.\&quot;,\n  \&quot;The main difference is that our models are trained end-to-end, which is convenient in practice, while Clark et\&quot;,\n  \&quot;al.'s method requires two steps, first training a bias-only model and then using this pre-trained model to train a robust model.\&quot;,\n  \&quot;The Reweight baseline in Clark et al. is a special case of our DFL with  =1 and performs similarly to our DFL method (using default  =2 ).\&quot;,\n  \&quot;Their Learned-Mixin+H method requires hyperparameter tuning.\&quot;,\n  \&quot;Since the assumption is not having access to any out-of-domain test data, and there is no available dev set for HANS, it is challenging to perform hyper-parameter tuning.\&quot;,\n  \&quot;Clark et al. follow prior work (Grand and Belinkov, 2019; Ramakrishnan et al., 2018) and perform model section on the test set.\&quot;,\n  \&quot;To provide a fair comparison, we consequently also tuned  in DFL by sweeping over { 0 .\&quot;,\n  \&quot;5 , 1 , 2 , 3 , 4 } .\&quot;,\n  \&quot;DFL (cid:68) is the selected model, with  = 3 .\&quot;,\n  \&quot;With this hyperparameter tuning, DFL is even more effective, and our best result performs 2.8 points better than Clark et al. (2019).\&quot;,\n  \&quot;To evaluate combating multiple bias patterns, we jointly debias a base model on the hypothesis artifacts and syntactic biases.\&quot;,\n  \&quot;Results: Table 5 shows the results.\&quot;,\n  \&quot;Models trained to be robust to hypothesis biases ( (cid:168) ) do not generalize to HANS.\&quot;,\n  \&quot;On the other hand, models trained to be robust on HANS ( (cid:170) ) use a powerful bias-only model resulting in a slight improvement on MNLI mismatched hard dev set.\&quot;,\n  \&quot;We expect a slight degradation when debiasing for both biases since models need to select samples accommodating both debiasing needs.\&quot;,\n  \&quot;The jointly debiased models successfully obtain improvements on both datasets, which are close to the improvements on each dataset by the individually debiased models.\&quot;,\n  \&quot;To evaluate how well the baseline and proposed models generalize to solving textual entailment in domains that do not share the same annotation biases as the large NLI training sets, we take trained NLI models and test them on several NLI datasets.\&quot;,\n  \&quot;Datasets: We consider a total of 12 different NLI datasets.\&quot;,\n  \&quot;We use the 11 datasets studied by Poliak et al. (2018).\&quot;,\n  \&quot;These datasets include MNLI, SNLI, SciTail (Khot et al., 2018), AddOneRTE (ADD1) (Pavlick and Callison-Burch, 2016), Johns Hopkins Ordinal Commonsense Inference (JOCI) (Zhang et al., 2017), Multiple Premise Entailment (MPE) (Lai et al., 2017), Sentences Involving Compositional Knowledge (SICK) (Marelli et al., 2014), and three datasets from White et al. (2017) which are automatically generated from existing datasets for other NLP tasks including: Semantic Proto-Roles (SPR) (Reisinger et al., 2015), Definite Pronoun Resolution (DPR) (Rahman and Ng, 2012), FrameNet Plus (FN+) (Pavlick et al., 2015), and the GLUE benchmark's diagnostic test (Wang et al., 2019).\&quot;,\n  \&quot;We additionally consider the Quora Question Pairs (QQP) dataset, where the task is to determine whether two given questions are semantically matching (duplicate) or not.\&quot;,\n  \&quot;As in Gong et al. (2017), we interpret duplicate question pairs as an entailment relation and neutral otherwise.\&quot;,\n  \&quot;We use the same split ratio mentioned by Wang et al. (2017).\&quot;,\n  \&quot;Since the datasets considered have different label spaces, when evaluating on each target dataset, we map the model's labels to the corresponding target dataset's space.\&quot;,\n  \&quot;See Appendix D for more details.\&quot;,\n  \&quot;We strictly refrained from using any out-of-domain data when evaluating on the unbiased split of the same benchmark in Section 4.\&quot;,\n  \&quot;However, as shown by prior work (Belinkov et al., 2019a), since different NLI target datasets contain different amounts of the bias found in the large-scale NLI dataset, we need to adjust the amount of debiasing according to each target dataset.\&quot;,\n  \&quot;We consequently introduce a hyperparameter  for PoE to modulate the strength of the bias-only model in ensembling.\&quot;,\n  \&quot;We follow prior work (Belinkov et al., 2019a) and perform model selection on the dev set of each target dataset Data CE DFL  PoE  SICK 57.05 57.91 +0.9 57.28 +0.2 ADD1 87.34 88.89 +1.5 87.86 +0.5 DPR 49.50 50.68 +1.2 50.14 +0.6 SPR 59.85 61.41 +1.6 62.45 +2.6 FN+ 53.16 54.77 +1.6 53.51 +0.4 JOCI 50.06 51.13 +1.1 50.85 +0.8 MPE 69.50 70.2 +0.7 70.1 +0.6 SCITAIL 67.64 69.33 +1.7 71.40 +3.8 GLUE 54.08 54.80 +0.7 54.71 +0.6 QQP 67.78 69.28 +1.5 68.61 +0.8 MNLI 74.40 73.58 -0.8 73.61 -0.8 MNLI-M 73.98 74.0 0.0 73.49 -0.5 Table 6: Accuracy results of models with BERT transferring to new target datasets.\&quot;,\n  \&quot;Results: Table 6 shows the results of the debiased models and baseline with BERT.\&quot;,\n  \&quot;As shown in prior work (Belinkov et al., 2019a), the MNLI datasets have very similar biases to SNLI, which the models are trained on, so we do not expect any improvement in the relative performance of our models and the baseline for MNLI and MNLI-M.\&quot;,\n  \&quot;On all the remaining datasets, our proposed models perform better than the baseline, showing a substantial improvement in generalization by using our debasing techniques.\&quot;,\n  \&quot;We additionally compare with Belinkov et al. (2019a) in Appendix D and show that our methods substantially surpass their results.\&quot;,\n  \&quot;4 Since the test sets are not available for MNLI, we tune on the matched dev set and evaluate on the mismatched dev set or vice versa.\&quot;,\n  \&quot;For GLUE, we tune on MNLI mismatched dev set.\&quot;,\n  \&quot;Analysis of Debiased Focal Loss: As expected, improving the out-of-domain performance could come at the expense of decreased in-domain performance since the removed biases are useful for performing the in-domain task.\&quot;,\n  \&quot;This happens especially for DFL, in which there is a trade-off between in-domain and out-of-domain performance that depends on the parameter  , and when the baseline model is not very powerful like InferSent.\&quot;,\n  \&quot;To understand the impact of  in DFL, we train an InferSent model using DFL for different values of  on the SNLI dataset and evaluate its performance on SNLI test and SNLI hard sets.\&quot;,\n  \&quot;As illustrated in Figure 2, increasing  increases debiasing and thus hurts in-domain accuracy on SNLI, but out-of-domain accuracy on the SNLI hard set is increased within a wide range of values (see a similar plot for BERT in Appendix E).\&quot;,\n  \&quot;Correlation Analysis: In contrast to Belinkov et al. (2019a), who encourage only the encoder to not capture the unwanted biases, our learning strategies influence the parameters of the full model to reduce the reliance on unwanted patterns more effectively.\&quot;,\n  \&quot;To test this assumption, in Figure 3, we report the correlation between the element-wise loss of the debiased models and the loss of a bias-only model on the considered datasets.\&quot;,\n  \&quot;The results show that compared to the baselines, our debiasing methods, DFL and PoE, reduce the correlation to the bias-only model, confirming that our models are effective at reducing biases.\&quot;,\n  \&quot;Interestingly, on MNLI, PoE has less correlation with the bias-only model than DFL and also has better performance on the unbiased split of this dataset.\&quot;,\n  \&quot;On the other hand, on the HANS dataset, DFL loss is less correlated with the bias-only model than PoE and also obtains higher performance on the HANS dataset.\&quot;,\n  \&quot;We propose two novel techniques, product-of-experts and debiased focal loss, to reduce biases learned by neural models, which are applicable whenever one can specify the biases in the form of one or more bias-only models.\&quot;,\n  \&quot;The bias-only models are designed to leverage biases and shortcuts in the datasets.\&quot;,\n  \&quot;Our debiasing strategies then work by adjusting the cross-entropy loss based on the performance of these bias-only models, to focus learning on the hard examples and downweight the importance of the biased examples.\&quot;,\n  \&quot;Additionally, we extend our methods to combat multiple bias patterns simultaneously.\&quot;,\n  \&quot;Our proposed debiasing techniques are model agnostic, simple, and highly effective.\&quot;,\n  \&quot;Extensive experiments show that our methods substantially improve the model robustness to domain-shift, including 9.8 points gain on FEVER symmetric test set, 7.4 on HANS dataset, and 4.8 points on SNLI hard set.\&quot;,\n  \&quot;Furthermore, we show that our debiasing techniques result in better generalization to other NLI datasets.\&quot;,\n  \&quot;Future work may include developing debiasing strategies that do not require prior knowledge of bias patterns and can automatically identify them.\&quot;,\n  \&quot;We would like to thank Daniel Andor and Suraj Srinivas for their helpful comments.\&quot;,\n  \&quot;We additionally would like to thank the authors of Schuster et al. (2019); Cadene et al. (2019); McCoy et al. (2019b); Belinkov et al. (2019a) for their support to reproduce their results.\&quot;,\n  \&quot;This research was supported by the Swiss National Science Foundation under the project Learning Representations of Abstraction for Opinion Summarization (LAOS), grant number FNS-30216.\&quot;,\n  \&quot;Y.B. was supported by the Harvard Mind, Brain, and Behavior Initiative.\&quot;\n]&quot;},&quot;labels&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;result&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;method&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;result&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;objective&quot;,&quot;other&quot;,&quot;method&quot;,&quot;other&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;other&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;result&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;],&quot;string&quot;:&quot;[\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;result\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;method\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;result\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;other\&quot;,\n  \&quot;method\&quot;,\n  \&quot;other\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;other\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;result\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;\n]&quot;}}},{&quot;rowIdx&quot;:1,&quot;cells&quot;:{&quot;sentences&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images.&quot;,&quot;However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning.&quot;,&quot;To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT.&quot;,&quot;Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects).&quot;,&quot;We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations.&quot;,&quot;Finally, these representations provide an attention-based context vector for the decoder.&quot;,&quot;We evaluate our proposed encoder on the Multi30K datasets.&quot;,&quot;Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.&quot;,&quot;Multi-modal neural machine translation (NMT) (Huang et al., 2016; Calixto et al., 2017) has become an important research direction in machine translation, due to its research significance in multimodal deep learning and wide applications, such as translating multimedia news and web product information (Zhou et al., 2018).&quot;,&quot;It significantly extends the conventional text-based machine translation by taking images as additional inputs.&quot;,&quot;The assumption behind this is that the translation is expected to be more accurate compared to purely text-based (cid:3) This work is done when Yongjing Yin was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China.&quot;,&quot;translation, since the visual context helps to resolve ambiguous multi-sense words (Ive et al., 2019).&quot;,&quot;Apparently, how to fully exploit visual information is one of the core issues in multi-modal NMT, which directly impacts the model performance.&quot;,&quot;To this end, a lot of efforts have been made, roughly consisting of: (1) encoding each input image into a global feature vector, which can be used to initialize different components of multi-modal NMT models, or as additional source tokens (Huang et al., 2016; Calixto et al., 2017), or to learn the joint multi-modal representation (Zhou et al., 2018; Calixto et al., 2019); (2) extracting object-based image features to initialize the model, or supplement source sequences, or generate attention-based visual context (Huang et al., 2016; Ive et al., 2019); and (3) representing each image as spatial features, which can be exploited as extra context (Calixto et al., 2017; Delbrouck and Dupont, 2017a; Ive et al., 2019), or a supplement to source semantics (Delbrouck and Dupont, 2017b) via an attention mechanism.&quot;,&quot;Despite their success, the above studies do not fully exploit the fine-grained semantic correspondences between semantic units within an input sentence-image pair.&quot;,&quot;For example, as shown in Figure 1, the noun phrase  a toy car  semantically corresponds to the blue dashed region.&quot;,&quot;The neglect of this important clue may be due to two big challenges: 1) how to construct a unified representation to bridge the semantic gap between two different modalities, and 2) how to achieve semantic interactions based on the unified representation.&quot;,&quot;However, we believe that such semantic correspondences can be exploited to refine multimodal representation learning, since they enable the representations within one modality to incorporate cross-modal information as supplement during multi-modal semantic interactions (Lee et al., 2018; Tan and Bansal, 2019).&quot;,&quot;In this paper, we propose a novel graph-based multi-modal fusion encoder for NMT.&quot;,&quot;We first represent the input sentence and image with a unified multi-modal graph.&quot;,&quot;In this graph, each node indicates a semantic unit: textual word or visual object , and two types of edges are introduced to model semantic relationships between semantic units within the same modality ( intra-modal edges ) and semantic correspondences between semantic units of different modalities ( inter-modal edges ) respectively.&quot;,&quot;Based on the graph, we then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions among the nodes to conduct graph encoding.&quot;,&quot;Particularly, during this process, we distinguish the parameters of two modalities, and sequentially conduct intra-and inter-modal fusions to learn multi-modal node representations.&quot;,&quot;Finally, these representations can be exploited by the decoder via an attention mechanism.&quot;,&quot;Compared with previous models, ours is able to fully exploit semantic interactions among multimodal semantic units for NMT.&quot;,&quot;Overall, the major contributions of our work are listed as follows: (cid:15) We propose a unified graph to represent the input sentence and image, where various semantic relationships between multi-modal semantic units can be captured for NMT.&quot;,&quot;(cid:15)&quot;,&quot;We propose a graph-based multi-modal fusion encoder to conduct graph encoding based on the above graph.&quot;,&quot;To the best of our knowledge, our work is the first attempt to explore multimodal graph neural network (GNN) for NMT.&quot;,&quot;(cid:15)&quot;,&quot;We conduct extensive experiments on Multi30k datasets of two language pairs.&quot;,&quot;Experimental results and in-depth analysis indicate that our encoder is effective to fuse multi-modal information for NMT.&quot;,&quot;Particularly, our multi-modal NMT model significantly outperforms several competitive baselines.&quot;,&quot;(cid:15)&quot;,&quot;We release the code at https://github.com/ DeepLearnXMU/GMNMT.&quot;,&quot;Our multi-modal NMT model is based on attentional encoder-decoder framework with maximizing the log likelihood of training data as the objective function.&quot;,&quot;Essentially, our encoder can be regarded as a multimodal extension of GNN.&quot;,&quot;To construct our encoder, we first represent the input sentence-image pair as a unified multi-modal graph.&quot;,&quot;Then, based on this graph, we stack multiple multi-modal fusion layers to learn node representations, which provides the attention-based context vector to the decoder.&quot;,&quot;In this section, we take the sentence and the image shown in Figure 1 as an example, and describe how to use a multi-modal graph to represent them.&quot;,&quot;Formally, our graph is undirected and can be formalized as G =( V , E ), which is constructed as follows: In the node set V , each node represents either a textual word or a visual object.&quot;,&quot;Specifically, we adopt the following strategies to construct these two kinds of nodes: (1) We include all words as separate textual nodes in order to fully exploit textual 3027 Multi-modal Graph Embedding Layer Cross-modal Gating Visual FFN Textual FFN Cross-modal Gating Intra-modal Fusion Inter-modal Fusion Target Inputs Embedding Layer       Textual Self-Attention Visual Self-Attention Softmax Layer Target Outputs Self-Attention Encoder-DecoderAttention FFN Encoder Decoder Figure 2: The architecture of our NMT model with the graph-based multi-modal fusion encoder.&quot;,&quot;information.&quot;,&quot;For example, in Figure 1, the multimodal graph contains totally eight textual nodes, each of which corresponds to a word in the input sentence; (2) We employ the Stanford parser to identify all noun phrases in the input sentence, and then apply a visual grounding toolkit (Yang et al., 2019) to detect bounding boxes (visual objects) for each noun phrase.&quot;,&quot;Subsequently, all detected visual objects are included as independent visual nodes .&quot;,&quot;In this way, we can effectively reduce the negative impact of abundant unrelated visual objects.&quot;,&quot;Let us revisit the example in Figure 1, where we can identify two noun phrases  Two boys  and  a toy car  from the input sentence, and then include three visual objects into the multi-modal graph.&quot;,&quot;To capture various semantic relationships between multi-modal semantic units for NMT, we consider two kinds of edges in the edge set E : (1) Any two nodes in the same modality are connected by an intra-modal edge ; and (2) Each textual node representing any noun phrase and the corresponding visual node are connected by an inter-modal edge .&quot;,&quot;Back to Figure 1, we can observe that all visual nodes are connected to each other, and all textual nodes are fully-connected.&quot;,&quot;However, only nodes v o 1 and v x 1 , v o 1 and v x 2 , v o 2 and v x 1 , v o 2 and v x 2 , v o 3 and v x 6 , v o 3 and v x 7 , v o 3 and v x 8 are connected by inter-modal edges.&quot;,&quot;Before inputting the multi-modal graph into the stacked fusion layers, we introduce an embedding&quot;,&quot;layer to initialize the node states.&quot;,&quot;Specifically, for each textual node v x i , we define its initial state H (0) x i as the sum of its word embedding and position encoding (Vaswani et al., 2017).&quot;,&quot;To obtain the initial state H (0) o j of the visual node v o j , we first extract visual features from the fully-connected layer that follows the ROI pooling layer in Faster-RCNN (Ren et al., 2015), and then employ a multilayer perceptron with ReLU activation function to project these features onto the same space as textual representations.&quot;,&quot;As shown in the left part of Figure 2, on the top of embedding layer, we stack L e graph-based multimodal fusion layers to encode the above-mentioned multi-modal graph.&quot;,&quot;At each fusion layer, we sequentially conduct intraand inter-modal fusions to update all node states.&quot;,&quot;In this way, the final node states encode both the context within the same modality and the cross-modal semantic information simultaneously.&quot;,&quot;Particularly, since visual nodes and textual nodes are two types of semantic units containing the information of different modalities, we apply similar operations but with different parameters to model their state update process, respectively.&quot;,&quot;Specifically, in the l -th fusion layer, both updates of textual node states H ( l ) x = f H ( l ) x i g and visual node states H ( l ) o = f H ( l ) o j g mainly involve the following steps: 3028 Step1: Intra-modal fusion .&quot;,&quot;At this step, we employ self-attention to generate the contextual representation of each node by collecting the message from its neighbors of the same modality.&quot;,&quot;Formally, the contextual representations C ( l ) x of all textual nodes are calculated as follows: 1 C ( l ) x = MultiHead ( H ( l (cid:0) 1) x ; H ( l (cid:0) 1) x ; H ( l (cid:0) 1) x ) ; (1) where MultiHead( Q , K , V ) is a multi-head self-attention function taking a query matrix Q , a key matrix K , and a value matrix V as inputs.&quot;,&quot;Similarly, we generate the contextual representations C ( l ) o of all visual nodes as C ( l ) o = MultiHead ( H ( l (cid:0) 1) o ; H ( l (cid:0) 1) o ; H ( l (cid:0) 1) o ) : (2) In particular, since the initial representations of visual objects are extracted from deep CNNs, we apply a simplified multi-head self-attention to preserve the initial representations of visual objects, where the learned linear projects of values and final outputs are removed.&quot;,&quot;Step2: Inter-modal fusion .&quot;,&quot;Inspired by studies in multi-modal feature fusion (Teney et al., 2018; Kim et al., 2018), we apply a cross-modal gating mechanism with an element-wise operation to gather the semantic information of the cross-modal neighbours of each node.&quot;,&quot;Concretely, we generate the representation M ( l ) x i of a text node v x i in the following way: M ( l ) x i = X j 2 A ( v xi ) (cid:11) i;j (cid:12) C ( l ) o j ; (3) (cid:11) i;j = Sigmoid ( W ( l ) 1 C ( l ) x i + W ( l ) 2 C ( l ) o j ) ; (4) where A ( v x i ) is the set of neighboring visual nodes of v x i , and W ( l ) 1 and W ( l ) 2 are parameter matrices.&quot;,&quot;Likewise, we produce the representation M ( l ) o j of a visual node v o j as follows: M ( l ) o j = X i 2 A ( v oj ) (cid:12) j;i (cid:12) C ( l ) x i ; (5) (cid:12) j;i = Sigmoid ( W ( l ) 3 C ( l ) o j + W ( l ) 4 C ( l ) x i ) ; (6) where A ( v o j ) is the set of adjacent textual nodes of v o j , and W ( l ) 3 and W ( l ) 4 are also parameter matrices.&quot;,&quot;The advantage is that the above fusion approach can better determine the degree of inter-modal fusion according to the contextual representations of 1 For simplicity, we omit the descriptions of layer normalization and residual connection.&quot;,&quot;each modality.&quot;,&quot;Finally, we adopt position-wise feed forward networks FFN ( (cid:3) ) to generate the textual node states H ( l ) x and visual node states H ( l ) o : H ( l ) x = FFN ( M ( l ) x ) ; (7) H ( l ) o = FFN ( M ( l ) o ) ; (8) where M ( l ) x = f M ( l ) x i g , M ( l ) o = f M ( l ) o j g denote the above updated representations of all textual nodes and visual nodes respectively.&quot;,&quot;Our decoder is similar to the conventional Transformer decoder.&quot;,&quot;Since visual information has been incorporated into all textual nodes via multiple graph-based multi-modal fusion layers, we allow the decoder to dynamically exploit the multi-modal context by only attending to textual node states.&quot;,&quot;As shown in the right part of Figure 2, we follow Vaswani et al. (2017) to stack L d identical layers to generate target-side hidden states, where each layer l is composed of three sub-layers.&quot;,&quot;Concretely, the first two sub-layers are a masked self-attention and an encoder-decoder attention to integrate target-and source-side contexts respectively: E ( l ) = MultiHead ( S ( l (cid:0) 1) ; S ( l (cid:0) 1) ; S ( l (cid:0) 1) ) ; (9) T ( l ) = MultiHead ( E ( l ) ; H ( L e ) x ; H ( L e ) x ) ; (10) where S ( l (cid:0) 1) denotes the target-side hidden states in the l 1 -th layer.&quot;,&quot;In particular, S (0) are the embed-dings of input target words.&quot;,&quot;Then, a position-wise fully-connected forward neural network is uesd to produce S ( l ) as follows: S ( l ) = FFN ( T ( l ) ) : (11) Finally, the probability distribution of generating the target sentence is defined by using a softmax layer, which takes the hidden states in the top layer as input: P ( Y j X; I ) = Y t Softmax ( WS ( L d ) t + b ) ; (12) where X is the input sentence, I is the input image, Y is the target sentence, and W and b are the parameters of the softmax layer.&quot;,&quot;We carry out experiments on multi-modal English ) German (En ) De) and English ) French (En ) Fr) translation tasks.&quot;,&quot;Datasets We use the Multi30K dataset (Elliott et al., 2016), where each image is paired with one English description and human translations into German and French.&quot;,&quot;Training, validation and test sets contain 29,000, 1,014 and 1,000 instances respectively.&quot;,&quot;In addition, we evaluate various models on the WMT17 test set and the ambiguous MSCOCO test set, which contain 1,000 and 461 instances respectively.&quot;,&quot;Here, we directly use the preprocessed sentences 2 and segment words into subwords via byte pair encoding (Sennrich et al., 2016) with 10,000 merge operations.&quot;,&quot;Visual Features We first apply the Stanford parser to identify noun phrases from each source sentence, and then employ the visual ground toolkit released by Yang et al. (2019) to detect associated visual objects of the identified noun phrases.&quot;,&quot;For each phrase, we keep the visual object with the highest prediction probability, so as to reduce negative effects of abundant visual objects.&quot;,&quot;In each sentence, the average numbers of objects and words are around 3.5 and 15.0 respectively.&quot;,&quot;3 Finally, we compute 2,048-dimensional features for these objects with the pre-trained ResNet-100 Faster-RCNN (Ren et al., 2015).&quot;,&quot;Settings We use Transformer (Vaswani et al., 2017) as our baseline.&quot;,&quot;Since the size of training corpus is small and the trained model tends to be over-fitting, we first perform a small grid search to obtain a set of hyper-parameters on the En ) De validation set.&quot;,&quot;Specifically, the word embedding dimension and hidden size are 128 and 256 respectively.&quot;,&quot;The decoder has L d =4 layers 4 and the number of attention heads is&quot;,&quot;4. The dropout is set to 0.5.&quot;,&quot;Each batch consists of approximately 2,000 source and target tokens.&quot;,&quot;We apply the Adam optimizer with a scheduled learning rate to optimize various models, and we use other same settings as (Vaswani et al., 2017).&quot;,&quot;Finally, we use the metrics BLEU (Pa-pineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) to evaluate the quality of translations.&quot;,&quot;Particularly, we run all models three times for each experiment and report the average results.&quot;,&quot;2 http://www.statmt.org/wmt18/multimodal-task.html 3 There is no parsing failure for this dataset.&quot;,&quot;If no noun is detected for a sentence, the object representations will be set to zero vectors and the model will degenerate to Transformer.&quot;,&quot;4 The encoder of the text-based Transformer also has 4 layers.&quot;,&quot;Baseline Models In addition to the text-based Transformer (Vaswani et al., 2017), we adapt several effective approaches to Transformer using our visual features, and compare our model with them 5 : (cid:15) ObjectAsToken(TF) (Huang et al., 2016).&quot;,&quot;It is a variant of the Transformer, where all visual objects are regarded as extra source tokens and placed at the front of the input sentence.&quot;,&quot;(cid:15)&quot;,&quot;Enc-att(TF) (Delbrouck and Dupont, 2017b).&quot;,&quot;An encoder-based image attention mechanism is incorporated into Transformer, which augments each source annotation with an attention-based visual feature vector.&quot;,&quot;(cid:15)&quot;,&quot;Doubly-att(TF) (Helcl et al., 2018).&quot;,&quot;It is a doubly attentive Transformer.&quot;,&quot;In each decoder layer, a cross-modal multi-head attention sublayer is inserted before the fully connected feed-forward layer to generate the visual context vector from visual features.&quot;,&quot;We also display the performance of several dominant multi-modal NMT models such as Doubly-att(RNN) (Calixto et al., 2017), Soft-att(RNN) (Delbrouck and Dupont, 2017a), Stochastic-att(RNN) (Delbrouck and Dupont, 2017a), Fusion-conv(RNN) (Caglayan et al., 2017), Trg-mul(RNN) (Caglayan et al., 2017), VMMT(RNN) (Calixto et al., 2019) and Deliberation Network(TF) (Ive et al., 2019) on the same datasets.&quot;,&quot;The number L e of multi-modal fusion layer is an important hyper-parameter that directly determines&quot;,&quot;5 We use suffixes ( RNN ) and ( TF ) to represent RNN-and Transformer-style NMT models, respectively.&quot;,&quot;the degree of fine-grained semantic fusion in our encoder.&quot;,&quot;Thus, we first inspect its impact on the EN ) DE validation set.&quot;,&quot;Figure 3 provides the experimental results using different L e and our model achieves the best performance when L e is&quot;,&quot;3. Hence, we use L e =3 in all subsequent experiments.&quot;,&quot;Table 1 shows the main results on the En ) De translation task.&quot;,&quot;Ours outperforms most of the existing models and all baselines, and is comparable to Fusion-conv(RNN) and Trg-mul(RNN) on METEOR.&quot;,&quot;The two results are from the state-of-the-art system on the WMT2017 test set, which is selected based on METEOR.&quot;,&quot;Comparing the baseline models, we draw the following interesting conclusions: First , our model outperforms ObjectAsTo-ken(TF), which concatenates regional visual features with text to form attendable sequences and employs self-attention mechanism to conduct inter-modal fusion.&quot;,&quot;The underlying reasons consist of two aspects: explicitly modeling semantic correspondences between semantic units of different modalities, and distinguishing model parameters for different modalities.&quot;,&quot;Second , our model also significantly outperforms Enc-att(TF).&quot;,&quot;Note that Enc-att(TF) can be considered as a single-layer semantic fusion encoder.&quot;,&quot;In addition to the advantage of explicitly modeling semantic correspondences, we conjecture that multi-layer multi-modal semantic interactions are also beneficial to NMT.&quot;,&quot;Third , compared with Doubly-att(TF) simply using an attention mechanism to exploit visual in-15 20 25 30 35 40 [5,10) [10,15) [15,20) [20,25) [25,...) BLEU Sentence Length TransformerObjectAsToken(TF)Enc-att(TF) Doubly-att(TF) Our model Figure 4: BLEU scores on different translation groups divided according to source sentence lengths.&quot;,&quot;formation, our model achieves a significant improvement, because of sufficient multi-modal fusion in our encoder.&quot;,&quot;Besides, we divide our test sets into different groups based on the lengths of source sentences and the numbers of noun phrases, and then compare the performance of different models in each group.&quot;,&quot;Figures 4 and 5 report the BLEU scores on these groups.&quot;,&quot;Overall, our model still consistently achieves the best performance in all groups.&quot;,&quot;Thus, we confirm again the effectiveness and gen-3031 Model En ) De Test2016 Test2017 MSCOCO BLEU METEOR BLEU METEOR BLEU METEOR Our model 39.8 57.6 32.2 51.9 28.7 47.6 w/o inter-modal fusion 38.7 56.7 30.7 50.6 27.0 46.7 visual grounding ) fully-connected 36.4 53.4 28.3 47.0 24.4 42.9 different parameters ) unified parameters 39.2 57.3 31.9 51.4 27.7 47.4 w/ attending to visual nodes 39.6 57.3 32.0 51.3 27.9 46.8 attending to textual nodes ) attending to visual nodes 30.9 48.6 22.3 41.5 20.4 38.7 Table 2: Ablation study of our model on the EN ) DE translation task.&quot;,&quot;erality of our proposed model.&quot;,&quot;Note that in the sentences with more phrases, which are usually long sentences, the improvements of our model over baselines are more significant.&quot;,&quot;We speculate that long sentences often contain more ambiguous words.&quot;,&quot;Thus compared with short sentences, long sentences may require visual information to be better exploited as supplementary information, which can be achieved by the multi-modal semantic interaction of our model.&quot;,&quot;We also show the training and decoding speed of our model and the baselines in Table&quot;,&quot;4. During training, our model can process approximately 1.1K tokens per second, which is comparable to other multi-modal baselines.&quot;,&quot;When it comes to decoding procedure, our model translates about 16.7 sentences per second and the speed drops slightly compared to Transformer.&quot;,&quot;Moreover, our model only introduces a small number of extra parameters and achieves better performance.&quot;,&quot;To investigate the effectiveness of different components, we further conduct experiments to compare our model with the following variants in Table 2:&quot;,&quot;(1) w/o inter-modal fusion .&quot;,&quot;In this variant, we apply two separate Transformer encoders to learn the semantic representations of words and visual objects, respectively, and then use the doubly-attentive decoder (Helcl et al., 2018) to incorporate textual and visual contexts into the decoder.&quot;,&quot;The result in line 3 indicates that removing the inter-modal fusion leads to a significant performance drop.&quot;,&quot;It suggests that semantic interactions among multi-modal semantic units are indeed useful for multi-modal representation learning.&quot;,&quot;(2) visual grounding ) fully-connected .&quot;,&quot;We make the words and visual objects fully-connected to establish the inter-modal correspondences.&quot;,&quot;The result in line 4 shows that this change causes a significant performance decline.&quot;,&quot;The underlying reason is the fully-connected semantic correspondences introduce much noise to our model.&quot;,&quot;(3) different parameters ) unified parameters .&quot;,&quot;When constructing this variant, we assign unified parameters to update node states in different modalities.&quot;,&quot;Apparently, the performance drop reported in line 5 also demonstrates the validity of our ap-3032 proach using different parameters.&quot;,&quot;(4) w/ attending to visual nodes .&quot;,&quot;Different from our model attending to only textual nodes, we allow our decoder of this variant to consider both two types of nodes using doubly-attentive decoder.&quot;,&quot;From line 6, we can observe that considering all nodes does not bring further improvement.&quot;,&quot;The result confirms our previous assumption that visual information has been fully incorporated into textual nodes in our encoder.&quot;,&quot;(5) attending to textual nodes ) attending to visual nodes .&quot;,&quot;However, when only considering visual nodes, the model performance drops drastically (line 7).&quot;,&quot;This is because the number of visual nodes is far fewer than that of textual nodes, which is unable to produce sufficient context for translation.&quot;,&quot;Figure 6 displays the 1-best translations of a sampled test sentence generated by different models.&quot;,&quot;The phrase  a skateboarding ramp  is not translated correctly by all baselines, while our model correctly translates it.&quot;,&quot;This reveals that our encoder is able to learn more accurate representations.&quot;,&quot;We also conduct experiments on the EN ) Fr dataset.&quot;,&quot;From Table 3, our model still achieves better performance compared to all baselines, which demonstrates again that our model is effective and general to different language pairs in multi-modal NMT.&quot;,&quot;Multi-modal NMT Huang et al. (2016) first incorporate global or regional visual features into attention-based NMT.&quot;,&quot;Calixto and Liu (2017) also study the effects of incorporating global visual features into different NMT components.&quot;,&quot;Elliott and K adar (2017) share an encoder between a translation model and an image prediction model to learn visually grounded representations.&quot;,&quot;Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018).&quot;,&quot;Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-modal interactions for multi-modal NMT.&quot;,&quot;Apart from model design, Elliott (2018) reveal that visual information seems to be ignored by the multimodal NMT models.&quot;,&quot;Caglayan et al. (2019) conduct a systematic analysis and show that visual information can be better leveraged under limited textual context.&quot;,&quot;Different from the above-mentioned studies, we first represent the input sentence-image pair as a unified graph, where various semantic relationships between multi-modal semantic units can be effectively captured for multi-modal NMT.&quot;,&quot;Benefiting from the multi-modal graph, we further introduce an extended GNN to conduct graph encoding via multi-modal semantic interactions.&quot;,&quot;Note that if we directly adapt the approach proposed by Huang et al. (2016) into Transformer, the model (ObjectAsToken(TF)) also involves multimodal fusion.&quot;,&quot;However, ours is different from it in following aspects: (1) We first learn the contextual representation of each node within the same modality, so that it can better determine the degree of inter-modal fusion according to its own context.&quot;,&quot;(2) We assign different encoding parameters to different modalities, which has been shown effective in our experiments.&quot;,&quot;Additionally, the recent study LXMERT (Tan and Bansal, 2019) also models relationships between vision and language, which differs from ours in following aspects: (1) Tan and Bansal (2019) first apply two transformer encoders for two modalities, and then stack two cross-modality encoders to conduct multi-modal fusion.&quot;,&quot;In contrast, we sequentially conduct self-attention and cross-modal gating at each layer.&quot;,&quot;(2) Tan and Bansal (2019) leverage an attention mechanism to implicitly establish cross-modal relationships via large-scale pretraining, while we utilize visual grounding to capture explicit cross-modal correspondences.&quot;,&quot;(3) We focus on multi-modal NMT rather than vision-and-language reasoning in (Tan and Bansal, 2019).&quot;,&quot;Graph Neural Networks Recently, GNNs (Marco Gori and Scarselli, 2005) including gated graph neural network (Li et al., 2016), graph convolutional network (Duvenaud et al., 2015; Kipf and Welling, 2017) and graph attention network (Velickovic et al., 2018) have been shown effective in many tasks such as VQA (Teney et al., 2017; Norcliffe-Brown et al., 2018; Li et al., 2019), text generation (Gildea et al., 2018; Becky et al., 2018; Song et al., 2018b, 2019) and text representation (Zhang et al., 2018; Yin et al., 2019; Song et al., 3033 Source : A boy riding a skateboard on a skateboarding ramp .&quot;,&quot;In this work, we mainly focus on how to extend GNN to fuse multi-modal information in NMT.&quot;,&quot;Close to our work, Teney et al. (2017) introduce GNN for VQA.&quot;,&quot;The main difference between their work and ours is that they build an individual graph for each modality, while we use a unified multimodal graph.&quot;,&quot;In this paper, we have proposed a novel graph-based multi-modal fusion encoder, which exploits various semantic relationships between multimodal semantic units for NMT.&quot;,&quot;Experiment results and analysis on the Multi30K dataset demonstrate the effectiveness of our model.&quot;,&quot;In the future, we plan to incorporate attributes of visual objects and dependency trees to enrich the multi-modal graphs.&quot;,&quot;Besides, how to introduce scene graphs into multi-modal NMT is a worthy problem to explore.&quot;,&quot;Finally, we will apply our model into other multi-modal tasks such as multimodal sentiment analysis.&quot;,&quot;This work was supported by the Beijing Advanced Innovation Center for Language Resources (No. TYR17002), the National Natural Science Foundation of China (No. 61672440), and the Scientific Research Project of National Language Committee of China (No. YB135-49).&quot;],&quot;string&quot;:&quot;[\n  \&quot;Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images.\&quot;,\n  \&quot;However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning.\&quot;,\n  \&quot;To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT.\&quot;,\n  \&quot;Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects).\&quot;,\n  \&quot;We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations.\&quot;,\n  \&quot;Finally, these representations provide an attention-based context vector for the decoder.\&quot;,\n  \&quot;We evaluate our proposed encoder on the Multi30K datasets.\&quot;,\n  \&quot;Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.\&quot;,\n  \&quot;Multi-modal neural machine translation (NMT) (Huang et al., 2016; Calixto et al., 2017) has become an important research direction in machine translation, due to its research significance in multimodal deep learning and wide applications, such as translating multimedia news and web product information (Zhou et al., 2018).\&quot;,\n  \&quot;It significantly extends the conventional text-based machine translation by taking images as additional inputs.\&quot;,\n  \&quot;The assumption behind this is that the translation is expected to be more accurate compared to purely text-based (cid:3) This work is done when Yongjing Yin was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China.\&quot;,\n  \&quot;translation, since the visual context helps to resolve ambiguous multi-sense words (Ive et al., 2019).\&quot;,\n  \&quot;Apparently, how to fully exploit visual information is one of the core issues in multi-modal NMT, which directly impacts the model performance.\&quot;,\n  \&quot;To this end, a lot of efforts have been made, roughly consisting of: (1) encoding each input image into a global feature vector, which can be used to initialize different components of multi-modal NMT models, or as additional source tokens (Huang et al., 2016; Calixto et al., 2017), or to learn the joint multi-modal representation (Zhou et al., 2018; Calixto et al., 2019); (2) extracting object-based image features to initialize the model, or supplement source sequences, or generate attention-based visual context (Huang et al., 2016; Ive et al., 2019); and (3) representing each image as spatial features, which can be exploited as extra context (Calixto et al., 2017; Delbrouck and Dupont, 2017a; Ive et al., 2019), or a supplement to source semantics (Delbrouck and Dupont, 2017b) via an attention mechanism.\&quot;,\n  \&quot;Despite their success, the above studies do not fully exploit the fine-grained semantic correspondences between semantic units within an input sentence-image pair.\&quot;,\n  \&quot;For example, as shown in Figure 1, the noun phrase  a toy car  semantically corresponds to the blue dashed region.\&quot;,\n  \&quot;The neglect of this important clue may be due to two big challenges: 1) how to construct a unified representation to bridge the semantic gap between two different modalities, and 2) how to achieve semantic interactions based on the unified representation.\&quot;,\n  \&quot;However, we believe that such semantic correspondences can be exploited to refine multimodal representation learning, since they enable the representations within one modality to incorporate cross-modal information as supplement during multi-modal semantic interactions (Lee et al., 2018; Tan and Bansal, 2019).\&quot;,\n  \&quot;In this paper, we propose a novel graph-based multi-modal fusion encoder for NMT.\&quot;,\n  \&quot;We first represent the input sentence and image with a unified multi-modal graph.\&quot;,\n  \&quot;In this graph, each node indicates a semantic unit: textual word or visual object , and two types of edges are introduced to model semantic relationships between semantic units within the same modality ( intra-modal edges ) and semantic correspondences between semantic units of different modalities ( inter-modal edges ) respectively.\&quot;,\n  \&quot;Based on the graph, we then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions among the nodes to conduct graph encoding.\&quot;,\n  \&quot;Particularly, during this process, we distinguish the parameters of two modalities, and sequentially conduct intra-and inter-modal fusions to learn multi-modal node representations.\&quot;,\n  \&quot;Finally, these representations can be exploited by the decoder via an attention mechanism.\&quot;,\n  \&quot;Compared with previous models, ours is able to fully exploit semantic interactions among multimodal semantic units for NMT.\&quot;,\n  \&quot;Overall, the major contributions of our work are listed as follows: (cid:15) We propose a unified graph to represent the input sentence and image, where various semantic relationships between multi-modal semantic units can be captured for NMT.\&quot;,\n  \&quot;(cid:15)\&quot;,\n  \&quot;We propose a graph-based multi-modal fusion encoder to conduct graph encoding based on the above graph.\&quot;,\n  \&quot;To the best of our knowledge, our work is the first attempt to explore multimodal graph neural network (GNN) for NMT.\&quot;,\n  \&quot;(cid:15)\&quot;,\n  \&quot;We conduct extensive experiments on Multi30k datasets of two language pairs.\&quot;,\n  \&quot;Experimental results and in-depth analysis indicate that our encoder is effective to fuse multi-modal information for NMT.\&quot;,\n  \&quot;Particularly, our multi-modal NMT model significantly outperforms several competitive baselines.\&quot;,\n  \&quot;(cid:15)\&quot;,\n  \&quot;We release the code at https://github.com/ DeepLearnXMU/GMNMT.\&quot;,\n  \&quot;Our multi-modal NMT model is based on attentional encoder-decoder framework with maximizing the log likelihood of training data as the objective function.\&quot;,\n  \&quot;Essentially, our encoder can be regarded as a multimodal extension of GNN.\&quot;,\n  \&quot;To construct our encoder, we first represent the input sentence-image pair as a unified multi-modal graph.\&quot;,\n  \&quot;Then, based on this graph, we stack multiple multi-modal fusion layers to learn node representations, which provides the attention-based context vector to the decoder.\&quot;,\n  \&quot;In this section, we take the sentence and the image shown in Figure 1 as an example, and describe how to use a multi-modal graph to represent them.\&quot;,\n  \&quot;Formally, our graph is undirected and can be formalized as G =( V , E ), which is constructed as follows: In the node set V , each node represents either a textual word or a visual object.\&quot;,\n  \&quot;Specifically, we adopt the following strategies to construct these two kinds of nodes: (1) We include all words as separate textual nodes in order to fully exploit textual 3027 Multi-modal Graph Embedding Layer Cross-modal Gating Visual FFN Textual FFN Cross-modal Gating Intra-modal Fusion Inter-modal Fusion Target Inputs Embedding Layer       Textual Self-Attention Visual Self-Attention Softmax Layer Target Outputs Self-Attention Encoder-DecoderAttention FFN Encoder Decoder Figure 2: The architecture of our NMT model with the graph-based multi-modal fusion encoder.\&quot;,\n  \&quot;information.\&quot;,\n  \&quot;For example, in Figure 1, the multimodal graph contains totally eight textual nodes, each of which corresponds to a word in the input sentence; (2) We employ the Stanford parser to identify all noun phrases in the input sentence, and then apply a visual grounding toolkit (Yang et al., 2019) to detect bounding boxes (visual objects) for each noun phrase.\&quot;,\n  \&quot;Subsequently, all detected visual objects are included as independent visual nodes .\&quot;,\n  \&quot;In this way, we can effectively reduce the negative impact of abundant unrelated visual objects.\&quot;,\n  \&quot;Let us revisit the example in Figure 1, where we can identify two noun phrases  Two boys  and  a toy car  from the input sentence, and then include three visual objects into the multi-modal graph.\&quot;,\n  \&quot;To capture various semantic relationships between multi-modal semantic units for NMT, we consider two kinds of edges in the edge set E : (1) Any two nodes in the same modality are connected by an intra-modal edge ; and (2) Each textual node representing any noun phrase and the corresponding visual node are connected by an inter-modal edge .\&quot;,\n  \&quot;Back to Figure 1, we can observe that all visual nodes are connected to each other, and all textual nodes are fully-connected.\&quot;,\n  \&quot;However, only nodes v o 1 and v x 1 , v o 1 and v x 2 , v o 2 and v x 1 , v o 2 and v x 2 , v o 3 and v x 6 , v o 3 and v x 7 , v o 3 and v x 8 are connected by inter-modal edges.\&quot;,\n  \&quot;Before inputting the multi-modal graph into the stacked fusion layers, we introduce an embedding\&quot;,\n  \&quot;layer to initialize the node states.\&quot;,\n  \&quot;Specifically, for each textual node v x i , we define its initial state H (0) x i as the sum of its word embedding and position encoding (Vaswani et al., 2017).\&quot;,\n  \&quot;To obtain the initial state H (0) o j of the visual node v o j , we first extract visual features from the fully-connected layer that follows the ROI pooling layer in Faster-RCNN (Ren et al., 2015), and then employ a multilayer perceptron with ReLU activation function to project these features onto the same space as textual representations.\&quot;,\n  \&quot;As shown in the left part of Figure 2, on the top of embedding layer, we stack L e graph-based multimodal fusion layers to encode the above-mentioned multi-modal graph.\&quot;,\n  \&quot;At each fusion layer, we sequentially conduct intraand inter-modal fusions to update all node states.\&quot;,\n  \&quot;In this way, the final node states encode both the context within the same modality and the cross-modal semantic information simultaneously.\&quot;,\n  \&quot;Particularly, since visual nodes and textual nodes are two types of semantic units containing the information of different modalities, we apply similar operations but with different parameters to model their state update process, respectively.\&quot;,\n  \&quot;Specifically, in the l -th fusion layer, both updates of textual node states H ( l ) x = f H ( l ) x i g and visual node states H ( l ) o = f H ( l ) o j g mainly involve the following steps: 3028 Step1: Intra-modal fusion .\&quot;,\n  \&quot;At this step, we employ self-attention to generate the contextual representation of each node by collecting the message from its neighbors of the same modality.\&quot;,\n  \&quot;Formally, the contextual representations C ( l ) x of all textual nodes are calculated as follows: 1 C ( l ) x = MultiHead ( H ( l (cid:0) 1) x ; H ( l (cid:0) 1) x ; H ( l (cid:0) 1) x ) ; (1) where MultiHead( Q , K , V ) is a multi-head self-attention function taking a query matrix Q , a key matrix K , and a value matrix V as inputs.\&quot;,\n  \&quot;Similarly, we generate the contextual representations C ( l ) o of all visual nodes as C ( l ) o = MultiHead ( H ( l (cid:0) 1) o ; H ( l (cid:0) 1) o ; H ( l (cid:0) 1) o ) : (2) In particular, since the initial representations of visual objects are extracted from deep CNNs, we apply a simplified multi-head self-attention to preserve the initial representations of visual objects, where the learned linear projects of values and final outputs are removed.\&quot;,\n  \&quot;Step2: Inter-modal fusion .\&quot;,\n  \&quot;Inspired by studies in multi-modal feature fusion (Teney et al., 2018; Kim et al., 2018), we apply a cross-modal gating mechanism with an element-wise operation to gather the semantic information of the cross-modal neighbours of each node.\&quot;,\n  \&quot;Concretely, we generate the representation M ( l ) x i of a text node v x i in the following way: M ( l ) x i = X j 2 A ( v xi ) (cid:11) i;j (cid:12) C ( l ) o j ; (3) (cid:11) i;j = Sigmoid ( W ( l ) 1 C ( l ) x i + W ( l ) 2 C ( l ) o j ) ; (4) where A ( v x i ) is the set of neighboring visual nodes of v x i , and W ( l ) 1 and W ( l ) 2 are parameter matrices.\&quot;,\n  \&quot;Likewise, we produce the representation M ( l ) o j of a visual node v o j as follows: M ( l ) o j = X i 2 A ( v oj ) (cid:12) j;i (cid:12) C ( l ) x i ; (5) (cid:12) j;i = Sigmoid ( W ( l ) 3 C ( l ) o j + W ( l ) 4 C ( l ) x i ) ; (6) where A ( v o j ) is the set of adjacent textual nodes of v o j , and W ( l ) 3 and W ( l ) 4 are also parameter matrices.\&quot;,\n  \&quot;The advantage is that the above fusion approach can better determine the degree of inter-modal fusion according to the contextual representations of 1 For simplicity, we omit the descriptions of layer normalization and residual connection.\&quot;,\n  \&quot;each modality.\&quot;,\n  \&quot;Finally, we adopt position-wise feed forward networks FFN ( (cid:3) ) to generate the textual node states H ( l ) x and visual node states H ( l ) o : H ( l ) x = FFN ( M ( l ) x ) ; (7) H ( l ) o = FFN ( M ( l ) o ) ; (8) where M ( l ) x = f M ( l ) x i g , M ( l ) o = f M ( l ) o j g denote the above updated representations of all textual nodes and visual nodes respectively.\&quot;,\n  \&quot;Our decoder is similar to the conventional Transformer decoder.\&quot;,\n  \&quot;Since visual information has been incorporated into all textual nodes via multiple graph-based multi-modal fusion layers, we allow the decoder to dynamically exploit the multi-modal context by only attending to textual node states.\&quot;,\n  \&quot;As shown in the right part of Figure 2, we follow Vaswani et al. (2017) to stack L d identical layers to generate target-side hidden states, where each layer l is composed of three sub-layers.\&quot;,\n  \&quot;Concretely, the first two sub-layers are a masked self-attention and an encoder-decoder attention to integrate target-and source-side contexts respectively: E ( l ) = MultiHead ( S ( l (cid:0) 1) ; S ( l (cid:0) 1) ; S ( l (cid:0) 1) ) ; (9) T ( l ) = MultiHead ( E ( l ) ; H ( L e ) x ; H ( L e ) x ) ; (10) where S ( l (cid:0) 1) denotes the target-side hidden states in the l 1 -th layer.\&quot;,\n  \&quot;In particular, S (0) are the embed-dings of input target words.\&quot;,\n  \&quot;Then, a position-wise fully-connected forward neural network is uesd to produce S ( l ) as follows: S ( l ) = FFN ( T ( l ) ) : (11) Finally, the probability distribution of generating the target sentence is defined by using a softmax layer, which takes the hidden states in the top layer as input: P ( Y j X; I ) = Y t Softmax ( WS ( L d ) t + b ) ; (12) where X is the input sentence, I is the input image, Y is the target sentence, and W and b are the parameters of the softmax layer.\&quot;,\n  \&quot;We carry out experiments on multi-modal English ) German (En ) De) and English ) French (En ) Fr) translation tasks.\&quot;,\n  \&quot;Datasets We use the Multi30K dataset (Elliott et al., 2016), where each image is paired with one English description and human translations into German and French.\&quot;,\n  \&quot;Training, validation and test sets contain 29,000, 1,014 and 1,000 instances respectively.\&quot;,\n  \&quot;In addition, we evaluate various models on the WMT17 test set and the ambiguous MSCOCO test set, which contain 1,000 and 461 instances respectively.\&quot;,\n  \&quot;Here, we directly use the preprocessed sentences 2 and segment words into subwords via byte pair encoding (Sennrich et al., 2016) with 10,000 merge operations.\&quot;,\n  \&quot;Visual Features We first apply the Stanford parser to identify noun phrases from each source sentence, and then employ the visual ground toolkit released by Yang et al. (2019) to detect associated visual objects of the identified noun phrases.\&quot;,\n  \&quot;For each phrase, we keep the visual object with the highest prediction probability, so as to reduce negative effects of abundant visual objects.\&quot;,\n  \&quot;In each sentence, the average numbers of objects and words are around 3.5 and 15.0 respectively.\&quot;,\n  \&quot;3 Finally, we compute 2,048-dimensional features for these objects with the pre-trained ResNet-100 Faster-RCNN (Ren et al., 2015).\&quot;,\n  \&quot;Settings We use Transformer (Vaswani et al., 2017) as our baseline.\&quot;,\n  \&quot;Since the size of training corpus is small and the trained model tends to be over-fitting, we first perform a small grid search to obtain a set of hyper-parameters on the En ) De validation set.\&quot;,\n  \&quot;Specifically, the word embedding dimension and hidden size are 128 and 256 respectively.\&quot;,\n  \&quot;The decoder has L d =4 layers 4 and the number of attention heads is\&quot;,\n  \&quot;4. The dropout is set to 0.5.\&quot;,\n  \&quot;Each batch consists of approximately 2,000 source and target tokens.\&quot;,\n  \&quot;We apply the Adam optimizer with a scheduled learning rate to optimize various models, and we use other same settings as (Vaswani et al., 2017).\&quot;,\n  \&quot;Finally, we use the metrics BLEU (Pa-pineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) to evaluate the quality of translations.\&quot;,\n  \&quot;Particularly, we run all models three times for each experiment and report the average results.\&quot;,\n  \&quot;2 http://www.statmt.org/wmt18/multimodal-task.html 3 There is no parsing failure for this dataset.\&quot;,\n  \&quot;If no noun is detected for a sentence, the object representations will be set to zero vectors and the model will degenerate to Transformer.\&quot;,\n  \&quot;4 The encoder of the text-based Transformer also has 4 layers.\&quot;,\n  \&quot;Baseline Models In addition to the text-based Transformer (Vaswani et al., 2017), we adapt several effective approaches to Transformer using our visual features, and compare our model with them 5 : (cid:15) ObjectAsToken(TF) (Huang et al., 2016).\&quot;,\n  \&quot;It is a variant of the Transformer, where all visual objects are regarded as extra source tokens and placed at the front of the input sentence.\&quot;,\n  \&quot;(cid:15)\&quot;,\n  \&quot;Enc-att(TF) (Delbrouck and Dupont, 2017b).\&quot;,\n  \&quot;An encoder-based image attention mechanism is incorporated into Transformer, which augments each source annotation with an attention-based visual feature vector.\&quot;,\n  \&quot;(cid:15)\&quot;,\n  \&quot;Doubly-att(TF) (Helcl et al., 2018).\&quot;,\n  \&quot;It is a doubly attentive Transformer.\&quot;,\n  \&quot;In each decoder layer, a cross-modal multi-head attention sublayer is inserted before the fully connected feed-forward layer to generate the visual context vector from visual features.\&quot;,\n  \&quot;We also display the performance of several dominant multi-modal NMT models such as Doubly-att(RNN) (Calixto et al., 2017), Soft-att(RNN) (Delbrouck and Dupont, 2017a), Stochastic-att(RNN) (Delbrouck and Dupont, 2017a), Fusion-conv(RNN) (Caglayan et al., 2017), Trg-mul(RNN) (Caglayan et al., 2017), VMMT(RNN) (Calixto et al., 2019) and Deliberation Network(TF) (Ive et al., 2019) on the same datasets.\&quot;,\n  \&quot;The number L e of multi-modal fusion layer is an important hyper-parameter that directly determines\&quot;,\n  \&quot;5 We use suffixes ( RNN ) and ( TF ) to represent RNN-and Transformer-style NMT models, respectively.\&quot;,\n  \&quot;the degree of fine-grained semantic fusion in our encoder.\&quot;,\n  \&quot;Thus, we first inspect its impact on the EN ) DE validation set.\&quot;,\n  \&quot;Figure 3 provides the experimental results using different L e and our model achieves the best performance when L e is\&quot;,\n  \&quot;3. Hence, we use L e =3 in all subsequent experiments.\&quot;,\n  \&quot;Table 1 shows the main results on the En ) De translation task.\&quot;,\n  \&quot;Ours outperforms most of the existing models and all baselines, and is comparable to Fusion-conv(RNN) and Trg-mul(RNN) on METEOR.\&quot;,\n  \&quot;The two results are from the state-of-the-art system on the WMT2017 test set, which is selected based on METEOR.\&quot;,\n  \&quot;Comparing the baseline models, we draw the following interesting conclusions: First , our model outperforms ObjectAsTo-ken(TF), which concatenates regional visual features with text to form attendable sequences and employs self-attention mechanism to conduct inter-modal fusion.\&quot;,\n  \&quot;The underlying reasons consist of two aspects: explicitly modeling semantic correspondences between semantic units of different modalities, and distinguishing model parameters for different modalities.\&quot;,\n  \&quot;Second , our model also significantly outperforms Enc-att(TF).\&quot;,\n  \&quot;Note that Enc-att(TF) can be considered as a single-layer semantic fusion encoder.\&quot;,\n  \&quot;In addition to the advantage of explicitly modeling semantic correspondences, we conjecture that multi-layer multi-modal semantic interactions are also beneficial to NMT.\&quot;,\n  \&quot;Third , compared with Doubly-att(TF) simply using an attention mechanism to exploit visual in-15 20 25 30 35 40 [5,10) [10,15) [15,20) [20,25) [25,...) BLEU Sentence Length TransformerObjectAsToken(TF)Enc-att(TF) Doubly-att(TF) Our model Figure 4: BLEU scores on different translation groups divided according to source sentence lengths.\&quot;,\n  \&quot;formation, our model achieves a significant improvement, because of sufficient multi-modal fusion in our encoder.\&quot;,\n  \&quot;Besides, we divide our test sets into different groups based on the lengths of source sentences and the numbers of noun phrases, and then compare the performance of different models in each group.\&quot;,\n  \&quot;Figures 4 and 5 report the BLEU scores on these groups.\&quot;,\n  \&quot;Overall, our model still consistently achieves the best performance in all groups.\&quot;,\n  \&quot;Thus, we confirm again the effectiveness and gen-3031 Model En ) De Test2016 Test2017 MSCOCO BLEU METEOR BLEU METEOR BLEU METEOR Our model 39.8 57.6 32.2 51.9 28.7 47.6 w/o inter-modal fusion 38.7 56.7 30.7 50.6 27.0 46.7 visual grounding ) fully-connected 36.4 53.4 28.3 47.0 24.4 42.9 different parameters ) unified parameters 39.2 57.3 31.9 51.4 27.7 47.4 w/ attending to visual nodes 39.6 57.3 32.0 51.3 27.9 46.8 attending to textual nodes ) attending to visual nodes 30.9 48.6 22.3 41.5 20.4 38.7 Table 2: Ablation study of our model on the EN ) DE translation task.\&quot;,\n  \&quot;erality of our proposed model.\&quot;,\n  \&quot;Note that in the sentences with more phrases, which are usually long sentences, the improvements of our model over baselines are more significant.\&quot;,\n  \&quot;We speculate that long sentences often contain more ambiguous words.\&quot;,\n  \&quot;Thus compared with short sentences, long sentences may require visual information to be better exploited as supplementary information, which can be achieved by the multi-modal semantic interaction of our model.\&quot;,\n  \&quot;We also show the training and decoding speed of our model and the baselines in Table\&quot;,\n  \&quot;4. During training, our model can process approximately 1.1K tokens per second, which is comparable to other multi-modal baselines.\&quot;,\n  \&quot;When it comes to decoding procedure, our model translates about 16.7 sentences per second and the speed drops slightly compared to Transformer.\&quot;,\n  \&quot;Moreover, our model only introduces a small number of extra parameters and achieves better performance.\&quot;,\n  \&quot;To investigate the effectiveness of different components, we further conduct experiments to compare our model with the following variants in Table 2:\&quot;,\n  \&quot;(1) w/o inter-modal fusion .\&quot;,\n  \&quot;In this variant, we apply two separate Transformer encoders to learn the semantic representations of words and visual objects, respectively, and then use the doubly-attentive decoder (Helcl et al., 2018) to incorporate textual and visual contexts into the decoder.\&quot;,\n  \&quot;The result in line 3 indicates that removing the inter-modal fusion leads to a significant performance drop.\&quot;,\n  \&quot;It suggests that semantic interactions among multi-modal semantic units are indeed useful for multi-modal representation learning.\&quot;,\n  \&quot;(2) visual grounding ) fully-connected .\&quot;,\n  \&quot;We make the words and visual objects fully-connected to establish the inter-modal correspondences.\&quot;,\n  \&quot;The result in line 4 shows that this change causes a significant performance decline.\&quot;,\n  \&quot;The underlying reason is the fully-connected semantic correspondences introduce much noise to our model.\&quot;,\n  \&quot;(3) different parameters ) unified parameters .\&quot;,\n  \&quot;When constructing this variant, we assign unified parameters to update node states in different modalities.\&quot;,\n  \&quot;Apparently, the performance drop reported in line 5 also demonstrates the validity of our ap-3032 proach using different parameters.\&quot;,\n  \&quot;(4) w/ attending to visual nodes .\&quot;,\n  \&quot;Different from our model attending to only textual nodes, we allow our decoder of this variant to consider both two types of nodes using doubly-attentive decoder.\&quot;,\n  \&quot;From line 6, we can observe that considering all nodes does not bring further improvement.\&quot;,\n  \&quot;The result confirms our previous assumption that visual information has been fully incorporated into textual nodes in our encoder.\&quot;,\n  \&quot;(5) attending to textual nodes ) attending to visual nodes .\&quot;,\n  \&quot;However, when only considering visual nodes, the model performance drops drastically (line 7).\&quot;,\n  \&quot;This is because the number of visual nodes is far fewer than that of textual nodes, which is unable to produce sufficient context for translation.\&quot;,\n  \&quot;Figure 6 displays the 1-best translations of a sampled test sentence generated by different models.\&quot;,\n  \&quot;The phrase  a skateboarding ramp  is not translated correctly by all baselines, while our model correctly translates it.\&quot;,\n  \&quot;This reveals that our encoder is able to learn more accurate representations.\&quot;,\n  \&quot;We also conduct experiments on the EN ) Fr dataset.\&quot;,\n  \&quot;From Table 3, our model still achieves better performance compared to all baselines, which demonstrates again that our model is effective and general to different language pairs in multi-modal NMT.\&quot;,\n  \&quot;Multi-modal NMT Huang et al. (2016) first incorporate global or regional visual features into attention-based NMT.\&quot;,\n  \&quot;Calixto and Liu (2017) also study the effects of incorporating global visual features into different NMT components.\&quot;,\n  \&quot;Elliott and K adar (2017) share an encoder between a translation model and an image prediction model to learn visually grounded representations.\&quot;,\n  \&quot;Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018).\&quot;,\n  \&quot;Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-modal interactions for multi-modal NMT.\&quot;,\n  \&quot;Apart from model design, Elliott (2018) reveal that visual information seems to be ignored by the multimodal NMT models.\&quot;,\n  \&quot;Caglayan et al. (2019) conduct a systematic analysis and show that visual information can be better leveraged under limited textual context.\&quot;,\n  \&quot;Different from the above-mentioned studies, we first represent the input sentence-image pair as a unified graph, where various semantic relationships between multi-modal semantic units can be effectively captured for multi-modal NMT.\&quot;,\n  \&quot;Benefiting from the multi-modal graph, we further introduce an extended GNN to conduct graph encoding via multi-modal semantic interactions.\&quot;,\n  \&quot;Note that if we directly adapt the approach proposed by Huang et al. (2016) into Transformer, the model (ObjectAsToken(TF)) also involves multimodal fusion.\&quot;,\n  \&quot;However, ours is different from it in following aspects: (1) We first learn the contextual representation of each node within the same modality, so that it can better determine the degree of inter-modal fusion according to its own context.\&quot;,\n  \&quot;(2) We assign different encoding parameters to different modalities, which has been shown effective in our experiments.\&quot;,\n  \&quot;Additionally, the recent study LXMERT (Tan and Bansal, 2019) also models relationships between vision and language, which differs from ours in following aspects: (1) Tan and Bansal (2019) first apply two transformer encoders for two modalities, and then stack two cross-modality encoders to conduct multi-modal fusion.\&quot;,\n  \&quot;In contrast, we sequentially conduct self-attention and cross-modal gating at each layer.\&quot;,\n  \&quot;(2) Tan and Bansal (2019) leverage an attention mechanism to implicitly establish cross-modal relationships via large-scale pretraining, while we utilize visual grounding to capture explicit cross-modal correspondences.\&quot;,\n  \&quot;(3) We focus on multi-modal NMT rather than vision-and-language reasoning in (Tan and Bansal, 2019).\&quot;,\n  \&quot;Graph Neural Networks Recently, GNNs (Marco Gori and Scarselli, 2005) including gated graph neural network (Li et al., 2016), graph convolutional network (Duvenaud et al., 2015; Kipf and Welling, 2017) and graph attention network (Velickovic et al., 2018) have been shown effective in many tasks such as VQA (Teney et al., 2017; Norcliffe-Brown et al., 2018; Li et al., 2019), text generation (Gildea et al., 2018; Becky et al., 2018; Song et al., 2018b, 2019) and text representation (Zhang et al., 2018; Yin et al., 2019; Song et al., 3033 Source : A boy riding a skateboard on a skateboarding ramp .\&quot;,\n  \&quot;In this work, we mainly focus on how to extend GNN to fuse multi-modal information in NMT.\&quot;,\n  \&quot;Close to our work, Teney et al. (2017) introduce GNN for VQA.\&quot;,\n  \&quot;The main difference between their work and ours is that they build an individual graph for each modality, while we use a unified multimodal graph.\&quot;,\n  \&quot;In this paper, we have proposed a novel graph-based multi-modal fusion encoder, which exploits various semantic relationships between multimodal semantic units for NMT.\&quot;,\n  \&quot;Experiment results and analysis on the Multi30K dataset demonstrate the effectiveness of our model.\&quot;,\n  \&quot;In the future, we plan to incorporate attributes of visual objects and dependency trees to enrich the multi-modal graphs.\&quot;,\n  \&quot;Besides, how to introduce scene graphs into multi-modal NMT is a worthy problem to explore.\&quot;,\n  \&quot;Finally, we will apply our model into other multi-modal tasks such as multimodal sentiment analysis.\&quot;,\n  \&quot;This work was supported by the Beijing Advanced Innovation Center for Language Resources (No. TYR17002), the National Natural Science Foundation of China (No. 61672440), and the Scientific Research Project of National Language Committee of China (No. YB135-49).\&quot;\n]&quot;},&quot;labels&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;method&quot;,&quot;result&quot;,&quot;result&quot;,&quot;objective&quot;,&quot;other&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;other&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;objective&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;objective&quot;,&quot;method&quot;,&quot;method&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;other&quot;],&quot;string&quot;:&quot;[\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;method\&quot;,\n  \&quot;result\&quot;,\n  \&quot;result\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;other\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;other\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;other\&quot;\n]&quot;}}},{&quot;rowIdx&quot;:2,&quot;cells&quot;:{&quot;sentences&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding.&quot;,&quot;At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment ), improving upon available resources in both its coverage and difficulty.&quot;,&quot;MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation.&quot;,&quot;In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.&quot;,&quot;Many of the most actively studied problems in NLP, including question answering, translation, and dialog, depend in large part on natural language understanding (NLU) for success.&quot;,&quot;While there has been a great deal of work that uses representation learning techniques to pursue progress on these applied NLU problems directly, in order for a representation learning model to fully succeed at one of these problems, it must simultaneously succeed both at NLU, and at one or more additional hard machine learning problems like structured prediction or memory access.&quot;,&quot;This makes it difficult to accurately judge the degree to which current models extract reasonable representations of language meaning in these settings.&quot;,&quot;The task of natural language inference (NLI) is well positioned to serve as a benchmark task for research on NLU.&quot;,&quot;In this task, also known as recognizing textual entailment (Cooper et al., 1996; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentenceslike one of those in Figure 1and asked to judge the relationship between their meanings by picking a label from a small set: typically ENTAILMENT , NEUTRAL , and CONTRADICTION .&quot;,&quot;Succeeding at NLI does not require a system to solve any difficult machine learning problems except, crucially, that of extracting effective and thorough representations for the meanings of sentences (i.e., their lexical and compositional semantics).&quot;,&quot;In particular, a model must handle phenomena like lexical entailment, quantification, coreference, tense, belief, modality, and lexical and syntactic ambiguity.&quot;,&quot;As the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention (Wang and Jiang, 2016; Parikh et al., 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016; Chen et al., 2017).&quot;,&quot;However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways.&quot;,&quot;First, the sentences in SNLI are derived from only a single text genreimage captionsand are thus limited to descriptions of concrete visual scenes, rendering the hypothesis sentences used to describe these scenes short and simple, and rendering many important phenomenalike temporal reasoning (e.g., yesterday ), belief (e.g., know ), and modality (e.g., should )rare enough to be irrelevant to task performance.&quot;,&quot;Second, because of these issues, SNLI is not sufficiently demanding to serve as an effective benchmark for NLU, with the best current model performance falling within a few percentage points of human accuracy and limited room left for fine-grained comparisons between strong models.&quot;,&quot;This paper introduces a new challenge dataset, the Multi-Genre NLI Corpus (MultiNLI), whose chief purpose is to remedy these limitations by making it possible to run large-scale NLI evaluations that capture more of the complexity of modern English.&quot;,&quot;While its size (433k pairs) and mode of collection are modeled closely on SNLI, unlike that corpus, MultiNLI represents both written and spoken speech in a wide range of styles, degrees of formality, and topics.&quot;,&quot;Our chief motivation in creating this corpus is to provide a benchmark for ambitious machine learning research on the core problems of NLU, but we are additionally interested in constructing a corpus that facilitates work on domain adaptation and cross-domain transfer learning.&quot;,&quot;These techniqueswhich use labeled training data for a source domain, and aim to train a model that performs well on test data from a target domain with a different distributionhave resulted in gains across many tasks (Daume III and Marcu, 2006; Ben-David et al., 2007), including sequence and part-of-speech tagging (Blitzer et al., 2006; Peng and Dredze, 2017).&quot;,&quot;Moreover, in application areas outside NLU, artificial neural network techniques have made it possible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data (Krizhevsky et al., 2012; Zeiler and Fergus, 2014; Donahue et al., 2014).&quot;,&quot;However, attempts to bring this kind of general purpose representation learning to NLU have seen only very limited success (see, for example, Mou et al., 2016a).&quot;,&quot;Nearly all successful applications of representation learning to NLU have involved models that are trained on data closely resembling the target evaluation data in both task and style.&quot;,&quot;This fact limits the usefulness of these tools for problems involving styles of language not represented in large annotated training sets.&quot;,&quot;With this in mind, we construct MultiNLI so as to make it possible to explicitly evaluate models both on the quality of their sentence representations within the training domain and on their ability to derive reasonable representations in unfamiliar domains.&quot;,&quot;The corpus is derived from ten different genres of written and spoken English, which are collectively meant to approximate the full diversity of ways in which modern standard 1113 This task will involve reading a line from a non-fiction article and writing three sentences that relate to it.&quot;,&quot;The line will describe a situation or event.&quot;,&quot;Using only this description and what you know about the world:  Write one sentence that is definitely correct about the situation or event in the line.&quot;,&quot;Write one sentence that might be correct about the situation or event in the line.&quot;,&quot;Write one sentence that is definitely incorrect about the situation or event in the line.&quot;,&quot;American English is used.&quot;,&quot;All of the genres appear in the test and development sets, but only five are included in the training set.&quot;,&quot;Models thus can be evaluated on both the matched test examples, which are derived from the same sources as those in the training set, and on the mismatched examples, which do not closely resemble any of those seen at training time.&quot;,&quot;The data collection methodology for MultiNLI is similar to that of SNLI: We create each sentence pair by selecting a premise sentence from a preexisting text source and asking a human annotator to compose a novel sentence to pair with it as a hypothesis.&quot;,&quot;This section discusses the sources of our premise sentences, our collection method for hypotheses, and our validation (relabeling) strategy.&quot;,&quot;Premise Text Sources The MultiNLI premise sentences are derived from ten sources of freely available text which are meant to be maximally diverse and roughly represent the full range of American English.&quot;,&quot;We selected nine sources from the second release of the Open American National Corpus (OANC; Fillmore et al., 1998; Macleod et al., 2000; Ide and Macleod, 2001; Ide and Su-derman, 2006, downloaded 12/2016 1 ), balancing the volume of source text roughly evenly across genres, and avoiding genres with content that would be too difficult for untrained annotators.&quot;,&quot;and Conversation Collection of two-sided, in-person conversations that took place in the early 2000s (FACE-TO-FACE ); reports, speeches, letters, and press releases from public domain government websites (GOVERNMENT ); letters from the Indiana Center for Intercultural Communication of Philanthropic Fundraising Discourse written in the late 1990searly 2000s (LETTERS ); the public report from the National Commission on Terrorist Attacks Upon the United States released on July 22, 2004 2 (9/11); five non-fiction works on the textile industry and child development published by the Oxford University Press (OUP); popular culture articles from the archives of Slate Magazine (SLATE ) written between 19962000; transcriptions from University of Pennsylvania's Linguistic Data Consortium Switchboard corpus of two-sided, telephone conversations that took place in 1990 or 1991 (TELEPHONE ); travel guides published by Berlitz Publishing in the early 2000s (TRAVEL ); and short posts about linguistics for non-specialists from the Verbatim archives written between 1990 and 1996 (VERBATIM ).&quot;,&quot;For our tenth genre, FICTION , we compile several freely available works of contemporary fiction written between 1912 and 2010, spanning various genres, including mystery ( The Mysterious Affair at Styles , 3 Christie, 1921; The Secret Adversary , 4 Christie, 1922; Murder in the Gun Room , 5 Piper, 1953), humor ( Password Incorrect , 6 Name, 2008), western ( Rebel Spurs , 7 Norton, 1962), science fiction ( Seven Swords , 8 Shea, 2008; Living History , 9 Essex, 2016; The Sky Is Falling , 10 Del Rey, 1973; Youth , 11 Asimov, May 1952), and adventure ( Captain Blood , 12 Sabatini, 1922).&quot;,&quot;We construct premise sentences from these ten source texts with minimal preprocessing; unique the sentences within genres, exclude very short 2 https://9-11commission.gov/ 3 gutenberg.org/files/863/863-0.txt 4 gutenberg.org/files/1155/1155-0.txt 5 gutenberg.org/files/17866/17866.txt 6 http://manybooks.net/pages/ namenother09password_incorrect/0.html 7 gutenberg.org/files/20840/20840-0.txt 8 http://mikeshea.net/stories/seven_ swords.html , shared with the author's permission.&quot;,&quot;9 manybooks.net/pages/ essexbother10living_history/0.html 10 gutenberg.org/cache/epub/18768/ pg18768.txt 11 gutenberg.org/cache/epub/31547/ pg31547.txt 12 gutenberg.org/files/1965/1965-0.txt 1114 sentences (under eight characters), and manually remove certain types of non-narrative writing, such as mathematical formulae, bibliographic references, and lists.&quot;,&quot;Although SNLI is collected in largely the same way as MultiNLI, and is also permissively licensed, we do not include SNLI in the MultiNLI corpus distribution.&quot;,&quot;SNLI can be appended and treated as an unusually large additional CAPTIONS genre, built on image captions from the Flickr30k corpus (Young et al., 2014).&quot;,&quot;Hypothesis Collection To collect a sentence pair, we present a crowdworker with a sentence from a source text and ask them to compose three novel sentences (the hypotheses): one which is necessarily true or appropriate whenever the premise is true (paired with the premise and labeled ENTAILMENT ), one which is necessarily false or inappropriate whenever the premise is true ( CONTRADICTION ), and one where neither condition applies ( NEUTRAL ).&quot;,&quot;This method of data collection ensures that the three classes will be represented equally in the raw corpus.&quot;,&quot;The prompts that surround each premise sentence during hypothesis collection are slightly tailored to fit the genre of that premise sentence.&quot;,&quot;We pilot these prompts prior to data collection to ensure that the instructions are clear and that they yield hypothesis sentences that fit the intended meanings of the three classes.&quot;,&quot;There are five unique prompts in total: one for written non-fiction genres (SLATE , OUP, GOVERNMENT , VERBATIM , TRAVEL ; Figure 1), one for spoken genres (TELEPHONE , FACE-TO-FACE ), one for each of the less formal written genres (FICTION , LETTERS ), and a specialized one for 9/11, tailored to fit its potentially emotional content.&quot;,&quot;Each prompt is accompanied by example premises and hypothesis that are specific to each genre.&quot;,&quot;Below the instructions, we present three text fieldsone for each labelfollowed by a field for reporting issues, and a link to the frequently asked questions (FAQ) page.&quot;,&quot;We provide one FAQ page per prompt.&quot;,&quot;FAQs are modeled on their SNLI counterparts (supplied by the authors of that work) and include additional curated examples, answers to genre-specific questions arising from our pilot phase, and information about logistical concerns like payment.&quot;,&quot;For both hypothesis collection and validation, we present prompts to annotators using Hybrid Statistic SNLI MultiNLI Pairs w/ unanimous gold label 58.3% 58.2% Individual label = gold label 89.0% 88.7% Individual label = author's label 85.8% 85.2% Gold label = author's label 91.2% 92.6% Gold label 6 = author's label 6.8% 5.6% No gold label (no 3 labels match) 2.0% 1.8% Table 2: Key validation statistics for SNLI (copied from Bowman et al., 2015) and MultiNLI.&quot;,&quot;( gethybrid.io ), a crowdsoucring platform similar to the Amazon Mechanical Turk platform used for SNLI.&quot;,&quot;We used this platform to hire an organized group of workers.&quot;,&quot;387 annotators contributed through this group, and at no point was any identifying information about them, including demographic information, available to the authors.&quot;,&quot;Validation We perform an additional round of annotation on test and development examples to ensure accurate labelling.&quot;,&quot;The validation phase follows the same procedure used for SICK (Marelli et al., 2014b) and SNLI: Workers are presented with pairs of sentences and asked to supply a single label ( ENTAILMENT , CONTRADICTION , NEUTRAL ) for the pair.&quot;,&quot;Each pair is relabeled by four workers, yielding a total of five labels per example.&quot;,&quot;Validation instructions are tailored by genre, based on the main data collection prompt (Figure 1); a single FAQ, modeled after the validation FAQ from SNLI, is provided for reference.&quot;,&quot;In order to encourage thoughtful labeling, we manually label one percent of the validation examples and offer a $1 bonus each time a worker selects a label that matches ours.&quot;,&quot;For each validated sentence pair, we assign a gold label representing a majority vote between the initial label assigned to the pair by the original annotator, and the four additional labels assigned by validation annotators.&quot;,&quot;A small number of examples did not receive a three-vote consensus on any one label.&quot;,&quot;These examples are included in the distributed corpus, but are marked with  ' in the gold label field, and should not be used in standard evaluations.&quot;,&quot;Table 2 shows summary statistics capturing the results of validation, alongside corresponding figures for SNLI.&quot;,&quot;These statistics indicate that the labels included in MultiNLI are about as reliable as those included in SNLI, despite MultiNLI's more diverse text contents.&quot;,&quot;Table 1 shows randomly chosen development set examples from the collected corpus.&quot;,&quot;Hypotheses tend to be fluent and correctly spelled, though not all are complete sentences.&quot;,&quot;Punctuation is often omitted.&quot;,&quot;Hypotheses can rely heavily on knowledge about the world, and often don't correspond closely with their premises in syntactic structure.&quot;,&quot;Unlabeled test data is available on Kaggle for both matched and mismatched sets as competitions that will be open indefinitely; Evaluations on a subset of the test set have previously been conducted with different leaderboards through the RepEval 2017 Workshop (Nangia et al., 2017).&quot;,&quot;The corpus is available in two formatstab separated text and JSON Lines ( jsonl ), following SNLI.&quot;,&quot;For each example, premise and hypothesis strings, unique identifiers for the pair and prompt, and the following additional fields are specified:  gold label : label used for classification.&quot;,&quot;In examples rejected during the validation process, the value of this field will be  '.&quot;,&quot;sentence { 1,2 } parse : Each sentence as parsed by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003).&quot;,&quot;sentence { 1,2 } binary parse : parses in unlabeled binary-branching format.&quot;,&quot;label[1] : The label assigned during the creation of the sentence pair.&quot;,&quot;In rare cases this may be different from gold label , if a consensus of annotators chose a different label during the validation phase.&quot;,&quot;label[2...5] : The four labels assigned during validation by individual annotators to each development and test example.&quot;,&quot;These fields will be empty for training examples.&quot;,&quot;The current version of the corpus is freely available at nyu.edu/projects/bowman/multinli/ for typical machine learning uses, and may be modified and redistributed.&quot;,&quot;The majority of the corpus is released under the OANC's license, which allows all content to be freely used, modified, and shared under permissive terms.&quot;,&quot;The data in the FICTION section falls under several permissive licenses; Seven Swords is available under a Creative Commons Share-Alike 3.0 Unported License, and with the explicit permission of the author, Living History and Password Incorrect are available under Creative Commons Attribution 3.0 Unported Licenses; the remaining works of fiction are in the public domain in the United States (but may be licensed differently elsewhere).&quot;,&quot;Partition The distributed corpus comes with an explicit train/test/development split.&quot;,&quot;The test and development sets contain 2,000 randomly selected examples each from each of the genres, resulting in a total of 20,000 examples per set.&quot;,&quot;No premise sentence occurs in more than one set.&quot;,&quot;Statistics Table 3 shows some additional statistics.&quot;,&quot;Premise sentences in MultiNLI tend to be longer (max 401 words, mean 22.3 words) than their hypotheses (max 70 words, mean 11.4 words), and much longer, on average, than premises in SNLI (mean 14.1 words); premises in MultiNLI also tend to be parsed as complete sentences at a much higher rate on average (91%) than their SNLI counterparts (74%).&quot;,&quot;We observe that the two spoken genres differ in thiswith FACE-TO-FACE showing more complete sentences (91%) than TELEPHONE (71%)and speculate that the lack of visual feedback in a telephone setting may result in a high incidence of interrupted or otherwise incomplete sentences.&quot;,&quot;Hypothesis sentences in MultiNLI generally cannot be derived from their premise sentences using only trivial editing strategies.&quot;,&quot;While 2 .&quot;,&quot;5 % of the hypotheses in SNLI differ from their premises by deletion, only 0 .&quot;,&quot;9 % of those in MultiNLI (170 examples total) are constructed in this way.&quot;,&quot;Similarly, in SNLI, 1 .&quot;,&quot;6 % of hypotheses differ from their premises by addition, substitution, or shuf-fling a single word, while in MultiNLI this only happens in 1 .&quot;,&quot;2 % of examples.&quot;,&quot;The percentage of hypothesis-premise pairs with high token overlap ( > 37%) was comparable between MultiNLI (30% of pairs) and SNLI (29%).&quot;,&quot;These statistics suggest that MultiNLI's annotations are comparable in quality to those of SNLI.&quot;,&quot;To test the difficulty of the corpus, we experiment with three neural network models.&quot;,&quot;The first is a simple continuous bag of words (CBOW) model in which each sentence is represented as the sum of the embedding representations of its words.&quot;,&quot;The second computes representations by averaging the states of a bidirectional LSTM RNN (BiL-STM; Hochreiter and Schmidhuber, 1997) over words.&quot;,&quot;For the third, we implement and evaluate Chen et&quot;,&quot;al.'s Enhanced Sequential Inference Model (ESIM), which is roughly tied for the state of the art on SNLI at the time of writing.&quot;,&quot;We use the base ESIM without ensembling with a TreeL-STM (as in the HIM' runs in that work).&quot;,&quot;The first two models produce separate vector representations for each sentence and compute label predictions for pairs of representations.&quot;,&quot;To do this, they concatenate the representations for premise and hypothesis, their difference, and their element-wise product, following Mou et al. (2016b), and pass the result to a single tanh layer followed by a three-way softmax classifier.&quot;,&quot;All models are initialized with 300D reference GloVe vectors (840B token version; Pennington et al., 2014).&quot;,&quot;Out-of-vocabulary (OOV) words are initialized randomly and word embeddings are fine-tuned during training.&quot;,&quot;The models use 300D hidden states, as in most prior work on SNLI.&quot;,&quot;We use Dropout (Srivastava et al., 2014) for regularization.&quot;,&quot;For ESIM, we use a dropout rate of 0.5, following the paper.&quot;,&quot;For CBOW and BiLSTM models, we tune Dropout on the SNLI development set and find that a drop rate of 0.1 works well.&quot;,&quot;We use the Adam (Kingma and Ba, 2015) optimizer with default parameters.&quot;,&quot;Code is available at github.com/nyu-mll/multiNLI/ .&quot;,&quot;We train models on SNLI, MultiNLI, and a mixture; Table 4 shows the results.&quot;,&quot;In the mixed setting, we use the full MultiNLI training set and randomly select 15% of the SNLI training set at each epoch, ensuring that each available genre is seen during training with roughly equal frequency.&quot;,&quot;We also train a separate CBOW model on each individual genre to establish the degree to which simple models already allow for effective transfer across genres, using a dropout rate of 0.2.&quot;,&quot;When training on SNLI, a single random sample of 15% of the original training set is used.&quot;,&quot;For each genre represented in the training set, the model that performs best on it was trained on that genre; a model trained only on SNLI performs worse on every genre than comparable models trained on any genre from MultiNLI.&quot;,&quot;Models trained on a single genre from MultiNLI perform well on similar genres; for example, the model trained on TELEPHONE attains the best accuracy (63%) on FACE-TO-FACE , which was nearly one point better than it received on itself.&quot;,&quot;SLATE seems to be a difficult and relatively unusual genre and performance on it is relatively poor in this setting; when averaging over runs trained on SNLI and all genres in the matched section of the training set, average performance on SLATE was only 57.5%.&quot;,&quot;Sentences in SLATE cover a wide range of topics and phenomena, making it hard to do well on, but also forcing models trained on it be broadly capable; the model trained on SLATE achieves the highest accuracy of any model on 9/11 (55.6%) and VERBATIM (57.2%), and relatively high accuracy on TRAVEL (57.4%) and GOVERNMENT (58.3%).&quot;,&quot;We also observe that our models perform similarly on both the matched and mismatched test sets of MultiNLI.&quot;,&quot;We expect genre mismatch issues to become more conspicuous as models are developed that can better fit MultiNLI's training genres.&quot;,&quot;To evaluate the contribution of sentence length to corpus difficulty, we binned premises and hypotheses by length in 25-word increments for premises and 10-word increments for hypotheses.&quot;,&quot;Using the ESIM model, our strong baseline, we find a small effect (stronger for matched than mismatched) of premise length on model accuracy: accuracy decreases slightly as premise sentences increase in length.&quot;,&quot;We find no effect of hypothesis length on accuracy.&quot;,&quot;In data collection for NLI, different annotator decisions about the coreference between entities and events across the two sentences in a pair can lead to very different assignments of pairs to labels (de Marneffe et al., 2008; Marelli et al., 2014a; Bowman et al., 2015).&quot;,&quot;Drawing an example from Bowman et al., the pair a boat sank in the Pacific Ocean and a boat sank in the Atlantic Ocean can be labeled either CONTRADICTION or NEUTRAL depending on (among other things) whether the two mentions of boats are assumed to refer to the same entity in the world.&quot;,&quot;This uncertainty can present a serious problem for inter-annotator agreement, since it is not clear that it is possible to define an explicit set of rules around coreference that would be easily intelligible to an untrained annotator (or any non-expert).&quot;,&quot;Bowman et al. attempt to avoid this problem by using an annotation prompt that is highly dependent on the concreteness of image descriptions; but, as we engage with the much more abstract writing that is found in, for example, government documents, there is no reason to assume a priori that any similar prompt and annotation strategy can work.&quot;,&quot;We are surprised to find that this is not a major issue.&quot;,&quot;Through a relatively straightforward trial-and-error piloting phase, followed by discussion with our annotators, we manage to design prompts for abstract genres that yield high inter-annotator agreement scores nearly identical to those of SNLI (see Table 2).&quot;,&quot;These high scores suggest that our annotators agreed on a single task definition, and were able to apply it consistently across genres.&quot;,&quot;As expected, both the increase in the diversity of linguistic phenomena in MultiNLI and its longer average sentence length conspire to make MultiNLI dramatically more difficult than SNLI.&quot;,&quot;Our three baseline models perform better on SNLI than MultiNLI by about 15% when trained on the respective datasets.&quot;,&quot;All three models achieve accuracy above 80% on the SNLI test set when trained only on SNLI.&quot;,&quot;However, when trained on MultiNLI, only ESIM surpasses 70% accuracy on MultiNLI's test sets.&quot;,&quot;When we train models on MultiNLI and downsampled SNLI, we see an expected significant improvement on SNLI, but no significant change in performance on the MultiNLI test sets, suggesting including SNLI in training doesn't drive substantial improvement.&quot;,&quot;These results attest to MultiNLI's difficulty, and with its relatively high inter-annotator agreement, suggest that it presents a problem with substantial headroom for future work.&quot;,&quot;To better understand the types of language understanding skills that MultiNLI tests, we analyze the collected corpus using a set of annotation tags chosen to reflect linguistic phenomena which are known to be potentially difficult.&quot;,&quot;We use two methods to assign tags to sentences.&quot;,&quot;First, we use the Penn Treebank (PTB; Marcus et al., 1993) part-of-speech tag set (via the included Stanford Parser parses) to automatically isolate sentences 1118 Dev.&quot;,&quot;containing a range of easily-identified phenomena like comparatives.&quot;,&quot;Second, we isolate sentences that contain hand-chosen key words indicative of additional interesting phenomena.&quot;,&quot;The hand-chosen tag set covers the following phenomena: QUANTIFIERS contains single words with quantificational force (see, for example, Heim and Kratzer, 1998; Szabolcsi, 2010, e.g., many, all, few, some ); BELIEFVERBS contains sentence-embedding verbs denoting mental states (e.g., know, believe, think ), including irregular past tense forms; TIME TERMS contains single words with abstract temporal interpretation, (e.g., then, today ) and month names and days of the week; DISCOURSE MARKERS contains words that facilitate discourse coherence (e.g., yet, however, but, thus, despite ); PRESUPPOSITIONTRIGGERS contains words with lexical presuppositions (Stal-naker, 1974; Schlenker, 2016, e.g., again, too, anymore 13 ); CONDITIONALS contains the word if .&quot;,&quot;Table 5 presents the frequency of the tags in SNLI and MultiNLI, and model accuracy on MultiNLI (trained only on MultiNLI).&quot;,&quot;The incidence of tags varies by genre; the percentage of sentence pairs containing a particular annotation tag differs by a maximum over 30% across genres.&quot;,&quot;Sentence pairs containing pronouns are predictably common for all genres, with 93% of Government and Face-to-face pairs including at 13 Because their high frequency in the corpus, extremely common triggers like the were excluded from this tag.&quot;,&quot;least one.&quot;,&quot;The Telephone genre has the highest percentage of sentence pairs containing one occurrence of negation, WH-words, belief -verbs and time terms, Verbatim has the highest percentage of pairs containing quantifiers and conversational pivots, and Letters has the highest percentage of pairs that contain one or more modals.&quot;,&quot;Pairs containing comparatives and/or superlatives, which is the tag that our baseline models perform worst on, are most common in the Oxford University Press genre.&quot;,&quot;Based on this, we conclude that the genres are sufficiently different, because they are not uniform with respect to the percentages of sentence pairs that contain each of the annotation tags.&quot;,&quot;The distributions of labels within each tagged subset of the corpus roughly mirrors the balanced overall distribution.&quot;,&quot;The most frequent class overall (in this case, ENTAILMENT ) occurs with a frequency of roughly one third (see Table&quot;,&quot;4) in most.&quot;,&quot;Only two annotation tags differ from the baseline percentage of the most frequent class in the corpus by at least 5%: sentences containing negation, and sentences exceeding 20 words.&quot;,&quot;Sentences that contain negation are slightly more likely than average to be labeled CONTRADICTION , reflecting a similar finding in SNLI, while long sentences are slightly more likely to be labeled ENTAILMENT .&quot;,&quot;None of the baseline models perform substantially better on any tagged set than they do on the corpus overall, with average model accuracies on sentences containing specific tags falling within 1119 about 3 points of overall averages.&quot;,&quot;Using baseline model test accuracy overall as a metric (see Table 4), our baseline models had the most trouble on sentences containing comparatives or superlatives (losing 3-4 points each).&quot;,&quot;Despite the fact that 17% of sentence pairs in the corpus contained at least one instance of comparative or superlative, our baseline models don't utilize the information present in these sentences to predict the correct label for the pair, although presence of a comparative or superlative is slightly more predictive of a NEUTRAL label.&quot;,&quot;Moreover, the baseline models perform below average on discourse markers, such as despite and however , losing roughly 2 to 3 points each.&quot;,&quot;Unsurprisingly, the attention-based ESIM model performs better than the other two on sentences with greater than 20 words.&quot;,&quot;Additionally, our baseline models do show slight improvements in accuracy on negation, suggesting that they may be tracking it as a predictor of CONTRADICTION .&quot;,&quot;Natural language inference makes it easy to judge the degree to which neural network models for sentence understanding capture the full meanings for natural language sentences.&quot;,&quot;Existing NLI datasets like SNLI have facilitated substantial advances in modeling, but have limited headroom and coverage of the full diversity of meanings expressed in English.&quot;,&quot;This paper presents a new dataset that offers dramatically greater linguistic difficulty and diversity, and also serves as a benchmark for cross-genre domain adaptation.&quot;,&quot;Our new corpus, MultiNLI, improves upon SNLI in its empirical coveragebecause it includes a representative sample of text and speech from ten different genres, as opposed to just simple image captionsand its difficulty, containing a much higher percentage of sentences tagged with one or more elements from our tag set of thirteen difficult linguistic phenomena.&quot;,&quot;This greater diversity is reflected in the dramatically lower baseline model performance on MultiNLI than on SNLI (see Table&quot;,&quot;5) and comparable inter-annotator agreement, suggesting that MultiNLI has a lot of headroom remaining for future work.&quot;,&quot;The MultiNLI corpus was first released in draft form in the first half of 2017, and in the time since its initial release, work by others (Conneau et al., 2017) has shown that NLI can also be an effective source task for pre-training and transfer learning in the context of sentence-to-vector models, with models trained on SNLI and MultiNLI substantially outperforming all prior models on a suite of established transfer learning benchmarks.&quot;,&quot;We hope that this corpus will continue to serve for many years as a resource for the development and evaluation of methods for sentence understanding.&quot;,&quot;This work was made possible by a Google Faculty Research Award.&quot;,&quot;SB also gratefully acknowledges support from Tencent Holdings and Samsung Research.&quot;,&quot;We also thank George Dahl, the organizers of the RepEval 2016 and RepEval 2017 workshops, Andrew Drozdov, Angeliki Lazaridou, and our other NYU colleagues for help and advice.&quot;],&quot;string&quot;:&quot;[\n  \&quot;This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding.\&quot;,\n  \&quot;At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment ), improving upon available resources in both its coverage and difficulty.\&quot;,\n  \&quot;MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation.\&quot;,\n  \&quot;In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.\&quot;,\n  \&quot;Many of the most actively studied problems in NLP, including question answering, translation, and dialog, depend in large part on natural language understanding (NLU) for success.\&quot;,\n  \&quot;While there has been a great deal of work that uses representation learning techniques to pursue progress on these applied NLU problems directly, in order for a representation learning model to fully succeed at one of these problems, it must simultaneously succeed both at NLU, and at one or more additional hard machine learning problems like structured prediction or memory access.\&quot;,\n  \&quot;This makes it difficult to accurately judge the degree to which current models extract reasonable representations of language meaning in these settings.\&quot;,\n  \&quot;The task of natural language inference (NLI) is well positioned to serve as a benchmark task for research on NLU.\&quot;,\n  \&quot;In this task, also known as recognizing textual entailment (Cooper et al., 1996; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentenceslike one of those in Figure 1and asked to judge the relationship between their meanings by picking a label from a small set: typically ENTAILMENT , NEUTRAL , and CONTRADICTION .\&quot;,\n  \&quot;Succeeding at NLI does not require a system to solve any difficult machine learning problems except, crucially, that of extracting effective and thorough representations for the meanings of sentences (i.e., their lexical and compositional semantics).\&quot;,\n  \&quot;In particular, a model must handle phenomena like lexical entailment, quantification, coreference, tense, belief, modality, and lexical and syntactic ambiguity.\&quot;,\n  \&quot;As the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention (Wang and Jiang, 2016; Parikh et al., 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016; Chen et al., 2017).\&quot;,\n  \&quot;However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways.\&quot;,\n  \&quot;First, the sentences in SNLI are derived from only a single text genreimage captionsand are thus limited to descriptions of concrete visual scenes, rendering the hypothesis sentences used to describe these scenes short and simple, and rendering many important phenomenalike temporal reasoning (e.g., yesterday ), belief (e.g., know ), and modality (e.g., should )rare enough to be irrelevant to task performance.\&quot;,\n  \&quot;Second, because of these issues, SNLI is not sufficiently demanding to serve as an effective benchmark for NLU, with the best current model performance falling within a few percentage points of human accuracy and limited room left for fine-grained comparisons between strong models.\&quot;,\n  \&quot;This paper introduces a new challenge dataset, the Multi-Genre NLI Corpus (MultiNLI), whose chief purpose is to remedy these limitations by making it possible to run large-scale NLI evaluations that capture more of the complexity of modern English.\&quot;,\n  \&quot;While its size (433k pairs) and mode of collection are modeled closely on SNLI, unlike that corpus, MultiNLI represents both written and spoken speech in a wide range of styles, degrees of formality, and topics.\&quot;,\n  \&quot;Our chief motivation in creating this corpus is to provide a benchmark for ambitious machine learning research on the core problems of NLU, but we are additionally interested in constructing a corpus that facilitates work on domain adaptation and cross-domain transfer learning.\&quot;,\n  \&quot;These techniqueswhich use labeled training data for a source domain, and aim to train a model that performs well on test data from a target domain with a different distributionhave resulted in gains across many tasks (Daume III and Marcu, 2006; Ben-David et al., 2007), including sequence and part-of-speech tagging (Blitzer et al., 2006; Peng and Dredze, 2017).\&quot;,\n  \&quot;Moreover, in application areas outside NLU, artificial neural network techniques have made it possible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data (Krizhevsky et al., 2012; Zeiler and Fergus, 2014; Donahue et al., 2014).\&quot;,\n  \&quot;However, attempts to bring this kind of general purpose representation learning to NLU have seen only very limited success (see, for example, Mou et al., 2016a).\&quot;,\n  \&quot;Nearly all successful applications of representation learning to NLU have involved models that are trained on data closely resembling the target evaluation data in both task and style.\&quot;,\n  \&quot;This fact limits the usefulness of these tools for problems involving styles of language not represented in large annotated training sets.\&quot;,\n  \&quot;With this in mind, we construct MultiNLI so as to make it possible to explicitly evaluate models both on the quality of their sentence representations within the training domain and on their ability to derive reasonable representations in unfamiliar domains.\&quot;,\n  \&quot;The corpus is derived from ten different genres of written and spoken English, which are collectively meant to approximate the full diversity of ways in which modern standard 1113 This task will involve reading a line from a non-fiction article and writing three sentences that relate to it.\&quot;,\n  \&quot;The line will describe a situation or event.\&quot;,\n  \&quot;Using only this description and what you know about the world:  Write one sentence that is definitely correct about the situation or event in the line.\&quot;,\n  \&quot;Write one sentence that might be correct about the situation or event in the line.\&quot;,\n  \&quot;Write one sentence that is definitely incorrect about the situation or event in the line.\&quot;,\n  \&quot;American English is used.\&quot;,\n  \&quot;All of the genres appear in the test and development sets, but only five are included in the training set.\&quot;,\n  \&quot;Models thus can be evaluated on both the matched test examples, which are derived from the same sources as those in the training set, and on the mismatched examples, which do not closely resemble any of those seen at training time.\&quot;,\n  \&quot;The data collection methodology for MultiNLI is similar to that of SNLI: We create each sentence pair by selecting a premise sentence from a preexisting text source and asking a human annotator to compose a novel sentence to pair with it as a hypothesis.\&quot;,\n  \&quot;This section discusses the sources of our premise sentences, our collection method for hypotheses, and our validation (relabeling) strategy.\&quot;,\n  \&quot;Premise Text Sources The MultiNLI premise sentences are derived from ten sources of freely available text which are meant to be maximally diverse and roughly represent the full range of American English.\&quot;,\n  \&quot;We selected nine sources from the second release of the Open American National Corpus (OANC; Fillmore et al., 1998; Macleod et al., 2000; Ide and Macleod, 2001; Ide and Su-derman, 2006, downloaded 12/2016 1 ), balancing the volume of source text roughly evenly across genres, and avoiding genres with content that would be too difficult for untrained annotators.\&quot;,\n  \&quot;and Conversation Collection of two-sided, in-person conversations that took place in the early 2000s (FACE-TO-FACE ); reports, speeches, letters, and press releases from public domain government websites (GOVERNMENT ); letters from the Indiana Center for Intercultural Communication of Philanthropic Fundraising Discourse written in the late 1990searly 2000s (LETTERS ); the public report from the National Commission on Terrorist Attacks Upon the United States released on July 22, 2004 2 (9/11); five non-fiction works on the textile industry and child development published by the Oxford University Press (OUP); popular culture articles from the archives of Slate Magazine (SLATE ) written between 19962000; transcriptions from University of Pennsylvania's Linguistic Data Consortium Switchboard corpus of two-sided, telephone conversations that took place in 1990 or 1991 (TELEPHONE ); travel guides published by Berlitz Publishing in the early 2000s (TRAVEL ); and short posts about linguistics for non-specialists from the Verbatim archives written between 1990 and 1996 (VERBATIM ).\&quot;,\n  \&quot;For our tenth genre, FICTION , we compile several freely available works of contemporary fiction written between 1912 and 2010, spanning various genres, including mystery ( The Mysterious Affair at Styles , 3 Christie, 1921; The Secret Adversary , 4 Christie, 1922; Murder in the Gun Room , 5 Piper, 1953), humor ( Password Incorrect , 6 Name, 2008), western ( Rebel Spurs , 7 Norton, 1962), science fiction ( Seven Swords , 8 Shea, 2008; Living History , 9 Essex, 2016; The Sky Is Falling , 10 Del Rey, 1973; Youth , 11 Asimov, May 1952), and adventure ( Captain Blood , 12 Sabatini, 1922).\&quot;,\n  \&quot;We construct premise sentences from these ten source texts with minimal preprocessing; unique the sentences within genres, exclude very short 2 https://9-11commission.gov/ 3 gutenberg.org/files/863/863-0.txt 4 gutenberg.org/files/1155/1155-0.txt 5 gutenberg.org/files/17866/17866.txt 6 http://manybooks.net/pages/ namenother09password_incorrect/0.html 7 gutenberg.org/files/20840/20840-0.txt 8 http://mikeshea.net/stories/seven_ swords.html , shared with the author's permission.\&quot;,\n  \&quot;9 manybooks.net/pages/ essexbother10living_history/0.html 10 gutenberg.org/cache/epub/18768/ pg18768.txt 11 gutenberg.org/cache/epub/31547/ pg31547.txt 12 gutenberg.org/files/1965/1965-0.txt 1114 sentences (under eight characters), and manually remove certain types of non-narrative writing, such as mathematical formulae, bibliographic references, and lists.\&quot;,\n  \&quot;Although SNLI is collected in largely the same way as MultiNLI, and is also permissively licensed, we do not include SNLI in the MultiNLI corpus distribution.\&quot;,\n  \&quot;SNLI can be appended and treated as an unusually large additional CAPTIONS genre, built on image captions from the Flickr30k corpus (Young et al., 2014).\&quot;,\n  \&quot;Hypothesis Collection To collect a sentence pair, we present a crowdworker with a sentence from a source text and ask them to compose three novel sentences (the hypotheses): one which is necessarily true or appropriate whenever the premise is true (paired with the premise and labeled ENTAILMENT ), one which is necessarily false or inappropriate whenever the premise is true ( CONTRADICTION ), and one where neither condition applies ( NEUTRAL ).\&quot;,\n  \&quot;This method of data collection ensures that the three classes will be represented equally in the raw corpus.\&quot;,\n  \&quot;The prompts that surround each premise sentence during hypothesis collection are slightly tailored to fit the genre of that premise sentence.\&quot;,\n  \&quot;We pilot these prompts prior to data collection to ensure that the instructions are clear and that they yield hypothesis sentences that fit the intended meanings of the three classes.\&quot;,\n  \&quot;There are five unique prompts in total: one for written non-fiction genres (SLATE , OUP, GOVERNMENT , VERBATIM , TRAVEL ; Figure 1), one for spoken genres (TELEPHONE , FACE-TO-FACE ), one for each of the less formal written genres (FICTION , LETTERS ), and a specialized one for 9/11, tailored to fit its potentially emotional content.\&quot;,\n  \&quot;Each prompt is accompanied by example premises and hypothesis that are specific to each genre.\&quot;,\n  \&quot;Below the instructions, we present three text fieldsone for each labelfollowed by a field for reporting issues, and a link to the frequently asked questions (FAQ) page.\&quot;,\n  \&quot;We provide one FAQ page per prompt.\&quot;,\n  \&quot;FAQs are modeled on their SNLI counterparts (supplied by the authors of that work) and include additional curated examples, answers to genre-specific questions arising from our pilot phase, and information about logistical concerns like payment.\&quot;,\n  \&quot;For both hypothesis collection and validation, we present prompts to annotators using Hybrid Statistic SNLI MultiNLI Pairs w/ unanimous gold label 58.3% 58.2% Individual label = gold label 89.0% 88.7% Individual label = author's label 85.8% 85.2% Gold label = author's label 91.2% 92.6% Gold label 6 = author's label 6.8% 5.6% No gold label (no 3 labels match) 2.0% 1.8% Table 2: Key validation statistics for SNLI (copied from Bowman et al., 2015) and MultiNLI.\&quot;,\n  \&quot;( gethybrid.io ), a crowdsoucring platform similar to the Amazon Mechanical Turk platform used for SNLI.\&quot;,\n  \&quot;We used this platform to hire an organized group of workers.\&quot;,\n  \&quot;387 annotators contributed through this group, and at no point was any identifying information about them, including demographic information, available to the authors.\&quot;,\n  \&quot;Validation We perform an additional round of annotation on test and development examples to ensure accurate labelling.\&quot;,\n  \&quot;The validation phase follows the same procedure used for SICK (Marelli et al., 2014b) and SNLI: Workers are presented with pairs of sentences and asked to supply a single label ( ENTAILMENT , CONTRADICTION , NEUTRAL ) for the pair.\&quot;,\n  \&quot;Each pair is relabeled by four workers, yielding a total of five labels per example.\&quot;,\n  \&quot;Validation instructions are tailored by genre, based on the main data collection prompt (Figure 1); a single FAQ, modeled after the validation FAQ from SNLI, is provided for reference.\&quot;,\n  \&quot;In order to encourage thoughtful labeling, we manually label one percent of the validation examples and offer a $1 bonus each time a worker selects a label that matches ours.\&quot;,\n  \&quot;For each validated sentence pair, we assign a gold label representing a majority vote between the initial label assigned to the pair by the original annotator, and the four additional labels assigned by validation annotators.\&quot;,\n  \&quot;A small number of examples did not receive a three-vote consensus on any one label.\&quot;,\n  \&quot;These examples are included in the distributed corpus, but are marked with  ' in the gold label field, and should not be used in standard evaluations.\&quot;,\n  \&quot;Table 2 shows summary statistics capturing the results of validation, alongside corresponding figures for SNLI.\&quot;,\n  \&quot;These statistics indicate that the labels included in MultiNLI are about as reliable as those included in SNLI, despite MultiNLI's more diverse text contents.\&quot;,\n  \&quot;Table 1 shows randomly chosen development set examples from the collected corpus.\&quot;,\n  \&quot;Hypotheses tend to be fluent and correctly spelled, though not all are complete sentences.\&quot;,\n  \&quot;Punctuation is often omitted.\&quot;,\n  \&quot;Hypotheses can rely heavily on knowledge about the world, and often don't correspond closely with their premises in syntactic structure.\&quot;,\n  \&quot;Unlabeled test data is available on Kaggle for both matched and mismatched sets as competitions that will be open indefinitely; Evaluations on a subset of the test set have previously been conducted with different leaderboards through the RepEval 2017 Workshop (Nangia et al., 2017).\&quot;,\n  \&quot;The corpus is available in two formatstab separated text and JSON Lines ( jsonl ), following SNLI.\&quot;,\n  \&quot;For each example, premise and hypothesis strings, unique identifiers for the pair and prompt, and the following additional fields are specified:  gold label : label used for classification.\&quot;,\n  \&quot;In examples rejected during the validation process, the value of this field will be  '.\&quot;,\n  \&quot;sentence { 1,2 } parse : Each sentence as parsed by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003).\&quot;,\n  \&quot;sentence { 1,2 } binary parse : parses in unlabeled binary-branching format.\&quot;,\n  \&quot;label[1] : The label assigned during the creation of the sentence pair.\&quot;,\n  \&quot;In rare cases this may be different from gold label , if a consensus of annotators chose a different label during the validation phase.\&quot;,\n  \&quot;label[2...5] : The four labels assigned during validation by individual annotators to each development and test example.\&quot;,\n  \&quot;These fields will be empty for training examples.\&quot;,\n  \&quot;The current version of the corpus is freely available at nyu.edu/projects/bowman/multinli/ for typical machine learning uses, and may be modified and redistributed.\&quot;,\n  \&quot;The majority of the corpus is released under the OANC's license, which allows all content to be freely used, modified, and shared under permissive terms.\&quot;,\n  \&quot;The data in the FICTION section falls under several permissive licenses; Seven Swords is available under a Creative Commons Share-Alike 3.0 Unported License, and with the explicit permission of the author, Living History and Password Incorrect are available under Creative Commons Attribution 3.0 Unported Licenses; the remaining works of fiction are in the public domain in the United States (but may be licensed differently elsewhere).\&quot;,\n  \&quot;Partition The distributed corpus comes with an explicit train/test/development split.\&quot;,\n  \&quot;The test and development sets contain 2,000 randomly selected examples each from each of the genres, resulting in a total of 20,000 examples per set.\&quot;,\n  \&quot;No premise sentence occurs in more than one set.\&quot;,\n  \&quot;Statistics Table 3 shows some additional statistics.\&quot;,\n  \&quot;Premise sentences in MultiNLI tend to be longer (max 401 words, mean 22.3 words) than their hypotheses (max 70 words, mean 11.4 words), and much longer, on average, than premises in SNLI (mean 14.1 words); premises in MultiNLI also tend to be parsed as complete sentences at a much higher rate on average (91%) than their SNLI counterparts (74%).\&quot;,\n  \&quot;We observe that the two spoken genres differ in thiswith FACE-TO-FACE showing more complete sentences (91%) than TELEPHONE (71%)and speculate that the lack of visual feedback in a telephone setting may result in a high incidence of interrupted or otherwise incomplete sentences.\&quot;,\n  \&quot;Hypothesis sentences in MultiNLI generally cannot be derived from their premise sentences using only trivial editing strategies.\&quot;,\n  \&quot;While 2 .\&quot;,\n  \&quot;5 % of the hypotheses in SNLI differ from their premises by deletion, only 0 .\&quot;,\n  \&quot;9 % of those in MultiNLI (170 examples total) are constructed in this way.\&quot;,\n  \&quot;Similarly, in SNLI, 1 .\&quot;,\n  \&quot;6 % of hypotheses differ from their premises by addition, substitution, or shuf-fling a single word, while in MultiNLI this only happens in 1 .\&quot;,\n  \&quot;2 % of examples.\&quot;,\n  \&quot;The percentage of hypothesis-premise pairs with high token overlap ( > 37%) was comparable between MultiNLI (30% of pairs) and SNLI (29%).\&quot;,\n  \&quot;These statistics suggest that MultiNLI's annotations are comparable in quality to those of SNLI.\&quot;,\n  \&quot;To test the difficulty of the corpus, we experiment with three neural network models.\&quot;,\n  \&quot;The first is a simple continuous bag of words (CBOW) model in which each sentence is represented as the sum of the embedding representations of its words.\&quot;,\n  \&quot;The second computes representations by averaging the states of a bidirectional LSTM RNN (BiL-STM; Hochreiter and Schmidhuber, 1997) over words.\&quot;,\n  \&quot;For the third, we implement and evaluate Chen et\&quot;,\n  \&quot;al.'s Enhanced Sequential Inference Model (ESIM), which is roughly tied for the state of the art on SNLI at the time of writing.\&quot;,\n  \&quot;We use the base ESIM without ensembling with a TreeL-STM (as in the HIM' runs in that work).\&quot;,\n  \&quot;The first two models produce separate vector representations for each sentence and compute label predictions for pairs of representations.\&quot;,\n  \&quot;To do this, they concatenate the representations for premise and hypothesis, their difference, and their element-wise product, following Mou et al. (2016b), and pass the result to a single tanh layer followed by a three-way softmax classifier.\&quot;,\n  \&quot;All models are initialized with 300D reference GloVe vectors (840B token version; Pennington et al., 2014).\&quot;,\n  \&quot;Out-of-vocabulary (OOV) words are initialized randomly and word embeddings are fine-tuned during training.\&quot;,\n  \&quot;The models use 300D hidden states, as in most prior work on SNLI.\&quot;,\n  \&quot;We use Dropout (Srivastava et al., 2014) for regularization.\&quot;,\n  \&quot;For ESIM, we use a dropout rate of 0.5, following the paper.\&quot;,\n  \&quot;For CBOW and BiLSTM models, we tune Dropout on the SNLI development set and find that a drop rate of 0.1 works well.\&quot;,\n  \&quot;We use the Adam (Kingma and Ba, 2015) optimizer with default parameters.\&quot;,\n  \&quot;Code is available at github.com/nyu-mll/multiNLI/ .\&quot;,\n  \&quot;We train models on SNLI, MultiNLI, and a mixture; Table 4 shows the results.\&quot;,\n  \&quot;In the mixed setting, we use the full MultiNLI training set and randomly select 15% of the SNLI training set at each epoch, ensuring that each available genre is seen during training with roughly equal frequency.\&quot;,\n  \&quot;We also train a separate CBOW model on each individual genre to establish the degree to which simple models already allow for effective transfer across genres, using a dropout rate of 0.2.\&quot;,\n  \&quot;When training on SNLI, a single random sample of 15% of the original training set is used.\&quot;,\n  \&quot;For each genre represented in the training set, the model that performs best on it was trained on that genre; a model trained only on SNLI performs worse on every genre than comparable models trained on any genre from MultiNLI.\&quot;,\n  \&quot;Models trained on a single genre from MultiNLI perform well on similar genres; for example, the model trained on TELEPHONE attains the best accuracy (63%) on FACE-TO-FACE , which was nearly one point better than it received on itself.\&quot;,\n  \&quot;SLATE seems to be a difficult and relatively unusual genre and performance on it is relatively poor in this setting; when averaging over runs trained on SNLI and all genres in the matched section of the training set, average performance on SLATE was only 57.5%.\&quot;,\n  \&quot;Sentences in SLATE cover a wide range of topics and phenomena, making it hard to do well on, but also forcing models trained on it be broadly capable; the model trained on SLATE achieves the highest accuracy of any model on 9/11 (55.6%) and VERBATIM (57.2%), and relatively high accuracy on TRAVEL (57.4%) and GOVERNMENT (58.3%).\&quot;,\n  \&quot;We also observe that our models perform similarly on both the matched and mismatched test sets of MultiNLI.\&quot;,\n  \&quot;We expect genre mismatch issues to become more conspicuous as models are developed that can better fit MultiNLI's training genres.\&quot;,\n  \&quot;To evaluate the contribution of sentence length to corpus difficulty, we binned premises and hypotheses by length in 25-word increments for premises and 10-word increments for hypotheses.\&quot;,\n  \&quot;Using the ESIM model, our strong baseline, we find a small effect (stronger for matched than mismatched) of premise length on model accuracy: accuracy decreases slightly as premise sentences increase in length.\&quot;,\n  \&quot;We find no effect of hypothesis length on accuracy.\&quot;,\n  \&quot;In data collection for NLI, different annotator decisions about the coreference between entities and events across the two sentences in a pair can lead to very different assignments of pairs to labels (de Marneffe et al., 2008; Marelli et al., 2014a; Bowman et al., 2015).\&quot;,\n  \&quot;Drawing an example from Bowman et al., the pair a boat sank in the Pacific Ocean and a boat sank in the Atlantic Ocean can be labeled either CONTRADICTION or NEUTRAL depending on (among other things) whether the two mentions of boats are assumed to refer to the same entity in the world.\&quot;,\n  \&quot;This uncertainty can present a serious problem for inter-annotator agreement, since it is not clear that it is possible to define an explicit set of rules around coreference that would be easily intelligible to an untrained annotator (or any non-expert).\&quot;,\n  \&quot;Bowman et al. attempt to avoid this problem by using an annotation prompt that is highly dependent on the concreteness of image descriptions; but, as we engage with the much more abstract writing that is found in, for example, government documents, there is no reason to assume a priori that any similar prompt and annotation strategy can work.\&quot;,\n  \&quot;We are surprised to find that this is not a major issue.\&quot;,\n  \&quot;Through a relatively straightforward trial-and-error piloting phase, followed by discussion with our annotators, we manage to design prompts for abstract genres that yield high inter-annotator agreement scores nearly identical to those of SNLI (see Table 2).\&quot;,\n  \&quot;These high scores suggest that our annotators agreed on a single task definition, and were able to apply it consistently across genres.\&quot;,\n  \&quot;As expected, both the increase in the diversity of linguistic phenomena in MultiNLI and its longer average sentence length conspire to make MultiNLI dramatically more difficult than SNLI.\&quot;,\n  \&quot;Our three baseline models perform better on SNLI than MultiNLI by about 15% when trained on the respective datasets.\&quot;,\n  \&quot;All three models achieve accuracy above 80% on the SNLI test set when trained only on SNLI.\&quot;,\n  \&quot;However, when trained on MultiNLI, only ESIM surpasses 70% accuracy on MultiNLI's test sets.\&quot;,\n  \&quot;When we train models on MultiNLI and downsampled SNLI, we see an expected significant improvement on SNLI, but no significant change in performance on the MultiNLI test sets, suggesting including SNLI in training doesn't drive substantial improvement.\&quot;,\n  \&quot;These results attest to MultiNLI's difficulty, and with its relatively high inter-annotator agreement, suggest that it presents a problem with substantial headroom for future work.\&quot;,\n  \&quot;To better understand the types of language understanding skills that MultiNLI tests, we analyze the collected corpus using a set of annotation tags chosen to reflect linguistic phenomena which are known to be potentially difficult.\&quot;,\n  \&quot;We use two methods to assign tags to sentences.\&quot;,\n  \&quot;First, we use the Penn Treebank (PTB; Marcus et al., 1993) part-of-speech tag set (via the included Stanford Parser parses) to automatically isolate sentences 1118 Dev.\&quot;,\n  \&quot;containing a range of easily-identified phenomena like comparatives.\&quot;,\n  \&quot;Second, we isolate sentences that contain hand-chosen key words indicative of additional interesting phenomena.\&quot;,\n  \&quot;The hand-chosen tag set covers the following phenomena: QUANTIFIERS contains single words with quantificational force (see, for example, Heim and Kratzer, 1998; Szabolcsi, 2010, e.g., many, all, few, some ); BELIEFVERBS contains sentence-embedding verbs denoting mental states (e.g., know, believe, think ), including irregular past tense forms; TIME TERMS contains single words with abstract temporal interpretation, (e.g., then, today ) and month names and days of the week; DISCOURSE MARKERS contains words that facilitate discourse coherence (e.g., yet, however, but, thus, despite ); PRESUPPOSITIONTRIGGERS contains words with lexical presuppositions (Stal-naker, 1974; Schlenker, 2016, e.g., again, too, anymore 13 ); CONDITIONALS contains the word if .\&quot;,\n  \&quot;Table 5 presents the frequency of the tags in SNLI and MultiNLI, and model accuracy on MultiNLI (trained only on MultiNLI).\&quot;,\n  \&quot;The incidence of tags varies by genre; the percentage of sentence pairs containing a particular annotation tag differs by a maximum over 30% across genres.\&quot;,\n  \&quot;Sentence pairs containing pronouns are predictably common for all genres, with 93% of Government and Face-to-face pairs including at 13 Because their high frequency in the corpus, extremely common triggers like the were excluded from this tag.\&quot;,\n  \&quot;least one.\&quot;,\n  \&quot;The Telephone genre has the highest percentage of sentence pairs containing one occurrence of negation, WH-words, belief -verbs and time terms, Verbatim has the highest percentage of pairs containing quantifiers and conversational pivots, and Letters has the highest percentage of pairs that contain one or more modals.\&quot;,\n  \&quot;Pairs containing comparatives and/or superlatives, which is the tag that our baseline models perform worst on, are most common in the Oxford University Press genre.\&quot;,\n  \&quot;Based on this, we conclude that the genres are sufficiently different, because they are not uniform with respect to the percentages of sentence pairs that contain each of the annotation tags.\&quot;,\n  \&quot;The distributions of labels within each tagged subset of the corpus roughly mirrors the balanced overall distribution.\&quot;,\n  \&quot;The most frequent class overall (in this case, ENTAILMENT ) occurs with a frequency of roughly one third (see Table\&quot;,\n  \&quot;4) in most.\&quot;,\n  \&quot;Only two annotation tags differ from the baseline percentage of the most frequent class in the corpus by at least 5%: sentences containing negation, and sentences exceeding 20 words.\&quot;,\n  \&quot;Sentences that contain negation are slightly more likely than average to be labeled CONTRADICTION , reflecting a similar finding in SNLI, while long sentences are slightly more likely to be labeled ENTAILMENT .\&quot;,\n  \&quot;None of the baseline models perform substantially better on any tagged set than they do on the corpus overall, with average model accuracies on sentences containing specific tags falling within 1119 about 3 points of overall averages.\&quot;,\n  \&quot;Using baseline model test accuracy overall as a metric (see Table 4), our baseline models had the most trouble on sentences containing comparatives or superlatives (losing 3-4 points each).\&quot;,\n  \&quot;Despite the fact that 17% of sentence pairs in the corpus contained at least one instance of comparative or superlative, our baseline models don't utilize the information present in these sentences to predict the correct label for the pair, although presence of a comparative or superlative is slightly more predictive of a NEUTRAL label.\&quot;,\n  \&quot;Moreover, the baseline models perform below average on discourse markers, such as despite and however , losing roughly 2 to 3 points each.\&quot;,\n  \&quot;Unsurprisingly, the attention-based ESIM model performs better than the other two on sentences with greater than 20 words.\&quot;,\n  \&quot;Additionally, our baseline models do show slight improvements in accuracy on negation, suggesting that they may be tracking it as a predictor of CONTRADICTION .\&quot;,\n  \&quot;Natural language inference makes it easy to judge the degree to which neural network models for sentence understanding capture the full meanings for natural language sentences.\&quot;,\n  \&quot;Existing NLI datasets like SNLI have facilitated substantial advances in modeling, but have limited headroom and coverage of the full diversity of meanings expressed in English.\&quot;,\n  \&quot;This paper presents a new dataset that offers dramatically greater linguistic difficulty and diversity, and also serves as a benchmark for cross-genre domain adaptation.\&quot;,\n  \&quot;Our new corpus, MultiNLI, improves upon SNLI in its empirical coveragebecause it includes a representative sample of text and speech from ten different genres, as opposed to just simple image captionsand its difficulty, containing a much higher percentage of sentences tagged with one or more elements from our tag set of thirteen difficult linguistic phenomena.\&quot;,\n  \&quot;This greater diversity is reflected in the dramatically lower baseline model performance on MultiNLI than on SNLI (see Table\&quot;,\n  \&quot;5) and comparable inter-annotator agreement, suggesting that MultiNLI has a lot of headroom remaining for future work.\&quot;,\n  \&quot;The MultiNLI corpus was first released in draft form in the first half of 2017, and in the time since its initial release, work by others (Conneau et al., 2017) has shown that NLI can also be an effective source task for pre-training and transfer learning in the context of sentence-to-vector models, with models trained on SNLI and MultiNLI substantially outperforming all prior models on a suite of established transfer learning benchmarks.\&quot;,\n  \&quot;We hope that this corpus will continue to serve for many years as a resource for the development and evaluation of methods for sentence understanding.\&quot;,\n  \&quot;This work was made possible by a Google Faculty Research Award.\&quot;,\n  \&quot;SB also gratefully acknowledges support from Tencent Holdings and Samsung Research.\&quot;,\n  \&quot;We also thank George Dahl, the organizers of the RepEval 2016 and RepEval 2017 workshops, Andrew Drozdov, Angeliki Lazaridou, and our other NYU colleagues for help and advice.\&quot;\n]&quot;},&quot;labels&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;],&quot;string&quot;:&quot;[\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;\n]&quot;}}},{&quot;rowIdx&quot;:3,&quot;cells&quot;:{&quot;sentences&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;Intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks.&quot;,&quot;In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails.&quot;,&quot;We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action.&quot;,&quot;We design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence learning, obtaining BLEU and ROUGE scores of 0 .&quot;,&quot;23 and 0 .&quot;,&quot;63 for this task.&quot;,&quot;To the best of our knowledge, this is the first work to address the problem of composing To-Do items from emails.&quot;,&quot;Email is one of the most used forms of communication especially in enterprise and work settings (Radicati and Levenstein, 2015).&quot;,&quot;With the growing number of users in email platforms, service providers are constantly seeking to improve user experience for a myriad of applications such as online retail, instant messaging and event management (Feddern-Bekcan, 2008).&quot;,&quot;Smart Reply (Kan-nan et al., 2016) and Smart Compose (Chen et al., 2019) are two recent features that provide contextual assistance to users aiming to reduce typing efforts.&quot;,&quot;Another line of work in this direction is for automated task management and scheduling.&quot;,&quot;For example.&quot;,&quot;the recent Nudge feature 1 in Gmail and Insights in Outlook 2 are designed to remind users to follow-up on an email or pay attention to pending tasks.&quot;,&quot;Smart To-Do takes a step further in task assistance and seeks to boost user productivity by automatically generating To-Do items from their email  Work done as an intern at Microsoft Research.&quot;,&quot;1 Gmail Nudge 2 Outlook Insights From: Alice To: john@contoso.com Subject: Sales Report Hi John, From: John To: alice@contoso.com Subject: RE: Sales Report I am doing well.&quot;,&quot;context.&quot;,&quot;Text generation from emails, like creating To-Do items, is replete with complexities due to the diversity of conversations in email threads, heterogeneous structure of emails and various meta-deta involved.&quot;,&quot;As opposed to prior works in text generation like news headlines, email subject lines and email conversation summarization, To-Do items are action-focused , requiring the identification of a specific task to be performed.&quot;,&quot;In this work, we introduce the task of automatically generating To-Do items from email context and meta-data to assist users with following up on their promised actions (also referred to as commitments in this work).&quot;,&quot;Refer to Figure 1 for an illustration.&quot;,&quot;Given an email, its temporal context (i.e. thread), and associated meta-data like the name of the sender and recipient, we want to generate a short and succinct To-Do item for the task mentioned in the email.&quot;,&quot;This requires identifying the task sentence (also referred to as a query ), relevant sentences in the email that provide contextual information about the query along with the entities (e.g., people) associated with the task.&quot;,&quot;We utilize existing work to identify the task sentence via a commitment classifier that detects action intents in the emails.&quot;,&quot;Thereafter C Commitment Classifier D Does the email contain commitment ?&quot;,&quot;we use an unsupervised technique to extract key sentences in the email that are helpful in providing contextual information about the query.&quot;,&quot;These pieces of information are further combined to generate the To-Do item using a sequence-to-sequence architecture with deep neural networks.&quot;,&quot;Figure 2 shows a schematic diagram of the process.&quot;,&quot;Since there is no existing work or dataset on this problem, our first step is to collect annotated data for this task.&quot;,&quot;Overall, our contributions can be summarized as follows:  We create a new dataset for To-Do item generation from emails containing action items based on the publicly available email corpus Avocado (Oard et al., 2015).&quot;,&quot;3  We develop a two-stage algorithm, based on unsupervised task-focused content selection and subsequent text generation combining contextual information and email meta-data.&quot;,&quot;We conduct experiments on this new dataset and show that our model performs at par with human judgments on multiple performance metrics.&quot;,&quot;Summarization of email threads has been the focus of multiple research works in the past (Rambow et al., 2004; Carenini et al., 2007; Dredze et al., 2008).&quot;,&quot;There has also been considerable research on identifying speech acts or tasks in emails (Car-valho and Cohen, 2005; Lampert et al., 2010; Scerri et al., 2010) and how it can be robustly adapted across diverse email corpora (Azarbonyad et al., 2019).&quot;,&quot;Recently, novel neural architectures have been explored for modeling action items in emails 3 We will release the code and data (in accordance with LDC and Avocado policy) at https://aka.ms/SmartToDo .&quot;,&quot;Email examples in this paper are similar to those in our dataset but are not reproducing text from the Avocado dataset.&quot;,&quot;(Lin et al., 2018) and identifying intents in email conversations (Wang et al., 2019).&quot;,&quot;However, there has been less focus on task-specific email summarization (Corston-Oliver et al., 2004).&quot;,&quot;The closest to our work is that of email subject line generation (Zhang and Tetreault, 2019).&quot;,&quot;But it focuses on a common email theme and uses a supervised approach for sentence selection, whereas our method relies on identifying the task-related context.&quot;,&quot;We build upon the Avocado dataset (Oard et al., 2015) 4 containing an anonymized version of the Outlook mailbox for 279 employees with various meta-data and 938 , 035 emails overall.&quot;,&quot;Emails contain various user intents including planning and scheduling meetings, requests for information, exchange of information, casual conversations, etc. (Wang et al., 2019).&quot;,&quot;For the purpose of this work, we first need to extract emails containing at least one sentence where the sender has promised to perform an action.&quot;,&quot;It could be performing a task, providing some information, keeping others informed about a topic and so on.&quot;,&quot;We use the term commitment to refer to such intent in an email and the term commitment sentence to refer to each sentence with that intent.&quot;,&quot;Commitment classifier: A commitment classifier C : S (cid:55) [0 , 1] takes as input an email sentence S and returns a probability of whether the sentence is a commitment or not.&quot;,&quot;The classifier is built using labels from an annotation task with 3 judges.&quot;,&quot;The Cohen's kappa value is 0 .&quot;,&quot;694 , depicting substantial agreement.&quot;,&quot;The final label is obtained from the majority vote, generating a total of 9076 instances (with 2586 positive/commitment labels and 6490 negative labels).&quot;,&quot;The classifier is an RNN-based model with word embeddings and self-attention geared for binary classification with the input being the entire email context (Wang et al., 2019).&quot;,&quot;The classifier has a precision of 86% and recall of 84% on sentences in the Avocado corpus.&quot;,&quot;Candidate emails: We extracted 500 k raw sentences from Avocado emails and passed them&quot;,&quot;4 Avocado is a more appropriate test bed than the Enron collection (Klimt and Yang, 2004) since it contains additional meta-data and it entered the public domain via the cooperation and consent of the legal owner of the corpus.&quot;,&quot;through the commitment classifier.&quot;,&quot;We threshold the commitment classifier confidence to 0 .&quot;,&quot;9 and obtained 29 k potential candidates for To-Do items.&quot;,&quot;Of these, a random subset of 12 k instances were selected for annotation.&quot;,&quot;Annotation guideline: For each candidate email e c and the previous email in the thread e p (if present), we obtained meta-data like  From ',  Sent-To ',  Subject ' and  Body '.&quot;,&quot;The commitment sentence in e c was highlighted and annotators were asked to write a To-Do item using all of the information in e c and e p .&quot;,&quot;We prepared a comprehensive guideline to help human annotators write To-Do Items containing the definition and structure of To-Do Items and commitment sentences, along with illustrative examples.&quot;,&quot;Annotators were instructed to use words and phrases from the email context as closely as possible and introduce new vocabulary only when required.&quot;,&quot;Each instance was annotated by 2 judges.&quot;,&quot;Analysis of human annotations: We obtained a total of 9349 email instances with To-Do items, each of which was annotated by two annotators.&quot;,&quot;To-Do items have a median token length of 9 and a mean length of 9 .&quot;,&quot;71 .&quot;,&quot;For 60 .&quot;,&quot;42% of the candidate emails, both annotators agreed that the subject line was helpful in writing the To-Do Item.&quot;,&quot;To further analyze the annotation quality, we randomly sampled 100 annotated To-Do items and asked a judge to rate them on&quot;,&quot;(a) fluency (grammat-ical and spelling correctness), and&quot;,&quot;(b) completeness (capturing all the action items in the email) on a 4 point scale ( 1 : Poor, 2 : Fair, 3 : Good, 4 : Excellent).&quot;,&quot;Overall, we obtained a mean rating of 3.1 and 2.9 respectively for fluency and completeness.&quot;,&quot;Table 1 shows a snapshot of the analysis.&quot;,&quot;In this section, we describe our two-stage approach to generate To-Do items.&quot;,&quot;In the first stage, we select sentences that are helpful in writing the To-Do item.&quot;,&quot;Emails contain generic sentences such as salutations, thanks and casual conversations not relevant to the commitment task.&quot;,&quot;The objective of the first stage is to select sentences containing informative concepts necessary to write the To-Do.&quot;,&quot;In the absence of reliable labels to extract helpful sentences in a supervised fashion, we resort to an unsupervised matching-based approach.&quot;,&quot;Let the commitment sentence in the email be denoted as H , and the rest of the sentences from the current email e c and previous email e p be denoted as { s 1 , s 2 , . . . s d } .&quot;,&quot;The unsupervised approach seeks to obtain a relevance score ( s i ) for each sentence.&quot;,&quot;The top K sentences with the highest scores will be selected as the extractive summary for the commitment sentence (also referred to as the query).&quot;,&quot;Enriched query context: We first extract top  maximum frequency tokens from all the sentences in the given email, the commitment and the subject (i.e., { s 1 , s 2 , . . . s d }  H  Subject ).&quot;,&quot;Tokens are lemmatized and stop-words are removed.&quot;,&quot;We set  = 10 in our experiments.&quot;,&quot;An enriched context for the query E is formed by concatenating the commitment sentence H , subject and top  tokens.&quot;,&quot;Relevance score computation: Task-specific relevance score  for a sentence s i is obtained by inner product in the embedding space with the enriched context.&quot;,&quot;Let h (  ) be the function denoting the embedding of a sentence with ( s i ) = h ( s i ) T h ( E ) .&quot;,&quot;Our objective is to find helpful sentences for the commitment given by semantic similarity between concepts in the enriched context and a target sentence.&quot;,&quot;In case of a short or less informative query, the subject and topic of the email provide useful information via the enriched context.&quot;,&quot;We experiment with three different embedding functions.&quot;,&quot;frequency vector is used to represent the sentence.&quot;,&quot;(2) FastText Word Embeddings  We trained FastText embeddings (Bojanowski et al., 2017) of dimension 300 on all sentences in the Avocado corpus.&quot;,&quot;The embedding function h ( s j ) is given by taking the max (or mean) across the word-embedding dimension of all tokens in the sentence s j .&quot;,&quot;(3) Contextualized Word Embeddings  We utilize recent advances in contextualized representations from pre-trained language models like BERT (Devlin et al., 2019).&quot;,&quot;We use the second last layer of pre-trained BERT for sentence embeddings.&quot;,&quot;We also fine-tuned BERT on the labeled dataset for commitment classifier.&quot;,&quot;The dataset is first made balanced ( 2586 positive and 2586 negative instances).&quot;,&quot;Uncased BERT is trained for 5 epochs for commitment classification, with the input being word-piece tokenized email sentences.&quot;,&quot;This model is denoted as BERT (fine-tuned) in Table 2. Evaluation of unsupervised approaches: Retrieving at-least one helpful sentence is crucial to obtain contextual information for the To-Do item.&quot;,&quot;Therefore, we evaluate our approaches based on the proportion of emails where at-least one helpful sentence is present in the top K retrieved sentences.&quot;,&quot;We manually annotated 100 email instances and labeled every sentence as helpful or not based on&quot;,&quot;(a) whether the sentence contains concepts appearing in the target To-Do item, and&quot;,&quot;(b) whether the sentence helps to understand the task context.&quot;,&quot;Inter-annotator agreement between 2 judgments for this task has a Cohen Kappa score of 0 .&quot;,&quot;69 .&quot;,&quot;This annotation task also demonstrates the importance of the previous email in a thread.&quot;,&quot;Out of 100 annotated instances, 44 have a replied-to email of which 31 contains a helpful sentence in the replied-to email body ( 70 . 4% ).&quot;,&quot;Table 2 shows the performance of the various unsupervised extractive algorithms.&quot;,&quot;FastText with max-pooling of embeddings performed the best and used in the subsequent generation stage.&quot;,&quot;The generation phase of our approach can be formulated as sequence-to-sequence (Seq2Seq) learning with attention (Sutskever et al., 2014; Bahdanau et al., 2014).&quot;,&quot;It consists of two neural networks, an encoder and a decoder.&quot;,&quot;The input to the encoder consists of concatenated tokens from different meta-data fields of the email like sent-to', subject', commitment sentence H and extracted sentences I separated by special markers.&quot;,&quot;For instance, the input to the encoder for the example in Figure 1 is given as: < to > alice < sub > hello ?&quot;,&quot;generation model as follows: Vanilla Seq2Seq : Input tokens { x 1 , x 2 , . . . x T } are passed through a word-embedding layer and a single layer LSTM to obtain encoded representations h t = f ( x t , h t  1 )  t for the input.&quot;,&quot;The decoder is another LSTM that makes use of the encoder state h t and prior decoder state s t  1 to generate the target words at every timestep t .&quot;,&quot;We consider Seq2Seq with attention mechanism where the decoder LSTM uses attention distribution a t over timesteps t to focus on important hidden states to generate the context vector h t .&quot;,&quot;This is the first baseline in our work.&quot;,&quot;e t,t (cid:48) = v T tanh ( W h  h t + W s  s t (cid:48) + b ) a t,t (cid:48) = softmax ( e t,t (cid:48) ) h t = (cid:80) t (cid:48) a t,t (cid:48)  h t (cid:48) (1) Seq2Seq with copy mechanism : As the second model, we consider Seq2Seq with copy mechanism (See et al., 2017) to copy tokens from important email fields.&quot;,&quot;Copying is pivotal for To-Do item generation since every task involves named From: John Carter To: Helena Watson; Daniel Craig; Rupert Grint Subject: Thanks Thank you for helping me prepare the paper draft for ACL conference.&quot;,&quot;entities in terms of the persons involved, specific times and dates when the task has to be accomplished and other task-specific details present in the email context.&quot;,&quot;To understand the copy mechanism, consider the decoder input at each decoding step as y t and the context vector as h t .&quot;,&quot;The decoder at each timestep t has the choice of generating the output word from the vocabulary V with probability p gen =  ( h t , s t , y t ) , or with probability 1  p gen it can copy the word from the input context.&quot;,&quot;To allow that, the vocabulary is extended as V (cid:48) = V  { x 1 , x 2 , . . . x T } .&quot;,&quot;The model is trained end-to-end to maximize the log-likelihood of target words (To-Do items) given the email context.&quot;,&quot;Seq2Seq BiFocal : As a third model, we experimented with query-focused attention having two encoders  one containing only tokens of the query and the other containing rest of the input context.&quot;,&quot;We use a bifocal copy mechanism that can copy tokens from either of the encoders.&quot;,&quot;We refer the reader to the Appendix for more details about training and hyper-parameters used in our models.&quot;,&quot;9349 email instances with To-Do items, we used 7349 for training and 1000 each for validation and testing.&quot;,&quot;For each instance, we chose the annotation with fewer tokens as ground-truth reference.&quot;,&quot;The median token length of the encoder input is 43 (including the helpful sentence).&quot;,&quot;Table 4 shows the performance comparison of various models.&quot;,&quot;We report BLEU-4 (Papineni et al., 2002) and the F1-scores for Rouge-1, Rouge-2 and Rouge-L (Lin, 2004).&quot;,&quot;We also report the human performance for this task in terms of the above metrics computed between annotations from the two judges.&quot;,&quot;A trivial baseline  which concatenates tokens from the sent-to' and subject' fields and the commitment sentence  is included for comparison.&quot;,&quot;The best performance is obtained with Seq2Seq using copying mechanism.&quot;,&quot;We observe our model to perform at par with human performance for writing To-Do items.&quot;,&quot;Table 3 shows some examples of To-Do item generation from our best model.&quot;,&quot;In this work, we study the problem of automatic To-Do item generation from email context and meta-data to provide smart contextual assistance in email applications.&quot;,&quot;To this end, we introduce a new task and dataset for action-focused text intelligence.&quot;,&quot;We design a two stage framework with deep neural networks for task-focused text generation.&quot;,&quot;There are several directions for future work including better architecture design for utilizing structured meta-data and replacing the two-stage framework with a multi-task generation model that can jointly identify helpful context for the task and perform corresponding text generation.&quot;],&quot;string&quot;:&quot;[\n  \&quot;Intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks.\&quot;,\n  \&quot;In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails.\&quot;,\n  \&quot;We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action.\&quot;,\n  \&quot;We design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence learning, obtaining BLEU and ROUGE scores of 0 .\&quot;,\n  \&quot;23 and 0 .\&quot;,\n  \&quot;63 for this task.\&quot;,\n  \&quot;To the best of our knowledge, this is the first work to address the problem of composing To-Do items from emails.\&quot;,\n  \&quot;Email is one of the most used forms of communication especially in enterprise and work settings (Radicati and Levenstein, 2015).\&quot;,\n  \&quot;With the growing number of users in email platforms, service providers are constantly seeking to improve user experience for a myriad of applications such as online retail, instant messaging and event management (Feddern-Bekcan, 2008).\&quot;,\n  \&quot;Smart Reply (Kan-nan et al., 2016) and Smart Compose (Chen et al., 2019) are two recent features that provide contextual assistance to users aiming to reduce typing efforts.\&quot;,\n  \&quot;Another line of work in this direction is for automated task management and scheduling.\&quot;,\n  \&quot;For example.\&quot;,\n  \&quot;the recent Nudge feature 1 in Gmail and Insights in Outlook 2 are designed to remind users to follow-up on an email or pay attention to pending tasks.\&quot;,\n  \&quot;Smart To-Do takes a step further in task assistance and seeks to boost user productivity by automatically generating To-Do items from their email  Work done as an intern at Microsoft Research.\&quot;,\n  \&quot;1 Gmail Nudge 2 Outlook Insights From: Alice To: john@contoso.com Subject: Sales Report Hi John, From: John To: alice@contoso.com Subject: RE: Sales Report I am doing well.\&quot;,\n  \&quot;context.\&quot;,\n  \&quot;Text generation from emails, like creating To-Do items, is replete with complexities due to the diversity of conversations in email threads, heterogeneous structure of emails and various meta-deta involved.\&quot;,\n  \&quot;As opposed to prior works in text generation like news headlines, email subject lines and email conversation summarization, To-Do items are action-focused , requiring the identification of a specific task to be performed.\&quot;,\n  \&quot;In this work, we introduce the task of automatically generating To-Do items from email context and meta-data to assist users with following up on their promised actions (also referred to as commitments in this work).\&quot;,\n  \&quot;Refer to Figure 1 for an illustration.\&quot;,\n  \&quot;Given an email, its temporal context (i.e. thread), and associated meta-data like the name of the sender and recipient, we want to generate a short and succinct To-Do item for the task mentioned in the email.\&quot;,\n  \&quot;This requires identifying the task sentence (also referred to as a query ), relevant sentences in the email that provide contextual information about the query along with the entities (e.g., people) associated with the task.\&quot;,\n  \&quot;We utilize existing work to identify the task sentence via a commitment classifier that detects action intents in the emails.\&quot;,\n  \&quot;Thereafter C Commitment Classifier D Does the email contain commitment ?\&quot;,\n  \&quot;we use an unsupervised technique to extract key sentences in the email that are helpful in providing contextual information about the query.\&quot;,\n  \&quot;These pieces of information are further combined to generate the To-Do item using a sequence-to-sequence architecture with deep neural networks.\&quot;,\n  \&quot;Figure 2 shows a schematic diagram of the process.\&quot;,\n  \&quot;Since there is no existing work or dataset on this problem, our first step is to collect annotated data for this task.\&quot;,\n  \&quot;Overall, our contributions can be summarized as follows:  We create a new dataset for To-Do item generation from emails containing action items based on the publicly available email corpus Avocado (Oard et al., 2015).\&quot;,\n  \&quot;3  We develop a two-stage algorithm, based on unsupervised task-focused content selection and subsequent text generation combining contextual information and email meta-data.\&quot;,\n  \&quot;We conduct experiments on this new dataset and show that our model performs at par with human judgments on multiple performance metrics.\&quot;,\n  \&quot;Summarization of email threads has been the focus of multiple research works in the past (Rambow et al., 2004; Carenini et al., 2007; Dredze et al., 2008).\&quot;,\n  \&quot;There has also been considerable research on identifying speech acts or tasks in emails (Car-valho and Cohen, 2005; Lampert et al., 2010; Scerri et al., 2010) and how it can be robustly adapted across diverse email corpora (Azarbonyad et al., 2019).\&quot;,\n  \&quot;Recently, novel neural architectures have been explored for modeling action items in emails 3 We will release the code and data (in accordance with LDC and Avocado policy) at https://aka.ms/SmartToDo .\&quot;,\n  \&quot;Email examples in this paper are similar to those in our dataset but are not reproducing text from the Avocado dataset.\&quot;,\n  \&quot;(Lin et al., 2018) and identifying intents in email conversations (Wang et al., 2019).\&quot;,\n  \&quot;However, there has been less focus on task-specific email summarization (Corston-Oliver et al., 2004).\&quot;,\n  \&quot;The closest to our work is that of email subject line generation (Zhang and Tetreault, 2019).\&quot;,\n  \&quot;But it focuses on a common email theme and uses a supervised approach for sentence selection, whereas our method relies on identifying the task-related context.\&quot;,\n  \&quot;We build upon the Avocado dataset (Oard et al., 2015) 4 containing an anonymized version of the Outlook mailbox for 279 employees with various meta-data and 938 , 035 emails overall.\&quot;,\n  \&quot;Emails contain various user intents including planning and scheduling meetings, requests for information, exchange of information, casual conversations, etc. (Wang et al., 2019).\&quot;,\n  \&quot;For the purpose of this work, we first need to extract emails containing at least one sentence where the sender has promised to perform an action.\&quot;,\n  \&quot;It could be performing a task, providing some information, keeping others informed about a topic and so on.\&quot;,\n  \&quot;We use the term commitment to refer to such intent in an email and the term commitment sentence to refer to each sentence with that intent.\&quot;,\n  \&quot;Commitment classifier: A commitment classifier C : S (cid:55) [0 , 1] takes as input an email sentence S and returns a probability of whether the sentence is a commitment or not.\&quot;,\n  \&quot;The classifier is built using labels from an annotation task with 3 judges.\&quot;,\n  \&quot;The Cohen's kappa value is 0 .\&quot;,\n  \&quot;694 , depicting substantial agreement.\&quot;,\n  \&quot;The final label is obtained from the majority vote, generating a total of 9076 instances (with 2586 positive/commitment labels and 6490 negative labels).\&quot;,\n  \&quot;The classifier is an RNN-based model with word embeddings and self-attention geared for binary classification with the input being the entire email context (Wang et al., 2019).\&quot;,\n  \&quot;The classifier has a precision of 86% and recall of 84% on sentences in the Avocado corpus.\&quot;,\n  \&quot;Candidate emails: We extracted 500 k raw sentences from Avocado emails and passed them\&quot;,\n  \&quot;4 Avocado is a more appropriate test bed than the Enron collection (Klimt and Yang, 2004) since it contains additional meta-data and it entered the public domain via the cooperation and consent of the legal owner of the corpus.\&quot;,\n  \&quot;through the commitment classifier.\&quot;,\n  \&quot;We threshold the commitment classifier confidence to 0 .\&quot;,\n  \&quot;9 and obtained 29 k potential candidates for To-Do items.\&quot;,\n  \&quot;Of these, a random subset of 12 k instances were selected for annotation.\&quot;,\n  \&quot;Annotation guideline: For each candidate email e c and the previous email in the thread e p (if present), we obtained meta-data like  From ',  Sent-To ',  Subject ' and  Body '.\&quot;,\n  \&quot;The commitment sentence in e c was highlighted and annotators were asked to write a To-Do item using all of the information in e c and e p .\&quot;,\n  \&quot;We prepared a comprehensive guideline to help human annotators write To-Do Items containing the definition and structure of To-Do Items and commitment sentences, along with illustrative examples.\&quot;,\n  \&quot;Annotators were instructed to use words and phrases from the email context as closely as possible and introduce new vocabulary only when required.\&quot;,\n  \&quot;Each instance was annotated by 2 judges.\&quot;,\n  \&quot;Analysis of human annotations: We obtained a total of 9349 email instances with To-Do items, each of which was annotated by two annotators.\&quot;,\n  \&quot;To-Do items have a median token length of 9 and a mean length of 9 .\&quot;,\n  \&quot;71 .\&quot;,\n  \&quot;For 60 .\&quot;,\n  \&quot;42% of the candidate emails, both annotators agreed that the subject line was helpful in writing the To-Do Item.\&quot;,\n  \&quot;To further analyze the annotation quality, we randomly sampled 100 annotated To-Do items and asked a judge to rate them on\&quot;,\n  \&quot;(a) fluency (grammat-ical and spelling correctness), and\&quot;,\n  \&quot;(b) completeness (capturing all the action items in the email) on a 4 point scale ( 1 : Poor, 2 : Fair, 3 : Good, 4 : Excellent).\&quot;,\n  \&quot;Overall, we obtained a mean rating of 3.1 and 2.9 respectively for fluency and completeness.\&quot;,\n  \&quot;Table 1 shows a snapshot of the analysis.\&quot;,\n  \&quot;In this section, we describe our two-stage approach to generate To-Do items.\&quot;,\n  \&quot;In the first stage, we select sentences that are helpful in writing the To-Do item.\&quot;,\n  \&quot;Emails contain generic sentences such as salutations, thanks and casual conversations not relevant to the commitment task.\&quot;,\n  \&quot;The objective of the first stage is to select sentences containing informative concepts necessary to write the To-Do.\&quot;,\n  \&quot;In the absence of reliable labels to extract helpful sentences in a supervised fashion, we resort to an unsupervised matching-based approach.\&quot;,\n  \&quot;Let the commitment sentence in the email be denoted as H , and the rest of the sentences from the current email e c and previous email e p be denoted as { s 1 , s 2 , . . . s d } .\&quot;,\n  \&quot;The unsupervised approach seeks to obtain a relevance score ( s i ) for each sentence.\&quot;,\n  \&quot;The top K sentences with the highest scores will be selected as the extractive summary for the commitment sentence (also referred to as the query).\&quot;,\n  \&quot;Enriched query context: We first extract top  maximum frequency tokens from all the sentences in the given email, the commitment and the subject (i.e., { s 1 , s 2 , . . . s d }  H  Subject ).\&quot;,\n  \&quot;Tokens are lemmatized and stop-words are removed.\&quot;,\n  \&quot;We set  = 10 in our experiments.\&quot;,\n  \&quot;An enriched context for the query E is formed by concatenating the commitment sentence H , subject and top  tokens.\&quot;,\n  \&quot;Relevance score computation: Task-specific relevance score  for a sentence s i is obtained by inner product in the embedding space with the enriched context.\&quot;,\n  \&quot;Let h (  ) be the function denoting the embedding of a sentence with ( s i ) = h ( s i ) T h ( E ) .\&quot;,\n  \&quot;Our objective is to find helpful sentences for the commitment given by semantic similarity between concepts in the enriched context and a target sentence.\&quot;,\n  \&quot;In case of a short or less informative query, the subject and topic of the email provide useful information via the enriched context.\&quot;,\n  \&quot;We experiment with three different embedding functions.\&quot;,\n  \&quot;frequency vector is used to represent the sentence.\&quot;,\n  \&quot;(2) FastText Word Embeddings  We trained FastText embeddings (Bojanowski et al., 2017) of dimension 300 on all sentences in the Avocado corpus.\&quot;,\n  \&quot;The embedding function h ( s j ) is given by taking the max (or mean) across the word-embedding dimension of all tokens in the sentence s j .\&quot;,\n  \&quot;(3) Contextualized Word Embeddings  We utilize recent advances in contextualized representations from pre-trained language models like BERT (Devlin et al., 2019).\&quot;,\n  \&quot;We use the second last layer of pre-trained BERT for sentence embeddings.\&quot;,\n  \&quot;We also fine-tuned BERT on the labeled dataset for commitment classifier.\&quot;,\n  \&quot;The dataset is first made balanced ( 2586 positive and 2586 negative instances).\&quot;,\n  \&quot;Uncased BERT is trained for 5 epochs for commitment classification, with the input being word-piece tokenized email sentences.\&quot;,\n  \&quot;This model is denoted as BERT (fine-tuned) in Table 2. Evaluation of unsupervised approaches: Retrieving at-least one helpful sentence is crucial to obtain contextual information for the To-Do item.\&quot;,\n  \&quot;Therefore, we evaluate our approaches based on the proportion of emails where at-least one helpful sentence is present in the top K retrieved sentences.\&quot;,\n  \&quot;We manually annotated 100 email instances and labeled every sentence as helpful or not based on\&quot;,\n  \&quot;(a) whether the sentence contains concepts appearing in the target To-Do item, and\&quot;,\n  \&quot;(b) whether the sentence helps to understand the task context.\&quot;,\n  \&quot;Inter-annotator agreement between 2 judgments for this task has a Cohen Kappa score of 0 .\&quot;,\n  \&quot;69 .\&quot;,\n  \&quot;This annotation task also demonstrates the importance of the previous email in a thread.\&quot;,\n  \&quot;Out of 100 annotated instances, 44 have a replied-to email of which 31 contains a helpful sentence in the replied-to email body ( 70 . 4% ).\&quot;,\n  \&quot;Table 2 shows the performance of the various unsupervised extractive algorithms.\&quot;,\n  \&quot;FastText with max-pooling of embeddings performed the best and used in the subsequent generation stage.\&quot;,\n  \&quot;The generation phase of our approach can be formulated as sequence-to-sequence (Seq2Seq) learning with attention (Sutskever et al., 2014; Bahdanau et al., 2014).\&quot;,\n  \&quot;It consists of two neural networks, an encoder and a decoder.\&quot;,\n  \&quot;The input to the encoder consists of concatenated tokens from different meta-data fields of the email like sent-to', subject', commitment sentence H and extracted sentences I separated by special markers.\&quot;,\n  \&quot;For instance, the input to the encoder for the example in Figure 1 is given as: < to > alice < sub > hello ?\&quot;,\n  \&quot;generation model as follows: Vanilla Seq2Seq : Input tokens { x 1 , x 2 , . . . x T } are passed through a word-embedding layer and a single layer LSTM to obtain encoded representations h t = f ( x t , h t  1 )  t for the input.\&quot;,\n  \&quot;The decoder is another LSTM that makes use of the encoder state h t and prior decoder state s t  1 to generate the target words at every timestep t .\&quot;,\n  \&quot;We consider Seq2Seq with attention mechanism where the decoder LSTM uses attention distribution a t over timesteps t to focus on important hidden states to generate the context vector h t .\&quot;,\n  \&quot;This is the first baseline in our work.\&quot;,\n  \&quot;e t,t (cid:48) = v T tanh ( W h  h t + W s  s t (cid:48) + b ) a t,t (cid:48) = softmax ( e t,t (cid:48) ) h t = (cid:80) t (cid:48) a t,t (cid:48)  h t (cid:48) (1) Seq2Seq with copy mechanism : As the second model, we consider Seq2Seq with copy mechanism (See et al., 2017) to copy tokens from important email fields.\&quot;,\n  \&quot;Copying is pivotal for To-Do item generation since every task involves named From: John Carter To: Helena Watson; Daniel Craig; Rupert Grint Subject: Thanks Thank you for helping me prepare the paper draft for ACL conference.\&quot;,\n  \&quot;entities in terms of the persons involved, specific times and dates when the task has to be accomplished and other task-specific details present in the email context.\&quot;,\n  \&quot;To understand the copy mechanism, consider the decoder input at each decoding step as y t and the context vector as h t .\&quot;,\n  \&quot;The decoder at each timestep t has the choice of generating the output word from the vocabulary V with probability p gen =  ( h t , s t , y t ) , or with probability 1  p gen it can copy the word from the input context.\&quot;,\n  \&quot;To allow that, the vocabulary is extended as V (cid:48) = V  { x 1 , x 2 , . . . x T } .\&quot;,\n  \&quot;The model is trained end-to-end to maximize the log-likelihood of target words (To-Do items) given the email context.\&quot;,\n  \&quot;Seq2Seq BiFocal : As a third model, we experimented with query-focused attention having two encoders  one containing only tokens of the query and the other containing rest of the input context.\&quot;,\n  \&quot;We use a bifocal copy mechanism that can copy tokens from either of the encoders.\&quot;,\n  \&quot;We refer the reader to the Appendix for more details about training and hyper-parameters used in our models.\&quot;,\n  \&quot;9349 email instances with To-Do items, we used 7349 for training and 1000 each for validation and testing.\&quot;,\n  \&quot;For each instance, we chose the annotation with fewer tokens as ground-truth reference.\&quot;,\n  \&quot;The median token length of the encoder input is 43 (including the helpful sentence).\&quot;,\n  \&quot;Table 4 shows the performance comparison of various models.\&quot;,\n  \&quot;We report BLEU-4 (Papineni et al., 2002) and the F1-scores for Rouge-1, Rouge-2 and Rouge-L (Lin, 2004).\&quot;,\n  \&quot;We also report the human performance for this task in terms of the above metrics computed between annotations from the two judges.\&quot;,\n  \&quot;A trivial baseline  which concatenates tokens from the sent-to' and subject' fields and the commitment sentence  is included for comparison.\&quot;,\n  \&quot;The best performance is obtained with Seq2Seq using copying mechanism.\&quot;,\n  \&quot;We observe our model to perform at par with human performance for writing To-Do items.\&quot;,\n  \&quot;Table 3 shows some examples of To-Do item generation from our best model.\&quot;,\n  \&quot;In this work, we study the problem of automatic To-Do item generation from email context and meta-data to provide smart contextual assistance in email applications.\&quot;,\n  \&quot;To this end, we introduce a new task and dataset for action-focused text intelligence.\&quot;,\n  \&quot;We design a two stage framework with deep neural networks for task-focused text generation.\&quot;,\n  \&quot;There are several directions for future work including better architecture design for utilizing structured meta-data and replacing the two-stage framework with a multi-task generation model that can jointly identify helpful context for the task and perform corresponding text generation.\&quot;\n]&quot;},&quot;labels&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;abstain&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;result&quot;,&quot;method&quot;,&quot;objective&quot;,&quot;method&quot;,&quot;abstain&quot;],&quot;string&quot;:&quot;[\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;result\&quot;,\n  \&quot;method\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;\n]&quot;}}},{&quot;rowIdx&quot;:4,&quot;cells&quot;:{&quot;sentences&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;Semantic parsing using sequence-to-sequence models allows parsing of deeper representations compared to traditional word tagging based models.&quot;,&quot;In spite of these advantages, widespread adoption of these models for real-time conversational use cases has been stymied by higher compute requirements and thus higher latency.&quot;,&quot;In this work, we propose a non-autoregressive approach to predict semantic parse trees with an efficient seq2seq model architecture.&quot;,&quot;By combining non-autoregressive prediction with convolutional neural networks, we achieve significant latency gains and parameter size reduction compared to traditional RNN models.&quot;,&quot;Our novel architecture achieves up to an 81% reduction in latency on TOP dataset and retains competitive performance to non-pretrained models on three different semantic parsing datasets.&quot;,&quot;Our code is available at https://github.&quot;,&quot;com/facebookresearch/pytext .&quot;,&quot;Advances in conversational assistants have helped to improve the usability of smart speakers and consumer wearables for different tasks.&quot;,&quot;Semantic parsing is one of the fundamental components of these assistants and it helps to convert the user input in natural language to a structure representation that can be understood by downstream systems.&quot;,&quot;Majority of the semantic parsing systems deployed on various devices, rely on server-side inference because of the lower compute/memory available on these edge devices.&quot;,&quot;This poses a few drawbacks such as flaky user experience with spotty internet connectivity and compromised user data privacy due to the dependence on a centralized server to which all user interactions are sent to.&quot;,&quot;Thus, semantic parsing on-device has numerous advantages.&quot;,&quot;For the semantic parsing task, the meaning representation used decides the capabilities of the system built.&quot;,&quot;Limitations of the representation with one intent and slot labels were studied in the context of nested queries and multi turn utterances in Aghajanyan et al. (2020) and Gupta et al. (2018).&quot;,&quot;New representations were proposed to overcome these limitations and sequence-to-sequence models were proposed as the solution to model these complex forms.&quot;,&quot;But using these new models in real-time conversational assistants still remains a challenge due to higher latency requirements.&quot;,&quot;In our work, we propose a novel architecture and generation scheme to significantly improve the end2end latency of sequence-to-sequence models for the semantic parsing task.&quot;,&quot;Due to the autoregressive nature of generation in sequence-to-sequence semantic parsing models, the recurrence relationship between target tokens creates a limitation that decoding cannot be parallelized.&quot;,&quot;There are multiple works in machine translation which try to solve this problem.&quot;,&quot;These approaches relax the decoder token-by-token generation by allowing multiple target tokens to be generated at once.&quot;,&quot;Fully non-autoregressive models (Gu et al., 2017; Ma et al., 2019; Ghazvininejad et al., 2020a; Saharia et al., 2020) and conditional masked language models with iterative decoding (Ghazvinine-jad et al., 2019; Gu et al., 2019; Ghazvininejad et al., 2020b) are some of them.&quot;,&quot;To enable non-autoregressive generation in semantic parsing, we modify the objective of the standard seq2seq model to predict the entire target structure at once.&quot;,&quot;We build upon the CMLM (Con-ditional Masked Language Model) (Ghazvininejad et al., 2019) and condition the generation of the full target structure on the encoder representation.&quot;,&quot;By eliminating the recurrent relationship between individual target tokens, the decoding process can be parallelized.&quot;,&quot;While this drastically improves latency, the representation of each token is still dependent on previous tokens if we continue to use an RNN architecture.&quot;,&quot;Thus, we propose a novel model architecture for semantic parsing based on convolutional networks (Wu et al., 2019b) to solve this issue.&quot;,&quot;Our non-autoregressive model achieves up to an 81% reduction in latency on the TOP dataset (Gupta et al., 2018), while achieving 80.23% exact match accuracy.&quot;,&quot;We also achieve 88.16% exact match accuracy on DSTC2 (Henderson et al., 2014) and 80.86% on SNIPS (Coucke et al., 2018) which is competitive to prior work without pretraining.&quot;,&quot;To summarize, our two main contributions are:  We propose a novel alternative to the traditional autoregressive generation scheme for semantic parsing using sequence-to-sequence models.&quot;,&quot;With a new model training strategy and generation approach, the semantic parse structure is predicted in one step improving parallelization and thus leading to significant reduction in model latency with minimal accuracy impact.&quot;,&quot;We also study the limitations of original CMLM (Ghazvininejad et al., 2019) when applied for conversational semantic parsing task and provide motivations for our simple yet critical modifications.&quot;,&quot;We propose LightConv Pointer, a model architecture for non-autoregressive semantic parsing, using convolutional neural networks which provides significant latency and model size improvements over RNN models.&quot;,&quot;Our novel model architecture is particularly suitable for limited compute use-cases like on-device conversational assistants.&quot;,&quot;In this section, we propose a novel, convolutional, non-autoregressive architecture for semantic parsing.&quot;,&quot;While non-autoregressive decoding has been previously explored in machine translation, we describe how it can be applied to semantic parsing with several critical modifications to retain performance.&quot;,&quot;We then describe our convolutional architecture.&quot;,&quot;By incorporating these advances together, our approach achieves both high accuracy and efficient decoding.&quot;,&quot;The task is to predict the semantic parse tree given the raw text.&quot;,&quot;We use the decoupled representation (Aghajanyan et al., 2020), an extension of the compositional form proposed in Gupta et al. (2018) for task oriented semantic parsing.&quot;,&quot;Decoupled representation is obtained by removing all text in the compositional form that does not appear in a leaf slot.&quot;,&quot;Efficient models require representations which are compact, with least number of tokens, to reduce number of floating point operations during inference.&quot;,&quot;Decoupled representation was found to be suitable due to this.&quot;,&quot;Figure 1 shows the semantic parse for a sample utterance.&quot;,&quot;Our model predicts the serialized representation of this tree which is [IN:CREATE_REMINDER [SL:PERSON_REMINDED me ] [SL:TODO [IN:CREATE_CALL [SL:METHOD call ] [SL:CONTACT John ] ] ] ] 2.1 Non-Autoregressive Decoding While autoregressive models (Figure 2), which predict a sequence token by token, have achieved strong results in various tasks including semantic parsing, they have a large downside.&quot;,&quot;The main challenge in practical applications is the slow decoding time.&quot;,&quot;We investigate how to incorporate recent advances in non-autoregressive decoding for efficient semantic parsing models.&quot;,&quot;We build upon the Conditional Masked Language Model (CMLM) proposed in Ghazvininejad et al. (2019) by applying it to the structured prediction task of semantic parsing for task-oriented dialog.&quot;,&quot;Ghazvininejad et al. (2019) uses CMLM to first predict a token-level representation for each source token and a target sequence length; then the model predicts and iterates on the target sequence prediction in a non-autoregressive fashion.&quot;,&quot;We describe our changes and the motivations for these changes below.&quot;,&quot;One of the main differences between our work and Ghazvininejad et al. (2019) is that target length prediction plays a more important role in semantic parsing.&quot;,&quot;For the translation task, if the target length is off by one or more, the model can slightly rephrase the sentence to still return a high quality translation.&quot;,&quot;In our case, if the length prediction is Figure 2: Traditional Sequence to Sequence architecture which uses autoregressive generation scheme for decoder.&quot;,&quot;To resolve this important challenge, we propose a specialized length prediction module that more accurately predicts the target sequence length.&quot;,&quot;While Ghazvininejad et al. (2019) uses a special CLS token in the source sequence to predict the target length, we have a separate module of multiple layers of CNNs with gated linear units to predict the target sequence length (Wu et al., 2019b).&quot;,&quot;We also use label smoothing and differently weighing losses as explained in section 2.3, to avoid the easy over-fitting in semantic parsing compared to translation.&quot;,&quot;As shown in Aghajanyan et al. (2020), transformers without pre-training perform poorly on TOP dataset.&quot;,&quot;The architectural changes that we propose to solve the data efficiency can be found in the section 2.2.1.&quot;,&quot;Further, we find that the random masking strategy proposed in Ghazvininejad et al. (2019) works poorly for semantic parsing.&quot;,&quot;When we use the same strategy for the semantic parsing task where the output has a structure, model is highly likely to see invalid trees during training as masking random tokens in the linearized representation of a tree mostly gives invalid tree representations.&quot;,&quot;This makes it hard for the model to learn the structure especially when the structure is complicated (in the case of trees, deep trees were harder to learn).&quot;,&quot;To remedy this problem, we propose a different strategy for model training where all the tokens in the target sequence are masked during training.&quot;,&quot;Our model architecture (Figure 3) is based on the classical seq2seq model (Sutskever et al., 2014) and follows the encoder-decoder architecture.&quot;,&quot;In order to optimize for efficient encoding and decoding, we look to leverage a fully parallel model architecture.&quot;,&quot;While transformer models are fully parallel and popular in machine translation (Vaswani et al., 2017), they are known to perform poorly in low resource settings and require careful tuning using techniques like Neural Architecture Search to get good performance (van Biljon et al., 2020; Murray et al., 2019).&quot;,&quot;Similarly, randomly initialized transformers performed poorly on TOP dataset achieving only 64.5 % accuracy when SOTA was above 80% (Aghajanyan et al., 2020).&quot;,&quot;We overcome this limitation by augmenting Transformers with Convolutional Neural Networks.&quot;,&quot;Details of our architecture are explained below.&quot;,&quot;For token representations, we use word embeddings concatenated with the sinusoidal positional embeddings (Vaswani et al., 2017).&quot;,&quot;Encoder and decoder consist of multiple layers with residual connections as shown in Figure 4.&quot;,&quot;First sub-block in each layer consists of MHA (Vaswani et al., 2017).&quot;,&quot;In decoder, we do not do masking of future tokens during model training.&quot;,&quot;This is needed for non-autoregressive generation of target tokens during inference.&quot;,&quot;Second sub-block consists of multiple convolutional layers.&quot;,&quot;We use depthwise convolutions with weight sharing (Wu et al., 2019b).&quot;,&quot;Convolution layer helps in learning representation for tokens for a fixed context size and multiple layers helps with bigger receptive fields.&quot;,&quot;We use non-causal convolutions for both encoder as well as decoder.&quot;,&quot;Third sub-block is the FFN (Vaswani et al., 2017; Wu et al., 2019b) which consists of two linear layers and relu.&quot;,&quot;The decoder has source-target attention after the convolution layer.&quot;,&quot;Pointer-Generator Projection layer The decoder has a final projection layer which generates the target tokens from the decoder/encoder representations.&quot;,&quot;Rongali et al. (2020) proposes an idea based Pointer Generator Network (See et al., 2017) to convert the decoder representation to target tokens using the encoder output.&quot;,&quot;Similarly, we use a pointer based projection head, which decides whether to copy tokens from the source-sequence or generate from the pre-defined ontology at every Figure 3: Sequence to Sequence model architecture which uses Non-Autoregressive strategy for generation decoding step (Aghajanyan et al., 2020).&quot;,&quot;Length Prediction Module Length prediction Module receives token level representations from the encoder as input.&quot;,&quot;It uses stacked CNNs with gated linear units and mean pooling to generation the length prediction.&quot;,&quot;Suppose the source sequence is of length L and source tokens in the raw text are s 1 , s 2 , s 3 . . . s L .&quot;,&quot;Encoder generates a representation of for each token in the source sequence.&quot;,&quot;Using the predicted length T, we create a target sequence of length T consisting of identical MASK tokens.&quot;,&quot;This sequence is passed through possibly multiple decoder layers and generates a representation for each token in the masked target sequence.&quot;,&quot;We make a strong assumption that each token in the target sentence is conditionally independent of each other given the source and the target length.&quot;,&quot;Thus, the individual probabilities for each token is P ( y i | X, T ) where X is the input sequence and T is the length of target sequence.&quot;,&quot;Beam Search During inference, length prediction module explained in 2.2.1 predicts top k lengths.&quot;,&quot;For each predicted length, we create a decoder input sequence of all masked tokens.&quot;,&quot;This is similar to the beam search with beam size k in autoregressive systems.&quot;,&quot;The main difference in our model architecture is that we expect only one candidate for each predicted length.&quot;,&quot;These all masked sequences are given as input to the model and the model predicts target tokens for each masked token.&quot;,&quot;Once we have predicted target sequences for k different lengths, they are ranked based on the ranking algorithm described in (5), where X is the input sequence and Y is the predicted output sequence, note the predicted token y i is conditioned on both the sequence ( X ) and the predicted target length T .&quot;,&quot;During training, we jointly optimize for two weighted losses.&quot;,&quot;The first loss is calculated for the predicted target tokens against the real target and the second loss is calculated for predicted target length against real target length.&quot;,&quot;During forward-pass, we replace all the tokens in the target sequence with a special <MASK> token and give this as an input to the decoder.&quot;,&quot;Decoder predicts the token for each masked token and the cross-entropy loss is calculated for each predicted token.&quot;,&quot;The length prediction module in the model predicts the target length using the encoder representation.&quot;,&quot;Similar to CMLMs in (Ghazvininejad et al., 2019), length prediction is modeled as a classifica-tion task with class labels for each possible length.&quot;,&quot;Cross entropy loss is calculated for length prediction.&quot;,&quot;For our semantic parsing task, label smoothing (Szegedy et al., 2015) was found to be very critical as the length prediction module tends to easily overfit and strong regularization methods are needed.&quot;,&quot;This was because length prediction was a much well-defined task compared to predicting all the tokens in the sequence.&quot;,&quot;Total loss was calculated by taking a weighted sum of cross entropy loss for labels and length, with lower weight for length loss.&quot;,&quot;As training progresses through different epochs, the best model is picked by comparing the exact match (EM) accuracy of different snapshots on validation set.&quot;,&quot;We use 3 datasets across various domains to evaluate our semantic parsing approach.&quot;,&quot;Length distribution of each dataset is described using median, 90th percentile and 99th percentile lengths.&quot;,&quot;TOP Dataset Task Oriented Parsing (Gupta et al., 2018) is a dataset for compositional utterances in the navigation and events domains.&quot;,&quot;The training set consists of 31 , 279 instances and the test set consists of 9 , 042 .&quot;,&quot;The test set has a median target length of 15, P90 27 and P99 39.&quot;,&quot;SNIPS The SNIPS (Coucke et al., 2018) dataset is a public dataset used for benchmarking semantic parsing intent slot models.&quot;,&quot;This dataset is considered flat, since it does not contain compositional queries and can be solved with word-tagging models.&quot;,&quot;Recently, however seq2seq models have started to out perform word-tagging models (Rongali et al., 2020; Aghajanyan et al., 2020).&quot;,&quot;The training set consists of 13 , 084 instances, the test set consists of 700 instances.&quot;,&quot;The test set has a median target length of 11, P90 17, P99 21.&quot;,&quot;DSTC2 Dialogue State Tracking Challenge 2 (Henderson et al., 2014), is a dataset for conversational understanding.&quot;,&quot;The dataset involves users searching for restaurants, by specifying constraints such as cuisine type and price range, we encode these constraints as slots and use this to formulate the decoupled representation.&quot;,&quot;The training set consists of 12 , 611 instances and a test set of 9890 .&quot;,&quot;The test set has a median target length of 6, P90 9 and P99 10.&quot;,&quot;Semantic Parsing Performance For all our datasets, we convert the representation of either the compositional form or flat intent slot form to the decoupled representation (Aghajanyan et al., 2020) .&quot;,&quot;We compare the model prediction with the serialized structure representation and look for exact match (EM).&quot;,&quot;Benchmarking Latency For the latency analysis for the models trained from scratch: AR LightConv Pointer, NAR LightConv Pointer, and BiLSTM.&quot;,&quot;We chose these 3 architectures, to compare NAR vs AR variants of LightConv Pointer, as well as the best performant baseline: Pointer BiLSTM (Aghajanyan et al., 2020).&quot;,&quot;We use Samsung Galaxy S8 with Android OS and Octa-core processor.&quot;,&quot;We chose to benchmark latency to be consistent with prior work on on-device modeling (Wu et al., 2019a; Howard et al., 2019).&quot;,&quot;All models are trained in PyTorch (Paszke et al., 2019) and exported using Torchscript.&quot;,&quot;We measure wall clock time as it is preferred instead of other options because it relates more to real world inference.&quot;,&quot;1 Latency results can be found in section 4.2.&quot;,&quot;For each of our datasets, we report accuracy metrics on the following models:&quot;,&quot;NAR LightConv Pointer : A non-autoregressive (NAR) variant of the above model to allow for parallel decoding.&quot;,&quot;We compare against the best reported numbers across datasets where the models don't use pretraining.&quot;,&quot;During training of our model we use the same base model across all datasets and sweep over hyper parameters for the length module and the batch size and learning rate, an equivalent sweep was done for the AR variant as well.&quot;,&quot;The base model we use for NAR LightConv Pointer model uses 5 encoder layers with convolutional kernel sizes [3,7,15,21,27], where each encoder layer has embedding and convolutional dimensions of 160, 1 self attenion head, and 2 decoder layers with kernel sizes [7,27], and embedding dimension of 160, 1 self-attention head and 2 encoder-attention heads.&quot;,&quot;Our length prediction module leverages a two convolution layers of 512 embedding dimensions and kernel sizes of 3 and 9.&quot;,&quot;and uses hidden dimension in [128,256,512] determined by hyper parameter sweeps.&quot;,&quot;We also use 8 attention heads for the decoupled projection head.&quot;,&quot;For the convolutional layer, we use lightweight convolutions (Wu et al., 2019b) with number of heads set to 2.&quot;,&quot;We train with the Adam (Kingma and Ba, 2014) optimizer, learning rate is selected to be between [0.00007, 0.0004].&quot;,&quot;If our evaluation accuracy has not increased in 10 epochs, we also reduce our learning rate by a factor of 10, and we employ early stopping if the accuracy has not changed in 20 epochs.&quot;,&quot;We train with our batch size fixed to be 8.&quot;,&quot;We optimize a joint loss for label prediction and length prediction.&quot;,&quot;Both losses consist of label smoothed cross entropy loss (  is the weight of the uniform distribution) (Pereyra et al., 2017), our label loss has  = 0 .&quot;,&quot;1 and our length loss has  = 0 .&quot;,&quot;5 , we also weight our length loss lower,  = 0 .&quot;,&quot;25 .&quot;,&quot;For inference, we use a length beam size of k = 5 .&quot;,&quot;Our AR variant follows the same parameters however it does not have length prediction and self-attention in encoder and decoder.&quot;,&quot;We show that our proposed non-autoregressive convolutional architecture for semantic parsing is competitive with auto-regressive baselines and word tagging baselines without pre-training on three different benchmarks and reduces latency up to 81% on the TOP dataset.&quot;,&quot;We first compare accuracy and latency, then discuss model performance by analyzing errors by length, and the importance of knowledge distillation.&quot;,&quot;We do our analysis on the TOP dataset, due to its inherent compositional nature, however we expect our analysis to hold for other datasets as well.&quot;,&quot;Non-compositional datasets like DSTC2 and SNIPS can be modeled by word tagging models making seq2seq models more relevant in the case of compositional datasets.&quot;,&quot;In table 5a we show our NAR and AR variants for LightConv Pointer perform quite similarly across all datasets.&quot;,&quot;We can see that our proposed NAR LightConv Pointer is also competitive with state of the art models without pre-training: -0.66% TOP, -0.17% DSTC2, -4.57% SNIPS (-0.04% compared to word tagging models).&quot;,&quot;Following the prior work on Non-Autoregressive models, we also report our experiments with sequence-level knowledge distillation in subsection Knowledge Distillation under section.&quot;,&quot;4.3.&quot;,&quot;In figure 5b we show the latency of our model with different generation approaches (NAR vs AR) over increasing target sequence lengths on the TOP dataset.&quot;,&quot;Firstly, we show that our LightConv Pointer is significantly faster than the BiLSTM baseline (Aghajanyan et al., 2020), achieving up to a 54% reduction in median latency.&quot;,&quot;BiLSTM was used as baseline as that was the SOTA without pretraining for TOP and Transformers performed poorly.&quot;,&quot;By comparing our model with AR and NAR generation strategy, it can be seen that increase in latency with increase in target length is much smaller for NAR due to better parallelization of decoder, resulting in up to an 81% reduction in Length Bucket NAR (%) AR (%) Bucket Size < 10 82.80 83.13 2798 10-20 84.18 84.36 5167 20-30 62.50 65.72 992 30-40 21.25 41.25 80 > 40 0.00 20.00 5 Table 2: EM accuracy of the NAR LightConv Pointer (distilled) vs AR LightConv Pointer distilled across different target length buckets along with the number of instances in each bucket on the TOP dataset.&quot;,&quot;median latency compared to the BiLSTM model.&quot;,&quot;Also note that both the LightConv Pointer models are able to achieve parity in terms of EM Accuracy compared to the baseline BiLSTM model, while using many fewer parameters, the BiLSTM model uses 20M parameters, while the NAR LightConv Pointer uses 12M and the AR LightConv Pointer uses 10M.&quot;,&quot;Ablation experiments We compare the modifications proposed by this work (LightConv, Conv length prediction module and Mask everything strategy) with the original model proposed in Ghazvininejad et al. (2019) in table 1.&quot;,&quot;The motivations for each modification was already discussed in sub-section 2.1.&quot;,&quot;Our mean EM accuracy results based on 3 trials show the significance of techniques proposed in this paper especially for longer target sequences.&quot;,&quot;Errors by length It is known that non-autoregressive models have difficulty at larger sequence lengths (Ghazvininejad et al., 2019).&quot;,&quot;In table 2, we show our model's accuracy in each respective length bucket on the TOP dataset.&quot;,&quot;We see that the AR and NAR model follow a similar distribution of errors, however the NAR model seems to error at a higher rate for the longer lengths.&quot;,&quot;Knowledge Distillation Following prior work (Ghazvininejad et al., 2019; Zhou et al., 2020), we train our model with sequence-level knowledge distillation (Kim and Rush, 2016).&quot;,&quot;We train our system on data generated by the current SOTA autoregressive models BART (Lewis et al., 2019; Aghajanyan et al., 2020).&quot;,&quot;In table 3 we show the impact of knowledge distillation in our task on both the non-autoregressive and autoregressive variants of LightConv Pointer.&quot;,&quot;These results support prior work in machine translation for distillation of au-Figure 6: Distilled NAR LightConv Pointer Top-K accuracy for exact match (EM) accuracy (blue) and Top-K length accuracy (orange), as well as the EM accuracy with gold length (dotted red line) for the TOP dataset.&quot;,&quot;toregressive teachers to non-autoregressive models showing distillation improving our models on TOP and SNIPS, however we notice minimal changes on DSTC2.&quot;,&quot;The importance of length prediction An important part of our non-autoregressive model is length prediction.&quot;,&quot;In figure 6, we report exact match accuracy @ top k beams and length accuracy @ top k beams (where top K refers to whether the correct answer was in the top K predictions) for the TOP dataset.&quot;,&quot;We can see a tight correlation between our length accuracy and exact match accuracy, showing how our model is bottle necked by the length prediction.&quot;,&quot;Providing gold length as a feature, led to an exact match accuracy of 88.20% (shown in red on figure 6), an absolute 7.31 point improvement over our best result with our non-autoregressive LightConv Pointer.&quot;,&quot;Non-autoregressive Decoding Recent work in machine translation has made a lot of progress in fully non-autoregressive models (Gu et al., 2017; Ma et al., 2019; Ghazvininejad et al., 2020a; Saharia et al., 2020) and parallel decoding (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019; Ghazvininejad et al., 2020b; Kasai et al., 2020).&quot;,&quot;While many advancements have been made in machine translation, we believe we are the first to explore the non-autoregressive semantic parsing setting.&quot;,&quot;In our work, we extend the CMLM to work for semantic parsing.&quot;,&quot;We make two important adjustments: first, we use a different masking approach where we mask everything and do one-step generation.&quot;,&quot;Second, we note the importance of the length prediction task for parsing and improve the length prediction module in the CMLM.&quot;,&quot;Seq2Seq For Semantic Parsing Recent advances in language understanding have lead to increased reliance on seq2seq architectures.&quot;,&quot;Recent work by Rongali et al. 2020; Aghajanyan et al. 2020, showed the advantages from using a pointer generator architecture for resolving complex queries (e.g. composition and cross domain queries) that could not be handled by word tagging models.&quot;,&quot;Since we target the same task, we adapt their pointer decoder into our proposed architecture.&quot;,&quot;However, to optimize for latency and compression we train CNN based architectures (Desai et al. 2020 and Wu et al. 2019b) to leverage the inherent model parallelism compared to the BiLSTM model proposed in Aghajanyan et al. 2020 and more compression compared to the transformer seq2seq baseline proposed in Rongali et al. 2020.&quot;,&quot;To further improve latency we look at parallel decoding through non-autoregressive decoding compared to prior work leveraging autoregressive models.&quot;,&quot;This work introduces a novel alternative to autoregressive decoding and efficient encoder-decoder architecture for semantic parsing.&quot;,&quot;We show that in 3 semantic parsing datasets, we are able to speed up decoding significantly while minimizing accuracy regression.&quot;,&quot;Our model is able to generate parse trees competitive with state of the art autoregressive models with significant latency savings, allowing complex NLU systems to be delivered on edge devices.&quot;,&quot;There are a couple of limitations of our proposed model that naturally extend themselves to future work.&quot;,&quot;Primarily, we cannot support true beam decoding, we decode a single prediction for each length prediction however there may exist multiple beams for each length prediction.&quot;,&quot;Also for longer parse trees and more complex semantic parsing systems such as session based understanding, our NAR decoding scheme could benefit from multiple iterations.&quot;,&quot;Lastly, though we explored models without pre-training in this work, recent developments show the power of leveraging pre-trained models such as RoBERTa and BART.&quot;,&quot;We leave it to future work to extend our non-autoregressive decoding for pre-trained models.&quot;,&quot;We would like to thank Sandeep Subramanian (MILA), Karthik Prasad (Facebook AI), Arash Einolghozati (Facebook) and Yinhan Liu for the&quot;,&quot;helpful discussions.&quot;,&quot;References Armen Aghajanyan, Jean Maillard, Akshat Shrivastava, Keith Diedrick, Michael Haeger, Haoran Li, Yashar Mehdad, Veselin Stoyanov, Anuj Kumar, Mike Lewis, and Sonal Gupta.&quot;,&quot;2020.&quot;,&quot;Conversational semantic parsing.&quot;,&quot;In EMNLP/IJCNLP .&quot;,&quot;Alice Coucke, Alaa Saade, Adrien Ball, Thodore Bluche, Alexandre Caulier, David Leroy, Clment Doumouro, Thibault Gisselbrecht, Francesco Calta-girone, Thibaut Lavril, et al. 2018.&quot;,&quot;Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces.&quot;,&quot;arXiv preprint arXiv:1805.10190 .&quot;,&quot;Shrey Desai, Geoffrey Goh, Arun Babu, and Ahmed Aly.&quot;,&quot;2020.&quot;,&quot;Lightweight convolutional representations for on-device natural language processing.&quot;,&quot;arXiv preprint arXiv:2002.01535 .&quot;,&quot;Arash Einolghozati, Panupong Pasupat, Sonal Gupta, Rushin Shah, Mrinal Mohit, Mike Lewis, and Luke Zettlemoyer.&quot;,&quot;2018.&quot;,&quot;Improving semantic parsing for task oriented dialog.&quot;,&quot;In Conversational AI Workshop at NeurIPS 2018 .&quot;,&quot;Marjan Ghazvininejad, Vladimir Karpukhin, Luke Zettlemoyer, and Omer Levy.&quot;,&quot;2020a.&quot;,&quot;Aligned cross entropy for non-autoregressive machine translation.&quot;,&quot;arXiv preprint arXiv:2004.01655 .&quot;,&quot;Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer.&quot;,&quot;2019.&quot;,&quot;Mask-predict: Parallel decoding of conditional masked language models.&quot;,&quot;Marjan Ghazvininejad, Omer Levy, and Luke Zettle-moyer.&quot;,&quot;2020b.&quot;,&quot;Semi-autoregressive training improves mask-predict decoding.&quot;,&quot;arXiv preprint arXiv:2001.08785 .&quot;,&quot;Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Huo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-Nung Chen.&quot;,&quot;2018.&quot;,&quot;Slot-gated modeling for joint slot filling and intent prediction.&quot;,&quot;In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pages 753757.&quot;,&quot;Victor OK Li, and Richard Socher.&quot;,&quot;2017.&quot;,&quot;Non-autoregressive neural machine translation.&quot;,&quot;arXiv preprint arXiv:1711.02281 .&quot;,&quot;Jiatao Gu, Changhan Wang, and Junbo Zhao.&quot;,&quot;Levenshtein transformer.&quot;,&quot;In Advances in Neural Information Processing Systems , pages 1117911189.&quot;,&quot;Jungo Kasai, James Cross, Marjan Ghazvininejad, and Jiatao Gu.&quot;,&quot;2020.&quot;,&quot;Parallel machine translation with disentangled context transformer.&quot;,&quot;arXiv preprint arXiv:2001.05136 .&quot;,&quot;Yoon Kim and Alexander M Rush.&quot;,&quot;2016.&quot;,&quot;Sequence-level knowledge distillation.&quot;,&quot;arXiv preprint arXiv:1606.07947 .&quot;,&quot;Jason D. Lee, Elman Mansimov, and Kyunghyun Cho.&quot;,&quot;2018.&quot;,&quot;Deterministic non-autoregressive neural sequence modeling by iterative refinement.&quot;,&quot;In Proc.&quot;,&quot;of EMNLP .&quot;,&quot;Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-big, and Eduard Hovy.&quot;,&quot;2019.&quot;,&quot;Flowseq: Non-autoregressive conditional sequence generation with generative flow.&quot;,&quot;arXiv preprint arXiv:1909.02480 .&quot;,&quot;Sonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Kumar, and Mike Lewis.&quot;,&quot;2018.&quot;,&quot;Semantic parsing for task oriented dialog using hierarchical representations.&quot;,&quot;In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 27872792, Brussels, Belgium.&quot;,&quot;Association for Computational Linguistics.&quot;,&quot;Dilek Hakkani-Tr, Gkhan Tr, Asli Celikyilmaz, Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-Yi Wang.&quot;,&quot;2016.&quot;,&quot;Multi-domain joint semantic frame parsing using bi-directional rnn-lstm.&quot;,&quot;In Interspeech , pages 715719.&quot;,&quot;Matthew Henderson, Blaise Thomson, and Jason D. Williams.&quot;,&quot;2014.&quot;,&quot;The second dialog state tracking challenge.&quot;,&quot;In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL) , pages 263272, Philadelphia, PA, U.S.A. Association for Computational Linguistics.&quot;,&quot;Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. 2019.&quot;,&quot;Searching for mobilenetv3.&quot;,&quot;In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 13141324.&quot;,&quot;Diederik P Kingma and Jimmy Ba.&quot;,&quot;2014.&quot;,&quot;Adam: A method for stochastic optimization.&quot;,&quot;arXiv preprint arXiv:1412.6980 .&quot;,&quot;Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.&quot;,&quot;2019.&quot;,&quot;Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.&quot;,&quot;Kenton Murray, Jeffery Kinnison, Toan Q. Nguyen, Walter Scheirer, and David Chiang.&quot;,&quot;2019.&quot;,&quot;Auto-sizing the transformer network: Improving speed, efficiency, and performance for low-resource machine translation.&quot;,&quot;In Proceedings of the 3rd Workshop on Neural Generation and Translation , pages 231240, Hong Kong.&quot;,&quot;Association for Computational Linguistics.&quot;,&quot;Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Te-jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.&quot;,&quot;2019.&quot;,&quot;Pytorch: An imperative style, high-performance deep learning library.&quot;,&quot;In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32 , pages 80248035.&quot;,&quot;Curran Associates, Inc.&quot;,&quot;Gabriel Pereyra, George Tucker, Jan Chorowski, ukasz Kaiser, and Geoffrey Hinton.&quot;,&quot;2017.&quot;,&quot;Regularizing neural networks by penalizing confident output distributions.&quot;,&quot;arXiv preprint arXiv:1701.06548 .&quot;,&quot;Subendhu Rongali, Luca Soldaini, Emilio Monti, and Wael Hamza.&quot;,&quot;2020.&quot;,&quot;Don't parse, generate! a sequence to sequence architecture for task-oriented semantic parsing.&quot;,&quot;arXiv preprint arXiv:2001.11458 .&quot;,&quot;Chitwan Saharia, William Chan, Saurabh Saxena, and Mohammad Norouzi.&quot;,&quot;2020.&quot;,&quot;Non-autoregressive machine translation with latent alignments.&quot;,&quot;arXiv preprint arXiv:2004.07437 .&quot;,&quot;Abigail See, Peter J. Liu, and Christopher D. Manning.&quot;,&quot;2017.&quot;,&quot;Get to the point: Summarization with pointer-generator networks.&quot;,&quot;In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1073 1083, Vancouver, Canada.&quot;,&quot;Association for Computational Linguistics.&quot;,&quot;Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.&quot;,&quot;2014.&quot;,&quot;Sequence to sequence learning with neural networks.&quot;,&quot;Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.&quot;,&quot;2015.&quot;,&quot;Rethinking the inception architecture for computer vision.&quot;,&quot;Elan van Biljon, Arnu Pretorius, and Julia Kreutzer.&quot;,&quot;2020.&quot;,&quot;On optimal transformer depth for low-resource language translation.&quot;,&quot;Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,  ukasz Kaiser, and Illia Polosukhin.&quot;,&quot;2017.&quot;,&quot;Attention is all you need.&quot;,&quot;In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30 , pages 59986008.&quot;,&quot;Curran Associates, Inc.&quot;,&quot;Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer.&quot;,&quot;2019a.&quot;,&quot;Fb-net: Hardware-aware efficient convnet design via differentiable neural architecture search.&quot;,&quot;In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1073410742.&quot;,&quot;Victor Zhong, Caiming Xiong, and Richard Socher.&quot;,&quot;2018.&quot;,&quot;Global-locally self-attentive encoder for dialogue state tracking.&quot;,&quot;In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1458 1467, Melbourne, Australia.&quot;,&quot;Association for Computational Linguistics.&quot;,&quot;Chunting Zhou, Jiatao Gu, and Graham Neubig.&quot;,&quot;2020.&quot;,&quot;Understanding knowledge distillation in non-autoregressive machine translation.&quot;,&quot;In International Conference on Learning Representations .&quot;],&quot;string&quot;:&quot;[\n  \&quot;Semantic parsing using sequence-to-sequence models allows parsing of deeper representations compared to traditional word tagging based models.\&quot;,\n  \&quot;In spite of these advantages, widespread adoption of these models for real-time conversational use cases has been stymied by higher compute requirements and thus higher latency.\&quot;,\n  \&quot;In this work, we propose a non-autoregressive approach to predict semantic parse trees with an efficient seq2seq model architecture.\&quot;,\n  \&quot;By combining non-autoregressive prediction with convolutional neural networks, we achieve significant latency gains and parameter size reduction compared to traditional RNN models.\&quot;,\n  \&quot;Our novel architecture achieves up to an 81% reduction in latency on TOP dataset and retains competitive performance to non-pretrained models on three different semantic parsing datasets.\&quot;,\n  \&quot;Our code is available at https://github.\&quot;,\n  \&quot;com/facebookresearch/pytext .\&quot;,\n  \&quot;Advances in conversational assistants have helped to improve the usability of smart speakers and consumer wearables for different tasks.\&quot;,\n  \&quot;Semantic parsing is one of the fundamental components of these assistants and it helps to convert the user input in natural language to a structure representation that can be understood by downstream systems.\&quot;,\n  \&quot;Majority of the semantic parsing systems deployed on various devices, rely on server-side inference because of the lower compute/memory available on these edge devices.\&quot;,\n  \&quot;This poses a few drawbacks such as flaky user experience with spotty internet connectivity and compromised user data privacy due to the dependence on a centralized server to which all user interactions are sent to.\&quot;,\n  \&quot;Thus, semantic parsing on-device has numerous advantages.\&quot;,\n  \&quot;For the semantic parsing task, the meaning representation used decides the capabilities of the system built.\&quot;,\n  \&quot;Limitations of the representation with one intent and slot labels were studied in the context of nested queries and multi turn utterances in Aghajanyan et al. (2020) and Gupta et al. (2018).\&quot;,\n  \&quot;New representations were proposed to overcome these limitations and sequence-to-sequence models were proposed as the solution to model these complex forms.\&quot;,\n  \&quot;But using these new models in real-time conversational assistants still remains a challenge due to higher latency requirements.\&quot;,\n  \&quot;In our work, we propose a novel architecture and generation scheme to significantly improve the end2end latency of sequence-to-sequence models for the semantic parsing task.\&quot;,\n  \&quot;Due to the autoregressive nature of generation in sequence-to-sequence semantic parsing models, the recurrence relationship between target tokens creates a limitation that decoding cannot be parallelized.\&quot;,\n  \&quot;There are multiple works in machine translation which try to solve this problem.\&quot;,\n  \&quot;These approaches relax the decoder token-by-token generation by allowing multiple target tokens to be generated at once.\&quot;,\n  \&quot;Fully non-autoregressive models (Gu et al., 2017; Ma et al., 2019; Ghazvininejad et al., 2020a; Saharia et al., 2020) and conditional masked language models with iterative decoding (Ghazvinine-jad et al., 2019; Gu et al., 2019; Ghazvininejad et al., 2020b) are some of them.\&quot;,\n  \&quot;To enable non-autoregressive generation in semantic parsing, we modify the objective of the standard seq2seq model to predict the entire target structure at once.\&quot;,\n  \&quot;We build upon the CMLM (Con-ditional Masked Language Model) (Ghazvininejad et al., 2019) and condition the generation of the full target structure on the encoder representation.\&quot;,\n  \&quot;By eliminating the recurrent relationship between individual target tokens, the decoding process can be parallelized.\&quot;,\n  \&quot;While this drastically improves latency, the representation of each token is still dependent on previous tokens if we continue to use an RNN architecture.\&quot;,\n  \&quot;Thus, we propose a novel model architecture for semantic parsing based on convolutional networks (Wu et al., 2019b) to solve this issue.\&quot;,\n  \&quot;Our non-autoregressive model achieves up to an 81% reduction in latency on the TOP dataset (Gupta et al., 2018), while achieving 80.23% exact match accuracy.\&quot;,\n  \&quot;We also achieve 88.16% exact match accuracy on DSTC2 (Henderson et al., 2014) and 80.86% on SNIPS (Coucke et al., 2018) which is competitive to prior work without pretraining.\&quot;,\n  \&quot;To summarize, our two main contributions are:  We propose a novel alternative to the traditional autoregressive generation scheme for semantic parsing using sequence-to-sequence models.\&quot;,\n  \&quot;With a new model training strategy and generation approach, the semantic parse structure is predicted in one step improving parallelization and thus leading to significant reduction in model latency with minimal accuracy impact.\&quot;,\n  \&quot;We also study the limitations of original CMLM (Ghazvininejad et al., 2019) when applied for conversational semantic parsing task and provide motivations for our simple yet critical modifications.\&quot;,\n  \&quot;We propose LightConv Pointer, a model architecture for non-autoregressive semantic parsing, using convolutional neural networks which provides significant latency and model size improvements over RNN models.\&quot;,\n  \&quot;Our novel model architecture is particularly suitable for limited compute use-cases like on-device conversational assistants.\&quot;,\n  \&quot;In this section, we propose a novel, convolutional, non-autoregressive architecture for semantic parsing.\&quot;,\n  \&quot;While non-autoregressive decoding has been previously explored in machine translation, we describe how it can be applied to semantic parsing with several critical modifications to retain performance.\&quot;,\n  \&quot;We then describe our convolutional architecture.\&quot;,\n  \&quot;By incorporating these advances together, our approach achieves both high accuracy and efficient decoding.\&quot;,\n  \&quot;The task is to predict the semantic parse tree given the raw text.\&quot;,\n  \&quot;We use the decoupled representation (Aghajanyan et al., 2020), an extension of the compositional form proposed in Gupta et al. (2018) for task oriented semantic parsing.\&quot;,\n  \&quot;Decoupled representation is obtained by removing all text in the compositional form that does not appear in a leaf slot.\&quot;,\n  \&quot;Efficient models require representations which are compact, with least number of tokens, to reduce number of floating point operations during inference.\&quot;,\n  \&quot;Decoupled representation was found to be suitable due to this.\&quot;,\n  \&quot;Figure 1 shows the semantic parse for a sample utterance.\&quot;,\n  \&quot;Our model predicts the serialized representation of this tree which is [IN:CREATE_REMINDER [SL:PERSON_REMINDED me ] [SL:TODO [IN:CREATE_CALL [SL:METHOD call ] [SL:CONTACT John ] ] ] ] 2.1 Non-Autoregressive Decoding While autoregressive models (Figure 2), which predict a sequence token by token, have achieved strong results in various tasks including semantic parsing, they have a large downside.\&quot;,\n  \&quot;The main challenge in practical applications is the slow decoding time.\&quot;,\n  \&quot;We investigate how to incorporate recent advances in non-autoregressive decoding for efficient semantic parsing models.\&quot;,\n  \&quot;We build upon the Conditional Masked Language Model (CMLM) proposed in Ghazvininejad et al. (2019) by applying it to the structured prediction task of semantic parsing for task-oriented dialog.\&quot;,\n  \&quot;Ghazvininejad et al. (2019) uses CMLM to first predict a token-level representation for each source token and a target sequence length; then the model predicts and iterates on the target sequence prediction in a non-autoregressive fashion.\&quot;,\n  \&quot;We describe our changes and the motivations for these changes below.\&quot;,\n  \&quot;One of the main differences between our work and Ghazvininejad et al. (2019) is that target length prediction plays a more important role in semantic parsing.\&quot;,\n  \&quot;For the translation task, if the target length is off by one or more, the model can slightly rephrase the sentence to still return a high quality translation.\&quot;,\n  \&quot;In our case, if the length prediction is Figure 2: Traditional Sequence to Sequence architecture which uses autoregressive generation scheme for decoder.\&quot;,\n  \&quot;To resolve this important challenge, we propose a specialized length prediction module that more accurately predicts the target sequence length.\&quot;,\n  \&quot;While Ghazvininejad et al. (2019) uses a special CLS token in the source sequence to predict the target length, we have a separate module of multiple layers of CNNs with gated linear units to predict the target sequence length (Wu et al., 2019b).\&quot;,\n  \&quot;We also use label smoothing and differently weighing losses as explained in section 2.3, to avoid the easy over-fitting in semantic parsing compared to translation.\&quot;,\n  \&quot;As shown in Aghajanyan et al. (2020), transformers without pre-training perform poorly on TOP dataset.\&quot;,\n  \&quot;The architectural changes that we propose to solve the data efficiency can be found in the section 2.2.1.\&quot;,\n  \&quot;Further, we find that the random masking strategy proposed in Ghazvininejad et al. (2019) works poorly for semantic parsing.\&quot;,\n  \&quot;When we use the same strategy for the semantic parsing task where the output has a structure, model is highly likely to see invalid trees during training as masking random tokens in the linearized representation of a tree mostly gives invalid tree representations.\&quot;,\n  \&quot;This makes it hard for the model to learn the structure especially when the structure is complicated (in the case of trees, deep trees were harder to learn).\&quot;,\n  \&quot;To remedy this problem, we propose a different strategy for model training where all the tokens in the target sequence are masked during training.\&quot;,\n  \&quot;Our model architecture (Figure 3) is based on the classical seq2seq model (Sutskever et al., 2014) and follows the encoder-decoder architecture.\&quot;,\n  \&quot;In order to optimize for efficient encoding and decoding, we look to leverage a fully parallel model architecture.\&quot;,\n  \&quot;While transformer models are fully parallel and popular in machine translation (Vaswani et al., 2017), they are known to perform poorly in low resource settings and require careful tuning using techniques like Neural Architecture Search to get good performance (van Biljon et al., 2020; Murray et al., 2019).\&quot;,\n  \&quot;Similarly, randomly initialized transformers performed poorly on TOP dataset achieving only 64.5 % accuracy when SOTA was above 80% (Aghajanyan et al., 2020).\&quot;,\n  \&quot;We overcome this limitation by augmenting Transformers with Convolutional Neural Networks.\&quot;,\n  \&quot;Details of our architecture are explained below.\&quot;,\n  \&quot;For token representations, we use word embeddings concatenated with the sinusoidal positional embeddings (Vaswani et al., 2017).\&quot;,\n  \&quot;Encoder and decoder consist of multiple layers with residual connections as shown in Figure 4.\&quot;,\n  \&quot;First sub-block in each layer consists of MHA (Vaswani et al., 2017).\&quot;,\n  \&quot;In decoder, we do not do masking of future tokens during model training.\&quot;,\n  \&quot;This is needed for non-autoregressive generation of target tokens during inference.\&quot;,\n  \&quot;Second sub-block consists of multiple convolutional layers.\&quot;,\n  \&quot;We use depthwise convolutions with weight sharing (Wu et al., 2019b).\&quot;,\n  \&quot;Convolution layer helps in learning representation for tokens for a fixed context size and multiple layers helps with bigger receptive fields.\&quot;,\n  \&quot;We use non-causal convolutions for both encoder as well as decoder.\&quot;,\n  \&quot;Third sub-block is the FFN (Vaswani et al., 2017; Wu et al., 2019b) which consists of two linear layers and relu.\&quot;,\n  \&quot;The decoder has source-target attention after the convolution layer.\&quot;,\n  \&quot;Pointer-Generator Projection layer The decoder has a final projection layer which generates the target tokens from the decoder/encoder representations.\&quot;,\n  \&quot;Rongali et al. (2020) proposes an idea based Pointer Generator Network (See et al., 2017) to convert the decoder representation to target tokens using the encoder output.\&quot;,\n  \&quot;Similarly, we use a pointer based projection head, which decides whether to copy tokens from the source-sequence or generate from the pre-defined ontology at every Figure 3: Sequence to Sequence model architecture which uses Non-Autoregressive strategy for generation decoding step (Aghajanyan et al., 2020).\&quot;,\n  \&quot;Length Prediction Module Length prediction Module receives token level representations from the encoder as input.\&quot;,\n  \&quot;It uses stacked CNNs with gated linear units and mean pooling to generation the length prediction.\&quot;,\n  \&quot;Suppose the source sequence is of length L and source tokens in the raw text are s 1 , s 2 , s 3 . . . s L .\&quot;,\n  \&quot;Encoder generates a representation of for each token in the source sequence.\&quot;,\n  \&quot;Using the predicted length T, we create a target sequence of length T consisting of identical MASK tokens.\&quot;,\n  \&quot;This sequence is passed through possibly multiple decoder layers and generates a representation for each token in the masked target sequence.\&quot;,\n  \&quot;We make a strong assumption that each token in the target sentence is conditionally independent of each other given the source and the target length.\&quot;,\n  \&quot;Thus, the individual probabilities for each token is P ( y i | X, T ) where X is the input sequence and T is the length of target sequence.\&quot;,\n  \&quot;Beam Search During inference, length prediction module explained in 2.2.1 predicts top k lengths.\&quot;,\n  \&quot;For each predicted length, we create a decoder input sequence of all masked tokens.\&quot;,\n  \&quot;This is similar to the beam search with beam size k in autoregressive systems.\&quot;,\n  \&quot;The main difference in our model architecture is that we expect only one candidate for each predicted length.\&quot;,\n  \&quot;These all masked sequences are given as input to the model and the model predicts target tokens for each masked token.\&quot;,\n  \&quot;Once we have predicted target sequences for k different lengths, they are ranked based on the ranking algorithm described in (5), where X is the input sequence and Y is the predicted output sequence, note the predicted token y i is conditioned on both the sequence ( X ) and the predicted target length T .\&quot;,\n  \&quot;During training, we jointly optimize for two weighted losses.\&quot;,\n  \&quot;The first loss is calculated for the predicted target tokens against the real target and the second loss is calculated for predicted target length against real target length.\&quot;,\n  \&quot;During forward-pass, we replace all the tokens in the target sequence with a special <MASK> token and give this as an input to the decoder.\&quot;,\n  \&quot;Decoder predicts the token for each masked token and the cross-entropy loss is calculated for each predicted token.\&quot;,\n  \&quot;The length prediction module in the model predicts the target length using the encoder representation.\&quot;,\n  \&quot;Similar to CMLMs in (Ghazvininejad et al., 2019), length prediction is modeled as a classifica-tion task with class labels for each possible length.\&quot;,\n  \&quot;Cross entropy loss is calculated for length prediction.\&quot;,\n  \&quot;For our semantic parsing task, label smoothing (Szegedy et al., 2015) was found to be very critical as the length prediction module tends to easily overfit and strong regularization methods are needed.\&quot;,\n  \&quot;This was because length prediction was a much well-defined task compared to predicting all the tokens in the sequence.\&quot;,\n  \&quot;Total loss was calculated by taking a weighted sum of cross entropy loss for labels and length, with lower weight for length loss.\&quot;,\n  \&quot;As training progresses through different epochs, the best model is picked by comparing the exact match (EM) accuracy of different snapshots on validation set.\&quot;,\n  \&quot;We use 3 datasets across various domains to evaluate our semantic parsing approach.\&quot;,\n  \&quot;Length distribution of each dataset is described using median, 90th percentile and 99th percentile lengths.\&quot;,\n  \&quot;TOP Dataset Task Oriented Parsing (Gupta et al., 2018) is a dataset for compositional utterances in the navigation and events domains.\&quot;,\n  \&quot;The training set consists of 31 , 279 instances and the test set consists of 9 , 042 .\&quot;,\n  \&quot;The test set has a median target length of 15, P90 27 and P99 39.\&quot;,\n  \&quot;SNIPS The SNIPS (Coucke et al., 2018) dataset is a public dataset used for benchmarking semantic parsing intent slot models.\&quot;,\n  \&quot;This dataset is considered flat, since it does not contain compositional queries and can be solved with word-tagging models.\&quot;,\n  \&quot;Recently, however seq2seq models have started to out perform word-tagging models (Rongali et al., 2020; Aghajanyan et al., 2020).\&quot;,\n  \&quot;The training set consists of 13 , 084 instances, the test set consists of 700 instances.\&quot;,\n  \&quot;The test set has a median target length of 11, P90 17, P99 21.\&quot;,\n  \&quot;DSTC2 Dialogue State Tracking Challenge 2 (Henderson et al., 2014), is a dataset for conversational understanding.\&quot;,\n  \&quot;The dataset involves users searching for restaurants, by specifying constraints such as cuisine type and price range, we encode these constraints as slots and use this to formulate the decoupled representation.\&quot;,\n  \&quot;The training set consists of 12 , 611 instances and a test set of 9890 .\&quot;,\n  \&quot;The test set has a median target length of 6, P90 9 and P99 10.\&quot;,\n  \&quot;Semantic Parsing Performance For all our datasets, we convert the representation of either the compositional form or flat intent slot form to the decoupled representation (Aghajanyan et al., 2020) .\&quot;,\n  \&quot;We compare the model prediction with the serialized structure representation and look for exact match (EM).\&quot;,\n  \&quot;Benchmarking Latency For the latency analysis for the models trained from scratch: AR LightConv Pointer, NAR LightConv Pointer, and BiLSTM.\&quot;,\n  \&quot;We chose these 3 architectures, to compare NAR vs AR variants of LightConv Pointer, as well as the best performant baseline: Pointer BiLSTM (Aghajanyan et al., 2020).\&quot;,\n  \&quot;We use Samsung Galaxy S8 with Android OS and Octa-core processor.\&quot;,\n  \&quot;We chose to benchmark latency to be consistent with prior work on on-device modeling (Wu et al., 2019a; Howard et al., 2019).\&quot;,\n  \&quot;All models are trained in PyTorch (Paszke et al., 2019) and exported using Torchscript.\&quot;,\n  \&quot;We measure wall clock time as it is preferred instead of other options because it relates more to real world inference.\&quot;,\n  \&quot;1 Latency results can be found in section 4.2.\&quot;,\n  \&quot;For each of our datasets, we report accuracy metrics on the following models:\&quot;,\n  \&quot;NAR LightConv Pointer : A non-autoregressive (NAR) variant of the above model to allow for parallel decoding.\&quot;,\n  \&quot;We compare against the best reported numbers across datasets where the models don't use pretraining.\&quot;,\n  \&quot;During training of our model we use the same base model across all datasets and sweep over hyper parameters for the length module and the batch size and learning rate, an equivalent sweep was done for the AR variant as well.\&quot;,\n  \&quot;The base model we use for NAR LightConv Pointer model uses 5 encoder layers with convolutional kernel sizes [3,7,15,21,27], where each encoder layer has embedding and convolutional dimensions of 160, 1 self attenion head, and 2 decoder layers with kernel sizes [7,27], and embedding dimension of 160, 1 self-attention head and 2 encoder-attention heads.\&quot;,\n  \&quot;Our length prediction module leverages a two convolution layers of 512 embedding dimensions and kernel sizes of 3 and 9.\&quot;,\n  \&quot;and uses hidden dimension in [128,256,512] determined by hyper parameter sweeps.\&quot;,\n  \&quot;We also use 8 attention heads for the decoupled projection head.\&quot;,\n  \&quot;For the convolutional layer, we use lightweight convolutions (Wu et al., 2019b) with number of heads set to 2.\&quot;,\n  \&quot;We train with the Adam (Kingma and Ba, 2014) optimizer, learning rate is selected to be between [0.00007, 0.0004].\&quot;,\n  \&quot;If our evaluation accuracy has not increased in 10 epochs, we also reduce our learning rate by a factor of 10, and we employ early stopping if the accuracy has not changed in 20 epochs.\&quot;,\n  \&quot;We train with our batch size fixed to be 8.\&quot;,\n  \&quot;We optimize a joint loss for label prediction and length prediction.\&quot;,\n  \&quot;Both losses consist of label smoothed cross entropy loss (  is the weight of the uniform distribution) (Pereyra et al., 2017), our label loss has  = 0 .\&quot;,\n  \&quot;1 and our length loss has  = 0 .\&quot;,\n  \&quot;5 , we also weight our length loss lower,  = 0 .\&quot;,\n  \&quot;25 .\&quot;,\n  \&quot;For inference, we use a length beam size of k = 5 .\&quot;,\n  \&quot;Our AR variant follows the same parameters however it does not have length prediction and self-attention in encoder and decoder.\&quot;,\n  \&quot;We show that our proposed non-autoregressive convolutional architecture for semantic parsing is competitive with auto-regressive baselines and word tagging baselines without pre-training on three different benchmarks and reduces latency up to 81% on the TOP dataset.\&quot;,\n  \&quot;We first compare accuracy and latency, then discuss model performance by analyzing errors by length, and the importance of knowledge distillation.\&quot;,\n  \&quot;We do our analysis on the TOP dataset, due to its inherent compositional nature, however we expect our analysis to hold for other datasets as well.\&quot;,\n  \&quot;Non-compositional datasets like DSTC2 and SNIPS can be modeled by word tagging models making seq2seq models more relevant in the case of compositional datasets.\&quot;,\n  \&quot;In table 5a we show our NAR and AR variants for LightConv Pointer perform quite similarly across all datasets.\&quot;,\n  \&quot;We can see that our proposed NAR LightConv Pointer is also competitive with state of the art models without pre-training: -0.66% TOP, -0.17% DSTC2, -4.57% SNIPS (-0.04% compared to word tagging models).\&quot;,\n  \&quot;Following the prior work on Non-Autoregressive models, we also report our experiments with sequence-level knowledge distillation in subsection Knowledge Distillation under section.\&quot;,\n  \&quot;4.3.\&quot;,\n  \&quot;In figure 5b we show the latency of our model with different generation approaches (NAR vs AR) over increasing target sequence lengths on the TOP dataset.\&quot;,\n  \&quot;Firstly, we show that our LightConv Pointer is significantly faster than the BiLSTM baseline (Aghajanyan et al., 2020), achieving up to a 54% reduction in median latency.\&quot;,\n  \&quot;BiLSTM was used as baseline as that was the SOTA without pretraining for TOP and Transformers performed poorly.\&quot;,\n  \&quot;By comparing our model with AR and NAR generation strategy, it can be seen that increase in latency with increase in target length is much smaller for NAR due to better parallelization of decoder, resulting in up to an 81% reduction in Length Bucket NAR (%) AR (%) Bucket Size < 10 82.80 83.13 2798 10-20 84.18 84.36 5167 20-30 62.50 65.72 992 30-40 21.25 41.25 80 > 40 0.00 20.00 5 Table 2: EM accuracy of the NAR LightConv Pointer (distilled) vs AR LightConv Pointer distilled across different target length buckets along with the number of instances in each bucket on the TOP dataset.\&quot;,\n  \&quot;median latency compared to the BiLSTM model.\&quot;,\n  \&quot;Also note that both the LightConv Pointer models are able to achieve parity in terms of EM Accuracy compared to the baseline BiLSTM model, while using many fewer parameters, the BiLSTM model uses 20M parameters, while the NAR LightConv Pointer uses 12M and the AR LightConv Pointer uses 10M.\&quot;,\n  \&quot;Ablation experiments We compare the modifications proposed by this work (LightConv, Conv length prediction module and Mask everything strategy) with the original model proposed in Ghazvininejad et al. (2019) in table 1.\&quot;,\n  \&quot;The motivations for each modification was already discussed in sub-section 2.1.\&quot;,\n  \&quot;Our mean EM accuracy results based on 3 trials show the significance of techniques proposed in this paper especially for longer target sequences.\&quot;,\n  \&quot;Errors by length It is known that non-autoregressive models have difficulty at larger sequence lengths (Ghazvininejad et al., 2019).\&quot;,\n  \&quot;In table 2, we show our model's accuracy in each respective length bucket on the TOP dataset.\&quot;,\n  \&quot;We see that the AR and NAR model follow a similar distribution of errors, however the NAR model seems to error at a higher rate for the longer lengths.\&quot;,\n  \&quot;Knowledge Distillation Following prior work (Ghazvininejad et al., 2019; Zhou et al., 2020), we train our model with sequence-level knowledge distillation (Kim and Rush, 2016).\&quot;,\n  \&quot;We train our system on data generated by the current SOTA autoregressive models BART (Lewis et al., 2019; Aghajanyan et al., 2020).\&quot;,\n  \&quot;In table 3 we show the impact of knowledge distillation in our task on both the non-autoregressive and autoregressive variants of LightConv Pointer.\&quot;,\n  \&quot;These results support prior work in machine translation for distillation of au-Figure 6: Distilled NAR LightConv Pointer Top-K accuracy for exact match (EM) accuracy (blue) and Top-K length accuracy (orange), as well as the EM accuracy with gold length (dotted red line) for the TOP dataset.\&quot;,\n  \&quot;toregressive teachers to non-autoregressive models showing distillation improving our models on TOP and SNIPS, however we notice minimal changes on DSTC2.\&quot;,\n  \&quot;The importance of length prediction An important part of our non-autoregressive model is length prediction.\&quot;,\n  \&quot;In figure 6, we report exact match accuracy @ top k beams and length accuracy @ top k beams (where top K refers to whether the correct answer was in the top K predictions) for the TOP dataset.\&quot;,\n  \&quot;We can see a tight correlation between our length accuracy and exact match accuracy, showing how our model is bottle necked by the length prediction.\&quot;,\n  \&quot;Providing gold length as a feature, led to an exact match accuracy of 88.20% (shown in red on figure 6), an absolute 7.31 point improvement over our best result with our non-autoregressive LightConv Pointer.\&quot;,\n  \&quot;Non-autoregressive Decoding Recent work in machine translation has made a lot of progress in fully non-autoregressive models (Gu et al., 2017; Ma et al., 2019; Ghazvininejad et al., 2020a; Saharia et al., 2020) and parallel decoding (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019; Ghazvininejad et al., 2020b; Kasai et al., 2020).\&quot;,\n  \&quot;While many advancements have been made in machine translation, we believe we are the first to explore the non-autoregressive semantic parsing setting.\&quot;,\n  \&quot;In our work, we extend the CMLM to work for semantic parsing.\&quot;,\n  \&quot;We make two important adjustments: first, we use a different masking approach where we mask everything and do one-step generation.\&quot;,\n  \&quot;Second, we note the importance of the length prediction task for parsing and improve the length prediction module in the CMLM.\&quot;,\n  \&quot;Seq2Seq For Semantic Parsing Recent advances in language understanding have lead to increased reliance on seq2seq architectures.\&quot;,\n  \&quot;Recent work by Rongali et al. 2020; Aghajanyan et al. 2020, showed the advantages from using a pointer generator architecture for resolving complex queries (e.g. composition and cross domain queries) that could not be handled by word tagging models.\&quot;,\n  \&quot;Since we target the same task, we adapt their pointer decoder into our proposed architecture.\&quot;,\n  \&quot;However, to optimize for latency and compression we train CNN based architectures (Desai et al. 2020 and Wu et al. 2019b) to leverage the inherent model parallelism compared to the BiLSTM model proposed in Aghajanyan et al. 2020 and more compression compared to the transformer seq2seq baseline proposed in Rongali et al. 2020.\&quot;,\n  \&quot;To further improve latency we look at parallel decoding through non-autoregressive decoding compared to prior work leveraging autoregressive models.\&quot;,\n  \&quot;This work introduces a novel alternative to autoregressive decoding and efficient encoder-decoder architecture for semantic parsing.\&quot;,\n  \&quot;We show that in 3 semantic parsing datasets, we are able to speed up decoding significantly while minimizing accuracy regression.\&quot;,\n  \&quot;Our model is able to generate parse trees competitive with state of the art autoregressive models with significant latency savings, allowing complex NLU systems to be delivered on edge devices.\&quot;,\n  \&quot;There are a couple of limitations of our proposed model that naturally extend themselves to future work.\&quot;,\n  \&quot;Primarily, we cannot support true beam decoding, we decode a single prediction for each length prediction however there may exist multiple beams for each length prediction.\&quot;,\n  \&quot;Also for longer parse trees and more complex semantic parsing systems such as session based understanding, our NAR decoding scheme could benefit from multiple iterations.\&quot;,\n  \&quot;Lastly, though we explored models without pre-training in this work, recent developments show the power of leveraging pre-trained models such as RoBERTa and BART.\&quot;,\n  \&quot;We leave it to future work to extend our non-autoregressive decoding for pre-trained models.\&quot;,\n  \&quot;We would like to thank Sandeep Subramanian (MILA), Karthik Prasad (Facebook AI), Arash Einolghozati (Facebook) and Yinhan Liu for the\&quot;,\n  \&quot;helpful discussions.\&quot;,\n  \&quot;References Armen Aghajanyan, Jean Maillard, Akshat Shrivastava, Keith Diedrick, Michael Haeger, Haoran Li, Yashar Mehdad, Veselin Stoyanov, Anuj Kumar, Mike Lewis, and Sonal Gupta.\&quot;,\n  \&quot;2020.\&quot;,\n  \&quot;Conversational semantic parsing.\&quot;,\n  \&quot;In EMNLP/IJCNLP .\&quot;,\n  \&quot;Alice Coucke, Alaa Saade, Adrien Ball, Thodore Bluche, Alexandre Caulier, David Leroy, Clment Doumouro, Thibault Gisselbrecht, Francesco Calta-girone, Thibaut Lavril, et al. 2018.\&quot;,\n  \&quot;Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces.\&quot;,\n  \&quot;arXiv preprint arXiv:1805.10190 .\&quot;,\n  \&quot;Shrey Desai, Geoffrey Goh, Arun Babu, and Ahmed Aly.\&quot;,\n  \&quot;2020.\&quot;,\n  \&quot;Lightweight convolutional representations for on-device natural language processing.\&quot;,\n  \&quot;arXiv preprint arXiv:2002.01535 .\&quot;,\n  \&quot;Arash Einolghozati, Panupong Pasupat, Sonal Gupta, Rushin Shah, Mrinal Mohit, Mike Lewis, and Luke Zettlemoyer.\&quot;,\n  \&quot;2018.\&quot;,\n  \&quot;Improving semantic parsing for task oriented dialog.\&quot;,\n  \&quot;In Conversational AI Workshop at NeurIPS 2018 .\&quot;,\n  \&quot;Marjan Ghazvininejad, Vladimir Karpukhin, Luke Zettlemoyer, and Omer Levy.\&quot;,\n  \&quot;2020a.\&quot;,\n  \&quot;Aligned cross entropy for non-autoregressive machine translation.\&quot;,\n  \&quot;arXiv preprint arXiv:2004.01655 .\&quot;,\n  \&quot;Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer.\&quot;,\n  \&quot;2019.\&quot;,\n  \&quot;Mask-predict: Parallel decoding of conditional masked language models.\&quot;,\n  \&quot;Marjan Ghazvininejad, Omer Levy, and Luke Zettle-moyer.\&quot;,\n  \&quot;2020b.\&quot;,\n  \&quot;Semi-autoregressive training improves mask-predict decoding.\&quot;,\n  \&quot;arXiv preprint arXiv:2001.08785 .\&quot;,\n  \&quot;Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Huo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-Nung Chen.\&quot;,\n  \&quot;2018.\&quot;,\n  \&quot;Slot-gated modeling for joint slot filling and intent prediction.\&quot;,\n  \&quot;In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pages 753757.\&quot;,\n  \&quot;Victor OK Li, and Richard Socher.\&quot;,\n  \&quot;2017.\&quot;,\n  \&quot;Non-autoregressive neural machine translation.\&quot;,\n  \&quot;arXiv preprint arXiv:1711.02281 .\&quot;,\n  \&quot;Jiatao Gu, Changhan Wang, and Junbo Zhao.\&quot;,\n  \&quot;Levenshtein transformer.\&quot;,\n  \&quot;In Advances in Neural Information Processing Systems , pages 1117911189.\&quot;,\n  \&quot;Jungo Kasai, James Cross, Marjan Ghazvininejad, and Jiatao Gu.\&quot;,\n  \&quot;2020.\&quot;,\n  \&quot;Parallel machine translation with disentangled context transformer.\&quot;,\n  \&quot;arXiv preprint arXiv:2001.05136 .\&quot;,\n  \&quot;Yoon Kim and Alexander M Rush.\&quot;,\n  \&quot;2016.\&quot;,\n  \&quot;Sequence-level knowledge distillation.\&quot;,\n  \&quot;arXiv preprint arXiv:1606.07947 .\&quot;,\n  \&quot;Jason D. Lee, Elman Mansimov, and Kyunghyun Cho.\&quot;,\n  \&quot;2018.\&quot;,\n  \&quot;Deterministic non-autoregressive neural sequence modeling by iterative refinement.\&quot;,\n  \&quot;In Proc.\&quot;,\n  \&quot;of EMNLP .\&quot;,\n  \&quot;Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-big, and Eduard Hovy.\&quot;,\n  \&quot;2019.\&quot;,\n  \&quot;Flowseq: Non-autoregressive conditional sequence generation with generative flow.\&quot;,\n  \&quot;arXiv preprint arXiv:1909.02480 .\&quot;,\n  \&quot;Sonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Kumar, and Mike Lewis.\&quot;,\n  \&quot;2018.\&quot;,\n  \&quot;Semantic parsing for task oriented dialog using hierarchical representations.\&quot;,\n  \&quot;In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 27872792, Brussels, Belgium.\&quot;,\n  \&quot;Association for Computational Linguistics.\&quot;,\n  \&quot;Dilek Hakkani-Tr, Gkhan Tr, Asli Celikyilmaz, Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-Yi Wang.\&quot;,\n  \&quot;2016.\&quot;,\n  \&quot;Multi-domain joint semantic frame parsing using bi-directional rnn-lstm.\&quot;,\n  \&quot;In Interspeech , pages 715719.\&quot;,\n  \&quot;Matthew Henderson, Blaise Thomson, and Jason D. Williams.\&quot;,\n  \&quot;2014.\&quot;,\n  \&quot;The second dialog state tracking challenge.\&quot;,\n  \&quot;In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL) , pages 263272, Philadelphia, PA, U.S.A. Association for Computational Linguistics.\&quot;,\n  \&quot;Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. 2019.\&quot;,\n  \&quot;Searching for mobilenetv3.\&quot;,\n  \&quot;In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 13141324.\&quot;,\n  \&quot;Diederik P Kingma and Jimmy Ba.\&quot;,\n  \&quot;2014.\&quot;,\n  \&quot;Adam: A method for stochastic optimization.\&quot;,\n  \&quot;arXiv preprint arXiv:1412.6980 .\&quot;,\n  \&quot;Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.\&quot;,\n  \&quot;2019.\&quot;,\n  \&quot;Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\&quot;,\n  \&quot;Kenton Murray, Jeffery Kinnison, Toan Q. Nguyen, Walter Scheirer, and David Chiang.\&quot;,\n  \&quot;2019.\&quot;,\n  \&quot;Auto-sizing the transformer network: Improving speed, efficiency, and performance for low-resource machine translation.\&quot;,\n  \&quot;In Proceedings of the 3rd Workshop on Neural Generation and Translation , pages 231240, Hong Kong.\&quot;,\n  \&quot;Association for Computational Linguistics.\&quot;,\n  \&quot;Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Te-jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.\&quot;,\n  \&quot;2019.\&quot;,\n  \&quot;Pytorch: An imperative style, high-performance deep learning library.\&quot;,\n  \&quot;In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32 , pages 80248035.\&quot;,\n  \&quot;Curran Associates, Inc.\&quot;,\n  \&quot;Gabriel Pereyra, George Tucker, Jan Chorowski, ukasz Kaiser, and Geoffrey Hinton.\&quot;,\n  \&quot;2017.\&quot;,\n  \&quot;Regularizing neural networks by penalizing confident output distributions.\&quot;,\n  \&quot;arXiv preprint arXiv:1701.06548 .\&quot;,\n  \&quot;Subendhu Rongali, Luca Soldaini, Emilio Monti, and Wael Hamza.\&quot;,\n  \&quot;2020.\&quot;,\n  \&quot;Don't parse, generate! a sequence to sequence architecture for task-oriented semantic parsing.\&quot;,\n  \&quot;arXiv preprint arXiv:2001.11458 .\&quot;,\n  \&quot;Chitwan Saharia, William Chan, Saurabh Saxena, and Mohammad Norouzi.\&quot;,\n  \&quot;2020.\&quot;,\n  \&quot;Non-autoregressive machine translation with latent alignments.\&quot;,\n  \&quot;arXiv preprint arXiv:2004.07437 .\&quot;,\n  \&quot;Abigail See, Peter J. Liu, and Christopher D. Manning.\&quot;,\n  \&quot;2017.\&quot;,\n  \&quot;Get to the point: Summarization with pointer-generator networks.\&quot;,\n  \&quot;In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1073 1083, Vancouver, Canada.\&quot;,\n  \&quot;Association for Computational Linguistics.\&quot;,\n  \&quot;Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.\&quot;,\n  \&quot;2014.\&quot;,\n  \&quot;Sequence to sequence learning with neural networks.\&quot;,\n  \&quot;Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\&quot;,\n  \&quot;2015.\&quot;,\n  \&quot;Rethinking the inception architecture for computer vision.\&quot;,\n  \&quot;Elan van Biljon, Arnu Pretorius, and Julia Kreutzer.\&quot;,\n  \&quot;2020.\&quot;,\n  \&quot;On optimal transformer depth for low-resource language translation.\&quot;,\n  \&quot;Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,  ukasz Kaiser, and Illia Polosukhin.\&quot;,\n  \&quot;2017.\&quot;,\n  \&quot;Attention is all you need.\&quot;,\n  \&quot;In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30 , pages 59986008.\&quot;,\n  \&quot;Curran Associates, Inc.\&quot;,\n  \&quot;Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer.\&quot;,\n  \&quot;2019a.\&quot;,\n  \&quot;Fb-net: Hardware-aware efficient convnet design via differentiable neural architecture search.\&quot;,\n  \&quot;In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1073410742.\&quot;,\n  \&quot;Victor Zhong, Caiming Xiong, and Richard Socher.\&quot;,\n  \&quot;2018.\&quot;,\n  \&quot;Global-locally self-attentive encoder for dialogue state tracking.\&quot;,\n  \&quot;In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1458 1467, Melbourne, Australia.\&quot;,\n  \&quot;Association for Computational Linguistics.\&quot;,\n  \&quot;Chunting Zhou, Jiatao Gu, and Graham Neubig.\&quot;,\n  \&quot;2020.\&quot;,\n  \&quot;Understanding knowledge distillation in non-autoregressive machine translation.\&quot;,\n  \&quot;In International Conference on Learning Representations .\&quot;\n]&quot;},&quot;labels&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;result&quot;,&quot;objective&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;result&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;other&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;result&quot;,&quot;result&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;objective&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;],&quot;string&quot;:&quot;[\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;result\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;result\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;result\&quot;,\n  \&quot;result\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;\n]&quot;}}},{&quot;rowIdx&quot;:5,&quot;cells&quot;:{&quot;sentences&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;Rationale-Centric Framework for Human-in-the-loop Machine Learning&quot;,&quot;{yanglinyi, zhangyue}@westlake.edu.cn Abstract&quot;,&quot;We present a novel rationale-centric framework with human-in-the-loop  R ationales-centric D ouble-robustness L earning (RDL)  to boost model out-of-distribution performance in few-shot learning scenarios.&quot;,&quot;By using static semi-factual generation and dynamic human-intervened correction, RDL exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation.&quot;,&quot;Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests compared to many state-of-the-art benchmarksespecially for few-shot learning scenarios.&quot;,&quot;We also perform extensive ablation studies to support in-depth analyses of each component in our framework.&quot;,&quot;Recent work finds that natural artefacts (Guru-rangan et al., 2018) or spurious patterns (Keith et al., 2020; Srivastava et al., 2020) in datasets can cause sub-optimal model performance for neural networks.&quot;,&quot;As shown in Figure 1, the bold phrases  100% bad  and  brain cell killing  are underlying causes for a negative sentiment prediction that most human readers would recognise.&quot;,&quot;These are defined as rationales in this paper.&quot;,&quot;The underlined phraseacting and plot has been incorrectly recognised as a causal term by the model used fort this example, and is referred to as a spurious pattern .&quot;,&quot;Spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021), and are usually useless, or even harmful, at test time.&quot;,&quot;This issue can be severe in few-shot learning (FSL) * These authors contributed equally to this work.&quot;,&quot;scenarios.&quot;,&quot;For instance, Kulesza et al. (2010) suggests that when a model is trained with a small subset of labelled data, it is prone to exploiting spurious patterns leading to poor generalisability that is evident in the performance decay in out-of-distribution (OOD) datasets.&quot;,&quot;In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu and MacNamee, 2020; Lu et al., 2021).&quot;,&quot;There is a strand of research addressing this scenario that seeks to improve model performance by introducing methods and resources for training models less sensitive to spurious patterns&quot;,&quot;(Kaushik et al., 2020).&quot;,&quot;Most of this work relies on generating counterfactual augmented data&quot;,&quot;(CAD), either manually&quot;,&quot;(Kaushik et al., 2021)&quot;,&quot;or automatically&quot;,&quot;(Feng et al., 2021; Qian et al., 2021; Yang et al., 2021, 2020a; Delaney et al., 2021).&quot;,&quot;For example, Kaushik et al.&quot;,&quot;(2020)&quot;,&quot;proposed a human-in-the-loop framework where human annotators are required to make minimal changes to original movie reviews to produce sentiment-flipped counterfactual reviews, which enables models to learn useful associations between input texts and output labels&quot;,&quot;(Kaushik et al., 2021).&quot;,&quot;Generating manual counterfactuals, however, is expensive and time-consumingKaushik et al.&quot;,&quot;(2020)&quot;,&quot;report the cost of revising 2 .&quot;,&quot;5 k instances at over $10,000.&quot;,&quot;On the other hand, fully automatic methods are task-specific and therefore have weak robustness across domains and less reliabil-6986 Semi-factual Generation&quot;,&quot;ity compared to manual counterfactuals.&quot;,&quot;To address these issues, we propose R ationales-centric D ouble-robustness L earning&quot;,&quot;(RDL), a human-in-the-loop framework for data augmentation in a few-shot setting, which is efficient, robust, model-agnostic, and general across tasks.&quot;,&quot;Our main idea is a rationale-centric strategy for eliminating the effect of spurious patterns by leveraging human knowledge as shown in Figure&quot;,&quot;2. Our double-robustness framework consists of two main modules.&quot;,&quot;The first is a Static Semi-factual Generation module that generates a set of semifactual data automatically for a given instance by using human-identified rationales.&quot;,&quot;Such labelling requires less human input compared to fully manual counterfactual generation&quot;,&quot;(see Section 3.1).&quot;,&quot;In contrast with counterfactuals&quot;,&quot;(Roese, 1997)&quot;,&quot;that rely on what might have been different&quot;,&quot;(i.e. the label would be changed if certain terms have been changed), semi-factuals&quot;,&quot;(McCloy and Byrne, 2002; Kenny and Keane, 2021), as used in our work, aim to guide a model to identify terms less causally related to the label&quot;,&quot;(i.e. even if certain terms had been changed, the label would be kept the same).&quot;,&quot;Second, we apply a Dynamic Human-intervened Correction module , where the most salient features are identified for model predictions over a set of training examples, and human workers intervene by checking the correctness of the rationale in case first-round modifications introduce new artefacts.&quot;,&quot;We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power, both for in-distribution and OOD predictions.&quot;,&quot;also used in Kaushik et al.&quot;,&quot;(2020), demonstrate that the double-robust models can be less sensitive to spurious patterns.&quot;,&quot;In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than fully-supervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.&quot;,&quot;The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual&quot;,&quot;(Kaushik et al., 2020)&quot;,&quot;and automatic CAD&quot;,&quot;(Yang et al., 2021)&quot;,&quot;using the full augmented training set of 3,414 examples.&quot;,&quot;To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals and human-intervention for improving the generalisation abilities of deep neural networks in few-shot learning scenarios.&quot;,&quot;* 2 Related Work Data augmentation has been used for resolving artefacts in training datasets before&quot;,&quot;(Gururangan et al., 2018; Srivastava et al., 2020; Kaushik et al., 2021).&quot;,&quot;In particular, previous work&quot;,&quot;(Kaushik et al., 2020)&quot;,&quot;relied on large-scale crowd-sourcing to generate useful augmented data.&quot;,&quot;More recently, Yang et al.&quot;,&quot;(2021), and Wang and Culotta&quot;,&quot;(2021)&quot;,&quot;investigated the efficacy of the automatically generated counterfactuals for sentiment analysis.&quot;,&quot;Similar to our work, these methods also consider the most salient features that a model uses when generating augmented data, which is in line with our rationale definition.&quot;,&quot;However, they use sentiment lexicon matching for identifying rationales, which is task-specific and not necessarily fully relevant.&quot;,&quot;In contrast, we employ human annotators to identify rationales, which can be task-agnostic and robust.&quot;,&quot;Moreover, our method generates semi-factuals instead of counterfactuals used in previous work.&quot;,&quot;Human-the-loop Machine Learning&quot;,&quot;(Wu et al., 2021)&quot;,&quot;has received increasing research attention.&quot;,&quot;Active learning&quot;,&quot;(Settles, 2009; Margatina et al., 2021), the most common example of human-in-the-loop machine learning, asks human annotators only to provide high-level annotations&quot;,&quot;(i.e. labels)&quot;,&quot;for important examples.&quot;,&quot;There is also some work exploring more explainable AI systems by exploiting feature-based information.&quot;,&quot;Such methods use relatively simple models such as Nave Bayes&quot;,&quot;(Stumpf * All resources are available at https://github.com/GeorgeLuImmortal/RDL-Rationales-centric-Double-robustness-Learning/ 6987 et al., 2009; Kulesza et al., 2015)&quot;,&quot;and Linear Regression with bag-of-words features&quot;,&quot;(Jia and Liang, 2017; Teso and Kersting, 2019; Ghai et al., 2021; Shao et al., 2021), because these classifiers are relatively intuitive in generating explanations and amenable to incorporating human feedback.&quot;,&quot;Some other work uses simple neural networks such as multi-layer perceptrons&quot;,&quot;(Shao et al., 2021)&quot;,&quot;and shallow CNNs&quot;,&quot;(Lertvittayakumjorn et al., 2020; Stammer et al., 2021; Teso et al., 2021)&quot;,&quot;because the predictions of such models can be explained in the form of features.&quot;,&quot;Very recently, Yao et al.&quot;,&quot;(2021)&quot;,&quot;proposed a human-in-the-loop method to inspect more complicated models&quot;,&quot;(e.g. BERT)&quot;,&quot;with the help of model-agnostic post-hoc explanation algorithms&quot;,&quot;(Ribeiro et al., 2018)&quot;,&quot;that can explain predictions of any linear or non-linear model without exploiting its weights.&quot;,&quot;However, previous work focuses on increasing the explainability of AI systems for high-stakes domains such as health and finance&quot;,&quot;(Li et al., 2020; Yang et al., 2020b), instead of improving model robustness or generalisation ability.&quot;,&quot;Also, they assume access to a large amount of labelled data.&quot;,&quot;In contrast, we focus on few-shot learning scenarios which are more compelling.&quot;,&quot;The RDL pipeline is shown in Figure 2 and consists of two modules: Static Semi-factual Generation and Dynamic Human-intervened Correction .&quot;,&quot;Static semi-factual generation is a more efficient alternative to manually generated counterfactuals&quot;,&quot;(Kaushik et al., 2020).&quot;,&quot;In the first phase, Rationale Marking&quot;,&quot;(Section 3.1), human annotators review each document in the training set to provide rationales&quot;,&quot;(i.e. phrases that support the document classification decisions shown as bold text in Figure 2).&quot;,&quot;The second phase is a semi-factual generation method based on synonym replacement&quot;,&quot;(Section 3.2)&quot;,&quot;that produces augmented examples&quot;,&quot;(blue text in Figure 2 indicates replaced words), which are added into the training set.&quot;,&quot;Dynamic human-intervened correction&quot;,&quot;(Section 3.3)&quot;,&quot;is a rationales-powered human-in-the-loop framework to dynamically correct the model's behaviours.&quot;,&quot;At the outset, sampling and sensitivity of contextual decomposition&quot;,&quot;(SCD)&quot;,&quot;(Jin et al., 2019)&quot;,&quot;is applied to detect the rationales given by the model that is obtained in the previous step.&quot;,&quot;Then, all model-identified rationales&quot;,&quot;(underlined texts in Figure 2)&quot;,&quot;are examined by human annotators to identify false rationales&quot;,&quot;(i.e. words or phrases that do not support the classifications but are falsely included by the model)&quot;,&quot;and missing rationales&quot;,&quot;(i.e. words or phrases that support the classifications but are not included by the model).&quot;,&quot;Both false rationales and missing rationales are corrected to produce augmented examples.&quot;,&quot;Finally, newly generated examples are added into the training set to re-train the deep learning model.&quot;,&quot;Following Kaushik et al.&quot;,&quot;(2020)&quot;,&quot;and Yang et al.&quot;,&quot;(2021), we use the IMDb movie review dataset&quot;,&quot;(Maas et al., 2011)&quot;,&quot;in our experiments.&quot;,&quot;It consists of positive and negative movie reviews that are easy for human participants to understand, re-annotate, and provide feedback upon&quot;,&quot;(Zaidan et al., 2007).&quot;,&quot;We use a crowdsourcing company to recruit editors and annotators for marking rationales that support classification decisions.&quot;,&quot;At the outset, annotators were given instructions and examples that gently guided them to annotate rationales.&quot;,&quot;Only adjectives, adverbs, nouns, and verbs were considered as rationales.&quot;,&quot;Besides, rationales were required to carry complete semantic information.&quot;,&quot;For example, for a phrase starting with a negation word such as  not great , annotators are instructed to mark the whole phrase  not great  as a rationale instead of just marking  not .&quot;,&quot;We also limited rationales to at most three consecutive words&quot;,&quot;(i.e. unigrams, bigrams and trigrams).&quot;,&quot;Phrases consisting of numerical scores are not counted as rationales&quot;,&quot;(e.g. 5 or 10 stars)&quot;,&quot;since different datasets may use different rating scales, and annotating digits may hurt OOD performance.&quot;,&quot;Overall, we encouraged annotators to try their best to mark as many rationales as possible to explain classification labels.&quot;,&quot;However, to guarantee the quality of rationale marking and prevent annotators from over including non-rationales for more payment, we also manually inspected annotated examples and rejected examples that contained incorrect rationales.&quot;,&quot;After inspection, we rejected 10.6% of negative reviews and 7.6% of positive reviews.&quot;,&quot;Editors and annotators re-annotated the rejected examples, which were then presented to us for another inspection.&quot;,&quot;All re-annotated examples were approved only if all authors were happy with the quality of the annotations.&quot;,&quot;Otherwise, the examples were re-annotated again.&quot;,&quot;rationales in 855 movie reviews involved in Section 3.1 and 3.3&quot;,&quot;(note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments).&quot;,&quot;Human annotators spent on average 183.68 seconds to identify rationales in a review and our method generated semi-factual examples automatically.&quot;,&quot;On the contrary, workers spent on average 300 seconds to revise a review to generate a counterfactual manually as reported by Kaushik et al.&quot;,&quot;(2020).&quot;,&quot;Note that our approach using 100 labelled examples can outperform manual CAD&quot;,&quot;(Kaushik et al., 2020)&quot;,&quot;using the entire training set of 1,707 examples&quot;,&quot;(see Section 5.3), making our approach 300  1707 183 .&quot;,&quot;68  100  27 .&quot;,&quot;88 times more efficient than manually generated CAD.&quot;,&quot;We take a simple replacement strategy, which has been taken by Yang et al.&quot;,&quot;(2021), to generate semifactual examples.&quot;,&quot;Given a human-identified rationale, our method constructs augmented examples by automatically replacing non-rationale words, thus leading to examples with the same labels.&quot;,&quot;This augmentation is consistent with semi-factual thinking: even if those non-rationales were changed, the label would not change.&quot;,&quot;Formally, given a training example x i = [ t i 1 , t i 2 , ..., t ij ]&quot;,&quot;(where t ij is the j th token of the i th document)&quot;,&quot;and its ground truth label y i , we create a rationale vector r i = [ a i 1 , a i 2 , ..., a ij ] where a ij is the value that indicates whether t ij is a rationale or not&quot;,&quot;(we set a ij = 1 to indicate that t ij is a rationale and 0 otherwise).&quot;,&quot;To generate a semi-factual example, x  i , we randomly replace a certain number of non-rationales&quot;,&quot;(where a ij = 0 ), except for punctuation, with synonymous terms.&quot;,&quot;The synonyms can be provided by a human, retrieved automatically from a lexicon such as WordNet&quot;,&quot;(Miller, 1995), or generated using the mask-filling function of a pretrained context-aware language model&quot;,&quot;(Liu et al., 2019).&quot;,&quot;In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x  i , with some replaced non-rationales and all the other tokens identical to x i .&quot;,&quot;The label, y i , of a newly generated example is the same as the label of the original example, x i .&quot;,&quot;Examples of generated data are shown in Table&quot;,&quot;1. Afterwards, the augmented examples are added into the training set used to train the model.&quot;,&quot;Dynamic human-intervened correction further improves the robustness of the model by allowing human annotators to correct the model rationales online.&quot;,&quot;Firstly, SCD is applied to detect unigrams, bigrams or trigrams that are salient to the model.&quot;,&quot;SCD is a technique to assess the importance of terms by continuously removing terms and measuring changes in prediction&quot;,&quot;(Jin et al., 2019).&quot;,&quot;Human annotators examine all rationales given by the model from all documents to discover two types of incorrect rationale: false rationales and missing rationales.&quot;,&quot;The next phase allows human feedback to influence the learning process.&quot;,&quot;To this end, for each type of incorrect rationale, we propose a corresponding strategy to correct them.&quot;,&quot;For false rationales&quot;,&quot;(i.e. phrases that actually do not support classifications but are incorrectly identified by the model), we use synonym replacement again to generate semi-factual examples.&quot;,&quot;Unlike the static semi-factual generation&quot;,&quot;(Section 3.2), in this component we replace all false rationales with their synonyms instead of randomly replacing 5% of non-rationales in a document.&quot;,&quot;Examples of generated data are shown in Table&quot;,&quot;2. For missing rationales&quot;,&quot;(i.e. phrases that actually support classifications but are not identified by the model), we take another simple semi-factual generation strategy, that is, extracting sentences that contain missing rationales to form semi-factual data.&quot;,&quot;Specifically, given a sentence containing missing rationales, we use this sentence as a new example, and the label of this newly generated example is identical to that of the document where the sentence is extracted.&quot;,&quot;For example, there is a positive movie review&quot;,&quot;(bold font for rationales)&quot;,&quot;Robert Urich was a fine actor, and he makes this TV movie believable . I remember watching this film when I was 15 .... .&quot;,&quot;The model fails to identify  fine  and  believable  as rationales.&quot;,&quot;Thus we extract the text Robert Urich was a fine actor, and he makes this TV movie believable . as a new example, and the class of this example is still positive.&quot;,&quot;We extract the whole sentence rather than just the missing rationales to reserve more semantic information.&quot;,&quot;Note that the two correction methods in dynamic human-intervened correction can operate in parallel and the generated examples are added to the small training set to re-train the model.&quot;,&quot;Broadly speaking, our RDL framework takes advantage of invariance that makes a model less sensitive to non-rationale words or spurious patterns (Tu et al., 2020; Wang et al., 2021) in favour of focusing on useful mappings of rationales to labels.&quot;,&quot;More specifically, by using static semi-factual generation (Section 3.2) and false rationale correction (Section 3.3), we expect to break spurious associations.&quot;,&quot;For example, if a model incorrectly determines that  Soylent Green  is associated with positive sentiment (Table 2), the augmented examples that replace  Soylent Green  with other phrases such as  Gang Orange  break the spurious association.&quot;,&quot;Besides, using synonym replacement can generate examples that are similar to the original one, which is equivalent to adding noisy data to prevent models from overfitting (Wei and Zou, 2019).&quot;,&quot;Missing rationale correction (Section 3.3) emphasizes the ground truth associations between rationales and labels, enabling the model to better estimate the generally useful underlying distributions for OOD datasets, even in few-shot learning scenarios.&quot;,&quot;In the next section, we present experiments and empirical evidence to demonstrate the utility of the proposed RDL framework in improving model robustness.&quot;,&quot;Our intention is to improve the generalisability of models, and we use both in-distribution and OOD&quot;,&quot;performance for evaluation.&quot;,&quot;Our experiments are designed to address the following research questions:  RQ1 Can we use static semi-factual generation to achieve better in-distribution and OOD performance?&quot;,&quot;RQ2 Does dynamic human-intervened correction improve generalisability of models?&quot;,&quot;For fair comparison with previous work (Kaushik et al., 2020; Yang et al., 2021), we use the IMDb sentiment classification dataset (Maas et al., 2011) as the in-distribution dataset.&quot;,&quot;Following Kaushik et al. (2020), all models were trained with the IMDb dataset predefined training, validation and test partitions containing 1 , 707 , 245 , and 488 reviews respectively and an enforced 50:50 class ratio.&quot;,&quot;To measure the generalisation ability of different models, we focus on OOD performance.&quot;,&quot;To this end, we test models on another four binary sentiment classification datasets: the sampled Amazon reviews dataset (Ni et al., 2019) (100,000 positives and 100,000 negatives) from six genres: beauty, fashion, appliances, gift cards, magazines, and software; the Yelp review dataset (Zhang et al., 2015) (19,000 positives and 19,000 negatives); the SST-2 dataset (Socher et al., 2013) (1,067 positives and 1,143 negatives), and the SemEval-2017 Twitter dataset (Rosenthal et al., 2017) (2,339 positives 6990 Training Data In-domain SemEval-2017 SST-2 Yelp Amazon Static (50 gold) 88.60 1.11 77.28 9.11 79.29 5.14 91.53 2.06 89.63 1.65 Full (1,707 gold) 93.23 0.46 71.17 2.54 80.23 2.09 93.66 0.84 90.29 0.57 DP (Static + 350 auto) (400) 86.70 2.92 74.36 2.92 77.33 6.01 89.60 2.51 89.15 1.89 RR (Static + 350 auto) (400) 89.65 1.27 79.20 1.27 78.89 5.95 91.93 2.10 89.73 1.26 Our Methods Static + 150 auto (200) 90.08 1.25 78.88 6.67 79.40 3.28 92.19 1.51 89.81 1.73 Static + 350 auto (400) 90.16 0.85 80.54 2.81 81.26 1.97 93.03 1.08 90.09 1.79 Static + 550 auto (600) 90.04 1.50 80.69 3.42 81.23 1.83 92.10 3.07 89.67 1.27 Static + 750 auto (800) 90.08 1.01 80.55 3.96 80.75 2.30 92.36 1.87 90.18 1.44 Static + 950 auto (1000) 89.83 1.28 80.90 3.29 80.58 2.57 92.30 2.19 90.62 1.29 Static + 1150 auto (1200) 90.12 1.82 79.31 1.82 79.52 3.15 91.47 3.61 90.16 1.46 Table 3: Results on in-distribution and OOD data.&quot;,&quot;To address RQ1 , we compare the performance of models trained by the static semi-factual generation strategy with models trained with the original 50 examples, referred to as Static .&quot;,&quot;We also compare to a model trained with the full training set (1,707 labelled examples), referred to as Full .&quot;,&quot;To simulate the few-shot training scenario, we randomly sample 50 examples (we also forced a 50:50 class balance) from the IMDb dataset as training data.&quot;,&quot;For each experiment, the training is repeated 10 times with training datasets sampled by 10 different random seeds.&quot;,&quot;We report the average result of these 10 repetitions and use accuracy to measure the classification performance.&quot;,&quot;Our experiments rely on an off-the-shelf cased RoBERTa-base model implemented by Hugging Face * to either perform mask-filling to provide synonyms or as a predictive model.&quot;,&quot;Following Kaushik et al. (2020), we fine-tune RoBERTa for up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).&quot;,&quot;We also explore the impact of the number of semi-factual examples on model performance.&quot;,&quot;To this end, we conduct static semi-factual generation with a different number of augmented examples for each instance: {3, 7, 11, 15, 19, 23}.&quot;,&quot;Considering we have 50 original examples, this would result in {150, 350, 550, 750, 950, 1,150} additional examples in the training set, respectively (we call * https://huggingface.co/transformers/model_doc/roberta.html this Static+ n , where n is the number of generated semi-factuals).&quot;,&quot;We use the Adam optimizer (Kingma and Ba, 2014) with a batch size of 4.&quot;,&quot;We found that setting the learning rate to {5e-5, 5e-6 and 5e-6} could optimise Static, Static+ n , and Full, respectively.&quot;,&quot;As shown in Table 3, all static semi-factual generation (Static+ n ) methods can outperform the baseline method (Static) in both in-distribution and OOD tests, demonstrating the utility of static semifactual generation.&quot;,&quot;Among all Static+ n methods, Static+350 seems the best-performing method and exceeds Static with a 1.56% in-distribution improvement in average accuracy.&quot;,&quot;Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, and 0.46% OOD improvement in the SemEval-2017 , SST-2 , Yelp and Amazon datasets respectively.&quot;,&quot;Although the improvement on the Amazon dataset appears modest, given that there are 200,000 examples in the Amazon test set, this actually stands for nearly 1,000 documents being correctly classified.&quot;,&quot;The Static+ n methods can even outperform Full (i.e. normal training with the full training set) on the SemEval , SST-2 , and Amazon datasets and are comparable on the Yelp dataset.&quot;,&quot;The performance of models with the full training set is best on the in-distribution dataset but the worst on the SemEval dataset, which can be caused by the big difference between underlying distributions of these two datasets.&quot;,&quot;In other words, a model that fits well with one dataset can cause performance decay on others.&quot;,&quot;In this case, training with a smaller training set is more likely to reduce overfitting with the in-distribution dataset and fit well with the SemEval dataset, which explains the big improvement.&quot;,&quot;It is interesting to note that models trained with the en-6991 tire training set perform slightly better on the OOD Yelp dataset (93.66 0.84 ) than on the in-distribution dataset (93.23 0.46 ), which could also be explained by the high similarity between the underlying distributions of these two datasets.&quot;,&quot;First, we test whether the improvement in model performance is brought about by static semi-factual generation (Static+ n ) or simply by an increase in the size of the training set.&quot;,&quot;We compare Static+350 (due to its relatively good performance) with another baseline called Duplication ( DP heareafter).&quot;,&quot;We multiply the original training set (50 examples) up into 400 examples identical to the size of the training set of Static+350, and fine-tune RoBERTa on this dataset with the same hyperparameters as Static+350.&quot;,&quot;As shown in Table 3, in most cases, DP un-derperforms other algorithms and is even worse than Static, demonstrating that solely increasing the dataset size cannot improve the performance.&quot;,&quot;We believe that the duplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models.&quot;,&quot;Second, synonym replacement has been used previously for data augmentation (Wei and Zou, 2019), and we compare static semi-factual generation with simply replacing any words (i.e. both rationales and non-rationales).&quot;,&quot;Following Wei and Zou (2019), we replace 5% of words at random and set the training set size to 400 to ensure fair comparison (we use RoBERTa and the same hyperparameters of Static+350).&quot;,&quot;We call this Random Replacement ( RR hereafter).&quot;,&quot;As shown in Table 3, RR is slightly better than the baseline Static approach.&quot;,&quot;This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to the original data, introducing noise that helps prevent overfitting to some extent.&quot;,&quot;However, the magnitude of improvement of the Static+ n method is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals.&quot;,&quot;These observations show that the model trained with Static+ n does improve both in-distribution and OOD performance, and the improvement is actually derived from static semi-factual generation.&quot;,&quot;As shown in Table 3 and Figure 3, the performance gain of static semi-factual generation (Static+ n ) marginalises when augmented data is increased.&quot;,&quot;Using too much augmented data even hurts the Static+1150 performance.&quot;,&quot;This observation is consistent with existing work on data augmentation (Wei and Zou, 2019).&quot;,&quot;We believe one reason could be that the use of static augmented examples could also introduce new spurious patterns that degrade model performance, necessitating a method that exploits rationales without generating too many augmented examples.&quot;,&quot;Human-in-the-loop can address this issue by dynamically correcting the model.&quot;,&quot;To address RQ2 , we compare the performance of models trained by dynamic human-intervened correction with a popular few-shot human-in-the-loop learning framework, Active Learning, as well as two other state-of-the-art CAD-based methods (Kaushik et al., 2020; Yang et al., 2021).&quot;,&quot;Lastly, we provide an ablation study to examine the influence of different correction methods, as well as an analysis regarding model sensitivity to spurious patterns.&quot;,&quot;We build up an active learning procedure as a baseline based on the model trained with Static.&quot;,&quot;In particular, we select another 50 examples by Uncertainty Sampling (i.e. prediction scores for two classes in these examples were close) and add them into the training set (called AL hereafter).&quot;,&quot;The training set size of the baseline becomes 100.&quot;,&quot;The best performing static semi-factual generation method Static+350 is also listed as a baseline.&quot;,&quot;For fair comparison, we also use Uncertainty Sampling to select another 50 examples (i.e. 100 original examples in the training set now) for the proposed dynamic human-intervened correction in-6992 Baseline Methods In-domain SemEval-2017 SST-2 Yelp Amazon Static (50 gold) 88.60 1.11 77.28 9.11 79.29 5.14 91.53 2.06 89.63 1.65 Static + 350 auto (400) 90.16 0.85 80.54 2.81 81.26 1.97 93.03 1.08 90.09 1.79 AL (100 gold) 88.64 1.75 78.61 5.90 80.50 3.37 92.47 0.68 89.80 1.91 CAD-based Methods Manual CAD (3,414 gold) 92.70 0.53 69.98 3.99 80.30 2.03 91.87 1.09 90.48 1.09 Automatics CAD (1,707 gold+1,707 auto) 91.82 0.74 79.39 5.37 80.60 3.10 91.92 0.97 90.46 1.08 Our Dynamic Methods Dynamic (100 gold + 700 auto) 90.84 0.99 80.32 4.31 82.40 2.14 93.19 1.24 90.51 2.17 Dynamic-MR (100 gold + 700 auto) 91.06 1.21 79.04 4.92 82.24 2.59 93.03 1.92 90.22 2.74 Dynamic-FR (100 gold + 700 auto) 89.85 1.38 82.39 1.88 81.59 1.82 92.98 0.91 90.12 2.42 Table 4: Results on in-distribution and OOD data.&quot;,&quot;cluding both False Rationale Correction and Missing Rationale Correction (called Dynamic ).&quot;,&quot;For Dynamic, we control the number of augmented examples for each review to 7 (4 from Missing Rationale Correction and 3 from False Rationale Correction), resulting in 800 examples in the training set.&quot;,&quot;For Automatic CAD (Yang et al., 2021) and Manual CAD (Kaushik et al., 2020), we use the entire training set to produce counterfactuals to build up two challenging baselines (one counterfactual for one example, which is limited by the method), resulting in 3,414 examples in the training set.&quot;,&quot;To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction ( Dynamic-FR hereafter) and Missing Rationale Correction ( Dynamic-MR hereafter).&quot;,&quot;Again, experiments all rely on a RoBERTa model and all hyperparameters are identical to those described in Section 5.2.1, except for the learning rate of AL which is set to 1.25e-5 (we found this value optimised AL perfor-mance).&quot;,&quot;As shown in Table 4, both AL and Dynamic outperform Static in in-distribution and OOD datasets which makes sense, because we use Uncertainty Sampling to add new labelled data to minimise model uncertainty and increase model performance.&quot;,&quot;However, AL fails to compete with Static+350 even if more original data is added, which again demonstrates the utility of static semi-factual generation.&quot;,&quot;On the contrary, Dynamic does better than Static+350 with a 0.68% in-distribution improvement in average accuracy.&quot;,&quot;Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2 , Yelp and Amazon datasets, but no improvement for the SemEval Non-rationales Rationales Static 0.572 0.428 Dynamic 0.433 0.567 Table 5: Static versus Dynamic models on average sensitivity (normalised) to rationales and non-rationales for IMDb test samples.&quot;,&quot;dataset.&quot;,&quot;Finally, the performance of our methods is better that the state-of-the-art manual CAD method in few-shot learning scenarios on all OOD datasets.&quot;,&quot;Overall, these observations demonstrate that applying dynamic human-intervened correction (i.e. Missing Rationale Correction and False Rationale Correction) can further increase the robustness of a model on generalisation ability, effectively avoiding the improvement marginalisation caused by the increased volume of augmented data.&quot;,&quot;Missing Rationales vs. False Rationales We conduct an ablation study by examining the performance of Dynamic-MR and Dynamic-FR in Table 4.&quot;,&quot;Interestingly, Dynamic-FR is specifically good at improving model performance on the in-distribution and SemEval datasets while Dynamic-MR does a good job on the SST-2 dataset.&quot;,&quot;We believe that it is because Dynamic-MR biases the model to estimate an underlying distribution that is useful for SST-2 and in-distribution datasets, while Dynamic-FR biases the model to estimate a distribution similar to SemEval dataset.&quot;,&quot;The performance of Dynamic can be explained as a compromise of two correction methods.&quot;,&quot;Sensitivity to Spurious Patterns We conduct an analysis to explore whether the double-robust models are less sensitive to spurious patterns.&quot;,&quot;We compute models mean sensitivity to all rationales and non-rationales through SCD in the IMDb test set.&quot;,&quot;As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increase in the 6993 sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.&quot;,&quot;We proposed a rationale-centric human-in-the-loop framework, RDL, for better model generalisability in few-shot learning scenarios.&quot;,&quot;Experimental results show that our method can boost performance of deep neural networks in both in-distribution and OOD datasets and make models less sensitive to spurious patterns, enabling fast generalisation.&quot;,&quot;In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.&quot;,&quot;We honor the ACL Code of Ethics.&quot;,&quot;No private data or non-public information was used in this work.&quot;,&quot;All annotators have received labor fees corresponding to the amount of their annotated instances.&quot;,&quot;We acknowledge with thanks the discussion with Chenyang Lyu from Dublin City University, as well as the many others who have helped.&quot;,&quot;We would also like to thank anonymous reviewers for their insightful comments and suggestions to help improve the paper.&quot;,&quot;This publication has emanated from research conducted with the financial support of the Pioneer and \&quot;Leading Goose\&quot; R&amp;D Program of Zhejiang under Grant Number 2022SDXHDX0003 and Science Foundation Ireland (SFI) under Grant Number [12/RC/2289_P2].&quot;,&quot;Yue Zhang is the corresponding author.&quot;],&quot;string&quot;:&quot;[\n  \&quot;Rationale-Centric Framework for Human-in-the-loop Machine Learning\&quot;,\n  \&quot;{yanglinyi, zhangyue}@westlake.edu.cn Abstract\&quot;,\n  \&quot;We present a novel rationale-centric framework with human-in-the-loop  R ationales-centric D ouble-robustness L earning (RDL)  to boost model out-of-distribution performance in few-shot learning scenarios.\&quot;,\n  \&quot;By using static semi-factual generation and dynamic human-intervened correction, RDL exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation.\&quot;,\n  \&quot;Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests compared to many state-of-the-art benchmarksespecially for few-shot learning scenarios.\&quot;,\n  \&quot;We also perform extensive ablation studies to support in-depth analyses of each component in our framework.\&quot;,\n  \&quot;Recent work finds that natural artefacts (Guru-rangan et al., 2018) or spurious patterns (Keith et al., 2020; Srivastava et al., 2020) in datasets can cause sub-optimal model performance for neural networks.\&quot;,\n  \&quot;As shown in Figure 1, the bold phrases  100% bad  and  brain cell killing  are underlying causes for a negative sentiment prediction that most human readers would recognise.\&quot;,\n  \&quot;These are defined as rationales in this paper.\&quot;,\n  \&quot;The underlined phraseacting and plot has been incorrectly recognised as a causal term by the model used fort this example, and is referred to as a spurious pattern .\&quot;,\n  \&quot;Spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021), and are usually useless, or even harmful, at test time.\&quot;,\n  \&quot;This issue can be severe in few-shot learning (FSL) * These authors contributed equally to this work.\&quot;,\n  \&quot;scenarios.\&quot;,\n  \&quot;For instance, Kulesza et al. (2010) suggests that when a model is trained with a small subset of labelled data, it is prone to exploiting spurious patterns leading to poor generalisability that is evident in the performance decay in out-of-distribution (OOD) datasets.\&quot;,\n  \&quot;In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu and MacNamee, 2020; Lu et al., 2021).\&quot;,\n  \&quot;There is a strand of research addressing this scenario that seeks to improve model performance by introducing methods and resources for training models less sensitive to spurious patterns\&quot;,\n  \&quot;(Kaushik et al., 2020).\&quot;,\n  \&quot;Most of this work relies on generating counterfactual augmented data\&quot;,\n  \&quot;(CAD), either manually\&quot;,\n  \&quot;(Kaushik et al., 2021)\&quot;,\n  \&quot;or automatically\&quot;,\n  \&quot;(Feng et al., 2021; Qian et al., 2021; Yang et al., 2021, 2020a; Delaney et al., 2021).\&quot;,\n  \&quot;For example, Kaushik et al.\&quot;,\n  \&quot;(2020)\&quot;,\n  \&quot;proposed a human-in-the-loop framework where human annotators are required to make minimal changes to original movie reviews to produce sentiment-flipped counterfactual reviews, which enables models to learn useful associations between input texts and output labels\&quot;,\n  \&quot;(Kaushik et al., 2021).\&quot;,\n  \&quot;Generating manual counterfactuals, however, is expensive and time-consumingKaushik et al.\&quot;,\n  \&quot;(2020)\&quot;,\n  \&quot;report the cost of revising 2 .\&quot;,\n  \&quot;5 k instances at over $10,000.\&quot;,\n  \&quot;On the other hand, fully automatic methods are task-specific and therefore have weak robustness across domains and less reliabil-6986 Semi-factual Generation\&quot;,\n  \&quot;ity compared to manual counterfactuals.\&quot;,\n  \&quot;To address these issues, we propose R ationales-centric D ouble-robustness L earning\&quot;,\n  \&quot;(RDL), a human-in-the-loop framework for data augmentation in a few-shot setting, which is efficient, robust, model-agnostic, and general across tasks.\&quot;,\n  \&quot;Our main idea is a rationale-centric strategy for eliminating the effect of spurious patterns by leveraging human knowledge as shown in Figure\&quot;,\n  \&quot;2. Our double-robustness framework consists of two main modules.\&quot;,\n  \&quot;The first is a Static Semi-factual Generation module that generates a set of semifactual data automatically for a given instance by using human-identified rationales.\&quot;,\n  \&quot;Such labelling requires less human input compared to fully manual counterfactual generation\&quot;,\n  \&quot;(see Section 3.1).\&quot;,\n  \&quot;In contrast with counterfactuals\&quot;,\n  \&quot;(Roese, 1997)\&quot;,\n  \&quot;that rely on what might have been different\&quot;,\n  \&quot;(i.e. the label would be changed if certain terms have been changed), semi-factuals\&quot;,\n  \&quot;(McCloy and Byrne, 2002; Kenny and Keane, 2021), as used in our work, aim to guide a model to identify terms less causally related to the label\&quot;,\n  \&quot;(i.e. even if certain terms had been changed, the label would be kept the same).\&quot;,\n  \&quot;Second, we apply a Dynamic Human-intervened Correction module , where the most salient features are identified for model predictions over a set of training examples, and human workers intervene by checking the correctness of the rationale in case first-round modifications introduce new artefacts.\&quot;,\n  \&quot;We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power, both for in-distribution and OOD predictions.\&quot;,\n  \&quot;also used in Kaushik et al.\&quot;,\n  \&quot;(2020), demonstrate that the double-robust models can be less sensitive to spurious patterns.\&quot;,\n  \&quot;In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than fully-supervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.\&quot;,\n  \&quot;The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual\&quot;,\n  \&quot;(Kaushik et al., 2020)\&quot;,\n  \&quot;and automatic CAD\&quot;,\n  \&quot;(Yang et al., 2021)\&quot;,\n  \&quot;using the full augmented training set of 3,414 examples.\&quot;,\n  \&quot;To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals and human-intervention for improving the generalisation abilities of deep neural networks in few-shot learning scenarios.\&quot;,\n  \&quot;* 2 Related Work Data augmentation has been used for resolving artefacts in training datasets before\&quot;,\n  \&quot;(Gururangan et al., 2018; Srivastava et al., 2020; Kaushik et al., 2021).\&quot;,\n  \&quot;In particular, previous work\&quot;,\n  \&quot;(Kaushik et al., 2020)\&quot;,\n  \&quot;relied on large-scale crowd-sourcing to generate useful augmented data.\&quot;,\n  \&quot;More recently, Yang et al.\&quot;,\n  \&quot;(2021), and Wang and Culotta\&quot;,\n  \&quot;(2021)\&quot;,\n  \&quot;investigated the efficacy of the automatically generated counterfactuals for sentiment analysis.\&quot;,\n  \&quot;Similar to our work, these methods also consider the most salient features that a model uses when generating augmented data, which is in line with our rationale definition.\&quot;,\n  \&quot;However, they use sentiment lexicon matching for identifying rationales, which is task-specific and not necessarily fully relevant.\&quot;,\n  \&quot;In contrast, we employ human annotators to identify rationales, which can be task-agnostic and robust.\&quot;,\n  \&quot;Moreover, our method generates semi-factuals instead of counterfactuals used in previous work.\&quot;,\n  \&quot;Human-the-loop Machine Learning\&quot;,\n  \&quot;(Wu et al., 2021)\&quot;,\n  \&quot;has received increasing research attention.\&quot;,\n  \&quot;Active learning\&quot;,\n  \&quot;(Settles, 2009; Margatina et al., 2021), the most common example of human-in-the-loop machine learning, asks human annotators only to provide high-level annotations\&quot;,\n  \&quot;(i.e. labels)\&quot;,\n  \&quot;for important examples.\&quot;,\n  \&quot;There is also some work exploring more explainable AI systems by exploiting feature-based information.\&quot;,\n  \&quot;Such methods use relatively simple models such as Nave Bayes\&quot;,\n  \&quot;(Stumpf * All resources are available at https://github.com/GeorgeLuImmortal/RDL-Rationales-centric-Double-robustness-Learning/ 6987 et al., 2009; Kulesza et al., 2015)\&quot;,\n  \&quot;and Linear Regression with bag-of-words features\&quot;,\n  \&quot;(Jia and Liang, 2017; Teso and Kersting, 2019; Ghai et al., 2021; Shao et al., 2021), because these classifiers are relatively intuitive in generating explanations and amenable to incorporating human feedback.\&quot;,\n  \&quot;Some other work uses simple neural networks such as multi-layer perceptrons\&quot;,\n  \&quot;(Shao et al., 2021)\&quot;,\n  \&quot;and shallow CNNs\&quot;,\n  \&quot;(Lertvittayakumjorn et al., 2020; Stammer et al., 2021; Teso et al., 2021)\&quot;,\n  \&quot;because the predictions of such models can be explained in the form of features.\&quot;,\n  \&quot;Very recently, Yao et al.\&quot;,\n  \&quot;(2021)\&quot;,\n  \&quot;proposed a human-in-the-loop method to inspect more complicated models\&quot;,\n  \&quot;(e.g. BERT)\&quot;,\n  \&quot;with the help of model-agnostic post-hoc explanation algorithms\&quot;,\n  \&quot;(Ribeiro et al., 2018)\&quot;,\n  \&quot;that can explain predictions of any linear or non-linear model without exploiting its weights.\&quot;,\n  \&quot;However, previous work focuses on increasing the explainability of AI systems for high-stakes domains such as health and finance\&quot;,\n  \&quot;(Li et al., 2020; Yang et al., 2020b), instead of improving model robustness or generalisation ability.\&quot;,\n  \&quot;Also, they assume access to a large amount of labelled data.\&quot;,\n  \&quot;In contrast, we focus on few-shot learning scenarios which are more compelling.\&quot;,\n  \&quot;The RDL pipeline is shown in Figure 2 and consists of two modules: Static Semi-factual Generation and Dynamic Human-intervened Correction .\&quot;,\n  \&quot;Static semi-factual generation is a more efficient alternative to manually generated counterfactuals\&quot;,\n  \&quot;(Kaushik et al., 2020).\&quot;,\n  \&quot;In the first phase, Rationale Marking\&quot;,\n  \&quot;(Section 3.1), human annotators review each document in the training set to provide rationales\&quot;,\n  \&quot;(i.e. phrases that support the document classification decisions shown as bold text in Figure 2).\&quot;,\n  \&quot;The second phase is a semi-factual generation method based on synonym replacement\&quot;,\n  \&quot;(Section 3.2)\&quot;,\n  \&quot;that produces augmented examples\&quot;,\n  \&quot;(blue text in Figure 2 indicates replaced words), which are added into the training set.\&quot;,\n  \&quot;Dynamic human-intervened correction\&quot;,\n  \&quot;(Section 3.3)\&quot;,\n  \&quot;is a rationales-powered human-in-the-loop framework to dynamically correct the model's behaviours.\&quot;,\n  \&quot;At the outset, sampling and sensitivity of contextual decomposition\&quot;,\n  \&quot;(SCD)\&quot;,\n  \&quot;(Jin et al., 2019)\&quot;,\n  \&quot;is applied to detect the rationales given by the model that is obtained in the previous step.\&quot;,\n  \&quot;Then, all model-identified rationales\&quot;,\n  \&quot;(underlined texts in Figure 2)\&quot;,\n  \&quot;are examined by human annotators to identify false rationales\&quot;,\n  \&quot;(i.e. words or phrases that do not support the classifications but are falsely included by the model)\&quot;,\n  \&quot;and missing rationales\&quot;,\n  \&quot;(i.e. words or phrases that support the classifications but are not included by the model).\&quot;,\n  \&quot;Both false rationales and missing rationales are corrected to produce augmented examples.\&quot;,\n  \&quot;Finally, newly generated examples are added into the training set to re-train the deep learning model.\&quot;,\n  \&quot;Following Kaushik et al.\&quot;,\n  \&quot;(2020)\&quot;,\n  \&quot;and Yang et al.\&quot;,\n  \&quot;(2021), we use the IMDb movie review dataset\&quot;,\n  \&quot;(Maas et al., 2011)\&quot;,\n  \&quot;in our experiments.\&quot;,\n  \&quot;It consists of positive and negative movie reviews that are easy for human participants to understand, re-annotate, and provide feedback upon\&quot;,\n  \&quot;(Zaidan et al., 2007).\&quot;,\n  \&quot;We use a crowdsourcing company to recruit editors and annotators for marking rationales that support classification decisions.\&quot;,\n  \&quot;At the outset, annotators were given instructions and examples that gently guided them to annotate rationales.\&quot;,\n  \&quot;Only adjectives, adverbs, nouns, and verbs were considered as rationales.\&quot;,\n  \&quot;Besides, rationales were required to carry complete semantic information.\&quot;,\n  \&quot;For example, for a phrase starting with a negation word such as  not great , annotators are instructed to mark the whole phrase  not great  as a rationale instead of just marking  not .\&quot;,\n  \&quot;We also limited rationales to at most three consecutive words\&quot;,\n  \&quot;(i.e. unigrams, bigrams and trigrams).\&quot;,\n  \&quot;Phrases consisting of numerical scores are not counted as rationales\&quot;,\n  \&quot;(e.g. 5 or 10 stars)\&quot;,\n  \&quot;since different datasets may use different rating scales, and annotating digits may hurt OOD performance.\&quot;,\n  \&quot;Overall, we encouraged annotators to try their best to mark as many rationales as possible to explain classification labels.\&quot;,\n  \&quot;However, to guarantee the quality of rationale marking and prevent annotators from over including non-rationales for more payment, we also manually inspected annotated examples and rejected examples that contained incorrect rationales.\&quot;,\n  \&quot;After inspection, we rejected 10.6% of negative reviews and 7.6% of positive reviews.\&quot;,\n  \&quot;Editors and annotators re-annotated the rejected examples, which were then presented to us for another inspection.\&quot;,\n  \&quot;All re-annotated examples were approved only if all authors were happy with the quality of the annotations.\&quot;,\n  \&quot;Otherwise, the examples were re-annotated again.\&quot;,\n  \&quot;rationales in 855 movie reviews involved in Section 3.1 and 3.3\&quot;,\n  \&quot;(note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments).\&quot;,\n  \&quot;Human annotators spent on average 183.68 seconds to identify rationales in a review and our method generated semi-factual examples automatically.\&quot;,\n  \&quot;On the contrary, workers spent on average 300 seconds to revise a review to generate a counterfactual manually as reported by Kaushik et al.\&quot;,\n  \&quot;(2020).\&quot;,\n  \&quot;Note that our approach using 100 labelled examples can outperform manual CAD\&quot;,\n  \&quot;(Kaushik et al., 2020)\&quot;,\n  \&quot;using the entire training set of 1,707 examples\&quot;,\n  \&quot;(see Section 5.3), making our approach 300  1707 183 .\&quot;,\n  \&quot;68  100  27 .\&quot;,\n  \&quot;88 times more efficient than manually generated CAD.\&quot;,\n  \&quot;We take a simple replacement strategy, which has been taken by Yang et al.\&quot;,\n  \&quot;(2021), to generate semifactual examples.\&quot;,\n  \&quot;Given a human-identified rationale, our method constructs augmented examples by automatically replacing non-rationale words, thus leading to examples with the same labels.\&quot;,\n  \&quot;This augmentation is consistent with semi-factual thinking: even if those non-rationales were changed, the label would not change.\&quot;,\n  \&quot;Formally, given a training example x i = [ t i 1 , t i 2 , ..., t ij ]\&quot;,\n  \&quot;(where t ij is the j th token of the i th document)\&quot;,\n  \&quot;and its ground truth label y i , we create a rationale vector r i = [ a i 1 , a i 2 , ..., a ij ] where a ij is the value that indicates whether t ij is a rationale or not\&quot;,\n  \&quot;(we set a ij = 1 to indicate that t ij is a rationale and 0 otherwise).\&quot;,\n  \&quot;To generate a semi-factual example, x  i , we randomly replace a certain number of non-rationales\&quot;,\n  \&quot;(where a ij = 0 ), except for punctuation, with synonymous terms.\&quot;,\n  \&quot;The synonyms can be provided by a human, retrieved automatically from a lexicon such as WordNet\&quot;,\n  \&quot;(Miller, 1995), or generated using the mask-filling function of a pretrained context-aware language model\&quot;,\n  \&quot;(Liu et al., 2019).\&quot;,\n  \&quot;In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x  i , with some replaced non-rationales and all the other tokens identical to x i .\&quot;,\n  \&quot;The label, y i , of a newly generated example is the same as the label of the original example, x i .\&quot;,\n  \&quot;Examples of generated data are shown in Table\&quot;,\n  \&quot;1. Afterwards, the augmented examples are added into the training set used to train the model.\&quot;,\n  \&quot;Dynamic human-intervened correction further improves the robustness of the model by allowing human annotators to correct the model rationales online.\&quot;,\n  \&quot;Firstly, SCD is applied to detect unigrams, bigrams or trigrams that are salient to the model.\&quot;,\n  \&quot;SCD is a technique to assess the importance of terms by continuously removing terms and measuring changes in prediction\&quot;,\n  \&quot;(Jin et al., 2019).\&quot;,\n  \&quot;Human annotators examine all rationales given by the model from all documents to discover two types of incorrect rationale: false rationales and missing rationales.\&quot;,\n  \&quot;The next phase allows human feedback to influence the learning process.\&quot;,\n  \&quot;To this end, for each type of incorrect rationale, we propose a corresponding strategy to correct them.\&quot;,\n  \&quot;For false rationales\&quot;,\n  \&quot;(i.e. phrases that actually do not support classifications but are incorrectly identified by the model), we use synonym replacement again to generate semi-factual examples.\&quot;,\n  \&quot;Unlike the static semi-factual generation\&quot;,\n  \&quot;(Section 3.2), in this component we replace all false rationales with their synonyms instead of randomly replacing 5% of non-rationales in a document.\&quot;,\n  \&quot;Examples of generated data are shown in Table\&quot;,\n  \&quot;2. For missing rationales\&quot;,\n  \&quot;(i.e. phrases that actually support classifications but are not identified by the model), we take another simple semi-factual generation strategy, that is, extracting sentences that contain missing rationales to form semi-factual data.\&quot;,\n  \&quot;Specifically, given a sentence containing missing rationales, we use this sentence as a new example, and the label of this newly generated example is identical to that of the document where the sentence is extracted.\&quot;,\n  \&quot;For example, there is a positive movie review\&quot;,\n  \&quot;(bold font for rationales)\&quot;,\n  \&quot;Robert Urich was a fine actor, and he makes this TV movie believable . I remember watching this film when I was 15 .... .\&quot;,\n  \&quot;The model fails to identify  fine  and  believable  as rationales.\&quot;,\n  \&quot;Thus we extract the text Robert Urich was a fine actor, and he makes this TV movie believable . as a new example, and the class of this example is still positive.\&quot;,\n  \&quot;We extract the whole sentence rather than just the missing rationales to reserve more semantic information.\&quot;,\n  \&quot;Note that the two correction methods in dynamic human-intervened correction can operate in parallel and the generated examples are added to the small training set to re-train the model.\&quot;,\n  \&quot;Broadly speaking, our RDL framework takes advantage of invariance that makes a model less sensitive to non-rationale words or spurious patterns (Tu et al., 2020; Wang et al., 2021) in favour of focusing on useful mappings of rationales to labels.\&quot;,\n  \&quot;More specifically, by using static semi-factual generation (Section 3.2) and false rationale correction (Section 3.3), we expect to break spurious associations.\&quot;,\n  \&quot;For example, if a model incorrectly determines that  Soylent Green  is associated with positive sentiment (Table 2), the augmented examples that replace  Soylent Green  with other phrases such as  Gang Orange  break the spurious association.\&quot;,\n  \&quot;Besides, using synonym replacement can generate examples that are similar to the original one, which is equivalent to adding noisy data to prevent models from overfitting (Wei and Zou, 2019).\&quot;,\n  \&quot;Missing rationale correction (Section 3.3) emphasizes the ground truth associations between rationales and labels, enabling the model to better estimate the generally useful underlying distributions for OOD datasets, even in few-shot learning scenarios.\&quot;,\n  \&quot;In the next section, we present experiments and empirical evidence to demonstrate the utility of the proposed RDL framework in improving model robustness.\&quot;,\n  \&quot;Our intention is to improve the generalisability of models, and we use both in-distribution and OOD\&quot;,\n  \&quot;performance for evaluation.\&quot;,\n  \&quot;Our experiments are designed to address the following research questions:  RQ1 Can we use static semi-factual generation to achieve better in-distribution and OOD performance?\&quot;,\n  \&quot;RQ2 Does dynamic human-intervened correction improve generalisability of models?\&quot;,\n  \&quot;For fair comparison with previous work (Kaushik et al., 2020; Yang et al., 2021), we use the IMDb sentiment classification dataset (Maas et al., 2011) as the in-distribution dataset.\&quot;,\n  \&quot;Following Kaushik et al. (2020), all models were trained with the IMDb dataset predefined training, validation and test partitions containing 1 , 707 , 245 , and 488 reviews respectively and an enforced 50:50 class ratio.\&quot;,\n  \&quot;To measure the generalisation ability of different models, we focus on OOD performance.\&quot;,\n  \&quot;To this end, we test models on another four binary sentiment classification datasets: the sampled Amazon reviews dataset (Ni et al., 2019) (100,000 positives and 100,000 negatives) from six genres: beauty, fashion, appliances, gift cards, magazines, and software; the Yelp review dataset (Zhang et al., 2015) (19,000 positives and 19,000 negatives); the SST-2 dataset (Socher et al., 2013) (1,067 positives and 1,143 negatives), and the SemEval-2017 Twitter dataset (Rosenthal et al., 2017) (2,339 positives 6990 Training Data In-domain SemEval-2017 SST-2 Yelp Amazon Static (50 gold) 88.60 1.11 77.28 9.11 79.29 5.14 91.53 2.06 89.63 1.65 Full (1,707 gold) 93.23 0.46 71.17 2.54 80.23 2.09 93.66 0.84 90.29 0.57 DP (Static + 350 auto) (400) 86.70 2.92 74.36 2.92 77.33 6.01 89.60 2.51 89.15 1.89 RR (Static + 350 auto) (400) 89.65 1.27 79.20 1.27 78.89 5.95 91.93 2.10 89.73 1.26 Our Methods Static + 150 auto (200) 90.08 1.25 78.88 6.67 79.40 3.28 92.19 1.51 89.81 1.73 Static + 350 auto (400) 90.16 0.85 80.54 2.81 81.26 1.97 93.03 1.08 90.09 1.79 Static + 550 auto (600) 90.04 1.50 80.69 3.42 81.23 1.83 92.10 3.07 89.67 1.27 Static + 750 auto (800) 90.08 1.01 80.55 3.96 80.75 2.30 92.36 1.87 90.18 1.44 Static + 950 auto (1000) 89.83 1.28 80.90 3.29 80.58 2.57 92.30 2.19 90.62 1.29 Static + 1150 auto (1200) 90.12 1.82 79.31 1.82 79.52 3.15 91.47 3.61 90.16 1.46 Table 3: Results on in-distribution and OOD data.\&quot;,\n  \&quot;To address RQ1 , we compare the performance of models trained by the static semi-factual generation strategy with models trained with the original 50 examples, referred to as Static .\&quot;,\n  \&quot;We also compare to a model trained with the full training set (1,707 labelled examples), referred to as Full .\&quot;,\n  \&quot;To simulate the few-shot training scenario, we randomly sample 50 examples (we also forced a 50:50 class balance) from the IMDb dataset as training data.\&quot;,\n  \&quot;For each experiment, the training is repeated 10 times with training datasets sampled by 10 different random seeds.\&quot;,\n  \&quot;We report the average result of these 10 repetitions and use accuracy to measure the classification performance.\&quot;,\n  \&quot;Our experiments rely on an off-the-shelf cased RoBERTa-base model implemented by Hugging Face * to either perform mask-filling to provide synonyms or as a predictive model.\&quot;,\n  \&quot;Following Kaushik et al. (2020), we fine-tune RoBERTa for up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).\&quot;,\n  \&quot;We also explore the impact of the number of semi-factual examples on model performance.\&quot;,\n  \&quot;To this end, we conduct static semi-factual generation with a different number of augmented examples for each instance: {3, 7, 11, 15, 19, 23}.\&quot;,\n  \&quot;Considering we have 50 original examples, this would result in {150, 350, 550, 750, 950, 1,150} additional examples in the training set, respectively (we call * https://huggingface.co/transformers/model_doc/roberta.html this Static+ n , where n is the number of generated semi-factuals).\&quot;,\n  \&quot;We use the Adam optimizer (Kingma and Ba, 2014) with a batch size of 4.\&quot;,\n  \&quot;We found that setting the learning rate to {5e-5, 5e-6 and 5e-6} could optimise Static, Static+ n , and Full, respectively.\&quot;,\n  \&quot;As shown in Table 3, all static semi-factual generation (Static+ n ) methods can outperform the baseline method (Static) in both in-distribution and OOD tests, demonstrating the utility of static semifactual generation.\&quot;,\n  \&quot;Among all Static+ n methods, Static+350 seems the best-performing method and exceeds Static with a 1.56% in-distribution improvement in average accuracy.\&quot;,\n  \&quot;Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, and 0.46% OOD improvement in the SemEval-2017 , SST-2 , Yelp and Amazon datasets respectively.\&quot;,\n  \&quot;Although the improvement on the Amazon dataset appears modest, given that there are 200,000 examples in the Amazon test set, this actually stands for nearly 1,000 documents being correctly classified.\&quot;,\n  \&quot;The Static+ n methods can even outperform Full (i.e. normal training with the full training set) on the SemEval , SST-2 , and Amazon datasets and are comparable on the Yelp dataset.\&quot;,\n  \&quot;The performance of models with the full training set is best on the in-distribution dataset but the worst on the SemEval dataset, which can be caused by the big difference between underlying distributions of these two datasets.\&quot;,\n  \&quot;In other words, a model that fits well with one dataset can cause performance decay on others.\&quot;,\n  \&quot;In this case, training with a smaller training set is more likely to reduce overfitting with the in-distribution dataset and fit well with the SemEval dataset, which explains the big improvement.\&quot;,\n  \&quot;It is interesting to note that models trained with the en-6991 tire training set perform slightly better on the OOD Yelp dataset (93.66 0.84 ) than on the in-distribution dataset (93.23 0.46 ), which could also be explained by the high similarity between the underlying distributions of these two datasets.\&quot;,\n  \&quot;First, we test whether the improvement in model performance is brought about by static semi-factual generation (Static+ n ) or simply by an increase in the size of the training set.\&quot;,\n  \&quot;We compare Static+350 (due to its relatively good performance) with another baseline called Duplication ( DP heareafter).\&quot;,\n  \&quot;We multiply the original training set (50 examples) up into 400 examples identical to the size of the training set of Static+350, and fine-tune RoBERTa on this dataset with the same hyperparameters as Static+350.\&quot;,\n  \&quot;As shown in Table 3, in most cases, DP un-derperforms other algorithms and is even worse than Static, demonstrating that solely increasing the dataset size cannot improve the performance.\&quot;,\n  \&quot;We believe that the duplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models.\&quot;,\n  \&quot;Second, synonym replacement has been used previously for data augmentation (Wei and Zou, 2019), and we compare static semi-factual generation with simply replacing any words (i.e. both rationales and non-rationales).\&quot;,\n  \&quot;Following Wei and Zou (2019), we replace 5% of words at random and set the training set size to 400 to ensure fair comparison (we use RoBERTa and the same hyperparameters of Static+350).\&quot;,\n  \&quot;We call this Random Replacement ( RR hereafter).\&quot;,\n  \&quot;As shown in Table 3, RR is slightly better than the baseline Static approach.\&quot;,\n  \&quot;This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to the original data, introducing noise that helps prevent overfitting to some extent.\&quot;,\n  \&quot;However, the magnitude of improvement of the Static+ n method is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals.\&quot;,\n  \&quot;These observations show that the model trained with Static+ n does improve both in-distribution and OOD performance, and the improvement is actually derived from static semi-factual generation.\&quot;,\n  \&quot;As shown in Table 3 and Figure 3, the performance gain of static semi-factual generation (Static+ n ) marginalises when augmented data is increased.\&quot;,\n  \&quot;Using too much augmented data even hurts the Static+1150 performance.\&quot;,\n  \&quot;This observation is consistent with existing work on data augmentation (Wei and Zou, 2019).\&quot;,\n  \&quot;We believe one reason could be that the use of static augmented examples could also introduce new spurious patterns that degrade model performance, necessitating a method that exploits rationales without generating too many augmented examples.\&quot;,\n  \&quot;Human-in-the-loop can address this issue by dynamically correcting the model.\&quot;,\n  \&quot;To address RQ2 , we compare the performance of models trained by dynamic human-intervened correction with a popular few-shot human-in-the-loop learning framework, Active Learning, as well as two other state-of-the-art CAD-based methods (Kaushik et al., 2020; Yang et al., 2021).\&quot;,\n  \&quot;Lastly, we provide an ablation study to examine the influence of different correction methods, as well as an analysis regarding model sensitivity to spurious patterns.\&quot;,\n  \&quot;We build up an active learning procedure as a baseline based on the model trained with Static.\&quot;,\n  \&quot;In particular, we select another 50 examples by Uncertainty Sampling (i.e. prediction scores for two classes in these examples were close) and add them into the training set (called AL hereafter).\&quot;,\n  \&quot;The training set size of the baseline becomes 100.\&quot;,\n  \&quot;The best performing static semi-factual generation method Static+350 is also listed as a baseline.\&quot;,\n  \&quot;For fair comparison, we also use Uncertainty Sampling to select another 50 examples (i.e. 100 original examples in the training set now) for the proposed dynamic human-intervened correction in-6992 Baseline Methods In-domain SemEval-2017 SST-2 Yelp Amazon Static (50 gold) 88.60 1.11 77.28 9.11 79.29 5.14 91.53 2.06 89.63 1.65 Static + 350 auto (400) 90.16 0.85 80.54 2.81 81.26 1.97 93.03 1.08 90.09 1.79 AL (100 gold) 88.64 1.75 78.61 5.90 80.50 3.37 92.47 0.68 89.80 1.91 CAD-based Methods Manual CAD (3,414 gold) 92.70 0.53 69.98 3.99 80.30 2.03 91.87 1.09 90.48 1.09 Automatics CAD (1,707 gold+1,707 auto) 91.82 0.74 79.39 5.37 80.60 3.10 91.92 0.97 90.46 1.08 Our Dynamic Methods Dynamic (100 gold + 700 auto) 90.84 0.99 80.32 4.31 82.40 2.14 93.19 1.24 90.51 2.17 Dynamic-MR (100 gold + 700 auto) 91.06 1.21 79.04 4.92 82.24 2.59 93.03 1.92 90.22 2.74 Dynamic-FR (100 gold + 700 auto) 89.85 1.38 82.39 1.88 81.59 1.82 92.98 0.91 90.12 2.42 Table 4: Results on in-distribution and OOD data.\&quot;,\n  \&quot;cluding both False Rationale Correction and Missing Rationale Correction (called Dynamic ).\&quot;,\n  \&quot;For Dynamic, we control the number of augmented examples for each review to 7 (4 from Missing Rationale Correction and 3 from False Rationale Correction), resulting in 800 examples in the training set.\&quot;,\n  \&quot;For Automatic CAD (Yang et al., 2021) and Manual CAD (Kaushik et al., 2020), we use the entire training set to produce counterfactuals to build up two challenging baselines (one counterfactual for one example, which is limited by the method), resulting in 3,414 examples in the training set.\&quot;,\n  \&quot;To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction ( Dynamic-FR hereafter) and Missing Rationale Correction ( Dynamic-MR hereafter).\&quot;,\n  \&quot;Again, experiments all rely on a RoBERTa model and all hyperparameters are identical to those described in Section 5.2.1, except for the learning rate of AL which is set to 1.25e-5 (we found this value optimised AL perfor-mance).\&quot;,\n  \&quot;As shown in Table 4, both AL and Dynamic outperform Static in in-distribution and OOD datasets which makes sense, because we use Uncertainty Sampling to add new labelled data to minimise model uncertainty and increase model performance.\&quot;,\n  \&quot;However, AL fails to compete with Static+350 even if more original data is added, which again demonstrates the utility of static semi-factual generation.\&quot;,\n  \&quot;On the contrary, Dynamic does better than Static+350 with a 0.68% in-distribution improvement in average accuracy.\&quot;,\n  \&quot;Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2 , Yelp and Amazon datasets, but no improvement for the SemEval Non-rationales Rationales Static 0.572 0.428 Dynamic 0.433 0.567 Table 5: Static versus Dynamic models on average sensitivity (normalised) to rationales and non-rationales for IMDb test samples.\&quot;,\n  \&quot;dataset.\&quot;,\n  \&quot;Finally, the performance of our methods is better that the state-of-the-art manual CAD method in few-shot learning scenarios on all OOD datasets.\&quot;,\n  \&quot;Overall, these observations demonstrate that applying dynamic human-intervened correction (i.e. Missing Rationale Correction and False Rationale Correction) can further increase the robustness of a model on generalisation ability, effectively avoiding the improvement marginalisation caused by the increased volume of augmented data.\&quot;,\n  \&quot;Missing Rationales vs. False Rationales We conduct an ablation study by examining the performance of Dynamic-MR and Dynamic-FR in Table 4.\&quot;,\n  \&quot;Interestingly, Dynamic-FR is specifically good at improving model performance on the in-distribution and SemEval datasets while Dynamic-MR does a good job on the SST-2 dataset.\&quot;,\n  \&quot;We believe that it is because Dynamic-MR biases the model to estimate an underlying distribution that is useful for SST-2 and in-distribution datasets, while Dynamic-FR biases the model to estimate a distribution similar to SemEval dataset.\&quot;,\n  \&quot;The performance of Dynamic can be explained as a compromise of two correction methods.\&quot;,\n  \&quot;Sensitivity to Spurious Patterns We conduct an analysis to explore whether the double-robust models are less sensitive to spurious patterns.\&quot;,\n  \&quot;We compute models mean sensitivity to all rationales and non-rationales through SCD in the IMDb test set.\&quot;,\n  \&quot;As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increase in the 6993 sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.\&quot;,\n  \&quot;We proposed a rationale-centric human-in-the-loop framework, RDL, for better model generalisability in few-shot learning scenarios.\&quot;,\n  \&quot;Experimental results show that our method can boost performance of deep neural networks in both in-distribution and OOD datasets and make models less sensitive to spurious patterns, enabling fast generalisation.\&quot;,\n  \&quot;In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.\&quot;,\n  \&quot;We honor the ACL Code of Ethics.\&quot;,\n  \&quot;No private data or non-public information was used in this work.\&quot;,\n  \&quot;All annotators have received labor fees corresponding to the amount of their annotated instances.\&quot;,\n  \&quot;We acknowledge with thanks the discussion with Chenyang Lyu from Dublin City University, as well as the many others who have helped.\&quot;,\n  \&quot;We would also like to thank anonymous reviewers for their insightful comments and suggestions to help improve the paper.\&quot;,\n  \&quot;This publication has emanated from research conducted with the financial support of the Pioneer and \\\&quot;Leading Goose\\\&quot; R&amp;D Program of Zhejiang under Grant Number 2022SDXHDX0003 and Science Foundation Ireland (SFI) under Grant Number [12/RC/2289_P2].\&quot;,\n  \&quot;Yue Zhang is the corresponding author.\&quot;\n]&quot;},&quot;labels&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;other&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;abstain&quot;,&quot;objective&quot;,&quot;result&quot;,&quot;result&quot;,&quot;abstain&quot;,&quot;method&quot;,&quot;abstain&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;,&quot;other&quot;],&quot;string&quot;:&quot;[\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;other\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;objective\&quot;,\n  \&quot;result\&quot;,\n  \&quot;result\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;method\&quot;,\n  \&quot;abstain\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;,\n  \&quot;other\&quot;\n]&quot;}}},{&quot;rowIdx&quot;:6,&quot;cells&quot;:{&quot;sentences&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[\&quot;Simile interpretation is a crucial task in natural language processing.\&quot;,\&quot;Nowadays, pre-trained la&quot;},&quot;labels&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[\&quot;abstain\&quot;,\&quot;abstain\&quot;,\&quot;abstain\&quot;,\&quot;objective\&quot;,\&quot;method\&quot;,\&quot;result\&quot;,\&quot;objective\&quot;,\&quot;result\&quot;,\&quot;other\&quot;,\&quot;abstain\&quot;,&quot;}}},{&quot;rowIdx&quot;:7,&quot;cells&quot;:{&quot;sentences&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[\&quot;Memory augmented encoder-decoder framework has achieved promising progress for natural language ge&quot;},&quot;labels&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[\&quot;abstain\&quot;,\&quot;abstain\&quot;,\&quot;abstain\&quot;,\&quot;objective\&quot;,\&quot;method\&quot;,\&quot;objective\&quot;,\&quot;abstain\&quot;,\&quot;abstain\&quot;,\&quot;abstain\&quot;,\&quot;absta&quot;}}},{&quot;rowIdx&quot;:8,&quot;cells&quot;:{&quot;sentences&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[\&quot;Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding ca&quot;},&quot;labels&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[\&quot;abstain\&quot;,\&quot;abstain\&quot;,\&quot;objective\&quot;,\&quot;objective\&quot;,\&quot;result\&quot;,\&quot;abstain\&quot;,\&quot;abstain\&quot;,\&quot;abstain\&quot;,\&quot;abstain\&quot;,\&quot;absta&quot;}}},{&quot;rowIdx&quot;:9,&quot;cells&quot;:{&quot;sentences&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[\&quot;Abstract\&quot;,\&quot;This paper addresses the problem of dialogue reasoning with contextualized commonsense &quot;},&quot;labels&quot;:{&quot;kind&quot;:&quot;truncated&quot;,&quot;value&quot;:&quot;[\&quot;abstain\&quot;,\&quot;abstain\&quot;,\&quot;objective\&quot;,\&quot;abstain\&quot;,\&quot;objective\&quot;,\&quot;result\&quot;,\&quot;objective\&quot;,\&quot;abstain\&quot;,\&quot;abstain\&quot;,\&quot;abs&quot;}}}],&quot;truncated&quot;:true},&quot;paginationData&quot;:{&quot;pageIndex&quot;:0,&quot;numItemsPerPage&quot;:100,&quot;numTotalItems&quot;:3051,&quot;offset&quot;:0,&quot;length&quot;:100}},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sImlhdCI6MTc0MjkyMzA1NCwic3ViIjoiL2RhdGFzZXRzL3NjaW0vbmFhY2xfZGF0YV9wcm9nIiwiZXhwIjoxNzQyOTI2NjU0LCJpc3MiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvIn0.Ka0w5mBlccNOxNwOnqmdke4esP007em8DAD4dX6AgNvDP05Ua9EN6MTArplPAe3OFJa3-tJXC-JiaAJ6iBpWAA&quot;,&quot;displayUrls&quot;:true},&quot;dataset&quot;:&quot;scim/naacl_data_prog&quot;,&quot;isGated&quot;:false,&quot;isPrivate&quot;:false,&quot;hasParquetFormat&quot;:true,&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://www.gravatar.com/avatar/b044ed6f48fb9b7eb557ea402e28e44a?d=retro&amp;size=100&quot;,&quot;fullname&quot;:&quot;SCIM Project&quot;,&quot;name&quot;:&quot;scim&quot;,&quot;type&quot;:&quot;org&quot;,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;isEnterprise&quot;:false,&quot;followerCount&quot;:3},&quot;compact&quot;:true}"><div class="flex flex-col overflow-hidden shadow-xs mx-auto mb-10 rounded-lg border pt-2  px-2.5"><div class="mb-2 flex flex-wrap items-center gap-2"><div class="mr-auto flex items-center"><svg class="mr-1 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
	<div class="whitespace-nowrap font-semibold">Dataset Viewer</div>
	</div>
				<a href="/datasets/scim/naacl_data_prog/tree/refs%2Fconvert%2Fparquet/default" class="group mr-1 text-xs text-gray-400 max-sm:hidden"><svg class="text-[.6rem] mr-1 inline -translate-y-px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill="currentColor" d="M12 10H6.78A11 11 0 0 1 27 16h2A13 13 0 0 0 6 7.68V4H4v8h8zm8 12h5.22A11 11 0 0 1 5 16H3a13 13 0 0 0 23 8.32V28h2v-8h-8z"></path></svg>
						<span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Auto-converted</span> to Parquet
					</a>
				<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>API</button>
					<button class="btn shadow-xs flex cursor-pointer items-center rounded-sm border px-1 py-0.5 text-xs font-normal text-gray-700 hover:text-gray-800 hover:shadow-inner dark:hover:text-gray-200"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path d="M9.80603 2.86737H3.56107C3.37704 2.86737 3.20055 2.94048 3.07042 3.0706C2.94029 3.20073 2.86719 3.37723 2.86719 3.56126V9.80622C2.86719 9.99025 2.94029 10.1667 3.07042 10.2969C3.20055 10.427 3.37704 10.5001 3.56107 10.5001H9.80603C9.99006 10.5001 10.1666 10.427 10.2967 10.2969C10.4268 10.1667 10.4999 9.99025 10.4999 9.80622V3.56126C10.4999 3.37723 10.4268 3.20073 10.2967 3.0706C10.1666 2.94048 9.99006 2.86737 9.80603 2.86737Z" fill="currentColor" fill-opacity="0.3"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M2.40942 1.66191C2.05175 1.66191 1.7618 1.95186 1.7618 2.30953V6.76191H1.43799V2.30953C1.43799 1.77303 1.87291 1.3381 2.40942 1.3381H6.45704V1.66191H2.40942Z" fill="currentColor"></path></svg>Embed</button>

					<button class="bg-linear-to-b shadow-xs flex items-center gap-1.5 rounded-full border from-white to-red-100/90 px-2 py-0.5 text-xs font-medium text-[#2D3648] transition-shadow hover:shadow-inner dark:from-gray-900 dark:to-red-800/30 dark:text-gray-100 dark:hover:shadow-inner dark:hover:shadow-red-800/30" ><svg class="h-3.5 w-3.5 text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>
						<span>Data Studio</span></button></div>
		<div class="flex flex-1 flex-col overflow-hidden -mx-2.5"><div class="flex flex-1 flex-col overflow-hidden"><div class="flex min-h-0 flex-1"><div class="flex flex-1 flex-col overflow-hidden"><div class="md:-mx-2.5 flex min-w-0 flex-wrap border-t"><div class="flex min-w-0 flex-1 flex-wrap"><div class="grid flex-1 grid-cols-1 overflow-hidden text-sm md:grid-cols-2 md:place-content-center sm:mx-2.5"><label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-950 md:border-r md:border-r-0 hidden" title="default"><span class="text-gray-500">Subset (1)</span>
			<div class="flex items-center whitespace-nowrap"><span class="truncate">default</span>
				<span class="mx-2 text-gray-500">Â·</span>
					<span class="text-gray-500">3.05k rows</span>
				<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
			<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Subset (1)"><option value="default" selected>default (3.05k rows)</option></optgroup></select></label>
		<label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-900 md:border-r md:border-r" title="train"><div class="text-gray-500">Split (1)</div>
				<div class="flex items-center overflow-hidden whitespace-nowrap"><span class="truncate">train</span>
					<span class="mx-2 text-gray-500">Â·</span>
						<span class="text-gray-500">3.05k rows</span>
					<svg class="ml-auto min-w-6 pl-2" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div>
				<select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Split (1)"><option value="train" selected>train (3.05k rows)</option></optgroup></select></label></div></div>
								</div>

							<div class="flex min-h-0 flex-1 flex-col ">
	<div class="bg-linear-to-r text-smd relative flex items-center dark:border-gray-900 dark:bg-gray-950 false border-t [&amp;:has(:focus)]:from-gray-50 [&amp;:has(:focus)]:to-transparent [&amp;:has(:focus)]:to-20% dark:[&amp;:has(:focus)]:from-gray-900"><form class="flex-1"><svg class="absolute left-3 top-1/2 transform -translate-y-1/2 pointer-events-none text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				<input disabled class="outline-hidden h-9 w-full border-none bg-transparent px-1 pl-9 pr-3 placeholder:text-gray-400 " placeholder="Search this dataset" dir="auto"></form>
			<div class="flex items-center gap-2 px-2 py-1"><button type="button" class="hover:bg-yellow-200/70 flex items-center gap-1 rounded-md border border-yellow-200 bg-yellow-100 pl-0.5 pr-1 text-[.8rem] leading-normal text-gray-700 dark:border-orange-500/25 dark:bg-orange-500/20 dark:text-gray-300 dark:hover:brightness-110 hidden"><div class="rounded-sm bg-yellow-300 px-1 font-mono text-[.7rem] font-bold text-black dark:bg-yellow-700 dark:text-gray-200">SQL
	</div>
	Console
</button></div></div>


<div class="flex flex-1 flex-col overflow-hidden min-h-64 border-t">
		

<div class="max-h-96 relative overflow-auto"><table class="w-full table-auto rounded-lg font-mono text-xs text-gray-900"><thead class="shadow-xs sticky left-0 right-0 top-0 z-1 bg-white align-top"><tr class="space-y-54 h-full min-w-fit divide-x border-b text-left"><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">sentences
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>sequence</span></div></div>

		<div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th><th class="h-full max-w-sm p-2 text-left  relative w-auto"><div class="flex h-full flex-col flex-nowrap justify-between"><div><div class="flex items-center justify-between">labels
				<form class="flex flex-col"><button id="asc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="-rotate-180 transform text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button>
						<button id="desc" class="-mr-1 ml-2 h-[0.4rem] w-[0.8rem] transition ease-in-out"><svg class="text-gray-300 hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 64 256 128" fill="currentColor" aria-hidden="true"><path d="M213.65674,101.657l-80,79.99976a7.99945,7.99945,0,0,1-11.31348,0l-80-79.99976A8,8,0,0,1,48,88H208a8,8,0,0,1,5.65674,13.657Z"></path></svg></button></form></div>

			<div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>sequence</span></div></div>

		<div></div></div>
	<div class="absolute right-0 top-0 z-10 h-full w-1 cursor-col-resize hover:bg-indigo-100 active:bg-indigo-500 dark:hover:bg-indigo-800 dark:active:bg-indigo-600/80"><div class="absolute right-0 top-0 h-full w-1"></div>
								</div>
						</th></tr></thead>
			<tbody class="h-16 overflow-scroll"><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="0"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios.",
  "We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets.",
  "The biases are specified in terms of one or more bias-only models , which learn to leverage the dataset biases.",
  "During training, the bias-only models' predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing training on the hard examples.",
  "We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data.",
  "Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets.",
  "Our code and data are publicly available in https: //github.com/rabeehk/robust-nli .",
  "Recent neural models (Devlin et al., 2019; Radford et al., 2018; Chen et al., 2017) have achieved high and even near human-performance on several large-scale natural language understanding benchmarks.",
  "However, it has been demonstrated that neural models tend to rely on existing idiosyncratic biases in the datasets, and leverage superficial correlations between the label and existing shortcuts in the training dataset to perform surprisingly well, 1 without learning the underlying task (Kaushik and Lipton, 2018; Gururangan et al., 2018; Poliak et al., 2018; Schuster et al., 2019; 1 We use biases, heuristics or shortcuts interchangeably. McCoy et al., 2019b).",
  "For instance, natural language inference (NLI) is supposed to test the ability of a model to determine whether a hypothesis sentence ( There is no teacher in the room ) can be inferred from a premise sentence ( Kids work at computers with a teacher's help ) (Dagan et al., 2006).",
  "2 However, recent work has demonstrated that large-scale NLI benchmarks contain annotation artifacts; certain words in the hypothesis that are highly indicative of inference class and allow models that do not consider the premise to perform unexpectedly well (Poliak et al., 2018; Gururangan et al., 2018).",
  "As an example, in some NLI benchmarks, negation words such as nobody, no, and not in the hypothesis are often highly correlated with the contradiction label.",
  "As a result of the existence of such biases, models exploiting statistical shortcuts during training often perform poorly on out-of-domain datasets, especially if the datasets are carefully designed to limit the spurious cues.",
  "To allow proper evaluation, recent studies have tried to create new evaluation datasets that do not contain such biases (Gururangan et al., 2018; Schuster et al., 2019; McCoy et al., 2019b).",
  "Unfortunately, it is hard to avoid spurious statistical cues in the construction of large-scale benchmarks, and collecting new datasets is costly (Sharma et al., 2018).",
  "It is, therefore, crucial to develop techniques to reduce the reliance on biases during the training of the neural models.",
  "We propose two end-to-end debiasing techniques that can be used when the existing bias patterns are identified.",
  "These methods work by adjusting the cross-entropy loss to reduce the biases learned from the training dataset, down-weighting the biased examples so that the model focuses on learning the hard examples.",
  "Figure 1 illustrates an example of applying our strategy to prevent an NLI model from predicting the labels using existing biases in the hypotheses, where the bias-only model only sees the hypothesis.",
  "Our strat-2 The given sentences are in the contradictory relation, and the hypothesis cannot be inferred from the premise.",
  "egy involves adding this bias-only branch f B on top of the base model f M during training.",
  "We then compute the combination of the two models f C in a way that motivates the base model to learn different strategies than the ones used by the bias-only branch f B .",
  "At the end of the training, we remove the bias-only classifier and use the predictions of the base model.",
  "In our first proposed method, Product of Experts, the training loss is computed on an ensemble of the base model and the bias-only model, which reduces the base model's loss for the examples that the bias-only model classifies correctly.",
  "For the second method, Debiased Focal Loss, the bias-only predictions are used to directly weight the loss of the base model, explicitly modulating the loss depending on the accuracy of the bias-only model.",
  "We also extend these methods to be robust against multiple sources of bias by training multiple bias-only models.",
  "Our approaches are simple and highly effective.",
  "They require training only a simple model on top of the base model.",
  "They are model agnostic and general enough to be applicable for addressing common biases seen in many datasets in different domains.",
  "We evaluate our models on challenging benchmarks in textual entailment and fact verification, including HANS (Heuristic Analysis for NLI Systems) (McCoy et al., 2019b), hard NLI sets (Gururangan et al., 2018) of Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and MultiNLI (MNLI) (Williams et al., 2018), and FEVER Symmetric test set (Schuster et al., 2019).",
  "The selected datasets are highly challenging and have been carefully designed to be unbiased to allow proper evaluation of the out-of-domain performance of the models.",
  "We additionally construct hard MNLI datasets from MNLI development sets to facilitate the out-of-domain evaluation on this dataset.",
  "3 We show that including our strategies on training baseline models, including BERT (Devlin et al., 2019), provides a substantial gain on out-of-domain performance in all the experiments.",
  "In summary, we make the following contributions:",
  "1) Proposing two debiasing strategies to train neural models robust to dataset bias.",
  "2) An empirical evaluation of the methods on two large-scale NLI datasets and a fact verification benchmark; obtaining a substantial gain on their challenging out-of-domain data, including 7.4 points on HANS, 4.8 points on SNLI hard set, and 9.8 points on FEVER symmetric test set, setting a new state-of-the-art.",
  "3) Proposing debiasing strategies capable of combating multiple sources of bias.",
  "4) Evaluating the transfer performance of the debiased models on 12 NLI datasets and demonstrating improved transfer to other NLI benchmarks.",
  "To facilitate future work, we release our datasets and code.",
  "To address dataset biases, researchers have proposed to augment datasets by balancing the existing cues (Schuster et al., 2019) or to create an adversarial dataset (Jia and Liang, 2017).",
  "However, collecting new datasets, especially at a large scale, is costly, and thus remains an unsatisfactory solution.",
  "It is, therefore, crucial to develop strategies to allow models to be trained on the existing biased datasets.",
  "3 Removing the need to submit to an online evaluation system for MNLI hard test sets.",
  "Schuster et al. (2019) propose to first compute the n-grams in the dataset's claims that are the most associated with each fact-verification label.",
  "They then solve an optimization problem to assign a balancing weight to each training sample to alleviate the biases.",
  "In contrast, we propose several end-to-end debiasing strategies.",
  "Additionally, Belinkov et al. (2019a) propose adversarial techniques to remove from the NLI sentence encoder the features that allow a hypothesis-only model to succeed.",
  "However, we believe that in general, the features used by the hypothesis-only model can include some information necessary to perform the NLI task, and removing such information from the sentence representation can hurt the performance of the full model.",
  "Their approach consequently degrades the performance on the hard SNLI set, which is expected to be less biased.",
  "In contrast, we propose to train a bias-only model to use its predictions to dynamically adapt the classification loss to reduce the importance of the most biased examples.",
  "Concurrently to our work, Clark et al. (2019) and He et al. (2019) have also proposed to use the product of experts (PoE) models for avoiding biases.",
  "They train their models in two stages, first training a bias-only model and then using it to train a robust model.",
  "In contrast, our methods are trained in an end-to-end manner, which is convenient in practice.",
  "We additionally show that our proposed Debiased Focal Loss model is an effective method to reduce biases, sometimes superior to PoE.",
  "We have evaluated on new domains of NLI hard sets and fact verification.",
  "Moreover, we have included an analysis showing that our debiased models indeed have lower correlations with the bias-only models, and have extended our methods to guard against multiple bias patterns simultaneously.",
  "We furthermore study transfer performance to other NLI datasets.",
  "Problem formulation We consider a general multi-class classification problem.",
  "Given a dataset D = { x i ,y i } Ni =1 consisting of the input data x i  X , and labels y i  Y , the goal of the base model is to learn a mapping f M parameterized by  M that computes the predictions over the label space given the input data, shown as f M : X  R |Y| .",
  "Our goal is to optimize  M parameters such that we build a model that is more resistant to benchmark dataset biases, to improve its robustness to domain changes where the biases typically observed in the training data do not exist in the evaluation dataset.",
  "The key idea of our approach, depicted in Figure 1, is first to identify the dataset biases that the base model is susceptible to relying on, and define a bias-only model to capture them.",
  "We then propose two strategies to incorporate this bias-only knowledge into the training of the base model to make it robust against the biases.",
  "After training, we remove the bias-only model and use the predictions of the base model.",
  "We assume that we do not have access to any data from the out-of-domain dataset, so we need to know a priori about the possible types of shortcuts we would like the base model to avoid relying on.",
  "Once these patterns are identified, we train a bias-only model designed to capture the identified shortcuts that only uses biased features .",
  "For instance, a hypothesis-only model in the large-scale NLI datasets can correctly classify the majority of samples using annotation artifacts (Poliak et al., 2018; Gururangan et al., 2018).",
  "Motivated by this work, our bias-only model for NLI only uses hypothesis sentences.",
  "Note that the bias-only model can, in general, have any form, and is not limited to models using only a part of the input data.",
  "For instance, on the HANS dataset, our bias-only model makes use of syntactic heuristics and similarity features (see Section 4.3).",
  "Let x bi  X b be biased features of x i that are predictive of y i .",
  "We then formalize this bias-only model as a mapping f B : X b  R |Y| , parameterized by  B and trained using cross-entropy (CE) loss LB : LB (  B )=  1 NN (cid:88) i =1 log(  ( f y i B ( x bi ;  B ))) , (1) where f jB ( x bi , B ) is the j th element of f B ( . ) , and  ( u j )= e u j / (cid:80) |Y| k =1 e u k is the softmax function.",
  "We propose two strategies to incorporate the bias-only f B knowledge into the training of the base model f M .",
  "In our strategies, the predictions of the bias-only model are combined with either the predictions of the base model or its error, to down-weight the loss for the examples that the bias-only model can predict correctly.",
  "We then update parameters of the base model  M based on this modified loss LC .",
  "Our learning strategies are end-to-end.",
  "Therefore, to prevent the base model from learning the biases, the bias-only loss LB is not back-propagated to any shared parameters of the base model, such as a shared sentence encoder.",
  "Our first approach is based on the product of experts (PoE) method (Hinton, 2002).",
  "Here, we use this method to combine the bias-only and base model's predictions by computing the element-wise product (cid:12) between their predictions as  ( f B ( x bi )) (cid:12)  ( f M ( x i )) .",
  "We compute this combination in the logarithmic space, making it appropriate for the normalized exponential below: f C ( x i , x bi )=log(  ( f B ( x bi )))+log(  ( f M ( x i ))) , The key intuition behind this model is to combine the probability distributions of the bias-only and the base model to allow them to make predictions based on different characteristics of the input; the bias-only branch covers prediction based on biases, and the base model focuses on learning the actual task.",
  "Then the base model parameters  M are trained using the cross-entropy loss LC of the combined classifier f C : LC (  M ;  B )=  1 NN (cid:88) i =1 log(  ( f y i C ( x i , x bi ))) .",
  "the updates for examples that it can accurately predict.",
  "Justification: Probability of label y i for the example x i in the PoE model is computed as:  ( f y i C ( x i , x bi ))=  ( f y i B ( x bi ))  ( f y i M ( x i )) (cid:80) |Y| k =1  ( f kB ( x bi ))  ( f kM ( x i )) Then the gradient of cross-entropy loss of the combined classifier (2) w.r.t  M is (Hinton, 2002):   MLC (  M ;  B )=  1 NN (cid:88) i =1 |Y| (cid:88) k =1 (cid:20) (cid:16)  y i k   ( f kC ( x i , x bi )) (cid:17)   M log(  ( f kM ( x i ))) (cid:21) , where  y i k is 1 when k = y i and 0 otherwise.",
  "Generally, the closer the ensemble's prediction  ( f kC ( . )) is to the target  y i k , the more the gradient is decreased through the modulating term, which only happens when the bias-only and base models are both capturing biases.",
  "In the extreme case, when the bias-only model correctly classifies the sample,  ( f y i C ( x i , x bi )) = 1 and therefore   MLC (  M ;  B ) = 0 , the biased examples are ignored during training.",
  "Conversely, when the example is fully unbiased, the bias-only classifier predicts the uniform distribution over all labels  ( f kB ( x bi )) = 1 |Y| for k  Y , therefore  ( f y i C ( x i , x bi )) =  ( f y i M ( x i )) and the gradient of ensemble classifier remains the same as the CE loss.",
  "Focal loss was originally proposed in Lin et al. (2017) to improve a single classifier by down-weighting the well-classified points.",
  "We propose a novel variant of this loss that leverages the bias-only branch's predictions to reduce the relative importance of the most biased examples and allows the model to focus on learning the hard examples.",
  "We define Debiased Focal Loss (DFL) as: LC (  M ;  B )= (3)  1 NN (cid:88) i =1 (cid:16) 1   ( f y i B ( x bi )) (cid:17)  log(  ( f y i M ( x i ))) where  is the focusing parameter, which impacts the down-weighting rate.",
  "When  is set to 0, DFL is equivalent to the cross-entropy loss.",
  "For  > 0 , as the value of  is increased, the effect of down-weighting is increased.",
  "We set  =2 through all experiments, which works well in practice, and avoid fine-tuning it further.",
  "We note the properties of this loss: (1) When the example x i is unbiased, and the bias-only branch does not do well,  ( f y i B ( x bi )) is small, therefore the scaling factor is close to 1 , and the loss remains unaffected.",
  "(2) As the sample is more biased and  ( f y i B ( x bi )) is closer to 1, the modulating factor approaches 0 and the loss for the most biased examples is down-weighted.",
  "We compare our models to RUBi (Cadene et al., 2019), a recently proposed model to alleviate unimodal biases learned by Visual Question Answering (VQA) models.",
  "Cadene et al. (2019)'s study is limited to VQA datasets.",
  "We, however, evaluate the effectiveness of their formulation on multiple challenging NLU benchmarks.",
  "RUBi consists in first applying a sigmoid function  to the bias-only model's predictions to obtain a mask containing an importance weight between 0 and 1 for each label.",
  "It then computes the element-wise product between the obtained mask and the base model's predictions: f C ( x i , x bi )= f M ( x i ) (cid:12)  ( f B ( x bi )) , The main intuition is to dynamically adjust the predictions of the base model to prevent it from leveraging the shortcuts.",
  "Then the parameters of the base model  M are updated by back-propagating the cross-entropy loss LC of the combined classifier.",
  "Neural models can, in practice, be prone to multiple types of biases in the datasets.",
  "We, therefore, propose methods for combining several bias-only models.",
  "To avoid learning relations between biased features, we do not consider training a classifier on top of their concatenation.",
  "Instead, let { x b j i } Kj =1 be different sets of biased features of x i that are predictive of y i , and let f B j be an individual bias-only model capturing x b j i .",
  "Next, we extend our debiasing strategies to handle multiple bias patterns.",
  "Method 1: Joint Product of Experts We extend our proposed PoE model to multiple bias-only models by computing the element-wise product between the predictions of bias-only models and the base model as:  ( f B 1 ( x b 1 i )) (cid:12)(cid:12)  ( f BK ( x b K i )) (cid:12)  ( f M ( x i )) , computed in the logarithmic space: f C ( x i , { x b j i } Kj =1 )= K (cid:88) j =1 log(  ( f B j ( x b j i ))) +log(  ( f M ( x i ))) .",
  "Then the base model parameters  M are trained using the cross-entropy loss of the combined classifier f C .",
  "Method 2: Joint Debiased Focal Loss To extend DFL to handle multiple bias patterns, we first compute the element-wise average of the predictions of the multiple bias-only models: f B ( { x b j i } Kj =1 ) = 1 K (cid:80) Kj =1 f B j ( x b j i ) , and then compute the DFL (3) using the computed joint bias-only model.",
  "We provide experiments on a fact verification (FEVER) and two large-scale NLI datasets (SNLI and MNLI).",
  "We evaluate the models' performance on recently-proposed challenging unbiased evaluation sets.",
  "We use the BERT (Devlin et al., 2019) implementation of Wolf et al. (2019) as our main baseline, known to work well for these tasks.",
  "In all the experiments, we use the default hyperparameters of the baselines.",
  "Dataset: The FEVER dataset contains claim-evidence pairs generated from Wikipedia.",
  "Schuster et al. (2019) collected a new evaluation set for the FEVER dataset to avoid the idiosyncrasies observed in the claims of this benchmark.",
  "They made the original claim-evidence pairs of the FEVER evaluation dataset symmetric, by augmenting them and making each claim and evidence appear with each label.",
  "Therefore, by balancing the artifacts, relying on statistical cues in claims to classify samples is equivalent to a random guess.",
  "The collected dataset is challenging, and the performance of the models relying on biases evaluated on this dataset drops significantly.",
  "Base models: We consider BERT as the base model, which works the best on this dataset (Schuster et al., 2019), and predicts the relations based on the concatenation of the claim and the evidence with a delimiter token (see Appendix A).",
  "Results: Table 1 shows the results.",
  "Our proposed debiasing methods, PoE and DFL, are highly effective, boosting the performance of the baseline by 9.8 and 7.5 points respectively, significantly surpassing the prior work of Schuster et al. (2019).",
  "Datasets: We evaluate on hard datasets of SNLI and MNLI (Gururangan et al., 2018), which are the splits of these datasets where a hypothesis-only model cannot correctly predict the labels.",
  "Gururangan et al. (2018) show that the success of the recent textual entailment models is attributed to the biased examples, and the performance of these models is substantially lower on the hard sets.",
  "Base models: We consider BERT and InferSent (Conneau et al., 2017) as our base models.",
  "We choose InferSent to be able to compare with the prior work of Belinkov et al. (2019b).",
  "Bias-only model: The bias-only model predicts the labels using the hypothesis (Appendix B).",
  "Results on SNLI: Table 2 shows the SNLI results.",
  "With InferSent, DFL and PoE result in 4.1 and 4.8 points gain.",
  "With BERT, DFL and PoE improve the results by 2.5 and 1.6 absolute points.",
  "Compared to the prior work of Belinkov et al. (2019b) (AdvCls), our PoE model obtains a 7.4 points gain, setting a new state-of-the-art.",
  "Results on MNLI: We construct hard sets from the validation sets of MNLI Matched and Mismatched (MNLI-M).",
  "Following Gururangan et al. (2018), we train a fastText classifier (Joulin et al., 2017) that predicts the labels using only the hypothesis and consider the subset on which it fails as hard examples.",
  "We report the results on MNLI mismatched sets in Table 3 (see Appendix B for similar results on MNLI matched).",
  "With BERT, DFL and PoE obtain 1.4 and 1.7 points gain on the hard development set, while with InferSent, they improve the results by 2.5 and 2.6 points.",
  "To comply with limited access to the MNLI submission system, we evaluate only the best result of the baselines and our models on the test sets.",
  "Our PoE model improves the performance on the hard test set by 1.1 points while retaining in-domain accuracy.",
  "Dataset: McCoy et al. (2019b) show that NLI models trained on MNLI can adopt superficial syntactic heuristics.",
  "They introduce HANS, consisting of several examples on which the syntactic heuristics fail.",
  "Base model: We use BERT as our base model and train it on the MNLI dataset.",
  "Bias-only model: We consider the following features for the bias-only model.",
  "The first four features are based on the syntactic heuristics proposed in McCoy et al. (2019b):",
  "1) Whether all words in the hypothesis are included in the premise;",
  "2) If the hypothesis is the contiguous subsequence of the premise;",
  "3) If the hypothesis is a subtree in the premise's parse tree;",
  "4) The number of tokens shared between premise and hypothesis normalized by the number of tokens in the premise.",
  "We additionally include some similarity features:",
  "5) The cosine similarity between premise and hypothesis's pooled token representations from BERT followed by min, mean, and max-pooling.",
  "We consider the same weight for contradiction and neutral labels in the bias-only loss to allow the model to recognize entailment from not-entailment.",
  "During the evaluation, we map the neutral and contradiction labels to not-entailment.",
  "Results: McCoy et al. (2019a) observe large variability in the linguistic generalization of neural models.",
  "We, therefore, report the averaged results across 4 runs with the standard deviation in Table 4.",
  "PoE and DFL obtain 4.4 and 7.4 points gain (see Appendix C for accuracy on individual heuristics of HANS).",
  "We compare our results with the concurrent work of Clark et al., who propose a PoE model similar to ours, which gets similar results.",
  "The main difference is that our models are trained end-to-end, which is convenient in practice, while Clark et",
  "al.'s method requires two steps, first training a bias-only model and then using this pre-trained model to train a robust model.",
  "The Reweight baseline in Clark et al. is a special case of our DFL with  =1 and performs similarly to our DFL method (using default  =2 ).",
  "Their Learned-Mixin+H method requires hyperparameter tuning.",
  "Since the assumption is not having access to any out-of-domain test data, and there is no available dev set for HANS, it is challenging to perform hyper-parameter tuning.",
  "Clark et al. follow prior work (Grand and Belinkov, 2019; Ramakrishnan et al., 2018) and perform model section on the test set.",
  "To provide a fair comparison, we consequently also tuned  in DFL by sweeping over { 0 .",
  "5 , 1 , 2 , 3 , 4 } .",
  "DFL (cid:68) is the selected model, with  = 3 .",
  "With this hyperparameter tuning, DFL is even more effective, and our best result performs 2.8 points better than Clark et al. (2019).",
  "To evaluate combating multiple bias patterns, we jointly debias a base model on the hypothesis artifacts and syntactic biases.",
  "Results: Table 5 shows the results.",
  "Models trained to be robust to hypothesis biases ( (cid:168) ) do not generalize to HANS.",
  "On the other hand, models trained to be robust on HANS ( (cid:170) ) use a powerful bias-only model resulting in a slight improvement on MNLI mismatched hard dev set.",
  "We expect a slight degradation when debiasing for both biases since models need to select samples accommodating both debiasing needs.",
  "The jointly debiased models successfully obtain improvements on both datasets, which are close to the improvements on each dataset by the individually debiased models.",
  "To evaluate how well the baseline and proposed models generalize to solving textual entailment in domains that do not share the same annotation biases as the large NLI training sets, we take trained NLI models and test them on several NLI datasets.",
  "Datasets: We consider a total of 12 different NLI datasets.",
  "We use the 11 datasets studied by Poliak et al. (2018).",
  "These datasets include MNLI, SNLI, SciTail (Khot et al., 2018), AddOneRTE (ADD1) (Pavlick and Callison-Burch, 2016), Johns Hopkins Ordinal Commonsense Inference (JOCI) (Zhang et al., 2017), Multiple Premise Entailment (MPE) (Lai et al., 2017), Sentences Involving Compositional Knowledge (SICK) (Marelli et al., 2014), and three datasets from White et al. (2017) which are automatically generated from existing datasets for other NLP tasks including: Semantic Proto-Roles (SPR) (Reisinger et al., 2015), Definite Pronoun Resolution (DPR) (Rahman and Ng, 2012), FrameNet Plus (FN+) (Pavlick et al., 2015), and the GLUE benchmark's diagnostic test (Wang et al., 2019).",
  "We additionally consider the Quora Question Pairs (QQP) dataset, where the task is to determine whether two given questions are semantically matching (duplicate) or not.",
  "As in Gong et al. (2017), we interpret duplicate question pairs as an entailment relation and neutral otherwise.",
  "We use the same split ratio mentioned by Wang et al. (2017).",
  "Since the datasets considered have different label spaces, when evaluating on each target dataset, we map the model's labels to the corresponding target dataset's space.",
  "See Appendix D for more details.",
  "We strictly refrained from using any out-of-domain data when evaluating on the unbiased split of the same benchmark in Section 4.",
  "However, as shown by prior work (Belinkov et al., 2019a), since different NLI target datasets contain different amounts of the bias found in the large-scale NLI dataset, we need to adjust the amount of debiasing according to each target dataset.",
  "We consequently introduce a hyperparameter  for PoE to modulate the strength of the bias-only model in ensembling.",
  "We follow prior work (Belinkov et al., 2019a) and perform model selection on the dev set of each target dataset Data CE DFL  PoE  SICK 57.05 57.91 +0.9 57.28 +0.2 ADD1 87.34 88.89 +1.5 87.86 +0.5 DPR 49.50 50.68 +1.2 50.14 +0.6 SPR 59.85 61.41 +1.6 62.45 +2.6 FN+ 53.16 54.77 +1.6 53.51 +0.4 JOCI 50.06 51.13 +1.1 50.85 +0.8 MPE 69.50 70.2 +0.7 70.1 +0.6 SCITAIL 67.64 69.33 +1.7 71.40 +3.8 GLUE 54.08 54.80 +0.7 54.71 +0.6 QQP 67.78 69.28 +1.5 68.61 +0.8 MNLI 74.40 73.58 -0.8 73.61 -0.8 MNLI-M 73.98 74.0 0.0 73.49 -0.5 Table 6: Accuracy results of models with BERT transferring to new target datasets.",
  "Results: Table 6 shows the results of the debiased models and baseline with BERT.",
  "As shown in prior work (Belinkov et al., 2019a), the MNLI datasets have very similar biases to SNLI, which the models are trained on, so we do not expect any improvement in the relative performance of our models and the baseline for MNLI and MNLI-M.",
  "On all the remaining datasets, our proposed models perform better than the baseline, showing a substantial improvement in generalization by using our debasing techniques.",
  "We additionally compare with Belinkov et al. (2019a) in Appendix D and show that our methods substantially surpass their results.",
  "4 Since the test sets are not available for MNLI, we tune on the matched dev set and evaluate on the mismatched dev set or vice versa.",
  "For GLUE, we tune on MNLI mismatched dev set.",
  "Analysis of Debiased Focal Loss: As expected, improving the out-of-domain performance could come at the expense of decreased in-domain performance since the removed biases are useful for performing the in-domain task.",
  "This happens especially for DFL, in which there is a trade-off between in-domain and out-of-domain performance that depends on the parameter  , and when the baseline model is not very powerful like InferSent.",
  "To understand the impact of  in DFL, we train an InferSent model using DFL for different values of  on the SNLI dataset and evaluate its performance on SNLI test and SNLI hard sets.",
  "As illustrated in Figure 2, increasing  increases debiasing and thus hurts in-domain accuracy on SNLI, but out-of-domain accuracy on the SNLI hard set is increased within a wide range of values (see a similar plot for BERT in Appendix E).",
  "Correlation Analysis: In contrast to Belinkov et al. (2019a), who encourage only the encoder to not capture the unwanted biases, our learning strategies influence the parameters of the full model to reduce the reliance on unwanted patterns more effectively.",
  "To test this assumption, in Figure 3, we report the correlation between the element-wise loss of the debiased models and the loss of a bias-only model on the considered datasets.",
  "The results show that compared to the baselines, our debiasing methods, DFL and PoE, reduce the correlation to the bias-only model, confirming that our models are effective at reducing biases.",
  "Interestingly, on MNLI, PoE has less correlation with the bias-only model than DFL and also has better performance on the unbiased split of this dataset.",
  "On the other hand, on the HANS dataset, DFL loss is less correlated with the bias-only model than PoE and also obtains higher performance on the HANS dataset.",
  "We propose two novel techniques, product-of-experts and debiased focal loss, to reduce biases learned by neural models, which are applicable whenever one can specify the biases in the form of one or more bias-only models.",
  "The bias-only models are designed to leverage biases and shortcuts in the datasets.",
  "Our debiasing strategies then work by adjusting the cross-entropy loss based on the performance of these bias-only models, to focus learning on the hard examples and downweight the importance of the biased examples.",
  "Additionally, we extend our methods to combat multiple bias patterns simultaneously.",
  "Our proposed debiasing techniques are model agnostic, simple, and highly effective.",
  "Extensive experiments show that our methods substantially improve the model robustness to domain-shift, including 9.8 points gain on FEVER symmetric test set, 7.4 on HANS dataset, and 4.8 points on SNLI hard set.",
  "Furthermore, we show that our debiasing techniques result in better generalization to other NLI datasets.",
  "Future work may include developing debiasing strategies that do not require prior knowledge of bias patterns and can automatically identify them.",
  "We would like to thank Daniel Andor and Suraj Srinivas for their helpful comments.",
  "We additionally would like to thank the authors of Schuster et al. (2019); Cadene et al. (2019); McCoy et al. (2019b); Belinkov et al. (2019a) for their support to reproduce their results.",
  "This research was supported by the Swiss National Science Foundation under the project Learning Representations of Abstraction for Opinion Summarization (LAOS), grant number FNS-30216.",
  "Y.B. was supported by the Harvard Mind, Brain, and Behavior Initiative."
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "abstain",
  "objective",
  "abstain",
  "abstain",
  "method",
  "result",
  "other",
  "abstain",
  "other",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "objective",
  "abstain",
  "result",
  "abstain",
  "abstain",
  "objective",
  "method",
  "objective",
  "abstain",
  "objective",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "result",
  "objective",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "objective",
  "other",
  "method",
  "other",
  "objective",
  "abstain",
  "other",
  "objective",
  "objective",
  "objective",
  "objective",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "other",
  "abstain",
  "method",
  "method",
  "abstain",
  "abstain",
  "method",
  "method",
  "method",
  "abstain",
  "abstain",
  "other",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "method",
  "abstain",
  "abstain",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "other",
  "abstain",
  "other",
  "method",
  "abstain",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "method",
  "other",
  "method",
  "abstain",
  "method",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "other",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "result",
  "abstain",
  "abstain",
  "objective",
  "abstain",
  "method",
  "objective",
  "objective",
  "result",
  "result",
  "abstain",
  "other",
  "other",
  "other",
  "other"
]</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="1"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images.",
  "However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning.",
  "To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT.",
  "Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects).",
  "We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations.",
  "Finally, these representations provide an attention-based context vector for the decoder.",
  "We evaluate our proposed encoder on the Multi30K datasets.",
  "Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.",
  "Multi-modal neural machine translation (NMT) (Huang et al., 2016; Calixto et al., 2017) has become an important research direction in machine translation, due to its research significance in multimodal deep learning and wide applications, such as translating multimedia news and web product information (Zhou et al., 2018).",
  "It significantly extends the conventional text-based machine translation by taking images as additional inputs.",
  "The assumption behind this is that the translation is expected to be more accurate compared to purely text-based (cid:3) This work is done when Yongjing Yin was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China.",
  "translation, since the visual context helps to resolve ambiguous multi-sense words (Ive et al., 2019).",
  "Apparently, how to fully exploit visual information is one of the core issues in multi-modal NMT, which directly impacts the model performance.",
  "To this end, a lot of efforts have been made, roughly consisting of: (1) encoding each input image into a global feature vector, which can be used to initialize different components of multi-modal NMT models, or as additional source tokens (Huang et al., 2016; Calixto et al., 2017), or to learn the joint multi-modal representation (Zhou et al., 2018; Calixto et al., 2019); (2) extracting object-based image features to initialize the model, or supplement source sequences, or generate attention-based visual context (Huang et al., 2016; Ive et al., 2019); and (3) representing each image as spatial features, which can be exploited as extra context (Calixto et al., 2017; Delbrouck and Dupont, 2017a; Ive et al., 2019), or a supplement to source semantics (Delbrouck and Dupont, 2017b) via an attention mechanism.",
  "Despite their success, the above studies do not fully exploit the fine-grained semantic correspondences between semantic units within an input sentence-image pair.",
  "For example, as shown in Figure 1, the noun phrase  a toy car  semantically corresponds to the blue dashed region.",
  "The neglect of this important clue may be due to two big challenges: 1) how to construct a unified representation to bridge the semantic gap between two different modalities, and 2) how to achieve semantic interactions based on the unified representation.",
  "However, we believe that such semantic correspondences can be exploited to refine multimodal representation learning, since they enable the representations within one modality to incorporate cross-modal information as supplement during multi-modal semantic interactions (Lee et al., 2018; Tan and Bansal, 2019).",
  "In this paper, we propose a novel graph-based multi-modal fusion encoder for NMT.",
  "We first represent the input sentence and image with a unified multi-modal graph.",
  "In this graph, each node indicates a semantic unit: textual word or visual object , and two types of edges are introduced to model semantic relationships between semantic units within the same modality ( intra-modal edges ) and semantic correspondences between semantic units of different modalities ( inter-modal edges ) respectively.",
  "Based on the graph, we then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions among the nodes to conduct graph encoding.",
  "Particularly, during this process, we distinguish the parameters of two modalities, and sequentially conduct intra-and inter-modal fusions to learn multi-modal node representations.",
  "Finally, these representations can be exploited by the decoder via an attention mechanism.",
  "Compared with previous models, ours is able to fully exploit semantic interactions among multimodal semantic units for NMT.",
  "Overall, the major contributions of our work are listed as follows: (cid:15) We propose a unified graph to represent the input sentence and image, where various semantic relationships between multi-modal semantic units can be captured for NMT.",
  "(cid:15)",
  "We propose a graph-based multi-modal fusion encoder to conduct graph encoding based on the above graph.",
  "To the best of our knowledge, our work is the first attempt to explore multimodal graph neural network (GNN) for NMT.",
  "(cid:15)",
  "We conduct extensive experiments on Multi30k datasets of two language pairs.",
  "Experimental results and in-depth analysis indicate that our encoder is effective to fuse multi-modal information for NMT.",
  "Particularly, our multi-modal NMT model significantly outperforms several competitive baselines.",
  "(cid:15)",
  "We release the code at https://github.com/ DeepLearnXMU/GMNMT.",
  "Our multi-modal NMT model is based on attentional encoder-decoder framework with maximizing the log likelihood of training data as the objective function.",
  "Essentially, our encoder can be regarded as a multimodal extension of GNN.",
  "To construct our encoder, we first represent the input sentence-image pair as a unified multi-modal graph.",
  "Then, based on this graph, we stack multiple multi-modal fusion layers to learn node representations, which provides the attention-based context vector to the decoder.",
  "In this section, we take the sentence and the image shown in Figure 1 as an example, and describe how to use a multi-modal graph to represent them.",
  "Formally, our graph is undirected and can be formalized as G =( V , E ), which is constructed as follows: In the node set V , each node represents either a textual word or a visual object.",
  "Specifically, we adopt the following strategies to construct these two kinds of nodes: (1) We include all words as separate textual nodes in order to fully exploit textual 3027 Multi-modal Graph Embedding Layer Cross-modal Gating Visual FFN Textual FFN Cross-modal Gating Intra-modal Fusion Inter-modal Fusion Target Inputs Embedding Layer       Textual Self-Attention Visual Self-Attention Softmax Layer Target Outputs Self-Attention Encoder-DecoderAttention FFN Encoder Decoder Figure 2: The architecture of our NMT model with the graph-based multi-modal fusion encoder.",
  "information.",
  "For example, in Figure 1, the multimodal graph contains totally eight textual nodes, each of which corresponds to a word in the input sentence; (2) We employ the Stanford parser to identify all noun phrases in the input sentence, and then apply a visual grounding toolkit (Yang et al., 2019) to detect bounding boxes (visual objects) for each noun phrase.",
  "Subsequently, all detected visual objects are included as independent visual nodes .",
  "In this way, we can effectively reduce the negative impact of abundant unrelated visual objects.",
  "Let us revisit the example in Figure 1, where we can identify two noun phrases  Two boys  and  a toy car  from the input sentence, and then include three visual objects into the multi-modal graph.",
  "To capture various semantic relationships between multi-modal semantic units for NMT, we consider two kinds of edges in the edge set E : (1) Any two nodes in the same modality are connected by an intra-modal edge ; and (2) Each textual node representing any noun phrase and the corresponding visual node are connected by an inter-modal edge .",
  "Back to Figure 1, we can observe that all visual nodes are connected to each other, and all textual nodes are fully-connected.",
  "However, only nodes v o 1 and v x 1 , v o 1 and v x 2 , v o 2 and v x 1 , v o 2 and v x 2 , v o 3 and v x 6 , v o 3 and v x 7 , v o 3 and v x 8 are connected by inter-modal edges.",
  "Before inputting the multi-modal graph into the stacked fusion layers, we introduce an embedding",
  "layer to initialize the node states.",
  "Specifically, for each textual node v x i , we define its initial state H (0) x i as the sum of its word embedding and position encoding (Vaswani et al., 2017).",
  "To obtain the initial state H (0) o j of the visual node v o j , we first extract visual features from the fully-connected layer that follows the ROI pooling layer in Faster-RCNN (Ren et al., 2015), and then employ a multilayer perceptron with ReLU activation function to project these features onto the same space as textual representations.",
  "As shown in the left part of Figure 2, on the top of embedding layer, we stack L e graph-based multimodal fusion layers to encode the above-mentioned multi-modal graph.",
  "At each fusion layer, we sequentially conduct intraand inter-modal fusions to update all node states.",
  "In this way, the final node states encode both the context within the same modality and the cross-modal semantic information simultaneously.",
  "Particularly, since visual nodes and textual nodes are two types of semantic units containing the information of different modalities, we apply similar operations but with different parameters to model their state update process, respectively.",
  "Specifically, in the l -th fusion layer, both updates of textual node states H ( l ) x = f H ( l ) x i g and visual node states H ( l ) o = f H ( l ) o j g mainly involve the following steps: 3028 Step1: Intra-modal fusion .",
  "At this step, we employ self-attention to generate the contextual representation of each node by collecting the message from its neighbors of the same modality.",
  "Formally, the contextual representations C ( l ) x of all textual nodes are calculated as follows: 1 C ( l ) x = MultiHead ( H ( l (cid:0) 1) x ; H ( l (cid:0) 1) x ; H ( l (cid:0) 1) x ) ; (1) where MultiHead( Q , K , V ) is a multi-head self-attention function taking a query matrix Q , a key matrix K , and a value matrix V as inputs.",
  "Similarly, we generate the contextual representations C ( l ) o of all visual nodes as C ( l ) o = MultiHead ( H ( l (cid:0) 1) o ; H ( l (cid:0) 1) o ; H ( l (cid:0) 1) o ) : (2) In particular, since the initial representations of visual objects are extracted from deep CNNs, we apply a simplified multi-head self-attention to preserve the initial representations of visual objects, where the learned linear projects of values and final outputs are removed.",
  "Step2: Inter-modal fusion .",
  "Inspired by studies in multi-modal feature fusion (Teney et al., 2018; Kim et al., 2018), we apply a cross-modal gating mechanism with an element-wise operation to gather the semantic information of the cross-modal neighbours of each node.",
  "Concretely, we generate the representation M ( l ) x i of a text node v x i in the following way: M ( l ) x i = X j 2 A ( v xi ) (cid:11) i;j (cid:12) C ( l ) o j ; (3) (cid:11) i;j = Sigmoid ( W ( l ) 1 C ( l ) x i + W ( l ) 2 C ( l ) o j ) ; (4) where A ( v x i ) is the set of neighboring visual nodes of v x i , and W ( l ) 1 and W ( l ) 2 are parameter matrices.",
  "Likewise, we produce the representation M ( l ) o j of a visual node v o j as follows: M ( l ) o j = X i 2 A ( v oj ) (cid:12) j;i (cid:12) C ( l ) x i ; (5) (cid:12) j;i = Sigmoid ( W ( l ) 3 C ( l ) o j + W ( l ) 4 C ( l ) x i ) ; (6) where A ( v o j ) is the set of adjacent textual nodes of v o j , and W ( l ) 3 and W ( l ) 4 are also parameter matrices.",
  "The advantage is that the above fusion approach can better determine the degree of inter-modal fusion according to the contextual representations of 1 For simplicity, we omit the descriptions of layer normalization and residual connection.",
  "each modality.",
  "Finally, we adopt position-wise feed forward networks FFN ( (cid:3) ) to generate the textual node states H ( l ) x and visual node states H ( l ) o : H ( l ) x = FFN ( M ( l ) x ) ; (7) H ( l ) o = FFN ( M ( l ) o ) ; (8) where M ( l ) x = f M ( l ) x i g , M ( l ) o = f M ( l ) o j g denote the above updated representations of all textual nodes and visual nodes respectively.",
  "Our decoder is similar to the conventional Transformer decoder.",
  "Since visual information has been incorporated into all textual nodes via multiple graph-based multi-modal fusion layers, we allow the decoder to dynamically exploit the multi-modal context by only attending to textual node states.",
  "As shown in the right part of Figure 2, we follow Vaswani et al. (2017) to stack L d identical layers to generate target-side hidden states, where each layer l is composed of three sub-layers.",
  "Concretely, the first two sub-layers are a masked self-attention and an encoder-decoder attention to integrate target-and source-side contexts respectively: E ( l ) = MultiHead ( S ( l (cid:0) 1) ; S ( l (cid:0) 1) ; S ( l (cid:0) 1) ) ; (9) T ( l ) = MultiHead ( E ( l ) ; H ( L e ) x ; H ( L e ) x ) ; (10) where S ( l (cid:0) 1) denotes the target-side hidden states in the l 1 -th layer.",
  "In particular, S (0) are the embed-dings of input target words.",
  "Then, a position-wise fully-connected forward neural network is uesd to produce S ( l ) as follows: S ( l ) = FFN ( T ( l ) ) : (11) Finally, the probability distribution of generating the target sentence is defined by using a softmax layer, which takes the hidden states in the top layer as input: P ( Y j X; I ) = Y t Softmax ( WS ( L d ) t + b ) ; (12) where X is the input sentence, I is the input image, Y is the target sentence, and W and b are the parameters of the softmax layer.",
  "We carry out experiments on multi-modal English ) German (En ) De) and English ) French (En ) Fr) translation tasks.",
  "Datasets We use the Multi30K dataset (Elliott et al., 2016), where each image is paired with one English description and human translations into German and French.",
  "Training, validation and test sets contain 29,000, 1,014 and 1,000 instances respectively.",
  "In addition, we evaluate various models on the WMT17 test set and the ambiguous MSCOCO test set, which contain 1,000 and 461 instances respectively.",
  "Here, we directly use the preprocessed sentences 2 and segment words into subwords via byte pair encoding (Sennrich et al., 2016) with 10,000 merge operations.",
  "Visual Features We first apply the Stanford parser to identify noun phrases from each source sentence, and then employ the visual ground toolkit released by Yang et al. (2019) to detect associated visual objects of the identified noun phrases.",
  "For each phrase, we keep the visual object with the highest prediction probability, so as to reduce negative effects of abundant visual objects.",
  "In each sentence, the average numbers of objects and words are around 3.5 and 15.0 respectively.",
  "3 Finally, we compute 2,048-dimensional features for these objects with the pre-trained ResNet-100 Faster-RCNN (Ren et al., 2015).",
  "Settings We use Transformer (Vaswani et al., 2017) as our baseline.",
  "Since the size of training corpus is small and the trained model tends to be over-fitting, we first perform a small grid search to obtain a set of hyper-parameters on the En ) De validation set.",
  "Specifically, the word embedding dimension and hidden size are 128 and 256 respectively.",
  "The decoder has L d =4 layers 4 and the number of attention heads is",
  "4. The dropout is set to 0.5.",
  "Each batch consists of approximately 2,000 source and target tokens.",
  "We apply the Adam optimizer with a scheduled learning rate to optimize various models, and we use other same settings as (Vaswani et al., 2017).",
  "Finally, we use the metrics BLEU (Pa-pineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) to evaluate the quality of translations.",
  "Particularly, we run all models three times for each experiment and report the average results.",
  "2 http://www.statmt.org/wmt18/multimodal-task.html 3 There is no parsing failure for this dataset.",
  "If no noun is detected for a sentence, the object representations will be set to zero vectors and the model will degenerate to Transformer.",
  "4 The encoder of the text-based Transformer also has 4 layers.",
  "Baseline Models In addition to the text-based Transformer (Vaswani et al., 2017), we adapt several effective approaches to Transformer using our visual features, and compare our model with them 5 : (cid:15) ObjectAsToken(TF) (Huang et al., 2016).",
  "It is a variant of the Transformer, where all visual objects are regarded as extra source tokens and placed at the front of the input sentence.",
  "(cid:15)",
  "Enc-att(TF) (Delbrouck and Dupont, 2017b).",
  "An encoder-based image attention mechanism is incorporated into Transformer, which augments each source annotation with an attention-based visual feature vector.",
  "(cid:15)",
  "Doubly-att(TF) (Helcl et al., 2018).",
  "It is a doubly attentive Transformer.",
  "In each decoder layer, a cross-modal multi-head attention sublayer is inserted before the fully connected feed-forward layer to generate the visual context vector from visual features.",
  "We also display the performance of several dominant multi-modal NMT models such as Doubly-att(RNN) (Calixto et al., 2017), Soft-att(RNN) (Delbrouck and Dupont, 2017a), Stochastic-att(RNN) (Delbrouck and Dupont, 2017a), Fusion-conv(RNN) (Caglayan et al., 2017), Trg-mul(RNN) (Caglayan et al., 2017), VMMT(RNN) (Calixto et al., 2019) and Deliberation Network(TF) (Ive et al., 2019) on the same datasets.",
  "The number L e of multi-modal fusion layer is an important hyper-parameter that directly determines",
  "5 We use suffixes ( RNN ) and ( TF ) to represent RNN-and Transformer-style NMT models, respectively.",
  "the degree of fine-grained semantic fusion in our encoder.",
  "Thus, we first inspect its impact on the EN ) DE validation set.",
  "Figure 3 provides the experimental results using different L e and our model achieves the best performance when L e is",
  "3. Hence, we use L e =3 in all subsequent experiments.",
  "Table 1 shows the main results on the En ) De translation task.",
  "Ours outperforms most of the existing models and all baselines, and is comparable to Fusion-conv(RNN) and Trg-mul(RNN) on METEOR.",
  "The two results are from the state-of-the-art system on the WMT2017 test set, which is selected based on METEOR.",
  "Comparing the baseline models, we draw the following interesting conclusions: First , our model outperforms ObjectAsTo-ken(TF), which concatenates regional visual features with text to form attendable sequences and employs self-attention mechanism to conduct inter-modal fusion.",
  "The underlying reasons consist of two aspects: explicitly modeling semantic correspondences between semantic units of different modalities, and distinguishing model parameters for different modalities.",
  "Second , our model also significantly outperforms Enc-att(TF).",
  "Note that Enc-att(TF) can be considered as a single-layer semantic fusion encoder.",
  "In addition to the advantage of explicitly modeling semantic correspondences, we conjecture that multi-layer multi-modal semantic interactions are also beneficial to NMT.",
  "Third , compared with Doubly-att(TF) simply using an attention mechanism to exploit visual in-15 20 25 30 35 40 [5,10) [10,15) [15,20) [20,25) [25,...) BLEU Sentence Length TransformerObjectAsToken(TF)Enc-att(TF) Doubly-att(TF) Our model Figure 4: BLEU scores on different translation groups divided according to source sentence lengths.",
  "formation, our model achieves a significant improvement, because of sufficient multi-modal fusion in our encoder.",
  "Besides, we divide our test sets into different groups based on the lengths of source sentences and the numbers of noun phrases, and then compare the performance of different models in each group.",
  "Figures 4 and 5 report the BLEU scores on these groups.",
  "Overall, our model still consistently achieves the best performance in all groups.",
  "Thus, we confirm again the effectiveness and gen-3031 Model En ) De Test2016 Test2017 MSCOCO BLEU METEOR BLEU METEOR BLEU METEOR Our model 39.8 57.6 32.2 51.9 28.7 47.6 w/o inter-modal fusion 38.7 56.7 30.7 50.6 27.0 46.7 visual grounding ) fully-connected 36.4 53.4 28.3 47.0 24.4 42.9 different parameters ) unified parameters 39.2 57.3 31.9 51.4 27.7 47.4 w/ attending to visual nodes 39.6 57.3 32.0 51.3 27.9 46.8 attending to textual nodes ) attending to visual nodes 30.9 48.6 22.3 41.5 20.4 38.7 Table 2: Ablation study of our model on the EN ) DE translation task.",
  "erality of our proposed model.",
  "Note that in the sentences with more phrases, which are usually long sentences, the improvements of our model over baselines are more significant.",
  "We speculate that long sentences often contain more ambiguous words.",
  "Thus compared with short sentences, long sentences may require visual information to be better exploited as supplementary information, which can be achieved by the multi-modal semantic interaction of our model.",
  "We also show the training and decoding speed of our model and the baselines in Table",
  "4. During training, our model can process approximately 1.1K tokens per second, which is comparable to other multi-modal baselines.",
  "When it comes to decoding procedure, our model translates about 16.7 sentences per second and the speed drops slightly compared to Transformer.",
  "Moreover, our model only introduces a small number of extra parameters and achieves better performance.",
  "To investigate the effectiveness of different components, we further conduct experiments to compare our model with the following variants in Table 2:",
  "(1) w/o inter-modal fusion .",
  "In this variant, we apply two separate Transformer encoders to learn the semantic representations of words and visual objects, respectively, and then use the doubly-attentive decoder (Helcl et al., 2018) to incorporate textual and visual contexts into the decoder.",
  "The result in line 3 indicates that removing the inter-modal fusion leads to a significant performance drop.",
  "It suggests that semantic interactions among multi-modal semantic units are indeed useful for multi-modal representation learning.",
  "(2) visual grounding ) fully-connected .",
  "We make the words and visual objects fully-connected to establish the inter-modal correspondences.",
  "The result in line 4 shows that this change causes a significant performance decline.",
  "The underlying reason is the fully-connected semantic correspondences introduce much noise to our model.",
  "(3) different parameters ) unified parameters .",
  "When constructing this variant, we assign unified parameters to update node states in different modalities.",
  "Apparently, the performance drop reported in line 5 also demonstrates the validity of our ap-3032 proach using different parameters.",
  "(4) w/ attending to visual nodes .",
  "Different from our model attending to only textual nodes, we allow our decoder of this variant to consider both two types of nodes using doubly-attentive decoder.",
  "From line 6, we can observe that considering all nodes does not bring further improvement.",
  "The result confirms our previous assumption that visual information has been fully incorporated into textual nodes in our encoder.",
  "(5) attending to textual nodes ) attending to visual nodes .",
  "However, when only considering visual nodes, the model performance drops drastically (line 7).",
  "This is because the number of visual nodes is far fewer than that of textual nodes, which is unable to produce sufficient context for translation.",
  "Figure 6 displays the 1-best translations of a sampled test sentence generated by different models.",
  "The phrase  a skateboarding ramp  is not translated correctly by all baselines, while our model correctly translates it.",
  "This reveals that our encoder is able to learn more accurate representations.",
  "We also conduct experiments on the EN ) Fr dataset.",
  "From Table 3, our model still achieves better performance compared to all baselines, which demonstrates again that our model is effective and general to different language pairs in multi-modal NMT.",
  "Multi-modal NMT Huang et al. (2016) first incorporate global or regional visual features into attention-based NMT.",
  "Calixto and Liu (2017) also study the effects of incorporating global visual features into different NMT components.",
  "Elliott and K adar (2017) share an encoder between a translation model and an image prediction model to learn visually grounded representations.",
  "Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018).",
  "Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-modal interactions for multi-modal NMT.",
  "Apart from model design, Elliott (2018) reveal that visual information seems to be ignored by the multimodal NMT models.",
  "Caglayan et al. (2019) conduct a systematic analysis and show that visual information can be better leveraged under limited textual context.",
  "Different from the above-mentioned studies, we first represent the input sentence-image pair as a unified graph, where various semantic relationships between multi-modal semantic units can be effectively captured for multi-modal NMT.",
  "Benefiting from the multi-modal graph, we further introduce an extended GNN to conduct graph encoding via multi-modal semantic interactions.",
  "Note that if we directly adapt the approach proposed by Huang et al. (2016) into Transformer, the model (ObjectAsToken(TF)) also involves multimodal fusion.",
  "However, ours is different from it in following aspects: (1) We first learn the contextual representation of each node within the same modality, so that it can better determine the degree of inter-modal fusion according to its own context.",
  "(2) We assign different encoding parameters to different modalities, which has been shown effective in our experiments.",
  "Additionally, the recent study LXMERT (Tan and Bansal, 2019) also models relationships between vision and language, which differs from ours in following aspects: (1) Tan and Bansal (2019) first apply two transformer encoders for two modalities, and then stack two cross-modality encoders to conduct multi-modal fusion.",
  "In contrast, we sequentially conduct self-attention and cross-modal gating at each layer.",
  "(2) Tan and Bansal (2019) leverage an attention mechanism to implicitly establish cross-modal relationships via large-scale pretraining, while we utilize visual grounding to capture explicit cross-modal correspondences.",
  "(3) We focus on multi-modal NMT rather than vision-and-language reasoning in (Tan and Bansal, 2019).",
  "Graph Neural Networks Recently, GNNs (Marco Gori and Scarselli, 2005) including gated graph neural network (Li et al., 2016), graph convolutional network (Duvenaud et al., 2015; Kipf and Welling, 2017) and graph attention network (Velickovic et al., 2018) have been shown effective in many tasks such as VQA (Teney et al., 2017; Norcliffe-Brown et al., 2018; Li et al., 2019), text generation (Gildea et al., 2018; Becky et al., 2018; Song et al., 2018b, 2019) and text representation (Zhang et al., 2018; Yin et al., 2019; Song et al., 3033 Source : A boy riding a skateboard on a skateboarding ramp .",
  "In this work, we mainly focus on how to extend GNN to fuse multi-modal information in NMT.",
  "Close to our work, Teney et al. (2017) introduce GNN for VQA.",
  "The main difference between their work and ours is that they build an individual graph for each modality, while we use a unified multimodal graph.",
  "In this paper, we have proposed a novel graph-based multi-modal fusion encoder, which exploits various semantic relationships between multimodal semantic units for NMT.",
  "Experiment results and analysis on the Multi30K dataset demonstrate the effectiveness of our model.",
  "In the future, we plan to incorporate attributes of visual objects and dependency trees to enrich the multi-modal graphs.",
  "Besides, how to introduce scene graphs into multi-modal NMT is a worthy problem to explore.",
  "Finally, we will apply our model into other multi-modal tasks such as multimodal sentiment analysis.",
  "This work was supported by the Beijing Advanced Innovation Center for Language Resources (No. TYR17002), the National Natural Science Foundation of China (No. 61672440), and the Scientific Research Project of National Language Committee of China (No. YB135-49)."
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "abstain",
  "abstain",
  "objective",
  "objective",
  "method",
  "abstain",
  "objective",
  "result",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "other",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "objective",
  "objective",
  "abstain",
  "method",
  "method",
  "abstain",
  "abstain",
  "objective",
  "objective",
  "objective",
  "objective",
  "objective",
  "method",
  "result",
  "result",
  "objective",
  "other",
  "objective",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "method",
  "method",
  "method",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "method",
  "other",
  "method",
  "abstain",
  "method",
  "method",
  "method",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "other",
  "abstain",
  "other",
  "abstain",
  "method",
  "abstain",
  "method",
  "method",
  "method",
  "method",
  "abstain",
  "method",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "method",
  "method",
  "other",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "method",
  "method",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "result",
  "abstain",
  "abstain",
  "abstain",
  "result",
  "abstain",
  "abstain",
  "result",
  "other",
  "abstain",
  "result",
  "abstain",
  "result",
  "result",
  "abstain",
  "abstain",
  "result",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "result",
  "result",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "result",
  "abstain",
  "abstain",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "objective",
  "method",
  "abstain",
  "objective",
  "abstain",
  "other",
  "objective",
  "method",
  "method",
  "other",
  "abstain",
  "abstain",
  "method",
  "objective",
  "objective",
  "abstain",
  "abstain",
  "method",
  "other"
]</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="2"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding.",
  "At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment ), improving upon available resources in both its coverage and difficulty.",
  "MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation.",
  "In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
  "Many of the most actively studied problems in NLP, including question answering, translation, and dialog, depend in large part on natural language understanding (NLU) for success.",
  "While there has been a great deal of work that uses representation learning techniques to pursue progress on these applied NLU problems directly, in order for a representation learning model to fully succeed at one of these problems, it must simultaneously succeed both at NLU, and at one or more additional hard machine learning problems like structured prediction or memory access.",
  "This makes it difficult to accurately judge the degree to which current models extract reasonable representations of language meaning in these settings.",
  "The task of natural language inference (NLI) is well positioned to serve as a benchmark task for research on NLU.",
  "In this task, also known as recognizing textual entailment (Cooper et al., 1996; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentenceslike one of those in Figure 1and asked to judge the relationship between their meanings by picking a label from a small set: typically ENTAILMENT , NEUTRAL , and CONTRADICTION .",
  "Succeeding at NLI does not require a system to solve any difficult machine learning problems except, crucially, that of extracting effective and thorough representations for the meanings of sentences (i.e., their lexical and compositional semantics).",
  "In particular, a model must handle phenomena like lexical entailment, quantification, coreference, tense, belief, modality, and lexical and syntactic ambiguity.",
  "As the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention (Wang and Jiang, 2016; Parikh et al., 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016; Chen et al., 2017).",
  "However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways.",
  "First, the sentences in SNLI are derived from only a single text genreimage captionsand are thus limited to descriptions of concrete visual scenes, rendering the hypothesis sentences used to describe these scenes short and simple, and rendering many important phenomenalike temporal reasoning (e.g., yesterday ), belief (e.g., know ), and modality (e.g., should )rare enough to be irrelevant to task performance.",
  "Second, because of these issues, SNLI is not sufficiently demanding to serve as an effective benchmark for NLU, with the best current model performance falling within a few percentage points of human accuracy and limited room left for fine-grained comparisons between strong models.",
  "This paper introduces a new challenge dataset, the Multi-Genre NLI Corpus (MultiNLI), whose chief purpose is to remedy these limitations by making it possible to run large-scale NLI evaluations that capture more of the complexity of modern English.",
  "While its size (433k pairs) and mode of collection are modeled closely on SNLI, unlike that corpus, MultiNLI represents both written and spoken speech in a wide range of styles, degrees of formality, and topics.",
  "Our chief motivation in creating this corpus is to provide a benchmark for ambitious machine learning research on the core problems of NLU, but we are additionally interested in constructing a corpus that facilitates work on domain adaptation and cross-domain transfer learning.",
  "These techniqueswhich use labeled training data for a source domain, and aim to train a model that performs well on test data from a target domain with a different distributionhave resulted in gains across many tasks (Daume III and Marcu, 2006; Ben-David et al., 2007), including sequence and part-of-speech tagging (Blitzer et al., 2006; Peng and Dredze, 2017).",
  "Moreover, in application areas outside NLU, artificial neural network techniques have made it possible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data (Krizhevsky et al., 2012; Zeiler and Fergus, 2014; Donahue et al., 2014).",
  "However, attempts to bring this kind of general purpose representation learning to NLU have seen only very limited success (see, for example, Mou et al., 2016a).",
  "Nearly all successful applications of representation learning to NLU have involved models that are trained on data closely resembling the target evaluation data in both task and style.",
  "This fact limits the usefulness of these tools for problems involving styles of language not represented in large annotated training sets.",
  "With this in mind, we construct MultiNLI so as to make it possible to explicitly evaluate models both on the quality of their sentence representations within the training domain and on their ability to derive reasonable representations in unfamiliar domains.",
  "The corpus is derived from ten different genres of written and spoken English, which are collectively meant to approximate the full diversity of ways in which modern standard 1113 This task will involve reading a line from a non-fiction article and writing three sentences that relate to it.",
  "The line will describe a situation or event.",
  "Using only this description and what you know about the world:  Write one sentence that is definitely correct about the situation or event in the line.",
  "Write one sentence that might be correct about the situation or event in the line.",
  "Write one sentence that is definitely incorrect about the situation or event in the line.",
  "American English is used.",
  "All of the genres appear in the test and development sets, but only five are included in the training set.",
  "Models thus can be evaluated on both the matched test examples, which are derived from the same sources as those in the training set, and on the mismatched examples, which do not closely resemble any of those seen at training time.",
  "The data collection methodology for MultiNLI is similar to that of SNLI: We create each sentence pair by selecting a premise sentence from a preexisting text source and asking a human annotator to compose a novel sentence to pair with it as a hypothesis.",
  "This section discusses the sources of our premise sentences, our collection method for hypotheses, and our validation (relabeling) strategy.",
  "Premise Text Sources The MultiNLI premise sentences are derived from ten sources of freely available text which are meant to be maximally diverse and roughly represent the full range of American English.",
  "We selected nine sources from the second release of the Open American National Corpus (OANC; Fillmore et al., 1998; Macleod et al., 2000; Ide and Macleod, 2001; Ide and Su-derman, 2006, downloaded 12/2016 1 ), balancing the volume of source text roughly evenly across genres, and avoiding genres with content that would be too difficult for untrained annotators.",
  "and Conversation Collection of two-sided, in-person conversations that took place in the early 2000s (FACE-TO-FACE ); reports, speeches, letters, and press releases from public domain government websites (GOVERNMENT ); letters from the Indiana Center for Intercultural Communication of Philanthropic Fundraising Discourse written in the late 1990searly 2000s (LETTERS ); the public report from the National Commission on Terrorist Attacks Upon the United States released on July 22, 2004 2 (9/11); five non-fiction works on the textile industry and child development published by the Oxford University Press (OUP); popular culture articles from the archives of Slate Magazine (SLATE ) written between 19962000; transcriptions from University of Pennsylvania's Linguistic Data Consortium Switchboard corpus of two-sided, telephone conversations that took place in 1990 or 1991 (TELEPHONE ); travel guides published by Berlitz Publishing in the early 2000s (TRAVEL ); and short posts about linguistics for non-specialists from the Verbatim archives written between 1990 and 1996 (VERBATIM ).",
  "For our tenth genre, FICTION , we compile several freely available works of contemporary fiction written between 1912 and 2010, spanning various genres, including mystery ( The Mysterious Affair at Styles , 3 Christie, 1921; The Secret Adversary , 4 Christie, 1922; Murder in the Gun Room , 5 Piper, 1953), humor ( Password Incorrect , 6 Name, 2008), western ( Rebel Spurs , 7 Norton, 1962), science fiction ( Seven Swords , 8 Shea, 2008; Living History , 9 Essex, 2016; The Sky Is Falling , 10 Del Rey, 1973; Youth , 11 Asimov, May 1952), and adventure ( Captain Blood , 12 Sabatini, 1922).",
  "We construct premise sentences from these ten source texts with minimal preprocessing; unique the sentences within genres, exclude very short 2 https://9-11commission.gov/ 3 gutenberg.org/files/863/863-0.txt 4 gutenberg.org/files/1155/1155-0.txt 5 gutenberg.org/files/17866/17866.txt 6 http://manybooks.net/pages/ namenother09password_incorrect/0.html 7 gutenberg.org/files/20840/20840-0.txt 8 http://mikeshea.net/stories/seven_ swords.html , shared with the author's permission.",
  "9 manybooks.net/pages/ essexbother10living_history/0.html 10 gutenberg.org/cache/epub/18768/ pg18768.txt 11 gutenberg.org/cache/epub/31547/ pg31547.txt 12 gutenberg.org/files/1965/1965-0.txt 1114 sentences (under eight characters), and manually remove certain types of non-narrative writing, such as mathematical formulae, bibliographic references, and lists.",
  "Although SNLI is collected in largely the same way as MultiNLI, and is also permissively licensed, we do not include SNLI in the MultiNLI corpus distribution.",
  "SNLI can be appended and treated as an unusually large additional CAPTIONS genre, built on image captions from the Flickr30k corpus (Young et al., 2014).",
  "Hypothesis Collection To collect a sentence pair, we present a crowdworker with a sentence from a source text and ask them to compose three novel sentences (the hypotheses): one which is necessarily true or appropriate whenever the premise is true (paired with the premise and labeled ENTAILMENT ), one which is necessarily false or inappropriate whenever the premise is true ( CONTRADICTION ), and one where neither condition applies ( NEUTRAL ).",
  "This method of data collection ensures that the three classes will be represented equally in the raw corpus.",
  "The prompts that surround each premise sentence during hypothesis collection are slightly tailored to fit the genre of that premise sentence.",
  "We pilot these prompts prior to data collection to ensure that the instructions are clear and that they yield hypothesis sentences that fit the intended meanings of the three classes.",
  "There are five unique prompts in total: one for written non-fiction genres (SLATE , OUP, GOVERNMENT , VERBATIM , TRAVEL ; Figure 1), one for spoken genres (TELEPHONE , FACE-TO-FACE ), one for each of the less formal written genres (FICTION , LETTERS ), and a specialized one for 9/11, tailored to fit its potentially emotional content.",
  "Each prompt is accompanied by example premises and hypothesis that are specific to each genre.",
  "Below the instructions, we present three text fieldsone for each labelfollowed by a field for reporting issues, and a link to the frequently asked questions (FAQ) page.",
  "We provide one FAQ page per prompt.",
  "FAQs are modeled on their SNLI counterparts (supplied by the authors of that work) and include additional curated examples, answers to genre-specific questions arising from our pilot phase, and information about logistical concerns like payment.",
  "For both hypothesis collection and validation, we present prompts to annotators using Hybrid Statistic SNLI MultiNLI Pairs w/ unanimous gold label 58.3% 58.2% Individual label = gold label 89.0% 88.7% Individual label = author's label 85.8% 85.2% Gold label = author's label 91.2% 92.6% Gold label 6 = author's label 6.8% 5.6% No gold label (no 3 labels match) 2.0% 1.8% Table 2: Key validation statistics for SNLI (copied from Bowman et al., 2015) and MultiNLI.",
  "( gethybrid.io ), a crowdsoucring platform similar to the Amazon Mechanical Turk platform used for SNLI.",
  "We used this platform to hire an organized group of workers.",
  "387 annotators contributed through this group, and at no point was any identifying information about them, including demographic information, available to the authors.",
  "Validation We perform an additional round of annotation on test and development examples to ensure accurate labelling.",
  "The validation phase follows the same procedure used for SICK (Marelli et al., 2014b) and SNLI: Workers are presented with pairs of sentences and asked to supply a single label ( ENTAILMENT , CONTRADICTION , NEUTRAL ) for the pair.",
  "Each pair is relabeled by four workers, yielding a total of five labels per example.",
  "Validation instructions are tailored by genre, based on the main data collection prompt (Figure 1); a single FAQ, modeled after the validation FAQ from SNLI, is provided for reference.",
  "In order to encourage thoughtful labeling, we manually label one percent of the validation examples and offer a $1 bonus each time a worker selects a label that matches ours.",
  "For each validated sentence pair, we assign a gold label representing a majority vote between the initial label assigned to the pair by the original annotator, and the four additional labels assigned by validation annotators.",
  "A small number of examples did not receive a three-vote consensus on any one label.",
  "These examples are included in the distributed corpus, but are marked with  ' in the gold label field, and should not be used in standard evaluations.",
  "Table 2 shows summary statistics capturing the results of validation, alongside corresponding figures for SNLI.",
  "These statistics indicate that the labels included in MultiNLI are about as reliable as those included in SNLI, despite MultiNLI's more diverse text contents.",
  "Table 1 shows randomly chosen development set examples from the collected corpus.",
  "Hypotheses tend to be fluent and correctly spelled, though not all are complete sentences.",
  "Punctuation is often omitted.",
  "Hypotheses can rely heavily on knowledge about the world, and often don't correspond closely with their premises in syntactic structure.",
  "Unlabeled test data is available on Kaggle for both matched and mismatched sets as competitions that will be open indefinitely; Evaluations on a subset of the test set have previously been conducted with different leaderboards through the RepEval 2017 Workshop (Nangia et al., 2017).",
  "The corpus is available in two formatstab separated text and JSON Lines ( jsonl ), following SNLI.",
  "For each example, premise and hypothesis strings, unique identifiers for the pair and prompt, and the following additional fields are specified:  gold label : label used for classification.",
  "In examples rejected during the validation process, the value of this field will be  '.",
  "sentence { 1,2 } parse : Each sentence as parsed by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003).",
  "sentence { 1,2 } binary parse : parses in unlabeled binary-branching format.",
  "label[1] : The label assigned during the creation of the sentence pair.",
  "In rare cases this may be different from gold label , if a consensus of annotators chose a different label during the validation phase.",
  "label[2...5] : The four labels assigned during validation by individual annotators to each development and test example.",
  "These fields will be empty for training examples.",
  "The current version of the corpus is freely available at nyu.edu/projects/bowman/multinli/ for typical machine learning uses, and may be modified and redistributed.",
  "The majority of the corpus is released under the OANC's license, which allows all content to be freely used, modified, and shared under permissive terms.",
  "The data in the FICTION section falls under several permissive licenses; Seven Swords is available under a Creative Commons Share-Alike 3.0 Unported License, and with the explicit permission of the author, Living History and Password Incorrect are available under Creative Commons Attribution 3.0 Unported Licenses; the remaining works of fiction are in the public domain in the United States (but may be licensed differently elsewhere).",
  "Partition The distributed corpus comes with an explicit train/test/development split.",
  "The test and development sets contain 2,000 randomly selected examples each from each of the genres, resulting in a total of 20,000 examples per set.",
  "No premise sentence occurs in more than one set.",
  "Statistics Table 3 shows some additional statistics.",
  "Premise sentences in MultiNLI tend to be longer (max 401 words, mean 22.3 words) than their hypotheses (max 70 words, mean 11.4 words), and much longer, on average, than premises in SNLI (mean 14.1 words); premises in MultiNLI also tend to be parsed as complete sentences at a much higher rate on average (91%) than their SNLI counterparts (74%).",
  "We observe that the two spoken genres differ in thiswith FACE-TO-FACE showing more complete sentences (91%) than TELEPHONE (71%)and speculate that the lack of visual feedback in a telephone setting may result in a high incidence of interrupted or otherwise incomplete sentences.",
  "Hypothesis sentences in MultiNLI generally cannot be derived from their premise sentences using only trivial editing strategies.",
  "While 2 .",
  "5 % of the hypotheses in SNLI differ from their premises by deletion, only 0 .",
  "9 % of those in MultiNLI (170 examples total) are constructed in this way.",
  "Similarly, in SNLI, 1 .",
  "6 % of hypotheses differ from their premises by addition, substitution, or shuf-fling a single word, while in MultiNLI this only happens in 1 .",
  "2 % of examples.",
  "The percentage of hypothesis-premise pairs with high token overlap ( > 37%) was comparable between MultiNLI (30% of pairs) and SNLI (29%).",
  "These statistics suggest that MultiNLI's annotations are comparable in quality to those of SNLI.",
  "To test the difficulty of the corpus, we experiment with three neural network models.",
  "The first is a simple continuous bag of words (CBOW) model in which each sentence is represented as the sum of the embedding representations of its words.",
  "The second computes representations by averaging the states of a bidirectional LSTM RNN (BiL-STM; Hochreiter and Schmidhuber, 1997) over words.",
  "For the third, we implement and evaluate Chen et",
  "al.'s Enhanced Sequential Inference Model (ESIM), which is roughly tied for the state of the art on SNLI at the time of writing.",
  "We use the base ESIM without ensembling with a TreeL-STM (as in the HIM' runs in that work).",
  "The first two models produce separate vector representations for each sentence and compute label predictions for pairs of representations.",
  "To do this, they concatenate the representations for premise and hypothesis, their difference, and their element-wise product, following Mou et al. (2016b), and pass the result to a single tanh layer followed by a three-way softmax classifier.",
  "All models are initialized with 300D reference GloVe vectors (840B token version; Pennington et al., 2014).",
  "Out-of-vocabulary (OOV) words are initialized randomly and word embeddings are fine-tuned during training.",
  "The models use 300D hidden states, as in most prior work on SNLI.",
  "We use Dropout (Srivastava et al., 2014) for regularization.",
  "For ESIM, we use a dropout rate of 0.5, following the paper.",
  "For CBOW and BiLSTM models, we tune Dropout on the SNLI development set and find that a drop rate of 0.1 works well.",
  "We use the Adam (Kingma and Ba, 2015) optimizer with default parameters.",
  "Code is available at github.com/nyu-mll/multiNLI/ .",
  "We train models on SNLI, MultiNLI, and a mixture; Table 4 shows the results.",
  "In the mixed setting, we use the full MultiNLI training set and randomly select 15% of the SNLI training set at each epoch, ensuring that each available genre is seen during training with roughly equal frequency.",
  "We also train a separate CBOW model on each individual genre to establish the degree to which simple models already allow for effective transfer across genres, using a dropout rate of 0.2.",
  "When training on SNLI, a single random sample of 15% of the original training set is used.",
  "For each genre represented in the training set, the model that performs best on it was trained on that genre; a model trained only on SNLI performs worse on every genre than comparable models trained on any genre from MultiNLI.",
  "Models trained on a single genre from MultiNLI perform well on similar genres; for example, the model trained on TELEPHONE attains the best accuracy (63%) on FACE-TO-FACE , which was nearly one point better than it received on itself.",
  "SLATE seems to be a difficult and relatively unusual genre and performance on it is relatively poor in this setting; when averaging over runs trained on SNLI and all genres in the matched section of the training set, average performance on SLATE was only 57.5%.",
  "Sentences in SLATE cover a wide range of topics and phenomena, making it hard to do well on, but also forcing models trained on it be broadly capable; the model trained on SLATE achieves the highest accuracy of any model on 9/11 (55.6%) and VERBATIM (57.2%), and relatively high accuracy on TRAVEL (57.4%) and GOVERNMENT (58.3%).",
  "We also observe that our models perform similarly on both the matched and mismatched test sets of MultiNLI.",
  "We expect genre mismatch issues to become more conspicuous as models are developed that can better fit MultiNLI's training genres.",
  "To evaluate the contribution of sentence length to corpus difficulty, we binned premises and hypotheses by length in 25-word increments for premises and 10-word increments for hypotheses.",
  "Using the ESIM model, our strong baseline, we find a small effect (stronger for matched than mismatched) of premise length on model accuracy: accuracy decreases slightly as premise sentences increase in length.",
  "We find no effect of hypothesis length on accuracy.",
  "In data collection for NLI, different annotator decisions about the coreference between entities and events across the two sentences in a pair can lead to very different assignments of pairs to labels (de Marneffe et al., 2008; Marelli et al., 2014a; Bowman et al., 2015).",
  "Drawing an example from Bowman et al., the pair a boat sank in the Pacific Ocean and a boat sank in the Atlantic Ocean can be labeled either CONTRADICTION or NEUTRAL depending on (among other things) whether the two mentions of boats are assumed to refer to the same entity in the world.",
  "This uncertainty can present a serious problem for inter-annotator agreement, since it is not clear that it is possible to define an explicit set of rules around coreference that would be easily intelligible to an untrained annotator (or any non-expert).",
  "Bowman et al. attempt to avoid this problem by using an annotation prompt that is highly dependent on the concreteness of image descriptions; but, as we engage with the much more abstract writing that is found in, for example, government documents, there is no reason to assume a priori that any similar prompt and annotation strategy can work.",
  "We are surprised to find that this is not a major issue.",
  "Through a relatively straightforward trial-and-error piloting phase, followed by discussion with our annotators, we manage to design prompts for abstract genres that yield high inter-annotator agreement scores nearly identical to those of SNLI (see Table 2).",
  "These high scores suggest that our annotators agreed on a single task definition, and were able to apply it consistently across genres.",
  "As expected, both the increase in the diversity of linguistic phenomena in MultiNLI and its longer average sentence length conspire to make MultiNLI dramatically more difficult than SNLI.",
  "Our three baseline models perform better on SNLI than MultiNLI by about 15% when trained on the respective datasets.",
  "All three models achieve accuracy above 80% on the SNLI test set when trained only on SNLI.",
  "However, when trained on MultiNLI, only ESIM surpasses 70% accuracy on MultiNLI's test sets.",
  "When we train models on MultiNLI and downsampled SNLI, we see an expected significant improvement on SNLI, but no significant change in performance on the MultiNLI test sets, suggesting including SNLI in training doesn't drive substantial improvement.",
  "These results attest to MultiNLI's difficulty, and with its relatively high inter-annotator agreement, suggest that it presents a problem with substantial headroom for future work.",
  "To better understand the types of language understanding skills that MultiNLI tests, we analyze the collected corpus using a set of annotation tags chosen to reflect linguistic phenomena which are known to be potentially difficult.",
  "We use two methods to assign tags to sentences.",
  "First, we use the Penn Treebank (PTB; Marcus et al., 1993) part-of-speech tag set (via the included Stanford Parser parses) to automatically isolate sentences 1118 Dev.",
  "containing a range of easily-identified phenomena like comparatives.",
  "Second, we isolate sentences that contain hand-chosen key words indicative of additional interesting phenomena.",
  "The hand-chosen tag set covers the following phenomena: QUANTIFIERS contains single words with quantificational force (see, for example, Heim and Kratzer, 1998; Szabolcsi, 2010, e.g., many, all, few, some ); BELIEFVERBS contains sentence-embedding verbs denoting mental states (e.g., know, believe, think ), including irregular past tense forms; TIME TERMS contains single words with abstract temporal interpretation, (e.g., then, today ) and month names and days of the week; DISCOURSE MARKERS contains words that facilitate discourse coherence (e.g., yet, however, but, thus, despite ); PRESUPPOSITIONTRIGGERS contains words with lexical presuppositions (Stal-naker, 1974; Schlenker, 2016, e.g., again, too, anymore 13 ); CONDITIONALS contains the word if .",
  "Table 5 presents the frequency of the tags in SNLI and MultiNLI, and model accuracy on MultiNLI (trained only on MultiNLI).",
  "The incidence of tags varies by genre; the percentage of sentence pairs containing a particular annotation tag differs by a maximum over 30% across genres.",
  "Sentence pairs containing pronouns are predictably common for all genres, with 93% of Government and Face-to-face pairs including at 13 Because their high frequency in the corpus, extremely common triggers like the were excluded from this tag.",
  "least one.",
  "The Telephone genre has the highest percentage of sentence pairs containing one occurrence of negation, WH-words, belief -verbs and time terms, Verbatim has the highest percentage of pairs containing quantifiers and conversational pivots, and Letters has the highest percentage of pairs that contain one or more modals.",
  "Pairs containing comparatives and/or superlatives, which is the tag that our baseline models perform worst on, are most common in the Oxford University Press genre.",
  "Based on this, we conclude that the genres are sufficiently different, because they are not uniform with respect to the percentages of sentence pairs that contain each of the annotation tags.",
  "The distributions of labels within each tagged subset of the corpus roughly mirrors the balanced overall distribution.",
  "The most frequent class overall (in this case, ENTAILMENT ) occurs with a frequency of roughly one third (see Table",
  "4) in most.",
  "Only two annotation tags differ from the baseline percentage of the most frequent class in the corpus by at least 5%: sentences containing negation, and sentences exceeding 20 words.",
  "Sentences that contain negation are slightly more likely than average to be labeled CONTRADICTION , reflecting a similar finding in SNLI, while long sentences are slightly more likely to be labeled ENTAILMENT .",
  "None of the baseline models perform substantially better on any tagged set than they do on the corpus overall, with average model accuracies on sentences containing specific tags falling within 1119 about 3 points of overall averages.",
  "Using baseline model test accuracy overall as a metric (see Table 4), our baseline models had the most trouble on sentences containing comparatives or superlatives (losing 3-4 points each).",
  "Despite the fact that 17% of sentence pairs in the corpus contained at least one instance of comparative or superlative, our baseline models don't utilize the information present in these sentences to predict the correct label for the pair, although presence of a comparative or superlative is slightly more predictive of a NEUTRAL label.",
  "Moreover, the baseline models perform below average on discourse markers, such as despite and however , losing roughly 2 to 3 points each.",
  "Unsurprisingly, the attention-based ESIM model performs better than the other two on sentences with greater than 20 words.",
  "Additionally, our baseline models do show slight improvements in accuracy on negation, suggesting that they may be tracking it as a predictor of CONTRADICTION .",
  "Natural language inference makes it easy to judge the degree to which neural network models for sentence understanding capture the full meanings for natural language sentences.",
  "Existing NLI datasets like SNLI have facilitated substantial advances in modeling, but have limited headroom and coverage of the full diversity of meanings expressed in English.",
  "This paper presents a new dataset that offers dramatically greater linguistic difficulty and diversity, and also serves as a benchmark for cross-genre domain adaptation.",
  "Our new corpus, MultiNLI, improves upon SNLI in its empirical coveragebecause it includes a representative sample of text and speech from ten different genres, as opposed to just simple image captionsand its difficulty, containing a much higher percentage of sentences tagged with one or more elements from our tag set of thirteen difficult linguistic phenomena.",
  "This greater diversity is reflected in the dramatically lower baseline model performance on MultiNLI than on SNLI (see Table",
  "5) and comparable inter-annotator agreement, suggesting that MultiNLI has a lot of headroom remaining for future work.",
  "The MultiNLI corpus was first released in draft form in the first half of 2017, and in the time since its initial release, work by others (Conneau et al., 2017) has shown that NLI can also be an effective source task for pre-training and transfer learning in the context of sentence-to-vector models, with models trained on SNLI and MultiNLI substantially outperforming all prior models on a suite of established transfer learning benchmarks.",
  "We hope that this corpus will continue to serve for many years as a resource for the development and evaluation of methods for sentence understanding.",
  "This work was made possible by a Google Faculty Research Award.",
  "SB also gratefully acknowledges support from Tencent Holdings and Samsung Research.",
  "We also thank George Dahl, the organizers of the RepEval 2016 and RepEval 2017 workshops, Andrew Drozdov, Angeliki Lazaridou, and our other NYU colleagues for help and advice."
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "other",
  "abstain",
  "abstain",
  "other",
  "abstain",
  "abstain",
  "abstain",
  "objective",
  "abstain",
  "objective",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "objective",
  "objective",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "other",
  "other",
  "other",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "method",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "method",
  "abstain",
  "abstain",
  "method",
  "other",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "objective",
  "objective",
  "abstain",
  "abstain",
  "other",
  "abstain",
  "other",
  "other",
  "other"
]</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="3"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "Intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks.",
  "In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails.",
  "We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action.",
  "We design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence learning, obtaining BLEU and ROUGE scores of 0 .",
  "23 and 0 .",
  "63 for this task.",
  "To the best of our knowledge, this is the first work to address the problem of composing To-Do items from emails.",
  "Email is one of the most used forms of communication especially in enterprise and work settings (Radicati and Levenstein, 2015).",
  "With the growing number of users in email platforms, service providers are constantly seeking to improve user experience for a myriad of applications such as online retail, instant messaging and event management (Feddern-Bekcan, 2008).",
  "Smart Reply (Kan-nan et al., 2016) and Smart Compose (Chen et al., 2019) are two recent features that provide contextual assistance to users aiming to reduce typing efforts.",
  "Another line of work in this direction is for automated task management and scheduling.",
  "For example.",
  "the recent Nudge feature 1 in Gmail and Insights in Outlook 2 are designed to remind users to follow-up on an email or pay attention to pending tasks.",
  "Smart To-Do takes a step further in task assistance and seeks to boost user productivity by automatically generating To-Do items from their email  Work done as an intern at Microsoft Research.",
  "1 Gmail Nudge 2 Outlook Insights From: Alice To: john@contoso.com Subject: Sales Report Hi John, From: John To: alice@contoso.com Subject: RE: Sales Report I am doing well.",
  "context.",
  "Text generation from emails, like creating To-Do items, is replete with complexities due to the diversity of conversations in email threads, heterogeneous structure of emails and various meta-deta involved.",
  "As opposed to prior works in text generation like news headlines, email subject lines and email conversation summarization, To-Do items are action-focused , requiring the identification of a specific task to be performed.",
  "In this work, we introduce the task of automatically generating To-Do items from email context and meta-data to assist users with following up on their promised actions (also referred to as commitments in this work).",
  "Refer to Figure 1 for an illustration.",
  "Given an email, its temporal context (i.e. thread), and associated meta-data like the name of the sender and recipient, we want to generate a short and succinct To-Do item for the task mentioned in the email.",
  "This requires identifying the task sentence (also referred to as a query ), relevant sentences in the email that provide contextual information about the query along with the entities (e.g., people) associated with the task.",
  "We utilize existing work to identify the task sentence via a commitment classifier that detects action intents in the emails.",
  "Thereafter C Commitment Classifier D Does the email contain commitment ?",
  "we use an unsupervised technique to extract key sentences in the email that are helpful in providing contextual information about the query.",
  "These pieces of information are further combined to generate the To-Do item using a sequence-to-sequence architecture with deep neural networks.",
  "Figure 2 shows a schematic diagram of the process.",
  "Since there is no existing work or dataset on this problem, our first step is to collect annotated data for this task.",
  "Overall, our contributions can be summarized as follows:  We create a new dataset for To-Do item generation from emails containing action items based on the publicly available email corpus Avocado (Oard et al., 2015).",
  "3  We develop a two-stage algorithm, based on unsupervised task-focused content selection and subsequent text generation combining contextual information and email meta-data.",
  "We conduct experiments on this new dataset and show that our model performs at par with human judgments on multiple performance metrics.",
  "Summarization of email threads has been the focus of multiple research works in the past (Rambow et al., 2004; Carenini et al., 2007; Dredze et al., 2008).",
  "There has also been considerable research on identifying speech acts or tasks in emails (Car-valho and Cohen, 2005; Lampert et al., 2010; Scerri et al., 2010) and how it can be robustly adapted across diverse email corpora (Azarbonyad et al., 2019).",
  "Recently, novel neural architectures have been explored for modeling action items in emails 3 We will release the code and data (in accordance with LDC and Avocado policy) at https://aka.ms/SmartToDo .",
  "Email examples in this paper are similar to those in our dataset but are not reproducing text from the Avocado dataset.",
  "(Lin et al., 2018) and identifying intents in email conversations (Wang et al., 2019).",
  "However, there has been less focus on task-specific email summarization (Corston-Oliver et al., 2004).",
  "The closest to our work is that of email subject line generation (Zhang and Tetreault, 2019).",
  "But it focuses on a common email theme and uses a supervised approach for sentence selection, whereas our method relies on identifying the task-related context.",
  "We build upon the Avocado dataset (Oard et al., 2015) 4 containing an anonymized version of the Outlook mailbox for 279 employees with various meta-data and 938 , 035 emails overall.",
  "Emails contain various user intents including planning and scheduling meetings, requests for information, exchange of information, casual conversations, etc. (Wang et al., 2019).",
  "For the purpose of this work, we first need to extract emails containing at least one sentence where the sender has promised to perform an action.",
  "It could be performing a task, providing some information, keeping others informed about a topic and so on.",
  "We use the term commitment to refer to such intent in an email and the term commitment sentence to refer to each sentence with that intent.",
  "Commitment classifier: A commitment classifier C : S (cid:55) [0 , 1] takes as input an email sentence S and returns a probability of whether the sentence is a commitment or not.",
  "The classifier is built using labels from an annotation task with 3 judges.",
  "The Cohen's kappa value is 0 .",
  "694 , depicting substantial agreement.",
  "The final label is obtained from the majority vote, generating a total of 9076 instances (with 2586 positive/commitment labels and 6490 negative labels).",
  "The classifier is an RNN-based model with word embeddings and self-attention geared for binary classification with the input being the entire email context (Wang et al., 2019).",
  "The classifier has a precision of 86% and recall of 84% on sentences in the Avocado corpus.",
  "Candidate emails: We extracted 500 k raw sentences from Avocado emails and passed them",
  "4 Avocado is a more appropriate test bed than the Enron collection (Klimt and Yang, 2004) since it contains additional meta-data and it entered the public domain via the cooperation and consent of the legal owner of the corpus.",
  "through the commitment classifier.",
  "We threshold the commitment classifier confidence to 0 .",
  "9 and obtained 29 k potential candidates for To-Do items.",
  "Of these, a random subset of 12 k instances were selected for annotation.",
  "Annotation guideline: For each candidate email e c and the previous email in the thread e p (if present), we obtained meta-data like  From ',  Sent-To ',  Subject ' and  Body '.",
  "The commitment sentence in e c was highlighted and annotators were asked to write a To-Do item using all of the information in e c and e p .",
  "We prepared a comprehensive guideline to help human annotators write To-Do Items containing the definition and structure of To-Do Items and commitment sentences, along with illustrative examples.",
  "Annotators were instructed to use words and phrases from the email context as closely as possible and introduce new vocabulary only when required.",
  "Each instance was annotated by 2 judges.",
  "Analysis of human annotations: We obtained a total of 9349 email instances with To-Do items, each of which was annotated by two annotators.",
  "To-Do items have a median token length of 9 and a mean length of 9 .",
  "71 .",
  "For 60 .",
  "42% of the candidate emails, both annotators agreed that the subject line was helpful in writing the To-Do Item.",
  "To further analyze the annotation quality, we randomly sampled 100 annotated To-Do items and asked a judge to rate them on",
  "(a) fluency (grammat-ical and spelling correctness), and",
  "(b) completeness (capturing all the action items in the email) on a 4 point scale ( 1 : Poor, 2 : Fair, 3 : Good, 4 : Excellent).",
  "Overall, we obtained a mean rating of 3.1 and 2.9 respectively for fluency and completeness.",
  "Table 1 shows a snapshot of the analysis.",
  "In this section, we describe our two-stage approach to generate To-Do items.",
  "In the first stage, we select sentences that are helpful in writing the To-Do item.",
  "Emails contain generic sentences such as salutations, thanks and casual conversations not relevant to the commitment task.",
  "The objective of the first stage is to select sentences containing informative concepts necessary to write the To-Do.",
  "In the absence of reliable labels to extract helpful sentences in a supervised fashion, we resort to an unsupervised matching-based approach.",
  "Let the commitment sentence in the email be denoted as H , and the rest of the sentences from the current email e c and previous email e p be denoted as { s 1 , s 2 , . . . s d } .",
  "The unsupervised approach seeks to obtain a relevance score ( s i ) for each sentence.",
  "The top K sentences with the highest scores will be selected as the extractive summary for the commitment sentence (also referred to as the query).",
  "Enriched query context: We first extract top  maximum frequency tokens from all the sentences in the given email, the commitment and the subject (i.e., { s 1 , s 2 , . . . s d }  H  Subject ).",
  "Tokens are lemmatized and stop-words are removed.",
  "We set  = 10 in our experiments.",
  "An enriched context for the query E is formed by concatenating the commitment sentence H , subject and top  tokens.",
  "Relevance score computation: Task-specific relevance score  for a sentence s i is obtained by inner product in the embedding space with the enriched context.",
  "Let h (  ) be the function denoting the embedding of a sentence with ( s i ) = h ( s i ) T h ( E ) .",
  "Our objective is to find helpful sentences for the commitment given by semantic similarity between concepts in the enriched context and a target sentence.",
  "In case of a short or less informative query, the subject and topic of the email provide useful information via the enriched context.",
  "We experiment with three different embedding functions.",
  "frequency vector is used to represent the sentence.",
  "(2) FastText Word Embeddings  We trained FastText embeddings (Bojanowski et al., 2017) of dimension 300 on all sentences in the Avocado corpus.",
  "The embedding function h ( s j ) is given by taking the max (or mean) across the word-embedding dimension of all tokens in the sentence s j .",
  "(3) Contextualized Word Embeddings  We utilize recent advances in contextualized representations from pre-trained language models like BERT (Devlin et al., 2019).",
  "We use the second last layer of pre-trained BERT for sentence embeddings.",
  "We also fine-tuned BERT on the labeled dataset for commitment classifier.",
  "The dataset is first made balanced ( 2586 positive and 2586 negative instances).",
  "Uncased BERT is trained for 5 epochs for commitment classification, with the input being word-piece tokenized email sentences.",
  "This model is denoted as BERT (fine-tuned) in Table 2. Evaluation of unsupervised approaches: Retrieving at-least one helpful sentence is crucial to obtain contextual information for the To-Do item.",
  "Therefore, we evaluate our approaches based on the proportion of emails where at-least one helpful sentence is present in the top K retrieved sentences.",
  "We manually annotated 100 email instances and labeled every sentence as helpful or not based on",
  "(a) whether the sentence contains concepts appearing in the target To-Do item, and",
  "(b) whether the sentence helps to understand the task context.",
  "Inter-annotator agreement between 2 judgments for this task has a Cohen Kappa score of 0 .",
  "69 .",
  "This annotation task also demonstrates the importance of the previous email in a thread.",
  "Out of 100 annotated instances, 44 have a replied-to email of which 31 contains a helpful sentence in the replied-to email body ( 70 . 4% ).",
  "Table 2 shows the performance of the various unsupervised extractive algorithms.",
  "FastText with max-pooling of embeddings performed the best and used in the subsequent generation stage.",
  "The generation phase of our approach can be formulated as sequence-to-sequence (Seq2Seq) learning with attention (Sutskever et al., 2014; Bahdanau et al., 2014).",
  "It consists of two neural networks, an encoder and a decoder.",
  "The input to the encoder consists of concatenated tokens from different meta-data fields of the email like sent-to', subject', commitment sentence H and extracted sentences I separated by special markers.",
  "For instance, the input to the encoder for the example in Figure 1 is given as: &lt; to > alice &lt; sub > hello ?",
  "generation model as follows: Vanilla Seq2Seq : Input tokens { x 1 , x 2 , . . . x T } are passed through a word-embedding layer and a single layer LSTM to obtain encoded representations h t = f ( x t , h t  1 )  t for the input.",
  "The decoder is another LSTM that makes use of the encoder state h t and prior decoder state s t  1 to generate the target words at every timestep t .",
  "We consider Seq2Seq with attention mechanism where the decoder LSTM uses attention distribution a t over timesteps t to focus on important hidden states to generate the context vector h t .",
  "This is the first baseline in our work.",
  "e t,t (cid:48) = v T tanh ( W h  h t + W s  s t (cid:48) + b ) a t,t (cid:48) = softmax ( e t,t (cid:48) ) h t = (cid:80) t (cid:48) a t,t (cid:48)  h t (cid:48) (1) Seq2Seq with copy mechanism : As the second model, we consider Seq2Seq with copy mechanism (See et al., 2017) to copy tokens from important email fields.",
  "Copying is pivotal for To-Do item generation since every task involves named From: John Carter To: Helena Watson; Daniel Craig; Rupert Grint Subject: Thanks Thank you for helping me prepare the paper draft for ACL conference.",
  "entities in terms of the persons involved, specific times and dates when the task has to be accomplished and other task-specific details present in the email context.",
  "To understand the copy mechanism, consider the decoder input at each decoding step as y t and the context vector as h t .",
  "The decoder at each timestep t has the choice of generating the output word from the vocabulary V with probability p gen =  ( h t , s t , y t ) , or with probability 1  p gen it can copy the word from the input context.",
  "To allow that, the vocabulary is extended as V (cid:48) = V  { x 1 , x 2 , . . . x T } .",
  "The model is trained end-to-end to maximize the log-likelihood of target words (To-Do items) given the email context.",
  "Seq2Seq BiFocal : As a third model, we experimented with query-focused attention having two encoders  one containing only tokens of the query and the other containing rest of the input context.",
  "We use a bifocal copy mechanism that can copy tokens from either of the encoders.",
  "We refer the reader to the Appendix for more details about training and hyper-parameters used in our models.",
  "9349 email instances with To-Do items, we used 7349 for training and 1000 each for validation and testing.",
  "For each instance, we chose the annotation with fewer tokens as ground-truth reference.",
  "The median token length of the encoder input is 43 (including the helpful sentence).",
  "Table 4 shows the performance comparison of various models.",
  "We report BLEU-4 (Papineni et al., 2002) and the F1-scores for Rouge-1, Rouge-2 and Rouge-L (Lin, 2004).",
  "We also report the human performance for this task in terms of the above metrics computed between annotations from the two judges.",
  "A trivial baseline  which concatenates tokens from the sent-to' and subject' fields and the commitment sentence  is included for comparison.",
  "The best performance is obtained with Seq2Seq using copying mechanism.",
  "We observe our model to perform at par with human performance for writing To-Do items.",
  "Table 3 shows some examples of To-Do item generation from our best model.",
  "In this work, we study the problem of automatic To-Do item generation from email context and meta-data to provide smart contextual assistance in email applications.",
  "To this end, we introduce a new task and dataset for action-focused text intelligence.",
  "We design a two stage framework with deep neural networks for task-focused text generation.",
  "There are several directions for future work including better architecture design for utilizing structured meta-data and replacing the two-stage framework with a multi-task generation model that can jointly identify helpful context for the task and perform corresponding text generation."
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "abstain",
  "objective",
  "objective",
  "result",
  "abstain",
  "abstain",
  "objective",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "objective",
  "abstain",
  "objective",
  "objective",
  "other",
  "other",
  "other",
  "abstain",
  "other",
  "other",
  "abstain",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "other",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "result",
  "result",
  "method",
  "objective",
  "method",
  "abstain"
]</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="4"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "Semantic parsing using sequence-to-sequence models allows parsing of deeper representations compared to traditional word tagging based models.",
  "In spite of these advantages, widespread adoption of these models for real-time conversational use cases has been stymied by higher compute requirements and thus higher latency.",
  "In this work, we propose a non-autoregressive approach to predict semantic parse trees with an efficient seq2seq model architecture.",
  "By combining non-autoregressive prediction with convolutional neural networks, we achieve significant latency gains and parameter size reduction compared to traditional RNN models.",
  "Our novel architecture achieves up to an 81% reduction in latency on TOP dataset and retains competitive performance to non-pretrained models on three different semantic parsing datasets.",
  "Our code is available at https://github.",
  "com/facebookresearch/pytext .",
  "Advances in conversational assistants have helped to improve the usability of smart speakers and consumer wearables for different tasks.",
  "Semantic parsing is one of the fundamental components of these assistants and it helps to convert the user input in natural language to a structure representation that can be understood by downstream systems.",
  "Majority of the semantic parsing systems deployed on various devices, rely on server-side inference because of the lower compute/memory available on these edge devices.",
  "This poses a few drawbacks such as flaky user experience with spotty internet connectivity and compromised user data privacy due to the dependence on a centralized server to which all user interactions are sent to.",
  "Thus, semantic parsing on-device has numerous advantages.",
  "For the semantic parsing task, the meaning representation used decides the capabilities of the system built.",
  "Limitations of the representation with one intent and slot labels were studied in the context of nested queries and multi turn utterances in Aghajanyan et al. (2020) and Gupta et al. (2018).",
  "New representations were proposed to overcome these limitations and sequence-to-sequence models were proposed as the solution to model these complex forms.",
  "But using these new models in real-time conversational assistants still remains a challenge due to higher latency requirements.",
  "In our work, we propose a novel architecture and generation scheme to significantly improve the end2end latency of sequence-to-sequence models for the semantic parsing task.",
  "Due to the autoregressive nature of generation in sequence-to-sequence semantic parsing models, the recurrence relationship between target tokens creates a limitation that decoding cannot be parallelized.",
  "There are multiple works in machine translation which try to solve this problem.",
  "These approaches relax the decoder token-by-token generation by allowing multiple target tokens to be generated at once.",
  "Fully non-autoregressive models (Gu et al., 2017; Ma et al., 2019; Ghazvininejad et al., 2020a; Saharia et al., 2020) and conditional masked language models with iterative decoding (Ghazvinine-jad et al., 2019; Gu et al., 2019; Ghazvininejad et al., 2020b) are some of them.",
  "To enable non-autoregressive generation in semantic parsing, we modify the objective of the standard seq2seq model to predict the entire target structure at once.",
  "We build upon the CMLM (Con-ditional Masked Language Model) (Ghazvininejad et al., 2019) and condition the generation of the full target structure on the encoder representation.",
  "By eliminating the recurrent relationship between individual target tokens, the decoding process can be parallelized.",
  "While this drastically improves latency, the representation of each token is still dependent on previous tokens if we continue to use an RNN architecture.",
  "Thus, we propose a novel model architecture for semantic parsing based on convolutional networks (Wu et al., 2019b) to solve this issue.",
  "Our non-autoregressive model achieves up to an 81% reduction in latency on the TOP dataset (Gupta et al., 2018), while achieving 80.23% exact match accuracy.",
  "We also achieve 88.16% exact match accuracy on DSTC2 (Henderson et al., 2014) and 80.86% on SNIPS (Coucke et al., 2018) which is competitive to prior work without pretraining.",
  "To summarize, our two main contributions are:  We propose a novel alternative to the traditional autoregressive generation scheme for semantic parsing using sequence-to-sequence models.",
  "With a new model training strategy and generation approach, the semantic parse structure is predicted in one step improving parallelization and thus leading to significant reduction in model latency with minimal accuracy impact.",
  "We also study the limitations of original CMLM (Ghazvininejad et al., 2019) when applied for conversational semantic parsing task and provide motivations for our simple yet critical modifications.",
  "We propose LightConv Pointer, a model architecture for non-autoregressive semantic parsing, using convolutional neural networks which provides significant latency and model size improvements over RNN models.",
  "Our novel model architecture is particularly suitable for limited compute use-cases like on-device conversational assistants.",
  "In this section, we propose a novel, convolutional, non-autoregressive architecture for semantic parsing.",
  "While non-autoregressive decoding has been previously explored in machine translation, we describe how it can be applied to semantic parsing with several critical modifications to retain performance.",
  "We then describe our convolutional architecture.",
  "By incorporating these advances together, our approach achieves both high accuracy and efficient decoding.",
  "The task is to predict the semantic parse tree given the raw text.",
  "We use the decoupled representation (Aghajanyan et al., 2020), an extension of the compositional form proposed in Gupta et al. (2018) for task oriented semantic parsing.",
  "Decoupled representation is obtained by removing all text in the compositional form that does not appear in a leaf slot.",
  "Efficient models require representations which are compact, with least number of tokens, to reduce number of floating point operations during inference.",
  "Decoupled representation was found to be suitable due to this.",
  "Figure 1 shows the semantic parse for a sample utterance.",
  "Our model predicts the serialized representation of this tree which is [IN:CREATE_REMINDER [SL:PERSON_REMINDED me ] [SL:TODO [IN:CREATE_CALL [SL:METHOD call ] [SL:CONTACT John ] ] ] ] 2.1 Non-Autoregressive Decoding While autoregressive models (Figure 2), which predict a sequence token by token, have achieved strong results in various tasks including semantic parsing, they have a large downside.",
  "The main challenge in practical applications is the slow decoding time.",
  "We investigate how to incorporate recent advances in non-autoregressive decoding for efficient semantic parsing models.",
  "We build upon the Conditional Masked Language Model (CMLM) proposed in Ghazvininejad et al. (2019) by applying it to the structured prediction task of semantic parsing for task-oriented dialog.",
  "Ghazvininejad et al. (2019) uses CMLM to first predict a token-level representation for each source token and a target sequence length; then the model predicts and iterates on the target sequence prediction in a non-autoregressive fashion.",
  "We describe our changes and the motivations for these changes below.",
  "One of the main differences between our work and Ghazvininejad et al. (2019) is that target length prediction plays a more important role in semantic parsing.",
  "For the translation task, if the target length is off by one or more, the model can slightly rephrase the sentence to still return a high quality translation.",
  "In our case, if the length prediction is Figure 2: Traditional Sequence to Sequence architecture which uses autoregressive generation scheme for decoder.",
  "To resolve this important challenge, we propose a specialized length prediction module that more accurately predicts the target sequence length.",
  "While Ghazvininejad et al. (2019) uses a special CLS token in the source sequence to predict the target length, we have a separate module of multiple layers of CNNs with gated linear units to predict the target sequence length (Wu et al., 2019b).",
  "We also use label smoothing and differently weighing losses as explained in section 2.3, to avoid the easy over-fitting in semantic parsing compared to translation.",
  "As shown in Aghajanyan et al. (2020), transformers without pre-training perform poorly on TOP dataset.",
  "The architectural changes that we propose to solve the data efficiency can be found in the section 2.2.1.",
  "Further, we find that the random masking strategy proposed in Ghazvininejad et al. (2019) works poorly for semantic parsing.",
  "When we use the same strategy for the semantic parsing task where the output has a structure, model is highly likely to see invalid trees during training as masking random tokens in the linearized representation of a tree mostly gives invalid tree representations.",
  "This makes it hard for the model to learn the structure especially when the structure is complicated (in the case of trees, deep trees were harder to learn).",
  "To remedy this problem, we propose a different strategy for model training where all the tokens in the target sequence are masked during training.",
  "Our model architecture (Figure 3) is based on the classical seq2seq model (Sutskever et al., 2014) and follows the encoder-decoder architecture.",
  "In order to optimize for efficient encoding and decoding, we look to leverage a fully parallel model architecture.",
  "While transformer models are fully parallel and popular in machine translation (Vaswani et al., 2017), they are known to perform poorly in low resource settings and require careful tuning using techniques like Neural Architecture Search to get good performance (van Biljon et al., 2020; Murray et al., 2019).",
  "Similarly, randomly initialized transformers performed poorly on TOP dataset achieving only 64.5 % accuracy when SOTA was above 80% (Aghajanyan et al., 2020).",
  "We overcome this limitation by augmenting Transformers with Convolutional Neural Networks.",
  "Details of our architecture are explained below.",
  "For token representations, we use word embeddings concatenated with the sinusoidal positional embeddings (Vaswani et al., 2017).",
  "Encoder and decoder consist of multiple layers with residual connections as shown in Figure 4.",
  "First sub-block in each layer consists of MHA (Vaswani et al., 2017).",
  "In decoder, we do not do masking of future tokens during model training.",
  "This is needed for non-autoregressive generation of target tokens during inference.",
  "Second sub-block consists of multiple convolutional layers.",
  "We use depthwise convolutions with weight sharing (Wu et al., 2019b).",
  "Convolution layer helps in learning representation for tokens for a fixed context size and multiple layers helps with bigger receptive fields.",
  "We use non-causal convolutions for both encoder as well as decoder.",
  "Third sub-block is the FFN (Vaswani et al., 2017; Wu et al., 2019b) which consists of two linear layers and relu.",
  "The decoder has source-target attention after the convolution layer.",
  "Pointer-Generator Projection layer The decoder has a final projection layer which generates the target tokens from the decoder/encoder representations.",
  "Rongali et al. (2020) proposes an idea based Pointer Generator Network (See et al., 2017) to convert the decoder representation to target tokens using the encoder output.",
  "Similarly, we use a pointer based projection head, which decides whether to copy tokens from the source-sequence or generate from the pre-defined ontology at every Figure 3: Sequence to Sequence model architecture which uses Non-Autoregressive strategy for generation decoding step (Aghajanyan et al., 2020).",
  "Length Prediction Module Length prediction Module receives token level representations from the encoder as input.",
  "It uses stacked CNNs with gated linear units and mean pooling to generation the length prediction.",
  "Suppose the source sequence is of length L and source tokens in the raw text are s 1 , s 2 , s 3 . . . s L .",
  "Encoder generates a representation of for each token in the source sequence.",
  "Using the predicted length T, we create a target sequence of length T consisting of identical MASK tokens.",
  "This sequence is passed through possibly multiple decoder layers and generates a representation for each token in the masked target sequence.",
  "We make a strong assumption that each token in the target sentence is conditionally independent of each other given the source and the target length.",
  "Thus, the individual probabilities for each token is P ( y i | X, T ) where X is the input sequence and T is the length of target sequence.",
  "Beam Search During inference, length prediction module explained in 2.2.1 predicts top k lengths.",
  "For each predicted length, we create a decoder input sequence of all masked tokens.",
  "This is similar to the beam search with beam size k in autoregressive systems.",
  "The main difference in our model architecture is that we expect only one candidate for each predicted length.",
  "These all masked sequences are given as input to the model and the model predicts target tokens for each masked token.",
  "Once we have predicted target sequences for k different lengths, they are ranked based on the ranking algorithm described in (5), where X is the input sequence and Y is the predicted output sequence, note the predicted token y i is conditioned on both the sequence ( X ) and the predicted target length T .",
  "During training, we jointly optimize for two weighted losses.",
  "The first loss is calculated for the predicted target tokens against the real target and the second loss is calculated for predicted target length against real target length.",
  "During forward-pass, we replace all the tokens in the target sequence with a special &lt;MASK> token and give this as an input to the decoder.",
  "Decoder predicts the token for each masked token and the cross-entropy loss is calculated for each predicted token.",
  "The length prediction module in the model predicts the target length using the encoder representation.",
  "Similar to CMLMs in (Ghazvininejad et al., 2019), length prediction is modeled as a classifica-tion task with class labels for each possible length.",
  "Cross entropy loss is calculated for length prediction.",
  "For our semantic parsing task, label smoothing (Szegedy et al., 2015) was found to be very critical as the length prediction module tends to easily overfit and strong regularization methods are needed.",
  "This was because length prediction was a much well-defined task compared to predicting all the tokens in the sequence.",
  "Total loss was calculated by taking a weighted sum of cross entropy loss for labels and length, with lower weight for length loss.",
  "As training progresses through different epochs, the best model is picked by comparing the exact match (EM) accuracy of different snapshots on validation set.",
  "We use 3 datasets across various domains to evaluate our semantic parsing approach.",
  "Length distribution of each dataset is described using median, 90th percentile and 99th percentile lengths.",
  "TOP Dataset Task Oriented Parsing (Gupta et al., 2018) is a dataset for compositional utterances in the navigation and events domains.",
  "The training set consists of 31 , 279 instances and the test set consists of 9 , 042 .",
  "The test set has a median target length of 15, P90 27 and P99 39.",
  "SNIPS The SNIPS (Coucke et al., 2018) dataset is a public dataset used for benchmarking semantic parsing intent slot models.",
  "This dataset is considered flat, since it does not contain compositional queries and can be solved with word-tagging models.",
  "Recently, however seq2seq models have started to out perform word-tagging models (Rongali et al., 2020; Aghajanyan et al., 2020).",
  "The training set consists of 13 , 084 instances, the test set consists of 700 instances.",
  "The test set has a median target length of 11, P90 17, P99 21.",
  "DSTC2 Dialogue State Tracking Challenge 2 (Henderson et al., 2014), is a dataset for conversational understanding.",
  "The dataset involves users searching for restaurants, by specifying constraints such as cuisine type and price range, we encode these constraints as slots and use this to formulate the decoupled representation.",
  "The training set consists of 12 , 611 instances and a test set of 9890 .",
  "The test set has a median target length of 6, P90 9 and P99 10.",
  "Semantic Parsing Performance For all our datasets, we convert the representation of either the compositional form or flat intent slot form to the decoupled representation (Aghajanyan et al., 2020) .",
  "We compare the model prediction with the serialized structure representation and look for exact match (EM).",
  "Benchmarking Latency For the latency analysis for the models trained from scratch: AR LightConv Pointer, NAR LightConv Pointer, and BiLSTM.",
  "We chose these 3 architectures, to compare NAR vs AR variants of LightConv Pointer, as well as the best performant baseline: Pointer BiLSTM (Aghajanyan et al., 2020).",
  "We use Samsung Galaxy S8 with Android OS and Octa-core processor.",
  "We chose to benchmark latency to be consistent with prior work on on-device modeling (Wu et al., 2019a; Howard et al., 2019).",
  "All models are trained in PyTorch (Paszke et al., 2019) and exported using Torchscript.",
  "We measure wall clock time as it is preferred instead of other options because it relates more to real world inference.",
  "1 Latency results can be found in section 4.2.",
  "For each of our datasets, we report accuracy metrics on the following models:",
  "NAR LightConv Pointer : A non-autoregressive (NAR) variant of the above model to allow for parallel decoding.",
  "We compare against the best reported numbers across datasets where the models don't use pretraining.",
  "During training of our model we use the same base model across all datasets and sweep over hyper parameters for the length module and the batch size and learning rate, an equivalent sweep was done for the AR variant as well.",
  "The base model we use for NAR LightConv Pointer model uses 5 encoder layers with convolutional kernel sizes [3,7,15,21,27], where each encoder layer has embedding and convolutional dimensions of 160, 1 self attenion head, and 2 decoder layers with kernel sizes [7,27], and embedding dimension of 160, 1 self-attention head and 2 encoder-attention heads.",
  "Our length prediction module leverages a two convolution layers of 512 embedding dimensions and kernel sizes of 3 and 9.",
  "and uses hidden dimension in [128,256,512] determined by hyper parameter sweeps.",
  "We also use 8 attention heads for the decoupled projection head.",
  "For the convolutional layer, we use lightweight convolutions (Wu et al., 2019b) with number of heads set to 2.",
  "We train with the Adam (Kingma and Ba, 2014) optimizer, learning rate is selected to be between [0.00007, 0.0004].",
  "If our evaluation accuracy has not increased in 10 epochs, we also reduce our learning rate by a factor of 10, and we employ early stopping if the accuracy has not changed in 20 epochs.",
  "We train with our batch size fixed to be 8.",
  "We optimize a joint loss for label prediction and length prediction.",
  "Both losses consist of label smoothed cross entropy loss (  is the weight of the uniform distribution) (Pereyra et al., 2017), our label loss has  = 0 .",
  "1 and our length loss has  = 0 .",
  "5 , we also weight our length loss lower,  = 0 .",
  "25 .",
  "For inference, we use a length beam size of k = 5 .",
  "Our AR variant follows the same parameters however it does not have length prediction and self-attention in encoder and decoder.",
  "We show that our proposed non-autoregressive convolutional architecture for semantic parsing is competitive with auto-regressive baselines and word tagging baselines without pre-training on three different benchmarks and reduces latency up to 81% on the TOP dataset.",
  "We first compare accuracy and latency, then discuss model performance by analyzing errors by length, and the importance of knowledge distillation.",
  "We do our analysis on the TOP dataset, due to its inherent compositional nature, however we expect our analysis to hold for other datasets as well.",
  "Non-compositional datasets like DSTC2 and SNIPS can be modeled by word tagging models making seq2seq models more relevant in the case of compositional datasets.",
  "In table 5a we show our NAR and AR variants for LightConv Pointer perform quite similarly across all datasets.",
  "We can see that our proposed NAR LightConv Pointer is also competitive with state of the art models without pre-training: -0.66% TOP, -0.17% DSTC2, -4.57% SNIPS (-0.04% compared to word tagging models).",
  "Following the prior work on Non-Autoregressive models, we also report our experiments with sequence-level knowledge distillation in subsection Knowledge Distillation under section.",
  "4.3.",
  "In figure 5b we show the latency of our model with different generation approaches (NAR vs AR) over increasing target sequence lengths on the TOP dataset.",
  "Firstly, we show that our LightConv Pointer is significantly faster than the BiLSTM baseline (Aghajanyan et al., 2020), achieving up to a 54% reduction in median latency.",
  "BiLSTM was used as baseline as that was the SOTA without pretraining for TOP and Transformers performed poorly.",
  "By comparing our model with AR and NAR generation strategy, it can be seen that increase in latency with increase in target length is much smaller for NAR due to better parallelization of decoder, resulting in up to an 81% reduction in Length Bucket NAR (%) AR (%) Bucket Size &lt; 10 82.80 83.13 2798 10-20 84.18 84.36 5167 20-30 62.50 65.72 992 30-40 21.25 41.25 80 > 40 0.00 20.00 5 Table 2: EM accuracy of the NAR LightConv Pointer (distilled) vs AR LightConv Pointer distilled across different target length buckets along with the number of instances in each bucket on the TOP dataset.",
  "median latency compared to the BiLSTM model.",
  "Also note that both the LightConv Pointer models are able to achieve parity in terms of EM Accuracy compared to the baseline BiLSTM model, while using many fewer parameters, the BiLSTM model uses 20M parameters, while the NAR LightConv Pointer uses 12M and the AR LightConv Pointer uses 10M.",
  "Ablation experiments We compare the modifications proposed by this work (LightConv, Conv length prediction module and Mask everything strategy) with the original model proposed in Ghazvininejad et al. (2019) in table 1.",
  "The motivations for each modification was already discussed in sub-section 2.1.",
  "Our mean EM accuracy results based on 3 trials show the significance of techniques proposed in this paper especially for longer target sequences.",
  "Errors by length It is known that non-autoregressive models have difficulty at larger sequence lengths (Ghazvininejad et al., 2019).",
  "In table 2, we show our model's accuracy in each respective length bucket on the TOP dataset.",
  "We see that the AR and NAR model follow a similar distribution of errors, however the NAR model seems to error at a higher rate for the longer lengths.",
  "Knowledge Distillation Following prior work (Ghazvininejad et al., 2019; Zhou et al., 2020), we train our model with sequence-level knowledge distillation (Kim and Rush, 2016).",
  "We train our system on data generated by the current SOTA autoregressive models BART (Lewis et al., 2019; Aghajanyan et al., 2020).",
  "In table 3 we show the impact of knowledge distillation in our task on both the non-autoregressive and autoregressive variants of LightConv Pointer.",
  "These results support prior work in machine translation for distillation of au-Figure 6: Distilled NAR LightConv Pointer Top-K accuracy for exact match (EM) accuracy (blue) and Top-K length accuracy (orange), as well as the EM accuracy with gold length (dotted red line) for the TOP dataset.",
  "toregressive teachers to non-autoregressive models showing distillation improving our models on TOP and SNIPS, however we notice minimal changes on DSTC2.",
  "The importance of length prediction An important part of our non-autoregressive model is length prediction.",
  "In figure 6, we report exact match accuracy @ top k beams and length accuracy @ top k beams (where top K refers to whether the correct answer was in the top K predictions) for the TOP dataset.",
  "We can see a tight correlation between our length accuracy and exact match accuracy, showing how our model is bottle necked by the length prediction.",
  "Providing gold length as a feature, led to an exact match accuracy of 88.20% (shown in red on figure 6), an absolute 7.31 point improvement over our best result with our non-autoregressive LightConv Pointer.",
  "Non-autoregressive Decoding Recent work in machine translation has made a lot of progress in fully non-autoregressive models (Gu et al., 2017; Ma et al., 2019; Ghazvininejad et al., 2020a; Saharia et al., 2020) and parallel decoding (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019; Ghazvininejad et al., 2020b; Kasai et al., 2020).",
  "While many advancements have been made in machine translation, we believe we are the first to explore the non-autoregressive semantic parsing setting.",
  "In our work, we extend the CMLM to work for semantic parsing.",
  "We make two important adjustments: first, we use a different masking approach where we mask everything and do one-step generation.",
  "Second, we note the importance of the length prediction task for parsing and improve the length prediction module in the CMLM.",
  "Seq2Seq For Semantic Parsing Recent advances in language understanding have lead to increased reliance on seq2seq architectures.",
  "Recent work by Rongali et al. 2020; Aghajanyan et al. 2020, showed the advantages from using a pointer generator architecture for resolving complex queries (e.g. composition and cross domain queries) that could not be handled by word tagging models.",
  "Since we target the same task, we adapt their pointer decoder into our proposed architecture.",
  "However, to optimize for latency and compression we train CNN based architectures (Desai et al. 2020 and Wu et al. 2019b) to leverage the inherent model parallelism compared to the BiLSTM model proposed in Aghajanyan et al. 2020 and more compression compared to the transformer seq2seq baseline proposed in Rongali et al. 2020.",
  "To further improve latency we look at parallel decoding through non-autoregressive decoding compared to prior work leveraging autoregressive models.",
  "This work introduces a novel alternative to autoregressive decoding and efficient encoder-decoder architecture for semantic parsing.",
  "We show that in 3 semantic parsing datasets, we are able to speed up decoding significantly while minimizing accuracy regression.",
  "Our model is able to generate parse trees competitive with state of the art autoregressive models with significant latency savings, allowing complex NLU systems to be delivered on edge devices.",
  "There are a couple of limitations of our proposed model that naturally extend themselves to future work.",
  "Primarily, we cannot support true beam decoding, we decode a single prediction for each length prediction however there may exist multiple beams for each length prediction.",
  "Also for longer parse trees and more complex semantic parsing systems such as session based understanding, our NAR decoding scheme could benefit from multiple iterations.",
  "Lastly, though we explored models without pre-training in this work, recent developments show the power of leveraging pre-trained models such as RoBERTa and BART.",
  "We leave it to future work to extend our non-autoregressive decoding for pre-trained models.",
  "We would like to thank Sandeep Subramanian (MILA), Karthik Prasad (Facebook AI), Arash Einolghozati (Facebook) and Yinhan Liu for the",
  "helpful discussions.",
  "References Armen Aghajanyan, Jean Maillard, Akshat Shrivastava, Keith Diedrick, Michael Haeger, Haoran Li, Yashar Mehdad, Veselin Stoyanov, Anuj Kumar, Mike Lewis, and Sonal Gupta.",
  "2020.",
  "Conversational semantic parsing.",
  "In EMNLP/IJCNLP .",
  "Alice Coucke, Alaa Saade, Adrien Ball, Thodore Bluche, Alexandre Caulier, David Leroy, Clment Doumouro, Thibault Gisselbrecht, Francesco Calta-girone, Thibaut Lavril, et al. 2018.",
  "Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces.",
  "arXiv preprint arXiv:1805.10190 .",
  "Shrey Desai, Geoffrey Goh, Arun Babu, and Ahmed Aly.",
  "2020.",
  "Lightweight convolutional representations for on-device natural language processing.",
  "arXiv preprint arXiv:2002.01535 .",
  "Arash Einolghozati, Panupong Pasupat, Sonal Gupta, Rushin Shah, Mrinal Mohit, Mike Lewis, and Luke Zettlemoyer.",
  "2018.",
  "Improving semantic parsing for task oriented dialog.",
  "In Conversational AI Workshop at NeurIPS 2018 .",
  "Marjan Ghazvininejad, Vladimir Karpukhin, Luke Zettlemoyer, and Omer Levy.",
  "2020a.",
  "Aligned cross entropy for non-autoregressive machine translation.",
  "arXiv preprint arXiv:2004.01655 .",
  "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer.",
  "2019.",
  "Mask-predict: Parallel decoding of conditional masked language models.",
  "Marjan Ghazvininejad, Omer Levy, and Luke Zettle-moyer.",
  "2020b.",
  "Semi-autoregressive training improves mask-predict decoding.",
  "arXiv preprint arXiv:2001.08785 .",
  "Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Huo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-Nung Chen.",
  "2018.",
  "Slot-gated modeling for joint slot filling and intent prediction.",
  "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pages 753757.",
  "Victor OK Li, and Richard Socher.",
  "2017.",
  "Non-autoregressive neural machine translation.",
  "arXiv preprint arXiv:1711.02281 .",
  "Jiatao Gu, Changhan Wang, and Junbo Zhao.",
  "Levenshtein transformer.",
  "In Advances in Neural Information Processing Systems , pages 1117911189.",
  "Jungo Kasai, James Cross, Marjan Ghazvininejad, and Jiatao Gu.",
  "2020.",
  "Parallel machine translation with disentangled context transformer.",
  "arXiv preprint arXiv:2001.05136 .",
  "Yoon Kim and Alexander M Rush.",
  "2016.",
  "Sequence-level knowledge distillation.",
  "arXiv preprint arXiv:1606.07947 .",
  "Jason D. Lee, Elman Mansimov, and Kyunghyun Cho.",
  "2018.",
  "Deterministic non-autoregressive neural sequence modeling by iterative refinement.",
  "In Proc.",
  "of EMNLP .",
  "Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-big, and Eduard Hovy.",
  "2019.",
  "Flowseq: Non-autoregressive conditional sequence generation with generative flow.",
  "arXiv preprint arXiv:1909.02480 .",
  "Sonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Kumar, and Mike Lewis.",
  "2018.",
  "Semantic parsing for task oriented dialog using hierarchical representations.",
  "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 27872792, Brussels, Belgium.",
  "Association for Computational Linguistics.",
  "Dilek Hakkani-Tr, Gkhan Tr, Asli Celikyilmaz, Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-Yi Wang.",
  "2016.",
  "Multi-domain joint semantic frame parsing using bi-directional rnn-lstm.",
  "In Interspeech , pages 715719.",
  "Matthew Henderson, Blaise Thomson, and Jason D. Williams.",
  "2014.",
  "The second dialog state tracking challenge.",
  "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL) , pages 263272, Philadelphia, PA, U.S.A. Association for Computational Linguistics.",
  "Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. 2019.",
  "Searching for mobilenetv3.",
  "In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 13141324.",
  "Diederik P Kingma and Jimmy Ba.",
  "2014.",
  "Adam: A method for stochastic optimization.",
  "arXiv preprint arXiv:1412.6980 .",
  "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.",
  "2019.",
  "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.",
  "Kenton Murray, Jeffery Kinnison, Toan Q. Nguyen, Walter Scheirer, and David Chiang.",
  "2019.",
  "Auto-sizing the transformer network: Improving speed, efficiency, and performance for low-resource machine translation.",
  "In Proceedings of the 3rd Workshop on Neural Generation and Translation , pages 231240, Hong Kong.",
  "Association for Computational Linguistics.",
  "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Te-jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.",
  "2019.",
  "Pytorch: An imperative style, high-performance deep learning library.",
  "In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32 , pages 80248035.",
  "Curran Associates, Inc.",
  "Gabriel Pereyra, George Tucker, Jan Chorowski, ukasz Kaiser, and Geoffrey Hinton.",
  "2017.",
  "Regularizing neural networks by penalizing confident output distributions.",
  "arXiv preprint arXiv:1701.06548 .",
  "Subendhu Rongali, Luca Soldaini, Emilio Monti, and Wael Hamza.",
  "2020.",
  "Don't parse, generate! a sequence to sequence architecture for task-oriented semantic parsing.",
  "arXiv preprint arXiv:2001.11458 .",
  "Chitwan Saharia, William Chan, Saurabh Saxena, and Mohammad Norouzi.",
  "2020.",
  "Non-autoregressive machine translation with latent alignments.",
  "arXiv preprint arXiv:2004.07437 .",
  "Abigail See, Peter J. Liu, and Christopher D. Manning.",
  "2017.",
  "Get to the point: Summarization with pointer-generator networks.",
  "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1073 1083, Vancouver, Canada.",
  "Association for Computational Linguistics.",
  "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.",
  "2014.",
  "Sequence to sequence learning with neural networks.",
  "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.",
  "2015.",
  "Rethinking the inception architecture for computer vision.",
  "Elan van Biljon, Arnu Pretorius, and Julia Kreutzer.",
  "2020.",
  "On optimal transformer depth for low-resource language translation.",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,  ukasz Kaiser, and Illia Polosukhin.",
  "2017.",
  "Attention is all you need.",
  "In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30 , pages 59986008.",
  "Curran Associates, Inc.",
  "Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer.",
  "2019a.",
  "Fb-net: Hardware-aware efficient convnet design via differentiable neural architecture search.",
  "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1073410742.",
  "Victor Zhong, Caiming Xiong, and Richard Socher.",
  "2018.",
  "Global-locally self-attentive encoder for dialogue state tracking.",
  "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1458 1467, Melbourne, Australia.",
  "Association for Computational Linguistics.",
  "Chunting Zhou, Jiatao Gu, and Graham Neubig.",
  "2020.",
  "Understanding knowledge distillation in non-autoregressive machine translation.",
  "In International Conference on Learning Representations ."
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "abstain",
  "abstain",
  "objective",
  "result",
  "objective",
  "other",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "objective",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "objective",
  "method",
  "abstain",
  "result",
  "abstain",
  "result",
  "result",
  "objective",
  "objective",
  "abstain",
  "objective",
  "objective",
  "method",
  "method",
  "method",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "method",
  "abstain",
  "method",
  "method",
  "abstain",
  "method",
  "method",
  "method",
  "method",
  "abstain",
  "method",
  "method",
  "method",
  "abstain",
  "method",
  "method",
  "method",
  "abstain",
  "abstain",
  "method",
  "method",
  "method",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "method",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "other",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "other",
  "objective",
  "objective",
  "objective",
  "abstain",
  "other",
  "other",
  "objective",
  "abstain",
  "abstain",
  "objective",
  "result",
  "result",
  "objective",
  "abstain",
  "abstain",
  "objective",
  "objective",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other",
  "other"
]</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="5"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "Rationale-Centric Framework for Human-in-the-loop Machine Learning",
  "{yanglinyi, zhangyue}@westlake.edu.cn Abstract",
  "We present a novel rationale-centric framework with human-in-the-loop  R ationales-centric D ouble-robustness L earning (RDL)  to boost model out-of-distribution performance in few-shot learning scenarios.",
  "By using static semi-factual generation and dynamic human-intervened correction, RDL exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation.",
  "Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests compared to many state-of-the-art benchmarksespecially for few-shot learning scenarios.",
  "We also perform extensive ablation studies to support in-depth analyses of each component in our framework.",
  "Recent work finds that natural artefacts (Guru-rangan et al., 2018) or spurious patterns (Keith et al., 2020; Srivastava et al., 2020) in datasets can cause sub-optimal model performance for neural networks.",
  "As shown in Figure 1, the bold phrases  100% bad  and  brain cell killing  are underlying causes for a negative sentiment prediction that most human readers would recognise.",
  "These are defined as rationales in this paper.",
  "The underlined phraseacting and plot has been incorrectly recognised as a causal term by the model used fort this example, and is referred to as a spurious pattern .",
  "Spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021), and are usually useless, or even harmful, at test time.",
  "This issue can be severe in few-shot learning (FSL) * These authors contributed equally to this work.",
  "scenarios.",
  "For instance, Kulesza et al. (2010) suggests that when a model is trained with a small subset of labelled data, it is prone to exploiting spurious patterns leading to poor generalisability that is evident in the performance decay in out-of-distribution (OOD) datasets.",
  "In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu and MacNamee, 2020; Lu et al., 2021).",
  "There is a strand of research addressing this scenario that seeks to improve model performance by introducing methods and resources for training models less sensitive to spurious patterns",
  "(Kaushik et al., 2020).",
  "Most of this work relies on generating counterfactual augmented data",
  "(CAD), either manually",
  "(Kaushik et al., 2021)",
  "or automatically",
  "(Feng et al., 2021; Qian et al., 2021; Yang et al., 2021, 2020a; Delaney et al., 2021).",
  "For example, Kaushik et al.",
  "(2020)",
  "proposed a human-in-the-loop framework where human annotators are required to make minimal changes to original movie reviews to produce sentiment-flipped counterfactual reviews, which enables models to learn useful associations between input texts and output labels",
  "(Kaushik et al., 2021).",
  "Generating manual counterfactuals, however, is expensive and time-consumingKaushik et al.",
  "(2020)",
  "report the cost of revising 2 .",
  "5 k instances at over $10,000.",
  "On the other hand, fully automatic methods are task-specific and therefore have weak robustness across domains and less reliabil-6986 Semi-factual Generation",
  "ity compared to manual counterfactuals.",
  "To address these issues, we propose R ationales-centric D ouble-robustness L earning",
  "(RDL), a human-in-the-loop framework for data augmentation in a few-shot setting, which is efficient, robust, model-agnostic, and general across tasks.",
  "Our main idea is a rationale-centric strategy for eliminating the effect of spurious patterns by leveraging human knowledge as shown in Figure",
  "2. Our double-robustness framework consists of two main modules.",
  "The first is a Static Semi-factual Generation module that generates a set of semifactual data automatically for a given instance by using human-identified rationales.",
  "Such labelling requires less human input compared to fully manual counterfactual generation",
  "(see Section 3.1).",
  "In contrast with counterfactuals",
  "(Roese, 1997)",
  "that rely on what might have been different",
  "(i.e. the label would be changed if certain terms have been changed), semi-factuals",
  "(McCloy and Byrne, 2002; Kenny and Keane, 2021), as used in our work, aim to guide a model to identify terms less causally related to the label",
  "(i.e. even if certain terms had been changed, the label would be kept the same).",
  "Second, we apply a Dynamic Human-intervened Correction module , where the most salient features are identified for model predictions over a set of training examples, and human workers intervene by checking the correctness of the rationale in case first-round modifications introduce new artefacts.",
  "We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power, both for in-distribution and OOD predictions.",
  "also used in Kaushik et al.",
  "(2020), demonstrate that the double-robust models can be less sensitive to spurious patterns.",
  "In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than fully-supervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.",
  "The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual",
  "(Kaushik et al., 2020)",
  "and automatic CAD",
  "(Yang et al., 2021)",
  "using the full augmented training set of 3,414 examples.",
  "To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals and human-intervention for improving the generalisation abilities of deep neural networks in few-shot learning scenarios.",
  "* 2 Related Work Data augmentation has been used for resolving artefacts in training datasets before",
  "(Gururangan et al., 2018; Srivastava et al., 2020; Kaushik et al., 2021).",
  "In particular, previous work",
  "(Kaushik et al., 2020)",
  "relied on large-scale crowd-sourcing to generate useful augmented data.",
  "More recently, Yang et al.",
  "(2021), and Wang and Culotta",
  "(2021)",
  "investigated the efficacy of the automatically generated counterfactuals for sentiment analysis.",
  "Similar to our work, these methods also consider the most salient features that a model uses when generating augmented data, which is in line with our rationale definition.",
  "However, they use sentiment lexicon matching for identifying rationales, which is task-specific and not necessarily fully relevant.",
  "In contrast, we employ human annotators to identify rationales, which can be task-agnostic and robust.",
  "Moreover, our method generates semi-factuals instead of counterfactuals used in previous work.",
  "Human-the-loop Machine Learning",
  "(Wu et al., 2021)",
  "has received increasing research attention.",
  "Active learning",
  "(Settles, 2009; Margatina et al., 2021), the most common example of human-in-the-loop machine learning, asks human annotators only to provide high-level annotations",
  "(i.e. labels)",
  "for important examples.",
  "There is also some work exploring more explainable AI systems by exploiting feature-based information.",
  "Such methods use relatively simple models such as Nave Bayes",
  "(Stumpf * All resources are available at https://github.com/GeorgeLuImmortal/RDL-Rationales-centric-Double-robustness-Learning/ 6987 et al., 2009; Kulesza et al., 2015)",
  "and Linear Regression with bag-of-words features",
  "(Jia and Liang, 2017; Teso and Kersting, 2019; Ghai et al., 2021; Shao et al., 2021), because these classifiers are relatively intuitive in generating explanations and amenable to incorporating human feedback.",
  "Some other work uses simple neural networks such as multi-layer perceptrons",
  "(Shao et al., 2021)",
  "and shallow CNNs",
  "(Lertvittayakumjorn et al., 2020; Stammer et al., 2021; Teso et al., 2021)",
  "because the predictions of such models can be explained in the form of features.",
  "Very recently, Yao et al.",
  "(2021)",
  "proposed a human-in-the-loop method to inspect more complicated models",
  "(e.g. BERT)",
  "with the help of model-agnostic post-hoc explanation algorithms",
  "(Ribeiro et al., 2018)",
  "that can explain predictions of any linear or non-linear model without exploiting its weights.",
  "However, previous work focuses on increasing the explainability of AI systems for high-stakes domains such as health and finance",
  "(Li et al., 2020; Yang et al., 2020b), instead of improving model robustness or generalisation ability.",
  "Also, they assume access to a large amount of labelled data.",
  "In contrast, we focus on few-shot learning scenarios which are more compelling.",
  "The RDL pipeline is shown in Figure 2 and consists of two modules: Static Semi-factual Generation and Dynamic Human-intervened Correction .",
  "Static semi-factual generation is a more efficient alternative to manually generated counterfactuals",
  "(Kaushik et al., 2020).",
  "In the first phase, Rationale Marking",
  "(Section 3.1), human annotators review each document in the training set to provide rationales",
  "(i.e. phrases that support the document classification decisions shown as bold text in Figure 2).",
  "The second phase is a semi-factual generation method based on synonym replacement",
  "(Section 3.2)",
  "that produces augmented examples",
  "(blue text in Figure 2 indicates replaced words), which are added into the training set.",
  "Dynamic human-intervened correction",
  "(Section 3.3)",
  "is a rationales-powered human-in-the-loop framework to dynamically correct the model's behaviours.",
  "At the outset, sampling and sensitivity of contextual decomposition",
  "(SCD)",
  "(Jin et al., 2019)",
  "is applied to detect the rationales given by the model that is obtained in the previous step.",
  "Then, all model-identified rationales",
  "(underlined texts in Figure 2)",
  "are examined by human annotators to identify false rationales",
  "(i.e. words or phrases that do not support the classifications but are falsely included by the model)",
  "and missing rationales",
  "(i.e. words or phrases that support the classifications but are not included by the model).",
  "Both false rationales and missing rationales are corrected to produce augmented examples.",
  "Finally, newly generated examples are added into the training set to re-train the deep learning model.",
  "Following Kaushik et al.",
  "(2020)",
  "and Yang et al.",
  "(2021), we use the IMDb movie review dataset",
  "(Maas et al., 2011)",
  "in our experiments.",
  "It consists of positive and negative movie reviews that are easy for human participants to understand, re-annotate, and provide feedback upon",
  "(Zaidan et al., 2007).",
  "We use a crowdsourcing company to recruit editors and annotators for marking rationales that support classification decisions.",
  "At the outset, annotators were given instructions and examples that gently guided them to annotate rationales.",
  "Only adjectives, adverbs, nouns, and verbs were considered as rationales.",
  "Besides, rationales were required to carry complete semantic information.",
  "For example, for a phrase starting with a negation word such as  not great , annotators are instructed to mark the whole phrase  not great  as a rationale instead of just marking  not .",
  "We also limited rationales to at most three consecutive words",
  "(i.e. unigrams, bigrams and trigrams).",
  "Phrases consisting of numerical scores are not counted as rationales",
  "(e.g. 5 or 10 stars)",
  "since different datasets may use different rating scales, and annotating digits may hurt OOD performance.",
  "Overall, we encouraged annotators to try their best to mark as many rationales as possible to explain classification labels.",
  "However, to guarantee the quality of rationale marking and prevent annotators from over including non-rationales for more payment, we also manually inspected annotated examples and rejected examples that contained incorrect rationales.",
  "After inspection, we rejected 10.6% of negative reviews and 7.6% of positive reviews.",
  "Editors and annotators re-annotated the rejected examples, which were then presented to us for another inspection.",
  "All re-annotated examples were approved only if all authors were happy with the quality of the annotations.",
  "Otherwise, the examples were re-annotated again.",
  "rationales in 855 movie reviews involved in Section 3.1 and 3.3",
  "(note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments).",
  "Human annotators spent on average 183.68 seconds to identify rationales in a review and our method generated semi-factual examples automatically.",
  "On the contrary, workers spent on average 300 seconds to revise a review to generate a counterfactual manually as reported by Kaushik et al.",
  "(2020).",
  "Note that our approach using 100 labelled examples can outperform manual CAD",
  "(Kaushik et al., 2020)",
  "using the entire training set of 1,707 examples",
  "(see Section 5.3), making our approach 300  1707 183 .",
  "68  100  27 .",
  "88 times more efficient than manually generated CAD.",
  "We take a simple replacement strategy, which has been taken by Yang et al.",
  "(2021), to generate semifactual examples.",
  "Given a human-identified rationale, our method constructs augmented examples by automatically replacing non-rationale words, thus leading to examples with the same labels.",
  "This augmentation is consistent with semi-factual thinking: even if those non-rationales were changed, the label would not change.",
  "Formally, given a training example x i = [ t i 1 , t i 2 , ..., t ij ]",
  "(where t ij is the j th token of the i th document)",
  "and its ground truth label y i , we create a rationale vector r i = [ a i 1 , a i 2 , ..., a ij ] where a ij is the value that indicates whether t ij is a rationale or not",
  "(we set a ij = 1 to indicate that t ij is a rationale and 0 otherwise).",
  "To generate a semi-factual example, x  i , we randomly replace a certain number of non-rationales",
  "(where a ij = 0 ), except for punctuation, with synonymous terms.",
  "The synonyms can be provided by a human, retrieved automatically from a lexicon such as WordNet",
  "(Miller, 1995), or generated using the mask-filling function of a pretrained context-aware language model",
  "(Liu et al., 2019).",
  "In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x  i , with some replaced non-rationales and all the other tokens identical to x i .",
  "The label, y i , of a newly generated example is the same as the label of the original example, x i .",
  "Examples of generated data are shown in Table",
  "1. Afterwards, the augmented examples are added into the training set used to train the model.",
  "Dynamic human-intervened correction further improves the robustness of the model by allowing human annotators to correct the model rationales online.",
  "Firstly, SCD is applied to detect unigrams, bigrams or trigrams that are salient to the model.",
  "SCD is a technique to assess the importance of terms by continuously removing terms and measuring changes in prediction",
  "(Jin et al., 2019).",
  "Human annotators examine all rationales given by the model from all documents to discover two types of incorrect rationale: false rationales and missing rationales.",
  "The next phase allows human feedback to influence the learning process.",
  "To this end, for each type of incorrect rationale, we propose a corresponding strategy to correct them.",
  "For false rationales",
  "(i.e. phrases that actually do not support classifications but are incorrectly identified by the model), we use synonym replacement again to generate semi-factual examples.",
  "Unlike the static semi-factual generation",
  "(Section 3.2), in this component we replace all false rationales with their synonyms instead of randomly replacing 5% of non-rationales in a document.",
  "Examples of generated data are shown in Table",
  "2. For missing rationales",
  "(i.e. phrases that actually support classifications but are not identified by the model), we take another simple semi-factual generation strategy, that is, extracting sentences that contain missing rationales to form semi-factual data.",
  "Specifically, given a sentence containing missing rationales, we use this sentence as a new example, and the label of this newly generated example is identical to that of the document where the sentence is extracted.",
  "For example, there is a positive movie review",
  "(bold font for rationales)",
  "Robert Urich was a fine actor, and he makes this TV movie believable . I remember watching this film when I was 15 .... .",
  "The model fails to identify  fine  and  believable  as rationales.",
  "Thus we extract the text Robert Urich was a fine actor, and he makes this TV movie believable . as a new example, and the class of this example is still positive.",
  "We extract the whole sentence rather than just the missing rationales to reserve more semantic information.",
  "Note that the two correction methods in dynamic human-intervened correction can operate in parallel and the generated examples are added to the small training set to re-train the model.",
  "Broadly speaking, our RDL framework takes advantage of invariance that makes a model less sensitive to non-rationale words or spurious patterns (Tu et al., 2020; Wang et al., 2021) in favour of focusing on useful mappings of rationales to labels.",
  "More specifically, by using static semi-factual generation (Section 3.2) and false rationale correction (Section 3.3), we expect to break spurious associations.",
  "For example, if a model incorrectly determines that  Soylent Green  is associated with positive sentiment (Table 2), the augmented examples that replace  Soylent Green  with other phrases such as  Gang Orange  break the spurious association.",
  "Besides, using synonym replacement can generate examples that are similar to the original one, which is equivalent to adding noisy data to prevent models from overfitting (Wei and Zou, 2019).",
  "Missing rationale correction (Section 3.3) emphasizes the ground truth associations between rationales and labels, enabling the model to better estimate the generally useful underlying distributions for OOD datasets, even in few-shot learning scenarios.",
  "In the next section, we present experiments and empirical evidence to demonstrate the utility of the proposed RDL framework in improving model robustness.",
  "Our intention is to improve the generalisability of models, and we use both in-distribution and OOD",
  "performance for evaluation.",
  "Our experiments are designed to address the following research questions:  RQ1 Can we use static semi-factual generation to achieve better in-distribution and OOD performance?",
  "RQ2 Does dynamic human-intervened correction improve generalisability of models?",
  "For fair comparison with previous work (Kaushik et al., 2020; Yang et al., 2021), we use the IMDb sentiment classification dataset (Maas et al., 2011) as the in-distribution dataset.",
  "Following Kaushik et al. (2020), all models were trained with the IMDb dataset predefined training, validation and test partitions containing 1 , 707 , 245 , and 488 reviews respectively and an enforced 50:50 class ratio.",
  "To measure the generalisation ability of different models, we focus on OOD performance.",
  "To this end, we test models on another four binary sentiment classification datasets: the sampled Amazon reviews dataset (Ni et al., 2019) (100,000 positives and 100,000 negatives) from six genres: beauty, fashion, appliances, gift cards, magazines, and software; the Yelp review dataset (Zhang et al., 2015) (19,000 positives and 19,000 negatives); the SST-2 dataset (Socher et al., 2013) (1,067 positives and 1,143 negatives), and the SemEval-2017 Twitter dataset (Rosenthal et al., 2017) (2,339 positives 6990 Training Data In-domain SemEval-2017 SST-2 Yelp Amazon Static (50 gold) 88.60 1.11 77.28 9.11 79.29 5.14 91.53 2.06 89.63 1.65 Full (1,707 gold) 93.23 0.46 71.17 2.54 80.23 2.09 93.66 0.84 90.29 0.57 DP (Static + 350 auto) (400) 86.70 2.92 74.36 2.92 77.33 6.01 89.60 2.51 89.15 1.89 RR (Static + 350 auto) (400) 89.65 1.27 79.20 1.27 78.89 5.95 91.93 2.10 89.73 1.26 Our Methods Static + 150 auto (200) 90.08 1.25 78.88 6.67 79.40 3.28 92.19 1.51 89.81 1.73 Static + 350 auto (400) 90.16 0.85 80.54 2.81 81.26 1.97 93.03 1.08 90.09 1.79 Static + 550 auto (600) 90.04 1.50 80.69 3.42 81.23 1.83 92.10 3.07 89.67 1.27 Static + 750 auto (800) 90.08 1.01 80.55 3.96 80.75 2.30 92.36 1.87 90.18 1.44 Static + 950 auto (1000) 89.83 1.28 80.90 3.29 80.58 2.57 92.30 2.19 90.62 1.29 Static + 1150 auto (1200) 90.12 1.82 79.31 1.82 79.52 3.15 91.47 3.61 90.16 1.46 Table 3: Results on in-distribution and OOD data.",
  "To address RQ1 , we compare the performance of models trained by the static semi-factual generation strategy with models trained with the original 50 examples, referred to as Static .",
  "We also compare to a model trained with the full training set (1,707 labelled examples), referred to as Full .",
  "To simulate the few-shot training scenario, we randomly sample 50 examples (we also forced a 50:50 class balance) from the IMDb dataset as training data.",
  "For each experiment, the training is repeated 10 times with training datasets sampled by 10 different random seeds.",
  "We report the average result of these 10 repetitions and use accuracy to measure the classification performance.",
  "Our experiments rely on an off-the-shelf cased RoBERTa-base model implemented by Hugging Face * to either perform mask-filling to provide synonyms or as a predictive model.",
  "Following Kaushik et al. (2020), we fine-tune RoBERTa for up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).",
  "We also explore the impact of the number of semi-factual examples on model performance.",
  "To this end, we conduct static semi-factual generation with a different number of augmented examples for each instance: {3, 7, 11, 15, 19, 23}.",
  "Considering we have 50 original examples, this would result in {150, 350, 550, 750, 950, 1,150} additional examples in the training set, respectively (we call * https://huggingface.co/transformers/model_doc/roberta.html this Static+ n , where n is the number of generated semi-factuals).",
  "We use the Adam optimizer (Kingma and Ba, 2014) with a batch size of 4.",
  "We found that setting the learning rate to {5e-5, 5e-6 and 5e-6} could optimise Static, Static+ n , and Full, respectively.",
  "As shown in Table 3, all static semi-factual generation (Static+ n ) methods can outperform the baseline method (Static) in both in-distribution and OOD tests, demonstrating the utility of static semifactual generation.",
  "Among all Static+ n methods, Static+350 seems the best-performing method and exceeds Static with a 1.56% in-distribution improvement in average accuracy.",
  "Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, and 0.46% OOD improvement in the SemEval-2017 , SST-2 , Yelp and Amazon datasets respectively.",
  "Although the improvement on the Amazon dataset appears modest, given that there are 200,000 examples in the Amazon test set, this actually stands for nearly 1,000 documents being correctly classified.",
  "The Static+ n methods can even outperform Full (i.e. normal training with the full training set) on the SemEval , SST-2 , and Amazon datasets and are comparable on the Yelp dataset.",
  "The performance of models with the full training set is best on the in-distribution dataset but the worst on the SemEval dataset, which can be caused by the big difference between underlying distributions of these two datasets.",
  "In other words, a model that fits well with one dataset can cause performance decay on others.",
  "In this case, training with a smaller training set is more likely to reduce overfitting with the in-distribution dataset and fit well with the SemEval dataset, which explains the big improvement.",
  "It is interesting to note that models trained with the en-6991 tire training set perform slightly better on the OOD Yelp dataset (93.66 0.84 ) than on the in-distribution dataset (93.23 0.46 ), which could also be explained by the high similarity between the underlying distributions of these two datasets.",
  "First, we test whether the improvement in model performance is brought about by static semi-factual generation (Static+ n ) or simply by an increase in the size of the training set.",
  "We compare Static+350 (due to its relatively good performance) with another baseline called Duplication ( DP heareafter).",
  "We multiply the original training set (50 examples) up into 400 examples identical to the size of the training set of Static+350, and fine-tune RoBERTa on this dataset with the same hyperparameters as Static+350.",
  "As shown in Table 3, in most cases, DP un-derperforms other algorithms and is even worse than Static, demonstrating that solely increasing the dataset size cannot improve the performance.",
  "We believe that the duplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models.",
  "Second, synonym replacement has been used previously for data augmentation (Wei and Zou, 2019), and we compare static semi-factual generation with simply replacing any words (i.e. both rationales and non-rationales).",
  "Following Wei and Zou (2019), we replace 5% of words at random and set the training set size to 400 to ensure fair comparison (we use RoBERTa and the same hyperparameters of Static+350).",
  "We call this Random Replacement ( RR hereafter).",
  "As shown in Table 3, RR is slightly better than the baseline Static approach.",
  "This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to the original data, introducing noise that helps prevent overfitting to some extent.",
  "However, the magnitude of improvement of the Static+ n method is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals.",
  "These observations show that the model trained with Static+ n does improve both in-distribution and OOD performance, and the improvement is actually derived from static semi-factual generation.",
  "As shown in Table 3 and Figure 3, the performance gain of static semi-factual generation (Static+ n ) marginalises when augmented data is increased.",
  "Using too much augmented data even hurts the Static+1150 performance.",
  "This observation is consistent with existing work on data augmentation (Wei and Zou, 2019).",
  "We believe one reason could be that the use of static augmented examples could also introduce new spurious patterns that degrade model performance, necessitating a method that exploits rationales without generating too many augmented examples.",
  "Human-in-the-loop can address this issue by dynamically correcting the model.",
  "To address RQ2 , we compare the performance of models trained by dynamic human-intervened correction with a popular few-shot human-in-the-loop learning framework, Active Learning, as well as two other state-of-the-art CAD-based methods (Kaushik et al., 2020; Yang et al., 2021).",
  "Lastly, we provide an ablation study to examine the influence of different correction methods, as well as an analysis regarding model sensitivity to spurious patterns.",
  "We build up an active learning procedure as a baseline based on the model trained with Static.",
  "In particular, we select another 50 examples by Uncertainty Sampling (i.e. prediction scores for two classes in these examples were close) and add them into the training set (called AL hereafter).",
  "The training set size of the baseline becomes 100.",
  "The best performing static semi-factual generation method Static+350 is also listed as a baseline.",
  "For fair comparison, we also use Uncertainty Sampling to select another 50 examples (i.e. 100 original examples in the training set now) for the proposed dynamic human-intervened correction in-6992 Baseline Methods In-domain SemEval-2017 SST-2 Yelp Amazon Static (50 gold) 88.60 1.11 77.28 9.11 79.29 5.14 91.53 2.06 89.63 1.65 Static + 350 auto (400) 90.16 0.85 80.54 2.81 81.26 1.97 93.03 1.08 90.09 1.79 AL (100 gold) 88.64 1.75 78.61 5.90 80.50 3.37 92.47 0.68 89.80 1.91 CAD-based Methods Manual CAD (3,414 gold) 92.70 0.53 69.98 3.99 80.30 2.03 91.87 1.09 90.48 1.09 Automatics CAD (1,707 gold+1,707 auto) 91.82 0.74 79.39 5.37 80.60 3.10 91.92 0.97 90.46 1.08 Our Dynamic Methods Dynamic (100 gold + 700 auto) 90.84 0.99 80.32 4.31 82.40 2.14 93.19 1.24 90.51 2.17 Dynamic-MR (100 gold + 700 auto) 91.06 1.21 79.04 4.92 82.24 2.59 93.03 1.92 90.22 2.74 Dynamic-FR (100 gold + 700 auto) 89.85 1.38 82.39 1.88 81.59 1.82 92.98 0.91 90.12 2.42 Table 4: Results on in-distribution and OOD data.",
  "cluding both False Rationale Correction and Missing Rationale Correction (called Dynamic ).",
  "For Dynamic, we control the number of augmented examples for each review to 7 (4 from Missing Rationale Correction and 3 from False Rationale Correction), resulting in 800 examples in the training set.",
  "For Automatic CAD (Yang et al., 2021) and Manual CAD (Kaushik et al., 2020), we use the entire training set to produce counterfactuals to build up two challenging baselines (one counterfactual for one example, which is limited by the method), resulting in 3,414 examples in the training set.",
  "To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction ( Dynamic-FR hereafter) and Missing Rationale Correction ( Dynamic-MR hereafter).",
  "Again, experiments all rely on a RoBERTa model and all hyperparameters are identical to those described in Section 5.2.1, except for the learning rate of AL which is set to 1.25e-5 (we found this value optimised AL perfor-mance).",
  "As shown in Table 4, both AL and Dynamic outperform Static in in-distribution and OOD datasets which makes sense, because we use Uncertainty Sampling to add new labelled data to minimise model uncertainty and increase model performance.",
  "However, AL fails to compete with Static+350 even if more original data is added, which again demonstrates the utility of static semi-factual generation.",
  "On the contrary, Dynamic does better than Static+350 with a 0.68% in-distribution improvement in average accuracy.",
  "Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2 , Yelp and Amazon datasets, but no improvement for the SemEval Non-rationales Rationales Static 0.572 0.428 Dynamic 0.433 0.567 Table 5: Static versus Dynamic models on average sensitivity (normalised) to rationales and non-rationales for IMDb test samples.",
  "dataset.",
  "Finally, the performance of our methods is better that the state-of-the-art manual CAD method in few-shot learning scenarios on all OOD datasets.",
  "Overall, these observations demonstrate that applying dynamic human-intervened correction (i.e. Missing Rationale Correction and False Rationale Correction) can further increase the robustness of a model on generalisation ability, effectively avoiding the improvement marginalisation caused by the increased volume of augmented data.",
  "Missing Rationales vs. False Rationales We conduct an ablation study by examining the performance of Dynamic-MR and Dynamic-FR in Table 4.",
  "Interestingly, Dynamic-FR is specifically good at improving model performance on the in-distribution and SemEval datasets while Dynamic-MR does a good job on the SST-2 dataset.",
  "We believe that it is because Dynamic-MR biases the model to estimate an underlying distribution that is useful for SST-2 and in-distribution datasets, while Dynamic-FR biases the model to estimate a distribution similar to SemEval dataset.",
  "The performance of Dynamic can be explained as a compromise of two correction methods.",
  "Sensitivity to Spurious Patterns We conduct an analysis to explore whether the double-robust models are less sensitive to spurious patterns.",
  "We compute models mean sensitivity to all rationales and non-rationales through SCD in the IMDb test set.",
  "As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increase in the 6993 sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.",
  "We proposed a rationale-centric human-in-the-loop framework, RDL, for better model generalisability in few-shot learning scenarios.",
  "Experimental results show that our method can boost performance of deep neural networks in both in-distribution and OOD datasets and make models less sensitive to spurious patterns, enabling fast generalisation.",
  "In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.",
  "We honor the ACL Code of Ethics.",
  "No private data or non-public information was used in this work.",
  "All annotators have received labor fees corresponding to the amount of their annotated instances.",
  "We acknowledge with thanks the discussion with Chenyang Lyu from Dublin City University, as well as the many others who have helped.",
  "We would also like to thank anonymous reviewers for their insightful comments and suggestions to help improve the paper.",
  "This publication has emanated from research conducted with the financial support of the Pioneer and \"Leading Goose\" R&amp;D Program of Zhejiang under Grant Number 2022SDXHDX0003 and Science Foundation Ireland (SFI) under Grant Number [12/RC/2289_P2].",
  "Yue Zhang is the corresponding author."
]</div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "abstain",
  "abstain",
  "objective",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "objective",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "objective",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "objective",
  "abstain",
  "result",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "objective",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "objective",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "objective",
  "abstain",
  "objective",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "other",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "objective",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "method",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "result",
  "abstain",
  "result",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "result",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "method",
  "method",
  "method",
  "method",
  "other",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "method",
  "abstain",
  "method",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "method",
  "method",
  "method",
  "method",
  "abstain",
  "abstain",
  "method",
  "abstain",
  "method",
  "method",
  "method",
  "method",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "abstain",
  "objective",
  "result",
  "result",
  "abstain",
  "method",
  "abstain",
  "other",
  "other",
  "other",
  "other"
]</div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="6"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">["Simile interpretation is a crucial task in natural language processing.","Nowadays, pre-trained la<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">["abstain","abstain","abstain","objective","method","result","objective","result","other","abstain",<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="7"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">["Memory augmented encoder-decoder framework has achieved promising progress for natural language ge<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">["abstain","abstain","abstain","objective","method","objective","abstain","abstain","abstain","absta<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="8"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">["Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding ca<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">["abstain","abstain","objective","objective","result","abstain","abstain","abstain","abstain","absta<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td>
					</tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850  last:border-none" tabindex="0" data-row-idx="9"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">["Abstract","This paper addresses the problem of dialogue reasoning with contextualized commonsense <span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">["abstain","abstain","objective","abstain","objective","result","objective","abstain","abstain","abs<span class="text-orange-500">(...TRUNCATED)</span></div></div>
							</td>
					</tr></tbody></table>
		<div class="bg-linear-to-b sticky left-0 border-t border-dashed border-gray-300 from-gray-100 to-white py-3 text-center font-mono text-xs dark:border-gray-700 dark:from-gray-950 dark:to-gray-900">End of preview. <a href="/datasets/scim/naacl_data_prog/viewer/default/train" class="group"><span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Expand</span>
						in <svg class="text-lg mr-0.5 inline -translate-y-px text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg>Data Studio
					</a></div></div>




			<div class="bg-linear-to-b from-gray-100 to-white dark:from-gray-950 dark:to-gray-900 "><hr class="flex-none -translate-y-px border-t border-dashed border-gray-300 bg-white dark:border-gray-700 dark:bg-gray-950">
					<nav><ul class="flex select-none items-center justify-between space-x-2 text-gray-700 sm:justify-center py-1 text-center font-mono text-xs rounded-b-lg"><li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 pointer-events-none cursor-default text-gray-400 hover:text-gray-700" href=""><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg>
		Previous</a></li>
			<li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1 bg-gray-50 font-semibold ring-1 ring-inset ring-gray-200 dark:bg-gray-900 dark:text-yellow-500 dark:ring-gray-900 hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/scim/naacl_data_prog/viewer/default/train?p=0">1</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/scim/naacl_data_prog/viewer/default/train?p=1">2</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/scim/naacl_data_prog/viewer/default/train?p=2">3</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  pointer-events-none cursor-default" href="#">...</a>
				</li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="/datasets/scim/naacl_data_prog/viewer/default/train?p=30">31</a>
				</li>
			<li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 " href="/datasets/scim/naacl_data_prog/viewer/default/train?p=1">Next
		<svg class="ml-1.5 transform rotate-180" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg></a></li></ul></nav></div></div>






</div></div></div></div></div></div></div>

				<div class="from-gray-50-to-white bg-linear-to-t rounded-lg border border-dotted border-gray-200 py-24 text-center md:px-6"><p class="mb-1 mt-2">No dataset card yet</p>
						</div></section>
			
			<section class="pt-6 border-gray-100 md:pb-24 md:pl-6 md:w-64 lg:w-80 xl:w-96 flex-none order-first md:order-none md:border-l pt-3! md:pt-6!"><dl class="flex items-baseline justify-between"><dt class="text-sm text-gray-500">Downloads last month</dt><div class="mx-4 flex-1 border-b border-dotted"></div><dd class="font-semibold">53</dd></dl>
				
				<div class="divider-column-vertical"></div>
				<div class="grid grid-cols-2 gap-x-2 md:flex md:flex-row md:flex-wrap"><div class="SVELTE_HYDRATER contents" data-target="DatasetAndModelActionsDropdown" data-props="{&quot;classNames&quot;:&quot;order-last&quot;,&quot;discussionsDisabled&quot;:false,&quot;repo&quot;:{&quot;type&quot;:&quot;dataset&quot;,&quot;name&quot;:&quot;scim/naacl_data_prog&quot;},&quot;canWrite&quot;:false,&quot;canDisable&quot;:false,&quot;repoIsPrivate&quot;:false,&quot;repoIsGated&quot;:false,&quot;repoIsDisabled&quot;:false,&quot;repoIsAdminFlaggedNFAA&quot;:false,&quot;repoHasBlockedOids&quot;:false}"><div class="order-last"><div class="relative ">
	<button class="btn px-1.5 py-1.5 " type="button">
		
			<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="p-0.5" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><circle cx="16" cy="7" r="3" fill="currentColor"></circle><circle cx="16" cy="16" r="3" fill="currentColor"></circle><circle cx="16" cy="25" r="3" fill="currentColor"></circle></svg>
		
		</button>
	
	
	</div></div>













</div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetLibrary" data-props="{&quot;classNames&quot;:&quot;md:w-full xl:w-auto xl:flex-none&quot;,&quot;libraries&quot;:[{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;datasets&quot;,&quot;function&quot;:&quot;load_dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{},&quot;code&quot;:&quot;from datasets import load_dataset\n\nds = load_dataset(\&quot;scim/naacl_data_prog\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;pandas&quot;,&quot;function&quot;:&quot;pd.read_parquet&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;data/train-00000-of-00001-3fd534de06b6ca96.parquet&quot;}},&quot;code&quot;:&quot;import pandas as pd\n\ndf = pd.read_parquet(\&quot;hf://datasets/scim/naacl_data_prog/data/train-00000-of-00001-3fd534de06b6ca96.parquet\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;mlcroissant&quot;,&quot;function&quot;:&quot;Dataset&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;record_set&quot;:&quot;default&quot;,&quot;partial&quot;:false},&quot;code&quot;:&quot;from mlcroissant import Dataset\n\nds = Dataset(jsonld=\&quot;https://huggingface.co/api/datasets/scim/naacl_data_prog/croissant\&quot;)\nrecords = ds.records(\&quot;default\&quot;)&quot;}]},{&quot;language&quot;:&quot;python&quot;,&quot;library&quot;:&quot;polars&quot;,&quot;function&quot;:&quot;pl.read_parquet&quot;,&quot;loading_codes&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;arguments&quot;:{&quot;splits&quot;:{&quot;train&quot;:&quot;data/train-00000-of-00001-3fd534de06b6ca96.parquet&quot;}},&quot;code&quot;:&quot;import polars as pl\n\ndf = pl.read_parquet('hf://datasets/scim/naacl_data_prog/data/train-00000-of-00001-3fd534de06b6ca96.parquet')\n&quot;}]}]}"><div class="relative md:w-full xl:w-auto xl:flex-none">
	<button class="from-gray-800! to-black! max-xl:mb-2 text-white! gap-1! border-gray-800! dark:border-gray-900!  btn w-full cursor-pointer text-sm" type="button">
		<svg class="mr-1.5 mr-0.5! " xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg>
			Use this dataset
		<svg class="-mr-1 text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path d="M16.293 9.293L12 13.586L7.707 9.293l-1.414 1.414L12 16.414l5.707-5.707z" fill="currentColor"></path></svg></button>
	
	
	</div>

</div>
					
					</div>
				<div class="divider-column-vertical"></div>
					<div class="flex flex-col flex-wrap xl:flex-row"><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of downloaded dataset files:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->47.7 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 " href="/datasets/scim/naacl_data_prog/tree/refs%2Fconvert%2Fparquet/" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of the auto-converted Parquet files:</div>
		<div class="truncate text-sm group-hover:underline">
			<!-- HTML_TAG_START -->47.7 MB<!-- HTML_TAG_END --></div></a><a class="bg-linear-to-r dark:via-none group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Number of rows:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->3,051<!-- HTML_TAG_END --></div></a></div>
				
				
				
				

				
				<div class="divider-column-vertical md:hidden"></div></section></div></main>

	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container relative flex flex-col justify-between space-y-2 py-6 text-gray-500 max-md:*:self-start md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="SVELTE_HYDRATER contents" data-target="ThemeSwitcher" data-props="{&quot;theme&quot;:&quot;system&quot;,&quot;isLoggedIn&quot;:false,&quot;menuClassNames&quot;:&quot;md:-top-24&quot;,&quot;classNames&quot;:&quot;max-md:mb-5 max-md:*:self-start&quot;}">
<div class="relative inline-block max-md:mb-5 max-md:*:self-start">
	<button class="rounded-full border border-gray-100 pl-2 py-1 pr-2.5  flex items-center text-sm text-gray-500 bg-white hover:bg-purple-50 hover:border-purple-200 dark:hover:bg-gray-800 dark:hover:border-gray-950 dark:border-gray-800 " type="button">
		<svg class="mr-1.5 text-gray-500" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M6 22.5h20a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2H6a2 2 0 0 0-2 2v13.5a2 2 0 0 0 2 2ZM7 7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h18a1 1 0 0 0 1-1V8a1 1 0 0 0-1-1H7Z" fill="currentColor"></path><path d="M6 8a1 1 0 0 1 1-1h18a1 1 0 0 1 1 1v11a1 1 0 0 1-1 1H7a1 1 0 0 1-1-1V8Z" fill="currentColor" fill-opacity=".4"></path><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path></svg>
			System theme
		</button>
	
	
	</div></div>
		<div class="font-semibold text-black md:hidden">Company</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="max-md:mb-4! max-md:mt-8! group flex-none max-md:order-last"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="max-md:mt-8! font-semibold text-black md:hidden">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("\/front\/build\/kube-68d7aa0\/index.js");
			window.moonSha = "kube-68d7aa0\/";
			window.__hf_deferred = {};
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>
	</body>
</html>
