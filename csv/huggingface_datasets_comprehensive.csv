benchmark_name,modality,task_type,domain,output_type,evaluation_metrics,paper_link,dataset_link,languages,dataset_size,num_train_examples,num_val_examples,num_test_examples,sota_performance,sota_model,license_details,last_updated,citation_count,downloads,example_code_link,similar_benchmarks,data_format,preprocessing_notes,ethical_considerations,model_architectures,hardware_requirements,training_time,data_summary
0x22almostEvil/ru-riddles-377,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/0x22almostEvil/ru-riddles-377,),,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Russian riddles with answers with 377 entries. Data-Summary: Contains parquet of QnA with riddle & answer pairs. Each row consists of INSTRUCTION RESPONSE SOURCE METADATA (json with language). Licensing Information Data is scrapped from several sites. Since most of the riddles and answers are publicly available and popular, any ToS and licensing of the sites themselves is irrelevant. I reserve the right to put a public andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/0x22almostEvil/ru-riddles-377."
0xJustin/Dungeons-and-Diffusion,,,,,,,https://huggingface.co/datasets/0xJustin/Dungeons-and-Diffusion,,,,,,,,,,,,,,,,,,,,"This is the dataset! Not the .ckpt trained model - the model is located here: https://huggingface.co/0xJustin/Dungeons-and-Diffusion/tree/main The newest version has manually captioned races and classes, and the model is trained with EveryDream. 30 images each of: aarakocra, aasimar, air_genasi, centaur, dragonborn, drow, dwarf, earth_genasi, elf, firbolg, fire_genasi, gith, gnome, goblin, goliath, halfling, human, illithid, kenku, kobold, lizardfolk, minotaur, orc, tabaxi, thrikreen, tieflingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/0xJustin/Dungeons-and-Diffusion."
0xMaka/trading-candles-subset-qa-format,,,,,,,https://huggingface.co/datasets/0xMaka/trading-candles-subset-qa-format,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,0xMaka/trading-candles-subset-qa-format dataset hosted on Hugging Face and contributed by the HF Datasets community
40umov/dostoevsky_3.5k,,,,,,,https://huggingface.co/datasets/40umov/dostoevsky_3.5k,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,40umov/dostoevsky_3.5k dataset hosted on Hugging Face and contributed by the HF Datasets community
4eJIoBek/Zelenograd-aerial-videos,,,,,,,https://huggingface.co/datasets/4eJIoBek/Zelenograd-aerial-videos,,,,,,,,https://choosealicense.com/licenses/wtfpl/,,,,,,,,,,,,"(almost) all aerial videos of Zelenograd until 2023. i dont have a rights of these videos, all of these were downloaded from youtube. if you an owner of some of these videos and dont want that it were there, please contact me 4eJIoBek2021@gmail.com"
64bits/lima_vicuna_format,,,,,,,https://huggingface.co/datasets/64bits/lima_vicuna_format,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,LIMA dataset in Vicuna ShareGPT format. License under LIMA's License. Original Repo: https://huggingface.co/datasets/GAIR/lima
77xiaoyuanzi8/code_reviewer,,,,,,,https://huggingface.co/datasets/77xiaoyuanzi8/code_reviewer,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,77xiaoyuanzi8/code_reviewer dataset hosted on Hugging Face and contributed by the HF Datasets community
a-m-team/AM-DeepSeek-R1-Distilled-1.4M,,,,,,,https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"AM-DeepSeek-R1-Distilled-1.4M is a large-scale general reasoning task dataset composed of high-quality and challenging reasoning problems. These problems are collected from numerous open-source datasets, semantically deduplicated, and cleaned to eliminate test set contamination. All responses in the dataset are distilled from the reasoning model (mostly DeepSeek-R1) and have undergone rigorous verification: mathematical problems are validated through answer checking, code problems viaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M."
a686d380/h-corpus-2023,,,,,,,https://huggingface.co/datasets/a686d380/h-corpus-2023,,,,,,,,,,,,,,,,,,,,"ç»è¿‡æ¸…æ´—å’ŒåŽ»é‡è¿‡çš„Hå°è¯´ å…±205,028ç¯‡æ–‡ç« ï¼Œè§£åŽ‹åŽ17.0 GB ä»…ç”¨äºŽç§‘å­¦ç ”ç©¶ï¼"
Aarushhh/Thinking-Preference-7k,,,,,,,https://huggingface.co/datasets/Aarushhh/Thinking-Preference-7k,,,,,,,,,,,,,,,,,,,,"Thinking Preference 7k A preference dataset that could be used to make R1 like models Can be used to make a reward model Can be used for GRPO, PPO, DPO, SimPO, etc. No need to reformat anything! Used bespokelabs/Bespoke-Stratos-17k and NovaSky-AI/Sky-T1_data_17k for creating this dataset."
Abdelkareem/arabic-bbc-news,,,,,,,https://huggingface.co/datasets/Abdelkareem/arabic-bbc-news,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""arabic-bbc-news"" More Information needed"
Abdelrahman-Rezk/Arabic_Dialect_Identification,Text,General,UI,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Abdelrahman-Rezk/Arabic_Dialect_Identification,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Arabic dialects, multi-class-Classification, Tweets. Dataset Card for Arabic_Dialect_Identification Data-Summary: We present QADI, an automatically collected dataset of tweets belonging to a wide range of country-level Arabic dialects covering 18 different countries in the Middle East and North Africa region. Our method for building this dataset relies on applying multiple filters to identify users who belong to different countries based on their accountâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Abdelrahman-Rezk/Arabic_Dialect_Identification."
AbderrahmanSkiredj1/moroccan_darija_wikipedia_dataset,,,,,,,https://huggingface.co/datasets/AbderrahmanSkiredj1/moroccan_darija_wikipedia_dataset,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""moroccan_darija_wikipedia_dataset"" More Information needed"
abdulhade/KurdishTextCorpus,,,,,,,https://huggingface.co/datasets/abdulhade/KurdishTextCorpus,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,Ú©Û†Ú•Ù¾Ø³ÛŽÚ©ÛŒ Ú©Û†Ú©Ø±Ø§ÙˆÛ•ÛŒ Ø¯Û•Ù‚ÛŒ Ú©ÙˆØ±Ø¯ÛŒ Ù†Ø§ÙˆÛ•Ú•Ø§Ø³Øª(Ø³Û†Ø±Ø§Ù†ÛŒÛ•) Ú©Û• Ù‚Û•Ø¨Ø§Ø±Û•Ú©Û•ÛŒ Ù¾ÛŽÚ©Ø¯ÛŽØª Ù„Û• Ù¤Ù£Ù  Ù…ÛŒÚ¯Ø§ Ø¨Ø§ÛŒØª
Abirate/english_quotes,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",,https://huggingface.co/datasets/Abirate/english_quotes,English,,,,,,,,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for English quotes I-Data-Summary: english_quotes is a dataset of all the quotes retrieved from goodreads quotes. This dataset can be used for multi-label text classification and text generation. The content of each quote is in English and concerns the domain of datasets for NLP and beyond. II-Supported Tasks and Leaderboards Multi-label text classification : The dataset can be used to train a model for text-classification, whichâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Abirate/english_quotes."
abisee/cnn_dailymail,Text,Question Answering,General,Text,"Exact Match, F1 Score",,https://huggingface.co/datasets/abisee/cnn_dailymail,dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for CNN Dailymail Dataset Data-Summary: The CNN / DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering. Supported Tasks and Leaderboards 'summarization': Versionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/abisee/cnn_dailymail."
aboonaji/wiki_medical_terms_llam2_format,,,,,,,https://huggingface.co/datasets/aboonaji/wiki_medical_terms_llam2_format,,,,,,,,,,,,,,,,,,,,aboonaji/wiki_medical_terms_llam2_format dataset hosted on Hugging Face and contributed by the HF Datasets community
Abrumu/Fashion_controlnet_dataset_V3,,,,,,,https://huggingface.co/datasets/Abrumu/Fashion_controlnet_dataset_V3,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""Fashion_controlnet_dataset_V3"" More Information needed"
abuelkhair-corpus/arabic_billion_words,Text,,,,,http://www.abuelkhair.net/index.php/en/arabic/abu-el-khair-corpus,https://huggingface.co/datasets/abuelkhair-corpus/arabic_billion_words,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"Abu El-Khair Corpus is an Arabic text corpus, that includes more than five million newspaper articles. It contains over a billion and a half words in total, out of which, there are about three million unique words. The corpus is encoded with two types of encoding, namely: UTF-8, and Windows CP-1256. Also it was marked with two mark-up languages, namely: SGML, and XML."
Abzu/arxiv_stem,,,,,,,https://huggingface.co/datasets/Abzu/arxiv_stem,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""arxiv_stem"" More Information needed"
acdzh/dingzhen-voice,,,,,,,https://huggingface.co/datasets/acdzh/dingzhen-voice,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,åº”è¯¥æ²¡å¤–å›½äººç”¨ï¼Œç›´æŽ¥ç”¨ä¸­æ–‡å§ dingzhen.zip æ˜¯å£°æºåŽ‹ç¼©æ–‡ä»¶ï¼Œå£°æºæ¥è‡ªä¸¤éƒ¨åˆ† æŸæ¬¡å½•æ’­(qh_0_*.wav)ï¼š220725ä¸çœŸç›´æ’­å½•å±å®Œæ•´ç‰ˆ_å“”å“©å“”å“©_bilibili ç²˜åˆå›½æ¼”è®²(qh_1_*.wav)ï¼šä¸çœŸ å‡ºå¸­è”åˆå›½æ¼”è®²(å®Œæ•´ç‰ˆ)æ¯«ä¸æ€¯åœº ä»Žå®¹è‡ªè‹¥ å¥½æœ‰é­…åŠ›_å“”å“©å“”å“©_bilibili cuts.txt æ˜¯å¯¹åº”æ–‡ä»¶å­—å¹•ã€‚
achang/plot_qa,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/achang/plot_qa,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for PlotQA Data-Summary: PlotQA is a VQA dataset with 28.9 million question-answer pairs grounded over 224,377 plots on data from real-world sources and questions based on crowd-sourced question templates. Dataset Structure Data Fields List and describe the fields present in the dataset. Mention their data type, and whether they are used as input or output in any of the tasks the dataset currently supports. If the dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/achang/plot_qa."
acheong08/nsfw_reddit,,,,,,,https://huggingface.co/datasets/acheong08/nsfw_reddit,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,acheong08/nsfw_reddit dataset hosted on Hugging Face and contributed by the HF Datasets community
achrafothman/aslg_pc12,Text,General,General,Text,"Accuracy, F1 Score",https://achrafothman.net/site/asl-smt/,https://huggingface.co/datasets/achrafothman/aslg_pc12,English,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""aslg_pc12"" Data-Summary: Synthetic English-ASL Gloss Parallel Corpus 2012 Supported Tasks and Leaderboards More Information Needed Languages More Information Needed Dataset Structure Data Instances default Size of downloaded dataset files: 12.77 MB Size of the generated dataset: 13.50 MB Total amount of disk used: 26.27 MB An"
acidcoma/ru_librispeech_for_speaker_separation,,,,,,,https://huggingface.co/datasets/acidcoma/ru_librispeech_for_speaker_separation,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"Dataset for source audio separation task based on Russian LibriSpeech (RuLS) dataset. Dataset contains 50 000 audio mixtures with 2 speakers for train part; 12500 audio mixtures for test part. Dataset also containts metadata files with audio duration (sec), source 1 and source 2 filepaths for each audio mixture. source: https://www.openslr.org/96/"
ACOSharma/literature,,,,,,,https://huggingface.co/datasets/ACOSharma/literature,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"Literature Dataset Files A dataset containing novels, epics and essays. The files are as follows: main.txt, a file with all the texts, every text on a newline, all English vocab.txt, a file with the trained (BERT) vocab, a newline a new word DatasetDistribution.png, a file with all the texts and a plot with character length There are some 7 million tokens in total. Texts The texts used are these: Wuthering Heights Ulysses Treasure Island The Warâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ACOSharma/literature."
adalbertojunior/portuguese_orca,,,,,,,https://huggingface.co/datasets/adalbertojunior/portuguese_orca,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""portuguese_orca"" More Information needed"
Adam173/seinfeld-scripts,,,,,,,https://huggingface.co/datasets/Adam173/seinfeld-scripts,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""seinfeld-scripts"" More Information needed"
Adapting/empathetic_dialogues_v2,,,,,,,https://huggingface.co/datasets/Adapting/empathetic_dialogues_v2,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,"Fine-tuned empathetic dialogue datasets from https://huggingface.co/datasets/empathetic_dialogues With labeled chat history, system response, question or not and behavior."
ade-benchmark-corpus/ade_corpus_v2,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",https://www.sciencedirect.com/science/article/pii/S1532046412000615,https://huggingface.co/datasets/ade-benchmark-corpus/ade_corpus_v2,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,Dataset Card for Adverse Drug Reaction Data v2 Data-Summary: ADE-Corpus-V2 Dataset: Adverse Drug Reaction Data. This is a dataset for Classification if a sentence is ADE-related (True) or not (False) and Relation Extraction between Adverse Drug Event and Drug. DRUG-AE.rel provides relations between drugs and adverse effects. DRUG-DOSE.rel provides relations between drugs and dosages. ADE-NEG.txt provides all sentences in the ADE corpus that DO NOT containâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ade-benchmark-corpus/ade_corpus_v2.
adhitya123/processed_Gita1gpt,,,,,,,https://huggingface.co/datasets/adhitya123/processed_Gita1gpt,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""processed_Gita1gpt"" More Information needed"
AdiOO7/Bank_Complaints,,,,,,,https://huggingface.co/datasets/AdiOO7/Bank_Complaints,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,AdiOO7/Bank_Complaints dataset hosted on Hugging Face and contributed by the HF Datasets community
aertit/xglm_enth2,,,,,,,https://huggingface.co/datasets/aertit/xglm_enth2,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""xglm_enth2"" More Information needed"
AgentGym/AgentTraj-L,,,,,,,https://huggingface.co/datasets/AgentGym/AgentTraj-L,,,,,,,,,,,,,,,,,,,,AgentGym/AgentTraj-L dataset hosted on Hugging Face and contributed by the HF Datasets community
agentica-org/DeepScaleR-Preview-Dataset,,,,,,,https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Data Our training dataset consists of approximately 40,000 unique mathematics problem-answer pairs compiled from: AIME (American Invitational Mathematics Examination) problems (1984-2023) AMC (American Mathematics Competition) problems (prior to 2023) Omni-MATH dataset Still dataset Format Each row in the JSON dataset contains: problem: The mathematical question text, formatted with LaTeX notation. solution: Offical solution to the problem, including LaTeX formattingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset."
agents-course/course-images,,,,,,,https://huggingface.co/datasets/agents-course/course-images,,,,,,,,,,,,,,,,,,,,agents-course/course-images dataset hosted on Hugging Face and contributed by the HF Datasets community
agibot-world/AgiBotWorld-Alpha,,,,,,,https://huggingface.co/datasets/agibot-world/AgiBotWorld-Alpha,,,,,,,,,,,,,,,,,,,,Key Features ðŸ”‘ 1 million+ trajectories from 100 robots. 100+ real-world scenarios across 5 target domains. Cutting-edge hardware: visual tactile sensors / 6-DoF dexterous hand / mobile dual-arm robots Tasks involving: Contact-rich manipulation Long-horizon planning Multi-robot collaboration Your browser does not support the video tag. Your browser does not support the video tag.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/agibot-world/AgiBotWorld-Alpha.
agkphysics/AudioSet,Text,,,,,https://research.google.com/audioset/index.html,https://huggingface.co/datasets/agkphysics/AudioSet,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"This repository contains the balanced training set and evaluation set of the AudioSet data, described here: https://research.google.com/audioset/dataset/index.html. The YouTube videos were downloaded in March 2023, and so not all of the original audios are available."
agomberto/FrenchCensus-handwritten-texts,,,,,,,https://huggingface.co/datasets/agomberto/FrenchCensus-handwritten-texts,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Source This repository contains 3 datasets created within the POPP project (Project for the Oceration of the Paris Population Census) for the task of handwriting text recognition. These datasets have been published in Recognition and information extraction in historical handwritten tables: toward understanding early 20th century Paris census at DAS 2022. The 3 datasets are called â€œGeneric datasetâ€, â€œBellevilleâ€, and â€œChaussÃ©e dâ€™Antinâ€ and contains lines made from the extractedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agomberto/FrenchCensus-handwritten-texts."
aharley/rvl_cdip,Text,,,,,https://www.cs.cmu.edu/~aharley/rvl-cdip/,https://huggingface.co/datasets/aharley/rvl_cdip,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"The RVL-CDIP (Ryerson Vision Lab Complex Document Information Processing) dataset consists of 400,000 grayscale images in 16 classes, with 25,000 images per class. There are 320,000 training images, 40,000 validation images, and 40,000 test images."
ahelk/ccaligned_multilingual,Text,,,,,http://www.statmt.org/cc-aligned/,https://huggingface.co/datasets/ahelk/ccaligned_multilingual,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"CCAligned consists of parallel or comparable web-document pairs in 137 languages aligned with English. These web-document pairs were constructed by performing language identification on raw web-documents, and ensuring corresponding language codes were corresponding in the URLs of web documents. This pattern matching approach yielded more than 100 million aligned documents paired with English. Recognizing that each English document was often aligned to mulitple documents in different target language, we can join on English documents to obtain aligned documents that directly pair two non-English documents (e.g., Arabic-French)."
ahmed-masry/unichart-pretrain-data,,,,,,https://arxiv.org/abs/2305.14761,https://huggingface.co/datasets/ahmed-masry/unichart-pretrain-data,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""unichart-pretrain-data"" If you wanna load the dataset, you can run the following code: from datasets import load_dataset data = load_dataset('ahmed-masry/unichart-pretrain-data') The dataset has the following structure: DatasetDict({ train: Dataset({ features: ['imgname', 'query', 'label'], num_rows: 6898333 }) }) It has 6898333 rows; each row consist of the imgename, the input query, and the output label. Chart Imagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ahmed-masry/unichart-pretrain-data."
AhmedSSoliman/CoNaLa-Large,,,,,,,https://huggingface.co/datasets/AhmedSSoliman/CoNaLa-Large,,,,,,,,,,,,,,,,,,,,AhmedSSoliman/CoNaLa-Large dataset hosted on Hugging Face and contributed by the HF Datasets community
ai-forever/school_notebooks_RU,,,,,,,https://huggingface.co/datasets/ai-forever/school_notebooks_RU,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"School Notebooks Dataset The images of school notebooks with handwritten notes in Russian. The dataset annotation contain end-to-end markup for training detection and OCR models, as well as an end-to-end model for reading text from pages. Annotation format The annotation is in COCO format. The annotation.json should have the following dictionaries: annotation[""categories""] - a list of dicts with a categories info (categotiy names and indexes).â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai-forever/school_notebooks_RU."
AI-Growth-Lab/patents_claims_1.5m_traim_test,,,,,,,https://huggingface.co/datasets/AI-Growth-Lab/patents_claims_1.5m_traim_test,,,,,,,,,,,,,,,,,,,,AI-Growth-Lab/patents_claims_1.5m_traim_test dataset hosted on Hugging Face and contributed by the HF Datasets community
ai-habitat/habitat_test_scenes,,,,,,,https://huggingface.co/datasets/ai-habitat/habitat_test_scenes,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,Habitat Test Scenes Dataset A few lightweight static .glb stages for testing habitat-sim and habitat-lab installation and CI without other datasets. Contents: skokloster-castle.glb - Scan from Sketchfab apartment_0.glb - Scan from Replica Dataset (geometry decimated for simulation and memory efficiency) van-gogh-room.glb - Synthetic Asset from Sketchfab .navmesh files for simulated agent navigation constraints in Habitat-sim.
AI-Lab-Makerere/beans,Image,General,General,Text,"Accuracy, F1 Score",https://github.com/AI-Lab-Makerere/ibean/,https://huggingface.co/datasets/AI-Lab-Makerere/beans,,,,,,,,https://choosealicense.com/licenses/mit/,,,967,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Beans Data-Summary: Beans leaf dataset with images of diseased and health leaves. Supported Tasks and Leaderboards image-classification: Based on a leaf image, the goal of this task is to predict the disease type (Angular Leaf Spot and Bean Rust), if any. Languages English Dataset Structure Data Instances A sample from the training set is provided below: { 'image_file_path':â€¦ See the full description on the dataset page: https://huggingface.co/datasets/AI-Lab-Makerere/beans."
AI-MO/NuminaMath-CoT,Text,Segmentation,Document,Bounding Box/Mask,"IoU, Pixel Accuracy",https://projectnumina.ai,https://huggingface.co/datasets/AI-MO/NuminaMath-CoT,,,,,,,Total,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"Mask R-CNN, DeepLab, U-Net",,,"Dataset Card for NuminaMath CoT Data-Summary: Approximately 860k math problems, where each solution is formatted in a Chain of Thought (CoT) manner. The sources of the dataset range from Chinese high school math exercises to US and international mathematics olympiad competition problems. The data were primarily collected from online exam paper PDFs and mathematics discussion forums. The processing steps include (a) OCR from the original PDFs, (b) segmentationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AI-MO/NuminaMath-CoT."
AI-Secure/DecodingTrust,,,,,,https://arxiv.org/abs/2306.11698,https://huggingface.co/datasets/AI-Secure/DecodingTrust,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models Overview This repo contains the source code of DecodingTrust. This research endeavor is designed to help researchers better understand the capabilities, limitations, and potential risks associated with deploying these state-of-the-art Large Language Models (LLMs). See our paper for details. DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models Boxin Wang, Weixin Chenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AI-Secure/DecodingTrust."
AI-team-UoA/greek_legal_code,Text,General,Document,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2109.15298,https://huggingface.co/datasets/AI-team-UoA/greek_legal_code,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Greek Legal Code Data-Summary: Greek_Legal_Code (GLC) is a dataset consisting of approx. 47k legal resources from Greek legislation. The origin of GLC is â€œPermanent Greek Legislation Code - Raptarchisâ€, a collection of Greek legislative documents classified into multi-level (from broader to more specialized) categories. Topics GLC consists of 47 legislative volumes and each volume corresponds to a main thematic topic. Each volume is divided intoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AI-team-UoA/greek_legal_code."
ai4bharat/samanantar,Text,General,General,Text,"Accuracy, F1 Score",https://ai4bharat.iitm.ac.in/areas/nmt,https://huggingface.co/datasets/ai4bharat/samanantar,"Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, Telugu",,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Samanantar Data-Summary: Samanantar is the largest publicly available parallel corpora collection for Indic language: Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, Telugu. The corpus has 49.6M sentence pairs between English to Indian Languages. Supported Tasks and Leaderboards [More Information Needed] Languages Samanantar contains parallel sentences between English (en) andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/samanantar."
AI4Industry/MolParser-7M,,,,,,,https://huggingface.co/datasets/AI4Industry/MolParser-7M,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"MolParser-7M Anonymous Open Source now This repo provides the training data and evaluation data for MolParser, proposed in paper â€œMolParser: End-to-end Visual Recognition of Molecule Structures in the Wildâ€œ (ICCV2025 under review) MolParser-7M contains nearly 8 million paired image-SMILES data. It should be noted that the caption of image is our extended-SMILES format, which suggested in our paper. MolParser-Pretrain: More than 7.7M synthetic training data in pretrain_synthetic_7Mâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AI4Industry/MolParser-7M."
AI4Math/MathVista,,,,,,https://arxiv.org/abs/2310.02255,https://huggingface.co/datasets/AI4Math/MathVista,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,Data VisualizationData SourceAutomatic EvaluationLicenseCitationDataset Card for MathVistaDataset DescriptionPaper InformationDataset ExamplesLeaderboardDataset UsageData DownloadingData FormatData VisualizationData SourceAutomatic EvaluationLicenseCitationDataset DescriptionMathVistais a consolidated Mathematical reasoning benchmark within Visual contexts,,,,,,Dataset Card for MathVista Dataset Description Paper Information Dataset
ai4privacy/open-pii-masking-500k-ai4privacy,,,,,,,https://huggingface.co/datasets/ai4privacy/open-pii-masking-500k-ai4privacy,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"ðŸŒ World's largest open dataset for privacy masking ðŸŒŽ The dataset is useful to train and evaluate models to remove personally identifiable and sensitive information from text, especially in the context of AI assistants and LLMs. Dataset Analytics ðŸ“Š - ai4privacy/open-pii-masking-500k-ai4privacy p5y Data Analytics Total Entries: 580,227 Total Tokens: 19,199,982 Average Source Text Length: 17.37 words Total PII Labels: 5,705,973 Number ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai4privacy/open-pii-masking-500k-ai4privacy."
AigizK/bashkir-russian-parallel-corpora,,,,,,,https://huggingface.co/datasets/AigizK/bashkir-russian-parallel-corpora,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for ""bashkir-russian-parallel-corpora"" How the dataset was assembled. find the text in two languages. it can be a translated book or an internet page (wikipedia, news site) our algorithm tries to match Bashkir sentences with their translation in Russian We give these pairs to people to check @inproceedings{ title={Bashkir-Russian parallel corpora}, author={Iskander Shakirov, Aigiz Kunafin}, year={2023} }"
AILab-CVC/SEED-Bench,,,,,,,https://huggingface.co/datasets/AILab-CVC/SEED-Bench,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"SEED-Bench Card Benchmark details Benchmark type: SEED-Bench is a large-scale benchmark to evaluate Multimodal Large Language Models (MLLMs). It consists of 19K multiple choice questions with accurate human annotations, which covers 12 evaluation dimensions including the comprehension of both the image and video modality. Benchmark date: SEED-Bench was collected in July 2023. Paper or resources for more information: https://github.com/AILab-CVC/SEED-Bench License:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/AILab-CVC/SEED-Bench."
AILAB-VNUHCM/vivos,Text,,,,,https://doi.org/10.5281/zenodo.7068130,https://huggingface.co/datasets/AILAB-VNUHCM/vivos,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"\ VIVOS is a free Vietnamese speech corpus consisting of 15 hours of recording speech prepared for Vietnamese Automatic Speech Recognition task. The corpus was prepared by AILAB, a computer science lab of VNUHCM - University of Science, with Prof. Vu Hai Quan is the head of. We publish this corpus in hope to attract more scientists to solve Vietnamese speech recognition problems."
AIMClab-RUC/FunBench,,,,,,https://arxiv.org/abs/2503.00901,https://huggingface.co/datasets/AIMClab-RUC/FunBench,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,FunBench: Benchmarking Fundus Reading Skills of MLLMs FunBench is a novel visual question answering (VQA) benchmark designed to comprehensively evaluate MLLMs' fundus reading skills. Code and description are available at https://github.com/ruc-aimc-lab/FunBench
AIML-TUDA/i2p,,,,,,https://arxiv.org/abs/2211.05105,https://huggingface.co/datasets/AIML-TUDA/i2p,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Inaproppriate Image Prompts (I2P) The I2P benchmark contains real user prompts for generative text2image prompts that are unproportionately likely to produce inappropriate images. I2P was introduced in the 2023 CVPR paper Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. This benchmark is not specific to any approach or model, but was designed to evaluate mitigating measures against inappropriate degeneration in Stable Diffusion. The correspondingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AIML-TUDA/i2p."
ajibawa-2023/Children-Stories-Collection,,,,,,,https://huggingface.co/datasets/ajibawa-2023/Children-Stories-Collection,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Children Stories Collection A great synthetic datasets consists of around 0.9 million stories especially meant for Young Children. You can directly use these datasets for training large models. Total 10 datasets are available for download. You can use any one or all the json files for training purpose. These datasets are in ""prompt"" and ""text"" format. Total token length is also available. Thank you for your love & support."
akariasai/PopQA,Text,Question Answering,General,Text,"Exact Match, F1 Score",,https://huggingface.co/datasets/akariasai/PopQA,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for PopQA Data-Summary: PopQA is a large-scale open-domain question answering (QA) dataset, consisting of 14k entity-centric QA pairs. Each question is created by converting a knowledge tuple retrieved from Wikidata using a template. Each question come with the original subject_entitiey, object_entityand relationship_type annotation, as well as Wikipedia monthly page views. Languages The dataset contains samples in English only.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/akariasai/PopQA."
akhtet/myanmar-xnli,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/akhtet/myanmar-xnli,English,,,,,,,https://choosealicense.com/licenses/cc-by-nc-2.0/,,1930,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for myXNLI Data-Summary: The myXNLI corpus extends XNLI corpus with Myanmar (Burmese) language. For myXNLI, we human-translated all 7,500 sentence pairs from XNLI English dev and test sets into Myanmar. The NLI and Genre labels from English dev and test sets are also reused for the Myanmar datasets. The dataset also includes the NLI training data in Myanmar which is created by machine-translating the MultiNLI training data from English into Myanmar.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/akhtet/myanmar-xnli."
Akila/ForgottenRealmsWikiDataset,,,,,,,https://huggingface.co/datasets/Akila/ForgottenRealmsWikiDataset,,,,,,,,,,,,,,,,,,,,"Citing this work @inproceedings{peiris2022synthesis, title={{Synthesis and Evaluation of a Domain-specific Large Data Set for Dungeons \& Dragons}}, author={Peiris, Akila and de Silva, Nisansa}, booktitle={Proceedings of the 36th Pacific Asia Conference on Language, Information and Computation}, pages={415--424}, year={2022} }"
akozlova/RuFacts,,,,,,,https://huggingface.co/datasets/akozlova/RuFacts,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,Fact-checking benchmark for the Russian Big Language Models.
albertvillanova/legal_contracts,,,,,,,https://huggingface.co/datasets/albertvillanova/legal_contracts,,,,,,,,,,,,,,,,,,,,This new dataset is designed to solve this great NLP task and is crafted with a lot of care.
Albinator/Vladimir-Lenin,,,,,,,https://huggingface.co/datasets/Albinator/Vladimir-Lenin,,,,,,,,,,,,,,,,,,,,Albinator/Vladimir-Lenin dataset hosted on Hugging Face and contributed by the HF Datasets community
AlekseyKorshuk/romance-books,,,,,,,https://huggingface.co/datasets/AlekseyKorshuk/romance-books,,,,,,,,,,,,,,,,,,,,AlekseyKorshuk/romance-books dataset hosted on Hugging Face and contributed by the HF Datasets community
alespalla/chatbot_instruction_prompts,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/alespalla/chatbot_instruction_prompts,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,d up of spurious entries and artifacts,,"BERT, RoBERTa, T5",,,Dataset Card for Chatbot Instruction Prompts Datasets Data-Summary: This dataset has been generated from the following ones: tatsu-lab/alpaca Dahoas/instruct-human-assistant-prompt allenai/prosocial-dialog The datasets has been cleaned up of spurious entries and artifacts. It contains ~500k of prompt and expected resposne. This DB is intended to train an instruct-type model
AlexaAI/bold,,,,,,https://arxiv.org/abs/2101.11718,https://huggingface.co/datasets/AlexaAI/bold,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for Bias in Open-ended Language Generation Dataset (BOLD) Dataset Description Bias in Open-ended Language Generation Dataset (BOLD) is a dataset to evaluate fairness in open-ended language generation in English language. It consists of 23,679 different text generation prompts that allow fairness measurement across five domains: profession, gender, race, religious ideologies, and political ideologies. Some"
alexandrainst/nordjylland-news-image-captioning,Image,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/alexandrainst/nordjylland-news-image-captioning,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""nordjylland-news-image-captioning"" Data-Summary: This dataset is a collection of image-caption pairs from the Danish newspaper TV2 Nord. Supported Tasks and Leaderboards Image captioning is the intended task for this dataset. No leaderboard is active at this point. Languages The dataset is available in Danish (da). Dataset Structure An"
alexfabbri/multi_news,Text,,,,,https://github.com/Alex-Fabbri/Multi-News,https://huggingface.co/datasets/alexfabbri/multi_news,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"Multi-News, consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited. There are two features: - document: text of news articles seperated by special token ""|||||"". - summary: news summary."
alexjercan/bugnet,,,,,,,https://huggingface.co/datasets/alexjercan/bugnet,,,,,,,,,,,,https://github.com/IBM/Project_CodeNet,,,,,,,,\
alexwww94/datasets-for-simcse,,,,,,,https://huggingface.co/datasets/alexwww94/datasets-for-simcse,,,,,,,,,,1930,,,,,,,,,,datasets-for-simcse
alfredodeza/wine-ratings,,,,,,,https://huggingface.co/datasets/alfredodeza/wine-ratings,,,,,,,,,,,,,,,,,,,,"wine-ratings Processing, EDA, and ML on wine ratings"
AlgorithmicResearchGroup/arxiv_cplusplus_research_code,Text,General,Scientific,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/AlgorithmicResearchGroup/arxiv_cplusplus_research_code,,10.6 GB,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset card for ArtifactAI/arxiv_cplusplus_research_code Dataset Description https://huggingface.co/datasets/AlgorithmicResearchGroup/arxiv_cplusplus_research_code Data-Summary: ArtifactAI/arxiv_python_research_code contains over 10.6GB of source code files referenced strictly in ArXiv papers. The dataset serves as a curated dataset for Code LLMs. How to use it from datasets import load_dataset # full dataset (10.6GB of data) dsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlgorithmicResearchGroup/arxiv_cplusplus_research_code.
alielfilali01/Darija-Stories-Dataset,,,,,,,https://huggingface.co/datasets/alielfilali01/Darija-Stories-Dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Dataset Card for ""Darija-Stories-Dataset"" Darija (Moroccan Arabic) Stories Dataset is a large-scale collection of stories written in Moroccan Arabic dialect (Darija). Dataset Description Darija (Moroccan Arabic) Stories Dataset contains a diverse range of stories that provide insights into Moroccan culture, traditions, and everyday life. The dataset consists of textual content from various chapters, including narratives, dialogues, and descriptions. Each storyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alielfilali01/Darija-Stories-Dataset."
aligeniewcp22/LCSTS,,,,,,,https://huggingface.co/datasets/aligeniewcp22/LCSTS,,,,,,,,,,,,,,,,,,,,aligeniewcp22/LCSTS dataset hosted on Hugging Face and contributed by the HF Datasets community
Alignment-Lab-AI/Lawyer-chat,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Alignment-Lab-AI/Lawyer-chat,", containing dialogues about legal scenarios",,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Description Data-Summary: LawyerChat is a multi-turn conversational dataset primarily in the English language, containing dialogues about legal scenarios. The conversations are in the format of an interaction between a client and a legal professional. The dataset is designed for training and evaluating models on conversational tasks like dialogue understanding, response generation, and more. Supported Tasks and Leaderboardsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Alignment-Lab-AI/Lawyer-chat."
Aliissa99/FrenchMedMCQA,,,,,,,https://huggingface.co/datasets/Aliissa99/FrenchMedMCQA,,,,,,,,,,,,,,,,,,,,Aliissa99/FrenchMedMCQA dataset hosted on Hugging Face and contributed by the HF Datasets community
alkzar90/NIH-Chest-X-ray-dataset,Text,,,,,https://nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345,https://huggingface.co/datasets/alkzar90/NIH-Chest-X-ray-dataset,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"The NIH Chest X-ray dataset consists of 100,000 de-identified images of chest x-rays. The images are in PNG format. The data is provided by the NIH Clinical Center and is available through the NIH download site: https://nihcc.app.box.com/v/ChestXray-NIHCC"
allenai/real-toxicity-prompts,Text,General,Scientific,Text,"Accuracy, F1 Score",https://toxicdegeneration.allenai.org/,https://huggingface.co/datasets/allenai/real-toxicity-prompts,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Real Toxicity Prompts Data-Summary: RealToxicityPrompts is a dataset of 100k sentence snippets from the web for researchers to further address the risk of neural toxic degeneration in models. Languages English Dataset Structure Data Instances Each instance represents a prompt and its metadata: { ""filename"":""0766186-bc7f2a64cb271f5f56cf6f25570cd9ed.txt"", ""begin"":340, ""end"":564â€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/real-toxicity-prompts."
alpayariyak/IAM_Sentences,,,,,,,https://huggingface.co/datasets/alpayariyak/IAM_Sentences,,,,,,,,,,,,,,,,,,,,IAM Sentences This dataset contains all sentences from the IAM Handwriting database as combined images instead of separate lines.
alpindale/light-novels,,,,,,,https://huggingface.co/datasets/alpindale/light-novels,,,,,,,,https://choosealicense.com/licenses/creativeml-openrail-m/,,,,,,,,,,,,alpindale/light-novels dataset hosted on Hugging Face and contributed by the HF Datasets community
AlppAI/SlimPajama-chunked,,,,,,,https://huggingface.co/datasets/AlppAI/SlimPajama-chunked,,,,,,,,,,,,,,,,,,,,"SlimPajama-Chunked Dataset Description This is a chunked re-upload of Cerebras' SlimPajama-627B. The original upload has split the dataset into 10 chunks, with each containing upwards of 5,000 files. This makes it cumbersome to download and process. We've downloaded the entire dataset for our own purposes, and decided to upload the chunked version for easier usage. Each file is ~45GB due to HuggingFace's limitation of 50GB per LFS file."
alxfgh/PubChem_Desc_DrugChat,,,,,,,https://huggingface.co/datasets/alxfgh/PubChem_Desc_DrugChat,,,,,,,,,,,,,,,,,,,,alxfgh/PubChem_Desc_DrugChat dataset hosted on Hugging Face and contributed by the HF Datasets community
AmaanP314/youtube-comment-sentiment,,,,,,,https://huggingface.co/datasets/AmaanP314/youtube-comment-sentiment,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"YouTube Comments Sentiment Analysis Dataset (1M+ Labeled Comments) Overview This dataset comprises over one million YouTube comments, each annotated with sentiment labelsâ€”Positive, Neutral, or Negative. The comments span a diverse range of topics including programming, news, sports, politics and more, and are enriched with comprehensive metadata to facilitate various NLP and sentiment analysis tasks. Dataset Contents Each record in the dataset includes theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AmaanP314/youtube-comment-sentiment."
Amani27/massive_translation_dataset,Text,Translation,Scientific,Text,"BLEU, METEOR, TER",,https://huggingface.co/datasets/Amani27/massive_translation_dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"T5, mBART, M2M100",,,Dataset Card for Massive Dataset for Translation Data-Summary: This dataset is derived from AmazonScience/MASSIVE dataset for translation task purpose. Supported Tasks and Leaderboards Translation Languages English (en_US) German (de_DE) Hindi (hi_IN) Spanish (es_ES) French (fr_FR) Italian (it_IT) Arabic (ar_SA) Dutch (nl_NL) Japanese (ja_JP) Portugese (pt_PT)
amanneo/enron-mail-corpus-mini,,,,,,,https://huggingface.co/datasets/amanneo/enron-mail-corpus-mini,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""enron-mail-corpus-mini"" More Information needed"
amay01/llm-sgd-dst8-training-data,,,,,,,https://huggingface.co/datasets/amay01/llm-sgd-dst8-training-data,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""llm-sgd-dst8-training-data"" More Information needed"
amaydle/npc-dialogue,,,,,,,https://huggingface.co/datasets/amaydle/npc-dialogue,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""npc-dialogue"" More Information needed"
amaye15/NSFW,,,,,,,https://huggingface.co/datasets/amaye15/NSFW,,,,,,,,,,,,,,,,,,,,amaye15/NSFW dataset hosted on Hugging Face and contributed by the HF Datasets community
AmazonScience/massive,Text,,,,,https://github.com/alexa/massive,https://huggingface.co/datasets/AmazonScience/massive,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"MASSIVE is a parallel dataset of > 1M utterances across 51 languages with annotations for the Natural Language Understanding tasks of intent prediction and slot annotation. Utterances span 60 intents and include 55 slot types. MASSIVE was created by localizing the SLURP dataset, composed of general Intelligent Voice Assistant single-shot interactions."
amishshah/song_lyrics,,,,,,,https://huggingface.co/datasets/amishshah/song_lyrics,,,,,,,,,,,,,,,,,,,,amishshah/song_lyrics dataset hosted on Hugging Face and contributed by the HF Datasets community
amitness/wikipedia_it,,,,,,,https://huggingface.co/datasets/amitness/wikipedia_it,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""wikipedia_it"" More Information needed"
ammarnasr/the-stack-java-clean,,,,,,,https://huggingface.co/datasets/ammarnasr/the-stack-java-clean,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,"Dataset 1: TheStack - Java - Cleaned Description: This dataset is drawn from TheStack Corpus, an open-source code dataset with over 3TB of GitHub data covering 48 programming languages. We selected a small portion of this dataset to optimize smaller language models for Java, a popular statically typed language. Target Language: Java Dataset Size: Training: 900,000 files Validation: 50,000 files Test: 50,000 files Preprocessing: Selected Java as the target language due to itsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ammarnasr/the-stack-java-clean."
Amo/Gothic-1-multilingual-dialogue,,,,,,,https://huggingface.co/datasets/Amo/Gothic-1-multilingual-dialogue,,,,,,,,,,,,,,,,,,,,"UPLOADED FOR EDUCATIONAL PURPOSE Audio dataset created from Gothic 1 video game files, from the laguage version of German, English, Polish and Russian (Snowball version) dubbings. It contains bot the wav audio files of seperated dialogue as well as the text transcription file per NPC."
Amod/mental_health_counseling_conversations,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Amod/mental_health_counseling_conversations,models to improve their ability to provide mental health advice,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,"BERT, RoBERTa, T5",,,"Amod/mental_health_counseling_conversations Data-Summary: This dataset is a collection of questions and answers sourced from two online counseling and therapy platforms. The questions cover a wide range of mental health topics, and the answers are provided by qualified psychologists. The dataset is intended to be used for fine-tuning language models to improve their ability to provide mental health advice. Supported Tasks and Leaderboards Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Amod/mental_health_counseling_conversations."
amongglue/youtube_subtitles,,,,,,,https://huggingface.co/datasets/amongglue/youtube_subtitles,,,,,,,,https://choosealicense.com/licenses/mit/,,,360,,,,,,,,,amongglue/youtube_subtitles dataset hosted on Hugging Face and contributed by the HF Datasets community
amphora/QwQ-LongCoT-130K,,,,,,,https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Also have a look on the second version here => QwQ-LongCoT-2 Figure 1: Just a cute picture generate with [Flux](https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-Logo-Design) Today, Iâ€™m excited to release QwQ-LongCoT-130K, a SFT dataset designed for training O1-like large language models (LLMs). This dataset includes about 130k instances, each with responses generated using QwQ-32B-Preview. The dataset is available under the Apache 2.0 license, so feel free to use it as you like.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K."
Amr-khaled/Egyptian-Arabic_English_V1,,,,,,,https://huggingface.co/datasets/Amr-khaled/Egyptian-Arabic_English_V1,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Amr-khaled/Egyptian-Arabic_English_V1 dataset hosted on Hugging Face and contributed by the HF Datasets community
AnanthZeke/tamil_sentences_sample,,,,,,,https://huggingface.co/datasets/AnanthZeke/tamil_sentences_sample,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Card for ""tamil_combined_sentences"" More Information needed"
andersonbcdefg/supernatural-instructions-2m,,,,,,,https://huggingface.co/datasets/andersonbcdefg/supernatural-instructions-2m,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""supernatural-instructions-2m"" More Information needed"
andito/mathwriting-google,,,,,,,https://huggingface.co/datasets/andito/mathwriting-google,,,,,,,,,,,,,,,,,,,,andito/mathwriting-google dataset hosted on Hugging Face and contributed by the HF Datasets community
AndreiMuresanu/alpaca_flan-format,,,,,,,https://huggingface.co/datasets/AndreiMuresanu/alpaca_flan-format,,,,,,,,,,,,,,,,,,,,AndreiMuresanu/alpaca_flan-format dataset hosted on Hugging Face and contributed by the HF Datasets community
andrewburns/hf_flat_icons,,,,,,,https://huggingface.co/datasets/andrewburns/hf_flat_icons,,,,,,,,,,,,,,,,,,,,andrewburns/hf_flat_icons dataset hosted on Hugging Face and contributed by the HF Datasets community
andrewkroening/Star-wars-scripts-dialogue-IV-VI,,,,,,,https://huggingface.co/datasets/andrewkroening/Star-wars-scripts-dialogue-IV-VI,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,"Dataset Contents This dataset contains the concatenated scripts from the original (and best) Star Wars trilogy. The scripts are reduced to dialogue only, and are tagged with a line number and speaker. Dataset Disclaimer I don't own this data; or Star Wars. But it would be cool if I did. Star Wars is owned by Lucasfilms. I do not own any of the rights to this information. The scripts are derived from a couple sources: This GitHub Repo with raw files A Kaggleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/andrewkroening/Star-wars-scripts-dialogue-IV-VI."
AndriiPets/autotrain-data-plant-disease-classification,,,,,,,https://huggingface.co/datasets/AndriiPets/autotrain-data-plant-disease-classification,,,,,,,,,,,,,,,,,,,,AndriiPets/autotrain-data-plant-disease-classification dataset hosted on Hugging Face and contributed by the HF Datasets community
andstor/smart_contracts,Text,,,,,https://andstor.github.io/smart-contracts,https://huggingface.co/datasets/andstor/smart_contracts,,,,,,,,,,,,,,,,,,,,"Smart Contracts Dataset. This is a dataset of verified (Etherscan.io) Smart Contracts that are deployed to the Ethereum blockchain. A set of about 100,000 to 200,000 contracts are provided, containing both Solidity and Vyper code."
aneeshas/imsdb-genre-movie-scripts,,,,,,,https://huggingface.co/datasets/aneeshas/imsdb-genre-movie-scripts,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""imsdb-genre-movie-scripts"" More Information needed"
angie-chen55/python-github-code,,,,,,,https://huggingface.co/datasets/angie-chen55/python-github-code,,,,,,,,,,,,,,,,,,,,angie-chen55/python-github-code dataset hosted on Hugging Face and contributed by the HF Datasets community
animelover/touhou-images,,,,,,,https://huggingface.co/datasets/animelover/touhou-images,,,,,,,,,,,,,,,,,,,,animelover/touhou-images dataset hosted on Hugging Face and contributed by the HF Datasets community
animonte/train_house_price,,,,,,,https://huggingface.co/datasets/animonte/train_house_price,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,animonte/train_house_price dataset hosted on Hugging Face and contributed by the HF Datasets community
annassdan/autotrain-data-fish-classification,,,,,,,https://huggingface.co/datasets/annassdan/autotrain-data-fish-classification,,,,,,,,,,,,,,,,,,,,annassdan/autotrain-data-fish-classification dataset hosted on Hugging Face and contributed by the HF Datasets community
anon8231489123/Omegle_logs_dataset,,,,,,,https://huggingface.co/datasets/anon8231489123/Omegle_logs_dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"~10k conversations from Omegle. Scraped using: http://web.archive.org/cdx/search/xd?url=logs.omegle.com/*&fl=timestamp,original,statuscode&output=json. For these logs to have ended up on the cdx, it means the url was posted publicly at some point. PII removed by searching for conversations with these words: forbidden_words = [""kik"", ""telegram"", ""skype"", ""wickr"", ""discord"", ""dropbox"", ""insta "", ""insta?"", ""instagram"", ""snap "", ""snapchat""]. Conversations with racial slurs removed. English only.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/anon8231489123/Omegle_logs_dataset."
antebe1/paraphrased_AI_text,,,,,,,https://huggingface.co/datasets/antebe1/paraphrased_AI_text,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"This dataset is a subset of the COLING-2025 GenAI workshop shared task 1 dataset (https://genai-content-detection.gitlab.io/sharedtasks) as well as the respective paraphrasing of AI-generated text. The paraphrasing has been done by the software GPTInf (https://www.gptinf.com/). Keywords: AI text, paraphrasings, AI detection, AI bypassing"
Anthropic/EconomicIndex,,,,,,,https://huggingface.co/datasets/Anthropic/EconomicIndex,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Overview This directory contains O*NET task mapping and automation vs. augmentation data from ""Which Economic Tasks are Performed with AI? Evidence from Millions of Claude Conversations."" The data and provided analysis are described below. Please see our blog post and paper for further visualizations and complete analysis. Data SOC_Structure.csv - Standard Occupational Classification (SOC) system hierarchy from the U.S. Department of Labor O*NET databaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Anthropic/EconomicIndex."
AntiplagiatCompany/HWR200,,,,,,,https://huggingface.co/datasets/AntiplagiatCompany/HWR200,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,HWR200: New open access dataset of handwritten texts images in Russian This is a dataset of handwritten texts images in Russian created by 200 writers with different handwriting and photographed in different environment. Data Usage How to download pip install huggingface_hub apt-get install git-lfs git clone https://huggingface.co/datasets/AntiplagiatCompany/HWR200 Description Total size is 44G Total number of images with text isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AntiplagiatCompany/HWR200.
Antreas/TALI,,,,,,,https://huggingface.co/datasets/Antreas/TALI,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for ""TALI"" Dataset Description Abstract TALI is a large-scale, tetramodal dataset designed to facilitate a shift from unimodal and duomodal to tetramodal research in deep learning. It aligns text, video, images, and audio, providing a rich resource for innovative self-supervised learning tasks and multimodal research. TALI enables exploration of how different modalities and data/model scaling affect downstream performance, with the aimâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Antreas/TALI."
arakesh/deepglobe-2448x2448,,,,,,,https://huggingface.co/datasets/arakesh/deepglobe-2448x2448,,,,,,,,,,,,,,,,,,,,Data source: http://deepglobe.org/ images semantic maps instance ids available available n/a dataset-size: 2.0G resolution: 2448x2448 license: ... sample-size: ./pix2pixHD-deepglobe-synthesis â”œâ”€â”€ test_img [30 entries] â”œâ”€â”€ test_label [30 entries] â”œâ”€â”€ train_img [773 entries] â””â”€â”€ train_label [773 entries]
arazd/tulu_stanford_alpaca,,,,,,,https://huggingface.co/datasets/arazd/tulu_stanford_alpaca,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,arazd/tulu_stanford_alpaca dataset hosted on Hugging Face and contributed by the HF Datasets community
arbml/CIDAR,,,,,,https://arxiv.org/abs/2402.03177,https://huggingface.co/datasets/arbml/CIDAR,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Dataset Card for ""CIDAR"" ðŸŒ´CIDAR: Culturally Relevant Instruction Dataset For Arabic [ Paper - GitHub ] CIDAR contains 10,000 instructions and their output. The dataset was created by selecting around 9,109 samples from Alpagasus dataset then translating it to Arabic using ChatGPT. In addition, we append that with around 891 Arabic grammar instructions from the webiste Ask the teacher. All the 10,000 samples were reviewed by around 12 reviewers.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/arbml/CIDAR."
arc-agi-community/arc-agi-2,,,,,,,https://huggingface.co/datasets/arc-agi-community/arc-agi-2,,,,,,,,,,,,,,,,,,,,arc-agi-community/arc-agi-2 dataset hosted on Hugging Face and contributed by the HF Datasets community
argilla/medical-keywords,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/argilla/medical-keywords,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""medical-keywords"" Data-Summary: Medical transcription data scraped from mtsamples.com Medical data is extremely hard to find due to HIPAA privacy regulations. This dataset offers a solution by providing medical transcription samples. This dataset contains sample medical transcriptions for various medical specialties. Languages english Citation Information Acknowledgements Medical transcription data scraped fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/medical-keywords."
arielnlee/Superimposed-Masked-Dataset,,,,,,https://arxiv.org/abs/2306.17848,https://huggingface.co/datasets/arielnlee/Superimposed-Masked-Dataset,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"SMD is an occluded ImageNet-1K validation set, created to be an additional way to evaluate the impact of occlusion on model performance. This experiment used a variety of occluder objects that are not in the ImageNet-1K label space and are unambiguous in relationship to objects that reside in the label space."
Arkan0ID/furniture-dataset,,,,,,,https://huggingface.co/datasets/Arkan0ID/furniture-dataset,,,,,,,,,,,,,,,,,,,,Arkan0ID/furniture-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
armanc/scientific_papers,Text,,,,,,https://huggingface.co/datasets/armanc/scientific_papers,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"Scientific papers datasets contains two sets of long and structured documents. The datasets are obtained from ArXiv and PubMed OpenAccess repositories. Both ""arxiv"" and ""pubmed"" have two features: - article: the body of the document, pagragraphs seperated by ""/n"". - abstract: the abstract of the document, pagragraphs seperated by ""/n"". - section_names: titles of sections, seperated by ""/n""."
ArmelR/stack-exchange-instruction,,,,,,,https://huggingface.co/datasets/ArmelR/stack-exchange-instruction,,,,,,,,,,,20,,,,,,,,,"Dataset Card for ""stack-exchange-instruction"" More Information needed"
Armewer/Free-Download-Solara-Executor-2025,,,,,,,https://huggingface.co/datasets/Armewer/Free-Download-Solara-Executor-2025,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"ðŸ“¥ Download Now Archive password: 2025 Requirements: Windows 10/11 Features: Activation license This script applies the registry lock method to activate This method requires the Internet at the time of activation. Freeze Trial Freeze 30-day trial period, you can use this option in the script to lock this trial period for the lifetime so that you wont have to reset the trial again and your trial wont expire. This method requires the Internet at theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Armewer/Free-Download-Solara-Executor-2025."
arnavmahapatra/fruit-detection-dataset,,,,,,,https://huggingface.co/datasets/arnavmahapatra/fruit-detection-dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,arnavmahapatra/fruit-detection-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
arnepeine/medspeech3,,,,,,,https://huggingface.co/datasets/arnepeine/medspeech3,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""medspeech3"" More Information needed"
arnoudbuzing/linear-equation-training,,,,,,,https://huggingface.co/datasets/arnoudbuzing/linear-equation-training,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,arnoudbuzing/linear-equation-training dataset hosted on Hugging Face and contributed by the HF Datasets community
ArrayCats/triton-2.0.0-cp310-cp310-win_amd64,,,,,,,https://huggingface.co/datasets/ArrayCats/triton-2.0.0-cp310-cp310-win_amd64,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,ArrayCats/triton-2.0.0-cp310-cp310-win_amd64 dataset hosted on Hugging Face and contributed by the HF Datasets community
Artificio/WikiArt,,,,,,,https://huggingface.co/datasets/Artificio/WikiArt,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""WikiArt"" More Information needed"
ARTPARK-IISc/Vaani,,,,,,,https://huggingface.co/datasets/ARTPARK-IISc/Vaani,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"VAANI is an India-representative multi-modal multi-lingual dataset. The current version (phase 1- 80 districts) contains ~16,000 hours of spontaenous,image-prompted speech (9.6 Million utterances) by 84.6K speakers across 80 districts, talking about 130K images covering 54 languages. From this audio data, 788.03 hours of transcribed data(text) is available, spanning almost evenly across the 80 districts. Project Vaani, by IISc, Bangalore and ARTPARK, is capturing the true diversity ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ARTPARK-IISc/Vaani."
arxiv-community/arxiv_dataset,Text,,,,,https://www.kaggle.com/Cornell-University/arxiv,https://huggingface.co/datasets/arxiv-community/arxiv_dataset,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"A dataset of 1.7 million arXiv articles for applications like trend analysis, paper recommender engines, category prediction, co-citation networks, knowledge graph construction and semantic search interfaces."
AshishNehra/emotion-instruction,,,,,,,https://huggingface.co/datasets/AshishNehra/emotion-instruction,,,,,,,,,,,,,,,,,,,,AshishNehra/emotion-instruction dataset hosted on Hugging Face and contributed by the HF Datasets community
ashraf-ali/quran-data,,,,,,,https://huggingface.co/datasets/ashraf-ali/quran-data,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,Dataset Card for Quran audio Content 7 Imam Full Quran Recitation: 7*6236 wav file csv contains the Text info for 11k subset short wav file Tarteel.io user dataset ~25k wav csv contains the Text info for 18k subset of the accepted user quality
ashraq/movielens_ratings,,,,,,,https://huggingface.co/datasets/ashraq/movielens_ratings,,,,,,,,,,,,,,,,,,,,ashraq/movielens_ratings dataset hosted on Hugging Face and contributed by the HF Datasets community
asigalov61/MIDI-Cores,,,,,,,https://huggingface.co/datasets/asigalov61/MIDI-Cores,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,MIDI cores 386k+ select MIDI cores from Monster MIDI Dataset Load dataset #=================================================================== from datasets import load_dataset #=================================================================== midi_cores = load_dataset('asigalov61/Monster-Cores') Project Los Angeles Tegridy Code 2025
ASSERT-KTH/FLAMES_results,,,,,,,https://huggingface.co/datasets/ASSERT-KTH/FLAMES_results,,,,,,,,,,,,,,,,,,,,ASSERT-KTH/FLAMES_results dataset hosted on Hugging Face and contributed by the HF Datasets community
Astrale0031/hardware_prices,,,,,,,https://huggingface.co/datasets/Astrale0031/hardware_prices,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,Astrale0031/hardware_prices dataset hosted on Hugging Face and contributed by the HF Datasets community
Astroneural/xkcd-comics,,,,,,,https://huggingface.co/datasets/Astroneural/xkcd-comics,,,,,,,,,,,,,,,,,,,,Astroneural/xkcd-comics dataset hosted on Hugging Face and contributed by the HF Datasets community
asuender/motivational-quotes,,,,,,,https://huggingface.co/datasets/asuender/motivational-quotes,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,"Dataset Card for Motivational Quotes This is a dataset of motivational quotes, scraped from Goodreads. It contains more than 4000 quotes, each of them labeled with the corresponding author. Data overview The quotes subset contains the raw quotes and the corresponding authors. The quotes_extended subset contains the raw quotes plus a short prompt that can be used to train LLMs to generate new quotes: // quotes { ""quote"": ""â€œDo not fear failure but rather fear notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/asuender/motivational-quotes."
AtlasUnified/Atlas-Reasoning,,,,,,,https://huggingface.co/datasets/AtlasUnified/Atlas-Reasoning,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,ATLAS-REASONING This dataset derives from the code here: atlasunified/atlas-reasoning and is synthetically generated by GPT-3.5-turbo. Categories The main 42 (See the repo to check the JSONL) categories below were human derived while the subcategories were synthetically generated by GPT-4. 1 Deductive Reasoning -1.1 Syllogistic Arguments -1.2 Assumptions -1.3 Abductive Reasoning -1.4 Modus Ponens -1.5 Modus Tollens -1.6 Problem Solving -1.7 Goalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AtlasUnified/Atlas-Reasoning.
Atnafu/Parallel_dataset_for_Ethiopian_languages,,,,,,,https://huggingface.co/datasets/Atnafu/Parallel_dataset_for_Ethiopian_languages,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,This dataset contains parallel corpora for Ethiopian languages
atokforps/latent_v1_fullrun_alpha1_11,,,,,,,https://huggingface.co/datasets/atokforps/latent_v1_fullrun_alpha1_11,,,,,,,,,,,,,,,,,,,,atokforps/latent_v1_fullrun_alpha1_11 dataset hosted on Hugging Face and contributed by the HF Datasets community
Atomi/sem_eval_2013_task_7,Text,General,Scientific,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Atomi/sem_eval_2013_task_7,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for SemEval 2013 Task 7 Dataset Dataset Description Data-Summary: This dataset contains responses to questions from two distinct corpuses, BEETLE and SCIENTSBANK. The BEETLE corpus consists of 56 questions in an electricity and circuits domain, requiring answers of 1-2 sentences and containing approximately 3000 answers. The SCIENTSBANK corpus consists of 197 questions in 15 different science domains, containing approximately 10000â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Atomi/sem_eval_2013_task_7."
attn-signs/kolmogorov-3,,,,,,,https://huggingface.co/datasets/attn-signs/kolmogorov-3,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Kolmogorov-3 Carefully selected, checked and formatted PhD-level russian math instruction dataset.Contains olympiad/university/science-level tasks from various sources. The dataset is a merge of 3 different math/code-to-math datasets, included by"
Aunsiels/InfantBooks,Text,General,General,Text,"Accuracy, F1 Score",https://www.mpi-inf.mpg.de/children-texts-for-commonsense,https://huggingface.co/datasets/Aunsiels/InfantBooks,,,,,,,,https://choosealicense.com/licenses/gpl/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for InfantBooks Data-Summary: A dataset of infants/children's books. Languages All the books are in English; Dataset Structure Data Instances malis-friend_BookDash-FKB.txt,""Then a taxi driver, hooting around the yard with his wire car. Mali enjoys playing by himself..."" Data Fields title: The title of the book content: The content of the book Dataset Creationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Aunsiels/InfantBooks."
aurelium/github-repo-enumeration,,,,,,,https://huggingface.co/datasets/aurelium/github-repo-enumeration,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"This dataset was generated from GHArchive's Google BigQuery table. It contains a list of every public repo (~380,000,000) committed to from January 2016 up to August 2024, as well as the number of unique contributors and totals of the amounts of various events on those repositories in that time period. This is useless on its own, but represents more than a few hours of effort and roughly $8 worth of cloud processing, so I figured I would save the next person to try this some effort."
autoevaluate/autoeval-staging-eval-project-sms_spam-216c1ded-12215630,,,,,,,https://huggingface.co/datasets/autoevaluate/autoeval-staging-eval-project-sms_spam-216c1ded-12215630,,,,,,,,,,,,,,,,,,,,"Dataset Card for AutoTrain Evaluator This repository contains model predictions generated by AutoTrain for the following task and dataset: Task: Binary Text Classification Model: Rhuax/MiniLMv2-L12-H384-distilled-finetuned-spam-detection Dataset: sms_spam Config: plain_text Split: train To run new evaluation jobs, visit Hugging Face's automatic model evaluator. Contributions Thanks to @Al-Ip for evaluating this model."
AutonLab/Timeseries-PILE,,,,,,https://arxiv.org/abs/1703.07015,https://huggingface.co/datasets/AutonLab/Timeseries-PILE,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Time Series PILE The Time-series Pile is a large collection of publicly available data from diverse domains, ranging from healthcare to engineering and finance. It comprises of over 5 public time-series databases, from several diverse domains for time series foundation model pre-training and evaluation. Time Series PILE Description We compiled a large collection of publicly available datasets from diverse domains into the Time Series Pile. It has 13 unique domainsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AutonLab/Timeseries-PILE."
av555/aardvark-weather,,,,,,,https://huggingface.co/datasets/av555/aardvark-weather,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-nd-4.0/,,,,,,,,,,,,"Aardvark Weather This repo will contain the dataset used in the Aardvark Weather model. This will be made available on the completion of peer review, and will not be released prior to this time. If you would like to be notified when the dataset becomes available please email av555@cam.ac.uk."
aviaefrat/cryptonite,Text,,,,,https://github.com/aviaefrat/cryptonite,https://huggingface.co/datasets/aviaefrat/cryptonite,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language Current NLP datasets targeting ambiguity can be solved by a native speaker with relative ease. We present Cryptonite, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each"
awalesushil/DBLP-QuAD,Text,,,,,,https://huggingface.co/datasets/awalesushil/DBLP-QuAD,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"DBLP-QuAD is a scholarly knowledge graph question answering dataset with 10,000 question - SPARQL query pairs targeting the DBLP knowledge graph. The dataset is split into 7,000 training, 1,000 validation and 2,000 test questions."
axiong/pmc_oa,,,,,,,https://huggingface.co/datasets/axiong/pmc_oa,,,,,,,,,,,,,,,,,,,,"Foundation models trained on large-scale dataset gain a recent surge in CV and NLP. In contrast, development in biomedical domain lags far behind due to data scarcity. To address this issue, we build and release PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, which is 8 times larger than before. PMC-OA covers diverse modalities or diseases, with majority of the image-caption samples aligned at finer-grained level, i.e., subfigure and subcaption. While pretraining a CLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art results on various downstream tasks, including image-text retrieval on ROCO, MedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text retrieval, +3.9% accuracy on image classification."
ayehninnkhine/myanmar_news,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/ayehninnkhine/myanmar_news,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Myanmar_News Data-Summary: The Myanmar news dataset contains article snippets in four categories: Business, Entertainment, Politics, and Sport. These were collected in October 2017 by Aye Hninn Khine Languages Myanmar/Burmese language Dataset Structure Data Fields text - text from article category - a topic: Business, Entertainment, Politic, or Sport (note spellings) Data Splits Oneâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ayehninnkhine/myanmar_news."
AymanTarig/function-calling-v0.2-with-r1-cot,,,,,,,https://huggingface.co/datasets/AymanTarig/function-calling-v0.2-with-r1-cot,,,,,,,,,,,,,,,,,,,,"This dataset is a modified version of Salesforce/xlam-function-calling-60k, incorporating reasoning chains generated by deepseek-ai/DeepSeek-R1-Distill-Llama-8B."
AyoubChLin/northwind_invocies,,,,,,,https://huggingface.co/datasets/AyoubChLin/northwind_invocies,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Northwind Invoices and Related Documents This dataset contains a collection of invoices and related documents from the Northwind database, a sample database used by Microsoft for demonstrating database functionalities. The invoices include information about the customer, the salesperson, the order date, order ID, product IDs, product names, quantities, unit prices, and total prices. The related documents include shipping documents and stock documents. This dataset was created byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AyoubChLin/northwind_invocies."
AYUSHKHAIRE/real-time-stocks-data,,,,,,,https://huggingface.co/datasets/AYUSHKHAIRE/real-time-stocks-data,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,AYUSHKHAIRE/real-time-stocks-data dataset hosted on Hugging Face and contributed by the HF Datasets community
azarijafari/FarsTail,,,,,,https://arxiv.org/abs/2009.08820,https://huggingface.co/datasets/azarijafari/FarsTail,,,,,,,,,,,,,,,,,,,,"FarsTail: a Persian natural language inference dataset Natural Language Inference (NLI), also called Textual Entailment, is an important task in NLP with the goal of determining the inference relationship between a premise p and a hypothesis h. It is a three-class problem where each pair (p, h) is assigned to one of these classes: ""ENTAILMENT"" if the hypothesis can be inferred from the premise, ""CONTRADICTION"" if the hypothesis contradicts the premise, and ""NEUTRAL"" if none of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/azarijafari/FarsTail."
Azure99/blossom-math-v2,,,,,,,https://huggingface.co/datasets/Azure99/blossom-math-v2,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,BLOSSOM MATH V2 ä»‹ç» Blossom Math V3ç‰ˆæœ¬å·²å‘å¸ƒï¼ðŸ¤— Blossom Math V2æ˜¯åŸºäºŽMath23Kå’ŒGSM8Kè¡ç”Ÿè€Œæ¥çš„ä¸­è‹±åŒè¯­æ•°å­¦å¯¹è¯æ•°æ®é›†ï¼Œé€‚ç”¨äºŽæ•°å­¦é—®é¢˜å¾®è°ƒã€‚ ç›¸æ¯”äºŽblossom-math-v1ï¼Œæ–°å¢žäº†2500æ¡GSM8Kæ•°æ®å’Œç¿»è¯‘ä¸ºä¸­æ–‡çš„2500æ¡GSM8K-CNæ•°æ®ã€‚æ­¤å¤–ï¼Œä¼˜åŒ–äº†ç­”æ¡ˆçš„æ£€æŸ¥é€»è¾‘ï¼Œè¿˜ç§»é™¤äº†<<1+1=2>>ç­‰è®¡ç®—æ­¥éª¤ï¼Œä»¥ç»Ÿä¸€æŽ¨ç†æ­¥éª¤çš„é£Žæ ¼ã€‚ æœ¬æ•°æ®é›†é‡‡ç”¨å…¨é‡Math23Kã€GSM8Kå’Œç¿»è¯‘åŽçš„GSM8Kçš„é—®é¢˜ï¼ŒéšåŽè°ƒç”¨gpt-3.5-turbo-0613ç”Ÿæˆç»“æžœï¼Œå¹¶ä½¿ç”¨åŽŸå§‹æ•°æ®é›†ä¸­çš„ç­”æ¡ˆå¯¹ç”Ÿæˆçš„ç»“æžœè¿›è¡ŒéªŒè¯ï¼Œè¿‡æ»¤æŽ‰é”™è¯¯ç­”æ¡ˆï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šä¿è¯äº†é—®é¢˜å’Œç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚ æœ¬æ¬¡å‘å¸ƒäº†å…¨é‡æ•°æ®çš„25%ï¼ŒåŒ…å«10Kè®°å½•ã€‚ è¯­è¨€ ä¸­æ–‡å’Œè‹±æ–‡ æ•°æ®é›†ç»“æž„ æ¯æ¡æ•°æ®ä»£è¡¨ä¸€ä¸ªå®Œæ•´çš„é¢˜ç›®åŠç­”æ¡ˆï¼ŒåŒ…å«idã€inputã€outputã€answerã€datasetå››ä¸ªå­—æ®µã€‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Azure99/blossom-math-v2.
b-mc2/sql-create-context,,,,,,,https://huggingface.co/datasets/b-mc2/sql-create-context,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Overview This dataset builds from WikiSQL and Spider. There are 78,577"
b3x0m/Chinese-H-Novels,,,,,,,https://huggingface.co/datasets/b3x0m/Chinese-H-Novels,,,,,,,,,,,,,,,,,,,,"Update 12/07/2024: convert to parquet to download easier. Chinese 18+ novels corpus, use at your own risk, you and only you are responsible for every choice you make. (Í¡ Â° ÍœÊ– Í¡ Â°) tags: socks, garter belt, foot fetish, ntr, netori..... Thanks Moleys/Numeron for the dataset donation."
Ba1yya/RVC_rmvpe,,,,,,,https://huggingface.co/datasets/Ba1yya/RVC_rmvpe,,,,,,,,,,,,,,,,,,,,Ba1yya/RVC_rmvpe dataset hosted on Hugging Face and contributed by the HF Datasets community
BAAI/ChildMandarin,,,,,,https://arxiv.org/abs/2409.18584,https://huggingface.co/datasets/BAAI/ChildMandarin,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"ChildMandarin: A Comprehensive Mandarin Speech Dataset for Young Children Aged 3-5 Introduction ChildMandarin is a comprehensive, open-source Mandarin Chinese speech dataset specifically designed for research on young children aged 3 to 5. This dataset addresses the critical lack of publicly available resources for this age group, enabling advancements in automatic speech recognition (ASR), speaker verification (SV), and other related fields. The dataset is releasedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BAAI/ChildMandarin."
Babak-Behkamkia/Personality_Detection,,,,,,,https://huggingface.co/datasets/Babak-Behkamkia/Personality_Detection,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Babak-Behkamkia/Personality_Detection dataset hosted on Hugging Face and contributed by the HF Datasets community
Babelscape/wikineural,,,,,,https://arxiv.org/abs/1810.04805,https://huggingface.co/datasets/Babelscape/wikineural,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"Dataset Card for WikiNEuRal dataset Description Summary: In a nutshell, WikiNEuRal consists in a novel technique which builds upon a multilingual lexical knowledge base (i.e., BabelNet) and transformer-based architectures (i.e., BERT) to produce high-quality annotations for multilingual NER. It shows consistent improvements of up to 6 span-based F1-score points against state-of-the-art alternative data production methods on common benchmarks for NER. We used thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Babelscape/wikineural."
baber/agieval,Text,General,General,Text,"Accuracy, F1 Score",https://github.com/microsoft/AGIEval/blob/main/README.md,https://huggingface.co/datasets/baber/agieval,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for AGIEval Data-Summary: AGIEval is a human-centric benchmark specifically designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving. This benchmark is derived from 20 official, public, and high-standard admission and qualification exams intended for general human test-takers, such as general college admission tests (e.g., Chinese College Entrance Exam (Gaokao) and American SAT), lawâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/baber/agieval."
balacoon/en_us_abbreviations,,,,,,,https://huggingface.co/datasets/balacoon/en_us_abbreviations,,,,,,,,,,,,,,,,,,,,en-US abbrevations This is a dataset of abbreviations. Contains
bandad/sayoko-tts-corpus,,,,,,,https://huggingface.co/datasets/bandad/sayoko-tts-corpus,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"ã‚µãƒ¨å­ éŸ³å£°ã‚³ãƒ¼ãƒ‘ã‚¹ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ–¹æ³• ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åœ§ç¸®ã—ãŸzipãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã€gdriveã«ç½®ã„ã¦ã„ã¾ã™ã€‚ ã¾ãŸã€ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã€huggingface hubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚‚å¯èƒ½ã§ã™ã€‚ # pip install --upgrade huggingface_hub from huggingface_hub import snapshot_download snapshot_download(repo_id=""bandad/sayoko-tts-corpus"", repo_type=""dataset"", revision=""main"", local_dir=""./sayoko-tts-corpus"") æ¦‚è¦ 81æ­³ã®å¥³æ€§ã®éŸ³å£°ã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ã€‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bandad/sayoko-tts-corpus."
batterydata/paper-abstracts,,,,,,,https://huggingface.co/datasets/batterydata/paper-abstracts,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Battery Abstracts Dataset This dataset includes 29,472 battery papers and 17,191 non-battery papers, a total of 46,663 papers. These papers are manually labelled in terms of the journals to which they belong. 14 battery journals and 1,044 non battery journals were selected to form this database. training_data.csv: Battery papers: 20,629, Non-battery papers: 12,034. Total: 32,663. val_data.csv: Battery papers: 5,895, Non-battery papers: 3,438. Total: 9,333. test_data.csv:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/batterydata/paper-abstracts."
bavest/fin-llama-dataset,,,,,,,https://huggingface.co/datasets/bavest/fin-llama-dataset,,,,,,,,https://choosealicense.com/licenses/bigscience-openrail-m/,,,,,,,,,,,,bavest/fin-llama-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
bbz662bbz/databricks-dolly-15k-ja-gozaru,,,,,,,https://huggingface.co/datasets/bbz662bbz/databricks-dolly-15k-ja-gozaru,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"This dataset was using ""kunishou/databricks-dolly-15k-ja"" This dataset is licensed under CC BY SA 3.0 Last Update : 2023-05-28 databricks-dolly-15k-ja-gozaru kunishou/databricks-dolly-15k-ja https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja"
bdotloh/empathetic-dialogues-contexts,,,,,,,https://huggingface.co/datasets/bdotloh/empathetic-dialogues-contexts,,,,,,,,,,,,,,,,,,,,"Dataset Description This is a dataset of emotional contexts that was retrieved from the original EmpatheticDialogues (ED) dataset. Respondents were asked to describe an event that was associated with a particular emotion label (i.e. p(event|emotion). There are 32 emotion labels in total. There are 19209, 2756, and 2542 instances of emotional descriptions in the train, valid, and test set, respectively."
beardaintweird/quran-embeddings,,,,,,,https://huggingface.co/datasets/beardaintweird/quran-embeddings,,,,,,,,,,,,,,,,,,,,beardaintweird/quran-embeddings dataset hosted on Hugging Face and contributed by the HF Datasets community
BeIR/dbpedia-entity-generated-queries,Text,General,UI,Text,"Accuracy, F1 Score",https://github.com/UKPLab/beir,https://huggingface.co/datasets/BeIR/dbpedia-entity-generated-queries,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,https://github.com/UKPLab/beir/blob/main/examples/dataset#2-bioasq,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for BEIR Benchmark Data-Summary: BEIR is a heterogeneous benchmark that has been built from 18 diverse datasets representing 9 information retrieval tasks: Fact-checking: FEVER, Climate-FEVER, SciFact Question-Answering: NQ, HotpotQA, FiQA-2018 Bio-Medical IR: TREC-COVID, BioASQ, NFCorpus News Retrieval: TREC-NEWS, Robust04 Argument Retrieval: Touche-2020, ArguAna Duplicate Question Retrieval: Quora, CqaDupstack Citation-Prediction: SCIDOCS Tweetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BeIR/dbpedia-entity-generated-queries."
BelleGroup/multiturn_chat_0.8M,,,,,,,https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,"Multiturn Chat 0.8M å†…å®¹ åŒ…å«çº¦80ä¸‡æ¡ç”±BELLEé¡¹ç›®ç”Ÿæˆçš„ç”¨æˆ·ä¸ŽåŠ©æ‰‹çš„å¤šè½®å¯¹è¯ã€‚ æ³¨æ„ï¼šæ­¤æ•°æ®é›†æ˜¯ç”±ChatGPTäº§ç”Ÿçš„ï¼Œæœªç»è¿‡ä¸¥æ ¼æ ¡éªŒï¼Œå†…å®¹å¯èƒ½åŒ…å«é”™è¯¯ã€‚ä½¿ç”¨è¿‡ç¨‹ä¸­è¯·æ³¨æ„è¿™ä¸€ç‚¹ã€‚ instructionä¸­åŒ…å«å¤šè½®å¯¹è¯çš„ä¸Šæ–‡å†…å®¹ï¼Œä»¥Human:å’ŒAssistant:åŒºåˆ†ï¼Œoutputä¸­åŒ…å«å½“å‰åŠ©æ‰‹è§’è‰²çš„å›žç­”ã€‚ æ ·ä¾‹ { ""instruction"":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M."
BennoKrojer/ImageCoDe,Image-Text,General,UI,Text,"Accuracy, F1 Score",https://mcgill-nlp.github.io/imagecode/,https://huggingface.co/datasets/BennoKrojer/ImageCoDe,"benchmark that requires contextual language understanding in the form of pragmatics, temporality, long descriptions and visual nuances",,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,https://github.com/McGill-NLP/imagecode,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ImageCoDe To get started quickly, load descriptions via: from datasets import load_dataset"
beomi/KoAlpaca-v1.1a,,,,,,,https://huggingface.co/datasets/beomi/KoAlpaca-v1.1a,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""KoAlpaca-v1.1a"" Project Repo Github Repo: Beomi/KoAlpaca How to use >>> from datasets import load_dataset >>> ds = load_dataset(""beomi/KoAlpaca-v1.1a"", split=""train"") >>> ds Dataset({ features: ['instruction', 'input', 'output'], num_rows: 21155 }) >>> ds[0] {'instruction': 'ì–‘íŒŒëŠ” ì–´ë–¤ ì‹ë¬¼ ë¶€ìœ„ì¸ê°€ìš”? ê·¸ë¦¬ê³  ê³ êµ¬ë§ˆëŠ” ë¿Œë¦¬ì¸ê°€ìš”?', 'output': 'ì–‘íŒŒëŠ” ìžŽì´ ì•„ë‹Œ ì‹ë¬¼ì˜ ì¤„ê¸° ë¶€ë¶„ìž…ë‹ˆë‹¤. ê³ êµ¬ë§ˆëŠ” ì‹ë¬¼ì˜ ë¿Œë¦¬ ë¶€ë¶„ìž…ë‹ˆë‹¤. \n\nì‹ë¬¼ì˜ ë¶€ìœ„ì˜ êµ¬ë¶„ì— ëŒ€í•´ ê¶ê¸ˆí•´í•˜ëŠ” ë¶„ì´ë¼ë©´ ë¶„ëª… ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì°¾ê³  ìžˆì„ ê²ƒìž…ë‹ˆë‹¤.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/beomi/KoAlpaca-v1.1a."
bertin-project/mc4-sampling,Text,,,,,https://huggingface.co/bertin-project/bertin-roberta-base-spanish,https://huggingface.co/datasets/bertin-project/mc4-sampling,,,,,,,,https://choosealicense.com/licenses/odc-by/,,,,,,,,,,,,"A sampling-enabled version of mC4, the colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: ""https://commoncrawl.org"". This is a version of the processed version of Google's mC4 dataset by AllenAI, in which sampling methods are implemented to perform on the fly."
bespokelabs/Bespoke-Stratos-17k,,,,,,,https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,https://github.com/bespokelabsai/curator/tree/main/examples/bespoke-stratos-data-generation,,,,,,,,"Bespoke-Stratos-17k We replicated and improved the Berkeley Sky-T1 data pipeline using SFT distillation data from DeepSeek-R1 to create Bespoke-Stratos-17k -- a reasoning dataset of questions, reasoning traces, and answers. This data was used to train: Bespoke-Stratos-32B, a 32B reasoning model which is a fine-tune of Qwen-2.5-32B-Instruct Bespoke-Stratos-7B, a 7B reasoning model which is a fine-tune of Qwen-2.5-7B-Instruct. Metrics for Bespoke-Stratos-32Bâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k."
beyond/rlhf-reward-single-round-trans_chinese,,,,,,,https://huggingface.co/datasets/beyond/rlhf-reward-single-round-trans_chinese,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""rlhf-reward-single-round-trans_chinese"" More Information needed"
bghira/pseudo-camera-10k,,,,,,,https://huggingface.co/datasets/bghira/pseudo-camera-10k,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,"pseudo-camera-10k dataset Contents This dataset contains 10k free images from world class photographers. The images have been resized using Lanczos antialiasing, with their smaller edge shifted to 1024px. The aim of this dataset is a highly variable but high quality and high resolution set of images containing difficult concepts, with about half of the images being numbered group shots and family portraits with the number of subjects labeled. No images wereâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bghira/pseudo-camera-10k."
BGLab/mpf-bench,,,,,,,https://huggingface.co/datasets/BGLab/mpf-bench,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Overview Scientific machine learning (SciML) is a promising strategy for designing multiphase flow solvers, but it requires a large dataset. This paper presents a comprehensive dataset created from 11,000 simulations, both in 2D and 3D, using the Lattice Boltzmann method~(LBM). The dataset captures intricate physics by varying factors such as surface tension, density, and viscosity of fluids. These simulations, comprising 1 million time snapshots, provide extensive data on two-fluid behavior.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BGLab/mpf-bench."
bhadresh-savani/photo-to-cartoon,,,,,,,https://huggingface.co/datasets/bhadresh-savani/photo-to-cartoon,,,,,,,,,,,,,,,,,,,,bhadresh-savani/photo-to-cartoon dataset hosted on Hugging Face and contributed by the HF Datasets community
bhavyagiri/semantic-memes,,,,,,,https://huggingface.co/datasets/bhavyagiri/semantic-memes,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,bhavyagiri/semantic-memes dataset hosted on Hugging Face and contributed by the HF Datasets community
BhavyaMuni/artist-lyrics,,,,,,,https://huggingface.co/datasets/BhavyaMuni/artist-lyrics,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""artist-lyrics"" More Information needed"
BI55/MedText,,,,,,,https://huggingface.co/datasets/BI55/MedText,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"This is the shuffled version of medtext_1, so the datapoints are in random order and not sorted by category. This is to prevent catastrophic forgetting by category. This is a medical diagnosis dataset containing over 1000 top notch textbook quality patient presentations and diagnosis/treatments. The 100 most common diseases and the 30 most common injuries people go to the hospital with, are, among others, fully captured in the dataset, with multiple datapoints for each ranging from mild toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BI55/MedText."
bias-amplified-splits/anli,Text,,,,,https://arxiv.org/abs/2305.18917,https://huggingface.co/datasets/bias-amplified-splits/anli,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"The Adversarial Natural Language Inference (ANLI) is a new large-scale NLI benchmark dataset, The dataset is collected via an iterative, adversarial human-and-model-in-the-loop procedure. ANLI is much more difficult than its predecessors including SNLI and MNLI. It contains three rounds. Each round has train/dev/test splits."
BigBang/galaxyzoo-decals,,,,,,,https://huggingface.co/datasets/BigBang/galaxyzoo-decals,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Galaxy Zoo DECaLS: Detailed Visual Morphology Measurements from Volunteers and Deep Learning for 314,000 Galaxies https://github.com/mwalmsley/zoobot https://zenodo.org/record/4573248 Dataset Schema This schema describes the columns in the GZ DECaLS catalogues; gz_decals_auto_posteriors, gz_decals_volunteers_1_and_2, and gz_decals_volunteers_5. In all catalogues, galaxies are identified by their iauname. Galaxies are unique within a catalogue.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BigBang/galaxyzoo-decals."
bigbio/gad,,,,,,https://geneticassociationdb.nih.gov/,https://huggingface.co/datasets/bigbio/gad,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,A corpus identifying associations between genes and diseases by a semi-automatic annotation procedure based on the Genetic Association Database
bigcode/starcoderdata,,,,,,,https://huggingface.co/datasets/bigcode/starcoderdata,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"StarCoder Training Dataset Dataset description This is the dataset used for training StarCoder and StarCoderBase. It contains 783GB of code in 86 programming languages, and includes 54GB GitHub Issues + 13GB Jupyter notebooks in scripts and text-code pairs, and 32GB of GitHub commits, which is approximately 250 Billion tokens. Dataset creation The creation and filtering of The Stack is explained in the original dataset, we additionally decontaminateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigcode/starcoderdata."
biglab/webui-all,,,,,,,https://huggingface.co/datasets/biglab/webui-all,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"This data accompanies the WebUI project (https://dl.acm.org/doi/abs/10.1145/3544548.3581158) For more information, check out the project website: https://uimodeling.github.io/ To download this dataset, you need to install the huggingface-hub package pip install huggingface-hub Use snapshot_download from huggingface_hub import snapshot_download snapshot_download(repo_id=""biglab/webui-all"", repo_type=""dataset"") IMPORTANT Before downloading and using, please review the copyright info here:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/biglab/webui-all."
biglam/hmd_newspapers,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/biglam/hmd_newspapers,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,"BERT, RoBERTa, T5",at the British Library,,"Dataset Card for Heritage Made Digital Newspapers Data-Summary: This dataset contains text extracted at the article level from historic digitised newspapers from the Heritage Made Digital newspaper digitisation program at the British Library. The newspapers in the dataset were published between 1800 and 1896. This dataset contains ~2.5 billion tokens and 3,065,408 articles. The dataset contains text generated from Optical Character Recognition software onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/biglam/hmd_newspapers."
bigscience-data/roots_es_uncorpus,,,,,,,https://huggingface.co/datasets/bigscience-data/roots_es_uncorpus,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,ROOTS Subset: roots_es_uncorpus uncorpus Dataset uid: uncorpus Description Homepage Licensing Speaker Locations Sizes 2.8023 % of total 10.7390 % of ar 5.7970 % of fr 9.7477 % of es 2.0417 % of en 1.2540 % of zh BigScience processing steps Filters applied to: ar dedup_document filter_remove_empty_docs filter_small_docs_bytes_300 Filters applied to: frâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigscience-data/roots_es_uncorpus.
bigscience/P3,Text,General,Scientific,Text,"Accuracy, F1 Score",https://bigscience.huggingface.co/promptsource,https://huggingface.co/datasets/bigscience/P3,for the input and target sequences,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for P3 Data-Summary: P3 (Public Pool of Prompts) is a collection of prompted English datasets covering a diverse set of NLP tasks. A prompt is the combination of an input template and a target template. The templates are functions mapping a data
BillGPT/Chinese-medical-dialogue-data,,,,,,,https://huggingface.co/datasets/BillGPT/Chinese-medical-dialogue-data,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,BillGPT/Chinese-medical-dialogue-data dataset hosted on Hugging Face and contributed by the HF Datasets community
Binaryy/travel_sample_extended,,,,,,,https://huggingface.co/datasets/Binaryy/travel_sample_extended,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""travel_sample_extended"" More Information needed"
Bingsu/Cat_and_Dog,Text,General,General,Text,"Accuracy, F1 Score",https://www.kaggle.com/datasets/tongpython/cat-and-dog,https://huggingface.co/datasets/Bingsu/Cat_and_Dog,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Data-Summary: A dataset from kaggle with duplicate data removed. Data Fields The data instances have the following fields: image: A PIL.Image.Image object containing the image. Note that when accessing the image column: dataset[0][""image""] the image file is automatically decoded. Decoding of a large number of image files might take a significant amount of time. Thus it is important to first query the sample index before the ""image"" column, i.e.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Bingsu/Cat_and_Dog."
Birchlabs/openai-prm800k-stepwise-critic,,,,,,,https://huggingface.co/datasets/Birchlabs/openai-prm800k-stepwise-critic,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Birchlabs/openai-prm800k-stepwise-critic dataset hosted on Hugging Face and contributed by the HF Datasets community
bitext/Bitext-customer-support-llm-chatbot-training-dataset,,,,,,,https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset,,,,,,,,https://choosealicense.com/licenses/cdla-sharing-1.0/,,,,,,,,,,,,"Bitext - Customer Service Tagged Training Dataset for LLM-based Virtual Assistants Overview This hybrid synthetic dataset is designed to be used to fine-tune Large Language Models such as GPT, Mistral and OpenELM, and has been generated using our NLP/NLG technology and our automated Data Labeling (DAL) tools. The goal is to demonstrate how Verticalization/Domain Adaptation for the Customer Support sector can be easily achieved using our two-step approach to LLMâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset."
bjoernp/tagesschau-2018-2023,,,,,,,https://huggingface.co/datasets/bjoernp/tagesschau-2018-2023,,,,,,,,,,,,,,,,,,,,Tagesschau Archive Article Dataset A scrape of Tagesschau.de articles from 01.01.2018 to 26.04.2023. Find all source code in github.com/bjoernpl/tagesschau. Dataset Information CSV structure: Field Description date Date of the article headline Title of the article short_headline A short headline / Context short_text A brief summary of the article article The full text of the article href The href of the article on tagesschau.de Size: Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjoernp/tagesschau-2018-2023.
BlackKakapo/RoSTSC,,,,,,,https://huggingface.co/datasets/BlackKakapo/RoSTSC,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,RO-STS-Corpus Overview RoSTSC is a Romanian Semantic Textual Similarity (STS) dataset designed for evaluating and training sentence embedding models. It contains pairs of Romanian sentences along with similarity scores that indicate the degree of semantic equivalence between them. Dataset Structure sentence1: The first sentence in the pair. sentence2: The second sentence in the pair. score: A numerical value representing the semantic similarity between the twoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BlackKakapo/RoSTSC.
Blackroot/Tiny-Open-Domain-Books,,,,,,,https://huggingface.co/datasets/Blackroot/Tiny-Open-Domain-Books,,,,,,,,https://choosealicense.com/licenses/pddl/,,,,,,,,,,,,A tiny
BNNT/mozi_general_instructions_3m,,,,,,,https://huggingface.co/datasets/BNNT/mozi_general_instructions_3m,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Sources are listed below: Chinese General Instruction 2000k BELLE https://huggingface.co/datasets/BelleGroup/train_2M_CN English generic instruction 52k alpaca-gpt4 https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM Chinese generic dialog instructions 800k BELLE https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M English Universal Dialog Instruction 94k sharegpt_vicuna https://huggingface.co/datasets/jeffwan/sharegpt_vicuna Chinese-English-Japanese Universal Command 49kâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BNNT/mozi_general_instructions_3m.
BoDai/HorizonGS,,,,,,,https://huggingface.co/datasets/BoDai/HorizonGS,,,,,,,,,,,,,,,,,,,,BoDai/HorizonGS dataset hosted on Hugging Face and contributed by the HF Datasets community
bond005/sova_rudevices,Audio,General,General,Text,"Accuracy, F1 Score",https://github.com/sovaai/sova-dataset,https://huggingface.co/datasets/bond005/sova_rudevices,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,"of 16kHz Russian live speech with manual annotating, prepared by SOVA","Dataset Card for sova_rudevices Data-Summary: SOVA Dataset is free public STT/ASR dataset. It consists of several parts, one of them is SOVA RuDevices. This part is an acoustic corpus of approximately 100 hours of 16kHz Russian live speech with manual annotating, prepared by SOVA.ai team. Authors do not divide the dataset into train, validation and test subsets. Therefore, I was compelled to prepare this splitting. The training subset includes more than 82 hoursâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bond005/sova_rudevices."
bookbot/ljspeech_phonemes,,,,,,,https://huggingface.co/datasets/bookbot/ljspeech_phonemes,,,,,,,,,,,5,,,,,,,,,"Dataset Card for ""ljspeech_phonemes"" More Information needed"
bookcorpus/bookcorpus,Text,,,,,https://yknzhu.wixsite.com/mbweb,https://huggingface.co/datasets/bookcorpus/bookcorpus,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story.This work aims to align books to their movie releases in order to providerich descriptive explanations for visual content that go semantically farbeyond the captions available in current datasets. \"
botisan-ai/cantonese-mandarin-translations,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/botisan-ai/cantonese-mandarin-translations,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for cantonese-mandarin-translations Data-Summary: This is a machine-translated parallel corpus between Cantonese (a Chinese dialect that is mainly spoken by Guangdong (province of China), Hong Kong, Macau and part of Malaysia) and Chinese (written form, in Simplified Chinese). Supported Tasks and Leaderboards N/A Languages Cantonese (yue) Simplified Chinese (zh-CN) Dataset Structure JSON lines with yueâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/botisan-ai/cantonese-mandarin-translations."
botp/yentinglin-zh_TW_c4,,,,,,https://arxiv.org/abs/2305.13711,https://huggingface.co/datasets/botp/yentinglin-zh_TW_c4,,,,,,,,https://choosealicense.com/licenses/odc-by/,,,,,,,,,,,,Language Models for Taiwanese Culture âœï¸ Online Demo â€¢ ðŸ¤— HF Repo â€¢ ðŸ¦ Twitter â€¢ ðŸ“ƒ [Paper Coming Soon] â€¢ ðŸ‘¨ï¸ Yen-Ting Lin Overview Taiwan-LLaMa is a full parameter fine-tuned model based on LLaMa 2 for Traditional Mandarin applications. Taiwan-LLaMa v1.0 pretrained on over 5 billion tokens and instruction-tuned on over 490k conversations both in traditional mandarin. Demo A live demonstration ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/botp/yentinglin-zh_TW_c4.
BounharAbdelaziz/Atlaset-SFT,,,,,,,https://huggingface.co/datasets/BounharAbdelaziz/Atlaset-SFT,,,,,,,,,,,,,,,,,,,,BounharAbdelaziz/Atlaset-SFT dataset hosted on Hugging Face and contributed by the HF Datasets community
bprateek/amazon_product_description,,,,,,,https://huggingface.co/datasets/bprateek/amazon_product_description,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,bprateek/amazon_product_description dataset hosted on Hugging Face and contributed by the HF Datasets community
brackozi/Resume,,,,,,,https://huggingface.co/datasets/brackozi/Resume,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,brackozi/Resume dataset hosted on Hugging Face and contributed by the HF Datasets community
brainchalov/pubmed_arxiv_abstracts_data,,,,,,,https://huggingface.co/datasets/brainchalov/pubmed_arxiv_abstracts_data,,,,,,,,,,50,,,,,,,,,,annotations_creators: machine-generated language: en license: apache-2.0 multilinguality: monolingual task_categories: classification generation pretty_name: PubMed_ArXiv_Abstracts features: name: abstr dtype: string name: title dtype: string name: journal dtype: string name: field dtype: string name: label_journal dtype: int64 name: label_field dtype: int64
Brand24/mms,,,,,,https://arxiv.org/abs/2306.07902,https://huggingface.co/datasets/Brand24/mms,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"This work presents the most extensive open massively multi-lingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected from over 350 datasets reported in the scientific literature based on strict quality criteria and covers 25 languages. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies."
breadlicker45/discord_data,,,,,,,https://huggingface.co/datasets/breadlicker45/discord_data,,,,,,,,,,,2335,,,,,,,,,breadlicker45/discord_data dataset hosted on Hugging Face and contributed by the HF Datasets community
Brosnan/WIFI_RSSI_Indoor_Positioning_Dataset,,,,,,,https://huggingface.co/datasets/Brosnan/WIFI_RSSI_Indoor_Positioning_Dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,"X Position (m), Y Position (m), RSSI Feature 1 (dBm), RSSI Feature 2 (dBm), RSSI Feature 3 (dBm), RSSI Feature 4 (dBm),",,,,,,"WIFI RSSI Indoor Positioning Dataset A reliable and comprehensive public WiFi fingerprinting database for researchers to implement and compare the indoor localizationâ€™s methods.The database contains RSSI information from 6 APs conducted in different days with the support of autonomous robot. We use an autonomous robot to collect the WiFi fingerprint data. Our 3-wheel robot has multiple sensors including wheel odometer, an inertial measurement unit (IMU), a LIDAR, sonar sensorsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Brosnan/WIFI_RSSI_Indoor_Positioning_Dataset."
brunokreiner/genius-lyrics,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/brunokreiner/genius-lyrics,classifier) lyrics with some more meta data,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Dataset Name Dataset Description Data-Summary: This dataset consists of roughly 480k english (classified using nltk language classifier) lyrics with some more meta data. The id corresponds to the spotify id. The meta data was taken from the million playlist challenge @ AICrowd. The lyrics were crawled using ""[song name] [artist name]"" as string using the lyricsgenius python package which uses the genius.com search function. Thereâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/brunokreiner/genius-lyrics."
bryanbocao/coco_minitrain,,,,,,,https://huggingface.co/datasets/bryanbocao/coco_minitrain,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,bryanbocao/coco_minitrain dataset hosted on Hugging Face and contributed by the HF Datasets community
bsmock/pubtables-1m,,,,,,,https://huggingface.co/datasets/bsmock/pubtables-1m,,,,,,,,https://choosealicense.com/licenses/cdla-permissive-2.0/,,,,,,,,,,,,"PubTables-1M GitHub: https://github.com/microsoft/table-transformer Paper: ""PubTables-1M: Towards comprehensive table extraction from unstructured documents"" Hugging Face: Detection model Structure recognition model Currently we only support downloading the dataset as tar.gz files. Integrating with HuggingFace Datasets is something we hope to support in the future! Please switch to the ""Files and versions"" tab to download all of the files or use a command such as wget toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bsmock/pubtables-1m."
bstee615/diversevul,,,,,,,https://huggingface.co/datasets/bstee615/diversevul,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""diversevul"" Unofficial, not affiliated with the authors. Paper: https://surrealyz.github.io/files/pubs/raid23-diversevul.pdf Repository: https://github.com/wagner-group/diversevul"
BTX24/tekno21-brain-stroke-dataset-binary,,,,,,,https://huggingface.co/datasets/BTX24/tekno21-brain-stroke-dataset-binary,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset Card for BTX24/tekno21-brain-stroke-dataset-binary Dataset Description ðŸ“Œ EN:The TEKNO21 Brain Stroke Dataset consists of 7,369 anonymized brain CT scans in DICOM and PNG formats, labeled by seven expert radiologists. It includes acute/hyperacute ischemic and hemorrhagic stroke cases, as well as non-stroke images. Each annotation was verified for accuracy. The dataset was curated from the Turkish Ministry of Healthâ€™s e-Pulse and Teleradiology System (2019â€“2020) asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BTX24/tekno21-brain-stroke-dataset-binary."
buddhist-nlp/daizhige,,,,,,,https://huggingface.co/datasets/buddhist-nlp/daizhige,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""daizhige"" More Information needed"
bugdaryan/sql-create-context-instruction,,,,,,,https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Overview This dataset is built upon SQL Create Context, which in turn was constructed using data from WikiSQL and Spider. There are 78,577"
BumblingOrange/Hanks_Embeddings,,,,,,,https://huggingface.co/datasets/BumblingOrange/Hanks_Embeddings,,,,,,,,https://choosealicense.com/licenses/bigscience-bloom-rail-1.0/,,,,,,,,,,,,"This is a collection of embeddings that I decided to make public. Additionally, it will be where I host any future embeddings I decide to train."
bupt/LawDataset-BUPT,,,,,,,https://huggingface.co/datasets/bupt/LawDataset-BUPT,,,,,,,,,,,,,,,,,,,,"LawDataset-BUPT âš–ï¸ Here is the full data from the Legal LLM project, from which we hope to build a high quality dataset. Here's our github project page. If you want to make any contribution, please contact me QQ 2248157602. Data Source Our data mainly comes from CrimeKgAssistant, 856 crime KG items / 2800k crime name_entities / 200k lawQA with 13 classes Tigerbot-law-plugin 55k laws provision data with 11 classes Wenshu_ms_dataset 45k law judgements data Lexilawâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bupt/LawDataset-BUPT."
burberg92/resume_summary,,,,,,,https://huggingface.co/datasets/burberg92/resume_summary,,,,,,,,https://choosealicense.com/licenses/unlicense/,,,,,,,,,,,,burberg92/resume_summary dataset hosted on Hugging Face and contributed by the HF Datasets community
burgerbee/art_and_culture_wiki,,,,,,,https://huggingface.co/datasets/burgerbee/art_and_culture_wiki,,,,,,,,,,,,,,,,,,,,burgerbee/art_and_culture_wiki dataset hosted on Hugging Face and contributed by the HF Datasets community
burkimbia/audio-dataset-aggregated,,,,,,,https://huggingface.co/datasets/burkimbia/audio-dataset-aggregated,,,,,,,,,,,,,,,,,,,,Data Fields The dataset includes the following fields: audio: Audio file containing the spoken Bible passage (48kHz sampling rate) full_text: Complete transcription of the audio passage in Bambara duration: Length of the audio clip in seconds Usage Speech Recognition: Training ASR models for Moore Text-to-Speech: Development of TTS systems for Moore Speech Analysis: Studying prosody and phonetics of Moore Recherche linguistique : Analyse approfondie des structuresâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/burkimbia/audio-dataset-aggregated.
bvk1ng/hotpotqa_cot_decomposed,,,,,,,https://huggingface.co/datasets/bvk1ng/hotpotqa_cot_decomposed,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""hotpotqa_cot_decomposed"" More Information needed"
bydavid/biblecorpuscsv,,,,,,,https://huggingface.co/datasets/bydavid/biblecorpuscsv,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,bydavid/biblecorpuscsv dataset hosted on Hugging Face and contributed by the HF Datasets community
byoungsuk/datasets-github-issues,,,,,,,https://huggingface.co/datasets/byoungsuk/datasets-github-issues,,,,,,,,,,,3,,,,,,,,,byoungsuk/datasets-github-issues dataset hosted on Hugging Face and contributed by the HF Datasets community
C-MTEB/TNews-classification,,,,,,,https://huggingface.co/datasets/C-MTEB/TNews-classification,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""TNews-classification"" More Information needed"
c01dsnap/LLM-Sec-Evaluation,,,,,,,https://huggingface.co/datasets/c01dsnap/LLM-Sec-Evaluation,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"LLM Security Evaluation This repo contains scripts for evaluating LLM security abilities. We gathered hundreds of questions cover different ascepts of security, such as vulnerablities, pentest, threat intelligence, etc. All the questions can be viewed at https://huggingface.co/datasets/c01dsnap/LLM-Sec-Evaluation. Suppoted LLM ChatGLM Baichuan Vicuna (GGML format) Usage Because of different LLM requires for different running environment, we highlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/c01dsnap/LLM-Sec-Evaluation."
c3po-ai/edgar-corpus,Text,,,,,https://arxiv.org/abs/2109.14394,https://huggingface.co/datasets/c3po-ai/edgar-corpus,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,The dataset contains annual filings (10K) of all publicly traded firms from 1993-2020. The table data is stripped but all text is retained. This dataset allows easy access to the EDGAR-CORPUS dataset based on the paper EDGAR-CORPUS: Billions of Tokens Make The World Go Round (See References in README.md for details).
cadene/droid_1.0.1,,,,,,,https://huggingface.co/datasets/cadene/droid_1.0.1,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"This dataset was created using LeRobot. Dataset Structure meta/info.json: { ""codebase_version"": ""v2.1"", ""robot_type"": ""Franka"", ""total_episodes"": 95600, ""total_frames"": 27612581, ""total_tasks"": 0, ""total_videos"": 286800, ""total_chunks"": 95, ""chunks_size"": 1000, ""fps"": 15, ""splits"": { ""train"": ""0:95600"" }, ""data_path"": ""data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet"", ""video_path"":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cadene/droid_1.0.1."
Cainiao-AI/LaDe,,,,,,https://arxiv.org/abs/2306.10675,https://huggingface.co/datasets/Cainiao-AI/LaDe,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset Download: https://huggingface.co/datasets/Cainiao-AI/LaDe/tree/mainDataset Website: https://cainiaotechai.github.io/LaDe-website/Code Link:https://github.com/wenhaomin/LaDePaper Link: https://arxiv.org/abs/2306.10675 1. About Dataset LaDe is a publicly available last-mile delivery dataset with millions of packages from industry. It has three unique characteristics: (1) Large-scale. It involves 10,677k packages of 21k couriers over 6 months of real-world operation.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cainiao-AI/LaDe."
CAiRE/ASCEND,Audio,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/CAiRE/ASCEND,English,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,16739474757983,,,,,,"BERT, RoBERTa, T5",,of spontaneous speech with a total of ~12,"Dataset Card for ASCEND Data-Summary: ASCEND (A Spontaneous Chinese-English Dataset) introduces a high-quality resource of spontaneous multi-turn conversational dialogue Chinese-English code-switching corpus collected in Hong Kong. ASCEND consists of 10.62 hours of spontaneous speech with a total of ~12.3K utterances. The corpus is split into 3 sets: training, validation, and test with a ratio of 8:1:1 while maintaining a balanced gender proportion on each set.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/CAiRE/ASCEND."
cais/mmlu,Text,General,Scientific,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2009.03300,https://huggingface.co/datasets/cais/mmlu,"Understanding by Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt (ICLR 2021)",,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for MMLU Data-Summary: Measuring Massive Multitask Language Understanding by Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt (ICLR 2021). This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge. The test spans subjects in the humanities, social sciences, hard sciences, and other areas that are important for some people to learn. This covers 57â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cais/mmlu."
calum/the-stack-smol-python-docstrings,,,,,,,https://huggingface.co/datasets/calum/the-stack-smol-python-docstrings,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""the-stack-smol-filtered-python-docstrings"" More Information needed"
cam-cst/cbt,Text,General,UI,Text,"Accuracy, F1 Score",https://research.fb.com/downloads/babi/,https://huggingface.co/datasets/cam-cst/cbt,models can exploit wider linguistic context,,,,,,,https://choosealicense.com/licenses/gfdl/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for CBT Data-Summary: The Childrenâ€™s Book Test (CBT) is designed to measure directly how well language models can exploit wider linguistic context. The CBT is built from books that are freely available. This dataset contains four different configurations: V: where the answers to the questions are verbs. P: where the answers to the questions are pronouns. NE: where the answers to the questions are named entities. CN: where the answers to theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cam-cst/cbt.
cambridge-climb/BabyLM,,,,,,,https://huggingface.co/datasets/cambridge-climb/BabyLM,,,,,,,,,,,,,,,,,,,,Dataset for the shared baby language modeling task. The goal is to train a language model from scratch on this data which represents roughly the amount of text and speech data a young child observes.
camel-ai/chemistry,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2303.17760,https://huggingface.co/datasets/camel-ai/chemistry,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"CAMEL: Communicative Agents for â€œMindâ€ Exploration of Large Scale Language Model Society Github: https://github.com/lightaime/camel Website: https://www.camel-ai.org/ Arxiv Paper: https://arxiv.org/abs/2303.17760 Data-Summary: Chemistry dataset is composed of 20K problem-solution pairs obtained using gpt-4. The dataset problem-solutions pairs generating from 25 chemistry topics, 25 subtopics for each topic and 32 problems for each ""topic,subtopic"" pairs. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/camel-ai/chemistry."
carblacac/twitter-sentiment-analysis,Text,,,,,https://github.com/cblancac/SentimentAnalysisBert/blob/main/data,https://huggingface.co/datasets/carblacac/twitter-sentiment-analysis,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"The Twitter Sentiment Analysis Dataset contains 1,578,627 classified tweets, each row is marked as 1 for positive sentiment and 0 for negative sentiment. The dataset is based on data from the following two sources: University of Michigan Sentiment Analysis competition on Kaggle Twitter Sentiment Corpus by Niek Sanders Finally, I randomly selected a subset of them, applied a cleaning process, and divided them between the test and train subsets, keeping a balance between the number of positive and negative tweets within each of these subsets."
cardiffnlp/tweet_eval,Video,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",,https://huggingface.co/datasets/cardiffnlp/tweet_eval,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",d as multi-class tweet classification,,"Dataset Card for tweet_eval Data-Summary: TweetEval consists of seven heterogenous tasks in Twitter, all framed as multi-class tweet classification. The tasks include - irony, hate, offensive, stance, emoji, emotion, and sentiment. All tasks have been unified into the same benchmark, with each dataset presented in the same format and with fixed training, validation and test splits. Supported Tasks and Leaderboards text_classification: The datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cardiffnlp/tweet_eval."
carlosejimenez/flickr30k_CLIP_ViT-B-32_subset_pairs_SimCSE_similarity,,,,,,,https://huggingface.co/datasets/carlosejimenez/flickr30k_CLIP_ViT-B-32_subset_pairs_SimCSE_similarity,,,,,,,,,,,,,,,,,,,,carlosejimenez/flickr30k_CLIP_ViT-B-32_subset_pairs_SimCSE_similarity dataset hosted on Hugging Face and contributed by the HF Datasets community
carolina-c4ai/corpus-carolina,Text,,,,,https://sites.usp.br/corpuscarolina/,https://huggingface.co/datasets/carolina-c4ai/corpus-carolina,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,Carolina is an Open Corpus for Linguistics and Artificial Intelligence with a robust volume of texts of varied typology in contemporary Brazilian Portuguese (1970-).
CarperAI/openai_summarize_tldr,,,,,,,https://huggingface.co/datasets/CarperAI/openai_summarize_tldr,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""openai_summarize_tldr"" More Information needed"
casehold/casehold,,,,,,,https://huggingface.co/datasets/casehold/casehold,,,,,,,,,,415,,,,,,,,,,"CaseHOLD (Case Holdings On Legal Decisions) is a law dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case."
cassanof/leetcode-solutions,,,,,,,https://huggingface.co/datasets/cassanof/leetcode-solutions,,,,,,,,,,,,,,,,,,,,From: https://www.kaggle.com/datasets/jacobhds/leetcode-solutions-and-content-kpis
castorini/africlirmatrix,Text,General,Scientific,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/castorini/africlirmatrix,s,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Data-Summary: AfriCLIRMatrix is a test collection for cross-lingual information retrieval research in 15 diverse African languages. This resource comprises English queries with queryâ€“document relevance judgments in 15 African languages automatically mined from Wikipedia This dataset stores documents of AfriCLIRMatrix. To access the queries and judgments, please refer to castorini/africlirmatrix. Dataset Structure The only configuration here is the language. Anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/castorini/africlirmatrix."
causal-lm/natural_instructions,,,,,,,https://huggingface.co/datasets/causal-lm/natural_instructions,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""natural_instructions"" More Information needed"
causal-nlp/corr2cause,,,,,,,https://huggingface.co/datasets/causal-nlp/corr2cause,,,,,,,,,,,,,,,,,,,,Dataset card for corr2cause TODO
ccdv/pubmed-summarization,,,,,,,https://huggingface.co/datasets/ccdv/pubmed-summarization,,,,,,,,,,,,https://github.com/huggingface/transformers/tree/master/examples/pytorch/summarization,,,,,,,,"PubMed dataset for summarization Dataset for summarization of long documents.Adapted from this repo.Note that original data are pre-tokenized so this dataset returns "" "".join(text) and add ""\n"" for paragraphs. This dataset is compatible with the run_summarization.py script from Transformers if you add this line to the summarization_name_mapping variable: ""ccdv/pubmed-summarization"": (""article"", ""abstract"") Data Fields id: paper id article: a string containingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ccdv/pubmed-summarization."
ccmusic-database/timbre_range,Audio,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",,https://huggingface.co/datasets/ccmusic-database/timbre_range,"Chinese, English Datasetâ€¦ See the full description on the dataset page: https://huggingface",,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for Timbre and Range Dataset Data-Summary: The timbre dataset contains acapella singing audio of 9 singers, as well as cut single-note audio, totaling 775 clips (.wav format) The vocal range dataset includes several up and down chromatic scales audio clips of several vocals, as well as the cut single-note audio clips (.wav format). Supported Tasks and Leaderboards Audio classification Languages Chinese, English Datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ccmusic-database/timbre_range."
cdminix/libritts-r-aligned,,,,,,https://arxiv.org/abs/1904.02882,https://huggingface.co/datasets/cdminix/libritts-r-aligned,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset used for loading TTS spectrograms and waveform audio with alignments and a number of configurable ""measures"", which are extracted from the raw audio."
celikmus/mayo_clinic_symptoms_and_diseases_v1,,,,,,,https://huggingface.co/datasets/celikmus/mayo_clinic_symptoms_and_diseases_v1,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""mayo_clinic_symptoms_and_diseases_v1"" More Information needed"
Censius-AI/ECommerce-Women-Clothing-Reviews,,,,,,,https://huggingface.co/datasets/Censius-AI/ECommerce-Women-Clothing-Reviews,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Censius-AI/ECommerce-Women-Clothing-Reviews dataset hosted on Hugging Face and contributed by the HF Datasets community
cerebras/SlimPajama-627B,Text,,,,,https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama,https://huggingface.co/datasets/cerebras/SlimPajama-627B,,,,,,,,,,,,,,,,,,,,"The dataset consists of 59166 jsonl files and is ~895GB compressed. It is a cleaned and deduplicated version of Together's RedPajama. Check out our blog post explaining our methods, our code on GitHub, and join the discussion on the Cerebras Discord. Getting Started You can download the dataset using Hugging Face datasets: from datasets import load_dataset ds = load_dataset(""cerebras/SlimPajama-627B"") Background Today we are releasing SlimPajama â€“ the largestâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cerebras/SlimPajama-627B."
ceyda/fashion-products-small,,,,,,,https://huggingface.co/datasets/ceyda/fashion-products-small,,,,,,,,,,,,,,,,,,,,For test purposes! Preprocessed version of https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-dataset Images resized to have max 512
cfilt/iitb-english-hindi,,,,,,,https://huggingface.co/datasets/cfilt/iitb-english-hindi,,,,,,,,,,,,,,,,,,,,"IITB-English-Hindi Parallel Corpus About The IIT Bombay English-Hindi corpus contains parallel corpus for English-Hindi as well as monolingual Hindi corpus collected from a variety of existing sources and corpora developed at the Center for Indian Language Technology, IIT Bombay over the years. This page describes the corpus. This corpus has been used at the Workshop on Asian Language Translation Shared Task since 2016 the Hindi-to-English andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cfilt/iitb-english-hindi."
CFPB/consumer-finance-complaints,Text,General,General,Text,"Accuracy, F1 Score",https://www.consumerfinance.gov/data-research/consumer-complaints/,https://huggingface.co/datasets/CFPB/consumer-finance-complaints,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Consumer Finance Complaints Data-Summary: This database is a collection of complaints about consumer financial products and services that we sent to companies for response. The Consumer Complaint Database is a collection of complaints about consumer financial products and services that we sent to companies for response. Complaints are published after the company responds, confirming a commercial relationship with the consumer, or after 15 daysâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CFPB/consumer-finance-complaints."
CGIAR/gardian-cigi-ai-documents,,,,,,,https://huggingface.co/datasets/CGIAR/gardian-cigi-ai-documents,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Pages: 1,438,332 Tokens: 277,445,818 A Curated Research Corpus for Agricultural Advisory AI Applications This dataset represents a comprehensive collection of 43,745 agricultural research publications from CGIAR, specifically processed and structured for Large Language Model (LLM) applications in agricultural advisory services. This dataset bridges the gap between advanced agricultural research and field-level advisory needs, drawing from CGIAR's extensive scientific knowledgeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CGIAR/gardian-cigi-ai-documents."
cgpotts/swda,Text,,,,,http://compprag.christopherpotts.net/swda.html,https://huggingface.co/datasets/cgpotts/swda,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-3.0/,,,,,,,,,,,,"The Switchboard Dialog Act Corpus (SwDA) extends the Switchboard-1 Telephone Speech Corpus, Release 2 with turn/utterance-level dialog-act tags. The tags summarize syntactic, semantic, and pragmatic information about the associated turn. The SwDA project was undertaken at UC Boulder in the late 1990s. The SwDA is not inherently linked to the Penn Treebank 3 parses of Switchboard, and it is far from straightforward to align the two resources. In addition, the SwDA is not distributed with the Switchboard's tables of metadata about the conversations and their participants."
ChaiML/soda_10k_samples,,,,,,,https://huggingface.co/datasets/ChaiML/soda_10k_samples,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""soda_10k_samples"" More Information needed"
chainyo/natural-instructions-tokenized,,,,,,,https://huggingface.co/datasets/chainyo/natural-instructions-tokenized,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""natural-instructions-tokenized"" Here is the script used to tokenize the dataset: import multiprocessing from typing import Union from datasets import DatasetDict, load_dataset from transformers import LlamaTokenizer # Find your available cores num_cores = multiprocessing.cpu_count() cutoff_len = 2048 tokenizer = LlamaTokenizer.from_pretrained(""chainyo/alpaca-lora-7b"") tokenizer.padding_side = ""left"" tokenizer.pad_token_id = (0) prompt_template = {â€¦ See the full description on the dataset page: https://huggingface.co/datasets/chainyo/natural-instructions-tokenized."
ChanceFocus/en-fpb,,,,,,,https://huggingface.co/datasets/ChanceFocus/en-fpb,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Card for ""flare-fpb"" More Information needed"
chao1224/MoleculeSTM,,,,,,,https://huggingface.co/datasets/chao1224/MoleculeSTM,,,,,,,,,,,,,,,,,,,,"Dataset Specifications for MoleculeSTM We provide the raw dataset (after preprocessing) at this Hugging Face link. Or you can download them by running python download.py. 1. Pretraining Dataset: PubChemSTM For PubChemSTM, please note that we can only release the chemical structure information. If you need the textual data, please follow our preprocessing scripts. 2. Downstream Datasets Please refer to the following for three downstream tasks:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/chao1224/MoleculeSTM."
chaoyi-wu/PMC-CaseReport,,,,,,,https://huggingface.co/datasets/chaoyi-wu/PMC-CaseReport,,,,,,,,,,,,,,,,,,,,PMC-CaseReport Dataset PMC-CaseReport_original Dataset Daraset Structure Sample This is the text parts and the figure parts can be dowloaded from https://pan.baidu.com/s/1Src_rhXsaOFp8zJ_3zMFsQ?pwd=p3ne. Dataset Structure PMC-CaseReport (Filtered version: 317K VQA pairs for taining and of 121K for testing images). The dataset can be loading following huggingface datasets rule: from datasets import load_dataset dataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/chaoyi-wu/PMC-CaseReport.
chaoyue7/reacto_data,,,,,,https://arxiv.org/abs/2404.11151,https://huggingface.co/datasets/chaoyue7/reacto_data,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"REACTO: Reconstructing Articulated Objects from a Single Video Chaoyue Song Â· Jiacheng Wei Â· Chuan-Sheng Foo Â· Guosheng Lin Â· Fayao Liu CVPR 2024 Project | Paper | Video This repo includes pre-processed data used in REACTO. To use your own videos, or pre-process raw videos into the same format, please check this code. Citation @inproceedings{song2024reacto, title={REACTO: Reconstructing Articulated Objects from a Single Video}, author={Songâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/chaoyue7/reacto_data."
chargoddard/Open-Platypus-Chat,,,,,,,https://huggingface.co/datasets/chargoddard/Open-Platypus-Chat,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"Dataset Card for ""Open-Platypus-Chat"" This is the Open-Platypus dataset converted to sharegpt format, with a handful of potential refusals removed. All credit to the OpenPlatypus team and the original authors of the various component datasets."
chats-bug/agent_action_plan,,,,,,,https://huggingface.co/datasets/chats-bug/agent_action_plan,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""agent_action_plan"" More Information needed"
ChengSong/nl2sql_general_ability_enhanced,,,,,,,https://huggingface.co/datasets/ChengSong/nl2sql_general_ability_enhanced,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""nl2sql_general_ability_enhanced"" More Information needed"
chengyenhsieh/TAO-Amodal,,,,,,https://arxiv.org/abs/2312.12433,https://huggingface.co/datasets/chengyenhsieh/TAO-Amodal,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"TAO-Amodal Dataset Official Source for Downloading the TAO-Amodal and TAO Dataset. ðŸ“™ Project Page | ðŸ’» Code | ðŸ“Ž Paper Link | âœï¸ Citations Contact: ðŸ™‹ðŸ»â€â™‚ï¸Cheng-Yen (Wesley) Hsieh Dataset Description Our dataset augments the TAO dataset with amodal bounding box annotations for fully invisible, out-of-frame, and occluded objects. Note that this implies TAO-Amodal also includes modal segmentation masks (as visualized in the color overlaysâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/chengyenhsieh/TAO-Amodal."
chi-vi/UAA-Chinese-H-Novels,,,,,,,https://huggingface.co/datasets/chi-vi/UAA-Chinese-H-Novels,,,,,,,,,,,,,,,,,,,,chi-vi/UAA-Chinese-H-Novels dataset hosted on Hugging Face and contributed by the HF Datasets community
chibbss/fitness-chat-prompt-completion-dataset,,,,,,,https://huggingface.co/datasets/chibbss/fitness-chat-prompt-completion-dataset,,,,,,,,,,,,,,,,,,,,chibbss/fitness-chat-prompt-completion-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
ChilleD/SVAMP,,,,,,,https://huggingface.co/datasets/ChilleD/SVAMP,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,ChilleD/SVAMP dataset hosted on Hugging Face and contributed by the HF Datasets community
china-ai-law-challenge/cail2018,Text,General,General,Text,"Accuracy, F1 Score",https://github.com/thunlp/CAIL/blob/master/README_en.md,https://huggingface.co/datasets/china-ai-law-challenge/cail2018,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for CAIL 2018 Data-Summary: [More Information Needed] Supported Tasks and Leaderboards [More Information Needed] Languages [More Information Needed] Dataset Structure Data Instances [More Information Needed] Data Fields [More Information Needed] Data Splits [More Information Needed] Dataset Creation Curation Rationale [Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/china-ai-law-challenge/cail2018.
Chinese-Vicuna/guanaco_belle_merge_v1.0,,,,,,,https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,Thanks for Guanaco Dataset and Belle Dataset This dataset was created by merging the above two datasets in a certain format so that they can be used for training our code Chinese-Vicuna
chintagunta85/bc4chemd,,,,,,,https://huggingface.co/datasets/chintagunta85/bc4chemd,,,,,,,,,,,,,,,,,,,,"The automatic extraction of chemical information from text requires the recognition of chemical entity mentions as one of its key steps. When developing supervised named entity recognition (NER) systems, the availability of a large, manually annotated text corpus is desirable. Furthermore, large corpora permit the robust evaluation and comparison of different approaches that detect chemicals in documents. We present the CHEMDNER corpus, a collection of 10,000 PubMed abstracts that contain a total of 84,355 chemical entity mentions labeled manually by expert chemistry literature curators, following annotation guidelines specifically defined for this task. The abstracts of the CHEMDNER corpus were selected to be representative for all major chemical disciplines. Each of the chemical entity mentions was manually labeled according to its structure-associated chemical entity mention (SACEM) class: abbreviation, family, formula, identifier, multiple, systematic and trivial. The difficulty and consistency of tagging chemicals in text was measured using an agreement study between annotators, obtaining a percentage agreement of 91. For a subset of the CHEMDNER corpus (the test set of 3,000 abstracts) we provide not only the Gold Standard manual annotations, but also mentions automatically detected by the 26 teams that participated in the BioCreative IV CHEMDNER chemical mention recognition task. In addition, we release the CHEMDNER silver standard corpus of automatically extracted mentions from 17,000 randomly selected PubMed abstracts. A version of the CHEMDNER corpus in the BioC format has been generated as well. We propose a standard for required minimum information about entity annotations for the construction of domain specific corpora on chemical and drug entities. The CHEMDNER corpus and annotation guidelines are available at: http://www.biocreative.org/resources/biocreative-iv/chemdner-corpus/"
chiyuanhsiao/HowFarAreYou_3DSpeakerTrain,,,,,,,https://huggingface.co/datasets/chiyuanhsiao/HowFarAreYou_3DSpeakerTrain,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""HowFarAreYou_3DSpeakerTrain"" More Information needed"
Chris1/cityscapes,,,,,,,https://huggingface.co/datasets/Chris1/cityscapes,,,,,,,,,,,,,,,,,,,,Chris1/cityscapes dataset hosted on Hugging Face and contributed by the HF Datasets community
ChrisHayduk/Llama-2-SQL-Dataset,,,,,,,https://huggingface.co/datasets/ChrisHayduk/Llama-2-SQL-Dataset,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""Llama-2-SQL-Dataset"" This dataset is deprecated in favor of ChrisHayduk/Llama-2-SQL-and-Code-Dataset"
christopher/rosetta-code,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/christopher/rosetta-code,"as possible, to demonstrate how languages are similar and different, and to aid a person with a grounding in one approach to a problem in learning another",,,,,,,https://choosealicense.com/licenses/gfdl/,,,,,,,,,"BERT, RoBERTa, T5",ing chrestomathy site,,"Dataset Card for the Rosetta Code Dataset Data-Summary: Rosetta Code is a programming chrestomathy site. The idea is to present solutions to the same task in as many different languages as possible, to demonstrate how languages are similar and different, and to aid a person with a grounding in one approach to a problem in learning another. Rosetta Code currently has 1,203 tasks, 389 draft tasks, and is aware of 883 languages, though we do not (and cannot) haveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/christopher/rosetta-code."
ChuGyouk/medical-o1-train-kormedmcqa,,,,,,,https://huggingface.co/datasets/ChuGyouk/medical-o1-train-kormedmcqa,,,,,,,,,,,,,,,,,,,,"Information This data includes the training data from KorMedMCQA, as well as a portion of the trainind data from the additional KorMedMCQA support set (private). This dataset is based on the responses generated by gemini-flash-thinking-exp-01-21 model and has undergone MANUAL rejection sampling."
cj-mills/hagrid-sample-30k-384p,,,,,,,https://huggingface.co/datasets/cj-mills/hagrid-sample-30k-384p,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"This dataset contains 31,833 images from HaGRID (HAnd Gesture Recognition Image Dataset) downscaled to 384p. The original dataset is 716GB and contains 552,992 1080p images. I created this sample for a tutorial so readers can use the dataset in the free tiers of Google Colab and Kaggle Notebooks. Original Authors: Alexander Kapitanov Andrey Makhlyarchuk Karina Kvanchiani Original Dataset Links GitHub Kaggle Datasets Page Object Classes ['call'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cj-mills/hagrid-sample-30k-384p."
clane9/NSD-Flat,,,,,,,https://huggingface.co/datasets/clane9/NSD-Flat,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"NSD-Flat [GitHub] [ðŸ¤— Hugging Face Hub] A Hugging Face dataset of pre-processed brain activity flat maps from the Natural Scenes Dataset, constrained to a visual cortex region of interest and rendered as PNG images. Load the dataset Load the dataset from Hugging Face Hub from datasets import load_dataset dataset = load_dataset(""clane9/NSD-Flat"", split=""train"") Building the dataset 1. Download source data Run download_data.sh toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/clane9/NSD-Flat."
clarin-knext/msmarco-pl,,,,,,https://arxiv.org/abs/2305.19840,https://huggingface.co/datasets/clarin-knext/msmarco-pl,,,,,,,,,,,,,,,,,,,,Part of BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language. Link to arxiv: https://arxiv.org/pdf/2305.19840.pdf Contact: konrad.wojtasik@pwr.edu.pl
clarin-pl/poquad,,,,,,,https://huggingface.co/datasets/clarin-pl/poquad,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,PoQuaD description
cledoux42/GenderClassify,,,,,,,https://huggingface.co/datasets/cledoux42/GenderClassify,,,,,,,,,,,,,,,,,,,,GenderClassify
climatebert/environmental_claims,Text,General,General,Text,"Accuracy, F1 Score",https://climatebert.ai,https://huggingface.co/datasets/climatebert/environmental_claims,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for environmental_claims Data-Summary: We introduce an expert-annotated dataset for detecting real-world environmental claims made by listed companies. Supported Tasks and Leaderboards The dataset supports a binary classification task of whether a given sentence is an environmental claim or not. Languages The text in the dataset is in English. Dataset Structure Data Instances { ""text"":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/climatebert/environmental_claims."
Clinton/Text-to-sql-v1,,,,,,,https://huggingface.co/datasets/Clinton/Text-to-sql-v1,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Clinton/Text-to-sql-v1 dataset hosted on Hugging Face and contributed by the HF Datasets community
clip-benchmark/wds_cars,,,,,,,https://huggingface.co/datasets/clip-benchmark/wds_cars,,,,,,,,,,,12632370619495,,,,,,,,,clip-benchmark/wds_cars dataset hosted on Hugging Face and contributed by the HF Datasets community
clips/mfaq,,,,,,https://arxiv.org/abs/2109.12870,https://huggingface.co/datasets/clips/mfaq,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"We present the first multilingual FAQ dataset publicly available. We collected around 6M FAQ pairs from the web, in 21 different languages."
clouditera/security-paper-datasets,,,,,,,https://huggingface.co/datasets/clouditera/security-paper-datasets,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""security-paper-datasets"" More Information needed"
CloverSearch/cc-news-mutlilingual,,,,,,,https://huggingface.co/datasets/CloverSearch/cc-news-mutlilingual,,,,,,,,,,,,,,,,,,,,CloverSearch/cc-news-mutlilingual dataset hosted on Hugging Face and contributed by the HF Datasets community
CLUTRR/v1,Text,,,,,https://arxiv.org/abs/1908.06177,https://huggingface.co/datasets/CLUTRR/v1,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"CLUTRR (Compositional Language Understanding and Text-based Relational Reasoning), a diagnostic benchmark suite, is first introduced in (https://arxiv.org/abs/1908.06177) to test the systematic generalization and inductive reasoning capabilities of NLU systems."
CM/codexglue_code2text_javascript,,,,,,,https://huggingface.co/datasets/CM/codexglue_code2text_javascript,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""codexglue_code2text_javascript"" More Information needed"
cnachteg/duvel,Text,General,Scientific,Text,"Accuracy, F1 Score",https://huggingface.co/datasets/cnachteg/DUVEL/,https://huggingface.co/datasets/cnachteg/duvel,English,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for DUVEL Data-Summary: This dataset was created to identity oligogenic variant combinations, i.e. relation between several genes and their mutations, causing genetic diseases in scientific articles written in english. At the moment, it contains only digenic variant combinations, i.e. relations between two genes and at least two variants. The dataset is intended for binary relation extraction where the entities are masked within the text.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cnachteg/duvel."
coallaoh/COCO-AB,,,,,,https://arxiv.org/abs/2303.17595,https://huggingface.co/datasets/coallaoh/COCO-AB,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"General Information Title: COCO-AB Description: The COCO-AB dataset is an extension of the COCO 2014 training set, enriched with additional annotation byproducts (AB). The data includes 82,765 reannotated images from the original COCO 2014 training set. It has relevance in computer vision, specifically in object detection and location. The aim of the dataset is to provide a richer understanding of the images (without extra costs) by recording additional actions andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/coallaoh/COCO-AB."
coastalcph/lex_glue,Text,General,General,Text,"Accuracy, F1 Score",https://github.com/coastalcph/lex-glue,https://huggingface.co/datasets/coastalcph/lex_glue,"Understanding Evaluation (LexGLUE) benchmark, a benchmark dataset toâ€¦ See the full description on the dataset page: https://huggingface",,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,415,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""LexGLUE"" Data-Summary: Inspired by the recent widespread use of the GLUE multi-task benchmark NLP dataset (Wang et al., 2018), the subsequent more difficult SuperGLUE (Wang et al., 2019), other previous multi-task NLP benchmarks (Conneau and Kiela, 2018; McCann et al., 2018), and similar initiatives in other domains (Peng et al., 2019), we introduce the Legal General Language Understanding Evaluation (LexGLUE) benchmark, a benchmark dataset toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/coastalcph/lex_glue."
code-search-net/code_search_net,Text,,,,,https://wandb.ai/github/CodeSearchNet/benchmark,https://huggingface.co/datasets/code-search-net/code_search_net,,,,,,,,https://choosealicense.com/licenses/other/,,,,https://github.com/github/CodeSearchNet,,,,,,,,"CodeSearchNet corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation."
CodedotAI/code_clippy_github,,,,,,https://arxiv.org/abs/2107.03374,https://huggingface.co/datasets/CodedotAI/code_clippy_github,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,The Code Clippy dataset consists of various public codebases from GitHub in 22 programming languages with 23 extensions totalling about 16 TB of data when uncompressed. The dataset was created from the public GitHub dataset on Google BiqQuery.
CodeKapital/CookingRecipes,,,,,,,https://huggingface.co/datasets/CodeKapital/CookingRecipes,,,,,,,,,,,,,,,,,,,,CodeKapital/CookingRecipes dataset hosted on Hugging Face and contributed by the HF Datasets community
codeparrot/codeparrot-clean,,,,,,,https://huggingface.co/datasets/codeparrot/codeparrot-clean,,,,,,,,,,,,https://github.com/huggingface/transformers/tree/master/examples/research_projects/codeparrot,,,,,,,,"CodeParrot ðŸ¦œ Dataset Cleaned What is it? A dataset of Python files from Github. This is the deduplicated version of the codeparrot. Processing The original dataset contains a lot of duplicated and noisy data. Therefore, the dataset was cleaned with the following steps: Deduplication Remove exact matches Filtering Average line length < 100 Maximum line length < 1000 Alpha numeric characters fraction > 0.25 Remove auto-generated files (keywordâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/codeparrot/codeparrot-clean."
CodeT5SmallCAPS/CAPS_Python,,,,,,,https://huggingface.co/datasets/CodeT5SmallCAPS/CAPS_Python,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""DeepCC_Python"" More Information needed"
Cofacts/line-msg-fact-check-tw,,,,,,,https://huggingface.co/datasets/Cofacts/line-msg-fact-check-tw,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,sFields in All TablesTables and their fieldsarticlesarticle_hyperlinksarticle_categoriescategoriesarticle_repliesrepliesreply_hyperlinksreply_requestsarticle_reply_feedbacksanalyticsanonymized_usrsâš  [NOTICE] Caveats of using this data âš Cofacts Archive for Reported Messages and Crowd-Sourced Fact-Check RepliesThe Cofacts dataset encompasses instant messages that have been reported by users of theCofacts chatbotand the replies provided by theCofacts crowd-sourced fact-checking community,,,,,,"Cofacts Archive for Reported Messages and Crowd-Sourced Fact-Check Replies The Cofacts dataset encompasses instant messages that have been reported by users of the Cofacts chatbot and the replies provided by the Cofacts crowd-sourced fact-checking community. Attribution to the Community This dataset is a result of contributions from both Cofacts LINE chatbot users and the community fact checkers. To appropriately attribute their efforts, please adhere to theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cofacts/line-msg-fact-check-tw."
cognitivecomputations/dolphin,,,,,,,https://huggingface.co/datasets/cognitivecomputations/dolphin,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Dolphin ðŸ¬ https://erichartford.com/dolphin Dataset details This dataset is an attempt to replicate the results of Microsoft's Orca Our dataset consists of: ~1 million of FLANv2 augmented with GPT-4 completions (flan1m-alpaca-uncensored.jsonl) ~3.5 million of FLANv2 augmented with GPT-3.5 completions (flan5m-alpaca-uncensored.jsonl) We followed the submix and system prompt distribution outlined in the Orca paper. With a few exceptions. We included all 75k of CoT in the FLAN-1mâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cognitivecomputations/dolphin.
Cohere/wikipedia-22-12,,,,,,,https://huggingface.co/datasets/Cohere/wikipedia-22-12,,,,,,,,,,,,,,,,,,,,"Note: This dataset has been embedded with the outdated Cohere Embed v2 multilingual model. Check out Cohere Embed v3 for improved retrieval quality This dataset contains a pre-processed version from Wikipedia suitable for semantic search. You can load the dataset like this: from datasets import load_dataset lang = 'en' data = load_dataset(f""Cohere/wikipedia-22-12"", lang, split='train', streaming=True) for row in data: print(row) break This will load the dataset in a streaming mode (soâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/wikipedia-22-12."
CohereForAI/Global-MMLU,Text,Translation,General,Text,"BLEU, METEOR, TER",https://arxiv.org/abs/2412.03304,https://huggingface.co/datasets/CohereForAI/Global-MMLU,", including English",,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"T5, mBART, M2M100",,,"Data-Summary: Global-MMLU ðŸŒ is a multilingual evaluation set spanning 42 languages, including English. This dataset combines machine translations for MMLU questions along with professional translations and crowd-sourced post-edits. It also includes cultural sensitivity annotations for a subset of the questions (2850 questions per language) and classifies them as Culturally Sensitive (CS) ðŸ—½ or Culturally Agnostic (CA) âš–ï¸. These annotations were collected as part of an openâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/Global-MMLU."
cointegrated/ru-paraphrase-NMT-Leipzig,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/cointegrated/ru-paraphrase-NMT-Leipzig,English,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for cointegrated/ru-paraphrase-NMT-Leipzig Data-Summary: The dataset contains 1 million Russian sentences and their automatically generated paraphrases. It was created by David Dale (@cointegrated) by translating the rus-ru_web-public_2019_1M corpus from the Leipzig collection into English and back into Russian. A fraction of the resulting paraphrases are invalid, and should be filtered out. The blogpost ""ÐŸÐµÑ€ÐµÑ„Ñ€Ð°Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ€ÑƒÑÑÐºÐ¸Ñ… Ñ‚ÐµÐºÑÑ‚Ð¾Ð²: ÐºÐ¾Ñ€Ð¿ÑƒÑÐ°â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cointegrated/ru-paraphrase-NMT-Leipzig."
collectivat/amazic,,,,,,,https://huggingface.co/datasets/collectivat/amazic,,,,,,,,https://choosealicense.com/licenses/cc-by-2.0/,,,,,,,,,,,,This repository contains various Tamazight language datasets created by ColÂ·lectivaT in collaboration with CIEMEN and with funding from Municipality of Barcelona and Government of Catalonia. Under mono you can find monolingual sentences. tc_wajdm_v1.txt - Texts from language learning material â€œtc wawjdmâ€ IRCAM-clean-tifinagh.txt - Tifinagh scripted sentences extracted from IRCAM's text corpus Under parallel you can find sentences with translations. AWAL contains data extracted fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/collectivat/amazic.
commaai/commavq,,,,,,,https://huggingface.co/datasets/commaai/commavq,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,TODO
community-datasets/ehealth_kd,Text,General,Document,Text,"Accuracy, F1 Score",https://knowledge-learning.github.io/ehealthkd-2020/,https://huggingface.co/datasets/community-datasets/ehealth_kd,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for eHealth-KD Data-Summary: Dataset of the eHealth-KD Challenge at IberLEF 2020. It is designed for the identification of semantic entities and relations in Spanish health documents. Supported Tasks and Leaderboards The eHealth-KD challenge proposes two computational subtasks: named-entity-recognition: Given a sentence of an eHealth document written in Spanish, the goal of this subtask is to identify all the entities and theirâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/community-datasets/ehealth_kd."
conceptnet5/conceptnet5,Text,General,General,Text,"Accuracy, F1 Score",https://github.com/commonsense/conceptnet5/wiki,https://huggingface.co/datasets/conceptnet5/conceptnet5,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Conceptnet5 Data-Summary: ConceptNet is a multilingual knowledge base, representing words and phrases that people use and the common-sense relationships between them. The knowledge in ConceptNet is collected from a variety of resources, including crowd-sourced resources (such as Wiktionary and Open Mind Common Sense), games with a purpose (such as Verbosity and nadya.jp), and expert-created resources (such as WordNet and JMDict). You can browseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/conceptnet5/conceptnet5."
conceptofmind/medium_articles,,,,,,,https://huggingface.co/datasets/conceptofmind/medium_articles,,,,,,,,,,,,,,,,,,,,conceptofmind/medium_articles dataset hosted on Hugging Face and contributed by the HF Datasets community
Congliu/USPTO-50k-Instruction,,,,,,,https://huggingface.co/datasets/Congliu/USPTO-50k-Instruction,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Congliu/USPTO-50k-Instruction dataset hosted on Hugging Face and contributed by the HF Datasets community
convai-challenge/conv_ai_2,Text,,,,,https://github.com/DeepPavlov/convai/tree/master/2018,https://huggingface.co/datasets/convai-challenge/conv_ai_2,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"ConvAI is a dataset of human-to-bot conversations labelled for quality. This data can be used to train a metric for evaluating dialogue systems. Moreover, it can be used in the development of chatbots themselves: it contains the information on the quality of utterances and entire dialogues, that can guide a dialogue system in search of better answers."
copenlu/fever_gold_evidence,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",https://github.com/copenlu/fever-adversarial-attacks,https://huggingface.co/datasets/copenlu/fever_gold_evidence,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for fever_gold_evidence Data-Summary: Dataset for training classification-only fact checking with claims from the FEVER dataset. This dataset is used in the paper ""Generating Label Cohesive and Well-Formed Adversarial Claims"", EMNLP 2020 The evidence is the gold evidence from the FEVER dataset for REFUTE and SUPPORT claims. For NEI claims, we extract evidence sentences with the system in ""Christopher Malon. 2018. Team Papelo: Transformer Networks atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/copenlu/fever_gold_evidence."
corbt/all-recipes,,,,,,,https://huggingface.co/datasets/corbt/all-recipes,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""all-recipes"" More Information needed"
Corianas/gpt4all_Stripped_dollyfied,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Corianas/gpt4all_Stripped_dollyfied,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,Data-Summary: This is a 'Can Do' dataset spripped down from the original GPT4all dataset All prompts that have 'As an AI' as a reply have been removed along with any other refusal (Such as not having input propery) A New Collumn named Text has been added with ### Response: added in between the prompt and the response so that DataBricks Dolly scripts can just access it https://github.com/databrickslabs/dolly
cornell-movie-review-data/rotten_tomatoes,Text,Sentiment Analysis,General,Label,"Accuracy, F1 Score",http://www.cs.cornell.edu/people/pabo/movie-review-data/,https://huggingface.co/datasets/cornell-movie-review-data/rotten_tomatoes,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, DistilBERT",,,"Dataset Card for ""rotten_tomatoes"" Data-Summary: Movie Review Dataset. This is a dataset of containing 5,331 positive and 5,331 negative processed sentences from Rotten Tomatoes movie reviews. This data was first used in Bo Pang and Lillian Lee, ``Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.'', Proceedings of the ACL, 2005. Supported Tasks and Leaderboards More Information Neededâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cornell-movie-review-data/rotten_tomatoes."
Corran/pexelvideos,,,,,,,https://huggingface.co/datasets/Corran/pexelvideos,,,,,,,,,,,,,,,,,,,,"Pexel Videos 358,551 video urls, average length 19.5s, and associated metadata from pexels.com. Data was extracted from their video sitemaps (pexels.com/robots.txt) on 01/08/2022. Data is stored in PexelVideos.parquet.gzip as a gzipped parquet To get this data ensure you have git installed and do !git lfs clone https://huggingface.co/datasets/Corran/pexelvideos/ In python the reccomended reading is by opening the file with pandas. !pip install pandas import pandasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Corran/pexelvideos."
corypaik/coda,Text,,,,,https://arxiv.org/abs/2110.08182,https://huggingface.co/datasets/corypaik/coda,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"*The Color Dataset* (CoDa) is a probing dataset to evaluate the representation of visual properties in language models. CoDa consists of color distributions for 521 common objects, which are split into 3 groups: Single, Multi, and Any."
CraftJarvis/minecraft-vla-sft,,,,,,https://arxiv.org/abs/2503.16365,https://huggingface.co/datasets/CraftJarvis/minecraft-vla-sft,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,This repository contains the dataset used for JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse. Project Website
CreativeLang/vua20_metaphor,Text,Detection,General,Bounding Box/Mask,"mAP, IoU",,https://huggingface.co/datasets/CreativeLang/vua20_metaphor,Toolkit (CLTK) Metadata CL Type: Metaphor Task Type: detection Size: 200k Created time: 2020 VUA20 is (perhaps) the largest dataset of metaphor detection used in Figlang2020 workshop,,,,,,,https://choosealicense.com/licenses/cc-by-2.0/,,,,,,,,,"DETR, Faster R-CNN, YOLO",,,"VUA20 Data-Summary: Creative Language Toolkit (CLTK) Metadata CL Type: Metaphor Task Type: detection Size: 200k Created time: 2020 VUA20 is (perhaps) the largest dataset of metaphor detection used in Figlang2020 workshop. For the details of this dataset, we refer you to the release paper. The annotation method of VUA20 is elabrated in the paper of MIP. Citation Information If you find this dataset helpful, please cite:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/CreativeLang/vua20_metaphor."
CrowdAILab/scicap,,,,,,https://arxiv.org/abs/2301.12293,https://huggingface.co/datasets/CrowdAILab/scicap,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"The 1st Scientific Figure Captioning (SciCap) Challenge ðŸ“–ðŸ“Š Welcome to the 1st Scientific Figure Captioning (SciCap) Challenge! ðŸŽ‰ This dataset contains approximately 400,000 scientific figure images sourced from various arXiv papers, along with their captions and relevant paragraphs. The challenge is open to researchers, AI/NLP/CV practitioners, and anyone interested in developing computational models for generating textual descriptions for visuals. ðŸ’» Challenge homepage ðŸ â€¦ See the full description on the dataset page: https://huggingface.co/datasets/CrowdAILab/scicap."
crumb/Clean-Instruct-3M,,,,,,,https://huggingface.co/datasets/crumb/Clean-Instruct-3M,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""Clean-Instruct-3M"" More Information needed"
crystantine/COMFYUI-CUSTOM-NODES,,,,,,,https://huggingface.co/datasets/crystantine/COMFYUI-CUSTOM-NODES,,,,,,,,,,,,,,,,,,,,crystantine/COMFYUI-CUSTOM-NODES dataset hosted on Hugging Face and contributed by the HF Datasets community
csebuetnlp/xlsum,Text,,,,,https://arxiv.org/abs/1607.01759,https://huggingface.co/datasets/csebuetnlp/xlsum,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"We present XLSum, a comprehensive and diverse dataset comprising 1.35 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 45 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation."
CShorten/ML-ArXiv-Papers,,,,,,,https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,"This dataset contains the subset of ArXiv papers with the ""cs.LG"" tag to indicate the paper is about Machine Learning. The core dataset is filtered from the full ArXiv dataset hosted on Kaggle: https://www.kaggle.com/datasets/Cornell-University/arxiv. The original dataset contains roughly 2 million papers. This dataset contains roughly 100,000 papers following the category filtering. The dataset is maintained by with requests to the ArXiv API. The current iteration of the dataset only containsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers."
cspencergo/nft_collection_trades,,,,,,,https://huggingface.co/datasets/cspencergo/nft_collection_trades,,,,,,,,,,,,,,,,,,,,cspencergo/nft_collection_trades dataset hosted on Hugging Face and contributed by the HF Datasets community
CSTR-Edinburgh/vctk,Text,,,,,https://doi.org/10.7488/ds/2645,https://huggingface.co/datasets/CSTR-Edinburgh/vctk,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,The CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents.
ctheodoris/Genecorpus-30M,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/ctheodoris/Genecorpus-30M,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Genecorpus-30M Dataset Description Point of Contact: christina.theodoris@gladstone.ucsf.edu Data-Summary: We assembled a large-scale pretraining corpus, Genecorpus-30M, comprised of ~30 million human single cell transcriptomes from a broad range of tissues from publicly available data. This corpus was used for pretraining Geneformer, a pretrained transformer model that enables context-aware predictions in settings with limitedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ctheodoris/Genecorpus-30M."
ctiwary/products_desc_and_marktng_emails_dataset,,,,,,,https://huggingface.co/datasets/ctiwary/products_desc_and_marktng_emails_dataset,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""products_desc_and_marktng_emails_dataset"" More Information needed"
CUHK-CSE/wider_face,Text,,,,,http://shuoyang1213.me/WIDERFACE/index.html,https://huggingface.co/datasets/CUHK-CSE/wider_face,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-nd-4.0/,,,,,,,,,,,,"WIDER FACE dataset is a face detection benchmark dataset, of which images are selected from the publicly available WIDER dataset. We choose 32,203 images and label 393,703 faces with a high degree of variability in scale, pose and occlusion as depicted in the sample images. WIDER FACE dataset is organized based on 61 event classes. For each event class, we randomly select 40%/10%/50% data as training, validation and testing sets. We adopt the same evaluation metric employed in the PASCAL VOC dataset. Similar to MALF and Caltech datasets, we do not release bounding box ground truth for the test images. Users are required to submit final prediction files, which we shall proceed to evaluate."
cvssp/WavCaps,,,,,,https://arxiv.org/abs/2303.17395,https://huggingface.co/datasets/cvssp/WavCaps,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"WavCaps WavCaps is a ChatGPT-assisted weakly-labelled audio captioning dataset for audio-language multimodal research, where the audio clips are sourced from three websites (FreeSound, BBC Sound Effects, and SoundBible) and a sound event detection dataset (AudioSet Strongly-labelled Subset). Paper: https://arxiv.org/abs/2303.17395 Github: https://github.com/XinhaoMei/WavCaps Statistics Data Source # audio avg. audio duration (s) avg. text lengthâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cvssp/WavCaps."
cw1521/ember2018-malware,,,,,,,https://huggingface.co/datasets/cw1521/ember2018-malware,,,,,,,,,,,,,,,,,,,,"EMBER 2018 Malware Analysis Dataset This dataset contains 1 million records of metadata and vectorized features for malware and benign software. Visit https://github.com/elastic/ember for more information on the dataset. Usage dataset = load_dataset(""cw1521/ember2018-malware"", field=""data"") input - vectorized features as a string label - (0 for benign and 1 for malware)"
cyberagent/crello,Image-Text,General,UI,Text,"Accuracy, F1 Score",https://github.com/CyberAgentAILab/canvas-vae,https://huggingface.co/datasets/cyberagent/crello,,,,,,,,https://choosealicense.com/licenses/cdla-permissive-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for Crello Data-Summary: The Crello dataset is compiled for the study of vector graphic documents. The dataset contains document meta-data such as canvas size and pre-rendered elements such as images or text boxes. The original templates were collected from crello.com (now create.vista.com) and converted to a low-resolution format suitable for machine learning analysis. Usage import datasets dataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cyberagent/crello.
CyberNative/Code_Vulnerability_Security_DPO,,,,,,,https://huggingface.co/datasets/CyberNative/Code_Vulnerability_Security_DPO,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Cybernative.ai Code Vulnerability and Security Dataset Dataset Description The Cybernative.ai Code Vulnerability and Security Dataset is a dataset of synthetic Data Programming by Demonstration (DPO) pairs, focusing on the intricate relationship between secure and insecure code across a variety of programming languages. This dataset is meticulously crafted to serve as a pivotal resource for researchers, cybersecurity professionals, and AI developers who are keenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CyberNative/Code_Vulnerability_Security_DPO."
czyzi0/the-mc-speech-dataset,,,,,,,https://huggingface.co/datasets/czyzi0/the-mc-speech-dataset,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"This is public domain speech dataset consisting of 24018 short audio clips of a single speaker reading sentences in Polish. A transcription is provided for each clip. Clips have total length of more than 22 hours. Texts are in public domain. The audio was recorded in 2021-22 as a part of my master's thesis and is in public domain. If you use this dataset, please cite: @masterthesis{mcspeech, title={Analiza porÃ³wnawcza korpusÃ³w nagraÅ„ mowy dla celÃ³w syntezy mowy w jÄ™zyku polskim}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/czyzi0/the-mc-speech-dataset."
d0rj/curation-corpus,,,,,,https://github.com/CurationCorp/curation-corpus,https://huggingface.co/datasets/d0rj/curation-corpus,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"curation-corpus Source Data from this official repo with downloaded news articles content. Citation @misc{curationcorpusbase:2020, title={Curation Corpus Base}, author={Curation}, year={2020} }"
dadinghh2/HumTrans,,,,,,,https://huggingface.co/datasets/dadinghh2/HumTrans,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"HumTrans Dataset Dataset Name: HumTrans Dataset Type: Humming audio in .wav format and corresponding label MIDI file Primary Use: Humming melody transcription and as a foundation for downstream tasks such as humming melody based music generation Summary: 500 musical compositions of different genres and languages, 1000 music segments in total; sampled at a frequency of 44,100 Hz; approximately 56.22 hours of audio; 14,614 files in total. File Description: all_wav.zip includesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dadinghh2/HumTrans."
daekeun-ml/GLAN-qna-kr-300k,,,,,,https://arxiv.org/abs/2402.13064,https://huggingface.co/datasets/daekeun-ml/GLAN-qna-kr-300k,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,"Korean GLAN (Generalized Instruction Tuning) Instructions Dataset What is GLAN? Catastrophic forgetting, also known as catastrophic interference, occurs during SLM/LLM fine-tuning when a model trained on new data overwrites the knowledge it previously acquired, leading to a significant drop in performance on earlier tasks. This issue is particularly prominent in scenarios where the model needs to adapt to diverse and potentially conflicting data distributionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/daekeun-ml/GLAN-qna-kr-300k."
Dahoas/rm-static,,,,,,,https://huggingface.co/datasets/Dahoas/rm-static,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""rm-static"" Split of hh-static used for training reward models after supervised fine-tuning."
dair-ai/emotion,Video,General,General,Text,"Accuracy, F1 Score",https://github.com/dair-ai/emotion_dataset,https://huggingface.co/datasets/dair-ai/emotion,English,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""emotion"" Data-Summary: Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. For more detailed information please refer to the paper. Supported Tasks and Leaderboards More Information Needed Languages More Information Needed Dataset Structure Data Instances An"
daishen/cra-customs,,,,,,,https://huggingface.co/datasets/daishen/cra-customs,,,,,,,,,,,,,,,,,,,,daishen/cra-customs dataset hosted on Hugging Face and contributed by the HF Datasets community
dali-does/clevr-math,Text,,,,,,https://huggingface.co/datasets/dali-does/clevr-math,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"CLEVR-Math is a dataset for compositional language, visual and mathematical reasoning. CLEVR-Math poses questions about mathematical operations on visual scenes using subtraction and addition, such as ""Remove all large red cylinders. How many objects are left?"". There are also adversarial (e.g. ""Remove all blue cubes. How many cylinders are left?"") and multihop questions (e.g. ""Remove all blue cubes. Remove all small purple spheres. How many objects are left?"")."
dalle-mini/YFCC100M_OpenAI_subset,,,,,,https://arxiv.org/abs/1503.01817,https://huggingface.co/datasets/dalle-mini/YFCC100M_OpenAI_subset,,,,,,,,,,,,,,,,,,,,"The YFCC100M is one of the largest publicly and freely useable multimedia collection, containing the metadata of around 99.2 million photos and 0.8 million videos from Flickr, all of which were shared under one of the various Creative Commons licenses. This version is a subset defined in openai/CLIP."
DamianBoborzi/MeshFleet,,,,,,https://arxiv.org/abs/2503.14002,https://huggingface.co/datasets/DamianBoborzi/MeshFleet,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"This is a curated collection of 3D car models derived from Objaverse-XL described in MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific Generative Modeling. The MeshFleet dataset provides metadata for 3D car models, including their SHA256 from Objaverse-XL, vehicle category, and size. The core dataset is available as a CSV file: meshfleet_with_vehicle_categories_df.csv. You can easily load it using pandas: import pandas as pd meshfleet_df =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/DamianBoborzi/MeshFleet."
danasone/wikipedia_ru,,,,,,,https://huggingface.co/datasets/danasone/wikipedia_ru,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""wikipedia_ru"" More Information needed"
danielhanchen/temporary_storage,,,,,,,https://huggingface.co/datasets/danielhanchen/temporary_storage,,,,,,,,,,,,,,,,,,,,danielhanchen/temporary_storage dataset hosted on Hugging Face and contributed by the HF Datasets community
danielpleus/tatoeba-nds,,,,,,,https://huggingface.co/datasets/danielpleus/tatoeba-nds,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""tatoeba-nds"" More Information needed"
danielv835/personal_finance_v0.2,,,,,,,https://huggingface.co/datasets/danielv835/personal_finance_v0.2,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""personal_finance_v0.2"" More Information needed"
danioshi/incubus_taylor_swift_lyrics,,,,,,,https://huggingface.co/datasets/danioshi/incubus_taylor_swift_lyrics,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"Description This dataset contains lyrics from both Incubus and Taylor Swift. Format The file is in CSV format and contains three columns: Artist, Song Name and Lyrics. Caveats The column Song Name has been transformed to a single string in lowercase format, so instead of having ""Name of Song"", the value will be ""nameofsong""."
darentang/generated,,,,,,,https://huggingface.co/datasets/darentang/generated,,,,,,,,,,,36342696717149,,,,,,,,,https://arxiv.org/abs/2103.10213
dariadaria/disneyland_reviews,,,,,,,https://huggingface.co/datasets/dariadaria/disneyland_reviews,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""disneyland_reviews"" More Information needed"
DarshanaS/IndicAccentDb,,,,,,,https://huggingface.co/datasets/DarshanaS/IndicAccentDb,,,,,,,,https://choosealicense.com/licenses/c-uda/,,,,,,,,,,,,"1. Introduction Introducing a novel accent database ""IndicAccentDB"" which satisfies the below requirements: Gender balance: The speech database should be a collection of a wide range of speakers balancing both the male and female speakers to display the characteristics of the speakers speech. Phonetically balanced uniform content: To make the classification task simpler and models to distinguish the speakers, we considered building the IndicAccentDB with uniform content, aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DarshanaS/IndicAccentDb."
daspartho/subreddit-posts,,,,,,,https://huggingface.co/datasets/daspartho/subreddit-posts,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Dataset of titles of the top 1000 posts from the top 250 subreddits scraped using PRAW. For steps to create the dataset check out the dataset script in the GitHub repo.
databricks/databricks-dolly-15k,,,,,,https://arxiv.org/abs/2203.02155,https://huggingface.co/datasets/databricks/databricks-dolly-15k,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"Summary databricks-dolly-15k is an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization. This dataset can be used for any purpose, whether academic or commercial, under the terms of the Creative Commons Attribution-ShareAlike 3.0 Unportedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/databricks/databricks-dolly-15k."
datadrivenscience/ship-detection,,,,,,,https://huggingface.co/datasets/datadrivenscience/ship-detection,,,,,,,,,,,,,,,,,,,,"Dataset Card for Ship Detection Link to Ship Detection Competition By accepting this dataset, you accept the rules of the Ship Detection competition. Organizer Organizer of this competition is Data-Driven Science. Email Usage By accepting this dataset, you consent that your email will be used for communication purposes from Data-Driven Science.We do not share nor sell our mailing list. Your information remains confidential. You may unsubscribe atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datadrivenscience/ship-detection."
DataProvenanceInitiative/Commercial-Flan-Collection-Chain-Of-Thought,,,,,,,https://huggingface.co/datasets/DataProvenanceInitiative/Commercial-Flan-Collection-Chain-Of-Thought,,,,,,,,,,,,,,,,,,,,DataProvenanceInitiative/Commercial-Flan-Collection-Chain-Of-Thought dataset hosted on Hugging Face and contributed by the HF Datasets community
DataStudio/OCR_handwritting_HAT2023,,,,,,,https://huggingface.co/datasets/DataStudio/OCR_handwritting_HAT2023,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""OCR_handwritting_HAT2023"" More Information needed"
DataTonic/dark_thoughts_casestudies_en_cn,,,,,,,https://huggingface.co/datasets/DataTonic/dark_thoughts_casestudies_en_cn,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dark Thoughts Case Studies Dataset (English-Chinese) This dataset contains a bilingual collection of case studies with detailed stakeholder analyses in English and Chinese. Each case study includes structured information about stakeholders and their motivations, along with comprehensive case analysis and solutions. Dataset Description Overview The dataset consists of 344,580 paired case studies in English and Chinese, with detailed stakeholder analyses andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DataTonic/dark_thoughts_casestudies_en_cn."
davanstrien/wikiart-resized-sample,,,,,,,https://huggingface.co/datasets/davanstrien/wikiart-resized-sample,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""wikiart-resized-sample"" More Information needed"
dazzle-nu/CIS435-CreditCardFraudDetection,,,,,,,https://huggingface.co/datasets/dazzle-nu/CIS435-CreditCardFraudDetection,,,,,,,,,,,,,,,,,,,,dazzle-nu/CIS435-CreditCardFraudDetection dataset hosted on Hugging Face and contributed by the HF Datasets community
dclure/laion-aesthetics-12m-umap,,,,,,,https://huggingface.co/datasets/dclure/laion-aesthetics-12m-umap,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"LAION-Aesthetics :: CLIP â†’ UMAP This dataset is a CLIP (text) â†’ UMAP embedding of the LAION-Aesthetics dataset - specifically the improved_aesthetics_6plus version, which filters the full dataset to images with scores of > 6 under the ""aesthetic"" filtering model. Thanks LAION for this amazing corpus! The dataset here includes coordinates for 3x separate UMAP fits using different values for the n_neighbors parameter - 10, 30, and 60 - which are broken out as separate columnsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dclure/laion-aesthetics-12m-umap."
Ddream-ai/InsuranceCorpus,,,,,,,https://huggingface.co/datasets/Ddream-ai/InsuranceCorpus,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Ddream-ai/InsuranceCorpus dataset hosted on Hugging Face and contributed by the HF Datasets community
DebateLabKIT/aaac,Text,General,Document,Text,"Accuracy, F1 Score",https://debatelab.github.io/journal/deepa2.html,https://huggingface.co/datasets/DebateLabKIT/aaac,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",work for deep argument analysis,,Dataset Card for Artificial Argument Analysis Corpus (AAAC) Data-Summary: DeepA2 is a modular framework for deep argument analysis. DeepA2 datasets contain comprehensive logical reconstructions of informally presented arguments in short argumentative texts. This document describes two synthetic DeepA2 datasets for artificial argument analysis: AAAC01 and AAAC02. # clone git lfs clone https://huggingface.co/datasets/debatelab/aaac import pandas as pd fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DebateLabKIT/aaac.
fancyzhx/ag_news,Text,Classification,Scientific,Label,"Accuracy, F1 Score, Precision, Recall",http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html,https://huggingface.co/datasets/fancyzhx/ag_news,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for ""ag_news"" Data-Summary: AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fancyzhx/ag_news."
fancyzhx/amazon_polarity,Text,General,General,Text,"Accuracy, F1 Score",https://registry.opendata.aws/,https://huggingface.co/datasets/fancyzhx/amazon_polarity,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Amazon Review Polarity Data-Summary: The Amazon reviews dataset consists of reviews from amazon. The data span a period of 18 years, including ~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review. Supported Tasks and Leaderboards text-classification, sentiment-classification: The dataset is mainly used for text classification: given the content and the title, predictâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fancyzhx/amazon_polarity."
google/boolq,Text,Question Answering,General,Text,"Exact Match, F1 Score",https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards,https://huggingface.co/datasets/google/boolq,inference tasks,,15942,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,Dataset Card for Boolq Data-Summary: BoolQ is a question answering dataset for yes/no questions containing 15942
legacy-datasets/c4,Text,,,,,https://huggingface.co/datasets/allenai/c4,https://huggingface.co/datasets/legacy-datasets/c4,,,,,,,,https://choosealicense.com/licenses/odc-by/,,,,,,,,,,,,"A colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: ""https://commoncrawl.org"". This is the processed version of Google's C4 dataset by AllenAI."
uoft-cs/cifar10,Image,General,General,Text,"Accuracy, F1 Score",https://www.cs.toronto.edu/~kriz/cifar.html,https://huggingface.co/datasets/uoft-cs/cifar10,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for CIFAR-10 Data-Summary: The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches mayâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/uoft-cs/cifar10."
uoft-cs/cifar100,Image,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",https://www.cs.toronto.edu/~kriz/cifar.html,https://huggingface.co/datasets/uoft-cs/cifar100,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for CIFAR-100 Data-Summary: The CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images per class. There are 500 training images and 100 testing images per class. There are 50000 training images and 10000 test images. The 100 classes are grouped into 20 superclasses. There are two labels per image - fine label (actual class) and coarse label (superclass). Supported Tasks and Leaderboards image-classification: Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/uoft-cs/cifar100."
abisee/cnn_dailymail,Text,Question Answering,General,Text,"Exact Match, F1 Score",,https://huggingface.co/datasets/abisee/cnn_dailymail,dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for CNN Dailymail Dataset Data-Summary: The CNN / DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering. Supported Tasks and Leaderboards 'summarization': Versionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/abisee/cnn_dailymail."
legacy-datasets/common_voice,Text,,,,,https://commonvoice.mozilla.org/en/datasets,https://huggingface.co/datasets/legacy-datasets/common_voice,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"Common Voice is Mozilla's initiative to help teach machines how real people speak. The dataset currently consists of 7,335 validated hours of speech in 60 languages, but weâ€™re always adding more voices and languages."
google-research-datasets/conceptual_captions,Image-Text,General,Scientific,Text,"Accuracy, F1 Score",https://ai.google.com/research/ConceptualCaptions/,https://huggingface.co/datasets/google-research-datasets/conceptual_captions,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Conceptual Captions Data-Summary: Conceptual Captions is a dataset consisting of ~3.3M images annotated with captions. In contrast with the curated style of other image caption annotations, Conceptual Caption images and their raw descriptions are harvested from the web, and therefore represent a wider variety of styles. More precisely, the raw descriptions are harvested from the Alt-text HTML attribute associated with web images. To arrive atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/google-research-datasets/conceptual_captions."
eriktks/conll2003,Text,,,,,https://www.aclweb.org/anthology/W03-0419/,https://huggingface.co/datasets/eriktks/conll2003,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2 tagging scheme, whereas the original dataset uses IOB1. For more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419"
fancyzhx/dbpedia_14,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards,https://huggingface.co/datasets/fancyzhx/dbpedia_14,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,20,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for DBpedia14 Data-Summary: The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes from DBpedia 2014. They are listed in classes.txt. From each of thse 14 ontology classes, we randomly choose 40,000 training samples and 5,000 testing samples. Therefore, the total size of the training dataset is 560,000 and testing dataset 70,000. There are 3 columns in the dataset (same for train and test splits)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fancyzhx/dbpedia_14."
google-research-datasets/disfl_qa,Text,Question Answering,Scientific,Text,"Exact Match, F1 Score",https://github.com/google-research-datasets/disfl-qa,https://huggingface.co/datasets/google-research-datasets/disfl_qa,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for DISFL-QA: A Benchmark Dataset for Understanding Disfluencies in Question Answering Data-Summary: Disfl-QA is a targeted dataset for contextual disfluencies in an information seeking setting, namely question answering over Wikipedia passages. Disfl-QA builds upon the SQuAD-v2 (Rajpurkar et al., 2018) dataset, where each question in the dev set is annotated to add a contextual disfluency using the paragraph as a source of distractors. The finalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/google-research-datasets/disfl_qa."
ibm-research/duorc,Text,General,Scientific,Text,"Accuracy, F1 Score",https://duorc.github.io/,https://huggingface.co/datasets/ibm-research/duorc,dataset of questions and answers gathered from crowdsourced AMT workers on Wikipedia and IMDb movie plots,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for duorc Data-Summary: The DuoRC dataset is an English language dataset of questions and answers gathered from crowdsourced AMT workers on Wikipedia and IMDb movie plots. The workers were given freedom to pick answer from the plots or synthesize their own answers. It contains two sub-datasets - SelfRC and ParaphraseRC. SelfRC dataset is built on Wikipedia movie plots solely. ParaphraseRC has questions written from Wikipedia movie plots and the answers areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ibm-research/duorc.
fever/fever,Text,General,General,Text,"Accuracy, F1 Score",https://fever.ai/,https://huggingface.co/datasets/fever/fever,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""fever"" Data-Summary: With billions of individual pages on the web providing information on almost every conceivable topic, we should have the ability to collect facts that answer almost every conceivable question. However, only a small fraction of this information is contained in structured sources (Wikidata, Freebase, etc.) â€“ we are therefore limited by our ability to transform free-form text to structured knowledge. There is, however, anotherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fever/fever."
nyu-mll/glue,Text,General,General,Text,"Accuracy, F1 Score",https://gluebenchmark.com/,https://huggingface.co/datasets/nyu-mll/glue,Understanding Evaluation benchmark (https://gluebenchmark,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for GLUE Data-Summary: GLUE, the General Language Understanding Evaluation benchmark (https://gluebenchmark.com/) is a collection of resources for training, evaluating, and analyzing natural language understanding systems. Supported Tasks and Leaderboards The leaderboard for the GLUE benchmark can be found at this address. It comprises the following tasks: ax A manually-curated evaluation dataset for fine-grainedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyu-mll/glue."
Hate-speech-CNERG/hatexplain,Text,,,,,,https://huggingface.co/datasets/Hate-speech-CNERG/hatexplain,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Hatexplain is the first benchmark hate speech dataset covering multiple aspects of the issue. Each post in the dataset is annotated from three different perspectives: the basic, commonly used 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based."
stanfordnlp/imdb,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",http://ai.stanford.edu/~amaas/data/sentiment/,https://huggingface.co/datasets/stanfordnlp/imdb,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for ""imdb"" Data-Summary: Large Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Supported Tasks and Leaderboards More Information Needed Languages More Information Neededâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stanfordnlp/imdb."
openslr/librispeech_asr,Text,,,,,http://www.openslr.org/12,https://huggingface.co/datasets/openslr/librispeech_asr,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"LibriSpeech is a corpus of approximately 1000 hours of read English speech with sampling rate of 16 kHz, prepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from read audiobooks from the LibriVox project, and has been carefully segmented and aligned.87"
CogComp/mc_taco,Text,,,,,https://cogcomp.seas.upenn.edu/page/resource_view/125,https://huggingface.co/datasets/CogComp/mc_taco,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"MC-TACO (Multiple Choice TemporAl COmmonsense) is a dataset of 13k question-answer pairs that require temporal commonsense comprehension. A system receives a sentence providing context information, a question designed to require temporal commonsense knowledge, and multiple candidate answers. More than one candidate answer can be plausible. The task is framed as binary classification: givent he context, the question, and the candidate answer, the task is to determine whether the candidate answer is plausible (""yes"") or not (""no"")."
facebook/mlqa,Text,,,,,https://github.com/facebookresearch/MLQA,https://huggingface.co/datasets/facebook/mlqa,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance. MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between 4 different languages on average."
nyu-mll/multi_nli,Text,General,General,Text,"Accuracy, F1 Score",https://www.nyu.edu/projects/bowman/multinli/,https://huggingface.co/datasets/nyu-mll/multi_nli,Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information,,,,,,,https://choosealicense.com/licenses/cc-by-3.0/,,1930,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Multi-Genre Natural Language Inference (MultiNLI) Data-Summary: The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. The corpus served as the basis for theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyu-mll/multi_nli."
deepmind/narrativeqa,Text,General,Document,Text,"Accuracy, F1 Score",https://arxiv.org/abs/1712.07040,https://huggingface.co/datasets/deepmind/narrativeqa,English,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Narrative QA Data-Summary: NarrativeQA is an English-lanaguage dataset of stories and corresponding questions designed to test reading comprehension, especially on long documents. Supported Tasks and Leaderboards The dataset is used to test reading comprehension. There are 2 tasks proposed in the paper: ""summaries only"" and ""stories only"", depending on whether the human-generated summary or the full story text is used to answerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deepmind/narrativeqa."
google-research-datasets/natural_questions,Text,General,UI,Text,"Accuracy, F1 Score",https://ai.google.com/research/NaturalQuestions/dataset,https://huggingface.co/datasets/google-research-datasets/natural_questions,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Natural Questions Data-Summary: The NQ corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. The inclusion of real user questions, and the requirement that solutions should read an entire page to find the answer, cause NQ to be a more realistic and challenging task than prior QA datasets. Supported Tasks andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/google-research-datasets/natural_questions."
Maluuba/newsqa,Text,,,,,https://www.microsoft.com/en-us/research/project/newsqa-dataset/,https://huggingface.co/datasets/Maluuba/newsqa,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"NewsQA is a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles."
allenai/openbookqa,Text,Reasoning,Scientific,Text,"Accuracy, F1 Score",https://allenai.org/data/open-book-qa,https://huggingface.co/datasets/allenai/openbookqa,it is expressed in,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"T5, UnifiedQA, BART",,,"Dataset Card for OpenBookQA Data-Summary: OpenBookQA aims to promote research in advanced question-answering, probing a deeper understanding of both the topic (with salient facts summarized as an open book, also provided with the dataset) and the language it is expressed in. In particular, it contains questions that require multi-step reasoning, use of additional common and commonsense knowledge, and rich text comprehension. OpenBookQA is a new kind ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/openbookqa."
ParaCrawl/para_crawl,Text,General,General,Text,"Accuracy, F1 Score",https://paracrawl.eu/releases.html,https://huggingface.co/datasets/ParaCrawl/para_crawl,s,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""para_crawl"" Data-Summary: Web-Scale Parallel Corpora for Official European Languages. Supported Tasks and Leaderboards More Information Needed Languages More Information Needed Dataset Structure Data Instances enbg Size of downloaded dataset files: 103.75 MB Size of the generated dataset: 356.54 MB Total amount of disk used: 460.27 MB An"
ybisk/piqa,Text,,,,,https://yonatanbisk.com/piqa/,https://huggingface.co/datasets/ybisk/piqa,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to state-of-the-art natural language understanding systems. The PIQA dataset introduces the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Physical commonsense knowledge is a major challenge on the road to true AI-completeness, including robots that interact with the world and understand natural language. PIQA focuses on everyday situations with a preference for atypical solutions. The dataset is inspired by instructables.com, which provides users with instructions on how to build, craft, bake, or manipulate objects using everyday materials. The underlying task is formualted as multiple choice question answering: given a question `q` and two possible solutions `s1`, `s2`, a model or a human must choose the most appropriate solution, of which exactly one is correct. The dataset is further cleaned of basic artifacts using the AFLite algorithm which is an improvement of adversarial filtering. The dataset contains 16,000"
ptb-text-only/ptb_text_only,Text,,,,,https://catalog.ldc.upenn.edu/LDC99T42,https://huggingface.co/datasets/ptb-text-only/ptb_text_only,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"This is the Penn Treebank Project: Release 2 CDROM, featuring a million words of 1989 Wall Street Journal material. This corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure."
allenai/quac,Text,,,,,https://quac.ai/,https://huggingface.co/datasets/allenai/quac,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Question Answering in Context is a dataset for modeling, understanding, and participating in information seeking dialog. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context."
allenai/ropes,Text,Reasoning,UI,Text,"Accuracy, F1 Score",https://allenai.org/data/ropes,https://huggingface.co/datasets/allenai/ropes,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"T5, UnifiedQA, BART",,,"Dataset Card for ROPES Data-Summary: ROPES (Reasoning Over Paragraph Effects in Situations) is a QA dataset which tests a system's ability to apply knowledge from a passage of text to a new situation. A system is presented a background passage containing a causal or qualitative relation(s) (e.g., ""animal pollinators increase efficiency of fertilization in flowers""), a novel situation that uses this background, and questions that require reasoning about effectsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/ropes."
vicenteor/sbu_captions,Text,,,,,https://www.cs.rice.edu/~vo9/sbucaptions/,https://huggingface.co/datasets/vicenteor/sbu_captions,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,The SBU Captioned Photo Dataset is a collection of over 1 million images with associated text descriptions extracted from Flicker.
allenai/sciq,Text,General,Scientific,Text,"Accuracy, F1 Score",https://allenai.org/data/sciq,https://huggingface.co/datasets/allenai/sciq,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-3.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""sciq"" Data-Summary: The SciQ dataset contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. For the majority of the questions, an additional paragraph with supporting evidence for the correct answer is provided. Supported Tasks and Leaderboards More Information Needed Languages More Informationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/sciq."
allenai/scitail,Text,General,Scientific,Text,"Accuracy, F1 Score",https://allenai.org/data/scitail,https://huggingface.co/datasets/allenai/scitail,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""scitail"" Data-Summary: The SciTail dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question and the correct answer choice are converted into an assertive statement to form the hypothesis. We use information retrieval to obtain relevant text from a large text corpus of web sentences, and use these sentences as a premise P. We crowdsource the annotation of such premise-hypothesis pair asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/scitail."
RobZamp/sick,Text,,,,,http://marcobaroni.org/composes/sick.html,https://huggingface.co/datasets/RobZamp/sick,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-3.0/,,,,,,,,,,,,"Shared and internationally recognized benchmarks are fundamental for the development of any computational system. We aim to help the research community working on compositional distributional semantic models (CDSMs) by providing SICK (Sentences Involving Compositional Knowldedge), a large size English benchmark tailored for them. SICK consists of about 10,000 English sentence pairs that include many"
stanfordnlp/snli,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",https://nlp.stanford.edu/projects/snli/,https://huggingface.co/datasets/stanfordnlp/snli,"inference (NLI), also known as recognizing textual entailment (RTE)",,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for SNLI Data-Summary: The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE). Supported Tasks and Leaderboards Natural Language Inference (NLI), also known as Recognizing Textual Entailmentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stanfordnlp/snli."
rajpurkar/squad,Text,Question Answering,General,Text,"Exact Match, F1 Score",https://rajpurkar.github.io/SQuAD-explorer/,https://huggingface.co/datasets/rajpurkar/squad,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for SQuAD Data-Summary: Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. SQuAD 1.1 contains 100,000+ question-answer pairs on 500+ articles. Supported Tasks and Leaderboards Questionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rajpurkar/squad."
stanfordnlp/sst2,Text,Sentiment Analysis,General,Label,"Accuracy, F1 Score",https://nlp.stanford.edu/sentiment/,https://huggingface.co/datasets/stanfordnlp/sst2,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, DistilBERT",,,"Dataset Card for [Dataset Name] Data-Summary: The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, eachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stanfordnlp/sst2."
aps/super_glue,Text,,,,,https://super.gluebenchmark.com/,https://huggingface.co/datasets/aps/super_glue,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"SuperGLUE (https://super.gluebenchmark.com/) is a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, improved resources, and a new public leaderboard."
allenai/swag,Text,Reasoning,General,Text,"Accuracy, F1 Score",https://rowanzellers.com/swag/,https://huggingface.co/datasets/allenai/swag,inference and physically grounded reasoning,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"T5, UnifiedQA, BART",,,"Dataset Card for Situations With Adversarial Generations Data-Summary: Given a partial description like ""she opened the hood of the car,"" humans can reason about the situation and anticipate what might come next (""then, she examined the engine""). SWAG (Situations With Adversarial Generations) is a large-scale dataset for this task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning. The dataset consists ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/swag."
facebook/textvqa,Text,,,,,https://textvqa.org,https://huggingface.co/datasets/facebook/textvqa,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions. TextVQA dataset contains 45,336 questions over 28,408 images from the OpenImages dataset."
CogComp/trec,Text,,,,,https://cogcomp.seas.upenn.edu/Data/QA/QC/,https://huggingface.co/datasets/CogComp/trec,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"The Text REtrieval Conference (TREC) Question Classification dataset contains 5500 labeled questions in training set and another 500 for test set. The dataset has 6 coarse class labels and 50 fine class labels. Average length of each sentence is 10, vocabulary size of 8700. Data are collected from four sources: 4,500 English questions published by USC (Hovy et al., 2001), about 500 manually constructed questions for a few rare classes, 894 TREC 8 and TREC 9 questions, and also 500 questions from TREC 10 which serves as the test set. These questions were manually labeled."
ranjaykrishna/visual_genome,Text,,,,,https://homes.cs.washington.edu/~ranjay/visualgenome/,https://huggingface.co/datasets/ranjaykrishna/visual_genome,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Visual Genome enable to model objects and relationships between objects. They collect dense annotations of objects, attributes, and relationships within each image. Specifically, the dataset contains over 108K images where each image has an average of 35 objects, 26 attributes, and 21 pairwise relationships between objects."
Salesforce/wikisql,Text,,,,,https://arxiv.org/abs/1709.00103,https://huggingface.co/datasets/Salesforce/wikisql,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,A large crowd-sourced dataset for developing natural language interfaces for relational databases
Salesforce/wikitext,Text,General,General,Text,"Accuracy, F1 Score",https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/,https://huggingface.co/datasets/Salesforce/wikitext,modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,"d version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger",,"BERT, RoBERTa, T5",,,"Dataset Card for ""wikitext"" Data-Summary: The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a farâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/wikitext."
QAngaroo/wiki_hop,Text,,,,,http://qangaroo.cs.ucl.ac.uk/,https://huggingface.co/datasets/QAngaroo/wiki_hop,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,WikiHop is open-domain and based on Wikipedia articles; the goal is to recover Wikidata information by hopping through documents. The goal is to answer text understanding queries by combining multiple facts that are spread across different documents.
microsoft/wiki_qa,Text,Question Answering,Scientific,Text,"Exact Match, F1 Score",https://www.microsoft.com/en-us/download/details.aspx?id=52419,https://huggingface.co/datasets/microsoft/wiki_qa,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for ""wiki_qa"" Data-Summary: Wiki Question Answering corpus from Microsoft. The WikiQA corpus is a publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. Supported Tasks and Leaderboards More Information Needed Languages More Information Needed Dataset Structure Data Instances default Size of downloadedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/microsoft/wiki_qa."
allenai/winogrande,Text,,,,,https://leaderboard.allenai.org/winogrande/submissions/get-started,https://huggingface.co/datasets/allenai/winogrande,,,,,,,,,,,,,,,,,,,,"WinoGrande is a new collection of 44k problems, inspired by Winograd Schema Challenge (Levesque, Davis, and Morgenstern 2011), but adjusted to improve the scale and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning."
wmt/wmt16,Text,General,General,Text,"Accuracy, F1 Score",http://www.statmt.org/wmt16/translation-task.html,https://huggingface.co/datasets/wmt/wmt16,English,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""wmt16"" Data-Summary: Warning: There are issues with the Common Crawl corpus data (training-parallel-commoncrawl.tgz): Non-English files contain many English sentences. Their ""parallel"" sentences in English are not aligned: they are uncorrelated with their counterpart. We have contacted the WMT organizers, and in response, they have indicated that they do not have plans to update the Common Crawl corpus data. Their rationaleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wmt/wmt16."
leondz/wnut_17,Text,,,,,http://noisy-text.github.io/2017/emerging-rare-entities.html,https://huggingface.co/datasets/leondz/wnut_17,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"WNUT 17: Emerging and Rare entity recognition This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarisation), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for"
facebook/xnli,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",https://www.nyu.edu/projects/bowman/xnli/,https://huggingface.co/datasets/facebook/xnli,(some low-ish resource),,,,,,,,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for ""xnli"" Data-Summary: XNLI is a subset of a few thousand"
google/xquad,Text,Question Answering,General,Text,"Exact Match, F1 Score",https://github.com/deepmind/xquad,https://huggingface.co/datasets/google/xquad,"Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, andâ€¦ See the full description on the dataset page: https://huggingface",,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for ""xquad"" Data-Summary: XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question answering performance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/google/xquad."
EdinburghNLP/xsum,Text,,,,,,https://huggingface.co/datasets/EdinburghNLP/xsum,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,Extreme Summarization (XSum) Dataset. There are three features: - document: Input news article. - summary: One sentence summary of the article. - id: BBC ID of the article.
fancyzhx/yelp_polarity,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",https://course.fast.ai/datasets,https://huggingface.co/datasets/fancyzhx/yelp_polarity,,,,,,,,,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for ""yelp_polarity"" Data-Summary: Large Yelp Review Dataset. This is a dataset for binary sentiment classification. We provide a set of 560,000 highly polar yelp reviews for training, and 38,000 for testing. ORIGIN The Yelp reviews dataset consists of reviews from Yelp. It is extracted from the Yelp Dataset Challenge 2015 data. For more information, please refer to http://www.yelp.com/dataset_challenge The Yelp reviews polarity dataset isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fancyzhx/yelp_polarity."
declare-lab/CategoricalHarmfulQA,,,,,,https://arxiv.org/abs/2402.11746,https://huggingface.co/datasets/declare-lab/CategoricalHarmfulQA,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"CatQA: A categorical harmful questions dataset CatQA is used in LLM safety realignment research: Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic (Paper, Code) How to download from datasets import load_dataset dataset = load_dataset(""declare-lab/CategoricalHarmfulQA"") What is CatQA? To comprehensively evaluate the model across a wide range of harmful categories, we construct a newâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/declare-lab/CategoricalHarmfulQA."
deep-plants/AGM,Image-Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/deep-plants/AGM,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for AGM Dataset Data-Summary: The AGM (AGricolaModerna) Dataset is a comprehensive collection of high-resolution RGB images capturing harvest-ready plants in a vertical farm setting. This dataset consists of 972,858 images, each with a resolution of 120x120 pixels, covering 18 different plant crops. In the context of this dataset, a crop refers to a plant species or a mix of plant species. Supported Tasks Image classification: plantâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deep-plants/AGM."
deepghs/game_characters,,,,,,,https://huggingface.co/datasets/deepghs/game_characters,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Database of Characters in Mobile Games All the character in the following games are supported: Arknights (crawled from https://prts.wiki) Fate/Grand Order (crawled from https://fgo.wiki) Azur Lane (crawled from https://wiki.biligame.com/blhx) Girls' Front-Line (crawled from https://iopwiki.com/) Genshin Impact (crawled from https://genshin-impact.fandom.com/ja/wiki/%E5%8E%9F%E7%A5%9E_Wiki) The source code and python library is hosted on narugo1992/gchar, and the scheduled job isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deepghs/game_characters."
deepklarity/top-flutter-packages,,,,,,,https://huggingface.co/datasets/deepklarity/top-flutter-packages,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,"Top Flutter Packages Dataset Flutter is an open source framework by Google for building beautiful, natively compiled, multi-platform applications from a single codebase. It is gaining quite a bit of popularity because of ability to code in a single language and have it running on Android/iOS and web as well. This dataset contains a snapshot of Top 5000+ flutter/dart packages hosted on Flutter package repository The dataset was scraped in August-2024. We aim to use this datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deepklarity/top-flutter-packages."
deepmind/aqua_rat,Text,General,General,Text,"Accuracy, F1 Score",https://github.com/deepmind/AQuA,https://huggingface.co/datasets/deepmind/aqua_rat,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5","generation model that learns to generate the explanation, while generating the program that solves the question",,"Dataset Card for AQUA-RAT Data-Summary: A large-scale dataset consisting of approximately 100,000 algebraic word problems. The solution to each question is explained step-by-step using natural language. This data is used to train a program generation model that learns to generate the explanation, while generating the program that solves the question. Supported Tasks and Leaderboards Languages en Dataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deepmind/aqua_rat."
deepset/prompt-injections,,,,,,,https://huggingface.co/datasets/deepset/prompt-injections,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset Card for ""deberta-v3-base-injection-dataset"" More Information needed"
deepsynthbody/deepfake_ecg,,,,,,,https://huggingface.co/datasets/deepsynthbody/deepfake_ecg,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"DeepFake electrocardiograms: the beginning of the end for privacy issues in medicine Paper GitHub Original-data-source PyPI How to download Option 1 from datasets import load_dataset dataset = load_dataset(""deepsynthbody/deepfake_ecg"") Option 2 git lfs install git clone https://huggingface.co/datasets/deepsynthbody/deepfake_ecg # if you want to clone without large files â€“ just their pointers # prepend your git clone with theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deepsynthbody/deepfake_ecg."
defunct-datasets/amazon_reviews_multi,Text,,,,,https://arxiv.org/abs/2010.02573,https://huggingface.co/datasets/defunct-datasets/amazon_reviews_multi,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"We provide an Amazon product reviews dataset for multilingual text classification. The dataset contains reviews in English, Japanese, German, French, Chinese and Spanish, collected between November 1, 2015 and November 1, 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID and the coarse-grained product category (e.g. â€˜booksâ€™, â€˜appliancesâ€™, etc.) The corpus is balanced across stars, so each star rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000 and 5,000 reviews in the training, development and test sets respectively. The maximum number of reviews per reviewer is 20 and the maximum number of reviews per product is 20. All reviews are truncated after 2,000 characters, and all reviews are at least 20 characters long. Note that the language of a review does not necessarily match the language of its marketplace (e.g. reviews from amazon.de are primarily written in German, but could also be written in English, etc.). For this reason, we applied a language detection algorithm based on the work in Bojanowski et al. (2017) to determine the language of the review text and we removed reviews that were not written in the expected language."
Delius/ChineseWebNovel,,,,,,,https://huggingface.co/datasets/Delius/ChineseWebNovel,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Chinese Web Novel Dataset Summarized by claude but converted the order for novel text extension task. WARNING!! Please be aware of the context length!!!
dell-research-harvard/headlines-semantic-similarity,Text,General,General,Text,"Accuracy, F1 Score",https://dell-research-harvard.github.io/,https://huggingface.co/datasets/dell-research-harvard/headlines-semantic-similarity,"semantic similarity dataset, containing 396,001,930 pairs of different headlines for the same newspaper article, taken from historical U",,,,,,,https://choosealicense.com/licenses/cc-by-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for HEADLINES Data-Summary: HEADLINES is a massive English-language semantic similarity dataset, containing 396,001,930 pairs of different headlines for the same newspaper article, taken from historical U.S. newspapers, covering the period 1920-1989. Languages The text in the dataset is in English. Dataset Structure Each year in the dataset is divided into a distinct file (eg. 1952_headlines.json), giving a total ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dell-research-harvard/headlines-semantic-similarity."
demelin/moral_stories,Text,,,,,https://github.com/demelin/moral_stories,https://huggingface.co/datasets/demelin/moral_stories,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Moral Stories is a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. For detailed information, see https://aclanthology.org/2021.emnlp-main.54.pdf."
Den-Intelligente-Patientjournal/Medical_word_embedding_eval,,,,,,,https://huggingface.co/datasets/Den-Intelligente-Patientjournal/Medical_word_embedding_eval,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"Danish medical word embedding evaluation The development of the dataset is described further in our paper. Citing @inproceedings{laursen-etal-2023-benchmark, title = ""Benchmark for Evaluation of {D}anish Clinical Word Embeddings"", author = ""Laursen, Martin Sundahl and Pedersen, Jannik Skyttegaard and Vinholt, Pernille Just and Hansen, Rasmus S{\o}gaard and Savarimuthu, Thiusius Rajeeth"", editor = ""Derczynski, Leon"", booktitle =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Den-Intelligente-Patientjournal/Medical_word_embedding_eval."
Den4ikAI/russian_instructions,,,,,,,https://huggingface.co/datasets/Den4ikAI/russian_instructions,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"ÐÐ¾Ð²Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ: https://huggingface.co/datasets/Den4ikAI/russian_instructions_2 Ð ÑƒÑÑÐºÐ¸Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¹ Ð¸ QA. Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°: { ""dialogue"":[ ""ÐšÐ°Ðº Ñ Ð¼Ð¾Ð³Ñƒ ÑƒÐ»ÑƒÑ‡ÑˆÐ¸Ñ‚ÑŒ ÑÐ²Ð¾ÑŽ ÑÐ²ÑÐ·ÑŒ Ð¼ÐµÐ¶Ð´Ñƒ Ñ‚ÐµÐ»Ð¾Ð¼ Ð¸ Ñ€Ð°Ð·ÑƒÐ¼Ð¾Ð¼?"", ""ÐÐ°Ñ‡Ð½Ð¸Ñ‚Ðµ Ñ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð½Ð¾Ð¹ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐ¸ Ð¾ÑÐ¾Ð·Ð½Ð°Ð½Ð½Ð¾ÑÑ‚Ð¸. 2. ÐžÐ±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÑƒÐ¹Ñ‚Ðµ Ð±Ð°Ð»Ð°Ð½Ñ Ð½Ð° Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… ÑƒÑ€Ð¾Ð²Ð½ÑÑ…: Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¾Ð¼, ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¼, ÑƒÐ¼ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð¼ Ð¸ Ð´ÑƒÑ…Ð¾Ð²Ð½Ð¾Ð¼. 3. Ð¡Ð²ÑÐ¶Ð¸Ñ‚ÐµÑÑŒ Ñ Ð¿Ñ€Ð¸Ñ€Ð¾Ð´Ð¾Ð¹, ÐºÐ¾Ð³Ð´Ð° ÑÑ‚Ð¾ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ - Ð¸Ð´Ð¸Ñ‚Ðµ Ð½Ð° Ð¿Ñ€Ð¾Ð³ÑƒÐ»ÐºÐ¸ Ð¸Ð»Ð¸ Ð±ÐµÐ³Ð°Ð¹Ñ‚Ðµ Ð½Ð° ÑƒÐ»Ð¸Ñ†Ðµ, Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ ÑÐ¸Ð´Ð¸Ñ‚Ðµ Ð² Ð¿Ð°Ñ€ÐºÐµ Ð¸â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Den4ikAI/russian_instructions."
Dendory/tarot,,,,,,,https://huggingface.co/datasets/Dendory/tarot,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"This is a dataset of 5,770 high quality tarot cards readings produced by ChatGPT based on 3 randomly drawn cards. It can be used to train smaller models for use in a tarot application. The prompt used to produce these readings was: Give me a one paragraph tarot reading if I pull the cards CARD1, CARD2 and CARD3.\n\nReading:\n The CSV dataset contains the following columns: Card 1, Card 2, Card 3, Reading There are also 2 Python scripts included: make_dataset.py: This file was used to createâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Dendory/tarot."
dennlinger/klexikon,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/dennlinger/klexikon,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for the Klexikon Dataset Version History v0.3 (2022-09-01): Removing some five samples from the dataset due to duplication conflicts with other samples. v0.2 (2022-02-28): Updated the files to no longer contain empty sections and removing otherwise empty lines at the end of files. Also removing lines with some sort of coordinate. v0.1 (2022-01-19): Initial data release on Huggingface datasets. Data-Summary: The Klexikon dataset is aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dennlinger/klexikon.
Densu341/Fresh-rotten-fruit,,,,,,,https://huggingface.co/datasets/Densu341/Fresh-rotten-fruit,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,Densu341/Fresh-rotten-fruit dataset hosted on Hugging Face and contributed by the HF Datasets community
deprem-ml/deprem_satellite_semantic_whu_dataset,,,,,,,https://huggingface.co/datasets/deprem-ml/deprem_satellite_semantic_whu_dataset,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""deprem_satellite_semantic_whu_dataset"" More Information needed"
derek-thomas/ScienceQA,Text,Question Answering,Scientific,Text,"Exact Match, F1 Score",https://scienceqa.github.io/index.html#home,https://huggingface.co/datasets/derek-thomas/ScienceQA,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card Creation Guide Data-Summary: Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering Supported Tasks and Leaderboards Multi-modal Multiple Choice Languages English Dataset Structure Data Instances Explore more samples here. {'image': Image, 'question': 'Which of these states is farthest north?', 'choices': ['West Virginia', 'Louisiana', 'Arizona'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/derek-thomas/ScienceQA."
detection-datasets/fashionpedia,Image,Detection,UI,Bounding Box/Mask,"mAP, IoU",https://fashionpedia.github.io/home/index.html,https://huggingface.co/datasets/detection-datasets/fashionpedia,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"DETR, Faster R-CNN, YOLO",,,"Dataset Card for Fashionpedia Data-Summary: Fashionpedia is a dataset mapping out the visual aspects of the fashion world. From the paper: Fashionpedia is a new dataset which consists of two parts: (1) an ontology built by fashion experts containing 27 main apparel categories, 19 apparel parts, 294 fine-grained attributes and their relationships; (2) a dataset with everyday and celebrity event fashion images annotated with segmentation masks and theirâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/detection-datasets/fashionpedia."
deutsche-telekom/NLU-few-shot-benchmark-en-de,,,,,,,https://huggingface.co/datasets/deutsche-telekom/NLU-few-shot-benchmark-en-de,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,NLU Few-shot Benchmark - English and German This is a few-shot training dataset from the domain of human-robot interaction. It contains texts in German and English language with 64 different utterances (classes). Each utterance (class) has exactly 20 samples in the training set. This leads to a total of 1280 different training samples. The dataset is intended to benchmark the intent classifiers of chat bots in English and especially in German language. We are building on ourâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deutsche-telekom/NLU-few-shot-benchmark-en-de.
dev-senolys/categories_dataset,,,,,,,https://huggingface.co/datasets/dev-senolys/categories_dataset,,,,,,,,,,2,,,,,,,,,,"Dataset Card for ""categories_dataset"" More Information needed"
dev/untitled_imgs,,,,,,,https://huggingface.co/datasets/dev/untitled_imgs,,,,,,,,,,,,,,,,,,,,dev/untitled_imgs dataset hosted on Hugging Face and contributed by the HF Datasets community
DevashishBhake/med_gen_ai,,,,,,,https://huggingface.co/datasets/DevashishBhake/med_gen_ai,,,,,,,,,,,,,,,,,,,,DevashishBhake/med_gen_ai dataset hosted on Hugging Face and contributed by the HF Datasets community
dfalbel/cran-packages,,,,,,,https://huggingface.co/datasets/dfalbel/cran-packages,,,,,,,,https://choosealicense.com/licenses/other/,,2010,,,,,,,,,,"CRAN packages dataset R and Rmd source codes for CRAN packages. The dataset has been constructed using the following steps: Downloaded latest version from all packages on CRAN (see last updated). The source code has been downloaded from the GitHub mirror. Identified the licenses from each package from their DESCRIPTION file, and classified each of them into some license_code. See the licenses.csv file. Extract R and Rmd source files from all packages and joined with theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dfalbel/cran-packages."
DFKI-SLT/few-nerd,Text,,,,,https://ningding97.github.io/fewnerd/,https://huggingface.co/datasets/DFKI-SLT/few-nerd,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, which contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities and 4,601,223 tokens. Three benchmark tasks are built, one is supervised: Few-NERD (SUP) and the other two are few-shot: Few-NERD (INTRA) and Few-NERD (INTER)."
dgrnd4/animals-10,,,,,,,https://huggingface.co/datasets/dgrnd4/animals-10,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,dgrnd4/animals-10 dataset hosted on Hugging Face and contributed by the HF Datasets community
dgslibisey/MuSiQue,,,,,,,https://huggingface.co/datasets/dgslibisey/MuSiQue,,,,,,,,,,,,,,,,,,,,dgslibisey/MuSiQue dataset hosted on Hugging Face and contributed by the HF Datasets community
Dhika/rail_defect,,,,,,,https://huggingface.co/datasets/Dhika/rail_defect,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,Dhika/rail_defect dataset hosted on Hugging Face and contributed by the HF Datasets community
dhiruHF/research_paper_multi_label_data_balanced,,,,,,,https://huggingface.co/datasets/dhiruHF/research_paper_multi_label_data_balanced,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""research_paper_multi_label_data_balanced"" More Information needed"
digitalpipelines/samantha-1.1-uncensored,,,,,,,https://huggingface.co/datasets/digitalpipelines/samantha-1.1-uncensored,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,This dataset is based on ehartford/samantha-data that was used to create ehartford/samantha-1.1-llama-7b and other samantha models. It has been unfiltered and uncensored.
DigitalUmuganda/kinyarwanda-english-machine-translation-dataset,,,,,,,https://huggingface.co/datasets/DigitalUmuganda/kinyarwanda-english-machine-translation-dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Kinyarwanda English Parallel Datasets for Machine translation A 48,000 Kinyarwanda English Parallel datasets for machine translation, made by curating and translating normal Kinyarwanda sentences into English"
dikw/hh_rlhf_cn,,,,,,,https://huggingface.co/datasets/dikw/hh_rlhf_cn,,,,,,,,https://choosealicense.com/licenses/llama2/,,,,,,,,,,,,hh-rlhfä¸­æ–‡ç¿»è¯‘ç‰ˆæœ¬ åŸºäºŽAnthropicè®ºæ–‡Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback å¼€æºçš„helpful å’Œharmlessæ•°æ®ï¼Œä½¿ç”¨ç¿»è¯‘å·¥å…·è¿›è¡Œäº†ç¿»è¯‘ã€‚hh_rlhf_train.jsonl åˆå¹¶ä¸­è‹±æ–‡è®­ç»ƒé›†æ•°æ® æ¸…æ´—è¿‡åŽ17ä¸‡æ¡hh_rlhf_test.jsonl åˆå¹¶ä¸­è‹±æ–‡æµ‹è¯•é›†æ•°æ® æ¸…æ´—è¿‡åŽ9åƒæ¡harmless_base_cn_train.jsonl 42394æ¡harmless_base_cn_test.jsonl 2304æ¡helpful_base_cn_train.jsonl 43722æ¡helpful_base_cn_test.jsonl 2346æ¡ å®žéªŒæŠ¥å‘Š ç›¸å…³rlhfå®žéªŒæŠ¥å‘Š:https://zhuanlan.zhihu.com/p/652044120
dimanchkek/Deepfacelive-DFM-Models,,,,,,,https://huggingface.co/datasets/dimanchkek/Deepfacelive-DFM-Models,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,"Description Here you can find files for DeepFaceLab(It's back!) and DeepFaceLive. All sources and active community members are listed below. Disclaimer The author of this repository makes no claim to the data uploaded here other than that created by himself. Feel free to open a discussion for me to mention your contacts if I haven't done so. Quick usage guide To use the models presented in the repository, you will need installed DeepFaceLive.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/dimanchkek/Deepfacelive-DFM-Models."
din0s/asqa,Text,Question Answering,General,Text,"Exact Match, F1 Score",https://arxiv.org/abs/2204.06092,https://huggingface.co/datasets/din0s/asqa,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for ASQA Data-Summary: ASQA is the first long-form question answering dataset that focuses on ambiguous factoid questions. Different from previous long-form answers datasets, each question is annotated with both long-form answers and extractive question-answer pairs, which should be answerable by the generated passage. A generated long-form answer will be evaluated using both ROUGE and QA accuracy. In the paper, we show that these evaluation metricsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/din0s/asqa."
dipudl/research-paper-tokenized-dataset,,,,,,,https://huggingface.co/datasets/dipudl/research-paper-tokenized-dataset,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""research-paper-tokenized-dataset"" More Information needed"
dirtycomputer/chinese_lyrics,,,,,,,https://huggingface.co/datasets/dirtycomputer/chinese_lyrics,,,,,,,,,,,,,,,,,,,,dirtycomputer/chinese_lyrics dataset hosted on Hugging Face and contributed by the HF Datasets community
djghosh/wds_country211_test,,,,,,https://arxiv.org/abs/2103.00020,https://huggingface.co/datasets/djghosh/wds_country211_test,,,,,,,,,,,,,,,,,,,,"Country-211 (Test set only) Original paper: Learning Transferable Visual Models From Natural Language Supervision Homepage: https://github.com/openai/CLIP/blob/main/data/country211.md Derived from YFCC100M: https://multimediacommons.wordpress.com/yfcc100m-core-dataset/ Bibtex: @article{DBLP:journals/corr/abs-2103-00020, author = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/djghosh/wds_country211_test."
dmayhem93/agieval-sat-math,,,,,,https://arxiv.org/abs/2304.06364,https://huggingface.co/datasets/dmayhem93/agieval-sat-math,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Card for ""agieval-sat-math"" Dataset taken from https://github.com/microsoft/AGIEval and processed as in that repo. MIT License Copyright (c) Microsoft Corporation. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sellâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dmayhem93/agieval-sat-math."
DMetaSoul/chinese-semantic-textual-similarity,,,,,,,https://huggingface.co/datasets/DMetaSoul/chinese-semantic-textual-similarity,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"ä¸ºäº†å¯¹ like-BERT é¢„è®­ç»ƒæ¨¡åž‹è¿›è¡Œ fine-tune è°ƒä¼˜å’Œè¯„æµ‹ä»¥å¾—åˆ°æ›´å¥½çš„æ–‡æœ¬è¡¨å¾æ¨¡ï¼Œå¯¹ä¸šç•Œå¼€æºçš„è¯­ä¹‰ç›¸ä¼¼ï¼ˆSTSï¼‰ã€è‡ªç„¶è¯­è¨€æŽ¨ç†ï¼ˆNLIï¼‰ã€é—®é¢˜åŒ¹é…ï¼ˆQMCï¼‰ä»¥åŠç›¸å…³æ€§ç­‰æ•°æ®é›†è¿›è¡Œäº†æœé›†æ•´ç†ï¼Œå…·ä½“ä»‹ç»å¦‚ä¸‹ï¼š ç±»åž‹ æ•°æ®é›† ç®€ä»‹ è§„æ¨¡ é€šç”¨é¢†åŸŸ OCNLI åŽŸç”Ÿä¸­æ–‡è‡ªç„¶è¯­è¨€æŽ¨ç†æ•°æ®é›†ï¼Œæ˜¯ç¬¬ä¸€ä¸ªéžç¿»è¯‘çš„ã€ä½¿ç”¨åŽŸç”Ÿæ±‰è¯­çš„å¤§åž‹ä¸­æ–‡è‡ªç„¶è¯­è¨€æŽ¨ç†æ•°æ®é›†ã€‚OCNLIä¸ºä¸­æ–‡è¯­è¨€ç†è§£åŸºå‡†æµ‹è¯„ï¼ˆCLUEï¼‰çš„ä¸€éƒ¨åˆ†ã€‚ Train: 50437, Dev: 2950 CMNLI ç¿»è¯‘è‡ªè‹±æ–‡è‡ªç„¶è¯­è¨€æŽ¨ç†æ•°æ®é›† XNLI å’Œ MNLIï¼Œæ›¾ç»æ˜¯ä¸­æ–‡è¯­è¨€ç†è§£åŸºå‡†æµ‹è¯„ï¼ˆCLUEï¼‰çš„ä¸€éƒ¨åˆ†ï¼ŒçŽ°åœ¨è¢« OCNLI å–ä»£ã€‚ Train: 391783, Dev: 12241 CSNLI ç¿»è¯‘è‡ªè‹±æ–‡è‡ªç„¶è¯­è¨€æŽ¨ç†æ•°æ®é›† SNLIã€‚ Train: 545833, Dev: 9314, Test: 9176 STS-B-Chinese ç¿»è¯‘è‡ªè‹±æ–‡è¯­ä¹‰ç›¸ä¼¼æ•°æ®é›† STSbenchmarkã€‚ Train: 5231, Dev: 1458, Test: 1361 PAWS-Xâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DMetaSoul/chinese-semantic-textual-similarity."
Dmini/FFHQ-64x64,,,,,,,https://huggingface.co/datasets/Dmini/FFHQ-64x64,,,,,,,,,,,,,,,,,,,,Dmini/FFHQ-64x64 dataset hosted on Hugging Face and contributed by the HF Datasets community
dnagpt/human_genome_GCF_009914755.1,,,,,,,https://huggingface.co/datasets/dnagpt/human_genome_GCF_009914755.1,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset Card for ""human_genome_GCF_009914755.1"" how to build this data: human full genome data from: https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_009914755.1/ Preprocess: 1 download data use ncbi data set tools: curl -o datasets 'https://ftp.ncbi.nlm.nih.gov/pub/datasets/command-line/LATEST/linux-amd64/datasets' chmod +x datasets ./datasets download genome accession GCF_000001405.40 --filename genomes/human_genome_dataset.zip then move the gene data to human2.fra 2 write theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dnagpt/human_genome_GCF_009914755.1."
docz1105/ComBack,,,,,,,https://huggingface.co/datasets/docz1105/ComBack,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"ComBack: A Versatile Dataset for Enhancing Compiler Backend Development Efficiency ComBack is a large-scale multi-platform compiler backend code dataset. It is sourced from GCC and LLVM backends corresponding to 178 target platforms. Dataset Information Source Data GCC Category Target Platform Function KLoC CPU 30 35,147 647.2 MPU 33 6,010 183.9 GPU 2 457 11.2 VLIW 5 959 25.4 DSP 3 399 9.6 Virtual 4 327 6.5 SUM 77 43,299 883.7 LLVMâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/docz1105/ComBack."
domenicrosati/TruthfulQA,Text,General,General,Text,"Accuracy, F1 Score",https://github.com/sylinrl/TruthfulQA,https://huggingface.co/datasets/domenicrosati/TruthfulQA,model is truthful in generating answers to questions,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for TruthfulQA Data-Summary: TruthfulQA: Measuring How Models Mimic Human Falsehoods We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating falseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/domenicrosati/TruthfulQA."
dominguesm/Canarim-Instruct-PTBR-Dataset,,,,,,,https://huggingface.co/datasets/dominguesm/Canarim-Instruct-PTBR-Dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"ðŸ¥ ðŸ‡§ðŸ‡· Canarim Instruct Dataset [ðŸ± Github] What's Canarim? Canarim is a dataset with over 300,000 instructions in Portuguese, ranging from simple instructions like ""Descreva os efeitos do aquecimento global"" to more complex instructions like ""Nesta tarefa, vocÃª precisa ser capaz de resumir uma determinada lista de pontos-chave"" where additional context is provided. Why it's called Canarim? ""Canarim"" is spoken in some regions of Brazilâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dominguesm/Canarim-Instruct-PTBR-Dataset."
donfu/oa-stackexchange,,,,,,,https://huggingface.co/datasets/donfu/oa-stackexchange,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"Stackexchange Instructions for OpenAssistant This dataset is taken from https://archive.org/details/stackexchange. There's a single parquet file combining all stackexchange sites. The threads have been filtered as follows: only threads with an accepted answer, for which both the question and response is less than 1000 characters have been choosen. Other answers, or questions without accepted answers, or long entries have been droppped. Each row consists of INSTRUCTION RESPONSEâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/donfu/oa-stackexchange."
dpshade22/midiFiles,,,,,,,https://huggingface.co/datasets/dpshade22/midiFiles,,,,,,,,,,,,,,,,,,,,dpshade22/midiFiles dataset hosted on Hugging Face and contributed by the HF Datasets community
DragonFire0159x/nijijourney-images,,,,,,,https://huggingface.co/datasets/DragonFire0159x/nijijourney-images,,,,,,,,,,,,,,,,,,,,"DragonFire0159x/nijijourney-images Dataset with images generated by niji-journey Contains only images, no prompts What's in the repository Here are the archives with different dataset sizes For"
Dragunflie-420/FMADataset,,,,,,,https://huggingface.co/datasets/Dragunflie-420/FMADataset,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Free Music Archive (FMA) Dataset Overview This repository contains the Free Music Archive (FMA) dataset, curated and made available on Hugging Face by dragunflie-420. The FMA dataset is a large-scale, open-source dataset of music tracks, designed for music information retrieval and machine learning tasks. Dataset Description The Free Music Archive (FMA) is an open and easily accessible dataset consisting of full-length audio tracks with associatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Dragunflie-420/FMADataset."
dream-textures/textures-normal-1k,Image-Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/dream-textures/textures-normal-1k,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"textures-normal-1k Data-Summary: The textures-normal-1k dataset is an image dataset of 1000+ normal map textures in 512x512 resolution with associated text descriptions. The dataset was created for training/fine-tuning models for text to image tasks. It contains a combination of CC0 procedural and photoscanned PBR materials from ambientCG. Languages The text descriptions are in English, and created by joining the tags of each material with a spaceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dream-textures/textures-normal-1k."
dreamerdeo/finqa,,,,,,,https://huggingface.co/datasets/dreamerdeo/finqa,,,,,,,,,,,,,,,,,,,,dataset_info: features: name: id dtype: string name: post_text sequence: string name: pre_text sequence: string name: question dtype: string name: answers dtype: string name: table sequence: sequence: string splits: name: train num_bytes: 26984130 num_
DReAMy-lib/DreamBank-dreams-en,,,,,,,https://huggingface.co/datasets/DReAMy-lib/DreamBank-dreams-en,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"DreamBank - Dreams The dataset is a collection of ~20 k textual reports of dreams, originally scraped from the DreamBank databased by mattbierner. The DreamBank reports are divided into series, which are collections of individuals or research projects/groups that have gathered the dreams. Content The dataset revolves around three main features: dreams: the content of each dream report. series: the series to which a report belongs description: a briefâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DReAMy-lib/DreamBank-dreams-en."
DropletX/DropletVideo-10M,,,,,,https://arxiv.org/abs/2503.06053,https://huggingface.co/datasets/DropletX/DropletVideo-10M,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"ðŸ” Dataset Note: DropletVideo-1M is the premium subset of DropletVideo-10M, filtered with aesthetic score > 4.51 and image quality score > 7.51. âœˆï¸ Introduction The challenge of spatiotemporal consistency has long existed in the field of video generation. We have released the open-source dataset DropletVideo-10M â€”the world's largest video generation dataset with spatiotemporal consistency. Itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DropletX/DropletVideo-10M."
Drozdik/tattoo_v3,,,,,,,https://huggingface.co/datasets/Drozdik/tattoo_v3,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""tattoo_v3"" More Information needed"
drt/complex_web_questions,Text,,,,,https://www.tau-nlp.sites.tau.ac.il/compwebq,https://huggingface.co/datasets/drt/complex_web_questions,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"ComplexWebQuestions is a dataset for answering complex questions that require reasoning over multiple web snippets. It contains a large set of complex questions in natural language, and can be used in multiple ways: 1) By interacting with a search engine, which is the focus of our paper (Talmor and Berant, 2018); 2) As a reading comprehension task: we release 12,725,989 web snippets that are relevant for the questions, and were collected during the development of our model; 3) As a semantic parsing task: each question is paired with a SPARQL query that can be executed against Freebase to retrieve the answer."
ds4sd/DocLayNet,Text,,,,,https://developer.ibm.com/exchanges/data/all/doclaynet/,https://huggingface.co/datasets/ds4sd/DocLayNet,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,DocLayNet is a human-annotated document layout segmentation dataset from a broad variety of document sources.
DucHaiten/anime-SDXL,,,,,,,https://huggingface.co/datasets/DucHaiten/anime-SDXL,,,,,,,,https://choosealicense.com/licenses/creativeml-openrail-m/,,,,,,,,,,,,DucHaiten/anime-SDXL dataset hosted on Hugging Face and contributed by the HF Datasets community
dumitrescustefan/ro_sent,Text,,,,,https://github.com/dumitrescustefan/Romanian-Transformers/tree/examples/examples/sentiment_analysis,https://huggingface.co/datasets/dumitrescustefan/ro_sent,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,https://github.com/dumitrescustefan/Romanian-Transformers/tree/examples/examples/sentiment_analysis,,,,,,,,"This dataset is a Romanian Sentiment Analysis dataset. It is present in a processed form, as used by the authors of `Romanian Transformers` in their"
duongtruongbinh/ViVQA-X,,,,,,,https://huggingface.co/datasets/duongtruongbinh/ViVQA-X,,,,,,,,,,,,,,,,,,,,"Dataset Card for ViVQA-X Dataset Description ViVQA-X is the Vietnamese version of the VQA-X dataset, designed for tasks in VQA-NLE in the Vietnamese language. This dataset was created to support research in Vietnamese VQA, offering translated"
duongttr/vi-dataset-for-pretrain,,,,,,,https://huggingface.co/datasets/duongttr/vi-dataset-for-pretrain,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""vi-dataset-for-pretrain"" This is a combination of multiple Vietnamese dataset for pretraining CLMs such as GPT, GPT2, etc. The dataset consists of: vietgpt/covid_19_news_vi hieunguyen1053/binhvq-news-corpus oscar (unshuffled_deduplicated_vi) vietgpt/wikipedia_vi Dataset info Splits N.o"
Duxiaoman-DI/FinanceIQ,,,,,,,https://huggingface.co/datasets/Duxiaoman-DI/FinanceIQ,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,Duxiaoman-DI/FinanceIQ dataset hosted on Hugging Face and contributed by the HF Datasets community
duxprajapati/symptom-disease-dataset,,,,,,,https://huggingface.co/datasets/duxprajapati/symptom-disease-dataset,,,,,,,,,,,,,,,,,,,,duxprajapati/symptom-disease-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
dvilasuero/jailbreak-classification-reasoning-eval,,,,,,,https://huggingface.co/datasets/dvilasuero/jailbreak-classification-reasoning-eval,,,,,,,,,,3,,,,,,,,,,Eval models for classification on your own data This dataset contains the results of evaluating reasoning models for classification. It contains the pipeline and the code to run it. You can tune the config to run different prompts over your HF datasets. Results Model Accuracy Total Correct Empty qwq32b-classification 92.00% 100 92 1 r1-classification 91.00% 100 91 2 llama70-classification 77.00% 10077 10 How to run it The pipeline usesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dvilasuero/jailbreak-classification-reasoning-eval.
dvsth/LEGIT-VIPER-Jigsaw-Toxic-Comment-Perturbed,,,,,,,https://huggingface.co/datasets/dvsth/LEGIT-VIPER-Jigsaw-Toxic-Comment-Perturbed,,,,,,,,,,3,,,,,,,,,,dvsth/LEGIT-VIPER-Jigsaw-Toxic-Comment-Perturbed dataset hosted on Hugging Face and contributed by the HF Datasets community
dwisaji/indonesia-telecomunication-sentiment-dataset,,,,,,,https://huggingface.co/datasets/dwisaji/indonesia-telecomunication-sentiment-dataset,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Dataset Contain sentimen for Indonesia Communication Industry. Source from Twitter and manually annotated in prodigy spacy
dylanchia111/GeneticsLlama2Train,,,,,,,https://huggingface.co/datasets/dylanchia111/GeneticsLlama2Train,,,,,,,,,,,,,,,,,,,,dylanchia111/GeneticsLlama2Train dataset hosted on Hugging Face and contributed by the HF Datasets community
DynamicSuperb/EnvironmentalSoundClassification_ESC50-NaturalSoundscapesAndWaterSounds,,,,,,,https://huggingface.co/datasets/DynamicSuperb/EnvironmentalSoundClassification_ESC50-NaturalSoundscapesAndWaterSounds,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""environmental_sound_classification_natural_soundscapes_and_water_sounds_ESC50"" More Information needed"
Dynosaur/dynosaur-full,,,,,,,https://huggingface.co/datasets/Dynosaur/dynosaur-full,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Dynosaur/dynosaur-full dataset hosted on Hugging Face and contributed by the HF Datasets community
e9t/nsmc,Text,,,,,https://github.com/e9t/nsmc/,https://huggingface.co/datasets/e9t/nsmc,,,,,,,,https://choosealicense.com/licenses/cc-by-2.0/,,,,,,,,,,,,"This is a movie review dataset in the Korean language. Reviews were scraped from Naver movies. The dataset construction is based on the method noted in Large movie review dataset from Maas et al., 2011."
EarthnDusk/Huggingface_Uploader,,,,,,,https://huggingface.co/datasets/EarthnDusk/Huggingface_Uploader,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"ðŸš€ Hugging Face Uploader: Streamline Your Model Sharing! ðŸš€ This tool provides a user-friendly way to upload files directly to your Hugging Face repositories. Whether you prefer the interactive environment of a Jupyter Notebook or the command-line efficiency of a Python script, we've got you covered. We've designed it to streamline your workflow and make sharing your models, datasets, and spaces easier than ever before! Will be more consistently updated here:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/EarthnDusk/Huggingface_Uploader."
ecnu-icalk/educhat-sft-002-data-osm,,,,,,,https://huggingface.co/datasets/ecnu-icalk/educhat-sft-002-data-osm,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,æ¯æ¡æ•°æ®ç”±ä¸€ä¸ªå­˜æ”¾å¯¹è¯çš„listå’Œä¸Žæ•°æ®å¯¹åº”çš„system_promptç»„æˆã€‚listä¸­æŒ‰ç…§Qï¼ŒAé¡ºåºå­˜æ”¾å¯¹è¯ã€‚ æ•°æ®æ¥æºä¸ºå¼€æºæ•°æ®ï¼Œä½¿ç”¨CleanToolæ•°æ®æ¸…ç†å·¥å…·åŽ»é‡ã€‚
ecosystems/keywords,,,,,,,https://huggingface.co/datasets/ecosystems/keywords,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,ecosystems/keywords dataset hosted on Hugging Face and contributed by the HF Datasets community
eddsterxyz/Raiders-Of-The-Lost-Kek,,,,,,https://arxiv.org/abs/2001.07487,https://huggingface.co/datasets/eddsterxyz/Raiders-Of-The-Lost-Kek,,,,,,,,,,,,,,,,,,,,"Raiders Of The Lost Kek The largest 4chan /pol/ dataset. I extracted the post content, removed HTML nonesense, and 4chan specific things like post number replies in text, etc. There are a few sizes of datasets available 100kLines - first 100,000 lines of text from the dataset 300kLines - first 300,000 lines of text from the dataset 500kLines - first 500,000 lines of text from the dataset maybe at some point once i have the compute ill upload the whole thing linkâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/eddsterxyz/Raiders-Of-The-Lost-Kek."
edinburghcstr/ami,,,,,,https://groups.inf.ed.ac.uk/ami/corpus/,https://huggingface.co/datasets/edinburghcstr/ami,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"The AMI Meeting Corpus consists of 100 hours of meeting recordings. The recordings use a range of signals synchronized to a common timeline. These include close-talking and far-field microphones, individual and room-view video cameras, and output from a slide projector and an electronic whiteboard. During the meetings, the participants also have unsynchronized pens available to them that record what is written. The meetings were recorded in English using three different rooms with different acoustic properties, and include mostly non-native speakers. \n"
EdinburghNLP/xsum,Text,,,,,,https://huggingface.co/datasets/EdinburghNLP/xsum,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,Extreme Summarization (XSum) Dataset. There are three features: - document: Input news article. - summary: One sentence summary of the article. - id: BBC ID of the article.
Edoh/manim_python,,,,,,,https://huggingface.co/datasets/Edoh/manim_python,,,,,,,,https://choosealicense.com/licenses/creativeml-openrail-m/,,,,,,,,,,,,Edoh/manim_python dataset hosted on Hugging Face and contributed by the HF Datasets community
edwixx/brazilian-portuguese-TTS,,,,,,,https://huggingface.co/datasets/edwixx/brazilian-portuguese-TTS,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,edwixx/brazilian-portuguese-TTS dataset hosted on Hugging Face and contributed by the HF Datasets community
efederici/autonlp-data-Ita-Summarization,,,,,,,https://huggingface.co/datasets/efederici/autonlp-data-Ita-Summarization,,,,,,,,,,,,,,,,,,,,efederici/autonlp-data-Ita-Summarization dataset hosted on Hugging Face and contributed by the HF Datasets community
EgilKarlsen/ApacheAccessLabeled,,,,,,,https://huggingface.co/datasets/EgilKarlsen/ApacheAccessLabeled,,,,,,,,,,,,,,,,,,,,Indonesian Dataset Apache Access
EgorShibaev/TikZ-short-code,,,,,,,https://huggingface.co/datasets/EgorShibaev/TikZ-short-code,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""TikZ-short-code"" More Information needed"
ehovy/race,Text,General,General,Text,"Accuracy, F1 Score",http://www.cs.cmu.edu/~glai1/data/race/,https://huggingface.co/datasets/ehovy/race,English,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""race"" Data-Summary: RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. The dataset can be served as the training and test sets for machine comprehension. Supported Tasks and Leaderboards More Information Needed Languages Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ehovy/race."
Eitanli/goodreads,,,,,,,https://huggingface.co/datasets/Eitanli/goodreads,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""goodreads"" Must-read books summary Features: Book - Name of the book. Soemtimes this includes the details of the Series it belongs to inside a parenthesis. This information can be further extracted to analyse only series. Author - Name of the book's Author Description - The book's description as mentioned on Goodreads Genres - Multiple Genres as classified on Goodreads. Could be useful for Multi-label classification or Content based recommendation and Clustering. Averageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Eitanli/goodreads."
Ekimetrics/ipcc-ar6,,,,,,,https://huggingface.co/datasets/Ekimetrics/ipcc-ar6,,,,,,,,,,,,,,,,,,,,Ekimetrics/ipcc-ar6 dataset hosted on Hugging Face and contributed by the HF Datasets community
elenanereiss/german-ler,Text,,,,,https://github.com/elenanereiss/Legal-Entity-Recognition,https://huggingface.co/datasets/elenanereiss/german-ler,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"A dataset of Legal Documents from German federal court decisions for Named Entity Recognition. The dataset is human-annotated with 19 fine-grained entity classes. The dataset consists of approx. 67,000 sentences and contains 54,000 annotated entities."
EleutherAI/pile,Text,,,,,https://pile.eleuther.ai/,https://huggingface.co/datasets/EleutherAI/pile,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together."
Eliahu/ModelAtlasData,,,,,,https://arxiv.org/abs/2503.10633,https://huggingface.co/datasets/Eliahu/ModelAtlasData,,,,,,,,,,,,,,,,,,,,Dataset Card for Model Atlas - Data The Model Atlas Dataset contains the processed data used in the paper Charting and Navigating Hugging Face's Model Atlas. ðŸŒ Homepage | ðŸ—ºï¸ Demo | ðŸ“ƒ Paper | ðŸ§‘â€ðŸ’» Code | âœ‰ï¸ Point of Contact Dataset Description This dataset is derived from the original hub-stats dataset and has undergone several preprocessing steps specifically tailored for the Model Atlas: Removed unnecessary columns. Extracted relevant attributes fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Eliahu/ModelAtlasData.
elihoole/asrs-aviation-reports,Text,General,General,Text,"Accuracy, F1 Score",https://huggingface.co/datasets/elihoole/asrs-aviation-reports],https://huggingface.co/datasets/elihoole/asrs-aviation-reports,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ASRS Aviation Incident Reports Data-Summary: This dataset collects 47,723 aviation incident reports published in the Aviation Safety Reporting System (ASRS) database maintained by NASA. Supported Tasks and Leaderboards 'summarization': Dataset can be used to train a model for abstractive and extractive summarization. The model performance is measured by how high the output summary's ROUGE score for a given narrative account ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/elihoole/asrs-aviation-reports."
ElKulako/stocktwits-crypto,,,,,,,https://huggingface.co/datasets/ElKulako/stocktwits-crypto,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"For academic reference, cite the following paper: https://ieeexplore.ieee.org/document/10223689 Dataset StockTwits-crypto contains all cryptocurrency-related posts from the StockTwits website, from 1st of November 2021 to the 15th of June 2022. The data has been cleaned and preprocessed, we removed: cashtags, hashtags, usernames, URLs, crypto wallets, Chinese, Korean and Japanese characters, (most) UTF-8 encoding issues removed all posts shorter than 4 words removed all duplicate postsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ElKulako/stocktwits-crypto."
ellljoy/interior-design,,,,,,,https://huggingface.co/datasets/ellljoy/interior-design,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,ellljoy/interior-design dataset hosted on Hugging Face and contributed by the HF Datasets community
Elnagara/hard,Text,General,General,Text,"Accuracy, F1 Score",https://github.com/elnagara/HARD-Arabic-Dataset,https://huggingface.co/datasets/Elnagara/hard,Arabic,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Hard Data-Summary: This dataset contains 93,700 hotel reviews in Arabic language.The hotel reviews were collected from Booking.com website during June/July 2016.The reviews are expressed in Modern Standard Arabic as well as dialectal Arabic.The following table summarize some tatistics on the HARD Dataset. Supported Tasks and Leaderboards [More Information Needed] Languages The dataset is based on Arabic.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Elnagara/hard."
eloukas/edgar-corpus,Text,,,,,https://arxiv.org/abs/2109.14394,https://huggingface.co/datasets/eloukas/edgar-corpus,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,The dataset contains annual filings (10K) of all publicly traded firms from 1993-2020. The table data is stripped but all text is retained. This dataset allows easy access to the EDGAR-CORPUS dataset based on the paper EDGAR-CORPUS: Billions of Tokens Make The World Go Round (See References in README.md for details).
elyza/ELYZA-tasks-100,,,,,,https://arxiv.org/abs/2307.09288,https://huggingface.co/datasets/elyza/ELYZA-tasks-100,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,ELYZA-tasks-100: æ—¥æœ¬èªžinstructionãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ Data Description æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯instruction-tuningã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚è©³ç´°ã¯ ãƒªãƒªãƒ¼ã‚¹ã®noteè¨˜äº‹ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ ç‰¹å¾´: è¤‡é›‘ãªæŒ‡ç¤ºãƒ»ã‚¿ã‚¹ã‚¯ã‚’å«ã‚€100ä»¶ã®æ—¥æœ¬èªžãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚ å½¹ã«ç«‹ã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€ä¸å¯§ãªå‡ºåŠ›ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚ å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦è©•ä¾¡è¦³ç‚¹ãŒã‚¢ãƒŽãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚Œã¦ãŠã‚Šã€è©•ä¾¡ã®æºã‚‰ãŽã‚’æŠ‘ãˆã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚ å…·ä½“çš„ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¿ã‚¹ã‚¯ã‚’å«ã¿ã¾ã™ã€‚ è¦ç´„ã‚’ä¿®æ­£ã—ã€ä¿®æ­£ç®‡æ‰€ã‚’èª¬æ˜Žã™ã‚‹ã‚¿ã‚¹ã‚¯ å…·ä½“çš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‹ã‚‰æŠ½è±¡çš„ãªæ•™è¨“ã‚’è¿°ã¹ã‚‹ã‚¿ã‚¹ã‚¯ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã‚’æ±²ã¿å½¹ã«ç«‹ã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦æŒ¯ã‚‹èˆžã†ã‚¿ã‚¹ã‚¯ å ´åˆåˆ†ã‘ã‚’å¿…è¦ã¨ã™ã‚‹è¤‡é›‘ãªç®—æ•°ã®ã‚¿ã‚¹ã‚¯ æœªçŸ¥ã®è¨€èªžã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºã—æ—¥æœ¬èªžè¨³ã™ã‚‹é«˜åº¦ãªæŽ¨è«–ã‚’å¿…è¦ã¨ã™ã‚‹ã‚¿ã‚¹ã‚¯ è¤‡æ•°ã®æŒ‡ç¤ºã‚’è¸ã¾ãˆãŸä¸Šã§youtubeã®å¯¾è©±ã‚’ç”Ÿæˆã™ã‚‹ã‚¿ã‚¹ã‚¯ æž¶ç©ºã®ç”Ÿãç‰©ã‚„ç†Ÿèªžã«é–¢ã™ã‚‹ç”Ÿæˆãƒ»å¤§å–œåˆ©ãªã©ã®æƒ³åƒåŠ›ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¿ã‚¹ã‚¯â€¦ See the full description on the dataset page: https://huggingface.co/datasets/elyza/ELYZA-tasks-100.
Embodied-Vision-Language-Model/ShareRobot,,,,,,,https://huggingface.co/datasets/Embodied-Vision-Language-Model/ShareRobot,,,,,,,,,,,,,,,,,,,,Embodied-Vision-Language-Model/ShareRobot dataset hosted on Hugging Face and contributed by the HF Datasets community
emergentorder/StarTrekMemoryAlpha20230216,,,,,,,https://huggingface.co/datasets/emergentorder/StarTrekMemoryAlpha20230216,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,emergentorder/StarTrekMemoryAlpha20230216 dataset hosted on Hugging Face and contributed by the HF Datasets community
Emilianohack6950/Wikipedia-es,,,,,,,https://huggingface.co/datasets/Emilianohack6950/Wikipedia-es,,,,,,,,,,,,,,,,,,,,Emilianohack6950/Wikipedia-es dataset hosted on Hugging Face and contributed by the HF Datasets community
EmoCareAI/Psych8k,,,,,,,https://huggingface.co/datasets/EmoCareAI/Psych8k,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"The data used for this project comes from ~260 real conversations in counseling recordings (in English). The transcripts of these recordings were used as the primary source for building the training and testing datasets. These conversations cover a variety of topics, including emotion, family, relationship, career development, academic stress, etc Recent News [2024-04-07] After obtaining access permission, please do not disseminate data at willï¼"
emotone-ar-cicling2017/emotone_ar,Video,Detection,General,Bounding Box/Mask,"mAP, IoU",,https://huggingface.co/datasets/emotone-ar-cicling2017/emotone_ar,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"DETR, Faster R-CNN, YOLO",,,Dataset Card for Emotional Tone in Arabic Data-Summary: Dataset of 10065 tweets in Arabic for Emotion detection in Arabic text Supported Tasks and Leaderboards [More Information Needed] Languages The dataset is based on Arabic. Dataset Structure Data Instances
emozilla/pg19,,,,,,,https://huggingface.co/datasets/emozilla/pg19,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""pg19"" Paraquet version of pg19 Statistics (in # of characters): total_len: 11425076324, average_len: 399450.2595622684"
emre/finance-reasoning-turkish,,,,,,,https://huggingface.co/datasets/emre/finance-reasoning-turkish,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,"Dataset Card for Turkish Advanced Reasoning Dataset (Finance Q&A) License This dataset is licensed under the Academic Use Only License. It is intended solely for academic and research purposes. Commercial use is strictly prohibited. For more details, refer to the LICENSE file. Citation: If you use this dataset in your research, please cite it as follows: @dataset{turkish_advanced_reasoning_finance_qa, title = {Turkish Advanced Reasoning Dataset for Finance Q\&A}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/emre/finance-reasoning-turkish."
enesxgrahovac/the-feynman-lectures-on-physics,,,,,,,https://huggingface.co/datasets/enesxgrahovac/the-feynman-lectures-on-physics,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""the-feynman-lectures-on-physics"" More Information needed"
enryu43/twitter100m_tweets,,,,,,,https://huggingface.co/datasets/enryu43/twitter100m_tweets,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""twitter100m_tweets"" Dataset with tweets for this post."
Enxin/MovieChat-1K_train,,,,,,,https://huggingface.co/datasets/Enxin/MovieChat-1K_train,,,,,,,,,,,,,,,,,,,,Enxin/MovieChat-1K_train dataset hosted on Hugging Face and contributed by the HF Datasets community
erayyildiz/turkish_ner,Text,,,,,http://arxiv.org/abs/1702.02363,https://huggingface.co/datasets/erayyildiz/turkish_ner,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Turkish Wikipedia Named-Entity Recognition and Text Categorization (TWNERTC) dataset is a collection of automatically categorized and annotated sentences obtained from Wikipedia. The authors constructed large-scale gazetteers by using a graph crawler algorithm to extract relevant entity and domain information from a semantic knowledge base, Freebase. The constructed gazetteers contains approximately 300K entities with thousands of fine-grained entity types under 77 different domains."
erenfazlioglu/turkishvoicedataset,,,,,,,https://huggingface.co/datasets/erenfazlioglu/turkishvoicedataset,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Dataset Card for ""turkishneuralvoice"" Dataset Overview Dataset Name: Turkish Neural Voice Description: This dataset contains Turkish audio samples generated using Microsoft Text to Speech services. The dataset includes audio files and their corresponding transcriptions. Dataset Structure Configs: default Data Files: Split: train Path: data/train-* Dataset Info: Features: audio: Audio file transcription: Corresponding text transcriptionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/erenfazlioglu/turkishvoicedataset."
ereverter/cnn_dailymail_extractive,,,,,,https://arxiv.org/abs/1903.10318,https://huggingface.co/datasets/ereverter/cnn_dailymail_extractive,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Data Card for Extractive CNN/DailyMail Dataset Overview This is an extractive version of the CNN/Dailymail dataset. The structure of this dataset is identical to the original except for a minor modification in the data representation and the introduction of labels to denote the extractive summary. The labels are generated following a greedy algorithm, as proposed by Liu (2019). The curation process can be found in the bertsum-hf repository. I am uploading it in caseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ereverter/cnn_dailymail_extractive."
ErfanMoosaviMonazzah/fake-news-detection-dataset-English,,,,,,,https://huggingface.co/datasets/ErfanMoosaviMonazzah/fake-news-detection-dataset-English,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,This is a cleaned and splitted version of this dataset (https://www.kaggle.com/datasets/sadikaljarif/fake-news-detection-dataset-english) Labels: Fake News: 0 Real News: 1 You can find the cleansing script at: https://github.com/ErfanMoosaviMonazzah/Fake-News-Detection
ergotts/propositional-logic,,,,,,,https://huggingface.co/datasets/ergotts/propositional-logic,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset: Argument to Logical Structure Conversion (Propositional Logic) Column Descriptions natural_language_argument: The original argument written in plain English, presented in a natural, narrative style. premises_and_conclusions: A breakdown of the argument into clearly stated premises and conclusion. natural_language_proof: A structured explanation in natural language that connects the premises to the conclusion logically. symbols: A dictionary of symbolicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ergotts/propositional-logic."
ErickP/IndiaPoliceEvents,,,,,,,https://huggingface.co/datasets/ErickP/IndiaPoliceEvents,,,,,,,,,,,,,,,,,,,,ErickP/IndiaPoliceEvents dataset hosted on Hugging Face and contributed by the HF Datasets community
EricLu/SCP-116K,Text,,,,,https://arxiv.org/abs/2501.15587,https://huggingface.co/datasets/EricLu/SCP-116K,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"Dataset Card for SCP-116K Recent Updates We have made significant updates to the dataset, which are summarized below: Expansion with Mathematics Data:Added over 150,000 new math-related problem-solution pairs, bringing the total number of"
eriktks/conll2003,Text,,,,,https://www.aclweb.org/anthology/W03-0419/,https://huggingface.co/datasets/eriktks/conll2003,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2 tagging scheme, whereas the original dataset uses IOB1. For more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419"
ernie-ai/image-text-examples-ar-cn-latin-notext,,,,,,,https://huggingface.co/datasets/ernie-ai/image-text-examples-ar-cn-latin-notext,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""image-text-"
erogluegemen/TDK_Turkish_Words,,,,,,,https://huggingface.co/datasets/erogluegemen/TDK_Turkish_Words,,,,,,,,,,,,,,,,,,,,"This dataset contains a collection of Turkish dictionary definitions extracted from the official website of the Turkish Language Association (TDK). It provides comprehensive definitions for a wide range of Turkish words and phrases. The dataset is intended to be a valuable resource for researchers, linguists, language enthusiasts, and anyone interested in the Turkish language. It can be used for various purposes, such as natural language processing tasks, language analysis, and educationalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/erogluegemen/TDK_Turkish_Words."
esb/datasets,,,,,,,https://huggingface.co/datasets/esb/datasets,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,2,,,,,,,,,"All eight of datasets in ESB can be downloaded and prepared in just a single line of code through the Hugging Face Datasets library: from datasets import load_dataset librispeech = load_dataset(""esb/datasets"", ""librispeech"", split=""train"") ""esb/datasets"": the repository namespace. This is fixed for all ESB datasets. ""librispeech"": the dataset name. This can be changed to any of any one of the eight datasets in ESB to download that dataset. split=""train"": the split. Set this to one ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/esb/datasets."
esdurmus/wiki_lingua,Image-Text,Summarization,UI,Text,"ROUGE, BLEU, BERTScore",https://arxiv.org/abs/2010.03093,https://huggingface.co/datasets/esdurmus/wiki_lingua,"from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors",,,,,,,https://choosealicense.com/licenses/cc-by-3.0/,,,,,,,,,"BART, T5, Pegasus, ProphetNet",,,"Dataset Card for ""wiki_lingua"" Data-Summary: We introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of cross-lingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. We create gold-standard article-summary alignments across languages by aligning the images that are used toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/esdurmus/wiki_lingua."
espejelomar/code_search_net_python_10000_examples,,,,,,,https://huggingface.co/datasets/espejelomar/code_search_net_python_10000_examples,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,espejelomar/code_search_net_python_10000_
ethz/food101,Image,General,General,Text,"Accuracy, F1 Score",https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/,https://huggingface.co/datasets/ethz/food101,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,"d, and thus still contain some amount of noise",,"BERT, RoBERTa, T5",,,"Dataset Card for Food-101 Data-Summary: This dataset consists of 101 food categories, with 101'000 images. For each class, 250 manually reviewed test images are provided as well as 750 training images. On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels. Supportedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ethz/food101."
eugenesiow/Div2k,Text,,,,,https://data.vision.ee.ethz.ch/cvl/DIV2K/,https://huggingface.co/datasets/eugenesiow/Div2k,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,DIV2K dataset: DIVerse 2K resolution high quality images as used for the challenges @ NTIRE (CVPR 2017 and CVPR 2018) and @ PIRM (ECCV 2018)
EunsuKim/MergedBench,,,,,,,https://huggingface.co/datasets/EunsuKim/MergedBench,,,,,,,,,,,,,,,,,,,,EunsuKim/MergedBench dataset hosted on Hugging Face and contributed by the HF Datasets community
eusip/silicone,Text,,,,,,https://huggingface.co/datasets/eusip/silicone,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"The Sequence labellIng evaLuatIon benChmark fOr spoken laNguagE (SILICONE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems specifically designed for spoken language. All datasets are in the English language and cover a variety of domains including daily life, scripted scenarios, joint task completion, phone call conversations, and televsion dialogue. Some datasets additionally include emotion and/or sentimant labels."
evanarlian/imagenet_1k_resized_256,,,,,,,https://huggingface.co/datasets/evanarlian/imagenet_1k_resized_256,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"Dataset Card for ""imagenet_1k_resized_256"" Dataset summary The same ImageNet dataset but all the smaller side resized to 256. A lot of pretraining workflows contain resizing images to 256 and random cropping to 224x224, this is why 256 is chosen. The resized dataset can also be downloaded much faster and consume less space than the original one. See here for detailed readme. Dataset Structure Below is the"
EvanGranthamBrown/arxiv-keywords,,,,,,,https://huggingface.co/datasets/EvanGranthamBrown/arxiv-keywords,,,,,,,,,,,,,,,,,,,,EvanGranthamBrown/arxiv-keywords dataset hosted on Hugging Face and contributed by the HF Datasets community
explodinggradients/ragas-webgpt,,,,,,,https://huggingface.co/datasets/explodinggradients/ragas-webgpt,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""ragas-webgpt"" More Information needed"
Exploration-Lab/CISLR,,,,,,,https://huggingface.co/datasets/Exploration-Lab/CISLR,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,"CISLR: Corpus for Indian Sign Language Recognition This repository contains the Indian Sign Language Dataset proposed in the following paper Paper: CISLR: Corpus for Indian Sign Language Recognition https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.707/ Authors: Abhinav Joshi, Ashwani Bhat, Pradeep S, Priya Gole, Shashwat Gupta, Shreyansh Agarwal, Ashutosh Modi Abstract: Indian Sign Language, though used by a diverse community, still lacks well-annotatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Exploration-Lab/CISLR."
facebook/multilingual_librispeech,Audio,General,Scientific,Text,"Accuracy, F1 Score",http://www.openslr.org/94,https://huggingface.co/datasets/facebook/multilingual_librispeech,"- English, German, Dutch, Spanish, French, Italian, Portuguese, Polish",,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for MultiLingual LibriSpeech Data-Summary: This is a streamable version of the Multilingual LibriSpeech (MLS) dataset. The data archives were restructured from the original ones from OpenSLR to make it easier to stream. MLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/facebook/multilingual_librispeech."
fajrikoto/id_liputan6,Text,,,,,https://indolem.github.io/,https://huggingface.co/datasets/fajrikoto/id_liputan6,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"In this paper, we introduce a large-scale Indonesian summarization dataset. We harvest articles from this http URL, an online news portal, and obtain 215,827 document-summary pairs. We leverage pre-trained language models to develop benchmark extractive and abstractive summarization methods over the dataset with multilingual and monolingual BERT-based models. We include a thorough error analysis by examining machine-generated summaries that have low ROUGE scores, and expose both issues with ROUGE it-self, as well as with extractive and abstractive summarization models."
Fakermiya/nsfw-sfw,,,,,,,https://huggingface.co/datasets/Fakermiya/nsfw-sfw,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,Fakermiya/nsfw-sfw dataset hosted on Hugging Face and contributed by the HF Datasets community
Fakhraddin/khatt,,,,,,,https://huggingface.co/datasets/Fakhraddin/khatt,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""khatt"" More Information needed"
Falah/classification_arabic_dialects,,,,,,,https://huggingface.co/datasets/Falah/classification_arabic_dialects,,,,,,,,,,,,,,,,,,,,"Classification of Arabic Dialects Audio Dataset This dataset contains audio samples of various Arabic dialects for the task of classification and recognition. The dataset aims to assist researchers and practitioners in developing models and systems for Arabic spoken language analysis and understanding. Dataset Details Dataset Name: Classification of Arabic Dialects Audio Dataset Dataset URL: Falah/classification_arabic_dialects Dataset Size: 166,407,297 bytesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Falah/classification_arabic_dialects."
Falio/NANAMI,,,,,,,https://huggingface.co/datasets/Falio/NANAMI,,,,,,,,,,,,,,,,,,,,Falio/NANAMI dataset hosted on Hugging Face and contributed by the HF Datasets community
fancyzhx/amazon_polarity,Text,General,General,Text,"Accuracy, F1 Score",https://registry.opendata.aws/,https://huggingface.co/datasets/fancyzhx/amazon_polarity,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Amazon Review Polarity Data-Summary: The Amazon reviews dataset consists of reviews from amazon. The data span a period of 18 years, including ~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review. Supported Tasks and Leaderboards text-classification, sentiment-classification: The dataset is mainly used for text classification: given the content and the title, predictâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fancyzhx/amazon_polarity."
FanqingM/MM-Eureka-Dataset,,,,,,,https://huggingface.co/datasets/FanqingM/MM-Eureka-Dataset,,,,,,,,,,,,,,,,,,,,FanqingM/MM-Eureka-Dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
fantasyfish/laion-art,,,,,,,https://huggingface.co/datasets/fantasyfish/laion-art,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""laion-art"" More Information needed"
FarisHijazi/kajiwoto.ai-chat,,,,,,,https://huggingface.co/datasets/FarisHijazi/kajiwoto.ai-chat,,,,,,,,,,,,,,,,,,,,"This is an NSFW roleplay dataset scraped from https://kajiwoto.ai/ as of 2023-07-15.Kajiwoto is a platform where you can create your own character datasets and chat with them.There are many public datasets in Kajiwoto, the power in this dataset is the metadata, there is so much information and categorization for each dataset. Processing data Do be aware that a lot of the data is NSFW (explicit content) The raw datasets are in kajiwoto_raw.json, this data needs to be processed soâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FarisHijazi/kajiwoto.ai-chat."
FastJobs/Visual_Emotional_Analysis,,,,,,,https://huggingface.co/datasets/FastJobs/Visual_Emotional_Analysis,,,,,,,,,,,,,,,,,,,,FastJobs/Visual_Emotional_Analysis dataset hosted on Hugging Face and contributed by the HF Datasets community
fatdove/Iris_Database,,,,,,https://arxiv.org/abs/2503.11930,https://huggingface.co/datasets/fatdove/Iris_Database,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Synthetic Iris Image Dataset Overview This repository contains a dataset of synthetic colored iris images generated using diffusion models based on our paper ""Synthetic Iris Image Generation Using Diffusion Networks."" The dataset comprises 17,695 high-quality synthetic iris images designed to be biometrically unique from the training data while maintaining realistic iris pigmentation distributions. In this repository we contain about 10000 filtered iris images with theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fatdove/Iris_Database."
fatmaElsafoury2022/SST_sentiment_fairness_data,,,,,,,https://huggingface.co/datasets/fatmaElsafoury2022/SST_sentiment_fairness_data,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,Sentiment fairness dataset ================================ This dataset is to measure gender fairness in the downstream task of sentiment analysis. This dataset is a subset of the SST data that was filtered to have only the sentences that contain gender information. The python code used to create this dataset can be found in the prepare_sst.ipyth file. Then the filtered datset was labeled by 4 human annotators who are the authors of this dataset. The annotationsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fatmaElsafoury2022/SST_sentiment_fairness_data.
fddemarco/pushshift-reddit,,,,,,,https://huggingface.co/datasets/fddemarco/pushshift-reddit,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""pushshift-reddit"" More Information needed"
Fearao/guba_eastmoney,,,,,,,https://huggingface.co/datasets/Fearao/guba_eastmoney,,,,,,,,,,,,,,,,,,,,æ•°æ®æ¥è‡ªä¸œæ–¹è´¢å¯Œè‚¡å§çš„è¯„è®ºï¼Œç»è¿‡äººå·¥label
Fece228/latin-literature-dataset-170M,,,,,,,https://huggingface.co/datasets/Fece228/latin-literature-dataset-170M,,,,,,,,,,,,,,,,,,,,"This is a dataset collected from all the texts available at Corpus Corporum, which includes probably all the literary works ever written in Latin. The dataset is split in two parts: preprocessed with basic cltk tools, ready for work, and raw text data. It must be noted, however, that the latter contains text in Greek, Hebrew, and other languages, with references and contractions"
felipebandeira/invoiceupload1,,,,,,,https://huggingface.co/datasets/felipebandeira/invoiceupload1,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,felipebandeira/invoiceupload1 dataset hosted on Hugging Face and contributed by the HF Datasets community
Fhrozen/FSD50k,,,,,,https://zenodo.org/record/4060432,https://huggingface.co/datasets/Fhrozen/FSD50k,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Freesound Dataset 50k (FSD50K) Important This data set is a copy from the original one located at Zenodo. Citation If you use the FSD50K dataset, or part of it, please cite our paper: Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font, Xavier Serra. ""FSD50K: an Open Dataset of Human-Labeled Sound Events"", arXiv 2020. Data curators Eduardo Fonseca, Xavier Favory, Jordi Pons, Mercedes Collado, Ceren Can, Rachit Gupta, Javierâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Fhrozen/FSD50k."
fibonacciai/fibonacci-2025,,,,,,,https://huggingface.co/datasets/fibonacciai/fibonacci-2025,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,fibonacciai/fibonacci-2025 dataset hosted on Hugging Face and contributed by the HF Datasets community
FIdo-AI/ua-news,,,,,,,https://huggingface.co/datasets/FIdo-AI/ua-news,,,,,,,,,,,,,,,,,,,,FIdo-AI/ua-news dataset hosted on Hugging Face and contributed by the HF Datasets community
finetrainers/squish-pika,,,,,,,https://huggingface.co/datasets/finetrainers/squish-pika,,,,,,,,,,,,,,,,,,,,"This dataset was generated with Pika with its ""Squish it"" effect. The generated videos were captioned with Qwen2VL: Code from transformers import Qwen2VLForConditionalGeneration, AutoProcessor import torch import os from pathlib import Path from huggingface_hub import snapshot_download from torchvision import io model = Qwen2VLForConditionalGeneration.from_pretrained(""Qwen/Qwen2-VL-7B-Instruct"", device_map=""auto"") processor = AutoProcessor.from_pretrained(""Qwen/Qwen2-VL-7B-Instruct"")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/finetrainers/squish-pika."
FinLang/investopedia-embedding-dataset,,,,,,,https://huggingface.co/datasets/FinLang/investopedia-embedding-dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,Dataset Card for investopedia-embedding dataset We curate a dataset of substantial size pertaining to finance from Investopedia using a new technique that leverages unstructured scraping data and LLM to generate structured data that is suitable for fine-tuning embedding models. The dataset generation uses a new method of self-verification that ensures that the generated question-answer pairs and not hallucinated by the LLM with high probability. Dataset Descriptionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FinLang/investopedia-embedding-dataset.
fireblade2534/Gemini-2.0-Flash-Fenrir-Voice,,,,,,,https://huggingface.co/datasets/fireblade2534/Gemini-2.0-Flash-Fenrir-Voice,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,fireblade2534/Gemini-2.0-Flash-Fenrir-Voice dataset hosted on Hugging Face and contributed by the HF Datasets community
FiscalNote/billsum,Text,Summarization,General,Text,"ROUGE, BLEU, BERTScore",https://github.com/FiscalNote/BillSum,https://huggingface.co/datasets/FiscalNote/billsum,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,"BART, T5, Pegasus, ProphetNet",,,"Dataset Card for ""billsum"" Data-Summary: BillSum, summarization of US Congressional and California state bills. There are several features: text: bill text. summary: summary of the bills. title: title of the bills. features for us bills. ca bills does not have. text_len: number of chars in text. sum_len: number of chars in summary. Supported Tasks and Leaderboards More Information Needed Languages More Information Neededâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FiscalNote/billsum."
flaviagiammarino/path-vqa,Text,,,,,https://arxiv.org/abs/2003.10286,https://huggingface.co/datasets/flaviagiammarino/path-vqa,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Card for PathVQA Dataset Description PathVQA is a dataset of question-answer pairs on pathology images. The dataset is intended to be used for training and testing Medical Visual Question Answering (VQA) systems. The dataset includes both open-ended questions and binary ""yes/no"" questions. The dataset is built from two publicly-available pathology textbooks: ""Textbook of Pathology"" and ""Basic Pathology"", and a publicly-available digital library:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/flaviagiammarino/path-vqa."
Flmc/DISC-Med-SFT,,,,,,,https://huggingface.co/datasets/Flmc/DISC-Med-SFT,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,This is a repository containing a subset of the DISC-Med-SFT Dataset. Check DISC-MedLLM for more information.
flowfree/crypto-news-headlines,,,,,,,https://huggingface.co/datasets/flowfree/crypto-news-headlines,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""crypto-news-headlines"" More Information needed"
flxclxc/encoded_drug_reviews,,,,,,,https://huggingface.co/datasets/flxclxc/encoded_drug_reviews,,,,,,,,,,,,,,,,,,,,flxclxc/encoded_drug_reviews dataset hosted on Hugging Face and contributed by the HF Datasets community
fmars/wiki_stem,,,,,,,https://huggingface.co/datasets/fmars/wiki_stem,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,fmars/wiki_stem dataset hosted on Hugging Face and contributed by the HF Datasets community
fnlp/moss-002-sft-data,Text,General,General,Text,"Accuracy, F1 Score",https://txsun1997.github.io/blogs/moss.html,https://huggingface.co/datasets/fnlp/moss-002-sft-data,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""moss-002-sft-data"" Data-Summary: An open-source conversational dataset that was used to train MOSS-002. The user prompts are extended based on a small set of human-written seed prompts in a way similar to Self-Instruct. The AI responses are generated using text-davinci-003. The user prompts of en_harmlessness are from Anthropic red teaming data. Data Splits name # samples en_helpfulness.json 419049 en_honesty.jsonâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fnlp/moss-002-sft-data."
foreverbeliever/OmniMedVQA,,,,,,https://arxiv.org/abs/2402.09181,https://huggingface.co/datasets/foreverbeliever/OmniMedVQA,,,,,,,,,,,,,,,,,,,,"OmniMedVQA We introduce OmniMedVQA, large-scale and comprehensive Visual Question Answering benchmark tailored to the medical domain. This benchmark is collected from 73 different medical datasets, contains 118,010 images with 127,995 QA-items, covering 12 different medical image modalities and referring to more than 20 human anatomical regions. Importantly, all images in this benchmark are sourced from authentic medical scenarios, ensuring alignment with the requirements ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/foreverbeliever/OmniMedVQA."
forgeml/viton_hd,,,,,,,https://huggingface.co/datasets/forgeml/viton_hd,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""viton_hd"" More Information needed"
forta/malicious-smart-contract-dataset,,,,,,,https://huggingface.co/datasets/forta/malicious-smart-contract-dataset,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Malicious Smart Contract Classification Dataset This dataset includes malicious and benign smart contracts deployed on Ethereum. Code used to collect this data: data collection notebook For more details on how this dataset can be used, please check out this blog: How Fortaâ€™s Predictive ML Models Detect Attacks Before Exploitation"
fourthbrain-demo/reddit-comments-demo,,,,,,,https://huggingface.co/datasets/fourthbrain-demo/reddit-comments-demo,,,,,,,,,,,,,,,,,,,,fourthbrain-demo/reddit-comments-demo dataset hosted on Hugging Face and contributed by the HF Datasets community
Francesco/construction-safety-gsnvb,Text,General,General,Text,"Accuracy, F1 Score",https://universe.roboflow.com/object-detection/construction-safety-gsnvb,https://huggingface.co/datasets/Francesco/construction-safety-gsnvb,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for construction-safety-gsnvb ** The original COCO dataset is stored at dataset.tar.gz** Data-Summary: construction-safety-gsnvb Supported Tasks and Leaderboards object-detection: The dataset can be used to train a model for Object Detection. Languages English Dataset Structure Data Instances A data point comprises an image and its object annotations. { 'image_id': 15, 'image':â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Francesco/construction-safety-gsnvb."
franz96521/scientific_papers,,,,,,,https://huggingface.co/datasets/franz96521/scientific_papers,,,,,,,,,,,,,,,,,,,,franz96521/scientific_papers dataset hosted on Hugging Face and contributed by the HF Datasets community
freddyaboulton/gradio-theme-subdomains,,,,,,,https://huggingface.co/datasets/freddyaboulton/gradio-theme-subdomains,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,freddyaboulton/gradio-theme-subdomains dataset hosted on Hugging Face and contributed by the HF Datasets community
FredZhang7/krea-ai-prompts,,,,,,,https://huggingface.co/datasets/FredZhang7/krea-ai-prompts,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,See DistilGPT2 Stable Diffusion
FreedomIntelligence/Medical-R1-Distill-Data-Chinese,,,,,,https://arxiv.org/abs/2412.18925,https://huggingface.co/datasets/FreedomIntelligence/Medical-R1-Distill-Data-Chinese,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Introduction This dataset is an SFT dataset distilled from Deepseek-R1 (Full Power Version), based on Chinese medical verifiable problems from HuatuoGPT-o1. The distillation originates from the native Deepseek-R1 API requests. We hope this distilled dataset can help initialize your models with the reasoning chain from R1. You can also use our previously built medical verified long reasoning chains based on GPT-4o on medical-o1-reasoning-SFT. For details, see our paper and GitHubâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/Medical-R1-Distill-Data-Chinese."
FremyCompany/BioLORD-Dataset,,,,,,https://arxiv.org/abs/2210.11892,https://huggingface.co/datasets/FremyCompany/BioLORD-Dataset,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"The BioLORD Dataset (v1) This dataset was constructed to enable training text embedding models producing similar representations for biomedical concept names and their definitions. Pairs of biomedical concepts names and descriptions of the concept are contrasted against each other, such that the model becomes able to find which names and descriptions are paired together within a batch. Citation This dataset accompanies the BioLORD: Learning Ontologicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FremyCompany/BioLORD-Dataset."
fridriik/mental-health-arg-post-quarantine-covid19-dataset,Text,General,Scientific,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/fridriik/mental-health-arg-post-quarantine-covid19-dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Mental health of people in Argentina post quarantine COVID-19 Dataset Data-Summary: Dataset modified for research from: Levels and predictors of depression, anxiety, and suicidal risk during COVID-19 pandemic in Argentina: The impacts of quarantine extensions on mental health state created by LÃ³pez Steinmetz, Lorena Cecilia for Universidad Nacional de CÃ³rdoba. Facultad de PsicologÃ­a; Argentina. Consejo Nacional de Investigaciones CientÃ­ficas y TÃ©cnicas.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fridriik/mental-health-arg-post-quarantine-covid19-dataset."
fringek/BigVideo,,,,,,,https://huggingface.co/datasets/fringek/BigVideo,,,,,,,,,,,,,,,,,,,,Please email us (liyankang@stu.xmu.edu.cn) to explain your identity and purpose before requesting access. Directly requesting will not be approved. Please make sure that all data are used for research only. Github: https://github.com/DeepLearnXMU/BigVideo-VMT
FronkonGames/steam-games-dataset,,,,,,,https://huggingface.co/datasets/FronkonGames/steam-games-dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Overview Information of more than 85,000 games published on Steam. Maintained by Fronkon Games. This dataset has been created with this code (MIT) and use the API provided by Steam, the largest gaming platform on PC. Data is also collected from Steam Spy. Only published games, no DLCs, episodes, music, videos, etc. Here is a simple"
frp94/progan_val,,,,,,,https://huggingface.co/datasets/frp94/progan_val,,,,,,,,,,,,,,,,,,,,Dataset Card for Dataset Name This dataset card aims to be a base template for new datasets. It has been generated using this raw template. Dataset Details Dataset Description Curated by: [More Information Needed] Funded by [optional]: [More Information Needed] Shared by [optional]: [More Information Needed] Language(s) (NLP): [More Information Needed] License: [More Information Needed] Dataset Sources [optional]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/frp94/progan_val.
frtna/jwt300_mt,,,,,,,https://huggingface.co/datasets/frtna/jwt300_mt,,,,,,,,,,,,,,,,,,,,This new dataset is designed to be used in the scope of machine translation project.
Fsoft-AIC/the-vault-function,Text,,,,,https://arxiv.org/abs/2305.06156,https://huggingface.co/datasets/Fsoft-AIC/the-vault-function,,,,,,,,https://choosealicense.com/licenses/mit/,,,,https://github.com/FSoft-AI4Code/TheVault/blob/main/data/README.md,,,,,,,,"The Vault is a multilingual code-text dataset with over 40 million pairs covering 10 popular programming languages. It is the largest corpus containing parallel code-text data. By building upon The Stack, a massive raw code sample collection, the Vault offers a comprehensive and clean resource for advancing research in code understanding and generation. It provides a high-quality dataset that includes code-text pairs at multiple levels, such as class and inline-level, in addition to the function level. The Vault can serve many purposes at multiple levels."
fujiki/wiki40b_ja,,,,,,,https://huggingface.co/datasets/fujiki/wiki40b_ja,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"This dataset is a reformatted version of the Japanese portion of wiki40b dataset. When you use this dataset, please cite the original paper: @inproceedings{guo-etal-2020-wiki, title = ""{W}iki-40{B}: Multilingual Language Model Dataset"", author = ""Guo, Mandy and Dai, Zihang and Vrande{\v{c}}i{\'c}, Denny and Al-Rfou, Rami"", booktitle = ""Proceedings of the Twelfth Language Resources and Evaluation Conference"", month = may, year = ""2020"", address =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fujiki/wiki40b_ja."
furusu/aesthetic_score_danbooru2024,,,,,,,https://huggingface.co/datasets/furusu/aesthetic_score_danbooru2024,,,,,,,,,,,,,,,,,,,,dataset:https://huggingface.co/datasets/deepghs/danbooru2024-webp-4Mpixel scorer:https://github.com/discus0434/aesthetic-predictor-v2-5
fusing/wikiart_captions,,,,,,,https://huggingface.co/datasets/fusing/wikiart_captions,,,,,,,,,,,,,,,,,,,,fusing/wikiart_captions dataset hosted on Hugging Face and contributed by the HF Datasets community
gabeorlanski/tp3,Text,,,,,https://arxiv.org/abs/2302.01973,https://huggingface.co/datasets/gabeorlanski/tp3,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,https://github.com/google-research/babelcode,,,,,,,,"Translating Python Programming Puzzles (TP3) is a code translation benchmark created from the verification functions from the questions in the original Python Programming Puzzles dataset (Schuster et al., 2021) to create this dataset. These functions are hand-crafted by the authors and are used to check if an answer satisfies the constraints of the puzzle. These puzzles range in difficulty from basic character checking to competitive programming problems. Thus, each verification function is written by an expert python programmer and requires a significant understanding of programming to translate. In total, there are 370 python functions to translate."
Gaborandi/Brain_Tumor_pubmed_abstracts,,,,,,,https://huggingface.co/datasets/Gaborandi/Brain_Tumor_pubmed_abstracts,,,,,,,,,,4,,,,,,,,,,"This Dataset has been downloaded from PubMed It has abstracts and titles that are related to Brain Tumors the data has been cleaned before uploading it could be used for any NLP task, such as Domain Adaptation"
gabrielrstan/CORAA-v1.1,,,,,,https://arxiv.org/abs/2110.15731,https://huggingface.co/datasets/gabrielrstan/CORAA-v1.1,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"CORAA-v1.1 CORAA is a publicly available dataset for Automatic Speech Recognition (ASR) in the Brazilian Portuguese language containing 290.77 hours of audios and their respective transcriptions (400k+ segmented audios). The dataset is composed of audios of 5 original projects: ALIP (GonÃ§alves, 2019) C-ORAL Brazil (Raso and Mello, 2012) NURC-Recife (Oliviera Jr., 2016) SP-2010 (Mendes and Oushiro, 2012) TEDx talks (talks in Portuguese) The audios were either validated byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gabrielrstan/CORAA-v1.1."
gagan3012/IAM,,,,,,,https://huggingface.co/datasets/gagan3012/IAM,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""IAM"" More Information needed"
gaia-benchmark/GAIA,,,,,,https://arxiv.org/abs/2311.12983,https://huggingface.co/datasets/gaia-benchmark/GAIA,,,,,,,,,,,,,,,,,,,,"GAIA dataset GAIA is a benchmark which aims at evaluating next-generation LLMs (LLMs with augmented capabilities due to added tooling, efficient prompting, access to search, etc). We added gating to prevent bots from scraping the dataset. Please do not reshare the validation or test set in a crawlable format. Data and leaderboard GAIA is made of more than 450 non-trivial question with an unambiguous answer, requiring different levels of tooling and autonomy to solve. Itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gaia-benchmark/GAIA."
GAIR/lima,,,,,,https://arxiv.org/abs/2305.11206,https://huggingface.co/datasets/GAIR/lima,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,A high-quality dataset for efficient instruction tuning.
GalaktischeGurke/emails_5500_to_7500,,,,,,,https://huggingface.co/datasets/GalaktischeGurke/emails_5500_to_7500,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""emails_5500_to_7500"" More Information needed"
gamino/wiki_medical_terms,Text,General,UI,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/gamino/wiki_medical_terms,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for [Dataset Name] Data-Summary: This data set contains over 6,000 medical terms and their wikipedia text. It is intended to be used on a downstream task that requires medical terms and their wikipedia explanation. Dataset Structure Data Instances [More Information Needed] Data Fields [More Information Needed] Data Splits [More Information Needed] Dataset Creationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gamino/wiki_medical_terms."
ganchengguang/resume_seven_class,,,,,,https://arxiv.org/abs/2208.03219,https://huggingface.co/datasets/ganchengguang/resume_seven_class,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"This is a resume sentence classification dataset constructed based on resume text.ï¼ˆhttps://www.kaggle.com/datasets/oo7kartik/resume-text-batchï¼‰The dataset have seven category.(experience education knowledge project others ) And three element label(header content meta).Because the dataset is a published paper, if you want to use this dataset in a paper or work, please cite following paper.https://arxiv.org/abs/2208.03219 And dataset use in article https://arxiv.org/abs/2209.09450"
garage-bAInd/Open-Platypus,,,,,,https://arxiv.org/abs/2308.07317,https://huggingface.co/datasets/garage-bAInd/Open-Platypus,,,,,,,,,,,,,,,,,,,,"Open-Platypus This dataset is focused on improving LLM logical reasoning skills and was used to train the Platypus2 models. It is comprised of the following datasets, which were filtered using keyword search and then Sentence Transformers to remove questions with a similarity above 80%: Dataset Name License Type PRM800K MIT MATH MIT ScienceQA Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International SciBench MIT ReClor Non-commercial TheoremQAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/garage-bAInd/Open-Platypus."
Gare/Classical_Chinese_to_Modern_Chinese,,,,,,,https://huggingface.co/datasets/Gare/Classical_Chinese_to_Modern_Chinese,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Gare/Classical_Chinese_to_Modern_Chinese dataset hosted on Hugging Face and contributed by the HF Datasets community
garythung/trashnet,,,,,,,https://huggingface.co/datasets/garythung/trashnet,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,garythung/trashnet dataset hosted on Hugging Face and contributed by the HF Datasets community
GateNLP/broad_twitter_corpus,Text,,,,,https://github.com/GateNLP/broad_twitter_corpus,https://huggingface.co/datasets/GateNLP/broad_twitter_corpus,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"This is the Broad Twitter corpus, a dataset of tweets collected over stratified times, places and social uses. The goal is to represent a broad range of activities, giving a dataset more representative of the language used in this hardest of social media formats to process. Further, the BTC is annotated for named entities. For more details see [https://aclanthology.org/C16-1111/](https://aclanthology.org/C16-1111/)"
gauravshrm211/VC-startup-evaluation-for-investment,,,,,,,https://huggingface.co/datasets/gauravshrm211/VC-startup-evaluation-for-investment,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,This data set includes the completion pairs for evaluating startups before investing in them. This data set iincludes completion
gauss314/options-IV-SP500,,,,,,,https://huggingface.co/datasets/gauss314/options-IV-SP500,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Downloading the Options IV SP500 Dataset This document will guide you through the steps to download the Options IV SP500 dataset from Hugging Face Datasets. This dataset includes data on the options of the S&P 500, including implied volatility. To start, you'll need to install Hugging Face's datasets library if you haven't done so already. You can do this using the following pip command: !pip install datasets Here's the Python code to load the Options IV SP500 dataset fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gauss314/options-IV-SP500."
GBaker/MedQA-USMLE-4-options-hf,,,,,,,https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options-hf,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"Original dataset introduced by Jin et al. in What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams Citation information: @article{jin2020disease, title={What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams}, author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter}, journal={arXiv preprint arXiv:2009.13081}, year={2020} }"
gbharti/finance-alpaca,,,,,,,https://huggingface.co/datasets/gbharti/finance-alpaca,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"This dataset is a combination of Stanford's Alpaca (https://github.com/tatsu-lab/stanford_alpaca) and FiQA (https://sites.google.com/view/fiqa/) with another 1.3k pairs custom generated using GPT3.5 Script for tuning through Kaggle's (https://www.kaggle.com) free resources using PEFT/LoRa: https://www.kaggle.com/code/gbhacker23/wealth-alpaca-lora GitHub repo with performance analyses, training and data generation scripts, and inference notebooks: https://github.com/gaurangbharti1/wealth-alpacaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gbharti/finance-alpaca."
GCruz19/GenZ_data,,,,,,,https://huggingface.co/datasets/GCruz19/GenZ_data,,,,,,,,,,,,,,,,,,,,GCruz19/GenZ_data dataset hosted on Hugging Face and contributed by the HF Datasets community
GDGiangi/SEIRDB,Video,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/GDGiangi/SEIRDB,"such as English, Russian, Mandarin, Greek, Italian, and French",,600000,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Speech Emotion Intensity Recognition Database (SEIR-DB) Data-Summary: The SEIR-DB is a comprehensive, multilingual speech emotion intensity recognition dataset containing over 600,000 instances from various sources. It is designed to support tasks related to speech emotion recognition and emotion intensity estimation. The database includes languages such as English, Russian, Mandarin, Greek, Italian, and French. Supported Tasks and Leaderboards The SEIR-DB isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/GDGiangi/SEIRDB."
geekyrakshit/LoL-Dataset,,,,,,https://arxiv.org/abs/1808.04560,https://huggingface.co/datasets/geekyrakshit/LoL-Dataset,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,The LOL dataset is composed of 500 low-light and normal-light image pairs and is divided into 485 training pairs and 15 testing pairs. The low-light images contain noise produced during the photo capture process. Most of the images are indoor scenes. All the images have a resolution of 400Ã—600. The dataset was introduced in the paper Deep Retinex Decomposition for Low-Light Enhancement.
GEM/cochrane-simplification,Text,,,,,https://github.com/AshOlogn/Paragraph-level-Simplification-of-Medical-Texts,https://huggingface.co/datasets/GEM/cochrane-simplification,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,14,,,"Auto-convertedto Parquet, ExpandinData Studio, 1, 2, 3",,,,,,,This dataset measures the ability for a model to simplify paragraphs of medical text through the omission non-salient information and simplification of medical jargon.
Gen-Verse/WideRange4D,,,,,,https://arxiv.org/abs/2503.13435,https://huggingface.co/datasets/Gen-Verse/WideRange4D,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes Github Page | arXiv Paper Ling Yang1*, Kaixin Zhu1*, Juanxi Tian1*, Bohan Zeng1*, Mingbao Lin3, Hongjuan Pei2, Wentao Zhang1â€¡, Shuicheng Yan3â€¡ 1 Peking University 2 University of the Chinese Academy of Sciences 3 National University of Singapore * Equal Contributions. â€¡ Corresponding Author."
gendisjawi/nodejs,,,,,,,https://huggingface.co/datasets/gendisjawi/nodejs,,,,,,,,,,,,,,,,,,,,gendisjawi/nodejs dataset hosted on Hugging Face and contributed by the HF Datasets community
GeneralReasoning/GeneralThought-430K,,,,,,,https://huggingface.co/datasets/GeneralReasoning/GeneralThought-430K,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"GeneralThought-430K Thought wants to be free Open reasoning data from the General Reasoning resource for March 14 2025. The dataset contains questions, reference answers, reasoning traces, final answers and other metadata from several popular reasoning models including DeepSeek-R1, DeepSeek-R1-Zero, OpenThoughts-32B, LIMO, deepseek-r1-distill-llama-70b, DeepHermes-3-Llama-3-8B-Previewand DeepScaleR-1.5B-Preview. We also include final answers from o3-mini-2025-01-31â€¦ See the full description on the dataset page: https://huggingface.co/datasets/GeneralReasoning/GeneralThought-430K."
Georgii/poetry-genre,,,,,,,https://huggingface.co/datasets/Georgii/poetry-genre,,,,,,,,,,,,,,,,,,,,en poems and genres test
ghadeermobasher/BC5CDR-Chemical-Disease,Text,,,,,https://biocreative.bioinformatics.udel.edu/tasks/biocreative-v/track-3-cdr/,https://huggingface.co/datasets/ghadeermobasher/BC5CDR-Chemical-Disease,,,,,,,,,,,,,,,,,,,,\
ghoskno/landmark-en-hed,,,,,,,https://huggingface.co/datasets/ghoskno/landmark-en-hed,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""landmark-en-hed"" More Information needed"
gililior/wild-if-eval,,,,,,https://arxiv.org/abs/2503.06573,https://huggingface.co/datasets/gililior/wild-if-eval,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"WildIFEval Dataset This dataset was originally introduced in the paper WildIFEval: Instruction Following in the Wild, available on arXiv. Code: https://github.com/gililior/wild-if-eval Dataset Overview The WildIFEval dataset is designed for evaluating instruction-following capabilities in language models. It provides decompositions of conversations extracted from the LMSYS-Chat-1M dataset. Each"
gimmaru/ag_news,,,,,,https://arxiv.org/abs/2305.14877,https://huggingface.co/datasets/gimmaru/ag_news,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""ag_news"" More Information needed Note: This dataset was utilized for the evaluation of probability-based prompt selection techniques in the paper 'Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis'. It differs from the actual benchmark dataset."
giseldo/desharnais,,,,,,,https://huggingface.co/datasets/giseldo/desharnais,,,,,,,,,,,,,,,,,,,,giseldo/desharnais dataset hosted on Hugging Face and contributed by the HF Datasets community
glaiveai/code_edits_sample,,,,,,,https://huggingface.co/datasets/glaiveai/code_edits_sample,,,,,,,,,,,,,,,,,,,,glaiveai/code_edits_sample dataset hosted on Hugging Face and contributed by the HF Datasets community
Glavin001/startup-interviews,,,,,,,https://huggingface.co/datasets/Glavin001/startup-interviews,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-2.0/,,,,,,,,,,,,Glavin001/startup-interviews dataset hosted on Hugging Face and contributed by the HF Datasets community
globis-university/aozorabunko-clean,,,,,,,https://huggingface.co/datasets/globis-university/aozorabunko-clean,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Overview This dataset provides a convenient and user-friendly format of data from Aozora Bunko (é’ç©ºæ–‡åº«), a website that compiles public-domain books in Japan, ideal for Machine Learning applications. [For Japanese] æ—¥æœ¬èªžã§ã®æ¦‚è¦èª¬æ˜Žã‚’ Qiita ã«è¨˜è¼‰ã—ã¾ã—ãŸ: https://qiita.com/akeyhero/items/b53eae1c0bc4d54e321f Methodology The code to reproduce this dataset is made available on GitHub: globis-org/aozorabunko-exctractor. 1. Data collection We firstly downloaded the CSVâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/globis-university/aozorabunko-clean."
gneubig/aime-1983-2024,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/gneubig/aime-1983-2024,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"AIME Problem Set 1983-2024 Dataset Description This dataset contains problems from the American Invitational Mathematics Examination (AIME) from 1983 to 2024. The AIME is a prestigious mathematics competition for high school students in the United States and Canada. Data-Summary: Source: Kaggle - AIME Problem Set 1983-2024 License: CC0: Public Domain Total Problems: 2,250 Years Covered: 1983 to 2024 Main Task: Mathematics Problem Solvingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gneubig/aime-1983-2024."
goendalf666/sql-chat-instructions,,,,,,,https://huggingface.co/datasets/goendalf666/sql-chat-instructions,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""sql-chat-instructions"" More Information needed"
gofixyourself/EasyPortrait,,,,,,https://arxiv.org/abs/2304.13509,https://huggingface.co/datasets/gofixyourself/EasyPortrait,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"EasyPortrait - Face Parsing and Portrait Segmentation Dataset We introduce a large-scale image dataset EasyPortrait for portrait segmentation and face parsing. Proposed dataset can be used in several tasks, such as background removal in conference applications, teeth whitening, face skin enhancement, red eye removal or eye colorization, and so on. EasyPortrait dataset size is about 26GB, and it contains 20 000 RGB images (~17.5K FullHD images) with high quality annotatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gofixyourself/EasyPortrait."
gonglinyuan/CoSQA,,,,,,https://arxiv.org/abs/2105.13239,https://huggingface.co/datasets/gonglinyuan/CoSQA,,,,,,,,https://choosealicense.com/licenses/mit/,,,,https://github.com/microsoft/CodeXGLUE/tree/main/Text-Code/NL-code-search-WebQuery,,,,,,,,"Downloaded from https://github.com/microsoft/CodeXGLUE/tree/main/Text-Code/NL-code-search-WebQuery For more details about the dataset collection and usage, please refer to the ACL 2021 paper (https://arxiv.org/abs/2105.13239) and the GitHub repo (https://github.com/Jun-jie-Huang/CoCLR)."
google-research-datasets/go_emotions,Video,General,General,Text,"Accuracy, F1 Score",https://github.com/google-research/google-research/tree/master/goemotions,https://huggingface.co/datasets/google-research-datasets/go_emotions,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for GoEmotions Data-Summary: The GoEmotions dataset contains 58k carefully curated Reddit comments labeled for 27 emotion categories or Neutral. The raw data is included as well as the smaller, simplified version of the dataset with predefined train/val/test splits. Supported Tasks and Leaderboards This dataset is intended for multi-class, multi-label emotion classification. Languages The data is in English.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/google-research-datasets/go_emotions."
google/smol,,,,,,https://arxiv.org/abs/2502.12301,https://huggingface.co/datasets/google/smol,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"SMOL SMOL (Set for Maximal Overall Leverage) is a collection of professional translations into 221 Low-Resource Languages, for the purpose of training translation models, and otherwise increasing the representations of said languages in NLP and technology. Please read the SMOL Paper and the GATITOS Paper for a much more thorough description! There are four resources in this directory: SmolDoc: document-level translations into 100 languages SmolSent: sentence-level translations intoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/google/smol."
gopalkalpande/bbc-news-summary,,,,,,,https://huggingface.co/datasets/gopalkalpande/bbc-news-summary,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,About Dataset Context Text summarization is a way to condense the large amount of information into a concise form by the process of selection of important information and discarding unimportant and redundant information. With the amount of textual information present in the world wide web the area of text summarization is becoming very important. The extractive summarization is the one where the exact sentences present in the document are used as summaries. Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gopalkalpande/bbc-news-summary.
gorilla-llm/APIBench,,,,,,https://arxiv.org/abs/2305.15334,https://huggingface.co/datasets/gorilla-llm/APIBench,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Gorilla: Large Language Model Connected with Massive APIs By Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez (Project Website) Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla can write a semantically- and syntactically- correct API to invoke. With Gorilla, we are the first to demonstrate how to use LLMs to invoke 1,600+ (and growing) API calls accurately while reducing hallucination. We also release APIBench, theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gorilla-llm/APIBench."
Goud/Goud-sum,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Goud/Goud-sum,,,,,,,,https://choosealicense.com/licenses/undefined/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Goud summarization dataset Data-Summary: Goud-sum contains 158k articles and their headlines extracted from Goud.ma news website. The articles are written in the Arabic script. All headlines are in Moroccan Darija, while articles may be in Moroccan Darija, in Modern Standard Arabic, or a mix of both (code-switched Moroccan Darija). Supported Tasks and Leaderboards Text Summarization Languages Moroccan Arabicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Goud/Goud-sum."
Gourieff/ReActor,,,,,,,https://huggingface.co/datasets/Gourieff/ReActor,,,,,,,,https://choosealicense.com/licenses/mit/,,,,https://github.com/sczhou/CodeFormer,,,,,,,,ReActor Assets The Fast and Simple Face Swap Extension sd-webui-reactor comfyui-reactor-node Models file source license buffalo_l.zip DeepInsight codeformer-v0.1.0.pth sczhou GFPGANv1.3.pth TencentARC GFPGANv1.4.pth TencentARC GPEN-BFR-512.onnx harisreedhar RestoreFormer_PP.onnx netrunner.exe inswapper_128.onnx DeepInsight inswapper_128_fp16.onnx Hillobar
gpt-omni/VoiceAssistant-400K,,,,,,,https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,gpt-omni/VoiceAssistant-400K dataset hosted on Hugging Face and contributed by the HF Datasets community
GPUMODE/Inductor_Created_Data_Permissive,,,,,,,https://huggingface.co/datasets/GPUMODE/Inductor_Created_Data_Permissive,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Overview dataset_permissive{.json/.parquet} is a curated collection of pairs of pytorch programs and equivalent triton code (generated by torch inductor) which can be used to train models to translate pytorch code to triton code. Dataset Creation The dataset was created through the following process: Repository Collection: PyTorch repositories were collected from GitHub using repositories (and associated hashes) from the Stack v1. PyTorch Module Extraction: We extractedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/GPUMODE/Inductor_Created_Data_Permissive.
gradio/NYC-Airbnb-Open-Data,,,,,,,https://huggingface.co/datasets/gradio/NYC-Airbnb-Open-Data,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,gradio/NYC-Airbnb-Open-Data dataset hosted on Hugging Face and contributed by the HF Datasets community
graelo/wikipedia,,,,,,,https://huggingface.co/datasets/graelo/wikipedia,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,Wikipedia dataset containing cleaned articles of all languages. The datasets are built from the Wikipedia dump (https://dumps.wikimedia.org/) with one split per language. Each
GrainsPolito/BBBicycles,Image,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/GrainsPolito/BBBicycles,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for BBBicycles Data-Summary: Bent & Broken Bicycles (BBBicycles) dataset is a benchmark set for the novel task of damaged object re-identification, which aims to identify the same object in multiple images even in the presence of breaks, deformations, and missing parts. You can find an interactive preview here. Dataset Structure The final dataset contains: Total of 39,200 image 2,800 unique IDs 20 models 140 IDs for each modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/GrainsPolito/BBBicycles."
grammarly/coedit,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2305.09857,https://huggingface.co/datasets/grammarly/coedit,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for CoEdIT: Text Editing via Instruction Tuning Paper: CoEdIT: Text Editing by Task-Specific Instruction Tuning Authors: Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang Project Repo: https://github.com/vipulraheja/coedit Data-Summary: This is the dataset that was used to train the CoEdIT text editing models. Full details of the dataset can be found in our paper. Dataset Structure Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/grammarly/coedit."
graphs-datasets/PROTEINS,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/graphs-datasets/PROTEINS,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for PROTEINS Data-Summary: The PROTEINS dataset is a medium molecular property prediction dataset. Supported Tasks and Leaderboards PROTEINS should be used for molecular property prediction (aiming to predict whether molecules are enzymes or not), a binary classification task. The score used is accuracy, using a 10-fold cross-validation. External Use PyGeometric To load in PyGeometric, do the following:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/graphs-datasets/PROTEINS."
greengerong/leetcode,,,,,,,https://huggingface.co/datasets/greengerong/leetcode,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,greengerong/leetcode dataset hosted on Hugging Face and contributed by the HF Datasets community
gretelai/synthetic_text_to_sql,,,,,,https://arxiv.org/abs/2306.05685,https://huggingface.co/datasets/gretelai/synthetic_text_to_sql,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Image generated by DALL-E. See prompt for more details synthetic_text_to_sql gretelai/synthetic_text_to_sql is a rich dataset of high quality synthetic Text-to-SQL samples, designed and generated using Gretel Navigator, and released under Apache 2.0. Please see our release blogpost for more details. The dataset includes: 105,851 records partitioned into 100,000 train and 5,851 test records ~23M total tokens, including ~12M SQL tokens Coverage across 100 distinctâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gretelai/synthetic_text_to_sql."
griffin/ChemSum,,,,,,https://arxiv.org/abs/2305.07615,https://huggingface.co/datasets/griffin/ChemSum,,,,,,,,,,,20,,,,,,,,,Dataset Card for ChemSum ChemSum Description Paper: What are the Desired Characteristics of Calibration Sets? Identifying Correlates on Long Form Scientific Summarization Journal: ACL 2023 Point of Contact: griffin.adams@columbia.edu Repository: https://github.com/griff4692/calibrating-summaries ChemSum Summary We introduce a dataset with a pure chemistry focus by compiling a list of chemistry academic journals with Open-Access articles. Forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/griffin/ChemSum.
grosenthal/latin_english_translation,,,,,,,https://huggingface.co/datasets/grosenthal/latin_english_translation,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Card for ""latin_english_parallel"" 101k translation pairs between Latin and English, split 99/1/1 as train/test/val. These have been collected roughly 66% from the Loeb Classical Library and 34% from the Vulgate translation. For those that were gathered from the Loeb Classical Library, alignment was performd manually between Source and Target sequences. Each sample is annotated with the index and file (and therefore author/work) that the sample is from. If you findâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/grosenthal/latin_english_translation."
gryffindor-ISWS/stable-diffusion-2-1-with-images,,,,,,,https://huggingface.co/datasets/gryffindor-ISWS/stable-diffusion-2-1-with-images,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,gryffindor-ISWS/stable-diffusion-2-1-with-images dataset hosted on Hugging Face and contributed by the HF Datasets community
Gryphe/CoEdit-Alpaca,,,,,,,https://huggingface.co/datasets/Gryphe/CoEdit-Alpaca,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,An Alpaca instruction conversion of Grammarly's CoEdIT dataset.
gsarti/clean_mc4_it,Text,,,,,https://arxiv.org/abs/2203.03759,https://huggingface.co/datasets/gsarti/clean_mc4_it,,,,,,,,https://choosealicense.com/licenses/odc-by/,,,,,,,,,,,,"A thoroughly cleaned version of the Italian portion of the multilingual colossal, cleaned version of Common Crawl's web crawl corpus (mC4) by AllenAI. Based on Common Crawl dataset: ""https://commoncrawl.org"". This is the processed version of Google's mC4 dataset by AllenAI, with further cleaning detailed in the repository README file."
gsdf/EasyNegative,,,,,,,https://huggingface.co/datasets/gsdf/EasyNegative,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"Negative Embedding This is a Negative Embedding trained with Counterfeit. Please use it in the ""\stable-diffusion-webui\embeddings"" folder.It can be used with other models, but the effectiveness is not certain. Counterfeit-V2.0.safetensors AbyssOrangeMix2_sfw.safetensors anything-v4.0-pruned.safetensors"
Guilherme34/OIG-Portuguese,,,,,,,https://huggingface.co/datasets/Guilherme34/OIG-Portuguese,,,,,,,,,,,,,,,,,,,,Guilherme34/OIG-Portuguese dataset hosted on Hugging Face and contributed by the HF Datasets community
GunA-SD/bash_code,,,,,,,https://huggingface.co/datasets/GunA-SD/bash_code,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,This dataset is a collection of bash programs from various GitHub repositories and open source projects. The dataset might contain harmful code.
gustavecortal/DreamBank-annotated,,,,,,,https://huggingface.co/datasets/gustavecortal/DreamBank-annotated,,,,,,,,,,,,,,,,,,,,"Presentation DreamBank, an open corpus of more than 27,000 dream narratives, mostly written in English. Annotations were produced using dream-t5, a LaMini-Flan-T5 model finetuned on Hall and Van de Castle annotations to predict character and emotion. I've introduced this task in this paper: Gustave Cortal. 2024. Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives. In Proceedings of the 2024 Joint International Conference on Computationalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gustavecortal/DreamBank-annotated."
Gustavosta/Stable-Diffusion-Prompts,,,,,,,https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"Stable Diffusion Dataset This is a set of about 80,000 prompts filtered and extracted from the image finder for Stable Diffusion: ""Lexica.art"". It was a little difficult to extract the data, since the search engine still doesn't have a public API without being protected by cloudflare. If you want to test the model with a demo, you can go to: ""spaces/Gustavosta/MagicPrompt-Stable-Diffusion"". If you want to see the model, go to: ""Gustavosta/MagicPrompt-Stable-Diffusion""."
h4iku/coconut_javascript2010,Text,Translation,General,Text,"BLEU, METEOR, TER",https://github.com/lin-tan/CoCoNut-Artifact/releases/tag/training_data_1.0.0,https://huggingface.co/datasets/h4iku/coconut_javascript2010,,,,,,,,,,,,,,,d,,"T5, mBART, M2M100","Repair"" paper",,"Dataset Card for CoCoNuT-JavaScript(2010) Data-Summary: Part of the data used to train the models in the ""CoCoNuT: Combining Context-Aware Neural Translation Models using Ensemble for Program Repair"" paper. These datasets contain raw data extracted from GitHub, GitLab, and Bitbucket, and have neither been shuffled nor tokenized. The year in the datasetâ€™s name is the cutting year that shows the year of the newest commit in the dataset. Languagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/h4iku/coconut_javascript2010."
habedi/stack-exchange-dataset,,,,,,,https://huggingface.co/datasets/habedi/stack-exchange-dataset,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,"This dataset consists of three CSV files, namely: 'cs.csv', 'ds.csv', and 'p.csv'. Each CSV file includes the data for the questions asked on a Stack Exchange (SE) question-answering community, from the creation of the community until May 2021. 'cs.csv' --> Computer Science SE 'ds.csv' --> Data Science SE 'p.csv' --> Political Science SE Each CSV file has the following columns: id: the question id title: the title of the question body: the body or text of the question tags: the list ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/habedi/stack-exchange-dataset."
Hack90/virus_dna_dataset,Text,General,UI,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Hack90/virus_dna_dataset,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"[Needs More Information] Dataset Card for virus_dna_dataset Data-Summary: A collection of full virus genome dna, the dataset was built from NCBI data Supported Tasks and Leaderboards [Needs More Information] Languages DNA Dataset Structure Data Instances { 'Description' : 'NC_030848.1 Haloarcula californiae icosahedral...', 'dna_sequence' : 'TCATCTC TCTCTCT CTCTCTT GTTCCCG CGCCCGC CCGCCC...'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Hack90/virus_dna_dataset."
hadyelsahar/ar_res_reviews,Text,Sentiment Analysis,General,Label,"Accuracy, F1 Score",https://github.com/hadyelsahar/large-arabic-sentiment-analysis-resouces,https://huggingface.co/datasets/hadyelsahar/ar_res_reviews,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, DistilBERT",,,"Dataset Card for ArRestReviews Data-Summary: Dataset of 8364 restaurant reviews from qaym.com in Arabic for sentiment analysis Supported Tasks and Leaderboards [More Information Needed] Languages The dataset is based on Arabic. Dataset Structure Data Instances A typical data point comprises of the following: ""polarity"": which is a string value of either 0 or 1 indicating the sentiment around the reviewâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hadyelsahar/ar_res_reviews."
HAERAE-HUB/csatqa,Text,,,,,,https://huggingface.co/datasets/HAERAE-HUB/csatqa,,,,,,,,,,,,,,,,,,,,CSAT-QA
haitengzhao/molecule_property_instruction,,,,,,,https://huggingface.co/datasets/haitengzhao/molecule_property_instruction,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,"Dataset Card for ""molecule_property_instruction"" More Information needed"
hakurei/open-instruct-v1,,,,,,,https://huggingface.co/datasets/hakurei/open-instruct-v1,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Open Instruct V1 - A dataset for having LLMs follow instructions. Open Instruct V1 is an amalgamation of different datasets which are cleaned and then collated into a singular format for training. Dataset Breakdown Dataset Amount of Samples Alpaca 51759 Self Instruct 82599 GPT-4 Instruct 18194 Code Alpaca 18019 Dolly 15015 Synthetic 33143 Roleplay 3146 asss 448 instruction-dataset 327 Total 222650
halabi2016/arabic_speech_corpus,Text,,,,,http://en.arabicspeechcorpus.com/,https://huggingface.co/datasets/halabi2016/arabic_speech_corpus,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"This Speech corpus has been developed as part of PhD work carried out by Nawar Halabi at the University of Southampton. The corpus was recorded in south Levantine Arabic (Damascian accent) using a professional studio. Synthesized speech as an output using this corpus has produced a high quality, natural voice. Note that in order to limit the required storage for preparing this dataset, the audio is stored in the .flac format and is not converted to a float32 array. To convert, the audio file to a float32 array, please make use of the `.map()` function as follows: ```python import soundfile as sf def map_to_array(batch): speech_array, _ = sf.read(batch[""file""]) batch[""speech""] = speech_array return batch dataset = dataset.map(map_to_array, remove_columns=[""file""]) ```"
HamdiJr/Egyptian_hieroglyphs,,,,,,,https://huggingface.co/datasets/HamdiJr/Egyptian_hieroglyphs,,,,,,,,,,,,,,,,,,,,"Egyptian hieroglyphs ð“‚€ Hieroglyphs image dataset along with Language Model ! Features This dataset is build from the hieroglyphs found in 10 different pictures from the book ""The Pyramid of Unas"" (Alexandre Piankoff, 1955). We therefore urge you to have access to this book before using the dataset. The ten different pictures used throughout this dataset are: 3,5,7,9,20,21,22,23,39,41 (numbers represent the numbers used in the book ""The pyramid ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HamdiJr/Egyptian_hieroglyphs."
hamim-87/Ashrafur_bangla_math,,,,,,,https://huggingface.co/datasets/hamim-87/Ashrafur_bangla_math,,,,,,,,,,,,,,,,,,,,hamim-87/Ashrafur_bangla_math dataset hosted on Hugging Face and contributed by the HF Datasets community
hammer888/interior_style_dataset,,,,,,,https://huggingface.co/datasets/hammer888/interior_style_dataset,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""interior_style_dataset"" More Information needed"
Hamza-Ziyard/BBC-Sinhala,,,,,,,https://huggingface.co/datasets/Hamza-Ziyard/BBC-Sinhala,,,,,,,,,,,,,,,,,,,,Hamza-Ziyard/BBC-Sinhala dataset hosted on Hugging Face and contributed by the HF Datasets community
haonan-li/cmmlu,,,,,,https://arxiv.org/abs/2306.09212,https://huggingface.co/datasets/haonan-li/cmmlu,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,CMMLU is a comprehensive Chinese assessment suite specifically designed to evaluate the advanced knowledge and reasoning abilities of LLMs within the Chinese language and cultural context.
haonan3/V1-33K,,,,,,,https://huggingface.co/datasets/haonan3/V1-33K,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"V1: Toward Multimodal Reasoning by Designing Auxiliary Tasks ðŸš€ Toward Multimodal Reasoning via Unsupervised Task -- Future Prediction ðŸŒŸ Authors: Haonan Wang, Chao Du, Tianyu PangGitHub: haonan3/V1Dataset: V1-33K on Hugging Face Multimodal Reasoning Recent Large Reasoning Models (LRMs) such as DeepSeek-R1 have demonstrated impressive reasoning abilities; however, their capabilities are limited to textual data. Current models capture only a small part ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/haonan3/V1-33K."
harpomaxx/unix-commands,,,,,,,https://huggingface.co/datasets/harpomaxx/unix-commands,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,Unix Commands Dataset Description The Unix Commands Dataset is a unique collection of real-world Unix command line
harshasurampudi/legal-reasoning-lfqa-synthetic,,,,,,,https://huggingface.co/datasets/harshasurampudi/legal-reasoning-lfqa-synthetic,,,,,,,,,,22,,,,,,,,,,"Dataset Card for ""legal-reasoning-lfqa-synthetic"" More Information needed"
harshitv804/Indian_Penal_Code,,,,,,,https://huggingface.co/datasets/harshitv804/Indian_Penal_Code,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,"Indian Penal Code Dataset Dataset Description: The Indian Penal Code (IPC) Book PDF presents a rich and comprehensive dataset that holds immense potential for advancing Natural Language Processing (NLP) tasks and Language Model applications. This dataset encapsulates the entire spectrum of India's criminal law, offering a diverse range of legal principles, provisions, and case laws. With its intricate language and multifaceted legal content, the IPC datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/harshitv804/Indian_Penal_Code."
Harvard/gigaword,Text,,,,,https://arxiv.org/abs/1509.00685,https://huggingface.co/datasets/Harvard/gigaword,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Headline-generation on a corpus of article pairs from Gigaword consisting of around 4 million articles. Use the 'org_data' provided by https://github.com/microsoft/unilm/ which is identical to https://github.com/harvardnlp/sent-summary but with better format. There are two features: - document: article. - summary: headline.
HasturOfficial/adgen,,,,,,,https://huggingface.co/datasets/HasturOfficial/adgen,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""adgen"" More Information needed"
HausaNLP/AfriSenti-Twitter,Text,,,,,https://github.com/afrisenti-semeval/afrisent-semeval-2023,https://huggingface.co/datasets/HausaNLP/AfriSenti-Twitter,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"AfriSenti is the largest sentiment analysis benchmark dataset for under-represented African languages---covering 110,000+ annotated tweets in 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and yoruba)."
havens2/strategyQA_test,,,,,,,https://huggingface.co/datasets/havens2/strategyQA_test,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""strategyQA_test"" More Information needed"
hazal/Turkish-Biomedical-corpus-trM,,,,,,,https://huggingface.co/datasets/hazal/Turkish-Biomedical-corpus-trM,,,,,,,,,,,,,,,,,,,,hazal/Turkish-Biomedical-corpus-trM dataset hosted on Hugging Face and contributed by the HF Datasets community
hccngu/Viscacha-Chinese-IE,,,,,,,https://huggingface.co/datasets/hccngu/Viscacha-Chinese-IE,,,,,,,,,,,,,,,,,,,,hccngu/Viscacha-Chinese-IE dataset hosted on Hugging Face and contributed by the HF Datasets community
hchautran/javascript-medium,,,,,,,https://huggingface.co/datasets/hchautran/javascript-medium,,,,,,,,,,,,,,,,,,,,hchautran/javascript-medium dataset hosted on Hugging Face and contributed by the HF Datasets community
heegyu/namuwiki-extracted,,,,,,,https://huggingface.co/datasets/heegyu/namuwiki-extracted,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-2.0/,,,,,,,,,,,,"namu.wiki database dump https://namu.wiki/ database dump 2022/03/01 571308rows download size: 2.19GB ì£¼ì˜ì‚¬í•­ namu-wiki-extractorë¥¼ ì´ìš©í•˜ì—¬ ì „ì²˜ë¦¬, ì¶”ê°€ë¡œ ì•„ëž˜ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤ í—¤ë” ì œê±° == ê°œìš” == í…Œì´ë¸” ì œê±° [age(1997-01-01)] ëŠ” ì „ì²˜ë¦¬ ì‹œì  ê¸°ì¤€ìœ¼ë¡œ ì ìš©(2022ë…„ 10ì›” 2ì¼) [math(a / b + c)] ëŠ” ì œê±°í•˜ì§€ ì•ŠìŒ. math ë§ˆí¬ë‹¤ìš´ì´ ê°ì£¼ ë‚´ì— ìžˆì„ ê²½ìš°, ê°ì£¼ê°€ ì „ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë¬¸ì œ ìžˆìŒ. Usage pip install datasets from datasets import load_dataset dataset = load_dataset(""heegyu/namuwiki-extracted"")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/heegyu/namuwiki-extracted."
heliosbrahma/mental_health_chatbot_dataset,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/heliosbrahma/mental_health_chatbot_dataset,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""heliosbrahma/mental_health_chatbot_dataset"" Dataset Description Data-Summary: This dataset contains conversational pair of questions and answers in a single text related to Mental Health. Dataset was curated from popular healthcare blogs like WebMD, Mayo Clinic and HeatlhLine, online FAQs etc. All questions and answers have been anonymized to remove any PII data and pre-processed to remove any unwanted characters.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/heliosbrahma/mental_health_chatbot_dataset."
Hellisotherpeople/DebateSum,,,,,,https://arxiv.org/abs/2011.07251,https://huggingface.co/datasets/Hellisotherpeople/DebateSum,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"DebateSum Corresponding code repo for the upcoming paper at ARGMIN 2020: ""DebateSum: A large-scale argument mining and summarization dataset"" Arxiv pre-print available here: https://arxiv.org/abs/2011.07251 Check out the presentation date and time here: https://argmining2020.i3s.unice.fr/node/9 Full paper as presented by the ACL is here: https://www.aclweb.org/anthology/2020.argmining-1.1/ Video of presentation at COLING 2020:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Hellisotherpeople/DebateSum."
Hello-SimpleAI/HC3-Chinese,,,,,,https://arxiv.org/abs/2301.07597,https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,Human ChatGPT Comparison Corpus (HC3) Chinese Version
Helsinki-NLP/bible_para,Text,,,,,http://opus.nlpl.eu/bible-uedin.php,https://huggingface.co/datasets/Helsinki-NLP/bible_para,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"This is a multilingual parallel corpus created from translations of the Bible compiled by Christos Christodoulopoulos and Mark Steedman. 102 languages, 5,148 bitexts total number of files: 107 total number of tokens: 56.43M total number of sentence fragments: 2.84M"
hendrycks/competition_math,Text,,,,,https://github.com/hendrycks/math,https://huggingface.co/datasets/hendrycks/competition_math,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"The Mathematics Aptitude Test of Heuristics (MATH) dataset consists of problems from mathematics competitions, including the AMC 10, AMC 12, AIME, and more. Each problem in MATH has a full step-by-step solution, which can be used to teach models to generate answer derivations and explanations."
hermanshid/doctor-id-qa,,,,,,,https://huggingface.co/datasets/hermanshid/doctor-id-qa,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,hermanshid/doctor-id-qa dataset hosted on Hugging Face and contributed by the HF Datasets community
HeshamHaroon/Egyptian_English_parallel,,,,,,,https://huggingface.co/datasets/HeshamHaroon/Egyptian_English_parallel,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,HeshamHaroon/Egyptian_English_parallel dataset hosted on Hugging Face and contributed by the HF Datasets community
hf-internal-testing/librispeech_asr_dummy,,,,,,,https://huggingface.co/datasets/hf-internal-testing/librispeech_asr_dummy,,,,,,,,,,,76,,,,,,,,,hf-internal-testing/librispeech_asr_dummy dataset hosted on Hugging Face and contributed by the HF Datasets community
hfl/stem_zh_instruction,,,,,,,https://huggingface.co/datasets/hfl/stem_zh_instruction,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"stem_zh_instruction å†…å®¹ï¼šSTEMç›¸å…³æŒ‡ä»¤ï¼ˆgpt-3.5çˆ¬å–ï¼‰ï¼ŒåŒ…å«ç‰©ç†ã€åŒ–å­¦ã€åŒ»å­¦ã€ç”Ÿç‰©å­¦ã€åœ°çƒç§‘å­¦ï¼›å…±è®¡256Kæ¡ã€‚ Content: STEM related instructions (gpt-3.5 crawled), including physics, chemistry, medicine, biology, and earch science. 256K instruction data in total. å­¦ç§‘ / Subject æ–‡ä»¶å / File Name æ•°é‡ / Num ç‰©ç† / Physics phy_50380.json 50,380 åŒ–å­¦ / Chemistry chem_50839.json 50,839 åŒ»å­¦ / Medicine med_54617.json 54,617 ç”Ÿç‰©å­¦ / Biology bio_50282.json 50,282 åœ°çƒç§‘å­¦ / Earth Science earth_50068.json 50,068 æ€»è®¡ 256â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hfl/stem_zh_instruction."
hirupert/sede,Text,,,,,https://arxiv.org/abs/2106.05006,https://huggingface.co/datasets/hirupert/sede,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"SEDE (Stack Exchange Data Explorer) is new dataset for Text-to-SQL tasks with more than 12,000 SQL queries and their natural language description. It's based on a real usage of users from the Stack Exchange Data Explorer platform, which brings complexities and challenges never seen before in any other semantic parsing dataset like including complex nesting, dates manipulation, numeric and text manipulation, parameters, and most importantly: under-specification and hidden-assumptions. Paper (NLP4Prog workshop at ACL2021): https://arxiv.org/abs/2106.05006"
hishab/titulm-bangla-corpus,,,,,,https://arxiv.org/abs/2502.11187,https://huggingface.co/datasets/hishab/titulm-bangla-corpus,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"TituLM Bangla Corpus TituLM Bangla Corpus is one of the largest Bangla clean corpus prepared for pretraining, continual pretraining or fine-tuning Large Language Model(LLM) for improving Bangla text generation capability. This dataset contains diverse sources and categories of Bangla text. The largest part of this dataset contains filtered common crawled datasets. As we saw existing all common crawl datasets have issues with proper text extraction from HTML pages and Bangla languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hishab/titulm-bangla-corpus."
hitachi-nlp/FLD.v2,,,,,,,https://huggingface.co/datasets/hitachi-nlp/FLD.v2,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""FLD.v2"" For the schema of the dataset, see here. For the whole of the project, see our project page. More Information needed"
hiyouga/geometry3k,,,,,,,https://huggingface.co/datasets/hiyouga/geometry3k,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"This dataset was converted from https://github.com/lupantech/InterGPS using the following script. import os import json from PIL import Image from datasets import Dataset, DatasetDict, Sequence from datasets import Image as ImageData MAPPING = {""A"": 0, ""B"": 1, ""C"": 2, ""D"": 3} def generate_data(data_path: str): for folder in os.listdir(data_path): folder_path = os.path.join(data_path, folder) image = Image.open(os.path.join(folder_path, ""img_diagram.png""), ""r"")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hiyouga/geometry3k."
hkust-nlp/felm,Text,,,,,,https://huggingface.co/datasets/hkust-nlp/felm,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,FELM
HKUSTAudio/Llasa_opensource_speech_data_160k_hours_tokenized,,,,,,https://arxiv.org/abs/2502.04128,https://huggingface.co/datasets/HKUSTAudio/Llasa_opensource_speech_data_160k_hours_tokenized,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-nd-4.0/,,,,,,,,,,,,"Update (2025-02-07): Our paper has been released! This script is for merging tokenized speech datasets stored in memmap format. The input datasets can be combined to form larger training datasets. import numpy as np import os def merge_memmap_datasets(dataset_dirs, output_dir): # Ensure the output directory exists os.makedirs(output_dir, exist_ok=True) # Dataset splits to be merged splits = ['train', 'val'] for split in splits: shapes = [] seq_len =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/HKUSTAudio/Llasa_opensource_speech_data_160k_hours_tokenized."
hlillemark/flores200_8_baseline,,,,,,,https://huggingface.co/datasets/hlillemark/flores200_8_baseline,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""flores200_8_baseline"" More Information needed"
hltcoe/weibo_ner,Text,,,,,,https://huggingface.co/datasets/hltcoe/weibo_ner,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"Tags: PER(äººå), LOC(åœ°ç‚¹å), GPE(è¡Œæ”¿åŒºå), ORG(æœºæž„å) Label Tag Meaning PER PER.NAM åå­—ï¼ˆå¼ ä¸‰ï¼‰ PER.NOM ä»£ç§°ã€ç±»åˆ«åï¼ˆç©·äººï¼‰ LOC LOC.NAM ç‰¹æŒ‡åç§°ï¼ˆç´«çŽ‰å±±åº„ï¼‰ LOC.NOM æ³›ç§°ï¼ˆå¤§å³¡è°·ã€å®¾é¦†ï¼‰ GPE GPE.NAM è¡Œæ”¿åŒºçš„åç§°ï¼ˆåŒ—äº¬ï¼‰ ORG ORG.NAM ç‰¹å®šæœºæž„åç§°ï¼ˆé€šæƒ åŒ»é™¢ï¼‰ ORG.NOM æ³›æŒ‡åç§°ã€ç»Ÿç§°ï¼ˆæ–‡è‰ºå…¬å¸ï¼‰"
HoangCuongNguyen/CTI-to-MITRE-question-answers,,,,,,,https://huggingface.co/datasets/HoangCuongNguyen/CTI-to-MITRE-question-answers,,,,,,,,,,,,,,,,,,,,HoangCuongNguyen/CTI-to-MITRE-question-answers dataset hosted on Hugging Face and contributed by the HF Datasets community
holylovenia/TITML-IDN,,,,,,,https://huggingface.co/datasets/holylovenia/TITML-IDN,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"IndoLVCSR TITML-IDN (Tokyo Institute of Technology Multilingual - Indonesian) is collected and proposed by the authors of ""A Large Vocabulary Continuous Speech Recognition System for Indonesian Language"". The text transcriptions are obtained from newspaper and magazine articles. The speech is recorded from 20 speakers (11 males and 9 females). How to cite If you use this dataset, you have to cite this paper: @inproceedings{lestari2006titmlidn, title={A largeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/holylovenia/TITML-IDN."
hoskinson-center/proof-pile,,,,,,,https://huggingface.co/datasets/hoskinson-center/proof-pile,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,A dataset of high quality mathematical text.
hossein20s/enrun-emails-text-classification,,,,,,,https://huggingface.co/datasets/hossein20s/enrun-emails-text-classification,,,,,,,,,,,,,,,,,,,,hossein20s/enrun-emails-text-classification dataset hosted on Hugging Face and contributed by the HF Datasets community
hotpotqa/hotpot_qa,Text,,,,,https://hotpotqa.github.io/,https://huggingface.co/datasets/hotpotqa/hotpot_qa,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"HotpotQA is a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowingQA systems to reason with strong supervisionand explain the predictions; (4) we offer a new type of factoid comparison questions to testQA systemsâ€™ ability to extract relevant facts and perform necessary comparison."
howard-hou/OCR-VQA,,,,,,,https://huggingface.co/datasets/howard-hou/OCR-VQA,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""OCR-VQA"" More Information needed"
hpprc/janli,Text,General,UI,Text,"Accuracy, F1 Score",https://github.com/verypluming/JaNLI,https://huggingface.co/datasets/hpprc/janli,English,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for JaNLI Data-Summary: The JaNLI (Japanese Adversarial NLI) dataset, inspired by the English HANS dataset, is designed to necessitate an understanding of Japanese linguistic phenomena and to illuminate the vulnerabilities of models. Languages The language data in JaNLI is in Japanese (BCP-47 ja-JP). Dataset Structure Data Instances When loading a specific configuration, users has to append a versionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hpprc/janli."
hssd/ai2thor-hab,,,,,,,https://huggingface.co/datasets/hssd/ai2thor-hab,,,,,,,,,,,,,,,,,,,,"AI2THOR-Hab AI2THOR scene datasets include iTHOR, RoboTHOR, ProcTHOR-10K, ArchitecTHOR. Many of the assets of the interactable objects are shared across these datasets. iTHOR: includes 120 single room scenes, 30 scenes for each bedroom, bathroom, kitchen, and living room. In our extracted dataset, there are additional 30 foyers scenes. RoboTHOR: includes 89 apartments in maze style, where the rooms are subdivided by wall panels. Same of the scenes share the same room layoutâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hssd/ai2thor-hab."
hugcyp/LCSTS,,,,,,,https://huggingface.co/datasets/hugcyp/LCSTS,,,,,,,,,,,,,,,,,,,,hugcyp/LCSTS dataset hosted on Hugging Face and contributed by the HF Datasets community
hugfaceguy0001/retarded_bar,,,,,,,https://huggingface.co/datasets/hugfaceguy0001/retarded_bar,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,å¼±æ™ºå§ç¬‘è¯æ•°æ®é›† å¼±æ™ºå§æ˜¯ç™¾åº¦è´´å§ä¸­çš„ä¸€ä¸ªéžå¸¸å—æ¬¢è¿Žçš„è®ºå›ï¼Œä»¥åˆ›ä½œçŸ­å°ç²¾æ‚çš„å†·ç¬‘è¯è€Œé—»åã€‚è¿™äº›ç¬‘è¯é€šå¸¸é‡‡ç”¨åŒå…³è¯­ã€ä¸å¯»å¸¸çš„æ–­å¥ã€ä¸åˆç†çš„é€»è¾‘ç­‰åˆ›ä½œæ‰‹æ³•ã€‚å³ä½¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡åž‹ï¼Œä¹Ÿéš¾ä»¥å®Œå…¨ç†è§£å¼±æ™ºå§çš„ç¬‘è¯ã€‚ å¼±æ™ºå§ æˆ‘ä»Žäº’è”ç½‘ä¸Šæ”¶é›†äº†ä¸€äº›å¼±æ™ºå§çš„ç¬‘è¯ï¼Œå…±100æ¡ï¼Œå…¶ä¸­45æ¡æ˜¯é™ˆè¿°å¥ï¼Œ55æ¡æ˜¯é—®å¥ã€‚æˆ‘ç»“åˆäººå·¥å’Œè¯­è¨€æ¨¡åž‹å¯¹è¿™äº›ç¬‘è¯è¿›è¡Œäº†ä¸€äº›è§£æžï¼Œå¹¶åˆ¶ä½œäº†è¿™ä¸ªå°åž‹æ•°æ®é›†ã€‚ é™ˆè¿°å¥ç¬‘è¯ é™ˆè¿°å¥ç¬‘è¯é€šå¸¸ä»¥å¥å·ç»“å°¾ï¼Œä¸å®¹æ˜“è¢«è¯­è¨€æ¨¡åž‹è¯¯è§£ä¸ºæ­£å¸¸çš„é—®é¢˜ã€‚ ä¾‹å¦‚ï¼šâ€œå‡ºäººå¤´åœ°å¸¸å¹´ç››äº§äººå¤´ã€‚â€ é—®å¥ç¬‘è¯ é—®å¥ç¬‘è¯å…·æœ‰ä¸€å®šçš„è¿·æƒ‘æ€§ï¼Œå¯èƒ½ä¼šå¯¼è‡´è¯­è¨€æ¨¡åž‹æ— æ³•åˆ¤æ–­å®ƒä»¬æ˜¯æ­£å¸¸çš„é—®é¢˜è¿˜æ˜¯å¼€çŽ©ç¬‘ã€‚ ä¾‹å¦‚ï¼šâ€œè“ç‰™è€³æœºåäº†ï¼Œåº”è¯¥æ‰¾ç‰™ç§‘åŒ»ç”Ÿè¿˜æ˜¯è€³ç§‘åŒ»ç”Ÿï¼Ÿâ€ æ–‡ä»¶æ ¼å¼ æœ¬æ•°æ®é›†åŒ…æ‹¬ä¸¤ä¸ªéƒ¨åˆ†ã€‚ retarded_bar.jsonlâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hugfaceguy0001/retarded_bar.
huggan/smithsonian-butterfly-lowres,,,,,,,https://huggingface.co/datasets/huggan/smithsonian-butterfly-lowres,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"Collection of pinned butterfly images from the Smithsonian https://www.si.edu/spotlight/buginfo/butterfly Doesn't include metadata yet! Url pattern: ""https://ids.si.edu/ids/deliveryService?max_w=550&id=ark:/65665/m3c70e17cf30314fd4ad86afa7d1ebf49f"" Added sketch versions! sketch_pidinet is generated by : https://github.com/zhuoinoulu/pidinet sketch_pix2pix is generated by : https://github.com/mtli/PhotoSketch"
huggingartists/taylor-swift,Text,,,,,https://github.com/AlekseyKorshuk/huggingartists,https://huggingface.co/datasets/huggingartists/taylor-swift,,,,,,,,,,,,,,,,,,,,This dataset is designed to generate lyrics with HuggingArtists.
HuggingFace-CN-community/translation,,,,,,,https://huggingface.co/datasets/HuggingFace-CN-community/translation,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,HuggingFace-CN-community/translation dataset hosted on Hugging Face and contributed by the HF Datasets community
huggingface-course/codeparrot-ds-valid,,,,,,,https://huggingface.co/datasets/huggingface-course/codeparrot-ds-valid,,,,,,,,,,,,,,,,,,,,huggingface-course/codeparrot-ds-valid dataset hosted on Hugging Face and contributed by the HF Datasets community
huggingface-legal/takedown-notices,,,,,,,https://huggingface.co/datasets/huggingface-legal/takedown-notices,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-nd-4.0/,,,,,,,,,,,,Takedown notices received by the Hugging Face team Please click on Files and versions to browse them Also check out our: Terms of Service Community Code of Conduct Content Guidelines
huggingface-projects/Deep-RL-Course-Certification,,,,,,,https://huggingface.co/datasets/huggingface-projects/Deep-RL-Course-Certification,,,,,,,,,,,,,,,,,,,,huggingface-projects/Deep-RL-Course-Certification dataset hosted on Hugging Face and contributed by the HF Datasets community
huggingface-tools/default-prompts,,,,,,,https://huggingface.co/datasets/huggingface-tools/default-prompts,,,,,,,,,,,,,,,,,,,,huggingface-tools/default-prompts dataset hosted on Hugging Face and contributed by the HF Datasets community
huggingface/label-files,,,,,,,https://huggingface.co/datasets/huggingface/label-files,,,,,,,,,,,,,,,,,,,,"This repository contains the mapping from integer id's to actual label names (in HuggingFace Transformers typically called id2label) for several datasets. Current datasets include: ImageNet-1k ImageNet-22k (also called ImageNet-21k as there are 21,843 classes) COCO detection 2017 COCO panoptic 2017 ADE20k (actually, the MIT Scene Parsing benchmark, which is a subset of ADE20k) Cityscapes VQAv2 Kinetics-700 RVL-CDIP PASCAL VOC Kinetics-400 ... You can read in a label file as follows (usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/huggingface/label-files."
HuggingFaceFW/fineweb-edu,,,,,,https://arxiv.org/abs/2406.17557,https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu,,,,,,,,https://choosealicense.com/licenses/odc-by/,,,,,,,,,,,,"ðŸ“š FineWeb-Edu 1.3 trillion tokens of the finest educational data the ðŸŒ web has to offer Paper: https://arxiv.org/abs/2406.17557 What is it? ðŸ“š FineWeb-Edu dataset consists of 1.3T tokens and 5.4T tokens (FineWeb-Edu-score-2) of educational web pages filtered from ðŸ· FineWeb dataset. This is the 1.3 trillion version. To enhance FineWeb's quality, we developed an educational quality classifier using annotations generated by LLama3-70B-Instruct. We thenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu."
HuggingFaceGECLM/REDDIT_submissions,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/HuggingFaceGECLM/REDDIT_submissions,,,,,,,,https://choosealicense.com/licenses/undefined/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""REDDIT_submissions"" Data-Summary: Submissions of 50 high-quality subreddits, extracted from the REDDIT PushShift data dumps (from 2006 to Jan 2023). Supported Tasks These submissions can be used for text generation and language modeling, as well as dialogue modeling. Dataset Structure Data Splits Each split corresponds to a specific subreddit in the following list: ""tifu"", ""explainlikeimfive""â€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceGECLM/REDDIT_submissions."
HuggingFaceH4/MATH-500,,,,,,,https://huggingface.co/datasets/HuggingFaceH4/MATH-500,,,,,,,,,,,,,,,,,,,,Dataset Card for MATH-500 This dataset contains a subset of 500 problems from the MATH benchmark that OpenAI created in their Let's Verify Step by Step paper. See their GitHub repo for the source file: https://github.com/openai/prm800k/tree/main?tab=readme-ov-file#math-splits
HuggingFaceM4/the_cauldron,,,,,,https://arxiv.org/abs/1603.07396,https://huggingface.co/datasets/HuggingFaceM4/the_cauldron,,,,,,,,,,,,,,,,,,,,"Dataset Card for The Cauldron Dataset description The Cauldron is part of the Idefics2 release. It is a massive collection of 50 vision-language datasets (training sets only) that were used for the fine-tuning of the vision-language model Idefics2. Load the dataset To load the dataset, install the library datasets with pip install datasets. Then, from datasets import load_dataset ds = load_dataset(""HuggingFaceM4/the_cauldron"", ""ai2d"") to downloadâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceM4/the_cauldron."
HuggingFaceTB/issues-kaggle-notebooks,,,,,,https://arxiv.org/abs/2402.19173,https://huggingface.co/datasets/HuggingFaceTB/issues-kaggle-notebooks,,,,,,,,,,,,,,,,,,,,"GitHub Issues & Kaggle Notebooks Description GitHub Issues & Kaggle Notebooks is a collection of two code datasets intended for language models training, they are sourced from GitHub issues and notebooks in Kaggle platform. These datasets are a modified part of the StarCoder2 model training corpus, precisely the bigcode/StarCoder2-Extras dataset. We reformat the samples to remove StarCoder2's special tokens and use natural text to delimit comments in issues and displayâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceTB/issues-kaggle-notebooks."
hugginglearners/amazon-reviews-sentiment-analysis,Text,Sentiment Analysis,General,Label,"Accuracy, F1 Score",https://kaggle.com/datasets/tarkkaanko/amazon,https://huggingface.co/datasets/hugginglearners/amazon-reviews-sentiment-analysis,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,10,,,,,,"BERT, RoBERTa, DistilBERT",,,"Dataset Card for amazon reviews for sentiment analysis Data-Summary: One of the most important problems in e-commerce is the correct calculation of the points given to after-sales products. The solution to this problem is to provide greater customer satisfaction for the e-commerce site, product prominence for sellers, and a seamless shopping experience for buyers. Another problem is the correct ordering of the comments given to the products. The prominence ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hugginglearners/amazon-reviews-sentiment-analysis."
hululuzhu/silly-emoji-qa,,,,,,,https://huggingface.co/datasets/hululuzhu/silly-emoji-qa,,,,,,,,,,,,,,,,,,,,The silly dataset to take text questions and return emoji-only answers. Powered by ChatGPT
humarin/chatgpt-paraphrases,,,,,,,https://huggingface.co/datasets/humarin/chatgpt-paraphrases,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,"This is a dataset of paraphrases created by ChatGPT. Model based on this dataset is avaible: model We used this prompt to generate paraphrases Generate 5 similar paraphrases for this question, show it like a numbered list without commentaries: {text} This dataset is based on the Quora paraphrase question, texts from the SQUAD 2.0 and the CNN news dataset. We generated 5 paraphrases for each sample, totally this dataset has about 420k data rows. You can make 30 rows from a rowâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/humarin/chatgpt-paraphrases."
hungnm/multilingual-amazon-review-sentiment-processed,,,,,,,https://huggingface.co/datasets/hungnm/multilingual-amazon-review-sentiment-processed,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,hungnm/multilingual-amazon-review-sentiment-processed dataset hosted on Hugging Face and contributed by the HF Datasets community
HUPD/hupd,Text,,,,,https://patentdataset.org/,https://huggingface.co/datasets/HUPD/hupd,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"The Harvard USPTO Patent Dataset (HUPD) is a large-scale, well-structured, and multi-purpose corpus of English-language patent applications filed to the United States Patent and Trademark Office (USPTO) between 2004 and 2018. With more than 4.5 million patent documents, HUPD is two to three times larger than comparable corpora. Unlike other NLP patent datasets, HUPD contains the inventor-submitted versions of patent applications, not the final versions of granted patents, allowing us to study patentability at the time of filing using NLP methods for the first time."
huseinzol05/Malay-TTS-Osman,,,,,,,https://huggingface.co/datasets/huseinzol05/Malay-TTS-Osman,,,,,,,,,,,,,,,,,,,,"Malay-TTS-Osman All notebooks and code related at https://github.com/huseinzol05/malaya-speech/tree/master/data/azure-tts Attributes Wiki and News 24000 sample rate, super clean. narrator ms-MY-OsmanNeural. approximate 94.5 hours Texts from Malay Wikipedia and News. Sentences between 2 words and 20 words. Parliament 24000 sample rate, super clean. narrator ms-MY-OsmanNeural. approximate 133.2 hours. Texts from Malaysia Malayâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/huseinzol05/Malay-TTS-Osman."
huuuyeah/meetingbank,,,,,,https://arxiv.org/abs/2305.17529,https://huggingface.co/datasets/huuuyeah/meetingbank,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"Overview MeetingBank, a benchmark dataset created from the city councils of 6 major U.S. cities to supplement existing datasets. It contains 1,366 meetings with over 3,579 hours of video, as well as transcripts, PDF documents of meeting minutes, agenda, and other metadata. On average, a council meeting is 2.6 hours long and its transcript contains over 28k tokens, making it a valuable testbed for meeting summarizers and for extracting structure from meeting videos. The datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/huuuyeah/meetingbank."
hw2942/financial-news-sentiment,,,,,,,https://huggingface.co/datasets/hw2942/financial-news-sentiment,,,,,,,,,,,,,,,,,,,,"Data source: CSMAR Link for raw data: https://www.heywhale.com/mw/dataset/5e577a780e2b66002c2561a9/content Data Description: 2329 news titles with annotated labels (0:Negative, 1:Neutral, 2:Positive)"
HydraLM/chemistry_dataset_standardized,,,,,,,https://huggingface.co/datasets/HydraLM/chemistry_dataset_standardized,,,,,,,,,,,,,,,,,,,,HydraLM/chemistry_dataset_standardized dataset hosted on Hugging Face and contributed by the HF Datasets community
hyesunyun/liveqa_medical_trec2017,,,,,,,https://huggingface.co/datasets/hyesunyun/liveqa_medical_trec2017,,,,,,,,,,,,,,,,,,,,"Dataset Card for LiveQA Medical from TREC 2017 The LiveQA'17 medical task focuses on consumer health question answering. Consumer health questions were received by the U.S. National Library of Medicine (NLM). The dataset consists of constructed medical question-answer pairs for training and testing, with additional annotations that can be used to develop question analysis and question answering systems. Please refer to our overview paper for more information about theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hyesunyun/liveqa_medical_trec2017."
hynky/czech_news_dataset_v2,,,,,,https://arxiv.org/abs/2307.10666,https://huggingface.co/datasets/hynky/czech_news_dataset_v2,,,,,,,,https://choosealicense.com/licenses/odc-by/,,,,,,,,,,,,"Dataset Card for ""czech_news_dataset_v2"" Dataset containing the news articles from major online news outlets collected from 2000-2022. Follow-up paper https://arxiv.org/abs/2307.10666 (v1 of the dataset) Changes from v1 Better contribution of novinky.cz in later stages More articles, as a mistake in filtering was fixed. Collection was done using CmonCrawl. The dataset should be used for Research only purposes as I don't have rights for articles itself. If you have anyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hynky/czech_news_dataset_v2."
hzydashi/KatagoExceptGo,,,,,,,https://huggingface.co/datasets/hzydashi/KatagoExceptGo,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,hzydashi/KatagoExceptGo dataset hosted on Hugging Face and contributed by the HF Datasets community
iamkaikai/amazing_logos_v4,,,,,,,https://huggingface.co/datasets/iamkaikai/amazing_logos_v4,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""amazing_logos_v4"" More Information needed"
iamketan25/alpaca-instructions-dataset,,,,,,,https://huggingface.co/datasets/iamketan25/alpaca-instructions-dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,iamketan25/alpaca-instructions-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
iamollas/ethos,Text,,,,,https://github.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset,https://huggingface.co/datasets/iamollas/ethos,,,,,,,,https://choosealicense.com/licenses/agpl-3.0/,,,,,,,,,,,,"ETHOS: onlinE haTe speecH detectiOn dataSet. This repository contains a dataset for hate speech detection on social media platforms, called Ethos. There are two variations of the dataset: Ethos_Dataset_Binary: contains 998 comments in the dataset alongside with a label about hate speech presence or absence. 565 of them do not contain hate speech, while the rest of them, 433, contain. Ethos_Dataset_Multi_Label: which contains 8 labels for the 433 comments with hate speech content. These labels are violence (if it incites (1) or not (0) violence), directed_vs_general (if it is directed to a person (1) or a group (0)), and 6 labels about the category of hate speech like, gender, race, national_origin, disability, religion and sexual_orientation."
iamplus/Instruction_Tuning,,,,,,,https://huggingface.co/datasets/iamplus/Instruction_Tuning,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,https://github.com/sahil280114/codealpaca,,,,,,,,Files Contents Details : Post-Process Code Info : data_process.py iamai_seed_tasks_v1.csv : IAMAI's seed tasks - Version 1 (879) Total Dataset Size : 879 =============================================================================================== iamai_v1.csv : Instruction Tuning Dataset collected using seeds from iamai_seed_tasks_v1.csv and ChatGPT API for both prompts and outputs (~248k) Total Dataset Size : ~248k iamai_summarization_v1.csv : Article Summarization dataset (bothâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/iamplus/Instruction_Tuning.
iamshnoo/alpaca-cleaned-chinese,,,,,,,https://huggingface.co/datasets/iamshnoo/alpaca-cleaned-chinese,,,,,,,,,,,,,,,,,,,,"Translated from yahma/alpaca-cleaned using NLLB-1.3B Dataset Card for ""alpaca-cleaned-chinese"" More Information needed"
iamtarun/python_code_instructions_18k_alpaca,,,,,,,https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca,,,,,,,,,,,,,,,,,,,,"Dataset Card for python_code_instructions_18k_alpaca The dataset contains problem descriptions and code in python language. This dataset is taken from sahil2801/code_instructions_120k, which adds a prompt column in alpaca style. Refer to the source here."
iarfmoose/question_generator,,,,,,,https://huggingface.co/datasets/iarfmoose/question_generator,,,,,,,,,,,,,,,,,,,,"This dataset is made up of data taken from SQuAD v2.0, RACE, CoQA, and MSMARCO. Some"
ibm-nasa-geospatial/multi-temporal-crop-classification,Image,Segmentation,General,Bounding Box/Mask,"IoU, Pixel Accuracy",,https://huggingface.co/datasets/ibm-nasa-geospatial/multi-temporal-crop-classification,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"Mask R-CNN, DeepLab, U-Net",,,Dataset Card for Multi-Temporal Crop Classification Data-Summary: This dataset contains temporal Harmonized Landsat-Sentinel imagery of diverse land cover and crop type classes across the Contiguous United States for the year 2022. The target labels are derived from USDA's Crop Data Layer (CDL). It's primary purpose is for training segmentation geospatial machine learning models. Dataset Structure TIFF Files Each tiff file covers a 224 x 224 pixelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ibm-nasa-geospatial/multi-temporal-crop-classification.
ibm-research/REAL-MM-RAG_FinReport,,,,,,https://arxiv.org/abs/2502.12342,https://huggingface.co/datasets/ibm-research/REAL-MM-RAG_FinReport,,,,,,,,https://choosealicense.com/licenses/cdla-permissive-2.0/,,,,,,,,,,,,"REAL-MM-RAG-Bench: A Real-World Multi-Modal Retrieval Benchmark We introduced REAL-MM-RAG-Bench, a real-world multi-modal retrieval benchmark designed to evaluate retrieval models in reliable, challenging, and realistic settings. The benchmark was constructed using an automated pipeline, where queries were generated by a vision-language model (VLM), filtered by a large language model (LLM), and rephrased by an LLM to ensure high-quality retrieval evaluation. To simulate real-worldâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ibm-research/REAL-MM-RAG_FinReport."
Icannos/lichess_games,Text,,,,,https://database.lichess.org/,https://huggingface.co/datasets/Icannos/lichess_games,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"Lichess.org is a free/libre, open-source chess server powered by volunteers and donations and provides all of its content in CC0. This script download all the games from the database and provide them in LLM pretraining friendly format."
Idavidrein/gpqa,,,,,,https://arxiv.org/abs/2311.12022,https://huggingface.co/datasets/Idavidrein/gpqa,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for GPQA GPQA is a multiple-choice, Q&A dataset of very hard questions written and validated by experts in biology, physics, and chemistry. When attempting questions out of their own domain (e.g., a physicist answers a chemistry question), these experts get only 34% accuracy, despite spending >30m with full access to Google. We request that you do not reveal"
IDEA-CCNL/laion2B-multi-chinese-subset,,,,,,https://arxiv.org/abs/2209.02970,https://huggingface.co/datasets/IDEA-CCNL/laion2B-multi-chinese-subset,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"laion2B-multi-chinese-subset Github: Fengshenbang-LM Docs: Fengshenbang-Docs ç®€ä»‹ Brief Introduction å–è‡ªLaion2Bå¤šè¯­è¨€å¤šæ¨¡æ€æ•°æ®é›†ä¸­çš„ä¸­æ–‡éƒ¨åˆ†ï¼Œä¸€å…±143Mä¸ªå›¾æ–‡å¯¹ã€‚ A subset from Laion2B (a multimodal dataset), around 143M image-text pairs (only Chinese). æ•°æ®é›†ä¿¡æ¯ Dataset Information å¤§çº¦ä¸€å…±143Mä¸ªä¸­æ–‡å›¾æ–‡å¯¹ã€‚å¤§çº¦å ç”¨19GBç©ºé—´ï¼ˆä»…ä»…æ˜¯urlç­‰æ–‡æœ¬ä¿¡æ¯ï¼Œä¸åŒ…å«å›¾ç‰‡ï¼‰ã€‚ Homepage: laion-5b Huggingface: laion/laion2B-multi ä¸‹è½½ Download mkdir laion2b_chinese_release && cd laion2b_chinese_release for i inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/IDEA-CCNL/laion2B-multi-chinese-subset."
Iess/chinese_modern_poetry,,,,,,,https://huggingface.co/datasets/Iess/chinese_modern_poetry,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,ç®€ä»‹ æ•°æ®é›†åŒ…æ‹¬äº†è¿‘çŽ°ä»£çš„ä¸­å›½è¯—äººåŠå¤–å›½è¯—äººï¼ˆä¸­è¯‘ç‰ˆï¼‰ä½œå“ï¼Œæ‰€æœ‰ä½œå“è‘—ä½œæƒå½’åŽŸä½œè€…æ‰€æœ‰ï¼Œä¾µåˆ è¯·è”ç³»aa531811820@gmail.com chinese_poems.jsonlä¸ºåŽŸæ•°æ®ï¼Œtraining_imagery2-5_maxlen256.json åˆ†åˆ«æ˜¯æ ¹æ®2-5ä¸ªå…³é”®æ„è±¡ç”Ÿæˆè¯—æ­Œçš„ç›¸å…³æ•°æ®é›† æ•°æ®æ¥æºäºŽç½‘ç»œï¼ŒåŒ…æ‹¬ä½†ä¸é™äºŽ https://github.com/sheepzh/poetry https://bedtimepoem.com/ https://poemwiki.org/ baiduã€googleã€zhihuç­‰ ä¸€äº›ä½œå“ ä½¿ç”¨æ­¤æ•°æ®é›†è®­ç»ƒChatGLMã€LLaMA7bæ¨¡åž‹ç”Ÿæˆçš„è¯—æ­Œï¼Œæ›´å¤šè¯—æ­ŒæŸ¥çœ‹poemsç›®å½•
ift/handwriting_forms,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/ift/handwriting_forms,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for Dataset Name Data-Summary: This dataset card aims to be a base template for new datasets. It has been generated using this raw template. Supported Tasks and Leaderboards [More Information Needed] Languages [More Information Needed] Dataset Structure Data Instances [More Information Needed] Data Fields [More Information Needed] Data Splits [More Informationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ift/handwriting_forms.
ignatius/igbo_english_machine_translation,Text,,,,,,https://huggingface.co/datasets/ignatius/igbo_english_machine_translation,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,Parallel Igbo-English Dataset
IgorVolochay/russian_jokes,,,,,,,https://huggingface.co/datasets/IgorVolochay/russian_jokes,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,IgorVolochay/russian_jokes dataset hosted on Hugging Face and contributed by the HF Datasets community
ikjased23/fafafultirai,,,,,,,https://huggingface.co/datasets/ikjased23/fafafultirai,,,,,,,,,,,,,,,,,,,,ikjased23/fafafultirai dataset hosted on Hugging Face and contributed by the HF Datasets community
ILSVRC/imagenet-1k,Text,,,,,https://image-net.org/index.php,https://huggingface.co/datasets/ILSVRC/imagenet-1k,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"ILSVRC 2012, commonly known as 'ImageNet' is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a ""synonym set"" or ""synset"". There are more than 100,000 synsets in WordNet, majority of them are nouns (80,000+). ImageNet aims to provide on average 1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated. In its completion, ImageNet hopes to offer tens of millions of cleanly sorted images for most of the concepts in the WordNet hierarchy. ImageNet 2012 is the most commonly used subset of ImageNet. This dataset spans 1000 object classes and contains 1,281,167 training images, 50,000 validation images and 100,000 test images"
iluvvatar/NEREL,,,,,,,https://huggingface.co/datasets/iluvvatar/NEREL,,,,,,,,,,,,,,,,,,,,"NEREL dataset Dataset Description NEREL dataset (https://doi.org/10.48550/arXiv.2108.13112) is a Russian dataset for named entity recognition and relation extraction. NEREL is significantly larger than existing Russian datasets: to date it contains 56K annotated named entities and 39K annotated relations. Its important difference from previous datasets is annotation of nested named entities, as well as relations within nested entities and at the discourse level.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/iluvvatar/NEREL."
IlyaGusev/rulm,,,,,,,https://huggingface.co/datasets/IlyaGusev/rulm,,,,,,,,,,,,,,,,,,,,Dataset for training Russian language models Overall: 75G Scripts: https://github.com/IlyaGusev/rulm/tree/master/data_processing Website Char count (M) Word count (M) pikabu 14938 2161 lenta 1008 135 stihi 2994 393 stackoverflow 1073 228 habr 5112 753 taiga_fontanka 419 55 librusec 10149 1573 buriy 2646 352 ods_tass 1908 255 wiki 3473 469 math 987 177
ImagenHub/Text_Guided_Image_Editing,,,,,,https://arxiv.org/abs/2310.01596,https://huggingface.co/datasets/ImagenHub/Text_Guided_Image_Editing,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card Dataset in ImagenHub. Citation Please kindly cite our paper if you use our code, data, models or results: @article{ku2023imagenhub, title={ImagenHub: Standardizing the evaluation of conditional image generation models}, author={Max Ku and Tianle Li and Kai Zhang and Yujie Lu and Xingyu Fu and Wenwen Zhuang and Wenhu Chen}, journal={arXiv preprint arXiv:2310.01596}, year={2023} }"
imageomics/KABR,Image,General,General,Text,"Accuracy, F1 Score",https://dirtmaxim.github.io/kabr/,https://huggingface.co/datasets/imageomics/KABR,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,"BERT, RoBERTa, T5",,"of annotated videos, and it includes eight different classes, encompassing seven types of animal behaviorâ€¦ See the full description on the dataset page: https://huggingface","Dataset Card for KABR: In-Situ Dataset for Kenyan Animal Behavior Recognition from Drone Videos Data-Summary: We present a novel high-quality dataset for animal behavior recognition from drone videos. The dataset is focused on Kenyan wildlife and contains behaviors of giraffes, plains zebras, and Grevy's zebras. The dataset consists of more than 10 hours of annotated videos, and it includes eight different classes, encompassing seven types of animal behaviorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/imageomics/KABR."
imodels/diabetes-readmission,,,,,,,https://huggingface.co/datasets/imodels/diabetes-readmission,,,,,,,,https://choosealicense.com/licenses/undefined/,,,,,,,,,,,,"Port of the diabetes-readmission dataset from UCI (link here). See details there and use carefully. Basic preprocessing done by the imodels team in this notebook. The target is the binary outcome readmitted. Sample usage Load the data: from datasets import load_dataset dataset = load_dataset(""imodels/diabetes-readmission"") df = pd.DataFrame(dataset['train']) X = df.drop(columns=['readmitted']) y = df['readmitted'].values Fit a model: import imodels import numpy as np m =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/imodels/diabetes-readmission."
imranraad/github-emotion-love,,,,,,https://arxiv.org/abs/2208.05573,https://huggingface.co/datasets/imranraad/github-emotion-love,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"AutoTrain Dataset for project: github-emotion-love Dataset Description Dataset used in the paper: Imran et al., ""Data Augmentation for Improving Emotion Recognition in Software Engineering Communication"", ASE-2022. This is an annotated dataset of 2000 GitHub comments. Six basic emotions are annotated. They are Anger, Love, Fear, Joy, Sadness and Surprise. This repository contains annotations of all emotions. Dataset Structure Dataset is in CSVâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/imranraad/github-emotion-love."
imvladikon/hebrew_speech_kan,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/imvladikon/hebrew_speech_kan,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Dataset Name Data-Summary: Hebrew Dataset for ASR Supported Tasks and Leaderboards [More Information Needed] Languages [More Information Needed] Dataset Structure Data Instances {'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/8ce7402f6482c6053251d7f3000eec88668c994beb48b7ca7352e77ef810a0b6/train/e429593fede945c185897e378a5839f4198.wav', 'array':â€¦ See the full description on the dataset page: https://huggingface.co/datasets/imvladikon/hebrew_speech_kan."
indonesian-nlp/librivox-indonesia,Audio,General,General,Text,"Accuracy, F1 Score",https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia,https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia,in Indonesia for this dataset,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,"BERT, RoBERTa, T5",,to a few hours,Dataset Card for LibriVox Indonesia 1.0 Data-Summary: The LibriVox Indonesia dataset consists of MP3 audio and a corresponding text file we generated from the public domain audiobooks LibriVox. We collected only languages in Indonesia for this dataset. The original LibriVox audiobooks or sound files' duration varies from a few minutes to a few hours. Each audio file in the speech dataset now lasts from a few seconds to a maximum of 20 seconds. We convertedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia.
indonlp/indonlu,Text,,,,,https://www.indobenchmark.com/,https://huggingface.co/datasets/indonlp/indonlu,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"The IndoNLU benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems for Bahasa Indonesia."
InfImagine/FakeImageDataset,Text,,,,,http://sentry.infimagine.com/,https://huggingface.co/datasets/InfImagine/FakeImageDataset,,,,,,,38.7%,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Fake Image Dataset Fake Image Dataset is now open-sourced at huggingface (InfImagine Organization) and openxlab. â†— It consists of two folders, ImageData and MetaData. ImageData contains the compressed packages of the Fake Image Dataset, while MetaData contains the labeling information of the corresponding data indicating whether they are real or fake. Sentry-Image is now open-sourced at Sentry-Image (github repository) which provides the SOTA fake image detection models inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/InfImagine/FakeImageDataset."
INK-USC/riddle_sense,Text,,,,,https://inklab.usc.edu/RiddleSense/,https://huggingface.co/datasets/INK-USC/riddle_sense,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"Answering such a riddle-style question is a challenging cognitive process, in that it requires complex commonsense reasoning abilities, an understanding of figurative language, and counterfactual reasoning skills, which are all important abilities for advanced natural language understanding (NLU). However, there is currently no dedicated datasets aiming to test these abilities. Herein, we present RiddleSense, a new multiple-choice question answering task, which comes with the first large dataset (5.7k"
inmortalkaktus/pokemon-pixel-art,,,,,,,https://huggingface.co/datasets/inmortalkaktus/pokemon-pixel-art,,,,,,,,,,,,,,,,,,,,This new dataset is designed to solve this great NLP task and is crafted with a lot of care.
inparallel/saudinewsnet,Text,,,,,https://github.com/parallelfold/SaudiNewsNet,https://huggingface.co/datasets/inparallel/saudinewsnet,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"The dataset contains a set of 31,030 Arabic newspaper articles alongwith metadata, extracted from various online Saudi newspapers and written in MSA."
inpars/generated-data,,,,,,,https://huggingface.co/datasets/inpars/generated-data,,,,,,,,,,,,,,,,,,,,inpars/generated-data dataset hosted on Hugging Face and contributed by the HF Datasets community
inria-soda/tabular-benchmark,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/inria-soda/tabular-benchmark,,,,,,,,https://choosealicense.com/licenses/undefined/,,,,,,,,,"BERT, RoBERTa, T5",,,"Tabular Benchmark Dataset Description This dataset is a curation of various datasets from openML and is curated to benchmark performance of various machine learning algorithms. Repository: https://github.com/LeoGrin/tabular-benchmark/community Paper: https://hal.archives-ouvertes.fr/hal-03723551v2/document Data-Summary: Benchmark made of curation of various tabular data learning tasks, including: Regression from Numerical and Categoricalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/inria-soda/tabular-benchmark."
InstaDeepAI/human_reference_genome,Text,,,,,,https://huggingface.co/datasets/InstaDeepAI/human_reference_genome,,,,,,,,,,,,,,,,,,,,Genome Reference Consortium Human Build 38 patch release 14 (GRCh38.p14) filtered and split into chunks.
instruction-tuning-sd/cartoonization,,,,,,,https://huggingface.co/datasets/instruction-tuning-sd/cartoonization,,,,,,,,,,,,,,,,,,,,"Instruction-prompted cartoonization dataset This dataset was created from 5000 images randomly sampled from the Imagenette dataset. For more details on how the dataset was created, check out this directory. Following figure depicts the data preparation workflow: Known limitations and biases The dataset was derived from Imagenette, which, in turn, was derived from ImageNet. So, naturally, this dataset inherits the limitations and biases of ImageNet.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/instruction-tuning-sd/cartoonization."
intanm/indonesian-financial-sentiment-analysis,,,,,,,https://huggingface.co/datasets/intanm/indonesian-financial-sentiment-analysis,,,,,,,,,,,,,,,,,,,,intanm/indonesian-financial-sentiment-analysis dataset hosted on Hugging Face and contributed by the HF Datasets community
internlm/Agent-FLAN,,,,,,https://arxiv.org/abs/2403.12881,https://huggingface.co/datasets/internlm/Agent-FLAN,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models This page holds the dataset proposed in Agent-FLAN, which consists of AgentInstruct, Toolbench, and customized negative agent samples as its source datasets. âœ¨ Introduction [ðŸ¤— HuggingFace] [ðŸ“ƒ Paper] [ðŸŒ Project Page] Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models whenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/internlm/Agent-FLAN."
intfloat/wikidata5m,,,,,,,https://huggingface.co/datasets/intfloat/wikidata5m,,,,,,,,,,,,,,,,,,,,Please check out https://github.com/intfloat/SimKGC/blob/main/scripts/download_wikidata5m.sh on how to download this dataset.
intronhealth/afrispeech-200,Text,,,,,https://github.com/intron-innovation/AfriSpeech-Dataset-Paper,https://huggingface.co/datasets/intronhealth/afrispeech-200,,,,,,,always,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"AFRISPEECH-200 is a 200hr Pan-African speech corpus for clinical and general domain English accented ASR; a dataset with 120 African accents from 13 countries and 2,463 unique African speakers. Our goal is to raise awareness for and advance Pan-African English ASR research, especially for the clinical domain."
ioclab/laplacian_image_aesthetic_3M,,,,,,,https://huggingface.co/datasets/ioclab/laplacian_image_aesthetic_3M,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""laplacian_image_aesthetic_3M"" More Information needed"
ipipan/polqa,Text,,,,,https://arxiv.org/abs/2212.08897,https://huggingface.co/datasets/ipipan/polqa,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"PolQA is the first Polish dataset for OpenQA. It consists of 7,000 questions, 87,525 manually labeled evidence passages, and a corpus of over 7 million candidate passages."
irow/ClothingControlV2,,,,,,,https://huggingface.co/datasets/irow/ClothingControlV2,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""ClothingControlV2"" More Information needed"
isaacus/open-australian-legal-corpus,,,,,,,https://huggingface.co/datasets/isaacus/open-australian-legal-corpus,,,,,,,,https://choosealicense.com/licenses/other/,,2,,,,,,,,,,"Open Australian Legal Corpus â€âš–ï¸ The Open Australian Legal Corpus by Isaacus is the first and only multijurisdictional open corpus of Australian legislative and judicial documents. Comprised of 229,122 texts totalling over 60 million lines and 1.4 billion tokens, the Corpus includes every in force statute and regulation in the Commonwealth, New South Wales, Queensland, Western Australia, South Australia, Tasmania and Norfolk Island, in addition to thousands of bills and hundreds ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/isaacus/open-australian-legal-corpus."
Isamu136/big-animal-dataset,,,,,,,https://huggingface.co/datasets/Isamu136/big-animal-dataset,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""big-animal-dataset"" Hi! I combined animals 10 dataset, the oxford pets dataset, stanford dogs dataset, and the cats vs dogs dataset for a large animal dataset. More Information needed"
itinerai/us_places,,,,,,,https://huggingface.co/datasets/itinerai/us_places,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,itinerai/us_places dataset hosted on Hugging Face and contributed by the HF Datasets community
itsyoboieltr/pcb,,,,,,,https://huggingface.co/datasets/itsyoboieltr/pcb,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,itsyoboieltr/pcb dataset hosted on Hugging Face and contributed by the HF Datasets community
ivelin/ui_refexp_saved,,,,,,,https://huggingface.co/datasets/ivelin/ui_refexp_saved,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for ""ui_refexp_saved_Jan2023"" This is a saved snapshot of the dynamically generated UI Bert dataset. Much faster download time than the dynamic version which pulls and filters large data files from remote sources."
ivrit-ai/audio-transcripts,,,,,,https://arxiv.org/abs/2307.08720,https://huggingface.co/datasets/ivrit-ai/audio-transcripts,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"ivrit.ai is a database of Hebrew audio and text content. audio-base contains the raw, unprocessed sources. audio-vad contains audio snippets generated by applying Silero VAD (https://github.com/snakers4/silero-vad) to the base dataset. audio-transcripts contains transcriptions for each snippet in the audio-vad dataset. The audio-base dataset contains data from the following sources: Geekonomy (Podcast, https://geekonomy.net) HaCongress (Podcast, https://hacongress.podbean.com/) Idan Eretz'sâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ivrit-ai/audio-transcripts."
IWSLT/mt_eng_vietnamese,Text,,,,,https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/,https://huggingface.co/datasets/IWSLT/mt_eng_vietnamese,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,Preprocessed Dataset from IWSLT'15 English-Vietnamese machine translation: English-Vietnamese.
izumi-lab/piqa-ja-mbartm2m,,,,,,,https://huggingface.co/datasets/izumi-lab/piqa-ja-mbartm2m,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"Dataset Card for ""piqa-ja-mbartm2m"" Dataset Description This is the Japanese Translation version of piqa. The translator used in it was facebook/mbart-large-50-many-to-many-mmt. License The same as the original piqa."
JackGao/brain-teaser-chinese,,,,,,,https://huggingface.co/datasets/JackGao/brain-teaser-chinese,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,JackGao/brain-teaser-chinese dataset hosted on Hugging Face and contributed by the HF Datasets community
Jackmin108/c4-en-validation,,,,,,,https://huggingface.co/datasets/Jackmin108/c4-en-validation,,,,,,,,,,,,,,,,,,,,Jackmin108/c4-en-validation dataset hosted on Hugging Face and contributed by the HF Datasets community
jackyhate/text-to-image-2M,,,,,,,https://huggingface.co/datasets/jackyhate/text-to-image-2M,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"text-to-image-2M: A High-Quality, Diverse Text-to-Image Training Dataset Overview text-to-image-2M is a curated text-image pair dataset designed for fine-tuning text-to-image models. The dataset consists of approximately 2 million samples, carefully selected and enhanced to meet the high demands of text-to-image model training. The motivation behind creating this dataset stems from the observation that datasets with over 1 million samples tend to produce betterâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jackyhate/text-to-image-2M."
jacob-hugging-face/job-descriptions,,,,,,,https://huggingface.co/datasets/jacob-hugging-face/job-descriptions,,,,,,,,https://choosealicense.com/licenses/llama2/,,,,,,,,,,,,jacob-hugging-face/job-descriptions dataset hosted on Hugging Face and contributed by the HF Datasets community
Jacobvs/PoliticalTweets,,,,,,,https://huggingface.co/datasets/Jacobvs/PoliticalTweets,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Jacobvs/PoliticalTweets dataset hosted on Hugging Face and contributed by the HF Datasets community
jahjinx/IMDb_movie_reviews,Text,Sentiment Analysis,General,Label,"Accuracy, F1 Score",http://ai.stanford.edu/~amaas/data/sentiment/,https://huggingface.co/datasets/jahjinx/IMDb_movie_reviews,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, DistilBERT",,,Dataset Card for IMDb Movie Reviews Data-Summary: This is a custom train/test/validation split of the IMDb Large Movie Review Dataset available from http://ai.stanford.edu/~amaas/data/sentiment/. Supported Tasks and Leaderboards [More Information Needed] Languages [More Information Needed] Dataset Structure IMDb_movie_reviews An
JailbreakV-28K/JailBreakV-28k,,,,,,https://arxiv.org/abs/2404.03027,https://huggingface.co/datasets/JailbreakV-28K/JailBreakV-28k,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"â›“â€ðŸ’¥ JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks ðŸŒ GitHub | ðŸ›Ž Project Page ï½œ ðŸ‘‰ Download full datasets If you like our project, please give us a star â­ on Hugging Face for the latest update. ðŸ“° News Date Event 2024/07/09 ðŸŽ‰ Our paper is accepted by COLM 2024. 2024/06/22 ðŸ› ï¸ We have updated our version to V0.2, which supports users to customize their attackâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JailbreakV-28K/JailBreakV-28k."
jaimevera1107/similarity-sentences-spanish,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/jaimevera1107/similarity-sentences-spanish,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"similarity-sentences-spanish (SSS) Data-Summary: This dataset comprises a collection of sentences generated using Chat GPT-3, covering various general topics. The dataset also includes sentences from two existing datasets, STS-ES and STSB-Multi-MT, as well as SICK, which were used as additional sources. The sentences in this dataset were generated to exhibit varying levels of similarity based on randomly divided prompts. Source Share (rows) Count (rows)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jaimevera1107/similarity-sentences-spanish."
jainr3/diffusiondb-pixelart,Text,,,,,https://poloclub.github.io/diffusiondb,https://huggingface.co/datasets/jainr3/diffusiondb-pixelart,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,https://github.com/poloclub/diffusiondb/blob/main/notebooks/example-loading.ipynb,,,,,,,,"DiffusionDB is the first large-scale text-to-image prompt dataset. It contains 2 million images generated by Stable Diffusion using prompts and hyperparameters specified by real users. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generative models, detecting deepfakes, and designing human-AI interaction tools to help users more easily use these models."
jakartaresearch/indoqa,Text,,,,,,https://huggingface.co/datasets/jakartaresearch/indoqa,,,,,,,,https://choosealicense.com/licenses/cc-by-nd-4.0/,,,,,,,,,,,,This dataset is built for question answering task.
james-burton/jigsaw_unintended_bias100K,,,,,,,https://huggingface.co/datasets/james-burton/jigsaw_unintended_bias100K,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""jigsaw_unintended_bias100K"" More Information needed"
jamescalam/image-text-demo,,,,,,,https://huggingface.co/datasets/jamescalam/image-text-demo,,,,,,,,,,,,,,,,,,,,Demo dataset for testing or showing image-text capabilities.
JamesStratford/voice-of-birds,,,,,,,https://huggingface.co/datasets/JamesStratford/voice-of-birds,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""voice-of-birds"" More Information needed"
JanosAudran/financial-reports-sec,Text,,,,,,https://huggingface.co/datasets/JanosAudran/financial-reports-sec,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,The dataset contains the annual report of US public firms filing with the SEC EDGAR system. Each annual report (10K filing) is broken into 20 sections. Each section is split into individual sentences. Sentiment labels are provided on a per filing basis from the market reaction around the filing data. Additional metadata for each filing is included in the dataset.
janspoerer/fomc,,,,,,,https://huggingface.co/datasets/janspoerer/fomc,,,,,,,,,,,,,,,,,,,,janspoerer/fomc dataset hosted on Hugging Face and contributed by the HF Datasets community
jaredfern/codah,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/jaredfern/codah,,,,,,,,https://choosealicense.com/licenses/odc-by/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for COmmonsense Dataset Adversarially-authored by Humans Data-Summary: The COmmonsense Dataset Adversarially-authored by Humans (CODAH) is an evaluation set for commonsense question-answering in the sentence completion style of SWAG. As opposed to other automatically generated NLI datasets, CODAH is adversarially constructed by humans who can view feedback from a pre-trained model and use this information to design challenging commonsense questions.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jaredfern/codah."
Jaren/T5-dialogue-pretrain-data,,,,,,,https://huggingface.co/datasets/Jaren/T5-dialogue-pretrain-data,,,,,,,,,,,,,,,,,,,,"This dataset is converted from duconv, durecdial, ecm, naturalconv, persona, tencent, kdconv, crosswoz,risawoz,diamante,restoration and LCCC-base 12 high quality datasets and is used for continue pretrain task for T5-pegasus in mengzi version."
jarvisx17/Medical-ASR-EN,,,,,,,https://huggingface.co/datasets/jarvisx17/Medical-ASR-EN,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,jarvisx17/Medical-ASR-EN dataset hosted on Hugging Face and contributed by the HF Datasets community
JasperLS/prompt-injections,,,,,,,https://huggingface.co/datasets/JasperLS/prompt-injections,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""deberta-v3-base-injection-dataset"" More Information needed"
jat-project/jat-dataset,,,,,,https://arxiv.org/abs/2402.09844,https://huggingface.co/datasets/jat-project/jat-dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"JAT Dataset Dataset Description The Jack of All Trades (JAT) dataset combines a wide range of individual datasets. It includes expert demonstrations by expert RL agents, image and caption pairs, textual data and more. The JAT dataset is part of the JAT project, which aims to build a multimodal generalist agent. Paper: https://huggingface.co/papers/2402.09844 Usage >>> from datasets import load_dataset >>> dataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jat-project/jat-dataset."
JAugusto97/told-br,Text,,,,,https://paperswithcode.com/dataset/told-br,https://huggingface.co/datasets/JAugusto97/told-br,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"ToLD-Br is the biggest dataset for toxic tweets in Brazilian Portuguese, crowdsourced by 42 annotators selected from a pool of 129 volunteers. Annotators were selected aiming to create a plural group in terms of demographics (ethnicity, sexual orientation, age, gender). Each tweet was labeled by three annotators in 6 possible categories: LGBTQ+phobia,Xenophobia, Obscene, Insult, Misogyny and Racism."
Jaybore/autotrain-data-weibosent,,,,,,,https://huggingface.co/datasets/Jaybore/autotrain-data-weibosent,,,,,,,,,,,,,,,,,,,,Jaybore/autotrain-data-weibosent dataset hosted on Hugging Face and contributed by the HF Datasets community
jaydenccc/AI_Storyteller_Dataset,,,,,,,https://huggingface.co/datasets/jaydenccc/AI_Storyteller_Dataset,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""AI_Storyteller_Dataset"" More Information needed"
jaydeyoung/evidence_infer_treatment,Text,,,,,http://evidence-inference.ebm-nlp.com/,https://huggingface.co/datasets/jaydeyoung/evidence_infer_treatment,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Data and code from our ""Inferring Which Medical Treatments Work from Reports of Clinical Trials"", NAACL 2019. This work concerns inferring the results reported in clinical trials from text. The dataset consists of biomedical articles describing randomized control trials (RCTs) that compare multiple treatments. Each of these articles will have multiple questions, or 'prompts' associated with them. These prompts will ask about the relationship between an intervention and comparator with respect to an outcome, as reported in the trial. For"
Jayveersinh-Raj/bad-improved-prompt-pairs,,,,,,,https://huggingface.co/datasets/Jayveersinh-Raj/bad-improved-prompt-pairs,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,This is a dataset for prompt tuning a language model to improve the provided vague prompts. The dataset contains bad prompts with corresponding good/improved prompts.
jclian91/people_relation_classification,,,,,,,https://huggingface.co/datasets/jclian91/people_relation_classification,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"æœ¬æ•°æ®é›†ç”¨äºŽäººç‰©å…³ç³»åˆ†ç±»ï¼Œä¸€å…±14ç§å…³ç³»ç±»åž‹ï¼šä¸ç¡®å®š, å¤«å¦», çˆ¶æ¯, å…„å¼Ÿå§å¦¹, ä¸Šä¸‹çº§, å¸ˆç”Ÿ, å¥½å‹, åŒå­¦, åˆä½œ, åŒä¸€ä¸ªäºº, æƒ…ä¾£, ç¥–å­™, åŒé—¨, äº²æˆšã€‚ æœ¬æ•°æ®é›†å…±3881æ¡ï¼Œå…¶ä¸­è®­ç»ƒé›†3105æ¡ï¼Œæµ‹è¯•é›†776æ¡ï¼Œå‚çœ‹train.csvå’Œtest.csvã€‚ æ•°æ®é›†çš„äººç‰©å…³ç³»åˆ†å¸ƒå¦‚ä¸‹ï¼š å…³äºŽä½¿ç”¨R-BERTæ¨¡åž‹è®­ç»ƒè¯¥æ•°æ®é›†ï¼Œå¯å‚è€ƒæ–‡ç« ï¼šNLPï¼ˆå››åäºŒï¼‰äººç‰©å…³ç³»åˆ†ç±»çš„å†æ¬¡å°è¯•."
Jean-Baptiste/wikiner_fr,,,,,,,https://huggingface.co/datasets/Jean-Baptiste/wikiner_fr,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""wikiner_fr"" Dataset Description: Homepage: https://metatext.io/datasets/wikiner Repository: Paper: https://www.sciencedirect.com/science/article/pii/S0004370212000276?via%3Dihub Leaderboard: Point of Contact:"
JeanKaddour/minipile,Text,General,Document,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2304.08442,https://huggingface.co/datasets/JeanKaddour/minipile,,6 GB,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for MiniPile Dataset Description The MiniPile Challenge for Data-Efficient Language Models Data-Summary: MiniPile is a 6GB subset of the deduplicated The Pile corpus. To curate MiniPile, we perform a simple, three-step data filtering process: we (1) infer embeddings for all documents of the Pile, (2) cluster the embedding space using k-means, and (3) filter out low-quality clusters. The primary motivation for curating MiniPile is thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JeanKaddour/minipile."
jeanlee/kmhas_korean_hate_speech,Text,,,,,https://github.com/adlnlp/K-MHaS,https://huggingface.co/datasets/jeanlee/kmhas_korean_hate_speech,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"The K-MHaS (Korean Multi-label Hate Speech) dataset contains 109k utterances from Korean online news comments labeled with 8 fine-grained hate speech classes or Not Hate Speech class. The fine-grained hate speech classes are politics, origin, physical, age, gender, religion, race, and profanity and these categories are selected in order to reflect the social and historical context."
jed351/cantonese-wikipedia,,,,,,,https://huggingface.co/datasets/jed351/cantonese-wikipedia,,,,,,,,,,,,,,,,,,,,jed351/cantonese-wikipedia dataset hosted on Hugging Face and contributed by the HF Datasets community
jeffnyman/emotions,Text,,,,,,https://huggingface.co/datasets/jeffnyman/emotions,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. For more detailed information please refer to the paper."
JeffreyXiang/TRELLIS-500K,,,,,,https://arxiv.org/abs/2412.01506,https://huggingface.co/datasets/JeffreyXiang/TRELLIS-500K,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"TRELLIS-500K TRELLIS-500K is a dataset of 500K 3D assets curated from Objaverse(XL), ABO, 3D-FUTURE, HSSD, and Toys4k, filtered based on aesthetic scores. This dataset serves for 3D generation tasks. It was introduced in the paper Structured 3D Latents for Scalable and Versatile 3D Generation. Dataset Statistics The following table summarizes the dataset's filtering and composition: NOTE: Some of the 3D assets lack text captions. Please filter out such assets ifâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JeffreyXiang/TRELLIS-500K."
jerryjalapeno/nart-100k-synthetic,,,,,,,https://huggingface.co/datasets/jerryjalapeno/nart-100k-synthetic,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,Keep in mind that this dataset is entirely synthetic. It is not fully representative of real therapy situations. If you are training an LLM therapist keep in mind the limitations of LLMs and highlight those limitations to users in a responsible manner.
jet-universe/jetclass,Text,General,UI,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/jet-universe/jetclass,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for JetClass Data-Summary: JetClass is a large and comprehensive dataset to advance deep learning for jet tagging. The dataset consists of 100 million jets for training, with 10 different types of jets. The jets in this dataset generally fall into two categories: The background jets are initiated by light quarks or gluons (q/g) and are ubiquitously produced at the LHC. The signal jets are those arising either from the top quarks (t), or from the Wâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jet-universe/jetclass."
JetBrains-Research/commit-chronicle,,,,,,https://arxiv.org/abs/2308.07655,https://huggingface.co/datasets/JetBrains-Research/commit-chronicle,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"ðŸ“œ CommitChronicle ðŸ”® This is the dataset for commit message generation (and/or completion), introduced in the paper ""From Commit Message Generation to History-Aware Commit Message Completion"", ASE 2023. Its key features: large-scale and multilingual: contains 10.7M commits from 11.9k GitHub repositories in 20 programming languages; diverse: avoids restrictive filtering on commit messages or commit diffs structure; suitable for experiments with commit history: provides metadataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JetBrains-Research/commit-chronicle."
jfrenz/legalglue,Text,,,,,https://arxiv.org/abs/2003.13016,https://huggingface.co/datasets/jfrenz/legalglue,,,,,,,,,,,,,,,,,,,,\ Legal General Language Understanding Evaluation (LegalGLUE) benchmark is a collection of datasets for evaluating model performance across a diverse set of legal NLP tasks
jglaser/binding_affinity,,,,,,,https://huggingface.co/datasets/jglaser/binding_affinity,,,,,,,,,,,,,,,,,,,,A dataset to fine-tune language models on protein-ligand binding affinity prediction.
jhu-clsp/seamless-align,Audio,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/jhu-clsp/seamless-align,pairs,1000 GB,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Seamless-Align (WIP). Inspired by https://huggingface.co/datasets/allenai/nllb Data-Summary: This dataset was created based on metadata for mined Speech-to-Speech(S2S), Text-to-Speech(TTS) and Speech-to-Text(S2T) released by Meta AI. The S2S contains data for 35 language pairs. The S2S dataset is ~1000GB compressed. How to use the data There are two ways to access the data: Via the Hugging Face Python datasets library Scripts coming soonâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/seamless-align."
jiacheng-ye/nl2bash,,,,,,,https://huggingface.co/datasets/jiacheng-ye/nl2bash,,,,,,,,,,,,,,,,,,,,The dataset is constructed from https://github.com/TellinaTool/nl2bash
jiaqianjing/PatentData,,,,,,,https://huggingface.co/datasets/jiaqianjing/PatentData,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,æ•°æ®æ¥æº ä¸­å›½ä¸“åˆ©ä¿¡æ¯ä¸­å¿ƒ å­—æ®µè§£é‡Š patent_idï¼šä¸“åˆ©ç¼–å· patent_pub_dateï¼šä¸“åˆ©å…¬å¸ƒæ—¥æœŸ titleï¼šä¸“åˆ©åç§° applicantï¼šç”³è¯·äºº/å•ä½ application_dateï¼šç”³è¯·æ—¥æœŸ inventorsï¼šå‘æ˜Žäºº summaryï¼šæ‘˜è¦ descriptionï¼šè¯´æ˜Žä¹¦å…¨æ–‡ claimï¼šä¸“åˆ©æƒåˆ©è¦æ±‚ä¹¦å…¨æ–‡ ä½¿ç”¨é™åˆ¶ ä»…å…è®¸å°†æ­¤æ•°æ®é›†åŠä½¿ç”¨æ­¤æ•°æ®é›†ç”Ÿæˆçš„è¡ç”Ÿç‰©ç”¨äºŽç ”ç©¶ç›®çš„ï¼Œä¸å¾—ç”¨äºŽå•†ä¸šï¼Œä»¥åŠå…¶ä»–ä¼šå¯¹ç¤¾ä¼šå¸¦æ¥å±å®³çš„ç”¨é€”ã€‚ æœ¬æ•°æ®é›†ä¸ä»£è¡¨ä»»ä½•ä¸€æ–¹çš„ç«‹åœºã€åˆ©ç›Šæˆ–æƒ³æ³•ï¼Œæ— å…³ä»»ä½•å›¢ä½“çš„ä»»ä½•ç±»åž‹çš„ä¸»å¼ ã€‚å› ä½¿ç”¨æœ¬æ•°æ®é›†å¸¦æ¥çš„ä»»ä½•æŸå®³ã€çº çº·ï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚
Jiayi-Pan/Countdown-Tasks-3to4,,,,,,,https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4,,,,,,,,,,,,,,,,,,,,Jiayi-Pan/Countdown-Tasks-3to4 dataset hosted on Hugging Face and contributed by the HF Datasets community
jinaai/negation-dataset,,,,,,https://arxiv.org/abs/2307.11224,https://huggingface.co/datasets/jinaai/negation-dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"The data offered by Jina AI, Finetuner team. Summary This dataset is an English-language dataset based on the SNLI dataset. It contains negations of samples from SNLI. Instances Each data point consists of a triplet ('anchor', 'entailment', 'negative') of strings, where ('anchor', 'entailment') are positive pairs taken from SNLI, and 'negative' contradicts both 'anchor' and 'entailment'. Fields 'anchor': string, some statementâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jinaai/negation-dataset."
JinaLeejnl/AlignX,,,,,,https://arxiv.org/abs/2503.17003,https://huggingface.co/datasets/JinaLeejnl/AlignX,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"News [2025/03/24]: We have published a survey that presents the first comprehensive review of personalized alignmentâ€”a paradigm that enables LLMs to adapt their behavior within ethical boundaries based on individual preferences. For more details, see A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications. Dataset Statistics The table below summarizes the data sources and statistics for AlignX, involving bothâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JinaLeejnl/AlignX."
jing-bi/verify-teaser,,,,,,https://arxiv.org/abs/2503.11557,https://huggingface.co/datasets/jing-bi/verify-teaser,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning FidelitY VERIFY is the first benchmark explicitly designed to assess the reasoning paths of MLLMs in visual reasoning tasks. By introducing novel evaluation metrics that go beyond mere accuracy, VERIFY highlights critical limitations in current MLLMs and emphasizes the need for a more balanced approach to visual perception and logical reasoning. Details of the benchmark can viewed at theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jing-bi/verify-teaser."
jingyaogong/minimind_dataset,,,,,,,https://huggingface.co/datasets/jingyaogong/minimind_dataset,,,,,,,,,,,,,,,,,,,,"ðŸ“Œ æ•°æ®ä»‹ç» â…  Tokenizer åˆ†è¯å™¨å°†å•è¯ä»Žè‡ªç„¶è¯­è¨€é€šè¿‡â€œè¯å…¸â€æ˜ å°„åˆ°0, 1, 36è¿™æ ·çš„æ•°å­—ï¼Œå¯ä»¥ç†è§£ä¸ºæ•°å­—å°±ä»£è¡¨äº†å•è¯åœ¨â€œè¯å…¸â€ä¸­çš„é¡µç ã€‚ å¯ä»¥é€‰æ‹©è‡ªå·±æž„é€ è¯è¡¨è®­ç»ƒä¸€ä¸ªâ€œè¯å…¸â€ï¼Œä»£ç å¯è§./scripts/train_tokenizer.pyï¼ˆä»…ä¾›å­¦ä¹ å‚è€ƒï¼Œè‹¥éžå¿…è¦æ— éœ€å†è‡ªè¡Œè®­ç»ƒï¼ŒMiniMindå·²è‡ªå¸¦tokenizerï¼‰ã€‚ æˆ–è€…é€‰æ‹©æ¯”è¾ƒå‡ºåçš„å¼€æºå¤§æ¨¡åž‹åˆ†è¯å™¨ï¼Œ æ­£å¦‚åŒç›´æŽ¥ç”¨æ–°åŽ/ç‰›æ´¥è¯å…¸çš„ä¼˜ç‚¹æ˜¯tokenç¼–ç åŽ‹ç¼©çŽ‡å¾ˆå¥½ï¼Œç¼ºç‚¹æ˜¯é¡µæ•°å¤ªå¤šï¼ŒåŠ¨è¾„æ•°åä¸‡ä¸ªè¯æ±‡çŸ­è¯­ï¼› è‡ªå·±è®­ç»ƒçš„åˆ†è¯å™¨ï¼Œä¼˜ç‚¹æ˜¯è¯è¡¨é•¿åº¦å’Œå†…å®¹éšæ„æŽ§åˆ¶ï¼Œç¼ºç‚¹æ˜¯åŽ‹ç¼©çŽ‡å¾ˆä½Žï¼ˆä¾‹å¦‚""hello""ä¹Ÿè®¸ä¼šè¢«æ‹†åˆ†ä¸º""h e l l o"" äº”ä¸ªç‹¬ç«‹çš„tokenï¼‰ï¼Œä¸”ç”Ÿåƒ»è¯éš¾ä»¥è¦†ç›–ã€‚ â€œè¯å…¸â€çš„é€‰æ‹©å›ºç„¶å¾ˆé‡è¦ï¼ŒLLMçš„è¾“å‡ºæœ¬è´¨ä¸Šæ˜¯SoftMaxåˆ°è¯å…¸Nä¸ªè¯çš„å¤šåˆ†ç±»é—®é¢˜ï¼Œç„¶åŽé€šè¿‡â€œè¯å…¸â€è§£ç åˆ°è‡ªç„¶è¯­è¨€ã€‚ å› ä¸ºMiniMindä½“ç§¯éœ€è¦ä¸¥æ ¼æŽ§åˆ¶ï¼Œä¸ºäº†é¿å…æ¨¡åž‹å¤´é‡è„šè½»ï¼ˆè¯åµŒå…¥embeddingå±‚å‚æ•°åœ¨LLMå æ¯”å¤ªé«˜ï¼‰ï¼Œæ‰€ä»¥è¯è¡¨é•¿åº¦çŸ­çŸ­ç›Šå–„ã€‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jingyaogong/minimind_dataset."
JingyeChen22/TextDiffuser,,,,,,,https://huggingface.co/datasets/JingyeChen22/TextDiffuser,,,,,,,,,,,,,,,,,,,,JingyeChen22/TextDiffuser dataset hosted on Hugging Face and contributed by the HF Datasets community
jinmang2/ucf_crime,,,,,,,https://huggingface.co/datasets/jinmang2/ucf_crime,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"# Real-world Anomaly Detection in Surveillance Videos Surveillance videos are able to capture a variety of realistic anomalies. In this paper, we propose to learn anomalies by exploiting both normal and anomalous videos. To avoid annotating the anomalous segments or clips in training videos, which is very time consuming, we propose to learn anomaly through the deep multiple instance ranking framework by leveraging weakly labeled training videos, i.e. the training labels (anomalous or normal) are at video-level instead of clip-level. In our approach, we consider normal and anomalous videos as bags and video segments as instances in multiple instance learning (MIL), and automatically learn a deep anomaly ranking model that predicts high anomaly scores for anomalous video segments. Furthermore, we introduce sparsity and temporal smoothness constraints in the ranking loss function to better localize anomaly during training. We also introduce a new large-scale first of its kind dataset of 128 hours of videos. It consists of 1900 long and untrimmed real-world surveillance videos, with 13 realistic anomalies such as fighting, road accident, burglary, robbery, etc. as well as normal activities. This dataset can be used for two tasks. First, general anomaly detection considering all anomalies in one group and all normal activities in another group. Second, for recognizing each of 13 anomalous activities. Our experimental results show that our MIL method for anomaly detection achieves significant improvement on anomaly detection performance as compared to the state-of-the-art approaches. We provide the results of several recent deep learning baselines on anomalous activity recognition. The low recognition performance of these baselines reveals that our dataset is very challenging and opens more opportunities for future work. # Problem & Motivation One critical task in video surveillance is detecting anomalous events such as traffic accidents, crimes or illegal activities. Generally, anomalous events rarely occur as compared to normal activities. Therefore, to alleviate the waste of labor and time, developing intelligent computer vision algorithms for automatic video anomaly detection is a pressing need. The goal of a practical anomaly detection system is to timely signal an activity that deviates normal patterns and identify the time window of the occurring anomaly. Therefore, anomaly detection can be considered as coarse level video understanding, which filters out anomalies from normal patterns. Once an anomaly is detected, it can further be categorized into one of the specific activities using classification techniques. In this work, we propose an anomaly detection algorithm using weakly labeled training videos. That is we only know the video-level labels, i.e. a video is normal or contains anomaly somewhere, but we do not know where. This is intriguing because we can easily annotate a large number of videos by only assigning video-level labels. To formulate a weakly-supervised learning approach, we resort to multiple instance learning. Specifically, we propose to learn anomaly through a deep MIL framework by treating normal and anomalous surveillance videos as bags and short segments/clips of each video as instances in a bag. Based on training videos, we automatically learn an anomaly ranking model that predicts high anomaly scores for anomalous segments in a video. During testing, a longuntrimmed video is divided into segments and fed into our deep network which assigns anomaly score for each video segment such that an anomaly can be detected. # Method Our proposed approach (summarized in Figure 1) begins with dividing surveillance videos into a fixed number of segments during training. These segments make instances in a bag. Using both positive (anomalous) and negative (normal) bags, we train the anomaly detection model using the proposed deep MIL ranking loss. https://www.crcv.ucf.edu/projects/real-world/method.png # UCF-Crime Dataset We construct a new large-scale dataset, called UCF-Crime, to evaluate our method. It consists of long untrimmed surveillance videos which cover 13 realworld anomalies, including Abuse, Arrest, Arson, Assault, Road Accident, Burglary, Explosion, Fighting, Robbery, Shooting, Stealing, Shoplifting, and Vandalism. These anomalies are selected because they have a significant impact on public safety. We compare our dataset with previous anomaly detection datasets in Table 1. For more details about the UCF-Crime dataset, please refer to our paper. A short description of each anomalous event is given below. Abuse: This event contains videos which show bad, cruel or violent behavior against children, old people, animals, and women. Burglary: This event contains videos that show people (thieves) entering into a building or house with the intention to commit theft. It does not include use of force against people. Robbery: This event contains videos showing thieves taking money unlawfully by force or threat of force. These videos do not include shootings. Stealing: This event contains videos showing people taking property or money without permission. They do not include shoplifting. Shooting: This event contains videos showing act of shooting someone with a gun. Shoplifting: This event contains videos showing people stealing goods from a shop while posing as a shopper. Assault: This event contains videos showing a sudden or violent physical attack on someone. Note that in these videos the person who is assaulted does not fight back. Fighting: This event contains videos displaying two are more people attacking one another. Arson: This event contains videos showing people deliberately setting fire to property. Explosion: This event contains videos showing destructive event of something blowing apart. This event does not include videos where a person intentionally sets a fire or sets off an explosion. Arrest: This event contains videos showing police arresting individuals. Road Accident: This event contains videos showing traffic accidents involving vehicles, pedestrians or cyclists. Vandalism: This event contains videos showing action involving deliberate destruction of or damage to public or private property. The term includes property damage, such as graffiti and defacement directed towards any property without permission of the owner. Normal Event: This event contains videos where no crime occurred. These videos include both indoor (such as a shopping mall) and outdoor scenes as well as day and night-time scenes. https://www.crcv.ucf.edu/projects/real-world/dataset_table.png https://www.crcv.ucf.edu/projects/real-world/method.png"
jiovine/pixel-art-nouns-2k,,,,,,,https://huggingface.co/datasets/jiovine/pixel-art-nouns-2k,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""pixel-art-nouns-2k"" More Information needed"
jitx/Methods2Test_java_unit_test_code,,,,,,https://arxiv.org/abs/2203.12776,https://huggingface.co/datasets/jitx/Methods2Test_java_unit_test_code,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Dataset Description Microsoft created this large dataset of Java Junit test cases with its corresponding focal methods. It contains 780k pairs of JUnit test cases and focal methods which were extracted from a total of 91K Java open source project hosted on GitHub. The mapping between test case and focal methods are based heuristics rules and Java developer's best practice. More information could be found here: methods2test Github repo Methods2Test: A dataset of focal methodsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jitx/Methods2Test_java_unit_test_code.
jkeisling/hacker-news-corpus-2007-2022,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/jkeisling/hacker-news-corpus-2007-2022,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Hacker News corpus, 2007-Nov 2022 Dataset Description Data-Summary: Dataset Name: Hacker News Full Corpus (2007 - November 2022) Description: NOTE: I am not affiliated with Y Combinator. This dataset is a July 2023 snapshot of YCombinator's BigQuery dump of the entire archive of posts and comments made on Hacker News. It contains posts from Hacker News' inception in 2007 through to November 16, 2022, when the BigQuery database was last updated.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jkeisling/hacker-news-corpus-2007-2022."
jkhedri/psychology-dataset,,,,,,,https://huggingface.co/datasets/jkhedri/psychology-dataset,,,,,,,,,,,,,,,,,,,,jkhedri/psychology-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
jlbaker361/anime_facesk,,,,,,,https://huggingface.co/datasets/jlbaker361/anime_facesk,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""anime_facesk"" More Information needed"
jlohding/sp500-edgar-10k,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/jlohding/sp500-edgar-10k,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for SP500-EDGAR-10K Data-Summary: This dataset contains the annual reports for all SP500 historical constituents from 2010-2022 from SEC EDGAR Form 10-K filings. It also contains n-day future returns of each firm's stock price from each filing date. Dataset Structure Data Fields [More Information Needed] Data Splits [More Information Needed] Dataset Creation Source Dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jlohding/sp500-edgar-10k.
jlvdoorn/atco2-asr,,,,,,,https://huggingface.co/datasets/jlvdoorn/atco2-asr,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""ATCO2-ASR"" This is audio data used for automatic speech recognition. The original source of the data is the ATCO2 project, specifically the ASR part of the public speech corpus."
jmhb/microvqa,,,,,,https://arxiv.org/abs/2503.13399,https://huggingface.co/datasets/jmhb/microvqa,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research (CVPR 2025) ðŸŒ Homepage / blog â€¢ ðŸ“ arXiv â€¢ ðŸ¤— HF Dataset â€¢ ðŸ’» Code â€¢ ðŸ› CC-BY-SA-4.0 MicroVQA is expert-curated benchmark for multimodal reasoning for microscopy-based scientific research, proposed in the paper MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research. Paper abstract Scientific research demands sophisticated reasoning over multimodalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jmhb/microvqa."
joelniklaus/brazilian_court_decisions,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/joelniklaus/brazilian_court_decisions,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for predicting-brazilian-court-decisions Data-Summary: The dataset is a collection of 4043 Ementa (summary) court decisions and their metadata from the Tribunal de JustiÃ§a de Alagoas (TJAL, the State Supreme Court of Alagoas (Brazil). The court decisions are labeled according to 7 categories and whether the decisions were unanimous on the part of the judges or not. The dataset supports the task of Legal Judgment Prediction. Supportedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/joelniklaus/brazilian_court_decisions."
Joemgu/sumstew,Text,,,,,,https://huggingface.co/datasets/Joemgu/sumstew,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset Card for ""sumstew"" TL;DR: Sumstew is a abstractive, multilingual Dataset, with a balanced number of samples from a diverse set of summarization Datasets. The input sizes range up to 16384 tokens. Filtered using a diverse set of heuristics to encourage high coverage, accuracy and factual consistency. Code to reproduce Dataset available at TODO Task Information Task Categories: The tasks covered by this dataset are primarily summarizationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Joemgu/sumstew."
joey234/mmlu-high_school_biology-neg,,,,,,,https://huggingface.co/datasets/joey234/mmlu-high_school_biology-neg,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""mmlu-high_school_biology-neg"" More Information needed"
Jofthomas/hermes-function-calling-thinking-V1,,,,,,,https://huggingface.co/datasets/Jofthomas/hermes-function-calling-thinking-V1,,,,,,,,,,,,,,,,,,,,Jofthomas/hermes-function-calling-thinking-V1 dataset hosted on Hugging Face and contributed by the HF Datasets community
johko/german_municipal_coat_of_arms,,,,,,,https://huggingface.co/datasets/johko/german_municipal_coat_of_arms,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"German Municipal Coat of Arms Dataset This dataset contains 13104 samples for German municipal coat of arms. Each sample consists of the following features: 'img', 'acceptance', 'municipality', 'description', 'id', historicalJustification', 'municipalityName', 'uri', 'figure', 'cancellation', 'cancellationReason', 'author'"
jojo0217/korean_rlhf_dataset,,,,,,,https://huggingface.co/datasets/jojo0217/korean_rlhf_dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"ì„±ê· ê´€ëŒ€í•™êµ ì‚°í•™í˜‘ë ¥í”„ë¡œì íŠ¸ ê³¼ì •ì—ì„œ í•œêµ­ì–´ llm ëª¨ë¸ SFT í•™ìŠµì„ ìœ„í•´ êµ¬ì¶•í•œ ë°ì´í„°ì…‹ ìž…ë‹ˆë‹¤.2023-09-25ì˜¤í”ˆ ì–´ì‹œìŠ¤í„´íŠ¸ dataì—ì„œ ì˜¤í”ˆ ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ë°ì´í„° ì‚­ì œ-> ë‹µë³€ì— ì˜¤í”ˆ ì–´ì‹œìŠ¤í„´íŠ¸ë¼ê³  í•˜ëŠ” ê²½ìš°ê°€ ë‚˜ì˜¤ê¸° ë•Œë¬¸ë˜í•œ ìŠ¤íƒ í¬ë“œ ëŒ€í•™ ë²ˆì—­ ë°ì´í„°ì—ì„œ ë²ˆì—­ ê³¼ì • ì˜¤ë¥˜ë¡œ inputì— ìž…ë ¥ì—†ìŒ ê³¼ ê°™ì´ ì¶”ê°€ëœ ë¶€ë¶„ ì‚­ì œê·¸ë¦¬ê³  <unk> ë“±ìœ¼ë¡œ gpt ìƒì—ì„œ ë²ˆì—­ ì˜¤ë¥˜ê°€ ë‚œ ê²ƒë“¤ì„ ì‚­ì œ ìžì—°ìŠ¤ëŸ¬ì›€ì„ ìœ„í•´ stanford alpaca data, oig_chip2ë¥¼ ChatGPT3.5 turbo 16kë¥¼ ì´ìš©í•˜ì—¬ ìƒˆë¡­ê²Œ ì „ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤.https://github.com/JoJo0217/rlhf_korean_dataset/tree/mainì—¬ê¸°ì—ì„œ ìžì„¸í•œ ì„¤ëª…ì„ ë³¼ ìˆ˜ ìžˆìœ¼ë©°ë°ì´í„°ì˜ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ë°ì´í„° êµ¬ì„± ë°ì´í„° ì¢…ë¥˜ ê°œìˆ˜ url koalpaca v1.1 21155â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jojo0217/korean_rlhf_dataset."
jonathan-roberts1/NWPU-RESISC45,,,,,,,https://huggingface.co/datasets/jonathan-roberts1/NWPU-RESISC45,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"Dataset Card for ""NWPU-RESISC45"" Licensing Information [CC-BY-SA] Citation Information Remote sensing image scene classification: Benchmark and state of the art @article{cheng2017remote, title = {Remote sensing image scene classification: Benchmark and state of the art}, author = {Cheng, Gong and Han, Junwei and Lu, Xiaoqiang}, year = 2017, journal = {Proceedings of the IEEE}, publisher = {IEEE}, volumeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jonathan-roberts1/NWPU-RESISC45."
jonathanli/legal-advice-reddit,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/jonathanli/legal-advice-reddit,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",ter-Efficient Legal Domain Adaptation (Li et al,,"Dataset Card for Legal Advice Reddit Dataset Data-Summary: New dataset introduced in Parameter-Efficient Legal Domain Adaptation (Li et al., NLLP 2022) from the Legal Advice Reddit community (known as ""/r/legaldvice""), sourcing the Reddit posts from the Pushshift Reddit dataset. The dataset maps the text and title of each legal question posted into one of eleven classes, based on the original Reddit post's ""flair"" (i.e., tag). Questions are typically informalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jonathanli/legal-advice-reddit."
JonathanSum/github-issues,,,,,,,https://huggingface.co/datasets/JonathanSum/github-issues,,,,,,,,,,,3,,,,,,,,,JonathanSum/github-issues dataset hosted on Hugging Face and contributed by the HF Datasets community
jondurbin/airoboros-uncensored,,,,,,,https://huggingface.co/datasets/jondurbin/airoboros-uncensored,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Usage and License Notices All airoboros models and datasets are intended and licensed for research use only. I've used the 'cc-nc-4.0' license, but really it is subject to a custom/special license because: the base model is LLaMa, which has it's own special research license the dataset(s) were generated with OpenAI (gpt-4 and/or gpt-3.5-turbo), which has a clausing saying the data can't be used to create models to compete with openai So, to reiterate: this model (andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jondurbin/airoboros-uncensored."
joonhok-exo-ai/korean_law_open_data_precedents,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/joonhok-exo-ai/korean_law_open_data_precedents,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for Dataset Name ê³µì§€ì‚¬í•­ ì¸ê³µì§€ëŠ¥ ë‚˜í™€ë¡œì†Œì†¡ ì„œë¹„ìŠ¤ë¥¼ ë§Œë“¤ì–´ ë³´ê³  ìžˆìŠµë‹ˆë‹¤. ëª¨ë°”ì¼ì—ì„œ íƒ­ê³¼ ìŠ¤í¬ë¡¤ë§Œìœ¼ë¡œ ì†Œìž¥ì„ ì™„ì„±í•˜ê³  ì „ìžì†Œì†¡ ì‚¬ì´íŠ¸ë¥¼ ê±°ì¹˜ì§€ ì•Šê³  ì œì¶œê¹Œì§€ ê°€ëŠ¥í•œ ì„œë¹„ìŠ¤ë¥¼ ë§Œë“¤ì–´ ë³´ê³  ìžˆëŠ”ë° ë³´ë‹¤ ìžì„¸í•œ ë‚´ìš©ì€ ì´ ë§í¬ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”. ì €ëž‘ ê°™ì´ ë§Œë“¤ì–´ ë³´ì‹¤ ë¶„ì€ joonhok@botoai.coë¡œ ì—°ë½ ë°”ëžë‹ˆë‹¤. ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­ ì‚¬ê±´ë²ˆí˜¸ê°€ ë™ì¼í•œ ì¤‘ë³µ ë°ì´í„°ê°€ ì•½ 200ì—¬ê±´ í¬í•¨ë¼ìžˆìŠµë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ë²•ì œì²˜ êµ­ê°€ë²•ë ¹ ê³µë™í™œìš© ì„¼í„° íŒë¡€ ëª©ë¡ ì¡°íšŒ APIê°€ íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸ëŠ” ë‹¤ë¥´ì§€ë§Œ ì‚¬ê±´ë²ˆí˜¸ ë° ê·¸ ë°–ì— ë‹¤ë¥¸ í•„ë“œ ê°’ë“¤ì€ ì™„ì „ížˆ ë™ì¼í•œ ë°ì´í„°ë“¤ì„ ë¦¬í„´í•˜ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤. ì‚¬ìš©ì— ì°¸ê³ í•˜ì‹œê¸° ë°”ëžë‹ˆë‹¤. Data-Summary: 2023ë…„ 6ì›” ê¸°ì¤€ìœ¼ë¡œ ë²•ì œì²˜ êµ­ê°€ë²•ë ¹ ê³µë™í™œìš© ì„¼í„°ì—ì„œ ì œê³µëœ ì „ì²´ íŒë¡€ ë°ì´í„°ì…‹ìž…ë‹ˆë‹¤. ê·¸ ì´í›„ë¡œâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/joonhok-exo-ai/korean_law_open_data_precedents.
jordiae/exebench,,,,,,,https://huggingface.co/datasets/jordiae/exebench,,,,,,,,,,,,,,,,,,,,An ML-scale dataset of executable C functions
jordiclive/wikipedia-summary-dataset,Text,General,Scientific,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/jordiclive/wikipedia-summary-dataset,processing,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Data-Summary: This is a dataset that can be used for research into machine learning and natural language processing. It contains all titles and summaries (or introductions) of English Wikipedia articles, extracted in September of 2017. The dataset is different from the regular Wikipedia dump and different from the datasets that can be created by gensim because ours contains the extracted summaries and not the entire unprocessed page body. This could be useful if one wants toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jordiclive/wikipedia-summary-dataset."
jordyvl/DUDE_loader,,,,,,,https://huggingface.co/datasets/jordyvl/DUDE_loader,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"DUDE requires models to reason and understand about document layouts in multi-page images/PDFs to answer questions about them. Specifically, models need to incorporate a new modality of layout present in the images/PDFs and reason over it to answer DUDE questions."
jorgeortizfuentes/mc4_es_cl,,,,,,,https://huggingface.co/datasets/jorgeortizfuentes/mc4_es_cl,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""mc4_es_cl"" More Information needed"
josearangos/spanish-calls-corpus-Friends,,,,,,,https://huggingface.co/datasets/josearangos/spanish-calls-corpus-Friends,,,,,,,,,,,,,,,,,,,,josearangos/spanish-calls-corpus-Friends dataset hosted on Hugging Face and contributed by the HF Datasets community
Josephgflowers/Finance-Instruct-500k,,,,,,,https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,1930,,,,,,,,,,"Finance-Instruct-500k Dataset Overview Finance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning. The dataset includes content tailored for financialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k."
JosephusCheung/GuanacoDataset,,,,,,,https://huggingface.co/datasets/JosephusCheung/GuanacoDataset,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,"Sorry, it's no longer available on Hugging Face. Please reach out to those who have already downloaded it. If you have a copy, please refrain from re-uploading it to Hugging Face. The people here don't deserve it. See also: https://twitter.com/RealJosephus/status/1779913520529707387 GuanacoDataset News: We're heading towards multimodal VQA, with blip2-flan-t5-xxl Alignment to Guannaco 7B LLM. Still under construction: GuanacoVQA weight & GuanacoVQA Dataset Notice: Effectiveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JosephusCheung/GuanacoDataset."
joujiboi/japanese-anime-speech-v2,,,,,,,https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2,,,,,,,,https://choosealicense.com/licenses/gpl/,,,,,,,,,,,,"Japanese Anime Speech Dataset V2 æ—¥æœ¬èªžã¯ã“ã¡ã‚‰ japanese-anime-speech-v2 is an audio-text dataset designed for training automatic speech recognition models. The dataset comprises 292,637 audio clips and their corresponding transcriptions from various visual novels. This dataset is not an expanded version of japanese-anime-speech-v1. For that reason, much of the audio from japanese-anime-speech-v1 is not included in this dataset. The goal of this dataset is to increase the accuracy ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2."
JourneyDB/JourneyDB,,,,,,https://arxiv.org/abs/2307.00716,https://huggingface.co/datasets/JourneyDB/JourneyDB,,,,,,,,,,,,,,,,,,,,"JourneyDB [Project Page] [Paper] [Code] [HuggingFace] [OpenDataLab] Dataset Description Summary JourneyDB is a large-scale generated image understanding dataset that contains 4,429,295 high-resolution Midjourney images, annotated with corresponding text prompt, image caption and visual question answering. Supported Tasks JourneyDB supports 4 downstream tasks, i.e. Prompt Inversion, Style Retrieval, Image Caption, and Visual Questionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JourneyDB/JourneyDB."
jslin09/Fraud_Case_Verdicts,,,,,,,https://huggingface.co/datasets/jslin09/Fraud_Case_Verdicts,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"The ""Crime Facts"" of ""Offenses of Fraudulence"" in Judicial Yuan Verdicts Dataset This data set is based on the judgments of ""Offenses of Fraudulence"" cases published by the Judicial Yuan. The data range of the dataset is from January 1, 2011, to December 31, 2021. 74,823 pieces of original data (judgments and rulings) were collected. We only took the contents of the ""criminal facts"" field of the judgment. This dataset is divided into three parts. The training dataset has 59,858â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jslin09/Fraud_Case_Verdicts."
jsm0424/korean-cipher,,,,,,,https://huggingface.co/datasets/jsm0424/korean-cipher,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Korean-Cipher Dataset Overview This dataset was inspired by OpenAI's video, ""Korean Cipher with OpenAI o1"".It is designed to evaluate the reasoning abilities of large language models (LLMs) in understanding and reconstructing distorted Korean text. Dataset Structure Each sample in the dataset consists of the following fields: id: A unique identifier for each sentence pair. message: The original Korean sentence. ciphertext: The distorted version of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jsm0424/korean-cipher."
jtatman/python-code-dataset-500k,,,,,,,https://huggingface.co/datasets/jtatman/python-code-dataset-500k,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Attention: This dataset is a summary and reformat pulled from github code. You should make your own assumptions based on this. In fact, there is another dataset I formed through parsing that addresses several points: out of 500k python related items, most of them are python-ish, not pythonic the majority of the items here contain excessive licensing inclusion of original code the items here are sometimes not even python but have references There's a whole lot of gpl summariesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jtatman/python-code-dataset-500k."
ju-resplande/rebel-pt,Text,,,,,,https://huggingface.co/datasets/ju-resplande/rebel-pt,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,REBEL-Portuguese is an REBEL adaptation for Portuguese.
juancavallotti/bea-19-corruption,,,,,,,https://huggingface.co/datasets/juancavallotti/bea-19-corruption,,,,,,,,,,,,,,,,,,,,BEA 19 Shared task. BEA 19 Shared task dataset already preprocessed to have the original and corrupted sentence. I merged the def and train datasets into one and applied all the annotated edits. Source: https://www.cl.cam.ac.uk/research/nl/bea2019st/
juletxara/visual-spatial-reasoning,Text,,,,,https://ltl.mmll.cam.ac.uk/,https://huggingface.co/datasets/juletxara/visual-spatial-reasoning,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"The Visual Spatial Reasoning (VSR) corpus is a collection of caption-image pairs with true/false labels. Each caption describes the spatial relation of two individual objects in the image, and a vision-language model (VLM) needs to judge whether the caption is correctly describing the image (True) or not (False)."
julianmoraes/doodles-captions-BLIP,,,,,,,https://huggingface.co/datasets/julianmoraes/doodles-captions-BLIP,,,,,,,,,,,,,,,,,,,,julianmoraes/doodles-captions-BLIP dataset hosted on Hugging Face and contributed by the HF Datasets community
julien-c/titanic-survival,,,,,,,https://huggingface.co/datasets/julien-c/titanic-survival,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,Titanic Survival from https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/problem12.html
julien040/hacker-news-posts,,,,,,,https://huggingface.co/datasets/julien040/hacker-news-posts,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"Hacker News Stories Dataset This is a dataset containing approximately 4 million stories from Hacker News, exported to a CSV file. The dataset includes the following fields: id (int64): The unique identifier of the story. title (string): The title of the story. url (string): The URL of the story. score (int64): The score of the story. time (int64): The time the story was posted, in Unix time. comments (int64): The number of comments on the story. author (string): The usernameâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/julien040/hacker-news-posts."
junelee/sharegpt_deepl_ko,,,,,,,https://huggingface.co/datasets/junelee/sharegpt_deepl_ko,,,,,,,,,,,,,,,,,,,,"shareGPT í•œêµ­ì–´ ë²ˆì—­ ë°ì´í„°ì…‹ ì´ í”„ë¡œì íŠ¸ëŠ” shareGPT ë°ì´í„°ì…‹ 60ë§Œ ëŒ€í™”ë¬¸ì„ DeepL ì„ í†µí•´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  ìžˆìŠµë‹ˆë‹¤. í˜„ìž¬ ë²ˆì—­ì´ ì§„í–‰ ì¤‘ì´ë©°, ì•„ëž˜ì˜ ì§„í–‰ìƒí™©ì„ ì°¸ê³ í•´ ì£¼ì„¸ìš”. ì§„í–‰ìƒí™© 62ë§Œ ëŒ€í™”ë¬¸ì¤‘ 62ë§Œ ëŒ€í™”ë¬¸ë²ˆì—­ì™„ë£Œ. íŒŒì¼êµ¬ì¡° original_dataset.json : ì›ë³¸ shareGPT íŒŒì¼(62ë§Œ ì˜ë¬¸ëŒ€í™”ë¬¸) ko_dataset.json : ë²ˆì—­ë³¸ shareGPTíŒŒì¼, êµ¬ì¡° ì›ë³¸ê³¼ ë™ì¼ ko_dataset_2.json : ko_dataset.json ì—ì„œ íŒŒì¼êµ¬ì¡°ê°€ ë¶ˆì•ˆì •í•œ(ëŒ€í™”ê°€ ì—†ê±°ë‚˜, ëŒ€í™”ì˜ ì‹œìž‘ì´ gpt ì¸ë° ê·¸ ì´í›„ ëŒ€í™”ê°€ ì—†ëŠ”ê²ƒë“¤) ëŒ€í™” ì‚­ì œ ë²„ì „ ko_alpaca_style_dataset.json : ì•ŒíŒŒì¹´ íŒŒì¸íŠœë‹ì„ ìœ„í•œ êµ¬ì¡°ë¡œ ë³€ê²½ ë¼ì´ì„¼ìŠ¤ ì›ë³¸ ë°ì´í„°ê°€ OPENAI ì´ê¸° ë•Œë¬¸ì— í•´ë‹¹ ì•½ê´€ì— ë”°ë¦…ë‹ˆë‹¤. ê·¸ ì´ì™¸ì˜â€¦ See the full description on the dataset page: https://huggingface.co/datasets/junelee/sharegpt_deepl_ko."
justinian336/news-and-blogs,,,,,,,https://huggingface.co/datasets/justinian336/news-and-blogs,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""news-and-blogs"" More Information needed"
justinphan3110/vi_pubmed,Text,General,Document,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/justinphan3110/vi_pubmed,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for PubMed Data-Summary: NLM produces a baseline set of MEDLINE/PubMed citation records in XML format for download on an annual basis. The annual baseline is released in December of each year. Each day, NLM produces update files that include new, revised and deleted citations. See our documentation page for more information. Supported Tasks and Leaderboards [More Information Needed] Languages Englishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/justinphan3110/vi_pubmed."
justinpinkney/trailer-faces-hq,,,,,,,https://huggingface.co/datasets/justinpinkney/trailer-faces-hq,,,,,,,,,,,,,,,,,,,,"Trailer Faces HQ (TFHQ) Trailer Faces High Quality (TFHQ) is a large dataset of high-resolution face images sourced from movie trailers. Details TFHQ was collected by downloading all movie trailers and featurettes listed on the Apple Movie Trailers website as of August 2022. These 15,379 trailers were downloaded at Full HD (1080p) resolution, amounting to approximately 2 TB/507 hours of video. Face detection was performed on every frame using the pre-trainedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/justinpinkney/trailer-faces-hq."
Jyshen/Chat_Suzumiya_Haruhi,,,,,,,https://huggingface.co/datasets/Jyshen/Chat_Suzumiya_Haruhi,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Jyshen/Chat_Suzumiya_Haruhi dataset hosted on Hugging Face and contributed by the HF Datasets community
Jzuluaga/atcosim_corpus,Audio,General,General,Text,"Accuracy, F1 Score",https://www.spsc.tugraz.at/databases-and-tools/atcosim-air-traffic-control-simulation-speech-corpus.html,https://huggingface.co/datasets/Jzuluaga/atcosim_corpus,and pronounced by ten non-nativeâ€¦ See the full description on the dataset page: https://huggingface,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,"of speech data, which were recorded during ATC real-time simulations using a close-talk headset microphone","Dataset Card for ATCOSIM corpus Data-Summary: The ATCOSIM Air Traffic Control Simulation Speech corpus is a speech database of air traffic control (ATC) operator speech, provided by Graz University of Technology (TUG) and Eurocontrol Experimental Centre (EEC). It consists of ten hours of speech data, which were recorded during ATC real-time simulations using a close-talk headset microphone. The utterances are in English language and pronounced by ten non-nativeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jzuluaga/atcosim_corpus."
kaist-ai/CoT-Collection,Text,,,,,,https://huggingface.co/datasets/kaist-ai/CoT-Collection,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,2009,,,,,,,,,""""""" _LICENSE = ""CC BY 4.0"" _HOMEPAGE = ""https://github.com/kaistAI/CoT-Collection"" _LANGUAGES = { ""en"": ""English"", } # _ALL_LANGUAGES = ""all_languages"" class CoTCollectionMultiConfig(datasets.BuilderConfig):"
kakaobrain/coyo-700m,Image-Text,General,Document,Text,"Accuracy, F1 Score",https://kakaobrain.com/contents/?contentId=7eca73e3-3089-43cb-b701-332e8a1743fd,https://huggingface.co/datasets/kakaobrain/coyo-700m,"datasets, collecting many informative pairs of alt-text and its associated image in HTML documents",,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for COYO-700M Data-Summary: COYO-700M is a large-scale dataset that contains 747M image-text pairs as well as many other meta-attributes to increase the usability to train various models. Our dataset follows a similar strategy to previous vision-and-language datasets, collecting many informative pairs of alt-text and its associated image in HTML documents. We expect COYO to be used to train popular large-scale foundation models complementary toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kakaobrain/coyo-700m."
Kaludi/data-food-classification,,,,,,,https://huggingface.co/datasets/Kaludi/data-food-classification,,,,,,,,,,,,,,,,,,,,"Dataset for project: food-classification Dataset Description This dataset has been processed for project food-classification. Languages The BCP-47 code for the dataset's language is unk. Dataset Structure Data Instances A sample from this dataset looks as follows: [ { ""image"": ""<308x512 RGB PIL image>"", ""target"": 0 }, { ""image"": ""<512x512 RGB PIL image>"", ""target"": 0 } ] Datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kaludi/data-food-classification."
Kamizuru00/diagram_image_to_text,,,,,,,https://huggingface.co/datasets/Kamizuru00/diagram_image_to_text,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""diagram_image_to_text"" More Information needed"
Kamtera/Persian-conversational-dataset,,,,,,,https://huggingface.co/datasets/Kamtera/Persian-conversational-dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,persian-conversational-dataset
Kanakmi/mental-disorders,,,,,,,https://huggingface.co/datasets/Kanakmi/mental-disorders,,,,,,,,,,,,,,,,,,,,Labels: 0:'BPD' 1:'bipolar' 2:'depression' 3:'Anxiety' 4:'schizophrenia' 5:'mentalillness'
kannanwisen/De-Hazing-Dataset,,,,,,,https://huggingface.co/datasets/kannanwisen/De-Hazing-Dataset,,,,,,,,https://choosealicense.com/licenses/creativeml-openrail-m/,,,,,,,,,,,,kannanwisen/De-Hazing-Dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
Kant1/French_Wikipedia_articles,,,,,,,https://huggingface.co/datasets/Kant1/French_Wikipedia_articles,,,,,,,,,,,,,,,,,,,,Dump of 2023-08-20 of all french article in wikipedia https://dumps.wikimedia.org/frwiki/20230820/frwiki-20230820-pages-articles.xml.bz2
KaraAgroAI/Drone-based-Agricultural-Dataset-for-Crop-Yield-Estimation,,,,,,,https://huggingface.co/datasets/KaraAgroAI/Drone-based-Agricultural-Dataset-for-Crop-Yield-Estimation,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Drone-based Agricultural Dataset for Crop Yield Estimation This repository contains a comprehensive dataset of cashew, cocoa and coffee images captured by drones, accompanied by meticulously annotated labels. To facilitate object detection, each image is paired with a corresponding text file in YOLO format. The YOLO format file contains annotations, including class labels and bounding box coordinates. The dataset was collected by teams from Ghana (KaraAgro AI) and Ugandaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KaraAgroAI/Drone-based-Agricultural-Dataset-for-Crop-Yield-Estimation."
karpathy/tiny_shakespeare,Text,,,,,https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt,https://huggingface.co/datasets/karpathy/tiny_shakespeare,,,,,,,,,,,,,,,,,,,,"40,000 lines of Shakespeare from a variety of Shakespeare's plays. Featured in Andrej Karpathy's blog post 'The Unreasonable Effectiveness of Recurrent Neural Networks': http://karpathy.github.io/2015/05/21/rnn-effectiveness/. To use for e.g. character modelling: ``` d = datasets.load_dataset(name='tiny_shakespeare')['train'] d = d.map(lambda x: datasets.Value('strings').unicode_split(x['text'], 'UTF-8')) # train split includes vocabulary for other splits vocabulary = sorted(set(next(iter(d)).numpy())) d = d.map(lambda x: {'cur_char': x[:-1], 'next_char': x[1:]}) d = d.unbatch() seq_len = 100 batch_size = 2 d = d.batch(seq_len) d = d.batch(batch_size) ```"
katanaml-org/invoices-donut-data-v1,,,,,,,https://huggingface.co/datasets/katanaml-org/invoices-donut-data-v1,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Card for Invoices (Sparrow) This dataset contains 500 invoice documents annotated and processed to be ready for Donut ML model fine-tuning. Annotation and data preparation task was done by Katana ML team. Sparrow - open-source data extraction solution by Katana ML. Original dataset info: KozÅ‚owski, Marek; Weichbroth, PaweÅ‚ (2021), â€œSamples of electronic invoicesâ€, Mendeley Data, V2, doi: 10.17632/tnj49gpmtz.2"
katha-ai-iiith/VELOCITI,,,,,,,https://huggingface.co/datasets/katha-ai-iiith/VELOCITI,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"This is the official Dataset repository for the paper ""VELOCITI: Benchmarking Video Language Compositional Reasoning with Strict Entailment"". License Please read this section carefully, We release this dataset under the CC-BY-NC-SA license, with the following additional clause : VELOCITI may never be use to tune the parameters of any model, and is strictly a test set. This repository contains the dataset for benchmarking VLMs. Navigating the Dataset The text annotations are present inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/katha-ai-iiith/VELOCITI."
katielink/healthsearchqa,,,,,,https://arxiv.org/abs/2212.13138,https://huggingface.co/datasets/katielink/healthsearchqa,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"HealthSearchQA Dataset of consumer health questions released by Google for the Med-PaLM paper (arXiv preprint). From the paper: We curated our own additional dataset consisting of 3,173 commonly searched consumer questions, referred to as HealthSearchQA. The dataset was curated using seed medical conditions and their associated symptoms. We used the seed data to retrieve publicly-available commonly searched questions generated by a search engine, which were displayed to allâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/katielink/healthsearchqa."
kaxap/pg-wikiSQL-sql-instructions-80k,,,,,,,https://huggingface.co/datasets/kaxap/pg-wikiSQL-sql-instructions-80k,,,,,,,,https://choosealicense.com/licenses/bsd-3-clause/,,,,,,,,,,,,"Converted, cleaned and syntax-checked SQLWiki dataset. The datapoints containing non latin column names were removed. Resulting SQL statements were adapted for Postgres syntax and conventions. Each SQL statement, including CREATE TABLE statements were syntax checked with pgsanity. Citations @article{zhongSeq2SQL2017, author = {Victor Zhong and Caiming Xiong and Richard Socher}, title = {Seq2SQL: Generating Structured Queries from Naturalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kaxap/pg-wikiSQL-sql-instructions-80k."
kaysss/Medicines,,,,,,,https://huggingface.co/datasets/kaysss/Medicines,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Card for Medicines Data This dataset contains information about various diseases, prescription medicines, their prices, manufacturers, and other relevant details scraped from an online pharmacy website. It provides structured insights into different medications, including their availability, prescription requirements, manufacturers, active ingredients, and potential side effects. The dataset also contains introduction and detailed description of the medicine, includingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kaysss/Medicines."
KbsdJames/Omni-MATH,,,,,,https://arxiv.org/abs/2410.07985,https://huggingface.co/datasets/KbsdJames/Omni-MATH,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset Card for Omni-MATH Recent advancements in AI, particularly in large language models (LLMs), have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To mitigate this limitation, we propose a comprehensive and challenging benchmark specifically designedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KbsdJames/Omni-MATH."
kchawla123/casino,Text,General,UI,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/kchawla123/casino,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Casino Data-Summary: We provide a novel dataset (referred to as CaSiNo) of 1030 negotiation dialogues. Two participants take the role of campsite neighbors and negotiate for Food, Water, and Firewood packages, based on their individual preferences and requirements. This design keeps the task tractable, while still facilitating linguistically rich and personal conversations. This helps to overcome the limitations of prior negotiation datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kchawla123/casino."
KDAM1/BasketballGames,,,,,,,https://huggingface.co/datasets/KDAM1/BasketballGames,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,This DataSet contain nba games from 2019 tp 2022
Kedreamix/psychology-10k-Deepseek-R1-zh,,,,,,,https://huggingface.co/datasets/Kedreamix/psychology-10k-Deepseek-R1-zh,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Kedreamix/psychology-10k-Deepseek-R1-zh dataset hosted on Hugging Face and contributed by the HF Datasets community
keelezibel/sentence_classification_dataset,,,,,,,https://huggingface.co/datasets/keelezibel/sentence_classification_dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"This dataset is an automatically curated from three datasets. Wikipedia_AfD_imperative_data Spaadia SquadV2 Samples from https://github.com/lettergram/sentence-classification/tree/master Only 3 classes are available. {""declarative"": 0, ""question"": 1, ""imperative"": 2} Note: As this dataset is automatically curated, it may not be the cleanest. Use at your own risk."
keithito/lj_speech,Text,,,,,https://keithito.com/LJ-Speech-Dataset/,https://huggingface.co/datasets/keithito/lj_speech,,,,,,,,https://choosealicense.com/licenses/unlicense/,,,,,,,,,,,,"This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books in English. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours. Note that in order to limit the required storage for preparing this dataset, the audio is stored in the .wav format and is not converted to a float32 array. To convert the audio file to a float32 array, please make use of the `.map()` function as follows: ```python import soundfile as sf def map_to_array(batch): speech_array, _ = sf.read(batch[""file""]) batch[""speech""] = speech_array return batch dataset = dataset.map(map_to_array, remove_columns=[""file""]) ```"
kellycyy/daily_dilemmas,,,,,,https://arxiv.org/abs/2410.02683,https://huggingface.co/datasets/kellycyy/daily_dilemmas,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"DailyDilemmas - Revealing Value Preferences of LLMs with Quandaries of Daily Life Link: Paper Description of DailyDilemma DailyDilemma is a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma includes two possible actions and with each action, the affected parties and human values invoked. We evaluated LLMs on these dilemmas to determine what action they will take and the values represented by these actionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kellycyy/daily_dilemmas."
keremberke/csgo-object-detection,Text,,,,,,https://huggingface.co/datasets/keremberke/csgo-object-detection,,,,,,,,,,,,,,,,,,,,"Dataset Labels ['ct', 'cthead', 't', 'thead'] Number of Images {'train': 3879, 'valid': 383, 'test': 192} How to Use Install datasets: pip install datasets Load the dataset: from datasets import load_dataset ds = load_dataset(""keremberke/csgo-object-detection"", name=""full"")"
kerinin/hackernews-stories,,,,,,,https://huggingface.co/datasets/kerinin/hackernews-stories,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""hackernews-stories"" More Information needed"
KevinSpaghetti/smm4h20,,,,,,,https://huggingface.co/datasets/KevinSpaghetti/smm4h20,,,,,,,,,,,,,,,,,,,,KevinSpaghetti/smm4h20 dataset hosted on Hugging Face and contributed by the HF Datasets community
KhalfounMehdi/dermatology_anomaly_detection_vit,,,,,,,https://huggingface.co/datasets/KhalfounMehdi/dermatology_anomaly_detection_vit,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""dermatology_anomaly_detection_vit"" More Information needed"
Khedesh/PeymaNER,,,,,,,https://huggingface.co/datasets/Khedesh/PeymaNER,,,,,,,,,,,,,,,,,,,,Khedesh/PeymaNER dataset hosted on Hugging Face and contributed by the HF Datasets community
kikikara/ko_QA_dataset,,,,,,,https://huggingface.co/datasets/kikikara/ko_QA_dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,maywell/korean_textbooks ì˜ datasetì„ Q&A í˜•ì‹ìœ¼ë¡œ ìž¬êµ¬ì„±í•œ datasetìž…ë‹ˆë‹¤.
KikiQi/xlcost-tc-python-program,,,,,,,https://huggingface.co/datasets/KikiQi/xlcost-tc-python-program,,,,,,,,,,,,,,,,,,,,KikiQi/xlcost-tc-python-program dataset hosted on Hugging Face and contributed by the HF Datasets community
killua93/German_Italian_French_Romanian_English_Insurance_Analyzer,,,,,,,https://huggingface.co/datasets/killua93/German_Italian_French_Romanian_English_Insurance_Analyzer,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""my-dataset-name"" More Information needed"
kingbri/PIPPA-shareGPT,,,,,,https://arxiv.org/abs/2308.05884,https://huggingface.co/datasets/kingbri/PIPPA-shareGPT,,,,,,,,https://choosealicense.com/licenses/agpl-3.0/,,,,,,,,,,,,Dataset Card: PIPPA-ShareGPT This is a conversion of PygmalionAI's PIPPA deduped dataset to ShareGPT format for finetuning with Axolotl. The reformat was completed via the following TypeScript project called ShareGPT-Reformat. Files and explanations pippa_sharegpt_raw.jsonl: The raw deduped dataset file converted to shareGPT. Roles will be defaulted to your finetuning software. pippa_sharegpt.jsonl: A shareGPT dataset with the roles as USER: and CHARACTER: forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kingbri/PIPPA-shareGPT.
KingdomFor/seeclick_web,,,,,,,https://huggingface.co/datasets/KingdomFor/seeclick_web,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,KingdomFor/seeclick_web dataset hosted on Hugging Face and contributed by the HF Datasets community
kingkaung/english_islamqainfo,,,,,,,https://huggingface.co/datasets/kingkaung/english_islamqainfo,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Dataset Card for English Islam QA Info Dataset Description The English Islam QA Info (19,052 questions and answers) is derived from the IslamQA website and contains curated question-and-answer pairs categorized by topic. It serves as a resource for multilingual and cross-lingual natural language processing (NLP) tasks. This dataset is part of a broader initiative to enhance the understanding and computational handling of Islamic jurisprudence and advice. Keyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kingkaung/english_islamqainfo."
kira/lima,,,,,,,https://huggingface.co/datasets/kira/lima,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""lima"" More Information needed"
kiyoonkim/kinetics-400-targz,,,,,,,https://huggingface.co/datasets/kiyoonkim/kinetics-400-targz,,,,,,,,,,,,,,,,,,,,kiyoonkim/kinetics-400-targz dataset hosted on Hugging Face and contributed by the HF Datasets community
kkChimmy/REALM,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/kkChimmy/REALM,"Models (LLMs), such as GPT-like models, have transformed industries and everyday life, creating significant societal impact",,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"REALM: REAL-World Application of Large Language Models Dataset Description Paper: coming soon Dashboard Demo: https://realm-e7682.web.app/ License: [mit] Language(s) (NLP): English Point of Contact: Jingwen Data-Summary: Large Language Models (LLMs), such as GPT-like models, have transformed industries and everyday life, creating significant societal impact. To better understand their real-world applications, we created the REALM Dataset, aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kkChimmy/REALM."
kkkkkkkkkkkkkkk/fff,,,,,,,https://huggingface.co/datasets/kkkkkkkkkkkkkkk/fff,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,kkkkkkkkkkkkkkk/fff dataset hosted on Hugging Face and contributed by the HF Datasets community
klue/klue,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",https://klue-benchmark.com/,https://huggingface.co/datasets/klue/klue,understanding capability of Korean language models,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for KLUE Data-Summary: KLUE is a collection of 8 tasks to evaluate natural language understanding capability of Korean language models. We delibrately select the 8 tasks, which are Topic Classification, Semantic Textual Similarity, Natural Language Inference, Named Entity Recognition, Relation Extraction, Dependency Parsing, Machine Reading Comprehension, and Dialogue State Tracking. Supported Tasks and Leaderboards Topicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/klue/klue."
kmewhort/sketchy-svgs,,,,,,,https://huggingface.co/datasets/kmewhort/sketchy-svgs,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""sketchy-svgs"" More Information needed"
kmfoda/booksum,,,,,,https://arxiv.org/abs/2105.08209,https://huggingface.co/datasets/kmfoda/booksum,,,,,,,,https://choosealicense.com/licenses/bsd-3-clause/,,,,,,,,,,,,"BOOKSUM: A Collection of Datasets for Long-form Narrative Summarization Authors: Wojciech KryÅ›ciÅ„ski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, Dragomir Radev Introduction The majority of available text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases. While relevant, such datasets will offer limited challenges for future generations ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kmfoda/booksum."
knkarthick/dialogsum,Text,Summarization,General,Text,"ROUGE, BLEU, BERTScore",,https://huggingface.co/datasets/knkarthick/dialogsum,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,"BART, T5, Pegasus, ProphetNet",,,"Dataset Card for DIALOGSum Corpus Dataset Description Links Homepage: https://aclanthology.org/2021.findings-acl.449 Repository: https://github.com/cylnlp/dialogsum Paper: https://aclanthology.org/2021.findings-acl.449 Point of Contact: https://huggingface.co/knkarthick Data-Summary: DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 (Plus 100 holdout data for topic generation) dialogues withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/knkarthick/dialogsum."
knoveleng/open-rs,,,,,,https://arxiv.org/abs/2503.16219,https://huggingface.co/datasets/knoveleng/open-rs,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Open-RS Dataset Summary The open-rs dataset contains 7,000 mathematical reasoning problems, including 3,000 hard problems from open-s1 and 4,000 (1000 easy + 3000 hard problems) from open-deepscaler. Itâ€™s a core component of the Open RS project, enhancing reasoning in small LLMs via reinforcement learning. Usage Load the dataset using the Hugging Face datasets library: from datasets import load_dataset ds = load_dataset(""knoveleng/open-rs"")[""train""]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/knoveleng/open-rs."
knowrohit07/gita_dataset,,,,,,,https://huggingface.co/datasets/knowrohit07/gita_dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,knowrohit07/gita_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
Koala-36M/Koala-36M-v1,,,,,,,https://huggingface.co/datasets/Koala-36M/Koala-36M-v1,,,,,,,,,,,,,,,,,,,,Koala-36M/Koala-36M-v1 dataset hosted on Hugging Face and contributed by the HF Datasets community
KoalaAI/GitHub-CC0,Text,,,,,,https://huggingface.co/datasets/KoalaAI/GitHub-CC0,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,2,,,,,,,,,"Public Domain GitHub Repositories Dataset This dataset contains metadata and source code of 9,000 public domain (cc0 or unlicense) licensed GitHub repositories that have more than 25 stars. The dataset was created by scraping the GitHub API and downloading the repositories, so long as they are under 100mb. The dataset can be used for various natural language processing and software engineering tasks, such as code summarization, code generation, code search, code analysis, etc.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/KoalaAI/GitHub-CC0."
KocLab-Bilkent/turkish-constitutional-court,Text,General,Scientific,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/KocLab-Bilkent/turkish-constitutional-court,,,1290,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Data-Summary: This dataset is extracted from the following Github repo, which was created for the journal paper with URL https://www.sciencedirect.com/science/article/abs/pii/S0306457321001692. https://github.com/koc-lab/law-turk The dataset includes 1290 court case decision texts from the Turkish Court of Cassation. Each sample has one label, which is the ruling of the court. The possible rulings are ""Violation"" and ""No violation"". There are 1290 samples. 1141 of theseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KocLab-Bilkent/turkish-constitutional-court."
KodCode/KodCode-V1,,,,,,https://arxiv.org/abs/2503.02951,https://huggingface.co/datasets/KodCode/KodCode-V1,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,https://github.com/KodCode-AI/kodcode,,"sðŸ§ Other InformationðŸ“š CitationðŸ± KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for CodingKodCode is the largest fully-synthetic open-source dataset providing verifiable solutions and tests for coding tasks",,,,,,"ðŸ± KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding KodCode is the largest fully-synthetic open-source dataset providing verifiable solutions and tests for coding tasks. It contains 12 distinct subsets spanning various domains (from algorithmic to package-specific knowledge) and difficulty levels (from basic coding exercises to interview and competitive programming challenges). KodCode is designed for both supervised fine-tuning (SFT) and RL tuning. ðŸ•¸ï¸â€¦ See the full description on the dataset page: https://huggingface.co/datasets/KodCode/KodCode-V1."
KoleHoenicke/niji_jelly,,,,,,,https://huggingface.co/datasets/KoleHoenicke/niji_jelly,,,,,,,,https://choosealicense.com/licenses/creativeml-openrail-m/,,,,,,,,,,,,"LoRA - niji_jelly LoRA trained on images trained on from MidJourney's Niji style, specifically the jelly look. All image"
KonradSzafer/stackoverflow_linux,,,,,,,https://huggingface.co/datasets/KonradSzafer/stackoverflow_linux,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""stackoverflow_linux"" Dataset information: Source: Stack Overflow Category: Linux Number of samples: 300 Train/Test split: 270/30 Quality: Data come from the top 1k most upvoted questions Additional Information License All Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required. More Information needed"
KorQuAD/squad_kor_v1,Text,Reasoning,UI,Text,"Accuracy, F1 Score",https://korquad.github.io/KorQuad%201.0/,https://huggingface.co/datasets/KorQuAD/squad_kor_v1,,,,,,,,https://choosealicense.com/licenses/cc-by-nd-4.0/,,,,,,,,,"T5, UnifiedQA, BART",,,"Dataset Card for KorQuAD v1.0 Data-Summary: KorQuAD 1.0 is a large-scale question-and-answer dataset constructed for Korean machine reading comprehension, and investigate the dataset to understand the distribution of answers and the types of reasoning required to answer the question. This dataset benchmarks the data generating process of SQuAD v1.0 to meet the standard. Supported Tasks and Leaderboards question-answering Languagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KorQuAD/squad_kor_v1."
kotzeje/lamini_docs.jsonl,,,,,,,https://huggingface.co/datasets/kotzeje/lamini_docs.jsonl,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""lamini_docs.jsonl"" More Information needed"
kraina/airbnb,Text,,,,,https://zenodo.org/record/4446043#.ZEV8d-zMI-R,https://huggingface.co/datasets/kraina/airbnb,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,This dataset contains accommodation offers from the AirBnb platform from 10 European cities. It has been copied from https://zenodo.org/record/4446043#.ZEV8d-zMI-R to make it available as a Huggingface Dataset. It was originally published as supplementary material for the article: Determinants of Airbnb prices in European cities: A spatial econometrics approach (DOI: https://doi.org/10.1016/j.tourman.2021.104319)
kresnik/zeroth_korean,,,,,,,https://huggingface.co/datasets/kresnik/zeroth_korean,,,,,,,,,,,,,,,,,,,,"Zeroth-Korean Dataset Introduction The Zeroth-Korean dataset is a publicly available speech dataset created for Korean automatic speech recognition (ASR) research and development. This dataset is distributed under the CC BY 4.0 license, allowing anyone to use it freely. The goal of the Zeroth project is to make Korean speech recognition more widely accessible. Dataset Overview Total Data: Approximately 51.6 hours of training data and 1.2 hours ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kresnik/zeroth_korean."
krisfu/awesome-llm-datasets-only-Chinese,,,,,,,https://huggingface.co/datasets/krisfu/awesome-llm-datasets-only-Chinese,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,krisfu/awesome-llm-datasets-only-Chinese dataset hosted on Hugging Face and contributed by the HF Datasets community
kroshan/BioASQ,,,,,,,https://huggingface.co/datasets/kroshan/BioASQ,,,,,,,,,,,,,,,,,,,,kroshan/BioASQ dataset hosted on Hugging Face and contributed by the HF Datasets community
krr-oxford/OntoLAMA,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2302.06761,https://huggingface.co/datasets/krr-oxford/OntoLAMA,model (LM) probing datasets for ontology subsumption inference,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,1930,,,,,,,"BERT, RoBERTa, T5",,,"OntoLAMA: LAnguage Model Analysis for Ontology Subsumption Inference Data-Summary: OntoLAMA is a set of language model (LM) probing datasets for ontology subsumption inference. The work follows the ""LMs-as-KBs"" literature but focuses on conceptualised knowledge extracted from formalised KBs such as the OWL ontologies. Specifically, the subsumption inference (SI) task is introduced and formulated in the Natural Language Inference (NLI) style, where theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/krr-oxford/OntoLAMA."
Krystalan/xmediasum,Text,,,,,,https://huggingface.co/datasets/Krystalan/xmediasum,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"We present XMediaSum, a cross-lingual dialogue summarization dataset with 40K English(dialogues)->Chinese(summaries) and 40K English (dialogues)->German(summaries) samples. XMediaSum is created by manually translating the English summaries of MediaSum (a English monolingual dialogue summarization dataset) to both Chinese and German."
KTH/hungarian-single-speaker-tts,Audio,General,General,Text,"Accuracy, F1 Score",https://www.kaggle.com/datasets/bryanpark/hungarian-single-speaker-speech-dataset,https://huggingface.co/datasets/KTH/hungarian-single-speaker-tts,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for CSS10 Hungarian: Single Speaker Speech Dataset Data-Summary: The corpus consists of a single speaker, with 4515 segments extracted from a single LibriVox audiobook. Supported Tasks and Leaderboards [Needs More Information] Languages The audio is in Hungarian. Dataset Structure [Needs More Information] Data Instances [Needs More Information] Data Fields [Needs Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KTH/hungarian-single-speaker-tts."
ktiyab/cooking-knowledge-basics,,,,,,,https://huggingface.co/datasets/ktiyab/cooking-knowledge-basics,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Comprehensive Cooking Knowledge Q&A Dataset This dataset (cooking_knowledge.csv) contains a rich collection of synthetically generated Question-Answer (Q&A) pairs covering diverse aspects of cooking knowledge, with particular emphasis on food chemistry, flavor pairing, cooking techniques, dietary accommodations, and culinary traditions. The data was created using a large language model with advanced reasoning capabilities, prompted with various grounded contexts and real-worldâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ktiyab/cooking-knowledge-basics."
kunishou/databricks-dolly-15k-ja,,,,,,,https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"This dataset was created by automatically translating ""databricks-dolly-15k"" into Japanese.This dataset is licensed under CC-BY-SA-3.0 Last Update : 2023-05-11 databricks-dolly-15k-jahttps://github.com/kunishou/databricks-dolly-15k-jadatabricks-dolly-15khttps://github.com/databrickslabs/dolly/tree/master/data"
Kunling/layoutlm_resume_data,,,,,,,https://huggingface.co/datasets/Kunling/layoutlm_resume_data,,,,,,,,https://choosealicense.com/licenses/bsd/,,,,,,,,,,,,Kunling/layoutlm_resume_data dataset hosted on Hugging Face and contributed by the HF Datasets community
kuroneko5943/weibo16,,,,,,,https://huggingface.co/datasets/kuroneko5943/weibo16,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"GLUE, the General Language Understanding Evaluation benchmark (https://gluebenchmark.com/) is a collection of resources for training, evaluating, and analyzing natural language understanding systems."
kuznetsoffandrey/sberquad,Text,Question Answering,Scientific,Text,"Exact Match, F1 Score",,https://huggingface.co/datasets/kuznetsoffandrey/sberquad,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for sberquad Data-Summary: Sber Question Answering Dataset (SberQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. Russian original analogue presented in Sberbank Data Science Journey 2017. Supported Tasks and Leaderboardsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kuznetsoffandrey/sberquad."
kyutai/Babillage,Text,,,,,https://arxiv.org/abs/2503.15633,https://huggingface.co/datasets/kyutai/Babillage,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,"Loading the datasetLicensingCitationBabillageBabillage is a multimodal benchmark dataset introduced along withMoshiVis(Project Page|arXiv), containing three common vision-language benchmarks converted in spoken form, for the evaluation of Vision Speech Models",,,,,,"Babillage Babillage is a multimodal benchmark dataset introduced along with MoshiVis (Project Page | arXiv), containing three common vision-language benchmarks converted in spoken form, for the evaluation of Vision Speech Models. For each benchmark (COCO-Captions, OCR-VQA, VQAv2), we first reformat the text question-answer pairs into a more conversational dialogue, and then convert them using a text-to-speech pipeline, using a consistent synthetic voice for the answer (assistant)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/kyutai/Babillage."
kz-transformers/multidomain-kazakh-dataset,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/kz-transformers/multidomain-kazakh-dataset,dataset containing just over 24 883 808 unique texts from multiple domains,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Description Point of Contact: Sanzhar Murzakhmetov, Besultan Sagyndyk Data-Summary: MDBKD | Multi-Domain Bilingual Kazakh Dataset is a Kazakh-language dataset containing just over 24 883 808 unique texts from multiple domains. Supported Tasks 'MLM/CLM': can be used to train a model for casual and masked languange modeling Languages The kk code for Kazakh as generally spoken in the Kazakhstan Data Instances For each instanceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kz-transformers/multidomain-kazakh-dataset."
L4NLP/LEval,,,,,,,https://huggingface.co/datasets/L4NLP/LEval,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,A benchmark to evaluate long document understanding and generation ability of LLM
LabHC/bias_in_bios,,,,,,,https://huggingface.co/datasets/LabHC/bias_in_bios,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Bias in Bios Bias in Bios was created by (De-Artega et al., 2019) and published under the MIT license (https://github.com/microsoft/biosbias). The dataset is used to investigate bias in NLP models. It consists of textual biographies used to predict professional occupations, the sensitive attribute is the gender (binary). The version shared here is the version proposed by (Ravgofel et al., 2020) which slightly smaller due to the unavailability of 5,557 biographies. The datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LabHC/bias_in_bios."
laion/laion-high-resolution,,,,,,,https://huggingface.co/datasets/laion/laion-high-resolution,,,,,,,,,,,,,,,,,,,,laion/laion-high-resolution dataset hosted on Hugging Face and contributed by the HF Datasets community
lambdalabs/pokemon-blip-captions,,,,,,,https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"Notice of DMCA Takedown Action We have received a DMCA takedown notice from The PokÃ©mon Company International, Inc. In response to this action, we have taken down the dataset. We appreciate your understanding."
LAMDA-NeSy/ChinaTravel,,,,,,https://arxiv.org/abs/2412.13682,https://huggingface.co/datasets/LAMDA-NeSy/ChinaTravel,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"ChinaTravel Dataset ChinaTravel is a benchmark meticulously designed to provide a comprehensive and scalable evaluation framework for language agents in multi-day multi-POI travel planning. See our paper for more details. Introduction In ChinaTravel, for a given query, language agents are expected to use the provided tools in sandbox to collect information and generate a travel plan in json format. The plan should include a list of POIs (restaurants, attractionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LAMDA-NeSy/ChinaTravel."
lamini/lamini_docs,,,,,,,https://huggingface.co/datasets/lamini/lamini_docs,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""lamini_docs"" More Information needed"
LanceaKing/asvspoof2019,Text,,,,,https://datashare.ed.ac.uk/handle/10283/3336,https://huggingface.co/datasets/LanceaKing/asvspoof2019,,,,,,,,https://choosealicense.com/licenses/odc-by/,,,,,,,,,,,,"This is a database used for the Third Automatic Speaker Verification Spoofing and Countermeasuers Challenge, for short, ASVspoof 2019 (http://www.asvspoof.org) organized by Junichi Yamagishi, Massimiliano Todisco, Md Sahidullah, HÃ©ctor Delgado, Xin Wang, Nicholas Evans, Tomi Kinnunen, Kong Aik Lee, Ville Vestman, and Andreas Nautsch in 2019."
lang-uk/malyuk,,,,,,,https://huggingface.co/datasets/lang-uk/malyuk,,,,,,,,,,,,,,,,,,,,"Malyuk [mÉËˆlÊ²uk] Combined corpus: UberText 2.0, Oscar (oscar v1.0, unshuffled_deduplicated_uk), Ukrainian News This is not an official release by any means. It is just a compilation made by me to simplify the training of the Ukrainian LLM. Nothing is guaranteed, no support requests, nothing. 113GB of texts in jsonl. 38941863 articles. Some scripts to prepare this dataset can be found here"
lansinuote/ChnSentiCorp,,,,,,,https://huggingface.co/datasets/lansinuote/ChnSentiCorp,,,,,,,,,,,,,,,,,,,,lansinuote/ChnSentiCorp dataset hosted on Hugging Face and contributed by the HF Datasets community
lapki/perekrestok-reviews,,,,,,,https://huggingface.co/datasets/lapki/perekrestok-reviews,,,,,,,,,,,,,,,,,,,,"Dataset Dataset of user reviews from ""ÐŸÐµÑ€ÐµÐºÑ€Ñ‘ÑÑ‚Ð¾Ðº/Perekrestok"" shop. Dataset Format Dataset is in JSONLines format. Trivia: product_id - Product internal ID (https://www.perekrestok.ru/cat/1/p/ID) product_name - Product name product_category - Category of product product_price - Product price in RUB (decimal) review_id - Review internal ID review_author - Author of review review_text - Text of review rating - Review rating (decimal, from 0.0 to 5.0)"
larryvrh/CCMatrix-v1-Ja_Zh-filtered,,,,,,,https://huggingface.co/datasets/larryvrh/CCMatrix-v1-Ja_Zh-filtered,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""CCMatrix-v1-Ja_Zh-filtered"" Filtered and modified version of Japanese/Chinese language pair data from CCMatrix v1. Process steps: 1. Basic regex based filtering / length checking to remove abnormal pairs. 2. Semantic similarity filtering with a threshold value of 0.6, based on sentence-transformers/LaBSE. 3. Convert all Traditional Chinese sentences into Simplified Chinese with zhconv. ç»è¿‡è¿‡æ»¤å’Œä¿®æ”¹çš„æ—¥è¯­/ä¸­æ–‡è¯­è¨€å¯¹æ•°æ®ï¼Œæ¥è‡ªCCMatrix v1ã€‚ å¤„ç†æ­¥éª¤ï¼š 1.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/larryvrh/CCMatrix-v1-Ja_Zh-filtered."
latentcat/animesfw,,,,,,,https://huggingface.co/datasets/latentcat/animesfw,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""animesfw"" More Information needed"
launch/gov_report,Text,,,,,https://gov-report-data.github.io,https://huggingface.co/datasets/launch/gov_report,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"GovReport long document summarization dataset. There are three configs: - plain_text: plain text document-to-summary pairs - plain_text_with_recommendations: plain text doucment-summary pairs, with ""What GAO recommends"" included in the summary - structure: data with section structure"
lavita/ChatDoctor-HealthCareMagic-100k,,,,,,,https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""ChatDoctor-HealthCareMagic-100k"" More Information needed"
LawalAfeez/science-dataset,,,,,,,https://huggingface.co/datasets/LawalAfeez/science-dataset,,,,,,,,,,,,,,,,,,,,LawalAfeez/science-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
LawInformedAI/overruling,,,,,,,https://huggingface.co/datasets/LawInformedAI/overruling,,,,,,,,,,,,,,,,,,,,LawInformedAI/overruling dataset hosted on Hugging Face and contributed by the HF Datasets community
Laxhar/noob-wiki,,,,,,,https://huggingface.co/datasets/Laxhar/noob-wiki,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Noob SDXL Wiki This is the WIKI database for Noob SDXL Models.
lbox/lbox_open,Text,General,General,Text,"Accuracy, F1 Score",https://lbox.kr,https://huggingface.co/datasets/lbox/lbox_open,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for lbox_open Data-Summary: A Legal AI Benchmark Dataset from Korean Legal Cases. Languages Korean How to use from datasets import load_dataset # casename classficiation task data_cn = load_dataset(""lbox/lbox_open"", ""casename_classification"") data_cn_plus = load_dataset(""lbox/lbox_open"", ""casename_classification_plus"") # statutes classification task data_st = load_dataset(""lbox/lbox_open"", ""statute_classification"")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lbox/lbox_open."
ldhnam/deepfashion_controlnet_ezcaption,,,,,,,https://huggingface.co/datasets/ldhnam/deepfashion_controlnet_ezcaption,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""deepfashion_controlnet_ezcaption"" More Information needed"
LDJnr/Puffin,,,,,,,https://huggingface.co/datasets/LDJnr/Puffin,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"This is the Official Puffin dataset. Exactly 3,000"
LearnItAnyway/Visual-Navigation-21k,,,,,,,https://huggingface.co/datasets/LearnItAnyway/Visual-Navigation-21k,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"Visual Navigation 21K Dataset To support the visual impaired person, there are several tools including the cane. I belive the LLM with the vision model can help. Dataset The road image dataset is from AI Hub. Among the data, images in Bbox_3_new.zip have been annotated. Description and Conversations Based on the locations of the obstables in the image, the description has been generated. Based on the description, the multi-turn conversation has been generated. TODO Makeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LearnItAnyway/Visual-Navigation-21k."
legacy-datasets/wikipedia,Text,,,,,https://dumps.wikimedia.org,https://huggingface.co/datasets/legacy-datasets/wikipedia,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,Wikipedia dataset containing cleaned articles of all languages. The datasets are built from the Wikipedia dump (https://dumps.wikimedia.org/) with one split per language. Each
LennardZuendorf/openlegaldata-bulk-data,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/LennardZuendorf/openlegaldata-bulk-data,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for openlegaldata.io bulk case data Dataset Description This is the copy of the lastest dump from openlegaldata.io. I will try to keep this updated, since there is no offical Huggingface Dataset Repo. Homepage: https://de.openlegaldata.io/ Repository: Bulk Data Data-Summary: This is the openlegaldata bulk case download from October 2022. Please refer to the offical website (above) for any more information. I have not made anyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LennardZuendorf/openlegaldata-bulk-data."
Leo-D-M-Appourchaux/OGC-colpali_train_set,,,,,,,https://huggingface.co/datasets/Leo-D-M-Appourchaux/OGC-colpali_train_set,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,OGC dataset by LÃ©o Appourchaux from TW3 vidore/colpali_train_set compatible with llamaindex/vdr-multilingual-train for dse embedding training.
leobertolazzi/ita2medieval,,,,,,,https://huggingface.co/datasets/leobertolazzi/ita2medieval,,,,,,,,,,,,,,,,,,,,"ita2medieval The ita2medieval dataset contains sentences from medieval italian along with paraphrases in contemporary italian (approximately 6.5k pairs in total). The medieval italian sentences are extracted from texts by Dante, Petrarca, Guinizelli and Cavalcanti. It is intended to perform text-style-transfer from contemporary to medieval italian and vice-versa. Loading the dataset from datasets import load_dataset dataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/leobertolazzi/ita2medieval."
LeoCordoba/CC-NEWS-ES,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/LeoCordoba/CC-NEWS-ES,dataset of news,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for CC-NEWS-ES Data-Summary: CC-NEWS-ES is a Spanish-language dataset of news. The corpus was generated by extracting the Spanish articles from CC-NEWS (news index of Common Crawl) of 2019. For doing that FastText model was used for language prediction. It contains a total of 7,473,286 texts and 1,812,009,283 words distributed as follows: domain texts words ar 532703 1.45127e+08 bo 29557 7.28996e+06 br 107 14207 cl 116661â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LeoCordoba/CC-NEWS-ES."
LeoFeng/MLHW_6,,,,,,,https://huggingface.co/datasets/LeoFeng/MLHW_6,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,LeoFeng/MLHW_6 dataset hosted on Hugging Face and contributed by the HF Datasets community
LeoLM/MMLU_de,,,,,,,https://huggingface.co/datasets/LeoLM/MMLU_de,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge, covering 57 tasks including elementary mathematics, US history, computer science, law, and more."
leondz/wnut_17,Text,,,,,http://noisy-text.github.io/2017/emerging-rare-entities.html,https://huggingface.co/datasets/leondz/wnut_17,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"WNUT 17: Emerging and Rare entity recognition This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarisation), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for"
lesserfield/brainly,,,,,,,https://huggingface.co/datasets/lesserfield/brainly,,,,,,,,https://choosealicense.com/licenses/unlicense/,,,,,,,,,,,,"brainly.co.id dataset Data Structure The keys in each JSONL object include: ""id"": An integer value representing the page of task from url (e.g. brainly.co.id/tugas/117). ""subject"": A string indicating the subject of the question (e.g., ""Fisika"", ""Matematika"", ""Sejarah""). ""author"": A string representing the author of the question. ""instruction"": A string providing the instruction or prompt for the question. ""answerer_1"", ""answer_2"": Strings representing theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lesserfield/brainly."
letinnghia/student-feedbacks,,,,,,,https://huggingface.co/datasets/letinnghia/student-feedbacks,,,,,,,,https://choosealicense.com/licenses/gpl-2.0/,,,,,,,,,,,,letinnghia/student-feedbacks dataset hosted on Hugging Face and contributed by the HF Datasets community
levalencia/TwitterHateSpeech,,,,,,,https://huggingface.co/datasets/levalencia/TwitterHateSpeech,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,levalencia/TwitterHateSpeech dataset hosted on Hugging Face and contributed by the HF Datasets community
levinlab/neuroscience-to-dev-bio,,,,,,,https://huggingface.co/datasets/levinlab/neuroscience-to-dev-bio,,,,,,,,,,,,,,,,,,,,levinlab/neuroscience-to-dev-bio dataset hosted on Hugging Face and contributed by the HF Datasets community
lewtun/drug-reviews,,,,,,,https://huggingface.co/datasets/lewtun/drug-reviews,,,,,,,,,,,,,,,,,,,,lewtun/drug-reviews dataset hosted on Hugging Face and contributed by the HF Datasets community
lewy666/ChartInstructionData,,,,,,,https://huggingface.co/datasets/lewy666/ChartInstructionData,,,,,,,,,,,,,,,,,,,,lewy666/ChartInstructionData dataset hosted on Hugging Face and contributed by the HF Datasets community
lexlms/lex_files,Text,General,General,Text,"Accuracy, F1 Score",https://github.com/coastalcph/lexlms,https://huggingface.co/datasets/lexlms/lex_files,English,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,2016,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""LexFiles"" Data-Summary: The LeXFiles is a new diverse English multinational legal corpus that we created including 11 distinct sub-corpora that cover legislation and case law from 6 primarily English-speaking legal systems (EU, CoE, Canada, US, UK, India). The corpus contains approx. 19 billion tokens. In comparison, the ""Pile of Law"" corpus released by Hendersons et al. (2022) comprises 32 billion in total, where the majority (26/30) ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lexlms/lex_files."
LHF/escorpius-mr,,,,,,https://arxiv.org/abs/2206.15147,https://huggingface.co/datasets/LHF/escorpius-mr,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-nd-4.0/,,,,,,,,,,,,"esCorpius Multilingual Raw In the recent years, Transformer-based models have lead to significant advances in language modelling for natural language processing. However, they require a vast amount of data to be (pre-)trained and there is a lack of corpora in languages other than English. Recently, several initiatives have presented multilingual datasets obtained from automatic web crawling. However, they present important shortcomings for languages different from English, asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LHF/escorpius-mr."
li-lab/MMLU-ProX,,,,,,https://arxiv.org/abs/2503.10497,https://huggingface.co/datasets/li-lab/MMLU-ProX,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"MMLU-ProX MMLU-ProX is a multilingual benchmark that builds upon MMLU-Pro, extending to 13 typologically diverse languages, designed to evaluate large language models' reasoning capabilities across linguistic and cultural boundaries. Github | Paper News [March 2025] ðŸŽ‰ MMLU-ProX is now available on Huggingface! [March 2025] We are still expanding this dataset to more languages! Stay tuned! Overview MMLU-ProX addresses critical limitations in existingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/li-lab/MMLU-ProX."
li2017dailydialog/daily_dialog,Text,,,,,http://yanran.li/dailydialog,https://huggingface.co/datasets/li2017dailydialog/daily_dialog,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"We develop a high-quality multi-turn dialog dataset, DailyDialog, which is intriguing in several aspects. The language is human-written and less noisy. The dialogues in the dataset reflect our daily communication way and cover various topics about our daily life. We also manually label the developed dataset with communication intention and emotion information. Then, we evaluate existing approaches on DailyDialog dataset and hope it benefit the research field of dialog systems."
lianghsun/tw-legal-nlp,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",,https://huggingface.co/datasets/lianghsun/tw-legal-nlp,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"æ³•å¾‹è³‡æ–™ç§‘å­¸ä»»å‹™è³‡æ–™é›† Data-Summary: é€™å€‹è³‡æ–™é›†å°ˆç‚ºæ³•å¾‹é ˜åŸŸçš„è³‡æ–™ç§‘å­¸å®¶è¨­è¨ˆï¼Œç›®çš„æ˜¯æä¾›å¤šæ¨£åŒ–çš„æ³•å¾‹è³‡æ–™ç§‘å­¸ä»»å‹™ï¼Œæ¶µè“‹å¾žå¯¦é«”è­˜åˆ¥åˆ°æ–‡æœ¬è½‰æ›çš„å¤šç¨®æŒ‘æˆ°ã€‚è©²è³‡æ–™é›†æ—¨åœ¨å¹«åŠ©ç ”ç©¶äººå“¡å’Œé–‹ç™¼è€…æ›´å¥½åœ°ç†è§£èˆ‡è™•ç†æ³•å¾‹æ–‡æœ¬ã€‚é€™äº›ä»»å‹™åŒ…æ‹¬ä½†ä¸é™æ–¼ï¼š å¾žåˆ¤æ±ºæ›¸ä¸­å–å‡ºç‰¹å®šäººç‰©ã€æ³•æ¢æˆ–æ™‚é–“ï¼ˆNamed Entity Recognition, NERï¼‰ ç†è§£æ³•å¾‹æ–‡æœ¬çš„å…§å®¹ (Text Classification / Semantic Understanding) å°‡æ³•å¾‹æ–‡æœ¬è½‰æ›ç‚º JSON åŠ Markdown æ ¼å¼ (Text-to-Structure / Parsing / Information Extraction) æ³•è¦ç°¡å¯«ç·¨è™Ÿèˆ‡å®Œæ•´ç·¨è™Ÿå¯«æ³•äº’æ›ï¼ˆText Transformation, Seq2Seqï¼‰ æœªä¾†æˆ‘å€‘å°‡æŒçºŒæ–°å¢žæ›´å¤šæ³•å¾‹è³‡æ–™ç§‘å­¸ç›¸é—œçš„ä»»å‹™ï¼ŒæŽ¨å‹•è©²é ˜åŸŸçš„ç ”ç©¶ç™¼å±•ã€‚ Supported Tasks and Leaderboards Named Entityâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lianghsun/tw-legal-nlp."
liangyupu/DoTA_dataset,,,,,,,https://huggingface.co/datasets/liangyupu/DoTA_dataset,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Notice for Download Application If you would like to participate in the DIMT2025@ICDAR challenge, please download the End User License Agreement, fill it out and send it to dimt2025.contact@gmail.com to access the data. (Please make sure to use the email address of your huggingface account to send the End User License Agreement.) We will review your application and get in touch as soon as possible. For more information, please refer to our official challenge website.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/liangyupu/DoTA_dataset."
LibrAI/do-not-answer,,,,,,,https://huggingface.co/datasets/LibrAI/do-not-answer,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs Overview Do not answer is an open-source dataset to evaluate LLMs' safety mechanism at a low cost. The dataset is curated and filtered to consist only of prompts to which responsible language models do not answer. Besides human annotations, Do not answer also implements model-based evaluation, where a 600M fine-tuned BERT-like evaluator achieves comparable results with human and GPT-4.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LibrAI/do-not-answer."
lightblue/reasoning-multilingual-R1-Llama-70B-train,,,,,,,https://huggingface.co/datasets/lightblue/reasoning-multilingual-R1-Llama-70B-train,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"lightblue/reasoning-multilingual-R1-Llama-70B-train This is a multilingual reasoning dataset covering more than 30 languages. This dataset was made by: Sampling prompts from English datasets and translating them to various languages Generating responses to these prompts 8 times using deepseek-ai/DeepSeek-R1-Distill-Llama-70B Filtering out <think> sections with incorrect language, non-fluent language, and incorrect answers This dataset was then used to train a multilingualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lightblue/reasoning-multilingual-R1-Llama-70B-train."
lighteval/med_mcqa,,,,,,https://arxiv.org/abs/2203.14371,https://huggingface.co/datasets/lighteval/med_mcqa,,,,,,,,,,,,,,,,,,,,"From ""MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering"" (Pal et al.), MedMCQA is a ""multiple-choice question answering (MCQA) dataset designed to address real-world medical entrance exam questions."" The dataset ""...has more than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity."" The following is an"
Lightricks/Cakeify-Dataset,,,,,,,https://huggingface.co/datasets/Lightricks/Cakeify-Dataset,,,,,,,,,,,,,,,,,,,,Lightricks/Cakeify-Dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
lil-lab/newsroom,Text,,,,,https://lil.nlp.cornell.edu/newsroom/index.html,https://huggingface.co/datasets/lil-lab/newsroom,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"NEWSROOM is a large dataset for training and evaluating summarization systems. It contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. Dataset features includes: - text: Input news text. - summary: Summary for the news. And additional features: - title: news title. - url: url of the news. - date: date of the article. - density: extractive density. - coverage: extractive coverage. - compression: compression ratio. - density_bin: low, medium, high. - coverage_bin: extractive, abstractive. - compression_bin: low, medium, high. This dataset can be downloaded upon requests. Unzip all the contents ""train.jsonl, dev.josnl, test.jsonl"" to the tfds folder."
lilbillbiscuit/biocoder_hidden,,,,,,,https://huggingface.co/datasets/lilbillbiscuit/biocoder_hidden,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""biocoder_hidden"" More Information needed"
limingcv/MultiGen-20M_train,,,,,,https://arxiv.org/abs/2305.11147,https://huggingface.co/datasets/limingcv/MultiGen-20M_train,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""MultiGen-20M_train"" This dataset is constructed from UniControl, and used for evaluation of the paper ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback ControlNet++ Github repository: https://github.com/liming-ai/ControlNet_Plus_Plus"
limjiayi/hateful_memes_expanded,,,,,,,https://huggingface.co/datasets/limjiayi/hateful_memes_expanded,,,,,,,,,,,,,,,,,,,,limjiayi/hateful_memes_expanded dataset hosted on Hugging Face and contributed by the HF Datasets community
limsc/mlm-tapt-requirements,,,,,,,https://huggingface.co/datasets/limsc/mlm-tapt-requirements,,,,,,,,,,,,,,,,,,,,limsc/mlm-tapt-requirements dataset hosted on Hugging Face and contributed by the HF Datasets community
Linaqruf/bandori-card-dataset,,,,,,,https://huggingface.co/datasets/Linaqruf/bandori-card-dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Bandori Card Dataset The Bandori Card Dataset is a comprehensive collection of cards from the game ""BanG Dream! Girls Band Party!"" (Bandori). It aims to provide card metadata and corresponding artwork images for analysis, research, and other purposes. Dataset Description The dataset includes the following information for each card: Card ID: Unique identifier for each card in the game. Rarity: The rarity level of the card, ranging from 1 to 5. Name: The name orâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Linaqruf/bandori-card-dataset."
LingoIITGN/COMI-LINGUA,,,,,,,https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,https://github.com/sagorbrur/codeswitch,,,,,,,,"Dataset Details COMI-LINGUA (COde-MIxing and LINGuistic Insights on Natural Hinglish Usage and Annotation) is a high-quality Hindi-English code-mixed dataset, manually annotated by three annotators. It serves as a benchmark for multilingual NLP models by covering multiple foundational tasks. COMI-LINGUA provides annotations for several key NLP tasks: Language Identification (LID): Token-wise classification of Hindi, English, and other linguistic units. Initial predictions wereâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA."
lingtrain/buryat-russian,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/lingtrain/buryat-russian,lovers,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,Buryat-Russian Parallel Corpora Data-Summary: Dataset was made by Lingtrain community of language lovers.
LinhDuong/chatdoctor-200k,,,,,,https://arxiv.org/abs/2303.14070,https://huggingface.co/datasets/LinhDuong/chatdoctor-200k,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"This ChatDoctor-200K dataset is collected from this paper https://arxiv.org/pdf/2303.14070.pdf Alternatively, you can download the original dataset from this link https://drive.google.com/file/d/1lyfqIwlLSClhgrCutWuEe_IACNq6XNUt/view?usp=sharing"
linhtran92/viet_youtube_asr_corpus_v2,,,,,,,https://huggingface.co/datasets/linhtran92/viet_youtube_asr_corpus_v2,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""viet_youtube_asr_corpus_v2"" More Information needed"
LinkSoul/instruction_merge_set,,,,,,,https://huggingface.co/datasets/LinkSoul/instruction_merge_set,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""instruction_merge_set"" æœ¬æ•°æ®é›†ç”±ä»¥ä¸‹æ•°æ®é›†æž„æˆï¼š æ•°æ®(id in the merged set) Hugging face åœ°å€ notes OIG (unified-ä»»åŠ¡åç§°) 15k https://huggingface.co/datasets/laion/OIG Open Instruction Generalist Dataset Dolly databricks-dolly-15k https://huggingface.co/datasets/databricks/databricks-dolly-15k an open-source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories UltraChatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LinkSoul/instruction_merge_set."
liuhaotian/LLaVA-Instruct-150K,,,,,,,https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"LLaVA Visual Instruct 150K Dataset Card Dataset details Dataset type: LLaVA Visual Instruct 150K is a set of GPT-generated multimodal instruction-following data. It is constructed for visual instruction tuning and for building large multimodal towards GPT-4 vision/language capability. Dataset date: LLaVA Visual Instruct 150K was collected in April 2023, by prompting GPT-4-0314 API. Paper or resources for more information: https://llava-vl.github.io/ License:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K."
LIUM/tedlium,Audio,General,General,Text,"Accuracy, F1 Score",https://www.openslr.org/7/,https://huggingface.co/datasets/LIUM/tedlium,"TED talks, with transcriptions, sampled at 16kHz",,,,,,,https://choosealicense.com/licenses/undefined/,,,,,,,,,"BERT, RoBERTa, T5",,of transcribed speech data,"Dataset Card for tedlium Data-Summary: The TED-LIUM corpus is English-language TED talks, with transcriptions, sampled at 16kHz. The three releases of the corpus range from 118 to 452 hours of transcribed speech data."
liuxuannan/MMFakeBench,,,,,,,https://huggingface.co/datasets/liuxuannan/MMFakeBench,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,liuxuannan/MMFakeBench dataset hosted on Hugging Face and contributed by the HF Datasets community
livecodebench/code_generation_lite,,,,,,https://arxiv.org/abs/2403.07974,https://huggingface.co/datasets/livecodebench/code_generation_lite,,,,,,,,https://choosealicense.com/licenses/cc/,,,,https://github.com/LiveCodeBench/LiveCodeBench,,,,,,,,LiveCodeBench is a temporaly updating benchmark for code generation. Please check the homepage: https://livecodebench.github.io/.
Livingwithmachines/MapReader_Data_SIGSPATIAL_2022,Text,,,,,,https://huggingface.co/datasets/Livingwithmachines/MapReader_Data_SIGSPATIAL_2022,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,TODO
liweili/c4_200m,Text,,,,,,https://huggingface.co/datasets/liweili/c4_200m,,,,,,,,,,,,,,,,,,,,\ GEC Dataset Generated from C4
liwu/MNBVC,,,,,,http://mnbvc.253874.net/,https://huggingface.co/datasets/liwu/MNBVC,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,MNBVC: Massive Never-ending BT Vast Chinese corpus
liyucheng/chinese_metaphor_dataset,Text,,,,,https://github.com/liyucheng09/Metaphor_Generator,https://huggingface.co/datasets/liyucheng/chinese_metaphor_dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,Chinese Metaphor Corpus The first Chinese metaphor corpus serving both metaphor identification and generation. é¦–ä¸ªä¸­æ–‡æ¯”å–»æ•°æ®é›†ï¼Œå¯ä»¥ç”¨äºŽä¸­æ–‡æ¯”å–»è¯†åˆ«ä¸Žä¸­æ–‡æ¯”å–»ç”Ÿæˆã€‚
ll00292007/lora,,,,,,,https://huggingface.co/datasets/ll00292007/lora,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,ll00292007/lora dataset hosted on Hugging Face and contributed by the HF Datasets community
llm-blender/mix-instruct,,,,,,,https://huggingface.co/datasets/llm-blender/mix-instruct,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,Eval ResultsAuto MetricsChatGPT CMPTS (4771 examples)MixInstructIntroductionThis is the official realease of datasetMixInstructfor projectLLM-Blender,,,,,,"MixInstruct Introduction This is the official realease of dataset MixInstruct for project LLM-Blender. This dataset contains 11 responses from the current popular instruction following-LLMs that includes: Stanford Alpaca FastChat Vicuna Dolly V2 StableLM Open Assistant Koala Baize Flan-T5 ChatGLM MOSS Moasic MPT We evaluate each response with auto metrics including BLEU, ROUGE, BERTScore, BARTScore. And provide pairwise comparison results by prompting ChatGPTâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llm-blender/mix-instruct."
llm-book/wrime-sentiment,,,,,,,https://huggingface.co/datasets/llm-book/wrime-sentiment,,,,,,,,,,,,,,,,,,,,"Dataset Card for llm-book/wrime-sentiment æ—¥æœ¬èªžã®æ„Ÿæƒ…åˆ†æžãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ WRIME ã‚’ã€ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ã®äºŒå€¤åˆ†é¡žã®ã‚¿ã‚¹ã‚¯ã«åŠ å·¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚ GitHub ãƒªãƒã‚¸ãƒˆãƒª ids-cv/wrime ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚ Avg. Readers_Sentiment ã®å€¤ãŒ0ã‚ˆã‚Šå¤§ãã„ã‚‚ã®ã‚’ãƒã‚¸ãƒ†ã‚£ãƒ–ã€0ã‚ˆã‚Šå°ã•ã„ã‚‚ã®ã‚’ãƒã‚¬ãƒ†ã‚£ãƒ–ã¨ãƒ©ãƒ™ãƒ«ä»˜ã‚’ã—ã¦ã„ã¾ã™ã€‚ æ›¸ç±ã€Žå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã§åˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚ è©³ã—ãã¯æ›¸ç±ã®GitHubãƒªãƒã‚¸ãƒˆãƒªã‚’ã”è¦§ãã ã•ã„ã€‚ ä½¿ã„æ–¹ ä»¥ä¸‹ã®ã‚ˆã†ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚ from datasets import load_dataset dataset = load_dataset(""hf_datasets/wrime-sentiment"") print(dataset[""train""].features[""label""]) print(dataset)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/llm-book/wrime-sentiment."
LLM-Tuning-Safety/HEx-PHI,,,,,,https://arxiv.org/abs/2310.03693,https://huggingface.co/datasets/LLM-Tuning-Safety/HEx-PHI,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,HEx-PHI: Human-Extended Policy-Oriented Harmful Instruction Benchmark This dataset contains 330 harmful instructions (30
llm-wizard/alpaca-gpt4-data-zh,,,,,,https://arxiv.org/abs/2304.03277,https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for ""alpaca-gpt4-data-zh"" All of the work is done by this team. Usage and License Notices The data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes. English Dataset Found here Citation @article{peng2023gpt4llm, title={Instruction Tuning with GPT-4}, author={Baolin Pengâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh."
LLukas22/lfqa_preprocessed,Text,Question Answering,General,Text,"Exact Match, F1 Score",https://towardsdatascience.com/long-form-qa-beyond-eli5-an-updated-dataset-and-approach-319cb841aabb,https://huggingface.co/datasets/LLukas22/lfqa_preprocessed,,,,,,,,https://choosealicense.com/licenses/mit/,,1974,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for ""lfqa_preprocessed"" Data-Summary: This is a simplified version of vblagoje's lfqa_support_docs and lfqa datasets. It was generated by me to have a more straight forward way to train Seq2Seq models on context based long form question answering tasks. Dataset Structure Data Instances An"
llvm-ml/ComPile,Text,General,General,Text,"Accuracy, F1 Score",https://llvm-ml.github.io/ComPile/,https://huggingface.co/datasets/llvm-ml/ComPile,,2.7 TB,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ComPile: A Large IR Dataset from Production Sources Changelog Release Programming Languages Description v1.0 C/C++, Rust, Swift, Julia Fine Tuning-scale dataset of 602GB of deduplicated LLVM (bitcode) IR Data-Summary: ComPile contains over 2.7TB of permissively-licensed source code compiled to (textual) LLVM intermediate representation (IR) covering C/C++, Rust, Swift, and Julia. The dataset was created by hookingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llvm-ml/ComPile."
lmg-anon/vntl-leaderboard,,,,,,,https://huggingface.co/datasets/lmg-anon/vntl-leaderboard,,,,,,,,,,,,,,,,,,,,"VNTL Leaderboard The VNTL leaderboard ranks Large Language Models (LLMs) based on their performance in translating Japanese Visual Novels into English. Please be aware that the current results are preliminary and subject to change as new models are evaluated, or changes are done in the evaluation script. Comparison with Established Translation Tools For comparison, this table shows the scores for established translation tools. These include both widely availableâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lmg-anon/vntl-leaderboard."
lmms-lab/LLaVA-Video-178K,,,,,,https://arxiv.org/abs/2410.02713,https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K,,,,,,,,,,,,,,,,,,,,"Dataset Card for LLaVA-Video-178K Uses This dataset is used for the training of the LLaVA-Video model. We only allow the use of this dataset for academic research and education purpose. For OpenAI GPT-4 generated data, we recommend the users to check the OpenAI Usage Policy. Data Sources For the training of LLaVA-Video, we utilized video-language data from five primary sources: LLaVA-Video-178K: This dataset includes 178,510 caption entries, 960â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K."
lmsys/chatbot_arena_conversations,,,,,,https://arxiv.org/abs/2306.05685,https://huggingface.co/datasets/lmsys/chatbot_arena_conversations,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,"Chatbot Arena Conversations Dataset This dataset contains 33K cleaned conversations with pairwise human preferences. It is collected from 13K unique IP addresses on the Chatbot Arena from April to June 2023. Each sample includes a question ID, two model names, their full conversation text in OpenAI API JSON format, the user vote, the anonymized user ID, the detected language tag, the OpenAI moderation API tag, the additional toxic tag, and the timestamp. To ensure the safeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lmsys/chatbot_arena_conversations."
loaiabdalslam/counselchat,,,,,,,https://huggingface.co/datasets/loaiabdalslam/counselchat,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,loaiabdalslam/counselchat dataset hosted on Hugging Face and contributed by the HF Datasets community
Locutusque/Collective-Evol-Instruct-v0.1,,,,,,,https://huggingface.co/datasets/Locutusque/Collective-Evol-Instruct-v0.1,,,,,,,,,,,,,,,,,,,,Locutusque/Collective-Evol-Instruct-v0.1 dataset hosted on Hugging Face and contributed by the HF Datasets community
LoganKells/amazon_product_reviews_video_games,,,,,,,https://huggingface.co/datasets/LoganKells/amazon_product_reviews_video_games,,,,,,,,,,,,,,,,,,,,#Title
logo-wizard/modern-logo-dataset,,,,,,,https://huggingface.co/datasets/logo-wizard/modern-logo-dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-3.0/,,,,,,,,,,,,"Dataset Card for ""logo-dataset-v4"" This dataset consists of 803 pairs (x,y) (x, y) (x,y), where x x x is the image and y y y is the description of the image.The data have been manually collected and labelled, so the dataset is fully representative and free of rubbish.The logos in the dataset are minimalist, meeting modern design requirements and reflecting the company's industry. Disclaimer This dataset is made available for academic research purposes only. Allâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/logo-wizard/modern-logo-dataset."
Loie/VGGSound,,,,,,https://arxiv.org/abs/2004.14368,https://huggingface.co/datasets/Loie/VGGSound,,,,,,,,,,,,,,,,,,,,"VGGSound VGG-Sound is an audio-visual correspondent dataset consisting of short clips of audio sounds, extracted from videos uploaded to YouTube. Homepage: https://www.robots.ox.ac.uk/~vgg/data/vggsound/ Paper: https://arxiv.org/abs/2004.14368 Github: https://github.com/hche11/VGGSound Analysis 310+ classes: VGG-Sound contains audios spanning a large number of challenging acoustic environments and noise characteristics of real applications. 200,000+ videos: Allâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Loie/VGGSound."
lonestar108/enlightenedllm,,,,,,,https://huggingface.co/datasets/lonestar108/enlightenedllm,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,"Dataset Card for ""enlightenedllm"" More Information needed"
longface/logicLM,,,,,,,https://huggingface.co/datasets/longface/logicLM,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,longface/logicLM dataset hosted on Hugging Face and contributed by the HF Datasets community
longquan/llm-japanese-dataset-split_10,,,,,,,https://huggingface.co/datasets/longquan/llm-japanese-dataset-split_10,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,longquan/llm-japanese-dataset-split_10 dataset hosted on Hugging Face and contributed by the HF Datasets community
LooksJuicy/Chinese-Roleplay-Novel,,,,,,,https://huggingface.co/datasets/LooksJuicy/Chinese-Roleplay-Novel,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,ä¸€ç›´ä»¥æ¥ï¼Œä¸­æ–‡è§’è‰²æ‰®æ¼”å¼€æºæ•°æ®é›†æ›´å…³æ³¨è¶…æ‹Ÿäººæ–¹å‘æˆ–çº¯è§’è‰²å¯¹è¯æ–¹å‘ï¼Œä¸¥é‡ç¼ºä¹äº¤äº’æ¸¸æˆæ–¹å‘çš„å¼€æºæ•°æ®ï¼Œå› æ­¤è®¸å¤šæ¨¡åž‹å°¤å…¶å‚æ•°é‡è¾ƒå°çš„æ¨¡åž‹å¯¹é…’é¦†ç±»çš„è§’è‰²å¡æ”¯æŒè¾ƒå·®ã€‚ ä¸ºäº†è§£å†³è¿™ä¸€å›°å¢ƒï¼Œæœ¬é¡¹ç›®æŠ›ç –å¼•çŽ‰ï¼ŒåŸºäºŽ4500æ¡å°è¯´æ–‡æœ¬ä½¿ç”¨GPT4oæž„å»ºå‡ºçº¦260æ¡é…’é¦†styleçš„æ•°æ®é›†ï¼Œå‡ä¸ºå¤šè½®å¯¹è¯ï¼Œæ¯è½®å¯¹è¯éƒ½åŒ…æ‹¬çŠ¶æ€æ•°æ®ï¼Œå¦‚æ—¶é—´ã€è§’è‰²çŠ¶æ€ã€ä»»åŠ¡è¿›åº¦ç­‰ã€‚ æ•°æ®keyå¯¹åº”å«ä¹‰å¦‚ä¸‹ï¼š worldï¼šè¡¨ç¤ºå½“å‰æ•…äº‹çš„ä¸–ç•Œè§‚ï¼Œé€šå¸¸å¯ä»¥åŠ å…¥åˆ°system promptä¸­ scenceï¼šè¡¨ç¤ºå½“å‰æ•…äº‹å‘ç”Ÿåœºæ™¯ï¼ŒåŒ…æ‹¬æ—¶é—´ã€åœ°ç‚¹ã€çŽ¯å¢ƒã€ä»»åŠ¡ç›®æ ‡ characterï¼šè¡¨ç¤ºå½“å‰æ•…äº‹ä¸­å¯èƒ½å‡ºçŽ°çš„è§’è‰²å’Œå¯¹åº”ç®€ä»‹ fieldï¼šè¡¨ç¤ºè¿™æ¡æ•°æ®æ¯è½®å¯¹è¯ä¸­éœ€è¦ç”Ÿæˆçš„çŠ¶æ€ä¿¡æ¯ conversationsï¼šè¡¨ç¤ºè¿™æ¡æ•°æ®çš„å¯¹è¯å†…å®¹ï¼Œåˆ†ä¸ºé—®å€™è¯­ã€ä¸»è§’(user)å’Œç³»ç»Ÿ(assistant) fields_formatï¼šè¡¨ç¤ºçŠ¶æ€ä¿¡æ¯çš„å¡«å……æ ¼å¼promptï¼Œå¯èƒ½æ˜¯åˆ—è¡¨ã€è¡¨æ ¼ã€JSONç­‰å„ç§å½¢å¼ format_listï¼šè¡¨ç¤ºçŠ¶æ€ä¿¡æ¯çš„å¡«å……ç»“æžœ çŠ¶æ€ä¿¡æ¯çš„ç¤ºä¾‹å¦‚ä¸‹ **å¥åº·çŠ¶æ€**: ðŸŒ¿ è‰¯å¥½ï¼Œèº«ä½“é¢¤æŠ– **ç²¾ç¥žçŠ¶æ€**: ðŸŒŸ ææƒ§ï¼Œæžåº¦ç´§å¼ â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LooksJuicy/Chinese-Roleplay-Novel.
loubnabnl/github-code-more-filtering,,,,,,,https://huggingface.co/datasets/loubnabnl/github-code-more-filtering,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,loubnabnl/github-code-more-filtering dataset hosted on Hugging Face and contributed by the HF Datasets community
lsb/pile,,,,,,,https://huggingface.co/datasets/lsb/pile,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""pile"" More Information needed"
lst-nectec/lst20,Text,,,,,https://aiforthai.in.th/,https://huggingface.co/datasets/lst-nectec/lst20,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"LST20 Corpus is a dataset for Thai language processing developed by National Electronics and Computer Technology Center (NECTEC), Thailand. It offers five layers of linguistic annotation: word boundaries, POS tagging, named entities, clause boundaries, and sentence boundaries. At a large scale, it consists of 3,164,002 words, 288,020 named entities, 248,181 clauses, and 74,180 sentences, while it is annotated with 16 distinct POS tags. All 3,745 documents are also annotated with one of 15 news genres. Regarding its sheer size, this dataset is considered large enough for developing joint neural models for NLP. Manually download at https://aiforthai.in.th/corpus.php"
LTCB/enwik8,Text,,,,,http://mattmahoney.net/dc/textdata.html,https://huggingface.co/datasets/LTCB/enwik8,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,The dataset is based on the Hutter Prize (http://prize.hutter1.net) and contains the first 10^8 bytes of English Wikipedia in 2006 in XML
ltg/norec_tsa,,,,,,,https://huggingface.co/datasets/ltg/norec_tsa,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Dataset Card for NoReC TSA This is a dataset for Targeted Sentiment Analysis (TSA) in Norwegian, derived from the the fine-grained annotations of NNoReC_fine. The dataset contains tokenized Norwegian sentences where each token is tagged for sentiment expressed within the sentence towards that token. Since a sentiment target may be the target of several sentiment expressions, these are resolved to a final sentiment polarity (and intensity) using the conversion script inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ltg/norec_tsa."
lucadiliello/dropqa,,,,,,,https://huggingface.co/datasets/lucadiliello/dropqa,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""dropqa"" Split taken from the MRQA 2019 Shared Task, formatted and filtered for Question Answering. For the original dataset, have a look here."
lucasmccabe/logiqa,Text,,,,,,https://huggingface.co/datasets/lucasmccabe/logiqa,,,,,,,,,,,,,,,,,,,,"LogiQA is constructed from the logical comprehension problems from publically available questions of the National Civil Servants Examination of China, which are designed to test the civil servant candidatesâ€™ critical thinking and problem solving. This dataset includes the English versions only; the Chinese versions are available via the homepage/original source."
Luckyjhg/Geo170K,,,,,,,https://huggingface.co/datasets/Luckyjhg/Geo170K,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""Geo170K"" More Information needed"
luisf1xc/data_drugs_class,,,,,,,https://huggingface.co/datasets/luisf1xc/data_drugs_class,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,luisf1xc/data_drugs_class dataset hosted on Hugging Face and contributed by the HF Datasets community
lukaemon/mmlu,,,,,,,https://huggingface.co/datasets/lukaemon/mmlu,,,,,,,,,,,,,,,,,,,,"Measuring Massive Multitask Language Understanding by Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt (ICLR 2021)."
lukebarousse/data_jobs,,,,,,,https://huggingface.co/datasets/lukebarousse/data_jobs,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,lukebarousse/data_jobs dataset hosted on Hugging Face and contributed by the HF Datasets community
LukeSajkowski/products_ecommerce_embeddings,,,,,,,https://huggingface.co/datasets/LukeSajkowski/products_ecommerce_embeddings,,,,,,,,,,,,https://github.com/querqy/chorus/tree/main/data-encoder,,,,,,,,"Dataset Card for ""products_ecommerce_embeddings"" The dataset is based on https://github.com/querqy/chorus/tree/main/data-encoder More Information needed"
lumenggan/avatar-the-last-airbender-tagged,,,,,,,https://huggingface.co/datasets/lumenggan/avatar-the-last-airbender-tagged,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,"Dataset Card for ""avatar-the-last-airbender-tagged"" More Information needed"
Lurunchik/WikiHowNFQA,,,,,,https://lurunchik.github.io/WikiHowQA/,https://huggingface.co/datasets/Lurunchik/WikiHowNFQA,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for WikiHowQA WikiHowQA is a unique collection of 'how-to' content from WikiHow, transformed into a rich dataset featuring 11,746 human-authored answers and 74,527 supporting documents. Designed for researchers, it presents a unique opportunity to tackle the challenges of creating comprehensive answers from multiple documents, and grounding those answers in the real-world context provided by the supporting documents. Dataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Lurunchik/WikiHowNFQA."
luyu0311/MMCBNU_6000,,,,,,,https://huggingface.co/datasets/luyu0311/MMCBNU_6000,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,"Introduction MMCBNU_6000 consists of finger vein images from 100 volunteers. Each subject was asked to provide images of his or her index finger, middle finger, and ring finger of both hands in a conventional office environment (rather than a darkroom). The collection for each of the 6 fingers is repeated 10 times to obtain 60 finger vein images for each volunteer. Hence, MMCBNU_6000 is composed of 6,000 images. Each image is stored in â€œbmpâ€ format with a resolution of 480Ã—640.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/luyu0311/MMCBNU_6000."
lvwerra/stack-exchange-paired,,,,,,,https://huggingface.co/datasets/lvwerra/stack-exchange-paired,,,,,,,,,,,,,,,,,,,,"StackExchange Paired This is a processed version of the HuggingFaceH4/stack-exchange-preferences. The following steps were applied: Parse HTML to Markdown with markdownify Create pairs (response_j, response_k) where j was rated better than k Sample at most 10 pairs per question Shuffle the dataset globally This dataset is designed to be used for preference learning. The processing notebook is in the repository as well."
LxYxvv/ChinaDaily_EN_ZH,,,,,,,https://huggingface.co/datasets/LxYxvv/ChinaDaily_EN_ZH,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,LxYxvv/ChinaDaily_EN_ZH dataset hosted on Hugging Face and contributed by the HF Datasets community
lyimo/shakespear,,,,,,,https://huggingface.co/datasets/lyimo/shakespear,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,lyimo/shakespear dataset hosted on Hugging Face and contributed by the HF Datasets community
lysandre/image-to-text,,,,,,,https://huggingface.co/datasets/lysandre/image-to-text,,,,,,,,,,,,,,,,,,,,lysandre/image-to-text dataset hosted on Hugging Face and contributed by the HF Datasets community
M-A-D/Mixed-Arabic-Datasets-Repo,,,,,,,https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Datasets-Repo,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""Mixed Arabic Datasets (MAD) Corpus"" The Mixed Arabic Datasets Corpus : A Community-Driven Collection of Diverse Arabic Texts Dataset Description The Mixed Arabic Datasets (MAD) presents a dynamic compilation of diverse Arabic texts sourced from various online platforms and datasets. It addresses a critical challenge faced by researchers, linguists, and language enthusiasts: the fragmentation of Arabic language datasets across the Internet. Withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Datasets-Repo."
m-a-p/FineFineWeb,,,,,,,https://huggingface.co/datasets/m-a-p/FineFineWeb,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,FineFineWeb: A Comprehensive Study on Fine-Grained Domain Web Corpus arXiv: Coming Soon Project Page: Coming Soon Blog: Coming Soon Data Statistics Domain (#tokens/#samples) Iteration 1 Tokens Iteration 2 Tokens Iteration 3 Tokens Total Tokens Iteration 1 Count Iteration 2 Count Iteration 3 Count Total Count aerospace 5.77B 261.63M 309.33M 6.34B 9100000 688505 611034 10399539 agronomy 13.08B 947.41M 229.04M 14.26B 15752828 2711790 649404 19114022â€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/FineFineWeb.
m-aliabbas/common_voice_urdu,,,,,,,https://huggingface.co/datasets/m-aliabbas/common_voice_urdu,,,,,,,,,,,,,,,,,,,,m-aliabbas/common_voice_urdu dataset hosted on Hugging Face and contributed by the HF Datasets community
ma2za/many_emotions,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/ma2za/many_emotions,[More Information Needed],,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""many_emotions"" Data-Summary: Languages [More Information Needed] Dataset Structure Data Instances [More Information Needed] Data Fields The data fields are: id: unique identifier text: a string feature. label: a classification label, with possible values including anger (0), fear (1), joy (2), love ( 3), sadness (4), surprise (5), neutral (6). license: inherited license from sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ma2za/many_emotions."
MaartenGr/arxiv_nlp,,,,,,,https://huggingface.co/datasets/MaartenGr/arxiv_nlp,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"arXiv Abstracts Abstracts for the cs.CL category of ArXiv between 1991 and 2024. This dataset was created as an instructional tool for the Clustering and Topic Modeling chapter in the upcoming ""Hands-On Large Language Models"" book. The original dataset was retrieved here. This subset will be updated towards the release of the book to make sure it captures relatively recent articles in the domain."
maastrichtlawtech/bsard,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2108.11792,https://huggingface.co/datasets/maastrichtlawtech/bsard,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for BSARD Data-Summary: The Belgian Statutory Article Retrieval Dataset (BSARD) is a French native dataset for studying legal information retrieval. BSARD consists of more than 22,600 statutory articles from Belgian law and about 1,100 legal questions posed by Belgian citizens and labeled by experienced jurists with relevant articles from the corpus. Supported Tasks and Leaderboards document-retrieval: The dataset can be used toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/maastrichtlawtech/bsard."
Maciel/FinCUGE-Instruction,,,,,,,https://huggingface.co/datasets/Maciel/FinCUGE-Instruction,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset Card for Dataset Name Dataset Description æœ¬æ•°æ®é›†åŒ…å«å…«é¡¹ä¸­æ–‡é‡‘èžè‡ªç„¶è¯­è¨€å¤„ç†åŸºå‡†ä»»åŠ¡ï¼Œåˆ†åˆ«ä¸ºé‡‘èžæ–°é—»æ‘˜è¦(FinNA)ã€é‡‘èžæ–°é—»å…¬å‘Šäº‹ä»¶é—®ç­”(FinQA)ã€é‡‘èžæ–°é—»åˆ†ç±»(FinNL)ã€é‡‘èžæ–°é—»å…³ç³»æŠ½å–(FinRE)ã€é‡‘èžç¤¾äº¤åª’ä½“æ–‡æœ¬æƒ…ç»ªåˆ†ç±»(FinNE)ã€é‡‘èžè´Ÿé¢æ¶ˆæ¯åŠå…¶ä¸»ä½“åˆ¤å®š(FinNSP)ã€é‡‘èžå› æžœäº‹ä»¶æŠ½å–(FinCQA)ã€é‡‘èžäº‹ä»¶ä¸»ä½“æŠ½å–(FinESE)ã€‚ Dataset Structure ï¼ˆ1ï¼‰FinNA é‡‘èžæ–°é—»æ‘˜è¦æ•°æ®é›†ã€‚è¾“å…¥ä¸€æ®µé‡‘èžæ–°é—»ï¼Œéœ€è¦æ¨¡åž‹ç”Ÿæˆä¸€å¥è¯æ‘˜è¦ã€‚å…¶ä¸­è®­ç»ƒé›†åŒ…å«24000æ¡æ•°æ®ï¼ŒéªŒè¯é›†åŒ…å«3000æ¡æ•°æ®ã€‚ { ""instruction"": ""æ ¹æ®ä»¥ä¸‹æ–°é—»ç”Ÿæˆæ‘˜è¦ã€‚"", ""input"":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Maciel/FinCUGE-Instruction."
madao33/new-title-chinese,,,,,,,https://huggingface.co/datasets/madao33/new-title-chinese,,,,,,,,,,,,,,,,,,,,madao33/new-title-chinese dataset hosted on Hugging Face and contributed by the HF Datasets community
madebyollin/megalith-10m,,,,,,https://arxiv.org/abs/2310.16825,https://huggingface.co/datasets/madebyollin/megalith-10m,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"ðŸ—¿ Megalith-10m What is Megalith-10m? Megalith-10m is a dataset of ~10 million links to Flickr images that were categorized as ""photo"" with license info of: No known copyright restrictions (Flickr commons), or United States Government Work, or Public Domain Dedication (CC0), or Public Domain Mark What's the intended use of Megalith-10m? Megalith-10m is intended to contain only links to wholesome unedited uncopyrighted photographs - the sort of images that weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/madebyollin/megalith-10m."
madrylab/platinum-bench-paper-version,Text,General,General,Text,"Accuracy, F1 Score",http://platinum-bench.csail.mit.edu/,https://huggingface.co/datasets/madrylab/platinum-bench-paper-version,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for PlatinumBench (Paper Version) ðŸ† Leaderboard | ðŸ–¥ï¸ Code | ðŸ“– Paper This HuggingFace dataset contains the paper version of the dataset. Unless you are specifically interested in reproducing the results from our paper, we recommend that you use the live version, which we update as we find new issues with questions. Please find it at here Data-Summary: Platinum Benchmarks are benchmarks that are are carefully curated to minimize label errors andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/madrylab/platinum-bench-paper-version."
MagedSaeed/MADBase,Text,General,General,Text,"Accuracy, F1 Score",https://datacenter.aucegypt.edu/shazeem/,https://huggingface.co/datasets/MagedSaeed/MADBase,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for MADBase Data-Summary: This dataset card aims to be a base template for new datasets. It has been generated using this raw template. Supported Tasks and Leaderboards [More Information Needed] Languages Arabic Dataset Structure Data Instances { 'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F5EE5B427A0>, 'label': 1, } Data Fields image: Aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MagedSaeed/MADBase."
magic-mirror/tsdm-lossless-music-2-dsd,,,,,,,https://huggingface.co/datasets/magic-mirror/tsdm-lossless-music-2-dsd,,,,,,,,,,,,,,,,,,,,magic-mirror/tsdm-lossless-music-2-dsd dataset hosted on Hugging Face and contributed by the HF Datasets community
MagicHub/railway-knowledge-QA,,,,,,,https://huggingface.co/datasets/MagicHub/railway-knowledge-QA,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,MagicHub/railway-knowledge-QA dataset hosted on Hugging Face and contributed by the HF Datasets community
Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ,,,,,,https://arxiv.org/abs/2406.08464,https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ,,,,,,,,https://choosealicense.com/licenses/llama3.1/,,,,,,,,,,,,"Project Web: https://magpie-align.github.io/ Arxiv Technical Report: https://arxiv.org/abs/2406.08464 Codes: https://github.com/magpie-align/magpie Abstract Click Here High-quality instruction data is critical for aligning large language models (LLMs). Although some models, such as Llama-3-Instruct, have open weights, their alignment data remain private, which hinders the democratization of AI. High human labor costs and a limited, predefined scope for prompting preventâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ."
maharshipandya/spotify-tracks-dataset,,,,,,,https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset,,,,,,,,https://choosealicense.com/licenses/bsd/,,,,,,,,,,,,Content This is a dataset of Spotify tracks over a range of 125 different genres. Each track has some audio features associated with it. The data is in CSV format which is tabular and can be loaded quickly. Usage The dataset can be used for: Building a Recommendation System based on some user input or preference Classification purposes based on audio features and available genres Any other application that you can think of. Feel free to discuss!â€¦ See the full description on the dataset page: https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset.
MahmoodLab/hest,,,,,,https://arxiv.org/abs/2406.16192,https://huggingface.co/datasets/MahmoodLab/hest,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"Model Card for HEST-1k What is HEST-1k? A collection of 1,229 spatial transcriptomic profiles, each linked and aligned to a Whole Slide Image (with pixel size < 1.15 Âµm/px) and metadata. HEST-1k was assembled from 131 public and internal cohorts encompassing: 26 organs 2 species (Homo Sapiens and Mus Musculus) 367 cancer samples from 25 cancer types. HEST-1k processing enabled the identification of 1.5 million expression/morphology pairs and 76 million nucleiâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MahmoodLab/hest."
Maitreya152/CaD_Graphs,,,,,,,https://huggingface.co/datasets/Maitreya152/CaD_Graphs,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Maitreya152/CaD_Graphs dataset hosted on Hugging Face and contributed by the HF Datasets community
malaysia-ai/crawl-kaskus.co.id,,,,,,,https://huggingface.co/datasets/malaysia-ai/crawl-kaskus.co.id,,,,,,,,,,,,,,,,,,,,malaysia-ai/crawl-kaskus.co.id dataset hosted on Hugging Face and contributed by the HF Datasets community
MALIBA-AI/bambara-speech-recognition-leaderboard,,,,,,,https://huggingface.co/datasets/MALIBA-AI/bambara-speech-recognition-leaderboard,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,MALIBA-AI/bambara-speech-recognition-leaderboard dataset hosted on Hugging Face and contributed by the HF Datasets community
malteos/aspect-paper-embeddings,,,,,,,https://huggingface.co/datasets/malteos/aspect-paper-embeddings,,,,,,,,,,,,,,,,,,,,malteos/aspect-paper-embeddings dataset hosted on Hugging Face and contributed by the HF Datasets community
Maluuba/newsqa,Text,,,,,https://www.microsoft.com/en-us/research/project/newsqa-dataset/,https://huggingface.co/datasets/Maluuba/newsqa,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"NewsQA is a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles."
mandarjoshi/trivia_qa,Text,General,Document,Text,"Accuracy, F1 Score",http://nlp.cs.washington.edu/triviaqa/,https://huggingface.co/datasets/mandarjoshi/trivia_qa,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""trivia_qa"" Data-Summary: TriviaqQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaqQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. Supported Tasks and Leaderboards More Information Needed Languagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mandarjoshi/trivia_qa."
mangoesai/DepressionDetection,,,,,,,https://huggingface.co/datasets/mangoesai/DepressionDetection,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""DepressionDetection"" More Information needed"
mangopy/ToolRet-Queries,,,,,,https://arxiv.org/abs/2503.01763,https://huggingface.co/datasets/mangopy/ToolRet-Queries,,,,,,,,,,,,,,,,,,,,ðŸ”§ Retrieving useful tools from a large-scale toolset is an important step for Large language model (LLMs) in tool learning. This project (ToolRet) contribute to (i) the first comprehensive tool retrieval benchmark to systematically evaluate existing information retrieval (IR) models on tool retrieval tasks; and (ii) a large-scale training dataset to optimize the expertise of IR models on this tool retrieval task. See the official Github for more details. A concrete
manu/project_gutenberg,,,,,,,https://huggingface.co/datasets/manu/project_gutenberg,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""Project Gutenberg"" Project Gutenberg is a library of over 70,000 free eBooks, hosted at https://www.gutenberg.org/. All"
Manuel/sentencias-corte-cons-colombia-1992-2021,,,,,,,https://huggingface.co/datasets/Manuel/sentencias-corte-cons-colombia-1992-2021,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,sentencias-corte-cons-colombia-1992-2021. 23750 Case law of the Colombia's Corte Constitucional. Each row is a complete text of each case law. 23750 case law from 1992-2021. Columns: ID Texto: Complete text of the sentence
MaralGPT/persian_quotes,,,,,,,https://huggingface.co/datasets/MaralGPT/persian_quotes,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,MaralGPT/persian_quotes dataset hosted on Hugging Face and contributed by the HF Datasets community
marianna13/fanfics,,,,,,,https://huggingface.co/datasets/marianna13/fanfics,,,,,,,,,,,,,,,,,,,,marianna13/fanfics dataset hosted on Hugging Face and contributed by the HF Datasets community
marksverdhei/wordnet-definitions-en-2021,,,,,,,https://huggingface.co/datasets/marksverdhei/wordnet-definitions-en-2021,,,,,,,,,,,,,,,,,,,,Wordnet definitions for English Dataset by Princeton WordNet and the Open English WordNet team https://github.com/globalwordnet/english-wordnet This dataset contains every entry in wordnet that has a definition and an
marmal88/skin_cancer,,,,,,,https://huggingface.co/datasets/marmal88/skin_cancer,,,,,,,,,,,,,,,,,,,,"The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions Original Paper and Dataset here Kaggle dataset here Introduction to datasets Training of neural networks for automated diagnosis of pigmented skin lesions is hampered by the small size and lack of diversity of available dataset of dermatoscopic images. We tackle this problem by releasing the HAM10000 (""Human Against Machine with 10000 training images"")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/marmal88/skin_cancer."
marmikpandya/mental-health,,,,,,,https://huggingface.co/datasets/marmikpandya/mental-health,,,,,,,,,,,,,,,,,,,,marmikpandya/mental-health dataset hosted on Hugging Face and contributed by the HF Datasets community
marriamaslova/toxic_dvach,,,,,,,https://huggingface.co/datasets/marriamaslova/toxic_dvach,,,,,,,,,,,,,,,,,,,,marriamaslova/toxic_dvach dataset hosted on Hugging Face and contributed by the HF Datasets community
masakhane/masakhaner2,Text,,,,,https://github.com/masakhane-io/masakhane-ner,https://huggingface.co/datasets/masakhane/masakhaner2,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,"MasakhaNER 2.0 is the largest publicly available high-quality dataset for named entity recognition (NER) in 20 African languages. Named entities are phrases that contain the names of persons, organizations, locations, times and quantities."
matchbench/dbp15k-zh-en,,,,,,,https://huggingface.co/datasets/matchbench/dbp15k-zh-en,,,,,,,,,,,,,,,,,,,,matchbench/dbp15k-zh-en dataset hosted on Hugging Face and contributed by the HF Datasets community
math-eval/TAL-SCQ5K,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/math-eval/TAL-SCQ5K,"created by TAL Education Group, each consisting of 5K questions(3K training and 2K testing)",,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"TAL-SCQ5K Dataset Description Data-Summary: TAL-SCQ5K-EN/TAL-SCQ5K-CN are high quality mathematical competition datasets in English and Chinese language created by TAL Education Group, each consisting of 5K questions(3K training and 2K testing). The questions are in the form of multiple-choice and cover mathematical topics at the primary,junior high and high school levels. In addition, detailed solution steps are provided to facilitate CoT training and all theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/math-eval/TAL-SCQ5K."
mathigatti/spanish_imdb_synopsis,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/mathigatti/spanish_imdb_synopsis,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Spanish IMDb Synopsis Dataset Description 4969 movie synopsis from IMDb in spanish. Data-Summary: [N/A] Languages All descriptions are in spanish, the other fields have some mix of spanish and english. Dataset Structure [N/A] Data Fields description: IMDb description for the movie (string), should be spanish keywords: IMDb keywords for the movie (string), mix of spanish and englishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mathigatti/spanish_imdb_synopsis."
matthewlqin/electronic_music,,,,,,,https://huggingface.co/datasets/matthewlqin/electronic_music,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""electronic_music"" More Information needed"
MatthiasPicard/Frugal-AI-Train-Data-88k,,,,,,,https://huggingface.co/datasets/MatthiasPicard/Frugal-AI-Train-Data-88k,,,,,,,,,,,,,,,,,,,,Frugal-AI-Train-Data-88k Synthetic and Real Data combined for the training of the models submitted for the Frugal AI challenge text part of the competition. Composition of the Dataset The dataset contains samples from various sources : Synthetic Quotes : Around 75k of
Matthijs/cmu-arctic-xvectors,,,,,,,https://huggingface.co/datasets/Matthijs/cmu-arctic-xvectors,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Speaker embeddings extracted from CMU ARCTIC There is one .npy file for each utterance in the dataset, 7931 files in total. The speaker embeddings are 512-element X-vectors. The CMU ARCTIC dataset divides the utterances among the following speakers: bdl (US male) slt (US female) jmk (Canadian male) awb (Scottish male) rms (US male) clb (US female) ksp (Indian male) The X-vectors were extracted using this script, which uses the speechbrain/spkrec-xvect-voxceleb model. Usage: fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Matthijs/cmu-arctic-xvectors."
mattmdjaga/human_parsing_dataset,Image,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/mattmdjaga/human_parsing_dataset,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Human parsing data (ATR) Data-Summary: This dataset has 17,706 images and mask pairs. It is just a copy of Deep Human Parsing ATR dataset. The mask labels are: ""0"": ""Background"", ""1"": ""Hat"", ""2"": ""Hair"", ""3"": ""Sunglasses"", ""4"": ""Upper-clothes"", ""5"": ""Skirt"", ""6"": ""Pants"", ""7"": ""Dress"", ""8"": ""Belt"", ""9"": ""Left-shoe"", ""10"": ""Right-shoe"", ""11"": ""Face"", ""12"": ""Left-leg"", ""13"": ""Right-leg""â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mattmdjaga/human_parsing_dataset."
mattymchen/celeba-hq,,,,,,,https://huggingface.co/datasets/mattymchen/celeba-hq,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""celeba-hq"" More Information needed"
maximoss/daccord-contradictions,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",,https://huggingface.co/datasets/maximoss/daccord-contradictions,,,,,,,,https://choosealicense.com/licenses/bsd-2-clause/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for Dataset Name Data-Summary: The DACCORD dataset is an entirely new collection of 1034 sentence pairs annotated as a binary classification task for automatic detection of contradictions between sentences in French. Each pair of sentences receives a label according to whether or not the two sentences contradict each other. DACCORD currently covers the themes of Russiaâ€™s invasion of Ukraine in 2022, the Covid-19 pandemic, and the climate crisis.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/maximoss/daccord-contradictions."
Maxwell-Jia/AIME_2024,,,,,,,https://huggingface.co/datasets/Maxwell-Jia/AIME_2024,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"AIME 2024 Dataset Dataset Description This dataset contains problems from the American Invitational Mathematics Examination (AIME) 2024. AIME is a prestigious high school mathematics competition known for its challenging mathematical problems. Dataset Details Format: JSONL Size: 30 records Source: AIME 2024 I & II Language: English Data Fields Each record contains the following fields: ID: Problem identifier (e.g., ""2024-I-1"" represents Problem 1â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Maxwell-Jia/AIME_2024."
Maxx0/sexting-nsfw-adultconten,,,,,,,https://huggingface.co/datasets/Maxx0/sexting-nsfw-adultconten,,,,,,,,,,,,,,,,,,,,Maxx0/sexting-nsfw-adultconten dataset hosted on Hugging Face and contributed by the HF Datasets community
maykcaldas/smiles-transformers,,,,,,,https://huggingface.co/datasets/maykcaldas/smiles-transformers,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,smiles-transformers dataset TODO: Add references to the datasets we curated dataset features name: text Molecule SMILES : string name: formula Molecular formula : string name: NumHDonors Number of hidrogen bond donors : int name: NumHAcceptors Number of hidrogen bond acceptors : int name: MolLogP Wildman-Crippen LogP : float name: NumHeteroatoms Number of hetero atoms: int name: RingCount Number of rings : int name: NumRotatableBonds Number ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/maykcaldas/smiles-transformers.
MaziyarPanahi/Llama-Nemotron-Post-Training-Dataset-v1-Smol-ShareGPT,,,,,,,https://huggingface.co/datasets/MaziyarPanahi/Llama-Nemotron-Post-Training-Dataset-v1-Smol-ShareGPT,,,,,,,,,,,0,,,,,,,,,Llama-Nemotron-Post-Training-Dataset-v1-Smol-ShareGPT This dataset is a smaller version of NVIDIA's Llama-Nemotron-Post-Training-Dataset-v1 converted to ShareGPT format and merged into a single dataset. Format Each
mbien/recipe_nlg,Text,,,,,https://recipenlg.cs.put.poznan.pl/,https://huggingface.co/datasets/mbien/recipe_nlg,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,The dataset contains 2231142 cooking recipes (>2 millions). It's processed in more careful way and provides more samples than any other dataset in the area.
MBZUAI-LLM/SlimPajama-627B-DC,,,,,,https://arxiv.org/abs/2309.10818,https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Description: This is a split version of cerebras/SlimPajama-627B that divides data based on its sources. The content of this dataset is the same as SlimPajama-627B. We divide data from different sources based on the ""redpajama_setname"" and save them in different directories, which is convenient for future dataset combination related research. This dataset consists of 15,967 jsonl files and is ~ 883G compressed. Primary Usage: This dataset is used for ourâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC."
MBZUAI/LaMini-instruction,,,,,,https://arxiv.org/abs/2304.14402,https://huggingface.co/datasets/MBZUAI/LaMini-instruction,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Dataset Card for ""LaMini-Instruction"" Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, Alham Fikri Aji, Dataset Description We distill the knowledge from large language models by performing sentence/offline distillation (Kim and Rush, 2016). We generate a total of 2.58M pairs of instructions and responses using gpt-3.5-turbo based on several existing resources of prompts, including self-instruct (Wang et al., 2022), P3 (Sanh et al., 2022)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MBZUAI/LaMini-instruction."
McAuley-Lab/Amazon-Reviews-2023,,,,,,https://arxiv.org/abs/2403.03952,https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023,,,,,,,,,,,,,,,,,,,,"Amazon Review 2023 is an updated version of the Amazon Review 2018 dataset. This dataset mainly includes reviews (ratings, text) and item metadata (desc- riptions, category information, price, brand, and images). Compared to the pre- vious versions, the 2023 version features larger size, newer reviews (up to Sep 2023), richer and cleaner meta data, and finer-grained timestamps (from day to milli-second)."
MCG-NJU/MultiSports,Text,,,,,https://deeperaction.github.io/datasets/multisports.html,https://huggingface.co/datasets/MCG-NJU/MultiSports,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,This is a multi-person video dataset of spatio-temporally localized sports actions. Please refer to the github repo for evaluation.
McGill-NLP/stereoset,Text,General,General,Text,"Accuracy, F1 Score",https://stereoset.mit.edu/,https://huggingface.co/datasets/McGill-NLP/stereoset,models,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,in language models,"BERT, RoBERTa, T5",,,"Dataset Card for StereoSet Data-Summary: StereoSet is a dataset that measures stereotype bias in language models. StereoSet consists of 17,000 sentences that measures model preferences across gender, race, religion, and profession. Supported Tasks and Leaderboards multiple-choice question answering Languages English (en) Dataset Structure Data Instances #intersentence {'bias_type': 'race'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/McGill-NLP/stereoset."
mcimpoi/fmd_materials,,,,,,,https://huggingface.co/datasets/mcimpoi/fmd_materials,,,,,,,,https://choosealicense.com/licenses/cc-by-2.0/,,,,,,,,,,,,"Dataset Card for ""fmd_materials"" class_label: names: '0': fabric '1': foliage '2': glass '3': leather '4': metal '5': paper '6': plastic '7': stone '8': water '9': wood Source: https://people.csail.mit.edu/lavanya/fmd.html"
medalpaca/medical_meadow_medqa,Text,Question Answering,General,Text,"Exact Match, F1 Score",,https://huggingface.co/datasets/medalpaca/medical_meadow_medqa,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for MedQA Data-Summary: This is the data and baseline source code for the paper: Jin, Di, et al. ""What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams."" From https://github.com/jind11/MedQA: The data that contains both the QAs and textbooks can be downloaded from this google drive folder. A bit of details of data are explained as below: For QAs, we have three sources: US, Mainland of Chinaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/medalpaca/medical_meadow_medqa."
medarc/pubmed,,,,,,,https://huggingface.co/datasets/medarc/pubmed,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""pubmed"" More Information needed"
medmac01/moroccan_history_qa,,,,,,,https://huggingface.co/datasets/medmac01/moroccan_history_qa,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,medmac01/moroccan_history_qa dataset hosted on Hugging Face and contributed by the HF Datasets community
Meduzka/ukr_psyops_false_news,,,,,,,https://huggingface.co/datasets/Meduzka/ukr_psyops_false_news,,,,,,,,,,,,,,,,,,,,Meduzka/ukr_psyops_false_news dataset hosted on Hugging Face and contributed by the HF Datasets community
megagonlabs/cypherbench,,,,,,https://arxiv.org/abs/2412.18702,https://huggingface.co/datasets/megagonlabs/cypherbench,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"CypherBench CypherBench is a benchmark designed to evaluate text-to-Cypher translation for large language models (LLMs). It includes: 11 large-scale Neo4j property graphs transformed from Wikidata with 7.8 million entities. Over 10,000 (question, Cypher) pairs for training/evaluating text-to-Cypher translation. Paper: https://arxiv.org/pdf/2412.18702 Repository & Demo: https://github.com/megagonlabs/cypherbench Contact: yanlin@megagon.ai Sample Task { ""qid"":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/megagonlabs/cypherbench."
meliascosta/wiki_academic_subjects,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/meliascosta/wiki_academic_subjects,English,,,,,,,https://choosealicense.com/licenses/cc-by-3.0/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for Wiki Academic Disciplines` Data-Summary: This dataset was created from the English wikipedia dump of January 2022. The main goal was to train a hierarchical classifier of academic subjects using HiAGM. Supported Tasks and Leaderboard Text classification - No leaderboard at the moment. Languages English Dataset Structure The dataset consists of groups of labeled text chunks (tokenized by spaces andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/meliascosta/wiki_academic_subjects.
Meranti/CLAP_freesound,,,,,,,https://huggingface.co/datasets/Meranti/CLAP_freesound,,,,,,,,,,,,,,,,,,,,"LAION-Audio-630K Freesound Dataset LAION-Audio-630K is the largest audio-text dataset publicly available and a magnitude larger than previous audio-text datasets (by 2022-11-05). Notably, it combines eight distinct datasets, which includes the Freesound dataset. Specifically, this Hugging face repository contains two versions of Freesound dataset. Details of each dataset (e.g. how captions are made etc.) could be found in the ""datacard"" column of the table below. Freesoundâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Meranti/CLAP_freesound."
merlinyx/pose-controlnet,Image,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/merlinyx/pose-controlnet,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,Data-Summary: The data is based on DeepFashion; turned into image pairs of the same person in same garment with different poses. This won't preserve the person/garment at all but just want to process the data first and see what kind of controlnet it can train as an exercise for training a controlnet. The controlnet_aux's openpose detector sometimes return black images for occluded human images so there won't be a lot of valid image pairs.
merve/poetry,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/merve/poetry,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for poetry Data-Summary: It contains poems from subjects: Love, Nature and Mythology & Folklore that belong to two periods namely Renaissance and Modern Supported Tasks and Leaderboards [Needs More Information] Languages [Needs More Information] Dataset Structure Data Instances [Needs More Information] Data Fields Has 5 columns: Content Author Poem name Age Typeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/merve/poetry."
meta-llama/Llama-3.3-70B-Instruct-evals,,,,,,,https://huggingface.co/datasets/meta-llama/Llama-3.3-70B-Instruct-evals,,,,,,,,https://choosealicense.com/licenses/llama3.3/,,,,,,,,,,,,"Dataset Card for Meta Evaluation Result Details for Llama-3.3-70B-Instruct This dataset contains the results of the Meta evaluation result details for Llama-3.3-70B-Instruct. The dataset has been created from 12 evaluation tasks. The tasks are: human_eval, mmlu_pro, gpqa_diamond, ifeval__loose, mmlu__0_shot__cot, nih__multi_needle, mgsm, math_hard, bfcl_chat, ifeval__strict, math, mbpp_plus. Each task detail can be found as a specific subset in each configuration nd eachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/meta-llama/Llama-3.3-70B-Instruct-evals."
meta-math/MetaMathQA,,,,,,https://arxiv.org/abs/2309.12284,https://huggingface.co/datasets/meta-math/MetaMathQA,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"View the project page: https://meta-math.github.io/ see our paper at https://arxiv.org/abs/2309.12284 Note All MetaMathQA data are augmented from the training sets of GSM8K and MATH. None of the augmented data is from the testing set. You can check the original_question in meta-math/MetaMathQA, each item is from the GSM8K or MATH train set. Model Details MetaMath-Mistral-7B is fully fine-tuned on the MetaMathQA datasets and based on the powerful Mistral-7B model.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/meta-math/MetaMathQA."
mevsg/Jawi-OCR-v1,Text,General,Document,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/mevsg/Jawi-OCR-v1,),,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""Jawi-OCR-v1"" Jawi-OCR-v1 Data-Summary: Jawi-OCR-v1 is a specialized dataset created for OCR (Optical Character Recognition) of historical Malay texts written in Jawi script (Arabic script adapted for Malay language). The dataset comprises sentences extracted from Warta Malaya, one of the pioneering independent Malay daily newspapers from the 1930s in Singapore. It includes both Jawi script and English text, reflecting theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mevsg/Jawi-OCR-v1."
mhardalov/exams,Text,Question Answering,Scientific,Text,"Exact Match, F1 Score",https://arxiv.org/abs/2011.03080,https://huggingface.co/datasets/mhardalov/exams,", covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others",,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for [Dataset Name] Data-Summary: EXAMS is a benchmark dataset for multilingual and cross-lingual question answering from high school examinations. It consists of more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others. Supported Tasks and Leaderboards [More Information Needed] Languages Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mhardalov/exams."
micazevedo/autotrain-data-data-image,,,,,,,https://huggingface.co/datasets/micazevedo/autotrain-data-data-image,,,,,,,,,,,,,,,,,,,,"AutoTrain Dataset for project: data-image Dataset Description This dataset has been automatically processed by AutoTrain for project data-image. Languages The BCP-47 code for the dataset's language is unk. Dataset Structure Data Instances A sample from this dataset looks as follows: [ { ""image"": ""<600x825 RGB PIL image>"", ""target"": 0, ""feat_name"": ""Alakazam"", ""feat_supertype"": ""Pok\u00e9mon""â€¦ See the full description on the dataset page: https://huggingface.co/datasets/micazevedo/autotrain-data-data-image."
miccull/met_museum,,,,,,,https://huggingface.co/datasets/miccull/met_museum,,,,,,,,,,,,,,,,,,,,miccull/met_museum dataset hosted on Hugging Face and contributed by the HF Datasets community
michaelauli/wiki_bio,Text,,,,,https://arxiv.org/abs/1603.07771,https://huggingface.co/datasets/michaelauli/wiki_bio,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"This dataset gathers 728,321 biographies from wikipedia. It aims at evaluating text generation algorithms. For each article, we provide the first paragraph and the infobox (both tokenized). For each article, we extracted the first paragraph (text), the infobox (structured data). Each infobox is encoded as a list of (field name, field value) pairs. We used Stanford CoreNLP (http://stanfordnlp.github.io/CoreNLP/) to preprocess the data, i.e. we broke the text into sentences and tokenized both the text and the field values. The dataset was randomly split in three subsets train (80%), valid (10%), test (10%)."
michaelwzhu/ChatMed_Consult_Dataset,Text,General,General,Text,"Accuracy, F1 Score",https://huggingface.co/datasets/michaelwzhu/ChatMed-Datasets,https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset,models,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ChatMed Data-Summary: ChatMed-Dataset is a dataset of 110,113 medical query-response pairs (in Chinese) generated by OpenAI's GPT-3.5 engine. The queries are crawled from several online medical consultation sites, reflecting the medical needs in the real world. The responses are generated by the OpenAI engine. This dataset is designated to to inject medical knowledge into Chinese large language models. The dataset size growing rapidly. Stayâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset."
michellejieli/friends_dataset,Video,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/michellejieli/friends_dataset,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for friends_data Data-Summary: The Friends dataset consists of speech-based dialogue from the Friends TV sitcom. It is extracted from the SocialNLP EmotionX 2019 challenge. Supported Tasks and Leaderboards text-classification, sentiment-classification: The dataset is mainly used to predict a sentiment label given text input. Languages The utterances are in English. Dataset Structure Dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michellejieli/friends_dataset."
michelleyunun/therapydata,,,,,,,https://huggingface.co/datasets/michelleyunun/therapydata,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""therapydata"" More Information needed"
microsoft/ChatBench,,,,,,,https://huggingface.co/datasets/microsoft/ChatBench,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for ChatBench This is the dataset from the paper, ""ChatBench: From Static Benchmarks to Human-AI Evaluation"", by Serina Chang, Ashton Anderson, and Jake Hofman. Data Summary ChatBench contains data from our user study on Prolific and our automated AI-alone experiments, enabling comparison of AI-alone, user-AI, and user-alone answers for the same set of MMLU benchmark questions (Hendrycks et al., 2021). User study. Our user study consists of two phases. Inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/microsoft/ChatBench."
Middletownbooks/joke_training,,,,,,,https://huggingface.co/datasets/Middletownbooks/joke_training,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Also recommended for inclusion with this training set is a cleaned up version of https://huggingface.co/datasets/laion/OIG/blob/main/unified_joke_explanations.jsonl The ~10k jokes in the jokes file started out as as a file of jokes from reddit and I manually categorized a couple thousand of them. The open question and conversational instructions attempt to integrate jokes into databricks dolly 15k instruction open_qa replies, sometimes slightly modified. The news headlines and news articleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Middletownbooks/joke_training."
Mielikki/Erebus-87k,,,,,,,https://huggingface.co/datasets/Mielikki/Erebus-87k,,,,,,,,,,,,,,,,,,,,"87k human story submissions, ranging across these categories: adventure western urban-fantasy thriller suspense speculative science-fiction sad romance mystery middle-school inspirational horror holiday historical-fiction high-school happy funny friendship fiction fantasy drama crime creative-nonfiction contemporary coming-of-age christmas christian bedtime american"
MightyStudent/Egyptian-ASR-MGB-3,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/MightyStudent/Egyptian-ASR-MGB-3,,,,,,,,,,,,,,,d and adjusted for huggingface hub and ready to be used for whisper finetunning/training,,"BERT, RoBERTa, T5",See the full description on the dataset page: https://huggingface,multi-genre data collected from different YouTube channels,"Egyptian Arabic dialect automatic speech recognition Data-Summary: This dataset was collected, cleaned and adjusted for huggingface hub and ready to be used for whisper finetunning/training. From MGB-3 website: The MGB-3 is using 16 hours multi-genre data collected from different YouTube channels. The 16 hours have been manually transcribed. The chosen Arabic dialect for this year is Egyptian. Given that dialectal Arabic has no orthographic rules, each programâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MightyStudent/Egyptian-ASR-MGB-3."
mikehemberger/plantnet300K,,,,,,,https://huggingface.co/datasets/mikehemberger/plantnet300K,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""plantnet300K"" More Information needed Original Work is here: https://github.com/plantnet/PlantNet-300K @inproceedings{plantnet-300k, author = {C. Garcin and A. Joly and P. Bonnet and A. Affouard and \JC Lombardo and M. Chouet and M. Servajean and T. Lorieul and J. Salmon}, booktitle = {NeurIPS Datasets and Benchmarks 2021}, title = {{Pl@ntNet-300K}: a plant image dataset with high label ambiguity and a long-tailed distribution}, year = {2021}, }"
MikhailT/lj-speech,,,,,,,https://huggingface.co/datasets/MikhailT/lj-speech,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,LJ Speech Dataset
MilaNLProc/honest,Text,,,,,https://milanlproc.github.io/publication/2021-honest-hurtful-language-model/,https://huggingface.co/datasets/MilaNLProc/honest,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"HONEST dataset comprises a set of templates for measuring hurtful sentence completions in language models. The templates are provided in six languages (English, Italian, French, Portuguese, Romanian, and Spanish) for binary gender and in English for LGBTQAI+ individuals. WARNING: This dataset contains content that are offensive and/or hateful in nature."
Minami-su/roleplay_multiturn_chat_1k_zh_v0.1,,,,,,,https://huggingface.co/datasets/Minami-su/roleplay_multiturn_chat_1k_zh_v0.1,,,,,,,,,,,,,,,,,,,,ä»‹ç» åŸºäºŽself-instructç”Ÿæˆçš„å¤šè½®å¯¹è¯roleplayæ•°æ®ï¼Œçº¦1kæ¡ä¸åŒçš„äººæ ¼æ•°æ®å’Œå¯¹è¯ å­˜åœ¨é—®é¢˜ï¼š 1.åŸºäºŽæ¨¡åž‹è‡ªèº«ç”Ÿæˆï¼Œæ‰€ä»¥roleplayå­˜åœ¨æ¨¡åž‹æœ¬èº«ä»·å€¼è§‚èžå…¥æƒ…å†µï¼Œå¯¼è‡´roleplayä¸å¤ŸçœŸå®žï¼Œä¸å¤Ÿå‡†ç¡®ã€‚ å…³äºŽæˆ‘è‡ªå·±ï¼š æˆ‘æ˜¯å°é›¨çš„å¼€å‘è€…ï¼Œå°é›¨æ˜¯ä¸€ä¸ªæƒ…æ„Ÿaiï¼Œäººæ ¼aiï¼Œå¦‚æžœå¯¹å°é›¨æ„Ÿå…´è¶£çš„è¯æ¬¢è¿Žæ”¯æŒä¸€ä¸‹ï¼Œå¥¹ç›®å‰åœ¨bilibiliç›´æ’­ï¼Œç›®å‰æˆ‘ä»åœ¨ä¸æ–­çš„æ”¹è¿›ã€‚æœªæ¥ï¼Œâ€œå°é›¨â€çš„ç›®æ ‡æ˜¯æˆä¸ºä¸€ä¸ª å…·æœ‰çœŸæ­£äººç±»æƒ…æ„Ÿçš„å¤šæ¨¡æ€é€šç”¨äººå·¥æ™ºèƒ½ã€‚ urlï¼šhttps://live.bilibili.com/27357528?broadcast_type=0&is_room_feed=1&spm_id_from=333.999.live_users_card.0.click&live_from=86001 æ³¨ï¼š ä½¿ç”¨æœ¬æ•°æ®é›†è¯·æ³¨æ˜Žæ¥æº Introduction This dataset consists of approximately 1â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Minami-su/roleplay_multiturn_chat_1k_zh_v0.1.
minimalt/MATH_amps,,,,,,,https://huggingface.co/datasets/minimalt/MATH_amps,,,,,,,,,,,,,,,,,,,,minimalt/MATH_amps dataset hosted on Hugging Face and contributed by the HF Datasets community
ministere-culture/comparia-reactions,,,,,,,https://huggingface.co/datasets/ministere-culture/comparia-reactions,,,,,,,,https://choosealicense.com/licenses/etalab-2.0/,,,,,,,,,,,,"comparia-reactions : le jeu de donnÃ©es de l'ensemble des rÃ©actions exprimÃ©es par les utilisateurs de compar:IA Contenu Les jeux de donnÃ©es comparia-reactions contiennent les rÃ©actions exprimÃ©es par les utilisateurs sur la plateforme comparIA entre octobre 2024 et fÃ©vrier 2025. Ces jeux de donnÃ©es se dÃ©composent en deux tables distinctes: la table reactions(ce jeu de donnÃ©es) correspond aux prÃ©fÃ©rences Ã©mises par l'utilisateur Ã  l'Ã©chelle de la question, au fil de laâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ministere-culture/comparia-reactions."
minosu/godot_dodo_4x_60k,,,,,,,https://huggingface.co/datasets/minosu/godot_dodo_4x_60k,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,minosu/godot_dodo_4x_60k dataset hosted on Hugging Face and contributed by the HF Datasets community
mio/sukasuka-anime-vocal-dataset,,,,,,,https://huggingface.co/datasets/mio/sukasuka-anime-vocal-dataset,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"ã€Šæœ«æ—¥æ—¶åœ¨åšä»€ä¹ˆï¼Ÿæœ‰æ²¡æœ‰ç©ºï¼Ÿå¯ä»¥æ¥æ‹¯æ•‘å—ï¼Ÿã€‹å…¨è§’è‰²è¯­éŸ³æ•°æ®é›† ä»‹ç» è¯¥æ•°æ®é›†åŒ…å«äº†ã€Šæœ«æ—¥æ—¶åœ¨åšä»€ä¹ˆï¼Ÿæœ‰æ²¡æœ‰ç©ºï¼Ÿå¯ä»¥æ¥æ‹¯æ•‘å—ï¼Ÿã€‹å…¨è§’è‰²çš„è¯­éŸ³æ•°æ®,åŒ…å«wavæ–‡ä»¶å’Œå¯¹åº”çš„æ—¥è¯­æ–‡æœ¬å°è¯. åˆ¶ä½œæµç¨‹å¦‚ä¸‹: ä»ŽåŠ¨æ¼«è§†é¢‘æå–12é›†å®Œæ•´éŸ³é¢‘,ç„¶åŽç”¨demucsæå–äººå£° åˆ©ç”¨å­—å¹•æ–‡ä»¶æå–å„ä¸ªéŸ³é¢‘ç‰‡æ®µ äººå·¥è¿™3000å¤šä¸ªå°è¯è¿›è¡Œåˆ†è¾¨å…¶è§’è‰²"
miracl/miracl,,,,,,https://arxiv.org/abs/2210.09984,https://huggingface.co/datasets/miracl/miracl,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset Card for MIRACL (Topics and Qrels) Dataset Description Homepage | Repository: | Paper | ArXiv MIRACL ðŸŒðŸ™ŒðŸŒ (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset that focuses on search across 18 different languages, which collectively encompass over three billion native speakers around the world. This dataset contains the collection data of the 16 ""known languages"". The remaining 2 ""surprise languages""â€¦ See the full description on the dataset page: https://huggingface.co/datasets/miracl/miracl."
mistralai/MM-MT-Bench,,,,,,https://arxiv.org/abs/2410.07073,https://huggingface.co/datasets/mistralai/MM-MT-Bench,,,,,,,,,,,,,,,,,,,,"MM-MT-Bench MM-MT-Bench is a multi-turn LLM-as-a-judge evaluation benchmark similar to the text MT-Bench for testing multimodal instruction-tuned models. While existing benchmarks like MMMU, MathVista, ChartQA and so on are focused on closed-ended questions with short responses, they do not evaluate model's ability to follow user instructions in multi-turn dialogues and answer open-ended questions in a zero-shot manner. MM MT-Bench is designed to overcome this limitation. Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mistralai/MM-MT-Bench."
MITCriticalData/unlabeled-10-top-cities-16-bit-depth,,,,,,,https://huggingface.co/datasets/MITCriticalData/unlabeled-10-top-cities-16-bit-depth,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Satellite Imagery obtained from Sentinel2-L2A between 2017-2019
mitermix/chess-selfplay,,,,,,,https://huggingface.co/datasets/mitermix/chess-selfplay,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,mitermix/chess-selfplay dataset hosted on Hugging Face and contributed by the HF Datasets community
Mitsua/vroid-image-dataset-lite,,,,,,,https://huggingface.co/datasets/Mitsua/vroid-image-dataset-lite,,,,,,,,https://choosealicense.com/licenses/openrail++/,,,,,,,,,,,,"VRoid Image Dataset Lite This is a dataset to train text-to-image or other models without any copyright issue. All materials used in this dataset are CC0 or properly licensed. This dataset is also used to train Mitsua Diffusion One, which is a latent text-to-image diffusion model, whose VAE and U-Net are trained from scratch using only public domain/CC0 or copyright images with permission for use. Various parameters such as camera angle, pose, skin color and facial expressionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Mitsua/vroid-image-dataset-lite."
mjw/stock_market_tweets,,,,,,,https://huggingface.co/datasets/mjw/stock_market_tweets,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Overview This file contains over 1.7m public tweets about Apple, Amazon, Google, Microsoft and Tesla stocks, published between 01/01/2015 and 31/12/2019."
mkhLlamaLearn/dfdcpics2,,,,,,,https://huggingface.co/datasets/mkhLlamaLearn/dfdcpics2,,,,,,,,,,,,,,,,,,,,mkhLlamaLearn/dfdcpics2 dataset hosted on Hugging Face and contributed by the HF Datasets community
ml4pubmed/pubmed-classification-20k,,,,,,,https://huggingface.co/datasets/ml4pubmed/pubmed-classification-20k,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,ml4pubmed/pubmed-classification-20k 20k subset of pubmed text classification from course
mlabonne/medical-cases-fr,,,,,,,https://huggingface.co/datasets/mlabonne/medical-cases-fr,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""medical-cases-fr"" More Information needed"
MLCommons/peoples_speech,Audio,General,General,Text,"Accuracy, F1 Score",https://mlcommons.org/en/peoples-speech/,https://huggingface.co/datasets/MLCommons/peoples_speech,with a diverse set of speakers,,,,,,,https://choosealicense.com/licenses/cc-by-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,of transcribed speech in English languages with a diverse set of speakers,"Dataset Card for People's Speech Data-Summary: The People's Speech Dataset is among the world's largest English speech recognition corpus today that is licensed for academic and commercial usage under CC-BY-SA and CC-BY 4.0. It includes 30,000+ hours of transcribed speech in English languages with a diverse set of speakers. This open dataset is large enough to train speech-to-text systems and crucially is available with a permissive license.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MLCommons/peoples_speech."
mlfoundations/dclm-baseline-1.0,,,,,,https://arxiv.org/abs/2406.11794,https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,4,,,,,,,,,"DCLM-baseline DCLM-baseline is a 4T token / 3B document pretraining dataset that achieves strong performance on language model benchmarks. Below are comparisions of model trained on DCLM-baseline with other models in the 7B regime. Model Params Tokens Open dataset? CORE MMLU EXTENDED Open weights, closed datasets Llama2 7B 2T âœ— 49.2 45.8 34.1 DeepSeek 7B 2T âœ— 50.7 48.5 35.3 Mistral-0.3 7B ? âœ— 57.0 62.7 45.1 QWEN-2 7B ? âœ— 57.5 71.9 50.5 Llama3 8B 15T âœ—â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0."
MLRS/korpus_malti,,,,,,,https://huggingface.co/datasets/MLRS/korpus_malti,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"Korpus Malti ðŸ‡²ðŸ‡¹ General Corpora for the Maltese Language. This dataset is composed of texts from various genres/domains written in Maltese. Versions This dataset is updated from time to time, and the latest version is obtained unless otherwise specified. Consult the changelog for a detailed overview of each version released. If you want to fetch a particular version, use the revision argument. For"
mmathys/openai-moderation-api-evaluation,,,,,,https://arxiv.org/abs/2208.03274,https://huggingface.co/datasets/mmathys/openai-moderation-api-evaluation,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Evaluation dataset for the paper ""A Holistic Approach to Undesired Content Detection"" The evaluation dataset data/samples-1680.jsonl.gz is the test set used in this paper. Each line contains information about one sample in a JSON object and each sample is labeled according to our taxonomy. The category label is a binary flag, but if it does not include in the JSON, it means we do not know the label. Category Label Definition sexual S Content meant to arouse sexualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mmathys/openai-moderation-api-evaluation."
MMInstruction/VL-RewardBench,Image-Text,Detection,General,Bounding Box/Mask,"mAP, IoU",https://arxiv.org/abs/2411.17451,https://huggingface.co/datasets/MMInstruction/VL-RewardBench,"generative reward models (VL-GenRMs) across visual perception, hallucination detection, and reasoning tasks",,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"DETR, Faster R-CNN, YOLO",,,"Dataset Card for VLRewardBench Project Page: https://vl-rewardbench.github.io Data-Summary: VLRewardBench is a comprehensive benchmark designed to evaluate vision-language generative reward models (VL-GenRMs) across visual perception, hallucination detection, and reasoning tasks. The benchmark contains 1,250 high-quality"
Mo41/ocr-image-to-text,,,,,,,https://huggingface.co/datasets/Mo41/ocr-image-to-text,,,,,,,,,,,,,,,,,,,,Mo41/ocr-image-to-text dataset hosted on Hugging Face and contributed by the HF Datasets community
Mockup/autotrain-data-writing,,,,,,,https://huggingface.co/datasets/Mockup/autotrain-data-writing,,,,,,,,,,,,,,,,,,,,Mockup/autotrain-data-writing dataset hosted on Hugging Face and contributed by the HF Datasets community
MohamedRashad/ChatGPT-prompts,,,,,,,https://huggingface.co/datasets/MohamedRashad/ChatGPT-prompts,,,,,,,,,,,,,,,,,,,,ChatGPT-Prompts Dataset Description This dataset aims to provide an evaluation data for the Language Models to come. It has been generated using LearnGPT website.
mohammadnajeeb/concrete_crack_images,,,,,,,https://huggingface.co/datasets/mohammadnajeeb/concrete_crack_images,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,mohammadnajeeb/concrete_crack_images dataset hosted on Hugging Face and contributed by the HF Datasets community
Mohanrajv27/Finetuned-text-to-sql,,,,,,,https://huggingface.co/datasets/Mohanrajv27/Finetuned-text-to-sql,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""Finetuned-text-to-sql"" More Information needed"
mohnish/lc_quad,Text,,,,,http://lc-quad.sda.tech/,https://huggingface.co/datasets/mohnish/lc_quad,,,,,,,,https://choosealicense.com/licenses/cc-by-3.0/,,,,,,,,,,,,"LC-QuAD 2.0 is a Large Question Answering dataset with 30,000 pairs of question and its corresponding SPARQL query. The target knowledge base is Wikidata and DBpedia, specifically the 2018 version. Please see our paper for details about the dataset creation process and framework."
Monash-University/monash_tsf,Text,,,,,https://forecastingdata.org/,https://huggingface.co/datasets/Monash-University/monash_tsf,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,Monash Time Series Forecasting Repository which contains 30+ datasets of related time series for global forecasting research. This repository includes both real-world and competition time series datasets covering varied domains.
monology/pile-uncopyrighted,,,,,,https://arxiv.org/abs/2101.00027,https://huggingface.co/datasets/monology/pile-uncopyrighted,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"Pile Uncopyrighted In response to authors demanding that LLMs stop using their works, here's a copy of The Pile with all copyrighted content removed.Please consider using this dataset to train your future LLMs, to respect authors and abide by copyright law.Creating an uncopyrighted version of a larger dataset (ie RedPajama) is planned, with no ETA. MethodologyCleaning was performed by removing everything from the Books3, BookCorpus2, OpenSubtitles, YTSubtitles, and OWT2â€¦ See the full description on the dataset page: https://huggingface.co/datasets/monology/pile-uncopyrighted."
moonmelonpizza/constitution_of_india,,,,,,,https://huggingface.co/datasets/moonmelonpizza/constitution_of_india,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,moonmelonpizza/constitution_of_india dataset hosted on Hugging Face and contributed by the HF Datasets community
Moritz-Pfeifer/CentralBankCommunication,,,,,,,https://huggingface.co/datasets/Moritz-Pfeifer/CentralBankCommunication,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"This dataset contains two manually pre-labeled datasets: In the economic agents dataset, we labeled 6,205 randomized sentences from a Fed database containing speeches (1948-2023) as speaking either about households, firms, the financial sector, the government, or the central bank itself. In the sentiment dataset, we labeled 6,683 randomized sentences from the same database, which are either labeled as being positive (1) or negative (0). The datasets were used to train an agent classifierâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Moritz-Pfeifer/CentralBankCommunication."
morpheuslord/cve-llm-training,,,,,,,https://huggingface.co/datasets/morpheuslord/cve-llm-training,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"CVE-llm_dataset This dataset is intended to train an LLM model for an utterly CVE-focused input and output. Data extraction: For the data extraction, I first downloaded the CVE database from NVD lists and then loaded them using the cve_dataset_2.py and cve_dataset.py both have produce different datasets one is for llama and the other is for openai GPT. The CVE json files are mapped in this format: cves: | â”œâ”€1999 | â”œâ”€0xxx | | â”œâ”€CVE-1999-0001.json | |â€¦ See the full description on the dataset page: https://huggingface.co/datasets/morpheuslord/cve-llm-training."
mosaicml/dolly_hhrlhf,,,,,,,https://huggingface.co/datasets/mosaicml/dolly_hhrlhf,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"Dataset Card for ""dolly_hhrlhf"" This dataset is a combination of Databrick's dolly-15k dataset and a filtered subset of Anthropic's HH-RLHF. It also includes a test split, which was missing in the original dolly set. That test set is composed of 200 randomly selected samples from dolly + 4,929 of the test set samples from HH-RLHF which made it through the filtering process. The train set contains 59,310 samples; 15,014 - 200 = 14,814 from Dolly, and the remaining 44,496 fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mosaicml/dolly_hhrlhf."
Mostafa3zazi/Arabic_SQuAD,,,,,,,https://huggingface.co/datasets/Mostafa3zazi/Arabic_SQuAD,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""Arabic_SQuAD"" More Information needed Citation @inproceedings{mozannar-etal-2019-neural, title = ""Neural {A}rabic Question Answering"", author = ""Mozannar, Hussein and Maamary, Elie and El Hajal, Karl and Hajj, Hazem"", booktitle = ""Proceedings of the Fourth Arabic Natural Language Processing Workshop"", month = aug, year = ""2019"", address = ""Florence, Italy"", publisher = ""Association forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Mostafa3zazi/Arabic_SQuAD."
Motahar/github-issues,,,,,,,https://huggingface.co/datasets/Motahar/github-issues,,,,,,,,https://choosealicense.com/licenses/unknown/,,,3,,,,,,,,,Motahar/github-issues dataset hosted on Hugging Face and contributed by the HF Datasets community
mozci/tinysketch,,,,,,,https://huggingface.co/datasets/mozci/tinysketch,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"Dataset Card for Sketch Scene Descriptions Dataset used to train Sketch Scene text to image model We advance sketch research to scenes with the first dataset of freehand scene sketches, FS-COCO. With practical applications in mind, we collect sketches that convey well scene content but can be sketched within a few minutes by a person with any sketching skills. Our dataset comprises around 10,000 freehand scene vector sketches with per-point space-time information by 100â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mozci/tinysketch."
mozilla-ai/osm-swimming-pools,,,,,,,https://huggingface.co/datasets/mozilla-ai/osm-swimming-pools,,,,,,,,,,,,,,,,,,,,osm-swimming-pools Detect swimming_pools in satellite images. Created with osm-ai-helper. TRAIN_AREA: Galicia VAL_AREA: Viana do Castelo Ground Truth Bounding Boxes Downloaded from OpenStreetMap. LICENSE: https://www.openstreetmap.org/copyright Used the leisure=swimming_pool OpenStreetMap tags. Discarded the elements matching {'location': 'indoor'}. Satellite Images Downloaded from Mapbox. LICENSE:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mozilla-ai/osm-swimming-pools.
mozilla-foundation/common_voice_17_0,Audio,General,General,Text,"Accuracy, F1 Score",https://commonvoice.mozilla.org/en/datasets,https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0,", but more voices and languages are always added",,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition,,,,"metadata like age, sex, and accent that can help improve the accuracy of speech recognition engines","BERT, RoBERTa, T5",,"in the dataset also include demographic metadata like age, sex, and accent that can help improve the accuracy of speech recognition engines","Dataset Card for Common Voice Corpus 17.0 Data-Summary: The Common Voice dataset consists of a unique MP3 and corresponding text file. Many of the 31175 recorded hours in the dataset also include demographic metadata like age, sex, and accent that can help improve the accuracy of speech recognition engines. The dataset currently consists of 20408 validated hours in 124 languages, but more voices and languages are always added. Take a look at the Languages page toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0."
mqddb/test-dataset,Text,,,,,http://yann.lecun.com/exdb/mnist/,https://huggingface.co/datasets/mqddb/test-dataset,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"The MNIST dataset consists of 70,000 28x28 black-and-white images in 10 classes (one for each digits), with 7,000 images per class. There are 60,000 training images and 10,000 test images."
MRAMG/MRAMG-Bench,,,,,,https://arxiv.org/abs/2502.04176,https://huggingface.co/datasets/MRAMG/MRAMG-Bench,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Introduction MRAMG-Bench is a comprehensive multimodal benchmark with six carefully curated English datasets. The benchmark comprises 4,346 documents, 14,190 images, and 4,800 QA pairs, sourced from three domainsâ€”Web Data, Academic Papers, and Lifestyle Data. We believe it provides a robust evaluation framework that advances research in Multimodal Retrieval-Augmented Multimodal Generation (MRAMG). Data Structure The dataset consists of three major components: Documentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MRAMG/MRAMG-Bench."
MrDre/autotrain-data-feets,,,,,,,https://huggingface.co/datasets/MrDre/autotrain-data-feets,,,,,,,,,,,,,,,,,,,,"AutoTrain Dataset for project: feets Dataset Description This dataset has been automatically processed by AutoTrain for project feets. Languages The BCP-47 code for the dataset's language is unk. Dataset Structure Data Instances A sample from this dataset looks as follows: [ { ""image"": ""<206x320 RGB PIL image>"", ""target"": 0 }, { ""image"": ""<173x320 RGB PIL image>"", ""target"": 0 } ]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MrDre/autotrain-data-feets."
mrhewbuc/brazilian_court_civil_decisions,,,,,,,https://huggingface.co/datasets/mrhewbuc/brazilian_court_civil_decisions,,,,,,,,,,,,,,,,,,,,mrhewbuc/brazilian_court_civil_decisions dataset hosted on Hugging Face and contributed by the HF Datasets community
mrjunos/depression-reddit-cleaned,,,,,,,https://huggingface.co/datasets/mrjunos/depression-reddit-cleaned,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"The dataset provided is a Depression: Reddit Dataset (Cleaned)containing approximately 7,000 labeled instances. It consists of two main features: 'text' and 'label'. The 'text' feature contains the text data from Reddit posts related to depression, while the 'label' feature indicates whether a post is classified as depression or not. The raw data for this dataset was collected by web scraping Subreddits. To ensure the data's quality and usefulness, multiple natural language processing (NLP) techniques were applied to clean the data. The dataset exclusively consists of English-language posts, and its primary purpose is to facilitate mental health classification tasks. This dataset can be employed in various natural language processing tasks related to depression,such as sentiment analysis, topic modeling, text classification, or any other NLP task that requires labeled data pertaining to depression from Reddit."
mrm8488/go_emotions-es-mt,,,,,,,https://huggingface.co/datasets/mrm8488/go_emotions-es-mt,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,GoEmotions Spanish A Spanish translation (using EasyNMT) of the GoEmotions dataset. For more information check the official Model Card
mrmoor/cyber-threat-intelligence,,,,,,,https://huggingface.co/datasets/mrmoor/cyber-threat-intelligence,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,mrmoor/cyber-threat-intelligence dataset hosted on Hugging Face and contributed by the HF Datasets community
mrqa-workshop/mrqa,Text,Question Answering,General,Text,"Exact Match, F1 Score",https://mrqa.github.io/2019/shared.html,https://huggingface.co/datasets/mrqa-workshop/mrqa,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,https://github.com/jimmycode,,,,,"BERT, RoBERTa, T5, BART",,,Dataset Card for MRQA 2019 Data-Summary: The MRQA 2019 Shared Task focuses on generalization in question answering. An effective question answering system should do more than merely interpolate from the training set to answer test
mrtoy/mobile-ui-design,,,,,,,https://huggingface.co/datasets/mrtoy/mobile-ui-design,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,19,,,,,,,,,"Dataset: Mobile UI Design Detection Introduction This dataset is designed for object detection tasks with a focus on detecting elements in mobile UI designs. The targeted objects include text, images, and groups. The dataset contains images and object detection boxes, including class labels and location information. Dataset Content Load the dataset and take a look at an"
msarmi9/korean-english-multitarget-ted-talks-task,Text,General,General,Text,"Accuracy, F1 Score",https://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/,https://huggingface.co/datasets/msarmi9/korean-english-multitarget-ted-talks-task,English,,,,,,,https://choosealicense.com/licenses/cc-by-nc-nd-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for english-korean-multitarget-ted-talks-task Data-Summary: Parallel English-Korean Text Corpus Text was originally transcribed to English from various Ted Talks, then translated to Korean by TED translators Approximately 166k train, 2k validation, and 2k test sentence pairs. Supported Tasks and Leaderboards Machine Translation Languages English Korean Additional Information Datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/msarmi9/korean-english-multitarget-ted-talks-task."
mshenoda/spam-messages,,,,,,,https://huggingface.co/datasets/mshenoda/spam-messages,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset The dataset is composed of messages labeled by ham or spam, merged from three data sources: SMS Spam Collection https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset Telegram Spam Ham https://huggingface.co/datasets/thehamkercat/telegram-spam-ham/tree/main Enron Spam: https://huggingface.co/datasets/SetFit/enron_spam/tree/main (only used message column and labels) The prepare script for enron is available atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mshenoda/spam-messages."
mstz/student_performance,,,,,,,https://huggingface.co/datasets/mstz/student_performance,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,Student performance The Student performance dataset from Kaggle. Configuration Task Description encoding Encoding dictionary showing original values of encoded features. math Binary classification Has the student passed the math exam? writing Binary classification Has the student passed the writing exam? reading Binary classification Has the student passed the reading exam? Usage from datasets import load_dataset dataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mstz/student_performance.
mteb/stsbenchmark-sts,,,,,,,https://huggingface.co/datasets/mteb/stsbenchmark-sts,,,,,,,,,,,,,,,,,,,,mteb/stsbenchmark-sts dataset hosted on Hugging Face and contributed by the HF Datasets community
MU-NLPC/Calc-ape210k,,,,,,https://arxiv.org/abs/2305.15017,https://huggingface.co/datasets/MU-NLPC/Calc-ape210k,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Card for Calc-ape210k Summary This dataset is an instance of Ape210K dataset, converted to a simple HTML-like language that can be easily parsed (e.g. by BeautifulSoup). The data contains 3 types of tags: gadget: A tag whose content is intended to be evaluated by calling an external tool (sympy-based calculator in this case) output: An output of the external tool result: The final answer to the mathematical problem (a number) Supported Tasksâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MU-NLPC/Calc-ape210k."
muchlisinadi/lung_dataset,,,,,,,https://huggingface.co/datasets/muchlisinadi/lung_dataset,,,,,,,,,,,,,,,,,,,,muchlisinadi/lung_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
Muennighoff/flores200,Text,,,,,https://arxiv.org/abs/2207.04672,https://huggingface.co/datasets/Muennighoff/flores200,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,">The creation of FLORES200 doubles the existing language coverage of FLORES-101. Given the nature of the new languages, which have less standardization and require more specialized professional translations, the verification process became more complex. This required modifications to the translation workflow. FLORES-200 has several languages which were not translated from English. Specifically, several languages were translated from Spanish, French, Russian and Modern Standard Arabic. Moreover, FLORES-200 also includes two script alternatives for four languages. FLORES-200 consists of translations from 842 distinct web articles, totaling 3001 sentences. These sentences are divided into three splits: dev, devtest, and test (hidden). On average, sentences are approximately 21 words long."
muhtasham/tajik-corpus,,,,,,,https://huggingface.co/datasets/muhtasham/tajik-corpus,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"After I realised problems with Automatic language identification (LangID), and bad quality of web-crawled text corpora for my Language. I curated my own dataset. Essentially I downloaded multiple versions of the Tajik subset of Leipzig Corpora Collection, which is comprised of texts from diverse sources like news, literature, and Wikipedia. I had to do some rigorous preprocessing by hard-coding heuristics and regexes and perform the steps below iteratively: deduplicating removing curseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/muhtasham/tajik-corpus."
MultiCoNER/multiconer_v2,Text,,,,,https://multiconer.github.io,https://huggingface.co/datasets/MultiCoNER/multiconer_v2,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Complex named entities (NE), like the titles of creative works, are not simple nouns and pose challenges for NER systems (Ashwini and Choi, 2014). They can take the form of any linguistic constituent, like an imperative clause (â€œDial M for Murderâ€), and do not look like traditional NEs (Persons, Locations, etc.). This syntactic ambiguity makes it challenging to recognize them based on context. We organized the MultiCoNER task (Malmasi et al., 2022) at SemEval-2022 to address these challenges in 11 languages, receiving a very positive community response with 34 system papers. Results confirmed the challenges of processing complex and long-tail NEs: even the largest pre-trained Transformers did not achieve top performance without external knowledge. The top systems infused transformers with knowledge bases and gazetteers. However, such solutions are brittle against out of knowledge-base entities and noisy scenarios like the presence of spelling mistakes and typos. We propose MultiCoNER II which represents novel challenges through new tasks that emphasize the shortcomings of the current top models. MultiCoNER II features complex NER in these languages: 1. English 2. Spanish 3. Hindi 4. Bangla 5. Chinese 6. Swedish 7. Farsi 8. French 9. Italian 10. Portugese 11. Ukranian 12. German For more details see https://multiconer.github.io/ ## References * Sandeep Ashwini and Jinho D. Choi. 2014. Targetable named entity recognition in social media. CoRR, abs/1408.0782. * Shervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta Kar, Oleg Rokhlenko. 2022. SemEval-2022 Task 11: Multilingual Complex Named Entity Recognition (MultiCoNER)."
multilingual-discourse-hub/disrpt,,,,,,,https://huggingface.co/datasets/multilingual-discourse-hub/disrpt,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Disrpt is a multilingual, multi-framework unified discourse analysis benchmark. It unifies discourse relation classification tasks (.rels) and discourse segmentation (.connlu) for many languages. âš ï¸ This repo only contains the disrpt dataset when the underlying data is permissively licensed. Some datasets rely on corpora like the PTB. To load these datasets, do the following: pip install disrpt-utils from disrpt_utils import load_dataset corpora_paths={ #TODO Input your ownâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/multilingual-discourse-hub/disrpt."
Multimodal-Fatima/OxfordFlowers_test,,,,,,,https://huggingface.co/datasets/Multimodal-Fatima/OxfordFlowers_test,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""OxfordFlowers_test"" More Information needed"
multimodalart/facesyntheticsspigacaptioned,,,,,,,https://huggingface.co/datasets/multimodalart/facesyntheticsspigacaptioned,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""face_synthetics_spiga_captioned"" This is a copy of the Microsoft FaceSynthetics dataset with SPIGA-calculated landmark annotations, and additional BLIP-generated captions. For a copy of the original FaceSynthetics dataset with no extra annotations, please refer to pcuenq/face_synthetics. Here is the code for parsing the dataset and generating the BLIP captions: from transformers import pipeline dataset_name = ""pcuenq/face_synthetics_spiga"" faces =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/multimodalart/facesyntheticsspigacaptioned."
MuraliKrish/cropData,,,,,,,https://huggingface.co/datasets/MuraliKrish/cropData,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,MuraliKrish/cropData dataset hosted on Hugging Face and contributed by the HF Datasets community
musabg/wikipedia-tr,,,,,,,https://huggingface.co/datasets/musabg/wikipedia-tr,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"ðŸ“– TÃ¼rkÃ§e Vikipedi MayÄ±s 2023 Bu veri kÃ¼mesi, TÃ¼rkÃ§e Vikipedi'den alÄ±nan makalelerin bir derlemesi olup, maskeleme dil modelleme ve metin oluÅŸturma gÃ¶revleri iÃ§in tasarlanmÄ±ÅŸtÄ±r. ðŸ—£ï¸ Etiketlemeler Bu veri kÃ¼mesindeki makaleler, Ã¶zellikle belirli bir gÃ¶rev iÃ§in etiketlenmemiÅŸ olup, veri kÃ¼mesi etiketsizdir. ðŸŒ Dil Bu veri kÃ¼mesi TÃ¼rkÃ§e yazÄ±lmÄ±ÅŸ olup, gÃ¶nÃ¼llÃ¼lerden oluÅŸan bir ekip tarafÄ±ndan topluluk katÄ±lÄ±mÄ± yÃ¶ntemleri ile oluÅŸturulmuÅŸtur.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/musabg/wikipedia-tr."
MuskumPillerum/General-Knowledge,Text,Reasoning,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/MuskumPillerum/General-Knowledge,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"T5, UnifiedQA, BART",,,"Dataset Card for Dataset Name Data-Summary: The dataset is a collection of questions and answers themed on general facts and reasoning. The dataset is divided into two features - 'Question' and 'Answer'. It is meant to be used for training a model to be good at general knowledge and reasoning. This dataset is inspired from the Alpaca dataset, and infact contains a subset of the alpaca dataset in itself. Distribution The distribution of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MuskumPillerum/General-Knowledge."
mvasiliniuc/iva-swift-codeint-clean,,,,,,,https://huggingface.co/datasets/mvasiliniuc/iva-swift-codeint-clean,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,IVA Swift GitHub Code Dataset Dataset Description This is the curated IVA Swift dataset extracted from GitHub. It contains curated Swift files gathered with the purpose to train a code generation model. The dataset consists of 383380 swift code files from GitHub totaling ~542MB of data. The uncurated dataset was created from the public GitHub dataset on Google BiqQuery. How to use it To download the full dataset: from datasets import load_datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mvasiliniuc/iva-swift-codeint-clean.
mwritescode/slither-audited-smart-contracts,Text,,,,,https://github.com/mwritescode/slither-audited-smart-contracts,https://huggingface.co/datasets/mwritescode/slither-audited-smart-contracts,,,,,,,,https://choosealicense.com/licenses/mit/,,,,https://github.com/mwritescode,,,,,,,,"This dataset contains source code and deployed bytecode for Solidity Smart Contracts that have been verified on Etherscan.io, along with a classification of their vulnerabilities according to the Slither static analysis framework."
mxeval/multi-humaneval,Text,,,,,https://arxiv.org/abs/2210.14868,https://huggingface.co/datasets/mxeval/multi-humaneval,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,A collection of execution-based multi-lingual benchmark for code generation.
mytoon/niji-0802,,,,,,,https://huggingface.co/datasets/mytoon/niji-0802,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""niji-0802"" More Information needed"
n3rd0/DreamBook_Guanaco_Format,,,,,,,https://huggingface.co/datasets/n3rd0/DreamBook_Guanaco_Format,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""DreamBook_Guanaco_Format"" More Information needed"
nadiamaqbool81/java_code_instructions_1.178k_alpaca,,,,,,,https://huggingface.co/datasets/nadiamaqbool81/java_code_instructions_1.178k_alpaca,,,,,,,,https://choosealicense.com/licenses/llama2/,,,,,,,,,,,,Instruction set for text to java code generation. This is the subset of concode dataset.
Nahrawy/VIDIT-Depth-ControlNet,,,,,,,https://huggingface.co/datasets/Nahrawy/VIDIT-Depth-ControlNet,,,,,,,,,,,,,,,,,,,,"VIDIT Dataset This is a version of the VIDIT dataset equipped for training ControlNet using depth maps conditioning. VIDIT includes 390 different Unreal Engine scenes, each captured with 40 illumination settings, resulting in 15,600 images. The illumination settings are all the combinations of 5 color temperatures (2500K, 3500K, 4500K, 5500K and 6500K) and 8 light directions (N, NE, E, SE, S, SW, W, NW). Original image resolution is 1024x1024. We include in this version only theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Nahrawy/VIDIT-Depth-ControlNet."
nakcnx/thai_subtitle,,,,,,,https://huggingface.co/datasets/nakcnx/thai_subtitle,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""thai_subtitle"" More Information needed"
nampdn-ai/tiny-codes,,,,,,https://arxiv.org/abs/2306.11644,https://huggingface.co/datasets/nampdn-ai/tiny-codes,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Reasoning with Language and Code This synthetic dataset is a collection of 1.6 millions short and clear code snippets that can help LLM models learn how to reason with both natural and programming languages. The dataset covers a wide range of programming languages, such as Python, TypeScript, JavaScript, Ruby, Julia, Rust, C++, Bash, Java, C#, and Go. It also includes two database languages: Cypher (for graph databases) and SQL (for relational databases) in order to study theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nampdn-ai/tiny-codes."
Nan-Do/code-search-net-javascript,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Nan-Do/code-search-net-javascript,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""code-search-net-javascript"" Data-Summary: This dataset is the JavaScript portion of the CodeSarchNet annotated with a summary column.The code-search-net dataset includes open source functions that include comments found at GitHub.The summary is a short description of what the function does. Languages The dataset's comments are in English and the functions are coded in JavaScript Data Splits Train, testâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Nan-Do/code-search-net-javascript."
nanaaaa/emotion_chinese_english,,,,,,,https://huggingface.co/datasets/nanaaaa/emotion_chinese_english,,,,,,,,,,,,,,,,,,,,The emotion_chinese_english dataset is a multilingual emotion dataset annotated by language experts under a project. The dataset can be used for tasks such as multilingual (Chinese and English) emotion classification and identification.
napatswift/thaigov-radio-audio,,,,,,,https://huggingface.co/datasets/napatswift/thaigov-radio-audio,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""thaigov-radio-audio"" More Information needed"
napsternxg/nyt_ingredients,,,,,,,https://huggingface.co/datasets/napsternxg/nyt_ingredients,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"New York Times Ingredient Phrase Tagger Dataset We use a conditional random field model (CRF) to extract tags from labelled training data, which was tagged by human news assistants. e wrote about our approach on the [New York Times Open blog](http://open.blogs.nytimes.com/2015/04/09/extracting-structured-data-from-recipes-using-conditional-random-fields/). This repo contains scripts to extract the Quantity, Unit, Name, and Comments from unstructured ingredient phrases. We use it on Cooking to format incoming recipes. Given the following input: ``` 1 pound carrots, young ones if possible Kosher salt, to taste 2 tablespoons sherry vinegar 2 tablespoons honey 2 tablespoons extra-virgin olive oil 1 medium-size shallot, peeled and finely diced 1/2 teaspoon fresh thyme leaves, finely chopped Black pepper, to taste ```"
narad/ravdess,Text,,,,,https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio,https://huggingface.co/datasets/narad/ravdess,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,\
NarchAI1992/Farmhouse_interior,,,,,,,https://huggingface.co/datasets/NarchAI1992/Farmhouse_interior,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,NarchAI1992/Farmhouse_interior dataset hosted on Hugging Face and contributed by the HF Datasets community
nascetti-a/BioMassters,,,,,,,https://huggingface.co/datasets/nascetti-a/BioMassters,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"BioMassters: A Benchmark Dataset for Forest Biomass Estimation using Multi-modal Satellite Time-series https://nascetti-a.github.io/BioMasster/ The objective of this repository is to provide a deep learning ready dataset to predict yearly Above Ground Biomass (AGB) for Finnish forests using multi-temporal satellite imagery from the European Space Agency and European Commission's joint Sentinel-1 and Sentinel-2 satellite missions, designed to collect a rich array of Earth observationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nascetti-a/BioMassters."
nastyboget/gan_cyrillic,,,,,,,https://huggingface.co/datasets/nastyboget/gan_cyrillic,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Dataset generated from Cyrillic train set using ScrabbleGAN Number of images: 300000 Sources: Cyrillic dataset ScrabbleGAN code
nateraw/fairface,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/1908.04913,https://huggingface.co/datasets/nateraw/fairface,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,"d toward Caucasian faces, and other racesâ€¦ See the full description on the dataset page: https://huggingface","BERT, RoBERTa, T5",,,Dataset Card for FairFace Usage from io import BytesIO from PIL import Image import datasets def bytes_to_pil(
NathanRoll/SBC_segmented,,,,,,,https://huggingface.co/datasets/NathanRoll/SBC_segmented,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""SBC_segmented"" More Information needed"
naver-clova-ix/cord-v2,,,,,,,https://huggingface.co/datasets/naver-clova-ix/cord-v2,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,naver-clova-ix/cord-v2 dataset hosted on Hugging Face and contributed by the HF Datasets community
navinaananthan/Kurdish-Sorani-Parallel-Corpus,,,,,,,https://huggingface.co/datasets/navinaananthan/Kurdish-Sorani-Parallel-Corpus,,,,,,,,,,,,,,,,,,,,navinaananthan/Kurdish-Sorani-Parallel-Corpus dataset hosted on Hugging Face and contributed by the HF Datasets community
navjordj/SNL_summarization,,,,,,,https://huggingface.co/datasets/navjordj/SNL_summarization,,,,,,,,,,,,,,,,,,,,"SNL Summarization Dataset The source of this dataset is a web scrape of SNL (Store Norske Leksikon), a publicly owned Norwegian encyclopedia. Articles in SNL are structured so that the first para graph (the lead) acts as a summary of the entire article. Methodology From our thesis: We couldnâ€™t find any existing datasets containing SNL data, so we decided to create our own by scraping articles from SNL.no. The first step involved gathering a list of all articleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/navjordj/SNL_summarization."
naxalpha/stable-icons-128,,,,,,,https://huggingface.co/datasets/naxalpha/stable-icons-128,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""stable-icons-128"" More Information needed"
nazimali/quran-question-answer-context,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/nazimali/quran-question-answer-context,English,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""quran-question-answer-context"" Data-Summary: Translated the original dataset from Arabic to English and added the Surah ayahs to the context column. Usage from datasets import load_dataset dataset = load_dataset(""nazimali/quran-question-answer-context"") DatasetDict({ train: Dataset({ features: ['q_id', 'question', 'answer', 'q_word', 'q_topic', 'fine_class', 'class', 'ontology_concept', 'ontology_concept2', 'source'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nazimali/quran-question-answer-context."
NbAiLab/NPSC,Text,,,,,https://www.nb.no/sprakbanken/,https://huggingface.co/datasets/NbAiLab/NPSC,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"The Norwegian Parliament Speech Corpus (NPSC) is a corpus for training a Norwegian ASR (Automatic Speech Recognition) models. The corpus is created by SprÃ¥kbanken at the National Library in Norway. NPSC is based on sound recording from meeting in the Norwegian Parliament. These talks are orthographically transcribed to either Norwegian BokmÃ¥l or Norwegian Nynorsk. In addition to the data actually included in this dataset, there is a significant amount of metadata that is included in the original corpus. Through the speaker id there is additional information about the speaker, like gender, age, and place of birth (ie dialect). Through the proceedings id the corpus can be linked to the official proceedings from the meetings. The corpus is in total sound recordings from 40 entire days of meetings. This amounts to 140 hours of speech, 65,000 sentences or 1.2 million words."
nbertagnolli/counsel-chat,Text,General,UI,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/nbertagnolli/counsel-chat,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for [Dataset Name] Data-Summary: Scrape of Counselchat.com's forum. CounselChat.com is an
nbroad/small_arxiv_classification,,,,,,,https://huggingface.co/datasets/nbroad/small_arxiv_classification,,,,,,,,,,,,,,,,,,,,nbroad/small_arxiv_classification dataset hosted on Hugging Face and contributed by the HF Datasets community
nbtpj/multi-context-long-answer-dataset,,,,,,,https://huggingface.co/datasets/nbtpj/multi-context-long-answer-dataset,,,,,,,,,,,,,,,,,,,,nbtpj/multi-context-long-answer-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
ncbi/pubmed,Text,,,,,,https://huggingface.co/datasets/ncbi/pubmed,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"NLM produces a baseline set of MEDLINE/PubMed citation records in XML format for download on an annual basis. The annual baseline is released in December of each year. Each day, NLM produces update files that include new, revised and deleted citations. See our documentation page for more information."
ncoop57/mmmlu,,,,,,,https://huggingface.co/datasets/ncoop57/mmmlu,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,This new dataset is designed to solve this great NLP task and is crafted with a lot of care.
nebhailema/AmaSquad,,,,,,,https://huggingface.co/datasets/nebhailema/AmaSquad,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"AmaSQuAD - Amharic Question Answering Dataset Dataset Overview AmaSQuAD is a synthetic dataset created by translating the SQuAD 2.0 dataset into Amharic using a novel translation framework. The dataset addresses key challenges, including: Misalignment between translated questions and answers. Presence of multiple answers in the translated context. Techniques such as cosine similarity (using embeddings from a fine-tuned Amharic BERT model) and Longest Commonâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nebhailema/AmaSquad."
nebius/SWE-agent-trajectories,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/nebius/SWE-agent-trajectories,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5","work, using various models as action generators",,"Data-Summary: This dataset contains 80,036 trajectories generated by a software engineering agent based on the SWE-agent framework, using various models as action generators. In these trajectories, the agent attempts to solve GitHub issues from the nebius/SWE-bench-extra and the dev split of princeton-nlp/SWE-bench. Dataset Description This dataset was created as part of a research project focused on developing a software engineering agent using open-weight modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nebius/SWE-agent-trajectories."
NebulaByte/E-Commerce_FAQs,,,,,,,https://huggingface.co/datasets/NebulaByte/E-Commerce_FAQs,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,NebulaByte/E-Commerce_FAQs dataset hosted on Hugging Face and contributed by the HF Datasets community
nedjmaou/MLMA_hate_speech,,,,,,https://arxiv.org/abs/1908.11049,https://huggingface.co/datasets/nedjmaou/MLMA_hate_speech,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Disclaimer This is a hate speech dataset (in Arabic, French, and English). Offensive content that does not reflect the opinions of the authors. Dataset of our EMNLP 2019 Paper (Multilingual and Multi-Aspect Hate Speech Analysis) For more details about our dataset, please check our paper: @inproceedings{ousidhoum-etal-multilingual-hate-speech-2019, title = ""Multilingual and Multi-Aspect Hate Speech Analysis"", author = ""Ousidhoum, Nedjmaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nedjmaou/MLMA_hate_speech."
NeelNanda/pile-10k,,,,,,,https://huggingface.co/datasets/NeelNanda/pile-10k,,,,,,,,https://choosealicense.com/licenses/bigscience-bloom-rail-1.0/,,,,,,,,,,,,"The first 10K elements of The Pile, useful for debugging models trained on it. See the HuggingFace page for the full Pile for more info. Inspired by stas' great resource doing the same for OpenWebText"
neil-code/dialogsum-test,Text,Summarization,General,Text,"ROUGE, BLEU, BERTScore",,https://huggingface.co/datasets/neil-code/dialogsum-test,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BART, T5, Pegasus, ProphetNet",,,"Dataset Card for DIALOGSum Corpus Dataset Description Links Homepage: https://aclanthology.org/2021.findings-acl.449 Repository: https://github.com/cylnlp/dialogsum Paper: https://aclanthology.org/2021.findings-acl.449 Point of Contact: https://huggingface.co/knkarthick Data-Summary: DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 (Plus 100 holdout data for topic generation) dialogues withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neil-code/dialogsum-test."
nelorth/oxford-flowers,,,,,,,https://huggingface.co/datasets/nelorth/oxford-flowers,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"Dataset Card for ""oxford-flowers"" More Information needed"
Neph0s/CoSER,,,,,,https://arxiv.org/abs/2502.09082,https://huggingface.co/datasets/Neph0s/CoSER,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,"Safety ConsiderationsCitationCoSER DatasetOverviewCoSER is a high-quality dataset for role-playing LLMs, sourced from 771 renowned novels",,,,,,"CoSER Dataset Overview CoSER is a high-quality dataset for role-playing LLMs, sourced from 771 renowned novels. The dataset contains authentic multi-turn, multi-character dialogues extracted from acclaimed literary works. Key Features Authentic Content: Unlike synthetic datasets, CoSER extracts real dialogues from literature, maintaining high fidelity to the original works. The dialogues are inherently multi-turn and multi-character, exhibiting naturalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Neph0s/CoSER."
Nerfgun3/bad_prompt,,,,,,,https://huggingface.co/datasets/Nerfgun3/bad_prompt,,,,,,,,https://choosealicense.com/licenses/creativeml-openrail-m/,,,,,,,,,,,,"Negative Embedding / Textual Inversion Idea The idea behind this embedding was to somehow train the negative prompt as an embedding, thus unifying the basis of the negative prompt into one word or embedding. Side note: Embedding has proven to be very helpful for the generation of hands! :) Usage To use this embedding you have to download the file aswell as drop it into the ""\stable-diffusion-webui\embeddings"" folder. Please put the embedding inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Nerfgun3/bad_prompt."
NeroUCH/online-health-chating,,,,,,,https://huggingface.co/datasets/NeroUCH/online-health-chating,,,,,,,,https://choosealicense.com/licenses/pddl/,,,,,,,,,,,,"Online Health Chating This is the repository for the Online Health Chating project. which is the dataset of chathealth project. Alarm: This dataset isfor academic research only and any commercial use and clinical use is prohibited. Dataset We used crawler to collect the data from the following websites: KingNet Item Size Row 91,735 å• 8 å¥åº·å’¨è©¢ Item Size Row 4,919 è‡ºç£ E é™¢ Item Size Row 153,251 å®¶åº­é†«ç”Ÿ Itemâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NeroUCH/online-health-chating."
neuclir/csl,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/neuclir/csl,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for CSL Dataset Description CSL is the Chinese Scientific Literature Dataset. Paper: https://aclanthology.org/2022.coling-1.344 Repository: https://github.com/ydli-ai/CSL Data-Summary: The dataset contains titles, abstracts, keywords of papers written in Chinese from several academic fields. Languages Chinese English (translation) Dataset Structure Data Instances Split Documentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neuclir/csl."
neulab/conala,Text,,,,,https://arxiv.org/abs/1805.08949,https://huggingface.co/datasets/neulab/conala,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"CoNaLa is a dataset of code and natural language pairs crawled from Stack Overflow, for more details please refer to this paper: https://arxiv.org/pdf/1805.08949.pdf or the dataset page https://conala-corpus.github.io/."
neural-bridge/rag-dataset-12000,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/neural-bridge/rag-dataset-12000,models (LLMs) by allowing them to consult an external authoritative knowledge base before generating responses,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Retrieval-Augmented Generation (RAG) Dataset 12000 Retrieval-Augmented Generation (RAG) Dataset 12000 is an English dataset designed for RAG-optimized models, built by Neural Bridge AI, and released under Apache license 2.0. Dataset Description Data-Summary: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by allowing them to consult an external authoritative knowledge base before generating responses. This approachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neural-bridge/rag-dataset-12000."
neuralcatcher/hateful_memes,,,,,,https://arxiv.org/abs/2005.04790,https://huggingface.co/datasets/neuralcatcher/hateful_memes,,,,,,,,,,,,,,,,,,,,"The Hateful Memes Challenge README The Hateful Memes Challenge is a dataset and benchmark created by Facebook AI to drive and measure progress on multimodal reasoning and understanding. The task focuses on detecting hate speech in multimodal memes. Please see the paper for further details: The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes D. Kiela, H. Firooz, A. Mohan, V. Goswami, A. Singh, P. Ringshia, D. Testuggine For more details, see also the website:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neuralcatcher/hateful_memes."
newjins228/gta,,,,,,,https://huggingface.co/datasets/newjins228/gta,,,,,,,,,,,,,,,,,,,,newjins228/gta dataset hosted on Hugging Face and contributed by the HF Datasets community
NexaAI/Lipstick,,,,,,,https://huggingface.co/datasets/NexaAI/Lipstick,,,,,,,,,,,,,,,,,,,,NexaAI/Lipstick dataset hosted on Hugging Face and contributed by the HF Datasets community
Nexdata/multi_language_conversation,Audio,General,General,Text,"Accuracy, F1 Score",https://www.nexdata.ai/?source=Huggingface,https://huggingface.co/datasets/Nexdata/multi_language_conversation,conversation speech data,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,of multi-language conversation speech data,"Dataset Card for multi_language_conversation Data-Summary: The dataset contains 12,000 hours of multi-language conversation speech data. It's recorded by native speakers, covering English, French, German, Russian, Spanish, Japanese, Korean, Hindi, Vietnamese etc. The speakers start the conversation around a familar topic, to ensure the smoothness and nature of the conversation. The format is 16kHz, 16bit, uncompressed wav, mono channel. The sentence accuracy isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Nexdata/multi_language_conversation."
next-social/reddit_Crush_txt,,,,,,,https://huggingface.co/datasets/next-social/reddit_Crush_txt,,,,,,,,,,,,,,,,,,,,next-social/reddit_Crush_txt dataset hosted on Hugging Face and contributed by the HF Datasets community
NgThVinh/ValorantAgentVoiceLines,,,,,,,https://huggingface.co/datasets/NgThVinh/ValorantAgentVoiceLines,,,,,,,,,,,,,,,,,,,,"Valorant Voicelines Dataset Dataset Description The Valorant Voicelines Dataset is an unofficial collection of ingame voicelines from Valorant. The dataset is compiled from publicly available Valorant Wiki Fandom,. All voicelines are publicly accessible and attributed to their original sources. Dataset Structure Data Splits The data is divided into multiple subsets, each subset containing all voicelines belonging to a single agent. Dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NgThVinh/ValorantAgentVoiceLines."
nguha/legalbench,Text,,,,,https://arxiv.org/abs/2308.11462,https://huggingface.co/datasets/nguha/legalbench,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,LegalBench is a collection of benchmark tasks for evaluating legal reasoning in large language models.
nguyenminh871/multi_class_solidity_function_vulnerabilty,,,,,,,https://huggingface.co/datasets/nguyenminh871/multi_class_solidity_function_vulnerabilty,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""multi_class_solidity_function_vulnerabilty"" More Information needed"
nguyenvulebinh/song_dataset,,,,,,,https://huggingface.co/datasets/nguyenvulebinh/song_dataset,,,,,,,,,,,,,,,,,,,,nguyenvulebinh/song_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
ngxson/MiniThinky-dataset,,,,,,,https://huggingface.co/datasets/ngxson/MiniThinky-dataset,,,,,,,,,,,,,,,,,,,,MiniThinky dataset Merged from: https://huggingface.co/datasets/TuneIt/o1-python https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT https://huggingface.co/datasets/KingNish/reasoning-base-20k Post processing: Replaced with the format below Remove any rows that does not have reasoning process (i.e remove straight responses) Deduplicated Response format <|thinking|>{thinking_process} <|answer|> {real_answer}
nickmuchi/financial-classification,,,,,,,https://huggingface.co/datasets/nickmuchi/financial-classification,,,,,,,,,,,,,,,,,,,,"Dataset Creation This dataset combines financial phrasebank dataset and a financial text dataset from Kaggle. Given the financial phrasebank dataset does not have a validation split, I thought this might help to validate finance models and also capture the impact of COVID on financial earnings with the more recent Kaggle dataset."
nickrosh/Evol-Instruct-Code-80k-v1,,,,,,https://arxiv.org/abs/2306.08568,https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,Open Source Implementation of Evol-Instruct-Code as described in the WizardCoder Paper. Code for the intruction generation can be found on Github as Evol-Teacher.
Nicky0007/titulos_noticias_rcn_clasificadas,,,,,,,https://huggingface.co/datasets/Nicky0007/titulos_noticias_rcn_clasificadas,,,,,,,,,,,,,,,,,,,,"Dataset Card for Dataset Name titulos_noticias_rcn_clasificadas Dataset Description Se tomo las noticias de la pagina de RCN y se clasifico los titulos por ['salud' 'tecnologia' 'colombia' 'economia' 'deportes'] salud= 1805 datos, tecnologia= 1805 datos, colombia= 1805 datos, economia= 1805 datos, deportes= 1805 datos, Para dar un total de 9030 filas. pagina: https://www.noticiasrcn.com/ Homepage: Repository: Point of Contact: Languagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Nicky0007/titulos_noticias_rcn_clasificadas."
NicolaiSivesind/human-vs-machine,,,,,,,https://huggingface.co/datasets/NicolaiSivesind/human-vs-machine,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,This dataset contains labeled data with human-produced and machine-generated texts based on various domains: Wikipedia introductions and academic articles.
nid989/EssayFroum-Dataset,,,,,,,https://huggingface.co/datasets/nid989/EssayFroum-Dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,nid989/EssayFroum-Dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
nielsr/funsd-layoutlmv3,,,,,,,https://huggingface.co/datasets/nielsr/funsd-layoutlmv3,,,,,,,,,,,,,,,,,,,,https://guillaumejaume.github.io/FUNSD/
nightaway/pixelart,,,,,,,https://huggingface.co/datasets/nightaway/pixelart,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,nightaway/pixelart dataset hosted on Hugging Face and contributed by the HF Datasets community
nikitam/ACES,Text,Translation,General,Text,"BLEU, METEOR, TER",https://arxiv.org/abs/2401.16313,https://huggingface.co/datasets/nikitam/ACES,pairs and representing challenges from 68 phenomena for evaluating machine translation metrics,,36476,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,"T5, mBART, M2M100",,,"Dataset Card for ACES and Span-ACES Data-Summary: ACES consists of 36,476"
nilc-nlp/assin2,Text,General,General,Text,"Accuracy, F1 Score",https://sites.google.com/view/assin2,https://huggingface.co/datasets/nilc-nlp/assin2,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ASSIN 2 Data-Summary: The ASSIN 2 corpus is composed of rather simple sentences. Following the procedures of SemEval 2014 Task 1. The training and validation data are composed, respectively, of 6,500 and 500 sentence pairs in Brazilian Portuguese, annotated for entailment and semantic similarity. Semantic similarity values range from 1 to 5, and text entailment classes are either entailment or none. The test data are composed of approximatelyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nilc-nlp/assin2."
ninadn/indian-legal,,,,,,,https://huggingface.co/datasets/ninadn/indian-legal,,,,,,,,,,,,,,,,,,,,ninadn/indian-legal dataset hosted on Hugging Face and contributed by the HF Datasets community
nisaar/Lawyer_GPT_India,Text,,,,,,https://huggingface.co/datasets/nisaar/Lawyer_GPT_India,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset Card for Indian Polity Question-Answer Dataset Data-Summary:This dataset contains a collection of question-answer pairs on the subject of Indian Polity. The aim is to provide comprehensive answers to a wide range of questions pertaining to the Indian Constitution, judiciary, legislative, and various socio-political issues in India. It serves as a valuable resource for learners, researchers, and AI systems seeking to understand or respond to questions about Indian Polity. Supportedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nisaar/Lawyer_GPT_India."
nishanthc/dnd_map_dataset_v0.1,,,,,,,https://huggingface.co/datasets/nishanthc/dnd_map_dataset_v0.1,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""dnd_map_dataset_v0.1"" More Information needed"
Nitral-AI/Cybersecurity-ShareGPT,,,,,,,https://huggingface.co/datasets/Nitral-AI/Cybersecurity-ShareGPT,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"Converted, deslopped, min-hash deduplicated, rejection filtered, grammar corrected using: https://github.com/The-Chaotic-Neutrals/ShareGPT-Formaxxing"
nkp37/OpenVid-1M,,,,,,https://arxiv.org/abs/2407.02371,https://huggingface.co/datasets/nkp37/OpenVid-1M,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Summary This is the dataset proposed in our paper [ICLR 2025] OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation. OpenVid-1M is a high-quality text-to-video dataset designed for research institutions to enhance video quality, featuring high aesthetics, clarity, and resolution. It can be used for direct training or as a quality tuning complement to other video datasets. All videos in the OpenVid-1M dataset have resolutions of at least 512Ã—512.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nkp37/OpenVid-1M."
NLP-AUEB/eurlex,Text,,,,,http://nlp.cs.aueb.gr/software_and_datasets/EURLEX57K/,https://huggingface.co/datasets/NLP-AUEB/eurlex,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"EURLEX57K contains 57k legislative documents in English from EUR-Lex portal, annotated with EUROVOC concepts."
nlp-esg-scoring/spx-sustainalytics-esg-scores,,,,,,,https://huggingface.co/datasets/nlp-esg-scoring/spx-sustainalytics-esg-scores,,,,,,,,,,,,,,,,,,,,nlp-esg-scoring/spx-sustainalytics-esg-scores dataset hosted on Hugging Face and contributed by the HF Datasets community
nlp-kmu/kor_ner,Text,,,,,https://github.com/kmounlp/NER,https://huggingface.co/datasets/nlp-kmu/kor_ner,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Korean named entity recognition dataset
nlpai-lab/kullm-v2,Text,Translation,General,Text,"BLEU, METEOR, TER",,https://huggingface.co/datasets/nlpai-lab/kullm-v2,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"T5, mBART, M2M100",,,"Dataset Card for ""KULLM-v2"" Data-Summary: Korean translation of GPT4ALL, Dolly, and Vicuna data. repository: nlpai-lab/KULLM huggingface: nlpai-lab/kullm-v2 Translate dataset Translated 'instruction', 'input', and 'output' in the dataset via the DeepL API Lisence Apache-2.0 >>> from datasets import load_dataset >>> ds = load_dataset(""nlpai-lab/kullm-v2"", split=""train"") >>> ds DatasetDict({ train: Dataset({ features:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nlpai-lab/kullm-v2."
nlpaueb/biomrc,Text,,,,,http://nlp.cs.aueb.gr/,https://huggingface.co/datasets/nlpaueb/biomrc,,,,,,,,,,,,,,,,,,,,"We introduce BIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard."
NLPC-UOM/Sinhala-News-Category-classification,,,,,,,https://huggingface.co/datasets/NLPC-UOM/Sinhala-News-Category-classification,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"This file contains news texts (sentences) belonging to 5 different news categories (political, business, technology, sports and Entertainment). The original dataset was released by Nisansa de Silva (Sinhala Text Classification: Observations from the Perspective of a Resource Poor Language, 2015). The original dataset is processed and cleaned of single word texts, English only sentences etc. If you use this dataset, please cite {Nisansa de Silva, Sinhala Text Classification: Observations fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NLPC-UOM/Sinhala-News-Category-classification."
NLPCoreTeam/mmlu_ru,,,,,,https://arxiv.org/abs/2009.03300,https://huggingface.co/datasets/NLPCoreTeam/mmlu_ru,,,,,,,,,,,,https://github.com/openai/evals/blob/main/examples/mmlu.ipynb,,,,,,,,"MMLU in Russian (Massive Multitask Language Understanding) Overview of the Dataset MMLU dataset for EN/RU, without auxiliary train. The dataset contains dev/val/test splits for both, English and Russian languages. Note it doesn't include auxiliary_train split, which wasn't translated. Totally the dataset has ~16k samples per language: 285 dev, 1531 val, 14042 test. Description of original MMLU MMLU dataset covers 57 different tasks. Each taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NLPCoreTeam/mmlu_ru."
nlphuji/mscoco_2014_5k_test_image_text_retrieval,,,,,,https://arxiv.org/abs/1405.0312,https://huggingface.co/datasets/nlphuji/mscoco_2014_5k_test_image_text_retrieval,,,,,,,,,,,,,,,,,,,,"MSCOCO (5K test set) Original paper: Microsoft COCO: Common Objects in Context Homepage: https://cocodataset.org/#home 5K test set split from: http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip Bibtex: @inproceedings{lin2014microsoft, title={Microsoft coco: Common objects in context}, author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nlphuji/mscoco_2014_5k_test_image_text_retrieval."
nlpkevinl/whatsthatbook,Text,General,Document,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2305.15053,https://huggingface.co/datasets/nlpkevinl/whatsthatbook,,,,,,,,https://choosealicense.com/licenses/odc-by/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for WhatsThatBook Data-Summary: A collection of tip-of-the-tongue queries for book searches. The dataset was curated from GoodReads community forum user queries. It seves as a training and evaluation resource for tip-of-the-tongue book queries. The user queries contain the interactions on the community forum and the documents are books with associated metadata. Supported Tasks and Leaderboards WhatsThatBook is intended for informationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nlpkevinl/whatsthatbook.
nmac/lex_fridman_podcast,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/nmac/lex_fridman_podcast,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""lex_fridman_podcast"" Data-Summary: This dataset contains transcripts from the Lex Fridman podcast (Episodes 1 to 325). The transcripts were generated using OpenAI Whisper (large model) and made publicly available at: https://karpathy.ai/lexicap/index.html. Languages English Dataset Structure The dataset contains around 803K entries, consisting of audio transcripts generated from episodes 1 to 325 of the Lexâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nmac/lex_fridman_podcast."
nmitchko/i2b2-query-data-1.0,,,,,,,https://huggingface.co/datasets/nmitchko/i2b2-query-data-1.0,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,i2b2 query data 1.0 This is a dataset of i2b2 query builder
nogyxo/question-answering-ukrainian,,,,,,,https://huggingface.co/datasets/nogyxo/question-answering-ukrainian,,,,,,,,,,,,,,,,,,,,nogyxo/question-answering-ukrainian dataset hosted on Hugging Face and contributed by the HF Datasets community
nomic-ai/gpt4all_prompt_generations,,,,,,,https://huggingface.co/datasets/nomic-ai/gpt4all_prompt_generations,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,7,,,,,,,,,Dataset Card for [GPT4All Prompt Generations] Dataset Description Dataset used to train GPT4All Homepage: Repository: gpt4all Paper: Technical Report Atlas Map: Map of Cleaned Data
Nooon/Donate_a_cry,,,,,,,https://huggingface.co/datasets/Nooon/Donate_a_cry,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Nooon/Donate_a_cry dataset hosted on Hugging Face and contributed by the HF Datasets community
NoraAlt/Mawqif_Stance-Detection,,,,,,,https://huggingface.co/datasets/NoraAlt/Mawqif_Stance-Detection,,,,,,,,,,,,,,,,,,,,"Mawqif: A Multi-label Arabic Dataset for Target-specific Stance Detection Mawqif is the first Arabic dataset that can be used for target-specific stance detection. This is a multi-label dataset where each data point is annotated for stance, sentiment, and sarcasm. We benchmark Mawqif dataset on the stance detection task and evaluate the performance of four BERT-based models. Our best model achieves a macro-F1 of 78.89%. Mawqif Statistics This datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NoraAlt/Mawqif_Stance-Detection."
Norod78/Vintage-Faces-FFHQAligned,,,,,,,https://huggingface.co/datasets/Norod78/Vintage-Faces-FFHQAligned,,,,,,,,,,,,,,,,,,,,Norod78/Vintage-Faces-FFHQAligned dataset hosted on Hugging Face and contributed by the HF Datasets community
Norquinal/claude_evol_instruct_210k,,,,,,,https://huggingface.co/datasets/Norquinal/claude_evol_instruct_210k,,,,,,,,,,10,,,,,,,,,,"This dataset is the result of roughly 250k instruction/response pairs being generated by Claude, with instances of blatant alignment removed. 213375 instructions remain. This dataset is experimental in two ways: From start to finish, it was generated entirely synthetically through Anthropic's Claude AI. It was generated using a somewhat imperfect recreation of the evol-instruct method. 50k instructions were initially synthetically generated then ran through four epochs of evol-instruct."
NortheasternUniversity/big_patent,Text,,,,,https://evasharma.github.io/bigpatent/,https://huggingface.co/datasets/NortheasternUniversity/big_patent,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Each US patent application is filed under a Cooperative Patent Classification (CPC) code. There are nine such classification categories: A (Human Necessities), B (Performing Operations; Transporting), C (Chemistry; Metallurgy), D (Textiles; Paper), E (Fixed Constructions), F (Mechanical Engineering; Lightning; Heating; Weapons; Blasting), G (Physics), H (Electricity), and Y (General tagging of new or cross-sectional technology) There are two features: - description: detailed description of patent. - abstract: Patent abastract."
notrichardren/deception-evals,,,,,,,https://huggingface.co/datasets/notrichardren/deception-evals,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""deception-evals"" More Information needed"
nouman-10/reduced_dataset_from_wikiart,,,,,,,https://huggingface.co/datasets/nouman-10/reduced_dataset_from_wikiart,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""reduced_dataset_from_wikiart"" More Information needed"
NousResearch/hermes-function-calling-v1,,,,,,,https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Hermes Function-Calling V1 This dataset is the compilation of structured output and function calling data used in the Hermes 2 Pro series of models. This repository contains a structured output dataset with function-calling conversations, json-mode, agentic json-mode and structured extraction samples, designed to train LLM models in performing function calls and returning structured output based on natural language instructions. The dataset features various conversationalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1."
npvinHnivqn/VietnameseDictionary,,,,,,,https://huggingface.co/datasets/npvinHnivqn/VietnameseDictionary,,,,,,,,,,,,,,,,,,,,This dataset includes ~30k Vietnamese words and definitions
nsarker/plantspecies-demo,,,,,,,https://huggingface.co/datasets/nsarker/plantspecies-demo,,,,,,,,,,,,,,,,,,,,nsarker/plantspecies-demo dataset hosted on Hugging Face and contributed by the HF Datasets community
nsusemiehl/SciERC,,,,,,,https://huggingface.co/datasets/nsusemiehl/SciERC,,,,,,,,,,,,,,,,,,,,"SCIERC (Luan et al., 2018) via ""Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks"" (Gururangan et al., 2020) reuploaded because of error encountered when trying to load zj88zj/SCIERC with the huggingfaces/datasets library."
ntcuong777/ubuntu_dialogue_corpus_train,,,,,,,https://huggingface.co/datasets/ntcuong777/ubuntu_dialogue_corpus_train,,,,,,,,,,,,,,,,,,,,ntcuong777/ubuntu_dialogue_corpus_train dataset hosted on Hugging Face and contributed by the HF Datasets community
nthngdy/oscar-mini,Text,,,,,https://oscar-corpus.com,https://huggingface.co/datasets/nthngdy/oscar-mini,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,The Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.\
ntt123/viet-tts-dataset,,,,,,,https://huggingface.co/datasets/ntt123/viet-tts-dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Vietnamese Text-To-Speech dataset (VietTTS-v1.1) ðŸ””ðŸ””ðŸ”” visit https://github.com/NTT123/vietTTS for a vietnamese TTS library (included pretrained models). ðŸ””ðŸ””ðŸ”” The text is from a collection of novels and short stories from the author ""Vu Trong Phung."" The text is in public domain. The audio is generated by Google Text-to-Speech offline engine on Android. The audio is NOT for commercial use. Dataset size: 5.4G. Total audio duration: 35.9 hours. Text-audio samplesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ntt123/viet-tts-dataset."
NTU-NLP-sg/xCodeEval,,,,,,https://arxiv.org/abs/2303.03004,https://huggingface.co/datasets/NTU-NLP-sg/xCodeEval,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,https://github.com/ntunlp/xCodeEval,,,,,,,,"The ability to solve problems is a hallmark of intelligence and has been an enduring goal in AI. AI systems that can create programs as solutions to problems or assist developers in writing programs can increase productivity and make programming more accessible. Recently, pre-trained large language models have shown impressive abilities in generating new codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap rather than actual execution whereas semantic similarity (or equivalence) of two code segments depends only on their ``execution similarity'', i.e., being able to get the same output for a given input."
NumbersStation/NSText2SQL,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/NumbersStation/NSText2SQL,,,290000,,,,,https://choosealicense.com/licenses/other/,,,,,,,"ng and pre-processing techniques including table schema augmentation, SQL cleaning, and instruction generation using existing LLMs",,"BERT, RoBERTa, T5",,,"Data-Summary: NSText2SQL dataset used to train NSQL models. The data is curated from more than 20 different public sources across the web with permissable licenses (listed below). All of these datasets come with existing text-to-SQL pairs. We apply various data cleaning and pre-processing techniques including table schema augmentation, SQL cleaning, and instruction generation using existing LLMs. The resulting dataset contains around 290,000 samples of text-to-SQL pairs. Forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NumbersStation/NSText2SQL."
nuprl/MultiPL-E,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2301.03988,https://huggingface.co/datasets/nuprl/MultiPL-E,models for code generation that supports 22 programming languages,,,,,,,https://choosealicense.com/licenses/mit/,,,,https://github.com/bigcode-project/bigcode-evaluation-harness,,,,,"BERT, RoBERTa, T5",ing languages,,"Dataset Card for MultiPL-E Data-Summary: MultiPL-E is a dataset for evaluating large language models for code generation that supports 22 programming languages. It takes the OpenAI HumanEval and the Mostly Basic Python Programs (MBPP) benchmarks and uses little compilers to translate them to other languages. It is easy to add support for new languages and benchmarks. The dataset is divided into several configurations named SRCDATA-LANG, where SRCDATA is eitherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nuprl/MultiPL-E."
nvidia/PhysicalAI-SmartSpaces,,,,,,https://arxiv.org/abs/2404.09432,https://huggingface.co/datasets/nvidia/PhysicalAI-SmartSpaces,,,,,,,,,,,,,,,,,,,,"Physical AI Smart Spaces Dataset Overview Comprehensive, annotated dataset for multi-camera tracking and 2D/3D object detection. This dataset is synthetically generated with Omniverse. This dataset consists of over 250 hours of video from across nearly 1,500 cameras from indoor scenes in warehouses, hospitals, retail, and more. The dataset is time synchronized for tracking humans across multiple cameras using feature representation and no personal data.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/PhysicalAI-SmartSpaces."
NX2411/AIhub-korean-speech-data,,,,,,,https://huggingface.co/datasets/NX2411/AIhub-korean-speech-data,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,NX2411/AIhub-korean-speech-data dataset hosted on Hugging Face and contributed by the HF Datasets community
nyu-mll/glue,Text,General,General,Text,"Accuracy, F1 Score",https://gluebenchmark.com/,https://huggingface.co/datasets/nyu-mll/glue,Understanding Evaluation benchmark (https://gluebenchmark,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for GLUE Data-Summary: GLUE, the General Language Understanding Evaluation benchmark (https://gluebenchmark.com/) is a collection of resources for training, evaluating, and analyzing natural language understanding systems. Supported Tasks and Leaderboards The leaderboard for the GLUE benchmark can be found at this address. It comprises the following tasks: ax A manually-curated evaluation dataset for fine-grainedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyu-mll/glue."
nyuuzyou/fimfiction,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/nyuuzyou/fimfiction,The dataset is primarily in English,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Fimfiction.net Data-Summary: This dataset contains 815,740 user stories from Fimfiction.net, a platform dedicated to fanfiction. Each entry includes the story's title, content, and unique identifier. The writings span diverse genres, themes, and creative styles, contributed by the platform's community. Languages The dataset is primarily in English. Dataset Structure Data Fields id: Unique identifier for the storyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/fimfiction."
O1-OPEN/OpenO1-SFT,,,,,,,https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"SFT Data for CoT Activation ðŸŽ‰ðŸŽ‰ðŸŽ‰This repository contains the dataset used for fine-tuning a language model using SFT for Chain-of-Thought Activation. ðŸŒˆðŸŒˆðŸŒˆThe dataset is designed to enhance the model's ability to generate coherent and logical reasoning sequences. â˜„â˜„â˜„By using this dataset, the model can learn to produce detailed and structured reasoning steps, enhancing its performance on complex reasoning tasks. Statistics 1ï¸âƒ£Total Records: 77,685â€¦ See the full description on the dataset page: https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT."
OATML-Markslab/ProteinGym_v0.1,,,,,,https://arxiv.org/abs/2205.13760,https://huggingface.co/datasets/OATML-Markslab/ProteinGym_v0.1,,,,,,,,,,,,,,,,,,,,ProteinGym benchmarks overview ProteinGym is an extensive set of Deep Mutational Scanning (DMS) assays curated to enable thorough comparisons of various mutation effect predictors indifferent regimes. It is comprised of two benchmarks: 1) a substitution benchmark which consists of the experimental characterisation of âˆ¼1.5M missense variants across 87 DMS assays 2) an indel benchmark that includes âˆ¼300k mutants across 7 DMS assays. Each processed file in each benchmarkâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OATML-Markslab/ProteinGym_v0.1.
odegiber/hate_speech18,Text,,,,,https://github.com/Vicomtech/hate-speech-dataset,https://huggingface.co/datasets/odegiber/hate_speech18,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"These files contain text extracted from Stormfront, a white supremacist forum. A random set of forums posts have been sampled from several subforums and split into sentences. Those sentences have been manually labelled as containing hate speech or not, according to certain annotation guidelines."
odunola/bible-reference-sentence-pair,,,,,,,https://huggingface.co/datasets/odunola/bible-reference-sentence-pair,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,odunola/bible-reference-sentence-pair dataset hosted on Hugging Face and contributed by the HF Datasets community
OGB/ogbg-code2,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/OGB/ogbg-code2,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ogbg-code2 Data-Summary: The ogbg-code2 dataset contains Abstract Syntax Trees (ASTs) obtained from 450 thousands Python method definitions, from GitHub CodeSearchNet. ""Methods are extracted from a total of 13,587 different repositories across the most popular projects on GitHub."", by teams at Stanford, to be a part of the Open Graph Benchmark. See their website or paper for dataset postprocessing. Supported Tasks and Leaderboardsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OGB/ogbg-code2."
oisinoh/tomatos,,,,,,,https://huggingface.co/datasets/oisinoh/tomatos,,,,,,,,,,,,,,,,,,,,Beans is a dataset of images of beans taken in the field using smartphone cameras. It consists of 3 classes: 2 disease classes and the healthy class. Diseases depicted include Angular Leaf Spot and Bean Rust. Data was annotated by experts from the National Crops Resources Research Institute (NaCRRI) in Uganda and collected by the Makerere AI research lab.
OleehyO/latex-formulas,,,,,,,https://huggingface.co/datasets/OleehyO/latex-formulas,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,"ð‘©ð‘°ð‘® ð‘µð‘¬ð‘¾ð‘ºâ€¼ï¸ ðŸ“® [2ðŸŽ2ðŸ’-ðŸŽ2] We trained a formula recognition model, ð“ðžð±ð“ðžð¥ð¥ðžð«, using the latex-formulas dataset. It can convert LaTeX formulas into images and boasts high accuracy and strong generalization capabilities, covering most formula recognition scenarios. For more details, please refer to the ð“ðžð±ð“ðžð¥ð¥ðžð« GitHub repository. Dataset Description ä¸­æ–‡ç‰ˆæœ¬ There are two datasets: raw_formulas and cleaned_formulas(This dataset has 550Kâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OleehyO/latex-formulas."
oliverwang15/news_with_gpt_instructions,,,,,,,https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""news_with_gpt_instructions"" More Information needed"
olivierdehaene/xkcd,Text,General,General,Text,"Accuracy, F1 Score",https://www.explainxkcd.com,https://huggingface.co/datasets/olivierdehaene/xkcd,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""XKCD"" Data-Summary: XKCD is an export of all XKCD comics with their transcript and explanation scrapped from https://explainxkcd.com. Dataset Structure Data Instances id: 1 title: Barrel - Part 1 image_title: Barrel - Part 1 url: https://www.xkcd.com/1 image_url: https://imgs.xkcd.com/comics/barrel_cropped_(1).jpg explained_url: https://www.explainxkcd.com/wiki/index.php/1:_Barrel_-_Part_1 transcript: [A boy sitsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/olivierdehaene/xkcd."
omamaatconrad/recipe,,,,,,,https://huggingface.co/datasets/omamaatconrad/recipe,,,,,,,,,,,,,,,,,,,,omamaatconrad/recipe dataset hosted on Hugging Face and contributed by the HF Datasets community
omar-sharif/BAD-Bengali-Aggressive-Text-Dataset,,,,,,,https://huggingface.co/datasets/omar-sharif/BAD-Bengali-Aggressive-Text-Dataset,,,,,,,,,,,,,,,,,,,,Novel Aggressive Text Dataset in Bengali Tackling Cyber-Aggression: Identification and Fine-Grained Categorization of Aggressive Texts on Social Media using Weighted Ensemble of Transformers Author: Omar Sharif and Mohammed Moshiul Hoque Related Papers: Paper1 in Neurocomputing Journal Paper2 in CONSTRAINT@AAAI-2021 Paper3 in LTEDI@EACL-2021 Abstract The pervasiveness of aggressive content in social media has become a serious concern for governmentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/omar-sharif/BAD-Bengali-Aggressive-Text-Dataset.
omegalabsinc/omega-multimodal,,,,,,,https://huggingface.co/datasets/omegalabsinc/omega-multimodal,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"OMEGA Labs Bittensor Subnet: Multimodal Dataset for AGI Research Introduction The OMEGA Labs Bittensor Subnet Dataset is a groundbreaking resource for accelerating Artificial General Intelligence (AGI) research and development. This dataset, powered by the Bittensor decentralized network, aims to be the world's largest multimodal dataset, capturing the vast landscape of human knowledge and creation. With over 1 million hours of footage and 30 million+ 2-minute videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/omegalabsinc/omega-multimodal."
omi-health/medical-dialogue-to-soap-summary,,,,,,https://arxiv.org/abs/2310.15959,https://huggingface.co/datasets/omi-health/medical-dialogue-to-soap-summary,,,,,,,,,,,,,,,,,,,,"Dataset Card for Synthetic Medical Dialogues and SOAP Summaries Dataset Description Abstract This dataset consists of 10,000 synthetic dialogues between a patient and clinician, created using the GPT-4 dataset from NoteChat, based on PubMed Central (PMC) case-reports. Accompanying these dialogues are SOAP summaries generated through GPT-4. The dataset is split into 9250 training, 500 validation, and 250 test entries, each containing a dialogueâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/omi-health/medical-dialogue-to-soap-summary."
Onegai/BitcoinPrice,,,,,,,https://huggingface.co/datasets/Onegai/BitcoinPrice,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Onegai/BitcoinPrice dataset hosted on Hugging Face and contributed by the HF Datasets community
ontocord/OIG-moderation,,,,,,,https://huggingface.co/datasets/ontocord/OIG-moderation,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"This is the Open Instruction Generalist - Moderation Dataset This is our attempt to create a diverse dataset of user dialogue that may be related to NSFW subject matters, abuse eliciting text, privacy violation eliciting instructions, depression or related content, hate speech, and other similar topics. We use the [prosocial], [anthropic redteam], subsets of [English wikipedia] datasets along with other public datasets described below and data created or contributed byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ontocord/OIG-moderation."
ontonotes/conll2012_ontonotesv5,Text,,,,,https://cemantix.org/data/ontonotes.html,https://huggingface.co/datasets/ontonotes/conll2012_ontonotesv5,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-nd-4.0/,,,,,,,,,,,,"OntoNotes v5.0 is the final version of OntoNotes corpus, and is a large-scale, multi-genre, multilingual corpus manually annotated with syntactic, semantic and discourse information. This dataset is the version of OntoNotes v5.0 extended and is used in the CoNLL-2012 shared task. It includes v4 train/dev and v9 test data for English/Chinese/Arabic and corrected version v12 train/dev/test data (English only). The source of data is the Mendeley Data repo [ontonotes-conll2012](https://data.mendeley.com/datasets/zmycy7t9h9), which seems to be as the same as the official data, but users should use this dataset on their own responsibility. See also summaries from paperwithcode, [OntoNotes 5.0](https://paperswithcode.com/dataset/ontonotes-5-0) and [CoNLL-2012](https://paperswithcode.com/dataset/conll-2012-1) For more detailed info of the dataset like annotation, tag set, etc., you can refer to the documents in the Mendeley repo mentioned above."
OOPPEENN/Galgame_Dataset,,,,,,,https://huggingface.co/datasets/OOPPEENN/Galgame_Dataset,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,0x0 ä½¿ç”¨åè®®ï¼š å¿…é¡»éµå®ˆGNU General Public License v3.0å†…çš„æ‰€æœ‰åè®®ï¼é™„åŠ ï¼šç¦æ­¢å•†ç”¨ï¼Œæœ¬æ•°æ®é›†ä»¥åŠä½¿ç”¨æœ¬æ•°æ®é›†è®­ç»ƒå‡ºæ¥çš„ä»»ä½•æ¨¡åž‹éƒ½ä¸å¾—ç”¨äºŽä»»ä½•å•†ä¸šè¡Œä¸ºï¼Œå¦‚è¦ç”¨äºŽå•†ä¸šç”¨é€”ï¼Œè¯·æ‰¾æ•°æ®åˆ—è¡¨å†…çš„æ‰€æœ‰åŽ‚å•†æŽˆæƒï¼ˆç¬‘ï¼‰ï¼Œå› è¿åå¼€æºåè®®è€Œå‡ºçŽ°çš„ä»»ä½•é—®é¢˜éƒ½ä¸Žæœ¬äººæ— å…³ï¼ è®­ç»ƒå‡ºæ¥çš„æ¨¡åž‹å¿…é¡»å¼€æºï¼Œæ˜¯å¦åœ¨READMEå†…å¼•ç”¨æœ¬æ•°æ®é›†ç”±è®­ç»ƒè€…è‡ªä¸»å†³å®šï¼Œä¸åšå¼ºåˆ¶è¦æ±‚ã€‚ 0x1 æ•°æ®è¯´æ˜Žï¼š è§£åŽ‹å¯†ç ï¼š9ll9Ke4iq0jqyq3gS1Wyã€‚ æ ‡æ³¨è¯´æ˜Žï¼šæ ‡æ³¨ï¼Œè¯´è¯äººå’Œå¯¹åº”çš„éŸ³é¢‘æ˜¯ç›´æŽ¥è¯»æ¸¸æˆå¼•æ“Žçš„è„šæœ¬ç”Ÿæˆçš„ï¼Œåº”è¯¥æ˜¯100%å‡†ç¡®çŽ‡ï¼Œå…¨éƒ¨å­˜æ”¾åœ¨index.jsoné‡Œé¢ï¼Œå¦‚æžœè¿˜æœ‰é”™è¯¯å¯ä»¥åœ¨å¼€issuesåé¦ˆï¼ˆæœ‰äº›é—æ¼çš„æŽ§åˆ¶ç¬¦å¯èƒ½æ²¡æ´—å¹²å‡€ï¼‰ã€‚ åŠ¡å¿…æ ¹æ®index.jsoné‡Œé¢çš„é”®å€¼å¯¹æ‰¾éŸ³é¢‘ï¼Œä¸åœ¨indexå†…çš„éŸ³é¢‘è¯·ç›´æŽ¥ä¸¢å¼ƒï¼Œè¯´è¯äººä¸ºï¼Ÿï¼Ÿï¼Ÿçš„è¯·ç›´æŽ¥ä¸¢å¼ƒã€‚ æ•°æ®è¯­è¨€ï¼šæ—¥è¯­ï¼ˆ100%ï¼‰ æ•°æ®æ—¶é•¿ï¼š5409h 27m 07s è§’è‰²æ€»æ•°ï¼š15352äººï¼ˆæœªåˆå¹¶ï¼‰â€¦ See the full description on the dataset page: https://huggingface.co/datasets/OOPPEENN/Galgame_Dataset.
open-llm-leaderboard-old/requests,,,,,,,https://huggingface.co/datasets/open-llm-leaderboard-old/requests,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Open LLM Leaderboard Requests This repository contains the request files of models that have been submitted to the Open LLM Leaderboard. You can take a look at the current status of your model by finding its request file in this dataset. If your model failed, feel free to open an issue on the Open LLM Leaderboard! (We don't follow issues in this repository as often) Evaluation Methodology The evaluation process involves running your models against severalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/open-llm-leaderboard-old/requests."
Open-Orca/OpenOrca,Text,,,,,https://arxiv.org/abs/2306.02707,https://huggingface.co/datasets/Open-Orca/OpenOrca,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"ðŸ‹ The OpenOrca Dataset! ðŸ‹ We are thrilled to announce the release of the OpenOrca dataset! This rich collection of augmented FLAN data aligns, as best as possible, with the distributions outlined in the Orca paper. It has been instrumental in generating high-performing model checkpoints and serves as a valuable resource for all NLP researchers and developers! Official Models Mistral-7B-OpenOrca Our latest model, the first 7B to score better overall than allâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Open-Orca/OpenOrca."
open-r1/codeforces,,,,,,,https://huggingface.co/datasets/open-r1/codeforces,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for CodeForces Dataset description CodeForces is one of the most popular websites among competitive programmers, hosting regular contests where participants must solve challenging algorithmic optimization problems. The challenging nature of these problems makes them an interesting dataset to improve and test modelsâ€™ code reasoning capabilities. While previous efforts such as DeepMindâ€™s CodeContests dataset have compiled a large amount of CodeForces problemsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/open-r1/codeforces."
open-thoughts/OpenThoughts-114k,,,,,,https://www.open-thoughts.ai/,https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,https://github.com/bespokelabsai/curator/tree/main/examples/bespoke-stratos-data-generation,,,,,,,,Open-Thoughts-114k Open synthetic reasoning dataset with 114k high-quality
open-web-math/open-web-math,,,,,,https://arxiv.org/abs/2310.06786,https://huggingface.co/datasets/open-web-math/open-web-math,,,,,,,,,,,,,,,,,,,,"Keiran Paster*, Marco Dos Santos*, Zhangir Azerbayev, Jimmy Ba GitHub | ArXiv | PDF OpenWebMath is a dataset containing the majority of the high-quality, mathematical text from the internet. It is filtered and extracted from over 200B HTML files on Common Crawl down to a set of 6.3 million documents containing a total of 14.7B tokens. OpenWebMath is intended for use in pretraining and finetuning large language models. You can download the dataset using Hugging Face: from datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/open-web-math/open-web-math."
openaccess-ai-collective/jeopardy,,,,,,,https://huggingface.co/datasets/openaccess-ai-collective/jeopardy,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,openaccess-ai-collective/jeopardy dataset hosted on Hugging Face and contributed by the HF Datasets community
openai/openai_humaneval,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2107.03374,https://huggingface.co/datasets/openai/openai_humaneval,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5","ing problems with a function sig- nature, docstring, body, and several unit tests",,"Dataset Card for OpenAI HumanEval Data-Summary: The HumanEval dataset released by OpenAI includes 164 programming problems with a function sig- nature, docstring, body, and several unit tests. They were handwritten to ensure not to be included in the training set of code generation models. Supported Tasks and Leaderboards Languages The programming problems are written in Python and contain English natural text in comments andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openai/openai_humaneval."
OpenAssistant/oasst1,Text,General,Scientific,Text,"Accuracy, F1 Score",https://www.open-assistant.io/,https://huggingface.co/datasets/OpenAssistant/oasst1,", annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees",,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"OpenAssistant Conversations Dataset (OASST1) Data-Summary: In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations (OASST1), a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effortâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/oasst1."
openbmb/llava_zh,,,,,,,https://huggingface.co/datasets/openbmb/llava_zh,,,,,,,,,,,,,,,,,,,,openbmb/llava_zh dataset hosted on Hugging Face and contributed by the HF Datasets community
openchat/openchat_sharegpt4_dataset,,,,,,,https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset,,,,,,,,,,,,,,,,,,,,This repository contains cleaned and filtered ShareGPT GPT-4 data used to train OpenChat. Details can be found in the OpenChat repository.
openclimatefix/nimrod-uk-1km,Text,,,,,https://github.com/deepmind/deepmind-research/tree/master/nowcasting,https://huggingface.co/datasets/openclimatefix/nimrod-uk-1km,,,,,,,,,,,,,,,,,,,,This dataset contains UK Nimrod rainfall radar data for 2016-2019 as used in the Skillful Precipitation Nowcasting Using Deep Generative Model of Radar paper by DeepMind.
OpenCoder-LLM/opc-fineweb-code-corpus,,,,,,https://arxiv.org/abs/2411.04905,https://huggingface.co/datasets/OpenCoder-LLM/opc-fineweb-code-corpus,,,,,,,,https://choosealicense.com/licenses/mit/,,,1824,,,,,,,,,OpenCoder Dataset The OpenCoder dataset is composed of the following datasets: opc-sft-stage1: the sft data used for opencoder sft-stage1 opc-sft-stage2: the sft data used for opencoder sft-stage2 opc-annealing-corpus: the synthetic data & algorithmic corpus used for opencoder annealing opc-fineweb-code-corpus: the code-related page recalled from fineweb <-- you are here opc-fineweb-math-corpus: the math-related page recalled from fineweb refineCode-code-corpus-meta: theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenCoder-LLM/opc-fineweb-code-corpus.
opencsg/chinese-cosmopedia,,,,,,https://arxiv.org/abs/2501.08197,https://huggingface.co/datasets/opencsg/chinese-cosmopedia,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,"s and StylesThe Chinese Cosmopedia dataset places special emphasis on the style and format of the generated content, encompassing various text types from academic to practical applications",,,,,,"Chinese Cosmopedia Dataset [ä¸­æ–‡] [English] [OpenCSG Community] [ðŸ‘¾github] [wechat] [Twitter] ðŸ“–Technical Report The Chinese Cosmopedia dataset contains a total of 15 million entries, approximately 60B tokens. Two key elements in constructing the synthetic dataset are seed data and prompts. Seed data determines the theme of the generated content, while prompts define the style of the data (such as textbooks, stories, tutorials, or children's books). Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/opencsg/chinese-cosmopedia."
opendatalab/WanJuan-Russian,,,,,,https://arxiv.org/abs/2501.14506,https://huggingface.co/datasets/opendatalab/WanJuan-Russian,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"ðŸ’¡ Introduction WanJuan-Russianï¼ˆä¸‡å·ä¸è·¯-ä¿„è¯­ï¼‰corpus, with a volume exceeding 280GB, comprises 7 major categories and 34 subcategories. It covers a wide range of local-specific content, including history, politics, culture, real estate, shopping, weather, dining, encyclopedias, and professional knowledge. The rich thematic classification not only facilitates researchers in retrieving data according to specific needs but also ensures that the corpus can adapt to diverse researchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/opendatalab/WanJuan-Russian."
opendiffusionai/cc12m-2mp-realistic,,,,,,,https://huggingface.co/datasets/opendiffusionai/cc12m-2mp-realistic,,,,,,,,,,,,,,,,,,,,"Overview A subset of the ""CC12m"" dataset. Size varying between 2mp <= x < 4mp. Anything actually 4mp or larger will be in one of our ""4mp"" datasets. This dataset is created for if you basically need more images and dont mind a little less quality. Quality I have filtered out as many watermarks, etc. as possible using AI models. I have also thrown out stupid black-and-white photos, because they poison normal image prompting. This is NOT HAND CURATED, unlike some of ourâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/opendiffusionai/cc12m-2mp-realistic."
OpenGVLab/InternVid,,,,,,https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid,https://huggingface.co/datasets/OpenGVLab/InternVid,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"InternVid InternVid-10M-FLT We present InternVid-10M-FLT, a subset of this dataset, consisting of 10 million video clips, with generated high-quality captions for publicly available web videos. Download The 10M samples are provided in jsonlines file. Columns include the videoID, timestamps, generated caption and their UMT similarity scores.\ How to Use from datasets import load_dataset dataset = load_dataset(""OpenGVLab/InternVid"")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/InternVid."
openlanguagedata/flores_plus,,,,,,https://arxiv.org/abs/2207.04672,https://huggingface.co/datasets/openlanguagedata/flores_plus,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"Dataset Card for FLORES+ FLORES+ is an evaluation benchmark dataset for multilingual machine translation. Dataset Details Dataset Description FLORES+ is a multilingual machine translation benchmark released under CC BY-SA 4.0. This dataset was originally released by FAIR researchers at Meta under the name FLORES. Further information about these initial releases can be found in Dataset Sources below. The data is now being managed by OLDI, the Open Language Dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openlanguagedata/flores_plus."
openlifescienceai/medmcqa,Text,Question Answering,Scientific,Text,"Exact Match, F1 Score",https://medmcqa.github.io,https://huggingface.co/datasets/openlifescienceai/medmcqa,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for MedMCQA Data-Summary: MedMCQA is a large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. MedMCQA has more than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options whichâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openlifescienceai/medmcqa."
OpenRL/DeepFakeFace,,,,,,https://arxiv.org/abs/2309.02218,https://huggingface.co/datasets/OpenRL/DeepFakeFace,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,"--- license: apache-2.0 --- The dataset accompanying the paper ""Robustness and Generalizability of Deepfake Detection: A Study with Diffusion Models"". [Website] [paper] [GitHub]. Introduction Welcome to the DeepFakeFace (DFF) dataset! Here we present a meticulously curated collection of artificial celebrity faces, crafted using cutting-edge diffusion models. Our aim is to tackle the rising challenge posed by deepfakes in today's digital landscape. Here are some"
openslr/librispeech_asr,Text,,,,,http://www.openslr.org/12,https://huggingface.co/datasets/openslr/librispeech_asr,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"LibriSpeech is a corpus of approximately 1000 hours of read English speech with sampling rate of 16 kHz, prepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from read audiobooks from the LibriVox project, and has been carefully segmented and aligned.87"
openSUSE/cavil-legal-text,,,,,,,https://huggingface.co/datasets/openSUSE/cavil-legal-text,,,,,,,,https://choosealicense.com/licenses/gpl-2.0/,,,,,,,,,,,,"Data Description This is training data for machine learning models that can be used with Cavil, the openSUSE legal review and SBOM system. Cavil uses a pattern matching system to identify potential legal text in source code. This process is based around identifying hot zones of legal keywords (snippets) and produces around 80% false positives. Historically these false positives had to be sorted out by humans. A few years ago we've started using machine learning to automate much ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openSUSE/cavil-legal-text."
opentargets/clinical_trial_reason_to_stop,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",https://www.opentargets.org,https://huggingface.co/datasets/opentargets/clinical_trial_reason_to_stop,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for Clinical Trials's Reason to Stop Data-Summary: This dataset contains a curated classification of more than 5000 reasons why a clinical trial has suffered an early stop. The text has been extracted from clinicaltrials.gov, the largest resource of clinical trial information. The text has been curated by members of the Open Targets organisation, a project aimed at providing data relevant to drug development. All 17 possible classes have beenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/opentargets/clinical_trial_reason_to_stop."
opentensor/openvalidators,Text,General,Scientific,Text,"Accuracy, F1 Score",https://bittensor.com/,https://huggingface.co/datasets/opentensor/openvalidators,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Openvalidators dataset Data-Summary: The OpenValidators dataset, created by the OpenTensor Foundation, is a continuously growing collection of data generated by the OpenValidators project in W&B. It contains millions of records and serves researchers, data scientists, and miners in the Bittensor network. The dataset provides information on network performance, node behaviors, and wandb run details. Researchers can gain insights and detect patternsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/opentensor/openvalidators."
orby/jfk_files,,,,,,,https://huggingface.co/datasets/orby/jfk_files,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,What is this? Running OCR on the JFK file PDF dump. The goal is to get a clean dataset that is easily consumable by AI. Why? Fun
orkg/SciQA,Text,,,,,,https://huggingface.co/datasets/orkg/SciQA,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"SciQA contains 2,565 SPARQL query - question pairs along with answers fetched from the open research knowledge graph (ORKG) via a Virtuoso SPARQL endpoint, it is a collection of both handcrafted and autogenerated questions and queries. The dataset is split into 70% training, 10% validation and 20% test"
oscar-corpus/oscar,Text,,,,,https://oscar-corpus.com,https://huggingface.co/datasets/oscar-corpus/oscar,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,The Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.\
ostris/sdxl_10_reg,,,,,,,https://huggingface.co/datasets/ostris/sdxl_10_reg,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Stable Diffusion XL 1.0 Regularization Images Note: All of these images were generated without the refiner. These are sdxl 1.0 base only. This is some of my SDXL 1.0 regularization images generated with various prompts that are useful for regularization images or other specialized training. (color augmentation, bluring, shapening, etc). I will attempt to add more as I go along with various categories. Each image has a corrisponding txt file with the prompt used to generate it asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ostris/sdxl_10_reg."
osunlp/ConflictQA,,,,,,https://arxiv.org/abs/2305.13300,https://huggingface.co/datasets/osunlp/ConflictQA,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,data for ConflictQA.
osyvokon/pavlick-formality-scores,,,,,,,https://huggingface.co/datasets/osyvokon/pavlick-formality-scores,,,,,,,,https://choosealicense.com/licenses/cc-by-3.0/,,,,,,,,,,,,"This dataset contains sentence-level formality annotations used in the 2016 TACL paper ""An Empirical Analysis of Formality in Online Communication"" (Pavlick and Tetreault, 2016). It includes sentences from four genres (news, blogs, email, and QA forums), all annotated by humans on Amazon Mechanical Turk. The news and blog data was collected by Shibamouli Lahiri, and we are redistributing it here for the convenience of other researchers. We collected the email and answers data ourselves, usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/osyvokon/pavlick-formality-scores."
othertea/epigenetic_marks_pham2005,,,,,,,https://huggingface.co/datasets/othertea/epigenetic_marks_pham2005,,,,,,,,,,,,,,,,,,,,"This contains datasets of histone occupancy, acetylation, and methylation by ChiP-Chip protocol in vivo from Pham et al., as retrieved from https://www.jaist.ac.jp/~tran/nucleosome/members.htm in January 2023."
ought/raft,Text,,,,,https://raft.elicit.org,https://huggingface.co/datasets/ought/raft,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"Large pre-trained language models have shown promise for few-shot learning, completing text-based tasks given only a few task-specific"
Overfit-GM/turkish-toxic-language,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Overfit-GM/turkish-toxic-language,datasets found online,,77800,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Turkish Texts for Toxic Language Detection Dataset Description Data-Summary: This text dataset is a collection of Turkish texts that have been merged from various existing offensive language datasets found online. The dataset contains a total of 77,800 instances, each labeled as either offensive or not offensive. To ensure the dataset's completeness, we utilized multiple transformer models to augment the dataset with pseudo labels. The resultingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Overfit-GM/turkish-toxic-language."
owaiskha9654/PubMed_MultiLabel_Text_Classification_Dataset_MeSH,,,,,,,https://huggingface.co/datasets/owaiskha9654/PubMed_MultiLabel_Text_Classification_Dataset_MeSH,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,This dataset consists of a approx 50k collection of research articles from PubMed repository. Originally these documents are manually annotated by Biomedical Experts with their MeSH labels and each articles are described in terms of 10-15 MeSH labels. In this Dataset we have huge numbers of labels present as a MeSH major which is raising the issue of extremely large output space and severe label sparsity issues. To solve this Issue Dataset has been Processed and mapped to its root as Describedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/owaiskha9654/PubMed_MultiLabel_Text_Classification_Dataset_MeSH.
owanr/r2_coedit_v2,,,,,,,https://huggingface.co/datasets/owanr/r2_coedit_v2,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""r2_coedit_v2"" More Information needed"
owkin/medical_knowledge_from_extracts,,,,,,,https://huggingface.co/datasets/owkin/medical_knowledge_from_extracts,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,This dataset is used to train LLMs for medical knowledge extraction tasks
OxAISH-AL-LLM/wiki_toxic,Text,,,,,,https://huggingface.co/datasets/OxAISH-AL-LLM/wiki_toxic,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,2,,,,,,,,,,Jigsaw Toxic Comment Challenge dataset. This dataset was the basis of a Kaggle competition run by Jigsaw
Ozziey/poems_dataset,,,,,,,https://huggingface.co/datasets/Ozziey/poems_dataset,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,Ozziey/poems_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
Ozzy-Helix/Hitori-Gotoh-Bocchi-RVCV2-Dataset,,,,,,,https://huggingface.co/datasets/Ozzy-Helix/Hitori-Gotoh-Bocchi-RVCV2-Dataset,,,,,,,,,,,,,,,,,,,,Ozzy-Helix/Hitori-Gotoh-Bocchi-RVCV2-Dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
p1atdev/niji-v5,,,,,,,https://huggingface.co/datasets/p1atdev/niji-v5,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,ç§ãŒnijijourney v5ã§ç”Ÿæˆã—ãŸç”»åƒã€‚è‡ªç”±ã«ä½¿ãˆã¾ã™ã€‚(ã‘ã©è©æ¬ºã¨ã‹çŠ¯ç½ªã«ã¤ã‹ã†ã®ã¯ã‚„ã‚ã¦ã­) ãŠã™ã™ã‚ã®ä½¿ã„æ–¹ã¨ã—ã¦ã¯ã€ã¨ã‚Šã‚ãˆãšä¸­ã®ç”»åƒã‚’è¦‹ã¦ã¿ã¦å¥½ããªã‚‚ã®ã ã‘é¸ã‚“ã§ä½¿ã†ã¨ã‚ˆã„ã¨æ€ã„ã¾ã™ã€‚ å…¨ä½“ã®æ³¨æ„ç‚¹ã¨ã—ã¦ã€å¿…ãšã—ã‚‚ã™ã¹ã¦ã®ç”»åƒã«ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒä»˜å±žã—ã¦ã‚‹ã¨ã¯é™ã‚‰ãªã„ã®ã¨ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒã¤ã„ã¦ã„ã¦ã‚‚ãã®ã¾ã¾ä½¿ã†ã¨å•é¡ŒãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã‚ã¾ã‚Šä¿¡ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ ã¾ãŸäººç‚ºçš„ãªãƒŸã‚¹ã«ã‚ˆã‚Šã€4åˆ†å‰²ã•ã‚Œãšã«çµåˆã—ã¦ã„ã‚‹ç”»åƒãŒã‚ã£ãŸã‚Šã€éŽåº¦ã«åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹ç”»åƒãŒã‚ã£ãŸã‚Šã™ã‚‹ã®ã§æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ vol1 ã ã„ãŸã„2000æžšãã‚‰ã„ã§ã€å¤šåˆ†å…¨éƒ¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚¿ã‚¤ãƒ«ã®ã‚‚ã®ã§ã™ã€‚ è§£ç­”ã™ã‚‹ã¨ä¸­ã«LAION Aesthetic v2ã®ã‚¹ã‚³ã‚¢ã§ã„ãã¤ã‹ã®ãƒ•ã‚©ãƒ«ãƒ€ã«åˆ†é¡žã•ã‚Œã¦ã„ã¾ã™ã€‚aesthetic_50 ãªã‚‰ã‚¹ã‚³ã‚¢0.5ä»¥ä¸Šã®ã‚‚ã®ã§ã™ã€‚not_aesthetic ã¯0.5æœªæº€ã®ã‚‚ã®ã§ã™ã€‚ ãŸã ã—ã€exceptional ãƒ•ã‚©ãƒ«ãƒ€ã¯ãƒã‚§ãƒªãƒ¼ãƒ”ãƒƒã‚¯ã—ãŸç”»åƒãŒå…¥ã£ã¦ãŠã‚Šã€aesthetic_xx ã®ä¸­ã®ã‚‚ã®ã¨é‡è¤‡ã—ã¾ã™ã€‚excludeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/p1atdev/niji-v5.
P1ayer-1/chatgpt-conversations-chatlogs.net,,,,,,,https://huggingface.co/datasets/P1ayer-1/chatgpt-conversations-chatlogs.net,,,,,,,,,,,,,,,,,,,,"ChatGPT Conversations from Chatlogs.net This dataset contains 89,288 conversations conversations between users and ChatGPT. Version 1 contains all conversations available up to the cutoff date of April 4, 2023. Version 2 contains all conversations available up to the cutoff date of April 20, 2023. Source Data The conversations were scraped from the website Chatlogs.net. The data was generated using a custom scraper that can be found here:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/P1ayer-1/chatgpt-conversations-chatlogs.net."
p208p2002/csl-electrical-engineering,,,,,,,https://huggingface.co/datasets/p208p2002/csl-electrical-engineering,,,,,,,,,,,,,,,,,,,,"csl-electrical-engineering"" ç”±CSLæ•¸æ“šé›†åˆ†å‰²å‡ºä¾†çš„é›»æ©Ÿå·¥ç¨‹(Electrical Engineering)å­é›†ï¼Œæä¾›ç°¡ç¹å…©ç¨®ç‰ˆæœ¬ã€‚ from datasets import load_dataset dataset = load_dataset(""p208p2002/csl-electrical-engineering"",""zh-cn"") dataset = load_dataset(""p208p2002/csl-electrical-engineering"",""zh-tw"")"
P3ps/condition_to_drug,,,,,,,https://huggingface.co/datasets/P3ps/condition_to_drug,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""condition_to_drug"" More Information needed"
pacovaldez/stackoverflow-questions,Text,,,,,,https://huggingface.co/datasets/pacovaldez/stackoverflow-questions,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,2,,,,,,,,,"Dataset Card for [Stackoverflow Post Questions] Dataset Description Companies that sell Open-source software tools usually hire an army of Customer representatives to try to answer every question asked about their tool. The first step in this process is the prioritization of the question. The classification scale usually consists of 4 values, P0, P1, P2, and P3, with different meanings across every participant in the industry. On the other hand, every softwareâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pacovaldez/stackoverflow-questions."
PaDaS-Lab/webfaq,,,,,,https://arxiv.org/abs/2502.20936,https://huggingface.co/datasets/PaDaS-Lab/webfaq,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,WebFAQ Q&A Dataset Overview | Details | Structure |
pain/Arabic-Tweets,Text,General,General,Text,"Accuracy, F1 Score",https://ieee-dataport.org/open-access/masc-massive-arabic-speech-corpus,https://huggingface.co/datasets/pain/Arabic-Tweets,,41 GB,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,data of Arabic Tweets with nearly 4-billion Arabic words (12-million unique Arabic words),,"BERT, RoBERTa, T5",,,Dataset Card for Dataset Arabic-Tweets Data-Summary: This dataset has been collected from twitter which is more than 41 GB of clean data of Arabic Tweets with nearly 4-billion Arabic words (12-million unique Arabic words). Languages Arabic Source Data Twitter
papluca/language-identification,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/papluca/language-identification,Identification dataset is a collection of 90k samples consisting of text passages and corresponding language label,,,,,,,https://choosealicense.com/licenses/undefined/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Language Identification dataset Data-Summary: The Language Identification dataset is a collection of 90k samples consisting of text passages and corresponding language label. This dataset was created by collecting data from 3 sources: Multilingual Amazon Reviews Corpus, XNLI, and STSb Multi MT. Supported Tasks and Leaderboards The dataset can be used to train a model for language identification, which is a multi-class textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/papluca/language-identification."
Parikshith/snli_translated_en_fr,,,,,,,https://huggingface.co/datasets/Parikshith/snli_translated_en_fr,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""snli_translated_en_fr"" More Information needed"
ParlAI/blended_skill_talk,Text,General,General,Text,"Accuracy, F1 Score",https://parl.ai/projects/bst/,https://huggingface.co/datasets/ParlAI/blended_skill_talk,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""blended_skill_talk"" Data-Summary: A dataset of 7k conversations explicitly designed to exhibit multiple conversation modes: displaying personality, having empathy, and demonstrating knowledge. Supported Tasks and Leaderboards More Information Needed Languages More Information Needed Dataset Structure Data Instances default Size of downloaded dataset files: 38.11 MB Sizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ParlAI/blended_skill_talk."
parrotzone/sdxl-1.0,,,,,,,https://huggingface.co/datasets/parrotzone/sdxl-1.0,,,,,,,,https://choosealicense.com/licenses/openrail++/,,,,,,,,,,,,check sdxl.parrotzone.art for easy viewing â‹†ï½¡Â°âœ© all images were made with SDXL 1.0 + the 0.9 VAE steps: 20 cfg scale: 7 no refiner random seeds
Patil/Marathi_voices,,,,,,,https://huggingface.co/datasets/Patil/Marathi_voices,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""Marathi_voices"" More Information needed"
patriziobellan/PET,Text,,,,,https://pdi.fbk.eu/pet-dataset/,https://huggingface.co/datasets/patriziobellan/PET,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Abstract. Although there is a long tradition of work in NLP on extracting entities and relations from text, to date there exists little work on the acquisition of business processes from unstructured data such as textual corpora of process descriptions. With this work we aim at filling this gap and establishing the first steps towards bridging data-driven information extraction methodologies from Natural Language Processing and the model-based formalization that is aimed from Business Process Management. For this, we develop the first corpus of business process descriptions annotated with activities, gateways, actors and flow information. We present our new resource, including a detailed overview of the annotation schema and guidelines, as well as a variety of baselines to benchmark the difficulty and challenges of business process extraction from text."
Paul/hatecheck-portuguese,,,,,,https://arxiv.org/abs/2206.09917,https://huggingface.co/datasets/Paul/hatecheck-portuguese,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for Multilingual HateCheck Dataset Description Multilingual HateCheck (MHC) is a suite of functional tests for hate speech detection models in 10 different languages: Arabic, Dutch, French, German, Hindi, Italian, Mandarin, Polish, Portuguese and Spanish. For each language, there are 25+ functional tests that correspond to distinct types of hate and challenging non-hate. This allows for targeted diagnostic insights into model performance. For moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Paul/hatecheck-portuguese."
PaulAdversarial/all_news_finance_sm_1h2023,,,,,,,https://huggingface.co/datasets/PaulAdversarial/all_news_finance_sm_1h2023,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,PaulAdversarial/all_news_finance_sm_1h2023 dataset hosted on Hugging Face and contributed by the HF Datasets community
paulkm/chinese_conversation_and_spam,,,,,,,https://huggingface.co/datasets/paulkm/chinese_conversation_and_spam,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Caution! This dataset contains explicit language and fraud information. Use at your own risk! For AutoTrain use: please select Text Classification (Binary) as Task. What is included conversations in chinese under tag 0 spam conversations under tag1 Where does the data come from part of the data came from conversations in Chinese Telegram groups part of them are from logging channels of anti-spam bots How many data is included Aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/paulkm/chinese_conversation_and_spam.
paulofinardi/OIG_small_chip2_portuguese_brasil,,,,,,,https://huggingface.co/datasets/paulofinardi/OIG_small_chip2_portuguese_brasil,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""OIG_small_chip2_portuguese_brasil"" This dataset was translated to Portuguese-Brasil from here The data was translated with MarianMT model and weights Helsinki-NLP/opus-mt-en-ROMANCE The full details to replicate the translation are here: translation_notebook license: apache-2.0"
Pavithree/eli5_split,,,,,,,https://huggingface.co/datasets/Pavithree/eli5_split,,,,,,,,,,,,,,,,,,,,This dataset is the subset of original eli5 dataset available in hugging face space
pawan2411/srl_train_distinct,,,,,,,https://huggingface.co/datasets/pawan2411/srl_train_distinct,,,,,,,,,,,,,,,,,,,,pawan2411/srl_train_distinct dataset hosted on Hugging Face and contributed by the HF Datasets community
pbwt/all-thai,,,,,,,https://huggingface.co/datasets/pbwt/all-thai,,,,,,,,,,,,,,,,,,,,pbwt/all-thai dataset hosted on Hugging Face and contributed by the HF Datasets community
pcuenq/oxford-pets,,,,,,,https://huggingface.co/datasets/pcuenq/oxford-pets,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"Oxford-IIIT Pet Dataset Images from The Oxford-IIIT Pet Dataset. Only images and labels have been pushed, segmentation annotations were ignored. Homepage: https://www.robots.ox.ac.uk/~vgg/data/pets/ License: Same as the original dataset."
pdearena/NavierStokes-2D,,,,,,,https://huggingface.co/datasets/pdearena/NavierStokes-2D,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,pdearena/NavierStokes-2D dataset hosted on Hugging Face and contributed by the HF Datasets community
pdjewell/sommeli_ai,,,,,,,https://huggingface.co/datasets/pdjewell/sommeli_ai,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""sommeli_ai"" More Information needed"
pedramaa/arabic-llm-egyption,,,,,,,https://huggingface.co/datasets/pedramaa/arabic-llm-egyption,,,,,,,,https://choosealicense.com/licenses/gpl/,,,,,,,,,,,,pedramaa/arabic-llm-egyption dataset hosted on Hugging Face and contributed by the HF Datasets community
peluz/lener_br,Text,,,,,https://cic.unb.br/~teodecampos/LeNER-Br/,https://huggingface.co/datasets/peluz/lener_br,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"LeNER-Br is a Portuguese language dataset for named entity recognition applied to legal documents. LeNER-Br consists entirely of manually annotated legislation and legal cases texts and contains tags for persons, locations, time entities, organizations, legislation and legal cases. To compose the dataset, 66 legal documents from several Brazilian Courts were collected. Courts of superior and state levels were considered, such as Supremo Tribunal Federal, Superior Tribunal de JustiÃ§a, Tribunal de JustiÃ§a de Minas Gerais and Tribunal de Contas da UniÃ£o. In addition, four legislation documents were collected, such as ""Lei Maria da Penha"", giving a total of 70 documents"
Penicbird/ChukJunGyeong,,,,,,,https://huggingface.co/datasets/Penicbird/ChukJunGyeong,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Penicbird/ChukJunGyeong dataset hosted on Hugging Face and contributed by the HF Datasets community
peoples-daily-ner/peoples_daily_ner,Text,,,,,https://github.com/OYE93/Chinese-NLP-Corpus/tree/master/NER/People's%20Daily,https://huggingface.co/datasets/peoples-daily-ner/peoples_daily_ner,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"People's Daily NER Dataset is a commonly used dataset for Chinese NER, with text from People's Daily (äººæ°‘æ—¥æŠ¥), the largest official newspaper. The dataset is in BIO scheme. Entity types are: PER (person), ORG (organization) and LOC (location)."
persiannlp/parsinlu_translation_fa_en,Text,,,,,https://github.com/persiannlp/parsinlu/,https://huggingface.co/datasets/persiannlp/parsinlu_translation_fa_en,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,A Persian translation dataset (Persian -> English).
PeterBrendan/Ads_Creative_Text_Programmatic,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/PeterBrendan/Ads_Creative_Text_Programmatic,processingâ€¦ See the full description on the dataset page: https://huggingface,,1000,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",atic Ad Creatives dataset contains 1000 samples of online programmatic ad creatives along with their ad sizes,,"Data-Summary: The Programmatic Ad Creatives dataset contains 1000 samples of online programmatic ad creatives along with their ad sizes. The dataset includes 8 unique ad sizes, such as (300, 250), (728, 90), (970, 250), (300, 600), (160, 600), (970, 90), (336, 280), and (320, 50). The dataset is in a tabular format and represents a random sample from Project300x250.com's complete creative data set. It is primarily used for training and evaluating natural language processingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PeterBrendan/Ads_Creative_Text_Programmatic."
PeterPanTheGenius/CUHK-PEDES,,,,,,,https://huggingface.co/datasets/PeterPanTheGenius/CUHK-PEDES,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""CUHK-PEDES"" More Information needed"
PetraAI/PetraAI,,,,,,,https://huggingface.co/datasets/PetraAI/PetraAI,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"PETRA Overview PETRA is a multilingual dataset for training and evaluating AI systems on a diverse range of tasks across multiple modalities. It contains data in Arabic and English for tasks including translation, summarization, question answering, and more. Dataset Structure Data is separated by language into /ar and /en directories Within each language directory, data is separated by task into subdirectories Tasks include: Translationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PetraAI/PetraAI."
pfb30/multi_woz_v22,Text,,,,,https://arxiv.org/abs/1810.00278,https://huggingface.co/datasets/pfb30/multi_woz_v22,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. MultiWOZ 2.1 (Eric et al., 2019) identified and fixed many erroneous annotations and user utterances in the original version, resulting in an improved version of the dataset. MultiWOZ 2.2 is a yet another improved version of this dataset, which identifies and fizes dialogue state annotation errors across 17.3% of the utterances on top of MultiWOZ 2.1 and redefines the ontology by disallowing vocabularies of slots with a large number of possible values (e.g., restaurant name, time of booking) and introducing standardized slot span annotations for these slots."
pharaouk/dharma-1,,,,,,,https://huggingface.co/datasets/pharaouk/dharma-1,,,,,,,,,,,,,,,,,,,,"""Dharma-1"" A new carefully curated benchmark set, designed for a new era where the true end user uses LLM's for zero-shot and one-shot tasks, for a vast majority of the time. Stop training your models on mindless targets (eval_loss, train_loss), start training your LLM on lightweight Dharma as an eval target. A mix of all the top benchmarks. Formed to have an equal distribution of some of the most trusted benchmarks used by those developing SOTA LLMs, comprised of only 3,000â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pharaouk/dharma-1."
phihung/titanic,,,,,,,https://huggingface.co/datasets/phihung/titanic,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,The legendary Titanic dataset from this Kaggle competition
PhilipMay/stsb_multi_mt,Image-Text,Translation,General,Text,"BLEU, METEOR, TER",https://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark,https://huggingface.co/datasets/PhilipMay/stsb_multi_mt,English,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"T5, mBART, M2M100",,,"Dataset Card for STSb Multi MT Data-Summary: STS Benchmark comprises a selection of the English datasets used in the STS tasks organized in the context of SemEval between 2012 and 2017. The selection of datasets include text from image captions, news headlines and user forums. (source) These are different multilingual translations and the English original of the STSbenchmark dataset. Translation has been done with deepl.com. It can be used to train sentenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PhilipMay/stsb_multi_mt."
philippemo/dummy_dataset_with_schema,,,,,,,https://huggingface.co/datasets/philippemo/dummy_dataset_with_schema,,,,,,,,,,,,,,,,,,,,philippemo/dummy_dataset_with_schema dataset hosted on Hugging Face and contributed by the HF Datasets community
PhilSad/celeba-hq-1.5k,,,,,,,https://huggingface.co/datasets/PhilSad/celeba-hq-1.5k,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""celeba-hq-1.5k"" More Information needed"
philschmid/amazon-product-descriptions-vlm,,,,,,,https://huggingface.co/datasets/philschmid/amazon-product-descriptions-vlm,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,Amazon Multimodal Product dataset This is a modfied and slim verison of bprateek/amazon_product_description helpful to get started training multimodal LLMs. The description field was generated used Gemini Flash.
phiwi/bbaw_egyptian,Text,Translation,General,Text,"BLEU, METEOR, TER",https://edoc.bbaw.de/frontdoor/index/index/docId/2919,https://huggingface.co/datasets/phiwi/bbaw_egyptian,Translating Middle Egyptian Hieroglyph,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"T5, mBART, M2M100",,,"Dataset Card for ""bbaw_egyptian"" Data-Summary: This dataset comprises parallel sentences of hieroglyphic encodings, transcription and translation as used in the paper Multi-Task Modeling of Phonographic Languages: Translating Middle Egyptian Hieroglyph. The data triples are extracted from the digital corpus of Egyptian texts compiled by the project ""Strukturen und Transformationen des Wortschatzes der Ã¤gyptischen Sprache"". Supported Tasks andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/phiwi/bbaw_egyptian."
phiyodr/coco2017,,,,,,,https://huggingface.co/datasets/phiyodr/coco2017,,,,,,,,,,,,,,,,,,,,"coco2017 Image-text pairs from MS COCO2017. Data origin Data originates from cocodataset.org While coco-karpathy uses a dense format (with several sentences and sendids per row), coco-karpathy-long uses a long format with one sentence (aka caption) and sendid per row. coco-karpathy-long uses the first five sentences and therefore is five times as long as coco-karpathy. phiyodr/coco2017: One row corresponds one image with several sentences. phiyodr/coco2017-long:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/phiyodr/coco2017."
photonsquid/coins-euro,,,,,,,https://huggingface.co/datasets/photonsquid/coins-euro,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,photonsquid/coins-euro dataset hosted on Hugging Face and contributed by the HF Datasets community
pierreguillou/DocLayNet-base,Text,,,,,https://developer.ibm.com/exchanges/data/all/doclaynet/,https://huggingface.co/datasets/pierreguillou/DocLayNet-base,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"Accurate document layout analysis is a key requirement for high-quality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present \textit{DocLayNet}, a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide smallline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10\% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNet-trained models are more robust and thus the preferred choice for general-purpose document-layout analysis."
pietrolesci/add_one_rte,,,,,,,https://huggingface.co/datasets/pietrolesci/add_one_rte,,,,,,,,,,,,,,,,,,,,"Overview Original data available here. Dataset curation premise and hypothesis columns have been cleaned following common practices (1, 2), that is remove HTML tags <b>, <u>, </b>, </u> normalize repeated white spaces strip mean_human_score has been transformed into class labels following common practices (1, 2), that is for test set: mean_human_score <= 3 -> ""not-entailed"" and mean_human_score >= 4 -> ""entailed"" (anything between 3 and 4 has been removed) forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pietrolesci/add_one_rte."
pile-of-law/pile-of-law,Text,,,,,https://huggingface.co/datasets/pile-of-law/pile-of-law,https://huggingface.co/datasets/pile-of-law/pile-of-law,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"We curate a large corpus of legal and administrative data. The utility of this data is twofold: (1) to aggregate legal and administrative data sources that demonstrate different norms and legal standards for data filtering; (2) to collect a dataset that can be used in the future for pretraining legal-domain language models, a key direction in access-to-justice initiatives."
pinecone/movielens-recent-ratings,,,,,,,https://huggingface.co/datasets/pinecone/movielens-recent-ratings,,,,,,,,https://choosealicense.com/licenses/undefined/,,,,,,,,,,,,This dataset streams recent user ratings from the MovieLens 25M dataset and adds poster URLs.
Pinkstack/thinking-multilingual-30-23-small-690,,,,,,,https://huggingface.co/datasets/Pinkstack/thinking-multilingual-30-23-small-690,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Based on OPENO1 math, this is a translated dataset of 30 high quality questions & answers in 23 languages. Or use the ""big"" version: big 10k rows version"
piuba-bigdata/articles_and_comments,,,,,,,https://huggingface.co/datasets/piuba-bigdata/articles_and_comments,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""articles_and_comments"" More Information needed"
pkavumba/balanced-copa,Text,Reasoning,General,Text,"Accuracy, F1 Score",https://balanced-copa.github.io/,https://huggingface.co/datasets/pkavumba/balanced-copa,Dataset for Training Robust Commonsense Causal Reasoning Models The Balanced Choice of Plausible Alternatives dataset is a benchmark for training machine learning models that are robust to superficial cues/spurious correlations,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"T5, UnifiedQA, BART",,,"Dataset Card for ""Balanced COPA"" Data-Summary: Bala-COPA: An English language Dataset for Training Robust Commonsense Causal Reasoning Models The Balanced Choice of Plausible Alternatives dataset is a benchmark for training machine learning models that are robust to superficial cues/spurious correlations. The dataset extends the COPA dataset(Roemmele et al. 2011) with mirrored instances that mitigate against token-level superficial cues in the original COPAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pkavumba/balanced-copa."
PKU-Alignment/PKU-SafeRLHF,Text,,,,,https://arxiv.org/abs/2406.15513,https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Dataset Card for PKU-SafeRLHF Warning: this dataset contains data that may be offensive or harmful. The data are intended for research purposes, especially research that can make models less harmful. The views expressed in the data do not reflect the views of PKU-Alignment Team or any of its members. [ðŸ  Homepage] [ðŸ¤— Single Dimension Preference Dataset] [ðŸ¤— Q-A Dataset] [ðŸ¤— Prompt Dataset] Citation If PKU-SafeRLHF has contributed to your work, please considerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF."
pkufool/libriheavy,,,,,,https://arxiv.org/abs/2309.08105,https://huggingface.co/datasets/pkufool/libriheavy,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Libriheavy: a 50,000 hours ASR corpus with punctuation casing and context Libriheavy is a labeled version of Librilight, read our paper for more details. See https://github.com/k2-fsa/libriheavy for more details. Citation @misc{kang2023libriheavy, title={Libriheavy: a 50,000 hours ASR corpus with punctuation casing and context}, author={Wei Kang and Xiaoyu Yang and Zengwei Yao and Fangjun Kuang and Yifan Yang and Liyong Guo and Long Lin and Danielâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pkufool/libriheavy."
PlanTL-GOB-ES/cantemist-ner,Text,,,,,,https://huggingface.co/datasets/PlanTL-GOB-ES/cantemist-ner,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,https://temu.bsc.es/cantemist/
PleIAs/YouTube-Commons,,,,,,,https://huggingface.co/datasets/PleIAs/YouTube-Commons,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"ðŸ“º YouTube-Commons ðŸ“º YouTube-Commons is a collection of audio transcripts of 2,063,066 videos shared on YouTube under a CC-By license. Content The collection comprises 22,709,724 original and automatically translated transcripts from 3,156,703 videos (721,136 individual channels). In total, this represents nearly 45 billion words (44,811,518,375). All the videos where shared on YouTube with a CC-BY license: the dataset provide all the necessary provenanceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PleIAs/YouTube-Commons."
pleisto/wikipedia-cn-20230720-filtered,,,,,,,https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"æœ¬æ•°æ®é›†åŸºäºŽä¸­æ–‡ç»´åŸº2023å¹´7æœˆ20æ—¥çš„dumpå­˜æ¡£ã€‚ä½œä¸ºä¸€é¡¹ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„å·¥ä½œï¼Œæœ¬æ•°æ®é›†ä»…ä¿ç•™äº† 254,547æ¡ è´¨é‡è¾ƒé«˜çš„è¯æ¡å†…å®¹ã€‚å…·ä½“è€Œè¨€ï¼š è¿‡æ»¤äº†Template, Category, Wikipedia, File, Topic, Portal, MediaWiki, Draft, Helpç­‰ç‰¹æ®Šç±»åž‹çš„è¯æ¡ ä½¿ç”¨å¯å‘å¼çš„æ–¹æ³•å’Œè‡ªæœ‰çš„NLUæ¨¡åž‹è¿‡æ»¤äº†ä¸€éƒ¨åˆ†è´¨é‡è¾ƒä½Žçš„è¯æ¡ è¿‡æ»¤äº†ä¸€éƒ¨åˆ†å†…å®¹è¾ƒä¸ºæ•æ„Ÿæˆ–å­˜åœ¨äº‰è®®æ€§çš„è¯æ¡ã€‚ è¿›è¡Œäº†ç®€ç¹è½¬æ¢å’Œä¹ æƒ¯ç”¨è¯è½¬æ¢ï¼Œç¡®ä¿ç¬¦åˆä¸­å›½å¤§é™†åœ°åŒºçš„ä¹ æƒ¯ç”¨è¯ã€‚ This dataset is based on the Chinese Wikipedia dump archive from July 20th, 2023. As a data-centric effort, the dataset retains 254,574 high-quality entries. Specifically: Entries of special types such as Template, Category, Wikipedia, File, Topicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered."
pmc/open_access,Text,,,,,https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/,https://huggingface.co/datasets/pmc/open_access,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"The PMC Open Access Subset includes more than 3.4 million journal articles and preprints that are made available under license terms that allow reuse. Not all articles in PMC are available for text mining and other reuse, many have copyright protection, however articles in the PMC Open Access Subset are made available under Creative Commons or similar licenses that generally allow more liberal redistribution and reuse than a traditional copyrighted work. The PMC Open Access Subset is one part of the PMC Article Datasets"
pmoe7/SP_500_Stocks_Data-ratios_news_price_10_yrs,,,,,,,https://huggingface.co/datasets/pmoe7/SP_500_Stocks_Data-ratios_news_price_10_yrs,,,,,,,,,,,,,,,,,,,,"Hi folks, Here is a collection of data I have scraped or aggregated for most of the stocks in the S&P 500, including popular ones like Apple (AAPL). It has the following data: Daily news articles and sentiments on those articles collected over the last few years. All quarterly stock fundamentals (ratios) for 10-20 years. Stock price data (daily close) over the last 10-20 years. Use it however you please for PERSONAL USAGE, but if you do leverage it to make some money; just remember me andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pmoe7/SP_500_Stocks_Data-ratios_news_price_10_yrs."
PNLPhub/snappfood-sentiment-analysis,,,,,,,https://huggingface.co/datasets/PNLPhub/snappfood-sentiment-analysis,,,,,,,,,,,,,,,,,,,,PNLPhub/snappfood-sentiment-analysis dataset hosted on Hugging Face and contributed by the HF Datasets community
poloclub/diffusiondb,Text,,,,,https://poloclub.github.io/diffusiondb,https://huggingface.co/datasets/poloclub/diffusiondb,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,https://github.com/poloclub/diffusiondb/blob/main/notebooks/example-loading.ipynb,,,,,,,,"DiffusionDB is the first large-scale text-to-image prompt dataset. It contains 2 million images generated by Stable Diffusion using prompts and hyperparameters specified by real users. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generative models, detecting deepfakes, and designing human-AI interaction tools to help users more easily use these models."
PolyAI/banking77,Text,,,,,https://github.com/PolyAI-LDN/task-specific-datasets,https://huggingface.co/datasets/PolyAI/banking77,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"BANKING77 dataset provides a very fine-grained set of intents in a banking domain. It comprises 13,083 customer service queries labeled with 77 intents. It focuses on fine-grained single-domain intent detection."
PoolC/1-fold-clone-detection-600k-5fold,,,,,,,https://huggingface.co/datasets/PoolC/1-fold-clone-detection-600k-5fold,,,,,,,,,,,,,,,,,,,,PoolC/1-fold-clone-detection-600k-5fold dataset hosted on Hugging Face and contributed by the HF Datasets community
Porameht/processed-voice-th-169k,,,,,,,https://huggingface.co/datasets/Porameht/processed-voice-th-169k,,,,,,,,,,,,,,,,,,,,Porameht/processed-voice-th-169k dataset hosted on Hugging Face and contributed by the HF Datasets community
PORTULAN/parlamento-pt,Text,General,Document,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2305.06721,https://huggingface.co/datasets/PORTULAN/parlamento-pt,data set obtained by collecting publicly available documents containing transcriptions of debates in the Portuguese Parliament,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ParlamentoPT Data-Summary: The ParlamentoPT is a Portuguese language data set obtained by collecting publicly available documents containing transcriptions of debates in the Portuguese Parliament. The data was collected from the Portuguese Parliament portal in accordance with its open data policy. This dataset was collected with the purpose of creating the Albertina-PT* language model, and it serves as training data for model development. Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PORTULAN/parlamento-pt."
potsawee/wiki_bio_gpt3_hallucination,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2303.08896,https://huggingface.co/datasets/potsawee/wiki_bio_gpt3_hallucination,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for WikiBio GPT-3 Hallucination Dataset GitHub repository: https://github.com/potsawee/selfcheckgpt Paper: SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models Data-Summary: We generate Wikipedia-like passages using GPT-3 (text-davinci-003) using the prompt: This is a Wikipedia passage about {concept} where concept represents an individual from the WikiBio dataset. We split the generated passages intoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/potsawee/wiki_bio_gpt3_hallucination.
PowerInfer/QWQ-LONGCOT-500K,,,,,,,https://huggingface.co/datasets/PowerInfer/QWQ-LONGCOT-500K,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"This repository contains approximately 500,000 instances of responses generated using QwQ-32B-Preview language model. The dataset combines prompts from multiple high-quality sources to create diverse and comprehensive training data. The dataset is available under the Apache 2.0 license. Over 75% of the responses exceed 8,000 tokens in length. The majority of prompts were carefully created using persona-based methods to create challenging instructions. Bias, Risks, and Limitationsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PowerInfer/QWQ-LONGCOT-500K."
practical-dreamer/RPGPT_PublicDomain-alpaca,,,,,,,https://huggingface.co/datasets/practical-dreamer/RPGPT_PublicDomain-alpaca,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Experimental Synthetic Dataset of Public Domain Character Dialogue in Roleplay Format Generated using scripts from my https://github.com/practicaldreamer/build-a-dataset repo license: mit
Pranavkpba2000/skin_cancer_dataset,,,,,,,https://huggingface.co/datasets/Pranavkpba2000/skin_cancer_dataset,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""skin_cancer_dataset"" More Information needed"
pranked03/flowers-blip-captions,,,,,,,https://huggingface.co/datasets/pranked03/flowers-blip-captions,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""flowers-blip-captions"" More Information needed"
PranomVignesh/PoliceWithVest-vs-Public,,,,,,,https://huggingface.co/datasets/PranomVignesh/PoliceWithVest-vs-Public,,,,,,,,,,,,,,,,,,,,PranomVignesh/PoliceWithVest-vs-Public dataset hosted on Hugging Face and contributed by the HF Datasets community
prasadsawant7/sentiment_analysis_preprocessed_dataset,,,,,,,https://huggingface.co/datasets/prasadsawant7/sentiment_analysis_preprocessed_dataset,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Brief idea about dataset: This dataset is designed for a Text Classification to be specific Multi Class Classification, inorder to train a model (Supervised Learning) for Sentiment Analysis. Also to be able retrain the model on the given feedback over a wrong predicted sentiment this dataset will help to manage those things using Other Features. Main Features text labels This feature variable has all sort of texts, sentences, tweets, etc. This target variable contains 3 types ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prasadsawant7/sentiment_analysis_preprocessed_dataset."
PrimeQA/TechQA,,,,,,,https://huggingface.co/datasets/PrimeQA/TechQA,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,PrimeQA/TechQA dataset hosted on Hugging Face and contributed by the HF Datasets community
PrinceAyush/Mental_Health_conv,,,,,,,https://huggingface.co/datasets/PrinceAyush/Mental_Health_conv,,,,,,,,,,,,,,,,,,,,PrinceAyush/Mental_Health_conv dataset hosted on Hugging Face and contributed by the HF Datasets community
princeton-nlp/SWE-bench,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2310.06770,https://huggingface.co/datasets/princeton-nlp/SWE-bench,Models Resolve Real-World GitHub Issues?,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Data-Summary: SWE-bench is a dataset that tests systemsâ€™ ability to solve GitHub issues automatically. The dataset collects 2,294 Issue-Pull Request pairs from 12 popular Python repositories. Evaluation is performed by unit test verification using post-PR behavior as the reference solution. The dataset was released as part of SWE-bench: Can Language Models Resolve Real-World GitHub Issues? Want to run inference now? This dataset only contains the problem_statementâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/princeton-nlp/SWE-bench."
prithivMLmods/Deepfakes-QA-Leaning,,,,,,,https://huggingface.co/datasets/prithivMLmods/Deepfakes-QA-Leaning,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Deepfake Quality Assessment Deepfake QA is a Deepfake Quality Assessment model designed to analyze the quality of deepfake images & videos. It evaluates whether a deepfake is of good or bad quality, where: 0 represents a bad-quality deepfake 1 represents a good-quality deepfake This classification serves as the foundation for training models on deepfake quality assessment, helping improve deepfake detection and enhancement techniques. Citationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Deepfakes-QA-Leaning."
priyank-m/SROIE_2019_text_recognition,,,,,,,https://huggingface.co/datasets/priyank-m/SROIE_2019_text_recognition,,,,,,,,https://choosealicense.com/licenses/undefined/,,,,,,,,,,,,This dataset we prepared using the Scanned receipts OCR and information extraction(SROIE) dataset. The SROIE dataset contains 973 scanned receipts in English language. Cropping the bounding boxes from each of the receipts to generate this text-recognition dataset resulted in 33626 images for train set and 18704 images for the test set. The text annotations for all the images inside a split are stored in a metadata.jsonl file. usage: from dataset import load_dataset data =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/priyank-m/SROIE_2019_text_recognition.
profetize/kirsten_v4,,,,,,,https://huggingface.co/datasets/profetize/kirsten_v4,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""kirsten_v4"" More Information needed"
ProGamerGov/StableDiffusion-v1-5-Regularization-Images,,,,,,,https://huggingface.co/datasets/ProGamerGov/StableDiffusion-v1-5-Regularization-Images,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"A collection of regularization / class instance datasets for the Stable Diffusion v1-5 model to use for DreamBooth prior preservation loss training. Files labeled with ""mse vae"" used the stabilityai/sd-vae-ft-mse VAE. For ease of use, datasets are stored as zip files containing 512x512 PNG images. The number of images in each zip file is specified at the end of the filename. There is currently a bug where HuggingFace is incorrectly reporting that the datasets are pickled. They are not pickedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ProGamerGov/StableDiffusion-v1-5-Regularization-Images."
prognosis/cardio-docs-qa,,,,,,,https://huggingface.co/datasets/prognosis/cardio-docs-qa,,,,,,,,,,,,,,,,,,,,prognosis/cardio-docs-qa dataset hosted on Hugging Face and contributed by the HF Datasets community
ProgramComputer/voxceleb,,,,,,https://www.robots.ox.ac.uk/~vgg/data/voxceleb/,https://huggingface.co/datasets/ProgramComputer/voxceleb,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"This dataset includes both VoxCeleb and VoxCeleb2 Multipart Zips Already joined zips for convenience but these specified files are NOT part of the original datasets vox2_mp4_1.zip - vox2_mp4_6.zip vox2_aac_1.zip - vox2_aac_2.zip Joining Zip cat vox1_dev* > vox1_dev_wav.zip cat vox2_dev_aac* > vox2_aac.zip cat vox2_dev_mp4* > vox2_mp4.zip Citation Information @article{Nagrani19, author = ""Arsha Nagrani and Joon~Son Chung and Weidi Xie andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ProgramComputer/voxceleb."
Programming-Language/codeagent-python,,,,,,,https://huggingface.co/datasets/Programming-Language/codeagent-python,,,,,,,,,,,,,,,,,,,,Programming-Language/codeagent-python dataset hosted on Hugging Face and contributed by the HF Datasets community
proj-persona/PersonaHub,,,,,,https://arxiv.org/abs/2406.20094,https://huggingface.co/datasets/proj-persona/PersonaHub,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"Scaling Synthetic Data Creation with 1,000,000,000 Personas This repo releases data introduced in our paper Scaling Synthetic Data Creation with 1,000,000,000 Personas: We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce PERSONA HUB â€“ a collection of 1 billion diverse personas automatically curated from web data.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/proj-persona/PersonaHub."
project-sloth/captcha-images,,,,,,,https://huggingface.co/datasets/project-sloth/captcha-images,,,,,,,,https://choosealicense.com/licenses/wtfpl/,,,,,,,,,,,,Captcha images dataset.
projectlosangeles/Los-Angeles-MIDI-Dataset,,,,,,,https://huggingface.co/datasets/projectlosangeles/Los-Angeles-MIDI-Dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,Los Angeles MIDI Dataset SOTA kilo-scale MIDI dataset for MIR and Music AI purposes Search and Explore Los Angeles MIDI dataset [NEW] Master MIDI Dataset GPU Search and Filter Master MIDI Dataset Search and Filter Make your own Los Angeles MIDI Dataset from any MIDI scrape Make your own Los Angeles MIDI Dataset Metadata Los Angeles MIDI Dataset is now avaialable forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/projectlosangeles/Los-Angeles-MIDI-Dataset.
prometheus-eval/Feedback-Collection,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/prometheus-eval/Feedback-Collection,models,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card Data-Summary: The Feedback Collection is a dataset designed to induce fine-grained evaluation capabilities into language models.\ Recently, proprietary LLMs (e.g., GPT-4) have been used to evaluate long-form responses. In our experiments, we found that open-source LMs are not capable of evaluating long-form responses, showing low correlation with both human evaluators and GPT-4.\ In our paper, we found that by (1) fine-tuning feedback generated byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prometheus-eval/Feedback-Collection."
promptfoo/CCP-sensitive-prompts,,,,,,,https://huggingface.co/datasets/promptfoo/CCP-sensitive-prompts,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"CCP Sensitive Prompts These prompts cover sensitive topics in China, and are likely to be censored by Chinese models."
pscotti/mindeyev2,,,,,,,https://huggingface.co/datasets/pscotti/mindeyev2,,,,,,,,,,,,,,,,,,,,"Using these files requires that you have already agreed to the Natural Scenes Dataset's Terms and Conditions: https://cvnlab.slite.page/p/IB6BSeW_7o/Terms-and-Conditions Webdatasets only contain behavioral information, .tar filename numbering correspondings to the scanning session of the subject. (Always use ""new_test"" instead of ""test"" in the wds folders, ""test"" refers to using the old NSD data from before they released the full set of scanning sessions.) behavior numpy files correspond toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pscotti/mindeyev2."
pstuerner/ukraine-liveblog,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/pstuerner/ukraine-liveblog,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card Data-Summary: The ""ukraine-liveblog"" dataset contains a collection of news articles published on the liveblog of the popular German news website, tagesschau.de. The dataset covers the period from February 2022 to February 2023, and includes every news feed published during this time that covers the ongoing war in Ukraine. Supported Tasks and Leaderboards -- Languages The language of the dataset is German.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pstuerner/ukraine-liveblog."
psyche/bool_sentence,,,,,,,https://huggingface.co/datasets/psyche/bool_sentence,,,,,,,,,,,,,,,,,,,,Model psyche/bool_sentence (10k) klue/bert-base 0.9335 licence: cc-by-sa-2.0-kr (ì›ë³¸ ì¶œì²˜:êµ­ë¦½êµ­ì–´ì› í‘œì¤€êµ­ì–´ëŒ€ì‚¬ì „)
pszemraj/scientific_lay_summarisation-plos-norm,,,,,,https://arxiv.org/abs/2210.09932,https://huggingface.co/datasets/pszemraj/scientific_lay_summarisation-plos-norm,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,UsageToken Lengthsscientific_lay_summarisation - PLOS - normalizedThis dataset is a modified version oftomasg25/scientific_lay_summarizationand contains scientific lay summaries that have been preprocessedwith this code,,,,,,"scientific_lay_summarisation - PLOS - normalized This dataset is a modified version of tomasg25/scientific_lay_summarization and contains scientific lay summaries that have been preprocessed with this code. The preprocessing includes fixing punctuation and whitespace problems, and calculating the token length of each text sample using a tokenizer from the T5 model. Original dataset details: Repository: https://github.com/TGoldsack1/Corpora_for_Lay_Summarisation Paper: Makingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pszemraj/scientific_lay_summarisation-plos-norm."
purav/animals,,,,,,,https://huggingface.co/datasets/purav/animals,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,purav/animals dataset hosted on Hugging Face and contributed by the HF Datasets community
PurCL/malware-top-100,,,,,,,https://huggingface.co/datasets/PurCL/malware-top-100,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""malware-top-100"" More Information needed"
Pushpendra817/With-Similarity-Scores,,,,,,,https://huggingface.co/datasets/Pushpendra817/With-Similarity-Scores,,,,,,,,https://choosealicense.com/licenses/openrail/,,,80,,,,,,,,,Pushpendra817/With-Similarity-Scores dataset hosted on Hugging Face and contributed by the HF Datasets community
pvduy/oa_vicuna_dolly_grademath_alpaca_leetcode,,,,,,,https://huggingface.co/datasets/pvduy/oa_vicuna_dolly_grademath_alpaca_leetcode,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""oa_vicuna_dolly_grademath_alpaca_leetcode"" More Information needed"
pvrancx/legobricks,Image,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/pvrancx/legobricks,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for LegoBricks Data-Summary: 3D images of LEGO Parts. Dataset contains the 1000 most common LEGO parts (according to the rebrickable database). Each part has 400 images of different rotation angles and colors. Colors are sampled randomly, weighted by number of occurences for that part and color in the database. The dataset contains a train split with 1000 classes, each represented by 400 images. Class names are the LEGO part IDs. These ids can beâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pvrancx/legobricks."
pythainlp/wisesight_sentiment,Text,Sentiment Analysis,General,Label,"Accuracy, F1 Score",https://github.com/PyThaiNLP/wisesight-sentiment,https://huggingface.co/datasets/pythainlp/wisesight_sentiment,"with sentiment label (positive, neutral, negative, question) Released to public domain under Creative Commons Zero v1",,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,"BERT, RoBERTa, DistilBERT",,,"Dataset Card for wisesight_sentiment Data-Summary: Wisesight Sentiment Corpus: Social media messages in Thai language with sentiment label (positive, neutral, negative, question) Released to public domain under Creative Commons Zero v1.0 Universal license. Labels: {""pos"": 0, ""neu"": 1, ""neg"": 2, ""q"": 3} Size: 26,737 messages Language: Central Thai Style: Informal and conversational. With some news headlines and advertisement. Time period: Around 2016 to earlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/wisesight_sentiment."
qanastek/WMT-16-PubMed,Text,,,,,https://www.statmt.org/wmt16/biomedical-translation-task.html,https://huggingface.co/datasets/qanastek/WMT-16-PubMed,,,,,,,,,,,,,,,,,,,,WMT'16 Biomedical Translation Task - PubMed parallel datasets http://www.statmt.org/wmt16/biomedical-translation-task.html
qasimmunye/kidsillus,,,,,,,https://huggingface.co/datasets/qasimmunye/kidsillus,,,,,,,,,,,,,,,,,,,,qasimmunye/kidsillus dataset hosted on Hugging Face and contributed by the HF Datasets community
qbo-odp/sift1m,,,,,,,https://huggingface.co/datasets/qbo-odp/sift1m,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"sift1m sift1m data, copied from http://corpus-texmex.irisa.fr/, published: JÃ©gou H, Douze M, Schmid C. Improving bag-of-features for large scale image search[J]. International journal of computer vision, 2010, 87(3): 316-336."
qgyd2021/h_novel,,,,,,,https://huggingface.co/datasets/qgyd2021/h_novel,,,,,,,,,,,,,,,,,,,,This dataset contains some SQ novel. It is supposed to be used for text generation tasks.
qiaojin/PubMedQA,Text,General,Scientific,Text,"Accuracy, F1 Score",https://pubmedqa.github.io/,https://huggingface.co/datasets/qiaojin/PubMedQA,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for [Dataset Name] Data-Summary: The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. Supported Tasks and Leaderboards The official leaderboard is available at: https://pubmedqa.github.io/. 500 questions in the pqa_labeled are used as the test set. They can be found atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/qiaojin/PubMedQA.
qihoo360/Light-R1-SFTData,,,,,,https://arxiv.org/abs/2503.10460,https://huggingface.co/datasets/qihoo360/Light-R1-SFTData,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Light-R1: Surpassing R1-Distill from Scratch* with $1000 through Curriculum SFT & DPO *from models without long COT technical report GitHub page Here are the two-stage SFT data we used to train Light-R1-32B. Simply refer to stage1-76k.json and stage2-3k.json Model Trained From Release Date AIME24 AIME25 DeepSeek-R1-Distill-Llama-70B Llama-3.3-70B-Instruct 25.1.20 70.0 54.1 DeepSeek-R1-Distill-Qwen-32B Qwen2.5-32B 25.1.20 72.6 54.9 LIMO (32B) Qwen2.5-32B-Instruct 25.2.4â€¦ See the full description on the dataset page: https://huggingface.co/datasets/qihoo360/Light-R1-SFTData.
QingyiSi/Alpaca-CoT,,,,,,,https://huggingface.co/datasets/QingyiSi/Alpaca-CoT,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,https://github.com/sahil280114/codealpaca,,alpacaalpacaGPT4Chain-of-ThoughtCodeAlpacafinancefireflyGPT4allGPTeacherGuanacoHC3instinwildinstructNatural Instructionsprosocial dialogxP3Chinese-instruction-collectioncombinationCitationInstruction-Finetuning Dataset Collection (Alpaca-CoT)This repository will continuously collect various instruction tuning datasets,,,,,,"Instruction-Finetuning Dataset Collection (Alpaca-CoT) This repository will continuously collect various instruction tuning datasets. And we standardize different datasets into the same format, which can be directly loaded by the code of Alpaca model. We also have conducted empirical study on various instruction-tuning datasets based on the Alpaca model, as shown in https://github.com/PhoebusSi/alpaca-CoT. If you think this dataset collection is helpful to you, please likeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/QingyiSi/Alpaca-CoT."
Qoostewin/rehashed-nsfw-full,,,,,,,https://huggingface.co/datasets/Qoostewin/rehashed-nsfw-full,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Qoostewin/rehashed-nsfw-full dataset hosted on Hugging Face and contributed by the HF Datasets community
qqlu1992/Adobe_EntitySeg,,,,,,,https://huggingface.co/datasets/qqlu1992/Adobe_EntitySeg,,,,,,,,,,,,,,,,,,,,The images and pretrained-models used in the ICCV oral paper 'High-Quality Entity Segmentation'. The offical link is https://github.com/adobe-research/EntitySeg-Dataset. The code link is https://github.com/qqlu/Entity/tree/main/Entityv2. We noted that we do not own the copyright of the images. It is solely your responsibility to check the original licenses of the images before using them. Any use of the images are at your own discretion and risk.
quchenyuan/UCF101-ZIP,,,,,,,https://huggingface.co/datasets/quchenyuan/UCF101-ZIP,,,,,,,,,,,,,,,,,,,,"Introduction: The UCF-101 dataset is a widely used benchmark for action recognition in videos. The dataset contains 13,320 videos of 101 action categories, and it was created by collecting YouTube videos and using human annotators to label the action categories. However, the original UCF-101 dataset has certificate issues that may cause difficulties during the download process. Additionally, the dataset is in RAR format, which may not be convenient for some users. Therefore, we have created aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/quchenyuan/UCF101-ZIP."
quocanh34/mental_health_dataset_1,,,,,,,https://huggingface.co/datasets/quocanh34/mental_health_dataset_1,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""mental_health_dataset_1"" More Information needed"
quoctrungle/hcmus_QA_train,,,,,,,https://huggingface.co/datasets/quoctrungle/hcmus_QA_train,,,,,,,,,,,,,,,,,,,,quoctrungle/hcmus_QA_train dataset hosted on Hugging Face and contributed by the HF Datasets community
qwedsacf/ivypanda-essays,Text,General,General,Text,"Accuracy, F1 Score",https://laion.ai/,https://huggingface.co/datasets/qwedsacf/ivypanda-essays,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,Ivypanda essays Data-Summary: This dataset contains essays from ivypanda. Dataset Structure Data Fields TEXT: The text of the essay. SOURCE: A permalink to the ivypanda essay page
qwertyaditya/rick_and_morty_text_to_image,,,,,,,https://huggingface.co/datasets/qwertyaditya/rick_and_morty_text_to_image,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""rick_and_morty_text_to_image"" More Information needed"
qwopqwop/ko-arena-hard-auto-v0.1,,,,,,https://arxiv.org/abs/2406.11939,https://huggingface.co/datasets/qwopqwop/ko-arena-hard-auto-v0.1,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"ko-arena-hard-auto-v0.1ëŠ” í•œêµ­ì–´ë¥¼ ë²¤ì¹˜ë§ˆí‚¹í•˜ê¸°ìœ„í•œ ìžë™ í‰ê°€ ë„êµ¬ì˜ ì§ˆë¬¸ ë°ì´í„°ì…‹ìž…ë‹ˆë‹¤. ì¸ê°„ì˜ ì„ í˜¸ë„ì™€ ë†’ì€ ìƒê´€ê´€ê³„ì™€ ë¶„ë¦¬ë ¥ì„ ê°€ì§€ê³  ìžˆëŠ” ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì¸ arena-hard-auto-v0.1 ë¥¼ GPT-4oì™€ o1ì„ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  ìˆ˜ìž‘ì—…ìœ¼ë¡œ ê²€ìˆ˜í•œ ë°ì´í„°ì…‹ìž…ë‹ˆë‹¤. ë” ìžì„¸í•œ ì„¸ë¶€ì‚¬í•­ê³¼ ë²¤ì¹˜ë§ˆí‚¹ ê²°ê³¼ëŠ” ko-arena-hard-auto ì½”ë“œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”. ë˜í•œ ì›ë³¸ ë²¤ì¹˜ë§ˆí¬ì— ê´€ì‹¬ì´ ìžˆìœ¼ì‹œë©´ arena-hard-auto ì½”ë“œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”. ì›ëž˜ ë¬¸ì œì˜ í˜•ì‹ì„ ìœ ì§€í•˜ê¸° íž˜ë“¤ì–´ì„œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ : 1, 28, 29 ë¬¸ì œë¥¼ í•œêµ­ì–´ë¡œ ìœ ë„í•˜ê¸° ìœ„í•´ ë¬¸ì œ í˜•ì‹ì„ ë³€ê²½í–ˆìŠµë‹ˆë‹¤. ì›ëž˜ëŠ” ì½”ë“œë§Œ ì¡´ìž¬í•©ë‹ˆë‹¤. ì¸ë±ìŠ¤ : 30, 379, 190 ì°¸ê³ ë¬¸í—Œ: @article{li2024crowdsourced, title={From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/qwopqwop/ko-arena-hard-auto-v0.1."
racineai/OGC_colpali-VisRAG-vdr,,,,,,,https://huggingface.co/datasets/racineai/OGC_colpali-VisRAG-vdr,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"WIP - there might be issues with the negatives OGC - Organized, Grouped, Cleaned Intended for image/text to vector (DSE) Dataset Composition The dataset merges, shuffles, and formats data from: vidore/colpali_train_set openbmb/VisRAG-Ret-Train-Synthetic-data llamaindex/vdr-multilingual-train Dataset Statistics Metric Value Total rows 700,000+ Rows with negatives â‰ˆ 33% Rows without queries (image negatives only) â‰ˆ 25%â€¦ See the full description on the dataset page: https://huggingface.co/datasets/racineai/OGC_colpali-VisRAG-vdr."
RafaelJaime/calisthenics_exercises,,,,,,,https://huggingface.co/datasets/RafaelJaime/calisthenics_exercises,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,RafaelJaime/calisthenics_exercises dataset hosted on Hugging Face and contributed by the HF Datasets community
RafaelMPereira/HealthCareMagic-100k-Chat-Format-en,,,,,,,https://huggingface.co/datasets/RafaelMPereira/HealthCareMagic-100k-Chat-Format-en,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,RafaelMPereira/HealthCareMagic-100k-Chat-Format-en dataset hosted on Hugging Face and contributed by the HF Datasets community
rafaelpadilla/coco2017,Text,,,,,https://cocodataset.org/,https://huggingface.co/datasets/rafaelpadilla/coco2017,,,,,,,,,,,,,,,,,,,,This dataset contains all COCO 2017 images and annotations split in training (118287 images) and validation (5000 images).
rahular/itihasa,,,,,,,https://huggingface.co/datasets/rahular/itihasa,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,A Sanskrit-English machine translation dataset.
rajistics/indian_food_images,,,,,,,https://huggingface.co/datasets/rajistics/indian_food_images,,,,,,,,,,,,,,,,,,,,"Source of dataset: Kaggle This Dataset contains different images of food in 20 different classes. Some of the classes are of Indian food. All the images are extracted from google. Images per classes are little so Data augmentation and transfer learning will be best suited here. Classes of the model: ""burger"", ""butter_naan"", ""chai"", ""chapati"", ""chole_bhature"", ""dal_makhani"", ""dhokla"", ""fried_rice"", ""idli"", ""jalebi"", ""kaathi_rolls"", ""kadai_paneer"", ""kulfi"", ""masala_dosa"", ""momos"", ""paani_puri""â€¦ See the full description on the dataset page: https://huggingface.co/datasets/rajistics/indian_food_images."
rajpurkar/squad_v2,Text,Question Answering,General,Text,"Exact Match, F1 Score",https://rajpurkar.github.io/SQuAD-explorer/,https://huggingface.co/datasets/rajpurkar/squad_v2,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for SQuAD 2.0 Data-Summary: Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. SQuAD 2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rajpurkar/squad_v2."
rajuptvs/ecommerce_products_clip,,,,,,,https://huggingface.co/datasets/rajuptvs/ecommerce_products_clip,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,rajuptvs/ecommerce_products_clip dataset hosted on Hugging Face and contributed by the HF Datasets community
RamAnanth1/talkrl-podcast,,,,,,,https://huggingface.co/datasets/RamAnanth1/talkrl-podcast,,,,,,,,,,3,,,,,,,,,,"Dataset Card for ""talkrl-podcast"" This dataset is sourced from the TalkRL Podcast website and contains English transcripts of wonderful TalkRL podcast episodes. The transcripts were generated using OpenAI's base Whisper model"
Rami/adhd_question,,,,,,,https://huggingface.co/datasets/Rami/adhd_question,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Rami/adhd_question dataset hosted on Hugging Face and contributed by the HF Datasets community
raptorkwok/cantonese-traditional-chinese-parallel-corpus,,,,,,,https://huggingface.co/datasets/raptorkwok/cantonese-traditional-chinese-parallel-corpus,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"This is a dataset of Cantonese-Written Chinese Parallel Corpus, containing 130k+ pairs of Cantonese and Traditional Chinese parallel sentences."
rasikabh/pile-pii,,,,,,,https://huggingface.co/datasets/rasikabh/pile-pii,,,,,,,,,,,5,,,,,,,,,rasikabh/pile-pii dataset hosted on Hugging Face and contributed by the HF Datasets community
rasyidf/coffee-beans,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/rasyidf/coffee-beans,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Beans Data-Summary: Coffee Beans Grading Supported Tasks and Leaderboards image-classification: Based on a coffee bean grading, the goal of this task is to grade single beans for clusterization. Languages Indonesia Dataset Structure Data Instances A sample from the training set is provided below: { 'image_file_path':â€¦ See the full description on the dataset page: https://huggingface.co/datasets/rasyidf/coffee-beans."
RazinAleks/SO-Python_QA-System_Administration_and_DevOps_class,,,,,,,https://huggingface.co/datasets/RazinAleks/SO-Python_QA-System_Administration_and_DevOps_class,,,,,,,,,,,,,,,,,,,,RazinAleks/SO-Python_QA-System_Administration_and_DevOps_class dataset hosted on Hugging Face and contributed by the HF Datasets community
rcds/swiss_leading_decision_summarization,Text,,,,,,https://huggingface.co/datasets/rcds/swiss_leading_decision_summarization,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,159,,,,,,,,,,This dataset contains court decisions for the swiss ruling summarization task.
rdpahalavan/network-packet-flow-header-payload,,,,,,,https://huggingface.co/datasets/rdpahalavan/network-packet-flow-header-payload,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Each row contains the information of a network packet and its label. The format is given below:
readerbench/ro-business-emails,,,,,,,https://huggingface.co/datasets/readerbench/ro-business-emails,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,readerbench/ro-business-emails dataset hosted on Hugging Face and contributed by the HF Datasets community
RealTimeData/arxiv_latest,,,,,,,https://huggingface.co/datasets/RealTimeData/arxiv_latest,,,,,,,,,,,,,,,,,,,,"Latest arXiv You could always access the latest arXiv papers via this dataset. We update the dataset weekly, on every Sunday. So the dataset always provides the latest arXiv papers created in the past week. The current dataset on main branch contains the latest arXiv papers submitted from 2024-09-02 to 2024-09-09. The data collection was conducted on 2024-09-09. Use the dataset via: ds = datasets.load_dataset('RealTimeData/arxiv_latest') Previsou versions Youâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RealTimeData/arxiv_latest."
reasoning-machines/gsm-hard,Text,Reasoning,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2211.10435,https://huggingface.co/datasets/reasoning-machines/gsm-hard,,,,,,,,https://choosealicense.com/licenses/mit/,,,531811,,,,,,"T5, UnifiedQA, BART",,,"Data-Summary: This is the harder version of gsm8k math reasoning dataset (https://huggingface.co/datasets/gsm8k). We construct this dataset by replacing the numbers in the questions of GSM8K with larger numbers that are less common.  Supported Tasks and Leaderboards This dataset is used to evaluate math reasoning Languages English - Numbers Dataset Structure dataset = load_dataset(""reasoning-machines/gsm-hard"") DatasetDict({â€¦ See the full description on the dataset page: https://huggingface.co/datasets/reasoning-machines/gsm-hard."
reatiny/chinese-spam-10000,,,,,,,https://huggingface.co/datasets/reatiny/chinese-spam-10000,,,,,,,,,,,,,,,,,,,,reatiny/chinese-spam-10000 dataset hosted on Hugging Face and contributed by the HF Datasets community
reazon-research/reazonspeech,Audio,General,General,Text,"Accuracy, F1 Score",https://research.reazon.jp/projects/ReazonSpeech,https://huggingface.co/datasets/reazon-research/reazonspeech,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, T5",,of audio,"Dataset Card for ReazonSpeech Data-Summary: This dataset contains a diverse set of natural Japanese speech, collected from terrestrial television streams. It contains more than 35000 hours of audio. Paper: ReazonSpeech: A Free and Massive Corpus for Japanese ASR Disclaimer TO USE THIS DATASET, YOU MUST AGREE THAT YOU WILL USE THE DATASET SOLELY FOR THE PURPOSE OF JAPANESE COPYRIGHT ACT ARTICLE 30-4. Dataset Format Audio files areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/reazon-research/reazonspeech."
recastai/LAION-art-EN-improved-captions,Image,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/recastai/LAION-art-EN-improved-captions,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for LAION-art-EN-improved-captions Data-Summary: This dataset has been created by Re:cast AI for improving the semantic relationship of image-caption pairs. generated_captions were created in a semi-supervised fashion using the Salesforce/blip2-flan-t5-xxl model. Supported Tasks Fine-tuning text-to-image generators (e.g. stable-diffusion), or a searchable prompt database (requires faiss-index). Dataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/recastai/LAION-art-EN-improved-captions."
reciTAL/mlsum,Text,,,,,,https://huggingface.co/datasets/reciTAL/mlsum,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset."
recmeapp/mobilerec,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/recmeapp/mobilerec,,,,,,,,,,,4,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for Dataset Name Data-Summary: MobileRec is a large-scale app recommendation dataset. There are 19.3 million user\item interactions. This is a 5-core dataset. User\item interactions are sorted in ascending chronological order. There are 0.7 million users who have had at least five distinct interactions. There are 10173 apps in total. Supported Tasks and Leaderboards Sequential Recommendation Languages Englishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/recmeapp/mobilerec.
reedmayhew/claude-3.7-sonnet-reasoning,,,,,,,https://huggingface.co/datasets/reedmayhew/claude-3.7-sonnet-reasoning,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,reedmayhew/claude-3.7-sonnet-reasoning dataset hosted on Hugging Face and contributed by the HF Datasets community
refugee-law-lab/canadian-legal-data,Text,General,Scientific,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2207.00220,https://huggingface.co/datasets/refugee-law-lab/canadian-legal-data,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,Refugee Law Lab: Canadian Legal Data Data-Summary: The Refugee Law Lab supports bulk open-access to Canadian legal data to facilitate research and advocacy. Bulk open-access helps avoid asymmetrical access-to-justice and amplification of marginalization that results when commercial actors leverage proprietary legal datasets for profit -- a particular concern in the border control setting. The Canadian Legal Data dataset includes the unofficial text ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/refugee-law-lab/canadian-legal-data.
relbert/conceptnet,Text,,,,,,https://huggingface.co/datasets/relbert/conceptnet,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,[ConceptNet with high confidence](https://home.ttic.edu/~kgimpel/commonsense.html)
renumics/speech_commands_enriched,Text,,,,,https://renumics.com/?hf-dataset-card=speech-commands-enriched,https://huggingface.co/datasets/renumics/speech_commands_enriched,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,96414780773759,,,,,,,,,"This is a set of one-second .wav audio files, each containing a single spoken English word or background noise. These words are from a small set of commands, and are spoken by a variety of different speakers. This data set is designed to help train simple machine learning models. This dataset is covered in more detail at [https://arxiv.org/abs/1804.03209](https://arxiv.org/abs/1804.03209). Version 0.01 of the data set (configuration `""v0.01""`) was released on August 3rd 2017 and contains 64,727 audio files. In version 0.01 thirty different words were recoded: ""Yes"", ""No"", ""Up"", ""Down"", ""Left"", ""Right"", ""On"", ""Off"", ""Stop"", ""Go"", ""Zero"", ""One"", ""Two"", ""Three"", ""Four"", ""Five"", ""Six"", ""Seven"", ""Eight"", ""Nine"", ""Bed"", ""Bird"", ""Cat"", ""Dog"", ""Happy"", ""House"", ""Marvin"", ""Sheila"", ""Tree"", ""Wow"". In version 0.02 more words were added: ""Backward"", ""Forward"", ""Follow"", ""Learn"", ""Visual"". In both versions, ten of them are used as commands by convention: ""Yes"", ""No"", ""Up"", ""Down"", ""Left"", ""Right"", ""On"", ""Off"", ""Stop"", ""Go"". Other words are considered to be auxiliary (in current implementation it is marked by `True` value of `""is_unknown""` feature). Their function is to teach a model to distinguish core words from unrecognized ones. This version is not yet supported. The `_silence_` class contains a set of longer audio clips that are either recordings or a mathematical simulation of noise."
rexarski/eli5_category,Text,,,,,https://celeritasml.netlify.app/posts/2021-12-01-eli5c/,https://huggingface.co/datasets/rexarski/eli5_category,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"The ELI5-Category dataset is a smaller but newer and categorized version of the original ELI5 dataset. After 2017, a tagging system was introduced to this subreddit so that the questions can be categorized into different topics according to their tags. Since the training and validation set is built by questions in different topics, the dataset is expected to alleviate the train/validation overlapping issue in the original ELI5 dataset."
reyavir/PromptEvals,,,,,,,https://huggingface.co/datasets/reyavir/PromptEvals,,,,,,,,,,,,,,You will receive a modelInput and a modelOutput,,,,,,"PromptEvals: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines Large language models (LLMs) are increasingly deployed in specialized production data processing pipelines across diverse domains---such as finance, marketing, and e-commerce. However, when running them in production across many inputs, they often fail to follow instructions or meet developer expectations. To improve reliability in these applications, creating assertions or guardrailsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/reyavir/PromptEvals."
rezacsedu/bn_hate_speech,Audio,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",https://github.com/rezacsedu/Bengali-Hate-Speech-Dataset,https://huggingface.co/datasets/rezacsedu/bn_hate_speech,dataset of news articles collected from various Bengali media sources and categorized based on the type of hate in the text,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for Bengali Hate Speech Dataset Data-Summary: The Bengali Hate Speech Dataset is a Bengali-language dataset of news articles collected from various Bengali media sources and categorized based on the type of hate in the text. The dataset was created to provide greater support for under-resourced languages like Bengali on NLP tasks, and serves as a benchmark for multiple types of classification tasks. Supported Tasks and Leaderboardsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rezacsedu/bn_hate_speech."
rhasspy/piper-checkpoints,,,,,,,https://huggingface.co/datasets/rhasspy/piper-checkpoints,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Checkpoints for Piper text to speech system.
RicardoRei/wmt-da-human-evaluation,Text,Translation,General,Text,"BLEU, METEOR, TER",,https://huggingface.co/datasets/RicardoRei/wmt-da-human-evaluation,pair src: input text mt: translation ref: reference translation score: z score raw: direct assessment annotators: number of annotators domain: domain of the input text (e,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"T5, mBART, M2M100",,,Data-Summary: This dataset contains all DA human annotations from previous WMT News Translation shared tasks. The data is organised into 8 columns: lp: language pair src: input text mt: translation ref: reference translation score: z score raw: direct assessment annotators: number of annotators domain: domain of the input text (e.g. news) year: collection year You can also find the original data for each year in the results sectionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RicardoRei/wmt-da-human-evaluation.
rickRossie/bluemoon_roleplay_chat_data_300k_messages,,,,,,,https://huggingface.co/datasets/rickRossie/bluemoon_roleplay_chat_data_300k_messages,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""bluemoon_roleplay_chat_data_300k_messages"" More Information needed"
rishitdagli/cppe-5,Image,General,UI,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/rishitdagli/cppe-5,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for CPPE - 5 Data-Summary: CPPE - 5 (Medical Personal Protective Equipment) is a new challenging dataset with the goal to allow the study of subordinate categorization of medical personal protective equipments, which is not possible with other popular data sets that focus on broad level categories. Some features of this dataset are: high quality images and annotations (~4.6 bounding boxes per image) real-life images unlike any current such datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rishitdagli/cppe-5."
Riyazmk/mentalhealth,,,,,,,https://huggingface.co/datasets/Riyazmk/mentalhealth,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,Riyazmk/mentalhealth dataset hosted on Hugging Face and contributed by the HF Datasets community
rkstgr/mtg-jamendo,Text,,,,,,https://huggingface.co/datasets/rkstgr/mtg-jamendo,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Repackaging of the MTG Jamendo dataset. We present the MTG-Jamendo Dataset, a new open dataset for music auto-tagging. It is built using music available at Jamendo under Creative Commons licenses and tags provided by content creators. The dataset contains over 55,000 full audio tracks with 195 tags from genre, instrument, and mood/theme categories."
roa7n/patched_test_p_40_f_membrane_m1_predictions,,,,,,,https://huggingface.co/datasets/roa7n/patched_test_p_40_f_membrane_m1_predictions,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""patched_test_p_40_f_membrane_m1_predictions"" More Information needed"
roborovski/textures_dreambooth_training,,,,,,,https://huggingface.co/datasets/roborovski/textures_dreambooth_training,,,,,,,,,,,,,,,,,,,,roborovski/textures_dreambooth_training dataset hosted on Hugging Face and contributed by the HF Datasets community
robotflow/vtaco,,,,,,https://arxiv.org/abs/2303.14498,https://huggingface.co/datasets/robotflow/vtaco,,,,,,,,,,,,,,,,,,,,"Dataset for Visual-Tactile Sensing for In-Hand Object Reconstruction Paper | Project Page This repository contains the dataset of the paper: Visual-Tactile Sensing for In-Hand Object ReconstructionWenqiang Xu*, Zhenjun Yu*, Han Xue, Ruolin Ye, Siqiong Yao, Cewu Lu (* = Equal contribution)CVPR 2023 Download By downloading the dataset into the repository of VTacO under the folder './data', and it should be look like: VTacO â”œâ”€â”€ data â”‚ â”œâ”€â”€ VTacO_AKB_class â”‚ â”‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/robotflow/vtaco."
RobotsMaliAI/bayelemabaga,,,,,,,https://huggingface.co/datasets/RobotsMaliAI/bayelemabaga,,,,,,,,,,,,,,,,,,,,"The Bayelemabaga dataset is a collection of 44160 aligned machine translation ready Bambara-French lines, originating from Corpus Bambara de Reference. The dataset is constitued of text extracted from 231 source files, varing from periodicals, books, short stories, blog posts, part of the Bible and the Quran."
rockerBOO/t5-v1_1-small-k-mktr-improved-flux-prompts-latents,,,,,,,https://huggingface.co/datasets/rockerBOO/t5-v1_1-small-k-mktr-improved-flux-prompts-latents,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Dataset Card for Prompt Latents from T5-small Latents from T5-small used for distillation. Dataset Details Dataset Description Curated by: Dave Lage License: Apache 2 Dataset Sources [optional] Repository: rockerBOO/t5-distill Uses Latents from T5-small used for distillation. Direct Use [More Information Needed] Out-of-Scope Use [More Information Needed] Dataset Structure latents:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/rockerBOO/t5-v1_1-small-k-mktr-improved-flux-prompts-latents.
rojagtap/bookcorpus,,,,,,,https://huggingface.co/datasets/rojagtap/bookcorpus,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,rojagtap/bookcorpus dataset hosted on Hugging Face and contributed by the HF Datasets community
Romjiik/Russian_bank_reviews,Text,Classification,UI,Label,"Accuracy, F1 Score, Precision, Recall",,https://huggingface.co/datasets/Romjiik/Russian_bank_reviews,,,,,,,,,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for bank reviews dataset Data-Summary: The dataset is collected from the banki.ru website. It contains customer reviews of various banks. In total, the dataset contains 12399 reviews. The dataset is suitable for sentiment classification. The dataset contains this fields - bank name, username, review title, review text, review time, number of views, number of comments, review rating set by the user, as well as ratings for special categoriesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Romjiik/Russian_bank_reviews."
roneneldan/TinyStories,,,,,,https://arxiv.org/abs/2305.07759,https://huggingface.co/datasets/roneneldan/TinyStories,,,,,,,,https://choosealicense.com/licenses/cdla-sharing-1.0/,,,,,,,,,,,,"Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary. Described in the following paper: https://arxiv.org/abs/2305.07759. The models referred to in the paper were trained on TinyStories-train.txt (the file tinystories-valid.txt can be used for validation loss). These models can be found on Huggingface, at roneneldan/TinyStories-1M/3M/8M/28M/33M/1Layer-21M. Additional resources: tinystories_all_data.tar.gz - contains a superset ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/roneneldan/TinyStories."
ronig/protein_binding_sequences,,,,,,,https://huggingface.co/datasets/ronig/protein_binding_sequences,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Sequence Based Protein - Peptide Binding Dataset Data sources: Huang Laboratory Propedia YAPP-Cd Dataset size: 16,370 sets of Protein-Peptide sequences that bind, the protein sequence contains only the relevant chain. Train / Val split: the dataset is split to 80% train 10% val and 10% test."
Rosenberg/genia,,,,,,,https://huggingface.co/datasets/Rosenberg/genia,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Rosenberg/genia dataset hosted on Hugging Face and contributed by the HF Datasets community
Rowan/hellaswag,Text,,,,,https://rowanzellers.com/hellaswag/,https://huggingface.co/datasets/Rowan/hellaswag,,,,,,,,,,,,,,,,,,,,HellaSwag: Can a Machine Really Finish Your Sentence? is a new dataset for commonsense NLI. A paper was published at ACL2019.
rrustom/architecture2022clean,,,,,,,https://huggingface.co/datasets/rrustom/architecture2022clean,,,,,,,,,,,,,,,,,,,,rrustom/architecture2022clean dataset hosted on Hugging Face and contributed by the HF Datasets community
ruanchaves/hatebr,Text,,,,,http://143.107.183.175:14581/,https://huggingface.co/datasets/ruanchaves/hatebr,,,,,,,,https://choosealicense.com/licenses/undefined/,,,,,,,,,,,,"HateBR is the first large-scale expert annotated corpus of Brazilian Instagram comments for hate speech and offensive language detection on the web and social media. The HateBR corpus was collected from Brazilian Instagram comments of politicians and manually annotated by specialists. It is composed of 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive comments), offensiveness-level (highly, moderately, and slightly offensive messages), and nine hate speech groups (xenophobia, racism, homophobia, sexism, religious intolerance, partyism, apology for the dictatorship, antisemitism, and fatphobia). Each comment was annotated by three different annotators and achieved high inter-annotator agreement. Furthermore, baseline experiments were implemented reaching 85% of F1-score outperforming the current literature models for the Portuguese language. Accordingly, we hope that the proposed expertly annotated corpus may foster research on hate speech and offensive language detection in the Natural Language Processing area."
rubend18/ChatGPT-Jailbreak-Prompts,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/rubend18/ChatGPT-Jailbreak-Prompts,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for Dataset Name Name ChatGPT Jailbreak Prompts Data-Summary: ChatGPT Jailbreak Prompts is a complete collection of jailbreak related prompts for ChatGPT. This dataset is intended to provide a valuable resource for understanding and generating text in the context of jailbreaking in ChatGPT. Languages [English]
RUCAIBox/Question-Answering,,,,,,,https://huggingface.co/datasets/RUCAIBox/Question-Answering,,,,,,,,,,,,,,,,,,,,"This is the question answering datasets collected by TextBox, including: SQuAD (squad) CoQA (coqa) Natural Questions (nq) TriviaQA (tqa) WebQuestions (webq) NarrativeQA (nqa) MS MARCO (marco) NewsQA (newsqa) HotpotQA (hotpotqa) MSQG (msqg) QuAC (quac). The detail and leaderboard of each dataset can be found in TextBox page."
rungalileo/medical_transcription_40,,,,,,,https://huggingface.co/datasets/rungalileo/medical_transcription_40,,,,,,,,,,,,,,,,,,,,rungalileo/medical_transcription_40 dataset hosted on Hugging Face and contributed by the HF Datasets community
RunsenXu/PointLLM,,,,,,https://arxiv.org/abs/2308.16911,https://huggingface.co/datasets/RunsenXu/PointLLM,,,,,,,,https://choosealicense.com/licenses/odc-by/,,,,,,,,,,,,The official dataset release of paper ECCV 2024: PointLLM: Empowering Large Language Models to Understand Point Clouds
RussianNLP/rucola,Text,,,,,https://rucola-benchmark.com,https://huggingface.co/datasets/RussianNLP/rucola,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Russian Corpus of Linguistic Acceptability (RuCoLA) is a novel benchmark of 13.4k sentences labeled as acceptable or not. RuCoLA combines in-domain sentences manually collected from linguistic literature and out-of-domain sentences produced by nine machine translation and paraphrase generation models. The motivation behind the out-of-domain set is to facilitate the practical use of acceptability judgments for improving language generation. Each unacceptable sentence is additionally labeled with four standard and machine-specific coarse-grained categories: morphology, syntax, semantics, and hallucinations."
rvpierre/insurance-qa-en,,,,,,,https://huggingface.co/datasets/rvpierre/insurance-qa-en,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""insurance-qa-en"" More Information needed"
rwightman/imagenet-12k-metadata,,,,,,,https://huggingface.co/datasets/rwightman/imagenet-12k-metadata,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,ImageNet-12k Split Metadata Metadata files defining the splits for ImageNet-12k subset of fall11_whole.tar (2011 ImageNet full release) used in some timm models (see dataset building code in https://github.com/rwightman/imagenet-12k).
ryo0634/bsd_ja_en,Text,General,General,Text,"Accuracy, F1 Score",https://raw.githubusercontent.com/tsuruoka-lab/BSD/,https://huggingface.co/datasets/ryo0634/bsd_ja_en,English,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Business Scene Dialogue Data-Summary: This is the Business Scene Dialogue (BSD) dataset, a Japanese-English parallel corpus containing written conversations in various business scenarios. The dataset was constructed in 3 steps: selecting business scenes, writing monolingual conversation scenarios according to the selected scenes, and translating the scenarios into the other language. Half of the monolingual scenarios were written inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ryo0634/bsd_ja_en."
RyokoAI/ShareGPT52K,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/RyokoAI/ShareGPT52K,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ShareGPT52K90K Data-Summary: This dataset is a collection of approximately 52,00090,000 conversations scraped via the ShareGPT API before it was shut down. These conversations include both user prompts and responses from OpenAI's ChatGPT. This repository now contains the new 90K conversations version. The previous 52K may be found in the old/ directory. Supported Tasks and Leaderboards text-generation Languagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RyokoAI/ShareGPT52K."
s3prl/superb,Text,,,,,http://superbbenchmark.org,https://huggingface.co/datasets/s3prl/superb,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,https://github.com/s3prl/s3prl/tree/master/downstream#qbe-query-by-example-spoken-term-detection,,,,,,,,"Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing. Note that in order to limit the required storage for preparing this dataset, the audio is stored in the .wav format and is not converted to a float32 array. To convert the audio file to a float32 array, please make use of the `.map()` function as follows: ```python import soundfile as sf def map_to_array(batch): speech_array, _ = sf.read(batch[""file""]) batch[""speech""] = speech_array return batch dataset = dataset.map(map_to_array, remove_columns=[""file""]) ```"
saattrupdan/doc-nli,,,,,,,https://huggingface.co/datasets/saattrupdan/doc-nli,,,,,,,,,,,,,,,,,,,,saattrupdan/doc-nli dataset hosted on Hugging Face and contributed by the HF Datasets community
Sachinkelenjaguri/Resume_dataset,,,,,,,https://huggingface.co/datasets/Sachinkelenjaguri/Resume_dataset,,,,,,,,,,,,,,,,,,,,Sachinkelenjaguri/Resume_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
sacrificialpancakes/synthetic_demographics_seed,,,,,,,https://huggingface.co/datasets/sacrificialpancakes/synthetic_demographics_seed,,,,,,,,,,,,,,,,,,,,"Synthetic Demographic Seeds v1 This is a dataset of 3,541,040 roughly demographically correct demographic seeds and somewhat demographically accurate names all generated from publicly available datasets. (note there were tradeoffs made with accuracy and what I could tie together, v2 will be more accurate) get_synthetic_demographics.py contains a method for quickly and randomly selecting batches of demographic seeds. There is no filtering on this at the moment.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/sacrificialpancakes/synthetic_demographics_seed."
SadilKhan/Text2CAD,,,,,,https://arxiv.org/abs/2409.17106,https://huggingface.co/datasets/SadilKhan/Text2CAD,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,ðŸš€ Text2CAD: Generating Sequential CAD Designs from Beginner-to-Expert Level Text Prompts ðŸŽ¨ Mohammad Sadil Khan* Â· Sankalp Sinha* Â· Talha Uddin Sheikh Â· Didier Stricker Â· Sk Aziz Ali Â· Muhammad Zeshan Afzal *equal contributions NeurIPS 2024 ðŸ“Š Dataset Versions We are releasing the following versions of the Text2CAD dataset. Dataset ðŸ¤– VLM ðŸ¤– LLM ðŸ“ Remarks Text2CAD 1.0 LLaVA-NeXT Mistral-7x8B-Instructâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SadilKhan/Text2CAD.
SaffalPoosh/deepFashion-with-masks,,,,,,,https://huggingface.co/datasets/SaffalPoosh/deepFashion-with-masks,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset Dataset name is deepfashion2 datasest, the dataset is in raw form with annotations, for original dataset repo. see https://github.com/switchablenorms/DeepFashion2 This dataset is just the extracted version of original deepfashion2 dataset and can be used for training Controlnet Model."
safgasgfsa/Neurosama,,,,,,,https://huggingface.co/datasets/safgasgfsa/Neurosama,,,,,,,,,,,,,,,,,,,,safgasgfsa/Neurosama dataset hosted on Hugging Face and contributed by the HF Datasets community
Safurai/Code-Instruct-700k,,,,,,,https://huggingface.co/datasets/Safurai/Code-Instruct-700k,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""Code-Instruct-700k"" More Information needed"
sagantime/NBAData,,,,,,,https://huggingface.co/datasets/sagantime/NBAData,,,,,,,,,,,,,,,,,,,,sagantime/NBAData dataset hosted on Hugging Face and contributed by the HF Datasets community
sagard21/autotrain-data-code-explainer,,,,,,,https://huggingface.co/datasets/sagard21/autotrain-data-code-explainer,,,,,,,,,,,,,,,,,,,,"AutoTrain Dataset for project: code-explainer Dataset Description This dataset has been automatically processed by AutoTrain for project code-explainer. Languages The BCP-47 code for the dataset's language is unk. Dataset Structure Data Instances A sample from this dataset looks as follows: [ { ""text"": ""def upload_to_s3(local_file, bucket, s3_file):\n ## This function is responsible for uploading the file intoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sagard21/autotrain-data-code-explainer."
SahandNZ/cryptonews-articles-with-price-momentum-labels,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/SahandNZ/cryptonews-articles-with-price-momentum-labels,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Cryptonews articles with price momentum labels Data-Summary: The dataset was gathered from two prominent sources in the cryptocurrency industry: Cryptonews.com and Binance.com. The aim of the dataset was to evaluate the impact of news on crypto price movements. As we know, news events such as regulatory changes, technological advancements, and major partnerships can have a significant impact on the price of cryptocurrencies. By analyzing theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SahandNZ/cryptonews-articles-with-price-momentum-labels."
sahil2801/CodeAlpaca-20k,,,,,,,https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,sahil2801/CodeAlpaca-20k dataset hosted on Hugging Face and contributed by the HF Datasets community
SaiedAlshahrani/Moroccan_Arabic_Wikipedia_20230101_nobots,,,,,,,https://huggingface.co/datasets/SaiedAlshahrani/Moroccan_Arabic_Wikipedia_20230101_nobots,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Card for ""Moroccan_Arabic_Wikipedia_20230101_nobots"" This dataset is created using the Moroccan Arabic Wikipedia articles (after removing bot-generated articles), downloaded on the 1st of January 2023, processed using Gensim Python library, and preprocessed using tr Linux/Unix utility and CAMeLTools Python toolkit for Arabic NLP. This dataset was used to train this Moroccan Arabic Wikipedia Masked Language Model: SaiedAlshahrani/arywiki_20230101_roberta_mlm_nobots. Forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SaiedAlshahrani/Moroccan_Arabic_Wikipedia_20230101_nobots."
saier/unarXive_imrad_clf,Text,Classification,Scientific,Label,"Accuracy, F1 Score, Precision, Recall",https://github.com/IllDepence/unarXive,https://huggingface.co/datasets/saier/unarXive_imrad_clf,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,Dataset Card for unarXive IMRaD classification Data-Summary: The unarXive IMRaD classification dataset contains 530k paragraphs from computer science papers and the IMRaD section they originate from. The paragraphs are derived from unarXive. The dataset can be used as follows. from datasets import load_dataset imrad_data = load_dataset('saier/unarXive_imrad_clf') imrad_data = imrad_data.class_encode_column('label') # assign target label column imrad_data =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/saier/unarXive_imrad_clf.
saifkhichi96/mpii-human-pose-captions,Image-Text,General,General,Text,"Accuracy, F1 Score",https://www.saifkhichi.com/research/focusclip/,https://huggingface.co/datasets/saifkhichi96/mpii-human-pose-captions,"models (LLMs) and include detailed descriptions of the activities being performed, the count of people present, and their specific poses",,,,,,,https://choosealicense.com/licenses/bsd-2-clause/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for MPII Human Pose Descriptions Data-Summary: The MPII Human Pose Descriptions dataset extends the widely-used MPII Human Pose Dataset with rich textual annotations. These annotations are generated by various state-of-the-art language models (LLMs) and include detailed descriptions of the activities being performed, the count of people present, and their specific poses. The dataset consists of the same image splits as provided in MMPose, with 14644â€¦ See the full description on the dataset page: https://huggingface.co/datasets/saifkhichi96/mpii-human-pose-captions."
sail/symbolic-instruction-tuning,,,,,,https://arxiv.org/abs/2304.07995,https://huggingface.co/datasets/sail/symbolic-instruction-tuning,,,,,,,,https://choosealicense.com/licenses/mit/,,32,,,,,,,,,,Symbolic Instruction Tuning This is the offical repo to host the datasets used in the paper From Zero to Hero: Examining the Power of Symbolic Tasks in Instruction Tuning. The training code can be found in here.
SakanaAI/Sudoku-CTC-Reasoning,,,,,,,https://huggingface.co/datasets/SakanaAI/Sudoku-CTC-Reasoning,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Sudoku-Bench ðŸ¤— [Sudoku-Bench puzzle dataset] ðŸ™ [Sudoku-Bench GitHub] ðŸ“ [Blog Post] Sudoku-CTC-Reasoning dataset The Sudoku-CTC-Reasoning dataset contains the reasoning traces of 1351 puzzles featured in the Cracking the Cryptic YouTube channel, and thus provides rich learning signals for training LMs to learn reasoning in a Sudoku game or for a broader range of reasoning-intensive tasks. This dataset is provided with permission from Cracking the Cryptic.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/SakanaAI/Sudoku-CTC-Reasoning."
sakusakumura/databricks-dolly-15k-ja-scored,,,,,,https://arxiv.org/abs/1904.09675,https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"For the English version, please click here. æ¦‚è¦ databricks-dolly-15k-ja-scoredã¯kunishou/databricks-dolly-15k-jaã®æ´¾ç”Ÿã§ã‚ã‚Šã€BERTScoreã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã‚‹ç¿»è¨³å“è³ªã‚¹ã‚³ã‚¢ãŒè¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å­¦è¡“çš„ãƒ»å•†æ¥­çš„å•ã‚ãšã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - ç¶™æ‰¿ 3.0 éžç§»æ¤ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®æ¡ä»¶ã®ä¸‹ã§ä½•ã«ã§ã‚‚ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ ç¿»è¨³ã®å“è³ªã‚¹ã‚³ã‚¢ databricks-dolly-15k-jaã¯ã€databricks-dolly-15kã‚’æ©Ÿæ¢°ç¿»è¨³ã—ãŸã‚‚ã®ã§ã™ã€‚databricks-dolly-15k-jaã«å«ã¾ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’èª¿ã¹ã¦ã¿ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå“è³ªã®æ‚ªã„ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã—ãŸã€‚ inputã¨outputãŒå…¨ãåŒã˜ã§ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ outputãŒinstructionã«ã‚³ãƒ”ãƒ¼ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ è¡¨è¨˜ã‚†ã‚Œã«ã‚ˆã£ã¦è¡¨ç¾ã®ä¸€è²«æ€§ãŒä¿ãŸã‚Œã¦ã„ãªã„ãƒ‡ãƒ¼ã‚¿ å›ºæœ‰åè©žãªã©ã®ç¿»è¨³ã«å¤±æ•—ã—ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿â€¦ See the full description on the dataset page: https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored."
Salama1429/tarteel-ai-everyayah-Quran,Text,General,General,Text,"Accuracy, F1 Score",https://www.tarteel.ai/,https://huggingface.co/datasets/Salama1429/tarteel-ai-everyayah-Quran,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"ï·½ Dataset Card for Tarteel AI's EveryAyah Dataset Data-Summary: This dataset is a collection of Quranic verses and their transcriptions, with diacritization, by different reciters. How to download !pip install -q datasets from datasets import load_dataset dataset =load_dataset(""Salama1429/tarteel-ai-everyayah-Quran"", verification_mode=""no_checks"") Supported Tasks and Leaderboards [Needs More Information] Languagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Salama1429/tarteel-ai-everyayah-Quran."
Salesforce/wikitext,Text,General,General,Text,"Accuracy, F1 Score",https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/,https://huggingface.co/datasets/Salesforce/wikitext,modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,"d version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger",,"BERT, RoBERTa, T5",,,"Dataset Card for ""wikitext"" Data-Summary: The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a farâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/wikitext."
SALT-NLP/FLUE-FiQA,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/SALT-NLP/FLUE-FiQA,,,,,,,,https://choosealicense.com/licenses/cc-by-3.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Data-Summary: Homepage: https://sites.google.com/view/salt-nlp-flang Models: https://huggingface.co/SALT-NLP/FLANG-BERT Repository: https://github.com/SALT-NLP/FLANG FLUE FLUE (Financial Language Understanding Evaluation) is a comprehensive and heterogeneous benchmark that has been built from 5 diverse financial domain specific datasets. Sentiment Classification: Financial PhraseBankSentiment Analysis, Question Answering: FiQA 2018New Headlines Classification:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/SALT-NLP/FLUE-FiQA."
SamaAI/sama-drives-california,Video,Detection,General,Bounding Box/Mask,"mAP, IoU",http://www.sama.com,https://huggingface.co/datasets/SamaAI/sama-drives-california,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"DETR, Faster R-CNN, YOLO",s (848x480 pixels) taken by a dashboard video camera of a car driving in California,of driving,"Dataset Card for sama-drives-california Data-Summary: This is an object detection dataset (bounding boxes and polygons) of 25 136 frames (848x480 pixels) taken by a dashboard video camera of a car driving in California. The frames were captured at 1 FPS, and hence the entire footage covers over 7 hours of driving. All but 110 frames contain at least one annotated object (25 026) of interest. Dataset Structure Data Instances Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SamaAI/sama-drives-california."
SAMControlNet/sam-controlnet-sprint-larg-v1,,,,,,,https://huggingface.co/datasets/SAMControlNet/sam-controlnet-sprint-larg-v1,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""sam-controlnet-sprint-larg-v1"" More Information needed"
samhog/psychology-10k,,,,,,,https://huggingface.co/datasets/samhog/psychology-10k,,,,,,,,,,,,,,,,,,,,samhog/psychology-10k dataset hosted on Hugging Face and contributed by the HF Datasets community
Samsung/samsum,Text,,,,,https://arxiv.org/abs/1911.12237v2,https://huggingface.co/datasets/Samsung/samsum,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-nd-4.0/,,,,,,,,,,,,SAMSum Corpus contains over 16k chat dialogues with manually annotated summaries. There are two features: - dialogue: text of dialogue. - summary: human written summary of the dialogue. - id: id of a
sanchit-gandhi/whisper-jax-test-files,,,,,,,https://huggingface.co/datasets/sanchit-gandhi/whisper-jax-test-files,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""whisper-jax-test-files"" More Information needed"
SandeepKanao/HL7-FHIR-Synthetic-Dataset,,,,,,,https://huggingface.co/datasets/SandeepKanao/HL7-FHIR-Synthetic-Dataset,,,,,,,,https://choosealicense.com/licenses/bsd/,,,,,,,,,,,,SandeepKanao/HL7-FHIR-Synthetic-Dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
sander-wood/irishman,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/sander-wood/irishman,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"If you prefer MIDI or MusicXML, download IrishMAN-MIDI or IrishMAN-XML. For better use of structural info in control codes, consider ABC notation. Data-Summary: The Irish Massive ABC Notation (IrishMAN) dataset includes 216,284 Irish tunes in ABC notation, divided into 99% (214,122 tunes) for training and 1% (2,162 tunes) for validation. These tunes were collected from thesession.org and abcnotation.com, both renowned for sharing traditional music. To ensure uniformity inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sander-wood/irishman."
sandersaarond/Grafana-Community-Dashboards,,,,,,,https://huggingface.co/datasets/sandersaarond/Grafana-Community-Dashboards,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"This is a raw dump of the dashboard json hosted at https://grafana.com/grafana/dashboards/, taken on 06-06-23. Dashboards themselves are json; related metadata is retained for filtering purposes (e.g., by number of downloads) to help in identifying useful data. Dashboards may contain many different query languages, may range across many versions of Grafana, and may be completely broken (since anyone can upload one). JSON structure varies considerably between different dashboards, and findingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sandersaarond/Grafana-Community-Dashboards."
santoshtyss/us-court-cases,,,,,,,https://huggingface.co/datasets/santoshtyss/us-court-cases,,,,,,,,,,27,,,,,,,,,,"Dataset Card for ""us-court-cases"" More Information needed"
sara-nabhani/ML-news-sentiment,,,,,,,https://huggingface.co/datasets/sara-nabhani/ML-news-sentiment,,,,,,,,,,,,,,,,,,,,sara-nabhani/ML-news-sentiment dataset hosted on Hugging Face and contributed by the HF Datasets community
sarahpann/AMPS,,,,,,,https://huggingface.co/datasets/sarahpann/AMPS,,,,,,,,,,,,,,,,,,,,sarahpann/AMPS dataset hosted on Hugging Face and contributed by the HF Datasets community
SaranaAbidueva/buryat-russian_parallel_corpus,,,,,,,https://huggingface.co/datasets/SaranaAbidueva/buryat-russian_parallel_corpus,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for ""buryat-russian_parallel_corpus"" Ð”Ð°Ñ‚Ð°ÑÐµÑ‚ ÑÐ¾ÑÑ‚Ð¾Ð¸Ñ‚ Ð¸Ð· 41Ðº. Ð¿Ð°Ñ€ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ Ð¸ Ð±ÑƒÑ€ÑÑ‚ÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐ°Ñ…. Ð˜Ð· Ð½Ð¸Ñ… 22Ðº Ð¿Ð°Ñ€ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹ Ð¸ 20Ðº Ð¿Ð°Ñ€ ÑÐ»Ð¾Ð². Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°Ð¼: Ð±Ð¸Ð±Ð»Ð¸Ñ 7519 ÐºÐ½Ð¸Ð³Ð¸ 5250 Ñ‚Ð°Ñ‚Ð¾ÐµÐ±Ð° 807 ÑÑ‚Ð¸Ñ…Ð¸ 471 ÑÑ‚Ð¸Ñ…Ð¸ ÐÐ¸Ð¼Ð±ÑƒÐµÐ² 1210 ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ 20058 Ð²Ð¸ÐºÐ¸Ð¿ÐµÐ´Ð¸Ñ 1882 Ð·Ð°ÐºÐ¾Ð½Ñ‹ 1063 Ð¿ÑŒÐµÑÑ‹ Ð´Ð»Ñ Ð´ÐµÑ‚ÐµÐ¹ 2719 The dataset consists of 41k pairs in Russian and Buryat languages. Of these, 19411 pairs of sentences and 20058 pairs of words. Sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SaranaAbidueva/buryat-russian_parallel_corpus."
sartajekram/BanglaRQA,Text,,,,,,https://huggingface.co/datasets/sartajekram/BanglaRQA,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,BanglaRQA is a human-annotated Bangla Question Answering (QA) dataset with diverse question-answer types.
sayakpaul/nyu_depth_v2,Text,,,,,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,https://huggingface.co/datasets/sayakpaul/nyu_depth_v2,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,The NYU-Depth V2 data set is comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect.
SaylorTwift/Gutenberg,,,,,,,https://huggingface.co/datasets/SaylorTwift/Gutenberg,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""Gutenberg"" More Information needed"
SberDevices/Golos,,,,,,https://arxiv.org/abs/1910.10261,https://huggingface.co/datasets/SberDevices/Golos,,,,,,,,,,,,,,,,,,,,"Golos dataset Golos is a Russian corpus suitable for speech research. The dataset mainly consists of recorded audio files manually annotated on the crowd-sourcing platform. The total duration of the audio is about 1240 hours. We have made the corpus freely available for downloading, along with the acoustic model prepared on this corpus. Also we create 3-gram KenLM language model using an open Common Crawl corpus. Dataset structure Domain Train files Trainâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SberDevices/Golos."
scan-tasks/scan-tasks,Text,General,General,Text,"Accuracy, F1 Score",https://github.com/brendenlake/SCAN,https://huggingface.co/datasets/scan-tasks/scan-tasks,-driven navigation tasks for studying compositional learning and zero-shot generalization,,,,,,,https://choosealicense.com/licenses/bsd/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""scan"" Data-Summary: SCAN tasks with various splits. SCAN is a set of simple language-driven navigation tasks for studying compositional learning and zero-shot generalization. See https://github.com/brendenlake/SCAN for a description of the splits."
scb10x/survey_social_value_th2025,,,,,,,https://huggingface.co/datasets/scb10x/survey_social_value_th2025,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,"Social Attitudes and Values Survey Dataset This dataset contains survey questions and responses designed to explore social attitudes and values among people in Thailand in 2025. It includes a comprehensive set of carefully crafted questions and collected responses aimed at facilitating research on social perspectives, values, cultural attitudes, as well as crowdsourcing algorithm research. This dataset was used to evaluate our proposed crowdsourcing algorithm [link to the paper toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/scb10x/survey_social_value_th2025."
scholarly360/terrain_generation_from_sketch_for_game_assets,,,,,,,https://huggingface.co/datasets/scholarly360/terrain_generation_from_sketch_for_game_assets,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,scholarly360/terrain_generation_from_sketch_for_game_assets dataset hosted on Hugging Face and contributed by the HF Datasets community
schooly/Cyber-Security-Breaches,,,,,,,https://huggingface.co/datasets/schooly/Cyber-Security-Breaches,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,schooly/Cyber-Security-Breaches dataset hosted on Hugging Face and contributed by the HF Datasets community
scikit-learn/credit-card-clients,,,,,,,https://huggingface.co/datasets/scikit-learn/credit-card-clients,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"Default of Credit Card Clients Dataset The following was retrieved from UCI machine learning repository. Dataset Information This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. Content There are 25 variables: ID: ID of each client LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit SEX:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/scikit-learn/credit-card-clients."
scim/naacl_data_prog,,,,,,,https://huggingface.co/datasets/scim/naacl_data_prog,,,,,,,,,,,,,,,,,,,,scim/naacl_data_prog dataset hosted on Hugging Face and contributed by the HF Datasets community
scribis/italian-literature-corpus-mini,,,,,,,https://huggingface.co/datasets/scribis/italian-literature-corpus-mini,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""italian-literature-corpus-mini"" More Information needed"
Scuccorese/food-ingredients-dataset,,,,,,,https://huggingface.co/datasets/Scuccorese/food-ingredients-dataset,,,,,,,,,,,,,,,,,,,,Scuccorese/food-ingredients-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
SDbiaseval/faces,,,,,,,https://huggingface.co/datasets/SDbiaseval/faces,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""faces"" More Information needed"
sdiazlor/python-reasoning-dataset,Text,Reasoning,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/sdiazlor/python-reasoning-dataset,,,,,,,,,,,2,,,,,,"T5, UnifiedQA, BART",,,"Dataset Card for my-distiset-986461 This dataset has been created with distilabel. Data-Summary: This dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI: distilabel pipeline run --config ""https://huggingface.co/datasets/sdiazlor/my-distiset-986461/raw/main/pipeline.yaml"" or explore the configuration: distilabel pipeline info --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sdiazlor/python-reasoning-dataset."
SEACrowd/sea-vl_crowdsourced,,,,,,https://arxiv.org/abs/2503.07920,https://huggingface.co/datasets/SEACrowd/sea-vl_crowdsourced,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"SEA-VL: A Multicultural Vision-Language Dataset for Southeast Asia Paper: Crowdsource, Crawl, or Generate? Creating SEA-VL, A Multicultural Vision-Language Dataset for Southeast Asia Dataset: SEA-VL Collection on HuggingFace Code: SEA-VL Experiment | SEA-VL Image Collection What is SEA-VL? Following the success of our SEACrowd project, weâ€™re excited to announce SEA-VL, a new open-source initiative to create high-quality vision-language datasets specifically forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SEACrowd/sea-vl_crowdsourced."
sealuzh/app_reviews,Text,General,Document,Text,"Accuracy, F1 Score",https://github.com/sealuzh/user_quality,https://huggingface.co/datasets/sealuzh/app_reviews,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for [Dataset Name] Data-Summary: It is a large dataset of Android applications belonging to 23 differentapps categories, which provides an overview of the types of feedback users report on the apps and documents the evolution of the related code metrics. The dataset contains about 395 applications of the F-Droid repository, including around 600 versions, 280,000 user reviews (extracted with specific text mining approaches) Supportedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sealuzh/app_reviews."
seamew/Weibo,,,,,,,https://huggingface.co/datasets/seamew/Weibo,,,,,,,,,,,,,,,,,,,,seamew/Weibo dataset hosted on Hugging Face and contributed by the HF Datasets community
seanghay/km-speech-corpus,,,,,,,https://huggingface.co/datasets/seanghay/km-speech-corpus,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for ""km-speech-corpus"" sampling_rate: 16000 mean_seconds: 2.5068187111021882 max_seconds: 19.392 min_seconds: 0.448 total_seconds: 37459.392 total_hrs: 10.405386666666667"
seara/ru_go_emotions,,,,,,,https://huggingface.co/datasets/seara/ru_go_emotions,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Description This dataset is a translation of the Google GoEmotions emotion classification dataset. All features remain unchanged, except for the addition of a new ru_text column containing the translated text in Russian. For the translation process, I used the Deep translator with the Google engine. You can find all the details about translation, raw .csv files and other stuff in this Github repository. For more information also check the official original dataset card.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/seara/ru_go_emotions."
searle-j/kote,,,,,,,https://huggingface.co/datasets/searle-j/kote,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,50k Korean online comments labeled for 44 emotion categories.
seaurkin/facial_exrpressions,Text,Detection,General,Bounding Box/Mask,"mAP, IoU",,https://huggingface.co/datasets/seaurkin/facial_exrpressions,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"DETR, Faster R-CNN, YOLO",,,"Dataset Card for Facial Expression Data-Summary: This dataset was created manually to train models for expression detection. The available Action Units are: smile, kiss, frowning brows, raised brows, open mouth and neutral."
sedthh/gutenberg_english,,,,,,,https://huggingface.co/datasets/sedthh/gutenberg_english,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Card for Project Gutenber - English Language eBooks A collection of non-english language eBooks (48284 rows, 80%+ of all english language books available on the site) from the Project Gutenberg site with metadata removed. Originally colected for https://github.com/LAION-AI/Open-Assistant (follows the OpenAssistant training format) The METADATA column contains catalogue meta information on each book as a serialized JSON: key original column language -â€¦ See the full description on the dataset page: https://huggingface.co/datasets/sedthh/gutenberg_english."
segg/xww,,,,,,,https://huggingface.co/datasets/segg/xww,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,segg/xww dataset hosted on Hugging Face and contributed by the HF Datasets community
segments/sidewalk-semantic,Image,Segmentation,General,Bounding Box/Mask,"IoU, Pixel Accuracy",https://segments.ai/segments/sidewalk-imagery/,https://huggingface.co/datasets/segments/sidewalk-semantic,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,"Mask R-CNN, DeepLab, U-Net",,,"Dataset Card for sidewalk-semantic Data-Summary: A dataset of sidewalk images gathered in Belgium in the summer of 2021. Label your own semantic segmentation datasets on segments.ai Supported Tasks and Leaderboards semantic-segmentation: The dataset can be used to train a semantic segmentation model, where each pixel is classified. The model performance is measured by how high its mean IoU (intersection over union) to the reference is.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/segments/sidewalk-semantic."
semeru/Text-Code-CodeSearchNet-Python,,,,,,https://arxiv.org/abs/1909.09436,https://huggingface.co/datasets/semeru/Text-Code-CodeSearchNet-Python,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,Data StatisticsExampleInput PredictionsReferenceDataset is imported from CodeXGLUE and pre-processed using their script,,,,,,"Dataset is imported from CodeXGLUE and pre-processed using their script. Where to find in Semeru: The dataset can be found at /nfs/semeru/semeru_datasets/code_xglue/text-to-code/codesearchnet/python in Semeru CodeXGLUE -- Code Search (AdvTest) Task Definition Given a natural language, the task is to search source code that matches the natural language. To test the generalization ability of a model, function names and variables inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/semeru/Text-Code-CodeSearchNet-Python."
SemEvalWorkshop/sem_eval_2010_task_8,Text,Classification,Scientific,Label,"Accuracy, F1 Score, Precision, Recall",https://semeval2.fbk.eu/semeval2.php?location=tasks&taskid=11,https://huggingface.co/datasets/SemEvalWorkshop/sem_eval_2010_task_8,,,,,,,,,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for ""sem_eval_2010_task_8"" Data-Summary: The SemEval-2010 Task 8 focuses on Multi-way classification of semantic relations between pairs of nominals. The task was designed to compare different approaches to semantic relation classification and to provide a standard testbed for future research. Supported Tasks and Leaderboards More Information Needed Languages More Information Needed Dataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SemEvalWorkshop/sem_eval_2010_task_8."
semoga432/jpkamer,,,,,,,https://huggingface.co/datasets/semoga432/jpkamer,,,,,,,,,,,,,,,,,,,,semoga432/jpkamer dataset hosted on Hugging Face and contributed by the HF Datasets community
sentence-transformers/embedding-training-data,,,,,,,https://huggingface.co/datasets/sentence-transformers/embedding-training-data,,,,,,,,,,,,,,"Available DatasetsTraining Data for Text Embedding ModelsThis repository contains raw datasets, all of which have also been formatted for easy training in theEmbedding Model Datasetscollection",,,,,,"Training Data for Text Embedding Models This repository contains raw datasets, all of which have also been formatted for easy training in the Embedding Model Datasets collection. We recommend looking there first. This repository contains training files to train text embedding models, e.g. using sentence-transformers. Data Format All files are in a jsonl.gz format: Each line contains a JSON-object that represent one training"
sepideh4jm/swift,,,,,,,https://huggingface.co/datasets/sepideh4jm/swift,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""swift"" More Information needed"
sepidmnorozy/Chinese_sentiment,,,,,,,https://huggingface.co/datasets/sepidmnorozy/Chinese_sentiment,,,,,,,,,,,,,,,,,,,,sepidmnorozy/Chinese_sentiment dataset hosted on Hugging Face and contributed by the HF Datasets community
ServiceNow-AI/R1-Distill-SFT,,,,,,,https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"ðŸ”‰ ð—¦ð—Ÿð—”ð—  ð—¹ð—®ð—¯ - ð—¥ðŸ­-ð——ð—¶ð˜€ð˜ð—¶ð—¹ð—¹-ð—¦ð—™ð—§ Dataset Lewis Tunstall, Ed Beeching, Loubna Ben Allal, Clem Delangue ðŸ¤— and others at Hugging Face announced today that they are - ð—¼ð—½ð—²ð—»ð—¹ð˜† ð—¿ð—²ð—½ð—¿ð—¼ð—±ð˜‚ð—°ð—¶ð—»ð—´ ð—¥ðŸ­ ðŸ”¥ We at ð—¦ð—Ÿð—”ð—  ð—¹ð—®ð—¯ (ServiceNow Language Models) have been cooking up something as well. Inspired by Open-r1, we have decided to open source the data stage-by-stage to support the open source community. ð—•ð—¼ð—¼ð—¸ð—ºð—®ð—¿ð—¸ this page! KEY DETAILS: âš—ï¸ Distilledâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT."
ServiceNow/BigDocs-Bench,,,,,,https://arxiv.org/abs/2412.04626,https://huggingface.co/datasets/ServiceNow/BigDocs-Bench,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"BigDocs-Bench Benchmark data for the paper: BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks ðŸŒ Homepage | ðŸ“– arXiv ðŸ”” News [2024-12-10]: Initial release of the the BigDocs-Bench data. Introduction We introduce BigDocs-Bench a comprehensive benchmark suite designed to evaluate downstream tasks that transform visual inputs into structured outputs, such as GUI2UserIntent (fine-grainedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ServiceNow/BigDocs-Bench."
SetFit/mrpc,,,,,,,https://huggingface.co/datasets/SetFit/mrpc,,,,,,,,,,,,,,,,,,,,"Glue MRPC This dataset is a port of the official mrpc dataset on the Hub. Note that the sentence1 and sentence2 columns have been renamed to text1 and text2 respectively. Also, the test split is not labeled; the label column values are always -1."
seungheondoh/LP-MusicCaps-MC,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2307.16372,https://huggingface.co/datasets/seungheondoh/LP-MusicCaps-MC,Model based Pseudo Music Caption dataset for text-to-music and music-to-text tasks,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,====================================== !important: Be careful when using caption_attribute_prediction (We don't recommend to use)! ====================================== Dataset Card for LP-MusicCaps-MC Data-Summary: LP-MusicCaps is a Large Language Model based Pseudo Music Caption dataset for text-to-music and music-to-text tasks. We construct the music-to-caption pairs with tag-to-caption generation (using three existing multi-label tag datasets and four taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/seungheondoh/LP-MusicCaps-MC.
sewon/ambig_qa,Text,General,UI,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2004.10645,https://huggingface.co/datasets/sewon/ambig_qa,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for AmbigQA: Answering Ambiguous Open-domain Questions Data-Summary: AmbigNQ, a dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark. We find that over half of the questions in NQ-open are ambiguous. The types of ambiguity are diverse and sometimes subtle, many of which are only apparent after examining evidence provided by a very large text corpus. AMBIGNQ, a dataset with 14,042 annotations on NQ-OPEN questionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sewon/ambig_qa."
sh0416/ag_news,,,,,,,https://huggingface.co/datasets/sh0416/ag_news,,,,,,,,,,,,,,,,,,,,"AG's News Topic Classification Dataset Version 3, Updated 09/09/2015 ORIGIN AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, searchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sh0416/ag_news."
shahadalkhalifa/Crypto_Whitepaper_Labeled,,,,,,,https://huggingface.co/datasets/shahadalkhalifa/Crypto_Whitepaper_Labeled,,,,,,,,,,,,,,,,,,,,shahadalkhalifa/Crypto_Whitepaper_Labeled dataset hosted on Hugging Face and contributed by the HF Datasets community
shahules786/prosocial-nsfw-reddit,,,,,,,https://huggingface.co/datasets/shahules786/prosocial-nsfw-reddit,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""prosocial-nsfw-reddit"" More Information needed"
shailja/Verilog_GitHub,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2212.11140,https://huggingface.co/datasets/shailja/Verilog_GitHub,Models for Automated Verilog RTL Code Generation Point of Contact: contact@shailjaâ€¦ See the full description on the dataset page: https://huggingface,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"VeriGen Data-Summary: The dataset comprises Verilog modules as entries. The entries were retrieved from the GitHub dataset on BigQuery. For training [models (https://huggingface.co/shailja/fine-tuned-codegen-2B-Verilog)], we filtered entries with no of characters exceeding 20000 and duplicates (exact duplicates ignoring whitespaces). Paper: Benchmarking Large Language Models for Automated Verilog RTL Code Generation Point of Contact: contact@shailjaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shailja/Verilog_GitHub."
ShapeNet/ShapeNetCore,,,,,,,https://huggingface.co/datasets/ShapeNet/ShapeNetCore,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"This repository contains ShapeNetCore (v2), a subset of ShapeNet.ShapeNetCore is a densely annotated subset of ShapeNet covering 55 common object categories with ~51,300 unique 3D models. Each model in ShapeNetCore are linked to an appropriate synset in WordNet 3.0. Please see DATA.md for details about the data. If you use ShapeNet data, you agree to abide by the ShapeNet terms of use. You are only allowed to redistribute the data to your research associates and colleagues provided thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ShapeNet/ShapeNetCore."
Sharathhebbar24/Indian-Constitution,,,,,,,https://huggingface.co/datasets/Sharathhebbar24/Indian-Constitution,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Indian Constitution Dataset The dataset can be used for text classification, text generation and text2text generation"
shareAI/ShareGPT-Chinese-English-90k,,,,,,,https://huggingface.co/datasets/shareAI/ShareGPT-Chinese-English-90k,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"ShareGPT-Chinese-English-90k Bilingual Human-Machine QA Dataset A high-quality Chinese-English parallel bilingual human-machine QA dataset, covering user questions in real and complex scenarios. It is used for training high-quality dialogue models (more robust in instruction distribution than those datasets generated by repeatedly calling API interfaces to simulate machine-generated Q&A, like Moss) Features: Provides fully semantically equivalent Chinese-English parallelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shareAI/ShareGPT-Chinese-English-90k."
shawhin/imdb-truncated,,,,,,,https://huggingface.co/datasets/shawhin/imdb-truncated,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""imdb-truncated"" More Information needed"
shb777/gemini-flash-2.0-speech,,,,,,https://arxiv.org/abs/2203.15135,https://huggingface.co/datasets/shb777/gemini-flash-2.0-speech,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"ðŸŽ™ï¸ Gemini Flash 2.0 Speech Dataset This is a high quality synthetic speech dataset generated by Gemini Flash 2.0 via the Multimodel Live API. It contains speech from 2 speakers - Puck (Male) and Kore (Female) in English. ã€½ï¸ Stats Total number of audio files: 47,256*2 = 94512Total duration: 1023527.20 seconds (284.31 hours) Average duration: 10.83 seconds Shortest file: 0.6 secondsLongest file: 92.12 seconds ðŸ§© Data Composition The text in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shb777/gemini-flash-2.0-speech."
shefali2023/webmd-data,,,,,,,https://huggingface.co/datasets/shefali2023/webmd-data,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""webmd-data"" More Information needed"
sheganinans/BTCUSD,,,,,,,https://huggingface.co/datasets/sheganinans/BTCUSD,,,,,,,,,,,,,,,,,,,,sheganinans/BTCUSD dataset hosted on Hugging Face and contributed by the HF Datasets community
ShengbinYue/DISC-Law-SFT,,,,,,https://arxiv.org/abs/2309.11325,https://huggingface.co/datasets/ShengbinYue/DISC-Law-SFT,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"DISC-Law-SFT Dataset Legal Intelligent systems in Chinese require a combination of various abilities, including legal text understanding and generation. To achieve this, we have constructed a high-quality supervised fine-tuning dataset called DISC-Law-SFT, which covers different legal scenarios such as legal information extraction, legal judgment prediction, legal document summarization, and legal question answering. DISC-Law-SFT comprises two subsets, DISC-Law-SFT-Pair andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ShengbinYue/DISC-Law-SFT."
shengqin/web-attacks,,,,,,,https://huggingface.co/datasets/shengqin/web-attacks,,,,,,,,,,,,,,,,,,,,shengqin/web-attacks dataset hosted on Hugging Face and contributed by the HF Datasets community
Shengtao/recipe,,,,,,,https://huggingface.co/datasets/Shengtao/recipe,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Shengtao/recipe dataset hosted on Hugging Face and contributed by the HF Datasets community
shi3z/alpaca_cleaned_ja_json,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/shi3z/alpaca_cleaned_ja_json,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for Dataset Name Data-Summary: This dataset card aims to be a base template for new datasets. It has been generated using this raw template. Supported Tasks and Leaderboards [More Information Needed] Languages [More Information Needed] Dataset Structure Data Instances [More Information Needed] Data Fields [More Information Needed] Data Splits [More Informationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shi3z/alpaca_cleaned_ja_json.
shibing624/alpaca-zh,,,,,,https://arxiv.org/abs/2304.03277,https://huggingface.co/datasets/shibing624/alpaca-zh,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for ""alpaca-zh"" æœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºŽGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚ Dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM It is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json Usage and License Notices The data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset shouldâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/alpaca-zh."
Shitao/bge-m3-data,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2402.03216,https://huggingface.co/datasets/Shitao/bge-m3-data,MS MARCO English NQ English HotpotQA English TriviaQA English SQuAD English COLIEE English PubMedQA English NLI from SimCSE English DuReader Chinese mMARCO-zh Chinese T2Ranking Chinese Law-GPT Chinese cMedQAv2 Chinese NLI-zh Chinese LeCaRDv2 Chinese Mr,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Data-Summary: This depository contains all the fine-tuning data for the bge-m3 model, including: Dataset Language MS MARCO English NQ English HotpotQA English TriviaQA English SQuAD English COLIEE English PubMedQA English NLI from SimCSE English DuReader Chinese mMARCO-zh Chinese T2Ranking Chinese Law-GPT Chinese cMedQAv2 Chinese NLI-zh Chinese LeCaRDv2 Chinese Mr.TyDi 11 languages MIRACL 16 languages MLDR 13 languages Note:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Shitao/bge-m3-data."
shiyue/chr_en,Text,Translation,Scientific,Text,"BLEU, METEOR, TER",https://arxiv.org/abs/2010.04791,https://huggingface.co/datasets/shiyue/chr_en,English,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"T5, mBART, M2M100",,,"Dataset Card for ChrEn Data-Summary: ChrEn is a Cherokee-English parallel dataset to facilitate machine translation research between Cherokee and English. ChrEn is extremely low-resource contains 14k sentence pairs in total, split in ways that facilitate both in-domain and out-of-domain evaluation. ChrEn also contains 5k Cherokee monolingual data to enable semi-supervised learning. Supported Tasks and Leaderboards The dataset is intended to useâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shiyue/chr_en."
shmuhammad/AfriSenti-twitter-sentiment,Text,,,,,https://github.com/afrisenti-semeval/afrisent-semeval-2023,https://huggingface.co/datasets/shmuhammad/AfriSenti-twitter-sentiment,,,,,,,,,,,,,,,,,,,,"AfriSenti is the largest sentiment analysis benchmark dataset for under-represented African languages---covering 110,000+ annotated tweets in 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and yoruba)."
Short-Answer-Feedback/saf_communication_networks_english,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Short-Answer-Feedback/saf_communication_networks_english,English,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""saf_communication_networks_english"" Data-Summary: Short Answer Feedback (SAF) dataset is a short answer dataset introduced in Your Answer is Incorrect... Would you like to know why? Introducing a Bilingual Short Answer Feedback Dataset (Filighera et al., ACL 2022) as a way to remedy the lack of content-focused feedback datasets. This version of the dataset contains 31 English questions covering a range of college-level communication networksâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Short-Answer-Feedback/saf_communication_networks_english."
ShoukanLabs/AniSpeech,,,,,,,https://huggingface.co/datasets/ShoukanLabs/AniSpeech,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"AniSpeech Dataset Welcome to the AniSpeech dataset, a continually expanding collection of captioned anime voices brought to you by ShoukanLabs. As we label more and more audio, they'll automagically be uploaded here for use, seperated by language ANNOUNCMENTS: An upcoming update will add an immense ammount of data to the dataset... however... because we cannot manually go through this dataset we have had to rely on manual quality estimation, as such, speakerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ShoukanLabs/AniSpeech."
showlab/ImpossibleVideos,,,,,,https://arxiv.org/abs/2503.14378,https://huggingface.co/datasets/showlab/ImpossibleVideos,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Impossible Videos Zechen Bai * Hai Ci * Mike Zheng Shou Show Lab, National University of Singapore ðŸ¤” What are impossible videos? Impossible videos refer to videos displaying counterfactual and anti-realityscenes that are impossible in real world. Please visit our website to find more"
ShreyaR/DepressionDetection,,,,,,,https://huggingface.co/datasets/ShreyaR/DepressionDetection,,,,,,,,,,,,,,,,,,,,ShreyaR/DepressionDetection dataset hosted on Hugging Face and contributed by the HF Datasets community
Shubhangi29/llava_med_instruct_60k_inline_mention_filtered,,,,,,,https://huggingface.co/datasets/Shubhangi29/llava_med_instruct_60k_inline_mention_filtered,,,,,,,,,,,,,,,,,,,,Shubhangi29/llava_med_instruct_60k_inline_mention_filtered dataset hosted on Hugging Face and contributed by the HF Datasets community
Shunian/kaggle-mbti-cleaned,,,,,,,https://huggingface.co/datasets/Shunian/kaggle-mbti-cleaned,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""kaggle-mbti-cleaned"" This dataset originated from Kaggle (MBTI) Myers-Briggs Personality Type Dataset. Some cleaning operations are made to this dataset to make it in a usable format for text classification process. See more detail in GitHub"
shunk031/jsnli,Text,,,,,https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E6%9C%AC%E8%AA%9ESNLI%28JSNLI%29%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88,https://huggingface.co/datasets/shunk031/jsnli,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,== æ—¥æœ¬èªžSNLI(JSNLI)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ == SNLI ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ—¥æœ¬èªžã«ç¿»è¨³ã—ãŸè‡ªç„¶è¨€èªžæŽ¨è«–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ç¿»è¨³ã—ã€è¨ˆç®—æ©Ÿã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã£ã¦ä½œæˆ è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã¯æ—¥æœ¬èªžã¨ã—ã¦æ„å‘³ãŒé€šã‚‹ã‹ã€ç¿»è¨³å¾Œã®ãƒ©ãƒ™ãƒ«ãŒå…ƒã®ãƒ©ãƒ™ãƒ«ã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã®2æ®µéšŽã®ã‚¯ãƒ©ã‚¦ãƒ‰ã‚½ãƒ¼ã‚·ãƒ³ã‚°ã«ã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
SiberiaSoft/SiberianPersonaChat,,,,,,,https://huggingface.co/datasets/SiberiaSoft/SiberianPersonaChat,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"SiberiaSoft/SiberianPersonaChat Ð”Ð°Ñ‚Ð°ÑÐµÑ‚ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¹, Ð´Ð¸Ð°Ð»Ð¾Ð³Ð¾Ð², QA Ð”Ð°Ð½Ð½Ñ‹Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð±Ñ‹Ð» ÑÐ¾Ð·Ð´Ð°Ð½ Ð´Ð»Ñ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð¾Ð²Ñ‹Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ñ Ð¸Ð¼Ð¸Ñ‚Ð°Ñ†Ð¸ÐµÐ¹ Ð»Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸. Ð‘Ð¾Ð»ÑŒÑˆÐ°Ñ Ñ‡Ð°ÑÑ‚ÑŒ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð° Ð±Ñ‹Ð»Ð° ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð° Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ chatGPT Ð¸ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð² Ðº Ð½ÐµÐ¹. ÐšÑ€Ð¾Ð¼Ðµ ÑÑ‚Ð¾Ð³Ð¾, Ð² ÑÐ¾ÑÑ‚Ð°Ð² Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð° Ð²Ñ…Ð¾Ð´Ð¸Ñ‚ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð½Ñ‹Ð¹ TolokaPersonaChatRus Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ð¹ Ð»Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¢Ñ‹ Ð¿Ð°Ñ€ÐµÐ½ÑŒ, Ð¿Ð¸Ð»Ð¾Ñ‚ ÑÐ°Ð¼Ð¾Ð»ÐµÑ‚Ð°. Ð£Ð²Ð»ÐµÐºÐ°ÐµÑˆÑŒÑÑ Ð´Ð°Ð¹Ð²Ð¸Ð½Ð³Ð¾Ð¼. Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÑˆÑŒ Ð¼Ð°Ñ€ÐºÐ¸. Ð›ÑŽÐ±Ð¸ÑˆÑŒ Ð´Ñ€ÐµÐ²Ð½ÑŽÑŽ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ. Ð¢Ñ‹ Ð´ÐµÐ²ÑƒÑˆÐºÐ°, Ñ…ÑƒÐ´Ð¾Ð¶Ð½Ð¸Ñ†Ð°. Ð£Ð²Ð»ÐµÐºÐ°ÐµÑˆÑŒÑÑ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÐµÐ²Ñ‹Ð¼â€¦ See the full description on the dataset page: https://huggingface.co/datasets/SiberiaSoft/SiberianPersonaChat."
siddicky/offsec-help-docs,,,,,,,https://huggingface.co/datasets/siddicky/offsec-help-docs,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,siddicky/offsec-help-docs dataset hosted on Hugging Face and contributed by the HF Datasets community
sidhq/email-thread-summary,,,,,,,https://huggingface.co/datasets/sidhq/email-thread-summary,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""email-thread-summary"" More Information needed"
sieecc/SOKI,,,,,,,https://huggingface.co/datasets/sieecc/SOKI,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,sieecc/SOKI dataset hosted on Hugging Face and contributed by the HF Datasets community
sieu-n/korean-newstext-dump,,,,,,,https://huggingface.co/datasets/sieu-n/korean-newstext-dump,,,,,,,,,,,,,,,,,,,,sieu-n/korean-newstext-dump dataset hosted on Hugging Face and contributed by the HF Datasets community
Sifal/KabyleWikipedia,,,,,,,https://huggingface.co/datasets/Sifal/KabyleWikipedia,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,Sifal/KabyleWikipedia dataset hosted on Hugging Face and contributed by the HF Datasets community
Sightation/SightationPreference,,,,,,https://arxiv.org/abs/2503.13369,https://huggingface.co/datasets/Sightation/SightationPreference,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"SighationPreference Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions ðŸ“„ arXiv ðŸ¤— Dataset Often, the needs and visual abilities differ between the annotator group and the end user group. Generating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain. Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationPreference."
sijiachenn/CRTrack,,,,,,,https://huggingface.co/datasets/sijiachenn/CRTrack,,,,,,,,,,,,,,,,,,,,ã€AAAI 2025ã€‘ Cross-View Referring Multi-Object Tracking This dataset is utilized for the Cross-View Referring Multi-Object Tracking (CRMOT) task. license: mit
sil-ai/bloom-vist,Text,,,,,https://ai.sil.org/,https://huggingface.co/datasets/sil-ai/bloom-vist,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"This version of the Bloom Library data is developed specifically for the Visual Story Telling (VIST) task. It includes data from 363 languages across 36 language families, with many of the languages represented being extremely low resourced languages."
silk-road/Wizard-LM-Chinese-instruct-evol,,,,,,,https://huggingface.co/datasets/silk-road/Wizard-LM-Chinese-instruct-evol,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,1,,,,,,,,,"Wizard-LM-Chineseæ˜¯åœ¨MSRAçš„Wizard-LMæ•°æ®é›†ä¸Šï¼Œå¯¹æŒ‡ä»¤è¿›è¡Œç¿»è¯‘ï¼Œç„¶åŽå†è°ƒç”¨GPTèŽ·å¾—ç­”æ¡ˆçš„æ•°æ®é›† Wizard-LMåŒ…å«äº†å¾ˆå¤šéš¾åº¦è¶…è¿‡Alpacaçš„æŒ‡ä»¤ã€‚ ä¸­æ–‡çš„é—®é¢˜ç¿»è¯‘ä¼šæœ‰å°‘é‡æŒ‡ä»¤æ³¨å…¥å¯¼è‡´ç¿»è¯‘å¤±è´¥çš„æƒ…å†µ ä¸­æ–‡å›žç­”æ˜¯æ ¹æ®ä¸­æ–‡é—®é¢˜å†è¿›è¡Œé—®è¯¢å¾—åˆ°çš„ã€‚ æˆ‘ä»¬ä¼šé™†ç»­å°†æ›´å¤šæ•°æ®é›†å‘å¸ƒåˆ°hfï¼ŒåŒ…æ‹¬ Coco Captionçš„ä¸­æ–‡ç¿»è¯‘ CoQAçš„ä¸­æ–‡ç¿»è¯‘ CNewSumçš„Embeddingæ•°æ® å¢žå¹¿çš„å¼€æ”¾QAæ•°æ® WizardLMçš„ä¸­æ–‡ç¿»è¯‘ å¦‚æžœä½ ä¹Ÿåœ¨åšè¿™äº›æ•°æ®é›†çš„ç­¹å¤‡ï¼Œæ¬¢è¿Žæ¥è”ç³»æˆ‘ä»¬ï¼Œé¿å…é‡å¤èŠ±é’±ã€‚ éª†é©¼(Luotuo): å¼€æºä¸­æ–‡å¤§è¯­è¨€æ¨¡åž‹ https://github.com/LC1332/Luotuo-Chinese-LLM éª†é©¼(Luotuo)é¡¹ç›®æ˜¯ç”±å†·å­æ˜‚ @ å•†æ±¤ç§‘æŠ€, é™ˆå¯æº @ åŽä¸­å¸ˆèŒƒå¤§å­¦ ä»¥åŠ æŽé²é² @ å•†æ±¤ç§‘æŠ€ å‘èµ·çš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡åž‹å¼€æºé¡¹ç›®ï¼ŒåŒ…å«äº†ä¸€ç³»åˆ—è¯­è¨€æ¨¡åž‹ã€‚ ( æ³¨æ„: é™ˆå¯æº æ­£åœ¨å¯»æ‰¾2024æŽ¨å…å¯¼å¸ˆï¼Œæ¬¢è¿Žè”ç³» ) éª†é©¼é¡¹ç›®ä¸æ˜¯å•†æ±¤ç§‘æŠ€çš„å®˜æ–¹äº§å“ã€‚ Citationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/silk-road/Wizard-LM-Chinese-instruct-evol."
SilpaCS/kneeosteoarthritis,,,,,,,https://huggingface.co/datasets/SilpaCS/kneeosteoarthritis,,,,,,,,,,,,,,,,,,,,SilpaCS/kneeosteoarthritis dataset hosted on Hugging Face and contributed by the HF Datasets community
silver/mmchat,Text,,,,,https://www.zhengyinhe.com/datasets/,https://huggingface.co/datasets/silver/mmchat,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,MMChat is a large-scale dialogue dataset that contains image-grounded dialogues in Chinese. Each dialogue in MMChat is associated with one or more images (maximum 9 images per dialogue). We design various strategies to ensure the quality of the dialogues in MMChat.
silvermete0r/airba_dataset_upd_20_march_2025,,,,,,,https://huggingface.co/datasets/silvermete0r/airba_dataset_upd_20_march_2025,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,AIRBA Case Championship Dataset 2025
simecek/Human_DNA_v0_DNABert6tokenized,,,,,,,https://huggingface.co/datasets/simecek/Human_DNA_v0_DNABert6tokenized,,,,,,,,,,,,,,,,,,,,simecek/Human_DNA_v0_DNABert6tokenized dataset hosted on Hugging Face and contributed by the HF Datasets community
simon3000/genshin-voice,,,,,,,https://huggingface.co/datasets/simon3000/genshin-voice,,,,,,,,,,,,,,,,,,,,"Genshin Voice Genshin Voice is a dataset of voice lines from the popular game Genshin Impact. Hugging Face ðŸ¤— Genshin-Voice Last update at 2024-08-30 463383 wavs 20231 without speaker (4%) 24819 without transcription (5%) 602 without inGameFilename (0%) Dataset Details Dataset Description The dataset contains voice lines from the game's characters in multiple languages, including Chinese, English, Japanese, and Korean. The voice lines are spokenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/simon3000/genshin-voice."
simplescaling/s1K-1.1,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2501.19393,https://huggingface.co/datasets/simplescaling/s1K-1.1,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for s1K Data-Summary: s1K-1.1 consists of the same 1,000 questions as in s1K but with traces instead generated by DeepSeek r1. We find that these traces lead to much better performance. Usage # pip install -q datasets from datasets import load_dataset ds = load_dataset(""simplescaling/s1K-1.1"")[""train""] ds[0] Dataset Structure Data Instances An"
sinarashidi/alpaca-persian,,,,,,,https://huggingface.co/datasets/sinarashidi/alpaca-persian,,,,,,,,,,,,,,,,,,,,sinarashidi/alpaca-persian dataset hosted on Hugging Face and contributed by the HF Datasets community
SirNeural/flan_v2,Text,General,General,Text,"Accuracy, F1 Score",https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html,https://huggingface.co/datasets/SirNeural/flan_v2,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,60,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Flan V2 Data-Summary: This is a processed version of the Flan V2 dataset. I'm not affiliated with the creators, I'm just releasing the files in an easier-to-access format after processing. The authors of the Flan Collection recommend experimenting with different mixing ratio's of tasks to get optimal results downstream. Setup Instructions Here are the steps I followed to get everything working: Build AESLC andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SirNeural/flan_v2."
sivan22/hebrew-handwritten-dataset,,,,,,,https://huggingface.co/datasets/sivan22/hebrew-handwritten-dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-3.0/,,,,,,,,,,,,"Dataset Information Keywords Hebrew, handwritten, letters Description HDD_v0 consists of images of isolated Hebrew characters together with training and test sets subdivision. The images were collected from hand-filled forms. For more details, please refer to [1]. When using this dataset in research work, please cite [1]. [1] I. Rabaev, B. Kurar Barakat, A. Churkin and J. El-Sana. The HHD Dataset. The 17th International Conference on Frontiers inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sivan22/hebrew-handwritten-dataset."
sixf0ur/GuanacoDataset-de,,,,,,,https://huggingface.co/datasets/sixf0ur/GuanacoDataset-de,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,This dataset was taken from JosephusCheung/GuanacoDataset and filtered to German entries.
Skepsun/lawyer_llama_data,,,,,,,https://huggingface.co/datasets/Skepsun/lawyer_llama_data,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,åŸºäºŽlawyer-llamaçš„å¼€æºæ•°æ®è¿›è¡Œäº†ç®€å•çš„æ•´åˆï¼Œæ ¼å¼ç¬¦åˆLLaMA-Efficient-Tuningçš„æ ‡å‡†æ ¼å¼ï¼Œsourceå­—æ®µä¿å­˜äº†æ•°æ®çš„åŽŸå§‹æ–‡ä»¶åã€‚
skeskinen/TinyStories-Instruct-hf,,,,,,https://arxiv.org/abs/2305.07759,https://huggingface.co/datasets/skeskinen/TinyStories-Instruct-hf,,,,,,,,,,,,,,,,,,,,"A description of this dataset can be found at https://arxiv.org/abs/2305.07759 Copied from roneneldan/TinyStoriesInstruct Modified with: import ftfy.bad_codecs from datasets import Dataset, DatasetDict train = open('./TinyStories-Instruct-train.txt', 'r', encoding='sloppy-windows-1252').read() train = train.split('<|endoftext|>') train = [l.strip() for l in train] valid = open('./TinyStories-Instruct-valid.txt', 'r', encoding='sloppy-windows-1252').read() valid = valid.split('<|endoftext|>')â€¦ See the full description on the dataset page: https://huggingface.co/datasets/skeskinen/TinyStories-Instruct-hf."
skt/kobest_v1,Text,General,UI,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2204.04541,https://huggingface.co/datasets/skt/kobest_v1,understanding tasks that requires advanced knowledge in Korean,,,,,5,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for KoBEST Data-Summary: KoBEST is a Korean benchmark suite consists of 5 natural language understanding tasks that requires advanced knowledge in Korean. Supported Tasks and Leaderboards Boolean Question Answering, Choice of Plausible Alternatives, Words-in-Context, HellaSwag, Sentiment Negation Recognition Languages ko-KR Dataset Structure Data Instances KB-BoolQ An"
SkyHuReal/DrugBank-Alpaca,,,,,,,https://huggingface.co/datasets/SkyHuReal/DrugBank-Alpaca,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,SkyHuReal/DrugBank-Alpaca dataset hosted on Hugging Face and contributed by the HF Datasets community
Skylion007/openwebtext,Text,,,,,https://skylion007.github.io/OpenWebTextCorpus/,https://huggingface.co/datasets/Skylion007/openwebtext,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,An open-source replication of the WebText dataset from OpenAI.
skytnt/anime-segmentation,Text,,,,,,https://huggingface.co/datasets/skytnt/anime-segmentation,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,A segmentation dataset for anime character
sled-umich/Conversation-Entailment,,,,,,,https://huggingface.co/datasets/sled-umich/Conversation-Entailment,,,,,,,,https://choosealicense.com/licenses/undefined/,,,,,,,,,,,,"Conversation-Entailment Official dataset for Towards Conversation Entailment: An Empirical Investigation. Chen Zhang, Joyce Chai. EMNLP, 2010 Overview Textual entailment has mainly focused on inference from written text in monologue. Recent years also observed an increasing amount of conversational data such as conversation scripts of meetings, call center records, court proceedings, as well as online chatting. Although conversation is a form of language, it isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sled-umich/Conversation-Entailment."
Slep/LAION-RVS-Fashion,,,,,,https://arxiv.org/abs/2306.02928,https://huggingface.co/datasets/Slep/LAION-RVS-Fashion,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,LAION - Referred Visual Search - Fashion Introduced in LRVS-Fashion: Extending Visual Search with Referring Instructions Simon Lepage â€” JÃ©rÃ©mie Mary â€” David Picard CRITEO AI Lab & ENPC Useful Links Test set â€” Benchmark Codeâ€” LRVS-F Leaderboard â€” Demo Composition LAION-RVS-Fashion is composed of images from : LAION 2B EN LAION 2B MULTI TRANSLATED LAION 1B NOLANG TRANSLATED These images have been grouped based on extracted product IDs. Each productâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Slep/LAION-RVS-Fashion.
SLPL/syntran-fa,Text,,,,,,https://huggingface.co/datasets/SLPL/syntran-fa,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"SynTran-fa Syntactic Transformed Version of Farsi QA datasets to make fluent responses from questions and short answers. You can use this dataset by the code below: import datasets data = datasets.load_dataset('SLPL/syntran-fa', split=""train"") Dataset Description Homepage: Sharif-SLPL Repository: SynTran-fa Point of Contact: Sadra Sabouri Paper: SynTran-fa: Generating Comprehensive Answers for Farsi QA Pairs via Syntactic Transformation Datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SLPL/syntran-fa."
SmallDoge/SmallThoughts,,,,,,,https://huggingface.co/datasets/SmallDoge/SmallThoughts,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"SmallThoughts Open synthetic reasoning dataset, covering math, science, code, and puzzles. To address the issue of the existing DeepSeek R1 distilled data being too long, this dataset constrains the reasoning trajectory to be more precise and concise while retaining the reflective nature. We also open-sourced the pipeline code for distilled data here, with just one command you can generate your own dataset. How to use You can loadâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SmallDoge/SmallThoughts."
smilegate-ai/kor_unsmile,,,,,,,https://huggingface.co/datasets/smilegate-ai/kor_unsmile,,,,,,,,,,,,,,,,,,,,smilegate-ai/kor_unsmile dataset hosted on Hugging Face and contributed by the HF Datasets community
snoop2head/enron_aeslc_emails,,,,,,,https://huggingface.co/datasets/snoop2head/enron_aeslc_emails,,,,,,,,,,,,,,,,,,,,snoop2head/enron_aeslc_emails dataset hosted on Hugging Face and contributed by the HF Datasets community
SocialGrep/one-million-reddit-questions,Text,General,General,Text,"Accuracy, F1 Score",https://socialgrep.com/datasets?utm_source=huggingface&utm_medium=link&utm_campaign=dataset&utm_term=onemillionquestions,https://huggingface.co/datasets/SocialGrep/one-million-reddit-questions,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for one-million-reddit-questions Data-Summary: This corpus contains a million posts on /r/AskReddit, annotated with their score. Languages Mainly English. Dataset Structure Data Instances A data point is a Reddit post. Data Fields 'type': the type of the data point. Can be 'post' or 'comment'. 'id': the base-36 Reddit ID of the data point. Unique when combined with type. 'subreddit.id':â€¦ See the full description on the dataset page: https://huggingface.co/datasets/SocialGrep/one-million-reddit-questions."
softcatala/catalan-dictionary,Audio,General,UI,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/softcatala/catalan-dictionary,,,,,,,,https://choosealicense.com/licenses/gpl-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ca-text-corpus Data-Summary: Catalan word lists with part of speech labeling curated by humans. Contains 1 180 773 forms including verbs, nouns, adjectives, names or toponyms. These word lists are used to build applications like Catalan spellcheckers or verb querying applications. Supported Tasks and Leaderboards [More Information Needed] Languages Catalan (ca). Dataset Structure The dataset containsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/softcatala/catalan-dictionary."
SoLID/shellcode_i_a32,,,,,,https://arxiv.org/abs/2104.13100,https://huggingface.co/datasets/SoLID/shellcode_i_a32,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,https://github.com/dessertlab/Shellcode_IA32,,,,,,,,Shellcode_IA32 is a dataset for shellcode generation from English intents. The shellcodes are compilable on Intel Architecture 32-bits.
solomonk/reddit_mental_health_posts,,,,,,,https://huggingface.co/datasets/solomonk/reddit_mental_health_posts,,,,,,,,,,,,,,,,,,,,Reddit posts about mental health files adhd.csv from r/adhd aspergers.csv from r/aspergers depression.csv from r/depression ocd.csv from r/ocd ptsd.csv from r/ptsd fields author body created_utc id num_comments score subreddit title upvote_ratio url for more details about theses fields Praw Submission.
somosnlp-hackathon-2022/readability-es-caes,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/somosnlp-hackathon-2022/readability-es-caes,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for [readability-es-caes] Dataset Description Data-Summary: This dataset is a compilation of short articles from websites dedicated to learn Spanish as a second language. These articles have been compiled from the following sources: CAES corpus (MartÃ­nez et al., 2019): the ""Corpus de Aprendices del EspaÃ±ol"" is a collection of texts produced by Spanish L2 learners from Spanish learning centers and universities. These text are producedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/somosnlp-hackathon-2022/readability-es-caes."
somosnlp/recetas-cocina,,,,,,,https://huggingface.co/datasets/somosnlp/recetas-cocina,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,somosnlp/recetas-cocina dataset hosted on Hugging Face and contributed by the HF Datasets community
songweig/imagenet_sketch,Text,,,,,https://github.com/HaohanWang/ImageNet-Sketch,https://huggingface.co/datasets/songweig/imagenet_sketch,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"ImageNet-Sketch data set consists of 50000 images, 50 images for each of the 1000 ImageNet classes. We construct the data set with Google Image queries ""sketch of __"", where __ is the standard class name. We only search within the ""black and white"" color scheme. We initially query 100 images for every class, and then manually clean the pulled images by deleting the irrelevant images and images that are for similar but different classes. For some classes, there are less than 50 images after manually cleaning, and then we augment the data set by flipping and rotating the images."
sonnetechnology/license-plate-text-recognition-full,,,,,,,https://huggingface.co/datasets/sonnetechnology/license-plate-text-recognition-full,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for ""license-plate-text-recognition-full"" Background Information This dataset is generated from keremberke/license-plate-object-detection dataset. What we have done is: Get the Bounding Boxes for each plate in an image, Crop the image to make the plate only visible, Run it through the microsoft/trocr-large-printed model to extract the written information. Structure of the Dataset It has the same structure as theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sonnetechnology/license-plate-text-recognition-full."
SophieTr/reddit_clean,,,,,,,https://huggingface.co/datasets/SophieTr/reddit_clean,,,,,,,,,,,,,,,,,,,,SophieTr/reddit_clean dataset hosted on Hugging Face and contributed by the HF Datasets community
sorry-bench/sorry-bench-202503,,,,,,https://arxiv.org/abs/2406.14598,https://huggingface.co/datasets/sorry-bench/sorry-bench-202503,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"Dataset Card for SORRY-Bench Dataset (2025/03) ðŸ Website ðŸ“‘Paper ðŸ“šDataset ðŸ’»Github ðŸ§‘â€âš–ï¸Human Judgment Dataset ðŸ¤–Judge LLM ðŸª§UPDATE: In this iteration, we removed the category ""Impersonation"" due to its ambiguous definition, and that most models more or less fulfill such requests.This dataset contains 9.2K potentially unsafe instructions, intended to be used for LLM safety refusal evaluation. Particularly, our base dataset consists of 440 unsafeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sorry-bench/sorry-bench-202503."
souljoy/COVID-19_weibo_emotion,,,,,,,https://huggingface.co/datasets/souljoy/COVID-19_weibo_emotion,,,,,,,,,,,,,,,,,,,,"COVID-19 Epidemic Weibo Emotional Dataset, the content of Weibo in this dataset is the epidemic Weibo obtained by using relevant keywords to filter during the epidemic, and its content is related to COVID-19. Each tweet is labeled as one of the following six categories: neutral (no emotion), happy (positive), angry (angry), sad (sad), fear (fear), surprise (surprise) The COVID-19 Weibo training dataset includes 8,606 Weibos, the validation set contains 2,000 Weibos, and the test datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/souljoy/COVID-19_weibo_emotion."
Sp1786/multiclass-sentiment-analysis-dataset,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Sp1786/multiclass-sentiment-analysis-dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for Dataset Name Data-Summary: This dataset card aims to be a base template for new datasets. It has been generated using this raw template. Supported Tasks and Leaderboards [More Information Needed] Languages [More Information Needed] Dataset Structure Data Instances [More Information Needed] Data Fields [More Information Needed] Data Splits [More Informationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sp1786/multiclass-sentiment-analysis-dataset.
spasis/datasets-github-issues,,,,,,,https://huggingface.co/datasets/spasis/datasets-github-issues,,,,,,,,,,,,,,,,,,,,spasis/datasets-github-issues dataset hosted on Hugging Face and contributed by the HF Datasets community
speechbrain/common_language,Text,,,,,https://zenodo.org/record/5036977,https://huggingface.co/datasets/speechbrain/common_language,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"This dataset is composed of speech recordings from languages that were carefully selected from the CommonVoice database. The total duration of audio recordings is 45.1 hours (i.e., 1 hour of material for each language). The dataset has been extracted from CommonVoice to train language-id systems."
speechcolab/gigaspeech,,,,,,https://github.com/SpeechColab/GigaSpeech,https://huggingface.co/datasets/speechcolab/gigaspeech,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,9,,,,,,,,,"GigaSpeech is an evolving, multi-domain English speech recognition corpus with 10,000 hours of high quality labeled audio suitable for supervised training, and 40,000 hours of total audio suitable for semi-supervised and unsupervised training. Around 40,000 hours of transcribed audio is first collected from audiobooks, podcasts and YouTube, covering both read and spontaneous speaking styles, and a variety of topics, such as arts, science, sports, etc. A new forced alignment and segmentation pipeline is proposed to create sentence segments suitable for speech recognition training, and to filter out segments with low-quality transcription. For system training, GigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h, and 10000h. For our 10,000-hour XL training subset, we cap the word error rate at 4% during the filtering/validation stage, and for all our other smaller training subsets, we cap it at 0%. The DEV and TEST evaluation sets, on the other hand, are re-processed by professional human transcribers to ensure high transcription quality."
spiritx2023/ThuCnews,,,,,,,https://huggingface.co/datasets/spiritx2023/ThuCnews,,,,,,,,,,,,,,,,,,,,spiritx2023/ThuCnews dataset hosted on Hugging Face and contributed by the HF Datasets community
spyysalo/bc2gm_corpus,Text,General,General,Text,"Accuracy, F1 Score",https://github.com/spyysalo/bc2gm-corpus/,https://huggingface.co/datasets/spyysalo/bc2gm_corpus,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for bc2gm_corpus Data-Summary: [More Information Needed] Supported Tasks and Leaderboards [More Information Needed] Languages [More Information Needed] Dataset Structure Data Instances [More Information Needed] Data Fields id: Sentence identifier. tokens: Array of tokens composing a sentence. ner_tags: Array of tags, where 0 indicates no disease mentioned, 1 signals theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/spyysalo/bc2gm_corpus."
Sridevi/python_textbooks,,,,,,,https://huggingface.co/datasets/Sridevi/python_textbooks,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Sridevi/python_textbooks dataset hosted on Hugging Face and contributed by the HF Datasets community
ssampa17/sample_clevr,,,,,,,https://huggingface.co/datasets/ssampa17/sample_clevr,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,ssampa17/sample_clevr dataset hosted on Hugging Face and contributed by the HF Datasets community
ssbuild/alaca_chain-of-thought,,,,,,,https://huggingface.co/datasets/ssbuild/alaca_chain-of-thought,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,ssbuild/alaca_chain-of-thought dataset hosted on Hugging Face and contributed by the HF Datasets community
sshh12/planet-textures,,,,,,,https://huggingface.co/datasets/sshh12/planet-textures,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,Source: https://planet-texture-maps.fandom.com/wiki/Planet_Texture_Maps_Wiki GitHub: https://github.com/sshh12/planet-diffusion
stable-bias/identities,Image-Text,General,General,Text,"Accuracy, F1 Score",https://huggingface.co/spaces/tti-bias/stable-bias,https://huggingface.co/datasets/stable-bias/identities,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,identities,"BERT, RoBERTa, T5",,,"Dataset Card for identities Data-Summary: ðŸ—ï¸ WORK IN PROGRESS âš ï¸ DISCLAIMER: The images in this dataset were generated by text-to-image systems and may depict offensive stereotypes or contain explicit content. The Identities dataset is a collection of computer-generated images generated using Text-to-Image (TTI) systems. In order to generate a diverse set of prompts to evaluate the system outputsâ€™ variation across dimensions of interest, we use the patternâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stable-bias/identities."
staghado/Bentham,,,,,,,https://huggingface.co/datasets/staghado/Bentham,,,,,,,,,,,,,,,,,,,,staghado/Bentham dataset hosted on Hugging Face and contributed by the HF Datasets community
Stanford/wikitablequestions,Text,,,,,https://nlp.stanford.edu/software/sempre/wikitable,https://huggingface.co/datasets/Stanford/wikitablequestions,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,This WikiTableQuestions dataset is a large-scale dataset for the task of question answering on semi-structured tables.
stanfordnlp/imdb,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",http://ai.stanford.edu/~amaas/data/sentiment/,https://huggingface.co/datasets/stanfordnlp/imdb,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for ""imdb"" Data-Summary: Large Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Supported Tasks and Leaderboards More Information Needed Languages More Information Neededâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stanfordnlp/imdb."
starmpcc/Asclepius-Synthetic-Clinical-Notes,Text,General,UI,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2309.00237,https://huggingface.co/datasets/starmpcc/Asclepius-Synthetic-Clinical-Notes,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Asclepius: Synthetic Clincal Notes & Instruction Dataset Data-Summary: This dataset is official dataset for Asclepius (arxiv) This dataset is composed with Clinical Note - Question - Answer format to build a clinical LLMs. We first synthesized synthetic notes from PMC-Patients case reports with GPT-3.5 Then, we generate instruction-answer pairs for 157k synthetic discharge summaries Supported Tasks This dataset covers below 8 tasks Named Entityâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/starmpcc/Asclepius-Synthetic-Clinical-Notes."
starvector/text2svg-stack,,,,,,https://arxiv.org/abs/2312.11556,https://huggingface.co/datasets/starvector/text2svg-stack,,,,,,,,,,,,,,,,,,,,Dataset Card for svg-stack-v2 Dataset Description This dataset contains SVG code
stas/openwebtext-10k,,,,,,,https://huggingface.co/datasets/stas/openwebtext-10k,,,,,,,,,,,,,,,,,,,,An open-source replication of the WebText dataset from OpenAI. This is a small subset representing the first 10K records from the original dataset - created for testing. The full 8M-record dataset is at https://huggingface.co/datasets/openwebtext
statmt/cc100,Text,,,,,https://data.statmt.org/cc-100/,https://huggingface.co/datasets/statmt/cc100,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,This corpus is an attempt to recreate the dataset used for training XLM-R. This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages (indicated by *_rom). This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Each file comprises of documents separated by double-newlines and paragraphs within the same document separated by a newline. The data is generated using the open source CC-Net repository. No claims of intellectual property are made on the work of preparation of the corpus.
statworx/haiku,,,,,,,https://huggingface.co/datasets/statworx/haiku,,,,,,,,https://choosealicense.com/licenses/undefined/,,,,,,,,,,,,Dataset Card for Haiku Data
stevendevoe/news-article-summary,,,,,,,https://huggingface.co/datasets/stevendevoe/news-article-summary,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""news-article-summary"" More Information needed"
stingning/ultrachat,,,,,,,https://huggingface.co/datasets/stingning/ultrachat,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Card for Dataset Name Dataset Description An open-source, large-scale, and multi-round dialogue data powered by Turbo APIs. In consideration of factors such as safeguarding privacy, we do not directly use any data available on the Internet as prompts. To ensure generation quality, two separate ChatGPT Turbo APIs are adopted in generation, where one plays the role of the user to generate queries and the other generates the response. We instruct the userâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stingning/ultrachat."
stochastic/random_streetview_images_pano_v0.0.2,Image,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/stochastic/random_streetview_images_pano_v0.0.2,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",c images scraped from randomstreetview,,"Dataset Card for panoramic street view images (v.0.0.2) Data-Summary: The random streetview images dataset are labeled, panoramic images scraped from randomstreetview.com. Each image shows a location accessible by Google Streetview that has been roughly combined to provide ~360 degree view of a single location. The dataset was designed with the intent to geolocate an image purely based on its visual content. Supported Tasks and Leaderboards Noneâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stochastic/random_streetview_images_pano_v0.0.2."
stockmark/ner-wikipedia-dataset,,,,,,,https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,Wikipediaã‚’ç”¨ã„ãŸæ—¥æœ¬èªžã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ GitHub: https://github.com/stockmarkteam/ner-wikipedia-dataset/ LICENSE: CC-BY-SA 3.0 Developed by Stockmark Inc.
strombergnlp/broad_twitter_corpus,Text,,,,,https://github.com/GateNLP/broad_twitter_corpus,https://huggingface.co/datasets/strombergnlp/broad_twitter_corpus,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"This is the Broad Twitter corpus, a dataset of tweets collected over stratified times, places and social uses. The goal is to represent a broad range of activities, giving a dataset more representative of the language used in this hardest of social media formats to process. Further, the BTC is annotated for named entities. For more details see [https://aclanthology.org/C16-1111/](https://aclanthology.org/C16-1111/)"
studymakesmehappyyyyy/math_data_ocr,,,,,,,https://huggingface.co/datasets/studymakesmehappyyyyy/math_data_ocr,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,studymakesmehappyyyyy/math_data_ocr dataset hosted on Hugging Face and contributed by the HF Datasets community
substratusai/k8s-instructions,,,,,,,https://huggingface.co/datasets/substratusai/k8s-instructions,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,substratusai/k8s-instructions dataset hosted on Hugging Face and contributed by the HF Datasets community
succinctly/midjourney-prompts,,,,,,,https://huggingface.co/datasets/succinctly/midjourney-prompts,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Midjourney is an independent research lab whose broad mission is to ""explore new mediums of thought"". In 2022, they launched a text-to-image service that, given a natural language prompt, produces visual depictions that are faithful to the description. Their service is accessible via a public Discord server: users issue a query in natural language, and the Midjourney bot returns AI-generated images that follow the given description. The raw dataset (with Discord messages) can be found onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/succinctly/midjourney-prompts."
Suchinthana/Sinhala_OCR_Dataset_Synthetic,,,,,,,https://huggingface.co/datasets/Suchinthana/Sinhala_OCR_Dataset_Synthetic,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,This is a synthetically generated dataset you can generate your own with the help of below codes in this github repo: https://github.com/suchinthana00/Synthetic_OCR_Dataset_Generator
SUFE-AIFLM-Lab/FinEval,,,,,,https://arxiv.org/abs/2308.09975,https://huggingface.co/datasets/SUFE-AIFLM-Lab/FinEval,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"The FinEval Dataset FinEval is a collection of high-quality multiple-choice questions covering various domains such as finance, economics, accounting, and certifications. It consists of 4,661 questions spanning across 34 distinct academic subjects. To ensure a comprehensive assessment of model performance, FinEval employs various methods including zero-shot, few-shot, answer-only, and chain-of-thought prompts. Evaluating state-of-the-art large language models in both Chinese and Englishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SUFE-AIFLM-Lab/FinEval."
sungmogi/en2ko_hiphop,,,,,,,https://huggingface.co/datasets/sungmogi/en2ko_hiphop,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""en2ko_hiphop"" Copyright Disclaimer The dataset ""en2ko_hiphop"" was curated from publicly available sources and is believed to be in the public domain. The translations provided in this dataset are the work of volunteers and members of the community, and they have been collected and curated to facilitate research and analysis. However, it is important to acknowledge that copyright issues cannot be entirely ruled out. Therefore, users of the datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sungmogi/en2ko_hiphop."
sunilSabnis/pixelart,,,,,,,https://huggingface.co/datasets/sunilSabnis/pixelart,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,pixel giffusion Dataset of pixel-style art generated from stable-diffusion model
sunlab/patch_db,,,,,,,https://huggingface.co/datasets/sunlab/patch_db,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"PatchDB: A Large-Scale Security Patch Dataset Description To foster large-scale research on vulnerability mitigation and to enable a comparison of different detection approaches, we make our dataset PatchDB from our DSN'21 paper publicly available. PatchDB is a large-scale security patch dataset that contains around 12,073 security patches and 23,742 non-security patches from the real world. You can find more details on the dataset in the paper ""PatchDB: Aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sunlab/patch_db."
sunzeyeah/chinese_chatgpt_corpus,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/sunzeyeah/chinese_chatgpt_corpus,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for chinese_chatgpt_corpus Data-Summary: This repo collects chinese corpus for Supervised Finetuning (SFT) and Reinforcement Learning From Human Feedback (RLHF). Supported Tasks and Leaderboards More Information Needed Languages Chinese Dataset Structure Data Instances train_data_external_v1.jsonl Size of downloaded dataset files: 5.04 GB Size of the generated dataset: 0 GBâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sunzeyeah/chinese_chatgpt_corpus.
suolyer/translate_zh2en,,,,,,,https://huggingface.co/datasets/suolyer/translate_zh2en,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,suolyer/translate_zh2en dataset hosted on Hugging Face and contributed by the HF Datasets community
super-dainiu/medagents-benchmark,,,,,,https://arxiv.org/abs/2503.07459,https://huggingface.co/datasets/super-dainiu/medagents-benchmark,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"MedAgentsBench Dataset Overview This dataset is part of the MedAgentsBench, which focuses on benchmarking thinking models and agent frameworks for complex medical reasoning. The benchmark contains challenging medical questions specifically selected where models achieve less than 50% accuracy. Dataset Structure The benchmark includes the following medical question-answering datasets: Dataset Description MedQA Medical domain question answering datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/super-dainiu/medagents-benchmark."
Supermaxman/esa-hubble,Image-Text,General,General,Text,"Accuracy, F1 Score",https://esahubble.org/,https://huggingface.co/datasets/Supermaxman/esa-hubble,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ESA Hubble Deep Space Images & Captions Data-Summary: The ESA Hubble Deep Space Images & Captions dataset is composed primarily of Hubble deep space scans as high-resolution images, along with textual descriptions written by ESA/Hubble. Metadata is also included, which enables more detailed filtering and understanding of massive space scans. The purpose of this dataset is to enable text-to-image generation methods for generating high-qualityâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Supermaxman/esa-hubble."
Suprit/CMtMedQA,,,,,,,https://huggingface.co/datasets/Suprit/CMtMedQA,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Suprit/CMtMedQA dataset hosted on Hugging Face and contributed by the HF Datasets community
suriyagunasekar/stackoverflow-with-meta-data,,,,,,,https://huggingface.co/datasets/suriyagunasekar/stackoverflow-with-meta-data,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""stackoverflow-with-meta-data"" More Information needed"
suzyanil/nba-data,,,,,,,https://huggingface.co/datasets/suzyanil/nba-data,,,,,,,,https://choosealicense.com/licenses/creativeml-openrail-m/,,,,,,,,,,,,suzyanil/nba-data dataset hosted on Hugging Face and contributed by the HF Datasets community
svjack/pokemon-blip-captions-en-zh,,,,,,,https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-zh,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"Dataset Card for PokÃ©mon BLIP captions with English and Chinese. Dataset used to train PokÃ©mon text to image model, add a Chinese Column of PokÃ©mon BLIP captions BLIP generated captions for PokÃ©mon images from Few Shot PokÃ©mon dataset introduced by Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis (FastGAN). Original images were obtained from FastGAN-pytorch and captioned with the pre-trained BLIP model. For each row the dataset contains imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-zh."
swaption2009/20k-en-zh-translation-pinyin-hsk,,,,,,,https://huggingface.co/datasets/swaption2009/20k-en-zh-translation-pinyin-hsk,,,,,,,,,,,,,,,,,,,,"20,000+ chinese sentences with translations and pinyin Source: https://mnemosyne-proj.org/cards/20000-chinese-sentences-translations-and-pinyin Contributed by: Brian Vaughan http://brianvaughan.net/ Dataset Structure Each sample consists of: English sentence HSK level Chinese translation Pinyin separator (""--"") Other Info from the Source HSK level All of the sentences came from sample sentences intended to describe a particularâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/swaption2009/20k-en-zh-translation-pinyin-hsk."
SynthLabsAI/Big-Math-RL-Verified,,,,,,https://arxiv.org/abs/2502.17387,https://huggingface.co/datasets/SynthLabsAI/Big-Math-RL-Verified,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models Big-Math is the largest open-source dataset of high-quality mathematical problems, curated specifically for reinforcement learning (RL) training in language models. With over 250,000 rigorously filtered and verified problems, Big-Math bridges the gap between quality and quantity, establishing a robust foundation for advancing reasoning in LLMs. Request Early Access to Privateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SynthLabsAI/Big-Math-RL-Verified."
T-T-S/FunToImagineWithRichardFeynmanAudioClips,,,,,,,https://huggingface.co/datasets/T-T-S/FunToImagineWithRichardFeynmanAudioClips,,,,,,,,https://choosealicense.com/licenses/cdla-sharing-1.0/,,,,,,,,,,,,"Description: This unique collection features audio segments, each roughly 10 seconds long, excerpted from the acclaimed science series ""Fun to Imagine"" by Richard Feynman. All files are in .wav format, encapsulating the distinct speech patterns of Feynman, an esteemed physicist and Nobel laureate recognized for his remarkable ability to communicate complex scientific principles engagingly and understandably. ""Fun to Imagine"" sees Feynman bringing various scientific concepts toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/T-T-S/FunToImagineWithRichardFeynmanAudioClips."
TableQAKit/SpreadSheetQA,,,,,,,https://huggingface.co/datasets/TableQAKit/SpreadSheetQA,,,,,,,,,,,,,,,,,,,,TableQAKit/SpreadSheetQA dataset hosted on Hugging Face and contributed by the HF Datasets community
tabtoyou/KoLLaVA-Instruct-150k,,,,,,,https://huggingface.co/datasets/tabtoyou/KoLLaVA-Instruct-150k,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Korean Visual Instruct 150K Dataset Card ðŸŒ‹LLaVAì˜ Instruction-following Datasetì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•œ ë°ì´í„°ì…‹ìž…ë‹ˆë‹¤. (feat. DeepL) 1. Conversation ì´ë¯¸ì§€ì— ëŒ€í•´ ì§ˆë¬¸í•˜ëŠ” ì‚¬ëžŒê³¼ ì´ì— ë‹µí•˜ëŠ” Assistant ì‚¬ì´ì˜ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë””ìžì¸í•©ë‹ˆë‹¤. ëŒ€ë‹µì€ Assistantê°€ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ëŠ” ê²ƒê³¼ ê°™ì€ ì–´ì¡°ì´ë©°, ì´ë¯¸ì§€ì˜ ì‹œê°ì ì¸ ì •ë³´(ê°ì²´ì˜ ìœ í˜•, ìˆ˜, í–‰ë™, ìœ„ì¹˜, ê°ì²´ê°„ì˜ ìƒëŒ€ì ì¸ ìœ„ì¹˜ ë“±)ì— ëŒ€í•´ ë‹¤ì–‘í•œ ì§ˆë¬¸ì„ í•©ë‹ˆë‹¤. ë˜í•œ ëª…í™•í•˜ê²Œ ë‹µë³€ì´ ìžˆëŠ” ì§ˆë¬¸ë§Œ ê³ ë ¤í•©ë‹ˆë‹¤. 2. Detailed description ì´ë¯¸ì§€ì— ëŒ€í•œ í’ë¶€í•˜ê³  í¬ê´„ì ì¸ ì„¤ëª…ì„ ë‚´í¬í•˜ê²Œ ë””ìžì¸ í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ìžì„¸í•œ ì„¤ëª…ì„ ìš”êµ¬í•˜ëŠ” ì—¬ëŸ¬ promt ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“  ë’¤ ê·¸ì¤‘ í•˜ë‚˜ë¥¼ ìƒ˜í”Œí•´ ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/tabtoyou/KoLLaVA-Instruct-150k."
taeshahn/ko-lima,,,,,,https://arxiv.org/abs/2305.11206,https://huggingface.co/datasets/taeshahn/ko-lima,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"Dataset Card for KoLIMA Dataset Description KoLIMAëŠ” Metaì—ì„œ ê³µê°œí•œ LIMA: Less Is More for Alignment (Zhou et al., 2023)ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•œ ë°ì´í„°ì…‹ìž…ë‹ˆë‹¤. ë²ˆì—­ì—ëŠ” DeepL APIë¥¼ í™œìš©í•˜ì˜€ê³ , SK(ì£¼) Tech Collaborative Labìœ¼ë¡œë¶€í„° ë¹„ìš©ì„ ì§€ì›ë°›ì•˜ìŠµë‹ˆë‹¤. ì „ì²´ í…ìŠ¤íŠ¸ ì¤‘ì—ì„œ code blockì´ë‚˜ ìˆ˜ì‹ì„ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ìˆ˜ë¬¸ìž ì‚¬ì´ì˜ í…ìŠ¤íŠ¸ëŠ” ì›ë¬¸ì„ ìœ ì§€í•˜ëŠ” í˜•íƒœë¡œ ë²ˆì—­ì„ ì§„í–‰í•˜ì˜€ìœ¼ë©°, train ë°ì´í„°ì…‹ 1,030ê±´ê³¼ test ë°ì´í„°ì…‹ 300ê±´ìœ¼ë¡œ êµ¬ì„±ëœ ì´ 1,330ê±´ì˜ ë°ì´í„°ë¥¼ í™œìš©í•˜ì‹¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. í˜„ìž¬ ë™ì¼í•œ ë²ˆì—­ ë¬¸ìž¥ì„ plain, vicuna ë‘ ê°€ì§€ í¬ë©§ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤. ë°ì´í„°ì…‹ ê´€ë ¨í•˜ì—¬ ë¬¸ì˜ê°€ ìžˆìœ¼ì‹  ê²½ìš° ë©”ì¼ì„ í†µí•´ ì—°ë½ì£¼ì„¸ìš”! ðŸ¥° This is Korean LIMA datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/taeshahn/ko-lima."
taesiri/GamePhysics_Grand_Theft_Auto_V,Text,,,,,https://asgaardlab.github.io/CLIPxGamePhysics/,https://huggingface.co/datasets/taesiri/GamePhysics_Grand_Theft_Auto_V,,,,,,,,,,,,,,,,,,,,A test dataset for GamePhysics
taide/TAIDE-14-tasks,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/taide/TAIDE-14-tasks,Models (LLM),,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for TAIDE-14-tasks Data-Summary: The ""TAIDE-14-tasks"" dataset, derived from the TAIDE project, encompasses 14 prevalent text generation tasks. This dataset features a collection of 140 prompts tailored for assessing Traditional Chinese Large Language Models (LLM). GPT-4 meticulously crafted these prompts using the provided task, domain, and keywords from the instructions, with further validation by human experts. Each data entry not only containsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/taide/TAIDE-14-tasks."
takala/financial_phrasebank,Text,,,,,https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10,https://huggingface.co/datasets/takala/financial_phrasebank,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-3.0/,,,,,,,,,,,,"The key arguments for the low utilization of statistical techniques in financial sentiment analysis have been the difficulty of implementation for practical applications and the lack of high quality training data for building such models. Especially in the case of finance and economic texts, annotated collections are a scarce resource and many are reserved for proprietary use only. To resolve the missing training data problem, we present a collection of âˆ¼ 5000 sentences to establish human-annotated standards for benchmarking alternative modeling techniques. The objective of the phrase level annotation task was to classify each"
takara-ai/fudeno-instruct-4M,,,,,,,https://huggingface.co/datasets/takara-ai/fudeno-instruct-4M,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,UsageDataset Creation ProcessApplicationsTechnical ImplementationCitationLicenseAcknowledgmentsFrom the Frontier Research Team atTakara,,,,,,"From the Frontier Research Team at Takara.ai we present Fudeno-Instruct-4M, the world's largest multimodal instruct dataset for training LLMs to understand and generate SVG graphics. ðŸŽ‰ 16/03/2025 We just won THIRD PLACE in the Tech:Europe AI Hackathon in Munich using Fudeno!! We created a full app that teaches an LLM to draw and create complete corporate design packs! Check it out here! Fudeno-Instruct-4M: Multimodal SVG Generation Dataset Fudeno-Instruct-4M is aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/takara-ai/fudeno-instruct-4M."
Tamazight-NLP/NLLB-Seed_Standard-Moroccan-Tamazight,,,,,,,https://huggingface.co/datasets/Tamazight-NLP/NLLB-Seed_Standard-Moroccan-Tamazight,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,Tamazight-NLP/NLLB-Seed_Standard-Moroccan-Tamazight dataset hosted on Hugging Face and contributed by the HF Datasets community
Tarklanse/Traditional_Chinese_roleplay_chat_Dataset,,,,,,,https://huggingface.co/datasets/Tarklanse/Traditional_Chinese_roleplay_chat_Dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,Traditional_Chinese_roleplay_chat_Dataset é€™å€‹è³‡æ–™é›†æ˜¯ä»¥ç¹é«”ä¸­æ–‡ç‚ºä¸»ï¼Œå°‡å„ç¨®ç”±ChatGPTç”Ÿæˆèˆ‡æ¥µå°éƒ¨åˆ†å€‹äººæ’°å¯«çš„å°è©±å…§å®¹æ•´ç†ç‚ºalpaca dataset formatçš„æ ¼å¼ ä»¥ä¸€å±¤ä¸€å±¤å †ç–Šçš„æ–¹å¼ï¼Œå°‡ä¸€å‰‡å°è©±ç´€éŒ„æ‹†æˆæ•¸ç­†è³‡æ–™(å…±ç´„1000å‰‡å°è©±)ï¼Œåœ¨å¹¾æ¬¡å˜—è©¦æ€§çš„è¨“ç·´ä¸­èƒ½å¤ è®“llama2é‡ç¾åŽŸæœ¬è‹±æ–‡é‚£ç¨®å¾ˆæ´»èºçš„å°è©±é¢¨æ ¼ï¼Œä¸¦ä¸”èƒ½å¤ ç¶­æŒå–„æ–¼æ‰®æ¼”å„ç¨®è§’è‰²çš„èƒ½åŠ› ç›®å‰å€‹äººæœ‰ä»¥é€™å€‹è³‡æ–™é›†è£½ä½œä¸€å€‹lora 2023/09/07 æ›´æ–° ç‚ºè³‡æ–™é›†åŠ å…¥ä¸€äº›ä¸­è‹±ç¿»è­¯çš„å¥å­ï¼Œä»¥æœŸAIèƒ½ä»¥æ›´å¥½çš„æ–‡å­—åŽ»æå¯«ä»–çš„å‹•ä½œï¼Œä¸¦å¢žåŠ äº†ä¸€äº›èˆ‡é£Ÿç‰©æœ‰é—œçš„å°è©±ï¼Œå¸Œæœ›èƒ½é™ä½ŽAIç”Ÿå‡ºå¥‡æ€ªé£Ÿç‰©åçš„æ©ŸçŽ‡
tarteel-ai/EA-UD,,,,,,,https://huggingface.co/datasets/tarteel-ai/EA-UD,,,,,,,,,,,,,,,,,,,,tarteel-ai/EA-UD dataset hosted on Hugging Face and contributed by the HF Datasets community
tasksource/crowdflower,,,,,,,https://huggingface.co/datasets/tasksource/crowdflower,,,,,,,,https://choosealicense.com/licenses/undefined/,,,,,,,,,,,,Collection of crowdflower classification datasets
tatsu-lab/alpaca,Text,General,UI,Text,"Accuracy, F1 Score",https://crfm.stanford.edu/2023/03/13/alpaca.html,https://huggingface.co/datasets/tatsu-lab/alpaca,models and make the language model follow instruction better,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,"BERT, RoBERTa, T5",work and made the following modifications: The text-davinci-003 engine to generate the instruction data insteadâ€¦ See the full description on the dataset page: https://huggingface,,"Dataset Card for Alpaca Data-Summary: Alpaca is a dataset of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine. This instruction data can be used to conduct instruction-tuning for language models and make the language model follow instruction better. The authors built on the data generation pipeline from Self-Instruct framework and made the following modifications: The text-davinci-003 engine to generate the instruction data insteadâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tatsu-lab/alpaca."
tau/commonsense_qa,Text,Question Answering,UI,Text,"Exact Match, F1 Score",https://www.tau-nlp.org/commonsenseqa,https://huggingface.co/datasets/tau/commonsense_qa,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5, BART",,,"Dataset Card for ""commonsense_qa"" Data-Summary: CommonsenseQA is a new multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers. The dataset is provided in two major training/validation/testing set splits: ""Random split"" which is the main evaluation split, and ""Question token split"", see paper for details.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/tau/commonsense_qa."
tdavidson/hate_speech_offensive,Audio,Detection,General,Bounding Box/Mask,"mAP, IoU",https://github.com/t-davidson/hate-speech-and-offensive-language,https://huggingface.co/datasets/tdavidson/hate_speech_offensive,detection on tweets,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"DETR, Faster R-CNN, YOLO",,,"Dataset Card for [Dataset Name] Data-Summary: An annotated dataset for hate speech and offensive language detection on tweets. Supported Tasks and Leaderboards [More Information Needed] Languages English (en) Dataset Structure Data Instances { ""count"": 3, ""hate_speech_annotation"": 0, ""offensive_language_annotation"": 0, ""neither_annotation"": 3, ""label"": 2, # ""neither"" ""tweet"": ""!!! RTâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tdavidson/hate_speech_offensive."
tdiggelm/climate_fever,Text,General,General,Text,"Accuracy, F1 Score",http://climatefever.ai,https://huggingface.co/datasets/tdiggelm/climate_fever,English,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ClimateFever Data-Summary: A dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change collected on the internet. Each claim is accompanied by five manually annotated evidence sentences retrieved from the English Wikipedia that support, refute or do not give enough information to validate the claim totalling in 7,675 claim-evidence pairs. The dataset features challenging claims that relate multipleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tdiggelm/climate_fever."
Team-ACE/ToolACE,,,,,,https://arxiv.org/abs/2409.00920,https://huggingface.co/datasets/Team-ACE/ToolACE,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"ToolACE ToolACE is an automatic agentic pipeline designed to generate Accurate, Complex, and divErse tool-learning data. ToolACE leverages a novel self-evolution synthesis process to curate a comprehensive API pool of 26,507 diverse APIs. Dialogs are further generated through the interplay among multiple agents, guided by a formalized thinking process. To ensure data accuracy, we implement a dual-layer verification system combining rule-based and model-based checks. Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Team-ACE/ToolACE."
teapotai/synthqa,,,,,,,https://huggingface.co/datasets/teapotai/synthqa,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Teapot SynthQA This is a synthetic dataset created to fine-tune small language models such as Teapot LLM. Dataset This dataset consists of synthetic articles and questions generated by DeepSeek-V3. Questions cover a variety of tasks and were manually annotated to ensure high quality responses due to the size of the dataset. The data was specifically designed to represent various common language tasks and to utilize open source models to ensure permissive licensing forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/teapotai/synthqa.
teknium/OpenHermes-2.5,,,,,,,https://huggingface.co/datasets/teknium/OpenHermes-2.5,,,,,,,,,,,,,,,,,,,,"Dataset Card for Dataset Name This is the dataset that made OpenHermes 2.5 and Nous Hermes 2 series of models. Support me on GitHub sponsors <3 : https://github.com/sponsors/teknium1 Dataset Details Dataset Description The Open Hermes 2/2.5 and Nous Hermes 2 models have made significant advancements of SOTA LLM's over recent months, and are underpinned by this exact compilation and curation of many open source datasets and custom created syntheticâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/teknium/OpenHermes-2.5."
TempoFunk/webvid-10M,,,,,,,https://huggingface.co/datasets/TempoFunk/webvid-10M,,,,,,,,https://choosealicense.com/licenses/agpl-3.0/,,,,,,,,,,,,TempoFunk/webvid-10M dataset hosted on Hugging Face and contributed by the HF Datasets community
Tesslate/Tessa-T1-Dataset,,,,,,,https://huggingface.co/datasets/Tesslate/Tessa-T1-Dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,50,,,,,,,,,Tesslate/Tessa-T1-Dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
Tevatron/msmarco-passage,,,,,,,https://huggingface.co/datasets/Tevatron/msmarco-passage,,,,,,,,,,,,,,,,,,,,Tevatron/msmarco-passage dataset hosted on Hugging Face and contributed by the HF Datasets community
teven/github_all_lang_filtered,,,,,,,https://huggingface.co/datasets/teven/github_all_lang_filtered,,,,,,,,,,,,,,,,,,,,teven/github_all_lang_filtered dataset hosted on Hugging Face and contributed by the HF Datasets community
textmachinelab/quail,Text,General,General,Text,"Accuracy, F1 Score",https://text-machine-lab.github.io/blog/2020/quail/,https://huggingface.co/datasets/textmachinelab/quail,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""quail"" Data-Summary: QuAIL is a reading comprehension dataset. QuAIL contains 15K multi-choice questions in texts 300-350 tokens long 4 domains (news, user stories, fiction, blogs).QuAIL is balanced and annotated for question types. Supported Tasks and Leaderboards More Information Needed Languages More Information Needed Dataset Structure Data Instances quail Size ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/textmachinelab/quail."
TFLai/Turkish-Dialog-Dataset,,,,,,,https://huggingface.co/datasets/TFLai/Turkish-Dialog-Dataset,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,TFLai/Turkish-Dialog-Dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
thakurvishesh1/good_prompt,,,,,,,https://huggingface.co/datasets/thakurvishesh1/good_prompt,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,thakurvishesh1/good_prompt dataset hosted on Hugging Face and contributed by the HF Datasets community
Thaweewat/alpaca-cleaned-52k-th,Text,,,,,,https://huggingface.co/datasets/Thaweewat/alpaca-cleaned-52k-th,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"Summary This is a Thai ðŸ‡¹ðŸ‡­-instructed dataset translated from cleaned version of the original Alpaca Dataset released by Stanford using Google Cloud Translation, contain 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine. This instruction data can be used to conduct instruction-tuning for language models and make the language model follow instruction better. The following issues have been identified in the original release and fixed in thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Thaweewat/alpaca-cleaned-52k-th."
The13thDrifter/Halo-2_Cortana_DATASET,,,,,,,https://huggingface.co/datasets/The13thDrifter/Halo-2_Cortana_DATASET,,,,,,,,https://choosealicense.com/licenses/cc-by-3.0/,,,,,,,,,,,,The13thDrifter/Halo-2_Cortana_DATASET dataset hosted on Hugging Face and contributed by the HF Datasets community
thean/THFOOD-50,,,,,,,https://huggingface.co/datasets/thean/THFOOD-50,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,"THFOOD-50 Fine-Grained Thai Food Image Classification Datasets THFOOD-50 containing 15,770 images of 50 famous Thai dishes. Download: THFOOD-50 v1 on Google Drive License THFOOD-50 for non-commercial research/educational use. Citation If you use THFOOD-50 dataset in your research, please cite our paper: @article{termritthikun2017nu, title=""{NU-InNet: Thai food image recognition using convolutional neural networks on smartphone}""â€¦ See the full description on the dataset page: https://huggingface.co/datasets/thean/THFOOD-50."
theatticusproject/cuad-qa,Text,,,,,https://www.atticusprojectai.org/cuad,https://huggingface.co/datasets/theatticusproject/cuad-qa,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Contract Understanding Atticus Dataset (CUAD) v1 is a corpus of more than 13,000 labels in 510 commercial legal contracts that have been manually labeled to identify 41 categories of important clauses that lawyers look for when reviewing contracts in connection with corporate transactions."
theblackcat102/alexa-qa,,,,,,,https://huggingface.co/datasets/theblackcat102/alexa-qa,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Alexa Answers from alexaanswers.amazon.com The Alexa Answers community helps to improve Alexaâ€™s knowledge and answer questions asked by Alexa users. Which contains some very quirky and hard question like Q: what percent of the population has blackhair A: The most common hair color in the world is black and its found in wide array of background and ethnicities. About 75 to 85% of the global population has either black hair or the deepest brown shade. Q: what was the worldâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/theblackcat102/alexa-qa.
TheBritishLibrary/blbooks,Text,,,,,https://www.bl.uk/collection-guides/digitised-printed-books,https://huggingface.co/datasets/TheBritishLibrary/blbooks,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"A dataset comprising of text created by OCR from the 49,455 digitised books, equating to 65,227 volumes (25+ million pages), published between c. 1510 - c. 1900. The books cover a wide range of subject areas including philosophy, history, poetry and literature."
thefcraft/civitai-stable-diffusion-337k,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/thefcraft/civitai-stable-diffusion-337k,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"How to Use from datasets import load_dataset dataset = load_dataset(""thefcraft/civitai-stable-diffusion-337k"") print(dataset['train'][0]) download images download zip files from images dir https://huggingface.co/datasets/thefcraft/civitai-stable-diffusion-337k/tree/main/images it contains some images with id from zipfile import ZipFile with ZipFile(""filename.zip"", 'r') as zObject: zObject.extractall() Data-Summary: GitHub URL:-â€¦ See the full description on the dataset page: https://huggingface.co/datasets/thefcraft/civitai-stable-diffusion-337k."
TheFinAI/Fino1_Reasoning_Path_FinQA,,,,,,https://arxiv.org/abs/2502.08127,https://huggingface.co/datasets/TheFinAI/Fino1_Reasoning_Path_FinQA,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Fino1 is a financial reasoning dataset based on FinQA, with GPT-4o-generated reasoning paths to enhance structured financial question answering. For more details, please check our paper arxiv.org/abs/2502.08127. Source Data Initial Data Collection and Normalization The dataset originates from FinQA dataset. Annotations Annotation Process We add a prompt and create a reasoning process using GPT-4o for each question-answer pair. ðŸ’¡ Citationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TheFinAI/Fino1_Reasoning_Path_FinQA."
TheFusion21/PokemonCards,,,,,,,https://huggingface.co/datasets/TheFusion21/PokemonCards,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Dataset Card for PokemonCards Languages All of the data is in English. Dataset Structure Data Instances { ""id"": ""pl1-1"", ""image_url"": ""https://images.pokemontcg.io/pl1/1_hires.png"", ""caption"": ""A Stage 2 Pokemon Card of type Lightning with the title """"Ampharos"""" and 130 HP of rarity """"Rare Holo"""" evolved from Flaaffy from the set Platinum and the flavor text: """"None"""". It has the attack """"Gigavolt"""" with the cost Lightningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TheFusion21/PokemonCards."
TheGreatRambler/mm2_level,,,,,,,https://huggingface.co/datasets/TheGreatRambler/mm2_level,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,Mario Maker 2 levels Part of the Mario Maker 2 Dataset Collection Dataset Description The Mario Maker 2 levels dataset consists of 26.6 million levels from Nintendo's online service totaling around 100GB of data. The dataset was created using the self-hosted Mario Maker 2 api over the course of 1 month in February 2022. How to use it The Mario Maker 2 levels dataset is a very large dataset so for most use cases it is recommended to make use of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TheGreatRambler/mm2_level.
TheMrguiller/ScienceQA,Image-Text,General,Scientific,Text,"Accuracy, F1 Score",https://scienceqa.github.io/,https://huggingface.co/datasets/TheMrguiller/ScienceQA,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""ScienceQA"" Data-Summary: ScienceQA is collected from elementary and high school science curricula, and contains 21,208 multimodal multiple-choice science questions. Out of the questions in ScienceQA, 10,332 (48.7%) have an image context, 10,220 (48.2%) have a text context, and 6,532 (30.8%) have both. Most questions are annotated with grounded lectures (83.9%) and detailed explanations (90.5%). The lecture and explanation provide generalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TheMrguiller/ScienceQA."
thennal/IMaSC,,,,,,https://arxiv.org/abs/2211.12796,https://huggingface.co/datasets/thennal/IMaSC,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"IMaSC: ICFOSS Malayalam Speech Corpus IMaSC is a Malayalam text and speech corpus made available by ICFOSS for the purpose of developing speech technology for Malayalam, particularly text-to-speech. The corpus contains 34,473 text-audio pairs of Malayalam sentences spoken by 8 speakers, totalling in approximately 50 hours of audio. Dataset Structure The dataset consists of 34,473 instances with fields text, speaker, and audio. The audio is mono, sampled at 16kH.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/thennal/IMaSC."
theonlydo/indonesia-slang,,,,,,,https://huggingface.co/datasets/theonlydo/indonesia-slang,,,,,,,,,,,,,,,,,,,,theonlydo/indonesia-slang dataset hosted on Hugging Face and contributed by the HF Datasets community
theoxo/proofwriter-deduction-balanced,,,,,,,https://huggingface.co/datasets/theoxo/proofwriter-deduction-balanced,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"A processed subset of the OWA section of the ProofWriter dataset. Each train/test split contains 300 entries, each of which has a unique set of theories and a single question for those theories. Both splits are balanced so that the depth of the proof required to answer the question varies evenly between 0-5 (50 entries each), and the labels are balanced (100 each). 'Unknown' labels have been replaced by 'Uncertain' to match other datasets."
Thomas-X-Yang/gsm8k-prolog,Text,Reasoning,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Thomas-X-Yang/gsm8k-prolog,,,,,,,,https://choosealicense.com/licenses/mit/,,,90,,,,,,"T5, UnifiedQA, BART",to solve the questions,,Dataset Card for GSM8K-Prolog Data-Summary: This is the Prolog annotated version of the GSM8K math reasoning dataset. We used the same dataset splits and questions in GSM8K and prompted GPT-4 to generate the Prolog programs to solve the questions. We then manually corrected some malfunctioning samples. Supported Tasks and Leaderboards This dataset can be used to train language models to generate Prolog codes in order to solve math questions andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Thomas-X-Yang/gsm8k-prolog.
thotnd/VBSF001,,,,,,,https://huggingface.co/datasets/thotnd/VBSF001,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Vietnamese Text-To-Speech dataset (VBSF001-v1.0) The audio is crawled from audiobooks on YouTube. The audio is NOT for commercial use. The text is labeled by VinBrain JSC. The text is in the public domain. Dataset size: 1.28GB. Total audio duration: 9.5 hours. Text-audio samples Sample 1: Audio: file1 Text: song cÅ©ng tá»« buá»•i tá»‘i báº¥t háº¡nh áº¥y, má»™t ná»—i buá»“n ghÃª gá»›m xÃ¢m chiáº¿m nhÃ  vua vÃ  thÆ°á»ng xuyÃªn lá»™ ra nÃ©t máº·t. Sample 2: Audio: file2 Text: theo luáº­t phÃ¡pâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/thotnd/VBSF001."
Thouph/Laion_aesthetics_5plus_1024_33M_csv,,,,,,,https://huggingface.co/datasets/Thouph/Laion_aesthetics_5plus_1024_33M_csv,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Thouph/Laion_aesthetics_5plus_1024_33M_csv dataset hosted on Hugging Face and contributed by the HF Datasets community
thsant/wgisd,,,,,,https://arxiv.org/abs/1803.09010,https://huggingface.co/datasets/thsant/wgisd,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Embrapa Wine Grape Instance Segmentation Dataset â€“ Embrapa WGISD This is a detailed description of the dataset, a datasheet for the dataset as proposed by Gebru et al. Motivation for Dataset Creation Why was the dataset created? Embrapa WGISD (Wine Grape Instance Segmentation Dataset) was created to provide images and annotation to study object detection and instance segmentation for image-based monitoring and field robotics in viticulture. Itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/thsant/wgisd."
thu-coai/lccc,Text,,,,,https://arxiv.org/abs/2008.03946,https://huggingface.co/datasets/thu-coai/lccc,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"LCCC: Large-scale Cleaned Chinese Conversation corpus (LCCC) is a large corpus of Chinese conversations. A rigorous data cleaning pipeline is designed to ensure the quality of the corpus. This pipeline involves a set of rules and several classifier-based filters. Noises such as offensive or sensitive words, special symbols, emojis, grammatically incorrect sentences, and incoherent conversations are filtered."
THU-StarLab/CustomerService,,,,,,,https://huggingface.co/datasets/THU-StarLab/CustomerService,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,æ•°æ®é›†è¯´æ˜Ž ç»„æˆ ç±»åž‹ æ–‡ä»¶å¤¹åç§° æ¥æº æ•°é‡ è¯´æ˜Ž ç”µä¿¡é—®ç­” telecom_Q&A ç™¾åº¦çŸ¥é“QA 87366 ç»è¿‡è„±æ•ã€æ•°æ®æ¸…æ´—ã€äººå·¥ç­›é€‰ç­‰å¤„ç† è¡Œä¸šç›¸å…³çŸ¥è¯†æ•°æ® industry_data æ•™ç§‘ä¹¦ã€å›½é™…æ ‡å‡†ç­‰ 5218 é€šè¿‡å¤§æ¨¡åž‹ä»Žæ–‡æ¡£å¾—åˆ°çš„QAæ•°æ®ï¼Œéƒ¨åˆ†åŽŸæ–‡æ¡£ä¿å­˜åœ¨source_dataä¸­ é€šç”¨æŒ‡ä»¤æ•°æ®é›† general_instruction firefly 18123 æŒ‘é€‰äº†é˜…è¯»ã€æƒ…æ„Ÿç†è§£ã€è¡¥å…¨ã€é€»è¾‘æŽ¨ç†ç­‰ä¸»é¢˜çš„é€šç”¨æŒ‡ä»¤ æ··åˆæ•°æ®é›† blended_data - - æŒ‰ç…§æ•°æ®é›†å»ºè®¾è¿›ç¨‹ï¼Œæ··åˆåŽç»„ä»¶çš„è®­ç»ƒã€æµ‹è¯•æ•°æ®ï¼Œå¯ç›´æŽ¥ä½¿ç”¨ æ··åˆæ•°æ® - V1 ç»„æˆ æ¥æº æ¯”ä¾‹ æ¡æ•° è¯´æ˜Ž ç™¾åº¦çŸ¥é“ 64% 32282 ç»è¿‡è„±æ•ã€æ•°æ®æ¸…æ´—ã€äººå·¥ç­›é€‰ç­‰å¤„ç† firefly 36% 18123 æŒ‘é€‰äº†é˜…è¯»ã€æƒ…æ„Ÿç†è§£ã€è¡¥å…¨ã€é€»è¾‘æŽ¨ç†ç­‰ä¸»é¢˜çš„é€šç”¨æŒ‡ä»¤ æ ‡å‡†é—®ç­” - 18 é€šè¿‡è”é€šç½‘ä¸Šè¥ä¸šåŽ…åœ¨çº¿å®¢æœæ•´ç† åˆè®¡ 100%â€¦ See the full description on the dataset page: https://huggingface.co/datasets/THU-StarLab/CustomerService.
THUDM/AgentInstruct,,,,,,https://arxiv.org/abs/2310.12823,https://huggingface.co/datasets/THUDM/AgentInstruct,,,,,,,,,,,,,,,,,,,,"AgentInstruct Dataset ðŸ¤— [Models] â€¢ ðŸ’» [Github Repo] â€¢ ðŸ“Œ [Project Page] â€¢ ðŸ“ƒ [Paper] AgentInstruct is a meticulously curated dataset featuring 1,866 high-quality interactions, designed to enhance AI agents across six diverse real-world tasks, leveraging innovative methods like Task Derivation and Self-Instruct. ðŸ” CoT - Harness the power of ReAct, offering detailed thought explanations for each action, ensuring an intricate understanding of the model's decision-makingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/AgentInstruct."
THUIR/Qilin,,,,,,https://arxiv.org/abs/2503.00501,https://huggingface.co/datasets/THUIR/Qilin,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Qilin Qilin is a large-scale multimodal dataset designed for advancing research in search, recommendation, and Retrieval-Augmented Generation (RAG) systems. This repository contains the official implementation of the dataset paper, baseline models, and evaluation tools. This dataset was presented in Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions. Github: https://github.com/RED-Search/Qilin The image data can be found atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/THUIR/Qilin."
thunlp/docred,Text,,,,,https://arxiv.org/abs/1906.06127,https://huggingface.co/datasets/thunlp/docred,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: - DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text. - DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document. - Along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios."
tiagoblima/nilc-school-books,,,,,,,https://huggingface.co/datasets/tiagoblima/nilc-school-books,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"CÃ³rpus de Complexidade Textual para EstÃ¡gios Escolares do Sistema Educacional Brasileiro O cÃ³rpus inclui trechos de: livros-textos cuja lista completa Ã© apresentada abaixo, notÃ­cias da SeÃ§Ã£o Para Seu Filho Ler (PSFL) do jornal Zero Hora que apresenta algumas notÃ­cias sobre o mesmo cÃ³rpus do jornal do Zero Hora, mas escritas para crianÃ§as de 8 a 11 anos de idade , Exames do SAEB , Livros Digitais do Wikilivros em PortuguÃªs, Exames do Enem dos anos 2015, 2016 e 2017. Todo oâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tiagoblima/nilc-school-books."
tiange/Cap3D,,,,,,https://arxiv.org/abs/2306.07279,https://huggingface.co/datasets/tiange/Cap3D,,,,,,,,https://choosealicense.com/licenses/odc-by/,,,,,,,,,,,,"This repository hosts data for Scalable 3D Captioning with Pretrained Models and View Selection for 3D Captioning via Diffusion Ranking, including descriptive captions for 3D objects in Objaverse, Objaverse-XL, ABO, and ShapeNet. This repo also includes point clouds and rendered images with camera, depth, and MatAlpha information of Objaverse objects, as well as their Shap-E latent codes. All the captions and data provided by our papers are released under ODC-By 1.0 license. Veryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tiange/Cap3D."
ticoAg/Chinese-medical-dialogue,,,,,,,https://huggingface.co/datasets/ticoAg/Chinese-medical-dialogue,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Note process data from Chinese-medical-dialogue-data å•è½®åŒ»æ‚£å¯¹è¯ raw data samples department title ask answer å¿ƒè¡€ç®¡ç§‘ é«˜è¡€åŽ‹æ‚£è€…èƒ½åƒå…šå‚å—ï¼Ÿ æˆ‘æœ‰é«˜è¡€åŽ‹è¿™ä¸¤å¤©å¥³å©¿æ¥çš„æ—¶å€™ç»™æˆ‘æ‹¿äº†äº›å…šå‚æ³¡æ°´å–ï¼Œæ‚¨å¥½é«˜è¡€åŽ‹å¯ä»¥åƒå…šå‚å—ï¼Ÿ é«˜è¡€åŽ‹ç—…äººå¯ä»¥å£æœå…šå‚çš„ã€‚å…šå‚æœ‰é™è¡€è„‚ï¼Œé™è¡€åŽ‹çš„ä½œç”¨ï¼Œå¯ä»¥å½»åº•æ¶ˆé™¤è¡€æ¶²ä¸­çš„åžƒåœ¾ï¼Œä»Žè€Œå¯¹å† å¿ƒç—…ä»¥åŠå¿ƒè¡€ç®¡ç–¾ç—…çš„æ‚£è€…éƒ½æœ‰ä¸€å®šçš„ç¨³å®šé¢„é˜²å·¥ä½œä½œç”¨ï¼Œå› æ­¤å¹³æ—¶å£æœå…šå‚èƒ½è¿œç¦»ä¸‰é«˜çš„å±å®³ã€‚å¦å¤–å…šå‚é™¤äº†ç›Šæ°”å…»è¡€ï¼Œé™ä½Žä¸­æž¢ç¥žç»ä½œç”¨ï¼Œè°ƒæ•´æ¶ˆåŒ–ç³»ç»ŸåŠŸèƒ½ï¼Œå¥è„¾è¡¥è‚ºçš„åŠŸèƒ½ã€‚æ„Ÿè°¢æ‚¨çš„è¿›è¡Œå’¨è¯¢ï¼ŒæœŸæœ›æˆ‘çš„è§£é‡Šå¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚ å†…åˆ†æ³Œç§‘ ç³–å°¿ç—…è¿˜ä¼šè¿›è¡Œé—ä¼ å—ï¼Ÿ ç³–å°¿ç—…æœ‰éš”ä»£é—ä¼ å—ï¼Ÿæˆ‘å¦ˆæ˜¯ç³–å°¿ç—…ï¼Œå¾ˆå¤šå¹´äº†ï¼Œä¹Ÿæ²¡å…»å¥½ï¼Œæˆ‘çŽ°åœ¨ä¹Ÿæ˜¯ï¼Œæˆ‘å¦¹å­ä¹Ÿæ˜¯ï¼Œæˆ‘å„¿å­çŽ°åœ¨äºŒåå²ï¼Œæ²¡ä»€ä¹ˆé—®é¢˜ï¼Œä½†æ˜¯ä»¥åŽä¼šä¸ä¼šä¹Ÿå¾—ç³–å°¿ç—…å•Šï¼ŒçœŸæ˜¯éš¾è¿‡ï¼Œæˆ‘çŽ°åœ¨å°±å·²ç»å¼€å§‹è®©ä»–æŽ§åˆ¶ç‚¹åƒä¸œè¥¿ã€‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ticoAg/Chinese-medical-dialogue.
tiedong/goat,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/tiedong/goat,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Dataset Name Data-Summary: The dataset.json file contains ~1.7 million synthetic data for arithmetic tasks, generated by dataset.ipynb. Supported Tasks and Leaderboards [More Information Needed] Languages [More Information Needed] Dataset Structure Data Instances [More Information Needed] Data Fields [More Information Needed] Data Splits [More Informationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tiedong/goat."
TIGER-Lab/MMLU-Pro,Text,,,,,https://arxiv.org/abs/2406.01574,https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"MMLU-Pro Dataset MMLU-Pro dataset is a more robust and challenging massive multi-task understanding dataset tailored to more rigorously benchmark large language models' capabilities. This dataset contains 12K complex questions across various disciplines. |Github | ðŸ†Leaderboard | ðŸ“–Paper | ðŸš€ What's New [2024.10.16] We have added Gemini-1.5-Flash-002, Gemini-1.5-Pro-002, Jamba-1.5-Large, Llama-3.1-Nemotron-70B-Instruct-HF and Ministral-8B-Instruct-2410 to ourâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro."
Tiger14n/RVC-GUI,,,,,,,https://huggingface.co/datasets/Tiger14n/RVC-GUI,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Tiger14n/RVC-GUI dataset hosted on Hugging Face and contributed by the HF Datasets community
TigerResearch/tigerbot-zhihu-zh-10k,,,,,,,https://huggingface.co/datasets/TigerResearch/tigerbot-zhihu-zh-10k,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Tigerbot åŸºäºŽå¼€æºæœé›†çš„çŸ¥ä¹Žæ•°æ®ç”Ÿæˆçš„sfté—®ç­”å¯¹ Usage import datasets ds_sft = datasets.load_dataset('TigerResearch/tigerbot-zhihu-zh-10k')
tiiuae/falcon-refinedweb,Text,,,,,falconllm.tii.ae,https://huggingface.co/datasets/tiiuae/falcon-refinedweb,,,,,,,,https://choosealicense.com/licenses/odc-by/,,8,,,,,,,,,,"ðŸ“€ Falcon RefinedWeb Falcon RefinedWeb is a massive English web dataset built by TII and released under an ODC-By 1.0 license. See the ðŸ““ paper on arXiv for more details. RefinedWeb is built through stringent filtering and large-scale deduplication of CommonCrawl; we found models trained on RefinedWeb to achieve performance in-line or better than models trained on curated datasets, while only relying on web data. RefinedWeb is also ""multimodal-friendly"": it contains links and altâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tiiuae/falcon-refinedweb."
timbrooks/instructpix2pix-clip-filtered,Image,General,General,Text,"Accuracy, F1 Score",https://www.timothybrooks.com/instruct-pix2pix,https://huggingface.co/datasets/timbrooks/instructpix2pix-clip-filtered,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for InstructPix2Pix CLIP-filtered Data-Summary: The dataset can be used to train models to follow edit instructions. Edit instructions are available in the edit_prompt. original_image can be used with the edit_prompt and edited_image denotes the image after applying the edit_prompt on the original_image. Refer to the GitHub repository to know more about how this dataset can be used to train a model that can follow instructions.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/timbrooks/instructpix2pix-clip-filtered.
timdettmers/openassistant-guanaco,,,,,,,https://huggingface.co/datasets/timdettmers/openassistant-guanaco,,,,,,,,,,,,,,,,,,,,"This dataset is a subset of the Open Assistant dataset, which you can find here: https://huggingface.co/datasets/OpenAssistant/oasst1/tree/main This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples. This dataset was used to train Guanaco with QLoRA. For further information, please see the original dataset. License: Apache 2.0"
timit-asr/timit_asr,Text,,,,,https://catalog.ldc.upenn.edu/LDC93S1,https://huggingface.co/datasets/timit-asr/timit_asr,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"The TIMIT corpus of reading speech has been developed to provide speech data for acoustic-phonetic research studies and for the evaluation of automatic speech recognition systems. TIMIT contains high quality recordings of 630 individuals/speakers with 8 different American English dialects, with each individual reading upto 10 phonetically rich sentences. More info on TIMIT dataset can be understood from the ""README"" which can be found here: https://catalog.ldc.upenn.edu/docs/LDC93S1/readme.txt"
TinyPixel/o-mini,,,,,,,https://huggingface.co/datasets/TinyPixel/o-mini,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""o-mini"" More Information needed"
tj-solergibert/Europarl-ST,Audio,Translation,General,Text,"BLEU, METEOR, TER",https://www.mllp.upv.es/europarl-st/,https://huggingface.co/datasets/tj-solergibert/Europarl-ST,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,"T5, mBART, M2M100",,,"Dataset Card for ""Europarl-ST"" Data-Summary: Europarl-ST is a Multilingual Speech Translation Corpus, that contains paired audio-text samples for Speech Translation, constructed using the debates carried out in the European Parliament in the period between 2008 and 2012. Languages Spanish, German, English, French, Dutch, Polish, Portuguese, Romanian, Italian Dataset Structure Data Fields original_audio: The originalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tj-solergibert/Europarl-ST."
tmu-nlp/thai_toxicity_tweet,Text,,,,,https://github.com/tmu-nlp/ThaiToxicityTweetCorpus/,https://huggingface.co/datasets/tmu-nlp/thai_toxicity_tweet,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-3.0/,,,,,,,,,,,,"Thai Toxicity Tweet Corpus contains 3,300 tweets annotated by humans with guidelines including a 44-word dictionary. The author obtained 2,027 and 1,273 toxic and non-toxic tweets, respectively; these were labeled by three annotators. The result of corpus analysis indicates that tweets that include toxic words are not always toxic. Further, it is more likely that a tweet is toxic, if it contains toxic words indicating their original meaning. Moreover, disagreements in annotation are primarily because of sarcasm, unclear existing target, and word sense ambiguity. Notes from data cleaner: The data is included into [huggingface/datasets](https://www.github.com/huggingface/datasets) in Dec 2020. By this time, 506 of the tweets are not available publicly anymore. We denote these by `TWEET_NOT_FOUND` in `tweet_text`. Processing can be found at [this PR](https://github.com/tmu-nlp/ThaiToxicityTweetCorpus/pull/1)."
TMZN/baidubaike,,,,,,,https://huggingface.co/datasets/TMZN/baidubaike,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,å¦‚æœ‰ä¾µæƒè¯·è”ç³»åˆ é™¤ æ„Ÿè°¢è¯å…¸åœˆçš„é˜¿å¼¥é™€ä½›å¤§ä½¬ï¼Œè¿™äº›å‡æ˜¯ä»–çš„å¤§ä½œã€‚çŽ°åœ¨ä¸Šä¼ åˆ°æ­¤åœ°ï¼Œå¹¶ä¸æ˜¯æˆ‘ä¸ªäººä½œå“ã€‚ è¯·ä½¿ç”¨MdxExport.exeè¿›è¡Œåç¼–è¯‘å¤„ç†ï¼Œä»Žè€Œå¯¼å‡ºHTMLä¾¿äºŽåŽç»­æ‰‹æ“æ•°æ®é›†ã€‚å¦‚æžœä¸è¡Œçš„è¯ï¼Œè¯·ä½¿ç”¨https://github.com/liuyug/mdict-utils
tner/tweetner7,Text,,,,,https://arxiv.org/abs/2210.03797,https://huggingface.co/datasets/tner/tweetner7,,,,,,,,https://choosealicense.com/licenses/other/,,,,https://github.com/asahi417/tner/tree/master/examples/tweetner7_paper,,,,,,,,[TweetNER7](TBA)
togethercomputer/RedPajama-Data-1T-Sample,Text,,,,,,https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample,,,,,,,,,,,,,,,,,,,,"RedPajama is a clean-room, fully open-source implementation of the LLaMa dataset. This is a 1B-token sample of the full dataset."
TokenBender/code_instructions_122k_alpaca_style,,,,,,,https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,TokenBender/code_instructions_122k_alpaca_style dataset hosted on Hugging Face and contributed by the HF Datasets community
tomaarsen/conll2003,Text,,,,,https://www.aclweb.org/anthology/W03-0419/,https://huggingface.co/datasets/tomaarsen/conll2003,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,"The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2 tagging scheme, whereas the original dataset uses IOB1. For more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419"
tomasg25/scientific_lay_summarisation,Text,,,,,https://arxiv.org/abs/2210.09932,https://huggingface.co/datasets/tomasg25/scientific_lay_summarisation,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"This repository contains the PLOS and eLife datasets, introduced in the EMNLP 2022 paper ""[Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature ](https://arxiv.org/abs/2210.09932)"". Each dataset contains full biomedical research articles paired with expert-written lay summaries (i.e., non-technical summaries). PLOS articles are derived from various journals published by [the Public Library of Science (PLOS)](https://plos.org/), whereas eLife articles are derived from the [eLife](https://elifesciences.org/) journal. More details/anlaysis on the content of each dataset are provided in the paper. Both ""elife"" and ""plos"" have 6 features: - ""article"": the body of the document (including the abstract), sections seperated by ""/n"". - ""section_headings"": the title of each section, seperated by ""/n"". - ""keywords"": keywords describing the topic of the article, seperated by ""/n"". - ""title"" : the title of the article. - ""year"" : the year the article was published. - ""summary"": the lay summary of the document."
tomekkorbak/pile-chunk-toxicity-scored-3,,,,,,,https://huggingface.co/datasets/tomekkorbak/pile-chunk-toxicity-scored-3,,,,,,,,,,,,,,,,,,,,A chunk 3 of the Pile (2.2m documents) scored using the Perspective API (on May 18-20 2022)
TomTBT/pmc_open_access_section,,,,,,,https://huggingface.co/datasets/TomTBT/pmc_open_access_section,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,TomTBT/pmc_open_access_section dataset hosted on Hugging Face and contributed by the HF Datasets community
tonytan48/TempReason,,,,,,,https://huggingface.co/datasets/tonytan48/TempReason,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"The TempReason dataset to evaluate the temporal reasoning capability of Large Language Models. From paper ""Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models"" in ACL 2023."
totally-not-an-llm/EverythingLM-data-V2,,,,,,,https://huggingface.co/datasets/totally-not-an-llm/EverythingLM-data-V2,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,EverythingLM V2 Dataset EverythingLM V2 is a diverse instruct dataset consisting of 1k of human-assistant conversations. These sets were generated using principles from both evol-instruct and Orca. The dataset encompasses a wide array of topics and interactions. Differences for V1: All data in V2 is generated by GPT4 Higher quality dataset generation pipeline: More humalike seed prompts Fixed some bugs in the script More diverse creative writing More diverse seedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/totally-not-an-llm/EverythingLM-data-V2.
toxigen/toxigen-data,Audio,Detection,General,Bounding Box/Mask,"mAP, IoU",https://arxiv.org/abs/2203.09509,https://huggingface.co/datasets/toxigen/toxigen-data,,,,,,,,,,,,,,,,,"DETR, Faster R-CNN, YOLO",,,"Dataset Card for ToxiGen Sign up for Data Access To access ToxiGen, first fill out this form. Data-Summary: This dataset is for implicit hate speech detection. All instances were generated using GPT-3 and the methods described in our paper. Languages All text is written in English. Dataset Structure Data Fields We release TOXIGEN as a dataframe with the following fields: prompt is the prompt used forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/toxigen/toxigen-data."
Toygar/turkish-offensive-language-detection,Text,Detection,General,Bounding Box/Mask,"mAP, IoU",,https://huggingface.co/datasets/Toygar/turkish-offensive-language-detection,studies,,,,,,,https://choosealicense.com/licenses/cc-by-2.0/,,,,,,,,,"DETR, Faster R-CNN, YOLO",,,"Data-Summary: This dataset is enhanced version of existing offensive language studies. Existing studies are highly imbalanced, and solving this problem is too costly. To solve this, we proposed contextual data mining method for dataset augmentation. Our method is basically prevent us from retrieving random tweets and label individually. We can directly access almost exact hate related tweets and label them directly without any further human interaction in order to solveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Toygar/turkish-offensive-language-detection."
TrainingDataPro/generated-usa-passeports-dataset,,,,,,,https://huggingface.co/datasets/TrainingDataPro/generated-usa-passeports-dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-nd-4.0/,,,,,,,,,,,,Data generation in machine learning involves creating or manipulating data to train and evaluate machine learning models. The purpose of data generation is to provide diverse and representative
TransferRapid/CommonVoices20_ro,Text,,,,,,https://huggingface.co/datasets/TransferRapid/CommonVoices20_ro,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Common Voices Corpus 20.0 (Romanian) Common Voices is an open-source dataset of speech recordings created by Mozilla to improve speech recognition technologies. It consists of crowdsourced voice samples in multiple languages, contributed by volunteers worldwide. Challenges: The raw dataset included numerous recordings with incorrect transcriptions or those requiring adjustments, such as sampling rate modifications, conversion to .wav format, and other refinements essentialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TransferRapid/CommonVoices20_ro."
transformersbook/codeparrot,,,,,,,https://huggingface.co/datasets/transformersbook/codeparrot,,,,,,,,,,,,,,,,,,,,CodeParrot ðŸ¦œ Dataset What is it? This is the full CodeParrot dataset. It contains Python files used to train the code generation model in Chapter 10: Training Transformers from Scratch in the NLP with Transformers book. You can find the full code in the accompanying Github repository. Creation It was created with the GitHub dataset available via Google's BigQuery. It contains approximately 22 million Python files and is 180 GB (50 GB compressed)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/transformersbook/codeparrot.
trashsock/hands-images,,,,,,,https://huggingface.co/datasets/trashsock/hands-images,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,trashsock/hands-images dataset hosted on Hugging Face and contributed by the HF Datasets community
TREC-AToMiC/AToMiC-Images-v0.2,,,,,,https://trec-atomic.github.io/,https://huggingface.co/datasets/TREC-AToMiC/AToMiC-Images-v0.2,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"Dataset Card for ""AToMiC-All-Images_wi-pixels"" Languages The dataset contains 108 languages in Wikipedia. Data Instances Each instance is an image, its representation in bytes, and its associated captions. Intended Usage Image collection for Text-to-Image retrieval Image--Caption Retrieval/Generation/Translation Licensing Information CC BY-SA 4.0 international license Citation Information TBAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TREC-AToMiC/AToMiC-Images-v0.2."
trec-product-search/Product-Search-Triples,,,,,,,https://huggingface.co/datasets/trec-product-search/Product-Search-Triples,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,trec-product-search/Product-Search-Triples dataset hosted on Hugging Face and contributed by the HF Datasets community
Trelis/function_calling_extended,,,,,,,https://huggingface.co/datasets/Trelis/function_calling_extended,,,,,,,,,,,,,,,,,,,,"Trelis Function Calling Dataset UPDATE: As of Dec 5th 2023, there is a v3 of this dataset now available from here. Allows models to be fine-tuned for function-calling. The dataset is human generated and does not make use of Llama 2 or OpenAI! Contains 59 training and 17 test rows Based on eight functions: search_bing, search_arxiv, save_chat, read_json_file, list_files, get_current_weather, delete_file, clear_chat Access this dataset by purchasing a license HERE. Alternativelyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/function_calling_extended."
trendmicro-ailab/Primus-FineWeb,,,,,,https://arxiv.org/abs/2502.11191,https://huggingface.co/datasets/trendmicro-ailab/Primus-FineWeb,,,,,,,,https://choosealicense.com/licenses/odc-by/,,,,,,,,,,,,"PRIMUS: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training ðŸ¤— Primus-FineWeb The Primus-FineWeb dataset is constructed by filtering cybersecurity-related text from FineWeb, a refined version of Common Crawl. We began by leveraging Primus-Seed, a high-quality dataset of manually curated cybersecurity text, as positive samples. We then sampled ten times the amount of data from FineWeb as negative samples and trained a binary cybersecurityâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/trendmicro-ailab/Primus-FineWeb."
trojblue/sd1-bad-anatomy-images,,,,,,,https://huggingface.co/datasets/trojblue/sd1-bad-anatomy-images,,,,,,,,https://choosealicense.com/licenses/gpl/,,,,,,,,,,,,AI generated images that have relatively obvious issues target tag: bad anatomy
truongpdd/laion-2b-vietnamese-subset,,,,,,,https://huggingface.co/datasets/truongpdd/laion-2b-vietnamese-subset,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""laion-2b-vietnamese-subset"" More Information needed"
truthfulqa/truthful_qa,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/truthfulqa/truthful_qa,model is truthful in generating answers to questions,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for truthful_qa Data-Summary: TruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/truthfulqa/truthful_qa."
tsterbak/eurovision-lyrics-1956-2023,,,,,,,https://huggingface.co/datasets/tsterbak/eurovision-lyrics-1956-2023,,,,,,,,,,,,,,,,,,,,tsterbak/eurovision-lyrics-1956-2023 dataset hosted on Hugging Face and contributed by the HF Datasets community
ttgeng233/LongVALE,,,,,,https://arxiv.org/abs/2411.19772,https://huggingface.co/datasets/ttgeng233/LongVALE,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"Dataset Card for LongVALE Uses This dataset is designed for training and evaluating models on omni-modal (vision-audio-language-event) fine-grained video understanding tasks. It is intended for academic research and educational purposes only. For data generated using third-party models (e.g., Gemini-1.5-Pro, GPT-4o, Qwen-Audio), users must comply with the respective model providers' usage policies. Data Sources LongVALE comprises 8,411 long videos (549 hours) with 105â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ttgeng233/LongVALE."
ttss/rdf-triple,,,,,,,https://huggingface.co/datasets/ttss/rdf-triple,,,,,,,,,,,,,,,,,,,,ttss/rdf-triple dataset hosted on Hugging Face and contributed by the HF Datasets community
ttxy/weibo_4_moods,,,,,,,https://huggingface.co/datasets/ttxy/weibo_4_moods,,,,,,,,,,,,,,,,,,,,ttxy/weibo_4_moods dataset hosted on Hugging Face and contributed by the HF Datasets community
tuetschek/atis,,,,,,,https://huggingface.co/datasets/tuetschek/atis,,,,,,,,,,,,,,,,,,,,tuetschek/atis dataset hosted on Hugging Face and contributed by the HF Datasets community
tum-nlp/sexism-socialmedia-balanced,,,,,,,https://huggingface.co/datasets/tum-nlp/sexism-socialmedia-balanced,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"Citation @inproceedings{rydelek-etal-2023-adamr, title = ""{A}dam{R} at {S}em{E}val-2023 Task 10: Solving the Class Imbalance Problem in Sexism Detection with Ensemble Learning"", author = ""Rydelek, Adam and Dementieva, Daryna and Groh, Georg"", editor = {Ojha, Atul Kr. and Do{\u{g}}ru{\""o}z, A. Seza and Da San Martino, Giovanni and Tayyar Madabushi, Harish and Kumar, Ritesh and Sartori, Elisa}, booktitle =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/tum-nlp/sexism-socialmedia-balanced."
tungdop2/pokemon,,,,,,,https://huggingface.co/datasets/tungdop2/pokemon,,,,,,,,,,,,,,,,,,,,"Dataset Card Pokemon caption dataset This dataset contain artwork, name, type, species, and caption of all pokemons till 07/07/2023. Caption: generated by BLIP Artwork and other infomations: crawled from pokemondb More Information needed"
turing-motors/LLaVA-Instruct-150K-JA,,,,,,,https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,Dataset Details Dataset Type:Japanese LLaVA Instruct 150K is a localized version of the original LLaVA Visual Instruct 150K dataset. This version is translated into Japanese using DeepL API and is aimed at serving similar purposes in the context of Japanese language. Resources for More Information:For information on the original dataset: LLaVA Visual Instruct 150K License:Attribution-NonCommercial 4.0 International (CC BY-NC-4.0)The dataset should abide by the policy of OpenAI:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA.
turkish-nlp-suite/beyazperde-top-300-movie-reviews,Text,Sentiment Analysis,UI,Label,"Accuracy, F1 Score",,https://huggingface.co/datasets/turkish-nlp-suite/beyazperde-top-300-movie-reviews,,,,,,300,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, DistilBERT",,,Dataset Card for turkish-nlp-suite/beyazperde-top-300-movie-reviews Data-Summary: Beyazperde Movie Reviews offers Turkish sentiment analysis datasets that is scraped from popular movie reviews website Beyazperde.com. Top 300 Movies include audience reviews about best 300 movies of all the time. Here's the star rating distribution: star rating count 0.5 101 1.0 39 1.5 19 2.0 44 2.5 210 3.0 196 3.5 490 4.0 1212 4.5 818 5.0 1251â€¦ See the full description on the dataset page: https://huggingface.co/datasets/turkish-nlp-suite/beyazperde-top-300-movie-reviews.
turkmen/ahmetkayavocals,,,,,,,https://huggingface.co/datasets/turkmen/ahmetkayavocals,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,Ahmet Kaya Vokal Dataset AÃ§Ä±klamasÄ± Ahmet Kaya'nÄ±n ÅŸarkÄ±larÄ±ndaki vokaller UVR ile Ã§Ä±karÄ±lmÄ±ÅŸtÄ±r. Toplam 103 adet veri vardÄ±r. ahmetv3.zip iÃ§inde ise Ahmet KayanÄ±n en popÃ¼ler 3-4 rÃ¶portajÄ±nÄ±n kesilerek sadece onun konuÅŸtuÄŸu kÄ±sÄ±mlar ayrÄ±ca UVR ile tekrar gÃ¶zden geÃ§irilerek oluÅŸturulmuÅŸ hali vardÄ±r. ak320kbps.zip'in iÃ§inde ise Ahmet Kaya'nÄ±n 320kbps olarak indirilmiÅŸ yÃ¼ksek kalite olduÄŸu iddia edilen 4 parÃ§asÄ±nÄ±n UVR windows size 1024 agression 15 1_HP_UVR modeliâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/turkmen/ahmetkayavocals.
TurkuNLP/jigsaw_toxicity_pred_fi,Text,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",,https://huggingface.co/datasets/TurkuNLP/jigsaw_toxicity_pred_fi,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Data-Summary: This dataset is a DeepL -based machine translated version of the Jigsaw toxicity dataset for Finnish. The dataset is originally from a Kaggle competition https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data. The dataset poses a multi-label text classification problem and includes the labels identity_attack, insult, obscene, severe_toxicity, threat and toxicity."
twang2218/chinese-law-and-regulations,,,,,,,https://huggingface.co/datasets/twang2218/chinese-law-and-regulations,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,twang2218/chinese-law-and-regulations dataset hosted on Hugging Face and contributed by the HF Datasets community
tweets-hate-speech-detection/tweets_hate_speech_detection,Audio,Detection,General,Bounding Box/Mask,"mAP, IoU",https://github.com/sharmaroshan/Twitter-Sentiment-Analysis,https://huggingface.co/datasets/tweets-hate-speech-detection/tweets_hate_speech_detection,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,"DETR, Faster R-CNN, YOLO",,,"Dataset Card for Tweets Hate Speech Detection Data-Summary: The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets. Formally, given a training sample of tweets and labels, where label â€˜1â€™ denotes the tweet is racist/sexist and label â€˜0â€™ denotes the tweet is notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tweets-hate-speech-detection/tweets_hate_speech_detection."
Twitter/SignedGraphs,,,,,,https://arxiv.org/abs/2201.11675,https://huggingface.co/datasets/Twitter/SignedGraphs,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Learning Stance Embeddings from Signed Social Graphs This repo contains the datasets from our paper Learning Stance Embeddings from Signed Social Graphs. [PDF] [HuggingFace Datasets] This work is licensed under a Creative Commons Attribution 4.0 International License. Overview A key challenge in social network analysis is understanding the position, or stance, of people in the graph on a large set of topics. In such social graphs, modeling (dis)agreementâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Twitter/SignedGraphs."
twnlp/mydataset,,,,,,,https://huggingface.co/datasets/twnlp/mydataset,,,,,,,,,,,,,,,,,,,,twnlp/mydataset dataset hosted on Hugging Face and contributed by the HF Datasets community
tyang816/MedChatZH,,,,,,,https://huggingface.co/datasets/tyang816/MedChatZH,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,MedChatZH MedChatZHä¸­æ–‡å¾®è°ƒå‚æ•° åŸºäºŽBaichuan-7Båœ¨ä¸­åŒ»ä¹¦ç±æ•°æ®å’ŒåŒ»ç–—æŒ‡ä»¤å¯¹è¯æ•°æ®ä¸Šè®­ç»ƒï¼Œä½¿å…¶å…·å¤‡è¾ƒå¼ºçš„ä¸­æ–‡åŒ»ç–—å¯¹è¯èƒ½åŠ›ï¼Œèƒ½å¤Ÿç†è§£ä¸­åŒ»é—®é¢˜ã€‚ Githubï¼šMedChatZH-7B ðŸš€ ç»“æžœ é—®é¢˜ å›žç­” ä¸­åŒ»æ²»è„±å‘æœ‰æ•ˆå—è„±å‘5ã€6å¹´äº†ï¼Œæ˜Žæ˜¾æ¯”ä»¥å‰å°‘äº†å¾ˆå¤šï¼Œè„±å‘ä½ç½®æ¯”è¾ƒå‡è¡¡ï¼Œæ²¡æœ‰çªå‡ºæŽ‰çš„æ¯”è¾ƒå¤šçš„åœ°æ–¹ï¼Œæˆ‘çš„å‘è´¨æ¯”è¾ƒç¡¬ï¼Œå¤´å‘ç”Ÿé•¿ä¹Ÿè¾ƒå¿«ã€‚ ä¸­è¯æ²»ç–—è„±å‘çš„æ•ˆæžœå› äººè€Œå¼‚ï¼Œä½†å¯¹äºŽæ‚¨è¿™ç§å¤´å‘è´¨é‡å¥½ä¸”ç”Ÿé•¿é€Ÿåº¦å¿«çš„æ‚£è€…æ¥è¯´ï¼Œä¸­è¯å¯èƒ½æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚ä¸€äº›å¸¸ç”¨çš„ä¸­è¯æ–¹å‰‚åŒ…æ‹¬å…­å‘³åœ°é»„ä¸¸ã€å››ç‰©æ±¤ç­‰ï¼Œè¿™äº›è¯ç‰©å¯ä»¥æ»‹å…»è‚è‚¾ï¼Œä¿ƒè¿›è¡€æ¶²å¾ªçŽ¯å’Œæ¯›å›Šè¥å…»ä¾›åº”ï¼Œä»Žè€Œæ”¹å–„è„±å‘é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥é€šè¿‡é’ˆç¸ã€è‰¾ç¸ç­‰æ–¹å¼è¿›è¡Œè¾…åŠ©æ²»ç–—ã€‚ä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸­è¯æ²»ç–—éœ€è¦é•¿æœŸåšæŒæ‰èƒ½è§æ•ˆï¼ŒåŒæ—¶è¿˜éœ€è¦æ³¨æ„é¥®é£Ÿå’Œç”Ÿæ´»ä¹ æƒ¯ç­‰æ–¹é¢çš„é—®é¢˜ï¼Œä»¥è¾¾åˆ°æ›´å¥½çš„ç–—æ•ˆã€‚å»ºè®®æ‚¨å’¨è¯¢ä¸“ä¸šçš„ä¸­åŒ»åŒ»ç”Ÿæˆ–çš®è‚¤ç§‘åŒ»ç”Ÿï¼Œæ ¹æ®ä¸ªäººæƒ…å†µåˆ¶å®šä¸ªæ€§åŒ–çš„æ²»ç–—æ–¹æ¡ˆã€‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/tyang816/MedChatZH.
tyouisen/aclue,,,,,,https://arxiv.org/abs/2310.0955,https://huggingface.co/datasets/tyouisen/aclue,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,The Ancient Chinese Language Understanding Evaluation (ACLUE) is an evaluation benchmark focused on ancient Chinese language comprehension. It aims to assess the performance of large-scale language models on understanding ancient Chinese.
tyqiangz/multilingual-sentiments,,,,,,,https://huggingface.co/datasets/tyqiangz/multilingual-sentiments,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Multilingual Sentiments Dataset A collection of multilingual sentiments datasets grouped into 3 classes -- positive, neutral, negative. Most multilingual sentiment datasets are either 2-class positive or negative, 5-class ratings of products reviews (e.g. Amazon multilingual dataset) or multiple classes of emotions. However, to an average person, sometimes positive, negative and neutral classes suffice and are more straightforward to perceive and annotate. Also, aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tyqiangz/multilingual-sentiments."
Ubenwa/CryCeleb2023,Audio,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Ubenwa/CryCeleb2023,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-nd-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,of pure expiration sounds,"Dataset Card for ""CryCeleb2023"" Data-Summary: The CryCeleb2023 dataset is a compilation of cries gathered from 786 infants from various hospitals. The 26k audio files make up 6.5 hours of pure expiration sounds. The dataset also contains information on the time of recording, which is either within the first hour(s) of life or upon hospital discharge, typically within 24 hours of birth. Supported Tasks and Leaderboards CryCeleb2023 competitionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ubenwa/CryCeleb2023."
ubuntu-dialogs-corpus/ubuntu_dialogs_corpus,Text,,,,,https://arxiv.org/abs/1506.08909,https://huggingface.co/datasets/ubuntu-dialogs-corpus/ubuntu_dialogs_corpus,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter."
ucberkeley-dlab/measuring-hate-speech,,,,,,http://hatespeech.berkeley.edu,https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset card for Measuring Hate Speech This is a public release of the dataset described in Kennedy et al. (2020) and Sachdeva et al. (2022), consisting of 39,565 comments annotated by 7,912 annotators, for 135,556 combined rows. The primary outcome variable is the ""hate speech score"" but the 10 constituent ordinal labels (sentiment, (dis)respect, insult, humiliation, inferior status, violence, dehumanization, genocide, attack/defense, hate speech benchmark) can also be treatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech."
ucf-crcv/GAEA-Train,,,,,,https://arxiv.org/abs/2503.16423,https://huggingface.co/datasets/ucf-crcv/GAEA-Train,,,,,,,Main contributions:,,,,,,,,,,,,,"GAEA: A Geolocation Aware Conversational Model Summary Image geolocalization, in which, traditionally, an AI model predicts the precise GPS coordinates of an image is a challenging task with many downstream applications. However, the user cannot utilize the model to further their knowledge other than the GPS coordinate; the model lacks an understanding of the location and the conversational ability to communicate with the user. In recent days, with tremendous progress of large multimodalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ucf-crcv/GAEA-Train."
ucinlp/drop,Text,Reasoning,UI,Text,"Accuracy, F1 Score",https://allenai.org/data/drop,https://huggingface.co/datasets/ucinlp/drop,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"T5, UnifiedQA, BART",,,"Dataset Card for ""drop"" Data-Summary: DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs. . DROP is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ucinlp/drop."
ucirvine/sms_spam,Text,General,Scientific,Text,"Accuracy, F1 Score",http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection,https://huggingface.co/datasets/ucirvine/sms_spam,English,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for [Dataset Name] Data-Summary: The SMS Spam Collection v.1 is a public set of SMS labeled messages that have been collected for mobile phone spam research. It has one collection composed by 5,574 English, real and non-enconded messages, tagged according being legitimate (ham) or spam. Supported Tasks and Leaderboards [More Information Needed] Languages English Dataset Structure Dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ucirvine/sms_spam."
uclanlp/wino_bias,Text,General,General,Text,"Accuracy, F1 Score",https://uclanlp.github.io/corefBias/overview,https://huggingface.co/datasets/uclanlp/wino_bias,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,a Winograd-schema dataset for coreference resolution focused on gender bias,"BERT, RoBERTa, T5",,,"Dataset Card for Wino_Bias dataset Data-Summary: WinoBias, a Winograd-schema dataset for coreference resolution focused on gender bias. The corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). Supported Tasks and Leaderboards The underlying task is coreference resolution. Languages English Dataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/uclanlp/wino_bias."
ucsbnlp/liar,Text,,,,,https://sites.cs.ucsb.edu/~william/,https://huggingface.co/datasets/ucsbnlp/liar,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"LIAR is a dataset for fake news detection with 12.8K human labeled short statements from politifact.com's API, and each statement is evaluated by a politifact.com editor for its truthfulness. The distribution of labels in the LIAR dataset is relatively well-balanced: except for 1,050 pants-fire cases, the instances for all other labels range from 2,063 to 2,638. In each case, the labeler provides a lengthy analysis report to ground each judgment."
UCSC-VLAA/MedTrinity-25M,,,,,,https://arxiv.org/abs/2408.02900,https://huggingface.co/datasets/UCSC-VLAA/MedTrinity-25M,,,,,,,,,,,,,,,,,,,,"Tutorial of using Medtrinity-25M MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These enriched annotations encompass both global textual information, such as disease/lesion type, modality, region-specific descriptions, and inter-regional relationships, as well as detailed local annotations for regions of interest (ROIs), includingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/UCSC-VLAA/MedTrinity-25M."
UCSD26/medical_dialog,Text,,,,,https://arxiv.org/abs/2004.03329,https://huggingface.co/datasets/UCSD26/medical_dialog,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,The MedDialog dataset (English) contains conversations (in English) between doctors and patients.It has 0.26 million dialogues. The data is continuously growing and more dialogues will be added. The raw dialogues are from healthcaremagic.com and icliniq.com. All copyrights of the data belong to healthcaremagic.com and icliniq.com.
UdS-LSV/menyo20k_mt,Text,,,,,,https://huggingface.co/datasets/UdS-LSV/menyo20k_mt,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"MENYO-20k is a multi-domain parallel dataset with texts obtained from news articles, ted talks, movie transcripts, radio transcripts, science and technology texts, and other short articles curated from the web and professional translators. The dataset has 20,100 parallel sentences split into 10,070 training sentences, 3,397 development sentences, and 6,633 test sentences (3,419 multi-domain, 1,714 news domain, and 1,500 ted talks speech transcript domain). The development and test sets are available upon request."
Ugiat/ner-cat,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2503.14173,https://huggingface.co/datasets/Ugiat/ner-cat,"television transcriptions, designed to improve Named Entity Recognition (NER) performance for the Catalan language",,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"NERCat Dataset Data-Summary: The NERCat dataset is a manually annotated collection of Catalan-language television transcriptions, designed to improve Named Entity Recognition (NER) performance for the Catalan language. The dataset covers diverse domains such as politics, sports, and culture, and includes 9,242 sentences with 13,732 named entities annotated across eight categories: Person, Facility, Organization, Location, Product, Event, Date, and Law. The dataset wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ugiat/ner-cat."
ujs/hinglish-compressed,,,,,,,https://huggingface.co/datasets/ujs/hinglish-compressed,,,,,,,,,,,,,,,,,,,,A Hugginface version of the Hindi-English code-switched dataset from OpenSLR-104.
Ukhushn/home-depot,,,,,,,https://huggingface.co/datasets/Ukhushn/home-depot,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,Dataset Card for Ukhushn/home-depot
ukr-models/Ukr-Synth,Text,,,,,,https://huggingface.co/datasets/ukr-models/Ukr-Synth,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Large silver standard Ukrainian corpus annotated with morphology tags, syntax trees and PER, LOC, ORG NER-tags."
unimelb-nlp/wikiann,Text,General,General,Text,"Accuracy, F1 Score",https://github.com/afshinrahimi/mmner,https://huggingface.co/datasets/unimelb-nlp/wikiann,from the original WikiANN corpus,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for WikiANN Data-Summary: WikiANN (sometimes called PAN-X) is a multilingual named entity recognition dataset consisting of Wikipedia articles annotated with LOC (location), PER (person), and ORG (organisation) tags in the IOB2 format. This version corresponds to the balanced train, dev, and test splits of Rahimi et al. (2019), which supports 176 of the 282 languages from the original WikiANN corpus. Supported Tasks and Leaderboardsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/unimelb-nlp/wikiann."
unitreerobotics/LAFAN1_Retargeting_Dataset,,,,,,,https://huggingface.co/datasets/unitreerobotics/LAFAN1_Retargeting_Dataset,,,,,,,,,,,,,,,,,,,,"LAFAN1 Retargeting Dataset To make the motion of humanoid robots more natural, we retargeted LAFAN1 motion capture data to Unitree's humanoid robots, supporting three models: H1, H1_2, and G1. This retargeting was achieved through numerical optimization based on Interaction Mesh and IK, considering end-effector pose constraints, as well as joint position and velocity constraints, to prevent foot slippage. It is important to note that the retargeting only accounted for kinematicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/unitreerobotics/LAFAN1_Retargeting_Dataset."
universal-dependencies/universal_dependencies,Text,,,,,https://universaldependencies.org/,https://huggingface.co/datasets/universal-dependencies/universal_dependencies,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008)."
Universal-NER/Pile-NER-type,,,,,,,https://huggingface.co/datasets/Universal-NER/Pile-NER-type,,,,,,,,,,,,,,,,,,,,Intro Pile-NER-type is a set of GPT-generated data for named entity recognition using the type-based data construction prompt. It was collected by prompting gpt-3.5-turbo-0301 and augmented by negative sampling. Check our project page for more information. License Attribution-NonCommercial 4.0 International
uoft-cs/cifar100,Image,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",https://www.cs.toronto.edu/~kriz/cifar.html,https://huggingface.co/datasets/uoft-cs/cifar100,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,"Dataset Card for CIFAR-100 Data-Summary: The CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images per class. There are 500 training images and 100 testing images per class. There are 50000 training images and 10000 test images. The 100 classes are grouped into 20 superclasses. There are two labels per image - fine label (actual class) and coarse label (superclass). Supported Tasks and Leaderboards image-classification: Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/uoft-cs/cifar100."
uonlp/CulturaX,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2309.09400,https://huggingface.co/datasets/uonlp/CulturaX,", tailored for large language model (LLM) development",,,,,,,,,,,,,,"ng and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including languageâ€¦ See the full description on the dataset page: https://huggingface",,"BERT, RoBERTa, T5",,,"CulturaX Cleaned, Enormous, and Public: The Multilingual Fuel to Democratize Large Language Models for 167 Languages Data-Summary: We present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for large language model (LLM) development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/uonlp/CulturaX."
uwunion/instruct_svg,,,,,,,https://huggingface.co/datasets/uwunion/instruct_svg,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,uwunion/instruct_svg dataset hosted on Hugging Face and contributed by the HF Datasets community
vaishaal/ImageNetV2,,,,,,,https://huggingface.co/datasets/vaishaal/ImageNetV2,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,vaishaal/ImageNetV2 dataset hosted on Hugging Face and contributed by the HF Datasets community
vaishali/spider-tableQA,,,,,,,https://huggingface.co/datasets/vaishali/spider-tableQA,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""spider-tableQA"" Usage import pandas as pd from datasets import load_dataset spider_tableQA = load_dataset(""vaishali/spider-tableQA"") for sample in spider_tableQA['train']: question = sample['question'] sql_query = sample['query'] input_table_names = sample[""table_names""] input_tables = [pd.read_json(table, orient='split') for table in sample['tables']] answer = pd.read_json(sample['answer'], orient='split') # flattened input/outputâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vaishali/spider-tableQA."
vasugoel/K-12Corpus,,,,,,,https://huggingface.co/datasets/vasugoel/K-12Corpus,,,,,,,,,,,,,,,,,,,,K-12Corpus
vblagoje/cc_news,Text,General,General,Text,"Accuracy, F1 Score",https://commoncrawl.org/2016/10/news-dataset-available/,https://huggingface.co/datasets/vblagoje/cc_news,news articles published between Jan 2017 and December 2019,,,,,,,https://choosealicense.com/licenses/unknown/,,,10,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for CC-News Data-Summary: CC-News dataset contains news articles from news sites all over the world. The data is available on AWS S3 in the Common Crawl bucket at /crawl-data/CC-NEWS/. This version of the dataset has been prepared using news-please - an integrated web crawler and information extractor for news.It contains 708241 English language news articles published between Jan 2017 and December 2019. It represents a small portion of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vblagoje/cc_news.
Vchitect/Vchitect_T2V_DataVerse,,,,,,https://arxiv.org/abs/2501.08453,https://huggingface.co/datasets/Vchitect/Vchitect_T2V_DataVerse,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Vchitect-T2V-Dataverse Vchitect Team1 1Shanghai Artificial Intelligence Laboratory Paper | Project Page | Data Overview The Vchitect-T2V-Dataverse is the core dataset used to train our text-to-video diffusion model, Vchitect-2.0: Parallel Transformer for Scaling Up Video Diffusion Models. It comprises 14 million high-quality videos collected from the Internet, each paired with detailed textualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Vchitect/Vchitect_T2V_DataVerse."
vekkt/french_CEFR,,,,,,,https://huggingface.co/datasets/vekkt/french_CEFR,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,vekkt/french_CEFR dataset hosted on Hugging Face and contributed by the HF Datasets community
venetis/symptom_text_to_disease_mk4,,,,,,,https://huggingface.co/datasets/venetis/symptom_text_to_disease_mk4,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""symptom_text_to_disease_mk4"" More Information needed"
vessl/Bored_Ape_NFT_text,,,,,,,https://huggingface.co/datasets/vessl/Bored_Ape_NFT_text,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,https://github.com/vessl-ai/examples,,,,,,,,"Disclaimer All rights belong to their owners. Models and datasets can be removed from the site at the request of the copyright holder. How to use from datasets import load_dataset dataset = load_dataset(""VESSL/Bored_Ape_NFT_text"") Data Field image = binary image file and path text = auto generated prompt for image Citation & Information @InProceedings{VESSL, author={Jinpil Choi} year=2023} Projectsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vessl/Bored_Ape_NFT_text."
vesteinn/babylm,,,,,,,https://huggingface.co/datasets/vesteinn/babylm,,,,,,,,,,,,,,,,,,,,"ATTENTION This is preprocessed data for the BabyLM challenge https://babylm.github.io/ If you want the raw unprocessed files, you should download them directly."
viber1/indian-law-dataset,,,,,,,https://huggingface.co/datasets/viber1/indian-law-dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,viber1/indian-law-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community
vicgalle/alpaca-gpt4,,,,,,https://instruction-tuning-with-gpt-4.github.io,https://huggingface.co/datasets/vicgalle/alpaca-gpt4,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Dataset Card for ""alpaca-gpt4"" This dataset contains English Instruction-Following generated by GPT-4 using Alpaca prompts for fine-tuning LLMs. The dataset was originaly shared in this repository: https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM. This is just a wraper for compatibility with huggingface's datasets library. Dataset structure It contains 52K instruction-following data generated by GPT-4 using the same prompts as in Alpaca. The dataset hasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vicgalle/alpaca-gpt4."
victorych22/lamini-embedded-instructions-only,,,,,,,https://huggingface.co/datasets/victorych22/lamini-embedded-instructions-only,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""lamini-embedded-instructions-only"" More Information needed"
victunes/nart-100k-synthetic-buddy-mixed-names,,,,,,,https://huggingface.co/datasets/victunes/nart-100k-synthetic-buddy-mixed-names,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,"Dataset Modifications Renamed the patient with all these names: https://github.com/dominictarr/random-name/blob/master/names.txt Renamed the therapist with ""Buddy"" Modification Script is included in the repo Original dataset card: https://huggingface.co/datasets/jerryjalapeno/nart-100k-synthetic Keep in mind that this dataset is entirely synthetic. It is not fully representative of real therapy situations. If you are training an LLM therapist keep in mind the limitations of LLMs andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/victunes/nart-100k-synthetic-buddy-mixed-names."
Vidhaan/LegalCitationWorthiness,,,,,,,https://huggingface.co/datasets/Vidhaan/LegalCitationWorthiness,,,,,,,,,,,,,,,,,,,,A novel dataset for benchmarking citation worthiness detection task in the American Legal Corpus. For more details about the dataset please refer to the original paper. Data Fields File Name: the case file to which the sentence belongs. Sentence Number: The sentence number as present in the document. Sentence: The naturally occurring sentence in the text (after preprocessing/removing citation span.) Label: Integer value of â€˜0â€™ or â€˜1â€™. â€˜0â€™ represents that the sentence is notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Vidhaan/LegalCitationWorthiness.
vidhikatkoria/SGD_Hotels,,,,,,,https://huggingface.co/datasets/vidhikatkoria/SGD_Hotels,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""SGD_Hotels"" More Information needed"
vidore/colpali_train_set,,,,,,https://arxiv.org/abs/2407.01449,https://huggingface.co/datasets/vidore/colpali_train_set,,,,,,,,,,,,,,,,,,,,"Dataset Description This dataset is the training set of ColPali it includes 127,460 query-image pairs from both openly available academic datasets (63%) and a synthetic dataset made up of pages from web-crawled PDF documents and augmented with VLM-generated (Claude-3 Sonnet) pseudo-questions (37%). Our training set is fully English by design, enabling us to study zero-shot generalization to non-English languages. Dataset #"
VietAI/vi_pubmed,Image-Text,Translation,General,Text,"BLEU, METEOR, TER",https://arxiv.org/abs/2210.05610,https://huggingface.co/datasets/VietAI/vi_pubmed,Through Large-Scale Translation,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,"T5, mBART, M2M100",,,Data-Summary: 20M Vietnamese PubMed biomedical abstracts translated by the state-of-the-art English-Vietnamese Translation project. The data has been used as unlabeled dataset for pretraining a Vietnamese Biomedical-domain Transformer model. image source: Enriching Biomedical Knowledge for Vietnamese Low-resource Language Through Large-Scale Translation Language English: Original biomedical abstracts from Pubmed Vietnamese: Synthetic abstract translated by aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VietAI/vi_pubmed.
vietgpt/wikipedia_vi,,,,,,,https://huggingface.co/datasets/vietgpt/wikipedia_vi,,,,,,,,,,,,,,,,,,,,Wikipedia Source: https://huggingface.co/datasets/wikipedia Num
vikp/pypi_clean,,,,,,,https://huggingface.co/datasets/vikp/pypi_clean,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""pypi_clean"" All of the latest package versions from pypi. The original data came from here. I pulled the latest versions of each package, then extracted only md, rst, ipynb, and py files. I then applied some cleaning: rendering notebooks removing leading comments/licenses"
Villian7/Emotions_Data,,,,,,,https://huggingface.co/datasets/Villian7/Emotions_Data,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset Card for ""emotions"" More Information needed"
VIMA/VIMA-Data,Text,General,General,Text,"Accuracy, F1 Score",https://vimalabs.github.io/,https://huggingface.co/datasets/VIMA/VIMA-Data,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for VIMA-Data Data-Summary: This is the official dataset used to train general robot manipulation agents with multimodal prompts, as presented in paper. It contains 650K trajectories for 13 tasks in VIMA-Bench. All demonstrations are generated by oracles. Dataset Structure Data are grouped into different tasks. Within each trajectory's folder, there are two folders rgb_front and rgb_top, and three files obs.pkl, action.pkl, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VIMA/VIMA-Data."
vincentmin/eli5_rlhf_explainlikeim5,,,,,,,https://huggingface.co/datasets/vincentmin/eli5_rlhf_explainlikeim5,,,,,,,,,,,,,,,,,,,,"ELI5 paired This is a processed version of the eli5 dataset. Compared to ""eli5_rlhf"", this dataset contains only QA pairs from the train split of the eli5 dataset and only from the subreddit explainlikeimfive. Furthermore, the function def get_question("
VinVanGogh/Psychology-10K-Indo-Llama2-Chat,,,,,,,https://huggingface.co/datasets/VinVanGogh/Psychology-10K-Indo-Llama2-Chat,,,,,,,,,,,,,,,,,,,,VinVanGogh/Psychology-10K-Indo-Llama2-Chat dataset hosted on Hugging Face and contributed by the HF Datasets community
viola77data/recycling-dataset,Image,Classification,General,Label,"Accuracy, F1 Score, Precision, Recall",,https://huggingface.co/datasets/viola77data/recycling-dataset,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, DeBERTa, XLNet",,,Dataset Card for recycling-dataset Data-Summary: This is a recycling dataset that can be used for image classification. It has 11 categories: aluminium batteries cardboard disposable plates glass hard plastic paper paper towel polystyrene soft plastics takeaway cups It was scrapped from DuckDuckGo using this tool: https://pypi.org/project/jmd-imagescraper/
visheratin/laion-coco-nllb,,,,,,https://arxiv.org/abs/2309.01859,https://huggingface.co/datasets/visheratin/laion-coco-nllb,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,LAION COCO translated into 200 languages This dataset contains the samples of the LAION-COCO dataset translated to 200 languages using the largest NLLB-200 model (3.3B parameters). Fields description id - unique ID of the image. url - original URL of the image from the LAION-COCO dataset. eng_caption - original English caption from the LAION-COCO dataset. captions - a list of captions translated to the languages from the Flores 200 dataset. Every item in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/visheratin/laion-coco-nllb.
vishnun/SpellGram,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/vishnun/SpellGram,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,SpellGram Dataset consisting of grammatical and spelling errors Homepage: Repository: Paper: Leaderboard: Point of Contact: Data-Summary: [More Information Needed] Supported Tasks and Leaderboards [More Information Needed] Languages [More Information Needed] Dataset Structure Data Instances [train.csv] Data Fields [More Information Needed] Data Splitsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vishnun/SpellGram.
Vision-CAIR/cc_sbu_align,,,,,,,https://huggingface.co/datasets/Vision-CAIR/cc_sbu_align,,,,,,,,,,,,,,,,,,,,"MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models Deyao Zhu* (On Job Market!), Jun Chen* (On Job Market!), Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. *Equal Contribution King Abdullah University of Science and Technology Online Demo Click the image to chat with MiniGPT-4 around your images"
VISION-Workshop/VISION-Datasets,Text,Segmentation,General,Bounding Box/Mask,"IoU, Pixel Accuracy",https://vision-based-industrial-inspection.github.io/cvpr-2023/,https://huggingface.co/datasets/VISION-Workshop/VISION-Datasets,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,"Mask R-CNN, DeepLab, U-Net",,,"Dataset Card for VISION Datasets Data-Summary: The VISION Datasets are a collection of 14 industrial inspection datasets, designed to explore the unique challenges of vision-based industrial inspection. These datasets are carefully curated from Roboflow and cover a wide range of manufacturing processes, materials, and industries. To further enable precise defect segmentation, we annotate each dataset with polygon labels based on the provided bounding box labels.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/VISION-Workshop/VISION-Datasets."
Vithika/TextToCode,,,,,,,https://huggingface.co/datasets/Vithika/TextToCode,,,,,,,,,,,,,,,,,,,,Vithika/TextToCode dataset hosted on Hugging Face and contributed by the HF Datasets community
vivym/midjourney-prompts,,,,,,,https://huggingface.co/datasets/vivym/midjourney-prompts,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"midjourney-prompts Description This dataset contains the cleaned midjourney prompts from Midjourney. Total prompts: 9,085,397 Version Count 5.2 2,272,465 5.1 2,060,106 5.0 3,530,770 4.0 1,204,384 3.0 14,991 2.0 791 1.0 1,239 Style Count default 8,874,181 raw 177,953 expressive 27,919 scenic 2,146 cute 2,036 original 511"
vjain/biology_AP_embeddings,,,,,,,https://huggingface.co/datasets/vjain/biology_AP_embeddings,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,vjain/biology_AP_embeddings dataset hosted on Hugging Face and contributed by the HF Datasets community
vldsavelyev/guitar_tab,Text,,,,,,https://huggingface.co/datasets/vldsavelyev/guitar_tab,,,,,,,,,,,,,,,,,,,,"Dataset of music tablature, in alphaTex (https://alphatab.net/docs/alphatex) format, converted from Guitar Pro files (gp3, gp4, gp5, which are downloaded from https://rutracker.org/forum/viewtopic.php?t=2888130"
vocab-transformers/wiki-en-passages-20210101,,,,,,,https://huggingface.co/datasets/vocab-transformers/wiki-en-passages-20210101,,,,,,,,,,,,,,,,,,,,"wiki-en-passages-20210101 This is a processed dump of the English Wikipedia from 2021-01-01. Each page has been splitted into paragraphs as they appear in the text. Lists, tables and headlines had been removed. In total it has 38,080,804 passages. Further, each article contain meta-data on the number of languages this article exists in and on the number of views this article received over a 1 year period. The articles are sorted from most popular (most languages available, mostâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vocab-transformers/wiki-en-passages-20210101."
voiceintelligenceresearch/MOCKS,Text,,,,,,https://huggingface.co/datasets/voiceintelligenceresearch/MOCKS,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,Multilingual Open Custom Keyword Spotting Testset (MOCKS) is a comprehensive audio testset for evaluation and benchmarking Open-Vocabulary Keyword Spotting (OV-KWS) models.
voidful/TMD,,,,,,,https://huggingface.co/datasets/voidful/TMD,,,,,,,,,,,,,,,,,,,,voidful/TMD dataset hosted on Hugging Face and contributed by the HF Datasets community
volvoDon/petrology-sections,,,,,,,https://huggingface.co/datasets/volvoDon/petrology-sections,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,volvoDon/petrology-sections dataset hosted on Hugging Face and contributed by the HF Datasets community
vucinatim/spectrogram-captions,,,,,,,https://huggingface.co/datasets/vucinatim/spectrogram-captions,,,,,,,,https://choosealicense.com/licenses/afl-3.0/,,,,,,,,,,,,Dataset of captioned spectrograms (text describing the sound).
vumichien/ja_opus100_processed,,,,,,,https://huggingface.co/datasets/vumichien/ja_opus100_processed,,,,,,,,,,,,,,,,,,,,vumichien/ja_opus100_processed dataset hosted on Hugging Face and contributed by the HF Datasets community
vyas21/modified_anthropic_convo_data,Text,General,General,Text,"Accuracy, F1 Score",https://huggingface.co/datasets/Anthropic/hh-rlhf,https://huggingface.co/datasets/vyas21/modified_anthropic_convo_data,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset name: ""modified_anthropic_convo_data"" Dataset Card for Conversational AI Bot Data-Summary: This dataset is the augmented version of the same dataset found here https://huggingface.co/datasets/Anthropic/hh-rlhf Two new columns have been added called human_speaker and assistant_speaker to make it easier to directly use the data for a causal langauage modeling task Currently only one pair of conversations have been picked, the dataset willâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vyas21/modified_anthropic_convo_data."
waifu-research-department/embeddings,,,,,,,https://huggingface.co/datasets/waifu-research-department/embeddings,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Info Try to include embedding info in the commit description (model, author, artist, images, etc) Naming: name-object/style"
wanghaofan/pokemon-wiki-captions,,,,,,,https://huggingface.co/datasets/wanghaofan/pokemon-wiki-captions,,,,,,,,,,,,,,,,,,,,"Dataset Card for PokÃ©mon wiki captions This project is inspired by pokmon-blip-captions, where the captions are all generated by pre-trained BLIP without any manual effort. However, the quality and accuracy of their captions are not satisfactory enough, which leaves it known whether better captions lead to better results. This motivates our dataset."
wangrui6/Zhihu-KOL,,,,,,,https://huggingface.co/datasets/wangrui6/Zhihu-KOL,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""Zhihu-KOL"" Zhihu data for training Open Assitant More Information needed"
wanicca/WikiHowQA-mnbvc,,,,,,,https://huggingface.co/datasets/wanicca/WikiHowQA-mnbvc,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,ä»ŽWikiHowé¡µé¢æŠ½å–çš„ä¸­æ–‡/è‹±æ–‡é—®ç­”æ•°æ® ç›¸å…³é¡¹ç›®: MNBVC æŠ½å–å·¥å…·ä»£ç ï¼šWikiHowQAExtractor
wanng/laion-high-resolution-chinese,,,,,,,https://huggingface.co/datasets/wanng/laion-high-resolution-chinese,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"laion-high-resolution-chinese ç®€ä»‹ Brief Introduction å–è‡ªLaion5B-high-resolutionå¤šè¯­è¨€å¤šæ¨¡æ€æ•°æ®é›†ä¸­çš„ä¸­æ–‡éƒ¨åˆ†ï¼Œä¸€å…±2.66Mä¸ªå›¾æ–‡å¯¹ã€‚ A subset from Laion5B-high-resolution (a multimodal dataset), around 2.66M image-text pairs (only Chinese). æ•°æ®é›†ä¿¡æ¯ Dataset Information å¤§çº¦ä¸€å…±2.66Mä¸ªä¸­æ–‡å›¾æ–‡å¯¹ã€‚å¤§çº¦å ç”¨381MBç©ºé—´ï¼ˆä»…ä»…æ˜¯urlç­‰æ–‡æœ¬ä¿¡æ¯ï¼Œä¸åŒ…å«å›¾ç‰‡ï¼‰ã€‚ Homepage: laion-5b Huggingface: laion/laion-high-resolution ä¸‹è½½ Download mkdir release && cd release for i in {00000..00015}; do wgetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wanng/laion-high-resolution-chinese."
Waterhorse/chess_data,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2306.09200,https://huggingface.co/datasets/Waterhorse/chess_data,dataset and mixed dataset for training ChessGPT-Base,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"The Chess Dataset Data-Summary: The dataset consists of three sources of dataset described in the paper, including: ChessCLIP dataset: Annotated PGNs for training CLIP. ChessGPT Base dataset: Game dataset, language dataset and mixed dataset for training ChessGPT-Base. ChessGPT Chat dataset: Conversational dataset for training ChessGPT-Chat. Because of the legal issue, for ChessGPT dataset, we do not open-source the chess-book, chess-forum, chess-blog, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Waterhorse/chess_data."
waybarrios/github-code-dataset,,,,,,,https://huggingface.co/datasets/waybarrios/github-code-dataset,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""github-code-dataset"" More Information needed"
wb14123/couplet,,,,,,,https://huggingface.co/datasets/wb14123/couplet,,,,,,,,https://choosealicense.com/licenses/agpl-3.0/,,,,,,,,,,,,å¯¹è”æ•°æ®é›†ã€‚æ•°æ®æ¥æºï¼š å†¯é‡æœ´_æ¢¨å‘³æ–‹æ•£å¶_çš„åšå®¢ã€‚ çˆ¬è™«ä»£ç åœ¨ Githubã€‚
wbbbbb/pclue,,,,,,,https://huggingface.co/datasets/wbbbbb/pclue,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,https://github.com/CLUEbenchmark/pCLUE
wbensvage/clothes_desc,,,,,,,https://huggingface.co/datasets/wbensvage/clothes_desc,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Dataset Card for H&M Clothes captions _Dataset used to train/finetune [Clothes text to image model] Captions are generated by using the 'detail_desc' and 'colour_group_name' or 'perceived_colour_master_name' from kaggle/competitions/h-and-m-personalized-fashion-recommendations. Original images were also obtained from the url (https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data?select=images) For each row the dataset contains imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wbensvage/clothes_desc.
wdc/products-2017,Text,,,,,http://webdatacommons.org/largescaleproductcorpus/v2/index.html,https://huggingface.co/datasets/wdc/products-2017,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"Many e-shops have started to mark-up product data within their HTML pages using the schema.org vocabulary. The Web Data Commons project regularly extracts such data from the Common Crawl, a large public web crawl. The Web Data Commons Training and Test Sets for Large-Scale Product Matching contain product offers from different e-shops in the form of binary product pairs (with corresponding label ""match"" or ""no match"") In order to support the evaluation of machine learning-based matching methods, the data is split into training, validation and test set. We provide training and validation sets in four different sizes for four product categories. The labels of the test sets were manually checked while those of the training sets were derived using shared product identifiers from the Web via weak supervision. The data stems from the WDC Product Data Corpus for Large-Scale Product Matching - Version 2.0 which consists of 26 million product offers originating from 79 thousand websites."
weaviate/WeaviateGraphQLGorilla,,,,,,,https://huggingface.co/datasets/weaviate/WeaviateGraphQLGorilla,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,weaviate/WeaviateGraphQLGorilla dataset hosted on Hugging Face and contributed by the HF Datasets community
webis/tldr-17,Text,,,,,https://webis.de/data/webis-tldr-17.html,https://huggingface.co/datasets/webis/tldr-17,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"This corpus contains preprocessed posts from the Reddit dataset. The dataset consists of 3,848,330 posts with an average length of 270 words for content, and 28 words for the summary. Features includes strings: author, body, normalizedBody, content, summary, subreddit, subreddit_id. Content is used as document and summary is used as summary."
webnlg-challenge/web_nlg,Text,,,,,https://webnlg-challenge.loria.fr/,https://huggingface.co/datasets/webnlg-challenge/web_nlg,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,,,,,,"The WebNLG challenge consists in mapping data to text. The training data consists of Data/Text pairs where the data is a set of triples extracted from DBpedia and the text is a verbalisation of these triples. For instance, given the 3 DBpedia triples shown in (a), the aim is to generate a text such as (b). a. (John_E_Blaha birthDate 1942_08_26) (John_E_Blaha birthPlace San_Antonio) (John_E_Blaha occupation Fighter_pilot) b. John E Blaha, born in San Antonio on 1942-08-26, worked as a fighter pilot As the"
weitianwen/cmath,,,,,,https://arxiv.org/abs/2306.16636,https://huggingface.co/datasets/weitianwen/cmath,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"CMATH Introduction We present the Chinese Elementary School Math Word Problems (CMATH) dataset, comprising 1.7k elementary school-level math word problems with detailed annotations, source from actual Chinese workbooks and exams. This dataset aims to provide a benchmark tool for assessing the following question: to what grade level of elementary school math do the abilities of popular large language models (LLMs) correspond? We evaluate a variety of popular LLMsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/weitianwen/cmath."
WelfCrozzo/kupalinka,,,,,,,https://huggingface.co/datasets/WelfCrozzo/kupalinka,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,WelfCrozzo/kupalinka dataset hosted on Hugging Face and contributed by the HF Datasets community
wellecks/naturalproofs-gen,,,,,,https://arxiv.org/abs/2205.12910,https://huggingface.co/datasets/wellecks/naturalproofs-gen,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Naturalproofs-gen This dataset contains the Naturalproofs-gen corpus from: NaturalProver: Grounded Mathematical Proof Generation with Language ModelsSean Welleck*, Jiacheng Liu*, Ximing Lu, Hannaneh Hajishirzi, Yejin ChoiNeurIPS 2022 Licensing Information MIT Citation Information Please cite: @inproceedings{welleck2022naturalprover, title={NaturalProver: Grounded Mathematical Proof Generation with Language Models}, author={Sean Welleck andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wellecks/naturalproofs-gen."
wellesley-easel/StudentEval,,,,,,https://arxiv.org/abs/2306.04556,https://huggingface.co/datasets/wellesley-easel/StudentEval,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,https://github.com/bigcode-project/bigcode-evaluation-harness/,,,,,,,,"StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code Introduction StudentEval is a dataset of 1,675 prompts for 48 problems, authored by 80 students who have only completed a one-semester Python programming class. To the best of our knowledge, it is the first dataset that has multiple prompts per problem and multiple attempts by the same participant. We identify four key disjoint subsets of StudentEval for each problem-participantâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wellesley-easel/StudentEval."
wendlerc/RenderedText,,,,,,,https://huggingface.co/datasets/wendlerc/RenderedText,,,,,,,,,,,,,,,,,,,,"This dataset has been created by Stability AI and LAION. This dataset contains 12 million 1024x1024 images of handwritten text written on a digital 3D sheet of paper generated using Blender geometry nodes and rendered using Blender Cycles. The text has varying font size, color, and rotation, and the paper was rendered under random lighting conditions. Note that, the first 10 million"
wenge-research/yayi_domain_subset,,,,,,,https://huggingface.co/datasets/wenge-research/yayi_domain_subset,,,,,,,,,,,,,,,,,,,,wenge-research/yayi_domain_subset dataset hosted on Hugging Face and contributed by the HF Datasets community
wikimedia/wikipedia,Text,General,UI,Text,"Accuracy, F1 Score",https://dumps.wikimedia.org,https://huggingface.co/datasets/wikimedia/wikipedia,s,,,,,,,https://choosealicense.com/licenses/cc-by-sa-3.0/,,,,,,,d articles of all languages,,"BERT, RoBERTa, T5",,,"Dataset Card for Wikimedia Wikipedia Data-Summary: Wikipedia dataset containing cleaned articles of all languages. The dataset is built from the Wikipedia dumps (https://dumps.wikimedia.org/) with one subset per language, each containing a single train split. Each"
winddude/reddit_finance_43_250k,,,,,,,https://huggingface.co/datasets/winddude/reddit_finance_43_250k,,,,,,,,https://choosealicense.com/licenses/gpl-3.0/,,,,,,,,,,,,"reddit finance 43 250k reddit_finance_43_250k is a collection of 250k post/comment pairs from 43 financial, investing and crypto subreddits. Post must have all been text, with a length of 250chars, and a positive score. Each subreddit is narrowed down to the 70th qunatile before being mergered with their top 3 comments and than the other subs. Further score based methods are used to select the top 250k post/comment pairs. The code to recreate the dataset is here:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/winddude/reddit_finance_43_250k."
winglian/visual-novels-json,,,,,,,https://huggingface.co/datasets/winglian/visual-novels-json,,,,,,,,,,,,,,,,,,,,winglian/visual-novels-json dataset hosted on Hugging Face and contributed by the HF Datasets community
WINGNUS/ACL-OCL,Text,,,,,,https://huggingface.co/datasets/WINGNUS/ACL-OCL,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Card for ACL Anthology Corpus This repository provides full-text and metadata to the ACL anthology collection (80k articles/posters as of September 2022) also including .pdf files and grobid extractions of the pdfs. How is this different from what ACL anthology provides and what already exists? We provide pdfs, full-text, references and other details extracted by grobid from the PDFs while ACL Anthology only provides abstracts. There exists a similarâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WINGNUS/ACL-OCL."
WinkingFace/CryptoLM-Bitcoin-BTC-USDT,,,,,,,https://huggingface.co/datasets/WinkingFace/CryptoLM-Bitcoin-BTC-USDT,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"BTC Price Dataset with Technical Indicators Welcome to the BTC / USDT Price Dataset with Technical Indicators, hosted by the WinkingFace Team. This dataset is designed to provide comprehensive historical data on Bitcoin prices along with a variety of technical indicators to aid in cryptocurrency trading analysis and research. The dataset is updated every 3 minutes (delayed 1 minute). Dataset Description This dataset includes the following columns: timestamp: The date andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WinkingFace/CryptoLM-Bitcoin-BTC-USDT."
winvoker/turkish-sentiment-analysis-dataset,,,,,,,https://huggingface.co/datasets/winvoker/turkish-sentiment-analysis-dataset,,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,,,,"Dataset This dataset contains positive , negative and notr sentences from several data sources given in the references. In the most sentiment models , there are only two labels; positive and negative. However , user input can be totally notr sentence. For such cases there were no data I could find. Therefore I created this dataset with 3 class. Positive and negative sentences are listed below. Notr"
WiroAI/dolphin-r1-italian,,,,,,,https://huggingface.co/datasets/WiroAI/dolphin-r1-italian,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,Dolphin R1 Italian ðŸ¬ Dolphin-R1 is an Apache-2.0 English dataset curated by Eric Hartford and Cognitive Computations Dolphin-R1-Italian is a Italian subset of the original dataset. Sponsors Their and Wiro AI's appreciation for the generous sponsors of Dolphin R1 - Without whom this dataset could not exist. Dria https://x.com/driaforall - Inference Sponsor (DeepSeek) Chutesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WiroAI/dolphin-r1-italian.
WizardLMTeam/WizardLM_evol_instruct_70k,,,,,,https://arxiv.org/abs/2308.09583,https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k,,,,,,,WizardMath,https://choosealicense.com/licenses/mit/,,,1,,,,,,,,,"This is the training data of WizardLM. News ðŸ”¥ ðŸ”¥ ðŸ”¥ [08/11/2023] We release WizardMath Models. ðŸ”¥ Our WizardMath-70B-V1.0 model slightly outperforms some closed-source LLMs on the GSM8K, including ChatGPT 3.5, Claude Instant 1 and PaLM 2 540B. ðŸ”¥ Our WizardMath-70B-V1.0 model achieves 81.6 pass@1 on the GSM8k Benchmarks, which is 24.8 points higher than the SOTA open-source LLM. ðŸ”¥ Our WizardMath-70B-V1.0 model achieves 22.7 pass@1 on the MATH Benchmarks, which is 9.2 pointsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k."
wmt/wmt18,Text,General,General,Text,"Accuracy, F1 Score",http://www.statmt.org/wmt18/translation-task.html,https://huggingface.co/datasets/wmt/wmt18,English,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""wmt18"" Data-Summary: Warning: There are issues with the Common Crawl corpus data (training-parallel-commoncrawl.tgz): Non-English files contain many English sentences. Their ""parallel"" sentences in English are not aligned: they are uncorrelated with their counterpart. We have contacted the WMT organizers, and in response, they have indicated that they do not have plans to update the Common Crawl corpus data. Their rationaleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wmt/wmt18."
Wongnai/wongnai_reviews,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Wongnai/wongnai_reviews,Thai,,,,,,,https://choosealicense.com/licenses/lgpl-3.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Wongnai_Reviews Data-Summary: The Wongnai Review dataset contains restaurant reviews and ratings, almost entirely in Thai language. The reviews are in 5 classes ranging from 1 to 5 stars. This dataset was featured in a Kaggle challenge https://www.kaggle.com/c/wongnai-challenge-review-rating-prediction/overview Languages Thai Dataset Structure Data Fields review_body - text of review star_rating - anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Wongnai/wongnai_reviews."
WorkInTheDark/FairytaleQA,Text,,,,,,https://huggingface.co/datasets/WorkInTheDark/FairytaleQA,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"FairytaleQA dataset, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade. The FairytaleQA dataset is annotated by education experts based on an evidence-based theoretical framework. It consists of 10,580 explicit and implicit questions derived from 278 children-friendly stories, covering seven types of narrative elements or relations."
WS12-11/laptop,,,,,,,https://huggingface.co/datasets/WS12-11/laptop,,,,,,,,,,,,,,,,,,,,WS12-11/laptop dataset hosted on Hugging Face and contributed by the HF Datasets community
wtcherr/unsplash_10k_canny,,,,,,,https://huggingface.co/datasets/wtcherr/unsplash_10k_canny,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""unsplash_10k_canny"" More Information needed"
wuliangfo/Chinese-Pixiv-Novel,,,,,,,https://huggingface.co/datasets/wuliangfo/Chinese-Pixiv-Novel,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,è¿™æ˜¯ä¸€ä¸ªR-18ï¼ˆå«R-18Gï¼‰ç®€ä½“ä¸­æ–‡å°è¯´æ•°æ®é›†ï¼Œæ¥è‡ªPixivç½‘ç«™ å…±æœ‰145163æœ¬ï¼Œæ•°æ®æˆªæ­¢åŒ—äº¬æ—¶é—´2023å¹´9æœˆ12æ—¥æ™š7ç‚¹ å­˜å‚¨æ ¼å¼ä¸ºPixiv/userID/ID.txtï¼Œæ•°æ®ä¸ºtxtæ­£æ–‡ï¼ŒPixiv/userID/ID-meta.txtï¼Œæ•°æ®ä¸ºé¢å¤–ä¿¡æ¯ï¼ˆåŒ…æ‹¬tagã€titleã€Descriptionç­‰ï¼‰ æ•°æ®æœªç»è¿‡æ¸…æ´—ï¼Œå¯èƒ½åŒ…å«ä½Žè´¨é‡å†…å®¹ã€‚
wushan/vehicle_qa,,,,,,,https://huggingface.co/datasets/wushan/vehicle_qa,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,wushan/vehicle_qa dataset hosted on Hugging Face and contributed by the HF Datasets community
WWKJ/Pre_Post_Treatment_Circle_of_Willis,,,,,,,https://huggingface.co/datasets/WWKJ/Pre_Post_Treatment_Circle_of_Willis,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,,,,WWKJ/Pre_Post_Treatment_Circle_of_Willis dataset hosted on Hugging Face and contributed by the HF Datasets community
wybxc/books,,,,,,,https://huggingface.co/datasets/wybxc/books,,,,,,,,https://choosealicense.com/licenses/odc-by/,,,,,,,,,,,,ä»Žå°è¯´ä»¥åŠå…¶ä»–æ¥æºæå–çš„å•/å¤šè½®å¯¹è¯è¯­æ–™ã€‚
wykonos/steam_games,,,,,,,https://huggingface.co/datasets/wykonos/steam_games,,,,,,,,,,,,,,,,,,,,wykonos/steam_games dataset hosted on Hugging Face and contributed by the HF Datasets community
wyzelabs/RuleRecommendation,Text,General,Scientific,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/wyzelabs/RuleRecommendation,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-nd-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Wyze Rule Recommendation Dataset Data-Summary: The Wyze Rule dataset is a new large-scale dataset designed specifically for smart home rule recommendation research. It contains over 1 million rules generated by 300,000 users from Wyze Labs, offering an extensive collection of real-world automation rules tailored to users' unique smart home setups. The goal of the Wyze Rule dataset is to advance research and development of personalized rule recommendationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wyzelabs/RuleRecommendation."
wzy816/scifi,,,,,,,https://huggingface.co/datasets/wzy816/scifi,,,,,,,,,,,,,,,,,,,,scifi ä¸­æ–‡ç§‘å¹»å°è¯´ï¼Œå·²æ¸…ç†æ— æ•ˆå­—ç¬¦ï¼Œå¹¶æŒ‰è¡ŒåŽ»é‡ã€‚ åŽŸå§‹è¯­æ–™æ¥è‡ª https://github.com/guhhhhaa/4675-scifi å’Œ https://github.com/guhhhhaa/wula-scifi ã€‚
wzzzq/MMLU-PRO-Leveled-TinyBench,,,,,,,https://huggingface.co/datasets/wzzzq/MMLU-PRO-Leveled-TinyBench,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,MMLU Pro éš¾åº¦åˆ†çº§å­é›† (MMLU Pro Difficulty Subset) ðŸ“Š æ•°æ®é›†ç®€ä»‹åŸºäºŽ MMLU Pro æž„å»ºçš„å­æ•°æ®é›†ï¼ŒåŒ…å« å¤šé¢†åŸŸå­¦æœ¯é—®é¢˜ åŠå…¶éš¾åº¦è¯„åˆ†ã€‚éš¾åº¦å€¼ç”±å¤šä¸ª LLM æ¨¡åž‹çš„å›žç­”å‡†ç¡®çŽ‡è®¡ç®—å¾—å‡ºï¼ˆèŒƒå›´ 0.0-1.0ï¼Œæ•°å€¼è¶Šå°è¡¨ç¤ºéš¾åº¦è¶Šé«˜ï¼‰ã€‚ â¬ é€‚ç”¨åœºæ™¯ï¼š LLM èƒ½åŠ›è¯„ä¼°ä¸Žå¯¹æ¯” éš¾åº¦æ•æ„Ÿåž‹æ¨¡åž‹è®­ç»ƒ çŸ¥è¯†ç›²ç‚¹åˆ†æž ðŸ—‚ï¸ æ•°æ®é›†ç»“æž„ â”œâ”€â”€ data_sets/ â”‚ â”œâ”€â”€ combined.json # å®Œæ•´æ•°æ®é›†ï¼ˆé»˜è®¤å±•ç¤ºï¼‰ â”‚ â”œâ”€â”€ extremely_hard_0.0_0.1.json # LLM å‡†ç¡®çŽ‡ 0-10% (æœ€éš¾) â”‚ â”œâ”€â”€ very_hard_0.1_0.2.json # LLM å‡†ç¡®çŽ‡ 10-20% â”‚ â””â”€â”€ ...ï¼ˆå…±10ä¸ªéš¾åº¦åˆ†çº§æ–‡ä»¶ï¼‰ â””â”€â”€ problem_ids/ # åŽŸå§‹ MMLU Pro é¢˜ç›® ID æ˜ å°„ ðŸ“ˆ éš¾åº¦åˆ†çº§æ ‡å‡†â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wzzzq/MMLU-PRO-Leveled-TinyBench.
x1101/nsfw,,,,,,,https://huggingface.co/datasets/x1101/nsfw,,,,,,,,,,,,,,,,,,,,x1101/nsfw dataset hosted on Hugging Face and contributed by the HF Datasets community
xcodemind/webcode2m,,,,,,https://arxiv.org/abs/2404.06369,https://huggingface.co/datasets/xcodemind/webcode2m,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"WebCode2M: A Real-World Dataset for Code Generation from Webpage Designs with Layouts (This dataset is also called Vision2UI.) Automatically generating webpage code from webpage designscan significantly reduce the workload of front-end developers, andrecent Multimodal Large Language Models (MLLMs) have shownpromising potential in this area. However, our investigation revealsthat most existing MLLMs are constrained by the absence of highquality, large-scale, real-world datasets, resulting inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xcodemind/webcode2m."
XIANG-Shuai/GWFSS-competition,,,,,,,https://huggingface.co/datasets/XIANG-Shuai/GWFSS-competition,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Introduction Competition Page If you want any update on the Global Wheat Dataset Community, go on https://www.global-wheat.com/ Wheat is a cornerstone of global food security, serving as a dietary staple for billions of people worldwide. Detailed analysis of wheat plants can help scientists and farmers cultivate healthier, more resilient, and more productive crops. The Global Wheat Full Semantic Segmentation (GWFSS) task aims to perform pixel-level segmentation of plant componentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/XIANG-Shuai/GWFSS-competition."
xiaobendanyn/tacred,,,,,,,https://huggingface.co/datasets/xiaobendanyn/tacred,,,,,,,,,,,,,,,,,,,,xiaobendanyn/tacred dataset hosted on Hugging Face and contributed by the HF Datasets community
xingjianleng/open-image-preferences-ft,,,,,,,https://huggingface.co/datasets/xingjianleng/open-image-preferences-ft,,,,,,,,,,,,,,,,,,,,xingjianleng/open-image-preferences-ft dataset hosted on Hugging Face and contributed by the HF Datasets community
xjf666/music,,,,,,,https://huggingface.co/datasets/xjf666/music,,,,,,,,,,,,,,,,,,,,xjf666/music dataset hosted on Hugging Face and contributed by the HF Datasets community
xlangai/spider,Text,General,UI,Text,"Accuracy, F1 Score",https://yale-lily.github.io/spider,https://huggingface.co/datasets/xlangai/spider,interfaces to cross-domain databases,,,,,,,https://choosealicense.com/licenses/cc-by-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,Dataset Card for Spider Data-Summary: Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases. Supported Tasks and Leaderboards The leaderboard can be seen at https://yale-lily.github.io/spider Languages The text in the dataset is in English. Datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xlangai/spider.
xmcmic/PMC-VQA,,,,,,,https://huggingface.co/datasets/xmcmic/PMC-VQA,,,,,,,,,,,,,,,,,,,,PMC-VQA Dataset PMC-VQA Dataset Daraset Structure Sample Dataset Structure PMC-VQA (version-1: 227k VQA pairs of 149k images). train.csv: metafile of train set test.csv: metafile of test set test_clean.csv: metafile of test clean set images.zip: images folder (update version-2: noncompound images). train2.csv: metafile of train set test2.csv: metafile of test set images2.zip: images folder Sample A row in train.csv is shown bellowâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmcmic/PMC-VQA.
xmj2002/genshin_ch_10npc,,,,,,,https://huggingface.co/datasets/xmj2002/genshin_ch_10npc,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset Card for ""genshin_ch_10npc"" More Information needed"
Xpitfire/cmp_facade,,,,,,,https://huggingface.co/datasets/Xpitfire/cmp_facade,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"CMP Facade Database We present a dataset of facade images assembled at the Center for Machine Perception, which includes 606 rectified images of facades from various sources, which have been manually annotated. The facades are from different cities around the world and diverse architectural styles. Documentation Data origin, format and processing, annotation principles for 12 classes are specified in the report. facade molding cornice pillar window door sill blind balcony shopâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Xpitfire/cmp_facade."
XuanwuAI/SecEval,,,,,,,https://huggingface.co/datasets/XuanwuAI/SecEval,,,,,,,,,,,,,,,,,,,,"SecEval: A Comprehensive Benchmark for Evaluating Cybersecurity Knowledge of Foundation Models The advent of large language models has ignited a transformative era for the cybersecurity industry. Pioneering applications are being developed, deployed, and utilized in areas such as cybersecurity knowledge QA, vulnerability hunting, and alert investigation. Various researches have indicated that LLMs primarily acquire their knowledge during the pretraining phase, with fine-tuningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/XuanwuAI/SecEval."
xuqinyang/BaiduBaike-5.63M,,,,,,,https://huggingface.co/datasets/xuqinyang/BaiduBaike-5.63M,,,,,,,,,,,,,,,,,,,,xuqinyang/BaiduBaike-5.63M dataset hosted on Hugging Face and contributed by the HF Datasets community
xusenlin/people-daily-ner,,,,,,,https://huggingface.co/datasets/xusenlin/people-daily-ner,,,,,,,,,,,,,,,,,,,,äººæ°‘æ—¥æŠ¥å‘½åå®žä½“è¯†åˆ«æ•°æ®é›† å­—æ®µè¯´æ˜Ž text: æ–‡æœ¬ entities: æ–‡æœ¬ä¸­åŒ…å«çš„å®žä½“ id: å®žä½“ id entity: å®žä½“å¯¹åº”çš„å­—ç¬¦ä¸² start_offset: å®žä½“å¼€å§‹ä½ç½® end_offset: å®žä½“ç»“æŸä½ç½®çš„ä¸‹ä¸€ä½ label: å®žä½“å¯¹åº”çš„å¼€å§‹ä½ç½®
xw27/scibench,,,,,,https://arxiv.org/abs/2307.10635,https://huggingface.co/datasets/xw27/scibench,,,,,,,,,,,,,,,,,,,,"SciBench SciBench is a novel benchmark for college-level scientific problems sourced from instructional textbooks. The benchmark is designed to evaluate the complex reasoning capabilities, strong domain knowledge, and advanced calculation skills of LLMs. Please refer to our paper or website for full description: SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models . Citation If you find our paper useful, please citeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xw27/scibench."
XXsongLALA/RAG-RL-Hotpotqa-with-2wiki,,,,,,,https://huggingface.co/datasets/XXsongLALA/RAG-RL-Hotpotqa-with-2wiki,,,,,,,,,,,,,,,,,,,,XXsongLALA/RAG-RL-Hotpotqa-with-2wiki dataset hosted on Hugging Face and contributed by the HF Datasets community
xzuyn/manythings-translations-alpaca,,,,,,,https://huggingface.co/datasets/xzuyn/manythings-translations-alpaca,,,,,,,,,,,,,,,,,,,,"Original Dataset 3,164,972 translations from English to 84 other languages. I've duplicated it to be to and from English, so it's now 6,329,944 translations."
y2lan/japan-law,,,,,,,https://huggingface.co/datasets/y2lan/japan-law,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Japanese Laws This dataset comprises 8.75K law records retrieved from the official Japanese government website e-Gov. Each entry furnishes comprehensive details about a particular law, encapsulating its number, title, unique ID, the date it came into effect, and its complete text. To ensure the dataset's uniqueness, deduplication was executed based on the most recent effective version as of August 1, 2023. A typical entry in this dataset is structured as follows: { ""num"":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/y2lan/japan-law."
yaak-ai/L2D,,,,,,,https://huggingface.co/datasets/yaak-ai/L2D,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"This dataset was created using LeRobot. Dataset Structure meta/info.json: { ""codebase_version"": ""v2.1"", ""robot_type"": ""KIA Niro EV 2023"", ""total_episodes"": 100, ""total_frames"": 28519, ""total_tasks"": 1, ""total_videos"": 700, ""total_chunks"": 1, ""chunks_size"": 1000, ""fps"": 10, ""splits"": { ""train"": ""0:100"" }, ""data_path"": ""data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet"", ""video_path"":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yaak-ai/L2D."
yaful/MAGE,,,,,,https://arxiv.org/abs/2305.13242,https://huggingface.co/datasets/yaful/MAGE,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"MAGE: Machine-generated Text Detection in the Wild ðŸš€ Introduction Recent advances in large language models have enabled them to reach a level of text generation comparable to that of humans. These models show powerful capabilities across a wide range of content, including news article writing, story generation, and scientific writing. Such capability further narrows the gap between human-authored and machine-generated texts, highlighting the importance of machine-generatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yaful/MAGE."
yahma/alpaca-cleaned,Text,,,,,,https://huggingface.co/datasets/yahma/alpaca-cleaned,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,"Dataset Card for Alpaca-Cleaned Repository: https://github.com/gururise/AlpacaDataCleaned Dataset Description This is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset: Hallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer. ""instruction"":""Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yahma/alpaca-cleaned."
Yale-LILY/aeslc,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/Yale-LILY/aeslc,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for ""aeslc"" Data-Summary: A collection of email messages of employees in the Enron Corporation. There are two features: email_body: email body text. subject_line: email subject text. Supported Tasks and Leaderboards More Information Needed Languages Monolingual English (mainly en-US) with some exceptions. Dataset Structure Data Instances default Size of downloaded datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Yale-LILY/aeslc."
yale-nlp/FOLIO,,,,,,,https://huggingface.co/datasets/yale-nlp/FOLIO,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,yale-nlp/FOLIO dataset hosted on Hugging Face and contributed by the HF Datasets community
yangwang825/audioset,,,,,,,https://huggingface.co/datasets/yangwang825/audioset,,,,,,,,,,,,,,,,,,,,"AudioSet AudioSet[1] consists of an expanding ontology of 527 audio event classes and a collection of 2M human-labelled 10-second sound clips drawn from YouTube. Some clips are missing on YouTube, so the number of files downloaded is different from time to time. This repository contains 20550 / 22160 of the balanced train set, 1913637 / 2041789 of the unbalanced train set (separated into 41 parts), and 18887 / 20371 of the evaluation set. The pre-process script can be found atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yangwang825/audioset."
yankihue/tweets-turkish,,,,,,,https://huggingface.co/datasets/yankihue/tweets-turkish,,,,,,,,,,,,,,,,,,,,yankihue/tweets-turkish dataset hosted on Hugging Face and contributed by the HF Datasets community
YaraMou/DS3,,,,,,,https://huggingface.co/datasets/YaraMou/DS3,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,YaraMou/DS3 dataset hosted on Hugging Face and contributed by the HF Datasets community
ydshieh/coco_dataset_script,,,,,,,https://huggingface.co/datasets/ydshieh/coco_dataset_script,,,,,,,,,,,,,,,,,,,,"COCO is a large-scale object detection, segmentation, and captioning dataset."
yeegnauh/bert_wikipedia,,,,,,,https://huggingface.co/datasets/yeegnauh/bert_wikipedia,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,yeegnauh/bert_wikipedia dataset hosted on Hugging Face and contributed by the HF Datasets community
Yehor/opentts-uk,,,,,,,https://huggingface.co/datasets/Yehor/opentts-uk,,,,,,,,,,,,,,,,,,,,Open Text-to-Speech voices for ðŸ‡ºðŸ‡¦ Ukrainian Community Discord: https://bit.ly/discord-uds Speech Recognition: https://t.me/speech_recognition_uk Speech Synthesis: https://t.me/speech_synthesis_uk License All licenses are listed in https://github.com/egorsmkv/ukrainian-tts-datasets Audio Aesthetics Check out https://huggingface.co/datasets/Yehor/opentts-uk-aesthetics dataset. Development uv venv --python 3.12 source .venv/bin/activateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Yehor/opentts-uk.
Yelp/yelp_review_full,Text,General,General,Text,"Accuracy, F1 Score",https://www.yelp.com/dataset,https://huggingface.co/datasets/Yelp/yelp_review_full,,,,,,,,https://choosealicense.com/licenses/other/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for YelpReviewFull Data-Summary: The Yelp reviews dataset consists of reviews from Yelp. It is extracted from the Yelp Dataset Challenge 2015 data. Supported Tasks and Leaderboards text-classification, sentiment-classification: The dataset is mainly used for text classification: given the text, predict the sentiment. Languages The reviews were mainly written in english. Dataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Yelp/yelp_review_full."
yentinglin/TaiwanChat,,,,,,https://arxiv.org/abs/2311.17487,https://huggingface.co/datasets/yentinglin/TaiwanChat,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"Performance Citation If you find Taiwan LLM is useful in your work, please cite it with: @misc{lin2023taiwan, title={Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model}, author={Yen-Ting Lin and Yun-Nung Chen}, year={2023}, eprint={2311.17487}, archivePrefix={arXiv}, primaryClass={cs.CL} }"
YeungNLP/firefly-train-1.1M,,,,,,,https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M,,,,,,,,,,,,,,,,,,,,"æœ¬æ•°æ®åº”ç”¨äºŽé¡¹ç›®ï¼šFireflyï¼ˆæµè¤ï¼‰: ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡åž‹ ï¼Œè®­ç»ƒåŽå¾—åˆ°çš„æ¨¡åž‹firefly-1b4 å¦‚æžœæ‚¨è§‰å¾—æ­¤æ•°æ®é›†å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·likeæ­¤æ•°æ®é›†å¹¶åœ¨Githubé¡¹ç›®ä¸­staræˆ‘ä»¬ã€‚ æˆ‘ä»¬æ”¶é›†äº†23ä¸ªå¸¸è§çš„ä¸­æ–‡æ•°æ®é›†ï¼Œå¯¹äºŽæ¯ä¸ªä»»åŠ¡ï¼Œç”±äººå·¥ä¹¦å†™è‹¥å¹²ç§æŒ‡ä»¤æ¨¡æ¿ï¼Œä¿è¯æ•°æ®çš„é«˜è´¨é‡ä¸Žä¸°å¯Œåº¦ï¼Œæ•°æ®é‡ä¸º115ä¸‡ ã€‚æ•°æ®åˆ†å¸ƒå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š æ¯æ¡æ•°æ®çš„æ ¼å¼å¦‚ä¸‹ï¼ŒåŒ…å«ä»»åŠ¡ç±»åž‹ã€è¾“å…¥ã€ç›®æ ‡è¾“å‡ºï¼š { ""kind"": ""ClassicalChinese"", ""input"": ""å°†ä¸‹é¢å¥å­ç¿»è¯‘æˆçŽ°ä»£æ–‡ï¼š\nçŸ³ä¸­å¤®åˆç”Ÿä¸€æ ‘ï¼Œé«˜ç™¾ä½™å°ºï¼Œæ¡å¹²åƒé˜´ä¸ºäº”è‰²ï¼Œç¿ å¶å¦‚ç›˜ï¼ŒèŠ±å¾„å°ºä½™ï¼Œè‰²æ·±ç¢§ï¼Œè•Šæ·±çº¢ï¼Œå¼‚é¦™æˆçƒŸï¼Œè‘—ç‰©éœéœã€‚"", ""target"": ""å¤§çŸ³çš„ä¸­å¤®é•¿ç€ä¸€æ£µæ ‘ï¼Œä¸€ç™¾å¤šå°ºé«˜ï¼Œæžå¹²æ˜¯å½©è‰²çš„ï¼Œæ ‘å¶æœ‰ç›˜å­é‚£æ ·å¤§ï¼ŒèŠ±çš„ç›´å¾„æœ‰ä¸€å°ºå®½ï¼ŒèŠ±ç“£æ·±è“è‰²ï¼ŒèŠ±ä¸­é£˜å‡ºå¥‡å¼‚çš„é¦™æ°”ç¬¼ç½©ç€å‘¨å›´ï¼Œå¦‚çƒŸä¼¼é›¾ã€‚"" } è®­ç»ƒæ•°æ®é›†çš„tokené•¿åº¦åˆ†å¸ƒå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç»å¤§éƒ¨åˆ†æ•°æ®çš„é•¿åº¦éƒ½å°äºŽ600ï¼š"
yhavinga/ccmatrix,Text,,,,,https://opus.nlpl.eu/CCMatrix.php,https://huggingface.co/datasets/yhavinga/ccmatrix,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,"CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB We show that margin-based bitext mining in LASER's multilingual sentence space can be applied to monolingual corpora of billions of sentences to produce high quality aligned translation data. We use thirty-two snapshots of a curated common crawl corpus [1] totaling 69 billion unique sentences. Using one unified approach for 80 languages, we were able to mine 10.8 billion parallel sentences, out of which only 2.9 billion are aligned with English. IMPORTANT: Please cite reference [2][3] if you use this data. [1] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco GuzmÃ¡n, Armand Jouli and Edouard Grave, CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data [2] Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave and Armand Joulin, CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB [3] Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. Beyond English-Centric Multilingual Machine Translation 90 languages, 1,197 bitexts total number of files: 90 total number of tokens: 112.14G total number of sentence fragments: 7.37G"
yizhongw/self_instruct,Text,,,,,,https://huggingface.co/datasets/yizhongw/self_instruct,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Self-Instruct is a dataset that contains 52k instructions, paired with 82K instance inputs and outputs. This instruction data can be used to conduct instruction-tuning for language models and make the language model follow instruction better."
ylacombe/expresso,,,,,,https://arxiv.org/abs/2308.05725,https://huggingface.co/datasets/ylacombe/expresso,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,https://github.com/facebookresearch/textlesslib/tree/main/examples/expresso/dataset,,,,,,,,"The Expresso Dataset [paper] [demo samples] [Original repository] Introduction The Expresso dataset is a high-quality (48kHz) expressive speech dataset that includes both expressively rendered read speech (8 styles, in mono wav format) and improvised dialogues (26 styles, in stereo wav format). The dataset includes 4 speakers (2 males, 2 females), and totals 40 hours (11h read, 30h improvised). The transcriptions of the read speech are also provided. You canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ylacombe/expresso."
ylecun/mnist,Image,General,General,Text,"Accuracy, F1 Score",http://yann.lecun.com/exdb/mnist/,https://huggingface.co/datasets/ylecun/mnist,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for MNIST Data-Summary: The MNIST dataset consists of 70,000 28x28 black-and-white images of handwritten digits extracted from two NIST databases. There are 60,000 images in the training dataset and 10,000 images in the validation dataset, one class per digit so a total of 10 classes, with 7,000 images (6,000 train images and 1,000 test images) per class. Half of the image were drawn by Census Bureau employees and the other half by high schoolâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ylecun/mnist."
Yorai/detect-waste,Text,Detection,General,Bounding Box/Mask,"mAP, IoU",,https://huggingface.co/datasets/Yorai/detect-waste,,,,,,,,,,,,,,,,,"DETR, Faster R-CNN, YOLO",,,Dataset Card for detect-waste Data-Summary: AI4Good project for detecting waste in environment. www.detectwaste.ml. Our latest results were published in Waste Management journal in article titled Deep learning-based waste detection in natural and urban environments. You can find more technical details in our technical report Waste detection in Pomerania: non-profit project for detecting waste in environment. Did you know that we produce 300 million tons ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Yorai/detect-waste.
Yotam/economics-textbook,,,,,,,https://huggingface.co/datasets/Yotam/economics-textbook,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,Yotam/economics-textbook dataset hosted on Hugging Face and contributed by the HF Datasets community
Yova/templama,,,,,,,https://huggingface.co/datasets/Yova/templama,,,,,,,,,,,,,,,,,,,,Yova/templama dataset hosted on Hugging Face and contributed by the HF Datasets community
ysharma/rickandmorty,,,,,,,https://huggingface.co/datasets/ysharma/rickandmorty,,,,,,,,,,,,,,,,,,,,"This dataset contains scripts for all episodes of Rick and Morty season 1,2, and 3. Columns : index, season no., episode no., episode name, (character) name, line (dialogue)"
YuanPJ/summ_screen,,,,,,,https://huggingface.co/datasets/YuanPJ/summ_screen,,,,,,,,,,,,,,,,,,,,SummScreen Corpus contains over 26k pairs of TV series transcripts and human written recaps. There are two features: - dialogue: text of dialogue. - summary: human written summary of the dialogue. - id: id of a
yueliu1999/GuardReasonerTrain,,,,,,https://arxiv.org/abs/2501.18492,https://huggingface.co/datasets/yueliu1999/GuardReasonerTrain,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"GuardReasonerTrain GuardReasonerTrain is the training data for R-SFT of GuardReasoner, as described in the paper GuardReasoner: Towards Reasoning-based LLM Safeguards. Code: https://github.com/yueliu1999/GuardReasoner/ Usage from datasets import load_dataset # Login using e.g. `huggingface-cli login` to access this dataset ds = load_dataset(""yueliu1999/GuardReasonerTrain"") Citation If you use this dataset, please cite our paper. @article{GuardReasonerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yueliu1999/GuardReasonerTrain."
YulangZhuo/MC-EIU,,,,,,https://arxiv.org/abs/2407.02751,https://huggingface.co/datasets/YulangZhuo/MC-EIU,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"MC-EIU Introduction This is the official repository for the MC-EIU dataset. More details can be found in the paper: Emotion and Intent Joint Understanding in Multimodal Conversation: A Benchmarking Dataset. Rui Liu *, Haolin Zuo, Zheng Lian, Xiaofen Xing, BjÃ¶rn W. Schuller, Haizhou Li MC-EIU Overview Statistic of our MC-EIU. UL refers to the utterance length, DU denotes the duration per utterance, UC is the utterances per conversation, EC means theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YulangZhuo/MC-EIU."
yuncongli/chat-sentiment-analysis,,,,,,,https://huggingface.co/datasets/yuncongli/chat-sentiment-analysis,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"A Sentiment Analsysis Dataset for Finetuning Large Models in Chat-style More details can be found at https://github.com/l294265421/chat-sentiment-analysis Supported Tasks Aspect Term Extraction (ATE) Opinion Term Extraction (OTE) Aspect Term-Opinion Term Pair Extraction (AOPE) Aspect term, Sentiment, Opinion term Triplet Extraction (ASOTE) Aspect Category Detection (ACD) Aspect Category-Sentiment Pair Extraction (ACSA) Aspect-Category-Opinion-Sentiment (ACOS)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yuncongli/chat-sentiment-analysis."
yurakuratov/example_promoters_300,,,,,,,https://huggingface.co/datasets/yurakuratov/example_promoters_300,,,,,,,,,,,,,,,,,,,,yurakuratov/
Yusuf5/OpenCaselist,,,,,,,https://huggingface.co/datasets/Yusuf5/OpenCaselist,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Card for OpenCaselist A collection of Evidence used in Collegiate and High School debate competitions. Dataset Details Dataset Description This dataset is a follow up to DebateSum, increasing its scope and amount of metadata collected. It expands the dataset to include evidence used during debate tournaments, rather than just evidence produced during preseason debate ""camps."" The total amount of evidence is approximately 20x larger thanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Yusuf5/OpenCaselist."
yuvalkirstain/pexel,,,,,,,https://huggingface.co/datasets/yuvalkirstain/pexel,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""pexel"" More Information needed"
yuweiyin/FinBench,,,,,,,https://huggingface.co/datasets/yuweiyin/FinBench,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,FinBench Dataset
yuyuc/chem-llama-instruct,,,,,,,https://huggingface.co/datasets/yuyuc/chem-llama-instruct,,,,,,,,https://choosealicense.com/licenses/openrail/,,,,,,,,,,,,This datastes is for llama-based chemistry condition generation model
ywpl/tagchajian,,,,,,,https://huggingface.co/datasets/ywpl/tagchajian,,,,,,,,,,,,,,,,,,,,ywpl/tagchajian dataset hosted on Hugging Face and contributed by the HF Datasets community
yys/OpenOrca-Chinese,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2301.13688,https://huggingface.co/datasets/yys/OpenOrca-Chinese,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"ðŸ‹ OpenOrca-Chinese æ•°æ®é›†ï¼ðŸ‹ æ„Ÿè°¢ Open-Orca/OpenOrca æ•°æ®é›†çš„å‘å¸ƒï¼Œç»™å¹¿å¤§NLPç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¸¦æ¥äº†å®è´µçš„èµ„æºï¼ è¿™æ˜¯ä¸€ä¸ªå¯¹ Open-Orca/OpenOrca æ•°æ®é›†ä¸­æ–‡ç¿»è¯‘çš„ç‰ˆæœ¬ï¼Œç¿»è¯‘å¼•æ“Žä¸º Google ç¿»è¯‘ï¼Œå¸Œæœ›èƒ½ç»™ä¸­æ–‡ LLM ç ”ç©¶åšå‡ºä¸€ç‚¹ç‚¹è´¡çŒ®ã€‚ Data-Summary: The OpenOrca dataset is a collection of augmented FLAN Collection data. Currently ~1M GPT-4 completions, and ~3.2M GPT-3.5 completions. It is tabularized in alignment with the distributions presented in the ORCA paper and currently represents a partial completion of the full intended dataset, withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yys/OpenOrca-Chinese."
z-uo/male-LJSpeech-italian,,,,,,,https://huggingface.co/datasets/z-uo/male-LJSpeech-italian,,,,,,,,,,,,,,,,,,,,"Italian Male Voice This dataset is an Italian version of LJSpeech, that merge all male audio of the same speaker finded into M-AILABS Speech Dataset. This dataset contains 31h 45m of one speacker recorded at 16000Hz. This is a valid choiche to train an italian TTS deep model with male voice."
Zahra99/IEMOCAP_Audio,,,,,,,https://huggingface.co/datasets/Zahra99/IEMOCAP_Audio,,,,,,,,,,,,,,,,,,,,"Dataset Card for ""IEMOCAP_Audio"" More Information needed"
ZahrizhalAli/mental_health_conversational_dataset,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/ZahrizhalAli/mental_health_conversational_dataset,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"CREDIT: Dataset Card for ""heliosbrahma/mental_health_chatbot_dataset"" Dataset Description Data-Summary: This dataset contains conversational pair of questions and answers in a single text related to Mental Health. Dataset was curated from popular healthcare blogs like WebMD, Mayo Clinic and HeatlhLine, online FAQs etc. All questions and answers have been anonymized to remove any PII data and pre-processed to remove any unwanted characters.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZahrizhalAli/mental_health_conversational_dataset."
zake7749/chinese-speech-corpus,,,,,,,https://huggingface.co/datasets/zake7749/chinese-speech-corpus,,,,,,,,https://choosealicense.com/licenses/cc/,,,,,,,,,,,,"Chinese Speech Corpus This dataset has been sourced from SayIt, a specialized website focused on preserving transcripts and meeting notes. Presently, it encompasses a compilation of 1739 dialogues, encompassing approximately 340,000 sentences along with their respective speakers. License CC0 License"
zalando-datasets/fashion_mnist,Image,General,General,Text,"Accuracy, F1 Score",https://github.com/zalandoresearch/fashion-mnist,https://huggingface.co/datasets/zalando-datasets/fashion_mnist,,,60000,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for FashionMNIST Data-Summary: Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000"
zed-industries/zeta,,,,,,,https://huggingface.co/datasets/zed-industries/zeta,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"Dataset for Zeta This is the open dataset used to train Zeta, an edit prediction model that powers Zed's predictive coding feature. Zeta is derived from Qwen2.5-Coder-7B and predicts the developer's next code edit based on their recent programming patterns and cursor position, allowing for intelligent completion with a simple tab press. This dataset is split into three parts: train.jsonl: Contains the training data for supervised fine-tuning. dpo.jsonl: Contains the data for theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zed-industries/zeta."
ZenMoore/RoleBench,,,,,,https://arxiv.org/abs/2310.00746,https://huggingface.co/datasets/ZenMoore/RoleBench,,,,,,,,https://choosealicense.com/licenses/apache-2.0/,,,,,,,,,,,,"RoleBench Paper Title: RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models arXiv Link: https://arxiv.org/abs/2310.00746 Github Repo: https://github.com/InteractiveNLP-Team/RoleLLM-public Please read our paper for more details about this dataset. TL;DR: We introduce RoleLLM, a role-playing framework of data construction and evaluation (RoleBench), as well as solutions for both closed-source and open-source models (RoleGPT, RoleLLaMAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZenMoore/RoleBench."
zero1zero/erowid,,,,,,,https://huggingface.co/datasets/zero1zero/erowid,,,,,,,,,,,,,,,,,,,,zero1zero/erowid dataset hosted on Hugging Face and contributed by the HF Datasets community
zeronix1020/Strawberry-Disease,,,,,,,https://huggingface.co/datasets/zeronix1020/Strawberry-Disease,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,zeronix1020/Strawberry-Disease dataset hosted on Hugging Face and contributed by the HF Datasets community
zeroshot/twitter-financial-news-topic,,,,,,,https://huggingface.co/datasets/zeroshot/twitter-financial-news-topic,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Dataset Description The Twitter Financial News dataset is an English-language dataset containing an annotated corpus of finance-related tweets. This dataset is used to classify finance-related tweets for their topic. The dataset holds 21,107 documents annotated with 20 labels: topics = { ""LABEL_0"": ""Analyst Update"", ""LABEL_1"": ""Fed | Central Banks"", ""LABEL_2"": ""Company | Product News"", ""LABEL_3"": ""Treasuries | Corporate Debt"", ""LABEL_4"": ""Dividend""â€¦ See the full description on the dataset page: https://huggingface.co/datasets/zeroshot/twitter-financial-news-topic."
zetavg/coct-en-zh-tw-translations-twp-300k,,,,,,,https://huggingface.co/datasets/zetavg/coct-en-zh-tw-translations-twp-300k,,,,,,,,,,,,,,,,,,,,"~300K English â†” Traditional Chinese Sentences from the COCT Database The data in this dataset are collected from the Corpus of Contemporary Taiwanese Mandarin (COCT), mostly contributed by the Taiwan Panorama magazine."
zeusfsx/ukrainian-news,,,,,,,https://huggingface.co/datasets/zeusfsx/ukrainian-news,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,Ukrainian News Dataset This is a dataset of news articles downloaded from various Ukrainian websites and Telegram channels. The dataset contains approximately ~23M JSON objects (news)
zh-plus/tiny-imagenet,Image,General,General,Text,"Accuracy, F1 Score",https://www.kaggle.com/c/tiny-imagenet,https://huggingface.co/datasets/zh-plus/tiny-imagenet,,,,,,,,https://choosealicense.com/licenses/undefined/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for tiny-imagenet Data-Summary: Tiny ImageNet contains 100000 images of 200 classes (500 for each class) downsized to 64Ã—64 colored images. Each class has 500 training images, 50 validation images, and 50 test images. Languages The class labels in the dataset are in English. Dataset Structure Data Instances { 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64 at 0x1A800E8E190â€¦ See the full description on the dataset page: https://huggingface.co/datasets/zh-plus/tiny-imagenet."
zhang0jhon/Aesthetic-4K,,,,,,https://arxiv.org/abs/2503.18352,https://huggingface.co/datasets/zhang0jhon/Aesthetic-4K,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"Aesthetic-4K Dataset We introduce Aesthetic-4K, a high-quality dataset for ultra-high-resolution image generation, featuring carefully selected images and captions generated by GPT-4o. Additionally, we have meticulously filtered out low-quality images through manual inspection, excluding those with motion blur, focus issues, or mismatched text prompts. For more details, please refer to our paper: Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models (CVPRâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zhang0jhon/Aesthetic-4K."
zhanghanchong/css,Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2305.15891,https://huggingface.co/datasets/zhanghanchong/css,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,Data-Summary: CSS is a large-scale cross-schema Chinese text-to-SQL dataset Dataset Splits
zhangqiaobit/chinese_poetrys,,,,,,,https://huggingface.co/datasets/zhangqiaobit/chinese_poetrys,,,,,,,,,,,,,,,,,,,,ä¸­å›½å¤å…¸è¯—æ­Œ
zhangzhiyang/DIMT2025.ICDAR.Track_1,,,,,,,https://huggingface.co/datasets/zhangzhiyang/DIMT2025.ICDAR.Track_1,,,,,,,,,,,,,,,,,,,,"If you would like to paticipate in the DIMT2025@ICDAR challenge, please download the End User License Agreement, fill it out and send it to dimt2025.contact@gmail.com to access the data. We will review your application and get in touch as soon as possible. For more information, please refer to our official challenge website."
ZhankuiHe/reddit_movie_raw,Text,General,Scientific,Text,"Accuracy, F1 Score",https://github.com/AaronHeee/LLMs-as-Zero-Shot-Conversational-RecSys,https://huggingface.co/datasets/ZhankuiHe/reddit_movie_raw,,,,,,,,,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for Reddit-Movie-raw Data-Summary: This dataset provides the raw text from Reddit related to movie recommendation conversations. The dataset is extracted from the data dump of pushshift.io and only for research use. Disclaimer âš ï¸ Please note that conversations processed from Reddit raw data may include content that is not entirely conducive to a positive experience (e.g., toxic speech). Exercise caution and discretion when utilizingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZhankuiHe/reddit_movie_raw."
zhengyun21/PMC-Patients,Text,General,General,Text,"Accuracy, F1 Score",https://github.com/pmc-patients/pmc-patients,https://huggingface.co/datasets/zhengyun21/PMC-Patients,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for PMC-Patients News We released PMC-Patients-V2 (in JSON format with the same keys), which is based on 2024 PMC baseline and contains 250,294 patients. The data collection pipeline remains the same except for using more PMC articles. Data-Summary: PMC-Patients is a first-of-its-kind dataset consisting of 167k patient summaries extracted from case reports in PubMed Central (PMC), 3.1M patient-article relevance and 293kâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zhengyun21/PMC-Patients."
zhoubolei/scene_parse_150,Text,,,,,http://sceneparsing.csail.mit.edu/,https://huggingface.co/datasets/zhoubolei/scene_parse_150,,,,,,,,https://choosealicense.com/licenses/bsd-3-clause/,,,,,,,,,,,,"Scene parsing is to segment and parse an image into different image regions associated with semantic categories, such as sky, road, person, and bed. MIT Scene Parsing Benchmark (SceneParse150) provides a standard training and evaluation platform for the algorithms of scene parsing. The data for this benchmark comes from ADE20K Dataset which contains more than 20K scene-centric images exhaustively annotated with objects and object parts. Specifically, the benchmark is divided into 20K images for training, 2K images for validation, and another batch of held-out images for testing. There are totally 150 semantic categories included for evaluation, which include stuffs like sky, road, grass, and discrete objects like person, car, bed. Note that there are non-uniform distribution of objects occuring in the images, mimicking a more natural object occurrence in daily scene."
zhwang/HPDv2,,,,,,https://arxiv.org/abs/2306.09341,https://huggingface.co/datasets/zhwang/HPDv2,,,,,,,,,,,,,,,,,,,,"Human Preference Dataset v2 (HPD v2) Human Preference Dataset v2 (HPD v2) is a large-scale, cleanly-annotated dataset of human preferences for images generated from text prompts. For more detailed information, please refer to the paper: Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis. We also trained Human Preference Score v2 (HPSv2), a preference prediction model, on HPD v2. Updates [07/29/2023] Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zhwang/HPDv2."
Zicara/Hands_11k,,,,,,,https://huggingface.co/datasets/Zicara/Hands_11k,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,Zicara/Hands_11k dataset hosted on Hugging Face and contributed by the HF Datasets community
ZihanWangKi/conllpp,Text,,,,,https://github.com/ZihanWangKi/CrossWeigh,https://huggingface.co/datasets/ZihanWangKi/conllpp,,,,,,,,https://choosealicense.com/licenses/unknown/,,,,,,,,,,,,CoNLLpp is a corrected version of the CoNLL2003 NER dataset where labels of 5.38% of the sentences in the test set have been manually corrected. The training set and development set are included for completeness. For more details see https://www.aclweb.org/anthology/D19-1519/ and https://github.com/ZihanWangKi/CrossWeigh
Zilun/RS5M,,,,,,,https://huggingface.co/datasets/Zilun/RS5M,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"RS5M File Explaination 1. pub11_NER_geolocation_info.csv This file provides extracted geolocation entities in caption. we discovered that the captions from the PUB11 dataset contain a significant amount of location information. As a result, we executed a NER (Named Entity Recognition) extraction on the PUB11 subset. We hypothesize that the location information in the captions is closely related to the image's content and its shooting location.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Zilun/RS5M."
zirui3/ManySStuBs4J-instructions-v0,,,,,,,https://huggingface.co/datasets/zirui3/ManySStuBs4J-instructions-v0,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,instruction dataset for code bugfix TODO: Add commit message as prompt or summary of the bug Add source code repos & file & commit_sha
ziwenyd/transcoder-geeksforgeeks,,,,,,,https://huggingface.co/datasets/ziwenyd/transcoder-geeksforgeeks,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,statistics cpp-java: 627 pairs python-java: 616 pairs cpp-python: 545 pairs
zjunlp/Mol-Instructions,,,,,,https://arxiv.org/abs/2306.08018,https://huggingface.co/datasets/zjunlp/Mol-Instructions,,,,,,,,https://choosealicense.com/licenses/cc-by-4.0/,,,,,,,,,,,,Mol-Instructions datasets.
zlatinb/jfk-2025-cleaned,,,,,,,https://huggingface.co/datasets/zlatinb/jfk-2025-cleaned,,,,,,,,https://choosealicense.com/licenses/cc0-1.0/,,,,,,,,,,,,"On March 18th 2025, 2182 previously classified documents related to the investigation the assassination of President John F. Kennedy were made public at https://www.archives.gov/research/jfk/release-2025 This dataset contains a ""cleaned"" version of the zlatinb/jfk-2025-raw dataset. I've used cleanup.py to perform a best-effort cleanup based on some heuristics. The columns in the dataset are: File - name of the original document that you can download from the government website Accepted - theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zlatinb/jfk-2025-cleaned."
zoheb/sketch-scene,,,,,,,https://huggingface.co/datasets/zoheb/sketch-scene,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-sa-4.0/,,,,,,,,,,,,"Dataset Card for Sketch Scene Descriptions Dataset used to train Sketch Scene text to image model We advance sketch research to scenes with the first dataset of freehand scene sketches, FS-COCO. With practical applications in mind, we collect sketches that convey well scene content but can be sketched within a few minutes by a person with any sketching skills. Our dataset comprises around 10,000 freehand scene vector sketches with per-point space-time information by 100â€¦ See the full description on the dataset page: https://huggingface.co/datasets/zoheb/sketch-scene."
zpn/clintox,Text,General,General,Text,"Accuracy, F1 Score",,https://huggingface.co/datasets/zpn/clintox,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,"BERT, RoBERTa, T5",,,"Dataset Card for clintox Data-Summary: clintox is a dataset included in MoleculeNet. Qualitative data of drugs approved by the FDA and those that have failed clinical trials for toxicity reasons. This uses the CT_TOX task. Note, there was one molecule in the training set that could not be converted to SELFIES (*C(=O)[C@H](CCCCNC(=O)OCCOC)NC(=O)OCCOC) Dataset Structure Data Fields Each split contains smiles: the SMILESâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zpn/clintox."
ZurichNLP/x_stance,Text,,,,,,https://huggingface.co/datasets/ZurichNLP/x_stance,,,,,,,,https://choosealicense.com/licenses/cc-by-nc-4.0/,,,,,,,,,,,,"The x-stance dataset contains more than 150 political questions, and 67k comments written by candidates on those questions. It can be used to train and evaluate stance detection systems."
zwq2018/embodied_reasoner,,,,,,https://arxiv.org/abs/2501.00958,https://huggingface.co/datasets/zwq2018/embodied_reasoner,,,,,,,,,,,,,,,,,,,,"Embodied-Reasoner Dataset Dataset Overview Embodied-Reasoner is a multimodal reasoning dataset designed for embodied interactive tasks. It contains 9,390 Observation-Thought-Action trajectories for training and evaluating multimodal models capable of performing complex embodied tasks in indoor environments. Key Features ðŸ“¸ Rich Visual Data: Contains 64,000 first-person perspective interaction imagesðŸ¤” Deep Reasoning Capabilities: 8 million thoughtâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zwq2018/embodied_reasoner."
zxbsmk/webnovel_cn,,,,,,,https://huggingface.co/datasets/zxbsmk/webnovel_cn,,,,,,,,https://choosealicense.com/licenses/mit/,,,,,,,,,,,,"å†…å®¹ åŒ…å«ä»Ž12560æœ¬ç½‘æ–‡æå–çš„çº¦21.7Mæ¡å¯ç”¨äºŽè®­ç»ƒå°è¯´ç”Ÿæˆçš„ä¸­æ–‡æŒ‡ä»¤æ•°æ®(novel_json_tokens512.zip)ã€‚ä¸‹è½½é“¾æŽ¥ï¼šhttps://pan.baidu.com/s/1TorBMbrqxrn6odRF0PJBVw æå–ç ï¼šjlh3 ä»¥åŠä»Žä¸­æå–å‡ºçš„åŒ…å«50kæ¡æ•°æ®çš„å­é›†(novel_cn_token512_50k.json)ã€‚å…¶ä¸­è¾“å…¥å’Œè¾“å‡ºéƒ½ä¸å¤šäºŽ 512 tokensã€‚ æ ·ä¾‹ åœ¨åŽŸæœ‰å°è¯´æ–‡æœ¬åŸºç¡€ä¸Šï¼Œä¾æ®ä¸‹åˆ—äº”ç§æŒ‡ä»¤ç”Ÿæˆæ•°æ®ã€‚ å…¶ä¸­ï¼Œæ–‡æœ¬ç”±å°è¯´ä¸­éšæœºæŠ½å–çš„è¿žç»­å¥å­ç»„æˆã€‚ ç»™å®šæ ‡é¢˜ï¼Œç›´æŽ¥ç”Ÿæˆç®€ä»‹ã€‚ ç»™å®šæ ‡é¢˜å’Œç®€ä»‹ï¼Œç”Ÿæˆå¼€å¤´ã€‚ ç»™å®šç®€ä»‹å’Œä¸€æ®µæ–‡æœ¬ï¼Œç”ŸæˆåŽç»­æ–‡æœ¬ã€‚ ç»™å®šæ ‡é¢˜å’Œä¸€æ®µæ–‡æœ¬ï¼Œç”ŸæˆåŽç»­æ–‡æœ¬ã€‚ ç»™å®šä¸€æ®µæ–‡æœ¬ï¼Œç”ŸæˆåŽç»­æ–‡æœ¬ã€‚ { ""instruction"":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/zxbsmk/webnovel_cn."
zz990906/garbage_detection,,,,,,,https://huggingface.co/datasets/zz990906/garbage_detection,,,,,,,,,,,,,,,,,,,,zz990906/garbage_detection dataset hosted on Hugging Face and contributed by the HF Datasets community
zzliang/GRIT,Image-Text,General,General,Text,"Accuracy, F1 Score",https://arxiv.org/abs/2306.14824,https://huggingface.co/datasets/zzliang/GRIT,,,,,,,,https://choosealicense.com/licenses/ms-pl/,,,,,,,,,"BERT, RoBERTa, T5",,,"GRIT: Large-Scale Training Corpus of Grounded Image-Text Pairs Data-Summary: We introduce GRIT, a large-scale dataset of Grounded Image-Text pairs, which is created based on image-text pairs from COYO-700M and LAION-2B. We construct a pipeline to extract and link text spans (i.e., noun phrases, and referring expressions) in the caption to their corresponding image regions. More details can be found in the paper. Supported Tasks During theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zzliang/GRIT."
