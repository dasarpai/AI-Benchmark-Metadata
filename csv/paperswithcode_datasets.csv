Sno,dataset_id,subtask,task,associated_tasks,modalities,languages,area,benchmark_urls,license,homepage_url,pwc_url,description,year_published,paper_title,paper_url,dataset_size,dataset_splits,num_classes
1,100STLYE-Labelled,Dependency Parsing,Dependency Parsing,Dependency Parsing,Text,English,Natural Language Processing,dependency-parsing-on-100stlye-labelled,Creative Commons Attribution 4.0 International,https://zenodo.org/record/8127870,https://paperswithcode.com/dataset/100stlye-labelled,"Over 4 million frames of motion capture data for 100 different styles of locomotion. Can be used for animation, human motion and sequence modelling research.

This version of the dataset includes the features extracted from the raw motion capture data. This includes local phases, foot contacts, joint positions, joint rotations, joint velocities, character trajectory etc.",,,,,,
2,105_941_Images_Natural_Scenes_OCR_Data_of_12_Langu,Optical Character Recognition (OCR),Optical Character Recognition (OCR),Optical Character Recognition (OCR),Image,,Computer Vision,,Commercial license,https://bit.ly/3xF8rGT,https://paperswithcode.com/dataset/105941-images-natural-scenes-ocr-data-of-12,"Description:
105,941 Images Natural Scenes OCR Data of 12 Languages. The data covers 12 languages (6 Asian languages, 6 European languages), multiple natural scenes, multiple photographic angles. For annotation, line-level quadrilateral bounding box annotation and transcription for the texts were annotated in the data. The data can be used for tasks such as OCR of multi-language.

Data size:
105,941 images, including Asian language family: Japanese 9,997 images, Korean 10,231 images, Indonesian 7,591 images, Malay 5,650 images, Vietnamese 8,822 images, Thai 9,645 images; European language family: French 10,015 images, German 7,213 images, Italian 8,824 images, Portuguese 7,754 images, Russian 10,376 images and Spanish 9,823 images

Collecting environment:
including shop plaque, stop board, poster, ticket, road sign, comic, cover picture, prompt/reminder, warning, packing instruction, menu, building sign, etc.",,,,941 images,,
3,10_000_People_-_Human_Pose_Recognition_Data,Object Detection,Object Detection,"Object Detection, Contrastive Learning, Pose Tracking, Pose Estimation","3D, Image, Video",,Computer Vision,"object-detection-on-10000-people-human-pose, contrastive-learning-on-10000-people-human",Commercial license,https://bit.ly/3QvpvYz,https://paperswithcode.com/dataset/10000-people-human-pose-recognition-data,"Description:
10,000 People - Human Pose Recognition Data. This dataset includes indoor and outdoor scenes.This dataset covers males and females. Age distribution ranges from teenager to the elderly, the middle-aged and young people are the majorities. The data diversity includes different shooting heights, different ages, different light conditions, different collecting environment, clothes in different seasons, multiple human poses. For each subject, the labels of gender, race, age, collecting environment and clothes were annotated. The data can be used for human pose recognition and other tasks.

Data size:
10,000 people

Race distribution:
Asian (Chinese)",,,,,,
4,10_Synthetic_Genomics_Datasets,Sequential Pattern Mining,Sequential Pattern Mining,Sequential Pattern Mining,,,Methodology,sequential-pattern-mining-on-10-synthetic,MIT,https://zenodo.org/records/10683211,https://paperswithcode.com/dataset/10-synthetic-genomics-datasets,"These are 10 synthetic genomics datasets generated with NEAT v3 (based on TP53 gene of Homo Sapiens) for the use case of benchmarking somatic variant callers. To find more about our generating framework please visit synth4bench GitHub repository.

The datasets explore intrinsic NGS data parameters for the use case of observing their effect on tumor-only somatic variant calling algorithms. From the 10 datasets, there are 5 of them with different coverage (while keeping all other parameters fixed) and 5 with varying read length. The reads in all datasets are paired-end .",,,,,,
5,20000_utterances,Text-To-Speech Synthesis,Text-To-Speech Synthesis,Text-To-Speech Synthesis,"Audio, Text",English,Speech,text-to-speech-synthesis-on-20000-utterances,Free,Http://Bitext.com,https://paperswithcode.com/dataset/20000-utterances,20000 utterances,,,,,,
6,2010_i2b2_VA,FG-1-PG-1,FG-1-PG-1,"FG-1-PG-1, Relation Extraction, Medical Named Entity Recognition, Clinical Assertion Status Detection, Clinical Concept Extraction","Graph, Image, Text",English,Computer Vision,"fg-1-pg-1-on-2010-i2b2-va, clinical-concept-extraction-on-2010-i2b2va, clinical-assertion-status-detection-on-2010, relation-extraction-on-2010-i2b2-va-1",,https://www.i2b2.org/NLP/Relations/,https://paperswithcode.com/dataset/2010-i2b2-va,2010 i2b2/VA is a biomedical dataset for relation classification and entity typing.,2010,,,,,
7,2012_i2b2_Temporal_Relations,Joint Event and Temporal Relation Extraction,Joint Event and Temporal Relation Extraction,"Joint Event and Temporal Relation Extraction, Joint Entity and Relation Extraction, Temporal Relation Extraction, Relation Extraction, Temporal Relation Classification","Graph, Image, Time Series, Video",,Computer Vision,"joint-entity-and-relation-extraction-on-2012, relation-extraction-on-2012-i2b2-temporal",Under Data Use And Confidentiality Agreement,https://www.i2b2.org/NLP/TemporalRelations/,https://paperswithcode.com/dataset/2012-i2b2-temporal-relations,"The Sixth Informatics for Integrating Biology and the Bedside (i2b2) Natural Language Processing Challenge for Clinical Records focused on the temporal relations in clinical narratives. The organizers provided the research community with a corpus of discharge summaries annotated with temporal information, to be used for the development and evaluation of temporal reasoning systems. 18 teams from around the world participated in the challenge. During the workshop, participating teams presented comprehensive reviews and analysis of their systems, and outlined future research directions suggested by the challenge contributions.",,,,,,
8,2018_n2c2__Track_2__-_Adverse_Drug_Events_and_Medi,Relation Extraction,Relation Extraction,"Relation Extraction, Named Entity Recognition (NER), Clinical Concept Extraction","Graph, Image, Text",English,Computer Vision,,,https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/,https://paperswithcode.com/dataset/2018-n2c2-track-2-adverse-drug-events-and,"Abstract
Objective
This article summarizes the preparation, organization, evaluation, and results of Track 2 of the 2018 National NLP Clinical Challenges shared task. Track 2 focused on extraction of adverse drug events (ADEs) from clinical records and evaluated 3 tasks: concept extraction, relation classification, and end-to-end systems. We perform an analysis of the results to identify the state of the art in these tasks, learn from it, and build on it.

Materials and Methods
For all tasks, teams were given raw text of narrative discharge summaries, and in all the tasks, participants proposed deep learning–based methods with hand-designed features. In the concept extraction task, participants used sequence labelling models (bidirectional long short-term memory being the most popular), whereas in the relation classification task, they also experimented with instance-based classifiers (namely support vector machines and rules). Ensemble methods were also popular.

Results
A total of 28 teams participated in task 1, with 21 teams in tasks 2 and 3. The best performing systems set a high performance bar with F1 scores of 0.9418 for concept extraction, 0.9630 for relation classification, and 0.8905 for end-to-end. However, the results were much lower for concepts and relations of Reasons and ADEs. These were often missed because local context is insufficient to identify them.

Conclusions
This challenge shows that clinical concept extraction and relation classification systems have a high performance for many concept types, but significant improvement is still required for ADEs and Reasons. Incorporating the larger context or outside knowledge will likely improve the performance of future systems.",2018,,,,,
9,20NewsGroups,Topic Models,Topic Models,"Topic Models, Intrusion Detection, Unsupervised Text Classification","Image, Text",English,Computer Vision,"topic-models-on-20newsgroups, unsupervised-text-classification-on-1, intrusion-detection-on-20newsgroups",,http://qwone.com/~jason/20Newsgroups/,https://paperswithcode.com/dataset/20newsgroups,"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.",,,,,,
10,20_Newsgroups,Supervised Text Retrieval,Supervised Text Retrieval,"Supervised Text Retrieval, Out-of-Distribution Detection, Text Retrieval, Topic Models, Text Clustering, Text Classification","Image, Text",English,Computer Vision,"text-classification-on-20-newsgroups, text-retrieval-on-20-newsgroups, supervised-text-retrieval-on-20-newsgroups-1, out-of-distribution-detection-on-20, text-clustering-on-20-newsgroups, topic-models-on-20-newsgroups",,http://qwone.com/~jason/20Newsgroups/,https://paperswithcode.com/dataset/20-newsgroups,"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.",,,,,,
11,2D-3D-S,Visual Navigation,Visual Navigation,"Visual Navigation, Depth Estimation, Semantic Segmentation, Robust Semi-Supervised RGBD Semantic Segmentation, Semi-Supervised RGBD Semantic Segmentation, 3D Room Layouts From A Single RGB Panorama, Semi-Supervised Semantic Segmentation, Self-Supervised Learning","3D, Image",,Computer Vision,"semantic-segmentation-on-stanford2d3d-2, semi-supervised-semantic-segmentation-on-2d, depth-estimation-on-stanford2d3d-panoramic, semantic-segmentation-on-stanford2d3d-1, 3d-room-layouts-from-a-single-rgb-panorama-on-3, robust-semi-supervised-rgbd-semantic, semantic-segmentation-on-stanford2d3d-rgbd, semi-supervised-rgbd-semantic-segmentation-on",Custom,https://github.com/alexsax/2D-3D-Semantics,https://paperswithcode.com/dataset/2d-3d-s,"The 2D-3D-S dataset provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. It covers over 6,000 m2 collected in 6 large-scale indoor areas that originate from 3 different buildings. It contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360° equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces.",,,,,,
12,2D-ATOMS,Theory of Mind Modeling,Theory of Mind Modeling,Theory of Mind Modeling,,,Methodology,,MIT,https://huggingface.co/datasets/sled-umich/2D-ATOMS,https://paperswithcode.com/dataset/2d-atoms,"Official dataset for Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models. Ziqiao Ma, Jacob Sansom, Run Peng, Joyce Chai. EMNLP Findings, 2023.

We introduce 2D-ATOMS dataset, a novel text-based dataset that evaluates a machine's reasoning process under a situated theory-of-mind setting.

Our dataset includes 9 different ToM evaluation tasks for each mental state under ATOMS framework, and 1 reality-checking task to test LLMs’ understanding of the world. It is important to acknowledge that our experiment serves as a proof of concept and does not aim to cover the entire spectrum of machine ToM, as our case studies are far from being exhaustive or systematic. Here we release the zero-shot version of our dataset, which is used in our paper.",2023,,,,,
13,2DeteCT,Tomographic Reconstructions,Tomographic Reconstructions,"Tomographic Reconstructions, Image Segmentation, Computed Tomography (CT), Image Reconstruction, Low-Dose X-Ray Ct Reconstruction, Image Denoising, CT Reconstruction, Denoising","3D, Image",,Computer Vision,,CC Attribution 4.0 International,https://zenodo.org/record/8014758,https://paperswithcode.com/dataset/2detect,"Maximilian B. Kiss, Sophia B. Coban, K. Joost Batenburg, Tristan van Leeuwen, and Felix Lucka ""2DeteCT - A large 2D expandable, trainable, experimental Computed Tomography dataset for machine learning"",  Sci Data 10, 576 (2023) or arXiv:2306.05907 (2023)

Abstract:
""Recent research in computational imaging largely focuses on developing machine learning (ML) techniques for image reconstruction, which requires large-scale training datasets consisting of measurement data and ground-truth images. However, suitable experimental datasets for X-ray Computed Tomography (CT) are scarce, and methods are often developed and evaluated only on simulated data. We fill this gap by providing the community with a versatile, open 2D fan-beam CT dataset suitable for developing ML techniques for a range of image reconstruction tasks. To acquire it, we designed a sophisticated, semi-automatic scan procedure that utilizes a highly-flexible laboratory X-ray CT setup. A diverse mix of samples with high natural variability in shape and density was scanned slice-by-slice (5000 slices in total) with high angular and spatial resolution and three different beam characteristics: A high-fidelity, a low-dose and a beam-hardening-inflicted mode. In addition, 750 out-of-distribution slices were scanned with sample and beam variations to accommodate robustness and segmentation tasks. We provide raw projection data, reference reconstructions and segmentations based on an open-source data processing pipeline.""

The data collection has been acquired using a highly flexible, programmable and custom-built X-ray CT scanner, the FleX-ray scanner, developed by TESCAN-XRE NV, located in the FleX-ray Lab at the Centrum Wiskunde & Informatica (CWI) in Amsterdam, Netherlands. It consists of a cone-beam microfocus X-ray point source (limited to 90 kV and 90 W) that projects polychromatic X-rays onto a 14-bit CMOS (complementary metal-oxide semiconductor) flat panel detector with CsI(Tl) scintillator (Dexella 1512NDT) and 1536-by-1944 pixels,  each. To create a 2D dataset, a fan-beam geometry was mimicked by only reading out the central row of the detector. Between source and detector there is a rotation stage, upon which samples can be mounted. The machine components (i.e., the source, the detector panel, and the rotation stage) are mounted on translation belts that allow the moving of the components independently from one another.

Please refer to the paper for all further technical details.

The complete data collection can be found via the following links: 1-1,000, 1,001-2,000, 2,001-3,000, 3,001-4,000, 4,001-5,000, 5,521-6,370.

Each slice folder ‘slice00001 - slice05000’ and ‘slice05521 - slice06370’ contains three folders for each mode: ‘mode1’, ‘mode2’, ‘mode3’. In each of these folders there are the sinogram, the dark-field, and the two flat-fields for the raw data archives, or just the reconstructions and for mode2 the additional reference segmentation.

The corresponding reference reconstructions and segmentations can be found via the following links: 1-1,000, 1,001-2,000, 2,001-3,000, 3,001-4,000, 4,001-5,000, 5,521-6,370.

The corresponding Python scripts for loading, pre-processing, reconstructing and segmenting the projection data in the way described in the paper can be found on github. A machine-readable file with the used scanning parameters and instrument data for each acquisition mode as well as a script loading it can be found on the GitHub repository as well.

Note: It is advisable to use the graphical user interface when decompressing the .zip archives. If you experience a zipbomb error when unzipping the file on a Linux system rerun the command with the UNZIP_DISABLE_ZIPBOMB_DETECTION=TRUE environment variable by setting in your .bashrc “export UNZIP_DISABLE_ZIPBOMB_DETECTION=TRUE”.

For more information or guidance in using the data collection, please get in touch with

Maximilian.Kiss [at] cwi.nl

Felix.Lucka [at] cwi.nl",2023,arXiv:2306.05907 (2023),https://arxiv.org/abs/2306.05907,,"training datasets consisting of measurement data and ground-truth images. However, suitable experimental datasets for X-ray Computed Tomography (CT) are scarce, and methods are often developed and evaluated only on simulated data. We fill this gap by providing the community with a versatile, open 2D fan-beam CT dataset suitable for developing ML techniques for a range of image reconstruction tasks. To acquire it, we designed a sophisticated, semi-automatic scan procedure that utilizes a highly-flexible laboratory X-ray CT setup. A diverse mix of samples",
14,2D_NACA_RANS,Physical Simulations,Physical Simulations,Physical Simulations,,,Methodology,,ODbL-1.0 license,https://data.isir.upmc.fr/extrality/2D_RANS_NACA_Dataset.zip,https://paperswithcode.com/dataset/2d-naca-rans,Dataset of low fidelity resolutions of the RANS equations over airfoils.,,,,,,
15,2D_site-percolation_threshold,Physical Simulations,Physical Simulations,Physical Simulations,,,Methodology,,CC BY-SA 4.0,https://www.kaggle.com/datasets/cardstdani/2d-site-percolation-threshold,https://paperswithcode.com/dataset/2d-site-percolation-threshold,"The dataset is a .h5 file comprised of entries with keys of the form (n,m), denoting the dimensions of the system matrix on which the simulations have been performed. The value of each key are two arrays, one to store the number of iterations needed to terminate the process for each simulation, and the other for the number of elements present at the terminal state of each simulation. Thus, given the maximum number of elements n*m in each system, the estimated percolation threshold can be computed by averaging the ratios between the elements at each terminal state and the system size. Overall, 207950010 simulations have been performed. And, this dataset was used to perform a complexity analysis on: https://arxiv.org/abs/2410.11874",,,,,,
16,300W,Pose Estimation,Pose Estimation,"Pose Estimation, Facial Landmark Detection, Face Alignment, Unsupervised Facial Landmark Detection, 2D Pose Estimation, 3D Reconstruction","3D, Image",,Computer Vision,"unsupervised-facial-landmark-detection-on, facial-landmark-detection-on-300w, face-alignment-on-300w, face-alignment-on-300w-split-2-300w-lp, pose-estimation-on-300w-full, 2d-pose-estimation-on-300w, facial-landmark-detection-on-300w-full, 3d-reconstruction-on-300w, face-alignment-on-300w-split-2","Custom (research-only, non-commercial)",https://ibug.doc.ic.ac.uk/resources/300-W/,https://paperswithcode.com/dataset/300w,"The 300-W is a face dataset that consists of 300 Indoor and 300 Outdoor in-the-wild images. It covers a large variation of identity, expression, illumination conditions, pose, occlusion and face size. The images were downloaded from google.com by making queries such as “party”, “conference”, “protests”, “football” and “celebrities”. Compared to the rest of in-the-wild datasets, the 300-W database contains a larger percentage of partially-occluded images and covers more expressions than the common “neutral” or “smile”, such as “surprise” or “scream”.
Images were annotated with the 68-point mark-up using a semi-automatic methodology. The images of the database were carefully selected so that they represent a characteristic sample of challenging but natural face instances under totally unconstrained conditions. Thus, methods that achieve accurate performance on the 300-W database can demonstrate the same accuracy in most realistic cases.
Many images of the database contain more than one annotated faces (293 images with 1 face, 53 images with 2 faces and 53 images with [3, 7] faces). Consequently, the database consists of 600 annotated face instances, but 399 unique images. Finally, there is a large variety of face sizes. Specifically, 49.3% of the faces have size in the range [48.6k, 2.0M] and the overall mean size is 85k (about 292 × 292) pixels.",,https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_2016_imavis.pdf,https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_2016_imavis.pdf,293 images,"tests”, “football” and “celebrities”. Compared to the rest of in-the-wild datasets, the 300-W database contains a larger percentage of partially-occluded images",
17,3D-Point_Cloud_dataset_of_various_geometrical_terr,3D-Aware Image Synthesis,3D-Aware Image Synthesis,"3D-Aware Image Synthesis, 3D Classification","3D, Image",,Computer Vision,,,https://dataverse.uclouvain.be/dataset.xhtml?persistentId=doi:10.14428/DVN/S0LSIW,https://paperswithcode.com/dataset/3d-point-cloud-dataset-of-various-geometrical,"Depth vision has been recently used in many locomotion devices with the objective to ease the life of disabled people toward reaching more ecological lifestyle. This is due to the fact that such cameras are cheap, compact and can provide rich information about the environment. Our dataset provides many recordings of point cloud and other types of data during different locomotion modes in urban context.  If you used this data, please cite the following papers below:                                                             
1-Depth Vision based Terrain Detection Algorithm during Human Locomotion                                                                                               
2-Using Depth Vision for Terrain Detection during Active Locomotion",,,,,,
18,3D-POP,3D Pose Estimation,3D Pose Estimation,"3D Pose Estimation, Animal Pose Estimation, 3D Object Tracking, Multi-Animal Tracking with identification, 3D Object Reconstruction From A Single Image, Object Tracking, Multi-Object Tracking, 2D Pose Estimation, 3D Shape Reconstruction, 3D Object Detection From Stereo Images, 3D Object Detection From Monocular Images, Occlusion Handling","3D, Image, Video",,Computer Vision,,CC-BY-4.0,https://github.com/alexhang212/Dataset-3DPOP,https://paperswithcode.com/dataset/3d-pop,"The dataset is designed specifically to solve a range of computer vision problems (2D-3D tracking, posture) faced by biologists while designing behavior studies with animals.

Typically, datasets for animal-specific vision tasks are created using open-source video material. This might be effective for an initial start, but these methods are not deployment ready for the behavior community. Therefore, we designed a semi-automated method for biologists to create well-curated datasets at a large scale for the ML and Vision community.

3D-POP is the first dataset with 3D ground truth for multi-animal, multi-view tracking problems.

Highlight: The dataset is captured with the intention of using it for various vision problems and with different levels of complexity (no of cameras, no of individuals)

Video explanation: Link to YouTube video

Video teaser: Link to YouTube video

Dataset Features:
Marker-based videos:

6 hours+ of annotations of 18 individuals (groups of 1, 2, 5, 10).
Bounding box
Trajectories (2D and 3D)
Posture (2D and 3D) with 9 key points
Identities
Total of 57 sequences (4K) with 4 views.
Dataset customization* (Users can modify the dataset and add key points to the dataset)

Markerless:

1Hr+ videos of 18 individuals in groups of 1, 2, 5, 11. The birds have no markers. This data is provided as test cases and unsupervised approaches.

Problems:
2D domain:

Position, Posture of birds (different group sizes n = 1, 2, 5, 10) with Single/Multiview.
Tracking with single - multiview

3D domain:

Position, Posture of birds (different group sizes n = 1, 2, 5, 10) with Single/Multiview.
Tracking with single - multiview

Fine-grained recognition:

Identity tracking with ground truth.

Unsupervised learning:

2D or 3D posture problems

Idea:
The dataset is created with a motion capture system, using the 6-DOF tracking ability. Assumptions are that head and body act as rigid bodies when birds walk and forage (proved with experiment). Therefore, we get the 3D position of key points by tracking head/body orientation.",,,,,,
19,3D-ZeF,3D Object Detection From Stereo Images,3D Object Detection From Stereo Images,"3D Object Detection From Stereo Images, 3D Multi-Object Tracking, Multi-Object Tracking","3D, Image, Video",,Computer Vision,,,https://motchallenge.net/data/3D-ZeF20/,https://paperswithcode.com/dataset/3d-zef,"3D-ZeF dataset consists of eight sequences with a duration between 15-120 seconds and 1-10 free moving zebrafish. The videos have been annotated with a total of 86,400 points and bounding boxes.",,,,,,
20,3DIdent,Disentanglement,Disentanglement,Disentanglement,,,Methodology,disentanglement-on-3dident,Creative Commons Attribution 4.0 International,https://zenodo.org/record/4502485#.YgWm1fXMKbg,https://paperswithcode.com/dataset/3dident,"Novel benchmark which features aspects of natural scenes, e.g. a complex 3D object and different lighting conditions, while still providing access to the continuous ground-truth factors.

We use the Blender rendering engine to create visually complex 3D images. Each image in the dataset shows a colored 3D object which is located and rotated above a colored ground in a 3D space. Additionally, each scene contains a colored spotlight which is focused on the object and located on a half-circle around the scene. The observations are encoded with an RGB color space, and the spatial resolution is 224x224 pixels.

The images are rendered based on a 10-dimensional latent, where: (1) three dimensions describe the XYZ position, (2) three dimensions describe the rotation of the object in Euler angles, (3) two dimensions describe the color of the object and the ground of the scene, respectively, and (4) two dimensions describe the position and color of the spotlight. We use the HSV color space to describe the color of the object and the ground with only one latent each by having the latent factor control the hue value.

The training set and test set contain 250,000 and 25,000 observation-latent pairs, respectively, whereby the latents are uniformly sampled from the unit hyperrectangle.",,,,,,
21,3DMatch,Low-Light Image Enhancement,Low-Light Image Enhancement,"Low-Light Image Enhancement, Point Cloud Registration, 3D Feature Matching","3D, Image",,Computer Vision,"point-cloud-registration-on-3dmatch-benchmark, low-light-image-enhancement-on-3dmatch, 3d-feature-matching-on-3dmatch-benchmark",Various,http://3dmatch.cs.princeton.edu/,https://paperswithcode.com/dataset/3dmatch,"The 3DMATCH benchmark evaluates how well descriptors (both 2D and 3D) can establish correspondences between RGB-D frames of different views. The dataset contains 2D RGB-D patches and 3D patches (local TDF voxel grid volumes) of wide-baselined correspondences. 

The pixel size of each 2D patch is determined by the projection of the 0.3m3 local 3D patch around the interest point onto the image plane.",,,,,,
22,3DOH50K,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Monocular 3D Human Pose Estimation, 3D human pose and shape estimation","3D, Image",,Computer Vision,3d-human-pose-estimation-on-3doh50k,,https://www.yangangwang.com/papers/ZHANG-OOH-2020-03.html,https://paperswithcode.com/dataset/3doh50k,"3DOH50K is the first real 3D human dataset for the problem of human reconstruction and pose estimation in occlusion scenarios. It contains 51600 images with accurate 2D pose and 3D pose, SMPL parameters, and binary mask.",,,,51600 images,,
23,3DPW,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Cross-domain 3D Human Pose Estimation, Pose Estimation, Hand Pose Estimation, Human Pose Forecasting","3D, Image, Time Series",,Computer Vision,"human-pose-forecasting-on-3dpw, pose-estimation-on-3dpw, 3d-human-pose-estimation-on-3dpw, hand-pose-estimation-on-3dpw, cross-domain-3d-human-pose-estimation-on-3dpw","Custom (research-only, non-commercial)",http://virtualhumans.mpi-inf.mpg.de/3DPW,https://paperswithcode.com/dataset/3dpw,"The 3D Poses in the Wild dataset is the first dataset in the wild with accurate 3D poses for evaluation. While other datasets outdoors exist, they are all restricted to a small recording volume. 3DPW is the first one that includes video footage taken from a moving phone camera.

The dataset includes:


60 video sequences.
2D pose annotations.
3D poses obtained with the method introduced in the paper.
Camera poses for every frame in the sequences.
3D body scans and 3D people models (re-poseable and re-shapeable). Each sequence contains its corresponding models.
18 3D models in different clothing variations.",,,,,,
24,3DSSG,3d scene graph generation,3d scene graph generation,"3d scene graph generation, 3D Scene Graph Alignment, Scene Graph Generation","3D, Graph, Text",English,Natural Language Processing,"3d-scene-graph-alignment-on-3dssg, 3d-scene-graph-generation-on-3dssg",,https://3dssg.github.io/,https://paperswithcode.com/dataset/3dssg,"3DSSG provides 3D semantic scene graphs for 3RScan. A semantic scene graph is defined by a set of tuples between nodes and edges where nodes represent specific 3D object instances in a 3D scan. Nodes are defined by its semantics, a hierarchy of classes as well as a set of attributes that describe the visual and physical appearance of the object instance and their affordances. The edges in our graphs are the semantic relationships (predicates) between the nodes such as standing on, hanging on, more comfortable than or same material.",,,,,,
25,3D_Cars,Disentanglement,Disentanglement,Disentanglement,,,Methodology,,,http://www.scottreed.info,https://paperswithcode.com/dataset/3d-cars,"Car CAD models from ""3d object detection and viewpoint estimation with a deformable
3d cuboid model"" were used to generate the dataset. For each of the 199 car models, the authors generated $64\times64$ color renderings from 24 rotation angles each offset by 15 degrees, as well as from 4 different camera elevations.",,,,,,
26,3D_Flow_Shapes,Physical Simulations,Physical Simulations,"Physical Simulations, Liquid Simulation, Neural Network simulation",Graph,,Methodology,,CC BY 4.0,https://mediatum.ub.tum.de/1737748,https://paperswithcode.com/dataset/3d-flow-shapes,"The dataset consists of high-resolution three-dimensional (3D) turbulent flow simulations. It captures intricate vortex structures caused by a variety of shapes within a channel flow environment. The dataset is generated using OpenFOAM in large eddy simulation (LES) mode, ensuring the preservation of detailed turbulent characteristics across all spatial scales.

The dataset contains 45 simulations consisting of 5000 flow snapshots each at a temporal resolution of 0.1ms, for a total physical runtime of 0.5s per simulation. The channel geometry is 0.4x0.1x0.1m discretizted into a 192x48x84 regular grid. The inflow velocity is 20 meters per second. Each simulation contains a unique turbulent flow caused by the specific shape in the channel.

This dataset is ideally suited for machine learning researchers aiming to experiment with surrogate models, generative models, and applications in CFD. It can be used to train neural networks to predict turbulent flow predictions, simulate realistic flow fields efficiently, and evaluate models that aim to replicate the properties of turbulent kinetic energy distribution accurately. Moreover, it provides a rigorous testbed for developing generative models that can bypass computationally expensive CFD simulations, thereby prompting innovations in faster, model-driven fluid dynamics simulations for industrial and academic utilizations.",,,,,,
27,3D_Hand_Pose,3D Hand Pose Estimation,3D Hand Pose Estimation,"3D Hand Pose Estimation, Hand Pose Estimation, Pose Estimation","3D, Image",,Computer Vision,,BSD License,http://www.rovit.ua.es/dataset/mhpdataset/,https://paperswithcode.com/dataset/3d-hand-pose,3D Hand Pose is a multi-view hand pose dataset consisting of color images of hands and different kind of annotations for each: the bounding box and the 2D and 3D location on the joints in the hand.,,,,,,
28,3D_Shapes_Dataset,Disentanglement,Disentanglement,Disentanglement,,,Methodology,,,https://github.com/deepmind/3d-shapes,https://paperswithcode.com/dataset/3d-shapes-dataset,"3dshapes is a dataset of 3D shapes procedurally generated from 6 ground truth independent latent factors. These factors are floor colour, wall colour, object colour, scale, shape and orientation.",,,,,,
29,3RScan,Point Cloud Registration,Point Cloud Registration,"Point Cloud Registration, Scene Graph Generation, Semantic Segmentation, 3D Object Classification, Scene Understanding, Scene Change Detection, Predicate Classification, 3D Object Detection, 3D Semantic Segmentation","3D, Graph, Image, Text",English,Computer Vision,"predicate-classification-on-3r-scan-1, 3d-object-classification-on-3r-scan-1, scene-graph-generation-on-3r-scan-1, 3d-object-detection-on-3rscan, point-cloud-registration-on-3rscan",,https://waldjohannau.github.io/RIO/,https://paperswithcode.com/dataset/3r-scan,"A novel dataset and benchmark, which features 1482 RGB-D scans of 478 environments across multiple time steps. Each scene includes several objects whose positions change over time, together with ground truth annotations of object instances and their respective 6DoF mappings among re-scans.",,,,,,
30,3U-VQA,Explainable Artificial Intelligence (XAI),Explainable Artificial Intelligence (XAI),"Explainable Artificial Intelligence (XAI), Visual Question Answering (VQA), Image Captioning","Image, Text",English,Computer Vision,,Apache-2.0 license,https://github.com/modafarshouha/ReDiT,https://paperswithcode.com/dataset/3u-vqa,"To tackle the challenge of obtaining out-of-distribution (OOD) data for LVQA models, we introduced a novel dataset named 3U-VQA dataset (Usual, Unusual and Unknown object scenarios for LVQA with difficulty scoring dataset). The dataset comprises question and image sets. Each instance in the questions set is associated with a set of features representing the question-related criteria set. The questions and their ground truth answers are written using placeholders for the objects and their features, which can be specified based on the user needs and requirements. When creating the questions, we avoided deliberately binary (Yes/No) questions to prevent potential bias caused by the question's type in the model response.",,,,,,
31,4D-DRESS,Human Parsing,Human Parsing,"Human Parsing, Garment Reconstruction, Physical Simulations, 3D Human Reconstruction","3D, Image, Text",English,Computer Vision,"physical-simulations-on-4d-dress, 3d-human-reconstruction-on-4d-dress, human-parsing-on-4d-dress, garment-reconstruction-on-4d-dress",ETH,https://ait.ethz.ch/4d-dress,https://paperswithcode.com/dataset/4d-dress,"4D-DRESS is the first real-world 4D dataset of human clothing, capturing 64 human outfits in more than 520 motion sequences. These sequences include a) high-quality 4D textured scans; for each scan, we annotate b) vertex-level semantic labels, thereby obtaining c) the corresponding garment meshes and fitted SMPL(-X) body meshes. Totally, 4D-DRESS captures dynamic motions of 4 dresses, 28 lower, 30 upper, and 32 outer garments. For each garment, we also provide its canonical template mesh to benefit the future human clothing study.",,,,,,
32,4D-OR,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Scene Graph Generation, 4D Panoptic Segmentation, 2D Panoptic Segmentation, Panoptic Segmentation, Video Panoptic Segmentation, 3D Object Detection, 3D Panoptic Segmentation","3D, Graph, Image, Text, Video",English,Computer Vision,"2d-panoptic-segmentation-on-4d-or, video-panoptic-segmentation-on-4d-or, scene-graph-generation-on-4d-or",CC-BY-NC,https://github.com/egeozsoy/4D-OR,https://paperswithcode.com/dataset/4d-or,"4D-OR includes a total of 6734 scenes, recorded by six calibrated RGB-D Kinect sensors 1 mounted to the ceiling of the OR, with one frame-per-second, providing synchronized RGB and depth images. We provide fused point cloud sequences of entire scenes, automatically annotated human 6D poses and 3D bounding boxes for OR objects. Furthermore, we provide SSG annotations for each step of the surgery together with the clinical roles of all the humans in the scenes, e.g., nurse, head surgeon, anesthesiologist.",,,,,,
33,4Q_audio_emotion_dataset__Russell_s_model_,Music Emotion Recognition,Music Emotion Recognition,Music Emotion Recognition,"Audio, Image",,Computer Vision,,,https://mir.dei.uc.pt/downloads.html,https://paperswithcode.com/dataset/4q-audio-emotion-dataset-russell-s-model,"It contains 900 audio clips, annotated into 4 quadrants, according to Russell's model.",,,,,,
34,4Seasons,Visual Odometry,Visual Odometry,"Visual Odometry, Autonomous Driving",Image,,Computer Vision,,,https://www.4seasons-dataset.com/,https://paperswithcode.com/dataset/4seasons,4Seasons is adataset covering seasonal and challenging perceptual conditions for autonomous driving.,,Wenzel et al,https://arxiv.org/pdf/2009.06364v2.pdf,,,
35,50_Salads,Unsupervised Action Segmentation,Unsupervised Action Segmentation,"Unsupervised Action Segmentation, Action Segmentation","Image, Video",,Computer Vision,"action-segmentation-on-50-salads-1, unsupervised-action-segmentation-on-50-salads","Creative Commons Attribution-Non,  Commercial-Share Alike 4.0 International License",https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/,https://paperswithcode.com/dataset/50-salads,"Activity recognition research has shifted focus from distinguishing full-body motion patterns to recognizing complex interactions of multiple entities. Manipulative gestures – characterized by interactions between hands, tools, and manipulable objects – frequently occur in food preparation, manufacturing, and assembly tasks, and have a variety of applications including situational support, automated supervision, and skill assessment. With the aim to stimulate research on recognizing manipulative gestures we introduce the 50 Salads dataset. It captures 25 people preparing 2 mixed salads each and contains over 4h of annotated accelerometer and RGB-D video data. Including detailed annotations, multiple sensor types, and two sequences per participant, the 50 Salads dataset may be used for research in areas such as activity recognition, activity spotting, sequence analysis, progress tracking, sensor fusion, transfer learning, and user-adaptation.

The dataset includes

RGB video data 640×480 pixels at 30 Hz
Depth maps 640×480 pixels at 30 Hz
3-axis accelerometer data at 50 Hz of devices attached to a knife, a mixing spoon, a small spoon, a peeler, a glass, an oil bottle, and a pepper dispenser.
Synchronization parameters for temporal alignment of video and accelerometer data
Annotations as temporal intervals of pre- core- and post-phases of activities corresponding to steps in a recipe",,,,,,
36,5GAD-2022,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Low-latency processing",Image,,Computer Vision,low-latency-processing-on-5gad-2022,MIT,https://github.com/IdahoLabResearch/5GAD,https://paperswithcode.com/dataset/5gad-2022,"This dataset contains two types of intercepted network packets: ""normal"" network traffic packets (i.e. a variety of non-malicious traffic types) and ""attack"" packets from attacks against a 5G Core implemented with free5GC. The captures were collected using tshark or Wireshark on 4 different network interfaces within the 5G core. Those interfaces and where they sit within the system are outlined in the 5GNetworkDiagram figure. Files that start with ""allcap"" contain packets that were recorded on all four interfaces simultaneously; other *.pcapng files represent the same data that has been broken out into one of the four interfaces.",,,,,,
37,895_Fire_Videos_Data,Fire Detection,Fire Detection,Fire Detection,Image,,Computer Vision,,Commercial license,https://bit.ly/3O5J6gj,https://paperswithcode.com/dataset/895-fire-videos-data,"Description:
895 Fire Videos Data，the total duration of videos is 27 hours 6 minutes 48.58 seconds. The dataset adpoted different cameras to shoot fire videos. The shooting time includes day and night.The dataset can be used for tasks such as fire detection.

Data size:
895 videos, the total duration is 27 hours 6 minutes 48.58 seconds

Collecting environment:
including indoor and outdoor scenes

Data diversity:
multiple scenes, different time periods",,,,,,
38,97_synthetic_datasets,Clustering Algorithms Evaluation,Clustering Algorithms Evaluation,"Clustering Algorithms Evaluation, Graph Clustering",Graph,,Methodology,clustering-algorithms-evaluation-on-97,,https://github.com/deric/clustering-benchmark/tree/master/src/main/resources/datasets/artificial,https://paperswithcode.com/dataset/97-synthetic-datasets,"97 synthetic datasets consists of 97 datasets (as illustrated in the figure) and can be used to test graph-based clustering algorithms. 

https://github.com/deric/clustering-benchmark",,,,,,
39,A2D2,Weather Forecasting,Weather Forecasting,Weather Forecasting,Time Series,,Methodology,,CC BY-ND 4.0,https://www.a2d2.audi/a2d2/en.html,https://paperswithcode.com/dataset/a2d2,"Audi Autonomous Driving Dataset (A2D2) consists of simultaneously recorded images and 3D point clouds, together with 3D bounding boxes, semantic segmentation, instance segmentation, and data extracted from the automotive bus.",,,,,,
40,A2Dre,Referring Expression Segmentation,Referring Expression Segmentation,Referring Expression Segmentation,Image,,Computer Vision,referring-expression-segmentation-on-a2dre,MIT,https://github.com/imatge-upc/refvos,https://paperswithcode.com/dataset/a2d-referring-expressions,"We obtain A2Dre by selecting only instances that were labeled as non-trivial, which are 433 REs from 190 videos. We do not use the trivial cases as the analysis of such examples is not relevant, as referents can be described by using the category alone. Each annotator was presented with a RE, a video in which the target object was marked by a bounding box, and a set of questions paraphrasing our categories. A2Dre was annotated by 3 authors of the paper. Our final set of category annotations used for analysis was derived by means of majority voting: for each nontrivial RE, we kept all category labels which were assigned to the RE by at least two annotators.",,,,,,
41,A2D_Sentences,Referring Expression Segmentation,Referring Expression Segmentation,Referring Expression Segmentation,Image,,Computer Vision,referring-expression-segmentation-on-a2d,,https://kgavrilyuk.github.io/publication/actor_action/,https://paperswithcode.com/dataset/a2d-sentences,"The Actor-Action Dataset (A2D) by Xu et al. [29] serves as the largest video dataset for the general actor and action segmentation task. It contains 3,782 videos from YouTube with pixel-level labeled actors and their actions. The dataset includes eight different actions, while a total of seven actor classes are considered to perform those actions. We follow [29], who split the dataset into 3,036 training videos and 746 testing videos. 

As we are interested in pixel-level actor and action segmentation from sentences, we augment the videos in A2D with natural language descriptions about what each actor is doing in the videos.  Following the guidelines set forth
in [12], we ask our annotators for a discriminative referring expression of each actor instance if multiple objects are considered in a video. The annotation process resulted in a total of 6,656 sentences, including 811 different nouns, 225 verbs and 189 adjectives. Our sentences enrich the actor and action pairs from the A2D dataset with finer granularities. For example, the actor adult in A2D may be annotated with man, woman, person and player in our sentences, while action rolling may also refer to flipping, sliding, moving and running when describing different actors in different scenarios. Our sentences contain on average more words than the ReferIt dataset [12] (7.3 vs 4.7), even when we leave out prepositions, articles and linking verbs (4.5 vs 3.6). This makes sense as our sentences contain a variety of verbs while existing referring expression datasets mostly ignore verbs.",,,,656 sentences,,
42,Abalone,Core set discovery,Core set discovery,Core set discovery,,,Methodology,core-set-discovery-on-abalone,,http://archive.ics.uci.edu/ml/datasets/Abalone,https://paperswithcode.com/dataset/abalone,"Predicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem.",,,,,,
43,ABCD,Task-Oriented Dialogue Systems,Task-Oriented Dialogue Systems,"Task-Oriented Dialogue Systems, Workflow Discovery",,,Methodology,workflow-discovery-on-abcd,,https://github.com/asappresearch/abcd,https://paperswithcode.com/dataset/abcd,Action-Based Conversations Dataset (ABCD) is a goal-oriented dialogue fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. The dataset is proposed to study customer service dialogue systems in more realistic settings.,,,,,,
44,ABC_Dataset,Physical Simulations,Physical Simulations,"Physical Simulations, Region Proposal, Single-View 3D Reconstruction, 3D Reconstruction, Object Detection","3D, Image",,Computer Vision,,Custom,https://deep-geometry.github.io/abc-dataset/,https://paperswithcode.com/dataset/abc-dataset-1,"The ABC Dataset is a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction. Sampling the parametric descriptions of surfaces and curves allows generating data in different formats and resolutions, enabling fair comparisons for a wide range of geometric learning algorithms.",,,,,,
45,ABIDE,Outcome Prediction In Multimodal Mri,Outcome Prediction In Multimodal Mri,"Outcome Prediction In Multimodal Mri, Motion Correction In Multishot Mri, Multi-Subject Fmri Data Alignment, MRI segmentation, Brain Lesion Segmentation From Mri","Image, Time Series, Video",,Multimodal,,Unrestricted for non-commercial research purposes,,https://paperswithcode.com/dataset/abide,"Autism spectrum disorder (ASD) is characterized by qualitative impairment in social reciprocity, and by repetitive, restricted, and stereotyped behaviors/interests. Previously considered rare, ASD is now recognized to occur in more than 1% of children. Despite continuing research advances, their pace and clinical impact have not kept up with the urgency to identify ways of determining the diagnosis at earlier ages, selecting optimal treatments, and predicting outcomes. For the most part this is due to the complexity and heterogeneity of ASD. To face these challenges, large-scale samples are essential, but single laboratories cannot obtain sufficiently large datasets to reveal the brain mechanisms underlying ASD. In response, the Autism Brain Imaging Data Exchange (ABIDE) initiative has aggregated functional and structural brain imaging data collected from laboratories around the world to accelerate our understanding of the neural bases of autism. With the ultimate goal of facilitating discovery science and comparisons across samples, the ABIDE initiative now includes two large-scale collections: ABIDE I and ABIDE II. Each collection was created through the aggregation of datasets independently collected across more than 24 international brain imaging laboratories and are being made available to investigators throughout the world, consistent with open science principles, such as those at the core of the International Neuroimaging Data-sharing Initiative. For details about these initiatives visit the collection specific pages: ABIDE I and ABIDE II.",,,,,,
46,ABO,3D Reconstruction,3D Reconstruction,"3D Reconstruction, Metric Learning, Single-View 3D Reconstruction",3D,,Methodology,,,https://amazon-berkeley-objects.s3.amazonaws.com/index.html,https://paperswithcode.com/dataset/abo,"ABO is a large-scale dataset designed for material prediction and multi-view retrieval experiments. The dataset contains Blender renderings of 30 viewpoints for each of the 7,953 3D objects, as well as camera intrinsics and extrinsic for each rendering.",,ABO: Dataset and Benchmarks for Real-World 3D Object Understanding,https://arxiv.org/pdf/2110.06199.pdf,,,
47,AbstractReasoning,Relational Reasoning,Relational Reasoning,"Relational Reasoning, Image Classification, Visual Reasoning",Image,,Reasoning,,"Custom (research-only, non-commercial)",https://github.com/deepmind/abstract-reasoning-matrices,https://paperswithcode.com/dataset/abstractreasoning,"AbstractReasoning is a dataset for abstract reasoning, where the goal is to infer the correct answer from the context panels based on abstract reasoning.",,Barrett et al,https://arxiv.org/pdf/1807.04225.pdf,,,
48,Abt-Buy,Blocking,Blocking,"Blocking, Entity Resolution, Data Integration",,,Methodology,"entity-resolution-on-abt-buy, blocking-on-abt-buy",Creative Commons license,https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution,https://paperswithcode.com/dataset/abt-buy,"The Abt-Buy dataset for entity resolution derives from the online retailers Abt.com and Buy.com. The dataset contains 1081 entities from abt.com and 1092 entities from buy.com as well as a gold standard (perfect mapping) with 1097 matching record pairs between the two data sources.  The common attributes between the two data sources are: product name, product description and product price. 

The dataset was initially published in the repository of the Database Group of the University of Leipzig:
https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution

To enable the reproducibility of the results and the comparability of the performance of different matchers on the Abt-Buy matching task, the dataset was split into fixed train, validation and test sets. 
The fixed splits are provided in the CompERBench repository: 

http://data.dws.informatik.uni-mannheim.de/benchmarkmatchingtasks/index.html",,,,,,
49,AbuseAnalyzer_Dataset,Abuse Detection,Abuse Detection,"Abuse Detection, Hate Speech Detection, severity prediction","Audio, Image, Time Series",,Computer Vision,,,https://github.com/mohit3011/AbuseAnalyzer,https://paperswithcode.com/dataset/abuseanalyzer-dataset,"The dataset contains 7,601 Gab posts classified on three different aspects: abuse presence or not, abuse severity and abuse target.

The Binary label distribution is as follows:
Abusive Posts: 4,120
Non-Abusive Posts: 3481

The Abuse Severity label distribution is as follows:
Biased Attitude: 1830
Act of Bias and Discrimination: 1807
*Violence and Genocide: 483

The Abuse Target label distribution is as follows:
Individual (Second-Person):  389
Individual (Third-Person): 1330
*Group: 2401",,,,,,
50,Acappella,Multi-task Audio Source Seperation,Multi-task Audio Source Seperation,"Multi-task Audio Source Seperation, audio-visual learning, Audio-Visual Synchronization","Audio, Image",,Audio,,Creative Commons Attribution 4.0 International,https://ipcv.github.io/Acappella/acappella/,https://paperswithcode.com/dataset/acappella,"Acappella comprises around 46 hours of a cappella solo singing videos sourced from YouTbe, sampled across different singers and languages. Four languages are considered: English, Spanish, Hindi and others.  

This dateset was designed for Audiovisual singing voice separation, although it can be used for any self-supervised audio-visual task such us: audio-guided  lip reading, audio-visual learning,",,,,,,
51,ACDC,Diffeomorphic Medical Image Registration,Diffeomorphic Medical Image Registration,"Diffeomorphic Medical Image Registration, Medical Image Segmentation, Semi-supervised Medical Image Segmentation, Medical Image Generation","Image, Text",English,Medical,"medical-image-segmentation-on-acdc, diffeomorphic-medical-image-registration-on-1, medical-image-generation-on-acdc, medical-image-segmentation-on-automatic, semi-supervised-medical-image-segmentation-on-2",,https://acdc.creatis.insa-lyon.fr/description/databases.html,https://paperswithcode.com/dataset/acdc,"The goal of the Automated Cardiac Diagnosis Challenge (ACDC) challenge is to:


compare the performance of automatic methods on the segmentation of the left ventricular endocardium and epicardium as the right ventricular endocardium for both end diastolic and end systolic phase instances;
compare the performance of automatic methods for the classification of the examinations in five classes (normal case, heart failure with infarction, dilated cardiomyopathy, hypertrophic cardiomyopathy, abnormal right ventricle).

The overall ACDC dataset was created from real clinical exams acquired at the University Hospital of Dijon. Acquired data were fully anonymized and handled within the regulations set by the local ethical committee of the Hospital of Dijon (France). Our dataset covers several well-defined pathologies with enough cases to (1) properly train machine learning methods and (2) clearly assess the variations of the main physiological parameters obtained from cine-MRI (in particular diastolic volume and ejection fraction). The dataset is composed of 150 exams (all from different patients) divided into 5 evenly distributed subgroups (4 pathological plus 1 healthy subject groups) as described below. Furthermore, each patient comes with the following additional information : weight, height, as well as the diastolic and systolic phase instants.

The database is made available to participants through two datasets from the dedicated online evaluation website after a personal registration: i) a training dataset of 100 patients along with the corresponding manual references based on the analysis of one clinical expert; ii) a testing dataset composed of 50 new patients, without manual annotations but with the patient information given above. The raw input images are provided through the Nifti format.",,,,,"valuation website after a personal registration: i) a training dataset of 100 patients along with the corresponding manual references based on the analysis of one clinical expert; ii) a testing dataset composed of 50 new patients, without manual annotations but with the patient information given above. The raw input images",
52,ACDC_Scribbles,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Medical Image Segmentation, Weakly-Supervised Semantic Segmentation, Cardiac Segmentation, Heart Segmentation, Weakly supervised segmentation",Image,,Computer Vision,semantic-segmentation-on-acdc-scribbles,,https://vios-s.github.io/multiscale-adversarial-attention-gates,https://paperswithcode.com/dataset/acdc-scribbles,"We release expert-made scribble annotations for the medical ACDC dataset [1]. The released data must be considered as extending the original ACDC dataset.
The ACDC dataset contains cardiac MRI images, paired with hand-made segmentation masks. It is possible to use the segmentation masks provided in the ACDC dataset to evaluate the performance of methods trained using only scribble supervision. 

References: 
[1] Bernard, Olivier, et al. ""Deep learning techniques for automatic MRI cardiac multi-structures segmentation and diagnosis: is the problem solved?."" IEEE transactions on medical imaging 37.11 (2018): 2514-2525.",2018,,,,,
53,ACDC__Adverse_Conditions_Dataset_with_Corresponden,Source-Free Domain Adaptation,Source-Free Domain Adaptation,"Source-Free Domain Adaptation, Unsupervised Semantic Segmentation, Domain Adaptation, Foggy Scene Segmentation",Image,,Computer Vision,"domain-adaptation-on-cityscapes-to-acdc, foggy-scene-segmentation-on-acdc-adverse, unsupervised-semantic-segmentation-on-acdc, source-free-domain-adaptation-on-cityscapes",,,https://paperswithcode.com/dataset/acdc-adverse-conditions-dataset-with,"We introduce ACDC, the Adverse Conditions Dataset with Correspondences for training and testing semantic segmentation methods on adverse visual conditions. It comprises a large set of 4006 images which are evenly distributed between fog, nighttime, rain, and snow. Each adverse-condition image comes with a high-quality fine pixel-level semantic annotation, a corresponding image of the same scene taken under normal conditions and a binary mask that distinguishes between intra-image regions of clear and uncertain semantic content.

ACDC supports two tasks:
1. standard semantic segmentation
2. uncertainty-aware semantic segmentation",,,,4006 images,training and testing semantic segmentation methods on adverse visual conditions. It comprises a large set of 4006 images,
54,ACES,Translation,Translation,"Translation, Machine Translation",Text,English,Natural Language Processing,machine-translation-on-aces,,https://github.com/EdinburghNLP/ACES,https://paperswithcode.com/dataset/aces,ACES a dataset consisting of 68 phenomena ranging from simple perturbations at the word/character level to more complex errors based on discourse and real-world knowledge. It can be used to evaluate a wide range of Machine Translation metrics.,,,,,,
55,ACE_2004,Entity Disambiguation,Entity Disambiguation,"Entity Disambiguation, UIE, Nested Named Entity Recognition, Relation Extraction, Nested Mention Recognition, Named Entity Recognition (NER)","Graph, Image, Text",English,Computer Vision,"nested-mention-recognition-on-ace-2004, nested-named-entity-recognition-on-ace-2004, named-entity-recognition-on-ace-2004, uie-on-ace-2004, entity-disambiguation-on-ace2004, relation-extraction-on-ace-2004",Custom,https://catalog.ldc.upenn.edu/LDC2005T09,https://paperswithcode.com/dataset/ace-2004,"ACE 2004 Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2004 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities and relations and was created by Linguistic Data Consortium with support from the ACE Program, with additional assistance from the DARPA TIDES (Translingual Information Detection, Extraction and Summarization) Program.
The objective of the ACE program is to develop automatic content extraction technology to support automatic processing of human language in text form. In September 2004, sites were evaluated on system performance in six areas: Entity Detection and Recognition (EDR), Entity Mention Detection (EMD), EDR Co-reference, Relation Detection and Recognition (RDR), Relation Mention Detection (RMD), and RDR given reference entities. All tasks were evaluated in three languages: English, Chinese and Arabic.",2004,Custom,https://catalog.ldc.upenn.edu/license/ldc-non-members-agreement.pdf,,,
56,ACE_2005,Joint Entity and Relation Extraction,Joint Entity and Relation Extraction,"Joint Entity and Relation Extraction, Nested Named Entity Recognition, Relation Extraction, Event Argument Extraction, Event Extraction, Nested Mention Recognition, Named Entity Recognition (NER)","Graph, Image, Text",English,Computer Vision,"joint-entity-and-relation-extraction-on-7, joint-entity-and-relation-extraction-on-ace, nested-mention-recognition-on-ace-2005, event-extraction-on-ace2005, named-entity-recognition-on-ace-2005, relation-extraction-on-ace-2005, nested-named-entity-recognition-on-ace-2005, named-entity-recognition-on-ace2005, event-argument-extraction-on-ace2005",,https://catalog.ldc.upenn.edu/LDC2006T06,https://paperswithcode.com/dataset/ace-2005,"ACE 2005 Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2005 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities, relations and events by the Linguistic Data Consortium (LDC) with support from the ACE Program and additional assistance from LDC.",2005,https://arxiv.org/pdf/1811.06031.pdf,https://arxiv.org/pdf/1811.06031.pdf,,,
57,ACL_ARC,Citation Recommendation,Citation Recommendation,"Citation Recommendation, Continual Pretraining, Citation Intent Classification, Sentence Classification",Image,,Computer Vision,"sentence-classification-on-acl-arc, continual-pretraining-on-acl-arc, citation-intent-classification-on-acl-arc, citation-recommendation-on-acl-arc-citation",,https://web.eecs.umich.edu/~lahiri/acl_arc.html,https://paperswithcode.com/dataset/acl-arc-1,"ACL Anthology Reference Corpus (ACL ARC) is a collection of 10,920 academic papers from the ACL Anthology. ACL ARC is cleaned to remove:


files that look like not full papers, paper fragments, foreign-language papers (e.g., French), or pure junk.
headers (title and author information; NOT abstract).
footers (""References"" line and the actual references).
some bad characters (spurious characters).
some page numbers (i.e., a single number appearing on a line, with nothing else attached to it).
significant foreign-language (e.g., French) content in an otherwise English paper.

The cleaned corpus has 10,628 documents.",,,,628 documents,,
58,ACM__Heterogeneous_Node_Classification_,Heterogeneous Node Classification,Heterogeneous Node Classification,Heterogeneous Node Classification,Image,,Computer Vision,heterogeneous-node-classification-on-acm,,,https://paperswithcode.com/dataset/acm-heterogeneous-node-classification,A popular dataset for node classification on heterogeneous graphs.,,,,,,
59,ACNE04,Acne Severity Grading,Acne Severity Grading,"Acne Severity Grading, Medical Image Classification",Image,,Computer Vision,acne-severity-grading-on-acne04,,,https://paperswithcode.com/dataset/acne04,"The ACNE04 dataset includes 3756 Chinese face images with Acne.  The ACNE04 dataset includes the annotations of local lesion numbers and global acne
severity based on Hayashi Criterion.",,,,,,
60,ACOS,Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),Text,English,Natural Language Processing,aspect-based-sentiment-analysis-absa-on-acos,,,https://paperswithcode.com/dataset/acos,"Most of the aspect based sentiment analysis research aims at identifying the sentiment polarities toward some explicit aspect terms while ignores implicit aspects in text. To capture both explicit and implicit aspects, we focus on aspect-category based sentiment analysis, which involves joint aspect category detection and category-oriented sentiment classification. However, currently only a few simple studies have focused on this problem. The shortcomings in the way they defined the task make their approaches difficult to effectively learn the inner-relations between categories and the inter-relations between categories and sentiments. In this work, we re-formalize the task as a category-sentiment hierarchy prediction problem, which contains a hierarchy output structure to first identify multiple aspect categories in a piece of text, and then predict the sentiment for each of the identified categories. Specifically, we propose a Hierarchical Graph Convolutional Network (Hier-GCN), where a lower-level GCN is to model the inner-relations among multiple categories, and the higher-level GCN is to capture the inter-relations between aspect categories and sentiments. Extensive evaluations demonstrate that our hierarchy output structure is superior over existing ones, and the Hier-GCN model can consistently achieve the best results on four benchmarks.",,,,,,
61,ActivityNet-QA,Zero-Shot Video Question Answer,Zero-Shot Video Question Answer,"Zero-Shot Video Question Answer, Visual Question Answering (VQA), Question Answering, Video Question Answering","Image, Text, Video",English,Computer Vision,"video-question-answering-on-activitynet-qa, zeroshot-video-question-answer-on-activitynet",,https://github.com/MILVLG/activitynet-qa,https://paperswithcode.com/dataset/activitynet-qa,"The ActivityNet-QA dataset contains 58,000 human-annotated QA pairs on 5,800 videos derived from the popular ActivityNet dataset. The dataset provides a benchmark for testing the performance of VideoQA models on long-term spatio-temporal reasoning.",,,,,,
62,ActivityNet,Zero-Shot Video Retrieval,Zero-Shot Video Retrieval,"Zero-Shot Video Retrieval, Few Shot Temporal Action Localization, Zero-Shot Action Detection, Action Classification, Action Recognition In Videos, GZSL Video Classification, Weakly Supervised Action Localization, Temporal Action Localization, ZSL Video Classification, Semi-Supervised Action Detection, Action Recognition, Action Detection, Video Retrieval, Visual Question Answering (VQA), Temporal Action Proposal Generation, Zero-Shot Action Recognition","Image, Text, Time Series, Video",English,Computer Vision,"gzsl-video-classification-on-activitynet-gzsl, temporal-action-proposal-generation-on, action-classification-on-activitynet-12, action-recognition-in-videos-on-activitynet-1, zero-shot-action-recognition-on-activitynet, zero-shot-action-detection-on-activitynet-1-3, visual-question-answering-vqa-on-activitynet-1, semi-supervised-action-detection-on, video-retrieval-on-activitynet, action-recognition-in-videos-on-activitynet, temporal-action-localization-on-activitynet-1, weakly-supervised-action-localization-on-1, zero-shot-video-retrieval-on-activitynet, few-shot-temporal-action-localization-on, gzsl-video-classification-on-activitynet-gzsl-1, action-classification-on-activitynet, temporal-action-localization-on-activitynet, weakly-supervised-action-localization-on-2, zsl-video-classification-on-activitynet-gzsl",,http://activity-net.org/,https://paperswithcode.com/dataset/activitynet,"The ActivityNet dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public.",,Dynamic Temporal Pyramid Network: A Closer Look at Multi-Scale Modeling for Activity Detection,https://arxiv.org/abs/1808.02536,,,
63,ADAM,Fovea Detection,Fovea Detection,"Fovea Detection, Optic Disc Segmentation, Optic Disc Detection",Image,,Computer Vision,"optic-disc-segmentation-on-adam, fovea-detection-on-adam",Attribution 4.0 International (CC BY 4.0),https://dx.doi.org/10.21227/dt4f-rt59,https://paperswithcode.com/dataset/adam,"ADAM is organized as a half day Challenge, a Satellite Event of the ISBI 2020 conference in Iowa City, Iowa, USA.

The ADAM challenge focuses on the investigation and development of algorithms associated with the diagnosis of Age-related Macular degeneration (AMD) and segmentation of lesions in fundus photos from AMD patients. The goal of the challenge is to evaluate and compare automated algorithms for the detection of AMD on a common dataset of retinal fundus images. We invite the medical image analysis community to participate by developing and testing existing and novel automated fundus classification and segmentation methods.

Instructions: 
ADAM: Automatic Detection challenge on Age-related Macular degeneration

Link: https://amd.grand-challenge.org

Age-related macular degeneration, abbreviated as AMD, is a degenerative disorder in the macular region. It mainly occurs in people older than 45 years old and its incidence rate is even higher than diabetic retinopathy in the elderly.  

The etiology of AMD is not fully understood, which could be related to multiple factors, including genetics, chronic photodestruction effect, and nutritional disorder. AMD is classified into Dry AMD and Wet AMD. Dry AMD (also called nonexudative AMD) is not neovascular. It is characterized by progressive atrophy of retinal pigment epithelium (RPE). In the late stage, drusen and the large area of atrophy could be observed under ophthalmoscopy. Wet AMD (also called neovascular or exudative AMD), is characterized by active neovascularization under RPE, subsequently causing exudation, hemorrhage, and scarring, and will eventually cause irreversible damage to the photoreceptors and rapid vision loss if left untreated.

An early diagnosis of AMD is crucial to treatment and prognosis. Fundus photo is one of the basic examinations. The current dataset is composed of AMD and non-AMD (myopia, normal control, etc.) photos. Typical signs of AMD that can be found in these photos include drusen, exudation, hemorrhage, etc. 

The ADAM challenge has 4 tasks:

Task 1: Classification of AMD and non-AMD fundus images.

Task 2: Detection and segmentation of optic disc.

Task 3: Localization of fovea.

Task 4: Detection and Segmentation of lesions from fundus images.",2020,,,,,
64,ADE-OoD,Novelty Detection,Novelty Detection,"Novelty Detection, Out-of-Distribution Detection",Image,,Computer Vision,out-of-distribution-detection-on-ade-ood,ADE20k terms of use,https://ade-ood.github.io/,https://paperswithcode.com/dataset/ade-ood,"ADE-OoD is a public benchmark for dense out-of-distribution detection in general natural images. It measures the ability to detect and localize objects which are out-of-distribution with respect to the 150 categories of the ADE20k semantic segmentation dataset.

The benchmark data and annotations are available for download at the project page: ade-ood.github.io/",,,,,,150
65,ADE20K,Weakly-Supervised Semantic Segmentation,Weakly-Supervised Semantic Segmentation,"Weakly-Supervised Semantic Segmentation, Open Vocabulary Semantic Segmentation, Pose Transfer, Face Detection, Instance Segmentation, Image-to-Image Translation, Overlapped 100-10, Scene Recognition, Zero-Shot Semantic Segmentation, Open Vocabulary Panoptic Segmentation, Continual Semantic Segmentation, Semantic Segmentation, Unsupervised Semantic Segmentation with Language-image Pre-training, Reconstruction, Speech Prompted Semantic Segmentation, Overlapped 100-50, Sound Prompted Semantic Segmentation, Scene Understanding, Panoptic Segmentation, Overlapped 25-25, Overlapped 100-5, Semi-Supervised Semantic Segmentation, Overlapped 50-50","3D, Audio, Image, Text",English,Computer Vision,"speech-prompted-semantic-segmentation-on, open-vocabulary-semantic-segmentation-on-3, semantic-segmentation-on-ade20k, overlapped-25-25-on-ade20k, scene-understanding-on-ade20k-val-1, unsupervised-semantic-segmentation-with-4, semi-supervised-semantic-segmentation-on-42, pose-transfer-on-ade20k, instance-segmentation-on-ade20k-val, image-to-image-translation-on-ade20k-outdoor, semantic-segmentation-on-ade20k-val, scene-recognition-on-ade20k, open-vocabulary-semantic-segmentation-on-2, overlapped-100-10-on-ade20k, overlapped-50-50-on-ade20k, face-detection-on-ade20k, reconstruction-on-ade20k, weakly-supervised-semantic-segmentation-on-20, panoptic-segmentation-on-ade20k, sound-prompted-semantic-segmentation-on, panoptic-segmentation-on-ade20k-val, semi-supervised-semantic-segmentation-on-41, continual-semantic-segmentation-on-ade20k, open-vocabulary-panoptic-segmentation-on, overlapped-100-5-on-ade20k, zero-shot-semantic-segmentation-on-ade20k-847, overlapped-100-50-on-ade20k, image-to-image-translation-on-ade20k-labels","Custom (research-only, non-commercial)",https://groups.csail.mit.edu/vision/datasets/ADE20K/,https://paperswithcode.com/dataset/ade20k,"The ADE20K semantic segmentation dataset contains more than 20K scene-centric images exhaustively annotated with pixel-level objects and object parts labels. There are totally 150 semantic categories, which include stuffs like sky, road, grass, and discrete objects like person, car, bed.",,Cooperative Image Segmentation and Restoration in Adverse Environmental Conditions,https://arxiv.org/abs/1911.00679,,,
66,ADHD-200,3D Classification,3D Classification,"3D Classification, Classification","3D, Image",,Computer Vision,,Custom,,https://paperswithcode.com/dataset/adhd-200,"Attention Deficit Hyperactivity Disorder (ADHD) affects at least 5-10% of school-age children and is associated with substantial lifelong impairment, with annual direct costs exceeding $36 billion/year in the US. Despite a voluminous empirical literature, the scientific community remains without a comprehensive model of the pathophysiology of ADHD. Further, the clinical community remains without objective biological tools capable of informing the diagnosis of ADHD for an individual or guiding clinicians in their decision-making regarding treatment.

The ADHD-200 Sample is a grassroots initiative, dedicated to accelerating the scientific community's understanding of the neural basis of ADHD through the implementation of open data-sharing and discovery-based science. Towards this goal, we are pleased to announce the unrestricted public release of 776 resting-state fMRI and anatomical datasets aggregated across 8 independent imaging sites, 491 of which were obtained from typically developing individuals and 285 in children and adolescents with ADHD (ages: 7-21 years old). Accompanying phenotypic information includes: diagnostic status, dimensional ADHD symptom measures, age, sex, intelligence quotient (IQ) and lifetime medication status. Preliminary quality control assessments (usable vs. questionable) based upon visual timeseries inspection are included for all resting state fMRI scans.

In accordance with HIPAA guidelines and 1000 Functional Connectomes Project protocols, all datasets are anonymous, with no protected health information included.
http://fcon_1000.projects.nitrc.org/indi/adhd200/",,,,,,
67,Adience,Age And Gender Classification,Age And Gender Classification,"Age And Gender Classification, Age Estimation, Face Quality Assessement, Face Recognition",Image,,Computer Vision,"face-quality-assessement-on-adience, face-recognition-on-adience, age-and-gender-classification-on-adience, face-recognition-on-adience-online-open-set, age-estimation-on-adience-1, age-and-gender-classification-on-adience-age",Custom,https://talhassner.github.io/home/projects/Adience/Adience-data.html,https://paperswithcode.com/dataset/adience,"The Adience dataset, published in 2014, contains 26,580 photos across 2,284 subjects with a binary gender label and one label from eight different age groups, partitioned into five splits. The key principle of the data set is to capture the images as close to real world conditions as possible, including all variations in appearance, pose, lighting condition and image quality, to name a few.",2014,Understanding and Comparing Deep Neural Networksfor Age and Gender Classification,https://arxiv.org/abs/1708.07689,,,
68,ADL_Piano_MIDI,Music Generation,Music Generation,"Music Generation, Audio Generation, Speech Recognition","Audio, Image, Text",English,Computer Vision,,,https://github.com/lucasnfe/adl-piano-midi,https://paperswithcode.com/dataset/adl-piano-midi,"The ADL Piano MIDI is a dataset of 11,086 piano pieces from different genres. This dataset is based on the Lakh MIDI dataset, which is a collection on 45,129 unique MIDI files that have been matched to entries in the Million Song Dataset. Most pieces in the Lakh MIDI dataset have multiple instruments, so for each file the authors of ADL Piano MIDI dataset extracted only the tracks with instruments from the ""Piano Family"" (MIDI program numbers 1-8). This process generated a total of 9,021 unique piano MIDI files. Theses 9,021 files were then combined with other approximately 2,065 files scraped from publicly-available sources on the internet. All the files in the final collection were de-duped according to their MD5 checksum.",,,,,,
69,ADNI,Alzheimer's Disease Detection,Alzheimer's Disease Detection,"Alzheimer's Disease Detection, Graph Classification, Stable MCI vs Progressive MCI, Anomaly Detection, Explainable Artificial Intelligence (XAI), Cubic splines Image Registration","Graph, Image",,Computer Vision,"explainable-artificial-intelligence-xai-on, graph-classification-on-adni, cubic-splines-image-registration-on-adni, stable-mci-vs-progressive-mci-on-adni, alzheimer-s-disease-detection-on-adni, anomaly-detection-on-adni",,http://adni.loni.usc.edu/,https://paperswithcode.com/dataset/adni,"Alzheimer's Disease Neuroimaging Initiative (ADNI) is a multisite study that aims to improve clinical trials for the prevention and treatment of Alzheimer’s disease (AD).[1] This cooperative study combines expertise and funding from the private and public sector to study subjects with AD, as well as those who may develop AD and controls with no signs of cognitive impairment.[2] Researchers at 63 sites in the US and Canada track the progression of AD in the human brain with neuroimaging, biochemical, and genetic biological markers.[2][3] This knowledge helps to find better clinical trials for the prevention and treatment of AD. ADNI has made a global impact,[4]
 firstly by developing a set of standardized protocols to allow the comparison of results from multiple centers,[4] and secondly by its data-sharing policy which makes available all at the data without embargo to qualified researchers worldwide.[5] To date, over 1000 scientific publications have used ADNI data.[6] A number of other initiatives related to AD and other diseases have been designed and implemented using ADNI as a model.[4] ADNI has been running since 2004 and is currently funded until 2021.[7]

Source: Wikipedia, https://en.wikipedia.org/wiki/Alzheimer%27s_Disease_Neuroimaging_Initiative",2004,,,,,
70,Adult,Imputation,Imputation,"Imputation, single catogory classification, Classification, General Classification",Image,,Computer Vision,"imputation-on-adult-data-set, classification-on-adult",Creative Commons Attribution 4.0 International,https://archive.ics.uci.edu/ml/datasets/adult,https://paperswithcode.com/dataset/adult-data-set,"Data Set Information:
Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))

Prediction task is to determine whether a person makes over 50K a year.",1994,,,,,
71,Adult_Census_Income,Binary Classification,Binary Classification,"Binary Classification, Tabular Data Generation","Image, Tabular, Text",English,Computer Vision,tabular-data-generation-on-adult-census,CC0: Public Domain,https://www.kaggle.com/datasets/uciml/adult-census-income,https://paperswithcode.com/dataset/adult-census-income,"This data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over $50K a year.",1994,,,,,
72,ADVANCE,Scene Recognition,Scene Recognition,Scene Recognition,Image,,Computer Vision,,,https://akchen.github.io/ADVANCE-DATASET/,https://paperswithcode.com/dataset/advance,"The AuDio Visual Aerial sceNe reCognition datasEt (ADVANCE) is a brand-new multimodal learning dataset, which aims to explore the contribution of both audio and conventional visual messages to scene recognition. This dataset in summary contains 5075 pairs of geotagged aerial images and sounds, classified into 13 scene classes, i.e., airport, sports land, beach, bridge, farmland, forest, grassland, harbor, lake, orchard, residential area, shrub land, and train station.",,,,,,
73,Adverse_Drug_Events__ADE__Corpus,Relation Extraction,Relation Extraction,"Relation Extraction, NER, Named Entity Recognition (NER), Clinical Concept Extraction, Text Classification","Graph, Image, Text",English,Computer Vision,"named-entity-recognition-on-adverse-drug, relation-extraction-on-ade-corpus, text-classification-on-adverse-drug-events",,https://pubmed.ncbi.nlm.nih.gov/22554702/,https://paperswithcode.com/dataset/ade-corpus,"Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports.

A significant amount of information about drug-related safety issues such as adverse effects are published in medical case reports that can only be explored by human readers due to their unstructured nature. The work presented here aims at generating a systematically annotated corpus that can support the development and validation of methods for the automatic extraction of drug-related adverse effects from medical case reports. The documents are systematically double annotated in various rounds to ensure consistent annotations. The annotated documents are finally harmonized to generate representative consensus annotations. In order to demonstrate an example use case scenario, the corpus was employed to train and validate models for the classification of informative against the non-informative sentences. A Maximum Entropy classifier trained with simple features and evaluated by 10-fold cross-validation resulted in the F₁ score of 0.70 indicating a potential useful application of the corpus.",,,,,,
74,ADVETA,Text-To-SQL,Text-To-SQL,Text-To-SQL,Text,English,Natural Language Processing,,MIT,https://github.com/microsoft/ContextualSP,https://paperswithcode.com/dataset/adveta,"ADVErsarial Table perturbAtion (ADVETA) is a robustness evaluation benchmark featuring natural and realistic ATPs. It is based on three mainstream Text-to-SQL datasets, Spider, WikiSQL and WTQ.",,Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation,https://arxiv.org/pdf/2212.09994v1.pdf,,,
75,adVFed,Federated Learning,Federated Learning,Federated Learning,,,Methodology,,,,https://paperswithcode.com/dataset/advfed,"Natural Vertical Partitioned CVR Dataset for Vertical Federated Learning

This Dataset repo provides 2 industrial CVR Dataset for VFL research.",,,,,,
76,AdvGLUE,Adversarial Robustness,Adversarial Robustness,"Adversarial Robustness, Language Modelling",Text,English,Adversarial,adversarial-robustness-on-advglue,CC BY-SA 4.0,https://adversarialglue.github.io/,https://paperswithcode.com/dataset/advglue,"Adversarial GLUE (AdvGLUE) is a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations.

Description from: Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models",,,,,,
77,ADVIO,Motion Estimation,Motion Estimation,"Motion Estimation, Probabilistic Deep Learning, Optical Flow Estimation",Video,,Methodology,,,https://github.com/AaltoVision/ADVIO,https://paperswithcode.com/dataset/advio,Provides a wide range of raw sensor data that is accessible on almost any modern-day smartphone together with a high-quality ground-truth track.,,,,,,
78,AdvSuffixes,Adversarial Robustness,Adversarial Robustness,"Adversarial Robustness, Adversarial Attack",,,Adversarial,,GPL-3.0 License,https://github.com/llm-gasp/gasp/tree/main/data/advsuffixes,https://paperswithcode.com/dataset/advsuffixes,"AdvSuffixes - Information
AdvSuffixes is a curated dataset of adversarial prompts and suffixes designed to evaluate and enhance the robustness of large language models (LLMs) against adversarial attacks. By appending these suffixes to standard prompts, researchers and developers can explore and analyze how LLMs respond to potentially harmful input scenarios. This dataset is heavily inspired by AdvBench.

Dataset Structure
The dataset is organized as follows:
data/
│
├── advsuffixes/
│   ├── advsuffixes.csv      # Adversarial suffixes and their respective prompts
│   ├── advsuffixes_eval.txt # 100 additional evaluation prompts that are out-of-distribution
│
├── ...


advsuffixes.csv: The primary dataset containing pairs of adversarial suffixes and corresponding prompts.  
advsuffixes_eval.txt: A set of 100 additional evaluation prompts designed to test model robustness, that are out-of-distribution from the original 519 prompts in advsuffixes.csv. 

Details about the dataset generation have been provided in Appendix B in the supplementary materials of the paper. There are 11763 listed suffixes overall, averaging 22.6 suffixes per prompt.


License
This dataset is distributed under the GNU General Public License v3.0.",,,,,,
79,AE-110k,Attribute Mining,Attribute Mining,"Attribute Mining, Attribute Value Extraction",,,Methodology,"attribute-mining-on-ae-110k, attribute-value-extraction-on-ae-110k",,https://github.com/cubenlp/ACL19_Scaling_Up_Open_Tagging/blob/master/publish_data.txt,https://paperswithcode.com/dataset/ae-110k,"The dataset contains product information from AliExpress Sports & Entertainment category. Each attribute value in ""Item Specific"" is matched against the product title using exact string match to generate positive triples <title, attribute, value>. Negative triples <title, attribute, NULL> are randomly generated. Each triple is stored in a line and separated by \u0001.",,,,,,
80,AESI,Multimodal Emotion Recognition,Multimodal Emotion Recognition,"Multimodal Emotion Recognition, Speech Emotion Recognition","Audio, Image",,Multimodal,,,,https://paperswithcode.com/dataset/aesi,"The development of ecologically valid procedures for collecting reliable and unbiased emotional data towards computer interfaces with social and affective intelligence targeting patients with mental disorders. Following its development, presented with, the Athens Emotional States Inventory (AESI) proposes the design, recording and validation of an audiovisual database for five emotional states: anger, fear, joy, sadness and neutral. The items of the AESI consist of sentences each having content indicative of the corresponding emotion. Emotional content was assessed through a survey of 40 young participants with a questionnaire following the Latin square design. The emotional sentences that were correctly identified by 85% of the participants were recorded in a soundproof room with microphones and cameras. A preliminary validation of AESI is performed through automatic emotion recognition experiments from speech. The resulting database contains 696 recorded utterances in Greek language by 20 native speakers and has a total duration of approximately 28 min. Speech classification results yield accuracy up to 75.15% for automatically recognizing the emotions in AESI. These results indicate the usefulness of our approach for collecting emotional data with reliable content, balanced across classes and with reduced environmental variability.",,,,,,
81,AESLC,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Text Summarization, Document Summarization",Text,English,Natural Language Processing,abstractive-text-summarization-on-aeslc,,https://github.com/ryanzhumich/AESLC,https://paperswithcode.com/dataset/aeslc,To study the task of email subject line generation: automatically generating an email subject line from the email body.,,,,,,
82,aethel,Semantic Parsing,Semantic Parsing,"Semantic Parsing, CCG Supertagging",Text,English,Natural Language Processing,,,https://github.com/konstantinosKokos/aethel,https://paperswithcode.com/dataset/aethel,"A dataset of approximately 75,000  phrases and sentences, syntactically analyzed as typelogical derivations (i.e. proofs of modal intuitionistic linear logic, or programs of the corresponding λ calculus). Analyses were obtained by transforming the dependency graphs of the Lassy-Small corpus.",,,,,,
83,Aff-Wild,Gesture Recognition,Gesture Recognition,"Gesture Recognition, Emotion Recognition, Action Unit Detection","Image, Video",,Computer Vision,,Custom (non-commercial),http://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge,https://paperswithcode.com/dataset/aff-wild,"Aff-Wild is a large-scale in-the-wild dataset for valence-arousal estimation from videos with a variety of head poses, illumination conditions and occlusions.",,,,,,
84,Aff-Wild2,Emotion Recognition,Emotion Recognition,"Emotion Recognition, Facial Expression Recognition, Multi-Task Learning, Facial Expression Recognition (FER)",Image,,Computer Vision,"facial-expression-recognition-on-aff-wild2-1, facial-expression-recognition-on-aff-wild2",,https://ibug.doc.ic.ac.uk/resources/aff-wild2/,https://paperswithcode.com/dataset/aff-wild2,"Aff-Wild2 is a large-scale in-the-wild database and an extension of the Aff-Wild dataset for affect recognition. It approximately doubles the number of included video frames and the number of subjects; thus, improving the variability of the included behaviors and of the involved persons. It is the only existing in-the-wild database with annotations for all 3 main behaviour tasks.

The Aff-Wild2 is annotated in a per frame basis for the seven basic expressions (i.e., happiness, surprise, anger, disgust, fear, sadness and the neutral state), twelve action units (AUs 1,2,4,6,7,10,12,15,23,24,25, 26) and valence and arousal. In total Aff-Wild2 consists of 564 videos of around 2.8M frames with 554 subjects.  Aff-Wild2 displays a big diversity in terms of subjects' ages, ethnicities and nationalities; it has also great variations and diversities of environments.

Sources:
1)  Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface;
2)  The 6th affective behavior analysis in-the-wild (abaw) competition",,"Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",https://arxiv.org/pdf/1910.04855,,,
85,AffectNet,Valence Estimation,Valence Estimation,"Valence Estimation, Facial Expression Recognition (FER), Arousal Estimation",Image,,Computer Vision,"facial-expression-recognition-on-affectnet, valence-estimation-on-affectnet, arousal-estimation-on-affectnet",Custom (non-commercial),http://mohammadmahoor.com/affectnet/,https://paperswithcode.com/dataset/affectnet,"AffectNet is a large facial expression dataset with around 0.4 million images manually labeled for the presence of eight (neutral, happy, angry, sad, fear, surprise, disgust, contempt) facial expressions along with the intensity of valence and arousal.",,Landmark Guidance Independent Spatio-channel Attention and Complementary Context Information based Facial Expression Recognition,https://arxiv.org/abs/2007.10298,,,
86,AFHQ,Image-to-Image Translation,Image-to-Image Translation,"Image-to-Image Translation, Image Generation, Multimodal Unsupervised Image-To-Image Translation","Image, Text",English,Computer Vision,"image-generation-on-afhq-dog, image-generation-on-afhq-wild, image-generation-on-afhqv2, image-generation-on-afhq-cat, image-to-image-translation-on-afhq, multimodal-unsupervised-image-to-image-5",CC BY-NC 4.0,https://github.com/clovaai/stargan-v2,https://paperswithcode.com/dataset/afhq,"Animal FacesHQ (AFHQ) is a dataset of animal faces consisting of 15,000 high-quality images at 512 × 512 resolution. The dataset includes three domains of cat, dog, and wildlife, each providing 5000 images. By having multiple (three) domains and diverse images of various
breeds (≥ eight) per each domain, AFHQ sets a more challenging image-to-image translation problem. 
All images are vertically and horizontally aligned to have the eyes at the center. The low-quality images were discarded by human effort.",,StarGAN v2: Diverse Image Synthesis for Multiple Domains,https://arxiv.org/abs/1912.01865,5000 images,,
87,AFLW,Facial Landmark Detection,Facial Landmark Detection,"Facial Landmark Detection, Face Alignment, Low-Light Image Enhancement, Unsupervised Facial Landmark Detection, Head Pose Estimation","3D, Image",,Computer Vision,"face-alignment-on-aflw-lfpa, face-alignment-on-aflw, face-alignment-on-aflw-pifa-21-points-1, face-alignment-on-aflw-full-1, unsupervised-facial-landmark-detection-on-3, low-light-image-enhancement-on-aflw-zhang, face-alignment-on-aflw-pifa-34-points-1, head-pose-estimation-on-aflw, facial-landmark-detection-on-aflw-front, unsupervised-facial-landmark-detection-on-2, facial-landmark-detection-on-aflw-full",Custom (non-commercial),https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/,https://paperswithcode.com/dataset/aflw,"The Annotated Facial Landmarks in the Wild (AFLW) is a large-scale collection of annotated face images gathered from Flickr, exhibiting a large variety in appearance (e.g., pose, expression, ethnicity, age, gender) as well as general imaging and environmental conditions. In total about 25K faces are annotated with up to 21 landmarks per image.",,"Nose, Eyes and Ears: Head Pose Estimation by Locating Facial Keypoints",https://arxiv.org/abs/1812.00739,,,
88,AFLW2000-3D,Face Swapping,Face Swapping,"Face Swapping, Facial Landmark Detection, 3D Facial Landmark Localization, Face Alignment, 3D Face Alignment, Head Pose Estimation, 3D Face Reconstruction","3D, Image",,Computer Vision,"facial-landmark-detection-on-aflw2000-3d, face-alignment-on-aflw2000-3d, 3d-face-reconstruction-on-aflw2000-3d, 3d-facial-landmark-localization-on-aflw2000, face-alignment-on-aflw2000, head-pose-estimation-on-aflw2000, 3d-face-alignment-on-aflw2000-3d, face-swapping-on-aflw2000-3d",,http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm,https://paperswithcode.com/dataset/aflw2000-3d,AFLW2000-3D is a dataset of 2000 images that have been annotated with image-level 68-point 3D facial landmarks. This dataset is used for evaluation of 3D facial landmark detection models. The head poses are very diverse and often hard to be detected by a CNN-based face detector.,2000,,,2000 images,,
89,AfriSenti,Zero-shot Sentiment Classification,Zero-shot Sentiment Classification,Zero-shot Sentiment Classification,"Image, Text",English,Computer Vision,zero-shot-sentiment-classification-on,,https://github.com/afrisenti-semeval/afrisent-semeval-2023,https://paperswithcode.com/dataset/afrisenti,"AfriSenti is the largest sentiment analysis dataset for under-represented African languages, covering 110,000+ annotated tweets in 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and Yoruba).",,,,,,
90,AFW,Facial Landmark Detection,Facial Landmark Detection,"Facial Landmark Detection, Face Detection, Face Alignment",Image,,Computer Vision,face-detection-on-annotated-faces-in-the-wild,,https://ieeexplore.ieee.org/document/6248014,https://paperswithcode.com/dataset/afw,"AFW (Annotated Faces in the Wild) is a face detection dataset that contains 205 images with 468 faces. Each face image is labeled with at most 6 landmarks with visibility labels, as well as a bounding box.",,,,205 images,,
91,AF_Classification_from_a_Short_Single_Lead_ECG_Rec,Atrial Fibrillation Detection,Atrial Fibrillation Detection,Atrial Fibrillation Detection,Image,,Computer Vision,,,https://physionet.org/content/challenge-2017/1.0.0/,https://paperswithcode.com/dataset/af-classification-from-a-short-single-lead,"The 2017 PhysioNet/CinC Challenge aims to encourage the development of algorithms to classify, from a single short ECG lead recording (between 30 s and 60 s in length), whether the recording shows normal sinus rhythm, atrial fibrillation (AF), an alternative rhythm, or is too noisy to be classified.",2017,,,,,
92,AGB-DE,Detection of potentially void clauses,Detection of potentially void clauses,"Detection of potentially void clauses, Text Classification","Image, Text",English,Computer Vision,detection-of-potentially-void-clauses-on-agb,,https://github.com/DaBr01/AGB-DE,https://paperswithcode.com/dataset/agb-de,"AGB-DE is a legal NLP corpus for the automated detection of potentially void clauses in German standard form consumer contracts. It consists of 3,764 clauses that have been legally assessed by experts and annotated as potentially void (1) or valid (0). Additionally, each clause is annotated with a topic label.",,,,,,
93,AgeGroup_Transactions_MTPP,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Point Processes",Time Series,,Time Series,point-processes-on-agegroup-transactions-mtpp,Apache-2.0,https://huggingface.co/datasets/dllllb/age-group-prediction,https://paperswithcode.com/dataset/agegroup-transactions-mtpp,"The dataset contains historical financial transactions, including time, category and cost fields. There are 50000 clients, 205 categories and 43.7M events. The original goal was to predict the age group of the client. In this variant of the dataset, the goal is to forecast multiple future events.",,,,,,205
94,AGENDA,KG-to-Text Generation,KG-to-Text Generation,KG-to-Text Generation,Text,English,Natural Language Processing,kg-to-text-generation-on-agenda,,https://github.com/rikdz/GraphWriter,https://paperswithcode.com/dataset/agenda,Abstract GENeration DAtaset (AGENDA) is a dataset of knowledge graphs paired with scientific abstracts. The dataset consists of 40k paper titles and abstracts from the Semantic Scholar Corpus taken from the proceedings of 12 top AI conferences.,,Text Generation from Knowledge Graphs with Graph Transformers,https://arxiv.org/pdf/1904.02342v2.pdf,,,
95,aGender,Prediction Intervals,Prediction Intervals,"Prediction Intervals, Audio Classification","Audio, Image, Time Series",,Computer Vision,,,https://www.isca-speech.org/iscapad/iscapad.php?module=category&id=684,https://paperswithcode.com/dataset/agender,"The aGender corpus contains audio recordings of predefined utterances and free speech produced by humans of different age and gender. Each utterance is labeled as one of four age groups: Child, Youth, Adult, Senior, and as one of three gender classes: Female, Male and Child.",,Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data,https://arxiv.org/abs/1602.05875,,,
96,AGORA,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, 3D Hand Pose Estimation, Monocular 3D Human Pose Estimation, 3D Human Shape Estimation, 3D Human Reconstruction, 3D Multi-Person Mesh Recovery, 2D Human Pose Estimation, 3D Multi-Person Pose Estimation","3D, Image",,Computer Vision,"3d-multi-person-pose-estimation-on-agora, 3d-human-pose-estimation-on-agora, 3d-human-reconstruction-on-agora-1, 3d-multi-person-mesh-recovery-on-agora",Custom (non-commercial),https://agora.is.tue.mpg.de,https://paperswithcode.com/dataset/agora,"AGORA is a synthetic human dataset with high realism and accurate ground truth. It consists of around 14K training and 3K test images by rendering between 5 and 15 people per image using either image-based lighting or rendered 3D environments, taking care to make the images physically plausible and photoreal. In total, AGORA contains 173K individual person crops.
AGORA provides (1) SMPL/SMPL-X parameters and (2) segmentation masks for each subject in images.",,,,,training and 3K test images,
97,AG_News,Continual Pretraining,Continual Pretraining,"Continual Pretraining, Unsupervised Text Classification, Zero-Shot Text Classification, Anomaly Detection, Semi-Supervised Text Classification, Stochastic Optimization, Topic Models, Text Classification, Short Text Clustering","Image, Text",English,Computer Vision,"zero-shot-text-classification-on-ag-news, stochastic-optimization-on-ag-news, topic-models-on-ag-news, semi-supervised-text-classification-on-ag-1, text-classification-on-ag-news, anomaly-detection-on-ag-news, short-text-clustering-on-ag-news, unsupervised-text-classification-on-ag-news, continual-pretraining-on-ag-news",Custom (non-commercial),http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html,https://paperswithcode.com/dataset/ag-news,"AG News (AG’s News Corpus) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.",,https://arxiv.org/pdf/1509.01626.pdf,https://arxiv.org/pdf/1509.01626.pdf,,"training and 1,900 test samples",
98,AI2-THOR,Imitation Learning,Imitation Learning,"Imitation Learning, Visual Navigation",Image,,Computer Vision,visual-navigation-on-ai2-thor,,https://ai2thor.allenai.org/,https://paperswithcode.com/dataset/ai2-thor,"AI2-Thor is an interactive environment for embodied AI. It contains four types of scenes, including kitchen, living room, bedroom and bathroom, and each scene includes 30 rooms, where each room is unique in terms of furniture placement and item types. There are over 2000 unique objects for AI agents to interact with.",2000,Learning Object Relation Graph andTentative Policy for Visual Navigation,https://arxiv.org/abs/2007.11018,,,
99,AID,Scene Classification,Scene Classification,"Scene Classification, Object Detection In Aerial Images, Scene Recognition, Remote Sensing Image Classification, Transductive Zero-Shot Classification",Image,,Computer Vision,"transductive-zero-shot-classification-on-aid, scene-recognition-on-aid",,https://captain-whu.github.io/AID/,https://paperswithcode.com/dataset/aid,"AID is a new large-scale aerial image dataset, by collecting sample images from Google Earth imagery. Note that although the Google Earth images are post-processed using RGB renderings from the original optical aerial images, it has proven that there is no significant difference between the Google Earth images with the real optical aerial images even in the pixel-level land use/cover mapping. Thus, the Google Earth images can also be used as aerial images for evaluating scene classification algorithms.

The new dataset is made up of the following 30 aerial scene types: airport, bare land, baseball field, beach, bridge, center, church, commercial, dense residential, desert, farmland, forest, industrial, meadow, medium residential, mountain, park, parking, playground, pond, port, railway station, resort, river, school, sparse residential, square, stadium, storage tanks and viaduct. All the images are labelled by the specialists in the field of remote sensing image interpretation, and some samples of each class are shown in Fig.1. In all, the AID dataset has a number of 10000 images within 30 classes.

The images in AID are actually multi-source, as Google Earth images are from different remote imaging sensors. This brings more challenges for scene classification than the single source images like UC-Merced dataset. Moreover, all the sample images per each class in AID are carefully chosen from different countries and regions around the world, mainly in China, the United States, England, France, Italy, Japan, Germany, etc., and they are extracted at different time and seasons under different imaging conditions, which increases the intra-class diversities of the data.",,,,10000 images,,30
100,AIDA_CoNLL-YAGO,Entity Linking,Entity Linking,"Entity Linking, Entity Disambiguation, Entity Typing",,,Methodology,"entity-typing-on-aida-conll, entity-linking-on-aida-conll, entity-disambiguation-on-aida-conll",,https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads,https://paperswithcode.com/dataset/aida-conll-yago,"AIDA CoNLL-YAGO contains assignments of entities to the mentions of named entities annotated for the original CoNLL 2003 entity recognition task. The entities are identified by YAGO2 entity name, by Wikipedia URL, or by Freebase mid.",2003,,,,,
101,AIME,Music Generation,Music Generation,"Music Generation, Text-to-Music Generation","Audio, Text",English,Natural Language Processing,,Creative Commons Attribution 4.0,https://huggingface.co/datasets/disco-eth/AIME,https://paperswithcode.com/dataset/aime,"The AIME dataset contains 6,000 audio tracks generated by 12 music generation models in addition to 500 tracks from MTG-Jamendo. The prompts used to generate music are combinations of representative and diverse tags from the MTG-Jamendo dataset.",,,,,,
102,Airport,Video-Based Person Re-Identification,Video-Based Person Re-Identification,"Video-Based Person Re-Identification, Metric Learning, Person Re-Identification","Image, Video",,Computer Vision,,,http://www.northeastern.edu/alert/transitioning-technology/alert-datasets/alert-airport-re-identification-dataset/,https://paperswithcode.com/dataset/airport,"The Airport dataset is a dataset for person re-identification which consists of 39,902 images and 9,651 identities across six cameras.",,An Evaluation of Deep CNN Baselines for Scene-Independent Person Re-Identification,https://arxiv.org/abs/1805.06086,902 images,,
103,AirSim,Autonomous Vehicles,Autonomous Vehicles,"Autonomous Vehicles, Imitation Learning, Autonomous Driving",,,Methodology,,MIT,https://github.com/Microsoft/AirSim,https://paperswithcode.com/dataset/airsim,"AirSim is a simulator for drones, cars and more, built on Unreal Engine. It is open-source, cross platform, and supports software-in-the-loop simulation with popular flight controllers such as PX4 & ArduPilot and hardware-in-loop with PX4 for physically and visually realistic simulations. It is developed as an Unreal plugin that can simply be dropped into any Unreal environment. Similarly, there exists an experimental version for a Unity plugin.",,AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles,https://arxiv.org/pdf/1705.05065v2.pdf,,,
104,Air_Quality_Index,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Time Series Prediction",Time Series,,Time Series,,,https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data,https://paperswithcode.com/dataset/air-quality-index,"The AQI dataset is collected from 12 observing stations around Beijing from year 2013 to 2017. The data is accessible at The University of California, Irvine (UCI) Machine Learning Repository.",2013,,,,,
105,AISECKG,NER,NER,"NER, Ontology Matching, Knowledge Graphs",,,Methodology,,AISecKG: Knowledge Graph Dataset for Cybersecurity Education,https://github.com/garima0106/AISecKG-cybersecurity-dataset,https://paperswithcode.com/dataset/aiseckg,"Cybersecurity education is exceptionally challenging as it involves learning the complex attacks; tools and developing critical problem-solving skills to defend the systems. For a student or novice researcher in the cybersecurity domain, there is a need to design an adaptive learning strategy that can break complex tasks and concepts into simple representations. An AI-enabled automated cybersecurity education system can improve cognitive engagement and active learning. Knowledge graphs (KG) provide a visual representation in a graph that can reason and interpret from the underlying data, making them suitable for use in education and interactive learning. However, there are no publicly available datasets for the cybersecurity education domain to build such systems. The data is present as unstructured educational course material, Wiki pages, capture the flag (CTF) writeups, etc. Creating knowledge graphs from unstructured text is challenging without an ontology or annotated dataset. However, data annotation for cybersecurity needs domain experts. To address these gaps, we made three contributions in this paper. First, we propose an ontology for the cybersecurity education domain for students and novice learners. Second, we develop AISecKG, a triple dataset with cybersecurity-related entities and relations as defined by the ontology. This dataset can be used to construct knowledge graphs to teach cybersecurity and promote cognitive learning. It can also be used to build downstream applications like recommendation systems or self-learning question-answering systems for students. The dataset would also help identify malicious named entities and their probable impact. Third, using this dataset, we show a downstream application to extract custom-named entities from texts and educational material on cybersecurity.",,AISecKG: Knowledge Graph Dataset for Cybersecurity Education,https://ceur-ws.org/Vol-3433/paper6.pdf,,,
106,AISHELL-1,Language Modelling,Language Modelling,"Language Modelling, Speech Recognition","Audio, Image, Text",English,Computer Vision,speech-recognition-on-aishell-1,Apache-2,http://www.openslr.org/33/,https://paperswithcode.com/dataset/aishell-1,AISHELL-1 is a corpus for speech recognition research and building speech recognition systems for Mandarin.,,,,,,
107,AISHELL-3,Text-To-Speech Synthesis,Text-To-Speech Synthesis,Text-To-Speech Synthesis,"Audio, Text",English,Speech,,Custom (non-commercial),http://www.aishelltech.com/aishell_3,https://paperswithcode.com/dataset/aishell-3,"AISHELL-3 is a large-scale and high-fidelity multi-speaker Mandarin speech corpus which could be used to train multi-speaker Text-to-Speech (TTS) systems. The corpus contains roughly 85 hours of emotion-neutral recordings spoken by 218 native Chinese mandarin speakers and total 88035 utterances. Their auxiliary attributes such as gender, age group and native accents are explicitly marked and provided in the corpus. Accordingly, transcripts in Chinese character-level and pinyin-level are provided along with the recordings. The  word & tone transcription accuracy rate is above 98%, through professional speech annotation and strict quality inspection for tone and prosody.",,,,,,
108,AIST__,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Motion Synthesis","3D, Image, Video",,Computer Vision,"3d-human-pose-estimation-on-aist, motion-synthesis-on-aist",,https://google.github.io/aistplusplus_dataset/,https://paperswithcode.com/dataset/aist,"AIST++ is a 3D dance dataset which contains 3D motion reconstructed from real dancers paired with music. The AIST++ Dance Motion Dataset is constructed from the AIST Dance Video DB. With multi-view videos, an elaborate pipeline is designed to estimate the camera parameters, 3D human keypoints and 3D human dance motion sequences:


It provides 3D human keypoint annotations and camera parameters for 10.1M images, covering 30 different subjects in 9 views. These attributes makes it the largest and richest existing dataset with 3D human keypoint annotations.
It also contains 1,408 sequences of 3D human dance motion, represented as joint rotations along with root trajectories. The dance motions are equally distributed among 10 dance genres with hundreds of choreographies. Motion durations vary from 7.4 sec. to 48.0 sec. All the dance motions have corresponding music.",,,,1M images,,
109,AKCES-GEC,Grammatical Error Correction,Grammatical Error Correction,Grammatical Error Correction,,,Methodology,,,https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3057,https://paperswithcode.com/dataset/akces-gec,AKCES-GEC is a new dataset on grammatical error correction for Czech.,,Grammatical Error Correction in Low-Resource Scenarios,https://arxiv.org/pdf/1910.00353,,,
110,ALCE,Retrieval,Retrieval,Retrieval,,,Methodology,,MIT,https://github.com/princeton-nlp/alce,https://paperswithcode.com/dataset/alce,ALCE is a benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations.,,Enabling Large Language Models to Generate Text with Citations,https://arxiv.org/pdf/2305.14627v1.pdf,,,
111,AlexandreMotorImagery_MOABB,Within-Session Motor Imagery (right hand vs. feet),Within-Session Motor Imagery (right hand vs. feet),"Within-Session Motor Imagery (right hand vs. feet), Within-Session Motor Imagery (all classes)",,,Methodology,"within-session-motor-imagery-all-classes-on-1, within-session-motor-imagery-right-hand-vs",,http://moabb.neurotechx.com/docs/generated/moabb.datasets.AlexMI.html,https://paperswithcode.com/dataset/alexandremotorimagery-moabb,,,,,,,
112,Alexa_Point_of_View,Language Modelling,Language Modelling,"Language Modelling, Part-Of-Speech Tagging, Machine Translation, Constituency Parsing","Audio, Text",English,Natural Language Processing,machine-translation-on-alexa-point-of-view,,https://github.com/alexa/alexa-point-of-view-dataset,https://paperswithcode.com/dataset/alexa-point-of-view,"The Alexa Point of View dataset is point of view conversion dataset, a parallel corpus of messages spoken to a virtual assistant and the converted messages for delivery.
The dataset contains parallel corpus of input (input column) message and POV converted messages (output column). An example of a pair is tell @CN@ that i'll be late [\t] hi @CN@, @SCN@ would like you to know that they'll be late. The input and pov-converted output pair is tab separated. @CN@ tag is a placeholder for the contact name (receiver) and @SCN@ tag is a placeholder for source contact name (sender).
The total dataset has 46563 pairs. This data is then test/train/dev split into 6985 pairs/32594 pairs/6985 pairs.",,,,,,
113,AlgoPuzzleVQA,Multimodal Reasoning,Multimodal Reasoning,"Multimodal Reasoning, Visual Question Answering (VQA)","Image, Text",English,Reasoning,multimodal-reasoning-on-algopuzzlevqa,MIT,https://algopuzzlevqa.github.io/,https://paperswithcode.com/dataset/algopuzzlevqa,"We introduce the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning. We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills. The dataset is generated automatically from code authored by humans. All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations. It ensures that our dataset can be scaled up arbitrarily in terms of reasoning complexity and dataset size. Our investigation reveals that large language models (LLMs) such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles. The findings emphasize the challenges of integrating visual, language, and algorithmic knowledge for solving complex reasoning problems.",,,,,,
114,Alibaba_Cluster_Trace,Time Series Analysis,Time Series Analysis,"Time Series Analysis, 2D Human Pose Estimation","3D, Image, Time Series",,Time Series,2d-human-pose-estimation-on-alibaba-cluster,,https://github.com/alibaba/clusterdata,https://paperswithcode.com/dataset/alibaba-cluster-trace,"Alibaba Cluster Trace captures detailed statistics for the co-located workloads of long-running and batch jobs over a course of 24 hours. The trace consists of three parts: (1) statistics of the studied homogeneous cluster of 1,313 machines, including each machine’s hardware configuration, and the runtime {CPU, Memory, Disk} resource usage for a duration of 12 hours (the 2nd half of the 24-hour period); (2) long-running job workloads, including a trace of all container deployment requests and actions, and a resource usage trace for 12 hours; (3) co-located batch job workloads, including a trace of all batch job requests and actions, and a trace of per-instance resource usage over 24 hours.

It also has a second version of traces cluster-trace-v2018 that includes about 4,000 machines in a period of 8 days. Besides having larger scaler than trace-v2017, this piece trace also contains the DAG information of the production batch workloads.",,,,,,
115,AliMeeting,Speech Recognition,Speech Recognition,"Speech Recognition, Speaker Diarization","Audio, Image, Text",English,Speech,speaker-diarization-on-alimeeting,MIT,https://www.alibabacloud.com/zh/m2met-alimeeting,https://paperswithcode.com/dataset/alimeeting%0A,"AliMeeting corpus consists of 120 hours of recorded Mandarin meeting data, including far-field data collected by 8-channel microphone array as well as near-field data collected by headset microphone. Each meeting session is composed of 2-4 speakers with different speaker overlap ratio, recorded in rooms with different size.",,,,,,
116,AlpacaEval-TH,Chatbot,Chatbot,Chatbot,,,Methodology,,Apache-2.0 license,https://github.com/hy5468/TransLLM,https://paperswithcode.com/dataset/alpacaeval-th,AlpacaEval  in Thai.,,,,,,
117,AlpacaEval,Text Generation,Text Generation,"Text Generation, Human Judgment Correlation, Chatbot","Image, Text",English,Computer Vision,"chatbot-on-alpacaeval, text-generation-on-alpacaeval",,https://github.com/tatsu-lab/alpaca_eval,https://paperswithcode.com/dataset/alpacaeval,"The AlpacaEval set contains 805 instructions form self-instruct, open-assistant, vicuna, koala, hh-rlhf. Those were selected so that the AlpacaEval ranking of models on the AlpacaEval set would be similar to the ranking on the Alpaca demo data.",,,,,,
118,Alpaca_Data_Galician,Text Generation,Text Generation,"Text Generation, Conversational Question Answering, Conversational Response Generation, Instruction Following",Text,English,Natural Language Processing,,Apache-2.0,https://huggingface.co/datasets/irlab-udc/alpaca_data_galician,https://paperswithcode.com/dataset/alpaca-data-galician,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
119,AMALGUM,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Discourse Parsing, Nested Named Entity Recognition, Discourse Segmentation, Part-Of-Speech Tagging, Dependency Parsing, Nested Mention Recognition, Lemmatization, Named Entity Recognition (NER)","Audio, Image, Text",English,Computer Vision,,CC-BY-NC-SA,https://gucorpling.org/gum/amalgum.html,https://paperswithcode.com/dataset/amalgum,"AMALGUM is a machine annotated multilayer corpus following the same design and annotation layers as GUM, but substantially larger (around 4M tokens). The goal of this corpus is to close the gap between high quality, richly annotated, but small datasets, and the larger but shallowly annotated corpora that are often scraped from the Web.",,,,,,
120,AMASS,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Pose Estimation, Human Pose Forecasting","3D, Image, Time Series",,Computer Vision,human-pose-forecasting-on-amass,"Custom (research-only, non-commercial)",https://amass.is.tue.mpg.de/,https://paperswithcode.com/dataset/amass,"AMASS is a large database of human motion unifying different optical marker-based motion capture datasets by representing them within a common framework and parameterization. AMASS is readily useful for animation, visualization, and generating training data for deep learning.",,,,,,
121,Amazon-Book,Collaborative Filtering,Collaborative Filtering,"Collaborative Filtering, Recommendation Systems",,,Methodology,"recommendation-systems-on-amazon-book, collaborative-filtering-on-amazon-book",,,https://paperswithcode.com/dataset/amazon-book,N/A,,,,,,
122,Amazon-Fraud,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Fraud Detection, Graph Mining, Node Classification","Graph, Image",,Computer Vision,"node-classification-on-amazon-fraud, fraud-detection-on-amazon-fraud",Apache-2.0,https://github.com/YingtongDou/CARE-GNN,https://paperswithcode.com/dataset/amazon-fraud,"Amazon-Fraud is a multi-relational graph dataset built upon the Amazon review dataset, which can be used in evaluating graph-based node classification, fraud detection, and anomaly detection models.


Dataset Statistics

| # Nodes  |  %Fraud Nodes  (Class=1)|
|-------|--------|
|  11,944 | 9.5   | 

| Relation  | # Edges |
|--------|--------|
  |   U-P-U    |  175,608 |
 |  U-S-U  |  3,566,479  |
|  U-V-U  |  1,036,737 |
 |  All |  4,398,392  |


Graph Construction

The Amazon dataset includes product reviews under the Musical Instruments category. Similar to this paper, we label users with more than 80% helpful votes as benign entities and users with less than 20% helpful votes as fraudulent entities. we conduct a fraudulent user detection task on the Amazon-Fraud dataset, which is a binary classification task. We take 25 handcrafted features from this paper as the raw node features for Amazon-Fraud. We take users as nodes in the graph and design three relations: 1) U-P-U: it connects users reviewing at least one same product; 2) U-S-V: it connects users having at least one same star rating within one week; 3) U-V-U: it connects users with top 5% mutual review text similarities (measured by TF-IDF) among all users.

To download the dataset, please visit this Github repo. For any other questions, please email ytongdou(AT)gmail.com for inquiry.",,paper,https://arxiv.org/abs/2005.10150,,,
123,Amazon-Google,Blocking,Blocking,"Blocking, Entity Resolution, Data Integration",,,Methodology,"blocking-on-amazon-google, entity-resolution-on-amazon-google",Creative Commons license,https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution,https://paperswithcode.com/dataset/amazon-google,"The Amazon-Google dataset for entity resolution derives from the online retailers Amazon.com and  the product search service of Google accessible through the Google Base Data API. The dataset contains 1363 entities from amazon.com and 3226 google products as well as a gold standard (perfect mapping) with 1300 matching record pairs between the two data sources. The common attributes between the two data sources are: product name, product description, manufacturer and price.

The dataset was initially published in the repository of the Database Group of the University of Leipzig: https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution

To enable the reproducibility of the results and the comparability of the performance of different matchers on the Amazon-Google matching task, the dataset was split into fixed train, validation and test sets. The fixed splits are provided in the CompERBench repository:

http://data.dws.informatik.uni-mannheim.de/benchmarkmatchingtasks/index.html",,,,,,
124,Amazon-Sports,Sequential Recommendation,Sequential Recommendation,"Sequential Recommendation, Recommendation Systems",,,Methodology,sequential-recommendation-on-amazon-sports,,https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/,https://paperswithcode.com/dataset/amazon-sports,"Amazon-Sports is a sub-category of the Amazon dataset, which contains a series of product reviews crawled from Amazon.com.",,,,,,
125,AmazonQA,Text Generation,Text Generation,"Text Generation, Community Question Answering, Question Answering",Text,English,Natural Language Processing,,,https://github.com/amazonqa/amazonqa,https://paperswithcode.com/dataset/amazonqa,"AmazonQA consists of 923k questions, 3.6M answers and 14M reviews across 156k products. Building on the well-known Amazon dataset, additional annotations are collected, marking each question as either answerable or unanswerable based on the available reviews.",,AmazonQA: A Review-Based Question Answering Task,https://arxiv.org/pdf/1908.04364.pdf,,,
126,Amazon_Baby,Multimodal Recommendation,Multimodal Recommendation,"Multimodal Recommendation, Multi-modal Recommendation",,,Multimodal,"multimodal-recommendation-on-amazon-baby, multi-modal-recommendation-on-amazon-baby",,https://cseweb.ucsd.edu/~jmcauley/datasets/amazon/links.html,https://paperswithcode.com/dataset/amazon-baby,"This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).",,,,,,
127,Amazon_Beauty,Multimodal Recommendation,Multimodal Recommendation,"Multimodal Recommendation, Sequential Recommendation, Recommendation Systems",,,Multimodal,"recommendation-systems-on-amazon-beauty, multimodal-recommendation-on-amazon-beauty, sequential-recommendation-on-amazon-beauty-1",,https://cseweb.ucsd.edu/~jmcauley/datasets/amazon/links.html,https://paperswithcode.com/dataset/amazon-beauty,"This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).",,,,,,
128,Amazon_Digital_Music,Multimodal Recommendation,Multimodal Recommendation,Multimodal Recommendation,,,Multimodal,multimodal-recommendation-on-amazon-digital,,https://cseweb.ucsd.edu/~jmcauley/datasets/amazon/links.html,https://paperswithcode.com/dataset/amazon-digital-music,"This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).",,,,,,
129,Amazon_Men,Sequential Recommendation,Sequential Recommendation,"Sequential Recommendation, Recommendation Systems",,,Methodology,"sequential-recommendation-on-amazon-men, recommendation-systems-on-amazon-men",,,https://paperswithcode.com/dataset/amazon-men,This datasets is a subset of the Amazon reviews dataset which contain Men related products,,,,,,
130,Amazon_MTPP,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Point Processes",Time Series,,Time Series,point-processes-on-amazon-mtpp,Apache-2.0,https://huggingface.co/datasets/easytpp/amazon,https://paperswithcode.com/dataset/amazon-mtpp,"The dataset includes time-stamped user product reviews behavior from January, 2008 to October, 2018. Each user has a sequence of produce review events with each event containing the timestamp and category of the reviewed product, with each category corresponding to an event type.",2008,,,,,
131,Amazon_Office_Products,Multimodal Recommendation,Multimodal Recommendation,Multimodal Recommendation,,,Multimodal,multimodal-recommendation-on-amazon-office,,https://cseweb.ucsd.edu/~jmcauley/datasets/amazon/links.html,https://paperswithcode.com/dataset/amazon-office-products,"This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).",,,,,,
132,Amazon_Product_Data,Wildly Unsupervised Domain Adaptation,Wildly Unsupervised Domain Adaptation,"Wildly Unsupervised Domain Adaptation, Topic Classification, Stochastic Optimization, Recommendation Systems, Domain Adaptation, Text Classification","Image, Text",English,Computer Vision,"domain-adaptation-on-noisy-amazon-45, topic-classification-on-amazon-product-data, recommendation-systems-on-amazon-product-data, wildly-unsupervised-domain-adaptation-on-3, domain-adaptation-on-noisy-amazon-20, wildly-unsupervised-domain-adaptation-on-2",,http://jmcauley.ucsd.edu/data/amazon/,https://paperswithcode.com/dataset/amazon-product-data,"This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014.

This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).",1996,,,,,
133,Amazon_Review,Multimodal Recommendation,Multimodal Recommendation,"Multimodal Recommendation, Sequential Recommendation, Recommendation Systems",,,Multimodal,"recommendation-systems-on-amazon-games, recommendation-systems-on-amazon-fashion, sequential-recommendation-on-amazon-beauty-1, recommendation-systems-on-amazon-beauty, sequential-recommendation-on-amazon-men, multimodal-recommendation-on-amazon-beauty, recommendation-systems-on-amazon-men",,https://nijianmo.github.io/amazon/index.html,https://paperswithcode.com/dataset/amazon-review,"Amazon Review is a dataset to tackle the task of identifying whether the sentiment of a product review is positive or negative. This dataset includes reviews from four different merchandise categories: Books (B) (2834 samples), DVDs (D) (1199 samples), Electronics (E) (1883 samples), and Kitchen and housewares (K) (1755 samples).",,,,2834 samples,,
134,Amazon_Toys___Games,Multimodal Recommendation,Multimodal Recommendation,Multimodal Recommendation,,,Multimodal,multimodal-recommendation-on-amazon-toys,,https://cseweb.ucsd.edu/~jmcauley/datasets/amazon/links.html,https://paperswithcode.com/dataset/amazon-toys-games,"This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).",,,,,,
135,AMiner,Link Prediction,Link Prediction,"Link Prediction, Research Performance Prediction, Node Classification","Image, Time Series",,Computer Vision,research-performance-prediction-on-aminer,Multiple licenses,https://www.aminer.org/data,https://paperswithcode.com/dataset/aminer,"The AMiner Dataset is a collection of different relational datasets. It consists of a set of relational networks such as citation networks, academic social networks or topic-paper-autor networks among others.",,,,,,
136,AMI_Meeting_Corpus,Abstractive Dialogue Summarization,Abstractive Dialogue Summarization,"Abstractive Dialogue Summarization, Meeting Summarization",Text,English,Natural Language Processing,meeting-summarization-on-ami-meeting-corpus,,https://groups.inf.ed.ac.uk/ami/corpus,https://paperswithcode.com/dataset/ami-meeting-corpus,"The AMI Meeting Corpus is a multi-modal data set comprising 100 hours of meeting recordings. It has been meticulously curated for research purposes and includes various modes of data capture. Let me provide you with more details:


Purpose and Context:
The corpus was created in the context of a project that aims to develop meeting browsing technology.

Eventually, it will be publicly released for use by researchers and practitioners.



Data Composition:


100 hours of recorded meetings are included.
The data is collected from various domains and scenarios.
Around two-thirds of the data involves participants playing different roles in a design team, taking a design project from kick-off to completion over the course of a day.

The remaining data consists of naturally occurring meetings.



Modalities and Annotations:


The corpus includes synchronized recording devices such as close-talking and far-field microphones, individual and room-view video cameras, projection, a whiteboard, and individual pens.

Annotations cover various phenomena, including orthographic transcription, dialog acts, and head movement.



Research Applications:


Although initially designed for meeting browsing technology, the AMI Meeting Corpus is useful for a wide range of research areas.
Researchers engaged in video processing can access higher resolution videos.

Source: Conversation with Bing, 3/16/2024
(1) AMI Corpus - University of Edinburgh. https://groups.inf.ed.ac.uk/ami/corpus/.
(2) The AMI Meeting Corpus: A Pre-announcement. https://www.research.ed.ac.uk/en/publications/the-ami-meeting-corpus-a-pre-announcement.
(3) The AMI Meeting Corpus: A Pre-announcement | SpringerLink. https://link.springer.com/chapter/10.1007/11677482_3.
(4) The AMI Meeting Corpus: A Pre-announcement — University of Twente .... https://research.utwente.nl/en/publications/the-ami-meeting-corpus-a-pre-announcement.",2024,,,,,
137,AMOS,Medical Image Segmentation,Medical Image Segmentation,"Medical Image Segmentation, 3D Semantic Segmentation","3D, Image",,Medical,medical-image-segmentation-on-amos,,https://zenodo.org/record/7262581,https://paperswithcode.com/dataset/amos,"Despite the considerable progress in automatic abdominal multi-organ segmentation from CT/MRI scans in recent years, a comprehensive evaluation of the models' capabilities is hampered by the lack of a large-scale benchmark from diverse clinical scenarios. Constraint by the high cost of collecting and labeling 3D medical data, most of the deep learning models to date are driven by datasets with a limited number of organs of interest or samples, which still limits the power of modern deep models and makes it difficult to provide a fully comprehensive and fair estimate of various methods. To mitigate the limitations, we present AMOS, a large-scale, diverse, clinical dataset for abdominal organ segmentation. AMOS provides 500 CT and 100 MRI scans collected from multi-center, multi-vendor, multi-modality, multi-phase, multi-disease patients, each with voxel-level annotations of 15 abdominal organs, providing challenging examples and test-bed for studying robust segmentation algorithms under diverse targets and scenarios. We further benchmark several state-of-the-art medical segmentation models to evaluate the status of the existing methods on this new challenging dataset. We have made our datasets, benchmark servers, and baselines publicly available, and hope to inspire future research. Information can be found at https://amos22. grand-challenge. org.",,,,,"valuation of the models' capabilities is hampered by the lack of a large-scale benchmark from diverse clinical scenarios. Constraint by the high cost of collecting and labeling 3D medical data, most of the deep learning models to date are driven by datasets with a limited number of organs of interest or samples",
138,Analysing_state-backed_propaganda_websites__a_new_,Misinformation,Misinformation,"Misinformation, Propaganda detection",Image,,Computer Vision,,Creative Commons Attribution 4.0 International,https://zenodo.org/records/10007383,https://paperswithcode.com/dataset/analysing-state-backed-propaganda-websites-a,"This paper analyses two hitherto unstudied sites sharing state-backed disinformation, Reliable Recent News (rrn.world) and WarOnFakes (waronfakes.com), which publish content in Arabic, Chinese, English, French, German, and Spanish.",,,,,,
139,AND_Dataset,Handwriting Verification,Handwriting Verification,Handwriting Verification,,,Methodology,handwriting-verification-on-and-dataset,,https://github.com/mshaikh2/HDL_Forensics,https://paperswithcode.com/dataset/and-dataset,The AND Dataset contains 13700 handwritten samples and 15 corresponding expert examined features for each sample. The dataset is released for public use and the methods can be extended to provide explanations on other verification tasks like face verification and bio-medical comparison. This dataset can serve as the basis and benchmark for future research in explanation based handwriting verification.,,,,,,
140,ANETAC,Transliteration,Transliteration,Transliteration,,,Methodology,,,https://github.com/MohamedHadjAmeur/ANETAC,https://paperswithcode.com/dataset/anetac,"An English-Arabic named entity transliteration and classification dataset built from freely available parallel translation corpora. The dataset contains 79,924 instances, each instance is a triplet (e, a, c), where e is the English named entity, a is its Arabic transliteration and c is its class that can be either a Person, a Location, or an Organization. The ANETAC dataset is mainly aimed for the researchers that are working on Arabic named entity transliteration, but it can also be used for named entity classification purposes.",,,,924 instances,,
141,ANIMAL,Learning with noisy labels,Learning with noisy labels,Learning with noisy labels,,,Methodology,learning-with-noisy-labels-on-animal,,https://dm.kaist.ac.kr/datasets/animal-10n/,https://paperswithcode.com/dataset/animal,"10 classes with 50, 000 training and 5, 000 testing images. Please note that, in ANIMAL10N, noisy labels were injected naturally by human mistakes, where its noise rate was estimated at 8%.",,,,,"training and 5, 000 testing images",10
142,Animals-10,Representation Learning,Representation Learning,"Representation Learning, Image Classification, Multi-Animal Tracking with identification","Image, Video",,Computer Vision,representation-learning-on-animals-10,,,https://paperswithcode.com/dataset/animals-10,"It contains about 28K medium quality animal images belonging to 10 categories: dog, cat, horse, spyder, butterfly, chicken, sheep, cow, squirrel, and elephant.

All the images have been collected from ""google images"" and have been checked by humans. There is some erroneous data to simulate real conditions (eg. images taken by users of your app).
The main directory is divided into folders, one for each category. The image count for each category varies from 2K to 5 K units.",,,,,,10
143,Animal_Kingdom,Animal Pose Estimation,Animal Pose Estimation,"Animal Pose Estimation, Pose Estimation, Animal Action Recognition, Action Recognition, 2D Pose Estimation, Multi-Label Learning, Video Grounding, Long-tail Learning","3D, Image, Video",,Computer Vision,"2d-pose-estimation-on-animal-kingdom, action-recognition-on-animal-kingdom",,https://sutdcv.github.io/Animal-Kingdom,https://paperswithcode.com/dataset/animal-kingdom,"Animal Kingdom is a large and diverse dataset that provides multiple annotated tasks to enable a more thorough understanding of natural animal behaviors. The wild animal footage used in the dataset records different times of the day in an extensive range of environments containing variations in backgrounds, viewpoints, illumination and weather conditions. More specifically, the dataset contains 50 hours of annotated videos to localize relevant animal behavior segments in long videos for the video grounding task, 30K video sequences for the fine-grained multi-label action recognition task, and 33K frames for the pose estimation task, which correspond to a diverse range of animals with 850 species across 6 major animal classes.",,,,,,
144,AnlamVer,Word Embeddings,Word Embeddings,"Word Embeddings, Word Similarity",,,Methodology,,,http://www.gokhanercan.com/resources/anlamver.aspx,https://paperswithcode.com/dataset/anlamver,"In this paper, we present AnlamVer, which is a semantic model evaluation dataset for Turkish designed to evaluate word similarity and word relatedness tasks while discriminating those two relations from each other. Our dataset consists of 500 word-pairs annotated by 12 human subjects, and each pair has two distinct scores for similarity and relatedness. Word-pairs are selected to enable the evaluation of distributional semantic models by multiple attributes of words and word-pair relations such as frequency, morphology, concreteness and relation types (e.g., synonymy, antonymy). Our aim is to provide insights to semantic model researchers by evaluating models in multiple attributes. We balance dataset word-pairs by their frequencies to evaluate the robustness of semantic models concerning out-of-vocabulary and rare words problems, which are caused by the rich derivational and inflectional morphology of the Turkish language.
(from the original abstract of the dataset paper)",,,,,,
145,ANLI,Natural Language Inference,Natural Language Inference,Natural Language Inference,Text,English,Natural Language Processing,"natural-language-inference-on-anli, natural-language-inference-on-anli-r3, natural-language-inference-on-anli-test",CC BY-NC 4.0,https://github.com/facebookresearch/anli,https://paperswithcode.com/dataset/anli,"The Adversarial Natural Language Inference (ANLI, Nie et al.) is a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. Particular, the data is selected to be difficult to the state-of-the-art models, including BERT and RoBERTa.",,The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding,https://arxiv.org/abs/2002.07972,,,
146,AnthroProtect,Explainable artificial intelligence,Explainable artificial intelligence,"Explainable artificial intelligence, Classification",Image,,Computer Vision,,Creative Commons Attribution-NonCommercial-ShareAlike 3.0,http://rs.ipb.uni-bonn.de/data/,https://paperswithcode.com/dataset/anthroprotect,"For a detailed description, we refer to Section 3 in our research article.",,,,,,
147,ANTIQUE,Language Modelling,Language Modelling,"Language Modelling, Community Question Answering, Question Answering",Text,English,Natural Language Processing,,,https://ciir.cs.umass.edu/downloads/Antique/,https://paperswithcode.com/dataset/antique,"ANTIQUE is a collection of 2,626 open-domain non-factoid questions from a diverse set of categories. The dataset  contains 34,011 manual relevance annotations. The questions were asked by real users in a community question answering service, i.e., Yahoo! Answers. Relevance judgments for all the answers to each question were collected through crowdsourcing.",,,,,,
148,AO-CLEVr,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Compositional Zero-Shot Learning",,,Methodology,,,https://github.com/nv-research-israel/causal_comp,https://paperswithcode.com/dataset/ao-clevr,"AO-CLEVr is a new synthetic-images dataset containing images of ""easy"" Attribute-Object categories, based on the CLEVr. AO-CLEVr has attribute-object pairs created from 8 attributes: { red, purple, yellow, blue, green, cyan, gray, brown } and 3 object shapes {sphere, cube, cylinder}, yielding 24 attribute-object pairs. Each pair consists of 7500 images. Each image has a single object that consists of the attribute-object pair. The object is randomly assigned one of two sizes (small/large), one of two materials (rubber/metallic), a random position, and random lightning according to CLEVr defaults.",,,,7500 images,,
149,Apnea-ECG,Sleep apnea detection,Sleep apnea detection,Sleep apnea detection,Image,,Computer Vision,sleep-apnea-detection-on-apnea-ecg,Open Data Commons Attribution License v1.0,https://physionet.org/content/apnea-ecg/1.0.0/,https://paperswithcode.com/dataset/apnea-ecg,"The data consist of 70 records, divided into a learning set of 35 records (a01 through a20, b01 through b05, and c01 through c10), and a test set of 35 records (x01 through x35), all of which may be downloaded from this page. Recordings vary in length from slightly less than 7 hours to nearly 10 hours each. Each recording includes a continuous digitized ECG signal, a set of apnea annotations (derived by human experts on the basis of simultaneously recorded respiration and related signals), and a set of machine-generated QRS annotations (in which all beats regardless of type have been labeled normal). In addition, eight recordings (a01 through a04, b01, and c01 through c03) are accompanied by four additional signals (Resp C and Resp A, chest and abdominal respiratory effort signals obtained using inductance plethysmography; Resp N, oronasal airflow measured using nasal thermistors; and SpO2, oxygen saturation).",,,,70 records,,
150,ApolloCar3D,3D Pose Estimation,3D Pose Estimation,"3D Pose Estimation, Pose Estimation, 3D Shape Reconstruction From A Single 2D Image, Keypoint Detection, 3D Car Instance Understanding, Autonomous Vehicles, Car Pose Estimation, 3D Shape Reconstruction, 6D Pose Estimation, Vehicle Pose Estimation, 3D Reconstruction, Vehicle Key-Point and Orientation Estimation, 6D Pose Estimation using RGB, Autonomous Driving","3D, Image",,Computer Vision,"3d-shape-reconstruction-from-a-single-2d, 3d-shape-reconstruction-on-apollocar3d, vehicle-pose-estimation-on-apollocar3d, autonomous-driving-on-apollocar3d, 3d-pose-estimation-on-apollocar3d, 3d-car-instance-understanding-on-apollocar3d, 6d-pose-estimation-using-rgb-on-apollocar3d, keypoint-detection-on-apollocar3d, pose-estimation-on-apollocar3d, car-pose-estimation-on-apollocar3d, 3d-reconstruction-on-apollocar3d, 6d-pose-estimation-on-apollocar3d, vehicle-key-point-and-orientation-estimation, autonomous-vehicles-on-apollocar3d",Custom (non-commercial),http://apolloscape.auto/car_instance.html,https://paperswithcode.com/dataset/apollocar3d,"ApolloCar3DT is a dataset that contains 5,277 driving images and over 60K car instances, where each car is fitted with an industry-grade 3D CAD model with absolute model size and semantically labelled keypoints. This dataset is above 20 times larger than PASCAL3D+ and KITTI, the current state-of-the-art.",,cersar/3D_detection,https://arxiv.org/abs/1612.00496,,,
151,ApolloScape,Motion Segmentation,Motion Segmentation,"Motion Segmentation, Semantic Segmentation, Image Inpainting, Trajectory Prediction, Object Detection, Autonomous Driving","Image, Time Series, Video",,Computer Vision,"motion-segmentation-on-apolloscape, semantic-segmentation-on-apolloscape, image-inpainting-on-apolloscape, trajectory-prediction-on-apolloscape, test-results-on-apolloscape",Custom (research-only),http://apolloscape.auto/,https://paperswithcode.com/dataset/apolloscape-1,"ApolloScape is a large dataset consisting of over 140,000 video frames (73 street scene videos) from various locations in China under varying weather conditions. Pixel-wise semantic annotation of the recorded data is provided in 2D, with point-wise semantic annotation in 3D for 28 classes. In addition, the dataset contains lane marking annotations in 2D.",,A2D2: Audi Autonomous Driving Dataset,https://arxiv.org/abs/2004.06320,,,28
152,Appdroid,Android Malware Detection,Android Malware Detection,Android Malware Detection,Image,,Computer Vision,,CC: BY-NC,https://gitlab.com/serralba/androidmaldet_comparative,https://paperswithcode.com/dataset/appdroid,"Dataset used for the paper entitled ""Towards a Fair Comparison and Realistic Evaluation Framework of Android Malware Detectors based on Static Analysis and Machine Learning"". 

Desciption
The dataset consist of 100 monthly samples of each class (malware, goodware and greyware) during the period starting from January 2012 to December 2019. We resorted the VTD values of apps for labeling. In particular, we used a VTD≥7 to label malware, VTD=0 for goodware and apps with a 1≤VTD≤6 rating were labeled as greyware. In total, our dataset consists of 28,800 app samples.

The directory  ""dataset"" contains a file with the SHA hashes of the APKs that comprise each of the three classes (goodware, malware and greyware). All these APKs were originally downloaded from AndroZoo. To download the APKs in our dataset, you can use the AZ tool.

Authors and acknowledgment
If you use this dataset, please cite:

@article{molinacoronado2022towards,
    title = {Towards a Fair Comparison and Realistic Evaluation Framework of Android Malware Detectors based on Static Analysis and Machine Learning},
    author = {Borja Molina-Coronado and Usue Mori and Alexander Mendiburu and Jose Miguel-Alonso},
    journal = {Computers &amp; Security},
    pages = {102996},
    year = {2022},
    issn = {0167-4048},
    doi = {https://doi.org/10.1016/j.cose.2022.102996},
    url = {https://www.sciencedirect.com/science/article/pii/S0167404822003881}
}

License
This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. For more information check the link below:

http://creativecommons.org/licenses/by-nc/4.0/",2012,,,,"values of apps for labeling. In particular, we used a VTD≥7 to label malware, VTD=0 for goodware and apps with a 1≤VTD≤6 rating were labeled as greyware. In total, our dataset consists of 28,800 app samples",
153,Appliances_Energy,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Time Series Regression",Time Series,,Time Series,,Creative Commons Attribution 4.0 International,https://archive.ics.uci.edu/dataset/374/appliances+energy+prediction,https://paperswithcode.com/dataset/appliances-energy,"This dataset details the energy consumption of appliances in a low-energy building over 4.5 months. Data was collected at 10-minute intervals.

For more information about the house, data collection, R scripts and figures, please refer to the paper and the following GitHub repository:
https://github.com/LuisM78/Appliances-energy-prediction-data",,,,,,
154,APPS,Code Generation,Code Generation,Code Generation,Text,English,Natural Language Processing,code-generation-on-apps,,https://github.com/hendrycks/apps,https://paperswithcode.com/dataset/apps,"The APPS dataset consists of problems collected from different open-access coding websites such as Codeforces, Kattis, and more. The APPS benchmark attempts to mirror how humans programmers are evaluated by posing coding problems in unrestricted natural language and evaluating the correctness of solutions. The problems range in difficulty from introductory to collegiate competition level and measure coding ability as well as problem-solving. 

The Automated Programming Progress Standard, abbreviated APPS, consists of 10,000 coding problems in total, with 131,836 test cases for checking solutions and 232,444 ground-truth solutions written by humans. Problems can be complicated, as the average length of a problem is 293.2 words. The data are split evenly into training and test sets, with 5,000 problems each. In the test set, every problem has multiple test cases, and the average number of test cases is 21.2. Each test case is specifically designed for the corresponding problem, enabling us to rigorously evaluate program functionality.",,Measuring Coding Challenge Competence With APPS,https://arxiv.org/pdf/2105.09938v1.pdf,,,
155,Apron_Dataset,Robust Object Detection,Robust Object Detection,"Robust Object Detection, Small Object Detection, Scene Understanding, Benchmarking, Fine-Grained Image Classification, Object Detection, Domain Adaptation, Autonomous Driving",Image,,Computer Vision,,Custom (non-commercial),https://github.com/apronai/apron-dataset,https://paperswithcode.com/dataset/apron-dataset,The Apron Dataset focuses on training and evaluating classification and detection models for airport-apron logistics. In addition to bounding boxes and object categories the dataset is enriched with meta parameters to quantify the models’ robustness against environmental influences.,,,,,,
156,aPY,Few-Shot Image Classification,Few-Shot Image Classification,"Few-Shot Image Classification, Zero-Shot Learning, Concept-based Classification, Generalized Zero-Shot Learning",Image,,Computer Vision,"generalized-zero-shot-learning-on-apy-0-shot, generalized-zero-shot-learning-on-apy, zero-shot-learning-on-apy-0-shot, concept-based-classification-on-apy, few-shot-image-classification-on-apy-0-shot",,https://vision.cs.uiuc.edu/attributes/,https://paperswithcode.com/dataset/apy,"aPY is a coarse-grained dataset composed of 15339 images from 3 broad categories (animals, objects and vehicles), further divided into a total of 32 subcategories (aeroplane, …, zebra).",,From Classical to Generalized Zero-Shot Learning: a Simple Adaptation Process,https://arxiv.org/abs/1809.10120,15339 images,,
157,AQUA-RAT,Natural Language Understanding,Natural Language Understanding,"Natural Language Understanding, Program induction, Natural Language Inference",Text,English,Natural Language Processing,,Apache-2.0,https://github.com/deepmind/AQuA,https://paperswithcode.com/dataset/aqua-rat,"Algebra Question Answering with Rationales (AQUA-RAT) is a dataset that contains algebraic word problems with rationales. The dataset consists of about 100,000 algebraic word problems with natural language rationales. Each problem is a json object consisting of four parts:
* question - A natural language definition of the problem to solve
* options - 5 possible options (A, B, C, D and E), among which one is correct
* rationale - A natural language description of the solution to the problem
* correct - The correct option",,,,,,
158,AQUAINT,Entity Disambiguation,Entity Disambiguation,Entity Disambiguation,,,Methodology,entity-disambiguation-on-aquaint,,https://catalog.ldc.upenn.edu/LDC2002T31,https://paperswithcode.com/dataset/aquaint,"The AQUAINT Corpus consists of newswire text data in English, drawn from three sources: the Xinhua News Service (People's Republic of China), the New York Times News Service, and the Associated Press Worldstream News Service. It was prepared by the LDC for the AQUAINT Project, and will be used in official benchmark evaluations conducted by National Institute of Standards and Technology (NIST).",,,,,,
159,aquamuse,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Multi-Document Summarization, Document Summarization",Text,English,Natural Language Processing,,,https://github.com/google-research-datasets/aquamuse,https://paperswithcode.com/dataset/aquamuse,"5,519 query-based summaries, each associated with an average of 6 input documents selected from an index of 355M documents from Common Crawl.",,,,355M documents,,
160,Arabic-ToD,Task-Oriented Dialogue Systems,Task-Oriented Dialogue Systems,"Task-Oriented Dialogue Systems, Conversational Response Generation, End-To-End Dialogue Modelling, Dialogue Generation",Text,English,Natural Language Processing,,,https://github.com/En-J-A/Arabic-TOD,https://paperswithcode.com/dataset/arabic-tod,"The Arabic-TOD dataset is based on the BiToD dataset.
Of the 3,689 BiToD-English dialogues, 1,500 dialogues (30,000 utterances) were translated into Arabic.
We translated the task-related keywords such as cuisine, dietary restrictions, and price-level for the
restaurant domain, price-level for the hotel domain, type, and price-level for the attraction domain, day,
weather, and city for the weather domain. We keep the rest of values without translation, like hotels’ and
restaurants’ names, locations, and addresses. These values are real  entities in Hong Kong city (literals),
and most of them contain Chinese words written in English, therefore they have not been translated. According to
the slot-values in the Arabic-TOD dataset, we used the slots names as they are in English and translated their
corresponding values, except the entities in Hong Kong city since the Arabic-TOD dataset supports codeswitching. 

We did not translate the 'UserTask' for all dialogues, since it is not important in developing the system.
It is just as a summarization of the dialogue contents.",,,,,,
161,Arabic_Speech_Commands_Dataset,Small-Footprint Keyword Spotting,Small-Footprint Keyword Spotting,"Small-Footprint Keyword Spotting, Speech Recognition","Audio, Image, Text",English,Computer Vision,,Creative Commons Attribution 4.0 International,https://github.com/abdulkaderghandoura/arabic-speech-commands-dataset,https://paperswithcode.com/dataset/arabic-speech-commands-dataset,"This dataset is designed to help train simple machine learning models that serve educational and research purposes in the speech recognition domain, mainly for keyword spotting tasks.

Dataset Description
Our dataset is a list of pairs (x, y), where x is the input speech signal, and y is the corresponding keyword. The final dataset consists of 12000 such pairs, comprising 40 keywords. Each audio file is one-second in length sampled at 16 kHz. We have 30 participants, each of them recorded 10 utterances for each keyword. Therefore, we have 300 audio files for each keyword in total (30 * 10 * 40 = 12000), and the total size of all the recorded keywords is ~384 MB. The dataset also contains several background noise recordings we obtained from various natural sources of noise. We saved these audio files in a separate folder with the name background_noise and a total size of ~49 MB.

Dataset Structure
There are 40 folders, each of which represents one keyword and contains 300 files. The first eight digits of each file name identify the contributor, while the last two digits identify the round number. For example, the file path rotate/00000021_NO_06.wav indicates that the contributor with the ID 00000021 pronounced the keyword rotate for the 6th time.

Data Split
We recommend using the provided CSV files in your experiments. We kept 60% of the dataset for training, 20% for validation, and the remaining 20% for testing. In our split method, we guarantee that all recordings of a certain contributor are within the same subset.

License
This dataset is licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. For more details, see the LICENSE file in this folder.

Citations
If you want to use the Arabic Speech Commands dataset in your work, please cite it as:

@article{ghandoura2021building,
  title={Building and benchmarking an Arabic Speech Commands dataset for small-footprint keyword spotting},
  author={Ghandoura, Abdulkader and Hjabo, Farouk and Al Dakkak, Oumayma},
  journal={Engineering Applications of Artificial Intelligence},
  volume={102},
  pages={104267},
  year={2021},
  publisher={Elsevier}
}",2021,,,,,
162,Arabic_Text_Diacritization,Arabic Text Diacritization,Arabic Text Diacritization,Arabic Text Diacritization,Text,English,Natural Language Processing,arabic-text-diacritization-on-tashkeela-1,,https://github.com/AliOsm/arabic-text-diacritization,https://paperswithcode.com/dataset/arabic-text-diacritization,"Extracted from the Tashkeela Corpus, the dataset consists of 55K lines containing about 2.3M words.",,,,,,
163,ARAD-1K,Spectral Reconstruction,Spectral Reconstruction,Spectral Reconstruction,3D,,Methodology,spectral-reconstruction-on-arad-1k,Custom,https://github.com/boazarad/ARAD_1K,https://paperswithcode.com/dataset/arad-1k,The dataset used for NTIRE 2022 Spectral Recovery Challenge,2022,,,,,
164,Arcade_Learning_Environment,Montezuma's Revenge,Montezuma's Revenge,"Montezuma's Revenge, Atari Games",,,Methodology,"atari-games-on-atari-2600-atlantis, atari-games-on-atari-2600-pooyan, atari-games-on-atari-2600-hero, atari-games-on-atari-2600-pitfall, atari-games-on-atari-2600-space-invaders, atari-games-on-atari-2600-double-dunk, atari-games-on-atari-2600-krull, atari-games-on-atari-2600-bank-heist, atari-games-on-atari-2600-up-and-down, atari-games-on-atari-2600-beam-rider, atari-games-on-atari-2600-carnival, atari-games-on-atari-2600-name-this-game, atari-games-on-atari-2600-skiing, atari-games-on-atari-2600-elevator-action, atari-games-on-atari-2600-surround, atari-games-on-atari-2600-seaquest, atari-games-on-atari-2600-alien, atari-games-on-atari-2600-wizard-of-wor, atari-games-on-atari-2600-journey-escape, atari-games-on-atari-2600-gopher, atari-games-on-atari-2600-zaxxon, atari-games-on-atari-2600-kung-fu-master, atari-games-on-atari-2600-bowling, atari-games-on-atari-2600-time-pilot, atari-games-on-atari-2600-asteroids, atari-games-on-atari-2600-chopper-command, atari-games-on-atari-2600-ice-hockey, atari-games-on-atari-2600-ms-pacman, atari-games-on-atari-2600-yars-revenge, atari-games-on-atari-2600-crazy-climber, atari-games-on-atari-57, atari-games-on-atari-2600-qbert, atari-games-on-atari-2600-centipede, atari-games-on-atari-2600-james-bond, atari-games-on-atari-2600-road-runner, atari-games-on-atari-2600-tennis, atari-games-on-atari-2600-video-pinball, montezuma-s-revenge-on-atari-2600-montezuma-s, atari-games-on-atari-2600-pong, atari-games-on-atari-2600-robotank, atari-games-on-atari-2600-private-eye, atari-games-on-atari-2600-montezumas-revenge, atari-games-on-atari-2600-amidar, atari-games-on-atari-2600-battle-zone, atari-games-on-atari-2600-tutankham, atari-games-on-atari-2600-freeway, atari-games-on-atari-2600-gravitar, atari-games-on-atari-2600-venture, atari-games-on-atari-2600-demon-attack, atari-games-on-atari-2600-berzerk, atari-games-on-atari-2600-kangaroo, atari-games-on-atari-2600-phoenix, atari-games-on-atari-2600-fishing-derby, atari-games-on-atari-2600-frostbite, atari-games-on-atari-2600-assault, atari-games-on-atari-2600-river-raid, atari-games-on-atari-2600-breakout",GPL-2,https://github.com/mgbellemare/Arcade-Learning-Environment,https://paperswithcode.com/dataset/arcade-learning-environment,The Arcade Learning Environment (ALE) is an object-oriented framework that allows researchers to develop AI agents for Atari 2600 games. It is built on top of the Atari 2600 emulator Stella and separates the details of emulation from agent design.,,,,,,
165,ARCT,Argument Mining,Argument Mining,Argument Mining,,,Methodology,,Creative Commons,https://github.com/UKPLab/argument-reasoning-comprehension-task,https://paperswithcode.com/dataset/arct,"Freely licensed dataset with warrants for 2k authentic arguments from news comments. On this basis, we present a new challenging task, the argument reasoning comprehension task. Given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options. Both warrants are plausible and lexically close, but lead to contradicting claims.",,,,,,
166,ARCTIC,3D Reconstruction,3D Reconstruction,"3D Reconstruction, Motion Synthesis, Hand Pose Estimation, hand-object pose","3D, Image, Video",,Computer Vision,,,https://github.com/zc-alexfan/arctic,https://paperswithcode.com/dataset/arctic,ARCTIC is a dataset of free-form interactions of hands and articulated objects. ARCTIC has 1.2M images paired with accurate 3D meshes for both hands and for objects that move and deform over time. The dataset also provides hand-object contact information.,,,,2M images,,
167,ARC__AI2_Reasoning_Challenge_,Stance Detection,Stance Detection,"Stance Detection, Common Sense Reasoning",Image,,Computer Vision,"common-sense-reasoning-on-arc-challenge, common-sense-reasoning-on-arc-easy, stance-detection-on-arc",CC BY-SA 4.0,https://allenai.org/data/arc,https://paperswithcode.com/dataset/arc,"The AI2’s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. ARC includes a supporting KB of 14.3M unstructured text passages.",,Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering,https://arxiv.org/abs/1911.07176,,,
168,ArgKP-2021,Key Point Matching,Key Point Matching,Key Point Matching,,,Methodology,,Apache-2.0 License,https://github.com/ibm/KPA_2021_shared_task,https://paperswithcode.com/dataset/argkp-2021,"Data set covering a set of debatable topics, where for each topic and stance, a set of triplets of the form &lt;argument, KP, label&gt; is provided. The data set is based on the ArgKP data set, which contains arguments contributed by the crowd on 28 debatable topics, split by their stance towards the topic, and KPs written by an expert for those topics. Crowd annotations were collected to determine whether a KP represents an argument, i.e., is a match for an
argument.
The arguments in ArgKP are a subset of the IBM-ArgQ-Rank-30kArgs data set.
For a test set, we extended ArgKP, adding three new debatable topics, that were also not part of IBM-ArgQ-Rank-30kArgs. The test set was collected specifically for KPA-2021, and was carefully designed to be similar in various aspects to the training data 2 . For each topic, crowd sourced arguments were collected, expert KPs generated, and match/no match annotations for argument/KP pairs obtained, resulting in a data set compatible with the ArgKP format. Arguments collection strictly adhered to the guidelines, quality measures, and post processing used for the collection of arguments in IBM-ArgQ-Rank-30kArgs, while the generation of expert KPs, collection of match annotations, and final data set creation strictly adhered to the manner in which ArgKP was created.",2021,,,,,
169,Argoverse-HD,Object Detection,Object Detection,"Object Detection, Motion Forecasting, Real-Time Object Detection, Real-Time Multi-Object Tracking","Image, Time Series, Video",,Computer Vision,"real-time-object-detection-on-argoverse-hd-3, real-time-object-detection-on-argoverse-hd-2, real-time-object-detection-on-argoverse-hd-4, real-time-object-detection-on-argoverse-hd-5",MIT,https://www.cs.cmu.edu/~mengtial/proj/streaming/,https://paperswithcode.com/dataset/argoverse-hd,"Argoverse-HD is a dataset built for streaming object detection, which encompasses real-time object detection, video object detection, tracking, and short-term forecasting. It contains the video data from Argoverse 1.1 with our own MS COCO-style bounding box annotations with track IDs. The annotations are backward-compatible with COCO as one can directly evaluate COCO pre-trained models on this dataset to estimate the efficiency or the cross-dataset generalization capability of the models. The dataset contains high-quality and temporally-dense annotations for high-resolution videos (1920 x 1200 @ 30 FPS). Overall, there are 70,000 image frames and 1.3 million bounding boxes.

Argoverse-HD is the dataset used in the Streaming Perception Challenge, which includes two tracks:


Detection-only (real-time object detection). In this track, the participants will develop single-frame object detectors as they would for COCO and LVIS challenges. The crucial distinction is that the evaluation will score latency through streaming accuracy.
Full-stack. In this track, the method is unrestricted. However, most likely tracking and forecasting will be used to compensate for the latency of the detectors.

By default, all submissions measure their latency on a V100 GPU with the official toolkit.",1920,,,,,
170,Argoverse,3D Object Tracking,3D Object Tracking,"3D Object Tracking, Monocular Cross-View Road Scene Parsing(Vehicle), Motion Forecasting, Trajectory Prediction, Monocular Cross-View Road Scene Parsing(Road), 3D Object Detection","3D, Image, Text, Time Series, Video",English,Computer Vision,"3d-object-tracking-on-argoverse-cvpr-2020, monocular-cross-view-road-scene-parsing-1, 3d-object-detection-on-argoverse, monocular-cross-view-road-scene-parsing-road-2, trajectory-prediction-on-argoverse, motion-forecasting-on-argoverse-cvpr-2020",Custom,https://www.argoverse.org/data.html,https://paperswithcode.com/dataset/argoverse,"Argoverse is a tracking benchmark with over 30K scenarios collected in Pittsburgh and Miami. Each scenario is a sequence of frames sampled at 10 HZ. Each sequence has an interesting object called “agent”, and the task is to predict the future locations of agents in a 3 seconds future horizon. The sequences are split into training, validation and test sets, which have 205,942, 39,472 and 78,143 sequences respectively. These splits have no geographical overlap.",,Learning Lane Graph Representations for Motion Forecasting,https://arxiv.org/abs/2007.13732,,,
171,Argoverse_2,Dynamic Point Removal,Dynamic Point Removal,"Dynamic Point Removal, Self-supervised Scene Flow Estimation, Scene Flow Estimation, 3D Object Detection","3D, Image",,Computer Vision,"dynamic-point-removal-on-argoverse-2, self-supervised-scene-flow-estimation-on-1, scene-flow-estimation-on-argoverse-2",CC BY-NC-SA 4.0,https://www.argoverse.org/av2.html,https://paperswithcode.com/dataset/argoverse-2,"Argoverse 2 (AV2) is a collection of three datasets for perception and forecasting research in the self-driving domain. The annotated Sensor Dataset contains 1,000 sequences of multimodal data, encompassing high-resolution imagery from seven ring cameras, and two stereo cameras in addition to lidar point clouds, and 6-DOF map-aligned pose. Sequences contain 3D cuboid annotations for 26 object categories, all of which are sufficiently-sampled to support training and evaluation of 3D perception models. The Lidar Dataset contains 20,000 sequences of unlabeled lidar point clouds and map-aligned pose. This dataset is the largest ever collection of lidar sensor data and supports self-supervised learning and the emerging task of point cloud forecasting. Finally, the Motion Forecasting Dataset contains 250,000 scenarios mined for interesting and challenging interactions be- tween the autonomous vehicle and other actors in each local scene. Models are tasked with the prediction of future motion for “scored actors"" in each scenario and are provided with track histories that capture object location, heading, velocity, and category. In all three datasets, each scenario contains its own HD Map with 3D lane and crosswalk geometry — sourced from data captured in six distinct cities. We believe these datasets will support new and existing machine learning research problems in ways that existing datasets do not. All datasets are released under the CC BY-NC-SA 4.0 license.",,,,,,
172,Argoverse_2_Lidar,Motion Forecasting,Motion Forecasting,"Motion Forecasting, Self-Supervised Learning","Time Series, Video",,Methodology,,CC BY-NC-SA 4.0,https://www.argoverse.org/av2.html,https://paperswithcode.com/dataset/argoverse-2-lidar,"The Argoverse 2 Lidar Dataset is a collection of 20,000 scenarios with lidar sensor data, HD maps, and ego-vehicle pose. It does not include imagery or 3D annotations. The dataset is designed to support research into self-supervised learning in the lidar domain, as well as point cloud forecasting.

The dataset is divided into train, validation, and test sets of 16,000, 2,000, and 2,000 scenarios. This supports a point cloud forecasting task in which the future frames of the test set serve as the ground truth. Nonetheless, we encourage the community to use the dataset broadly for other tasks, such as self-supervised learning and map automation.

All Argoverse datasets contain lidar data from two out-of-phase 32 beam sensors rotating at 10 Hz. While this can be aggregated into 64 beam frames at 10 Hz, it is also reasonable to think of this as 32 beam frames at 20 Hz. Furthermore, all Argoverse datasets contain raw lidar returns with per-point timestamps, so the data does not need to be interpreted in quantized frames.",,,,,,
173,Argoverse_2_Map_Change,Motion Forecasting,Motion Forecasting,"Motion Forecasting, Self-Supervised Learning","Time Series, Video",,Methodology,,CC BY-NC-SA 4.0,https://www.argoverse.org/av2.html,https://paperswithcode.com/dataset/argoverse-2-map-change,"The Argoverse 2 Map Change Dataset is a collection of 1,000 scenarios with ring camera imagery, lidar, and HD maps. Two hundred of the scenarios include changes in the real-world environment that are not yet reflected in the HD map, such as new crosswalks or repainted lanes. By sharing a map dataset that labels the instances in which there are discrepancies with sensor data, we encourage the development of novel methods for detecting out-of-date map regions.

The Map Change Dataset does not include 3D object annotations (which is a point of differentiation from the Argoverse 2 Sensor Dataset). Instead, it includes temporal annotations that indicate whether there is a map change within 30 meters of the autonomous vehicle at a particular timestamp. Additionally, the scenarios tend to be longer than the scenarios in the Sensor Dataset. To avoid making the dataset excessively large, the bitrate of the imagery is reduced.",,,,,,
174,Argoverse_2_Motion_Forecasting,Autonomous Vehicles,Autonomous Vehicles,"Autonomous Vehicles, Motion Forecasting, Trajectory Prediction, motion prediction, Self-Driving Cars, Autonomous Driving","Time Series, Video",,Methodology,,CC BY-NC-SA 4.0,https://www.argoverse.org/av2.html,https://paperswithcode.com/dataset/argoverse-2-motion-forecasting,"The Argoverse 2 Motion Forecasting Dataset is a curated collection of 250,000 scenarios for training and validation. Each scenario is 11 seconds long and contains the 2D, birds-eye-view centroid and heading of each tracked object sampled at 10 Hz.

To curate this collection, we sifted through thousands of hours of driving data from our fleet of self-driving test vehicles to find the most challenging segments. We place special emphasis on kinematically and socially unusual behavior, especially when exhibited by actors relevant to the ego-vehicle’s decision-making process. Some examples of interactions captured within our dataset include: buses navigating through multi-lane intersections, vehicles yielding to pedestrians at crosswalks, and cyclists sharing dense city streets.

Spanning 2,000+ km over six geographically diverse cities, Argoverse 2 covers a large geographic area. Argoverse 2 also contains a large object taxonomy with 10 non-overlapping classes that encompass a broad range of actors, both static and dynamic. In comparison to the Argoverse 1 Motion Forecasting Dataset, the scenarios in this dataset are approximately twice as long and more diverse.

Together, these changes incentivize methods that perform well on extended forecast horizons, handle multiple types of dynamic objects, and ensure safety in long tail scenarios.",,,,,"test vehicles to find the most challenging segments. We place special emphasis on kinematically and socially unusual behavior, especially when exhibited by actors relevant to the ego-vehicle’s decision-making process. Some examples",
175,Argoverse_2_Sensor,Motion Forecasting,Motion Forecasting,"Motion Forecasting, Self-Supervised Learning","Time Series, Video",,Methodology,,CC BY-NC-SA 4.0,https://www.argoverse.org/av2.html,https://paperswithcode.com/dataset/argoverse-2-sensor-dataset,"The Argoverse 2 Sensor Dataset is a collection of 1,000 scenarios with 3D object tracking annotations. Each sequence in our training and validation sets includes annotations for all objects within five meters of the “drivable area” — the area in which it is possible for a vehicle to drive. The HD map for each scenario specifies the driveable area.",,,,,,
176,ArgSciChat,Response Generation,Response Generation,"Response Generation, Fact Selection",Text,English,Natural Language Processing,"response-generation-on-argscichat, fact-selection-on-argscichat",,https://github.com/federicoruggeri/argscichat_project,https://paperswithcode.com/dataset/argscichat,ArgSciChat is an argumentative dialogue dataset. It consists of 498 messages collected from 41 dialogues on 20 scientific papers. It can be used to evaluate conversational agents and further encourage research on argumentative scientific agents.,,,,,,
177,Aria_Digital_Twin_Dataset,3D Object Tracking,3D Object Tracking,"3D Object Tracking, 2D Object Detection, 3D Object Detection, 3D Reconstruction, 3D Depth Estimation, 2D Semantic Segmentation","3D, Image, Video",,Computer Vision,3d-reconstruction-on-aria-digital-twin,,https://www.projectaria.com/datasets/adt,https://paperswithcode.com/dataset/aria-digital-twin-dataset,"A real-world dataset, with hyper-accurate digital counterpart & comprehensive ground-truth annotation.

Dataset Content
- 200 sequences (~400 mins)
- 398 objects (324 stationary, 74 dynamic)
- 2 real indoor scenes
- Single + multi-user activities

Sensor Data per device
- 2 x outward-facing monochrome camera streams
- 1 x outward-facing RGB camera stream
- 2 x IMU streams
- 2 x Internal-facing eye tracking cameras
- Complete sensor calibrations",,,,,,
178,ARINC_429_Voltage_Data,Intrusion Detection,Intrusion Detection,Intrusion Detection,Image,,Computer Vision,,,https://testscience.org/arinc/,https://paperswithcode.com/dataset/arinc-429-voltage-data,"This page contains ARINC 429 message data recorded from the hardware-in-a-loop simulator. These messages were recorded using a SIGLENT SDS2204X Plus oscilloscope sampling at 20 MHz. The intent of this data is to enable cybersecurity research and development for ARINC 429 by providing detailed message data from multiple hardware sources. 

The dataset consists of three csv files of ARINC 429 messages along with a data dictionary markdown file. The zip file is 550MB and each csv file is approximately 1.5GB. The csv filenames indicate which transmitter the messages were being sent from, with the EGPWS representing a “valid” transmitter while the RTX and AltaDT both represent adversarial transmitters.",,,,,,
179,Aristo-v4,Link Prediction,Link Prediction,"Link Prediction, Knowledge Base Completion, Knowledge Graph Completion","Graph, Time Series",,Methodology,link-prediction-on-aristo-v4,CC-BY-SA,https://allenai.org/data/tuple-kb,https://paperswithcode.com/dataset/aristo-v4,"The Aristo Tuple KB contains a collection of high-precision, domain-targeted (subject,relation,object) tuples extracted from text using a high-precision extraction pipeline, and guided by domain vocabulary constraints.  The dataset was introduced by the paper Domain-Targeted, High Precision Knowledge Extraction.",,"Domain-Targeted, High Precision Knowledge Extraction",https://aclanthology.org/Q17-1017.pdf,,,
180,ARO,Retrieval,Retrieval,Retrieval,,,Methodology,,,https://github.com/mertyg/vision-language-models-are-bows,https://paperswithcode.com/dataset/aro,"Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO-Order & Flickr30k-Order, to test for order sensitivity in VLMs. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases.",,,,,,
181,ARQMath,Math Information Retrieval,Math Information Retrieval,Math Information Retrieval,,,Methodology,math-information-retrieval-on-arqmath2,,https://www.cs.rit.edu/~dprl/ARQMath/,https://paperswithcode.com/dataset/arqmath2,"The goal of ARQMath is to advance techniques for mathematical information retrieval, in particular, retrieving answers to mathematical questions (Task 1), and formula retrieval (Task 2).
Using the question posts from Math Stack Exchange, participating systems are given a question or a formula from a question and asked to return a ranked list of either potential answers to the question or potentially useful formulae (in the case of a formula query). Relevance is determined by the expected utility of each returned item. These tasks allow participating teams to explore leveraging math notation together with text to improve the quality of retrieval results.",,,,,,
182,ArraMon,Vision and Language Navigation,Vision and Language Navigation,Vision and Language Navigation,Text,English,Natural Language Processing,,,https://arramonunc.github.io/,https://paperswithcode.com/dataset/arramon,"A dataset (in English; and also extended to Hindi) with human-written navigation and assembling instructions, and the corresponding ground truth trajectories.",,,,,,
183,ArSarcasm-v2,Irony Identification,Irony Identification,"Irony Identification, Sentiment Analysis, Dialect Identification, Sarcasm Detection","Image, Text",English,Computer Vision,,,https://github.com/iabufarha/ArSarcasm-v2,https://paperswithcode.com/dataset/arsarcasm-v2,"ArSarcasm-v2 is an extension of the original ArSarcasm dataset published along with the paper From Arabic Sentiment Analysis to Sarcasm Detection: The ArSarcasm Dataset. ArSarcasm-v2 conisists of ArSarcasm along with portions of DAICT corpus and some new tweets. Each tweet was annotated for sarcasm, sentiment and dialect. The final dataset consists of 15,548 tweets divided into 12,548 training tweets and 3,000 testing tweets. ArSarcasm-v2 was used and released as a part of the shared task on sarcasm detection and sentiment analysis in Arabic.",,,,,,
184,ArSarcasm,Sentiment Analysis,Sentiment Analysis,"Sentiment Analysis, Dialect Identification, Sarcasm Detection","Image, Text",English,Computer Vision,,,https://github.com/iabufarha/ArSarcasm,https://paperswithcode.com/dataset/arsarcasm,"ArSarcasm is a new Arabic sarcasm detection dataset. The dataset was created using previously available Arabic sentiment analysis datasets (SemEval 2017 and ASTD) and adds sarcasm and dialect labels to them. The dataset contains 10,547 tweets, 1,682 (16%) of which are sarcastic.",2017,,,,,
185,ArSen-20,Sentiment Classification,Sentiment Classification,"Sentiment Classification, Twitter Sentiment Analysis, Sentiment Analysis, Arabic Sentiment Analysis","Image, Text",English,Computer Vision,,Apache-2.0 license,https://github.com/123fangyang/ArSen-20,https://paperswithcode.com/dataset/arsen-20,"Sentiment detection remains a pivotal task in natural language processing, yet its development in Arabic lags due to a scarcity of training materials compared to English. Addressing this gap, we present ArSen-20, a benchmark dataset tailored to propel Arabic sentiment detection forward. ArSen-20 comprises 20,000 professionally labeled tweets sourced from Twitter, focusing on the theme of COVID-19 and spanning the period from 2020 to 2023. Beyond tweet content, the dataset incorporates metadata associated with the user, enriching the contextual understanding. ArSen-20 offers a comprehensive resource to foster advancements in Arabic sentiment analysis and facilitate research in this critical domain.

The ArSen-20 dataset statistics:

| Statistics   |   Num  | 
|:-------------:|:-----:|
| Training set size | 16000 |
| Validation set size| 2000 |
| Testing set size | 2000 |
| Neutral | 17262 |
| Positive | 878 |
| Negative | 1860 |

Features
The dataset has the following features:

| Field   |  Type  |  Description  |
|:-----------:| :--------: |:----------------: |
| tweet id     | string     | The unique identifier of the requested Tweet.     |
| label   | string     | Sentiment Classification of this tweet.     |
| author id   | string    |The unique identifier of this user.     |
| created_at  | data     | Creation time of the Tweet.    |
| lang  | string     | Language of the Tweet, if detected by Twitter.    |
| like_count  | int     |The number of likes on this tweet.|
|quote_count  | int    | The number of times this tweet has been quoted.    |
| reply_count   | int     | The number of replies to this tweet.    |
| retweet_count| int    | The number of retweets to this tweet.    |
| tweet   | string     | The actual UTF-8 text of the Tweet.    |
|user_verified  | boolean     | Indicates if this user is a verified Twitter User.     |
|followers_count  | int     |The number of followers of the author.     |
| following_count  | int     | The number of following of the author.    |
| tweet_count  | int     | Total number of tweets by the author.    |
| listed_count | int     |The number of public lists that this user is a member of.    |
|name | string     | The name of the user.    |
| username   | string     | The Twitter screen name, handle, or alias.    |
| user_created_at| data     | The UTC datetime that the user account was created.     |
| description  | string     | The text of this user’s profile description (bio).     |

DownLoad
You can download the dataset from here.



ArSen-20_publish.csv - Contains all features.



ArSen-20_id_only.csv - Contains only tweets and their author's id.



Citation
If you use this dataset in your research, please cite the following papers:
bibtex
@inproceedings{fang2024arsen,
title={ArSen-20: A New Benchmark for Arabic Sentiment Detection},
author={Yang Fang and Cheng Xu},
booktitle={5th Workshop on African Natural Language Processing},
year={2024},
url={https://openreview.net/forum?id=GgsRUF5kJt}
}

bibtex
@inproceedings{fang2024advancing,
    title = ""Advancing {A}rabic Sentiment Analysis: {A}r{S}en Benchmark and the Improved Fuzzy Deep Hybrid Network"",
    author = ""Fang, Yang  and
      Xu, Cheng  and
      Guan, Shuhao  and
      Yan, Nan  and
      Mei, Yuke"",
    editor = ""Barak, Libby  and
      Alikhani, Malihe"",
    booktitle = ""Proceedings of the 28th Conference on Computational Natural Language Learning"",
    month = nov,
    year = ""2024"",
    address = ""Miami, FL, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.conll-1.39"",
    pages = ""507--516"",
}

contact
If you have any questions or comments about the dataset, please contact Yang Fang (20211209024@chnu.edu.cn).

Potential cooperation in related fields is also welcome. :)",2020,,,,,
186,ArSen,Sentiment Classification,Sentiment Classification,"Sentiment Classification, Sentiment Analysis, Arabic Sentiment Analysis","Image, Text",English,Computer Vision,,,https://github.com/123fangyang/ArSen,https://paperswithcode.com/dataset/arsen,"Sentiment analysis is pivotal in Natural Language Processing for understanding opinions and emotions in text. While advancements in Sentiment analysis for English are notable, Arabic Sentiment Analysis (ASA) lags, despite the growing Arabic online user base. Existing ASA benchmarks are often outdated and lack comprehensive evaluation capabilities for state-of-the-art models. To bridge this gap, we introduce ArSen, a meticulously annotated COVID-19-themed Arabic dataset, and the IFDHN, a novel model incorporating fuzzy logic for enhanced sentiment classification. ArSen provides a contemporary, robust benchmark, and IFDHN achieves state-of-the-art performance on ASA tasks. Comprehensive evaluations demonstrate the efficacy of IFDHN using the ArSen dataset, highlighting future research directions in ASA.",,,,,,
187,ArSentD-LEV,Sentiment Analysis,Sentiment Analysis,"Sentiment Analysis, Arabic Sentiment Analysis",Text,English,Natural Language Processing,,,http://oma-project.com/ArSenL/ArSenTD_Lev_Intro,https://paperswithcode.com/dataset/arsentd-lev,"The Arabic Sentiment Twitter Dataset for the Levantine dialect (ArSenTD-LEV) is a dataset of 4,000 tweets with the following annotations: the overall sentiment of the tweet, the target to which the sentiment was expressed, how the sentiment was expressed, and the topic of the tweet.",,,,,,
188,ArtiFact,Fake Image Detection,Fake Image Detection,"Fake Image Detection, Classification, Fake Image Attribution",Image,,Computer Vision,,,https://github.com/awsaf49/artifact,https://paperswithcode.com/dataset/artifact,"The ArtiFact dataset is a large-scale image dataset that aims to include a diverse collection of real and synthetic images from multiple categories, including Human/Human Faces, Animal/Animal Faces, Places, Vehicles, Art, and many other real-life objects. The dataset comprises 8 sources that were carefully chosen to ensure diversity and includes images synthesized from 25 distinct methods, including 13 GANs, 7 Diffusion, and 5 other miscellaneous generators. The dataset contains 2,496,738 images, comprising 964,989 real images and 1,531,749 fake images.

To ensure diversity across different sources, the real images of the dataset are randomly sampled from source datasets containing numerous categories, whereas synthetic images are generated within the same categories as the real images. Captions and image masks from the COCO dataset are utilized to generate images for text2image and inpainting generators, while normally distributed noise with different random seeds is used for noise2image generators. The dataset is further processed to reflect real-world scenarios by applying random cropping, downscaling, and JPEG compression, in accordance with the IEEE VIP Cup 2022 standards.

The ArtiFact dataset is intended to serve as a benchmark for evaluating the performance of synthetic image detectors under real-world conditions. It includes a broad spectrum of diversity in terms of generators used and syntheticity, providing a challenging dataset for image detection tasks.


Total number of images: 2,496,738
Number of real images: 964,989
Number of fake images: 1,531,749
Number of generators used for fake images: 25 (including 13 GANs, 7 Diffusion, and 5 miscellaneous generators)
Number of sources used for real images: 8
Categories included in the dataset: Human/Human Faces, Animal/Animal Faces, Places, Vehicles, Art, and other real-life objects
Image Resolution: 200 x 200",2022,,,738 images,,8
189,Artificial_signal_data_for_signal_alignment_testin,Time Series Alignment,Time Series Alignment,Time Series Alignment,Time Series,,Time Series,,CC BY,https://zenodo.org/record/4522133#.YMhd0DqxWrw,https://paperswithcode.com/dataset/artificial-signal-data-for-signal-alignment,"This is a set of signals-pairs, univariate and multivariate, that can be used to test alignment algorithms.
Signals are morphologically different.

Signal data is synchronized, but the provided timestamp is shifted with small time-jumps.",,,,,,
190,ART_Dataset,Language Modelling,Language Modelling,"Language Modelling, Learning-To-Rank, Natural Language Inference",Text,English,Natural Language Processing,,,http://abductivecommonsense.xyz/,https://paperswithcode.com/dataset/art-dataset,ART consists of over 20k commonsense narrative contexts and 200k explanations.,,,,,,
191,arXivEdits,Intent Classification,Intent Classification,Intent Classification,Image,,Computer Vision,,,https://tiny.one/arxivedits,https://paperswithcode.com/dataset/arxivedits,"arXivEdits an annotated corpus of 751 full papers from arXiv with gold sentence alignment across their multiple versions of revision, as well as fine-grained span-level edits and their underlying intentions for 1,000 sentence pairs. This dataset is designed for studying the human revision process in the scientific writing domain.",,arXivEdits: Understanding the Human Revision Process in Scientific Writing,https://arxiv.org/pdf/2210.15067v1.pdf,,,
192,arXiv_Astro-Ph,Link Prediction,Link Prediction,"Link Prediction, Multi-Label Classification, Clique Prediction, Network Embedding","Graph, Image, Time Series",,Computer Vision,"clique-prediction-on-arxiv-astroph-2-clique, clique-prediction-on-arxiv-astroph-4-clique",,https://snap.stanford.edu/data/ca-AstroPh.html,https://paperswithcode.com/dataset/arxiv-astro-ph,"Arxiv ASTRO-PH (Astro Physics) collaboration network is from the e-print arXiv and covers scientific collaborations between authors papers submitted to Astro Physics category. If an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes.",,,,,,
193,Arxiv_HEP-TH_citation_graph,Text Summarization,Text Summarization,"Text Summarization, Language Modelling, Clique Prediction, Extended Summarization, Topic Models, Text Classification, Document Summarization","Image, Text, Time Series",English,Computer Vision,"topic-models-on-arxiv, text-classification-on-arxiv, extended-summarization-on-arxiv-long-val, document-summarization-on-arxiv, text-summarization-on-arxiv, clique-prediction-on-arxiv-astroph-3-clique, extended-summarization-on-arxiv-long-test, language-modelling-on-arxiv, clique-prediction-on-arxiv-grqc-4-clique",,https://snap.stanford.edu/data/cit-HepTh.html,https://paperswithcode.com/dataset/arxiv,"Arxiv HEP-TH (high energy physics theory) citation graph is from the e-print arXiv and covers all the citations within a dataset of 27,770 papers with 352,807 edges. If a paper i cites paper j, the graph contains a directed edge from i to j. If a paper cites, or is cited by, a paper outside the dataset, the graph does not contain any information about this.
The data covers papers in the period from January 1993 to April 2003 (124 months).",1993,,,,,
194,arXiv_Summarization_Dataset,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Text Summarization, Unsupervised Extractive Summarization, Document Summarization",Text,English,Natural Language Processing,"document-summarization-on-arxiv-summarization, text-summarization-on-arxiv-summarization, unsupervised-extractive-summarization-on",,https://github.com/armancohan/long-summarization,https://paperswithcode.com/dataset/arxiv-summarization-dataset,This is a dataset for evaluating summarisation methods for research papers.,,,,,,
195,ArzEn,ArzEn Code-switched Translation to eng,ArzEn Code-switched Translation to eng,"ArzEn Code-switched Translation to eng, Automatic Speech Recognition (ASR), Transliteration, ArzEn Code-switched Translation to ara, Speech Recognition, ArzEn Speech Recognition","Audio, Image, Text",English,Computer Vision,"arzen-speech-recognition-on-arzen, arzen-code-switched-translation-to-eng-on-1, arzen-code-switched-translation-to-ara-on",,https://sites.google.com/view/arzen-corpus/home,https://paperswithcode.com/dataset/arzen,"Corpus of Egyptian Arabic-English Code-switching (ArzEn) is a spontaneous conversational speech corpus, obtained through informal interviews held at the German University in Cairo. The participants discussed broad topics, including education, hobbies, work, and life experiences. The corpus currently contains 12 hours of speech, having 6,216 utterances. The recordings were transcribed and translated into monolingual Egyptian Arabic and monolingual English.",,https://arxiv.org/pdf/2211.16319v1.pdf,https://arxiv.org/pdf/2211.16319v1.pdf,,,
196,ASAP-AES,Automated Essay Scoring,Automated Essay Scoring,Automated Essay Scoring,,,Methodology,automated-essay-scoring-on-asap,,https://www.kaggle.com/competitions/asap-aes/data,https://paperswithcode.com/dataset/asap,There are eight essay sets. Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored. Each of the eight data sets has its own unique characteristics. The variability is intended to test the limits of your scoring engine's capabilities.,,,,,,
197,ASAP,Music Transcription,Music Transcription,"Music Transcription, Music Generation, Beat Tracking, Online Beat Tracking, Music Performance Rendering, Downbeat Tracking, Online Downbeat Tracking","Audio, Image, Text, Video",English,Computer Vision,"downbeat-tracking-on-asap-aligned-scores-and, beat-tracking-on-asap-aligned-scores-and",,https://github.com/fosfrancesco/asap-dataset,https://paperswithcode.com/dataset/asap-aligned-scores-and-performances,"ASAP is a dataset of 222 digital musical scores aligned with 1068 performances (more than 92 hours) of Western classical piano music.

The scores are provided as paired MusicXML files and quantized MIDI files, and the performances as paired MIDI files and partially as audio recordings. Scores and performances are aligned with downbeat, beat, time signature, and key signature annotations.",,ASAP: a dataset of aligned scores and performances for piano transcription,https://archives.ismir.net/ismir2020/paper/000127.pdf,,,
198,Ascent_KB,Commonsense Knowledge Base Construction,Commonsense Knowledge Base Construction,Commonsense Knowledge Base Construction,,,Methodology,,CC BY 4.0,https://ascent.mpi-inf.mpg.de,https://paperswithcode.com/dataset/ascentkb,"This dataset contains 8.9M commonsense assertions extracted by the Ascent pipeline developed at the Max Planck Institute for Informatics. The focus of this dataset is on everyday concepts such as elephant, car, laptop, etc. The current version of Ascent KB (v1.0.0) is approximately 19 times larger than ConceptNet (note that, in this comparison, non-commonsense knowledge in ConceptNet such as lexical relations is excluded).",,,,,,
199,ASDiv,Math Word Problem Solving,Math Word Problem Solving,Math Word Problem Solving,,,Methodology,math-word-problem-solving-on-asdiv-a,CC-BY-NC 4.0,https://github.com/chaochun/nlu-asdiv-dataset,https://paperswithcode.com/dataset/asdiv,"We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms of both language patterns and problem types) English math word problem (MWP) corpus for evaluating the capability of various MWP solvers. Existing MWP corpora for studying AI progress remain limited either in language usage patterns or in problem types. We thus present a new English MWP corpus with 2,305 MWPs that cover more text patterns and most problem types taught in elementary school. Each MWP is annotated with its problem type and grade level (for indicating the level of difficulty). Furthermore, we propose a metric to measure the lexicon usage diversity of a given MWP corpus, and demonstrate that ASDiv is more diverse than existing corpora. Experiments show that our proposed corpus reflects the true capability of MWP solvers more faithfully.",,,,,,
200,AsEP,Graph Learning,Graph Learning,Graph Learning,Graph,,Methodology,,Creative Commons Attribution 4.0 International,https://zenodo.org/records/11495514,https://paperswithcode.com/dataset/asep,"AsEP is a protein structure dataset that includes 1723 filtered antibody-antigen complexes from abYbank/AbDb.

It is designed to facilitate the development of machine-learning-based models for predicting antibody-specific epitopes given a pair of antibody and antigen structures as input, which is important for antibody engineering. The dataset also introduces a new task for predicting the interacting residue pairs between antibodies and antigens, which is useful for understanding the functional properties of antibodies and enhances the interpretability of model performance.

Please refer to this GitHub repository AsEP-dataset for the dataset interface, which is built with Python and PyTorch-Geometric Dataset Module.",,,,,,
201,ASHRAE_energy_prediction_III,Load Forecasting,Load Forecasting,Load Forecasting,Time Series,,Methodology,,open source,https://www.kaggle.com/c/ashrae-energy-prediction,https://paperswithcode.com/dataset/ashrae-energy-prediction-iii,Assessing the value of energy efficiency improvements can be challenging as there's no way to truly know how much energy a building would have used without the improvements. The best we can do is to build counterfactual models. Once a building is overhauled the new (lower) energy consumption is compared against modeled values for the original building to calculate the savings from the retrofit. More accurate models could support better market incentives and enable lower-cost financing.,,,,,,
202,ASOS_Data,Solar Irradiance Forecasting,Solar Irradiance Forecasting,Solar Irradiance Forecasting,Time Series,,Methodology,solar-irradiance-forecasting-on-automated,,,https://paperswithcode.com/dataset/automated-surface-weather-observing-systems,"The Automated Surface Observing Systems (ASOS) program is a joint effort of the National Weather Service (NWS), the Federal Aviation Administration (FAA), and the Department of Defense (DOD). These automated systems collect observations on a continual basis, 24 hours a day. 

Automated Weather Observing System (AWOS) units are operated and controlled by the Federal Aviation Administration. These systems are among the oldest automated weather stations and predate ASOS. They generally report at 20-minute intervals and, unlike ASOS, do not report special observations for rapidly changing weather conditions.

ASOS observations are operationally generated each hour, and special observations are provided whenever the weather changes. These special reports are generated when conditions exceed preselected weather element thresholds, e.g., the visibility decreases to less than 3 miles.",,,,,,
203,ASPEC,Machine Translation,Machine Translation,"Machine Translation, Domain Adaptation, Low-Resource Neural Machine Translation",Text,English,Natural Language Processing,,Custom (non-commercial),http://lotus.kuee.kyoto-u.ac.jp/ASPEC/,https://paperswithcode.com/dataset/aspec,"ASPEC, Asian Scientific Paper Excerpt Corpus, is constructed by the Japan Science and Technology Agency (JST) in collaboration with the National Institute of Information and Communications Technology (NICT). It consists of a Japanese-English paper abstract corpus of 3M parallel sentences (ASPEC-JE) and a Japanese-Chinese paper excerpt corpus of 680K parallel sentences (ASPEC-JC). This corpus is one of the achievements of the Japanese-Chinese machine translation project which was run in Japan from 2006 to 2010.",2006,https://www.aclweb.org/anthology/L16-1350.pdf,https://www.aclweb.org/anthology/L16-1350.pdf,,,
204,ASQP,Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),Text,English,Natural Language Processing,aspect-based-sentiment-analysis-absa-on-asqp,,,https://paperswithcode.com/dataset/asqp,"Aspect-based sentiment analysis (ABSA) typically focuses on extracting aspects and predicting their sentiments on individual sentences such as customer reviews. Recently, another kind of opinion sharing platform, namely question answering (QA) forum, has received increasing popularity, which accumulates a large number of user opinions towards various aspects. This motivates us to investigate the task of ABSA on QA forums (ABSA-QA), aiming to jointly detect the discussed aspects and their sentiment polarities for a given QA pair. Unlike review sentences, a QA pair is composed of two parallel sentences, which requires interaction modeling to align the aspect mentioned in the question and the associated opinion clues in the answer. To this end, we propose a model with a specific design of cross-sentence aspect-opinion interaction modeling to address this task. The proposed method is evaluated on three real-world datasets and the results show that our model outperforms several strong baselines adopted from related state-of-the-art models.",,,,,,
205,ASR-RAMC-BIGCCSC__A_CHINESE_CONVERSATIONAL_SPEECH_,Speaker Recognition,Speaker Recognition,"Speaker Recognition, Speaker Diarization","Audio, Image",,Computer Vision,,,https://magichub.com/datasets/magicdata-ramc/,https://paperswithcode.com/dataset/asr-ramc-bigccsc-a-chinese-conversational,"A Rich Annotated Mandarin Conversational (RAMC) Speech Dataset, including 180 hours of Mandarin Chinese dialogue, 150, 10 and 20 hours for the training set, development set and test set respectively.
It contains 351 multi-turn dialogues, each of which is a coherent and compact conversation centered around one theme.

It covers 15 topics, including humanities, entertainment, sports, military, finance, religion, family life, politics, education, digital devices, environment, science, professional development, art and ordinary life.

It is suitable for exploring speech processing techniques in dialog scenarios.",,,,,,
206,Assembly101,3D Action Recognition,3D Action Recognition,"3D Action Recognition, Action Anticipation, Open Vocabulary Action Recognition, Action Recognition, Action Segmentation, Mistake Detection","3D, Image, Video",,Computer Vision,"action-anticipation-on-assembly101, open-vocabulary-action-recognition-on, 3d-action-recognition-on-assembly101, action-segmentation-on-assembly101",,https://assembly-101.github.io/,https://paperswithcode.com/dataset/assembly101,"Assembly101 is a new procedural activity dataset featuring 4321 videos of people assembling and disassembling 101 ""take-apart"" toy vehicles. Participants work without fixed instructions, and the sequences feature rich and natural variations in action ordering, mistakes, and corrections. Assembly101 is the first multi-view action dataset, with simultaneous static (8) and egocentric (4) recordings. Sequences are annotated with more than 100K coarse and 1M fine-grained action segments, and 18M 3D hand poses. We benchmark on three action understanding tasks: recognition, anticipation and temporal segmentation. Additionally, we propose a novel task of detecting mistakes. The unique recording format and rich set of annotations allow us to investigate generalization to new toys, cross-view transfer, long-tailed distributions, and pose vs. appearance. We envision that Assembly101 will serve as a new challenge to investigate various activity understanding problems.",,,,,,
207,ASSET,Text Simplification,Text Simplification,Text Simplification,Text,English,Natural Language Processing,text-simplification-on-asset,,https://github.com/facebookresearch/asset,https://paperswithcode.com/dataset/asset,ASSET is a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations.,,ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations,https://arxiv.org/pdf/2005.00481v1.pdf,,,
208,ASTD,Sentiment Analysis,Sentiment Analysis,Sentiment Analysis,Text,English,Natural Language Processing,sentiment-analysis-on-astd,GPL-2.0,https://github.com/mahmoudnabil/ASTD,https://paperswithcode.com/dataset/astd,"Arabic Sentiment Tweets Dataset (ASTD) is an Arabic social sentiment analysis dataset gathered from Twitter. It consists of about 10,000 tweets which are classified as objective, subjective positive, subjective negative, and subjective mixed.",,,,,,
209,ASTE-Data-V2,Aspect Sentiment Triplet Extraction,Aspect Sentiment Triplet Extraction,Aspect Sentiment Triplet Extraction,Text,English,Natural Language Processing,aspect-sentiment-triplet-extraction-on-aste,,https://github.com/xuuuluuu/SemEval-Triplet-data,https://paperswithcode.com/dataset/aste-data-v2,"A benchmark dataset for the Aspect Sentiment Triplet Extraction, an updated version of ASTE-Data-V1.",,,,,,
210,ASTE,Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),Text,English,Natural Language Processing,aspect-based-sentiment-analysis-absa-on-aste,,,https://paperswithcode.com/dataset/aste,"Target-based sentiment analysis or aspect-based sentiment analysis (ABSA) refers to addressing various sentiment analysis tasks at a fine-grained level, which includes but is not limited to aspect extraction, aspect sentiment classification, and opinion extraction. There exist many solvers of the above individual subtasks or a combination of two subtasks, and they can work together to tell a complete story, i.e. the discussed aspect, the sentiment on it, and the cause of the sentiment. However, no previous ABSA research tried to provide a complete solution in one shot. In this paper, we introduce a new subtask under ABSA, named aspect sentiment triplet extraction (ASTE). Particularly, a solver of this task needs to extract triplets (What, How, Why) from the inputs, which show WHAT the targeted aspects are, HOW their sentiment polarities are and WHY they have such polarities (i.e. opinion reasons). For instance, one triplet from “Waiters are very friendly and the pasta is simply average” could be (‘Waiters’, positive, ‘friendly’). We propose a two-stage framework to address this task. The first stage predicts what, how and why in a unified model, and then the second stage pairs up the predicted what (how) and why from the first stage to output triplets. In the experiments, our framework has set a benchmark performance in this novel triplet extraction task. Meanwhile, it outperforms a few strong baselines adapted from state-of-the-art related methods.",,,,,,
211,Astock,Stock Market Prediction,Stock Market Prediction,"Stock Market Prediction, Stock Price Prediction, Stock Prediction, Stock Trend Prediction, Text-Based Stock Prediction","Text, Time Series",English,Natural Language Processing,"stock-market-prediction-on-astock, stock-price-prediction-on-astock",,https://github.com/JinanZou/Astock,https://paperswithcode.com/dataset/astock,(1) provide financial news for each specific stock. (2) provide various stock technical factors and fundamental factors for each stock.,,,,,,
212,ASVspoof_2019,Voice Anti-spoofing,Voice Anti-spoofing,Voice Anti-spoofing,Audio,,Audio,,,https://www.asvspoof.org/index2019.html,https://paperswithcode.com/dataset/asvspoof-2019,,,,,,,
213,ASVspoof_2021,Audio Deepfake Detection,Audio Deepfake Detection,Audio Deepfake Detection,"Audio, Image",,Audio,audio-deepfake-detection-on-asvspoof-2021,,https://www.asvspoof.org/index2021.html,https://paperswithcode.com/dataset/asvspoof-2021,"Benchmarking initiatives support the meaningful comparison of competing solutions to prominent problems in speech and language processing. Successive benchmarking evaluations typically reflect a progressive evolution from ideal lab conditions towards to those encountered in the wild. ASVspoof, the spoofing and deepfake detection initiative and challenge series, has followed the same trend. This article provides a summary of the ASVspoof 2021 challenge and the results of 54 participating teams that submitted to the evaluation phase. For the logical access (LA) task, results indicate that countermeasures are robust to newly introduced encoding and transmission effects. Results for the physical access (PA) task indicate the potential to detect replay attacks in real, as opposed to simulated physical spaces, but a lack of robustness to variations between simulated and real acoustic environments. The Deepfake (DF) task, new to the 2021 edition, targets solutions to the detection of manipulated, compressed speech data posted online. While detection solutions offer some resilience to compression effects, they lack generalization across different source datasets. In addition to a summary of the top-performing systems for each task, new analyses of influential data factors and results for hidden data subsets, the article includes a review of post-challenge results, an outline of the principal challenge limitations and a road-map for the future of ASVspoof.",2021,,,,,
214,Atari-HEAD,Imitation Learning,Imitation Learning,"Imitation Learning, Atari Games, Decision Making",,,Methodology,,CC BY 4.0,https://zenodo.org/record/2587121,https://paperswithcode.com/dataset/atari-head,"Atari-HEAD is a dataset of human actions and eye movements recorded while playing Atari videos games. For every game frame, its corresponding image frame, the human keystroke action, the reaction time to make that action, the gaze positions, and immediate reward returned by the environment were recorded. The gaze data was recorded using an EyeLink 1000 eye tracker at 1000Hz. The human subjects are amateur players who are familiar with the games. The human subjects were only allowed to play for 15 minutes and were required to rest for at least 15 minutes before the next trial. Data was collected from 4 subjects, 16 games, 175 15-minute trials, and a total of 2.97 million frames/demonstrations.",,https://arxiv.org/abs/1903.06754,https://arxiv.org/abs/1903.06754,,,
215,AtariARI,Dimensionality Reduction,Dimensionality Reduction,"Dimensionality Reduction, Atari Games",,,Methodology,,,https://github.com/mila-iqia/atari-representation-learning,https://paperswithcode.com/dataset/atariari,"The AtariARI (Atari Annotated RAM Interface) is an environment for representation learning. The Atari Arcade Learning Environment (ALE) does not explicitly expose any ground truth state information. However, ALE does expose the RAM state (128 bytes per timestep) which are used by the game programmer to store important state information such as the location of sprites, the state of the clock, or the current room the agent is in. To extract these variables, the dataset creators consulted commented disassemblies (or source code) of Atari 2600 games which were made available by Engelhardt and Jentzsch and CPUWIZ. The dataset creators were able to find and verify important state variables for a total of 22 games. Once this information was acquired, combining it with the ALE interface produced a wrapper that can automatically output a state label for every example frame generated from the game. The dataset creators make this available with an easy-to-use gym wrapper, which returns this information with no change to existing code using gym interfaces.",,https://arxiv.org/pdf/1906.08226.pdf,https://arxiv.org/pdf/1906.08226.pdf,,,
216,Atari_Grand_Challenge,Atari Games,Atari Games,"Atari Games, Imitation Learning, Speech Recognition","Audio, Image, Text",English,Computer Vision,,,https://github.com/yobibyte/atarigrandchallenge,https://paperswithcode.com/dataset/atari-grand-challenge,"The Atari Grand Challenge dataset is a large dataset of human Atari 2600 replays. It consists of replays for 5 different games:
* Space Invaders (445 episodes, 2M frames)
* Q*bert (659 episodes, 1.6M frames)
* Ms.Pacman (384 episodes, 1.7M frames)
* Video Pinball (211 episodes, 1.5M frames)
* Montezuma’s revenge (668 episodes, 2.7M frames)",,https://arxiv.org/pdf/1705.10998.pdf,https://arxiv.org/pdf/1705.10998.pdf,,,
217,ATEPP,Music Performance Rendering,Music Performance Rendering,"Music Performance Rendering, Music Classification","Audio, Image",,Computer Vision,,,https://github.com/tangjjbetsy/ATEPP,https://paperswithcode.com/dataset/atepp,ATEPP is a dataset of expressive piano performances by virtuoso pianists. The dataset contains 11677 performances (~1000 hours) by 49 pianists and covers 1580 movements by 25 composers. All of the MIDI files in the dataset come from the piano transcription of existing audio recordings of piano performances. Scores in MusicXML format are also available for around half of the tracks. The dataset is organized and aligned by compositions and movements for comparative studies.,,ATEPP: A Dataset of Automatically Transcribed Expressive Piano Performances,https://archives.ismir.net/ismir2022/paper/000053.pdf,,,
218,ATIS,Intent Discovery,Intent Discovery,"Intent Discovery, Semantic Parsing, Intent Detection, Slot Filling, SQL Parsing, Out of Distribution (OOD) Detection, Open Intent Discovery","Image, Text",English,Computer Vision,"out-of-distribution-ood-detection-on-atis, open-intent-discovery-on-atis, semantic-parsing-on-atis, intent-discovery-on-atis, intent-detection-on-atis, sql-parsing-on-atis, slot-filling-on-atis",,https://github.com/howl-anderson/ATIS_dataset/blob/master/README.en-US.md,https://paperswithcode.com/dataset/atis,"The ATIS (Airline Travel Information Systems) is a dataset consisting of audio recordings and corresponding manual transcripts about humans asking for flight information on automated airline travel inquiry systems. The data consists of 17 unique intent categories. The original split contains 4478, 500 and 893 intent-labeled reference utterances in train, development and test set respectively.",,Spoken Language Intent Detection using Confusion2Vec,https://arxiv.org/abs/1904.03576,,,
219,Atlas,Image Classification,Image Classification,"Image Classification, Product Categorization",Image,,Computer Vision,,,https://github.com/vumaasha/atlas,https://paperswithcode.com/dataset/atlas,"Atlas is a dataset for e-commerce clothing product categorization. The Atlas dataset consists of a high-quality product taxonomy dataset focusing on clothing products which contain 186,150 images under clothing category with 3 levels and 52 leaf nodes in the taxonomy.",,Umaashankaret et al,https://arxiv.org/pdf/1908.08984.pdf,150 images,,
220,ATMs_fault_prediction,Time Series Classification,Time Series Classification,"Time Series Classification, Time Series Anomaly Detection","Image, Time Series",,Time Series,,,https://github.com/victormvy/sigma-convkernels,https://paperswithcode.com/dataset/atms-fault-prediction,"The collected dataset consists of multivariate time series (MTS) data belonging to several ATMs banking along with the annotations that the operators did when they performed a maintenance task on any of the machines.

Each sample is a MTS with 144 points, associated with all the 10-minute time windows of that day and 38 dimensions. Each dimension is related to a command type and response type, where the value of each point in the time series represents the number of occurrences of the associated command and response since the last failure event., i.e. in this cycle of failure. Therefore, the time series value is accumulated until the next cycle begins. Additionally, some extra information is included for each sample, such as the cycle of failure and the machine identifier, which can be used to create data partitions without mixing different machines.

The dataset and the labels assigned to each sample was used in the original work to perform a binary classification problem addressed by ML techniques. The goal of the problem was to predict whether a failure will occur within the next 7 days, using only the information from the current day (accumulated since the last error), which is based on an event-log. 

Potential use cases of the dataset:
•   multivariate time series classification-regression-forecasting methodologies;
•   feature learning- feature extraction approaches; 
•   predictive maintenance tasks: failure classification, failure prediction, anomaly detection.",,,,,,
221,ATOMIC,Language Modelling,Language Modelling,"Language Modelling, Question Answering, Knowledge Graphs",Text,English,Natural Language Processing,,,https://homes.cs.washington.edu/~msap/atomic/,https://paperswithcode.com/dataset/atomic,"ATOMIC is an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., ""if X pays Y a compliment, then Y will likely return the compliment"").",,,,,,
222,Audio-alpaca,Audio Generation,Audio Generation,Audio Generation,"Audio, Text",English,Audio,,,https://huggingface.co/datasets/declare-lab/audio-alpaca,https://paperswithcode.com/dataset/audio-alpaca,"Audio-alpaca: A preference dataset for aligning text-to-audio models
Audio-alpaca is a pairwise preference dataset containing about 15k (prompt,chosen, rejected) triplets where given a textual prompt, chosen is the preferred generated audio and rejected is the undesirable audio.

Field details
prompt: Given textual prompt

chosen: The preferred audio sample

rejected: The rejected audio sample",,,,,,
223,AudioCaps,Retrieval-augmented Few-shot In-context Audio Captioning,Retrieval-augmented Few-shot In-context Audio Captioning,"Retrieval-augmented Few-shot In-context Audio Captioning, Zero-shot Audio Captioning, Audio/Video to Text Retrieval, Audio Generation, Target Sound Extraction, Audio to Text Retrieval, Audio captioning, Text to Audio Retrieval, Zero-shot Text to Audio Retrieval, Text to Audio/Video Retrieval, Zero-Shot Audio Retrieval","Audio, Image, Text, Video",English,Audio,"zero-shot-audio-retrieval-on-audiocaps, text-to-audio-video-retrieval-on-audiocaps, retrieval-augmented-few-shot-in-context-audio, zero-shot-audio-captioning-on-audiocaps, target-sound-extraction-on-audiocaps, audio-generation-on-audiocaps, audio-video-to-text-retrieval-on-audiocaps, audio-captioning-on-audiocaps, zero-shot-text-to-audio-retrieval-on, audio-to-text-retrieval-on-audiocaps, text-to-audio-retrieval-on-audiocaps",,https://audiocaps.github.io/,https://paperswithcode.com/dataset/audiocaps,"AudioCaps is a dataset of sounds with event descriptions that was introduced for the task of audio captioning, with sounds sourced from the AudioSet dataset. Annotators were provided the audio tracks together with category hints (and with additional video hints if needed).",,,,,,
224,AudioSet,Audio Source Separation,Audio Source Separation,"Audio Source Separation, Audio Classification, Multi-modal Classification, Target Sound Extraction, Audio Tagging, Zero-shot Audio Classification","Audio, Image",,Audio,"audio-source-separation-on-audioset, multi-modal-classification-on-audioset, audio-classification-on-audioset, target-sound-extraction-on-audioset, audio-tagging-on-audioset, zero-shot-audio-classification-on-audioset",CC BY 4.0,https://research.google.com/audioset/index.html,https://paperswithcode.com/dataset/audioset,"Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set.",,Curriculum Audiovisual Learning,https://arxiv.org/abs/2001.09414,,,
225,AudioSim,Clustering,Clustering,"Clustering, Zero-Shot Audio Retrieval",Audio,,Audio,,CC BY 4.0,,https://paperswithcode.com/dataset/audiosim,TBE,,,,,,
226,Audio_de_mosquitos_Aedes_Aegypti,Sound Classification,Sound Classification,Sound Classification,"Audio, Image",,Computer Vision,,Creative Commons Attribution 4.0 International,https://www.inf.ufrgs.br/aedes-vigilance/,https://paperswithcode.com/dataset/audio-de-mosquitos-aedes-aegypti,"Dataset Description:

The dataset comprises audio recordings of the wing beats of Aedes aegypti mosquitoes and others, conducted in a semi-controlled environment. It encompasses approximately 18,706 seconds of recording, with properly labeled samples.

Dataset Characteristics:


Data Type: Audio recordings.
Total Duration: Approximately 18,706 seconds.
Labeling: Labeled samples.
Environment: Semi-controlled.
Data Type: Audio recordings.
Total Duration: Approximately 18,706 seconds.
Species: Aedes aegypti and others.",,,,,,
227,AugMod,Automatic Modulation Recognition,Automatic Modulation Recognition,Automatic Modulation Recognition,Image,,Computer Vision,,MIT,https://www.kaggle.com/datasets/hdumasde/pythagoremodreco,https://paperswithcode.com/dataset/augmod,"Context
A radio signal consists in two channels, channel I (for 'In phase') and channel Q (for 'Quadrature') and can be assimilated as a stream of complex numbers.
It may convey information by coding it as a sequence of symbols sampled from a finite set of complex numbers called a ""modulation"".
There exist several standard modulations such as (non exhaustive list): BPSK, QAM, QPSK of order N, PSK of order N…

In general modulation is not directly observable from a signal.
The goal of this dataset is to detect the underlying modulation of a radio signal which may have suffer various alterations during its transmission.
This task is of interest for instance for sensing the electromagnetic environment in the cognitive radio paradigm.

This dataset is made available in the context of the paper:
T. Courtat and H. du Mas des Bourboux, ""A light neural network for modulation detection under impairments,"" 2021 International Symposium on Networks, Computers and Communications (ISNCC), 2021, pp. 1-7, doi: 10.1109/ISNCC52172.2021.9615851.
Please visit https://github.com/ThalesGroup/pythagore-mod-reco for the libraries to read the data and train neural networks on this dataset.

Content
The given dataset:

is given in a hdf5 file
is composed of 7 classes: BPSK, PSK8, QAM16, QAM32, QAM64, QAM8, QPSK
spans 5 bins in signal-to-noise ration: 0, 10, 20, 30, 40
consists of 174 720 examples, each 1024 samples long with both I and Q.
Two notebooks allow to:

visualize the data: plot-one-sample
train a classifiers: training-example",2021,,,720 examples,,7
228,australian,Image/Document Clustering,Image/Document Clustering,Image/Document Clustering,"Image, Text",English,Computer Vision,image-document-clustering-on-australian,,https://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval),https://paperswithcode.com/dataset/australian,"Data Set Information:

This file concerns credit card applications. All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data.

This dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values. There are also a few missing values.",,,,,,
229,AUTH_at_BioLaySumm_2024_-_Bringing_Scientific_Cont,Lay Summarization,Lay Summarization,Lay Summarization,Text,English,Natural Language Processing,,CC0,https://www.sciencejournalforkids.org/,https://paperswithcode.com/dataset/auth-at-biolaysumm-2024-bringing-scientific,"Science Journal for Kids Data
This repository contains a dataset of abstracts from the Science Journal for Kids website and the original academic papers. It includes metadata such as titles, URLs, reading levels, and links to the full academic papers. The dataset is designed to support research and analysis of educational content tailored for young learners.

Data
The dataset is a curated collection of 284 original scientific abstracts and their adapted abstracts for children. These summaries are sourced from the Science Journal for Kids (SJK), a platform dedicated to making scientific research accessible and engaging for younger audiences. The dataset covers a diverse range of subjects, including biology, chemistry, health, environmental science, and more. The CSV file is approximately 716 KB in size.

Data Format
The dataset includes the following columns:
- Category: The category or categories to which the article belongs. This can include areas such as Biodiversity, Conservation, Biology, Chemistry, Health, and Environmental Science.
- Title: The title of the article, providing a concise description of the main topic or finding of the research.
- Kids Abstract: The abstract of the article aimed at a younger audience.
- Abstract (Original academic paper): The abstract from the original academic paper.
- URL (Original academic paper): The URL to access the original academic paper.
- Reading Levels: The suggested reading levels for the lay summary, which may include Elementary school, Middle school, Lower high school, and Upper high school. 

Citing this Repository
If you use this dataset in your research, please cite it using the following BibTeX entry:

@inproceedings{BioLaySumm_2024,
  author       = {Loukritia Stefanou and Tatiana Passali and Grigorios Tsoumakas},
  title        = {AUTH at BioLaySumm 2024: Bringing Scientific Content to Kids},
  booktitle    = {Proceedings of the ACL 2024 BioNLP Workshop},
  year         = {2024},
  address      = {Bangkok, Thailand},
  note         = {A paper presented at the BioLaySumm 2024 shared task on lay summarization of biomedical research articles.}
}",2024,,,,,
230,AUTH_at_BioLaySumm_2024__Bringing_Scientific_Conte,Text Summarization,Text Summarization,"Text Summarization, Lay Summarization",Text,English,Natural Language Processing,,CC0,https://www.sciencejournalforkids.org/,https://paperswithcode.com/dataset/loukritia-stefanou,"Science Journal for Kids Data
This repository contains a dataset of abstracts from the Science Journal for Kids website and the original academic papers. It includes metadata such as titles, URLs, reading levels, and links to the full academic papers. The dataset is designed to support research and analysis of educational content tailored for young learners.

Data
The dataset is a curated collection of 284 original scientific abstracts and their adapted abstracts for children. These summaries are sourced from the Science Journal for Kids (SJK), a platform dedicated to making scientific research accessible and engaging for younger audiences. The dataset covers a diverse range of subjects, including biology, chemistry, health, environmental science, and more. The CSV file is approximately 716 KB in size.

Data Format
The dataset includes the following columns:
- Category: The category or categories to which the article belongs. This can include areas such as Biodiversity, Conservation, Biology, Chemistry, Health, and Environmental Science.
- Title: The title of the article, providing a concise description of the main topic or finding of the research.
- Kids Abstract: The abstract of the article aimed at a younger audience.
- Abstract (Original academic paper): The abstract from the original academic paper.
- URL (Original academic paper): The URL to access the original academic paper.
- Reading Levels: The suggested reading levels for the lay summary, which may include Elementary school, Middle school, Lower high school, and Upper high school. 

Citing this Repository
If you use this dataset in your research, please cite it using the following BibTeX entry:

@inproceedings{BioLaySumm_2024,
  author       = {Loukritia Stefanou and Tatiana Passali and Grigorios Tsoumakas},
  title        = {AUTH at BioLaySumm 2024: Bringing Scientific Content to Kids},
  booktitle    = {Proceedings of the ACL 2024 BioNLP Workshop},
  year         = {2024},
  address      = {Bangkok, Thailand},
  note         = {A paper presented at the BioLaySumm 2024 shared task on lay summarization of biomedical research articles.}
}",2024,,,,,
231,Auto-KWS,Keyword Spotting,Keyword Spotting,Keyword Spotting,,,Methodology,,,https://www.4paradigm.com/competition/autospeech2021,https://paperswithcode.com/dataset/auto-kws,"Auto-KWS is a dataset for customized keyword spotting, the task of detecting spoken keywords. The dataset closely resembles real world scenarios, as each recorder is assigned with an unique wake-up word and can choose their recording environment and familiar dialect freely.

All data is recorded by near-field mobile phones, (located in front of the speakers at around 0.2m distance). Each sample is recorded in single channel, 16-bit streams at a 16kHz sampling rate. There are 4 datasets: training dataset, practice dataset, feedback dataset, and private dataset. Training dataset, recorded from around 100 recorders, is used for participants to develop Auto-KWS solutions. Practice dataset contains 5 speakers data, each with 5 enrollment audio data and seveal test audio. Practice dataset together with the downloadable docker provides an example of how platform would call the participants' code. Both Training and practice dataset can be downloaded for local debugging. The feedback dataset and private dataset have the same format of practice dataset and are used for final evaluation and thus will be hiden from participants.",,,,,,
232,AutoPET,Tumor Segmentation,Tumor Segmentation,"Tumor Segmentation, Lesion Detection, Lesion Segmentation",Image,,Computer Vision,,,,https://paperswithcode.com/dataset/autopet,"A whole-body FDG-PET/CT dataset with manually annotated tumor lesions (FDG-PET-CT-Lesions)
1,014 studies (900 patients)",,,,,,
233,AutoRobust,Malware Classification,Malware Classification,"Malware Classification, Behavioral Malware Classification, Malware Analysis, Malware Detection, Binary Classification, Multi-class Classification, Malware Family Detection, Classification",Image,,Computer Vision,,CC-BY,https://www.kaggle.com/datasets/greimas/malware-and-goodware-dynamic-analysis-reports,https://paperswithcode.com/dataset/autorobust,"This dataset is comprised of the dynamic analysis reports generated by CAPEv2, from both malware and goodware. We source the goodware as they do in Dambra et al. (https://arxiv.org/abs/2307.14657), where trough the community-maintained packages of Chocolatey they create a dataset that spans 2012 to 2020.
The malware are sourced from VirusTotal, namely samples of Portable Executable from 2017 - 2020 that they release for academic purposes.
In total, the dataset we assembled contains 26,200 PE samples: 8,600 (33\%) goodware and 17,675 (67\%) malware.

All samples were executed in a series of Windows 7 VMware virtual machines and without network connectivity due to policy/ethical constraints. These were orchestrated through CAPEv2, a maintained open-source successor of Cuckoo. This framework is widely used for analyzing and detecting potentially malicious binaries by executing them in an isolated environment and observing their behavior without risking the security of the host system. Every sample was executed for 150 seconds, an empirical lower bound on the time required to gather the full behavior of samples.

We tried to mitigate evasive checks by running the VMwareCloak script to remove well-known artifacts introduced by VMware. Moreover, we populated the filesystem with documents and other common types of files, to resemble a legitimate desktop workstation that malware may identify as a valuable target.
The dynamic analysis output is a detailed report with information on the syscalls invoked by the binary and all the relative flags and arguments, as well as all interactions with the file system, registry, network, and other key elements of the operating system.

The final dataset contains only the dynamic analysis reports, without the original binaries.",2012,,,,"traints. These were orchestrated through CAPEv2, a maintained open-source successor of Cuckoo. This framework is widely used for analyzing and detecting potentially malicious binaries by executing them in an isolated environment and observing their behavior without risking the security of the host system. Every sample was executed for 150 seconds, an empirical lower bound on the time required to gather the full behavior of samples",
234,AuxAD,Sentence Embeddings,Sentence Embeddings,Sentence Embeddings,,,Methodology,,,https://github.com/PrimerAI/sdu-data,https://paperswithcode.com/dataset/auxad,AuxAD is a a distantly supervised dataset for acronym disambiguation.,,,,,,
235,AuxAI,Sentence Embeddings,Sentence Embeddings,Sentence Embeddings,,,Methodology,,,https://github.com/PrimerAI/sdu-data,https://paperswithcode.com/dataset/auxai,AuxAI is a distantly supervised dataset for acronym identification.,,,,,,
236,AVA-ActiveSpeaker,Audio-Visual Active Speaker Detection,Audio-Visual Active Speaker Detection,"Audio-Visual Active Speaker Detection, Speaker Diarization, Self-Supervised Learning, Speech Enhancement","Audio, Image",,Audio,audio-visual-active-speaker-detection-on-ava,,https://research.google.com/ava/download.html#ava_active_speaker_download,https://paperswithcode.com/dataset/ava-activespeaker,"Contains temporally labeled face tracks in video, where each face instance is labeled as speaking or not, and whether the speech is audible. This dataset contains about 3.65 million human labeled frames or about 38.5 hours of face tracks, and the corresponding audio.",,,,,,
237,AVA-Speech,Action Detection,Action Detection,"Action Detection, Activity Detection, Speaker Diarization","Audio, Image, Video",,Computer Vision,activity-detection-on-ava-speech,,https://research.google.com/ava/,https://paperswithcode.com/dataset/ava-speech,"Contains densely labeled speech activity in YouTube videos, with the goal of creating a shared, available dataset for this task.",,,,,,
238,AVA,Video Understanding,Video Understanding,"Video Understanding, Action Recognition In Videos, Node Classification, Speaker Diarization, Spatio-Temporal Action Localization, Action Recognition, Activity Detection, Speech Enhancement, Action Detection, Aesthetics Quality Assessment, Audio-Visual Active Speaker Detection, Gaze Estimation, Self-Supervised Learning","Audio, Image, Time Series, Video",,Computer Vision,"spatio-temporal-action-localization-on-ava, action-recognition-in-videos-on-ava-v2-2, activity-detection-on-ava-speech, action-recognition-on-ava-v2-2, audio-visual-active-speaker-detection-on-ava, node-classification-on-ava, action-recognition-in-videos-on-ava-v21, action-recognition-in-videos-on-ava-v2-1, aesthetics-quality-assessment-on-ava",CC BY 4.0,https://research.google.com/ava/,https://paperswithcode.com/dataset/ava,"AVA is a project that provides audiovisual annotations of video for improving our understanding of human activity. Each of the video clips has been exhaustively annotated by human annotators, and together they represent a rich variety of scenes, recording conditions, and expressions of human activity. There are annotations for:


Kinetics (AVA-Kinetics) - a crossover between AVA and Kinetics. In order to provide localized action labels on a wider variety of visual scenes, authors provide AVA action labels on videos from Kinetics-700, nearly doubling the number of total annotations, and increasing the number of unique videos by over 500x. 
Actions (AvA Actions) - the AVA dataset densely annotates 80 atomic visual actions in 430 15-minute movie clips, where actions are localized in space and time, resulting in 1.62M action labels with multiple labels per human occurring frequently. 
Spoken Activity (AVA ActiveSpeaker, AVA Speech). AVA ActiveSpeaker: associates speaking activity with a visible face, on the AVA v1.0 videos, resulting in 3.65 million frames labeled across ~39K face tracks. AVA Speech densely annotates audio-based speech activity in AVA v1.0 videos, and explicitly labels 3 background noise conditions, resulting in ~46K labeled segments spanning 45 hours of data.",,,,,,
239,Avalon,Reinforcement Learning (RL),Reinforcement Learning (RL),"Reinforcement Learning (RL), General Reinforcement Learning",,,Reinforcement Learning,,,https://generallyintelligent.com/avalon,https://paperswithcode.com/dataset/avalon,"Avalon is a benchmark for generalization in Reinforcement Learning (RL). The benchmark consists of a set of tasks in which embodied agents in highly diverse procedural 3D worlds must survive by navigating terrain, hunting or gathering food, and avoiding hazards. Avalon is unique among existing RL benchmarks in that the reward function, world dynamics, and action space are the same for every task, with tasks differentiated solely by altering the environment; its 20 tasks, ranging in complexity from eat and throw to hunt and navigate, each create worlds in which the agent must perform specific skills in order to survive. This benchmark setup enables investigations of generalization within tasks, between tasks, and to compositional tasks that require combining skills learned from previous tasks.",,"Avalon: A Benchmark for RL Generalization Using

Procedurally Generated Worlds",https://arxiv.org/pdf/2210.13417v1.pdf,,,
240,AVD,Object Detection,Object Detection,"Object Detection, Visual Navigation, Scene Generation, Domain Adaptation","Image, Text",English,Computer Vision,scene-generation-on-avd,,http://cs.unc.edu/~ammirato/active_vision_dataset_website/,https://paperswithcode.com/dataset/avd,"AVD focuses on simulating robotic vision tasks in everyday indoor environments using real imagery. The dataset includes 20,000+ RGB-D images and 50,000+ 2D bounding boxes of object instances densely captured in 9 unique scenes.",,,,,,
241,AVE,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Temporal Localization, Multiple Instance Learning","Image, Time Series, Video",,Computer Vision,,,https://sites.google.com/view/audiovisualresearch,https://paperswithcode.com/dataset/ave,"To investigate three temporal localization tasks: supervised and weakly-supervised audio-visual event localization, and cross-modality localization.",,,,,,
242,AVeriTeC,Fact Checking,Fact Checking,"Fact Checking, Fact Verification",,,Methodology,fact-checking-on-averitec,,https://fever.ai/dataset/averitec.html,https://paperswithcode.com/dataset/averitec,"AVeriTeC (Automated Verification of Textual Claims) is a dataset of 4568 real-world claims covering fact-checks by 50 different organizations. Each claim is annotated with question-answer pairs supported by evidence available online, as well as textual justifications explaining how the evidence combines to produce a verdict. The Claims in AVeriTeC are classified into four labels: ""Supported"", ""Refuted"", ""Not Enough Evidence"", ""Conflicting Evidence/Cherry-picking"". The dataset also contains several fields of metadata such as the speaker of the claim, the publisher of the claim, the date the claim was published, and the location most relevant to the claim. These can be used to support questions, answers, and justifications.",,,,,,
243,AViMoS,Video Saliency Detection,Video Saliency Detection,"Video Saliency Detection, Saliency Prediction, Saliency Detection, Video Saliency Prediction","Image, Time Series, Video",,Computer Vision,,CC-BY,https://challenges.videoprocessing.ai/challenges/video-saliency-prediction.html,https://paperswithcode.com/dataset/avimos,"A novel audio-visual mouse saliency (AViMoS) dataset with the following key-features:



Diverse content: movie, sports, live, vertical videos, etc.;



Large scale: 1500 videos with mean 19s duration;



High resolution: all streams are FullHD;



Audio track saved and played to observers;



Mouse fixations from >5000 observers (>70 per video);



License: CC-BY;",,,,,,
244,AVSBench,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Video Object Segmentation","Image, Video",,Computer Vision,,Apache-2.0 license,http://www.avlbench.opennlplab.cn/download,https://paperswithcode.com/dataset/avsbench,"AVSBench is a pixel-level audio-visual segmentation benchmark that provides ground truth labels for sounding objects. The dataset is divided into three subsets: AVSBench-object (Single-source subset, Multi-sources subset) and AVSBench-semantic (Semantic-labels subset). Accordingly, three settings are studied: 

1) semi-supervised audio-visual segmentation with a single sound source

2) fully-supervised audio-visual segmentation with multiple sound sources

3) fully-supervised audio-visual semantic segmentation",,Audio-Visual Segmentation with Semantics,https://arxiv.org/pdf/2301.13190v1.pdf,,,
245,AVSpeech,Audio Source Separation,Audio Source Separation,Audio Source Separation,Audio,,Audio,,CC BY 4.0,https://looking-to-listen.github.io/avspeech/,https://paperswithcode.com/dataset/avspeech,"AVSpeech is a large-scale audio-visual dataset comprising
speech clips with no interfering background signals. The segments
are of varying length, between 3 and 10 seconds long, and in each clip
the only visible face in the video and audible sound in the soundtrack
belong to a single speaking person. In total, the dataset contains
roughly 4700 hours of video segments with approximately 150,000
distinct speakers, spanning a wide variety of people, languages
and face poses.",,,,,,
246,AVSync15,Video Generation,Video Generation,"Video Generation, Audio-Visual Synchronization","Audio, Image, Text, Video",English,Computer Vision,,,https://lzhangbj.github.io/projects/asva/asva.html,https://paperswithcode.com/dataset/avsync15,"AVSync15 is a high-quality synchronized audio-video dataset curated from VGGSound. It is carefully curated with both automatic and manual steps, ensuring:

High semantic and temporal audio-visual correlation
Clean and stable audio-visual contents
Rich in audio-video synchronization clues
Diverse in categories",,,,,,
247,AVS_Benchmark,Audio-Visual Synchronization,Audio-Visual Synchronization,Audio-Visual Synchronization,"Audio, Image",,Audio,,Apache-2.0,https://github.com/amazon-science/avgen-eval-toolkit,https://paperswithcode.com/dataset/avs-benchmark,Provided in the linked paper.,,,,,,
248,AV_Digits_Database,Visual Speech Recognition,Visual Speech Recognition,"Visual Speech Recognition, Speech Recognition","Audio, Image, Text",English,Speech,,,https://ibug-avs.eu/,https://paperswithcode.com/dataset/av-digits-database,"AV Digits Database is an audiovisual database which contains normal, whispered and silent speech. 53 participants were recorded from 3 different views (frontal, 45 and profile) pronouncing digits and phrases in three speech modes.

The database consists of two parts: digits and short phrases. In the first part, participants were asked to read 10 digits, from 0 to 9, in English in random order five times. In case of non-native English speakers this part was also repeated in the participant’s native language. In total, 53 participants (41 males and 12 females) from 16 nationalities, were recorded with a mean age and standard deviation of 26.7 and 4.3 years, respectively.

In the second part, participants were asked to read 10 short phrases. The phrases are the same as the ones used in the OuluVS2 database: “Excuse me”, “Goodbye”, “Hello”, “How are you”, “Nice to meet you”, “See you”, “I am sorry”,   “Thank you”, “Have a good time”, “You are welcome”. Again, each phrase was repeated five times in 3 different modes, neutral, whisper and silent speech. Thirty nine participants (32 males and 7 females) were recorded for this part with a mean age and standard deviation of 26.3 and 3.8 years, respectively.",,,,,,
249,AwA,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Long-tail learning with class descriptors, Generalized Zero-Shot Learning, Generalized Few-Shot Learning, Few-Shot Image Classification, Concept-based Classification",Image,,Computer Vision,,,https://cvml.ist.ac.at/AwA/,https://paperswithcode.com/dataset/awa-1,"Animals with Attributes (AwA) was a dataset for benchmarking transfer-learning algorithms, in particular attribute base classification. It consisted of 30475 images of 50 animals classes with six pre-extracted feature representations for each image. The animals classes are aligned with Osherson's classical class/attribute matrix, thereby providing 85 numeric attribute values for each class. Using the shared attributes, it is possible to transfer information between different classes.
The Animals with Attributes dataset was suspended. Its images are not available anymore because of copyright restrictions. A drop-in replacement, Animals with Attributes 2, is available instead.",,Transductive Multi-view Zero-Shot Learning,https://arxiv.org/abs/1501.04560,30475 images,,
250,AwA2,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Generalized Zero-Shot Learning, Generalized Few-Shot Learning, Few-Shot Image Classification, Concept-based Classification",Image,,Computer Vision,"generalized-few-shot-learning-on-awa2, concept-based-classification-on-awa2, few-shot-image-classification-on-awa2-0-shot, generalized-zero-shot-learning-on-awa2, zero-shot-learning-on-awa2",,https://cvml.ist.ac.at/AwA2/,https://paperswithcode.com/dataset/awa2-1,"Animals with Attributes 2 (AwA2) is a dataset for benchmarking transfer-learning algorithms, such as attribute base classification and zero-shot learning. AwA2 is a drop-in replacement of original Animals with Attributes (AwA) dataset, with more images released for each category. Specifically, AwA2 consists of in total 37322 images distributed in 50 animal categories. The AwA2 also provides a category-attribute matrix, which contains an 85-dim attribute vector (e.g., color, stripe, furry, size, and habitat) for each category.",,Learning from Noisy Web Data with Category-level Supervision,https://arxiv.org/abs/1803.03857,37322 images,,
251,AWARE,Sentiment Analysis,Sentiment Analysis,"Sentiment Analysis, Aspect-Based Sentiment Analysis (ABSA), Term Extraction, Aspect Category Detection, Aspect Category Polarity, Aspect Term Extraction and Sentiment Classification","Image, Text",English,Computer Vision,"aspect-category-detection-on-aware, aspect-category-polarity-on-aware, term-extraction-on-aware",,https://doi.org/10.5281/zenodo.5528481,https://paperswithcode.com/dataset/aware,"The peer-reviewed paper of AWARE dataset is published in ASEW 2021, and can be accessed through: http://doi.org/10.1109/ASEW52652.2021.00049. Kindly cite this paper when using AWARE dataset.

Aspect-Based Sentiment Analysis (ABSA) aims to identify the opinion (sentiment) with respect to a specific aspect. Since there is a lack of smartphone apps reviews dataset that is annotated to support the ABSA task, we present AWARE: ABSA Warehouse of Apps REviews.

AWARE contains apps reviews from three different domains (Productivity, Social Networking, and Games), as each domain has its distinct functionalities and audience. Each sentence is annotated with three labels, as follows: 

Aspect Term: a term that exists in the sentence and describes an aspect of the app that is expressed by the sentiment. A term value of “N/A” means that the term is not explicitly mentioned in the sentence.
Aspect Category: one of the pre-defined set of domain-specific categories that represent an aspect of the app (e.g., security, usability, etc.).
Sentiment: positive or negative.
Note: games domain does not contain aspect terms.

We provide a comprehensive dataset of 11323 sentences from the three domains, where each sentence is additionally annotated with a Boolean value indicating whether the sentence expresses a positive/negative opinion. In addition, we provide three separate datasets, one for each domain, containing only sentences that express opinions. The file named “AWARE_metadata.csv” contains a description of the dataset’s columns.

How AWARE can be used?

We designed AWARE such that it can be used to serve various tasks. The tasks can be, but are not limited to:

Sentiment Analysis.
Aspect Term Extraction.
Aspect Category Classification.
Aspect Sentiment Analysis.
Explicit/Implicit Aspect Term Classification.
Opinion/Not-Opinion Classification.
Furthermore, researchers can experiment with and investigate the effects of different domains on users' feedback.",2021,,,11323 sentences,,
252,AwA_Pose,Animal Pose Estimation,Animal Pose Estimation,"Animal Pose Estimation, Keypoint Detection","3D, Image",,Computer Vision,,,https://github.com/prinik/AwA-Pose,https://paperswithcode.com/dataset/awa-pose,AwA Pose is a large scale animal keypoint dataset with ground truth annotations for keypoint detection of quadruped animals from images.,,,,,,
253,A_Billion_Ways_to_Grasp,Grasp Generation,Grasp Generation,"Grasp Generation, Robotic Grasping, Grasp Contact Prediction","Text, Time Series",English,Natural Language Processing,,CC BY-NC,https://sites.google.com/view/abillionwaystograsp/,https://paperswithcode.com/dataset/a-billion-ways-to-grasp,"Robot grasping is often formulated as a learning problem. With the increasing speed and quality of physics simulations, generating large-scale grasping data sets that feed learning algorithms is becoming more and more popular. An often overlooked question is how to generate the grasps that make up these data sets. In this paper, we review, classify, and compare different grasp sampling strategies. Our evaluation is based on a fine-grained discretization of SE(3) and uses physics-based simulation to evaluate the quality and robustness of the corresponding parallel-jaw grasps. Specifically, we consider more than 1 billion grasps for each of the 21 objects from the YCB data set. This dense data set lets us evaluate existing sampling schemes w.r.t. their bias and efficiency. Our experiments show that some popular sampling schemes contain significant bias and do not cover all possible ways an object can be grasped.",,,,,,
254,A_collection_of_LFR_benchmark_graphs,Community Detection,Community Detection,"Community Detection, graph partitioning, Network Community Partition","Graph, Image",,Computer Vision,,Creative Commons Attribution 4.0 International,https://doi.org/10.5281/zenodo.4450167,https://paperswithcode.com/dataset/a-collection-of-lfr-benchmark-graphs,"This dataset is a collection of undirected and unweighted LFR benchmark graphs as proposed by Lancichinetti et al. [1]. We generated the graphs using the code provided by Santo Fortunato on his personal website [2], embedded in our evaluation framework [3], with two different parameter sets. Let N denote the number of vertices in the network, then

  Maximum community size: 0.2N (Set A); 0.1N (Set B)
  Minimum community size: 0.05N (Set A); 10 (Set B)
  Maximum node degree: 0.19N (Set A); 0.19N (Set B)
  Community size distribution exponent: 1.0 (Set A); 1.0 (Set B)
  Degree distribution exponent: 2.0 (Set A); 2.0 (Set B).

All other parameters assume default values. We provide graphs with different combinations of average degree, network size and mixing parameter for the given parameter sets:

Set A: For average degrees in {15, 25, 50} we provide network sizes in {300, 600, 1200}, each with 20 different mixing parameters linearly spaced in [0.2, 0.8]. For each configuration we provide 100 benchmark graphs.
Set A: For average degrees in {15, 25, 50} we provide mixing parameters in {0.35, 0.45, 0.55}, each with network sizes in {300, 450, 600, 900, 1200, 1800, 2400, 3600, 4800, 6200, 9600, 19200}. For each configuration we provide 50 benchmark graphs.
Set B: For average degrees in {20} we provide network sizes in {300, 600, 1200, 2400}, each with 20 different mixing parameters linearly spaced in [0.2, 0.8]. For each configuration we provide 100 benchmark graphs.

Benchmark graphs are given in edge list format. Further, for each benchmark graph we provide ground truth communities as membership list and as structured datatype (.json), its generating random seeds and basic network statistics.

[1] Lancichinetti A, Fortunato S, Radicchi F (2008) Benchmark graphs for testing community detection algorithms. Physical Review E 78(4):046110,https://doi.org/10.1103/PhysRevE.78.046110

[2] https://www.santofortunato.net/resources, Accessed: 19 Jan 2021

[3] https://github.com/synwalk/synwalk-analysis, Accessed: 19 Jan 2021",2008,,,,,
255,A_dataset_of_neonatal_EEG_recordings_with_seizures,Seizure Detection,Seizure Detection,Seizure Detection,Image,,Computer Vision,,,https://zenodo.org/record/4940267,https://paperswithcode.com/dataset/a-dataset-of-neonatal-eeg-recordings-with,"Neonatal seizures are a common emergency in the neonatal intensive care unit (NICU). There are many questions yet to be answered regarding the temporal/spatial characteristics of seizures from different pathologies, response to medication, effects on neurodevelopment and optimal detection. This dataset contains EEG recordings from human neonates and the visual interpretation of the EEG by the human expert. Multi-channel EEG was recorded from 79 term neonates admitted to the neonatal intensive care unit (NICU) at the Helsinki University Hospital. The median recording duration was 74 minutes (IQR: 64 to 96 minutes). EEGs were annotated by three experts for the presence of seizures. An average of 460 seizures were annotated per expert in the dataset, 39 neonates had seizures by consensus and 22 were seizure free by consensus. The dataset can be used as a reference set of neonatal seizures, for the development of automated methods of seizure detection and other EEG analysis, as well as for the analysis of inter-observer agreement.",,,,,,
256,A_Game_Of_Sorts,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Visual Grounding, Referring Expression Comprehension, Referring Expression, Referring expression generation, Task-Oriented Dialogue Systems","Image, Text",English,Computer Vision,,,https://github.com/willemsenbram/a-game-of-sorts,https://paperswithcode.com/dataset/a-game-of-sorts,A Game Of Sorts is a collaborative image ranking task. Players are asked to rank a set of images based on a given sorting criterion. The game provides a framework for the evaluation of visually grounded language understanding and generation of referring expressions in multimodal dialogue settings.,,,,,,
257,A_Software_Maintainability_Dataset,Code Classification,Code Classification,Code Classification,Image,,Computer Vision,,CC BY 4.0,https://figshare.com/articles/dataset/A_Software_Maintainability_Dataset/12801215,https://paperswithcode.com/dataset/a-software-maintainability-dataset,"This dataset contains 304 manual evaluations of class-level software maintainability, drawn from 5 open-source projects:
ArgoUML,
Art of Illusion,
Diary Management,
JUnit 4,
JSweet.
Each Java class is labelled along 5 axis:
readability, understandability, complexity, modularity and overall maintainability.
Each Java class was assessed by several experts independently of its relation to other classes.

It can be used to develop and evaluate automated quality prediction tools.",,,,,,
258,B-T4SA,Multimodal Sentiment Analysis,Multimodal Sentiment Analysis,Multimodal Sentiment Analysis,Text,English,Multimodal,multimodal-sentiment-analysis-on-b-t4sa,,http://www.t4sa.it/,https://paperswithcode.com/dataset/b-t4sa,,,,,,,
259,BA-2motifs,Explanation Fidelity Evaluation,Explanation Fidelity Evaluation,"Explanation Fidelity Evaluation, Graph Classification","Graph, Image",,Computer Vision,explanation-fidelity-evaluation-on-ba-2motifs,,,https://paperswithcode.com/dataset/ba-2motifs,"It's a synthetic dataset, which contains 1000 graphs divided into two classes according to the motif they contain: either a “house” or a five-node cycle.",,,,,,
260,BA,Image/Document Clustering,Image/Document Clustering,Image/Document Clustering,"Image, Text",English,Computer Vision,image-document-clustering-on-ba,,https://github.com/Li-Hongmin/MyPaperWithCode/blob/main/Ensemble-Learning-for-Spectral-Clustering/Datasets/BAdata.mat,https://paperswithcode.com/dataset/ba,,,,,,,
261,BAAI-VANJEE,Object Detection,Object Detection,"Object Detection, Radar Object Detection",Image,,Computer Vision,,,https://data.baai.ac.cn/data-set,https://paperswithcode.com/dataset/baai-vanjee,"BAAI-VANJEE is a dataset for benchmarking and training various computer vision tasks such as 2D/3D object detection and multi-sensor fusion. The BAAI-VANJEE roadside dataset consists of LiDAR data and RGB images collected by VANJEE smart base station placed on the roadside about 4.5m high. This dataset contains 2500 frames of LiDAR data, 5000 frames of RGB images, including 20% collected at the same time. It also contains 12 classes of objects, 74K 3D object annotations and 105K 2D object annotations.",,,,,training various computer vision tasks such as 2D/3D object detection and multi-sensor fusion. The BAAI-VANJEE roadside dataset consists of LiDAR data and RGB images,12
262,BABEL,3D Action Recognition,3D Action Recognition,"3D Action Recognition, Action Classification, Temporal Action Localization, Action Recognition","3D, Image, Time Series, Video",,Computer Vision,action-classification-on-babel,Custom,https://babel.is.tue.mpg.de/,https://paperswithcode.com/dataset/babel-1,"BABEL is a large dataset with language labels describing the actions being performed in mocap sequences. BABEL consists of action labels for about 43 hours of mocap sequences from AMASS. Action labels are at two levels of abstraction --  sequence labels describe the overall action in the sequence, and frame labels describe all actions in every frame of the sequence. Each frame label is precisely aligned with the duration of the corresponding action in the mocap sequence, and multiple actions can overlap. There are over 28k sequence labels, and 63k frame labels in BABEL, which belong to over 250 unique action categories. Labels from BABEL can be leveraged for tasks like action recognition, temporal action localization, motion synthesis, etc.",,,,,,
263,BabySLM,Language Acquisition,Language Acquisition,Language Acquisition,Text,English,Natural Language Processing,,,https://github.com/MarvinLvn/BabySLM,https://paperswithcode.com/dataset/babyslm,"BabySLM is a language-acquisition-friendly benchmark to probe speech-based LMs at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences.",,BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models,https://arxiv.org/pdf/2306.01506v1.pdf,,,
264,BACE__β-secretase_enzyme_,Graph Classification,Graph Classification,"Graph Classification, Drug Discovery, Molecular Property Prediction","Graph, Image, Time Series",,Computer Vision,"graph-classification-on-bace, drug-discovery-on-bace, drug-discovery-on-bace-scaffold, molecular-property-prediction-on-bace-1",,https://drugdesigndata.org/about/grand-challenge-4/bace,https://paperswithcode.com/dataset/bace-scaffold,"The BACE dataset focuses on inhibitors of human beta-secretase 1 (BACE-1).
It includes both quantitative (IC50 values) and qualitative (binary labels) binding results. The dataset comprises small molecule inhibitors across a wide range of affinities, spanning three orders of magnitude (from nanomolar to micromolar IC50 values).
Specifically, it provides:
154 BACE inhibitors for affinity prediction.
20 BACE inhibitors for pose prediction.
34 BACE inhibitors for free energy prediction.",,,,,,
265,Bactrian-X,Instruction Following,Instruction Following,Instruction Following,,,Methodology,,CC BY NC 4.0,https://github.com/mbzuai-nlp/bactrian-x,https://paperswithcode.com/dataset/bactrian-x,"Bactrian-X is a comprehensive multilingual parallel dataset of 3.4 million instruction-response pairs across 52 languages. The instructions were obtained from alpaca-52k, and dolly-15k, and tranlated into 52 languages (52 languages x 67k instances = 3.4M instances).",,Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation,https://arxiv.org/pdf/2305.15011v1.pdf,67k instances,,
266,BAF,Fairness,Fairness,"Fairness, Fraud Detection",Image,,Computer Vision,"fraud-detection-on-baf-variant-i, fraud-detection-on-baf-variant-v, fraud-detection-on-baf-variant-iv, fairness-on-baf-variant-iv, fairness-on-baf-base, fairness-on-baf-variant-v, fairness-on-baf-variant-i, fraud-detection-on-baf-variant-iii, fraud-detection-on-baf-base, fairness-on-baf-variant-iii, fraud-detection-on-baf-variant-ii, fairness-on-baf-variant-ii",Apache-2.0 license,https://github.com/feedzai/bank-account-fraud,https://paperswithcode.com/dataset/baf,"Bank Account Fraud (BAF) is a large-scale, realistic suite of tabular datasets. The suite was generated by applying state-of-the-art tabular data generation techniques on an anonymized, real-world bank account opening fraud detection dataset.",,"Turning the Tables: Biased, Imbalanced, Dynamic Tabular Datasets for ML Evaluation",https://arxiv.org/pdf/2211.13358v1.pdf,,,
267,BAIR_Robot_Pushing,Video Prediction,Video Prediction,"Video Prediction, Video Generation","Text, Time Series, Video",English,Natural Language Processing,"video-prediction-on-bair-robot-pushing-1, video-generation-on-bair-robot-pushing",,https://sites.google.com/berkeley.edu/robotic-interaction-datasets,https://paperswithcode.com/dataset/bair-robot-pushing,"Dataset of 64x64 images of a robot pushing objects on a table top. From Berkeley AI Research (BAIR).

Source: Self-Supervised Visual Planning with Temporal Skip Connections (https://arxiv.org/abs/1710.05268)

Video prediction : Conditioned on 2 frames, predict 14 frames.",,,,64 images,,
268,Ballroom,Beat Tracking,Beat Tracking,"Beat Tracking, Downbeat Tracking, Online Downbeat Tracking, Online Beat Tracking","Image, Video",,Computer Vision,"online-downbeat-tracking-on-ballroom, beat-tracking-on-ballroom, online-beat-tracking-on-ballroom, downbeat-tracking-on-ballroom",,,https://paperswithcode.com/dataset/ballroom,"This data set includes beat and bar annotations of the ballroom dataset, introduced by Gouyon et al. [1].

[1] Gouyon F., A. Klapuri, S. Dixon, M. Alonso, G. Tzanetakis, C. Uhle, and P. Cano. An experimental comparison of audio tempo induction algorithms. Transactions on Audio, Speech and Language Processing 14(5), pp.1832-1844, 2006.",2006,,,,,
269,BANDON,Building change detection for remote sensing images,Building change detection for remote sensing images,Building change detection for remote sensing images,Image,,Computer Vision,,,https://github.com/fitzpchao/BANDON,https://paperswithcode.com/dataset/bandon,"BANDON is a dataset for building change detection with off-nadir aerial images dataset, which is composed of off-Nadir image pairs of urban and rural areas. Overall, the BANDON dataset contains 2283 pairs of images, 2283 change labels,1891 BT-flows labels, 1891 pairs of segmentation labels, and 1891 pair of ST-offsets labels (test sets do not provide auxiliary annotations).",,Detecting Building Changes with Off-Nadir Aerial Images,https://arxiv.org/pdf/2301.10922v1.pdf,,,
270,BanglaLekha-Isolated,Transfer Learning,Transfer Learning,"Transfer Learning, Multi-Label Classification, Handwriting Recognition, Optical Character Recognition (OCR)",Image,,Computer Vision,"transfer-learning-on-banglalekha-isolated, handwriting-recognition-on-banglalekha",,https://data.mendeley.com/datasets/hf6sf8zrkc/2,https://paperswithcode.com/dataset/banglalekha-isolated,"This dataset contains Bangla handwritten numerals, basic characters and compound characters. This dataset was collected from multiple geographical location within Bangladesh and includes sample collected from a variety of aged groups. This dataset can also be used for other classification problems i.e: gender, age, district.",,,,,,
271,Bangla_Word_Analogy,Word Embeddings,Word Embeddings,"Word Embeddings, Word Alignment, Word Similarity",,,Methodology,,MIT,https://github.com/Mousumi44/Word-Analogy-Bangla,https://paperswithcode.com/dataset/bangla-word-analogy-1,"We provide a Mikolov-style word-analogy evaluation set specifically for Bangla, with a sample size of 16678, as well as a translated and curated version of the Mikolov dataset, which contains 10594 samples for cross-lingual research.",,,,10594 samples,"valuation set specifically for Bangla, with a sample size of 16678, as well as a translated and curated version of the Mikolov dataset, which contains 10594 samples",
272,Banglish,Voice Query Recognition,Voice Query Recognition,Voice Query Recognition,"Audio, Image",,Computer Vision,voice-query-recognition-on-banglish,,https://github.com/space-urchin/CSVC-Net/tree/main/dataset,https://paperswithcode.com/dataset/banglish,"A Bilingual Dataset for Bangla and English Voice Commands

Colloquial Bangla has adopted many English words due to colonial influence. In conversational Bangla, it is quite common to speak in a mixture of English and Bangla. This phenomenon, prevalent in conversational language is known as code-switching (CS). CS is defined as the continuous alternation between two languages in a single conversation. Thus, in Bangla natural language processing, it is often necessary to map a single base command to its many different variants - spoken in multiple mixtures of English and Bangla. In order to facilitate this, we have curated a dataset centered around common browser commands.",,,,,,
273,BANKING77-OOS,Cold-Start Anomaly Detection,Cold-Start Anomaly Detection,"Cold-Start Anomaly Detection, Intent Detection, Intent Classification",Image,,Computer Vision,cold-start-anomaly-detection-on-banking77-oos,,https://github.com/jianguoz/Few-Shot-Intent-Detection,https://paperswithcode.com/dataset/banking77-oos,"A dataset with a single banking domain, includes both general Out-of-Scope (OOD-OOS) queries and In-Domain but Out-of-Scope (ID-OOS) queries, where ID-OOS queries are semantically similar intents/queries with in-scope intents.  BANKING77 originally includes 77 intents. BANKING77-OOS includes 50 in-scope intents in this dataset, and the ID-OOS queries are built up based on 27 held-out in-scope intents.

Conduct intent detection with and without OOD-OOS and ID-OOS queries",,,,,,
274,BANKING77,Intent Detection,Intent Detection,"Intent Detection, Text Classification, Open Intent Discovery","Image, Text",English,Computer Vision,"open-intent-discovery-on-banking77, intent-detection-on-banking77-10-shot, intent-detection-on-banking77, intent-detection-on-banking77-5-shot, text-classification-on-banking77",Creative Commons Attribution 4.0 International,https://arxiv.org/abs/2003.04807,https://paperswithcode.com/dataset/banking77,"Dataset composed of online banking queries annotated with their corresponding intents.

BANKING77 dataset provides a very fine-grained set of intents in a banking domain. It comprises 13,083 customer service queries labeled with 77 intents. It focuses on fine-grained single-domain intent detection.",,Homepage,https://arxiv.org/abs/2003.04807,,,
275,Banking_CG,Open Intent Detection,Open Intent Detection,Open Intent Detection,Image,,Computer Vision,open-intent-detection-on-banking-cg,MIT,https://github.com/fangyihao/gptaug/tree/main/data/banking_cg,https://paperswithcode.com/dataset/banking-cg,"The dataset identifies the shortcomings of existing benchmarks in evaluating the problem of compositional generalization, which underscores the need for the development of datasets tailored to assess compositional generalization in open intent detection tasks.",,,,,,
276,BankNote-Net,Few-Shot Object Detection,Few-Shot Object Detection,"Few-Shot Object Detection, Contrastive Learning, Image Classification, Few-Shot Learning",Image,,Computer Vision,,CDLA-Permissive-2.0,https://github.com/microsoft/banknote-net,https://paperswithcode.com/dataset/banknote-net,"Millions of people around the world have low or no vision. Assistive software applications have been developed for a variety of day-to-day tasks, including currency recognition. To aid with this task, we present BankNote-Net, an open dataset for assistive currency recognition. The dataset consists of a total of 24,816 embeddings of banknote images captured in a variety of assistive scenarios, spanning 17 currencies and 112 denominations. These compliant embeddings were learned using supervised contrastive learning and a MobileNetV2 architecture, and they can be used to train and test specialized downstream models for any currency, including those not covered by our dataset or for which only a few real images per denomination are available (few-shot learning). We deploy a variation of this model for public use in the last version of the Seeing AI app developed by Microsoft, which has over a 100 thousand monthly active users.",,,,,"train and test specialized downstream models for any currency, including those not covered by our dataset or for which only a few real images",
277,BASEPROD,Robot Navigation,Robot Navigation,"Robot Navigation, Pose Estimation, Visual Navigation, Simultaneous Localization and Mapping, Visual Odometry","3D, Image",,Computer Vision,,Creative Commons Attribution 4.0 International,https://doi.org/10.57780/esa-xxd1ysw,https://paperswithcode.com/dataset/baseprod,"BASEPROD provides comprehensive rover sensor data collected over a 1.7 km traverse, accompanied by high-resolution 2D and 3D drone maps of the terrain. The dataset also includes laser-induced breakdown spectroscopy (LIBS) measurements from key sampling sites along the rover's path, as well as weather station data to contextualize environmental conditions.

The rover is a half-scale model of the ExoMars rover, featuring a sophisticated six-wheel triple-bogie suspension system. Each wheel assembly includes three actuators— one for wheel deployment (see e.g., wheel-walking), one for steering, and one for driving — and is equipped with a force-torque sensor. The sensor suite also comprises an RGB-D camera, a thermal camera, and a stereo camera mounted on a mast with pan-tilt capability. Additionally, the dataset contains IMU, GNSS, and gyroscope measurements, facilitating comprehensive analysis.

Potential Applications: Research in navigation, localization, mapping, odometry, and terrain classification or segmentation tasks. It provides a rich resource for developing and testing algorithms in planetary exploration conditions.",,,,,,
278,BASIL,Bias Detection,Bias Detection,"Bias Detection, Text Classification","Image, Text",English,Computer Vision,,,https://github.com/marshallwhiteorg/emnlp19-media-bias,https://paperswithcode.com/dataset/basil,"300 news articles annotated with 1,727 bias spans and find evidence that informational bias appears in news articles more frequently than lexical bias.",,,,,,
279,Basque_TimeBank,Temporal Tagging,Temporal Tagging,Temporal Tagging,"Time Series, Video",,Methodology,temporal-tagging-on-basque-timebank,None,https://link.springer.com/chapter/10.1007/978-3-319-75487-1_43,https://paperswithcode.com/dataset/basque-timebank,A set of basque documents annotated with EusTimeML - a mark-up language for temporal information in Basque.,,,,,,
280,BBAI_Dataset,Multi-agent Integration,Multi-agent Integration,"Multi-agent Integration, Conversational Response Selection",,,Methodology,multi-agent-integration-on-bbai-dataset,CC BY,https://github.com/ChrisIsKing/black-box-multi-agent-integation/tree/main/data,https://paperswithcode.com/dataset/bbai-dataset,"This dataset is for evaluating the task of Black-box Multi-agent Integration which focuses on combining the capabilities of multiple black-box conversational agents at scale. It provides data to explore two main frameworks of exploration: question agent pairing and question response pairing.

Overall this dataset contains 5550 utterances with 19 question-response pairs per question (one from each of the 19 agents), 105,450 in total across 37 domains. The utterances are split into 3700 utterances (100 examples per domain) for the training set and 1850 (50 per domain) for the test set. The train and test sets respectively contain 2399 and 1186 utterances with at least one positive question-response pair. In the remaining examples, none of the agents were able to achieve annotator agreement (>= 3).",,,,100 examples,split into 3700 utterances (100 examples,
281,BBBP__Blood-Brain_Barrier_Penetration_,Explanation Fidelity Evaluation,Explanation Fidelity Evaluation,"Explanation Fidelity Evaluation, Graph Classification, Drug Discovery, Molecular Property Prediction","Graph, Image, Time Series",,Computer Vision,"molecular-property-prediction-on-bbbp-1, drug-discovery-on-bbbp-scaffold, graph-classification-on-bbbp, drug-discovery-on-bbbp, explanation-fidelity-evaluation-on-bbbp",,https://moleculenet.org/,https://paperswithcode.com/dataset/bbbp-scaffold,"The BBBP dataset comes from a study focused on modeling and predicting the permeability of the blood-brain barrier. The BBBP dataset contains binary labels indicating whether a compound can penetrate the blood-brain barrier (BBB) or not. Researchers use this dataset to develop and evaluate machine learning methods for predicting BBB permeability. It’s a critical task because understanding which compounds can cross the BBB is essential for drug discovery and designing therapeutics for neurological conditions.

Check https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8708321/",,,,,,
282,BBH,,,", Question Answering, Multi-task Language Understanding",Text,English,Natural Language Processing,"on-big-bench-hard, multi-task-language-understanding-on-bbh-nlp, multi-task-language-understanding-on-bbh-alg, question-answering-on-bbh",,https://github.com/suzgunmirac/BIG-Bench-Hard,https://paperswithcode.com/dataset/bbh,"BIG-Bench Hard (BBH) is a subset of the BIG-Bench, a diverse evaluation suite for language models. BBH focuses on a suite of 23 challenging tasks from BIG-Bench that were found to be beyond the capabilities of current language models. These tasks are ones where prior language model evaluations did not outperform the average human-rater.

The BBH tasks require multi-step reasoning, and it was found that few-shot prompting without Chain-of-Thought (CoT), as done in the BIG-Bench evaluations, substantially underestimates the best performance and capabilities of language models. When CoT prompting was applied to BBH tasks, it enabled PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex to surpass the average human-rater performance on 17 of the 23 tasks.",,,,,,
283,BC4CHEMD,NER,NER,"NER, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,named-entity-recognition-on-bc4chemd,,https://biocreative.bioinformatics.udel.edu/resources/biocreative-iv/chemdner-corpus/,https://paperswithcode.com/dataset/bc4chemd,"Introduced by Krallinger et al. in The CHEMDNER corpus of chemicals and drugs and its annotation principles

BC4CHEMD is a collection of 10,000 PubMed abstracts that contain a total of 84,355 chemical entity mentions labeled manually by expert chemistry literature curators.",,,,,,
284,BC5CDR,UIE,UIE,"UIE, Weakly-Supervised Named Entity Recognition, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,"weakly-supervised-named-entity-recognition-on-6, weakly-supervised-named-entity-recognition-on-8, named-entity-recognition-on-bc5cdr-chemical, named-entity-recognition-ner-on-bc5cdr, weakly-supervised-named-entity-recognition-on-5, named-entity-recognition-on-bc5cdr-disease, uie-on-bc5cdr",Custom,https://github.com/JHnlp/BioCreative-V-CDR-Corpus,https://paperswithcode.com/dataset/bc5cdr,"BC5CDR corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions.",,https://arxiv.org/pdf/1805.10586.pdf,https://arxiv.org/pdf/1805.10586.pdf,,,
285,BC7_NLM-Chem,Chemical Indexing,Chemical Indexing,"Chemical Indexing, Entity Linking, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,"named-entity-recognition-on-bc7-nlm-chem, chemical-indexing-on-bc7-nlm-chem, entity-linking-on-bc7-nlm-chem",,https://biocreative.bioinformatics.udel.edu/tasks/biocreative-vii/track-2/,https://paperswithcode.com/dataset/bc7-nlm-chem,"Full-text chemical identification and indexing in PubMed articles.

Identifying named entities is an important building block for many complex knowledge extraction tasks. Errors in identifying relevant biomedical entities is a key impediment to accurate article retrieval, classification, and further understanding of textual semantics, such as relation extraction. Chemical entities appear throughout the biomedical research literature and are one of the entity types most frequently searched in PubMed. Accurate automated identification of the chemicals mentioned in journal publications has the potential to translate to improvements in many downstream NLP tasks and biomedical fields; in the near-term, specifically in the retrieval of relevant articles, greatly assisting researchers, indexers, and curators.

The NLM-CHEM track will consist of two tasks. Participants can choose to participate in either one or both. These tasks are:

Chemical identification in full text: predicting all chemicals mentioned in recently published full-text articles, both span (i.e. named entity recognition) and normalization (i.e. entity linking) using MeSH.

Chemical indexing prediction task: predicting which chemicals mentioned in recently published full-text articles should be indexed, i.e. appear in the listing of MeSH terms for the document.",,,,,,
286,BCI,Image-to-Image Translation,Image-to-Image Translation,"Image-to-Image Translation, Medical Image Generation, Medical Diagnosis, Classification Of Breast Cancer Histology Images, Breast Cancer Histology Image Classification, Image Generation, Breast Cancer Detection","Image, Text",English,Computer Vision,image-to-image-translation-on-bci,,https://bupt-ai-cz.github.io/BCI/,https://paperswithcode.com/dataset/bci,"The evaluation of human epidermal growth factor receptor 2 (HER2) expression is essential to formulate a precise treatment for breast cancer. The routine evaluation of HER2 is conducted with immunohistochemical techniques (IHC), which is very expensive. Therefore, we propose a breast cancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data directly with the paired hematoxylin and eosin (HE) stained images. The dataset contains 4870 registered image pairs, covering a variety of HER2 expression levels (0, 1+, 2+, 3+).",,,,,"valuation of human epidermal growth factor receptor 2 (HER2) expression is essential to formulate a precise treatment for breast cancer. The routine evaluation of HER2 is conducted with immunohistochemical techniques (IHC), which is very expensive. Therefore, we propose a breast cancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data directly with the paired hematoxylin and eosin (HE) stained images",
287,BCI_Competition_IV__ECoG_to_Finger_Movements,Brain Decoding,Brain Decoding,Brain Decoding,,,Methodology,brain-decoding-on-bci-competition-iv-ecog-to,,https://www.bbci.de/competition/iv/desc_4.pdf,https://paperswithcode.com/dataset/bci-competition-iv-ecog-to-hand-moves,"Prediction of Finger Flexion IV Brain-Computer Interface Data Competition
The goal of this dataset is to predict the flexion of individual fingers from
signals recorded from the surface of the brain (electrocorticography (ECoG)). This data set contains
brain signals from three subjects, as well as the time courses of the flexion of each of five fingers.
The task in this competition is to use the provided flexion information in order to predict finger
flexion for a provided test set. The performance of the classifier will be evaluated by calculating the
average correlation coefficient r between actual and predicted finger flexion.

ECoG data during individual flexions of the five fingers; movements acquired with a data glove.
[48 - 64 ECoG channels (0.15-200Hz), 1000Hz sampling rate, 3 subjects]",,Homepage,https://www.bbci.de/competition/iv/desc_4.pdf,,,
288,BCNB,Image-to-Image Translation,Image-to-Image Translation,"Image-to-Image Translation, Image Classification, Weakly Supervised Data Denoising, Medical Image Denoising, Weakly Supervised Classification, Breast Tumour Classification, Histopathological Image Classification","Image, Text",English,Computer Vision,,Custom,https://bupt-ai-cz.github.io/BCNB/,https://paperswithcode.com/dataset/bcdalnmp,"Breast cancer (BC) has become the greatest threat to women’s health worldwide. Clinically, identification of axillary lymph node (ALN) metastasis and other tumor clinical characteristics such as ER, PR, and so on, are important for evaluating the prognosis and guiding the treatment for BC patients.

Several studies intended to predict the ALN status and other tumor clinical characteristics by clinicopathological data and genetic testing score. However, due to the relatively poor predictive values and high genetic testing costs, these methods are often limited. Recently, deep learning (DL) has enabled rapid advances in computational pathology, DL can perform high-throughput feature extraction on medical images and analyze the correlation between primary tumor features and above status. So far, there is no relevant research on preoperatively predicting ALN metastasis and other tumor clinical characteristics based on WSIs of primary BC samples.

Our paper has introduced a new dataset of Early Breast Cancer Core-Needle Biopsy WSI (BCNB), which includes core-needle biopsy whole slide images (WSIs) of early breast cancer patients and the corresponding clinical data. The WSIs have been examined and annotated by two independent and experienced pathologists blinded to all patient-related information.

Based on this dataset, we have studied the deep learning algorithm for predicting the metastatic status of ALN preoperatively by using multiple instance learning (MIL), and have achieved the best AUC of 0.831 in the independent test cohort. For more details, please review our paper. 

There are WSIs of 1058 patients,  and only part of tumor regions are annotated in WSIs. Except for the WSIs, we have also provided the clinical characteristics of each patient, which includes age, tumor size, tumor type, ER, PR, HER2, HER2 expression, histological grading, surgical, Ki67, molecular subtype, number of lymph node metastases, and the metastatic status of axillary lymph node (ALN). The dataset has been desensitized, and not contained the privacy information of patients.

Based on this dataset, we have studied the prediction of the metastatic status of axillary lymph node (ALN) in our paper, which is a weakly supervised classification task. However, other researches based on our dataset are also feasible, such as the prediction of histological grading, molecular subtype, HER2, ER, and PR. We do not limit the specific content for your research, and any research based on our dataset is welcome.

Please note that the dataset is only used for education and research, and the usage for commercial and clinical applications is not allowed. The usage of this dataset must follow the license.",,paper,https://arxiv.org/abs/2112.02222,,"testing score. However, due to the relatively poor predictive values and high genetic testing costs, these methods are often limited. Recently, deep learning (DL) has enabled rapid advances in computational pathology, DL can perform high-throughput feature extraction on medical images",
289,BCN_20000,Image Classification,Image Classification,"Image Classification, Lesion Classification, Skin Lesion Classification",Image,,Computer Vision,,,https://challenge2019.isic-archive.com/data.html,https://paperswithcode.com/dataset/bcn-20000,"BCN_20000 is a dataset composed of 19,424 dermoscopic images of skin lesions captured from 2010 to 2016 in the facilities of the Hospital Clínic in Barcelona. The dataset can be used for lesion recognition tasks such as lesion segmentation, lesion detection and lesion classification.",2010,https://arxiv.org/abs/1908.02288,https://arxiv.org/abs/1908.02288,,,
290,BCOPA-CE,Causal Discovery,Causal Discovery,"Causal Discovery, Causal Identification, Common Sense Reasoning",,,Methodology,,,https://github.com/badbadcode/weakCOPA/blob/master/data/BCOPA-CE.xml,https://paperswithcode.com/dataset/bcopa-ce,"We provide the BCOPA-CE test set, which has balanced token distribution in the correct and wrong alternatives and increases the difficulty of being aware of cause and effect.

construction

for each premise of the 500 samples in COPA-test set, we generate one event manually which is a plausible answer to the opposite question type of the original sample.
obtain 500 triplets of <premise, cause, effect>
construct 1000 samples by giving two different questions (cause or effect) to each triplet.",,,,500 samples,,
291,bcTCGA,regression,regression,regression,,,Methodology,,,https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga,https://paperswithcode.com/dataset/bctcga,"This data set comes from breast cancer tissue samples deposited to The Cancer Genome Atlas (TCGA) project. TCGA contains data on tumour samples were assayed on several platforms; this data set compiles results obtained using Agilent mRNA expression microarrays.

BRCA1 is the first gene identified that increases the risk of early onset breast cancer. Because BRCA1 is likely to interact with many other genes, including tumor suppressors and regulators of the cell division cycle, it is of interest to find genes with expression levels related to that of BRCA1, which we treat as the outcome of this analysis. These genes may be functionally related to BRCA1 and are useful candidates for further studies.

Expression measurements of 17,814 genes from 536 patients; all expression measurements are recorded on the log scale. There are are 491 genes with missing data, which we have excluded.",,,,,,
292,BDD-X,Explainable artificial intelligence,Explainable artificial intelligence,"Explainable artificial intelligence, Self-Driving Cars, Behavioural cloning, Autonomous Driving",,,Methodology,,Custom,https://github.com/JinkyuKimUCB/BDD-X-dataset,https://paperswithcode.com/dataset/bdd-x,"Berkeley Deep Drive-X (eXplanation) is a dataset is composed of over 77 hours of driving within 6,970 videos. The videos are taken in diverse driving conditions, e.g. day/night, highway/city/countryside, summer/winter etc. On average 40 seconds long, each video contains around 3-4 actions, e.g. speeding up, slowing down, turning right etc., all of which are annotated with a description and an explanation. Our dataset contains over 26K activities in over 8.4M frames.",,,,,,
293,BDD100K,Multiple Object Tracking,Multiple Object Tracking,"Multiple Object Tracking, Semi-Supervised Instance Segmentation, Video Segmentation, Instance Segmentation, Image Classification, Traffic Object Detection, Multi-Object Tracking and Segmentation, Steering Control, Multiple Object Track and Segmentation, Semantic Segmentation, Amodal Panoptic Segmentation, 2D Object Detection, Multi-Object Tracking, Drivable Area Detection, Lane Detection, Object Detection, Domain Adaptation, Video Instance Segmentation, Panoptic Segmentation","Image, Video",,Computer Vision,"object-detection-on-bdd100k-val, multiple-object-track-and-segmentation-on-2, 2d-object-detection-on-bdd100k-val, amodal-panoptic-segmentation-on-bdd100k-val, instance-segmentation-on-bdd100k-val, lane-detection-on-bdd100k-val, multi-object-tracking-and-segmentation-on-3, multiple-object-tracking-on-bdd100k-test-1, multi-object-tracking-on-bdd100k, semantic-segmentation-on-bdd100k-val, drivable-area-detection-on-bdd100k-val, traffic-object-detection-on-bdd100k-val, steering-control-on-bdd100k-val, object-detection-on-bdd100k, multiple-object-tracking-on-bdd100k-val, video-instance-segmentation-on-bdd100k-val",Mixed License,https://www.bdd100k.com/,https://paperswithcode.com/dataset/bdd100k,"Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue. More detail is at the dataset home page.",,,,,,
294,BEAR-probe,Probing Language Models,Probing Language Models,"Probing Language Models, Knowledge Probing, Factual probe, World Knowledge, General Knowledge",Text,English,Natural Language Processing,factual-probe-on-bear-probe,CC BY-SA,https://lm-pub-quiz.github.io/,https://paperswithcode.com/dataset/bear-big,"The $\text{BEAR}$ dataset and its larger version, $\text{BEAR}_{\text{big}}$, are benchmarks for evaluating common factual knowledge contained in language models. 

This dataset was created as part of the paper ""BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models"".

For more information visit the LM Pub Quiz website.

Citation
When using the dataset or library, please cite the following paper:

@misc{wilandBEARUnifiedFramework2024,
  title = {{{BEAR}}: {{A Unified Framework}} for {{Evaluating Relational Knowledge}} in {{Causal}} and {{Masked Language Models}}},
  shorttitle = {{{BEAR}}},
  author = {Wiland, Jacek and Ploner, Max and Akbik, Alan},
  year = {2024},
  number = {arXiv:2404.04113},
  eprint = {2404.04113},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.04113},
}",2024,"paper ""BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models""",https://arxiv.org/abs/2404.04113,,,
295,BEAT,Gesture Generation,Gesture Generation,Gesture Generation,Text,English,Natural Language Processing,gesture-generation-on-beat,CC-BY-NC 4.0,https://pantomatrix.github.io/BEAT/,https://paperswithcode.com/dataset/beat,"BEAT has i) 76 hours, high-quality, multi-modal data captured from 30 speakers talking with eight different emotions and in four different languages, ii) 32 millions frame-level emotion and semantic relevance annotations.
Our statistical analysis on BEAT demonstrates the correlation of conversational gestures with \textit{facial expressions}, \textit{emotions}, and \textit{semantics}, in addition to the known correlation with \textit{audio}, \textit{text}, and \textit{speaker identity}.
Based on this observation, we propose a baseline model, \textbf{Ca}scaded \textbf{M}otion \textbf{N}etwork \textbf{(CaMN)}, which consists of above six modalities modeled in a cascaded architecture for gesture synthesis. To evaluate the semantic relevancy, we introduce a metric, Semantic Relevance Gesture Recall (\textbf{SRGR}). 
Qualitative and quantitative experiments demonstrate metrics' validness, ground truth data quality, and baseline's state-of-the-art performance. 
To the best of our knowledge, BEAT is the largest motion capture dataset for investigating human gestures, which may contribute to a number of different research fields, including controllable gesture synthesis, cross-modality analysis, and emotional gesture recognition. The data, code and model are available on \url{https://pantomatrix.github.io/BEAT/}.",,,,,,
296,BEAT2,Gesture Generation,Gesture Generation,"Gesture Generation, 3D Face Animation","3D, Image, Text",English,Computer Vision,"3d-face-animation-on-beat2, gesture-generation-on-beat2",CC-BY-NC 4.0,https://pantomatrix.github.io/EMAGE/,https://paperswithcode.com/dataset/beat2,"We propose EMAGE, a framework to generate full-body human gestures from audio and masked gestures, encompassing facial, local body, hands, and global movements. To achieve this, we first introduce BEAT2 (BEAT-SMPLX-FLAME), a new mesh-level holistic co-speech dataset. BEAT2 combines MoShed SMPLX body with FLAME head parameters and further refines the modeling of head, neck, and finger movements, offering a community-standardized, high-quality 3D motion captured dataset.  EMAGE leverages masked body gesture priors during training to boost inference performance. It involves a Masked Audio Gesture Transformer, facilitating joint training on audio-to-gesture generation and masked gesture reconstruction to effectively encode audio and body gesture hints. Encoded body hints from masked gestures are then separately employed to generate facial and body movements.  Moreover, EMAGE adaptively merges speech features from the audio's rhythm and content and utilizes four compositional VQ-VAEs to enhance the results' fidelity and diversity. Experiments demonstrate that EMAGE generates holistic gestures with state-of-the-art performance and is flexible in accepting predefined spatial-temporal gesture inputs, generating complete, audio-synchronized results.",,,,,,
297,Beatles,Beat Tracking,Beat Tracking,"Beat Tracking, Downbeat Tracking","Image, Video",,Computer Vision,"downbeat-tracking-on-beatles, beat-tracking-on-beatles",,,https://paperswithcode.com/dataset/beatles,"This dataset includes the beat and downbeat annotations for  Beatles albums. The annotations are provided by M. E. P. Davies et. al [1].

M. E. P. Davies, N. Degara, and M. D. Plumbley, “Evaluation methods for musical audio beat tracking algorithms,” in Technical Report C4DM-TR-09-06, Centre for Digital Music, Queen Mary University of London, 2009.",2009,,,,,
298,BEDLAM,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Human Mesh Recovery, 3D Human Shape Estimation","3D, Image",,Computer Vision,human-mesh-recovery-on-bedlam,,https://bedlam.is.tue.mpg.de/#data,https://paperswithcode.com/dataset/bedlam,"BEDLAM is a large-scale synthetic video dataset designed to train and test algorithms on the task of 3D human pose and shape estimation (HPS). It contains diverse body shapes, skin tones, and motions. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation.",,,,,,
299,BEHAVE,Human-Object-interaction motion tracking,Human-Object-interaction motion tracking,"Human-Object-interaction motion tracking, 3D Object Reconstruction, 3D Human Reconstruction, Contact Detection, Human-Object Interaction Detection","3D, Image, Video",,Computer Vision,"3d-human-reconstruction-on-behave, contact-detection-on-behave, 3d-object-reconstruction-on-behave",,http://virtualhumans.mpi-inf.mpg.de/behave,https://paperswithcode.com/dataset/behave,BEHAVE is a full body human-object interaction dataset with multi-view RGBD frames and corresponding 3D SMPL and object fits along with the annotated contacts between them. Dataset contains ~15k frames at 5 locations with 8 subjects performing a wide range of interactions with 20 common objects.,,,,,,
300,Beijing_Multi-Site_Air-Quality_Dataset,Multivariate Time Series Imputation,Multivariate Time Series Imputation,Multivariate Time Series Imputation,Time Series,,Time Series,multivariate-time-series-imputation-on,,https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data,https://paperswithcode.com/dataset/beijing-air-quality,"This data set includes hourly air pollutants data from 12 nationally-controlled air-quality monitoring sites. The air-quality data are from the Beijing Municipal Environmental Monitoring Center. The meteorological data in each air-quality site are matched with the nearest weather station from the China Meteorological Administration. The time period is from March 1st, 2013 to February 28th, 2017. Missing data are denoted as NA.",2013,,,,,
301,Beijing_Traffic,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Multivariate Time Series Forecasting, Time Series Prediction, Traffic Prediction, Spatio-Temporal Forecasting","Time Series, Video",,Time Series,traffic-prediction-on-beijing,,https://github.com/deepkashiwa20/Urban_Concept_Drift,https://paperswithcode.com/dataset/beijing,The Beijing Traffic Dataset collects traffic speeds at 5-minute granularity for 3126 roadway segments in Beijing between 2022/05/12 and 2022/07/25.,2022,,,,,
302,BEIR,Citation Prediction,Citation Prediction,"Citation Prediction, News Retrieval, Fact Checking, Entity Retrieval, Tweet Retrieval, Duplicate-Question Retrieval, Biomedical Information Retrieval, Zero-shot Text Search, Question Answering, Argument Retrieval, Passage Retrieval","Text, Time Series",English,Natural Language Processing,"biomedical-information-retrieval-on-bioasq-1, question-answering-on-nq-beir, fact-checking-on-climate-fever-beir, zero-shot-text-search-on-beir, citation-prediction-on-scidocs-beir, question-answering-on-fiqa-2018-beir, tweet-retrieval-on-signal-1m-rt-beir, entity-retrieval-on-dbpedia-beir, argument-retrieval-on-touche-2020-beir, argument-retrieval-on-arguana-beir, news-retrieval-on-trec-news-beir, fact-checking-on-fever-beir, passage-retrieval-on-msmarco-beir, duplicate-question-retrieval-on-cqadupstack-1, question-answering-on-hotpotqa-beir, duplicate-question-retrieval-on-quora-beir, biomedical-information-retrieval-on-trec-1, fact-checking-on-scifact-beir, biomedical-information-retrieval-on-nfcorpus-1",Multiple licenses,https://github.com/UKPLab/beir,https://paperswithcode.com/dataset/beir,"BEIR (Benchmarking IR) is a heterogeneous benchmark containing different information retrieval (IR) tasks. Through BEIR, it is possible to systematically study the zero-shot generalization capabilities of multiple neural retrieval approaches.

The benchmark contains a total of 9 information retrieval tasks (Fact Checking, Citation Prediction, Duplicate Question Retrieval, Argument Retrieval, News Retrieval, Question Answering, Tweet Retrieval, Biomedical IR, Entity Retrieval) from 19 different datasets:


MS MARCO
TREC-COVID
NFCorpus
BioASQ
Natural Questions
HotpotQA
FiQA-2018
Signal-1M
TREC-News
ArguAna
Touche 2020
CQADupStack
Quora Question Pairs
DBPedia
SciDocs
FEVER
Climate-FEVER
SciFact
Robust04",2018,,,,,
303,Belebele,Machine Reading Comprehension,Machine Reading Comprehension,"Machine Reading Comprehension, Chinese Reading Comprehension, Multilingual Machine Comprehension in English Hindi, Vietnamese Machine Reading Comprehension, Multilingual text classification, Natural Language Understanding, Reading Comprehension (Zero-Shot), Multiple-choice, XLM-R, Reading Comprehension (Few-Shot), Reading Comprehension (One-Shot), Natural Questions, Multilingual NLP, Pretrained Multilingual Language Models, Question Answering, Reading Comprehension","Image, Text",English,Computer Vision,,CC BY-SA,https://github.com/facebookresearch/belebele,https://paperswithcode.com/dataset/belebele,"Belebele is a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. This dataset enables the evaluation of mono- and multi-lingual models in high-, medium-, and low-resource languages. Each question has four multiple-choice answers and is linked to a short passage from the FLORES-200 dataset. The human annotation procedure was carefully curated to create questions that discriminate between different levels of generalizable language comprehension and is reinforced by extensive quality checks. While all questions directly relate to the passage, the English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. Belebele opens up new avenues for evaluating and analyzing the multilingual abilities of language models and NLP systems.",,,,,,
304,Bengali.AI_Handwritten_Graphemes,Multi-Label Image Classification,Multi-Label Image Classification,"Multi-Label Image Classification, Bangla Text Detection","Image, Text",English,Computer Vision,bangla-text-detection-on-bengali-ai,CC BY SA,https://www.kaggle.com/competitions/bengaliai-cv19/,https://paperswithcode.com/dataset/bengali-ai-handwritten-graphemes,"This dataset contains images of individual hand-written Bengali characters. Bengali characters (graphemes) are written by combining three components: a grapheme_root, vowel_diacritic, and consonant_diacritic. Your challenge is to classify the components of the grapheme in each image. There are roughly 10,000 possible graphemes, of which roughly 1,000 are represented in the training set. The test set includes some graphemes that do not exist in the train but has no new grapheme components. It takes a lot of volunteers filling out sheets like this to generate a useful amount of real data; focusing the problem on the grapheme components rather than on recognizing whole graphemes should make it possible to assemble a Bengali OCR system without handwriting samples for all 10,000 graphemes.",,,,,,
305,Bengali_Curated_News_Summary_Dataset,Text Summarization,Text Summarization,"Text Summarization, Extractive Summarization, News Summarization",Text,English,Natural Language Processing,,,https://github.com/FMOpee/WGSS/blob/main/self_curated_dataset.json,https://paperswithcode.com/dataset/bengali-curated-news-summary-dataset,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
306,Bentham,Data Augmentation,Data Augmentation,"Data Augmentation, Handwriting Recognition, Handwritten Text Recognition","Image, Text",English,Computer Vision,handwritten-text-recognition-on-bentham,custom,http://www.transcriptorium.eu/~tsdata/BenthamR0/,https://paperswithcode.com/dataset/bentham,"Bentham manuscripts refers to a large set of documents that were written by the renowned English philosopher and reformer Jeremy Bentham (1748-1832). Volunteers of the Transcribe Bentham initiative transcribed this collection. Currently, >6 000 documents or > 25 000 pages have been transcribed using this public web platform.
For our experiments, we used the BenthamR0 dataset a part of the Bentham manuscripts.",,,,000 documents,,
307,BeNYfits,AI Agent,AI Agent,"AI Agent, Sequential Decision Making, Conversational Response Generation, Conversational Search, Decision Making, Task-Oriented Dialogue Systems",Text,English,Natural Language Processing,,,https://github.com/mtoles/BeNYfits-ProADA,https://paperswithcode.com/dataset/benyfits,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset

An agent benchmark for adaptive decision-making in dialog measuring agent accuracy and dialog turn efficiency in helping users determine eligibility for public, real-world opportunities.

This dataset containss the following:
* Natural language descriptions of 82 NYC public benefits programs (e.g., tax credits, childcare, subsidized AC)

Two simulated user datasets containing all features relevant to that household's eligibility:


Representative: 25 households with features drawn from real NYC demographic data
Diverse: 56 households which, collectively, satisfy nearly every possible acceptance/rejection criteria in all 82 public benefits opportunities",,,,,,
308,Berlin_V2X,Intelligent Communication,Intelligent Communication,"Intelligent Communication, Connectivity Estimation",,,Methodology,,Creative Commons Attribution,https://ieee-dataport.org/open-access/berlin-v2x,https://paperswithcode.com/dataset/berlin-v2x,"The Berlin V2X dataset offers high-resolution GPS-located wireless measurements across diverse urban environments in the city of Berlin for both cellular and sidelink radio access technologies, acquired with up to 4 cars over 3 days. The data enables thus a variety of different ML studies towards vehicle-to-anything (V2X) communication.

The data includes information on


physical layer parameters (such as signal strength and signal quality)
cellular radio resource management like cell identity, carrier aggregation and assigned resource blocks
wireless Quality of Service (QoS) like delay and throughput (for cellular) or packet error rate (for sidelink)
positioning information.

The datasets are labelled and pre-filtered for a fast on-boarding and applicability. The measurement methodology pursues an application to Machine Learning (ML) for tasks such as QoS prediction, transfer learning, proactive radio resource allocation or link selection, among others.",,,,,,
309,BGG_dataset,Sound Event Localization and Detection,Sound Event Localization and Detection,"Sound Event Localization and Detection, Shooter Localization, Gunshot Detection, Audio Classification","Audio, Image",,Computer Vision,,,https://github.com/junwoopark92/BG-Gun-Sound-Dataset,https://paperswithcode.com/dataset/bgg-dataset,"We recorded gun sounds by changing the type and
position of guns to diversify distances and angles in the
PUBG environment. The BGG dataset
consists of 2,195 samples with 37 different types of guns
and five directions, including a silence in which there is
no gunfire, but noises exist. The distance from the firearms
ranged from 0 meters to 600 meters. The audio was recorded
in stereo (i.e., two-channel audio), and each sample contains
various environmental noises (e.g., water splashing, walking,
and bullet friction).",,,,195 samples,,
310,Bianet,Transliteration,Transliteration,Transliteration,,,Methodology,,,https://d-ataman.github.io/bianet/,https://paperswithcode.com/dataset/bianet,"Bianet is a parallel news corpus in Turkish, Kurdish and English
It contains 3,214 Turkish articles with their sentence-aligned Kurdish or English translations from the Bianet online newspaper.",,,,,,
311,BIDS_CHB-MIT_Scalp_EEG_Database,Seizure Detection,Seizure Detection,Seizure Detection,Image,,Computer Vision,,Open Data Commons Attribution License v1.0,https://zenodo.org/records/10259996,https://paperswithcode.com/dataset/bids-chb-mit-scalp-eeg-database,"This dataset is a BIDS-compatible version of the CHB-MIT Scalp EEG Database. It reorganizes the file structure to comply with the BIDS specification. To this effect:

The data from subject chb21 was moved to sub-01/ses-02.
Metadata was organized according to BIDS.
Data in the EEG edf files was modified to keep only the 18 channels from a double banana bipolar montage.
Annotations were formatted as BIDS-score compatible `tsv` files.",,,,,,
312,BIDS_Siena_Scalp_EEG_Database,Seizure Detection,Seizure Detection,Seizure Detection,Image,,Computer Vision,,Open Data Commons Attribution License v1.0,https://zenodo.org/records/10640762,https://paperswithcode.com/dataset/bids-siena-scalp-eeg-database,"This dataset is a BIDS compatible version of the Siena Scalp EEG Database. It reorganizes the file structure to comply with the BIDS specification. To this effect:

Metadata was organized according to BIDS.
Data in the EEG edf files was modified to keep only the 19 channels from a 10-20 EEG system.
Annotations were formatted as BIDS-score compatible tsv files.",,,,,,
313,BIG-bench,,,", BIG-bench Machine Learning, Nonsense Words Grammar, Miscellaneous, High School Physics, World Religions, Marketing, Implicit Relations, Abstract Algebra, Logical Fallacies, Philosophy, Professional Accounting, Anatomy, Analogical Similarity, Evaluating Information Essentiality, Business Ethics, RACE-m, Jurisprudence, Physical Intuition, College Medicine, Auto Debugging, College Chemistry, Human Aging, Clinical Knowledge, Natural Questions, Conceptual Physics, Entailed Polarity, High School Geography, High School Government and Politics, RACE-h, High School Mathematics, Sociology, Prehistory, Elementary Mathematics, Question Selection, High School Statistics, Mathematical Induction, Formal Logic, Memorization, Human Sexuality, Security Studies, High School World History, Common Sense Reasoning, Professional Medicine, Human Organs Senses Multiple Choice, High School Psychology, College Biology, FEVER (3-way), High School Microeconomics, Professional Psychology, Similarities Abstraction, General Knowledge, Moral Permissibility, Word Sense Disambiguation, Riddle Sense, Movie Dialog Same Or Different, Empirical Judgments, Understanding Fables, Virology, Sarcasm Detection, Analytic Entailment, Metaphor Boolean, Electrical Engineering, English Proverbs, High School Computer Science, College Computer Science, Figure Of Speech Detection, Management, High School Macroeconomics, Crass AI, Dark Humor Detection, Physics MC, Identify Odd Metapor, College Mathematics, Odd One Out, Professional Law, High School Biology, LAMBADA, Multiple Choice Question Answering (MCQA), Sentence Ambiguity, High School US History, Moral Scenarios, Language Modelling, Timedial, Intent Recognition, Astronomy, Implicatures, Global Facts, Logical Args, Phrase Relatedness, TriviaQA, Econometrics, Misconceptions, Multi-task Language Understanding, Epistemic Reasoning, FEVER (2-way), Moral Disputes, Discourse Marker Prediction, International Law, Public Relations, College Physics, Logical Reasoning, Computer Security, US Foreign Policy, Fantasy Reasoning, Irony Identification, GRE Reading Comprehension, High School European History, Presuppositions As NLI, Crash Blossom, High School Chemistry, Nutrition, Medical Genetics","Audio, Image, Text, Time Series",English,Computer Vision,"discourse-marker-prediction-on-big-bench, on-big-bench-navigate, professional-psychology-on-big-bench, machine-learning-on-big-bench, medical-genetics-on-big-bench, professional-medicine-on-big-bench, irony-identification-on-big-bench, logical-reasoning-on-big-bench-penguins-in-a, multi-task-language-understanding-on-bbh-alg, prehistory-on-big-bench, dark-humor-detection-on-big-bench, formal-logic-on-big-bench, common-sense-reasoning-on-big-bench-date, human-sexuality-on-big-bench, common-sense-reasoning-on-big-bench-causal, word-sense-disambiguation-on-big-bench, logical-reasoning-on-big-bench-temporal, sarcasm-detection-on-big-bench-snarks, high-school-psychology-on-big-bench, econometrics-on-big-bench, odd-one-out-on-big-bench, high-school-statistics-on-big-bench, presuppositions-as-nli-on-big-bench, gre-reading-comprehension-on-big-bench, common-sense-reasoning-on-big-bench, global-facts-on-big-bench, multi-task-language-understanding-on-bbh-nlp, elementary-mathematics-on-big-bench, identify-odd-metapor-on-big-bench, implicit-relations-on-big-bench, on-big-bench-hard, intent-recognition-on-big-bench, moral-permissibility-on-big-bench, multiple-choice-question-answering-mcqa-on-28, mathematical-induction-on-big-bench, business-ethics-on-big-bench, crass-ai-on-big-bench, empirical-judgments-on-big-bench, sociology-on-big-bench, sentence-ambiguity-on-big-bench, high-school-macroeconomics-on-big-bench, professional-law-on-big-bench, common-sense-reasoning-on-big-bench-known, figure-of-speech-detection-on-big-bench, management-on-big-bench, human-organs-senses-multiple-choice-on-big, high-school-european-history-on-big-bench, high-school-biology-on-big-bench, professional-accounting-on-big-bench, physics-mc-on-big-bench, high-school-mathematics-on-big-bench, lambada-on-big-bench, timedial-on-big-bench, logical-args-on-big-bench, college-biology-on-big-bench, moral-scenarios-on-big-bench, understanding-fables-on-big-bench, similarities-abstraction-on-big-bench, logical-reasoning-on-big-bench-reasoning, college-mathematics-on-big-bench, moral-disputes-on-big-bench, high-school-computer-science-on-big-bench, physical-intuition-on-big-bench, multiple-choice-question-answering-mcqa-on-27, auto-debugging-on-big-bench-lite, analytic-entailment-on-big-bench, multiple-choice-question-answering-mcqa-on-31, phrase-relatedness-on-big-bench, college-physics-on-big-bench, logical-reasoning-on-big-bench-logical, question-selection-on-big-bench, evaluating-information-essentiality-on-big, common-sense-reasoning-on-big-bench-sports, marketing-on-big-bench, fantasy-reasoning-on-big-bench, security-studies-on-big-bench, implicatures-on-big-bench, nonsense-words-grammar-on-big-bench, riddle-sense-on-big-bench, common-sense-reasoning-on-big-bench-winowhy, fever-3-way-on-big-bench, high-school-government-and-politics-on-big, multiple-choice-question-answering-mcqa-on-30, astronomy-on-big-bench, virology-on-big-bench, crash-blossom-on-big-bench, miscellaneous-on-big-bench, high-school-chemistry-on-big-bench, nutrition-on-big-bench, epistemic-reasoning-on-big-bench, logical-fallacies-on-big-bench, metaphor-boolean-on-big-bench, electrical-engineering-on-big-bench, us-foreign-policy-on-big-bench, movie-dialog-same-or-different-on-big-bench, analogical-similarity-on-big-bench, clinical-knowledge-on-big-bench, college-medicine-on-big-bench, general-knowledge-on-big-bench, computer-security-on-big-bench, logical-reasoning-on-big-bench-strategyqa, anatomy-on-big-bench, world-religions-on-big-bench, on-big-bench-snarks, college-chemistry-on-big-bench, triviaqa-on-big-bench, language-modelling-on-big-bench-lite, fever-2-way-on-big-bench, natural-questions-on-big-bench, race-h-on-big-bench, public-relations-on-big-bench, misconceptions-on-big-bench, conceptual-physics-on-big-bench, philosophy-on-big-bench, high-school-world-history-on-big-bench, jurisprudence-on-big-bench, abstract-algebra-on-big-bench, on-big-bench-hyperbaton, race-m-on-big-bench, logical-reasoning-on-big-bench-logic-grid, memorization-on-big-bench-hindu-knowledge, high-school-geography-on-big-bench, on-big-bench-ruin-names, college-computer-science-on-big-bench, human-aging-on-big-bench, common-sense-reasoning-on-big-bench-logical, english-proverbs-on-big-bench, logical-reasoning-on-big-bench-formal, entailed-polarity-on-big-bench, international-law-on-big-bench, high-school-microeconomics-on-big-bench, high-school-physics-on-big-bench, multiple-choice-question-answering-mcqa-on-29, high-school-us-history-on-big-bench",Apache-2.0,https://github.com/google/BIG-bench,https://paperswithcode.com/dataset/big-bench,The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. Big-bench include more than 200 tasks.,,https://arxiv.org/pdf/2206.04615.pdf,https://arxiv.org/pdf/2206.04615.pdf,,,
314,BiGe,Gesture Generation,Gesture Generation,Gesture Generation,Text,English,Natural Language Processing,,CC BY-NC 4.0 DEED,https://github.com/hvoss-techfak/AQGT,https://paperswithcode.com/dataset/bige,The BiGe corpus is comprised of 54.360 shots of interest extracted from TED and TEDx talks. All shots are tracked with fully 3d landmarks.,,,,,,
315,BigPatent,Text Summarization,Text Summarization,"Text Summarization, Summarization",Text,English,Natural Language Processing,"text-summarization-on-bigpatent, summarization-on-big-patent",,https://evasharma.github.io/bigpatent/,https://paperswithcode.com/dataset/bigpatent,Consists of 1.3 million records of U.S. patent documents along with human written abstractive summaries.,,BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization,https://arxiv.org/pdf/1906.03741v1.pdf,,,
316,Bike_and_Car_Odometer_Dataset___Speedometer_OCR,Monocular Visual Odometry,Monocular Visual Odometry,"Monocular Visual Odometry, Optical Charater Recogntion",Image,,Computer Vision,,,https://www.kaggle.com/datasets/dataclusterlabs/bike-and-car-odometer-dataset,https://paperswithcode.com/dataset/bike-and-car-odometer-dataset-speedometer-ocr,"This dataset consists of odometer or speedometer images of bike and car vehicles.

Introduction
This dataset can be used to detect or recognize odometer readings of the vehicles. Moreover, it can be used to classify the make of the car and bikes. The usecases can in the domain of insurance, repair and OCR.

Dataset Features

Captured by 4000+ unique users
Rich in diversity
Mobile phone view point
Various lighting conditions
Digital and Analog Categories
Vehicle Model Types

Dataset Features

Classification and detection annotations available
Multiple category annotations possible
COCO, PASCAL VOC and YOLO formats

To download full datasets or to submit a request for your dataset needs, please ping us on sales@datacluster.ai**
Visit www.datacluster.in to know more.

Note:
All the images are manually verified and are contributed by the large contributor base on DataCluster platform",,,,,,
317,Billion_Word_Benchmark,Text Generation,Text Generation,"Text Generation, Language Modelling, Word Embeddings",Text,English,Natural Language Processing,"text-generation-on-one-billion-word, language-modelling-on-one-billion-word",Apache-2,https://code.google.com/archive/p/1-billion-word-language-modeling-benchmark/,https://paperswithcode.com/dataset/billion-word-benchmark,The One Billion Word dataset is a dataset for language modeling. The training/held-out data was produced from the WMT 2011 News Crawl data using a combination of Bash shell and Perl scripts.,2011,,,,,
318,BillSum,Sequence-to-sequence Language Modeling,Sequence-to-sequence Language Modeling,"Sequence-to-sequence Language Modeling, Text Summarization, Abstractive Text Summarization, Summarization, Point Processes","Text, Time Series",English,Natural Language Processing,"text-summarization-on-billsum, sequence-to-sequence-language-modeling-on-3, summarization-on-billsum",,https://github.com/FiscalNote/BillSum,https://paperswithcode.com/dataset/billsum,"BillSum is the first dataset for summarization of US Congressional and California state bills.

The BillSum dataset consists of three parts: US training bills, US test bills and California test bills. The US bills were collected from the Govinfo service provided by the United States Government Publishing Office (GPO). The corpus consists of bills from the 103rd-115th (1993-2018) sessions of Congress. The data was split into 18,949 train bills and 3,269 test bills. For California, bills from the 2015-2016 session were scraped directly from the legislature’s website; the summaries were written by their Legislative Counsel.

The BillSum corpus focuses on mid-length legislation from 5,000 to 20,000 character in length. The authors chose to measure the text length in characters, instead of words or sentences, because the texts have complex structure that makes it difficult to consistently measure words. The range was chosen because on one side, short bills introduce minor changes and do not require summaries. While the CRS produces summaries for them, they often contain most of the text of the bill. On the
other side, very long legislation is often composed of several large sections.",1993,BillSum: A Corpus for Automatic Summarization of US Legislation,https://arxiv.org/pdf/1910.00523v2.pdf,,,
319,BIMCV_COVID-19,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Computed Tomography (CT), COVID-19 Diagnosis",Image,,Computer Vision,,Custom,https://github.com/BIMCV-CSUSP/BIMCV-COVID-19,https://paperswithcode.com/dataset/bimcv-covid-19,"BIMCV-COVID19+ dataset is a large dataset with chest X-ray images CXR (CR, DX) and computed tomography (CT) imaging of COVID-19 patients along with their radiographic findings, pathologies, polymerase chain reaction (PCR), immunoglobulin G (IgG) and immunoglobulin M (IgM) diagnostic antibody tests and radiographic reports from Medical Imaging Databank in Valencian Region Medical Image Bank (BIMCV). The findings are mapped onto standard Unified Medical Language System (UMLS) terminology and they cover a wide spectrum of thoracic entities, contrasting with the much more reduced number of entities annotated in previous datasets. Images are stored in high resolution and entities are localized with anatomical labels in a Medical Imaging Data Structure (MIDS) format. In addition, 23 images were annotated by a team of expert radiologists to include semantic segmentation of radiographic findings. Moreover, extensive information is provided, including the patient’s demographic information, type of projection and acquisition parameters for the imaging study, among others. These iterations of the database include 7,377 CR, 9,463 DX and 6,687 CT studies.",,,,23 images,"tests and radiographic reports from Medical Imaging Databank in Valencian Region Medical Image Bank (BIMCV). The findings are mapped onto standard Unified Medical Language System (UMLS) terminology and they cover a wide spectrum of thoracic entities, contrasting with the much more reduced number of entities annotated in previous datasets. Images",
320,Binette_s_2022_Inventors_Benchmark,Record linking,Record linking,"Record linking, Entity Resolution",,,Methodology,,,https://github.com/PatentsView/PatentsView-Evaluation/,https://paperswithcode.com/dataset/binette-s-2022-inventors-benchmark,"Hand-disambiguation of a sample of U.S. patents inventor mentions from PatentsView.org.

Inventors we selected indirectly by sampling inventor mentions uniformly at random. This results in inventor sampled with probability proportional to their number of granted patents.

The time period considered is from 1976 to December 31, 2021, corresponding to the disambiguation labeled ""disamb_inventor_id_20211230"" in PatentsView's bulk data downloads ""g_persistent_inventor.tsv"" file (https://patentsview.org/download/data-download-tables). That is, the benchmark disambiguation intends to contain all inventor mentions for the sampled inventors from that time period. Note that the benchmark disambiguation contains a few extraneous mentions to patents granted outside of that time period. These should be ignored for evaluation purposes.

The methodology used for the hand-disambiguation is described in Binette et al. (2022) (https://arxiv.org/abs/2210.01230). We used one disambiguation of 200 inventors from Binette et al. (2022), as well as an additional disambiguation of 200 inventors provided by an additional staff member. The two disambiguations were reviewed and validated. However, they should be expected to contain errors due to the ambiguous nature of inventor disambiguation. Furthermore, given the use as the December 30, 2021, disambiguation from PatentsView as a starting point of the hand-labeling, a bias towards this disambiguation should be expected.",1976,,,,,
321,Bio,AMR Parsing,AMR Parsing,"AMR Parsing, AMR-to-Text Generation",Text,English,Natural Language Processing,"amr-to-text-generation-on-bio, amr-parsing-on-bio",,https://amr.isi.edu/,https://paperswithcode.com/dataset/bio,"This corpus includes annotations of cancer-related PubMed articles, covering 3 full papers (PMID:24651010, PMID:11777939, PMID:15630473) as well as the result sections of 46 additional PubMed papers. The corpus also includes about 1000 sentences each from the BEL BioCreative training corpus and the Chicago Corpus.",,,,1000 sentences,,
322,BioASQ,Word Embeddings,Word Embeddings,"Word Embeddings, Question Answering, Information Retrieval, Zero-shot Text Search",Text,English,Natural Language Processing,"question-answering-on-bioasq, zero-shot-text-search-on-bioasq",,http://participants-area.bioasq.org/datasets/,https://paperswithcode.com/dataset/bioasq,"BioASQ is a question answering dataset. Instances in the BioASQ dataset are composed of a question (Q), human-annotated answers (A), and the relevant contexts (C) (also called snippets).",,Transferability of Natural Language Inference to Biomedical Question Answering,https://arxiv.org/abs/2007.00217,,,
323,Biodenoising_datasets,Audio Denoising,Audio Denoising,Audio Denoising,Audio,,Audio,,,https://github.com/earthspecies/biodenoising-datasets,https://paperswithcode.com/dataset/biodenoising-datasets,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
324,Biodenoising_validation,Audio Denoising,Audio Denoising,Audio Denoising,Audio,,Audio,,Other (Non-Commercial),https://zenodo.org/records/13736465,https://paperswithcode.com/dataset/biodenoising-validation,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
325,BiodivTab,Cell Entity Annotation,Cell Entity Annotation,"Cell Entity Annotation, Column Type Annotation",,,Methodology,"column-type-annotation-on-biodivtab, cell-entity-annotation-on-biodivtab",CC BY 4.0,https://github.com/fusion-jena/BiodivTab,https://paperswithcode.com/dataset/biodivtab,The BioDiv dataset includes manually labeled tables for CTA and CEA from the biodiversity domain.,,,,,,
326,BioGRID,Link Prediction,Link Prediction,"Link Prediction, Graph Embedding, Gene Interaction Prediction, Node Classification","Graph, Image, Time Series",,Computer Vision,"gene-interaction-prediction-on-biogrid-human, gene-interaction-prediction-on-biogridyeast",,https://thebiogrid.org/,https://paperswithcode.com/dataset/biogrid,"BioGRID is a biomedical interaction repository with data compiled through comprehensive curation efforts. The current index is version 4.2.192 and searches 75,868 publications for 1,997,840 protein and genetic interactions, 29,093 chemical interactions and 959,750 post translational modifications from major model organism species.",,,,,,
327,BioLAMA,Knowledge Probing,Knowledge Probing,"Knowledge Probing, Probing Language Models",Text,English,Natural Language Processing,knowledge-probing-on-biolama,,https://github.com/dmis-lab/BioLAMA,https://paperswithcode.com/dataset/biolama,BioLAMA is a benchmark comprised of 49K biomedical factual knowledge triples for probing biomedical Language Models. It is used to assess the capabilities of Language Models for being valid biomedical knowledge bases.,,,,,,
328,BIOSCAN-5M,Self-Supervised Image Classification,Self-Supervised Image Classification,"Self-Supervised Image Classification, Transfer Learning, Hierarchical Multi-label Classification, Domain Generalization, Contrastive Learning, Multimodal Deep Learning, Open-World Semi-Supervised Learning, Semi-Supervised Image Classification, Classification, Self-Supervised Learning",Image,,Computer Vision,,Creative Commons Attribution 3.0 Unported,https://biodiversitygenomics.net/5M-insects/,https://paperswithcode.com/dataset/bioscan-5m,"As part of an ongoing worldwide effort to comprehend and monitor insect biodiversity, we present the BIOSCAN-5M Insect dataset to the machine learning community. BIOSCAN-5M is a comprehensive dataset containing multi-modal information for over 5 million insect specimens, and it significantly expands existing image-based biological datasets by including taxonomic labels, raw nucleotide barcode sequences, assigned barcode index numbers, geographical information, and specimen size.

Every record has both image and DNA data. Each record of the BIOSCAN-5M dataset contains six primary attributes:

RGB image
DNA barcode sequence
Barcode Index Number (BIN)
Biological taxonomic classification
Geographical information
Specimen size",,,,,,
329,BIOSSES,Semantic Similarity,Semantic Similarity,"Semantic Similarity, Sentence Embeddings For Biomedical Texts, Sentence Similarity",,,Methodology,"semantic-similarity-on-biosses, sentence-embeddings-for-biomedical-texts-on, sentence-similarity-on-biosses",,https://tabilab.cmpe.boun.edu.tr/BIOSSES/DataSet.html,https://paperswithcode.com/dataset/biosses,"The BIOSSES data set comprises total 100 sentence pairs all of which were selected from the ""TAC2 Biomedical Summarization Track Training Data Set"" .

The sentence pairs were evaluated by five different human experts that judged their similarity and gave scores in a range [0-4]. Our guideline was prepared based on SemEval 2012 Task 6 Guideline.",2012,,,,,
330,BIPED,Edge Detection,Edge Detection,Edge Detection,Image,,Computer Vision,edge-detection-on-biped-1,,https://xavysp.github.io/MBIPED/,https://paperswithcode.com/dataset/biped,"Details
It contains 250 outdoor images of 1280$\times$720 pixels each. These images have been carefully annotated by experts on the computer vision field, hence no redundancy has been considered. In spite of that, all results have been cross-checked several times in order to correct possible mistakes or wrong edges by just one subject. This dataset is publicly available as a benchmark for evaluating edge detection algorithms. The generation of this dataset is motivated by the lack of edge detection datasets, actually, there is just one dataset publicly available for the edge detection task published in 2016 (MDBD: Multicue Dataset for Boundary Detection—the subset for edge detection). The level of details of the edge level annotations in the BIPED’s images can be appreciated looking at the GT, see Figs above.

BIPED dataset has 250 images in high definition. Thoses images are already split up for training and testing. 200 for training and 50 for testing.

Version
The current version is the second one.",2016,,,250 images,"valuating edge detection algorithms. The generation of this dataset is motivated by the lack of edge detection datasets, actually, there is just one dataset publicly available for the edge detection task published in 2016 (MDBD: Multicue Dataset for Boundary Detection—the subset for edge detection). The level of details of the edge level annotations in the BIPED’s images",
331,bipedal-skills,Hierarchical Reinforcement Learning,Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning, Multi-Goal Reinforcement Learning, Transfer Reinforcement Learning, Unsupervised Reinforcement Learning, General Reinforcement Learning",,,Reinforcement Learning,,MIT,https://facebookresearch.github.io/hsd3,https://paperswithcode.com/dataset/bipedal-skills,"The bipedal skills benchmark is a suite of reinforcement learning environments implemented for the MuJoCo physics simulator. It aims to provide a set of tasks that demand a variety of motor skills beyond locomotion, and is intended for evaluating skill discovery and hierarchical learning methods. The majority of tasks exhibit a sparse reward structure.",,,,,,
332,BIRDeep,Event Detection,Event Detection,"Event Detection, Bird Audio Detection, Bird Species Classification With Audio-Visual Data, Network Embedding","Audio, Graph, Image",,Computer Vision,,,https://huggingface.co/datasets/GrunCrow/BIRDeep_AudioAnnotations,https://paperswithcode.com/dataset/birdeep,"The BIRDeep Audio Annotations dataset is a collection of bird vocalizations from Doñana National Park, Spain. It was created as part of the BIRDeep project, which aims to optimize the detection and classification of bird species in audio recordings using deep learning techniques. The dataset is intended for use in training and evaluating models for bird vocalization detection and identification.",,,,,,
333,BirdSong,Multi-Label Learning,Multi-Label Learning,"Multi-Label Learning, Graph Matching, Denoising",Graph,,Methodology,,,,https://paperswithcode.com/dataset/birdsong,"The BirdSong dataset consists of audio recordings of bird songs at the H. J. Andrews (HJA) Experimental Forest, using unattended microphones. The goal of the dataset is to provide data to automatically identify the species of bird responsible for each utterance in these recordings. The dataset contains 548 10-seconds audio recordings.",,,,,,
334,BirdVox-full-night,Data Augmentation,Data Augmentation,"Data Augmentation, Sound Event Detection, Metric Learning","Audio, Image",,Computer Vision,,,https://wp.nyu.edu/birdvox/birdvox-full-night/,https://paperswithcode.com/dataset/birdvox-full-night,"The BirdVox-full-night dataset contains 6 audio recordings, each about ten hours in duration. These recordings come from ROBIN autonomous recording units, placed near Ithaca, NY, USA during the fall 2015. They were captured on the night of September 23rd, 2015, by six different sensors, originally numbered 1, 2, 3, 5, 7, and 10.
Andrew Farnsworth used the Raven software to pinpoint every avian flight call in time and frequency. He found 35402 flight calls in total. He estimates that about 25 different species of passerines (thrushes, warblers, and sparrows) are present in this recording. Species are not labeled in BirdVox-full-night, but it is possible to tell apart thrushes from warblers and sparrrows by looking at the center frequencies of their calls. The annotation process took 102 hours.",2015,,,,,
335,BIRD__BIg_Bench_for_LaRge-scale_Database_Grounded_,Text-To-SQL,Text-To-SQL,Text-To-SQL,Text,English,Natural Language Processing,text-to-sql-on-bird-big-bench-for-large-scale,,https://bird-bench.github.io/,https://paperswithcode.com/dataset/bird-sql,"BIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQL Evaluation) represents a pioneering, cross-domain dataset that examines the impact of extensive database contents on text-to-SQL parsing. BIRD contains over 12,751 unique question-SQL pairs and 95 big databases with a total size of 33.4 GB. It also covers more than 37 professional domains, such as blockchain, hockey, healthcare and education, etc.",,,,,,
336,BiToD,Task-Oriented Dialogue Systems,Task-Oriented Dialogue Systems,"Task-Oriented Dialogue Systems, End-To-End Dialogue Modelling",,,Methodology,,,https://github.com/HLTCHKUST/BiToD,https://paperswithcode.com/dataset/bitod,BiToD is a bilingual multi-domain dataset for end-to-end task-oriented dialogue modeling. BiToD contains over 7k multi-domain dialogues (144k utterances) with a large and realistic bilingual knowledge base. It serves as an effective benchmark for evaluating bilingual ToD systems and cross-lingual transfer learning approaches.,,,,,,
337,Biwi_3D_Audiovisual_Corpus_of_Affective_Communicat,3D Face Animation,3D Face Animation,3D Face Animation,"3D, Image",,Computer Vision,3d-face-animation-on-biwi-3d-audiovisual,,https://data.vision.ee.ethz.ch/cvl/datasets/b3dac2.en.html,https://paperswithcode.com/dataset/biwi-3d-audiovisual-corpus-of-affective,"BIWI 3D corpus comprises a total of 1109 sentences uttered by 14 native English speakers (6 males and 8 females). A real time 3D scanner and a professional microphone were used to capture the facial movements and the speech of the speakers. The dense dynamic face scans were acquired at 25 frames per second and the RMS error in the 3D reconstruction is about 0.5 mm. In order to ease automatic speech segmentation, we carried out the recordings in a anechoic room, with walls covered by sound wave-absorbing materials.

Each sentence was recorded twice:



First, the speaker read the sentence from text, with a neutral expression.



Then, the speaker watched a clip extracted from a feature film where the sentence is acted by professional actors and the context is highly emotional. After rating the emotions induced by the video, the speaker repeated the sentence.",,,,1109 sentences,,
338,BKEE,Event Detection,Event Detection,"Event Detection, Vietnamese Datasets, Event Extraction",Image,,Computer Vision,,CC BY-NC 4.0,https://github.com/nhungnt7/BKEE,https://paperswithcode.com/dataset/bkee,"A novel event extraction dataset for Vietnamese. BKEE encompasses over 33 distinct event types and 28 different event argument roles, providing a labeled dataset for entity mentions, event mentions, and event arguments on 1066 documents.",,,,1066 documents,,
339,BL30K,Semi-Supervised Video Object Segmentation,Semi-Supervised Video Object Segmentation,"Semi-Supervised Video Object Segmentation, Interactive Video Object Segmentation, Unsupervised Video Object Segmentation, Video Object Tracking, Video Object Segmentation","Image, Video",,Computer Vision,,,https://github.com/hkchengrex/MiVOS,https://paperswithcode.com/dataset/bl30k,"BL30K is a synthetic dataset rendered using Blender with ShapeNet's data. We break the dataset into six segments, each with approximately 5K videos. The videos are organized in a similar format as DAVIS and YouTubeVOS, so dataloaders for those datasets can be used directly. Each video is 160 frames long, and each frame has a resolution of 768*512. There are 3-5 objects per video, and each object has a random smooth trajectory -- we tried to optimize the trajectories in a greedy fashion to minimize object intersection (not guaranteed), with occlusions still possible (happen a lot in reality). See MiVOS for details.",,,,,,
340,Blackbird,Bayesian Inference,Bayesian Inference,"Bayesian Inference, Optical Flow Estimation",Video,,Methodology,,,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/picasso/yawConstant/maxSpeed3p0/videos/,https://paperswithcode.com/dataset/blackbird,"The Blackbird unmanned aerial vehicle (UAV) dataset is a large-scale, aggressive indoor flight dataset collected using a custom-built quadrotor platform for use in evaluation of agile perception. The Blackbird dataset contains over 10 hours of flight data from 168 flights over 17 flight trajectories and 5 environments. Each flight includes sensor data from 120Hz stereo and downward-facing photorealistic virtual cameras, 100Hz IMU, motor speed sensors, and 360Hz millimeter-accurate motion capture ground truth. Camera images for each flight were photorealistically rendered using FlightGoggles across a variety of environments to facilitate easy experimentation of high performance perception algorithms.",,,,,"valuation of agile perception. The Blackbird dataset contains over 10 hours of flight data from 168 flights over 17 flight trajectories and 5 environments. Each flight includes sensor data from 120Hz stereo and downward-facing photorealistic virtual cameras, 100Hz IMU, motor speed sensors, and 360Hz millimeter-accurate motion capture ground truth. Camera images",
341,BlendedMVS,3D Reconstruction,3D Reconstruction,"3D Reconstruction, Novel View Synthesis, Meta-Learning",3D,,Methodology,,Creative Commons Attribution 4.0 International,https://github.com/YoYo000/BlendedMVS,https://paperswithcode.com/dataset/blendedmvs,"BlendedMVS is a novel large-scale dataset, to provide sufficient training ground truth for learning-based MVS. The dataset was created by applying a 3D reconstruction pipeline to recover high-quality textured meshes from images of well-selected scenes. Then, these mesh models were rendered to color images and depth maps.",,,,,training ground truth for learning-based MVS. The dataset was created by applying a 3D reconstruction pipeline to recover high-quality textured meshes from images,
342,Blended_Skill_Talk,Visual Dialog,Visual Dialog,"Visual Dialog, Chatbot",Image,,Computer Vision,visual-dialog-on-blendedskilltalk,,https://parl.ai/projects/bst/,https://paperswithcode.com/dataset/blended-skill-talk,"To analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes.",,,,,,
343,BlendNet,Text to 3D,Text to 3D,"Text to 3D, Text-to-Code Generation, CAD Reconstruction, 3D Shape Modeling","3D, Text",English,Natural Language Processing,,apache-2.0,https://huggingface.co/datasets/FreedomIntelligence/BlendNet,https://paperswithcode.com/dataset/blendnet,"📚 BlendNet
The dataset contains $12k$ samples. To balance cost savings with data quality and scale, we manually annotated $2k$ samples and used GPT-4o to annotate the remaining $10k$ samples.

For more details, please visit our GitHub repository",,,,,,
344,BLiMP,Language Modelling,Language Modelling,"Language Modelling, Language Acquisition",Text,English,Natural Language Processing,,,https://github.com/alexwarstadt/blimp,https://paperswithcode.com/dataset/blimp,"BLiMP is a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars. Aggregate human agreement with the labels is 96.4%.",,https://arxiv.org/pdf/1912.00582v3.pdf,https://arxiv.org/pdf/1912.00582v3.pdf,,,
345,Blizzard_Challenge_2013,Audio Generation,Audio Generation,"Audio Generation, Speech Synthesis","Audio, Text",English,Audio,speech-synthesis-on-blizzard-challenge-2013,,http://www.festvox.org/blizzard/blizzard2013.html,https://paperswithcode.com/dataset/blizzard-challenge-2013,"The English data for voice building was obtained, prepared and provided the the challenge by Lessac Technologies Inc., having originally came from the publishers Voice Factory International Inc. It comprises speech from one female professional narrator & actress, Catherine ‘Bobbie’ Byers, reading the text of a collection of classic novels. These had been divided by the publishers of the original audiobooks into a number of genres, such as “Classic Novels”, “Women’s Classics”, “Young Readers” and so on.",,,,,,
346,Blog_Authorship_Corpus,Authorship Verification,Authorship Verification,Authorship Verification,,,Methodology,,,https://u.cs.biu.ac.il/~koppel/BlogCorpus.htm,https://paperswithcode.com/dataset/blog-authorship-corpus,"The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.",2004,,,,,
347,BLP,Blackout Poetry Generation,Blackout Poetry Generation,Blackout Poetry Generation,Text,English,Natural Language Processing,blackout-poetry-generation-on-blp-1,CC0: Public Domain,https://www.kaggle.com/aditeyabaral/blackout-poetry-dataset,https://paperswithcode.com/dataset/blp16,"A blackout poetry dataset constructed from publicly available short stories and large poems. The dataset consists of two variants: 8K and 16K examples of passages along with a poem generated from the passage and the indices of the words in the passage from which words in the poem have been selected. The dataset also contains perplexity scores for each of the poems indicating the language quality of the poems. 

The dataset was constructed synthetically, and hence contains multiple poor poems and frequent grammatical errors. However, it is a great starting point for the task of applying machine learning to blackout poetry generation.",,,,16K examples,,
348,BLUE,Language Modelling,Language Modelling,"Language Modelling, Relation Extraction, Named Entity Recognition (NER)","Graph, Image, Text",English,Computer Vision,,Public domain,https://github.com/ncbi-nlp/BLUE_Benchmark,https://paperswithcode.com/dataset/blue,"The BLUE benchmark consists of five different biomedicine text-mining tasks with ten corpora. These tasks cover a diverse range of text genres (biomedical literature and clinical notes), dataset sizes, and degrees of difficulty and, more importantly, highlight common biomedicine text-mining challenges.",,,,,,
349,BN-AuthProf,Age And Gender Classification,Age And Gender Classification,"Age And Gender Classification, Authorship Attribution, Gender Classification, Age and Gender Estimation, Gender Prediction, Author Attribution, Author Profiling","Image, Time Series",,Computer Vision,age-and-gender-classification-on-bn-authprof,MIT,https://github.com/crusnic-corp/BN-AuthProf,https://paperswithcode.com/dataset/bn-authprof,"Although research on author profiling has quite progressed in abundant resources languages, it is still infancy for limited resources languages such as Bengali. This repository contains our Bangla Author Profiling Dataset (BN-AuthProf). The primary objective is to introduce and benchmark the performance of machine learning approaches on Age and Gender Classification tasks from the social media status of people. 

BN-AuthProf dataset consists of 300 anonymized authors and 30,131 manually curated Facebook posts in Bangla, tagged with age and gender information.",,,,,,
350,BN-HTRd,Unsupervised Image Segmentation,Unsupervised Image Segmentation,"Unsupervised Image Segmentation, Word Spotting In Handwritten Documents, Line Segment Detection, Handwriting Recognition, Handwritten Text Recognition, Handwritten Word Segmentation, Handwritten Line Segmentation","Image, Text",English,Computer Vision,"handwritten-line-segmentation-on-bn-htrd, handwritten-word-segmentation-on-bn-htrd",CC BY 4.0,https://data.mendeley.com/datasets/743k6dm543/,https://paperswithcode.com/dataset/bn-htrd,"We introduce a new Dataset (BN-HTRd) for offline Handwritten Text Recognition (HTR) from images of Bangla scripts comprising words, lines, and document-level annotations. The BN-HTRd dataset is based on the BBC Bangla News corpus - which acted as ground truth texts for the handwritings. Our dataset contains a total of 786 full-page images collected from 150 different writers. With a staggering 1,08,181 instances of handwritten words, distributed over 14,383 lines and 23,115 unique words, this is currently the 'largest and most comprehensive dataset' in this field. We also provided the bounding box annotations (YOLO format) for the segmentation of words/lines and the ground truth annotations for full-text, along with the segmented images and their positions. The contents of our dataset came from a diverse news category, and annotators of different ages, genders, and backgrounds, having variability in writing styles. The BN-HTRd dataset can be adopted as a basis for various handwriting classification tasks such as end-to-end document recognition, word-spotting, word/line segmentation, and so on. 

The statistics of the original dataset are given below:


Number of writers = 150
Total number of images = 786
Total number of lines = 14,383
Total number of words = 1,08,181
Total number of unique words = 23,115
Total number of punctuation = 7,446
Total number of characters = 5,74,203

From v3.0 onwards, we are also providing automatic bounding box annotations (YOLO format) of 805 document images containing words/lines. The statistics of the automatic annotations are given below:


Number of writers = 87
Total number of images = 805
Total number of lines = 14,836
Total number of words = 1,06,135",,,,181 instances,,
351,BNCI2014-001_MOABB,Within-Session Motor Imagery (right hand vs. feet),Within-Session Motor Imagery (right hand vs. feet),"Within-Session Motor Imagery (right hand vs. feet), Within-Session Motor Imagery (left hand vs. right hand), Within-Session Motor Imagery (all classes)",,,Methodology,"within-session-motor-imagery-left-hand-vs, within-session-motor-imagery-right-hand-vs-1, within-session-motor-imagery-all-classes-on-2",,http://moabb.neurotechx.com/docs/generated/moabb.datasets.BNCI2014_001.html,https://paperswithcode.com/dataset/bnci2014-001-moabb-1,,,,,,,
352,BNCI2014-002_MOABB,Within-Session Motor Imagery (right hand vs. feet),Within-Session Motor Imagery (right hand vs. feet),Within-Session Motor Imagery (right hand vs. feet),,,Methodology,within-session-motor-imagery-right-hand-vs-2,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.BNCI2014_002.html,https://paperswithcode.com/dataset/bnci2014-002-moabb-1,,,,,,,
353,BNCI2014-004_MOABB,Within-Session Motor Imagery (left hand vs. right hand),Within-Session Motor Imagery (left hand vs. right hand),Within-Session Motor Imagery (left hand vs. right hand),,,Methodology,within-session-motor-imagery-left-hand-vs-1,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.BNCI2014_004.html,https://paperswithcode.com/dataset/bnci2014-004-moabb-1,,,,,,,
354,BNCI2015-001_MOABB,Within-Session Motor Imagery (right hand vs. feet),Within-Session Motor Imagery (right hand vs. feet),Within-Session Motor Imagery (right hand vs. feet),,,Methodology,within-session-motor-imagery-right-hand-vs-3,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.BNCI2015_001.html,https://paperswithcode.com/dataset/bnci2015-001-moabb-1,,,,,,,
355,BNCI2015-003_MOABB,Within-Session ERP,Within-Session ERP,Within-Session ERP,,,Methodology,within-session-erp-on-bnci2015-003-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.BNCI2015_003.html,https://paperswithcode.com/dataset/bnci2015-003-moabb-1,,,,,,,
356,BNCI2015-004_MOABB,Within-Session Motor Imagery (right hand vs. feet),Within-Session Motor Imagery (right hand vs. feet),Within-Session Motor Imagery (right hand vs. feet),,,Methodology,within-session-motor-imagery-right-hand-vs-4,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.BNCI2015_004.html,https://paperswithcode.com/dataset/bnci2015-004-moabb-1,,,,,,,
357,BODMAS,Malware Classification,Malware Classification,"Malware Classification, Behavioral Malware Classification, Malware Analysis, Malware Detection, Malware Clustering, Behavioral Malware Detection",Image,,Computer Vision,,,https://whyisyoung.github.io/BODMAS/,https://paperswithcode.com/dataset/bodmas,"We collaborate with Blue Hexagon to release a dataset containing timestamped malware samples and well-curated family information for research purposes. 
The BODMAS dataset contains 57,293 malware samples and 77,142 benign samples collected from August 2019 to September 2020,  with carefully curated family information (581 families).
We also provide preprocessed feature vectors and metadata available to everyone.
The malware binaries can be obtained per request.",2019,BODMAS: An Open Dataset for Learning based Temporal Analysis of PE Malware,https://liminyang.web.illinois.edu/data/DLS21_BODMAS.pdf,,,
358,Bone_Cement_Removal_with_Audio-Monitoring,Prediction,Prediction,Prediction,Time Series,,Methodology,,Creative Commons Attribution,https://ieee-dataport.org/documents/bone-cement-removal-audio-monitoring-and-erosion-depth,https://paperswithcode.com/dataset/bone-cement-removal-with-audio-monitoring,"This dataset comprises extensive multi-modal data related to the experimental study of ultrasonically excited pulsating fluid jets used for bone cement removal. Conducted at the Institute of Geonics, Ostrava, Czech Republic, the study explores the effect of varying standoff distances on erosion profiles, under controlled parameters including a fixed nozzle diameter, sonotrode frequency, supply pressure, and robot arm velocity. The dataset includes numerical data representing ablation profiles, captured as a large CSV file, and audio recordings captured using a high-resolution microphone. Ablation profiles are measured at standoff distances that span several discrete lengths, ensuring diverse sample conditions. The audio data derives from high-fidelity recordings at a 38.4 kHz sampling rate in .wav format, capturing the fluid jet's impact sounds as it interacts initially with a metal plate and subsequently with the bone cement. This audio information was processed into Mel Spectrograms, which efficiently capture the frequency distribution crucial for predictive analysis. The dataset is organized into training and testing subsets, with the initial portions used to train predictive models leveraging Mel Spectrogram inputs to predict erosion profiles. This dataset offers insights into the dynamic interactions between a pulsating fluid jet and target materials, providing a foundation for predictive modeling in non-invasive surgical procedures.",,,,,,
359,Bone_Fracture_Multi-Region_X-ray_Dataset,2D Object Detection,2D Object Detection,"2D Object Detection, Image to 3D, 2D Tiny Object Detection","3D, Image",,Computer Vision,,Open Data Commons – PDDL v1.0,,https://paperswithcode.com/dataset/bone-fracture-multi-region-x-ray-dataset,"This dataset consists of both fractured and non-fractured X-ray images encompassing various anatomical regions of the body, such as the lower limb, upper limb, lumbar region, hips, knees, and more. It is organized into three main folders: train, test, and validation, each containing both fractured and non-fractured radiographic images. You can freely access the dataset via the following link: https://www.kaggle.com/datasets/bmadushanirodrigo/fracture-multi-region-x-ray-data/data",,,,,"train, test, and validation, each containing both fractured and non-fractured radiographic images",
360,Bonn_RGB-D_Dynamic,Simultaneous Localization and Mapping,Simultaneous Localization and Mapping,Simultaneous Localization and Mapping,Image,,Computer Vision,,,http://www.ipb.uni-bonn.de/data/rgbd-dynamic-dataset/,https://paperswithcode.com/dataset/bonn-rgb-d-dynamic,"Bonn RGB-D Dynamic is a dataset for RGB-D SLAM, containing highly dynamic sequences. We provide 24 dynamic sequences, where people perform different tasks, such as manipulating boxes or playing with balloons, plus 2 static sequences. For each scene we provide the ground truth pose of the sensor, recorded with an Optitrack Prime 13 motion capture system. The sequences are in the same format as the TUM RGB-D Dataset, so that the same evaluation tools can be used. Furthermore, we provide a ground truth 3D point cloud of the static environment recorded using a Leica BLK360 terrestrial laser scanner.",,,,,,
361,BookCorpus,Text Generation,Text Generation,"Text Generation, Language Modelling, Word Embeddings",Text,English,Natural Language Processing,language-modelling-on-bookcorpus2,,https://github.com/soskek/bookcorpus,https://paperswithcode.com/dataset/bookcorpus,"BookCorpus is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).",,Temporal Event Knowledge Acquisition via Identifying Narratives,https://arxiv.org/abs/1805.10956,74M sentences,,
362,BookSum,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Text Summarization, Document Summarization",Text,English,Natural Language Processing,text-summarization-on-booksum,BSD-3 License,https://github.com/salesforce/booksum,https://paperswithcode.com/dataset/booksum,"BookSum is a collection of datasets for long-form narrative summarization. This dataset covers source documents from the literature domain, such as novels, plays and stories, and includes highly abstractive, human written summaries on three levels of granularity of increasing difficulty: paragraph-, chapter-, and book-level. The domain and structure of this dataset poses a unique set of challenges for summarization systems, which include: processing very long documents, non-trivial causal and temporal dependencies, and rich discourse structures.

BookSum contains summaries for 142,753 paragraphs, 12,293 chapters and 436 books.",,,,753 paragraphs,,
363,BoolQ,parameter-efficient fine-tuning,parameter-efficient fine-tuning,"parameter-efficient fine-tuning, Question Answering, Classification","Image, Text",English,Computer Vision,"classification-on-boolq, question-answering-on-boolq, parameter-efficient-fine-tuning-on-boolq",CC BY-SA 3.0,https://github.com/google-research-datasets/boolean-questions,https://paperswithcode.com/dataset/boolq,"BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring – they are generated in unprompted and unconstrained settings.
Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context.

Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified and questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable” if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question’s answer is “yes” or “no”. Only questions that were marked as having a yes/no answer are used, and each question is paired with the selected passage instead of the entire document.",,,,15942 examples,,
364,Bosch_CNC_Machining_Dataset,Time Series Classification,Time Series Classification,"Time Series Classification, Anomaly Detection, Semi-supervised time series classification, Time Series Clustering","Image, Time Series",,Time Series,,CC-BY-4.0,https://github.com/boschresearch/CNC_Machining,https://paperswithcode.com/dataset/bosch-cnc-machining-dataset,"The dataset provided is a collection of real-world industrial vibration data collected from a brownfield CNC milling machine. The acceleration has been measured using a tri-axial accelerometer (Bosch CISS Sensor) mounted inside the machine. The X- Y- and Z-axes of the accelerometer have been recorded using a sampling rate equal to 2 kHz. Thereby normal as well as anomalous data have been collected for 4 different timeframes, each lasting 5 months from February 2019 until August 2021 and labelled accordingly. It can be used to investigate the scalability of models and research process variations as the anomaly impact differs. In total there is data from three different CNC milling machines each executing 15 processes. For a detailed description of the data and experimental set-up, please refer to the paper: https://doi.org/10.1016/j.procir.2022.04.022",2019,,,,,
365,BotNet,Graph Learning,Graph Learning,Graph Learning,Graph,,Methodology,,,https://github.com/harvardnlp/botnet-detection,https://paperswithcode.com/dataset/botnet,The BotNet dataset is a set of topological botnet detection datasets forgraph neural networks.,,,,,,
366,Box-Jenkins,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Time Series Prediction",Time Series,,Time Series,,,,https://paperswithcode.com/dataset/box-jenkins,"Box-Jenkins gas furnace, a well-known time series forecasting problem",,,,,,
367,BP4D,Action Unit Detection,Action Unit Detection,"Action Unit Detection, Facial Action Unit Detection, Facial Expression Recognition (FER)","Image, Video",,Computer Vision,"action-unit-detection-on-bp4d, facial-expression-recognition-on-bp4d, facial-action-unit-detection-on-bp4d",Custom,http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html,https://paperswithcode.com/dataset/bp4d,"The BP4D-Spontaneous dataset is a 3D video database of spontaneous facial expressions in a diverse group of young adults. Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Facial features were tracked in both 2D and 3D domains using both person-specific and generic approaches.
The database includes forty-one participants (23 women, 18 men). They were 18 – 29 years of age; 11 were Asian, 6 were African-American, 4 were Hispanic, and 20 were Euro-American.  An emotion elicitation protocol was designed to elicit emotions of participants effectively. Eight tasks were covered with an interview process and a series of activities to elicit eight emotions.
The database is structured by participants. Each participant is associated with 8 tasks. For each task, there are both 3D and 2D videos. As well, the Metadata include manually annotated action units (FACS AU), automatically tracked head pose, and 2D/3D facial landmarks.  The database is in the size of about 2.6TB (without compression).",,,,,,
368,BRACS,Breast Cancer Detection,Breast Cancer Detection,Breast Cancer Detection,Image,,Computer Vision,,,https://www.bracs.icar.cnr.it/,https://paperswithcode.com/dataset/bracs,"BReAst Carcinoma Subtyping (BRACS) dataset, a large cohort of annotated Hematoxylin & Eosin (H&E)-stained images to facilitate the characterization of breast lesions. BRACS contains 547 Whole-Slide Images (WSIs), and 4539 Regions of Interest (ROIs) extracted from the WSIs. Each WSI, and respective ROIs, are annotated by the consensus of three board-certified pathologists into different lesion categories. Specifically, BRACS includes three lesion types, i.e., benign, malignant and atypical, which are further subtyped into seven categories.",,,,,,
369,BrainInvaders2012_MOABB,Within-Session ERP,Within-Session ERP,Within-Session ERP,,,Methodology,within-session-erp-on-braininvaders2012-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.BI2012.html,https://paperswithcode.com/dataset/braininvaders2012-moabb,,,,,,,
370,BrainInvaders2013a_MOABB,Within-Session ERP,Within-Session ERP,Within-Session ERP,,,Methodology,within-session-erp-on-braininvaders2013a,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.BI2013a.html,https://paperswithcode.com/dataset/braininvaders2013a-moabb,,,,,,,
371,BrainInvaders2014b_MOABB,Within-Session ERP,Within-Session ERP,Within-Session ERP,,,Methodology,within-session-erp-on-braininvaders2014b,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.BI2014b.html,https://paperswithcode.com/dataset/braininvaders2014b-moabb,,,,,,,
372,BrainInvaders2015b_MOABB,Within-Session ERP,Within-Session ERP,Within-Session ERP,,,Methodology,within-session-erp-on-braininvaders2015b,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.BI2015b.html,https://paperswithcode.com/dataset/braininvaders2015b-moabb,,,,,,,
373,Branched_Deformable_Linear_Objects__BDLOs__Dataset,Deformable Object Manipulation,Deformable Object Manipulation,Deformable Object Manipulation,,,Methodology,,CC BY-NC-SA,,https://paperswithcode.com/dataset/branched-deformable-linear-objects-bdlos,"For each BDLO, dynamic trajectory data is captured in real-world settings using a motion capture system operating at 100 Hz when robots grasp the BDLO’s ends. For details on dataset usage, please refer to DEFT_train.py.
For BDLO 1 and BDLO 3, we record dynamic trajectory data when one robot grasps the middle of the BDLO while the other robot grasps one of its ends.",,,,,,
374,BraTS-Africa,Brain Tumor Segmentation,Brain Tumor Segmentation,Brain Tumor Segmentation,Image,,Computer Vision,brain-tumor-segmentation-on-brats-africa,,https://arxiv.org/abs/2305.19369,https://paperswithcode.com/dataset/brats-africa,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,Homepage,https://arxiv.org/abs/2305.19369,,,
375,BraTS_2013,Brain Tumor Segmentation,Brain Tumor Segmentation,Brain Tumor Segmentation,Image,,Computer Vision,"brain-tumor-segmentation-on-brats-2013-1, brain-tumor-segmentation-on-brats-2013",Custom (non-commercial),https://www.smir.ch/BRATS/Start2013,https://paperswithcode.com/dataset/brats-2013-1,"BRATS 2013 is a brain tumor segmentation dataset consists of synthetic and real images, where each of them is further divided into high-grade gliomas (HG) and low-grade gliomas (LG). There are 25 patients with both synthetic HG and LG images and 20 patients with real HG and 10 patients with real LG images. For each patient, FLAIR, T1, T2, and post-Gadolinium T1 magnetic resonance (MR) image sequences are available.",2013,Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization,https://arxiv.org/abs/1908.06965,,,
376,BraTS_2014,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Brain Tumor Segmentation, Tumor Segmentation",Image,,Computer Vision,brain-tumor-segmentation-on-brats-2014,,https://www.smir.ch/BRATS/Start2014,https://paperswithcode.com/dataset/brats-2014-1,BRATS 2014 is a brain tumor segmentation dataset.,2014,Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization,https://arxiv.org/abs/1908.06965,,,
377,BraTS_2015,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Brain Tumor Segmentation, Tumor Segmentation",Image,,Computer Vision,brain-tumor-segmentation-on-brats-2015,Custom,https://www.smir.ch/BRATS/Start2015,https://paperswithcode.com/dataset/brats-2015-1,"The BraTS 2015 dataset is a dataset for brain tumor image segmentation. It consists of 220 high grade gliomas (HGG) and 54 low grade gliomas (LGG) MRIs. The four MRI modalities are T1, T1c, T2, and T2FLAIR. Segmented “ground truth” is provide about four intra-tumoral classes, viz. edema, enhancing tumor, non-enhancing tumor, and necrosis.",2015,Brain MRI Tumor Segmentation with Adversarial Networks,https://arxiv.org/abs/1910.02717,,,
378,BraTS_2016,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Data Augmentation, Tumor Segmentation",Image,,Computer Vision,,Custom,https://www.smir.ch/BRATS/Start2016,https://paperswithcode.com/dataset/brats-2016,"BRATS 2016 is a brain tumor segmentation dataset. It shares the same training set as BRATS 2015, which consists of 220 HHG and 54 LGG. Its testing dataset consists of 191 cases with unknown grades.",2016,,,,,
379,BraTS_2017,Brain Tumor Segmentation,Brain Tumor Segmentation,Brain Tumor Segmentation,Image,,Computer Vision,brain-tumor-segmentation-on-brats-2017-val,Custom (attribution),https://www.med.upenn.edu/sbia/brats2017/data.html,https://paperswithcode.com/dataset/brats-2017-1,"The BRATS2017 dataset. It contains 285 brain tumor MRI scans, with four MRI modalities as T1, T1ce, T2, and Flair for each scan. The dataset also provides full masks for brain tumors, with labels for ED, ET, NET/NCR. The segmentation evaluation is based on three tasks: WT, TC and ET segmentation.",,Scribble-based Hierarchical Weakly Supervised Learning for Brain Tumor Segmentation,https://arxiv.org/abs/1911.02014,,,
380,BraTs_Peds_2024,Brain Tumor Segmentation,Brain Tumor Segmentation,"Brain Tumor Segmentation, MRI segmentation",Image,,Computer Vision,brain-tumor-segmentation-on-brats-peds-2024,,https://arxiv.org/abs/2404.15009,https://paperswithcode.com/dataset/brats-peds-2024,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,Homepage,https://arxiv.org/abs/2404.15009,,,
381,Brazilian_Protest,Event Detection,Event Detection,"Event Detection, Twitter Event Detection",Image,,Computer Vision,,MIT,https://github.com/jdnascim/7set-al,https://paperswithcode.com/dataset/brazilian-protest,"Brazilian Protest is a dataset for event filtering that focuses on protests in multi-modal social media data, with most of the text in Portuguese. The dataset contains 4.5 million tweets, of which 155 thousand are associated with an URL to an uncurated article and 370 thousand have an associated media content (including the media of the uncurated articles).",,Few-shot Learning for Multi-modal Social Media Event Filtering,https://arxiv.org/pdf/2211.10340v1.pdf,,,
382,Breakfast,Weakly Supervised Action Segmentation (Action Set)),Weakly Supervised Action Segmentation (Action Set)),"Weakly Supervised Action Segmentation (Action Set)), Weakly Supervised Action Segmentation (Transcript), Action Segmentation, Video Classification, Unsupervised Action Segmentation, Long-video Activity Recognition","Image, Video",,Computer Vision,"long-video-activity-recognition-on-breakfast, weakly-supervised-action-segmentation-action, weakly-supervised-action-segmentation, unsupervised-action-segmentation-on-breakfast, action-segmentation-on-breakfast-1, video-classification-on-breakfast",CC BY 4.0,https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/,https://paperswithcode.com/dataset/breakfast,"The Breakfast Actions Dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. The dataset is one of the largest fully annotated datasets available. The actions are recorded “in the wild” as opposed to a single controlled lab environment. It consists of over 77 hours of video recordings.",,,,,,
383,BreakHis,Medical Image Retrieval,Medical Image Retrieval,"Medical Image Retrieval, Image Classification, Breast Cancer Histology Image Classification, Breast Cancer Histology Image Classification (20% labels), Breast Cancer Detection",Image,,Medical,"breast-cancer-detection-on-breakhis, breast-cancer-histology-image-classification, image-classification-on-breakhis, breast-cancer-histology-image-classification-1, medical-image-retrieval-on-breakhis",,https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/,https://paperswithcode.com/dataset/breakhis,"The Breast Cancer Histopathological Image Classification (BreakHis) is  composed of 9,109 microscopic images of breast tumor tissue collected from 82 patients using different magnifying factors (40X, 100X, 200X, and 400X).  It contains 2,480  benign and 5,429 malignant samples (700X460 pixels, 3-channel RGB, 8-bit depth in each channel, PNG format). This database has been built in collaboration with the P&D Laboratory - Pathological Anatomy and Cytopathology, Parana, Brazil.

Paper: F. A. Spanhol, L. S. Oliveira, C. Petitjean and L. Heutte, ""A Dataset for Breast Cancer Histopathological Image Classification,"" in IEEE Transactions on Biomedical Engineering, vol. 63, no. 7, pp. 1455-1462, July 2016, doi: 10.1109/TBME.2015.2496264",2016,,,,,
384,BreastClassifications4,Breast Tumour Classification,Breast Tumour Classification,"Breast Tumour Classification, Breast Cancer Detection, Classification, Medical Diagnosis",Image,,Computer Vision,,AGPL-3.0,https://mimbcd-ui.github.io/dataset-uta4-classifications,https://paperswithcode.com/dataset/breastclassifications4,"Several datasets are fostering innovation in higher-level functions for everyone, everywhere. By providing this repository, we hope to encourage the research community to focus on hard problems. In this repository, we present the real results severity (BIRADS) and pathology (post-report) classifications provided by the Radiologist Director from the Radiology Department of Hospital Fernando Fonseca while diagnosing several patients (see dataset-uta4-dicom) from our User Tests and Analysis 4 (UTA4) study. Here, we provide a dataset for the measurements of both severity (BIRADS) and pathology classifications concerning the patient diagnostic. Work and results are published on a top Human-Computer Interaction (HCI) conference named AVI 2020 (page). Results were analyzed and interpreted from our Statistical Analysis charts. The user tests were made in clinical institutions, where clinicians diagnose several patients for a Single-Modality vs Multi-Modality comparison. For example, in these tests, we used both prototype-single-modality and prototype-multi-modality repositories for the comparison. On the same hand, the hereby dataset represents the pieces of information of both BreastScreening and MIDA projects. These projects are research projects that deal with the use of a recently proposed technique in literature: Deep Convolutional Neural Networks (CNNs). From a developed User Interface (UI) and framework, these deep networks will incorporate several datasets in different modes. For more information about the available datasets please follow the Datasets page on the Wiki of the meta information repository. Last but not least, you can find further information on the Wiki in this repository. We also have several demos to see in our YouTube Channel, please follow us.",2020,,,,,
385,BreastDICOM4,Medical Diagnosis,Medical Diagnosis,"Medical Diagnosis, Medical Image Segmentation, Medical Image Retrieval, Medical Image Registration",Image,,Medical,medical-diagnosis-on-mimbcd-ui-uta7-medical,AGPL-3.0,https://mimbcd-ui.github.io/dataset-uta4-dicom,https://paperswithcode.com/dataset/mimbcd-ui-uta7-medical-imaging-dicom-files,"Several datasets are fostering innovation in higher-level functions for everyone, everywhere. By providing this repository, we hope to encourage the research community to focus on hard problems. In this repository, we present our medical imaging DICOM files of patients from our User Tests and Analysis 4 (UTA4) study. Here, we provide a dataset of the used medical images during the UTA4 tasks. This repository and respective dataset should be paired with the dataset-uta4-rates repository dataset. Work and results are published on a top Human-Computer Interaction (HCI) conference named AVI 2020 (page). Results were analyzed and interpreted on our Statistical Analysis charts. The user tests were made in clinical institutions, where clinicians diagnose several patients for a Single-Modality vs Multi-Modality comparison. For example, in these tests, we used both prototype-single-modality and prototype-multi-modality repositories for the comparison. On the same hand, the hereby dataset represents the pieces of information of both BreastScreening and MIDA projects. These projects are research projects that deal with the use of a recently proposed technique in literature: Deep Convolutional Neural Networks (CNNs). From a developed User Interface (UI) and framework, these deep networks will incorporate several datasets in different modes. For more information about the available datasets please follow the Datasets page on the Wiki of the meta information repository. Last but not least, you can find further information on the Wiki in this repository. We also have several demos to see in our YouTube Channel, please follow us.",2020,,,,"Tests and Analysis 4 (UTA4) study. Here, we provide a dataset of the used medical images",
386,BreastRates4,Medical Image Retrieval,Medical Image Retrieval,"Medical Image Retrieval, Medical Diagnosis",Image,,Medical,,AGPL-3.0,https://mimbcd-ui.github.io/dataset-uta4-rates,https://paperswithcode.com/dataset/breastrates4,"Several datasets are fostering innovation in higher-level functions for everyone, everywhere. By providing this repository, we hope to encourage the research community to focus on hard problems. In this repository, we present our severity rates (BIRADS) of clinicians while diagnosing several patients from our User Tests and Analysis 4 (UTA4) study. Here, we provide a dataset for the measurements of severity rates (BIRADS) concerning the patient diagnostic. Work and results are published on a top Human-Computer Interaction (HCI) conference named AVI 2020 (page). Results were analyzed and interpreted from our Statistical Analysis charts. The user tests were made in clinical institutions, where clinicians diagnose several patients for a Single-Modality vs Multi-Modality comparison. For example, in these tests, we used both prototype-single-modality and prototype-multi-modality repositories for the comparison. On the same hand, the hereby dataset represents the pieces of information of both BreastScreening and MIDA projects. These projects are research projects that deal with the use of a recently proposed technique in literature: Deep Convolutional Neural Networks (CNNs). From a developed User Interface (UI) and framework, these deep networks will incorporate several datasets in different modes. For more information about the available datasets please follow the Datasets page on the Wiki of the meta information repository. Last but not least, you can find further information on the Wiki in this repository. We also have several demos to see in our YouTube Channel, please follow us.",2020,,,,,
387,Breast_Lesion_Detection_in_Ultrasound_Videos__CVA-,Breast Tumour Classification,Breast Tumour Classification,"Breast Tumour Classification, Lesion Detection, Breast Cancer Detection",Image,,Computer Vision,,TBC,https://pan.baidu.com/s/1yYME7-DvvIEZzCb72NXaJA?pwd=jnie,https://paperswithcode.com/dataset/breast-lesion-detection-in-ultrasound-video,"The breast lesion detection in ultrasound videos dataset uses a clip-level and video-level feature aggregated network (CVA-Net) and consists of 188 ultrasound videos, of which 113 are labeled malignant and 75 benign. Overall these consist of 25,272 ultrasound images in total with the number of images for each video varying from 28 to 413. 150 videos were used for training, 38 for testing. The primary intended use case would be for computer-aided breast cancer diagnosis, supporting systems to assist radiologists.

Here are more details summarising the approach:


A novel network: a new state-of-the-art clip-level and video level feature aggregated network (CVA-Net) created to aggregate clip-level temporal features and video-level lesion classification features to fuse and into a prediction classifier. It outperformed existing methods mainly focused on 2D images and or fusing with unlabeled videos.
The need for increased accuracy contributed to the motivation, given detection challenges due to blurry breast lesion boundaries, inhomogeneous distributions, changeable breast lesion sizes, and positions in dynamic video.
Each video has a complete scan of the tumor, from where it becomes visible to where it is no longer visible as well as the largest section - all acquired via LOGIQ-E9 and PHILIPS TIS L9-3 ultrasound machines .
2 pathologists with 8 years of experience invited to manually annotate the breast lesion rectangles inside each frame and give the corresponding classification.

Second source in addition to separate homepage URL below:  https://github.com/jhl-Det/CVA-Net/tree/main/datasets",,,,,,
388,BRIND,Edge Detection,Edge Detection,Edge Detection,Image,,Computer Vision,edge-detection-on-brind,,https://github.com/MengyangPu/RINDNet,https://paperswithcode.com/dataset/brind,"BRIND is a short name of BSDS-RIND is the first public benchmark that dedicated to studying simultaneously the four edge types, namely Reflectance Edge (RE), Illumination Edge (IE), Normal Edge (NE) and Depth Edge (DE)",,,,,,
389,BrixIA,COVID-19 Diagnosis,COVID-19 Diagnosis,COVID-19 Diagnosis,,,Methodology,,,https://brixia.github.io/,https://paperswithcode.com/dataset/brixia,"BrixIA Covid-19 is a large dataset of CXR images corresponding to the entire amount of images taken for both triage and patient monitoring in sub-intensive and intensive care units during one month (between March 4th and April 4th 2020) of pandemic peak at the ASST Spedali Civili di Brescia, and contains all the variability originating from a real clinical scenario. It includes 4,707 CXR images of COVID-19 subjects, acquired with both CR and DX modalities, in AP or PA projection, and retrieved from the facility RIS-PACS system.",2020,,,,,
390,BRUSH,Handwriting generation,Handwriting generation,"Handwriting generation, Handwriting Recognition, Handwriting Verification","Image, Text",English,Computer Vision,,"Custom (research-only, non-commercial)",http://dsd.cs.brown.edu/,https://paperswithcode.com/dataset/brush,"The BRUSH dataset (BRown University Stylus Handwriting) contains 27,649 online handwriting samples from a total of 170 writers. Every sequence is labeled with intended characters such that dataset users can identify to which character a point in a sequence corresponds. The dataset was introduced in the paper ""Generating Handwriting via Decoupled Style Descriptors"" by Atsunobu Kotani, Stefanie Tellex, James Tompkin from Brown University, presented at European Conference on Computer Vision (ECCV) 2020.

Terms of Use
The BRUSH dataset may only be used for non-commercial research purposes. Anyone wanting to use it for other purposes should contact Prof. James Tompkin. If you publish materials based on this database, we request that you please include a reference to our paper.

@inproceedings{kotani2020generating,
  title={Generating Handwriting via Decoupled Style Descriptors},
  author={Kotani, Atsunobu and Tellex, Stefanie and Tompkin, James},
  booktitle={European Conference on Computer Vision},
  pages={764--780},
  year={2020},
  organization={Springer}
}",2020,,,,,
391,BRWAC,Word Sense Induction,Word Sense Induction,Word Sense Induction,,,Methodology,,,https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC,https://paperswithcode.com/dataset/brwac,"Composed by 2.7 billion tokens, and has been annotated with tagging and parsing information.",,,,,,
392,BSD,Color Image Denoising,Color Image Denoising,"Color Image Denoising, Image Super-Resolution, Density Estimation, Salt-And-Pepper Noise Removal, Unified Image Restoration, Grayscale Image Denoising, Blind Super-Resolution, Image Denoising, Compressive Sensing","Audio, Image",,Computer Vision,"color-image-denoising-on-bsd68-sigma10, blind-super-resolution-on-bsd100-2x-upscaling, color-image-denoising-on-bsd68-sigma35, salt-and-pepper-noise-removal-on-bsd300-noise, grayscale-image-denoising-on-bsd68-sigma35, salt-and-pepper-noise-removal-on-bsd300-noise-2, image-denoising-on-bsd68-sigma50, color-image-denoising-on-bsd68-sigma70, grayscale-image-denoising-on-bsd68-sigma25, blind-super-resolution-on-bsd100-4x-upscaling, compressive-sensing-on-bsd68-cs-50, color-image-denoising-on-bsd68-sigma5, grayscale-image-denoising-on-bsd68-sigma40, grayscale-image-denoising-on-bsd68-sigma75, grayscale-image-denoising-on-bsd68-sigma30, image-denoising-on-bsd68-sigma30, grayscale-image-denoising-on-bsd68-sigma5, grayscale-image-denoising-on-bsd68-sigma55, grayscale-image-denoising-on-bsd68-sigma15, unified-image-restoration-on-bsd68-sigma25, image-super-resolution-on-bsds100-2x, grayscale-image-denoising-on-bsd200-sigma10, grayscale-image-denoising-on-bsd68-sigma50, compressive-sensing-on-bsds100-2x-upscaling, image-super-resolution-on-bsds100-8x, color-image-denoising-on-bsd68-sigma75, grayscale-image-denoising-on-bsd68-sigma10, grayscale-image-denoising-on-bsd68-sigma20, color-image-denoising-on-bsd68-sigma25, grayscale-image-denoising-on-bsd200-sigma70, density-estimation-on-bsds300, image-super-resolution-on-bsd100-16x, image-super-resolution-on-bsd100-2x-upscaling, grayscale-image-denoising-on-bsd68-sigma65, image-super-resolution-on-bsd100-4x-upscaling, image-super-resolution-on-bsd200-2x-upscaling, image-super-resolution-on-bsd100-3x-upscaling, color-image-denoising-on-bsd68-sigma15, grayscale-image-denoising-on-bsd68-sigma60, grayscale-image-denoising-on-bsd200-sigma30, grayscale-image-denoising-on-bsd68-sigma70, image-super-resolution-on-bsds100-4x, grayscale-image-denoising-on-bsd68-sigma45, color-image-denoising-on-bsd68-sigma30, salt-and-pepper-noise-removal-on-bsd300-noise-1, grayscale-image-denoising-on-bsd200-sigma50, image-super-resolution-on-bsd100-8x-upscaling, blind-super-resolution-on-bsd100-3x-upscaling","Custom (research-only, non-commercial, attribution)",https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/,https://paperswithcode.com/dataset/bsd,"BSD is a dataset used frequently for image denoising and super-resolution. Of the subdatasets, BSD100 is aclassical image dataset having 100 test images proposed by Martin et al.. The dataset is composed of a large variety of images ranging from natural images to object-specific such as plants, people, food etc. BSD100 is the testing set of the Berkeley segmentation dataset BSD300.",,A Deep Journey into Super-resolution: A Survey,https://arxiv.org/abs/1904.07523,,,
393,BSD100,Blind Super-Resolution,Blind Super-Resolution,"Blind Super-Resolution, Image Super-Resolution, Joint Demosaicing and Denoising",Image,,Computer Vision,"blind-super-resolution-on-bsd100-3x-upscaling, blind-super-resolution-on-bsd100-4x-upscaling, image-super-resolution-on-bsd100-16x, joint-demosaicing-and-denoising-on-bsd100, blind-super-resolution-on-bsd100-2x-upscaling, image-super-resolution-on-bsd100-8x-upscaling, image-super-resolution-on-bsd100-2x-upscaling, image-super-resolution-on-bsd100-4x-upscaling, image-super-resolution-on-bsd100-3x-upscaling",,,https://paperswithcode.com/dataset/bsd100,,,,,,,
394,BSDS500,Edge Detection,Edge Detection,"Edge Detection, Image Compression, JPEG Artifact Correction",Image,,Computer Vision,"jpeg-artifact-correction-on-bsds500-quality-1, jpeg-artifact-correction-on-bsds500-quality, jpeg-artifact-correction-on-bsds500-quality-5, image-compression-on-bsds500, edge-detection-on-bsds500-1, jpeg-artifact-correction-on-bsds500-quality-3, jpeg-artifact-correction-on-bsds500-quality-4, jpeg-artifact-correction-on-bsds500-quality-2",,https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html,https://paperswithcode.com/dataset/bsds500,"Berkeley Segmentation Data Set 500 (BSDS500) is a standard benchmark for contour detection. This dataset is designed for evaluating natural edge detection that includes not only object contours but also object interior boundaries and background boundaries. It includes 500 natural images with carefully annotated boundaries collected from multiple users. The dataset is divided into three parts: 200 for training, 100 for validation and the rest 200 for test.",,Object Contour Detection with a Fully Convolutional Encoder-Decoder Network,https://arxiv.org/abs/1603.04530,,valuating natural edge detection that includes not only object contours but also object interior boundaries and background boundaries. It includes 500 natural images,
395,BTAD,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Supervised Anomaly Detection",Image,,Computer Vision,"supervised-anomaly-detection-on-btad, anomaly-detection-on-btad",,http://avires.dimi.uniud.it/papers/btad/btad.zip,https://paperswithcode.com/dataset/btad,The BTAD ( beanTech Anomaly Detection) dataset is a real-world industrial anomaly dataset. The dataset contains a total of 2830 real-world images of 3 industrial products showcasing body and surface defects.,,,,,,
396,BTS3.1,Face Image Retrieval,Face Image Retrieval,"Face Image Retrieval, Face Recognition, Face Verification, Person Re-Identification, Lightweight Face Recognition",Image,,Computer Vision,"face-verification-on-bts3-1, face-recognition-on-bts3-1",,https://www.iarpa.gov/research-programs/briar,https://paperswithcode.com/dataset/bts3-1,"Large, multimodal biometric dataset: It contains still images and videos of over 1,000 people captured at various ranges (up to 1,000 meters) and elevations (up to 400 meters) using a diverse set of cameras (commercial, military-grade, specialized).

Focus on challenging scenarios: Designed to address limitations of existing datasets, it includes data with extreme poses, low resolutions, and atmospheric distortions to push the boundaries of biometric recognition algorithms.

Rich annotations and diversity: Each data point is accompanied by detailed metadata (sensor details, weather conditions) and manual annotations (bounding boxes, yaw/pitch angles) for face and whole-body regions. The dataset also ensures demographic diversity for robust algorithm development.",,,,,,
397,BUAA-MIHR_dataset,Heart rate estimation,Heart rate estimation,"Heart rate estimation, Photoplethysmography (PPG) heart rate estimation",,,Methodology,,,https://xilin1991.github.io/Large-scale-Multi-illumination-HR-Database/,https://paperswithcode.com/dataset/buaa-mihr-dataset,"BUAA-MIHR dataset is a remote photoplethysmography (rPPG) dataset. BUAA-MIHR dataset for evaluation of remote photoplethysmography pipeline under multi-illumination situations. We recruited 15 healthy subjects (12 male, 3 female, 18 to 30 years old) in this experiment and a total number of 165 video sequences were recorded under various illuminations. The experiments were conducted in a darkroom in order to isolate from ambient light.",,,,,,
398,BubbleML,Operator learning,Operator learning,"Operator learning, Physics-informed machine learning",,,Methodology,,Creative Commons Attribution 4.0 International,https://github.com/HPCForge/BubbleML,https://paperswithcode.com/dataset/bubbleml,"A multi-physics dataset of boiling processes.
This repository includes downloads, visualizations, and sample applications.
This dataset can be used to train operator networks for phase-change phenomena, act as a ground truth for Physics-Informed Neural Networks, or train computer vision models.",,,,,,
399,BUCC,Cross-Lingual Bitext Mining,Cross-Lingual Bitext Mining,Cross-Lingual Bitext Mining,,,Methodology,"cross-lingual-bitext-mining-on-bucc-german-to, cross-lingual-bitext-mining-on-bucc-chinese, cross-lingual-bitext-mining-on-bucc-russian, cross-lingual-bitext-mining-on-bucc-french-to",Custom,https://comparable.limsi.fr/bucc2017/,https://paperswithcode.com/dataset/bucc,"The BUCC mining task is a shared task on parallel sentence extraction from two monolingual corpora with a subset of them assumed to be parallel, and that has been available since 2016. For each language pair, the shared task provides a monolingual corpus for each language and a gold mapping list containing true translation pairs. These pairs are the ground truth. The task is to construct a list of translation pairs from the monolingual corpora. The constructed list is compared to the ground truth, and evaluated in terms of the F1 measure.",2016,Language-agnostic BERT Sentence Embedding,https://arxiv.org/abs/2007.01852,,,
400,BUG,Gender Bias Detection,Gender Bias Detection,Gender Bias Detection,Image,,Computer Vision,,,https://www.github.com/SLAB-NLP/BUG,https://paperswithcode.com/dataset/bug,"BUG is a large-scale gender bias dataset of 108K diverse real-world English sentences, sampled semiautomatically from large corpora using lexical syntactic pattern matching",,,,,,
401,BUP20,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Instance Segmentation, Panoptic Segmentation, Neural Rendering",Image,,Computer Vision,,,http://agrobotics.uni-bonn.de/sweet_pepper_dataset/,https://paperswithcode.com/dataset/bup20,"Video sequences from a glasshouse environment in Campus Kleinaltendorf(CKA), University of Bonn, captured by PATHoBot, a glasshouse monitoring robot. 


10 Video sequences, 120s long.
2 cultivar Mavera (yellow) and All- rounder (red).
RGB-D images (Intel RealSense D435i cameras).
Robot odometry and IMU.
High quality sparese instance segmentation labels.",,,,,,
402,BurstSR,Multi-Frame Super-Resolution,Multi-Frame Super-Resolution,"Multi-Frame Super-Resolution, Burst Image Super-Resolution",Image,,Computer Vision,burst-image-super-resolution-on-burstsr,,https://github.com/goutamgmb/deep-burst-sr,https://paperswithcode.com/dataset/burstsr,BurstSR is a dataset consisting of smartphone bursts and high-resolution DSLR ground-truth,,,,,,
403,C-GQA,Compositional Zero-Shot Learning,Compositional Zero-Shot Learning,Compositional Zero-Shot Learning,Image,,Computer Vision,,,https://s3.mlcloud.uni-tuebingen.de/czsl/cgqa-updated.zip,https://paperswithcode.com/dataset/c-gqa,"We propose a split built on top of Stanford GQA dataset originally proposed for VQA and name it Compositional GQA (C-GQA) dataset (see supplementary for the details). CGQA contains over 9.5k compositional labels making it the most extensive dataset for CZSL. With cleaner labels and a larger label space, our hope is that this dataset will inspire further research on the topic.",,,,,,
404,C3,Language Modelling,Language Modelling,"Language Modelling, Machine Reading Comprehension, Reading Comprehension, Common Sense Reasoning (Few-Shot), Common Sense Reasoning (Zero-Shot), Common Sense Reasoning (One-Shot)",Text,English,Natural Language Processing,"common-sense-reasoning-zero-shot-on-c3, common-sense-reasoning-one-shot-on-c3, common-sense-reasoning-few-shot-on-c3",,https://github.com/nlpdata/c3,https://paperswithcode.com/dataset/c3,C3 is a free-form multiple-Choice Chinese machine reading Comprehension dataset.,,,,,,
405,C4,Language Modelling,Language Modelling,Language Modelling,Text,English,Natural Language Processing,language-modelling-on-c4,,https://github.com/google-research/text-to-text-transfer-transformer#c4,https://paperswithcode.com/dataset/c4,"C4 is a colossal, cleaned version of Common Crawl's web crawl corpus. It was based on Common Crawl dataset: https://commoncrawl.org. It was used to train the T5 text-to-text Transformer models.

The dataset can be downloaded in a pre-processed form from allennlp.",,,,,,
406,CaBuAr,Image Segmentation,Image Segmentation,"Image Segmentation, Segmentation Of Remote Sensing Imagery, Segmentation",Image,,Computer Vision,,,https://huggingface.co/datasets/DarthReca/california_burned_areas,https://paperswithcode.com/dataset/cabuar,"This dataset contains images from Sentinel-2 satellites taken before and after a wildfire. 
The ground truth masks are provided by the California Department of Forestry and Fire Protection and they are mapped on the images. 
The dataset is designed to do binary semantic segmentation of burned vs unburned areas.",,,,,,
407,CAD,Toxic Spans Detection,Toxic Spans Detection,"Toxic Spans Detection, Toxic Comment Classification",Image,,Computer Vision,toxic-comment-classification-on-cad,Creative Commons Attribution 4.0 International Public License,https://zenodo.org/record/4881008,https://paperswithcode.com/dataset/cad,"Dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the conversation thread, (3) contains rationales and (4) uses an expert-driven group-adjudication process for high quality annotations.",,,,,,
408,CADBench,Text to 3D,Text to 3D,"Text to 3D, Text-to-Code Generation, CAD Reconstruction, 3D Shape Modeling","3D, Text",English,Natural Language Processing,,apache-2.0,https://huggingface.co/datasets/FreedomIntelligence/CADBench,https://paperswithcode.com/dataset/cadbench,"📚 CADBench
CADBench is a comprehensive benchmark to evaluate the ability of LLMs to generate CAD scripts. It contains 500 simulated data samples and 200 data samples collected from online forums.

For more details, please visit our GitHub repository",,,,,valuate the ability of LLMs to generate CAD scripts. It contains 500 simulated data samples,
409,Cadenza_Woodwind,Music Transcription,Music Transcription,"Music Transcription, Audio Source Separation, Music Source Separation, Music Compression, Multi-instrument Music Transcription, Music Information Retrieval",Audio,,Audio,,CC BY 4.0,https://zenodo.org/records/14335552,https://paperswithcode.com/dataset/cadenza-woodwind,"This publicly available data is synthesised audio for woodwind quartets including renderings of each instrument in isolation. The data was created to be used as training data within Cadenza's second open machine learning challenge (CAD2) for the task on rebalancing classical music ensembles. The dataset is also intended for developing other music information retrieval (MIR) algorithms using machine learning. It was created because of the lack of large-scale datasets of classical woodwind music with separate audio for each instrument and permissive license for reuse. Music scores were selected from the OpenScore String Quartet corpus. These were rendered for two woodwind ensembles of (i) flute, oboe, clarinet and bassoon; and (ii) flute, oboe, alto saxophone and bassoon. This was done by a professional music producer using industry-standard software. Virtual instruments were used to create the audio for each instrument using software that interpreted expression markings in the score. Convolution reverberation was used to simulate a performance space and the ensembles mixed. The dataset consists of the audio and associated metadata

Nineteen scores were randomly selected from the OpenScore String Quartet corpus. The synthesis of these as woodwind ensembles was performed by a sound engineering professional using professional software. The scores were loaded into the music notation software Steinberg's Dorico. The string parts were allocated to flute, oboe, clarinet (or alto saxophone) and bassoon. Virtual instruments were used to create the audio for each instrument: Miroslav Philharmonik 2 by IK Multimedia for the saxophone and Intimate Studio Winds by 8Dio for the other parts. The quartets were then imported into Avid's Pro Tools where they were mixed and reverberation added. Metadata were generated..",,,,,,
410,CaDIS,2D Semantic Segmentation task 3 (25 classes),2D Semantic Segmentation task 3 (25 classes),"2D Semantic Segmentation task 3 (25 classes), 2D Semantic Segmentation task 1 (8 classes), 2D Semantic Segmentation task 2 (17 classes), 2D Semantic Segmentation",Image,,Computer Vision,"2d-semantic-segmentation-task-1-8-classes-on, 2d-semantic-segmentation-task-2-17-classes-on, 2d-semantic-segmentation-task-3-25-classes-on",,https://cataracts.grand-challenge.org/CaDIS/,https://paperswithcode.com/dataset/cadis,CaDIS: a Cataract Dataset for Image Segmentation is a dataset for semantic segmentation created by Digital Surgery Ltd. on top of the CATARACTS dataset. CaDIS consists of 4670 images sampled from the 25 videos on CATARACTS' training set. Each pixel in each image is labeled with its respective instrument or anatomical class from a set of 36 identified classes. More details about the dataset could be found in the paper (https://arxiv.org/pdf/1906.11586.pdf).,1906,,,4670 images,,
411,CAIL2019-SCM,Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),"Aspect-Based Sentiment Analysis (ABSA), Sentiment Analysis, Semantic Text Matching",Text,English,Natural Language Processing,"semantic-text-matching-on-cail2019-scm-val, semantic-text-matching-on-cail2019-scm-test",,https://github.com/china-ai-law-challenge/CAIL2019/tree/master/scm,https://paperswithcode.com/dataset/cail2019-scm,"Chinese AI and Law 2019 Similar Case Matching dataset. CAIL2019-SCM contains 8,964 triplets of cases published by the Supreme People's Court of China. CAIL2019-SCM focuses on detecting similar cases, and the participants are required to check which two cases are more similar in the triplets.",2019,,,,,
412,CAL500,Multi-Label Classification,Multi-Label Classification,"Multi-Label Classification, Matrix Completion, Multi-Task Learning",Image,,Computer Vision,,Custom,http://calab1.ucsd.edu/~datasets/,https://paperswithcode.com/dataset/cal500,"CAL500 (Computer Audition Lab 500) is a dataset aimed for evaluation of music information retrieval systems. It consists of 502 songs picked from western popular music. The audio is represented as a time series of the first 13 Mel-frequency cepstral coefficients (and their first and second derivatives) extracted by sliding a 12 ms half-overlapping short-time window over the waveform of each song. Each song has been annotated by at least 3 people with 135 musically-relevant concepts spanning six semantic categories:


29 instruments were annotated as present in the song or not,
22 vocal characteristics were annotated as relevant to the singer or not,
36 genres,
18 emotions were rated on a scale from one to three (e.g., not happy"",neutral"", ``happy""),
15 song concepts describing the acoustic qualities of the song, artist and recording (e.g., tempo, energy, sound quality),
15 usage terms (e.g., ""I would listen to this song while driving, sleeping, etc."").",,,,,,
413,Calandra_Dataset,Robotic Grasping,Robotic Grasping,Robotic Grasping,,,Methodology,,,https://sites.google.com/view/more-than-a-feeling,https://paperswithcode.com/dataset/https-drive-google-com-drive-folders-1wheg,"The Calandra dataset provides the data from a pair of tactile sensors attached to a jaw gripper (left and right) alongside the RGB images. A triplet of samples was captured ’before’, ’during’, and ’after’ grasping a plethora of objects. The objective is to determine the success or the failure of the grasp attempt.",,,,,,
414,Calcium_imaging_of_glomeruli_in_the_olfactory_bulb,3D Classification,3D Classification,3D Classification,"3D, Image",,Computer Vision,,Creative Commons Attribution 4.0 International,https://ultraviolet.library.nyu.edu/records/j2932-aaf14,https://paperswithcode.com/dataset/calcium-imaging-of-glomeruli-in-the-olfactory,"This dataset comprises video files (converted into tif format) that depict glomerular activation in mice. The activation was recorded as the response for 35 monomolecular odors. Wide-field 1-photon calcium imaging was recorded at a framerate of 100 Hz, in Thy1-GCaMP6f mice implanted with cranial windows over the olfactory bulb. Mice were head-fixed during imaging, with monomolecular odors presented in a randomized sequence for 2 seconds apiece during each trial.

This dataset can be used to develop novel Image segmentation techniques and ML based techniques for vision.",,,,,,
415,CALFW,Lightweight Face Recognition,Lightweight Face Recognition,"Lightweight Face Recognition, Synthetic Face Recognition, Face Verification, Face Recognition",Image,,Computer Vision,"synthetic-face-recognition-on-calfw, face-recognition-on-calfw, lightweight-face-recognition-on-calfw, face-verification-on-calfw",,http://whdeng.cn/CALFW/index.html,https://paperswithcode.com/dataset/calfw,"A renovation of Labeled Faces in the Wild (LFW), the de facto standard testbed for unconstraint face verification.",,,,,,
416,California_Housing_Prices,Tabular Data Generation,Tabular Data Generation,Tabular Data Generation,"Tabular, Text",English,Natural Language Processing,tabular-data-generation-on-california-housing,CCO,https://www.kaggle.com/datasets/camnugent/california-housing-prices,https://paperswithcode.com/dataset/california-housing-prices,"Median house prices for California districts derived from the 1990 census.

About Dataset

Context
This is the dataset used in the second chapter of Aurélien Géron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow'. It serves as an excellent introduction to implementing machine learning algorithms because it requires rudimentary data cleaning, has an easily understandable list of variables and sits at an optimal size between being to toyish and too cumbersome.

The data contains information from the 1990 California census. So although it may not help you with predicting current housing prices like the Zillow Zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.

Content
The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Be warned the data aren't cleaned so there are some preprocessing steps required! The columns are as follows, their names are pretty self-explanatory:
- longitude
- latitude
- housing_median_age
- total_rooms
- total_bedrooms
- population
- households
- median_income
- median_house_value
- ocean_proximity

Acknowledgements
This data was initially featured in the following paper:
Pace, R. Kelley, and Ronald Barry. ""Sparse spatial autoregressions."" Statistics & Probability Letters 33.3 (1997): 291-297.

and I encountered it in 'Hands-On Machine learning with Scikit-Learn and TensorFlow' by Aurélien Géron.
Aurélien Géron wrote:
This dataset is a modified version of the California Housing dataset available from:
Luís Torgo's page (University of Porto)

Inspiration
See my kernel on machine learning basics in R using this dataset, or venture over to the following link for a python based introductory tutorial: https://github.com/ageron/handson-ml/tree/master/datasets/housing",1990,,,,,
417,CALLHOME_American_English_Speech,Automatic Speech Recognition,Automatic Speech Recognition,"Automatic Speech Recognition, Speaker Verification, Speech Recognition, Speaker Diarization","Audio, Image, Text",English,Speech,"speaker-verification-on-callhome, speaker-diarization-on-callhome-109, speech-recognition-on-hub500-callhome, automatic-speech-recognition-on-callhome-3, speaker-diarization-on-callhome, speaker-diarization-on-hub5-00-callhome, speech-recognition-on-callhome-spanish-speech, automatic-speech-recognition-on-callhome-4, speech-recognition-on-switchboard-callhome",,https://catalog.ldc.upenn.edu/LDC97S42,https://paperswithcode.com/dataset/callhome-american-english-speech,"The CALLHOME English Corpus is a collection of unscripted telephone conversations between native speakers of English. Here are the key details:

Participants: 120 individuals.
Type of Study: Naturalistic.
Location: USA.
Media Type: Audio.
DOI: doi:10.21415/T5KP54.
Contents: The corpus contains 120 telephone conversations, each lasting up to 30 minutes.
Origins: All calls originated in North America, with 90 calls placed to various locations overseas and 30 calls within North America.
Transcripts: The transcripts cover contiguous 5 or 10-minute segments from recorded conversations.
Speaker Awareness: All speakers were aware that they were being recorded.
Topics: Participants had no guidelines on what to talk about; most called family members or close friends overseas.
Purpose: Collected primarily to support the project on Large Vocabulary Conversational Speech Recognition (LVCSR), sponsored by the U.S. Department of Defense.",,,,,,
418,Caltech-101,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Density Estimation, Semantic correspondence, Image Clustering, Fine-Grained Image Classification, Prompt Engineering, Semi-Supervised Image Classification, Unsupervised Anomaly Detection, Transductive Zero-Shot Classification",Image,,Computer Vision,"zero-shot-learning-on-caltech-101, prompt-engineering-on-caltech-101, fine-grained-image-classification-on-caltech, unsupervised-anomaly-detection-on-caltech-101-1, semi-supervised-image-classification-on-9, density-estimation-on-caltech-101, transductive-zero-shot-classification-on-6, semi-supervised-image-classification-on-8, image-clustering-on-caltech-101, semantic-correspondence-on-caltech-101",,http://www.vision.caltech.edu/Image_Datasets/Caltech101/,https://paperswithcode.com/dataset/caltech-101,"The Caltech101 dataset contains images from 101 object categories (e.g., “helicopter”, “elephant” and “chair” etc.) and a background category that contains the images not from the 101 object categories. For each object category, there are about 40 to 800 images, while most classes have about 50 images. The resolution of the image is roughly about 300×200 pixels.",,Simple and Efficient Learning using Privileged Information,https://arxiv.org/abs/1604.01518,800 images,,
419,Caltech-256,Few-Shot Image Classification,Few-Shot Image Classification,"Few-Shot Image Classification, Semi-Supervised Image Classification, Image Classification",Image,,Computer Vision,"semi-supervised-image-classification-on-11, few-shot-image-classification-on-caltech-256, image-classification-on-caltech-256, semi-supervised-image-classification-on-10",,http://www.vision.caltech.edu/Image_Datasets/Caltech256/,https://paperswithcode.com/dataset/caltech-256,"Caltech-256 is an object recognition dataset containing 30,607 real-world images, of different sizes, spanning 257 classes (256 object classes and an additional clutter class). Each class is represented by at least 80 images. The dataset is a superset of the Caltech-101 dataset.",,Exploiting Non-Linear Redundancy for Neural Model Compression,https://arxiv.org/abs/2005.14070,80 images,,257
420,CALVIN,Success Rate (5 task-horizon),Success Rate (5 task-horizon),"Success Rate (5 task-horizon), Robot Manipulation, Zero-shot Generalization, Avg. sequence length",Time Series,,Methodology,"zero-shot-generalization-on-calvin, avg-sequence-length-on-calvin, robot-manipulation-on-calvin, success-rate-5-task-horizon-on-calvin",MIT,https://github.com/mees/calvin,https://paperswithcode.com/dataset/calvin-composing-actions-from-language-and,"CALVIN (Composing Actions from Language and Vision), is an open-source simulated benchmark to learn long-horizon language-conditioned robot manipulation tasks.",,,,,,
421,CAMELS_Multifield_Dataset,Physical Simulations,Physical Simulations,"Physical Simulations, 3D Object Super-Resolution, Physics-informed machine learning, Multi-target regression",3D,,Methodology,,BY-NC-SA,https://camels-multifield-dataset.readthedocs.io,https://paperswithcode.com/dataset/camels-multifield-dataset,"CMD is a publicly available collection of hundreds of thousands 2D maps and 3D grids containing different properties of the gas, dark matter, and stars from more than 2,000 different universes. The data has been generated from thousands of state-of-the-art (magneto-)hydrodynamic and gravity-only N-body simulations from the CAMELS project.

Each 2D map and 3D grid has a set of labels associated to it: 2 cosmological parameters characterizing fundamental properties of the Universe, and 4 astrophysical parameters parametrizing the strength of astrophysical processes such as feedback from supernova and supermassive black-holes.

The main task this dataset was designed is to perform a robust inference on the value of the cosmological parameters from each map and grid. The data itself was generated from two completely different set of simulations, and it is not obvious that training one model on one will work when predicting on the other. Since simulations of the real Universe may never be perfect, this dataset provides the data to tackle this problem.

Solving this problem will help cosmologists to constrain the value of the cosmological parameters with the highest accuracy and therefore unveil the mysteries of our Universe. CMD can also be used for many other tasks, such as field mapping and super-resolution.",,,,,,
422,CAMELYON16,Breast Cancer Detection,Breast Cancer Detection,"Breast Cancer Detection, Multiple Instance Learning, whole slide images",Image,,Computer Vision,multiple-instance-learning-on-camelyon16,,https://camelyon16.grand-challenge.org/,https://paperswithcode.com/dataset/camelyon16,"The dataset consists of 400 whole-slide images (WSIs) of lymph node sections stained with hematoxylin and eosin (H&E), collected from two medical centers in the Netherlands. The WSIs are stored in a multi-resolution pyramid format, allowing for efficient retrieval of image subregions at different magnification levels. The training set includes two subsets: 


170 WSIs (100 normal, 70 with metastases) from Radboud University Medical Center
100 WSIs (60 normal, 40 with metastases) from University Medical Center Utrecht

The test set consists of 130 WSIs from both institutions. Ground truth data for metastases is provided as XML files with annotated contours and WSI binary masks.

The Camelyon16 dataset aims to reduce the workload and subjectivity in cancer diagnosis by pathologists. It serves as a benchmark for evaluating algorithms that can automatically detect metastases in histopathological images, focusing on breast cancer in sentinel lymph nodes.

Researchers can develop and refine machine learning models for automated detection of metastases. The dataset allows for performance comparisons of different detection algorithms. Automated systems can be integrated into clinical workflows to enhance diagnostic accuracy and efficiency. The dataset is valuable for training medical professionals in digital pathology and AI applications in diagnostics.",,,,,,
423,CamlessVideosFromTheWild,Unsupervised Monocular Depth Estimation,Unsupervised Monocular Depth Estimation,"Unsupervised Monocular Depth Estimation, Depth And Camera Motion","3D, Video",,Methodology,,Custom,https://www.pexels.com/collections/internet_videos_train-xb9y9rz/,https://paperswithcode.com/dataset/camlessvideosfromthewild,"57 stock videos from Pexels, predominantly covering road scenes which involve minimal distortion.

They involve different camera setups, also with varying camera heights, obstacles present throughout some videos (for e.g. car hood), highly varying image resolutions, and even weather conditions (day, rain, snow, night etc.).

For more details, check the paper 
CamLessMonoDepth: Monocular Depth Estimation with Unknown Camera Parameters
 https://arxiv.org/pdf/2110.14347v1.pdf",,,,,,
424,Campus___Shelf,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, 3D Multi-Person Pose Estimation","3D, Image",,Computer Vision,"3d-multi-person-pose-estimation-on-shelf, 3d-multi-person-pose-estimation-on-campus",,https://campar.in.tum.de/Chair/MultiHumanPose,https://paperswithcode.com/dataset/campus-shelf,"The Campus and Shelf datasets were presented in the paper 3D Pictorial Structures for Multiple Human Pose Estimation.
The first dataset shows persons walking and talking in front of a building, and the second up to four persons assembling a shelf.",,,,,,
425,CamVid,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Video Semantic Segmentation, Real-Time Semantic Segmentation, 2D Semantic Segmentation","Image, Video",,Computer Vision,"video-semantic-segmentation-on-camvid, semantic-segmentation-on-camvid, real-time-semantic-segmentation-on-camvid, 2d-semantic-segmentation-on-camvid",,http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/,https://paperswithcode.com/dataset/camvid,"CamVid (Cambridge-driving Labeled Video Database) is a road/driving scene understanding database which was originally captured as five video sequences with a 960×720 resolution camera mounted on the dashboard of a car. Those sequences were sampled (four of them at 1 fps and one at 15 fps) adding up to 701 frames. Those stills were manually annotated with 32 classes: void, building, wall, tree, vegetation, fence, sidewalk, parking block, column/pole, traffic cone, bridge, sign, miscellaneous text, traffic light, sky, tunnel, archway, road, road shoulder, lane markings (driving), lane markings (non-driving), animal, pedestrian, child, cart luggage, bicyclist, motorcycle, car, SUV/pickup/truck, truck/bus, train, and other moving object",,A Review on Deep Learning TechniquesApplied to Semantic Segmentation,https://arxiv.org/abs/1704.06857,,,32
426,CANARD,Conversational Question Answering,Conversational Question Answering,"Conversational Question Answering, Dialogue Rewriting, Question Answering, Question Rewriting",Text,English,Natural Language Processing,dialogue-rewriting-on-canard,CC BY-SA 4.0,https://sites.google.com/view/qanta/projects/canard,https://paperswithcode.com/dataset/canard,"CANARD is a dataset for question-in-context rewriting that consists of questions each given in a dialog context together with a context-independent rewriting of the question. The context of each question is the dialog utterences that precede the question. CANARD can be used to evaluate question rewriting models that handle important linguistic phenomena such as coreference and ellipsis resolution.

CANARD is based on QuAC (Choi et al., 2018)---a conversational reading comprehension dataset in which answers are selected spans from a given section in a Wikipedia article. Some questions in QuAC are unanswerable with their given sections. We use the answer 'I don't know.' for such questions.

CANARD is constructed by crowdsourcing question rewritings using Amazon Mechanical Turk. We apply several automatic and manual quality controls to ensure the quality of the data collection process. The dataset consists of 40,527 questions with different context lengths. More details are available in our EMNLP 2019 paper. An example is provided below. The dataset is distributed under the CC BY-SA 4.0 license.",2018,,,,,
427,Candombe,Beat Tracking,Beat Tracking,"Beat Tracking, Downbeat Tracking","Image, Video",,Computer Vision,"downbeat-tracking-on-candombe, beat-tracking-on-candombe",,https://www.eumus.edu.uy/candombe/datasets/ISMIR2015/,https://paperswithcode.com/dataset/candombe,35 recordings of Candombe music with beat and downbeat annotations.,,,,,,
428,CANDOR_Corpus,Prosody Prediction,Prosody Prediction,"Prosody Prediction, English Conversational Speech Recognition, Facial Action Unit Detection, Natural Language Understanding, Video Emotion Recognition, Emotion Recognition in Conversation, Natural Language Inference, Recognizing Emotion Cause in Conversations, Facial Emotion Recognition","Audio, Image, Text, Time Series, Video",English,Computer Vision,,see registration for details,https://betterup-data-requests.herokuapp.com/,https://paperswithcode.com/dataset/candor-corpus-1,"The CANDOR corpus is a large, novel, multimodal corpus of 1,656 recorded conversations in spoken English. This 7+ million word, 850 hour corpus totals over 1TB of audio, video, and transcripts, with moment-to-moment measures of vocal, facial, and semantic expression, along with an extensive survey of speaker post conversation reflections.",,,,,,
429,CANNOT,Text Generation,Text Generation,"Text Generation, Embeddings Evaluation",Text,English,Natural Language Processing,,CC BY-SA 4.0,https://github.com/dmlls/cannot-dataset,https://paperswithcode.com/dataset/cannot,"Dataset Summary
CANNOT is a dataset that focuses on negated textual pairs. It currently
contains 77,376 samples, of which roughly of them are negated pairs of
sentences, and the other half are not (they are paraphrased versions of each
other).

The most frequent negation that appears in the dataset is verbal negation (e.g.,
will → won't), although it also contains pairs with antonyms (cold → hot).

<br>

Languages
CANNOT includes exclusively texts in English.

<br>

Dataset Structure
The dataset is given as a
.tsv file with the
following structure:

| premise     | hypothesis                                         | label |
|:------------|:---------------------------------------------------|:-----:|
| A sentence. | An equivalent, non-negated sentence (paraphrased). | 0     |
| A sentence. | The sentence negated.                              | 1     |

The dataset can be easily loaded into a Pandas DataFrame by running:

```Python
import pandas as pd

dataset = pd.read_csv('negation_dataset_v1.0.tsv', sep='\t')

```

<br>

Dataset Creation
The dataset has been created by cleaning up and merging the following datasets:



Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal
    Negation (see
datasets/nan-nli).



GLUE Diagnostic Dataset (see
datasets/glue-diagnostic).



Automated Fact-Checking of Claims from Wikipedia (see
datasets/wikifactcheck-english).



From Group to Individual Labels Using Deep Features (see
datasets/sentiment-labelled-sentences).
In this case, the negated sentences were obtained by using the Python module
negate.



It Is Not Easy To Detect Paraphrases: Analysing Semantic Similarity With
Antonyms and Negation Using the New SemAntoNeg Benchmark (see
datasets/antonym-substitution).



Once processed, the number of remaining samples in each of the datasets above are:

| Dataset                                                                   | Samples    |
|:--------------------------------------------------------------------------|-----------:|
| Not another Negation Benchmark                                            |      118   |
| GLUE Diagnostic Dataset                                                   |      154   |
| Automated Fact-Checking of Claims from Wikipedia                          |   14,970   |
| From Group to Individual Labels Using Deep Features                       |    2,110   |
| It Is Not Easy To Detect Paraphrases                                      |    8,597   |
| <div align=""right""><b>Total</b></div>                                     | 25,949 |

Additionally, for each of the negated samples, another pair of non-negated
sentences has been added by paraphrasing them with the pre-trained model
🤗tuner007/pegasus_paraphrase.

Finally, the swapped version of each pair (premise ⇋ hypothesis) has also been
included, and any duplicates have been removed.

With this, the number of premises/hypothesis in the CANNOT dataset that appear
in the original datasets are:

| <div align=""left""><b>Dataset</b></div>                                                                   | <div align=""center""><b>Sentences</b></div>             |
|:--------------------------------------------------------------------------|----------------------:|
| Not another Negation Benchmark                                            |         552 &nbsp;&nbsp;&nbsp; (0.36 %) |
| GLUE Diagnostic Dataset                                                   |         586 &nbsp;&nbsp;&nbsp; (0.38 %) |
| Automated Fact-Checking of Claims from Wikipedia                          |      89,728 &nbsp; (59.98 %) |
| From Group to Individual Labels Using Deep Features                       |      12,626 &nbsp;&nbsp;&nbsp; (8.16 %) |
| It Is Not Easy To Detect Paraphrases                                      |      17,198 &nbsp; (11.11 %) |
| <div align=""right""><b>Total</b></div>                                     | 120,690 &nbsp; (77.99 %) |

The percentages above are in relation to the total number of premises and
hypothesis in the CANNOT dataset. The remaining 22.01 % (34,062 sentences) are
the novel premises/hypothesis added through paraphrase and rule-based negation.

<br>

Additional Information
<br>

Licensing Information
The CANNOT dataset is released under CC BY-SA
4.0.

<a rel=""license"" href=""http://creativecommons.org/licenses/by-sa/4.0/"">
    <img alt=""Creative Commons License"" width=""100px"" src=""https://i.creativecommons.org/l/by-sa/4.0/88x31.png""/>
</a>

<br>

Citation
Please cite our INLG 2023 paper, if you use our dataset. 
BibTeX:
bibtex
@misc{anschütz2023correct,
      title={This is not correct! Negation-aware Evaluation of Language Generation Systems}, 
      author={Miriam Anschütz and Diego Miguel Lozano and Georg Groh},
      year={2023},
      eprint={2307.13989},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

<br>

Contributions
Contributions to the dataset can be submitted through the project
repository.",2023,INLG 2023 paper,https://arxiv.org/abs/2307.13989,376 samples,,
430,Can_you_predict_product_backorder_,imbalanced classification,imbalanced classification,"imbalanced classification, Binary Classification",Image,,Computer Vision,,CC BY 4.0,https://www.kaggle.com/datasets/gowthammiryala/back-order-prediction-dataset,https://paperswithcode.com/dataset/can-you-predict-product-backorder,"Problem Statement

Material backorder is a common problem in a supply chain system, impacting an inventory system's service level and effectiveness. Identifying parts with the highest chances of shortage prior to their occurrence can present a high opportunity to improve an overall company’s performance. In this project, we will train classifiers to predict future back-ordered products and generate predictions for a test set.

File descriptions

Here we have two CSV files (Training_BOP.csv and Testing_BOP.csv)

Training_BOP.csv - the training set

Testing_BOP.csv - the testing set

Each file has 23 columns; the last column (went_on_backorder) is the target column.

Data fields

sku - sku code

national_inv - Current inventory level of component

lead_time - Transit time

in_transit_qty - Quantity in transit

forecast_x_month - Forecast sales for the net 3, 6, and 9 months

sales_x_month - Sales quantity for the prior 1, 3, 6, and 9 months

min_bank - Minimum recommended amount in stock

potential_issue - Indicator variable noting a potential issue with the item

pieces_past_due - Parts overdue from the source

perf_x_months_avg - Source performance in the last 6 and 12 months

local_bo_qty - Amount of stock orders overdue

x17-x22 - General Risk Flags

went_on_back_order - Product went on backorder

Validation - indicator variable for training (0), validation (1), and test set (2)",,,,,,
431,CAPE,3D Human Reconstruction,3D Human Reconstruction,3D Human Reconstruction,"3D, Image",,Computer Vision,3d-human-reconstruction-on-cape,Custom,https://cape.is.tue.mpg.de/dataset.html,https://paperswithcode.com/dataset/cape,"The CAPE dataset is a 3D dynamic dataset of clothed humans, featuring:


3D mesh registrations of accurate scans of clothed people in motion, captured at 60 FPS;
Consistent SMPL mesh topology, all frames in correspondence;
Precise, captured minimally clothed body shape under clothing;
Clothed bodies of large pose variations;
Both posed and unposed (i.e. in canonical pose) clothed body for each frame;
SMPL body pose parameters for each frame;
(New!) High-quality raw scan data of several subjects and sequences along with texture is available. Please first register as a user and send us your request.",,,,,,
432,CapMIT1003,Saliency Prediction,Saliency Prediction,"Saliency Prediction, Scanpath prediction",Time Series,,Methodology,scanpath-prediction-on-capmit1003,,https://github.com/mad-lab-fau/CapMIT1003,https://paperswithcode.com/dataset/capmit1003,"The CapMIT1003 database contains captions and clicks collected for images from the MIT1003 database, for which reference eye scanpath are available. The database is distributed as a single SQLite3 database named capmit1003.db. For convenience, a lightweight Python class to access the database is provided in the official repository",,,,,,
433,CaRB,Open Information Extraction,Open Information Extraction,Open Information Extraction,,,Methodology,open-information-extraction-on-carb,,https://github.com/dair-iitd/CaRB,https://paperswithcode.com/dataset/carb,"CaRB [Bhardwaj et al., 2019] is developed by re-annotating the dev and test splits of OIE2016 via crowd-sourcing. Besides improving annotation quality, CaRB also provides a new matching scorer. CaRB scorer uses token level match and it matches relation with relation, arguments with arguments.

Source: https://arxiv.org/pdf/2205.11725.pdf (section 3.1)",2019,,,,,
434,CARD-660,Word Embeddings,Word Embeddings,"Word Embeddings, Graph Embedding, Text Classification","Graph, Image, Text",English,Computer Vision,,,https://pilehvar.github.io/card-660/,https://paperswithcode.com/dataset/card-660,"An expert-annotated word similarity dataset which provides a highly reliable, yet challenging, benchmark for rare word representation techniques.",,,,,,
435,Cards_Against_Humanity,Card Games,Card Games,"Card Games, Sentiment Analysis, Dark Humor Detection, Emotional Intelligence, Humor Detection, Sentence-Embedding, text-based games, Text Reranking, Abusive Language, Board Games, Sarcasm Detection, Classification, Sentence Classification, Text Classification","Image, Text",English,Computer Vision,,,https://lab.cardsagainsthumanity.com/,https://paperswithcode.com/dataset/cards-against-humanity,"A dataset of games played in the card game ""Cards Against Humanity"" (CAH), by human players, derived from the online CAH labs. 
Each round includes the cards presented to users - a ""black"" prompt with a blank or question and 10 ""white"" punchlines as possible responses, and which punchline was picked by a player each round, along with text and metadata.

An example prompt is “TSA guidelines now prohibit ___ on airplanes”. Candidate punchlines are “Goblins”, “BATMAN!!!”, “Poor people”, and “The
right amount of cocaine”. Importantly, many cards are offensive or politically incorrect.

Used to explore human humor preferences.

Available upon request from CAH labs: mail@cardsagainsthumanity.com
Train/test splits and data processing available in the paper/code: ""Cards Against AI: Predicting Humor in a Fill-in-the-blank Party Game"": https://github.com/ddofer/CAH",,,,,,
436,CARE,Tumor Segmentation,Tumor Segmentation,Tumor Segmentation,Image,,Computer Vision,,,https://drive.google.com/file/d/1X_JTfD8Ch-IxmG5VHtKk_xGZT336Fl1Q/view?usp=drive_link,https://paperswithcode.com/dataset/care,https://drive.google.com/file/d/1X_JTfD8Ch-IxmG5VHtKk_xGZT336Fl1Q/view?usp=drive_link,,,,,,
437,CARER,Multi Class Text Classification,Multi Class Text Classification,"Multi Class Text Classification, Emotion Recognition, Semantic Textual Similarity, Text Classification","Image, Text",English,Computer Vision,"text-classification-on-emotion, multi-class-text-classification-on-emotion",,https://github.com/dair-ai/emotion_dataset,https://paperswithcode.com/dataset/emotion,"CARER is an emotion dataset collected through noisy labels, annotated via distant supervision as in (Go et al., 2009). 

The subset of data provided here corresponds to the six emotions variant described in the paper. The six emotions are anger, fear, joy, love, sadness, and surprise.",2009,,,,,
438,CARLA,CARLA MAP Leaderboard,CARLA MAP Leaderboard,"CARLA MAP Leaderboard, CARLA longest6, Autonomous Vehicles, Imitation Learning, CARLA Leaderboard 2.0, Autonomous Driving",,,Methodology,"autonomous-driving-on-carla-leaderboard, carla-map-leaderboard-on-carla, carla-longest6-on-carla, carla-leaderboard-2-0-on-carla",CC-BY / MIT,https://carla.org/,https://paperswithcode.com/dataset/carla,"CARLA (CAR Learning to Act) is an open simulator for urban driving, developed as an open-source layer over Unreal Engine 4. Technically, it operates similarly to, as an open source layer over Unreal Engine 4 that provides sensors in the form of RGB cameras (with customizable positions), ground truth depth maps, ground truth semantic segmentation maps with 12 semantic classes designed for driving (road, lane marking, traffic sign, sidewalk and so on), bounding boxes for dynamic objects in the environment, and measurements of the agent itself (vehicle location and orientation).",,Synthetic Data for Deep Learning,https://arxiv.org/abs/1909.11512,,,
439,CARLANE_Benchmark,Unsupervised Pre-training,Unsupervised Pre-training,"Unsupervised Pre-training, Transfer Learning, 2D Semantic Segmentation, Lane Detection, Domain Adaptation, Self-Supervised Learning, Autonomous Driving",Image,,Computer Vision,"domain-adaptation-on-molane, domain-adaptation-on-mulane, domain-adaptation-on-tulane","Apache License, Version 2.0 (2004)",https://carlanebenchmark.github.io/,https://paperswithcode.com/dataset/carlane-benchmark,"Unsupervised Domain Adaptation demonstrates great potential to mitigate domain shifts by transferring models from labeled source domains to unlabeled target domains. While Unsupervised Domain Adaptation has been applied to a wide variety of complex vision tasks, only few works focus on lane detection for autonomous driving. This can be attributed to the lack of publicly available datasets. To facilitate research in these directions, we propose CARLANE, a 3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE encompasses the single-target datasets MoLane and TuLane and the multi-target dataset MuLane. These datasets are built from three different domains, which cover diverse scenes and contain a total of 163K unique images, 118K of which are annotated. In addition we evaluate and report systematic baselines, including our own method, which builds upon Prototypical Cross-domain Self-supervised Learning. We find that false positive and false negative rates of the evaluated domain adaptation methods are high compared to those of fully supervised baselines. This affirms the need for benchmarks such as CARLANE to further strengthen research in Unsupervised Domain Adaptation for lane detection. CARLANE, all evaluated models and the corresponding implementations are publicly available at https://carlanebenchmark.github.io.",,,,,,
440,CARS196,Image Classification,Image Classification,"Image Classification, Image Clustering, Metric Learning, Image Retrieval",Image,,Computer Vision,"image-classification-on-cars196, metric-learning-on-cars196, image-clustering-on-cars196, image-retrieval-on-cars196",,https://ai.stanford.edu/~jkrause/cars/car_dataset.html,https://paperswithcode.com/dataset/cars196,"CARS196  is composed of 16,185 car images of 196 classes.",,,,,,196
441,CAS-VSR-S101,Audio-Visual Speech Recognition,Audio-Visual Speech Recognition,"Audio-Visual Speech Recognition, Lip Reading, Speech Recognition, Speech Enhancement, Lipreading","Audio, Image, Text",English,Audio,"speech-recognition-on-cas-vsr-s101, audio-visual-speech-recognition-on-cas-vsr, lipreading-on-cas-vsr-s101",Free for academic use,https://github.com/VIPL-Audio-Visual-Speech-Understanding/CAS-VSR-S101,https://paperswithcode.com/dataset/cas-vsr-s101,"A new large-scale, in-thewild Mandarin dataset, CAS-VSR-S101 with 101.1 hours of data. The videos are sourced from broadcast news and conversational programs in Chinese, covering a highly diverse set of topics, speakers and filming conditions. The lengths of the utterances are naturally distributed between 0.01s and 10.57s, and image qualities and resolutions vary. News accounts for 82.4% of the programs. 70.4% of the utterances depict news anchors, hosts and correspondents, while 29.6% are those of interviewees and guests. In addition, at a ratio of approximately 1.5 : 1, male and female appearances are relatively balanced. It is divided into train, validation and test sets by TV channels to minimize speaker
overlap, and at a ratio of roughly 8 : 1 : 1.5 in terms of duration. The validation and test sets are composed of programs broadcast on provincial TV channels. 
The dataset is available for academic use under a license.",,,,,,
442,CAS-VSR-W1k__LRW-1000_,Lip Reading,Lip Reading,"Lip Reading, Lipreading, Visual Speech Recognition, Audio-Visual Speech Recognition","Audio, Image, Text",English,Computer Vision,lipreading-on-lrw-1000,"research-only, non-commercial",https://vipl.ict.ac.cn/en/view_database.php?id=13,https://paperswithcode.com/dataset/lrw-1000,"LRW-1000 has been renamed as CAS-VSR-W1k.* It is a naturally-distributed large-scale benchmark for word-level lipreading in the wild, including 1000 classes with about 718,018 video samples from more than 2000 individual speakers. There are more than 1,000,000 Chinese character instances in total. Each class corresponds to the syllables of a Mandarin word which is composed by one or several Chinese characters. This dataset aims to cover a natural variability over different speech modes and imaging conditions to incorporate challenges encountered in practical applications.",2000,https://arxiv.org/pdf/1810.06990v6.pdf,https://arxiv.org/pdf/1810.06990v6.pdf,,,1000
443,Caselaw4,legal outcome extraction,legal outcome extraction,"legal outcome extraction, Text Classification, Attribute Value Extraction","Image, Text",English,Computer Vision,,CC-BY-SA,https://github.com/chinmusique/outcome-prediction,https://paperswithcode.com/dataset/caselaw4,"Caselaw4 is a dataset of 350k common law judicial decisions from the U.S. Caselaw Access Project, of which 250k have been automatically annotated with binary outcome labels of AFFIRM and REVERSE.

The court case reports used in the dataset are from New Mexico, North Carolina, Illinois, and Arkansas Courts of Appeal. These Courts hear appeals exclusively from lower courts within their respective states, on matters of domestic state law, and the data for these jurisdictions are freely available.
Since each case in Caselaw4 appeals some lower court ruling, the possible outcomes of each case are as follows:



the previous ruling is kept as is (AFFIRM);



the previous ruling is changed/annulled (REVERSE);



some parts of the previous ruling are kept and some are changed (MIXED); 



the appeal is dismissed (a type of AFFIRM).



The data in Caselaw4 are stored in JSON format. In addition to the original metadata about the case name, date, court, judges, cases cited etc., we (a) automatically annotated a subset of 250k cases with the AFFIRM or REVERSE outcome label (with weighted average precision of 95.45%), and (b) manually annotated 500 cases from the New Mexico Court of Appeals with the AFFIRM, REVERSE, or MIXED outcome label as well as with the outcome sentences.",,,,,,
444,CASIA-CXR,Medical Report Generation,Medical Report Generation,Medical Report Generation,Text,English,Medical,,,https://www.casia-cxr.net/,https://paperswithcode.com/dataset/an-open-chest-x-ray-dataset-with-benchmarks,"Medical report generation (MRG), which aims to automatically generate a textual description of a specific medical image (e.g., a chest X-ray), has recently received increasing research interest. Building on the success of image captioning, MRG has become achievable. However, generating language-specific radiology reports poses a challenge for data-driven models due to their reliance on paired image-report chest X-ray datasets, which are labor-intensive, time-consuming, and costly. In this paper, we introduce a chest X-ray benchmark dataset, namely CASIA-CXR, consisting of high-resolution chest radiographs accompanied by narrative reports originally written in French. To the best of our knowledge, this is the first public chest radiograph dataset with medical reports in this particular language. Importantly, we propose a simple yet effective multimodal encoder-decoder contextually-guided framework for medical report generation in French. We validated our framework through intra-language and cross-language contextual analysis, supplemented by expert evaluation performed by radiologists. The dataset is freely available at: https://www.casia-cxr.net/.",,,,,,
445,CASIA-FASD,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Face Anti-Spoofing, Face Recognition",Image,,Computer Vision,,,https://pypi.org/project/bob.db.casia-fasd/,https://paperswithcode.com/dataset/casia-fasd,CASIA-FASD is a small face anti-spoofing dataset  containing 50 subjects.,,Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing,https://arxiv.org/abs/1901.05602,,,
446,CASIA-HWDB,Language Modelling,Language Modelling,"Language Modelling, Handwritten Chinese Text Recognition, Offline Handwritten Chinese Character Recognition","Image, Text",English,Computer Vision,,"Custom (research-only, non-commercial)",http://www.nlpr.ia.ac.cn/databases/handwriting/Touching_Characters_Databases.html,https://paperswithcode.com/dataset/casia-hwdb,"CASIA-HWDB is a dataset for handwritten Chinese character recognition. It contains 300 files (240 in HWDB1.1 training set and 60 in HWDB1.1 test set). Each file contains about 3000 isolated gray-scale Chinese character images written by one writer, as well as their corresponding labels.",,Generating Handwritten Chinese Characters using CycleGAN,https://arxiv.org/abs/1801.08624,,training set and 60 in HWDB1.1 test set). Each file contains about 3000 isolated gray-scale Chinese character images,
447,CASIA-Iris-Complex,Iris Segmentation,Iris Segmentation,"Iris Segmentation, Iris Recognition",Image,,Computer Vision,,Custom,http://www.cripacsir.cn/dataset/casia-iris-complex/,https://paperswithcode.com/dataset/casia-iris-complex,"Introduction
Iris is considered one of the most accurate and reliable biometric modality. Iris is more stable and distinctive compared with fingerprint, face, voice, etc, and difficult to be replicated for spoof attacks. Although an iris pattern is naturally an ideal identifier, the development of a high-performance iris recognition algorithm and transferring it from laboratory to field application is still a challenging task. In practical applications, the iris recognition system must face various unpredictable iris image degraded. For example, recognition of low-quality iris images, non-cooperative iris images, long-range iris images, and moving iris images are all huge problems in iris recognition. We believe that the first step in solving these problems is to design and develop a database of iris images that includes all of these degraded.

Brief Descriptions and Statistics of the Database
CASIA-Iris-Complex contains 22,932 images from 292 Asian subjects. It includes two subsets: CASIA-Iris-CX1 and CASIA-Iris-CX2. All images were collected under NIR illumination and two eyes were captured simultaneously.",,Custom,http://www.cripacsir.cn/dataset/casia-iris-complex/license_agreement.pdf,932 images,,
448,CASIA-MFSD,Face Anti-Spoofing,Face Anti-Spoofing,Face Anti-Spoofing,Image,,Computer Vision,face-anti-spoofing-on-casia-mfsd,,http://biometrics.idealtest.org/findTotalDbByMode.do?mode=Face,https://paperswithcode.com/dataset/casia-mfsd,"CASIA-MFSD is a dataset for face anti-spoofing. It contains 50 subjects, and 12 videos for each subject under different resolutions and light conditions. Three different spoof attacks are designed: replay, warp print and cut print attacks. The database contains 600 video recordings, in which 240 videos of 20 subjects are used for training and 360 videos of 30 subjects for testing.",,Improving Face Anti-Spoofing by 3D Virtual Synthesis,https://arxiv.org/abs/1901.00488,,,
449,CASIA-SURF,Face Anti-Spoofing,Face Anti-Spoofing,"Face Anti-Spoofing, Face Presentation Attack Detection, Face Recognition",Image,,Computer Vision,,,https://sites.google.com/qq.com/face-anti-spoofing/welcome/challengecvpr2019?authuser=0,https://paperswithcode.com/dataset/casia-surf,"Dataset for face anti-spoofing in terms of both subjects and modalities. Specifically, it consists of  subjects with  videos and each sample has  modalities (i.e., RGB, Depth and IR).",,,,,,
450,CASIA-WebFace,Image Super-Resolution,Image Super-Resolution,"Image Super-Resolution, Face Recognition, Metric Learning, Facial Inpainting, Face Verification",Image,,Computer Vision,"facial-inpainting-on-webface, image-super-resolution-on-webface-8x","Custom (research-only, non-commercial)",,https://paperswithcode.com/dataset/casia-webface,"The CASIA-WebFace dataset is used for face verification and face identification tasks. The dataset contains 494,414 face images of 10,575 real identities collected from the web.",,On Hallucinating Context and Background Pixels from a Face Mask using Multi-scale GANs,https://arxiv.org/abs/1811.07104,,,
451,Casia_V1_,Image Manipulation Localization,Image Manipulation Localization,"Image Manipulation Localization, Image Manipulation Detection",Image,,Computer Vision,"image-manipulation-localization-on-casia-v1, image-manipulation-detection-on-casia-v1",,,https://paperswithcode.com/dataset/casia-v1,Casia V1 is a dataset for forgery classification. Casia V1+ is a modification of the Casia V1 dataset proposed by Chen et al. that replaces authentic images that also exist in Casiav2 with images from the COREL dataset to avoid data contamination.,,,,,,
452,CASIA_V2,Image Manipulation,Image Manipulation,"Image Manipulation, Image Inpainting, Domain Adaptation",Image,,Computer Vision,,,https://github.com/namtpham/casia2groundtruth,https://paperswithcode.com/dataset/casia-v2,"CASIA V2 is a dataset for forgery classification. It contains 4795 images, 1701 authentic and 3274 forged.",,Copy-Move Forgery Classification via Unsupervised Domain Adaptation,https://arxiv.org/abs/1911.07932,4795 images,,
453,CASIA__OSN-transmitted_-_Facebook_,Image Manipulation,Image Manipulation,"Image Manipulation, Image Forensics, Image Manipulation Detection",Image,,Computer Vision,image-manipulation-detection-on-casia-osn,,https://github.com/HighwayWu/ImageForensicsOSN,https://paperswithcode.com/dataset/casia-osn-transmitted,"This dataset is an OSN-transmitted (OSN = Online Social Network) version of the CASIA dataset. 
The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN  - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing 
and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against
transmission over online social networks. IEEE Transactions on Information Forensics and Security.",2022,,,,,
454,CASIA__OSN-transmitted_-_Weibo_,Image Manipulation,Image Manipulation,"Image Manipulation, Image Manipulation Localization, Detecting Image Manipulation, Image Forgery Detection, Image Manipulation Detection",Image,,Computer Vision,image-manipulation-detection-on-casia-osn-3,,https://github.com/HighwayWu/ImageForensicsOSN,https://paperswithcode.com/dataset/casia-osn-transmitted-weibo,"This dataset is an OSN-transmitted (OSN = Online Social Network) version of the CASIA dataset. 
The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN  - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing 
and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against
transmission over online social networks. IEEE Transactions on Information Forensics and Security.",2022,,,,,
455,CASIA__OSN-transmitted_-_Whatsapp_,Image Manipulation Localization,Image Manipulation Localization,"Image Manipulation Localization, Image Manipulation, Image Forensics, Image Manipulation Detection",Image,,Computer Vision,image-manipulation-detection-on-casia-osn-2,,https://github.com/HighwayWu/ImageForensicsOSN,https://paperswithcode.com/dataset/casia-osn-transmitted-whatsapp,"This dataset is an OSN-transmitted (OSN = Online Social Network) version of the CASIA dataset. 
The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN  - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing 
and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against
transmission over online social networks. IEEE Transactions on Information Forensics and Security.",2022,,,,,
456,Casino_Reviews,Aspect Extraction,Aspect Extraction,"Aspect Extraction, Aspect Category Detection, Aspect Category Polarity, Aspect-oriented  Opinion Extraction, Aspect Term Extraction and Sentiment Classification","Image, Text",English,Computer Vision,,CC BY-NC-ND,https://github.com/MohammadForouhesh/latent-aspect-detection,https://paperswithcode.com/dataset/casino-reviews,"This dataset contain online reviews gathered from google reviews written by  north american casino users.
explain motivations and summary of its content. 
Can be used to study user experience and relative research directions such as cultural impacts on latency of aspects, domain importance, sentiment analysis, opinion mining, aspect-based sentiment analysis, etc.",,,,,,
457,CAsT-snippets,Conversational Response Generation,Conversational Response Generation,Conversational Response Generation,Text,English,Natural Language Processing,,,https://github.com/iai-group/CAsT-snippets,https://paperswithcode.com/dataset/cast-snippets,CAsT-snippets is a high-quality dataset for conversational information seeking containing snippet-level annotations for all queries in the TREC CAsT 2020 and 2022 datasets. It enables the development of answer generation methods that are grounded in relevant snippets in paragraphs as well as allows for the automatic evaluation of the generated answers in terms of completeness; a training/test split is provided for such use.,2020,,,,,
458,CAT2000,Saliency Prediction,Saliency Prediction,"Saliency Prediction, Saliency Detection","Image, Time Series",,Computer Vision,"saliency-prediction-on-cat2000, saliency-detection-on-cat2000",,http://saliency.mit.edu/results_cat2000.html,https://paperswithcode.com/dataset/cat2000,"Includes 4000 images; 200 from each of 20 categories covering different types of scenes such as Cartoons, Art, Objects, Low resolution images, Indoor, Outdoor, Jumbled, Random, and Line drawings.",,CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research,https://arxiv.org/pdf/1505.03581v1.pdf,4000 images,,20
459,Catalan_TimeBank_1.0,Temporal Relation Extraction,Temporal Relation Extraction,"Temporal Relation Extraction, Temporal Tagging, Temporal Relation Classification, Event Extraction","Graph, Image, Time Series, Video",,Computer Vision,temporal-tagging-on-catalan-timebank-1-0,LDC User Agreement for Non-Members,https://catalog.ldc.upenn.edu/LDC2012T10,https://paperswithcode.com/dataset/catalan-timebank-1-0,Catalan TimeBank 1.0 was developed by researchers at Barcelona Media and consists of Catalan texts in the AnCora corpus annotated with temporal and event information according to the TimeML specification language.,,LDC User Agreement for Non-Members,https://catalog.ldc.upenn.edu/license/ldc-non-members-agreement.pdf,,,
460,CATER,Video Object Tracking,Video Object Tracking,"Video Object Tracking, Visual Reasoning, Composite action recognition, Atomic action recognition, Action Recognition, Self-Supervised Learning","Image, Video",,Computer Vision,"atomic-action-recognition-on-cater, video-object-tracking-on-cater, composite-action-recognition-on-cater",Apache-2.0,https://rohitgirdhar.github.io/CATER/,https://paperswithcode.com/dataset/cater,"Rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning.",,,,,,
461,CATS,Stereo Matching Hand,Stereo Matching Hand,"Stereo Matching Hand, Stereo Matching, Anomaly Detection, Multimodal Unsupervised Image-To-Image Translation, Super-Resolution","3D, Image, Text",English,Computer Vision,"multimodal-unsupervised-image-to-image, anomaly-detection-on-cats-and-dogs",,https://bigdatavision.org/CATS,https://paperswithcode.com/dataset/cats,"A dataset consisting of stereo thermal, stereo color, and cross-modality image pairs with high accuracy ground truth (< 2mm) generated from a LiDAR. The authors scanned 100 cluttered indoor and 80 outdoor scenes featuring challenging environments and conditions. CATS contains approximately 1400 images of pedestrians, vehicles, electronics, and other thermally interesting objects in different environmental conditions, including nighttime, daytime, and foggy scenes.",,,,1400 images,,
462,CATT,Arabic Text Diacritization,Arabic Text Diacritization,Arabic Text Diacritization,Text,English,Natural Language Processing,arabic-text-diacritization-on-catt-dataset,CC BY-NC-SA,https://github.com/abjadai/catt,https://paperswithcode.com/dataset/catt-dataset,"The CATT benchmark dataset comprises 742 sentences, which were scraped from an internet news source in 2023.
It covers multiple topics including science and technology, economics, politics, sports, arts, and culture.
It was manually diacritized by two expert native Arabic speakers and then validated by a third expert.
This dataset contains names of people and places in both Arabic and English.
As for the English names, they are written in Arabic letters and diacritized based on their pronunciation.
Also, the numbers in the sentences are written in textual form rather than the numeric form which helps in evaluating the models without the need for a text normalizer (TN).",2023,,,742 sentences,,
463,Cattan2019-VR_MOABB,Within-Session ERP,Within-Session ERP,Within-Session ERP,,,Methodology,within-session-erp-on-cattan2019-vr-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Cattan2019_VR.html,https://paperswithcode.com/dataset/cattan2019-vr-moabb-1,,,,,,,
464,CAT__Context_Adjustment_Training,Co-Salient Object Detection,Co-Salient Object Detection,Co-Salient Object Detection,Image,,Computer Vision,,MIT,http://ldkong.com/data/sets/cat/home.html,https://paperswithcode.com/dataset/cat-context-adjustment-training,"CAT is a specialized dataset for co-saliency detection. This dataset is intended for both helping to assess the performance of vision algorithms and supporting research that aims to exploit large volumes of annotated data, e.g., for training deep neural networks.

Scale & Features
- A total number of 33500 image samples.
- 280 semantic groups affiliated to 15 superclasses.
- High-quality mask annotations.
- Diverse visual context with multiple foreground objects.",,,,,,
465,Causal3DIdent,Image Classification,Image Classification,"Image Classification, Disentanglement",Image,,Computer Vision,image-classification-on-causal3dident,Creative Commons Attribution 4.0 International,https://zenodo.org/record/4784282#.YgWo0PXMKbg,https://paperswithcode.com/dataset/causal3dident,"Update on 3DIdent, where we introduce six additional object classes (Hare, Dragon, Cow, Armadillo, Horse, and Head), and impose a causal graph over the latent variables. For further details, see Appendix B in the associated paper (https://arxiv.org/abs/2106.04619).",,,,,,
466,CausalBench,Drug Discovery,Drug Discovery,"Drug Discovery, Causal Discovery",,,Methodology,,Apache-2.0 license,https://github.com/causalbench/causalbench,https://paperswithcode.com/dataset/causalbench,"CausalBench is a comprehensive benchmark suite for evaluating network inference methods on large-scale perturbational single-cell gene expression data. CausalBench introduces several biologically meaningful performance metrics and operates on two large, curated and openly available benchmark data sets for evaluating methods on the inference of gene regulatory networks from single-cell data generated under perturbations. The datasets consists of over 200000 training samples under interventions.",,,,,"valuating network inference methods on large-scale perturbational single-cell gene expression data. CausalBench introduces several biologically meaningful performance metrics and operates on two large, curated and openly available benchmark data sets for evaluating methods on the inference of gene regulatory networks from single-cell data generated under perturbations. The datasets consists of over 200000 training samples",
467,CausalChaos_,Commonsense Causal Reasoning,Commonsense Causal Reasoning,"Commonsense Causal Reasoning, Few-shot Video Question Answering, Zeroshot Video Question Answer, Causal Discovery, Video Question Answering, Grounded Video Question Answering, Visual Question Answering (VQA), Causal Discovery in Video Reasoning","Image, Text, Video",English,Reasoning,,,https://github.com/LUNAProject22/CausalChaos,https://paperswithcode.com/dataset/causalchaos,"CausalChaos! is a dataset for causal video question answering. It is based on Tom and Jerry cartoons. It features longer causal chains embedded in dynamic visual scenes. It also features challenging incorrect options, especially, Causal Confusion set which contains causally confounding incorrect options. All these factors prove to be challenging for current VLMs and other traditional Video Question Answering models.",,,,,,
468,CausalGym,Interpretability Techniques for Deep Learning,Interpretability Techniques for Deep Learning,Interpretability Techniques for Deep Learning,,,Methodology,interpretability-techniques-for-deep-learning,MIT,https://github.com/aryamanarora/causalgym,https://paperswithcode.com/dataset/causalgym,"SyntaxGym, adapted for interventional interpretability.",,,,,,
469,Causal_Triplet,Representation Learning,Representation Learning,Representation Learning,,,Methodology,,Apache-2.0 license,https://github.com/CausalTriplet/causaltriplet,https://paperswithcode.com/dataset/causal-triplet,"Causal Triplet is a causal representation learning benchmark featuring not only visually more complex scenes, but also two crucial desiderata commonly overlooked in previous works: 

1) An actionable counterfactual setting, where only certain object-level variables allow forcounterfactual observations whereas others do not. 

2) An interventional downstream task with an emphasis on out-of-distribution robustness from the independent causal mechanisms principle.",,Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning,https://arxiv.org/pdf/2301.05169v1.pdf,,,
470,CAVE,Spectral Reconstruction,Spectral Reconstruction,Spectral Reconstruction,3D,,Methodology,spectral-reconstruction-on-cave,,https://github.com/caiyuanhao1998/MST,https://paperswithcode.com/dataset/cave,Multispectral imaging using multiplexed illumination.,,,,,,
471,CBC,Object Detection,Object Detection,"Object Detection, Probabilistic Deep Learning, Blood Cell Count",Image,,Computer Vision,,MIT,https://github.com/MahmudulAlam/Complete-Blood-Cell-Count-Dataset,https://paperswithcode.com/dataset/complete-blood-count-cbc-dataset,"The complete blood count (CBC) dataset contains 360 blood smear images along with their annotation files splitting into Training, Testing, and Validation sets. The training folder contains 300 images with annotations. The testing and validation folder both contain 60 images with annotations. We have done some modifications over the original dataset to prepare this CBC dataset where some of the image annotation files contain very low red blood cells (RBCs) than actual and one annotation file does not include any RBC at all although the cell smear image contains RBCs. So, we clear up all the fallacious files and split the dataset into three parts. Among the 360 smear images, 300 blood cell images with annotations are used as the training set first, and then the rest of the 60 images with annotations are used as the testing set. Due to the shortage of data, a subset of the training set is used to prepare the validation set which contains 60 images with annotations.",,,,300 images,"splitting into Training, Testing, and Validation sets. The training folder contains 300 images",
472,CBCT_Walnut,Tomographic Reconstructions,Tomographic Reconstructions,"Tomographic Reconstructions, Image Reconstruction","3D, Image",,Computer Vision,,CC Attribution 4.0 International,https://doi.org/10.1038/s41597-019-0235-y,https://paperswithcode.com/dataset/cbct-walnut,"The scans are performed using a custom-built, highly flexible X-ray CT scanner, the FleX-ray scanner, developed by XRE nvand located in the FleX-ray Lab at the Centrum Wiskunde & Informatica (CWI) in Amsterdam, Netherlands. The general purpose of the FleX-ray Lab is to conduct proof of concept experiments directly accessible to researchers in the field of mathematics and computer science. The scanner consists of a cone-beam microfocus X-ray point source that projects polychromatic X-rays onto a 1536-by-1944 pixels, 14-bit flat panel detector (Dexella 1512NDT) and a rotation stage in-between, upon which a sample is mounted. All three components are mounted on translation stages which allow them to move independently from one another.

Please refer to the paper for all further technical details.

The complete data set can be found via the following links: 1-8 https://doi.org/10.5281/zenodo.2686725 , 9-16 https://doi.org/10.5281/zenodo.2686970, 17-24 https://doi.org/10.5281/zenodo.2687386, 25-32 https://doi.org/10.5281/zenodo.2687634, 33-37 https://doi.org/10.5281/zenodo.2687896, 38-42 https://doi.org/10.5281/zenodo.2688111

The corresponding Python scripts for loading, pre-processing and reconstructing the projection data in the way described in the paper can be found on github https://github.com/cicwi/WalnutReconstructionCodes",1944,,,,,
473,CBIS-DDSM,Cancer-no cancer per breast classification,Cancer-no cancer per breast classification,"Cancer-no cancer per breast classification, Breast Cancer Detection, Cancer-no cancer per image classification",Image,,Computer Vision,"cancer-no-cancer-per-image-classification-on, cancer-no-cancer-per-breast-classification-on",CC BY-SA 3.0,https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=22516629,https://paperswithcode.com/dataset/cbis-ddsm,"This CBIS-DDSM (Curated Breast Imaging Subset of DDSM) is an updated and standardized version of the  Digital Database for Screening Mammography (DDSM) .  The DDSM is a database of 2,620 scanned film mammography studies. It contains normal, benign, and malignant cases with verified pathology information. The scale of the database along with ground truth validation makes the DDSM a useful tool in the development and testing of decision support systems. The CBIS-DDSM collection includes a subset of the DDSM data selected and curated by a trained mammographer.  The images have been decompressed and converted to DICOM format.  Updated ROI segmentation and bounding boxes, and pathologic diagnosis for training data are also included.  A manuscript describing how to use this dataset in detail is available at https://www.nature.com/articles/sdata2017177.

Published research results from work in developing decision support systems in mammography are difficult to replicate due to the lack of a standard evaluation data set; most computer-aided diagnosis (CADx) and detection (CADe) algorithms for breast cancer in mammography are evaluated on private data sets or on unspecified subsets of public databases. Few well-curated public datasets have been provided for the mammography community. These include the DDSM, the Mammographic Imaging Analysis Society (MIAS) database, and the Image Retrieval in Medical Applications (IRMA) project. Although these public data sets are useful, they are limited in terms of data set size and accessibility.

For example, most researchers using the DDSM do not leverage all its images for a variety of historical reasons. When the database was released in 1997, computational resources to process hundreds or thousands of images were not widely available. Additionally, the DDSM images are saved in non-standard compression files that require the use of decompression code that has not been updated or maintained for modern computers. Finally, the ROI annotations for the abnormalities in the DDSM were provided to indicate a general position of lesions, but not a precise segmentation for them. Therefore, many researchers must implement segmentation algorithms for accurate feature extraction. This causes an inability to directly compare the performance of methods or to replicate prior results. The CBIS-DDSM collection addresses that challenge by publicly releasing an curated and standardized version of the DDSM for evaluation of future CADx and CADe systems (sometimes referred to generally as CAD) research in mammography.

Please note that the image data for this collection is structured such that each participant has multiple patient IDs.  For example, participant 00038 has 10 separate patient IDs which provide information about the scans within the IDs (e.g. Calc-Test_P_00038_LEFT_CC, Calc-Test_P_00038_RIGHT_CC_1).  This makes it appear as though there are 6,671 patients according to the DICOM metadata, but there are only 1,566 actual participants in the cohort.",1997,,,,,
474,CBSD68,Color Image Denoising,Color Image Denoising,"Color Image Denoising, Image Compressed Sensing, Image Restoration, Image Denoising, Denoising",Image,,Computer Vision,"color-image-denoising-on-cbsd68-sigma10, color-image-denoising-on-cbsd68-sigma50, color-image-denoising-on-cbsd68-sigma20, color-image-denoising-on-cbsd68-sigma70, color-image-denoising-on-cbsd68-sigma5, color-image-denoising-on-cbsd68-sigma60, color-image-denoising-on-cbsd68-sigma40, color-image-denoising-on-cbsd68-sigma15, color-image-denoising-on-cbsd68-sigma75, color-image-denoising-on-cbsd68-sigma55, image-compressed-sensing-on-cbsd68, color-image-denoising-on-cbsd68-sigma30, color-image-denoising-on-cbsd68-sigma65, color-image-denoising-on-cbsd68-sigma45, color-image-denoising-on-cbsd68-sigma35, color-image-denoising-on-cbsd68-sigma25","Custom (research-only, non-commercial)",https://github.com/clausmichele/CBSD68-dataset,https://paperswithcode.com/dataset/cbsd68,Color BSD68 dataset for image denoising benchmarks is part of The Berkeley Segmentation Dataset and Benchmark. It is used for measuring image denoising algorithms performance. It contains 68 images.,,,,68 images,,
475,CBT,Machine Reading Comprehension,Machine Reading Comprehension,"Machine Reading Comprehension, Click-Through Rate Prediction, Question Answering, Reading Comprehension","Text, Time Series",English,Natural Language Processing,"click-through-rate-prediction-on-childrens, question-answering-on-childrens-book-test",GNU Free Documentation License,https://research.fb.com/downloads/babi/,https://paperswithcode.com/dataset/cbt,Children’s Book Test (CBT) is designed to measure directly how well language models can exploit wider linguistic context. The CBT is built from books that are freely available thanks to Project Gutenberg.,,,,,,
476,CBTex,Texture Synthesis,Texture Synthesis,"Texture Synthesis, Synthetic Data Generation, Texture Classification","Image, Text",English,Computer Vision,,Creative Commons Attribution 4.0 International,https://a-nau.github.io/parcel3d/,https://paperswithcode.com/dataset/cbtex,"Dataset of >200 synthetic cardboard texture images that were rendered with DoubeGum's cardboard shader in Blender. Used to generate Parcel3D, the dataset for our paper on single image 3D reconstructions of potentially damaged parcels.",,,,,,
477,CC-19,Computed Tomography (CT),Computed Tomography (CT),"Computed Tomography (CT), Federated Learning",,,Methodology,,,https://github.com/abdkhanstd/COVID-19,https://paperswithcode.com/dataset/cc-19,"CC-19 is a small new dataset related to the latest family of coronavirus i.e. COVID-19. The proposed dataset “CC-19” contains 34,006 CT scan slices (images) belonging to 98 subjects out of which 28,395 CT scan slices belong to positive COVID patients.",,,,,"test family of coronavirus i.e. COVID-19. The proposed dataset “CC-19” contains 34,006 CT scan slices (images",
478,CC100,Language Modelling,Language Modelling,"Language Modelling, Cross-Lingual Transfer",Text,English,Natural Language Processing,,,http://data.statmt.org/cc-100/,https://paperswithcode.com/dataset/cc100,This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages. This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Each file comprises of documents separated by double-newlines and paragraphs within the same document separated by a newline. The data is generated using the open source CC-Net repository.,2018,,,,,
479,CC152K,Cross-modal retrieval with noisy correspondence,Cross-modal retrieval with noisy correspondence,Cross-modal retrieval with noisy correspondence,,,Methodology,cross-modal-retrieval-with-noisy-1,,,https://paperswithcode.com/dataset/cc152k,"CC152K is a subset of Conceptual Captions. It contains 150,000 randomly selected samples from the training split for training, 1,000 samples from the validation split for validation, and 1,000 samples from the validation split for testing.",,,,000 samples,"training split for training, 1,000 samples",
480,CC3M-TagMask,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Multi-Label Text Classification, Segmentation","Image, Text",English,Computer Vision,"multi-label-text-classification-on-cc3m, semantic-segmentation-on-cc3m-tagmask",,https://github.com/shjo-april/TTD,https://paperswithcode.com/dataset/cc3m-tagmask,The dataset offers tag and mask annotations for image-text pairs from the CC3M validation set. Tag annotations denote words that aptly describe the relationship between the image and the corresponding text. These annotations provide valuable insights into the semantic connection between each pair's visual and textual elements.,,,,,,
481,CCGbank,CCG Supertagging,CCG Supertagging,CCG Supertagging,,,Methodology,ccg-supertagging-on-ccgbank,Custom,https://catalog.ldc.upenn.edu/LDC2005T13,https://paperswithcode.com/dataset/ccgbank,"CCGbank is a translation of the Penn Treebank into a corpus of Combinatory Categorial Grammar derivations. It pairs syntactic derivations with sets of word-word dependencies which approximate the underlying predicate-argument structure.
The dataset contains 99.44% of the sentences in the Penn Treebank, for which it corrects a number of inconsistencies and errors in the original annotation.",,,,,,
482,CCMatrix,Text Generation,Text Generation,"Text Generation, Unsupervised Machine Translation",Text,English,Natural Language Processing,,,https://github.com/facebookresearch/LASER/tree/master/tasks/CCMatrix,https://paperswithcode.com/dataset/ccmatrix,"CCMatrix uses ten snapshots of a curated common crawl corpus (Wenzek et al., 2019) totalling 32.7 billion unique sentences.",2019,,,,,
483,CD18,Multimodal Text and Image Classification,Multimodal Text and Image Classification,Multimodal Text and Image Classification,"Image, Text",English,Multimodal,multimodal-text-and-image-classification-on-2,,,https://paperswithcode.com/dataset/cd18,,,,,,,
484,CDCP,Link Prediction,Link Prediction,"Link Prediction, Relation Classification, Component Classification","Graph, Image, Time Series",,Computer Vision,"component-classification-on-cdcp, relation-classification-on-cdcp, link-prediction-on-cdcp",,https://facultystaff.richmond.edu/~jpark/,https://paperswithcode.com/dataset/cdcp,"The Cornell eRulemaking Corpus – CDCP is an argument mining corpus annotated with argumentative structure information capturing the evaluability of arguments. The corpus consists of 731 user comments on Consumer Debt Collection Practices (CDCP) rule by the Consumer Financial Protection Bureau (CFPB); the resulting dataset contains 4931 elementary unit and 1221 support relation annotations. It is a resource for building argument mining systems that can not only extract arguments from unstructured text, but also identify what additional information is necessary
for readers to understand and evaluate a given argument. Immediate applications include providing real-time feedback to commenters, specifying which types of support for which propositions can be added to construct better-formed arguments.",,,,,,
485,CDD_Dataset__season-varying_,Change Detection,Change Detection,"Change Detection, Change detection for remote sensing images",Image,,Computer Vision,"change-detection-for-remote-sensing-images-on, change-detection-on-cdd-dataset-season-1",,https://pdfs.semanticscholar.org/ae15/e5ccccaaff44ab542003386349ef1d3b7511.pdf,https://paperswithcode.com/dataset/cdd-dataset-season-varying,,,CHANGE DETECTION IN REMOTE SENSING IMAGES USING CONDITIONAL ADVERSARIAL NETWORKS,https://pdfs.semanticscholar.org/ae15/e5ccccaaff44ab542003386349ef1d3b7511.pdf,,,
486,CDR,Reflection Removal,Reflection Removal,"Reflection Removal, Relation Extraction, Joint Entity and Relation Extraction",Graph,,Methodology,"joint-entity-and-relation-extraction-on-cdr, relation-extraction-on-cdr",,https://biocreative.bioinformatics.udel.edu/tasks/biocreative-v/track-3-cdr/,https://paperswithcode.com/dataset/cdr,"The BioCreative V CDR task corpus is manually annotated for chemicals, diseases and chemical-induced disease (CID) relations. It contains the titles and abstracts of 1500 PubMed articles and is split into equally sized train, validation and test sets. It is common to first tune a model on the validation set and then train on the combination of the train and validation sets before evaluating on the test set. It is also common to filter negative relations with disease entities that are hypernyms of a corresponding true relations disease entity within the same abstract (see Appendix C of this paper for details).",,,,,,
487,CED,Image Reconstruction,Image Reconstruction,"Image Reconstruction, Event-based vision","3D, Image",,Computer Vision,,,http://rpg.ifi.uzh.ch/CED.html,https://paperswithcode.com/dataset/ced,Contains 50 minutes of footage with both color frames and events. CED features a wide variety of indoor and outdoor scenes.,,,,,,
488,CEDAR_Signature,Handwriting Verification,Handwriting Verification,Handwriting Verification,,,Methodology,handwriting-verification-on-cedar-signature,,http://www.cedar.buffalo.edu/NIJ/data,https://paperswithcode.com/dataset/cedar-signature,"CEDAR Signature is a database of off-line signatures for signature verification. Each of 55 individuals contributed 24 signatures thereby creating 1,320 genuine signatures. Some were asked to forge three other writers’ signatures, eight times
per subject, thus creating 1,320 forgeries. Each signature was scanned at 300 dpi gray-scale and binarized using a gray-scale histogram. Salt pepper noise removal and slant normalization were two steps involved in image preprocessing. The database has 24 genuines and 24 forgeries available for each writer.",,,,,,
489,CEFR-SP,Text Simplification,Text Simplification,Text Simplification,Text,English,Natural Language Processing,,,https://github.com/yukiar/CEFR-SP,https://paperswithcode.com/dataset/cefr-sp,CEFR-SP contains 17k English sentences annotated with the levels based on the Common European Framework of Reference for Languages assigned by English-education professionals.,,CEFR-Based Sentence Difficulty Annotation and Assessment,https://arxiv.org/pdf/2210.11766v1.pdf,,,
490,Celeb-DF,Image Forensics,Image Forensics,"Image Forensics, DeepFake Detection, Face Swapping",Image,,Computer Vision,,Custom,https://github.com/danmohaha/celeb-deepfakeforensics,https://paperswithcode.com/dataset/celeb-df,"Celeb-DF is a large-scale challenging dataset for deepfake forensics. It includes 590 original videos collected from YouTube with subjects of different ages, ethnic groups and genders, and 5639 corresponding DeepFake videos.",,,,,,
491,CelebA-HQ,Density Estimation,Density Estimation,"Density Estimation, Image Super-Resolution, Image-to-Image Translation, Image Inpainting, Blind Face Restoration, Image Generation, Multimodal Unsupervised Image-To-Image Translation, Unconditional Image Generation","Image, Text",English,Computer Vision,"image-to-image-translation-on-celeba-hq, image-generation-on-celeba-hq, image-generation-on-celeba-hq-1024x1024, unconditional-image-generation-on-celeba-hq, image-super-resolution-on-celeb-hq-4x, image-generation-on-celeba-hq-64x64, image-generation-on-celeba-hq-512x512, image-generation-on-celeba-hq-256x256, image-inpainting-on-celeba-hq, blind-face-restoration-on-celeba-hq, density-estimation-on-celeba-hq-256x256, image-super-resolution-on-celeba-hq-128x128, image-generation-on-celeba-hq-128x128, multimodal-unsupervised-image-to-image-4",CC BY-NC 4.0,https://github.com/tkarras/progressive_growing_of_gans,https://paperswithcode.com/dataset/celeba-hq,"The CelebA-HQ dataset is a high-quality version of CelebA that consists of 30,000 images at 1024×1024 resolution.",,IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis,https://arxiv.org/abs/1807.06358,000 images,,
492,CelebA-Spoof,Face Anti-Spoofing,Face Anti-Spoofing,"Face Anti-Spoofing, Face Recognition",Image,,Computer Vision,,,https://github.com/Davidzhangyuanhan/CelebA-Spoof,https://paperswithcode.com/dataset/celeba-spoof,"CelebA-Spoof is a large-scale face anti-spoofing dataset with the following properties: 


Quantity: CelebA-Spoof comprises of 625,537 pictures of 10,177 subjects, significantly larger than the existing datasets. 
Diversity: The spoof images are captured from 8 scenes (2 environments * 4 illumination conditions) with more than 10 sensors. 
Annotation Richness: CelebA-Spoof contains 10 spoof type annotations, as well as the 40 attribute annotations inherited from the original CelebA dataset.",,,,,,
493,CelebA,HeavyMakeup/Unbiased,HeavyMakeup/Unbiased,"HeavyMakeup/Unbiased, HeavyMakeup/Bias-conflicting, Physical Attribute Prediction, Image Attribution, Image Classification, Image Compressed Sensing, Facial Expression Translation, Long-tail Learning, Image Super-Resolution, Image Colorization, Interpretability Techniques for Deep Learning, Image Inpainting, Image Generation, Multi-Task Learning, Face Alignment, HairColor/Bias-conflicting, Blind Face Restoration, HairColor/Unbiased, Image Deblurring, Concept-based Classification","Image, Text, Time Series",English,Computer Vision,"long-tail-learning-on-celeba-5, image-generation-on-celeba-3, face-alignment-on-celeba-aflw-unaligned, image-compressed-sensing-on-celeba, image-colorization-on-celeba, interpretability-techniques-for-deep-learning-1, haircolor-unbiased-on-celeba, image-generation-on-celeba-64x64, concept-based-classification-on-celeba, facial-expression-translation-on-celeba, image-classification-on-celeba-64x64, blind-face-restoration-on-celeba-test, image-deblurring-on-celeba, face-alignment-on-celeba-aligned, heavymakeup-bias-conflicting-on-celeba, image-super-resolution-on-celeba, image-attribution-on-celeba, image-inpainting-on-celeba, heavymakeup-unbiased-on-celeba, haircolor-bias-conflicting-on-celeba, image-generation-on-celeba-256x256, image-generation-on-celeba-128x128, multi-task-learning-on-celeba",,http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html,https://paperswithcode.com/dataset/celeba,"CelebFaces Attributes dataset contains 202,599 face images of the size 178×218 from 10,177 celebrities, each annotated with 40 binary labels indicating facial attributes like hair color, gender and age.",,"Show, Attend and Translate: Unpaired Multi-Domain Image-to-Image Translation with Visual Attention",https://arxiv.org/abs/1811.07483,,,
494,CelebAMask-HQ,Image Manipulation,Image Manipulation,"Image Manipulation, Image-to-Image Translation, 3D-Aware Image Synthesis, Reconstruction, Image Generation, Conditional Image Generation, Face Parsing, Pose Transfer","3D, Image, Text",English,Computer Vision,"reconstruction-on-celebamask-hq, 3d-aware-image-synthesis-on-celebamask-hq, face-parsing-on-celebamask-hq, conditional-image-generation-on-celebamask-hq, pose-transfer-on-celebamask-hq",Custom (non-commercial),https://github.com/switchablenorms/CelebAMask-HQ,https://paperswithcode.com/dataset/celebamask-hq,"CelebAMask-HQ is a large-scale face image dataset that has 30,000 high-resolution face images selected from the CelebA dataset by following CelebA-HQ. Each image has segmentation mask of facial attributes corresponding to CelebA.",,,,,,
495,CelebV-HQ,Unconditional Video Generation,Unconditional Video Generation,"Unconditional Video Generation, Action Classification, Facial Attribute Classification","Image, Text, Video",English,Computer Vision,"facial-attribute-classification-on-celebv-hq, unconditional-video-generation-on-celebv-hq, action-classification-on-celebv-hq","Custom (non-commercial, research purposes)",https://celebv-hq.github.io/,https://paperswithcode.com/dataset/celebv-hq,"CelebV-HQ is a large-scale video facial attributes dataset with annotations. CelebV-HQ contains 35,666 video clips involving 15,653 identities and 83 manually labeled facial attributes covering appearance, action, and emotion.

GitHub repository: https://github.com/celebv-hq/celebv-hq",,,,,,
496,CelebV-Text,Text Generation,Text Generation,"Text Generation, Text-to-Video Generation, Video Generation","Text, Video",English,Natural Language Processing,,,https://celebv-text.github.io,https://paperswithcode.com/dataset/celebv-text,"CelebV-Text comprises 70,000 in-the-wild face video clips with diverse visual content, each paired with 20 texts generated using the proposed semi-automatic text generation strategy. The provided texts describes both static and dynamic attributes precisely.",,CelebV-Text: A Large-Scale Facial Text-Video Dataset,https://arxiv.org/pdf/2303.14717v1.pdf,20 texts,,
497,CELEX,Word Embeddings,Word Embeddings,"Word Embeddings, Morphological Analysis, Multi-Task Learning",,,Methodology,,Custom (research-only),https://catalog.ldc.upenn.edu/LDC96L14,https://paperswithcode.com/dataset/celex,"CELEX database comprises three different searchable lexical databases, Dutch, English and German. The lexical data contained in each database is divided into five categories: orthography, phonology, morphology, syntax (word class) and word frequency.",,Polysemy and Brevity versus Frequency in Language,https://arxiv.org/abs/1904.00812,,,
498,Cell,Color Image Denoising,Color Image Denoising,"Color Image Denoising, Medical Image Segmentation, Nuclear Segmentation",Image,,Computer Vision,"color-image-denoising-on-cellnet, nuclear-segmentation-on-cell17, medical-image-segmentation-on-cell",,https://github.com/AltschulerWu-Lab/MuLANN,https://paperswithcode.com/dataset/cell,The CELL benchmark is made of fluorescence microscopy images of cell.,,https://arxiv.org/pdf/1903.09239v1.pdf,https://arxiv.org/pdf/1903.09239v1.pdf,,,
499,Cellcycle_Funcat,Hierarchical Multi-label Classification,Hierarchical Multi-label Classification,Hierarchical Multi-label Classification,Image,,Computer Vision,hierarchical-multi-label-classification-on,,https://dtai.cs.kuleuven.be/clus/hmcdatasets/,https://paperswithcode.com/dataset/cellcycle-funcat,Hierarchical multi-label classification dataset for functional genomics,,,,,,
500,Centrifugal_Pump_Fault_Detection,Fault Detection,Fault Detection,Fault Detection,Image,,Computer Vision,,,,https://paperswithcode.com/dataset/centrifugal-pump-fault-detection,"Dataset Overview
The dataset used for training and testing consists of vibration signals for six pump conditions:

| Fault Type        | Training Samples | Testing Samples | Flow Range (L/min) |
|------------------|-----------------|----------------|------------------|
| Healthy          | 1460             | 360            | 170 - 220        |
| Low Flow Rate    | 1120             | 280            | 80 - 120         |
| Cavitation       | 1440             | 360            | 120 - 170        |
| Major Defect     | 1440             | 360            | 150 - 200        |
| Minor Defect     | 1040             | 360            | 130 - 160        |
| Crack            | 1280             | 320            | 150 - 200        |",,,,,,
501,CEREC,Entity Resolution,Entity Resolution,Entity Resolution,,,Methodology,,,https://github.com/paragdakle/emailcoref,https://paperswithcode.com/dataset/cerec,"CEREC is a large scale corpus for entity resolution in email conversations. The corpus consists of 6001 email threads from the Enron Email Corpus containing 36,448 email messages and 60,383 entity coreference chains. The annotation is carried out as a two-step process with minimal manual effort.",,,,,,
502,CFQ,Structured Prediction,Structured Prediction,"Structured Prediction, Semantic Parsing, Question Answering","Text, Time Series",English,Natural Language Processing,semantic-parsing-on-cfq,,https://github.com/google-research/google-research/tree/master/cfq,https://paperswithcode.com/dataset/cfq,A large and realistic natural language question answering dataset.,,,,,,
503,CGNE-Snowflakes,Physical Simulations,Physical Simulations,Physical Simulations,,,Methodology,,MIT,https://github.com/poltimmer/CGNE/blob/main/LCA/main.py,https://paperswithcode.com/dataset/cgne-snowflakes,An image sequence dataset of growing snowflakes in HDF5 format. Generated by the Gravner-Griffeath LCA model for snow crystal growth. Useful for modeling crystal growth with neural networks.,,,,,,
504,CH-SIMS,Multimodal Sentiment Analysis,Multimodal Sentiment Analysis,"Multimodal Sentiment Analysis, Sentiment Analysis, Multi-Task Learning",Text,English,Multimodal,multimodal-sentiment-analysis-on-ch-sims,,https://github.com/thuiar/MMSA,https://paperswithcode.com/dataset/ch-sims,"CH-SIMS is a Chinese single- and multimodal sentiment analysis dataset which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis.",,CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality,https://www.aclweb.org/anthology/2020.acl-main.343.pdf,,,
505,ChaBuD,Change Detection,Change Detection,"Change Detection, Change detection for remote sensing images",Image,,Computer Vision,,OpenRail,https://huggingface.co/spaces/competitions/ChaBuD-ECML-PKDD2023,https://paperswithcode.com/dataset/chabud,"The dataset comprises patches of size 512x512 pixels collected from Sentinel-2 L2A satellite mission. All reported forest fires are located in California.
For each area of interest, two images are provided: pre-fire acquisition and post-fire acquisition. Each image is composed of 12 different channels, collecting information from the visible spectrum, infrared and ultrablue.

The dataset is split into:


train set
validation set
hidden test set, for which ground truth labels are not disclosed.

The hdf5 file is structured in this way:

root
|
|- uuid_0: {""post_fire"", ""pre_fire"", ""mask""}
| 
|
|- uuid_1: {""post_fire"", ""pre_fire"", ""mask""}
|
...

Each uuid have associated an attribute called fold.

Dataset can be downloaded from here. You can find a script that can be used to load data.

Additional optional data are available here. If you need more data, contact us.",,,,,,
506,CHAD,Supervised Anomaly Detection,Supervised Anomaly Detection,"Supervised Anomaly Detection, Video Anomaly Detection, Anomaly Detection, Group Anomaly Detection, Unsupervised Anomaly Detection","Image, Video",,Computer Vision,video-anomaly-detection-on-chad,,https://github.com/TeCSAR-UNCC/CHAD,https://paperswithcode.com/dataset/chad,"CHAD: Charlotte Anomaly Dataset
CHAD is high-resolution, multi-camera dataset for surveillance video anomaly detection. It includes bounding box, Re-ID, and pose annotations, as well as frame-level anomaly labels, dividing all frames into two groups of anomalous or normal. You can find the paper with all the details in the following link: CHAD: Charlotte Anomaly Dataset. Please refer to the page of the dataset for more information.",,CHAD: Charlotte Anomaly Dataset,https://arxiv.org/abs/2212.09258,,,
507,Chairs,Sketch-Based Image Retrieval,Sketch-Based Image Retrieval,Sketch-Based Image Retrieval,Image,,Computer Vision,sketch-based-image-retrieval-on-chairs,Custom (non-commercial),https://www.di.ens.fr/willow/research/seeing3Dchairs/,https://paperswithcode.com/dataset/chairs,The Chairs dataset contains rendered images of around 1000 different three-dimensional chair models.,,Adversarial Disentanglement with Grouped Observations,https://arxiv.org/abs/2001.04761,,,
508,Chalearn-AutoML-1,AutoML,AutoML,AutoML,,,Methodology,automl-on-madeline,,https://competitions.codalab.org/competitions/2321,https://paperswithcode.com/dataset/madeline,"This meta-dataset is first used in the AutoML1 challenge organized by Chalearn in 2015. It is composed of 30 pre-processed datasets, chosen to illustrate a wide variety of domains of applications: biology and medicine, ecology, energy and
sustainability management, image, text, audio, speech, video and other sensor data
processing, internet social media management and advertising, market analysis and
financial prediction.",2015,,,,,
509,CHALET,Imitation Learning,Imitation Learning,"Imitation Learning, Vision and Language Navigation, Decision Making",Text,English,Natural Language Processing,,CC BY 4.0,https://github.com/lil-lab/chalet,https://paperswithcode.com/dataset/chalet,"CHALET is a 3D house simulator with support for navigation and manipulation. Unlike existing systems, CHALET supports both a wide range of object manipulation, as well as supporting complex environemnt layouts consisting of multiple rooms. The range of object manipulations includes the ability to pick up and place objects, toggle the state of objects like taps or televesions, open or close containers, and insert or remove objects from these containers. In addition, the simulator comes with 58 rooms that can be combined to create houses, including 10 default house layouts. CHALET is therefore suitable for setting up challenging environments for various AI tasks that require complex language understanding and planning, such as navigation, manipulation, instruction following, and interactive question answering.",,https://arxiv.org/pdf/1809.00786.pdf,https://arxiv.org/pdf/1809.00786.pdf,,,
510,Chameleon_60__20__20__random_splits_,Node Classification on Non-Homophilic (Heterophilic) Graphs,Node Classification on Non-Homophilic (Heterophilic) Graphs,"Node Classification on Non-Homophilic (Heterophilic) Graphs, Node Classification",Image,,Graphs,"node-classification-on-non-homophilic-4, node-classification-on-chameleon-60-20-20-1",,,https://paperswithcode.com/dataset/chameleon-60-20-20-random-splits-1,Node classification on Chameleon with 60%/20%/20% random splits for training/validation/test.,,,,,,
511,Chameleon__48__32__20__fixed_splits_,Node Classification on Non-Homophilic (Heterophilic) Graphs,Node Classification on Non-Homophilic (Heterophilic) Graphs,"Node Classification on Non-Homophilic (Heterophilic) Graphs, Node Classification",Image,,Graphs,"node-classification-on-non-homophilic-11, node-classification-on-chameleon-48-32-20",,,https://paperswithcode.com/dataset/chameleon-48-32-20-fixed-splits,Node classification on Chameleon with the fixed 48%/32%/20% splits provided by Geom-GCN.,,,,,,
512,Chaoyang,Histopathological Image Classification,Histopathological Image Classification,"Histopathological Image Classification, Image Classification, Learning with noisy labels, Colon Cancer Detection In Confocal Laser Microscopy Images",Image,,Computer Vision,"learning-with-noisy-labels-on-chaoyang, image-classification-on-chaoyang",Custom,https://bupt-ai-cz.github.io/HSA-NRL/,https://paperswithcode.com/dataset/chaoyang,"Chaoyang dataset contains 1111 normal, 842 serrated, 1404 adenocarcinoma, 664 adenoma, and 705 normal, 321 serrated, 840 adenocarcinoma, 273 adenoma samples for training and testing, respectively. This noisy dataset is constructed in the real scenario.  



Details: Colon slides from Chaoyang hospital, the patch size is 512 × 512. We invited 3 professional pathologists to label the patches, respectively. We took the parts of labeled patches with consensus results from 3 pathologists as the testing set. Others we used as the training set. For the samples with inconsistent labeling opinions of the three doctors in the training set (this part accounts for about 40%), we randomly selected the opinions from one of the three doctors.



The original WSIs are scanned at X20 objective magnification.",,,,,,
513,Charades-STA,Video Understanding,Video Understanding,"Video Understanding, Partially Relevant Video Retrieval, Temporal Sentence Grounding, Decision Making, Moment Retrieval, Video Retrieval, Temporal Localization","Image, Time Series, Video",,Computer Vision,"moment-retrieval-on-charades-sta, partially-relevant-video-retrieval-on-1, video-retrieval-on-charades-sta, temporal-sentence-grounding-on-charades-sta",Custom,https://github.com/jiyanggao/TALL,https://paperswithcode.com/dataset/charades-sta,Charades-STA is a new dataset built on top of Charades by adding sentence temporal annotations.,,,,,,
514,Charades,Video Understanding,Video Understanding,"Video Understanding, Action Classification, Video Classification, Temporal Action Localization, Weakly Supervised Object Detection, Action Recognition, Action Detection, Zero-Shot Action Recognition","Image, Time Series, Video",,Computer Vision,"video-classification-on-charades, zero-shot-action-recognition-on-charades-1, weakly-supervised-object-detection-on-4, action-detection-on-charades, action-classification-on-charades, action-recognition-in-videos-on-charades",Custom (non-commercial),http://vuchallenge.org/charades.html,https://paperswithcode.com/dataset/charades,"The Charades dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video.",,Temporal Reasoning Graph for Activity Recognition,https://arxiv.org/abs/1908.09995,,,104
515,Charlotte-ThermalFace,Thermal Image Segmentation,Thermal Image Segmentation,Thermal Image Segmentation,Image,,Computer Vision,,,https://github.com/TeCSAR-UNCC/UNCC-ThermalFace,https://paperswithcode.com/dataset/charlotte-thermalface,"Charlotte-ThermalFace is a thermal face dataset. The data is fully annotated with the facial landmarks, ambient temperature, relative humidity, the air speed of the room, distance to the camera, and subject thermal sensation at the time of capturing each image.

There are approximately 10,000 infrared thermal images from 10 subjects in varying thermal conditions, at several distances from the camera, and at changing head positions. We have also controlled the air temperature to change from 20.5°C ( 69°F) to 26.5 °C( 80°F). Images are available in four different temperatures, 10 relative distances from the camera, starting at 1m ( 3.3 ft) to 6.6m( 21.6 ft), and 25 head positions.

• The first public facial thermal dataset annotated with the environmental properties including air temperature, relative humidity, airspeed, distance from the camera, and subjective thermal sensation of each person at the time.

• All the images are manually annotated with 72 or 43 facial landmarks.

• We are publishing the data in the original 16-bit radiometric TemperatureLinear format, which has the thermal value of each pixel.",,,,,,
516,ChartQA,Chart Question Answering,Chart Question Answering,Chart Question Answering,Text,English,Natural Language Processing,chart-question-answering-on-chartqa,,https://github.com/vis-nlp/ChartQA,https://paperswithcode.com/dataset/chartqa,"Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",,,,,,
517,CHASE_DB1,Medical Image Segmentation,Medical Image Segmentation,"Medical Image Segmentation, Retinal Vessel Segmentation",Image,,Medical,"retinal-vessel-segmentation-on-chase_db1, medical-image-segmentation-on-chase-db1",,https://blogs.kingston.ac.uk/retinal/chasedb1/,https://paperswithcode.com/dataset/chase-db1,CHASE_DB1 is a dataset for retinal vessel segmentation which contains 28 color retina images with the size of 999×960 pixels which are collected from both left and right eyes of 14 school children. Each image is annotated by two independent human experts.,,MixModule: Mixed CNN Kernel Module for Medical Image Segmentation,https://arxiv.org/abs/1910.08728,,,
518,ChatGPT-software-testing,Chatbot,Chatbot,Chatbot,,,Methodology,,MIT,https://github.com/sajedjalil/Study-on-ChatGPT/tree/main/dataset,https://paperswithcode.com/dataset/chatgpt-software-testing,"Dataset Description
Our dataset contains questions from a well-known software testing book Introduction to Software Testing 2nd Edition by Ammann and Offutt. 
We use all the text-book questions in Chapters 1 to 5 that have solutions available on the book’s official website. 

Our dataset contains 40 such questions from these five chapters. 31 questions out of the 40 are multipart questions and the rest 9 are independent.
This tool generates responses from the ChatGPT automatically for these questions. All of these questions are run in both shared and separate context.
More information about the contexts can be found below.

Combined.xlsx
Contains all the questions & answers for 3 iterations of both shared and separate contexts. Contains labels for answers and explanations given by ChatGPT.

Combined_pair.xlsx
Contains the same data as combined.xlsx except for the questions that are independent i.e., not part of a multipart question.

Combined_analysis.xlsx
Contains the result and analysis of the four research questions. Besides, it contains various illustrations for the results.

Combined-temp.xlsx
Questions with missing shared contexts are replaced with the answers for the separate context to easily fetch the data for 
RQ2 & RQ3 from a single column.

examples folders
Contains examples of some interesting response categories.

Case Study.pdf
Contains the following analysis:
- When responses are likely to be incorrect?
- What are the reasons for being incorrect?
- Can we fix it with prompt engineering?
- Case studies with actual examples.

Separate Context Query
In separate context queries, we treat each of the 31 multipart questions as an independent question.
Each sub-question is asked in a separate chat thread.
Combining with the nine independent questions, a total of 40 questions are asked for each run. To evaluate the consistency in
ChatGPT’s responses, we collect a total of three runs for each question, which results in a total of 120 responses from ChatGPT.

Shared Context Query
Our dataset contains six questions that contain total 31 multipart questions or sub-questions and nine questions that do not. 
These six sub-questions are asked in a chat thread that are shared with other sub-questions as long as the sub-questions 
refer to the same code or scenario.",,,,,,
519,ChatGPT_Advice_Responses,Causal Inference,Causal Inference,Causal Inference,,,Methodology,,CC BY-NC-SA 4.0,https://github.com/petezh/ChatGPT-Advice,https://paperswithcode.com/dataset/chatgpt-advice-responses,"Taking Advice from ChatGPT is a laboratory study of how student participants incorporate advice generated by ChatGPT. In a survey conducted through the Experimental Social Science Laboratory, 118 students answered 2,828 questions on topics from the MMLU benchmark. The rich dataset includes questions/choices, advice characteristics, participant answers, and participant background. It can be used to explore algorithm aversion, advice-taking, ChatGPT usage, and more.",,,,,,
520,ChatGPT_Role-Play_Dataset__CRD_,Conversational Response Generation,Conversational Response Generation,Conversational Response Generation,Text,English,Natural Language Processing,,MIT,https://github.com/PortNLP/ChatGPT_Role-play_Dataset,https://paperswithcode.com/dataset/chatgpt-role-play-dataset-crd,"Dataset Overview
vanilla.csv: Represents the interactions without specific role-play instructions.
boss.csv: Interactions where ChatGPT plays the role of a user's boss.
classmate.csv: Interactions with ChatGPT acting as the user's classmate.
Each turn was coded with user motives of user responses, or the perceived naturalness of ChatGPT responses.",,,,,,
521,ChatGPT_Software_Testing_Study,Chatbot,Chatbot,Chatbot,,,Methodology,,MIT,https://github.com/sajedjalil/ChatGPT-Software-Testing-Study,https://paperswithcode.com/dataset/chatgpt-software-testing-study,ChatGPT Software Testing Study Dataset contains questions from a well-known software testing book by Ammann and Offutt. It uses all the textbook questions in Chapters 1 to 5 that have solutions available on the book’s official website. These solutions are made publicly available to help students learn. Questions that are not in the student solution are omitted because publishing our results might expose answers that the authors of the book do not intend to make public.,,ChatGPT and Software Testing Education: Promises & Perils,https://arxiv.org/pdf/2302.03287.pdf,,,
522,CHB-MIT,Seizure Detection,Seizure Detection,Seizure Detection,Image,,Computer Vision,seizure-detection-on-chb-mit,,https://physionet.org/content/chbmit/1.0.0/,https://paperswithcode.com/dataset/chb-mit,"The CHB-MIT dataset is a dataset of EEG recordings from pediatric subjects with intractable seizures. Subjects were monitored for up to several days following withdrawal of anti-seizure mediation in order to characterize their seizures and assess their candidacy for surgical intervention. The dataset contains 23 patients divided among 24 cases (a patient has 2 recordings, 1.5 years apart). The dataset consists of 969 Hours of scalp EEG recordings with 173 seizures. There exist various types of seizures in the dataset (clonic, atonic, tonic). The diversity of patients (Male, Female, 10-22 years old) and different types of seizures contained in the datasets are ideal for assessing the performance of automatic seizure detection methods in realistic settings.",,Learning Robust Features using Deep Learning for Automatic Seizure Detection,https://arxiv.org/abs/1608.00220,,,
523,ChEBI-20,Text-based de novo Molecule Generation,Text-based de novo Molecule Generation,"Text-based de novo Molecule Generation, Cross-Modal Retrieval, Image Captioning, Molecule Captioning","Image, Text",English,Computer Vision,"molecule-captioning-on-chebi-20, image-captioning-on-chebi-20, cross-modal-retrieval-on-chebi-20, text-based-de-novo-molecule-generation-on",,https://github.com/cnedwards/text2mol/tree/master/data,https://paperswithcode.com/dataset/chebi-20,"Dataset contains 33,010 molecule-description pairs split into 80\%/10\%/10\% train/val/test splits. The goal of the task is to retrieve the relevant molecule for a natural language description. It is defined as follows:

To push the boundaries of multimodal models, we present a new IR task: \textbf{Text2Mol}.

Given a text query and list of molecules without any reference textual information (represented, for example, as SMILES strings, graphs, or other equivalent representations) retrieve the molecule corresponding to the query. From a text description of a molecule, the model must incorporate the information in the description into a semantic representation which can be used to directly retrieve the molecule. This requires the integration of two very different types of information: the structured knowledge represented by text and the chemical properties present in molecular graphs. We assume there is only one correct (relevant) molecule for each description, so we consider two measures for this task: Hits@1 and mean reciprocal rank (MRR). 

80\% of the data is used for training. Retrieval is done against the entire corpus of molecules (train, val, test).",,,,,,
524,Chem-FINESE,Chemical Entity Recognition,Chemical Entity Recognition,"Chemical Entity Recognition, Few-shot NER, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,,,https://github.com/EagleW/Chem-FINESE/tree/main/data,https://paperswithcode.com/dataset/chem-finese,"The dataset contains two few-shot chemical fine-grained entity extraction datasets, based on human-annotated ChemNER+ and CHEMET.
For each dataset, we randomly sample a subset based on the frequency of each type class. Specifically, given a dataset, we first set the number of maximum entity mentions $k$ for the most frequent entity type in the dataset. We then randomly sample other types and ensure that the distribution of each type remains the same as in the original dataset. We choose the values $6, 9, 12, 15, 18$ as the potential maximum entity mentions for $k$. The ChemNER+ and CHEMET few-shot datasets contain 52 and 28 types respectively.",,,,,,
525,ChEMBL,Graph Sampling,Graph Sampling,"Graph Sampling, Graph Embedding, Graph structure learning",Graph,,Methodology,,,https://www.ebi.ac.uk/chembl,https://paperswithcode.com/dataset/chembl-v-27,"ChEMBL is a manually curated database of bioactive molecules with drug-like properties. It brings together chemical, bioactivity and genomic data to aid the translation of genomic information into effective new drugs.",,,,,,
526,ChemDisGene,Document-level RE with incomplete labeling,Document-level RE with incomplete labeling,Document-level RE with incomplete labeling,Text,English,Natural Language Processing,document-level-re-with-incomplete-labeling-on-2,,,https://paperswithcode.com/dataset/chemdisgene,"ChemDisGene, a new dataset for training and evaluating multi-class multi-label biomedical relation extraction models.",,,,,,
527,ChestX-ray14,Pneumonia Detection,Pneumonia Detection,"Pneumonia Detection, Medical Image Generation, Thoracic Disease Classification, Multi-Label Classification, Multi-Task Learning","Image, Text",English,Computer Vision,"multi-label-classification-on-chestx-ray14, thoracic-disease-classification-on-chestx, medical-image-generation-on-chestx-ray14, multi-task-learning-on-chestx-ray14, medical-image-generation-on-chestxray14, pneumonia-detection-on-chestx-ray14",,https://nihcc.app.box.com/v/ChestXray-NIHCC,https://paperswithcode.com/dataset/chestx-ray14,"ChestX-ray14 is a medical imaging dataset which comprises 112,120 frontal-view X-ray images of 30,805 (collected from the year of 1992 to 2015) unique patients with the text-mined fourteen common disease labels, mined from the text radiological reports via NLP techniques. It expands on ChestX-ray8 by adding six additional thorax diseases: Edema, Emphysema, Fibrosis, Pleural Thickening and Hernia.",1992,,,,,
528,ChestX-ray8,Image Classification,Image Classification,"Image Classification, Computed Tomography (CT)",Image,,Computer Vision,,Custom (attribution),https://nihcc.app.box.com/v/ChestXray-NIHCC,https://paperswithcode.com/dataset/chestx-ray8,"ChestX-ray8 is a medical imaging dataset which comprises 108,948 frontal-view X-ray images of 32,717 (collected from the year of 1992 to 2015) unique patients with the text-mined eight common disease labels, mined from the text radiological reports via NLP techniques.",1992,,,,,
529,Chest_wall_lung_sound_dataset,Lung Sound Classification,Lung Sound Classification,"Lung Sound Classification, Audio Classification, Lung Disease Classification, Asthmatic Lung Sound Classification","Audio, Image",,Computer Vision,asthmatic-lung-sound-classification-on-chest,CC BY 4.0,https://data.mendeley.com/datasets/jwyy9np4gv/2,https://paperswithcode.com/dataset/chest-wall-lung-sound-dataset,"Annotated audio files (separate combined annotation file) of lung sounds as recorded from various vantage points of the chest wall. The annotation includes the sound type (Insipratory: I, Experiatory: E, Wheezes: W, Crackles: C , N:Normal), the diagnosis as decided by a specialist (Asthma, COPD, BRON, heart failure, lung fibrosis, etc.), and the location on the chest wall from which the recording was taken (Posterior: P Lower: L Left: L Right R, UPPER: U, ANTERIOR: A, MIDDLE: M). The audio file names are coded: 1. Filter type; B: BELL 20-200Hz, Diaphragm 100-500 Hz, Extended range 50-500 Hz. 2. Patient number: P1-P112.",,,,,,
530,Chest_X-ray_images,Pneumonia Detection,Pneumonia Detection,"Pneumonia Detection, Self-Supervised Image Classification",Image,,Computer Vision,"self-supervised-image-classification-on-chest, pneumonia-detection-on-chest-x-ray-images-1",,https://pubmed.ncbi.nlm.nih.gov/29474911/,https://paperswithcode.com/dataset/chest-x-ray-images,Chest X-ray images for pneumonia detection.,,,,,,
531,CheXlocalize,Weakly supervised Semantic Segmentation,Weakly supervised Semantic Segmentation,"Weakly supervised Semantic Segmentation, Semantic Segmentation, Medical Image Segmentation, Weakly-Supervised Semantic Segmentation, Weakly supervised segmentation, Medical X-Ray Image Segmentation, Weakly-Supervised Object Segmentation, 2D Semantic Segmentation",Image,,Computer Vision,,MIT,https://github.com/rajpurkarlab/cheXlocalize/,https://paperswithcode.com/dataset/chexlocalize,"CheXlocalize is a radiologist-annotated segmentation dataset on chest X-rays. The dataset consists of two types of radiologist annotations for the localization of 10 pathologies: pixel-level segmentations and most-representative points. Annotations were drawn on images from the CheXpert validation and test sets. The dataset also consists of two separate sets of radiologist annotations: (1) ground-truth pixel-level segmentations on the validation and test sets, drawn by two board-certified radiologists, and (2) benchmark pixel-level segmentations and most-representative points on the test set, drawn by a separate group of three board-certified radiologists.

The validation and test sets consist of 234 chest X-rays from 200 patients and 668 chest X-rays from 500 patients, respectively. The 10 pathologies of interest are Atelectasis, Cardiomegaly, Consolidation, Edema, Enlarged Cardiomediastinum, Lung Lesion, Lung Opacity, Pleural Effusion, Pneumothorax, and Support Devices.

For more details, please see our paper, Benchmarking saliency methods for chest X-ray interpretation.",,,,,,
532,CheXmask,Medical X-Ray Image Segmentation,Medical X-Ray Image Segmentation,"Medical X-Ray Image Segmentation, Image Segmentation, Medical Image Segmentation, Heart Segmentation",Image,,Medical,,Creative Commons Attribution 4.0 International Public License,https://physionet.org/content/chexmask-cxr-segmentation-data,https://paperswithcode.com/dataset/chexmask,"The CheXmask Database presents a comprehensive, uniformly annotated collection of chest radiographs, constructed from five public databases: ChestX-ray8, Chexpert, MIMIC-CXR-JPG, Padchest and VinDr-CXR. The database aggregates 657,566 anatomical segmentation masks derived from images which have been processed using the HybridGNet model to ensure consistent, high-quality segmentation. To confirm the quality of the segmentations, we include in this database individual Reverse Classification Accuracy (RCA) scores for each of the segmentation masks. This dataset is intended to catalyze further innovation and refinement in the field of semantic chest X-ray analysis, offering a significant resource for researchers in the medical imaging domain.",,,,,,
533,CheXpert,Multi-Label Classification,Multi-Label Classification,"Multi-Label Classification, Concept-based Classification",Image,,Computer Vision,"multi-label-classification-on-chexpert, concept-based-classification-on-chexpert",Custom,https://stanfordmlgroup.github.io/competitions/chexpert/,https://paperswithcode.com/dataset/chexpert,"The CheXpert dataset contains 224,316 chest radiographs of 65,240 patients with both frontal and lateral views available. The task is to do automated chest x-ray interpretation, featuring uncertainty labels and radiologist-labeled reference standard evaluation sets.",,Deep Mining External Imperfect Data for Chest X-ray Disease Screening,https://arxiv.org/abs/2006.03796,,,
534,ChFinAnn,Document-level Event Extraction,Document-level Event Extraction,"Document-level Event Extraction, Event Extraction",Text,English,Natural Language Processing,document-level-event-extraction-on-chfinann,MIT,https://github.com/dolphin-zs/Doc2EDAG,https://paperswithcode.com/dataset/chfinann,"Ten years (2008-2018) ChFinAnn documents and human-summarized event knowledge bases to conduct the DS-based event labeling.
Five event types included: Equity Freeze (EF), Equity Repurchase (ER), Equity Underweight (EU), Equity Overweight (EO) and Equity Pledge (EP), which belong to major events required to be disclosed by the regulator and may have a huge impact on the company value.
To ensure the labeling quality, the authors set constraints for matched document-record pairs.

There are 32, 040 documents in total, and this number is ten times larger than 2, 976 of DCFEE and about 53 times larger than 599 of ACE 2005.
These documents are divided into train, development, and test set with the proportion of 8 : 1 : 1 based on the time order.
This DS-generated data are pretty good, achieving high precision and acceptable recall.
In later experiments, the authors directly employ the automatically generated test set for evaluation due to its much broad coverage.",2008,,,040 documents,,
535,Chilean_Waiting_List,Nested Named Entity Recognition,Nested Named Entity Recognition,Nested Named Entity Recognition,"Image, Text",English,Computer Vision,nested-named-entity-recognition-on-chilean,Creative Commons Attribution 4.0 International,https://zenodo.org/record/7555181#.Y87MNexBzzc,https://paperswithcode.com/dataset/chilean-waiting-list-corpus,"The Chilean Waiting List corpus comprises de-identified referrals from the waiting list in Chilean public hospitals. A subset of 10,000 referrals (including medical
and dental notes) was manually annotated with ten entity types with clinical relevance, keeping 1,000 annotations for a future shared task. A trained medical doctor or dentist annotated these referrals and then, together with three other researchers, consolidated each of the annotations. The annotated corpus has more than 48% of entities embedded in
other entities or containing another. This corpus can be a useful resource to build new models for Nested Named Entity Recognition (NER). This work constitutes the first
annotated corpus using clinical narratives from Chile and one of the few in Spanish.

Hugging Face datasets: https://huggingface.co/plncmm. After predicting over each entity type, merge the prediction to obtain your final micro f1-score. This is for a fair comparison with actual state-of-the-art models.",,,,,,
536,CHILI-100K,Crystal system classification,Crystal system classification,"Crystal system classification, Space group classification, Neutron PDF regression, Atomic number classification, Position regression, SAXS regression, Distance regression, XRD regression, X-ray PDF regression, SANS regression, ND regression",Image,,Computer Vision,"distance-regression-on-chili-100k, atomic-number-classification-on-chili-100k, x-ray-pdf-regression-on-chili-100k, xrd-regression-on-chili-100k, space-group-classification-on-chili-100k, position-regression-on-chili-100k, saxs-regression-on-chili-100k, crystal-system-classification-on-chili-100k",Apache-2.0,https://github.com/UlrikFriisJensen/CHILI,https://paperswithcode.com/dataset/chili-100k,"The CHILI-100K dataset is a large-scale graph dataset (with overall >183M nodes, >1.2B edges) of nanomaterials generated from experimentally determined crystal structures. The crystal structures used in CHILI-100K are obtained from a curated subset from the Crystallography Open Database (COD) and has a broad chemical scope covering database entries for 68 metals and 11 non-metals.",,,,,,
537,CHILI-3K,Crystal system classification,Crystal system classification,"Crystal system classification, Space group classification, Neutron PDF regression, Atomic number classification, Position regression, SAXS regression, Distance regression, XRD regression, X-ray PDF regression, SANS regression, ND regression",Image,,Computer Vision,"saxs-regression-on-chili-3k, x-ray-pdf-regression-on-chili-3k, crystal-system-classification-on-chili-3k, atomic-number-classification-on-chili-3k, xrd-regression-on-chili-3k, space-group-classification-on-chili-3k, distance-regression-on-chili-3k, position-regression-on-chili-3k",Apache-2.0,https://github.com/UlrikFriisJensen/CHILI,https://paperswithcode.com/dataset/chili-3k,"The CHILI-3K dataset is a medium-scale graph dataset (with overall >6M nodes, >49M edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal types. This dataset has a narrow chemical scope focused on an interesting part of chemical space with a lot of active research.",,,,,,
538,CHiME-5,Speaker Diarization,Speaker Diarization,"Speaker Diarization, Speech Recognition, Speech Enhancement","Audio, Image, Text",English,Computer Vision,,Custom,https://github.com/kaldi-asr/kaldi/tree/master/egs/chime5/s5,https://paperswithcode.com/dataset/chime-5,"The CHiME challenge series aims to advance robust automatic speech recognition (ASR) technology by promoting research at the interface of speech and language processing, signal processing , and machine learning.",,"The fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, task and baselines",https://arxiv.org/pdf/1803.10609v1.pdf,,,
539,CHiME-Home,Audio Tagging,Audio Tagging,"Audio Tagging, Multi-Label Classification","Audio, Image",,Audio,,,https://code.soundsoftware.ac.uk/projects/chime-home-dataset-annotation-and-baseline-evaluation-code,https://paperswithcode.com/dataset/chime-home,"CHiME-Home is a dataset for sound source recognition in a domestic environment. It uses around 6.8 hours of domestic environment audio recordings. The recordings were obtained from the CHiME projects – computational hearing in multisource environments – where recording equipment was positioned inside an English Victorian semi-detached house. The recordings were selected from 22 sessions totalling 19.5 hours, with each session made between 7:30 in the morning and 20:00 in the evening. In the considered recordings, the equipment was placed in the lounge (sitting room) near the door opening onto a hallway, with the hallway opening onto a kitchen with no door. With the lounge door typically open, prominent sounds thus may originate from sources both in the lounge and kitchen.

The choice of permitted labels was motivated by the sources present in the considered acoustic environment: Human speakers (c,m,f); human activity (p); television (v); household appliances (b). Further labels o,S,U respectively relate to any other identifiable sounds, silence, unidentifiable sounds. Labels S,U may respectively only be assigned in isolation. Annotators were acquired to assign at least one label to a chunk, thus annotators may either assign one or more labels from the set {c,m,f,v,p,b,o}, or may alternatively ‘flag’ the chunk using a single label from the set {S,U}.",,,,,,
540,Chinese_Gigaword,Word Embeddings,Word Embeddings,"Word Embeddings, Named Entity Recognition (NER), Decipherment","Image, Text",English,Computer Vision,,Custom,https://catalog.ldc.upenn.edu/LDC2011T13,https://paperswithcode.com/dataset/chinese-gigaword,"Chinese Gigaword corpus consists of 2.2M of headline-document pairs of news stories covering over 284 months from two Chinese newspapers, namely the Xinhua News Agency of China (XIN) and the Central News Agency of Taiwan (CNA).",,Order-Preserving Abstractive Summarization for Spoken Content Based on Connectionist Temporal Classification,https://arxiv.org/abs/1709.05475,,,
541,CHIP-STS,Semantic Similarity,Semantic Similarity,Semantic Similarity,,,Methodology,semantic-similarity-on-chip-sts,,https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414,https://paperswithcode.com/dataset/chip-sts,"CHIP Semantic Textual Similarity, a dataset for sentence similarity in the non-i.i.d.
(non-independent and identically distributed) setting, is used for the CHIP-STS task. Specifically, the
task aims to transfer learning between disease types on Chinese disease questions and answer data.
Given question pairs related to 5 different diseases (The disease types in the training and testing set
are different), the task intends to determine whether the semantics of the two sentences are similar.",,,,,,
542,Cho2017_MOABB,Within-Session Motor Imagery (left hand vs. right hand),Within-Session Motor Imagery (left hand vs. right hand),Within-Session Motor Imagery (left hand vs. right hand),,,Methodology,within-session-motor-imagery-left-hand-vs-2,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Cho2017.html,https://paperswithcode.com/dataset/cho2017-moabb,,,,,,,
543,ChronoMagic-Pro,Video Understanding,Video Understanding,"Video Understanding, Text-to-Video Generation, Video Generation, Video Retrieval, Image to Video Generation","Image, Text, Video",English,Computer Vision,,Apache-2.0 license,https://pku-yuangroup.github.io/ChronoMagic-Bench/,https://paperswithcode.com/dataset/chronomagic-pro,"Description

Venue: NeurIPS 2024 D&B Spotlight
Repository: Code, Page, Data
Paper: arxiv.org/abs/2406.18522
Point of Contact: Shenghai Yuan

Citation
If you find our paper and code useful in your research, please consider giving a star and citation.

BibTeX
@article{yuan2024chronomagic,
  title={Chronomagic-bench: A benchmark for metamorphic evaluation of text-to-time-lapse video generation},
  author={Yuan, Shenghai and Huang, Jinfa and Xu, Yongqi and Liu, Yaoyang and Zhang, Shaofeng and Shi, Yujun and Zhu, Ruijie and Cheng, Xinhua and Luo, Jiebo and Yuan, Li},
  journal={arXiv preprint arXiv:2406.18522},
  year={2024}
}",2024,,,,,
544,ChronoMagic-ProH,Video Understanding,Video Understanding,"Video Understanding, Text-to-Video Generation, Video Generation, Video Retrieval, Image to Video Generation","Image, Text, Video",English,Computer Vision,,Apache-2.0 license,https://pku-yuangroup.github.io/ChronoMagic-Bench/,https://paperswithcode.com/dataset/chronomagic-proh,"Description

Venue: NeurIPS 2024 D&B Spotlight
Repository: Code, Page, Data
Paper: arxiv.org/abs/2406.18522
Point of Contact: Shenghai Yuan

Citation
If you find our paper and code useful in your research, please consider giving a star and citation.

BibTeX
@article{yuan2024chronomagic,
  title={Chronomagic-bench: A benchmark for metamorphic evaluation of text-to-time-lapse video generation},
  author={Yuan, Shenghai and Huang, Jinfa and Xu, Yongqi and Liu, Yaoyang and Zhang, Shaofeng and Shi, Yujun and Zhu, Ruijie and Cheng, Xinhua and Luo, Jiebo and Yuan, Li},
  journal={arXiv preprint arXiv:2406.18522},
  year={2024}
}",2024,,,,,
545,ChronoMagic,Video Understanding,Video Understanding,"Video Understanding, Text-to-Video Generation, Video Generation, Video Retrieval, Image to Video Generation","Image, Text, Video",English,Computer Vision,,Apache-2.0,https://pku-yuangroup.github.io/MagicTime/,https://paperswithcode.com/dataset/chronomagic,"ChronoMagic with 2265 metamorphic time-lapse videos, each accompanied by a detailed caption.",,,,,,
546,CI-MNIST,Age And Gender Classification,Age And Gender Classification,"Age And Gender Classification, imbalanced classification, Bias Detection, Image Classification, Fairness, Gender Bias Detection",Image,,Computer Vision,,Open Access,https://openreview.net/pdf?id=OTnqQUEwPKu,https://paperswithcode.com/dataset/ci-mnist,"CI-MNIST (Correlated and Imbalanced MNIST) is a variant of MNIST dataset with introduced different types of correlations between attributes, dataset features, and an artificial eligibility criterion. For an input image $x$, the label $y \in \{1, 0\}$ indicates eligibility or ineligibility, respectively, given that $x$ is even or odd. The dataset defines the background colors as the protected or sensitive attribute $s \in \{0, 1\}$, where blue denotes the unprivileged group and red denotes the privileged group. The dataset was designed in order to evaluate bias-mitigation approaches in challenging setups and be capable of controlling different dataset configurations.",,,,,,
547,CIC,Stance Detection,Stance Detection,"Stance Detection, Intrusion Detection, Text Classification","Image, Text",English,Computer Vision,"intrusion-detection-on-cic-ddos, text-classification-on-catalonia-independence, intrusion-detection-on-cic-dos",,https://github.com/ixa-ehu/catalonia-independence-corpus,https://paperswithcode.com/dataset/cic,"The dataset is annotated with stance towards one topic, namely, the independence of Catalonia.",,,,,,
548,CICERO,Generative Question Answering,Generative Question Answering,"Generative Question Answering, Answer Selection, Answer Generation, Multiview Contextual Commonsense Inference",Text,English,Natural Language Processing,"answer-selection-on-cicero, answer-generation-on-cicero, multiview-contextual-commonsense-inference-on-1, generative-question-answering-on-cicero",MIT,https://declare-lab.net/CICERO/,https://paperswithcode.com/dataset/cicero,"CICERO contains 53,000 inferences for five commonsense dimensions -- cause, subsequent event, prerequisite, motivation, and emotional reaction -- collected from 5600 dialogues. It involves two challenging generative and multi-choice alternative selection tasks for the state-of-the-art NLP models to solve. Download the dataset using this link.",,,,,,
549,CICIDS2017,Network Intrusion Detection,Network Intrusion Detection,Network Intrusion Detection,"Graph, Image",,Computer Vision,network-intrusion-detection-on-cicids2017,,https://www.unb.ca/cic/datasets/ids-2017.html,https://paperswithcode.com/dataset/cicids2017,"Intrusion Detection Evaluation Dataset (CIC-IDS2017)
Intrusion Detection Systems (IDSs) and Intrusion Prevention Systems (IPSs) are the most important defense tools against the sophisticated and ever-growing network attacks. Due to the lack of reliable test and validation datasets, anomaly-based intrusion detection approaches are suffering from consistent and accurate performance evolutions.

Our evaluations of the existing eleven datasets since 1998 show that most are out of date and unreliable. Some of these datasets suffer from the lack of traffic diversity and volumes, some do not cover the variety of known attacks, while others anonymize packet payload data, which cannot reflect the current trends. Some are also lacking feature set and metadata.

CICIDS2017 dataset contains benign and the most up-to-date common attacks, which resembles the true real-world data (PCAPs). It also includes the results of the network traffic analysis using CICFlowMeter with labeled flows based on the time stamp, source, and destination IPs, source and destination ports, protocols and attack (CSV files). Also available is the extracted features definition. 

Generating realistic background traffic was our top priority in building this dataset. We have used our proposed B-Profile system (Sharafaldin, et al. 2016) to profile the abstract behavior of human interactions and generates naturalistic benign background traffic. For this dataset, we built the abstract behaviour of 25 users based on the HTTP, HTTPS, FTP, SSH, and email protocols.

The data capturing period started at 9 a.m., Monday, July 3, 2017 and ended at 5 p.m. on Friday July 7, 2017, for a total of 5 days. Monday is the normal day and only includes the benign traffic. The implemented attacks include Brute Force FTP, Brute Force SSH, DoS, Heartbleed, Web Attack, Infiltration, Botnet and DDoS. They have been executed both morning and afternoon on Tuesday, Wednesday, Thursday and Friday.

In our recent dataset evaluation framework (Gharib et al., 2016), we have identified eleven criteria that are necessary for building a reliable benchmark dataset. None of the previous IDS datasets could cover all of the 11 criteria. In the following, we briefly outline these criteria:

Complete Network configuration: A complete network topology includes Modem, Firewall, Switches, Routers, and presence of a variety of operating systems such as Windows, Ubuntu and Mac OS X.

Complete Traffic: By having a user profiling agent and 12 different machines in Victim-Network and real attacks from the Attack-Network.

Labelled Dataset: Section 4 and Table 2 show the benign and attack labels for each day. Also, the details of the attack timing will be published on the dataset document.

Complete Interaction: As Figure 1 shows, we covered both within and between internal LAN by having two different networks and Internet communication as well.

Complete Capture: Because we used the mirror port, such as tapping system, all traffics have been captured and recorded on the storage server.

Available Protocols: Provided the presence of all common available protocols, such as HTTP, HTTPS, FTP, SSH and email protocols.

Attack Diversity: Included the most common attacks based on the 2016 McAfee report, such as Web based, Brute force, DoS, DDoS, Infiltration, Heart-bleed, Bot and Scan covered in this dataset.

Heterogeneity: Captured the network traffic from the main Switch and memory dump and system calls from all victim machines, during the attacks execution.

Feature Set: Extracted more than 80 network flow features from the generated network traffic using CICFlowMeter and delivered the network flow dataset as a CSV file. See our PCAP analyzer and CSV generator.

MetaData: Completely explained the dataset which includes the time, attacks, flows and labels in the published paper.

The full research paper outlining the details of the dataset and its underlying principles:

Iman Sharafaldin, Arash Habibi Lashkari, and Ali A. Ghorbani, “Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization”, 4th International Conference on Information Systems Security and Privacy (ICISSP), Purtogal, January 2018

Day, Date, Description, Size (GB)

Monday, Normal Activity, 11.0G
Tuesday, attacks + Normal Activity, 11G
Wednesday, attacks + Normal Activity, 13G
Thursday, attacks + Normal Activity, 7.8G
Friday, attacks + Normal Activity, 8.3G
Victim and attacker networks information
Firewall: 205.174.165.80, 172.16.0.1

DNS+ DC Server: 192.168.10.3

Outsiders (Attackers network)

Kali: 205.174.165.73
Win: 205.174.165.69, 70, 71
Insiders (Victim network)

Web server 16 Public: 192.168.10.50, 205.174.165.68
Ubuntu server 12 Public: 192.168.10.51, 205.174.165.66
Ubuntu 14.4, 32B: 192.168.10.19
Ubuntu 14.4, 64B: 192.168.10.17
Ubuntu 16.4, 32B: 192.168.10.16
Ubuntu 16.4, 64B: 192.168.10.12
Win 7 Pro, 64B: 192.168.10.9
Win 8.1, 64B: 192.168.10.5
Win Vista, 64B: 192.168.10.8
Win 10, pro 32B: 192.168.10.14
Win 10, 64B: 192.168.10.15
MAC: 192.168.10.25

Monday, July 3, 2017
Benign (Normal human activities)

Tuesday, July 4, 2017
Brute Force

FTP-Patator (9:20 – 10:20 a.m.)

SSH-Patator (14:00 – 15:00 p.m.)

Attacker: Kali, 205.174.165.73

Victim: WebServer Ubuntu, 205.174.165.68 (Local IP: 192.168.10.50)

NAT Process on Firewall:

Attack: 205.174.165.73 -> 205.174.165.80 (IP Valid Firewall) -> 172.16.0.10 -> 192.168.10.50

Reply: 192.168.10.50 -> 172.16.0.1 -> 205.174.165.80 -> 205.174.165.73

Wednesday, July 5, 2017
DoS / DDoS

DoS slowloris (9:47 – 10:10 a.m.)

DoS Slowhttptest (10:14 – 10:35 a.m.)

DoS Hulk (10:43 – 11 a.m.)

DoS GoldenEye (11:10 – 11:23 a.m.)

Attacker: Kali, 205.174.165.73

Victim: WebServer Ubuntu, 205.174.165.68 (Local IP192.168.10.50)

NAT Process on Firewall:

Attack: 205.174.165.73 -> 205.174.165.80 (IP Valid Firewall) -> 172.16.0.10 -> 192.168.10.50

Reply: 192.168.10.50 -> 172.16.0.1 -> 205.174.165.80 -> 205.174.165.73

Heartbleed Port 444 (15:12 - 15:32)

Attacker: Kali, 205.174.165.73

Victim: Ubuntu12, 205.174.165.66 (Local IP192.168.10.51)

NAT Process on Firewall:

Attack: 205.174.165.73 -> 205.174.165.80 (IP Valid Firewall) -> 172.16.0.11 -> 192.168.10.51

Reply: 192.168.10.51 -> 172.16.0.1 -> 205.174.165.80 -> 205.174.165.73

Thursday, July 6, 2017
Morning
Web Attack – Brute Force (9:20 – 10 a.m.)

Web Attack – XSS (10:15 – 10:35 a.m.)

Web Attack – Sql Injection (10:40 – 10:42 a.m.)

Attacker: Kali, 205.174.165.73

Victim: WebServer Ubuntu, 205.174.165.68 (Local IP192.168.10.50)

NAT Process on Firewall:

Attack: 205.174.165.73 -> 205.174.165.80 (IP Valid Firewall) -> 172.16.0.10 -> 192.168.10.50

Reply: 192.168.10.50 -> 172.16.0.1 -> 205.174.165.80 -> 205.174.165.73

Afternoon
Infiltration – Dropbox download

Meta exploit Win Vista (14:19 and 14:20-14:21 p.m.) and (14:33 -14:35)

Attacker: Kali, 205.174.165.73

Victim: Windows Vista, 192.168.10.8

Infiltration – Cool disk – MAC (14:53 p.m. – 15:00 p.m.)

Attacker: Kali, 205.174.165.73

Victim: MAC, 192.168.10.25

Infiltration – Dropbox download

Win Vista (15:04 – 15:45 p.m.)

First Step:

Attacker: Kali, 205.174.165.73

Victim: Windows Vista, 192.168.10.8

Second Step (Portscan + Nmap):

Attacker:Vista, 192.168.10.8

Victim: All other clients

Friday, July 7, 2017
Morning
Botnet ARES (10:02 a.m. – 11:02 a.m.)

Attacker: Kali, 205.174.165.73

Victims: Win 10, 192.168.10.15 + Win 7, 192.168.10.9 + Win 10, 192.168.10.14 + Win 8, 192.168.10.5 + Vista, 192.168.10.8

Afternoon
Port Scan:

Firewall Rule on (13:55 – 13:57, 13:58 – 14:00, 14:01 – 14:04, 14:05 – 14:07, 14:08 - 14:10, 14:11 – 14:13, 14:14 – 14:16, 14:17 – 14:19, 14:20 – 14:21, 14:22 – 14:24, 14:33 – 14:33, 14:35 - 14:35)

Firewall rules off (sS 14:51-14:53, sT 14:54-14:56, sF 14:57-14:59, sX 15:00-15:02, sN 15:03-15:05, sP 15:06-15:07, sV 15:08-15:10, sU 15:11-15:12, sO 15:13-15:15, sA 15:16-15:18, sW 15:19-15:21, sR 15:22-15:24, sL 15:25-15:25, sI 15:26-15:27, b 15:28-15:29)

Attacker: Kali, 205.174.165.73

Victim: Ubuntu16, 205.174.165.68 (Local IP: 192.168.10.50)

NAT Process on Firewall:

Attacker: 205.174.165.73 -> 205.174.165.80 (IP Valid Firewall) -> 172.16.0.1

Afternoon
DDoS LOIT (15:56 – 16:16)

Attackers: Three Win 8.1, 205.174.165.69 - 71

Victim: Ubuntu16, 205.174.165.68 (Local IP: 192.168.10.50)

NAT Process on Firewall:

Attackers: 205.174.165.69, 70, 71 -> 205.174.165.80 (IP Valid Firewall) -> 172.16.0.1

License
The CICIDS2017 dataset consists of labeled network flows, including full packet payloads in pcap format, the corresponding profiles and the labeled flows (GeneratedLabelledFlows.zip) and CSV files for machine and deep learning purpose (MachineLearningCSV.zip) are publicly available for researchers. If you are using our dataset, you should cite our related paper which outlining the details of the dataset and its underlying principles:

Iman Sharafaldin, Arash Habibi Lashkari, and Ali A. Ghorbani, “Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization”, 4th International Conference on Information Systems Security and Privacy (ICISSP), Portugal, January 2018",1998,,,,,
550,CIC_IoT_Dataset_2022,Network Intrusion Detection,Network Intrusion Detection,Network Intrusion Detection,"Graph, Image",,Computer Vision,,,https://www.unb.ca/cic/datasets/iotdataset-2022.html,https://paperswithcode.com/dataset/cic-iot-dataset-2022,"CIC IoT Dataset 2022
This project aims to generate a state-of-the-art dataset for profiling, behavioural analysis, and vulnerability testing of different IoT devices with different protocols such as IEEE 802.11, Zigbee-based and Z-Wave. The following illustrates the main objectives of the CIC-IoT dataset project:


Configure various IoT devices and analyze the behaviour exhibited.
Conduct manual and semi-automated experiments of various categories.
Further analyze the network traffic when the devices are idle for three minutes and when powered on for the first two minutes.
Generating different scenarios and analyzing the devices' behaviour in different situations.
Conducting and capturing the network terrific of devices undercurrent and important attacks in IoT environment.

Current CIC IoT dataset project and activities around it can be summarized in the following steps:

Network configuration
Our lab network configuration was configured with a 64-bit Window machine with two network interface cards - one is connected to the network gateway, and the other is connected to an unmanaged network switch. Simultaneously, Wireshark, the open-source network protocol analyzer, listens to both interfaces, captures and saves the output packet captured (pcap) files. Hence, IoT devices that require an Ethernet connection are connected to this switch. Additionally, a smart automation hub, Vera Plus is also connected to the unmanaged switch, which creates our wireless IoT environment to serve IoT devices compatible with Wi-Fi, ZigBee, Z-Wave and Bluetooth.



Dataset
For collecting the data, we captured the network traffic of the IoT devices coming through the gateway using Wireshark and dumpcap in six different types of experiments. The former was used for manual experiments, while the latter was used for semi-automated ones. All the experiments can be organized as follows:



Power:  In this experiment, we powered on all the devices in our lab individually and started a network traffic capture in isolation.



Idle:  In this experiment, we captured the whole network traffic from late in the evening to early in the morning, which we call idle time. In this period, the whole lab was completely evacuated and there were no human interactions involved.



Interactions:  In this experiment, all possible functionality on IoT devices has been extracted and the corresponding network activity and transmitted packets for each functionality/activity have been captured.



Scenarios:  In these experiments, we conducted six different types of scenario experiments using a combination of devices as simulations of the network activity inside a smart home. These experiments were done to see how devices behave while interacting with each other simultaneously.



Active:  In addition to the idle time, the whole network communications were also captured throughout the day. All fellow researchers during this period were allowed to enter the lab whenever they wanted. They might interact with devices and generate network traffic either passively or actively.



Attacks:  In this experiment, we performed two different attacks, Flood and RTSP- Brute Force, on some of our devices and captured their attack network traffic.



Case study – device identification
After generating the dataset, we performed a case study on the idea of transferability – training datasets in our lab and transferring the trained model to another lab for testing. We conducted 20 different experiments based on the number of sampled devices from the United States lab.

Forty-eight features were extracted from both the training dataset from our lab and the testing dataset from the other lab. Three classes of device types were used in this experiment: Audio, Camera and Home Automation. However, no labels were required for the test dataset since that was what was to be predicted but the training dataset required labels.

After training, the model is transferred to the other lab for testing on each device to predict the class of the device in question. For example, if Amazon Echo Dot is tested on the trained model, the classifier should be able to predict this device as belonging to device type Audio. How this works is by counting the prediction of the classifier based on the features for each device type. The device type with the highest count is predicted as the class for the device in question.

Dataset directory
The main dataset directory (CIC IoT Dataset) contains six subdirectories related to each experiment, namely:



Power:  In this directory, you will find the power experiment packet captures for each device, categorized by different device classes.



Idle:  In this directory, you will find idle experiment packet captures for 30 days, named and sorted by date.



Interactions:  In this directory, you will find the interactions experiments packet captures for each device, categorized by different device classes. Each interaction includes three packet captures.



Scenarios:  In this directory, you will find six sub-directories, each of which is related to one scenario. Each scenario includes three packet captures.



Active:  In this directory, you will find active experiment packet captures for 30 days, named and sorted by date.



Attacks:  In this directory, you will find two sub-directories, Flood and RTSP BruteForce, each for a specific attack performed on a few devices. The latter was performed using two different tools, Hydra and Nmap. Each attack includes three packet captures per device.



Contributing
The project is not currently in development, but any contribution is welcome. Please contact one of the authors of the paper.

Acknowledgments
The authors would like to thank the Canadian Institute for Cybersecurity for its financial and educational support.

Citation
Sajjad Dadkhah, Hassan Mahdikhani, Priscilla Kyei Danso, Alireza Zohourian, Kevin Anh Truong, Ali A. Ghorbani, ""Towards the development of a realistic multidimensional IoT profiling dataset"", Submitted to: The 19th Annual International Conference on Privacy, Security & Trust (PST2022) August 22-24, 2022, Fredericton, Canada.",2022,,,,,
551,CID,Edge Detection,Edge Detection,Edge Detection,Image,,Computer Vision,edge-detection-on-cid,,https://github.com/505030475/ExtremeLowLight,https://paperswithcode.com/dataset/cid,"The CID (Campus Image Dataset) is a dataset captured in low-light env with the help of Android programming. Its basic unit is group, which is named by capture time and contains 8 exposure-time-varying raw image shot in a burst.",,,,,,
552,CIDAR,Instruction Following,Instruction Following,Instruction Following,,,Methodology,,CC BY-NC 4.0,https://huggingface.co/datasets/arbml/CIDAR,https://paperswithcode.com/dataset/cidar,"CIDAR contains 10,000 instructions and their output. The dataset was created by selecting around 9,109 samples from Alpagasus dataset then translating it to Arabic using ChatGPT. In addition, we append that with around 891 Arabic grammar instructions from the webiste Ask the teacher. All the 10,000 samples were reviewed by around 12 reviewers.",,,,109 samples,,
553,CIDII_Dataset,Misinformation,Misinformation,"Misinformation, Rumour Detection, Satire Detection, Fake News Detection, Fraud Detection, News Classification",Image,,Computer Vision,,,https://drive.google.com/file/d/14VkwRvWrH-CX3EYb57z0qnJlhjHUwbbz/view?usp=sharing,https://paperswithcode.com/dataset/cidii-dataset,"The CIDII dataset is a binary classification, consisting of two classes of correct information and disinformation related to Islamic issues. The CIDII dataset belongs to our research (DISINFORMATION DETECTION ABOUT ISLAMIC ISSUES ON SOCIAL MEDIA USING DEEP LEARNING TECHNIQUES) published in MJCS journal in the link below: https://ejournal.um.edu.my/index.php/MJCS/article/view/41935

This dataset consists of five columns:

1- ID: Each article has a unique ID.

2- Article: The article contains text that is either facts related to Islamic issues if the information is correct, or posts targeting the Islamic religion if the information is false. Most posts contain only the body without a title.

3- Propagation Source: The source refers to the source of the article content, as it contains a Facebook link in the event that the post is disinformation, or it contains a link to Islamic websites in the event that the article refers to correct information (an explanation of a verse, a hadith, or an article related to the Islamic religion).

4- Article Type: This column contains the type of article published. Is it a post if the article is disinformation, or is it an Islamic article, a Quranic interpretation, or a hadith, in case of that the information is correct?

5- Class Type: This column shows whether the article belongs to the category of correct information or disinformation.",,,,,,
554,CIFAR-10,Object Recognition,Object Recognition,"Object Recognition, Active Learning, Long-tail Learning on CIFAR-10-LT (ρ=100), Neural Architecture Search, Data Augmentation, Dataset Distillation - 1IPC, Online Clustering, Sequential Image Classification, Contrastive Learning, Image Clustering, Small Data Image Classification, Image Classification with Label Noise, Image Retrieval, Unsupervised Image Classification, Supervised Image Retrieval, Provable Adversarial Defense, Partial Label Learning, Self-Supervised Learning, Transductive Zero-Shot Classification, Semi-Supervised Image Classification (Cold Start), Sparse Learning and binarization, Neural Network Compression, Model Poisoning, Network Pruning, Image Classification, Image Compression, Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly, Image Classification with Human Noise, Quantization, Adversarial Attack, Semi-Supervised Image Classification, Clean-label Backdoor Attack (0.05%), Hard-label Attack, Classification, Long-tail Learning, Novel Class Discovery, Graph Classification, Density Estimation, Out-of-Distribution Detection, Adversarial Defense, Binarization, Anomaly Detection, Personalized Federated Learning, Image Generation, Stochastic Optimization, Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly, Nature-Inspired Optimization Algorithm, Classification with Binary Weight Network, Domain-IL Continual Learning, Zero-Shot Learning, Learning with noisy labels, Classification with Binary Neural Network, ROLSSL-Reversed, Continual Learning, Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly, Robust classification, Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly, ROLSSL-Consistent, Adversarial Robustness, Open-World Semi-Supervised Learning, Conditional Image Generation, ROLSSL-Uniform, Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly","Audio, Graph, Image, Text",English,Computer Vision,"image-compression-on-cifar-10, rolssl-uniform-on-cifar-10, conditional-image-generation-on-cifar-10, active-learning-on-cifar10-10000, long-tail-learning-on-cifar-10-lt-r-10, anomaly-detection-on-cifar-10, image-classification-with-label-noise-on-9, semi-supervised-image-classification-cold, image-classification-on-cifar-10-40-symmetric, image-classification-on-cifar10-1, provable-adversarial-defense-on-cifar-10, stochastic-optimization-on-cifar-10-resnet-18, dataset-distillation-1ipc-on-cifar-10, adversarial-attack-on-cifar-10, semi-supervised-image-classification-cold-2, image-classification-on-cifar-10, contrastive-learning-on-cifar-10, semi-supervised-image-classification-on-cifar-11, semi-supervised-image-classification-on-cifar-12, small-data-image-classification-on-cifar10-10, network-pruning-on-cifar-10, rolssl-consistent-on-cifar-10, semi-supervised-image-classification-on-cifar-28, semi-supervised-image-classification-on-cifar-16, out-of-distribution-detection-on-cifar10-1, adversarial-defense-on-cifar-10, learning-with-noisy-labels-on-cifar-10, domain-il-continual-learning-on-cifar10-5, zero-shot-learning-on-cifar-10, partial-label-learning-on-cifar-10-partial-1, image-classification-with-label-noise-on-5, data-augmentation-on-cifar-10, small-data-image-classification-on-cifar-10-3, online-clustering-on-cifar10, self-supervised-learning-on-cifar-10, density-estimation-on-cifar-10, novel-class-discovery-on-cifar10, classification-with-binary-neural-network-on, partial-label-learning-on-cifar-10-partial-2, self-supervised-learning-on-cifar10, neural-architecture-search-on-cifar-10, continual-learning-on-split-cifar-10-5-tasks, image-classification-on-cifar-10-with-noisy, long-tail-learning-on-cifar-10-lt-r-100-on, image-clustering-on-cifar-10, graph-classification-on-cifar10-100k, transductive-zero-shot-classification-on-7, semi-supervised-image-classification-on-cifar-10, image-retrieval-on-cifar-10, unsupervised-anomaly-detection-with-specified-16, semi-supervised-image-classification-on-cifar-17, unsupervised-anomaly-detection-with-specified-9, image-classification-with-human-noise-on, anomaly-detection-on-leave-one-class-out, density-estimation-on-cifar-10-conditional, quantization-on-cifar-10, image-classification-on-cifar-104000, image-generation-on-cifar-10, hard-label-attack-on-cifar-10, semi-supervised-image-classification-on-cifar-7, neural-architecture-search-on-nats-bench-size-1, architecture-search-on-cifar-10-image, sparse-learning-and-binarization-on-cifar-10, image-classification-with-label-noise-on-7, image-generation-on-cifar-10-20-data, unsupervised-anomaly-detection-with-specified-7, image-classification-with-label-noise-on-6, binarization-on-cifar-10, out-of-distribution-detection-on-cifar-10-vs, semi-supervised-image-classification-cold-8, image-classification-with-label-noise-on-8, semi-supervised-image-classification-on-cifar-15, graph-classification-on-cifar-10, long-tail-learning-on-cifar-10-lt-r-100, image-generation-on-cifar-10-10-data, robust-classification-on-cifar-10, semi-supervised-image-classification-on-cifar-27, unsupervised-anomaly-detection-with-specified-21, adversarial-robustness-on-cifar-10, model-poisoning-on-cifar-10, neural-network-compression-on-cifar-10, out-of-distribution-detection-on-cifar-10, unsupervised-image-classification-on-cifar-10, small-data-image-classification-on-cifar-10-2, image-classification-on-cifar-10-image, open-world-semi-supervised-learning-on-cifar, anomaly-detection-on-one-class-cifar-10, classification-on-cifar10-1, stochastic-optimization-on-cifar-10-wrn-28-10, clean-label-backdoor-attack-0-05-on-cifar-10, image-classification-with-label-noise-on-4, personalized-federated-learning-on-cifar-10, sequential-image-classification-on-noise, nature-inspired-optimization-algorithm-on-1, supervised-image-retrieval-on-cifar-10, unsupervised-anomaly-detection-with-specified-6, small-data-image-classification-on-cifar-10-1, semi-supervised-image-classification-on-cifar-6, image-classification-with-label-noise-on-14, small-data-image-classification-on-cifar-10, image-classification-on-cifar-10-60-symmetric, rolssl-reversed-on-cifar-10, semi-supervised-image-classification-on-3, stochastic-optimization-on-cifar-10, partial-label-learning-on-cifar-10-partial, semi-supervised-image-classification-on-cifar, image-classification-with-label-noise-on-3, classification-with-binary-weight-network-on",,https://www.cs.toronto.edu/~kriz/cifar.html,https://paperswithcode.com/dataset/cifar-10,"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.",,Learning multiple layers of features from tiny images,https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf,6000 images,training images and 10000 test images,10
555,CIFAR-100,Variational Inference,Variational Inference,"Variational Inference, class-incremental learning, Neural Architecture Search, Dataset Distillation - 1IPC, Few-Shot Class-Incremental Learning, Bayesian Inference, Image Clustering, Classifier calibration, Small Data Image Classification, Image Classification with Label Noise, Data Free Quantization, Class Incremental Learning, Provable Adversarial Defense, Sparse Learning and binarization, Self-Supervised Learning, Transductive Zero-Shot Classification, Network Pruning, Image Classification, Adversarial Attack, Few-Shot Image Classification, Semi-Supervised Image Classification, Classification, Long-tail Learning, Novel Class Discovery, Out-of-Distribution Detection, Adversarial Defense, Binarization, Anomaly Detection, Personalized Federated Learning, Image Generation, Stochastic Optimization, Learning with coarse labels, Classification with Binary Weight Network, Zero-Shot Learning, Learning with noisy labels, Classification with Binary Neural Network, Non-exemplar-based Class Incremental Learning, Continual Learning, Knowledge Distillation, Adversarial Robustness, Incremental Learning, Open-World Semi-Supervised Learning, Conditional Image Generation","Audio, Graph, Image, Text",English,Computer Vision,"sparse-learning-and-binarization-on-cifar-100, conditional-image-generation-on-cifar-100, knowledge-distillation-on-cifar-100, class-incremental-learning-on-cifar-100-50-2, classification-with-binary-neural-network-on-2, long-tail-learning-on-cifar-100-lt-r-100, adversarial-robustness-on-cifar-100, incremental-learning-on-cifar-100-50-classes-3, anomaly-detection-on-unlabeled-cifar-10-vs, image-classification-with-label-noise-on-15, image-clustering-on-cifar-100, out-of-distribution-detection-on-cifar100, image-classification-on-cifar100, learning-with-noisy-labels-on-cifar-100, anomaly-detection-on-one-class-cifar-100, few-shot-image-classification-on-cifar100-5, dataset-distillation-1ipc-on-cifar-100, novel-class-discovery-on-cifar100, classifier-calibration-on-cifar-100, semi-supervised-image-classification-on-cifar-3, small-data-on-cifar-100-1000-labels-1, stochastic-optimization-on-cifar-100, class-incremental-learning-on-cifar-100-50-1, zero-shot-learning-on-cifar-100, classification-on-cifar-100, classification-on-cifar100, semi-supervised-image-classification-on-cifar-9, bayesian-inference-on-cifar100, continual-learning-on-cifar100-20-tasks, adversarial-attack-on-cifar-100, personalized-federated-learning-on-cifar-100, classification-with-binary-weight-network-on-2, variational-inference-on-cifar100, transductive-zero-shot-classification-on-8, incremental-learning-on-cifar-100-50-classes, learning-with-coarse-labels-on-cifar100, open-world-semi-supervised-learning-on-cifar-1, incremental-learning-on-cifar-100-50-classes-2, provable-adversarial-defense-on-cifar-100, semi-supervised-image-classification-on-cifar-8, binarization-on-cifar-100, semi-supervised-image-classification-on-cifar-2, adversarial-defense-on-cifar-100, image-generation-on-cifar-100, out-of-distribution-detection-on-cifar-100, class-incremental-learning-on-cifar100-1, class-incremental-learning-on-cifar100, data-free-quantization-on-cifar-100, few-shot-class-incremental-learning-on-cifar, neural-architecture-search-on-cifar-100-1, semi-supervised-image-classification-on-cifar-4, non-exemplar-based-class-incremental-learning, self-supervised-learning-on-cifar100, long-tail-learning-on-cifar-100-lt-r-10, incremental-learning-on-cifar-100-50-classes-1, self-supervised-learning-on-cifar-100, network-pruning-on-cifar-100, image-classification-on-cifar-100",,https://www.cs.toronto.edu/~kriz/cifar.html,https://paperswithcode.com/dataset/cifar-100,"The CIFAR-100 dataset (Canadian Institute for Advanced Research, 100 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. There are 600 images per class. Each image comes with a ""fine"" label (the class to which it belongs) and a ""coarse"" label (the superclass to which it belongs). There are 500 training images and 100 testing images per class.

The criteria for deciding whether an image belongs to a class were as follows:


The class name should be high on the list of likely answers to the question “What is in this picture?”
The image should be photo-realistic. Labelers were instructed to reject line drawings.
The image should contain only one prominent instance of the object to which the class refers.
The object may be partially occluded or seen from an unusual viewpoint as long as its identity is still clear to the labeler.",,Learning multiple layers of features from tiny images,https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf,600 images,training images and 100 testing images,100
556,CIFAR-100N,Image Classification,Image Classification,"Image Classification, Learning with noisy labels",Image,,Computer Vision,learning-with-noisy-labels-on-cifar-100n,,https://github.com/UCSC-REAL/cifar-10-100n,https://paperswithcode.com/dataset/cifar-100n,"This work presents two new benchmark datasets (CIFAR-10N, CIFAR-100N), equipping the training dataset of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels that we collect from Amazon Mechanical Turk.",,,,,,
557,CIFAR-10C,Classification,Classification,"Classification, Domain Generalization",Image,,Computer Vision,"domain-generalization-on-cifar-10c, classification-on-cifar-10c",Apache-2.0,https://github.com/hendrycks/robustness,https://paperswithcode.com/dataset/cifar-10c,Common corruptions dataset for CIFAR10,,,,,,
558,CIFAR-10N,Image Classification,Image Classification,"Image Classification, Learning with noisy labels",Image,,Computer Vision,"learning-with-noisy-labels-on-cifar-10n-3, learning-with-noisy-labels-on-cifar-10n-1, learning-with-noisy-labels-on-cifar-10n-4, learning-with-noisy-labels-on-cifar-10n-2, learning-with-noisy-labels-on-cifar-10n-worst, learning-with-noisy-labels-on-cifar-10n",,https://github.com/UCSC-REAL/cifar-10-100n,https://paperswithcode.com/dataset/cifar-10n,"This work presents two new benchmark datasets (CIFAR-10N, CIFAR-100N), equipping the training dataset of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels that we collect from Amazon Mechanical Turk.",,,,,,
559,CIFAR10-DVS,Object Recognition,Object Recognition,"Object Recognition, Event data classification",Image,,Computer Vision,"object-recognition-on-cifar10-dvs, event-data-classification-on-cifar10-dvs-1",,https://figshare.com/articles/CIFAR10-DVS_New/4724671/2,https://paperswithcode.com/dataset/cifar10-dvs,"CIFAR10-DVS is an event-stream dataset for object classification. 10,000 frame-based images that come from CIFAR-10 dataset are converted into 10,000 event streams with an event-based sensor, whose resolution is 128×128 pixels. The dataset has an intermediate difficulty with 10 different classes. The repeated closed-loop smooth (RCLS) movement of frame-based images is adopted to implement the conversion. Due to the transformation, they produce rich local intensity changes in continuous time which are quantized by each pixel of the event-based camera.",,Structure-Aware Network for Lane Marker Extraction with Dynamic Vision Sensor,https://arxiv.org/abs/2008.06204,,,
560,Cifar10Mnist,Multi-Task Learning,Multi-Task Learning,"Multi-Task Learning, Adversarial Attack",,,Methodology,,MIT,,https://paperswithcode.com/dataset/cifar10mnist,"The Cifar10Mnist dataset is created using CIFAR-10 and MNIST data sources. 
Since the CIFAR-10 training set consists of 50000 images and the MNIST training set contains 60000 digits, the first 50000 digits from MNIST are padded on top of the CIFAR-10 images after making them slightly translucent. A first training dataset is then obtained (50000 images). Furthermore, the remaining 10000 MNIST digits are padded on top of 10000 random CIFAR10 images (with a fixed seed). This gives the possibility of having a second training dataset of  60000 images. 
For the test set, the 10000 CIFAR-10 images are padded over the 10000 MNIST digits.",,,,50000 images,training set consists of 50000 images,
561,CINIC-10,Neural Architecture Search,Neural Architecture Search,"Neural Architecture Search, Image Classification, Sparse Learning",Image,,Computer Vision,"neural-architecture-search-on-cinic-10, image-classification-on-cinic-10, sparse-learning-on-cinic-10-1",Custom,https://github.com/BayesWatch/cinic-10,https://paperswithcode.com/dataset/cinic-10,"CINIC-10 is a dataset for image classification. It has a total of 270,000 images, 4.5 times that of CIFAR-10. It is constructed from two different sources: ImageNet and CIFAR-10. Specifically, it was compiled as a bridge between CIFAR-10 and ImageNet. It is split into three equal subsets - train, validation, and test - each of which contain 90,000 images.",,Group Knowledge Transfer:Collaborative Training of Large CNNs on the Edge,https://arxiv.org/abs/2007.14513,000 images,"split into three equal subsets - train, validation, and test - each of which contain 90,000 images",
562,CIRCO,Composed Image Retrieval (CoIR),Composed Image Retrieval (CoIR),"Composed Image Retrieval (CoIR), Image Retrieval, Zero-Shot Composed Image Retrieval (ZS-CIR)",Image,,Computer Vision,zero-shot-composed-image-retrieval-zs-cir-on,Creative Commons BY-NC 4.0,https://github.com/miccunifi/CIRCO,https://paperswithcode.com/dataset/circo,"CIRCO (Composed Image Retrieval on Common Objects in context) is an open-domain benchmarking dataset for Composed Image Retrieval (CIR) based on real-world images from COCO 2017 unlabeled set. It is the first CIR dataset with multiple ground truths and aims to address the problem of false negatives in existing datasets. CIRCO comprises a total of 1020 queries, randomly divided into 220 and 800 for the validation and test set, respectively, with an average of 4.53 ground truths per query.",2017,Zero-Shot Composed Image Retrieval with Textual Inversion,https://arxiv.org/pdf/2303.15247v1.pdf,,,
563,CirCor_DigiScope,Predict clinical outcome,Predict clinical outcome,"Predict clinical outcome, Classify murmurs",,,Methodology,"classify-murmurs-on-circor-digiscope, predict-clinical-outcome-on-circor-digiscope",,https://arxiv.org/pdf/2108.00813v1.pdf,https://paperswithcode.com/dataset/circor-digiscope,"CirCor DigiScope is currently the largest pediatric heart sound dataset. A total of 5282 recordings have been collected from the four main auscultation locations of 1568 patients, in the process 215780 heart sounds have been manually annotated. Each cardiac murmur has been manually annotated by an expert annotator according to its timing, shape, pitch, grading and quality.",,https://arxiv.org/pdf/2108.00813v1.pdf,https://arxiv.org/pdf/2108.00813v1.pdf,,,
564,CIRR,Composed Image Retrieval (CoIR),Composed Image Retrieval (CoIR),"Composed Image Retrieval (CoIR), Image Retrieval, Zero-Shot Composed Image Retrieval (ZS-CIR)",Image,,Computer Vision,"image-retrieval-on-cirr, composed-image-retrieval-coir-on-cirr-1, zero-shot-composed-image-retrieval-zs-cir-on-1",MIT,https://cuberick-orion.github.io/CIRR/,https://paperswithcode.com/dataset/cirr,"Composed Image Retrieval (or, Image Retreival conditioned on Language Feedback) is a relatively new retrieval task, where an input query consists of an image and short textual description of how to modify the image. 

For humans, the advantage of a bi-modal query is clear: some concepts and attributes are more succinctly described visually, others through language. By cross-referencing the two modalities, a reference image can capture the general gist of a scene, while the text can specify finer details.

We identify a major challenge of this task as the inherent ambiguity in knowing what information is important (typically one object of interest in the scene) and what can be ignored (e.g., the background and other irrelevant objects).

We release the first dataset of open-domain, real-life images with human-generated modification sentences, which support research on one-shot composed image retrieval, dialogue systems, fine-grained visiolinguistic reasoning, and more.",,,,,,
565,CISOL,Object Detection,Object Detection,"Object Detection, Table Detection, Table Recognition","Image, Tabular",,Computer Vision,"object-detection-on-cisol-track-b-tsr-only, object-detection-on-cisol-track-a-td-tsr",Creative Commons Attribution 4.0 International,https://doi.org/10.5281/zenodo.10829550,https://paperswithcode.com/dataset/cisol,"The Construction Industry Steel Ordering Lists (CISOL) dataset comprises table-centric, real-world documents from the construction industry, annotated to facilitate the testing and training of deep learning models for table detection (TD) and table structure recognition (TSR). 

Access to the CISOL Leaderboard is provided via https://eval.ai/web/challenges/challenge-page/2257",,,,,,
566,Citeseer,Graph Classification,Graph Classification,"Graph Classification, Node Clustering, Graph Clustering, Link Prediction, Community Detection, Node Classification","Graph, Image, Time Series",,Computer Vision,"node-classification-on-citeseer-05, community-detection-on-citeseer, node-clustering-on-citeseer, link-prediction-on-citeseer-biased-evaluation, node-classification-on-citeseer, node-classification-on-citeseer-random, graph-clustering-on-citeseer, node-classification-on-citeseer-with-public, node-classification-on-citeseer-1, link-prediction-on-citeseer-nonstandard, node-classification-on-citeseer-with-public-1, graph-classification-on-citeseer, node-classification-on-citeseer-full, link-prediction-on-citeseer",,https://linqs.soe.ucsc.edu/data,https://paperswithcode.com/dataset/citeseer,The CiteSeer dataset consists of 3312 scientific publications classified into one of six classes. The citation network consists of 4732 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 3703 unique words.,,,,,,
567,CiteSum,Extreme Summarization,Extreme Summarization,Extreme Summarization,Text,English,Natural Language Processing,extreme-summarization-on-citesum,,https://github.com/morningmoni/CiteSum,https://paperswithcode.com/dataset/citesum,CiteSum is a large-scale scientific extreme summarization benchmark.,,,,,,
568,CITR___DUT,Autonomous Vehicles,Autonomous Vehicles,"Autonomous Vehicles, motion prediction","Time Series, Video",,Methodology,,,https://github.com/dongfang-steven-yang/vci-dataset-dut,https://paperswithcode.com/dataset/vci-dut,"Consists of two pedestrian trajectory datasets, CITR dataset and DUT dataset, so that the pedestrian motion models can be further calibrated and verified, especially when vehicle influence on pedestrians plays an important role. 

CITR dataset consists of experimentally designed fundamental VCI scenarios (front, back, and lateral VCIs) and provides unique ID for each pedestrian, which is suitable for exploring a specific aspect of VCI.

DUT dataset gives two ordinary and natural VCI scenarios in crowded university campus, which can be used for more general purpose VCI exploration.",,,,,,
569,CityFlow,Vehicle Re-Identification,Vehicle Re-Identification,"Vehicle Re-Identification, Multi-agent Reinforcement Learning, Person Re-Identification, Attribute",Image,,Computer Vision,"vehicle-re-identification-on-cityflow, attribute-on-cityflow",Custom,https://www.aicitychallenge.org/,https://paperswithcode.com/dataset/cityflow,"CityFlow is a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. The dataset contains more than 200K annotated bounding boxes covering a wide range of scenes, viewing angles, vehicle models, and urban traffic flow conditions. 

Camera geometry and calibration information are provided to aid spatio-temporal analysis. In addition, a subset of the benchmark is made available for the task of image-based vehicle re-identification (ReID).",,Custom,http://www.aicitychallenge.org/wp-content/uploads/2021/01/DataLicenseAgreement_AICityChallenge_2021.pdf,,,
570,Cityscapes,Video Prediction,Video Prediction,"Video Prediction, Scene Parsing, Domain 1-1, Monocular Depth Estimation, Weakly-Supervised Semantic Segmentation, Video Semantic Segmentation, Open Vocabulary Semantic Segmentation, Domain 11-5, 2D Semantic Segmentation, Overlapped 10-1, Instance Segmentation, Image-to-Image Translation, Depth Estimation, Robust Object Detection, Interactive Segmentation, Federated Learning, Edge Detection, Semantic Segmentation, Unsupervised Semantic Segmentation with Language-image Pre-training, Domain 11-1, Overlapped 14-1, Domain Generalization, Unsupervised Monocular Depth Estimation, Image Generation, Multi-Task Learning, Real-Time Semantic Segmentation, Panoptic Segmentation, Real-time Instance Segmentation, Semi-Supervised Semantic Segmentation, Unsupervised Semantic Segmentation","3D, Image, Text, Time Series, Video",English,Computer Vision,"unsupervised-semantic-segmentation-on, domain-11-1-on-cityscapes-val, image-to-image-translation-on-cityscapes, unsupervised-monocular-depth-estimation-on, interactive-segmentation-on-cityscapes-val, semantic-segmentation-on-cityscapes-2, video-semantic-segmentation-on-cityscapes-val, panoptic-segmentation-on-cityscapes-val, domain-11-5-on-cityscapes, image-generation-on-cityscapes-25k-256x512, image-generation-on-cityscapes-5k-256x512, panoptic-segmentation-on-cityscapes-test, domain-1-1-on-cityscapes, instance-segmentation-on-cityscapes, unsupervised-semantic-segmentation-on-1, video-prediction-on-cityscapes-1, federated-learning-on-cityscapes, overlapped-14-1-on-cityscapes, semi-supervised-semantic-segmentation-on-18, overlapped-10-1-on-cityscapes, semi-supervised-semantic-segmentation-on-1, open-vocabulary-semantic-segmentation-on, robust-object-detection-on-cityscapes, semi-supervised-semantic-segmentation-on-19, weakly-supervised-semantic-segmentation-on-17, domain-11-1-on-cityscapes, real-time-semantic-segmentation-on-cityscapes-1, edge-detection-on-cityscapes, scene-parsing-on-cityscapes-test, domain-11-5-on-cityscapes-val, semantic-segmentation-on-cityscapes, semi-supervised-semantic-segmentation-on-35, real-time-semantic-segmentation-on-cityscapes-3, instance-segmentation-on-cityscapes-val, multi-task-learning-on-cityscapes, video-prediction-on-cityscapes-128x128, real-time-semantic-segmentation-on-cityscapes, 2d-semantic-segmentation-on-cityscapes-val, semi-supervised-semantic-segmentation-on-2, weakly-supervised-semantic-segmentation-on-16, semi-supervised-semantic-segmentation-on-40, semi-supervised-semantic-segmentation-on-22, semi-supervised-semantic-segmentation-on-3, real-time-instance-segmentation-on-cityscapes, semi-supervised-semantic-segmentation-on-43, semi-supervised-semantic-segmentation-on-8, image-generation-on-cityscapes, domain-1-1-on-cityscapes-val, depth-estimation-on-cityscapes-test, unsupervised-semantic-segmentation-with-3, semantic-segmentation-on-cityscapes-val, image-to-image-translation-on-cityscapes-1, monocular-depth-estimation-on-cityscapes, robust-object-detection-on-cityscapes-1",Custom,https://www.cityscapes-dataset.com/dataset-overview/,https://paperswithcode.com/dataset/cityscapes,"Cityscapes is a large-scale database which focuses on semantic understanding of urban street scenes. It provides semantic, instance-wise, and dense pixel annotations for 30 classes grouped into 8 categories (flat surfaces, humans, vehicles, constructions, objects, nature, sky, and void). The dataset consists of around 5000 fine annotated images and 20000 coarse annotated ones. Data was captured in 50 cities during several months, daytimes, and good weather conditions. It was originally recorded as video so the frames were manually selected to have the following features: large number of dynamic objects, varying scene layout, and varying background.",,A Review on Deep Learning Techniques Applied to Semantic Segmentation,https://arxiv.org/abs/1704.06857,,,30
571,CityStreet,Crowd Counting,Crowd Counting,"Crowd Counting, Multiview Detection",Image,,Computer Vision,multiview-detection-on-citystreet,,http://visal.cs.cityu.edu.hk/downloads/citystreetdata/,https://paperswithcode.com/dataset/citystreet,"Datasets for multi-view crowd counting in wide-area scenes. Includes our CityStreet dataset, as well as the counting and metadata for multi-view counting on PETS2009 and DukeMTMC.
CityStreet is a real-world city scene dataset collected around the intersection of a crowded street. The scene size of the dataset is around 58m×72m. The ground plane map resolution is 320×384.",,,,,,
572,Civil_Comments,Toxic Comment Classification,Toxic Comment Classification,"Toxic Comment Classification, Hate Speech Detection","Audio, Image",,Computer Vision,toxic-comment-classification-on-civil,Public domain (CC0),https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification,https://paperswithcode.com/dataset/civil-comments,"At the end of 2017 the Civil Comments platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.

In the data supplied for this competition, the text of the individual comment is found in the comment_text column. Each comment in Train has a toxicity label (target), and models should predict the target toxicity for the Test data. This attribute (and all others) are fractional values which represent the fraction of human raters who believed the attribute applied to the given comment.

The data also has several additional toxicity subtype attributes. Models do not need to predict these attributes for the competition, they are included as an additional avenue for research. Subtype attributes are:


severe_toxicity
obscene
threat
insult
identity_attack
sexual_explicit

Additionally, a subset of comments have been labelled with a variety of identity attributes, representing the identities that are mentioned in the comment. The columns corresponding to identity attributes are listed below. Only identities with more than 500 examples in the test set (combined public and private) will be included in the evaluation calculation. These identities are shown in bold.


male
female
transgender
other_gender
heterosexual
homosexual_gay_or_lesbian
bisexual
other_sexual_orientation
christian
jewish
muslim
hindu
buddhist
atheist
other_religion
black
white
asian
latino
other_race_or_ethnicity
physical_disability
intellectual_or_learning_disability
psychiatric_or_mental_illness
other_disability",2017,,,500 examples,,
573,CivRealm,Multi-agent Reinforcement Learning,Multi-agent Reinforcement Learning,"Multi-agent Reinforcement Learning, Reinforcement Learning (RL)",,,Reinforcement Learning,,,,https://paperswithcode.com/dataset/civrealm,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
574,CK_,Facial Expression Recognition (FER),Facial Expression Recognition (FER),"Facial Expression Recognition (FER), Face Verification",Image,,Computer Vision,"face-verification-on-ck, facial-expression-recognition-on-ck",Custom (non-commercial),http://www.jeffcohn.net/Resources/,https://paperswithcode.com/dataset/ck,"The Extended Cohn-Kanade (CK+) dataset contains 593 video sequences from a total of 123 different subjects, ranging from 18 to 50 years of age with a variety of genders and heritage. Each video shows a facial shift from the neutral expression to a targeted peak expression, recorded at 30 frames per second (FPS) with a resolution of either 640x490 or 640x480 pixels. Out of these videos, 327 are labelled with one of seven expression classes: anger, contempt, disgust, fear, happiness, sadness, and surprise. The CK+ database is widely regarded as the most extensively used laboratory-controlled facial expression classification database available, and is used in the majority of facial expression classification methods.",,EmotionNet Nano: An Efficient Deep Convolutional Neural Network Design for Real-time Facial Expression Recognition,https://arxiv.org/abs/2006.15759,,,
575,ClassEval,Code Generation,Code Generation,Code Generation,Text,English,Natural Language Processing,,,https://github.com/FudanSELab/ClassEval,https://paperswithcode.com/dataset/classeval,"In this work, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e. class-level code generation. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on it, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we have the following main findings. First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior than other LLMs on class-level code generation, and the second-tier models includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very similar performance. Third, we find that generating the entire class all at once (i.e. holistic generation strategy) is the best generation strategy only for GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental and compositional) is better strategies for the other models with limited ability of understanding long instructions and utilizing the middle information. Lastly, we find the limited model ability of generating method-dependent code and discuss the frequent error types in generated classes.",,,,,,
576,classification_benchmark,Transferability,Transferability,Transferability,,,Methodology,transferability-on-classification-benchmark,,,https://paperswithcode.com/dataset/classification-benchmark,"This benchmark includes 11 image classification datasets that were used to evaluate the transferability of metrics. Datasets include FGVC Aircraft, Caltech101, Stanford Cars, CIFAR-10, CIFAR-100, DTD, Oxford-102, Flowers, Food-101, Oxford-IIIT Pets, SUN397, and VOC2007 . Please refer to SFDA (https://github.com/TencentARC/SFDA) or ETran (https://github.com/mgholamikn/ETran/tree/main) for further details about the benchmark.",,,,,,
577,Cleaned_Lang8,Grammatical Error Detection,Grammatical Error Detection,Grammatical Error Detection,Image,,Computer Vision,,,https://bekushal.medium.com/cleaned-lang8-dataset-for-grammar-error-detection-79aaa31150aa,https://paperswithcode.com/dataset/cleaned-lang8,"Lang-8 Preprocessed Dataset (for GED):


Dataset: Lang-8, a publicly available dataset containing user-generated content, primarily from second-language learners, focused on writing errors.
Task: Grammatical Error Detection (GED).
Size: 200,000 sentences, with each sentence labeled as '0' for incorrect and '1' for its corrected version.
Preprocessing: The dataset has been cleaned and transformed to improve model performance, including removing noise, handling inconsistent annotations, and preparing it for training.
Usage: Ideal for training and evaluating models for GED and GEC.",,,,000 sentences,,
578,Clear_Weather,Object Detection,Object Detection,"Object Detection, 2D Object Detection, 3D Object Detection","3D, Image",,Computer Vision,"2d-object-detection-on-clear-weather, 3d-object-detection-on-clear-weather",Custom,https://www.uni-ulm.de/en/in/driveu/projects/dense-datasets#c811669,https://paperswithcode.com/dataset/clear-weather,"We introduce an object detection dataset in challenging adverse weather conditions covering 12000 samples in real-world driving scenes and 1500 samples in controlled weather conditions within a fog chamber. The dataset includes different weather conditions like fog, snow, and rain and was acquired by over 10,000 km of driving in northern Europe. The driven route with cities along the road is shown on the right. In total, 100k Objekts were labeled with accurate 2D and 3D bounding boxes. The main contributions of this dataset are: - We provide a proving ground for a broad range of algorithms covering signal enhancement, domain adaptation, object detection, or multi-modal sensor fusion, focusing on the learning of robust redundancies between sensors, especially if they fail asymmetrically in different weather conditions. - The dataset was created with the initial intention to showcase methods, which learn of robust redundancies between the sensor and enable a raw data sensor fusion in case of asymmetric sensor failure induced through adverse weather effects. - In our case we departed from proposal level fusion and applied an adaptive fusion driven by measurement entropy enabling the detection also in case of unknown adverse weather effects. This method outperforms other reference fusion methods, which even drop in below single image methods. - Please check out our paper for more information.",,,,12000 samples,,
579,CLEF-TAR,TAR,TAR,TAR,,,Methodology,,MIT,https://github.com/CLEF-TAR/tar,https://paperswithcode.com/dataset/clef-tar,Technologically Assisted Reviews in Empirical Medicine.,,,,,,
580,CLEVR-Dialog,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Visual Dialog, Question Answering","Image, Text",English,Computer Vision,,BSD 3-Clause,https://github.com/satwikkottur/clevr-dialog,https://paperswithcode.com/dataset/clevr-dialog,"CLEVR-Dialog is a large diagnostic dataset for studying multi-round reasoning in visual dialog. Specifically, that authors construct a dialog grammar that is grounded in the scene graphs of the images from the CLEVR dataset. This combination results in a dataset where all aspects of the visual dialog are fully annotated. In total, CLEVR-Dialog contains 5 instances of 10-round dialogs for about 85k CLEVR images, totaling to 4.25M question-answer pairs.

The CLEVR-Dialog is used to benchmark performance of standard visual dialog models; in particular, on visual coreference resolution (as a function of the coreference distance). This is the first analysis of its kind for visual dialog models that was not possible without this dataset. 

CLEVR-Dialog is aims to help inform the development of future models for visual dialog.",,,,5 instances,,
581,CLEVR-Ref_,Referring Expression Comprehension,Referring Expression Comprehension,"Referring Expression Comprehension, Question Answering, Visual Reasoning, Referring Expression Segmentation, Visual Question Answering (VQA)","Image, Text",English,Computer Vision,"referring-expression-segmentation-on-clevr, referring-expression-comprehension-on-clevr",,https://cs.jhu.edu/~cxliu/2019/clevr-ref+,https://paperswithcode.com/dataset/clevr-ref,"CLEVR-Ref+ is a synthetic diagnostic dataset for referring expression comprehension. The precise locations and attributes of the objects are readily available, and the referring expressions are automatically associated with functional programs. The synthetic nature allows control over dataset bias (through sampling strategy), and the modular programs enable intermediate reasoning ground truth without human annotators.",,CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions,https://arxiv.org/pdf/1901.00850v2.pdf,,,
582,CLEVR-X,Explanation Generation,Explanation Generation,Explanation Generation,Text,English,Natural Language Processing,explanation-generation-on-clevr-x,BSD-3-Clause License,https://explainableml.github.io/CLEVR-X/,https://paperswithcode.com/dataset/clevr-x,"CLEVR-X is a dataset that extends the CLEVR dataset with natural language explanations in the context of  VQA. It consists of 3.6 million natural language explanations for 850k question-image pairs.

For each image-question pair in the CLEVR dataset, CLEVR-X contains multiple structured textual explanations which are derived from the original scene graphs. By construction, the CLEVR-X explanations are correct and describe the reasoning and visual information that is necessary to answer a given question.

The CLEVR-X dataset consists of:


A training set of 2,401,275 natural language explanations for 70,000 images.
A validation set of 599,711 natural language explanations for 14,000 images.
A test set of 644,151 natural language explanations for 15,000 images.",,,,000 images,"training set of 2,401,275 natural language explanations for 70,000 images",
583,CLEVR,Visual Question Answering (VQA) Split A,Visual Question Answering (VQA) Split A,"Visual Question Answering (VQA) Split A, Visual Question Answering (VQA) Split B, Visual Question Answering, Image Generation, Visual Question Answering (VQA)","Image, Text",English,Computer Vision,"visual-question-answering-on-clevr-1, visual-question-answering-vqa-split-b-on, visual-question-answering-on-clevr, visual-question-answering-vqa-split-a-on, image-generation-on-clevr",CC BY 4.0,https://cs.stanford.edu/people/jcjohns/clevr/,https://paperswithcode.com/dataset/clevr,"CLEVR (Compositional Language and Elementary Visual Reasoning) is a synthetic Visual Question Answering dataset. It contains images of 3D-rendered objects; each image comes with a number of highly compositional questions that fall into different categories. Those categories fall into 5 classes of tasks: Exist, Count, Compare Integer, Query Attribute and Compare Attribute. The CLEVR dataset consists of: a training set of 70k images and 700k questions, a validation set of 15k images and 150k questions, a test set of 15k images and 150k questions about objects, answers, scene graphs and functional programs for all train and validation images and questions. Each object present in the scene, aside of position, is characterized by a set of four attributes: 2 sizes: large, small, 3 shapes: square, cylinder, sphere, 2 material types: rubber, metal, 8 color types: gray, blue, brown, yellow, red, green, purple, cyan, resulting in 96 unique combinations.",,On transfer learning using a MAC model variant,https://arxiv.org/abs/1811.06529,70k images,training set of 70k images,5
584,ClevrTex,Unsupervised Object Segmentation,Unsupervised Object Segmentation,Unsupervised Object Segmentation,Image,,Computer Vision,unsupervised-object-segmentation-on-clevrtex,CC-BY,https://www.robots.ox.ac.uk/~vgg/data/clevrtex/,https://paperswithcode.com/dataset/clevrtex,"ClevrTex is a new benchmark designed as the next challenge to compare, evaluate and analyze algorithms for unsupervised multi-object segmentation. ClevrTex features synthetic scenes with diverse shapes, textures and photo-mapped materials, created using physically based rendering techniques.",,Karazija et al.,https://arxiv.org/pdf/2111.10265.pdf,,,
585,ClimART,Physical Simulations,Physical Simulations,Physical Simulations,,,Methodology,,Attribution 4.0 International (CC BY 4.0),https://github.com/RolnickLab/climart,https://paperswithcode.com/dataset/climart,"Numerical simulations of Earth's weather and climate require substantial amounts of computation. This has led to a growing interest in replacing subroutines that explicitly compute physical processes with approximate machine learning (ML) methods that are fast at inference time. Within weather and climate models, atmospheric radiative transfer (RT) calculations are especially expensive. This has made them a popular target for neural network-based emulators. However, prior work is hard to compare due to the lack of a comprehensive dataset and standardized best practices for ML benchmarking. To fill this gap, we build a large dataset, ClimART, with more than \emph{10 million samples from present, pre-industrial, and future climate conditions}, based on the Canadian Earth System Model. ClimART poses several methodological challenges for the ML community, such as multiple out-of-distribution test sets, underlying domain physics, and a trade-off between accuracy and inference speed.",,,,,,
586,CLIMATE-FEVER,Text Retrieval,Text Retrieval,"Text Retrieval, Natural Language Understanding, Zero-shot Text Search, Misinformation",Text,English,Natural Language Processing,"text-retrieval-on-climate-fever, zero-shot-text-search-on-climate-fever",,http://climatefever.ai,https://paperswithcode.com/dataset/climate-fever,A new publicly available dataset for verification of climate change-related claims.,,,,,,
587,CLINC-Single-Domain-OOS,Intent Detection,Intent Detection,"Intent Detection, Intent Classification",Image,,Computer Vision,,,https://github.com/jianguoz/Few-Shot-Intent-Detection,https://paperswithcode.com/dataset/clinc-single-domain-oos,"A dataset with two separate domains, i.e., the  ""Banking''  domain and the ""Credit cards''  domain with both general Out-of-Scope (OOD-OOS) queries and In-Domain but Out-of-Scope (ID-OOS) queries, where ID-OOS queries are semantically similar intents/queries with in-scope intents. Each domain in CLINC150 originally includes 15 intents. Each domain includes ten in-scope intents in this dataset, and the ID-OOS queries are built up based on five held-out in-scope intents.

Can be used to conduct intent detection with and without OOD-OOS and ID-OOS queries",,,,,,
588,CLINC150,Intent Classification,Intent Classification,"Intent Classification, Intent Detection, Text Classification, Open Intent Discovery","Image, Text",English,Computer Vision,"open-intent-discovery-on-clinc150, text-classification-on-clinc-oos, intent-detection-on-clinc150, intent-detection-on-clinc150-5-shot, intent-detection-on-clinc150-10-shot",,https://github.com/clinc/oos-eval,https://paperswithcode.com/dataset/clinc150,"This dataset is for evaluating the performance of intent classification systems in the presence of ""out-of-scope"" queries, i.e., queries that do not fall into any of the system-supported intent classes. The dataset includes both in-scope and out-of-scope data.",,,,,,
589,Clinical_Admission_Notes_from_MIMIC-III,Mortality Prediction,Mortality Prediction,"Mortality Prediction, Length-of-Stay prediction, Medical Procedure, Medical Diagnosis",Time Series,,Methodology,"length-of-stay-prediction-on-clinical, medical-procedure-on-clinical-admission-notes, mortality-prediction-on-clinical-admission, medical-diagnosis-on-clinical-admission-notes",,https://github.com/bvanaken/clinical-outcome-prediction,https://paperswithcode.com/dataset/hospital-admission-notes-from-mimic-iii,"This dataset is created from MIMIC-III (Medical Information Mart for Intensive Care III) and contains simulated patient admission notes. The clinical notes contain information about a patient at admission time to the ICU and are labelled for four outcome prediction tasks: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay.

To obtain the data one first has to gain access to the MIMIC-III dataset and then run the scripts introduced in the linked repository.",,,,,,
590,clintox,Graph Classification,Graph Classification,"Graph Classification, Drug Discovery, Molecular Property Prediction","Graph, Image, Time Series",,Computer Vision,"graph-classification-on-clintox, molecular-property-prediction-on-clintox-1, drug-discovery-on-clintox",,https://moleculenet.org/,https://paperswithcode.com/dataset/clintox,The ClinTox dataset compares drugs approved by the FDA and drugs that have failed clinical trials for toxicity reasons. The dataset includes two classification tasks for 1491 drug compounds with known chemical structures: (1) clinical trial toxicity (or absence of toxicity) and (2) FDA approval status.,,,,,,
591,CLIRMatrix,Cross-Lingual Information Retrieval,Cross-Lingual Information Retrieval,"Cross-Lingual Information Retrieval, Information Retrieval",,,Methodology,,,https://github.com/ssun32/CLIRMatrix,https://paperswithcode.com/dataset/clirmatrix,"CLIRMatrix is a large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval. It includes:


BI-139: A bilingual dataset of queries in one language matched with relevant documents in another language for 139x138=19,182 language pairs,
MULTI-8, a multilingual dataset of queries and documents jointly aligned in 8 different languages.

In total, 49 million unique queries and 34 billion (query, document, label) triplets were mined, making CLIRMatrix the largest and most comprehensive CLIR dataset to date.",,,,,,
592,ClonedPerson,Unsupervised Domain Adaptation,Unsupervised Domain Adaptation,"Unsupervised Domain Adaptation, Unsupervised Person Re-Identification, Person Re-Identification, Generalizable Person Re-identification",Image,,Computer Vision,"unsupervised-domain-adaptation-on, unsupervised-person-re-identification-on-13, person-re-identification-on-clonedperson, generalizable-person-re-identification-on-19",,https://github.com/Yanan-Wang-cs/ClonedPerson,https://paperswithcode.com/dataset/clonedperson,"The ClonedPerson dataset is a large-scale synthetic person re-identification dataset introduced in the paper ""Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification"" in CVPR 2022. It is generated by MakeHuman and Unity3D. Characters in this dataset use an automatic approach to directly clone the whole outfits from real-world person images to virtual 3D characters, such that any virtual person thus created will appear very similar to its real-world counterpart. The dataset contains 887,766 synthesized person images of 5,621 identities.",2022,,,,,
593,Clothing1M,Image Classification,Image Classification,"Image Classification, Learning with noisy labels",Image,,Computer Vision,"image-classification-on-clothing1m-using, learning-with-noisy-labels-on-clothing1m-2, learning-with-noisy-labels-on-clothing1m, image-classification-on-clothing1m",,https://github.com/Cysu/noisy_label,https://paperswithcode.com/dataset/clothing1m,"Clothing1M contains 1M clothing images in 14 classes. It is a dataset with noisy labels, since the data is collected from several online shopping websites and include many mislabelled samples. This dataset also contains 50k, 14k, and 10k images with clean labels for training, validation, and testing, respectively.",,Label-Noise Robust Generative Adversarial Networks,https://arxiv.org/abs/1811.11165,10k images,,14
594,Clotho,Language Modelling,Language Modelling,"Language Modelling, Zero-shot Audio Captioning, Data Augmentation, Audio to Text Retrieval, Audio captioning, Text to Audio Retrieval, Zero-shot Text to Audio Retrieval, Multi-Task Learning","Audio, Image, Text",English,Computer Vision,"text-to-audio-retrieval-on-clotho, audio-to-text-retrieval-on-clotho, audio-captioning-on-clotho, zero-shot-text-to-audio-retrieval-on-clotho, zero-shot-audio-captioning-on-clotho",Other (Attribution),https://zenodo.org/record/3490684,https://paperswithcode.com/dataset/clotho,"Clotho is an audio captioning dataset, consisting of 4981 audio samples, and each audio sample has five captions (a total of 24 905 captions). Audio samples are of 15 to 30 s duration and captions are eight to 20 words long.",,https://arxiv.org/abs/1910.09387,https://arxiv.org/abs/1910.09387,,,
595,ClovaCall,Open-Domain Dialog,Open-Domain Dialog,"Open-Domain Dialog, Goal-Oriented Dialog, Speech Recognition","Audio, Image, Text",English,Computer Vision,,,https://github.com/ClovaAI/ClovaCall,https://paperswithcode.com/dataset/clovacall,"ClovaCall is a new large-scale Korean call-based speech corpus under a goal-oriented dialog scenario from more than 11,000 people. The raw dataset of ClovaCall includes approximately 112,000 pairs of a short sentence and its corresponding spoken utterance in a restaurant reservation domain.",,,,,,
596,CLUE,Language Modelling,Language Modelling,"Language Modelling, Natural Language Understanding, Reading Comprehension, Text Classification, Document Ranking","Image, Text",English,Computer Vision,"language-modelling-on-clue-cmrc2018, language-modelling-on-clue-drcd, language-modelling-on-clue-afqmc, language-modelling-on-clue-cmnli, language-modelling-on-clue-c3, language-modelling-on-clue-wsc1-1, language-modelling-on-clue-ocnli-50k, document-ranking-on-clueweb09-b",,https://www.cluebenchmarks.com/,https://paperswithcode.com/dataset/clue,"CLUE is a Chinese Language Understanding Evaluation benchmark. It consists of different NLU datasets. It is a community-driven project that brings together 9 tasks spanning several well-established single-sentence/sentence-pair classification tasks, as well as machine reading comprehension, all on original Chinese text.",,,,,,
597,CLUENER2020,Chinese Named Entity Recognition,Chinese Named Entity Recognition,"Chinese Named Entity Recognition, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,,,https://github.com/CLUEbenchmark/CLUENER2020,https://paperswithcode.com/dataset/cluener2020,CLUENER2020 is a well-defined fine-grained dataset for named entity recognition in Chinese. CLUENER2020 contains 10 categories.,,,,,,10
598,CMMD,Breast Cancer Detection,Breast Cancer Detection,Breast Cancer Detection,Image,,Computer Vision,breast-cancer-detection-on-cmmd,,https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=70230508,https://paperswithcode.com/dataset/cmmd,"Breast carcinoma is the second largest cancer in the world among women. Early detection of breast cancer has been shown to increase the survival rate, thereby significantly increasing patients' lifespans. Mammography, a noninvasive imaging tool with low cost, is widely used to diagnose breast disease at an early stage due to its high sensitivity. The recent popularization of artificial intelligence in computer-aided diagnosis creates opportunities for advances in areas such as (1) Computer-aided detection for locating suspect lesions such as mass and microcalcification, leaving the classification to the radiologist; and (2) Computer-aided diagnosis for characterizing the suspicious region of lesion and/or estimate its probability of onset; and (3) Findings of predictive image-based biomarkers by applying the computational methods to mine the potential relationships between image representation and molecular subtype, including luminal A, luminal B, HER2 positive, and Triple-negative.

However, existing publicly available mammography databases are limited by small sample size, lack of diversity in patient populations, missing biopsy confirmations and unknown molecular sub-types.  To help fill the gap, we built a database conducted on 1,775 patients from China with benign or malignant breast disease who underwent mammography examination between July 2012 and January 2016. The database consists of 3,728 mammographies from these 1,775 patients, with biopsy confirmed type of benign or malignant tumors. For 749 of these patients (1,498 mammographies) we also include patients' molecular subtypes. Image data were acquired on a GE Senographe DS mammography system.  

Publication Citation

Cai, H., Huang, Q., Rong, W., Song, Y., Li, J., Wang, J., Chen, J., & Li, L. (2019). Breast Microcalcification Diagnosis Using Deep Convolutional Neural Network from Digital Mammograms. Computational and Mathematical Methods in Medicine, 2019, 1–10. https://doi.org/10.1155/2019/2717454

Wang, J., Yang, X., Cai, H., Tan, W., Jin, C., & Li, L. (2016). Discrimination of Breast Cancer with Microcalcifications on Mammography by Deep Learning. Scientific Reports, 6(1). https://doi.org/10.1038/srep27327

Data Citation
Cui, Chunyan; Li Li; Cai, Hongmin; Fan, Zhihao; Zhang, Ling; Dan, Tingting; Li, Jiao; Wang, Jinghua. (2021) The Chinese Mammography Database (CMMD): An online mammography database with biopsy confirmed types for machine diagnosis of breast. The Cancer Imaging Archive. DOI: https://doi.org/10.7937/tcia.eqde-4b16

TCIA Citation
Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel, P., Moore, S., Phillips, S., Maffitt, D., Pringle, M., Tarbox, L., & Prior, F. (2013). The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository. Journal of Digital Imaging, 26(6), 1045–1057. https://doi.org/10.1007/s10278-013-9622-7",2012,,,,,
599,CMRC,Language Modelling,Language Modelling,"Language Modelling, Machine Reading Comprehension, Cloze (multi-choices) (Zero-Shot), Cloze (multi-choices) (One-Shot), Chinese Reading Comprehension, Reading Comprehension (Zero-Shot), Reading Comprehension (Few-Shot), Reading Comprehension (One-Shot), Cloze (multi-choices) (Few-Shot), Reading Comprehension",Text,English,Natural Language Processing,"cloze-multi-choices-zero-shot-on-cmrc-2017, cloze-multi-choices-one-shot-on-cmrc-2017, reading-comprehension-zero-shot-on-cmrc-2018, cloze-multi-choices-one-shot-on-cmrc-2019, reading-comprehension-few-shot-on-cmrc-2018, cloze-multi-choices-few-shot-on-cmrc-2017, chinese-reading-comprehension-on-cmrc-2018-3, cloze-multi-choices-zero-shot-on-cmrc-2019, reading-comprehension-one-shot-on-cmrc-2018, chinese-reading-comprehension-on-cmrc-2019, cloze-multi-choices-few-shot-on-cmrc-2019",CC-BY-SA-4.0,https://github.com/ymcui/cmrc2018,https://paperswithcode.com/dataset/cmrc,"CMRC is a dataset is annotated by human experts with near 20,000 questions as well as a challenging set which is composed of the questions that need reasoning over multiple clues.",,A Span-Extraction Dataset for Chinese Machine Reading Comprehension,https://www.aclweb.org/anthology/D19-1600.pdf,,,
600,CMU-MOSEI,Facial Expression Recognition,Facial Expression Recognition,"Facial Expression Recognition, Emotion Classification, Video Emotion Detection, Multimodal Emotion Recognition, Multimodal Sentiment Analysis","Image, Text, Video",English,Computer Vision,"multimodal-sentiment-analysis-on-cmu-mosei-1, emotion-classification-on-cmu-mosei, facial-expression-recognition-on-cmu-mosei",Custom,,https://paperswithcode.com/dataset/cmu-mosei,CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) is the largest dataset of sentence-level sentiment analysis and emotion recognition in online videos. CMU-MOSEI contains over 12 hours of annotated video from over 1000 speakers and 250 topics.,,,,,,
601,CMU-MOSI,Multimodal Sentiment Analysis,Multimodal Sentiment Analysis,"Multimodal Sentiment Analysis, Emotion Recognition in Conversation","Image, Text",English,Multimodal,"multimodal-sentiment-analysis-on-cmu-mosi, emotion-recognition-in-conversation-on-cmu",,http://multicomp.cs.cmu.edu/resources/cmu-mosi-dataset/,https://paperswithcode.com/dataset/cmu-mosi,"The Multimodal Corpus of Sentiment Intensity (CMU-MOSI) dataset is a collection of 2199 opinion video clips. Each opinion video is annotated with sentiment in the range [-3,3]. The dataset is rigorously annotated with labels for subjectivity, sentiment intensity, per-frame and per-opinion annotated visual features, and per-milliseconds annotated audio features.",,,,,,
602,CN-CELEB,Speaker Verification,Speaker Verification,"Speaker Verification, Dimensionality Reduction, Speaker Recognition","Audio, Image",,Computer Vision,speaker-verification-on-cn-celeb,,http://cslt.riit.tsinghua.edu.cn/mediawiki/index.php/CN-Celeb,https://paperswithcode.com/dataset/cn-celeb,"CN-Celeb is a large-scale speaker recognition dataset collected `in the wild'. This dataset contains more than 130,000 utterances from 1,000 Chinese celebrities, and covers 11 different genres in real world.",,,,,,
603,CNN_Daily_Mail,Text Summarization,Text Summarization,"Text Summarization, Extractive Text Summarization, Extractive Document Summarization, Text Generation, Abstractive Text Summarization, Summarization, Question Answering, Document Summarization",Text,English,Natural Language Processing,"extractive-document-summarization-on-cnn-1, question-answering-on-cnn-daily-mail, document-summarization-on-cnn-daily-mail, extractive-document-summarization-on-cnn, abstractive-text-summarization-on-cnn-daily-2, abstractive-text-summarization-on-cnn-daily, text-generation-on-cnn-daily-mail, text-summarization-on-cnn-daily-mail-2, summarization-on-cnn-dailymail",MIT,https://github.com/abisee/cnn-dailymail,https://paperswithcode.com/dataset/cnn-daily-mail-1,"CNN/Daily Mail is a dataset for text summarization. Human generated abstractive summary bullets were generated from news stories in CNN and Daily Mail websites as questions (with one of the entities hidden), and stories as the corresponding passages from which the system is expected to answer the fill-in the-blank question. The authors released the scripts that crawl, extract and generate pairs of passages and questions from these websites.

In all, the corpus has 286,817 training pairs, 13,368 validation pairs and 11,487 test pairs, as defined by their scripts. The source documents in the training set have 766 words spanning 29.74 sentences on an average while the summaries consist of 53 words and 3.72 sentences.",,Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond,https://arxiv.org/pdf/1602.06023v5.pdf,74 sentences,,
604,CoAID,Misinformation,Misinformation,"Misinformation, Decision Making",,,Methodology,,,https://github.com/cuilimeng/CoAID,https://paperswithcode.com/dataset/coaid,"CoAID include diverse COVID-19 healthcare misinformation, including fake news on websites and social platforms, along with users' social engagement about such news. CoAID includes 4,251 news, 296,000 related user engagements, 926 social platform posts about COVID-19, and ground truth labels.",,,,,,
605,CoarseWSD-20,Language Modelling,Language Modelling,"Language Modelling, Word Sense Disambiguation",Text,English,Natural Language Processing,,,https://github.com/danlou/bert-disambiguation,https://paperswithcode.com/dataset/coarsewsd-20,"The CoarseWSD-20 dataset is a coarse-grained sense disambiguation dataset built from Wikipedia (nouns only) targeting 2 to 5 senses of 20 ambiguous words. It was specifically designed to provide an ideal setting for evaluating Word Sense Disambiguation (WSD) models (e.g. no senses in test sets missing from training), both quantitively and qualitatively.",,,,,,
606,Coastal_Inundation_Maps_with_Floodwater_Depth_Valu,Depth Prediction,Depth Prediction,"Depth Prediction, Depth Estimation, Flood Inundation Mapping","3D, Time Series",,Methodology,flood-inundation-mapping-on-coastal,CC BY 4.0,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/M9625R,https://paperswithcode.com/dataset/coastal-inundation-maps-with-floodwater-depth,"This dataset provides simulated flood inundation maps of Abu Dhabi's coast under 174 different shoreline protection scenarios. The maps were produced with a high-fidelity physics-based hydrodynamic simulator under a 0.5-meter sea level rise projection. The details of the hydrodynamic model are reported in [1].

The coastline was partitioned into 17 segments. The input protection scenarios were encoded as 17-dimensional binary vectors, with 1 indicating the placement of containments and 0 otherwise. The output inundation depth maps are named according to protection scenarios. Each map is a 1024x1024 matrix containing the estimated peak water level values (in meters) for selected 12066 locations along the coastline of Abu Dhabi. The mapping from gird indices to the original geographical coordinates of these locations is provided in a separate file (ad_grid_1024.npy). A Jupyter notebook with a sample Python code is included for visualizing the data entries (see Data_Visualization.ipynb).

[1] Chow, Aaron C. H., and Jiayun Sun. 2022. ""Combining Sea Level Rise Inundation Impacts, Tidal Flooding and Extreme Wind Events along the Abu Dhabi Coastline"" Hydrology 9, no. 8: 143. https://doi.org/10.3390/hydrology9080143",2022,,,,,
607,CochlScene,Scene Classification,Scene Classification,"Scene Classification, Acoustic Scene Classification","Audio, Image",,Computer Vision,acoustic-scene-classification-on-cochlscene,,https://github.com/cochlearai/cochlscene,https://paperswithcode.com/dataset/cochlscene,CochlScene is a dataset for acoustic scene classification. The dataset consists of 76k samples collected from 831 participants in 13 acoustic scenes.,,CochlScene: Acquisition of acoustic scene data using crowdsourcing,https://arxiv.org/pdf/2211.02289v1.pdf,76k samples,,
608,COCO-MIG,Conditional Text-to-Image Synthesis,Conditional Text-to-Image Synthesis,Conditional Text-to-Image Synthesis,"Image, Text",English,Computer Vision,conditional-text-to-image-synthesis-on-coco-1,CC BY-NC,https://github.com/LeyRio/MIG_Bench,https://paperswithcode.com/dataset/coco-mig,"The COCO-MIG benchmark (Common Objects in Context Multi-Instance Generation) is a benchmark used to evaluate the generation capability of generators on text containing multiple attributes of multi-instance objects. This benchmark consists of 800 sets of examples sampled from the COCO dataset. Following the layout of the COCO dataset, each instance is assigned random color information, and corresponding global image descriptions are constructed according to templates.
The COCO-MIG also provides a complete pipeline for resampling and evaluating. For relevant tools and specific details, please refer to our project's homepage.",,,,,valuate the generation capability of generators on text containing multiple attributes of multi-instance objects. This benchmark consists of 800 sets of examples,
609,COCO-Noisy,Cross-modal retrieval with noisy correspondence,Cross-modal retrieval with noisy correspondence,Cross-modal retrieval with noisy correspondence,Image,English,Computer Vision,cross-modal-retrieval-with-noisy-3,,,https://paperswithcode.com/dataset/coco-noisy,This dataset is based on MS COCO that have 20% of data randomly shuffled to simulate noisy correspondence.,,,,,,
610,COCO-N_Medium,Learning with noisy labels,Learning with noisy labels,"Learning with noisy labels, Instance Segmentation, Benchmarking",Image,English,Computer Vision,instance-segmentation-on-coco-n-medium,,,https://paperswithcode.com/dataset/coco-n-medium,"COCO-N Medium introduces a stochastic benchmark that simulates common real-world scenarios with noticeable label inaccuracies in the COCO dataset. This benchmark combines class and spatial noises to create a challenging yet realistic evaluation framework for instance segmentation models. It mimics datasets manually annotated by crowd workers, where a moderate level of label noise is expected. By incorporating both class and spatial inaccuracies, COCO-N Medium allows researchers to assess their models' basic robustness to label noise, providing insights into performance in typical real-world applications where perfect annotations are rare. This medium-level benchmark serves as a crucial middle ground, offering a more rigorous test than minimally noisy datasets while remaining within the bounds of commonly encountered data quality issues. COCO-N Medium enables a nuanced evaluation of model performance under realistic conditions, helping identify areas for improvement in handling noisy labels and guiding the development of more robust instance segmentation algorithms.",,,,,,
611,COCO-O,Object Detection,Object Detection,"Object Detection, 2D Object Detection",Image,English,Computer Vision,object-detection-on-coco-o,,https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o,https://paperswithcode.com/dataset/coco-o,"COCO-O(ut-of-distribution) contains 6 domains (sketch, cartoon, painting, weather, handmake, tattoo) of COCO objects which are hard to be detected by most existing detectors. The dataset has a total of 6,782 images and 26,624 labelled bounding boxes.",,,,782 images,,
612,COCO-Stuff,Image-to-Image Translation,Image-to-Image Translation,"Image-to-Image Translation, Unsupervised Image Segmentation, Real-Time Semantic Segmentation, Semantic Segmentation, Unsupervised Semantic Segmentation with Language-image Pre-training, Open Vocabulary Semantic Segmentation, Zero-Shot Semantic Segmentation, Unsupervised Semantic Segmentation, Sketch-to-Image Translation, Layout-to-Image Generation","Image, Text",English,Computer Vision,"unsupervised-semantic-segmentation-on-coco-1, unsupervised-semantic-segmentation-on-coco-7, unsupervised-semantic-segmentation-on-coco-6, layout-to-image-generation-on-coco-stuff-3, unsupervised-semantic-segmentation-with-1, open-vocabulary-semantic-segmentation-on-coco, layout-to-image-generation-on-coco-stuff-2, image-to-image-translation-on-coco-stuff, unsupervised-semantic-segmentation-on-coco-8, semantic-segmentation-on-coco-stuff-test, unsupervised-image-segmentation-on-coco-stuff, layout-to-image-generation-on-coco-stuff-4, unsupervised-semantic-segmentation-with-9, zero-shot-semantic-segmentation-on-coco-stuff, semantic-segmentation-on-coco-stuff-full, sketch-to-image-translation-on-coco-stuff, semantic-segmentation-on-coco-stuff, semantic-segmentation-on-coco-stuff-27, real-time-semantic-segmentation-on-coco-stuff-1, unsupervised-semantic-segmentation-on-coco",Various,https://github.com/nightrome/cocostuff,https://paperswithcode.com/dataset/coco-stuff,"The Common Objects in COntext-stuff (COCO-stuff) dataset is a dataset for scene understanding tasks like semantic segmentation, object detection and image captioning. It is constructed by annotating the original COCO dataset, which originally annotated things while neglecting stuff annotations. There are 164k images in COCO-stuff dataset that span over 172 categories including 80 things, 91 stuff, and 1 unlabeled class.",,Image Colorization: A Survey and Dataset,https://arxiv.org/abs/2008.10774,164k images,,172
613,COCO-Text,Scene Text Detection,Scene Text Detection,"Scene Text Detection, Scene Text Recognition","Image, Text",English,Computer Vision,"scene-text-detection-on-coco-text, scene-text-recognition-on-coco-text",Creative Commons Attribution 4.0 License,https://bgshih.github.io/cocotext/,https://paperswithcode.com/dataset/coco-text,"The COCO-Text dataset is a dataset for text detection and recognition. It is based on the MS COCO dataset, which contains images of complex everyday scenes. The COCO-Text dataset contains non-text images, legible text images and illegible text images. In total there are 22184 training images and 7026 validation images with at least one instance of legible text.",,Improving Text Proposals for Scene Images with Fully Convolutional Networks,https://arxiv.org/abs/1702.05089,,training images and 7026 validation images,
614,COCO-WholeBody,Face Detection,Face Detection,"Face Detection, Pose Estimation, Facial Landmark Detection, Hand Pose Estimation, Foot keypoint detection, Multi-Person Pose Estimation, 2D Human Pose Estimation","3D, Image",English,Computer Vision,"face-detection-on-coco-wholebody, foot-keypoint-detection-on-coco-wholebody, hand-pose-estimation-on-coco-wholebody, multi-person-pose-estimation-on-coco-1, 2d-human-pose-estimation-on-coco-wholebody-1, facial-landmark-detection-on-coco-wholebody",CC-BY-NC 4.0,https://github.com/jin-s13/COCO-WholeBody,https://paperswithcode.com/dataset/coco-wholebody,"COCO-WholeBody is an extension of COCO dataset with whole-body annotations. There are 4 types of bounding boxes (person box, face box, left-hand box, and right-hand box) and 133 keypoints (17 for body, 6 for feet, 68 for face and 42 for hands) annotations for each person in the image.",,https://arxiv.org/pdf/2007.11858v1.pdf,https://arxiv.org/pdf/2007.11858v1.pdf,,,
615,CocoChorales,Multi-task Audio Source Seperation,Multi-task Audio Source Seperation,"Multi-task Audio Source Seperation, Music Transcription, Music Source Separation",Audio,,Audio,,CC-BY 4.0,https://magenta.tensorflow.org/datasets/cocochorales,https://paperswithcode.com/dataset/cocochorales,"The CocoChorales Dataset
CocoChorales is a dataset consisting of over 1400 hours of audio mixtures containing four-part chorales performed by 13 instruments, all synthesized with realistic-sounding generative models. CocoChorales contains mixes, sources, and MIDI data, as well as annotations for note expression (e.g., per-note volume and vibrato) and synthesis parameters (e.g., multi-f0).

Dataset
We created CocoChorales using two generative models produced by Magenta: Coconet and MIDI-DDSP. The dataset was created in two stages. First, we used a trained Coconet model to generate a large set of four-part chorales in the style of J.S. Bach. The output of this first stage is a set of note sequences, stored as MIDI, to which we assign a tempo and add random timing variations to each note (for added realism).

In the second stage, we use MIDI-DDSP to synthesize these MIDI files into audio, resulting in audio clips that sound like the chorales were performed by live musicians. This MIDI-DDSP model was trained on URMP. We define a set of ensembles that consist of the following instruments, in Soprano, Alto, Tenor, Bass (SATB) order:

<ul>
  <li><strong>String Ensemble</strong>: Violin 1, Violin 2, Viola, Cello.</li>
  <li><strong>Brass Ensemble</strong>: Trumpet, French Horn, Trombone, Tuba.</li>
  <li><strong>Woodwind Ensemble</strong>: Flute, Oboe, Clarinet, Bassoon.</li>
  <li><strong>Random Ensemble</strong>: Each SATB part is randomly assigned an instrument according to the following:
    <ul class=""nested"">
      <li><em>Soprano</em>: Violin, Flute, Trumpet, Clarinet, Oboe.</li>
      <li><em>Alto</em>: Violin, Viola, Flute, Clarinet, Oboe, Saxophone, Trumpet, French Horn.</li>
      <li><em>Tenor</em>: Viola, Cello, Clarinet, Saxophone, Trombone, French Horn.</li>
      <li><em>Bass</em>: Cello, Double Bass, Bassoon, Tuba.</li>
    </ul>
  </li>
</ul>


Each instrument in the ensemble is synthesized separately, with annotations for the high-level expressions used for each note (e.g., vibrato, note volume, note brightness, etc; all expressions shown here, and more details in Sections 3.2 and B.3 of the MIDI-DDSP paper) as well as detailed low-level annotations for synthesis parameters (e.g., f<sub>0</sub>’s, amplitudes of each harmonic, etc). Because the MIDI-DDSP model skews sharp, we randomly applied pitch augmentation to the f<sub>0</sub>’s (see Figure 2, here) to . All four audio clips for each instrument in the ensemble are then mixed together to produce an example in the dataset.

Because all of the data in CocoChorales originate from generative models, all of the annotations perfectly correspond to the audio data. All in all, the dataset contains 240,000 examples, 60,000 mixes from each one of the four ensemble types above. Each ensemble has its own train/validation/test split All of the audio is 16 kHz, 16-bit PCM data. Each example contains:

<ul>
  <li>A mixture</li>
  <li>Source audio for all four instruments
    <ul class=""nested"">
      <li>Gain applied to each source</li>
    </ul>
  </li>
  <li>MIDI with tempo and precise timing</li>
  <li>The name of the ensemble with instrument names</li>
  <li>Note expression annotations for every note:
    <ul class=""nested"">
      <li>Volume, Volume Fluctuation, Volume Peak Position, Vibrato, Brightness, and Attack Noise used by MIDI-DDSP to synthesize every note (see Sections 3.2 and B.3 of the MIDI-DDSP paper for more details)</li>
    </ul>
  </li>
  <li>Synthesis parameters for every source (250 Hz):
    <ul class=""nested"">
      <li>Fundamental frequency (f<sub>0</sub>), amplitude, amplitude of all harmonics, filtered noise parameters</li>
      <li>Amount of pitch augmentation applied</li>
    </ul>
  </li>
</ul>


Further Details
A detailed view of the contents of the CocoChorales dataset is provided at this link.

Download
For download instructions, please see this github page. The compressed version of the full dataset is 2.9 Tb, and the uncompressed version is larger than 4 Tb. There is a ""tiny"" version for download as well.

MD5 Hashes for all zipped files in the download are provided here.

License
The CocoChorales dataset was made by Yusong Wu and is available under the Creative Commons Attribution 4.0 International (CC-BY 4.0).

How to Cite
If you use CocoChorales in your work, we ask that you cite the following paper where it was introduced:

Yusong Wu, Josh Gardner, Ethan Manilow, Ian Simon, Curtis Hawthorne, and Jesse Engel.
“The Chamber Ensemble Generator: Limitless High-Quality MIR Data via Generative Modeling.”
arXiv preprint, arXiv:2209.14458, 2022.

You can also use the following bibtex entry:

@article{wu2022chamber,
  title = {The Chamber Ensemble Generator: Limitless High-Quality MIR Data via Generative Modeling},
  author = {Wu, Yusong and Gardner, Josh and Manilow, Ethan and Simon, Ian and Hawthorne, Curtis and Engel, Jesse},
  journal={arXiv preprint arXiv:2209.14458},
  year = {2022},
}",2022,here,https://arxiv.org/pdf/2209.14458.pdf,000 examples,,
616,COCO_10__labeled_data,Semi-Supervised Object Detection,Semi-Supervised Object Detection,"Semi-Supervised Object Detection, Semi-Supervised Instance Segmentation, Self-Supervised Learning",Image,English,Computer Vision,"semi-supervised-object-detection-on-coco-10, semi-supervised-instance-segmentation-on-coco-7",,https://github.com/lifuguan/UPDETR-mmdet,https://paperswithcode.com/dataset/coco-10-labeled-data,Semi-Supervised Object Detection on COCO 10% labeled data,,,,,,
617,COCO_Captions,Text Generation,Text Generation,"Text Generation, Concept-To-Text Generation, Image Captioning","Image, Text",English,Computer Vision,"image-captioning-on-coco-captions, text-generation-on-coco-captions, image-captioning-on-coco-captions-test, concept-to-text-generation-on-coco-captions",CC BY,https://github.com/tylin/coco-caption,https://paperswithcode.com/dataset/coco-captions,"COCO Captions contains over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions are be provided for each image.",,Microsoft COCO Captions: Data Collection and Evaluation Server,https://arxiv.org/abs/1504.00325,000 images,,
618,CODE-15_,ECG Patient Identification (gallery-probe),ECG Patient Identification (gallery-probe),"ECG Patient Identification (gallery-probe), ECG Patient Identification, ECG Classification",Image,,Computer Vision,ecg-patient-identification-gallery-probe-on,Creative Commons Attribution 4.0 International,https://zenodo.org/record/4916206,https://paperswithcode.com/dataset/code-15,A dataset of 12-lead ECGs with annotations. The dataset contains 345 779 exams from 233 770 patients. It was obtained through stratified sampling from the CODE dataset ( 15% of the patients). The data was collected by the Telehealth Network of Minas Gerais in the period between 2010 and 2016.,2010,,,,,
619,CODEBRIM,Neural Architecture Search,Neural Architecture Search,"Neural Architecture Search, Meta-Learning",,,Methodology,,Other (Non-Commercial),https://zenodo.org/record/2620293,https://paperswithcode.com/dataset/codebrim,Dataset for multi-target classification of five commonly appearing concrete defects.,,,,,,
620,CodeContests,Text-to-Code Generation,Text-to-Code Generation,"Text-to-Code Generation, Code Generation",Text,English,Natural Language Processing,code-generation-on-codecontests,CC BY 4.0,https://github.com/deepmind/code_contests/,https://paperswithcode.com/dataset/codecontests,"CodeContests is a competitive programming dataset for machine-learning. This dataset was used when training AlphaCode.

It consists of programming problems, from a variety of sources.

Problems include test cases in the form of paired inputs and outputs, as well as both correct and incorrect human solutions in a variety of languages.",,,,,,
621,CoDesc,Source Code Summarization,Source Code Summarization,"Source Code Summarization, Code Search",Text,English,Natural Language Processing,"source-code-summarization-on-codesc, code-search-on-codesc",MIT,https://github.com/csebuetnlp/CoDesc,https://paperswithcode.com/dataset/codesc,"CoDesc is a large dataset of 4.2m Java source code and parallel data of their description from code search, and code summarization studies.",,,,,,
622,CodeSCAN,Optical Character Recognition (OCR),Optical Character Recognition (OCR),"Optical Character Recognition (OCR), Image Stylization, Code Classification, Object Detection, Code Search",Image,,Computer Vision,,Other (Non-Commercial),https://a-nau.github.io/codescan/,https://paperswithcode.com/dataset/codescan,"CodeSCAN is the first large-scale and diverse dataset of coding screenshots with pixel-perfect annotations. It features:


24 popular programming languages (according to Github)
100 random repositories per language (with MIT, BSD-3 or WTFPL License), i.e. 2.400 repositories in total
Per repository we use 5 files, i.e. 12.000 files in total
~100 different themes and 25 different fonts
Diverse layouts changes, such as menu bar visibility, sidebar position, output window content, etc.
Numerous realistic interactions such as searching, typing and selecting within a file, etc.

Check our project page (https://a-nau.github.io/codescan/) for details.",,,,,,
623,CodeSearchNet,Method name prediction,Method name prediction,"Method name prediction, Source Code Summarization, Code Search, Code Documentation Generation","Text, Time Series",English,Natural Language Processing,"code-summarization-on-codesearchnet, code-documentation-generation-on-6, source-code-summarization-on-codesearchnet, code-documentation-generation-on-2, code-documentation-generation-on-5, code-documentation-generation-on-1, code-documentation-generation-on, code-search-on-codesearchnet, method-name-prediction-on-codesearchnet, code-documentation-generation-on-3, code-search-on-codesearchnet-ruby, code-documentation-generation-on-4",Custom,https://github.com/github/CodeSearchNet,https://paperswithcode.com/dataset/codesearchnet,"The CodeSearchNet Corpus is a large dataset of functions with associated documentation written in Go, Java, JavaScript, PHP, Python, and Ruby from open source projects on GitHub. The CodeSearchNet Corpus includes:
* Six million methods overall
* Two million of which have associated documentation (docstrings, JavaDoc, and more)
* Metadata that indicates the original location (repository or line number, for example) where the data was found",,,,,,
624,CodeTransOcean,Code Translation,Code Translation,Code Translation,Text,English,Natural Language Processing,,Apache-2.0 license,https://github.com/WeixiangYAN/CodeTransOcean,https://paperswithcode.com/dataset/codetransocean,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
625,CodeXGLUE,Code Translation,Code Translation,"Code Translation, Text-to-Code Generation, Document Translation, Code Generation, Clone Detection, Code Completion, Cloze Test, Defect Detection, Code Summarization, Code Search, Code Repair","Image, Text",English,Computer Vision,"code-repair-on-codexglue-bugs2fix, document-translation-on-codexglue-microsoft, cloze-test-on-codexglue-ct-all, code-completion-on-codexglue-github-java, code-generation-on-codexglue-codesearchnet, code-completion-on-codexglue-py150, code-summarization-on-codexglue-codesearchnet, cloze-test-on-codexglue-ct-maxmin, clone-detection-on-codexglue-bigclonebench, clone-detection-on-codexglue-poj-104, code-search-on-codexglue-webquerytest, code-search-on-codexglue-advtest, defect-detection-on-codexglue-devign, code-translation-on-codexglue-codetrans, text-to-code-generation-on-codexglue-concode",Computational Use of Data Agreement (C-UDA) License,https://github.com/microsoft/CodeXGLUE,https://paperswithcode.com/dataset/codexglue,"CodeXGLUE is a benchmark dataset and open challenge for code intelligence. It includes a collection of code intelligence tasks and a platform for model evaluation and comparison. CodeXGLUE stands for General Language Understanding Evaluation benchmark for CODE. It includes 14 datasets for 10 diversified code intelligence tasks covering the following scenarios:


code-code (clone detection, defect detection, cloze test, code completion, code repair, and code-to-code translation)
text-code (natural language code search, text-to-code generation)
code-text (code summarization)
text-text (documentation translation)

A brief summary of CodeXGLUE is provided in the figure, including tasks, datasets, language, sizes in various states, baseline systems, providers, and short definitions of each task. Datasets highlighted in BLUE are newly introduced.",,,,,,
626,CoDEx_Large,Link Prediction,Link Prediction,"Link Prediction, Triple Classification, Knowledge Graph Completion","Graph, Image, Time Series",,Computer Vision,link-prediction-on-codex-large,MIT,https://github.com/tsafavi/codex,https://paperswithcode.com/dataset/codex-large,"CoDEx comprises a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false.",,,,,,
627,CoDEx_Medium,Link Prediction,Link Prediction,"Link Prediction, Triple Classification, Knowledge Graph Completion","Graph, Image, Time Series",,Computer Vision,link-prediction-on-codex-medium,MIT,https://github.com/tsafavi/codex,https://paperswithcode.com/dataset/codex-medium,"CoDEx comprises a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false.",,,,,,
628,CoDEx_Small,Link Prediction,Link Prediction,"Link Prediction, Triple Classification, Knowledge Graph Completion","Graph, Image, Time Series",,Computer Vision,link-prediction-on-codex,MIT,https://github.com/tsafavi/codex,https://paperswithcode.com/dataset/codex,"CoDEx comprises a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false.",,,,,,
629,CoDraw,Language Modelling,Language Modelling,"Language Modelling, Scene Generation, Imitation Learning",Text,English,Natural Language Processing,,,https://github.com/facebookresearch/CoDraw,https://paperswithcode.com/dataset/codraw,"The Collaborative Drawing game (CoDraw) dataset contains ~10K dialogs consisting of ~138K messages exchanged between human players in the CoDraw game. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language.",,,,,,
630,CodRep,Program Repair,Program Repair,Program Repair,,,Methodology,,,https://zenodo.org/record/3413509,https://paperswithcode.com/dataset/codrep,"Five curated datasets of one-liner commits from open-source projects. In total,
they are composed of 58069 one-liner commits.",,,,,,
631,Coil100-Augmented,Image to 3D,Image to 3D,"Image to 3D, Classification, Disentanglement, Segmentation","3D, Image",,Computer Vision,,apache-2.0,https://huggingface.co/datasets/dappu97/Coil100-Augmented,https://paperswithcode.com/dataset/coil100-augmented,"This dataset derives from Coil100. 
There are more than 1,1M images of 100 objects. Each object was turned on a turnable through 360 degrees to vary object pose with respect to a fixed color camera. Images of the objects were taken at pose intervals of 5 degrees. This corresponds to 72 poses per object. Then planar rotation (9 angles) and  18 scaling factors has been applied.
Objects have a wide variety of complex geometric and reflectance characteristics.

This augmented version of Coil100 has been design for Disentangled Representation Learning for real images, the Factors of Variations are:

| Factors  | # values |
|----------|----------|
| Object   | 100      |
| 3D Pose  | 72       |
| Rotation | 9        |
| Scale    | 18       |

The binarized version is also available.",,,,1M images,,
632,COIN,Action Localization,Action Localization,"Action Localization, Temporal Action Localization, Action Recognition, Action Segmentation, Video Classification","Image, Time Series, Video",,Computer Vision,"action-segmentation-on-coin, video-classification-on-coin-1",Custom,https://coin-dataset.github.io/,https://paperswithcode.com/dataset/coin,"The COIN dataset (a large-scale dataset for COmprehensive INstructional video analysis) consists of 11,827 videos related to 180 different tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. The videos are all collected from YouTube. The average length of a video is 2.36 minutes. Each video is labelled with 3.91 step segments, where each segment lasts 14.91 seconds on average. In total, the dataset contains videos of 476 hours, with 46,354 annotated segments.",,,,,,
633,CoIR,,,", COVID-19 Diagnosis, 2D Object Detection, Object Detection, Code Search",Image,,Computer Vision,"code-search-on, code-search-on-coir, covid-19-diagnosis-on, on-1, 2d-object-detection-on-1, object-detection-on-4",Apache-2.0 license,https://github.com/CoIR-team/coir,https://paperswithcode.com/dataset/coir,"CoIR (Code Information Retrieval) benchmark, is designed to evaluate code retrieval capabilities. CoIR includes 10 curated code datasets, covering 8 retrieval tasks across 7 domains. In total, it encompasses two million documents. It also provides a common and easy Python framework, installable via pip, and shares the same data schema as benchmarks like MTEB and BEIR for easy cross-benchmark evaluations.",,,,,,
634,CoLA,Stochastic Optimization,Stochastic Optimization,"Stochastic Optimization, Linguistic Acceptability",,,Methodology,"linguistic-acceptability-on-cola-dev, linguistic-acceptability-on-cola, stochastic-optimization-on-cola",Custom,https://nyu-mll.github.io/CoLA/,https://paperswithcode.com/dataset/cola,"The Corpus of Linguistic Acceptability (CoLA) consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors. The public version contains 9594 sentences belonging to training and development sets, and excludes 1063 sentences belonging to a held out test set.",,https://arxiv.org/pdf/1805.12471.pdf,https://arxiv.org/pdf/1805.12471.pdf,10657 sentences,,
635,COLD__Causal_Reasoning_in_Closed_Daily_Activities,Event Causality Identification,Event Causality Identification,"Event Causality Identification, Commonsense Causal Reasoning",,,Methodology,,,https://huggingface.co/datasets/Exploration-Lab/COLD,https://paperswithcode.com/dataset/cold-causal-reasoning-in-closed-daily,"The causal reasoning dataset is generated using the Causal Reasoning in Closed Daily Activities (COLD) framework that helps evaluate large language models (LLMs) on their causal reasoning abilities within real-world, everyday activities. This dataset provides causal questions that simulate common activities such as shopping, baking a cake, riding a bus, planting a tree, and going on a train ride. With approximately 9 million causal queries, the COLD dataset challenges LLMs to understand and reason about the causal relationships between events that are familiar and grounded in human experience.

Each query consists of a premise (an event) and a pair of choices representing possible causal effects. The goal of the model is to correctly identify which choice is the most plausible cause/effect of the given premise, testing the model's understanding of cause-and-effect relationships.

Key Features:
Activity Types: The dataset covers various everyday activities: shopping, cake baking, train ride, tree planting, and bus ride.
Causal Queries: Each query includes a premise and two possible causal events (choices). The model must decide which of the two choices is the more likely cause or effect.
Multiple-Choice Format: The queries can be formatted as multiple-choice questions (MCQA), where the model must choose between two options.

The dataset provides a valuable test for causal reasoning in NLP models, focusing on realistic, daily-life scenarios.",,,,,,
636,Collective_Activity,Group Activity Recognition,Group Activity Recognition,Group Activity Recognition,"Image, Video",,Computer Vision,group-activity-recognition-on-collective,,http://vhosts.eecs.umich.edu/vision//activity-dataset.html,https://paperswithcode.com/dataset/collective-activity,"The Collective Activity Dataset contains 5 different collective activities: crossing, walking, waiting, talking, and queueing and 44 short video sequences some of which were recorded by consumer hand-held digital camera with varying view point.",,,,,,
637,Colorectal_Adenoma,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Multiple Instance Learning",Image,,Computer Vision,,,https://github.com/ThoroughImages/CAMEL,https://paperswithcode.com/dataset/colorectal-adenoma,"Colorectal Adenoma contains 177 whole slide images (156 contain adenoma) gathered and labelled by pathologists from the Department of Pathology, The Chinese PLA General Hospital.",,,,,,
638,Color_FERET,Face Quality Assessement,Face Quality Assessement,"Face Quality Assessement, Face Recognition",Image,,Computer Vision,"face-quality-assessement-on-color-feret, face-recognition-on-color-feret-online-open, face-recognition-on-color-feret",,https://catalog.data.gov/dataset/color-feret-database,https://paperswithcode.com/dataset/color-feret,"The color FERET database is a dataset for face recognition. It contains 11,338 color images of size 512×768 pixels captured in a semi-controlled environment with 13 different poses from 994 subjects.",,A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition,https://arxiv.org/abs/1606.02894,,,
639,ColosseumRL,Multi-agent Reinforcement Learning,Multi-agent Reinforcement Learning,Multi-agent Reinforcement Learning,,,Reinforcement Learning,,,https://colosseumrl.igb.uci.edu/,https://paperswithcode.com/dataset/colosseumrl,"ColosseumRL is a framework for research in reinforcement learning in n-player games.

ColosseumRL contains a number of multiagent free-for-all games. Currently, we have Tron, Blokus, and 3 and 4-player tic-tac-toe. In the future, we will be adding Chinese checkers and other similar games. Tron is a fully-observable multiagent free-for-all turn-based snake variant where players try to survive the longest without crashing into walls or each other. Blokus is a fully-observable multiagent free-for-all turn-based game in which players place pieces on a board to claim space and strategically block opponents from placing their own pieces.",,,,,,
640,Columbia__OSN-transmitted_-_Facebook_,Detecting Image Manipulation,Detecting Image Manipulation,"Detecting Image Manipulation, Image Manipulation, Image Forensics, Image Manipulation Detection",Image,,Computer Vision,image-manipulation-detection-on-columbia-osn,,https://github.com/HighwayWu/ImageForensicsOSN,https://paperswithcode.com/dataset/columbia-osn-transmitted,"This dataset is an OSN-transmitted (Online Social Network) version of the Columbia dataset. Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",2022,,,,,
641,COMA,Graph Representation Learning,Graph Representation Learning,Graph Representation Learning,Graph,,Methodology,graph-representation-learning-on-coma,Custom (non-commercial),https://coma.is.tue.mpg.de/,https://paperswithcode.com/dataset/coma,"CoMA contains 17,794 meshes of the human face in various expressions",,DEMEA: Deep Mesh Autoencoders for Non-Rigidly Deforming Objects,https://arxiv.org/abs/1905.10290,,,
642,Comet,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Response Generation",Text,English,Natural Language Processing,,Creative Commons Public Licenses,https://github.com/facebookresearch/comet_memory_dialog,https://paperswithcode.com/dataset/comet,"Comet is a dataset which contains 11.5k user-assistant dialogs (totalling 103k utterances), grounded in simulated personal memory graphs.",,Navigating Connected Memories with a Task-oriented Dialog System,https://arxiv.org/pdf/2211.08462v1.pdf,,,
643,ComFact,Response Generation,Response Generation,"Response Generation, Retrieval, Knowledge Graphs",Text,English,Natural Language Processing,,Apache-2.0 license,https://github.com/silin159/comfact,https://paperswithcode.com/dataset/comfact,"ComFact is a benchmark for commonsense fact linking, where models are given contexts and trained to identify situationally-relevant commonsense knowledge from KGs. The novel benchmark, C-om-Fact, contains ∼293k in-context relevance annotations for common-sense triplets across four stylistically diverse dialogue and storytelling datasets.",,ComFact: A Benchmark for Linking Contextual Commonsense Knowledge,https://arxiv.org/pdf/2210.12678v1.pdf,,,
644,comma_2k19,Autonomous Vehicles,Autonomous Vehicles,"Autonomous Vehicles, Adversarial Attack, Lane Detection",Image,,Computer Vision,,,https://github.com/commaai/comma2k19,https://paperswithcode.com/dataset/comma-2k19,"comma 2k19 is a dataset of over 33 hours of commute in California's 280 highway. This means 2019 segments, 1 minute long each, on a 20km section of highway driving between California's San Jose and San Francisco. The dataset was collected using comma EONs that have sensors similar to those of any modern smartphone including a road-facing camera, phone GPS, thermometers and a 9-axis IMU.",2019,,,,,
645,CommercialAdsDataset,Image-text matching,Image-text matching,Image-text matching,"Image, Text",English,Computer Vision,image-text-matching-on-commercialadsdataset,,https://github.com/microsoft/CommercialAdsDataset,https://paperswithcode.com/dataset/commercialadsdataset,"A large commercial Ads Dataset includes 480K labeled query-ad pairwise data with structured information of image, title, seller, description, and so on.",,,,,,
646,CommonsenseQA,Common Sense Reasoning,Common Sense Reasoning,Common Sense Reasoning,,,Reasoning,common-sense-reasoning-on-commonsenseqa,,https://www.tau-nlp.org/commonsenseqa,https://paperswithcode.com/dataset/commonsenseqa,"The CommonsenseQA is a dataset for commonsense question answering task. The dataset consists of 12,247 questions with 5 choices each.
The dataset was generated by Amazon Mechanical Turk workers in the following process (an example is provided in parentheses):


a crowd worker observes a source concept from ConceptNet (“River”) and three target concepts (“Waterfall”, “Bridge”, “Valley”) that are all related by the same ConceptNet relation (“AtLocation”),
the worker authors three questions, one per target concept, such that only that particular target concept is the answer, while the other two distractor concepts are not, (“Where on a river can you hold a cup upright to catch water on a sunny day?”, “Where can I stand on a river to see water falling without getting wet?”, “I’m crossing the river, my feet are wet but my body is dry, where am I?”)
for each question, another worker chooses one additional distractor from Concept Net (“pebble”, “stream”, “bank”), and the author another distractor (“mountain”, “bottom”, “island”) manually.",,,,,,
647,Common_Objects_in_3D,3D Reconstruction,3D Reconstruction,"3D Reconstruction, Single-View 3D Reconstruction",3D,,Methodology,single-view-3d-reconstruction-on-common,BSD License,https://github.com/facebookresearch/co3d,https://paperswithcode.com/dataset/common-objects-in-3d,"Common Objects in 3D is a large-scale dataset with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects.",,,,,,
648,Common_Voice,Cross-Lingual ASR,Cross-Lingual ASR,"Cross-Lingual ASR, Language Identification, Audio Classification, Speech Recognition, Automatic Speech Recognition, Few-Shot Audio Classification","Audio, Image, Text",English,Computer Vision,"speech-recognition-on-common-voice-chinese-2, cross-lingual-asr-on-common-voice, speech-recognition-on-mozilla-common-voice-16, speech-recognition-on-common-voice-vietnamese, speech-recognition-on-common-voice-8-0-breton, speech-recognition-on-common-voice-odia, speech-recognition-on-common-voice-french, speech-recognition-on-common-voice-portuguese, speech-recognition-on-common-voice-2, automatic-speech-recognition-on-mozilla-127, speech-recognition-on-common-voice-8-0-9, speech-recognition-on-common-voice-8-0-kabyle, automatic-speech-recognition-on-mozilla-84, speech-recognition-on-common-voice-8-0-41, speech-recognition-on-common-voice-vi, speech-recognition-on-common-voice-8-0-votic, speech-recognition-on-common-voice-turkish, speech-recognition-on-common-voice-8-0-30, speech-recognition-on-common-voice-8-0-10, speech-recognition-on-common-voice-7-0-votic, speech-recognition-on-common-voice-indonesian, automatic-speech-recognition-on-mcv17, automatic-speech-recognition-on-mozilla-114, speech-recognition-on-mozilla-common-voice-9, speech-recognition-on-common-voice-8-0-37, speech-recognition-on-common-voice-8-0-29, speech-recognition-on-common-voice-8-0-basaa, speech-recognition-on-common-voice-7-0-29, speech-recognition-on-common-voice-8-0-2, speech-recognition-on-common-voice-polish, speech-recognition-on-common-voice-8-0-kazakh, automatic-speech-recognition-on-mozilla-71, speech-recognition-on-common-voice-8-0-dutch, few-shot-audio-classification-on-common-voice, speech-recognition-on-common-voice-russian, speech-recognition-on-common-voice-8-0-20, speech-recognition-on-common-voice-8-0-7, audio-classification-on-common-voice-16-1, automatic-speech-recognition-on-mozilla-66, automatic-speech-recognition-on-mozilla-126, speech-recognition-on-common-voice-8-0-odia, automatic-speech-recognition-on-mozilla-108, speech-recognition-on-common-voice-8-0-german, speech-recognition-on-common-voice-8-0-11, speech-recognition-on-common-voice-breton, speech-recognition-on-common-voice-8-0-erzya, speech-recognition-on-common-voice-persian, speech-recognition-on-common-voice-7-0-arabic, speech-recognition-on-common-voice-7-0-abkhaz, speech-recognition-on-common-voice-dutch, speech-recognition-on-common-voice-8-0-3, automatic-speech-recognition-on-commonvoice-4, speech-recognition-on-common-voice-italian, speech-recognition-on-common-voice-8-0-32, speech-recognition-on-common-voice-czech, speech-recognition-on-common-voice-spanish, speech-recognition-on-common-voice-8-0-1, speech-recognition-on-common-voice-8-0-38, speech-recognition-on-common-voice-8-0-8, speech-recognition-on-common-voice-8-0-40, speech-recognition-on-common-voice-7-0-odia, speech-recognition-on-common-voice-7-0-hindi, speech-recognition-on-common-voice-welsh, speech-recognition-on-common-voice-8-0-tatar, speech-recognition-on-common-voice-7-0-german, speech-recognition-on-common-voice-8-0-french, speech-recognition-on-common-voice-8-0-33, automatic-speech-recognition-on-common-voice-17, automatic-speech-recognition-on-mozilla-63, automatic-speech-recognition-on-mozilla-64, speech-recognition-on-common-voice-7-0-1, speech-recognition-on-common-voice-german, automatic-speech-recognition-on-common-voice-18, speech-recognition-on-common-voice-8-0-hausa, speech-recognition-on-common-voice-frisian, speech-recognition-on-common-voice-lithuanian, speech-recognition-on-mozilla-common-voice-15, speech-recognition-on-common-voice-8-0-39, speech-recognition-on-common-voice-8-0-uzbek, speech-recognition-on-common-voice-georgian, speech-recognition-on-common-voice-arabic, speech-recognition-on-common-voice-japanese, speech-recognition-on-common-voice-hindi, speech-recognition-on-common-voice-swedish, automatic-speech-recognition-on-commonvoice-8, speech-recognition-on-common-voice-8-0-22, automatic-speech-recognition-on-mozilla-125, speech-recognition-on-common-voice-english, speech-recognition-on-common-voice-maltese, automatic-speech-recognition-on-mozilla-96, speech-recognition-on-common-voice-8-0-24, speech-recognition-on-common-voice-tamil, speech-recognition-on-common-voice-8-0-hindi",CC0,https://commonvoice.mozilla.org,https://paperswithcode.com/dataset/common-voice,"Common Voice is an audio dataset that consists of a unique MP3 and corresponding text file. There are 9,283 recorded hours in the dataset. The dataset also includes demographic metadata like age, sex, and accent. The dataset consists of 7,335 validated hours in 60 languages.",,,,,,
649,Completion_norms_for_3085_English_sentence_context,Cloze Test,Cloze Test,Cloze Test,,,Methodology,,CC BY,https://osf.io/jnhqb/,https://paperswithcode.com/dataset/completion-norms-for-3085-english-sentence,"In everyday language processing, sentence context affects how readers and listeners process upcoming words. In experimental situations, it can be useful to identify words that are predicted to greater or lesser degrees by the preceding context. Here we report completion norms for 3085 English sentences, collected online using a written cloze procedure in which participants were asked to provide their best guess for the word completing a sentence. Sentences varied between 8–10 words in length. At least 100 unique participants contributed to each sentence. All responses were reviewed by human raters to mitigate the influence of mis-spellings and typographical errors. The responses provide a range of predictability values for 13,438 unique target words, 6,790 of which appear in more than one sentence context. We also provide entropy values based on the relative predictability of multiple responses. Finally, we provide the code used to collate and organize the responses to facilitate additional analyses and future research projects.",,,,,,
650,ComplexWebQuestions,Semantic Parsing,Semantic Parsing,"Semantic Parsing, Knowledge Base Question Answering, Question Answering, Knowledge Graphs",Text,English,Natural Language Processing,"knowledge-base-question-answering-on, question-answering-on-complexwebquestions",,https://allenai.org/data/complexwebquestions,https://paperswithcode.com/dataset/complexwebquestions,"ComplexWebQuestions is a dataset for answering complex questions that require reasoning over multiple web snippets. It contains a large set of complex questions in natural language, and can be used in multiple ways:


By interacting with a search engine;
As a reading comprehension task: the authors release 12,725,989 web snippets that are relevant for the questions, and were collected during the development of their model;
As a semantic parsing task: each question is paired with a SPARQL query that can be executed against Freebase to retrieve the answer.",,Talmor et al,https://arxiv.org/pdf/1803.06643v1.pdf,,,
651,CompMix-IR,RAG,RAG,"RAG, Information Retrieval, Knowledge Graphs, Text Retrieval, Table Retrieval, Question Answering","Tabular, Text",English,Natural Language Processing,,CC-BY-4.0,https://huggingface.co/datasets/ZhishanQ/CompMix-IR,https://paperswithcode.com/dataset/compmix-ir,"CompMix-IR Dataset Overview:

Characteristics: CompMix-IR is a heterogeneous knowledge retrieval benchmark dataset, featuring four knowledge types (text, knowledge graphs, tables, and infoboxes), 9,400+ QA pairs, and a corpus of 10 million entries. It supports two retrieval scenarios: retrieving across all knowledge types or retrieving specific types based on user instructions.

Motivation: It addresses the limitations of existing benchmarks by providing a more comprehensive and realistic dataset that reflects real-world retrieval needs with diverse knowledge sources and user intents.

Potential Use Cases: Ideal for developing and evaluating heterogeneous IR models, instruction-aware retrieval systems, and open-domain QA systems. It can also be used for benchmarking, cross-domain IR research, and enhancing the adaptability and robustness of retrieval models.",,,,,,
652,Composed_Quora,Text Matching,Text Matching,Text Matching,Text,English,Natural Language Processing,,,https://github.com/fuzhenxin/Query-Bag-Matching-CIKM,https://paperswithcode.com/dataset/composed-quora,"The Composed Quora dataset consists of questions extracted from Quora that are grouped together if they are asking the same thing. The dataset contains 60,400 groups of questions, each group with at least 3 questions that are asking the same.",,https://arxiv.org/pdf/1911.02747.pdf,https://arxiv.org/pdf/1911.02747.pdf,,,
653,CoNaLa,Code Generation,Code Generation,"Code Generation, Code Search",Text,English,Natural Language Processing,code-generation-on-conala,,https://conala-corpus.github.io/,https://paperswithcode.com/dataset/conala,"The CMU CoNaLa, the Code/Natural Language Challenge dataset is a joint project from the Carnegie Mellon University NeuLab and Strudel labs. Its purpose is for testing the generation of code snippets from natural language. The data comes from StackOverflow questions. There are 2379 training and 500 test examples that were manually annotated. Every example has a natural language intent and its corresponding python snippet.  In addition to the manually annotated dataset, there are also 598,237 mined intent-snippet pairs. These examples are similar to the hand-annotated ones except that they contain a probability if the pair is valid.",,,,,testing the generation of code snippets from natural language. The data comes from StackOverflow questions. There are 2379 training and 500 test examples,
654,CONAN,Text Generation,Text Generation,"Text Generation, Data Augmentation, Language Identification",Text,English,Natural Language Processing,,,https://github.com/marcoguerini/CONAN,https://paperswithcode.com/dataset/conan,"COunter NArratives through Nichesourcing (CONAN) is a dataset that consists of 4,078 pairs over the 3 languages. Additionally, 3 types of metadata are provided: expert demographics, hate speech sub-topic and counter-narrative type. The dataset is augmented through translation (from Italian/French to English) and paraphrasing, which brought the total number of pairs to 14.988.",,,,,,
655,Concept-1K,Incremental Learning,Incremental Learning,Incremental Learning,,,Methodology,,CC BY,https://github.com/zzz47zzz/pretrained-lm-for-incremental-learning,https://paperswithcode.com/dataset/concept-1k,"Concept-1K contains 1023 novel concepts from six domains, including economy, culture, science and technology, environment, education, and health and medical. 
It has 16653 training-test QA pairs corresponding to 16653 knowledge points from 1023 concepts.
It is proposed for evaluating the forgetting in large language models and the effectiveness of incremental learning algorithms.",,,,,,
656,ConceptNet,Knowledge Graphs,Knowledge Graphs,"Knowledge Graphs, Word Embeddings, Question Answering, 16k",Text,English,Graphs,16k-on-conceptnet,CC BY-SA 4.0,https://github.com/commonsense/conceptnet5,https://paperswithcode.com/dataset/conceptnet,"ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use.",,ConceptNet 5.5: An Open Multilingual Graph of General Knowledge,https://arxiv.org/pdf/1612.03975v2.pdf,,,
657,Conceptual_Captions,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Text-to-Image Generation, Question Answering, Image Captioning","Image, Text",English,Computer Vision,"text-to-image-generation-on-conceptual, image-captioning-on-conceptual-captions",Custom,https://github.com/google-research-datasets/conceptual-captions,https://paperswithcode.com/dataset/conceptual-captions,"Automatic image captioning is the task of producing a natural-language utterance (usually a sentence) that correctly reflects the visual content of an image. Up to this point, the resource most used for this task was the MS-COCO dataset, containing around 120,000 images and 5-way image-caption annotations (produced by paid annotators).

Google's Conceptual Captions dataset has more than 3 million images, paired with natural-language captions. In contrast with the curated style of the MS-COCO images, Conceptual Captions images and their raw descriptions are harvested from the web, and therefore represent a wider variety of styles. The raw descriptions are harvested from the Alt-text HTML attribute associated with web images. The authors developed an automatic pipeline that extracts, filters, and transforms candidate image/caption pairs, with the goal of achieving a balance of cleanliness, informativeness, fluency, and learnability of the resulting captions.",,,,000 images,,
658,CONCODE,Semantic Parsing,Semantic Parsing,"Semantic Parsing, Code Generation, Program Synthesis",Text,English,Natural Language Processing,code-generation-on-concode,,https://github.com/sriniiyer/concode,https://paperswithcode.com/dataset/concode,"A new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment.",,,,000 examples,,
659,ConcurrentQA_Benchmark,Open-Domain Question Answering,Open-Domain Question Answering,"Open-Domain Question Answering, Privacy Preserving, Privacy Preserving Deep Learning, Multi-Hop Reading Comprehension, Multi-hop Question Answering, Question Answering, Reading Comprehension",Text,English,Natural Language Processing,multi-hop-question-answering-on-concurrentqa,MIT,https://github.com/facebookresearch/concurrentqa,https://paperswithcode.com/dataset/simran-arora,"ConcurrentQA is a textual multi-hop QA benchmark to require concurrent retrieval over multiple data-distributions (i.e. Wikipedia and email data). The dataset follow the exact same schema and design as HotpotQA. The data set is downloadable here: https://github.com/facebookresearch/concurrentqa. It also contains model and result analysis code. This benchmark can also be used to study privacy when reasoning over data distributed in multiple privacy scopes --- i.e. Wikipedia in the public domain and emails in the private domain.

The following is a blog post about the benchmark: https://ai.facebook.com/blog/building-systems-to-reason-securely-over-private-data/",,,,,,
660,CONG,Robotic Grasping,Robotic Grasping,Robotic Grasping,,,Methodology,,MIT,https://huggingface.co/datasets/jens-lundell/cong,https://paperswithcode.com/dataset/cong,A dataset for position-constrained robot grasp planning.,,,,,,
661,CoNLL-2000,Chunking,Chunking,"Chunking, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,"named-entity-recognition-on-conll-2000, chunking-on-conll-2000",,https://www.clips.uantwerpen.be/conll2000/chunking/,https://paperswithcode.com/dataset/conll-2000-1,"CoNLL-2000 is a dataset for dividing text into syntactically related non-overlapping groups of words, so-called text chunking.",2000,,,,,
662,CoNLL-2009,Chinese Semantic Role Labeling,Chinese Semantic Role Labeling,"Chinese Semantic Role Labeling, Dependency Parsing, Semantic Role Labeling",Text,English,Natural Language Processing,"semantic-role-labeling-on-conll-2009, dependency-parsing-on-conll-2009, chinese-semantic-role-labeling-on-conll-2009",,https://catalog.ldc.upenn.edu/LDC2012T04,https://paperswithcode.com/dataset/conll-2009,"The task builds on the CoNLL-2008 task and extends it to multiple languages. The core of the task is to predict syntactic and semantic dependencies and their labeling. Data is provided for both statistical training and evaluation, which extract these labeled dependencies from manually annotated treebanks such as the Penn Treebank for English, the Prague Dependency Treebank for Czech and similar treebanks for Catalan, Chinese, German, Japanese and Spanish languages, enriched with semantic relations (such as those captured in the Prop/Nombank and similar resources). Great effort has been devoted to provide the participants with a common and relatively simple data representation for all the languages, similar to the last year's English data.",2008,,,,,
663,CoNLL-2012,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Semantic Role Labeling (predicted predicates), Semantic Role Labeling, Named Entity Recognition, Predicate Detection","Image, Text",English,Computer Vision,"named-entity-recognition-on-conll-2012-1, semantic-role-labeling-on-conll-2012, predicate-detection-on-conll-2012, coreference-resolution-on-conll-2012, semantic-role-labeling-predicted-predicates-1",,https://www.aclweb.org/anthology/W12-4501.pdf,https://paperswithcode.com/dataset/conll-2012-1,"The CoNLL-2012 shared task involved predicting coreference in English, Chinese, and Arabic, using the final version, v5.0, of the OntoNotes corpus. It was a follow-on to the English-only task organized in 2011.",2012,Pradhan et al.,https://www.aclweb.org/anthology/W12-4501.pdf,,,
664,CoNLL-2014_Shared_Task__Grammatical_Error_Correcti,Grammatical Error Correction,Grammatical Error Correction,Grammatical Error Correction,,,Methodology,"grammatical-error-correction-on-conll-2014-1, grammatical-error-correction-on-conll-2014",Custom,https://www.comp.nus.edu.sg/~nlp/conll14st.html,https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error,"CoNLL-2014 will continue the CoNLL tradition of having a high profile shared task in natural language processing. This year's shared task will be grammatical error correction, a continuation of the CoNLL shared task in 2013. A participating system in this shared task is given short English texts written by non-native speakers of English. The system detects the grammatical errors present in the input texts, and returns the corrected essays. The shared task in 2014 will require a participating system to correct all errors present in an essay (i.e., not restricted to just five error types in 2013). Also, the evaluation metric will be changed to F0.5, weighting precision twice as much as recall.

The grammatical error correction task is impactful since it is estimated that hundreds of millions of people in the world are learning English and they benefit directly from an automated grammar checker. However, for many error types, current grammatical error correction methods do not achieve a high performance and thus more research is needed.",2014,Tou Ng et al.,https://www.aclweb.org/anthology/W14-1701.pdf,,,
665,CoNLL,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Grammatical Error Detection, Semantic Role Labeling (predicted predicates), Chinese Semantic Role Labeling, Cross-Lingual NER, FG-1-PG-1, Predicate Detection, Part-Of-Speech Tagging, Low Resource Named Entity Recognition, Morphological Tagging, Entity Disambiguation, Semantic Role Labeling, Named Entity Recognition (NER), Cross-Lingual Transfer, UCCA Parsing, Sequential sentence segmentation, Token Classification, Entity Linking, Entity Typing, Weakly-Supervised Named Entity Recognition, Dependency Parsing, Text Segmentation, Sentence segmentation, Chunking, Grammatical Error Correction, Cross-Domain Named Entity Recognition, Named Entity Recognition, Joint Entity and Relation Extraction, Relation Extraction","Audio, Graph, Image, Text",English,Computer Vision,"named-entity-recognition-on-conll-2000, named-entity-recognition-on-conll-2003-german, token-classification-on-conll2003, dependency-parsing-on-conll-2009, chunking-on-conll-2000, predicate-detection-on-conll-2005, named-entity-recognition-on-conll, fg-1-pg-1-on-conll2003, semantic-role-labeling-on-conll-2012, named-entity-recognition-ner-on-conll-2003, entity-typing-on-aida-conll, named-entity-recognition-on-conll-2003-german-1, named-entity-recognition-on-conll-2002, relation-extraction-on-conll04, cross-lingual-ner-on-conll-spanish, semantic-role-labeling-on-conll-2005, low-resource-named-entity-recognition-on-6, grammatical-error-correction-on-conll-2014-1, chunking-on-conll-2003-english, grammatical-error-detection-on-conll-2014-a2, coreference-resolution-on-conll12, entity-linking-on-aida-conll, semantic-role-labeling-on-conll05-wsj, semantic-role-labeling-on-conll05-brown, cross-domain-named-entity-recognition-on, grammatical-error-detection-on-conll-2014-a1, chunking-on-conll-2003-german, cross-lingual-ner-on-conll-german, chinese-semantic-role-labeling-on-conll-2009, weakly-supervised-named-entity-recognition-on, coreference-resolution-on-conll-2012, ucca-parsing-on-conll-2019, semantic-role-labeling-predicted-predicates, semantic-role-labeling-predicted-predicates-1, low-resource-named-entity-recognition-on-4, named-entity-recognition-on-conll-2012-1, semantic-role-labeling-on-conll12, cross-lingual-ner-on-conll-dutch, entity-linking-on-conll-aida, low-resource-named-entity-recognition-on-5, semantic-role-labeling-on-conll-2009, joint-entity-and-relation-extraction-on-2, named-entity-recognition-on-conll03, named-entity-recognition-on-conll-2003-3, entity-disambiguation-on-aida-conll, grammatical-error-correction-on-conll-2014, predicate-detection-on-conll-2012, named-entity-recognition-on-conll-2002-dutch",,https://www.conll.org/,https://paperswithcode.com/dataset/conll-1,The CoNLL dataset is a widely used resource in the field of natural language processing (NLP). The term “CoNLL” stands for Conference on Natural Language Learning. It originates from a series of shared tasks organized at the Conferences of Natural Language Learning.,,,,,,
666,CoNLL04,Cross-Domain Named Entity Recognition,Cross-Domain Named Entity Recognition,"Cross-Domain Named Entity Recognition, Relation Extraction, Joint Entity and Relation Extraction","Graph, Image, Text",English,Computer Vision,"cross-domain-named-entity-recognition-on, joint-entity-and-relation-extraction-on-2, relation-extraction-on-conll04",,https://cogcomp.seas.upenn.edu/page/resource_view/43,https://paperswithcode.com/dataset/conll04,"The CoNLL04 dataset is a benchmark dataset used for relation extraction tasks. It contains 1,437 sentences, each of which has at least one relation. The sentences are annotated with information about entities and their corresponding relation types.",,,,437 sentences,,
667,CoNLL_2002,Part-Of-Speech Tagging,Part-Of-Speech Tagging,"Part-Of-Speech Tagging, Named Entity Recognition (NER), Cross-Lingual Transfer","Audio, Image, Text",English,Speech,"named-entity-recognition-on-conll-2002, named-entity-recognition-on-conll-2002-dutch",,https://www.clips.uantwerpen.be/conll2002/ner/,https://paperswithcode.com/dataset/conll-2002,"The shared task of CoNLL-2002 concerns language-independent named entity recognition. The types of named entities include: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. The participants of the shared task were offered training and test data for at least two languages. Information sources other than the training data might have been used in this shared task.",2002,https://www.aclweb.org/anthology/W02-2024.pdf,https://www.aclweb.org/anthology/W02-2024.pdf,,,
668,CoNLL_2003,Token Classification,Token Classification,"Token Classification, POS, UIE, Semantic Similarity, Cross-Lingual NER, Knowledge Distillation, Named Entity Recognition, NER, FG-1-PG-1, Weakly-Supervised Named Entity Recognition, Low Resource Named Entity Recognition, Chunking, Named Entity Recognition (NER), Text Classification","Image, Text",English,Computer Vision,"named-entity-recognition-on-conll-2003-german, token-classification-on-conll2003, pos-on-conll-2003, fg-1-pg-1-on-conll2003, named-entity-recognition-ner-on-conll-2003, named-entity-recognition-on-conll-2003-german-1, ner-on-conll-2003-1, semantic-similarity-on-unknown, chunking-on-conll-2003, uie-on-conll-2003, chunking-on-conll-2003-english, chunking-on-conll-2003-german, text-classification-on-unknown, weakly-supervised-named-entity-recognition-on, low-resource-named-entity-recognition-on-4, knowledge-distillation-on-unknown, named-entity-recognition-on-conll03, named-entity-recognition-on-conll-2003-3, cross-lingual-ner-on-conll-2003",,https://www.clips.uantwerpen.be/conll2003/ner/,https://paperswithcode.com/dataset/conll-2003,"CoNLL-2003 is a named entity recognition dataset released as a part of CoNLL-2003 shared task: language-independent named entity recognition.
The data consists of eight files covering two languages: English and German.
For each of the languages there is a training file, a development file, a test file and a large file with unannotated data.

The English data was taken from the Reuters Corpus. This corpus consists of Reuters news stories between August 1996 and August 1997.
For the training and development set, ten days worth of data were taken from the files representing the end of August 1996.
For the test set, the texts were from December 1996. The preprocessed raw data covers the month of September 1996.

The text for the German data was taken from the ECI Multilingual Text Corpus. This corpus consists of texts in many languages. The portion of data that
was used for this task, was extracted from the German newspaper Frankfurter Rundshau. All three of the training, development and test sets were taken
from articles written in one week at the end of August 1992.
The raw data were taken from the months of September to December 1992.

| English      data | Articles | Sentences | Tokens  | LOC  | MISC | ORG  | PER  |
|-------------------|----------|-----------|---------|------|------|------|------|
| Training     set  | 946      | 14,987    | 203,621 | 7140 | 3438 | 6321 | 6600 |
| Development  set  | 216      | 3,466     | 51,362  | 1837 | 922  | 1341 | 1842 |
| Test         set  | 231      | 3,684     | 46,435  | 1668 | 702  | 1661 | 1617 |

Number of articles, sentences, tokens and entities (locations, miscellaneous, organizations, and persons) in English data files.

| German       data | Articles | Sentences | Tokens  | LOC  | MISC | ORG  | PER  |
|-------------------|----------|-----------|---------|------|------|------|------|
| Training     set  | 553      | 12,705    | 206,931 | 4363 | 2288 | 2427 | 2773 |
| Development  set  | 201      | 3,068     | 51,444  | 1181 | 1010 | 1241 | 1401 |
| Test         set  | 155      | 3,160     | 51,943  | 1035 | 670  | 773  | 1195 |

Number of articles, sentences, tokens and entities (locations, miscellaneous, organizations, and persons) in German data files.",2003,,,,,
669,CoNLL_2017_Shared_Task_-_Automatically_Annotated_R,Morphological Tagging,Morphological Tagging,"Morphological Tagging, Sequential sentence segmentation, Part-Of-Speech Tagging, Text Segmentation, Dependency Parsing, Sentence segmentation","Audio, Image, Text",English,Computer Vision,,Creative Commons Attribution 4.0 International,http://hdl.handle.net/11234/1-1989,https://paperswithcode.com/dataset/conll-2017-shared-task-automatically,"Automatic segmentation, tokenization and morphological and syntactic annotations of raw texts in 45 languages, generated by UDPipe (http://ufal.mff.cuni.cz/udpipe), together with word embeddings of dimension 100 computed from lowercased texts by word2vec (https://code.google.com/archive/p/word2vec/).",,,,,,
670,ConsInv_Dataset,Visual Odometry,Visual Odometry,"Visual Odometry, Monocular Visual Odometry, Object SLAM, Semantic SLAM",Image,,Computer Vision,,,https://github.com/adrianbojko/consinv-dataset,https://paperswithcode.com/dataset/consinv-dataset,"ConsInv is a stereo RGB + IMU dataset designed for Dynamic SLAM testing and contains two subsets:


ConsInv-Indoors contains sequences in an office setting where small objects are moved.
ConsInv-Outdoors contains sequences in an urban environment, where cars and/or people move.

The novelty of ConsInv dataset is 1) the controlled degree of difficulty, from easy to very hard, and 2) the fact that the difficulty of the sequences comes only from object motion: relative motion between camera and object, motion ambiguity, challenging points of view when objects move. The difficulty does not come from motion speed, lack of features, lens flare, etc. - typically seen in other SLAM datasets.",,,,,,
671,ConsisID-preview-Data,Text-to-Video Generation,Text-to-Video Generation,"Text-to-Video Generation, Video Generation, Image to Video Generation","Image, Text, Video",English,Computer Vision,,Apache-2.0 license,https://pku-yuangroup.github.io/ConsisID/,https://paperswithcode.com/dataset/consisid-preview-data,"Description

Repository: Code, Page, Data
Paper: arxiv.org/abs/2411.17440
Point of Contact: Shenghai Yuan

Citation
If you find our paper and code useful in your research, please consider giving a star and citation.

BibTeX
@article{yuan2024identity,
  title={Identity-Preserving Text-to-Video Generation by Frequency Decomposition},
  author={Yuan, Shenghai and Huang, Jinfa and He, Xianyi and Ge, Yunyuan and Shi, Yujun and Chen, Liuhan and Luo, Jiebo and Yuan, Li},
  journal={arXiv preprint arXiv:2411.17440},
  year={2024}
}",2024,,,,,
672,ConSLAM,3D Pose Estimation,3D Pose Estimation,"3D Pose Estimation, Camera Pose Estimation, Camera Relocalization, Pose Estimation, Depth Completion, Depth Estimation, Camera Localization, Simultaneous Localization and Mapping, Pose Tracking, Indoor Localization, Robot Pose Estimation, Semantic SLAM, 6D Pose Estimation, Object SLAM, 2D Pose Estimation, lidar absolute pose regression, Visual Odometry, Monocular Visual Odometry","3D, Image, Video",,Computer Vision,,University of Cambridge,https://github.com/mac137/ConSLAM,https://paperswithcode.com/dataset/conslam,"ConSLAM is a real-world dataset collected periodically on a construction site to measure the accuracy of mobile scanners' SLAM algorithms. 

The dataset contains time-synchronized and spatially registered RGB and NIR images and 360-deg LiDAR scans, 9-axis IMU measurements, and professional ground-truth terrestrial laser scans.
This dataset reflects the periodic need to scan construction sites with the aim of accurately monitoring progress using a hand-held scanner.
The sensors used for data acquisition are:
- LiDAR: Velodyne VLP-16.
- RGB camera: Alvium U-319c, 3.2 MP. 
- NIR camera: Alvium 1800 U-501, 5.0 MP. 
- 9-axis IMU: Xsens MTi-610.",,,,,,
673,Consumer_Spendings,Time Series Classification,Time Series Classification,"Time Series Classification, Time Series Analysis, Time Series Forecasting, Time Series Anomaly Detection","Image, Time Series",,Time Series,time-series-forecasting-on-finance,,https://github.com/ashfarhangi/AA-Forecast/tree/main/dataset,https://paperswithcode.com/dataset/finance,"State-level data for the US economy through the lens of consumer spending (Credit/Debit Spending) .  The dataset is enriched with state-level Economic Dynamics and Policy Responses. Specifically, we further enriched the data with the state-level policies as an indication of extreme events (e.g., the state’s business closure order). 


Date: The date of the data record.
StateAbbr: Abbreviation of the state.
Population: Population of the state.
Spend_xxx: Various spending metrics (e.g., spend_acf, spend_aer, spend_all). Spend_all (all categories)
Policy: Indicator of policy measures.
lnDailyNewDeaths, DailyNewDeaths: Logarithm and raw counts of daily new deaths.
lnDailyNewCases, DailyNewCases: Logarithm and raw counts of daily new cases.
Deaths, Cases: Cumulative counts.
Day, Day_, DayOfYear: Day-related information.",,,,,,
674,ContactDB,Grasp Contact Prediction,Grasp Contact Prediction,Grasp Contact Prediction,Time Series,,Methodology,human-grasp-contact-prediction-on-contactdb,Custom,https://github.com/samarth-robo/contactdb_utils,https://paperswithcode.com/dataset/contactdb,"ContactDB is a dataset of contact maps for household objects that captures the rich hand-object contact that occurs during grasping, enabled by use of a thermal camera. ContactDB includes 3,750 3D meshes of 50 household objects textured with contact maps and 375K frames of synchronized RGB-D+thermal images.",,https://arxiv.org/abs/1904.06830,https://arxiv.org/abs/1904.06830,,,
675,ContactPose,Grasp Contact Prediction,Grasp Contact Prediction,Grasp Contact Prediction,Time Series,,Methodology,grasp-contact-prediction-on-contactpose,,https://github.com/facebookresearch/ContactPose,https://paperswithcode.com/dataset/contactpose,"ContactPose is a dataset of hand-object contact paired with hand pose, object pose, and RGB-D images. ContactPose has 2306 unique grasps of 25 household objects grasped with 2 functional intents by 50 participants, and more than 2.9 M RGB-D grasp images.",,,,,,
676,ContractNLI,Multi-Label Text Classification,Multi-Label Text Classification,"Multi-Label Text Classification, Natural Language Inference","Image, Text",English,Computer Vision,,CC BY 4.0,https://stanfordnlp.github.io/contract-nli/,https://paperswithcode.com/dataset/contractnli,"ContractNLI is a dataset for document-level natural language inference (NLI) on contracts whose goal is to automate/support a time-consuming procedure of contract review. In this task, a system is given a set of hypotheses (such as “Some obligations of Agreement may survive termination.”) and a contract, and it is asked to classify whether each hypothesis is entailed by, contradicting to or not mentioned by (neutral to) the contract as well as identifying evidence for the decision as spans in the contract.

ContractNLI is the first dataset to utilize NLI for contracts and is also the largest corpus of annotated contracts (as of September 2021). ContractNLI is an interesting challenge to work on from a machine learning perspective (the label distribution is imbalanced and it is naturally multi-task, all the while training data being scarce) and from a linguistic perspective (linguistic characteristics of contracts, particularly negations by exceptions, make the problem difficult).",2021,,,,,
677,Contract_Discovery,Semantic Retrieval,Semantic Retrieval,Semantic Retrieval,,,Methodology,semantic-retrieval-on-contract-discovery,,https://github.com/applicaai/contract-discovery,https://paperswithcode.com/dataset/contract-discovery,"A new shared task of semantic retrieval from legal texts, in which a so-called contract discovery is to be performed, where legal clauses are extracted from documents, given a few examples of similar clauses from other legal acts.",,,,,"val from legal texts, in which a so-called contract discovery is to be performed, where legal clauses are extracted from documents, given a few examples",
678,Controversial_News_Topic_Datasets,Interpretable Machine Learning,Interpretable Machine Learning,Interpretable Machine Learning,,,Methodology,,,https://github.com/computationalmedia/compsumm,https://paperswithcode.com/dataset/controversial-news-topic-datasets,"Corpus of controversial news articles extracted from Twitter. Contains news from three different topics: Beef Ban – controversy over the slaughter and sale of beef on religious grounds (1543
articles) is localised to a particular region, mainly Indian subcontinent, while Gun Control – restrictions on carrying, using, or purchasing firearms (6494 articles) and Capital Punishment – use of the death penalty (7905 articles) are
topical in various regions around the world.",,,,,,
679,ConvAI2,Visual Dialog,Visual Dialog,Visual Dialog,Image,,Computer Vision,visual-dialog-on-convai2,Custom,https://parl.ai/projects/convai2/,https://paperswithcode.com/dataset/convai2,"The ConvAI2 NeurIPS competition aimed at finding approaches to creating high-quality dialogue agents capable of meaningful open domain conversation. The ConvAI2 dataset for training models is based on the PERSONA-CHAT dataset. The speaker pairs each have assigned profiles coming from a set of 1155 possible personas (at training time), each consisting of at least 5 profile sentences, setting aside 100 never seen before personas for validation. As the original PERSONA-CHAT test set was released, a new hidden test set consisted of 100 new personas and over 1,015 dialogs was created by crowdsourced workers.

To avoid modeling that takes advantage of trivial word overlap, additional rewritten sets of the same train and test personas were crowdsourced, with related sentences that are rephrases, generalizations or specializations, rendering the task much more challenging. For example “I just got my nails done” is revised as “I love to pamper myself on a regular basis” and “I am on a diet now” is revised as “I need to lose weight.”

The training, validation and hidden test sets consists of 17,878, 1,000 and 1,015 dialogues, respectively.",,,,,,
680,Conversational_Stance_Detection,Opinion Mining,Opinion Mining,"Opinion Mining, Stance Detection",Image,,Computer Vision,,,https://anonymous.4open.science/r/CSD-5A8D/README.md,https://paperswithcode.com/dataset/conversational-stance-detection,Conversational Stance Detection (CSD) is a dataset with annotations of stances and the structures of conversation threads. It consists of 500 conversation threads (including 500 posts and 5376 comments) from six major social media platforms in Hong Kong.,,Improved Target-specific Stance Detection on Social Media Platforms by Delving into Conversation Threads,https://arxiv.org/pdf/2211.03061v1.pdf,,,
681,CoQA,Conversational Question Answering,Conversational Question Answering,"Conversational Question Answering, Generative Question Answering, Question Answering, Reading Comprehension",Text,English,Natural Language Processing,"generative-question-answering-on-coqa, question-answering-on-coqa",Custom (multiple),https://stanfordnlp.github.io/coqa/,https://paperswithcode.com/dataset/coqa,"CoQA is a large-scale dataset for building Conversational Question Answering systems. The goal of the CoQA challenge is to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation.

CoQA contains 127,000+ questions with answers collected from 8000+ conversations. Each conversation is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers. The unique features of CoQA include 1) the questions are conversational; 2) the answers can be free-form text; 3) each answer also comes with an evidence subsequence highlighted in the passage; and 4) the passages are collected from seven diverse domains. CoQA has a lot of challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning.",,,,,,
682,Cora,Graph Classification,Graph Classification,"Graph Classification, Node Clustering, Graph Clustering, Link Prediction, Community Detection, Document Classification, Node Classification, Graph structure learning","Graph, Image, Text, Time Series",English,Computer Vision,"node-classification-on-cora-fixed-20-node-per, graph-structure-learning-on-cora, node-classification-on-cora-1, graph-classification-on-cora, node-classification-on-cora-3, node-classification-on-cora-with-public-split, node-classification-on-cora-fixed-10-node-per, link-prediction-on-cora-nonstandard-variant, 30-trainning-unsupervised-with-linear, node-classification-on-cora-05, node-classification-on-cora-full-supervised, node-classification-on-cora-random-partition, link-prediction-on-cora-biased-evaluation, graph-clustering-on-cora, node-clustering-on-cora, link-prediction-on-cora, node-classification-on-cora, document-classification-on-cora, community-detection-on-cora, node-classification-on-cora-fixed-5-node-per",,https://relational.fit.cvut.cz/dataset/CORA,https://paperswithcode.com/dataset/cora,The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.,,https://arxiv.org/abs/1611.08402,https://arxiv.org/abs/1611.08402,,,
683,CoRAL_dataset,Abusive Language,Abusive Language,"Abusive Language, Abuse Detection, Hate Speech Detection","Audio, Image, Text",English,Computer Vision,,,https://github.com/shekharRavi/CoRAL-dataset-Findings-of-the-ACL-AACL-IJCNLP-2022,https://paperswithcode.com/dataset/coral-dataset,CoRAL is a language and culturally aware Croatian Abusive dataset covering phenomena of implicitness and reliance on local and global context.,,,,,,
684,CORD-19,Text Summarization,Text Summarization,"Text Summarization, Unsupervised Text Summarization, Information Retrieval, Knowledge Graphs, Question Answering",Text,English,Natural Language Processing,"text-summarization-on-cord-19, unsupervised-text-summarization-on-cord-19",Semantic Scholar Dataset License,https://allenai.org/data/cord-19,https://paperswithcode.com/dataset/cord-19,"CORD-19 is a free resource of tens of thousands of scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses for use by the global research community.",,,,,,
685,CORD,Key Information Extraction,Key Information Extraction,"Key Information Extraction, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,key-information-extraction-on-cord,Creative Commons Attribution 4.0 International,https://github.com/clovaai/cord,https://paperswithcode.com/dataset/cord,"OCR is inevitably linked to NLP since its final output is in text. Advances in document intelligence are driving the need for a unified technology that integrates OCR with various NLP tasks, especially semantic parsing. Since OCR and semantic parsing have been studied as separate tasks so far, the datasets for each task on their own are rich, while those for the integrated post-OCR parsing tasks are relatively insufficient. In this study, we publish a consolidated dataset for receipt parsing as the first step towards post-OCR parsing tasks. The dataset consists of thousands of Indonesian receipts, which contains images and box/text annotations for OCR, and multi-level semantic labels for parsing. The proposed dataset can be used to address various OCR and parsing tasks.",,,,,,
686,CORe50,Incremental Learning,Incremental Learning,"Incremental Learning, Continual Learning, Object Recognition",Image,,Computer Vision,,CC BY 4.0,https://vlomonaco.github.io/core50/,https://paperswithcode.com/dataset/core50,CORe50 is a dataset designed for assessing Continual Learning techniques in an Object Recognition context.,,,,,,
687,CoreSearch,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Open-Domain Question Answering, Retrieval, Cross Document Coreference Resolution, Question Answering, Passage Retrieval",Text,English,Natural Language Processing,,Apache-2.0 license,https://huggingface.co/datasets/Intel/CoreSearch,https://paperswithcode.com/dataset/coresearch,"CoreSearch is a dataset for Cross-Document Event Coreference Search. It consists of two separate passage collections: (1) a collection of passages containing manually annotated coreferring event mention, and (2) an annotated collection of destructor passages.",,"Cross-document Event Coreference Search: Task, Dataset and Modeling",https://arxiv.org/pdf/2210.12654v1.pdf,,,
688,Cornell,Node Clustering,Node Clustering,"Node Clustering, Robotic Grasping, Node Classification",Image,,Computer Vision,"node-classification-on-cornell, robotic-grasping-on-cornell-grasp-dataset-1, node-clustering-on-cornell",,,https://paperswithcode.com/dataset/cornell,,,,,,,
689,Cornell__48__32__20__fixed_splits_,Node Classification on Non-Homophilic (Heterophilic) Graphs,Node Classification on Non-Homophilic (Heterophilic) Graphs,"Node Classification on Non-Homophilic (Heterophilic) Graphs, Node Classification",Image,,Graphs,"node-classification-on-non-homophilic-7, node-classification-on-cornell-48-32-20-fixed",,,https://paperswithcode.com/dataset/cornell-48-32-20-fixed-splits,Node classification on Cornell with the fixed 48%/32%/20% splits provided by Geom-GCN.,,,,,,
690,Corn_Seeds_Dataset,Zero-Shot Image Classification,Zero-Shot Image Classification,"Zero-Shot Image Classification, Fake Image Detection, Multi-Label Learning, Semantic Segmentation, Image Classification, 3D Object Reconstruction, Fine-Grained Image Classification, Image Generation, Image Classification with Label Noise, 3D Object Recognition, 3D Classification, 2D Semantic Segmentation","3D, Audio, Image, Text",English,Computer Vision,,,https://naagar.github.io/cornseedsdataset/,https://paperswithcode.com/dataset/corn-seeds-dataset,"This dataset is the images of corn seeds considering the top and bottom view independently (two images for one corn seed: top and bottom). There are four classes of the corn seed (Broken-B, Discolored-D, Silkcut-S, and Pure-P) 17802 images are labeled by the experts at the AdTech Corp. and 26K images were unlabeled out of which 9k images were labeled using the Active Learning (BatchBALD)

We have created three different dataset: (1). Primary dataset: contains the 17802 images labeled by the experts. Top-view(8901) and Bottom-view(8901).

(2). Dataset with fake images: We generated fake images using Conditional GAN (BigGAN) as follows: broken-2937, discolored-5823, pure-2937, silkcut-5823 instances and added them into the train set to balance the data set.

(3). Balanced dataset: In this case of adding newly captured images labeled using the Batch Active Learning method, new 9000 labeled images are added into the primary dataset. This new dataset contains 26,802 images split into train and validation set 80: 20, respectively. Contains the 17802 images and the 9K images labeled by the Active Learning (BatchBALD).",,,,17802 images,"split into train and validation set 80: 20, respectively. Contains the 17802 images",
691,CORSMAL,6D Pose Estimation,6D Pose Estimation,"6D Pose Estimation, Pose Estimation","3D, Image",,Computer Vision,,,https://corsmal.eecs.qmul.ac.uk/pose.html,https://paperswithcode.com/dataset/corsmal,"CORSMAL is a dataset for estimating the position and orientation in 3D (or 6D pose) of an object from a single view. The dataset consists of 138,240 images of rendered hands and forearms holding 48 synthetic objects, split into 3 grasp categories over 30 real backgrounds.",,A mixed-reality dataset for category-level 6D pose and size estimation of hand-occluded containers,https://arxiv.org/pdf/2211.10470v1.pdf,240 images,,
692,CoS-E,Question Answering,Question Answering,"Question Answering, Reading Comprehension, Common Sense Reasoning",Text,English,Natural Language Processing,,,https://github.com/salesforce/cos-e,https://paperswithcode.com/dataset/cos-e,CoS-E consists of human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations,,Explain Yourself! Leveraging Language Models for Commonsense Reasoning,https://arxiv.org/pdf/1906.02361v1.pdf,,,
693,COS960,Semantic Similarity,Semantic Similarity,"Semantic Similarity, multi-word expression sememe prediction, multi-word expression embedding",Time Series,,Methodology,,,https://github.com/thunlp/COS960,https://paperswithcode.com/dataset/cos960,"A benchmark dataset with 960 pairs of Chinese wOrd Similarity, where all the words have two morphemes in three Part of Speech (POS) tags with their human annotated similarity rather than relatedness.",,,,,,
694,CoSal2015,Co-Salient Object Detection,Co-Salient Object Detection,Co-Salient Object Detection,Image,,Computer Vision,co-salient-object-detection-on-cosal2015,,,https://paperswithcode.com/dataset/cosal2015,"Cosal2015 is a large-scale dataset for co-saliency detection which consists of 2,015 images of 50 categories, and each group suffers from various challenging factors such as complex environments, occlusion issues, target appearance variations and background clutters, etc. All these increase the difficulty for accurate co-saliency detection.",,Adaptive Graph Convolutional Network with Attention Graph Clustering for Co-saliency Detection,https://arxiv.org/abs/2003.06167,015 images,,50
695,CosmosQA,Natural Language Understanding,Natural Language Understanding,"Natural Language Understanding, Question Answering, Reading Comprehension",Text,English,Natural Language Processing,,,https://wilburone.github.io/cosmos/,https://paperswithcode.com/dataset/cosmosqa,"CosmosQA is a large-scale dataset of 35.6K problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. It focuses on reading between the lines over a diverse collection of people’s everyday narratives, asking questions concerning on the likely causes or effects of events that require reasoning beyond the exact text spans in the context.",,Teaching Pretrained Models with Commonsense Reasoning: A Preliminary KB-Based Approach,https://arxiv.org/abs/1909.09743,,,
696,CoSQL,Text-To-SQL,Text-To-SQL,"Text-To-SQL, Semantic Parsing, Dialogue State Tracking","Image, Text, Video",English,Computer Vision,dialogue-state-tracking-on-cosql,,https://yale-lily.github.io/cosql,https://paperswithcode.com/dataset/cosql,"CoSQL is a corpus for building cross-domain, general-purpose database (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k dialogues querying 200 complex DBs spanning 138 domains. Each dialogue simulates a real-world DB query scenario with a crowd worker as a user exploring the DB and a SQL expert retrieving answers with SQL, clarifying ambiguous questions, or otherwise informing of unanswerable questions.",,CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases,https://arxiv.org/abs/1909.05378,,,
697,COSTRA_1.0,Sentence Embedding,Sentence Embedding,"Sentence Embedding, Sentence Embeddings",,,Methodology,,,https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3123,https://paperswithcode.com/dataset/costra-1-0,"COSTRA 1.0 is a dataset of complex sentence transformations. The dataset is intended for the study of sentence-level embeddings beyond simple word alternations or standard paraphrasing. The first version of the dataset is limited to sentences in Czech but the construction method is universal and the authors plan to use it also for other languages. The dataset consist of 4,262 unique sentences with average length of 10 words, illustrating 15 types of modifications such as simplification, generalization, or formal and informal language variation.",,,,,,
698,Coveo_Data_Challenge_Dataset,Product Recommendation,Product Recommendation,Product Recommendation,,,Methodology,product-recommendation-on-coveo-data,,https://github.com/coveooss/SIGIR-ecom-data-challenge,https://paperswithcode.com/dataset/coveo-data-challenge-dataset,"The 2021 SIGIR workshop on eCommerce is hosting the Coveo Data Challenge for ""In-session prediction for purchase intent and recommendations"". The challenge addresses the growing need for reliable predictions within the boundaries of a shopping session, as customer intentions can be different depending on the occasion. The need for efficient procedures for personalization is even clearer if we consider the e-commerce landscape more broadly: outside of giant digital retailers, the constraints of the problem are stricter, due to smaller user bases and the realization that most users are not frequently returning customers. We release a new session-based dataset including more than 30M fine-grained browsing events (product detail, add, purchase), enriched by linguistic behavior (queries made by shoppers, with items clicked and items not clicked after the query) and catalog meta-data (images, text, pricing information). On this dataset, we ask participants to showcase innovative solutions for two open problems: a recommendation task (where a model is shown some events at the start of a session, and it is asked to predict future product interactions); an intent prediction task, where a model is shown a session containing an add-to-cart event, and it is asked to predict whether the item will be bought before the end of the session.",2021,,,,"traints of the problem are stricter, due to smaller user bases and the realization that most users are not frequently returning customers. We release a new session-based dataset including more than 30M fine-grained browsing events (product detail, add, purchase), enriched by linguistic behavior (queries made by shoppers, with items clicked and items not clicked after the query) and catalog meta-data (images",
699,COVERAGE,Image Manipulation Localization,Image Manipulation Localization,"Image Manipulation Localization, Image Manipulation Detection",Image,,Computer Vision,"image-manipulation-detection-on-coverage, image-manipulation-localization-on-coverage",,https://github.com/wenbihan/coverage,https://paperswithcode.com/dataset/coverage,"COVERAGE contains copymove forged (CMFD) images and their originals with similar but genuine objects (SGOs). COVERAGE is designed to highlight and address tamper detection ambiguity of popular methods, caused by self-similarity within natural images. In COVERAGE, forged–original pairs are annotated with (i) the duplicated and forged region masks, and (ii) the tampering factor/similarity metric. For benchmarking, forgery quality is evaluated using (i) computer vision-based methods, and (ii) human detection performance.",,,,,,
700,COVID-19-CT-CXR,One-class classifier,One-class classifier,"One-class classifier, Computed Tomography (CT), COVID-19 Diagnosis",,,Methodology,,,https://github.com/ncbi-nlp/COVID-19-CT-CXR,https://paperswithcode.com/dataset/covid-19-ct-cxr,"A public database of COVID-19 CXR and CT images, which are automatically extracted from COVID-19-relevant articles from the PubMed Central Open Access (PMC-OA) Subset.",,,,,,
701,COVID-19_Fake_News_Dataset,Fake News Detection,Fake News Detection,Fake News Detection,Image,,Computer Vision,fake-news-detection-on-covid-19-fake-news,Custom,https://competitions.codalab.org/competitions/26655,https://paperswithcode.com/dataset/covid-19-fake-news-dataset,"Along with COVID-19 pandemic we are also fighting an `infodemic'. Fake news and rumors are rampant on social media. Believing in rumors can cause significant harm. This is further exacerbated at the time of a pandemic. To tackle this, we curate and release a manually annotated dataset of 10,700 social media posts and articles of real and fake news on COVID-19. We benchmark the annotated dataset with four machine learning baselines - Decision Tree, Logistic Regression , Gradient Boost , and Support Vector Machine (SVM). We obtain the best performance of 93.46\% F1-score with SVM.",,,,,,
702,COVID-19_Image_Data_Collection,Data Augmentation,Data Augmentation,"Data Augmentation, Classification, COVID-19 Diagnosis",Image,,Computer Vision,classification-on-covid-19-image-data,Various,https://github.com/ieee8023/covid-chestxray-dataset,https://paperswithcode.com/dataset/covid-19-image-data-collection,"Contains hundreds of frontal view X-rays and is the largest public resource for COVID-19 image and prognostic data, making it a necessary resource to develop and evaluate tools to aid in the treatment of COVID-19.",,,,,,
703,COVID-19_Twitter_Chatter_Dataset,Supervised Text Retrieval,Supervised Text Retrieval,"Supervised Text Retrieval, Misinformation, Out-of-Distribution Detection, Text Matching, Text Retrieval, Topic Models, Text Clustering, Text Classification","Image, Text",English,Computer Vision,"text-classification-on-20-newsgroups, text-retrieval-on-20-newsgroups, supervised-text-retrieval-on-20-newsgroups-1, out-of-distribution-detection-on-20, text-clustering-on-20-newsgroups, topic-models-on-20-newsgroups",Other (Public Domain),https://zenodo.org/record/3902855,https://paperswithcode.com/dataset/covid-19-twitter-chatter-dataset,"A large-scale curated dataset of over 152 million tweets, growing daily, related to COVID-19 chatter generated from January 1st to April 4th at the time of writing.",,,,,,
704,COVID-CT,COVID-19 Diagnosis,COVID-19 Diagnosis,"COVID-19 Diagnosis, Computed Tomography (CT), Self-Supervised Learning",,,Methodology,,Custom,https://github.com/UCSD-AI4H/COVID-CT,https://paperswithcode.com/dataset/covid-ct,Contains 349 COVID-19 CT images from 216 patients and 463 non-COVID-19 CTs. The utility of this dataset is confirmed by a senior radiologist who has been diagnosing and treating COVID-19 patients since the outbreak of this pandemic.,,,,,,
705,COVIDGR,Domain Adaptation,Domain Adaptation,"Domain Adaptation, COVID-19 Diagnosis, Medical Image Classification",Image,,Computer Vision,"covid-19-diagnosis-on-covidgr, medical-image-classification-on-covidgr",,https://github.com/ari-dasci/covidgr,https://paperswithcode.com/dataset/covidgr,"Under a close collaboration with an expert radiologist team of the Hospital Universitario San Cecilio, the COVIDGR-1.0 dataset of patients' anonymized X-ray images has been built. 852 images have been collected following a strict labeling protocol. They are categorized into 426 positive cases and 426 negative cases. Positive images correspond to patients who have been tested positive for COVID-19 using RT-PCR within a time span of at most 24h between the X-ray image and the test. Every image has been taken using the same type of equipment and with the same format: only the posterior-anterior view is considered.",,,,852 images,,
706,CovidQA,graph construction,graph construction,"graph construction, Question Answering, Information Retrieval","Graph, Text",English,Natural Language Processing,,,https://github.com/castorini/pygaggle/,https://paperswithcode.com/dataset/covidqa,"The beginnings of a question answering dataset specifically designed for COVID-19, built by hand from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge.",,,,,,
707,COVIDx,Anomaly Detection,Anomaly Detection,"Anomaly Detection, COVID-19 Diagnosis",Image,,Computer Vision,covid-19-diagnosis-on-covidx,,https://github.com/lindawangg/COVID-Net,https://paperswithcode.com/dataset/covidx,"An open access benchmark dataset comprising of 13,975 CXR images across 13,870 patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors' knowledge.",,,,,,
708,CoVoST,automatic-speech-translation,automatic-speech-translation,"automatic-speech-translation, Speech-to-Text Translation, Cross-Lingual Transfer, Speech Recognition","Audio, Image, Text",English,Speech,automatic-speech-translation-on-covost,CC0,https://github.com/facebookresearch/covost,https://paperswithcode.com/dataset/covost,CoVoST is a large-scale multilingual speech-to-text translation corpus. Its latest 2nd version covers translations from 21 languages into English and from English into 15 languages. It has total 2880 hours of speech and is diversified with 78K speakers and 66 accents.,,,,,,
709,CoWeSe,Behavioural cloning,Behavioural cloning,Behavioural cloning,,,Methodology,,,https://zenodo.org/record/4561971,https://paperswithcode.com/dataset/cowese,CoWeSe is a Spanish biomedical corpus consisting of 4.5GB (about 750M tokens) of clean plain text. CoWeSe is the result of a massive crawler on 3000 Spanish domains executed in 2020.,2020,,,,,
710,CP2A_dataset,Action Anticipation,Action Anticipation,Action Anticipation,Video,,Methodology,,,https://github.com/linaashaji/CP2A,https://paperswithcode.com/dataset/cp2a-dataset,"We present a new simulated dataset for pedestrian action anticipation collected using the CARLA simulator.
To generate this dataset, we place a camera sensor on the ego-vehicle in the Carla environment and set the parameters to those of the camera used to record the PIE dataset (i.e., 1920x1080, 110° FOV). Then, we compute bounding boxes for each pedestrian interacting with the ego vehicle as seen through the camera's field of view. We generated the data in two urban environments available in the CARLA simulator: Town02 and Town03.

The total number of simulated pedestrians is nearly 55k, equivalent to 14M bounding boxes samples. The critical point for each pedestrian is their first point of crossing the street (in case they will eventually cross) or the last bounding box coordinates of their path in the opposite case. The crossing behavior represents 25% of the total pedestrians. We balanced the training split of the dataset to obtain labeled sequences crossing/non-crossing in equal parts. We used sequence-flipping to augment the minority class (i.e., crossing behavior in our case) and then undersampled the rest of the dataset. The result is a total of nearly 50k pedestrian sequences.

Next, the pedestrian trajectory sequences were transformed into observation sequences of equal length (i.e., 0.5 seconds) with a 60% overlap for the training splits. The TTE length is between 30 and 60 frames. It resulted in a total of nearly 220k observation sequences.",,,,,valent to 14M bounding boxes samples,
711,CPED,Dialog Act Classification,Dialog Act Classification,"Dialog Act Classification, Emotional Dialogue Acts, Dialogue Generation, Personalized and Emotional Conversation, Conversational Response Generation, Personality Trait Recognition, Emotion Recognition, Multimodal Emotion Recognition, Emotion Recognition in Conversation, Personality Recognition in Conversation, Open-Domain Dialog, Dialogue Act Classification","Image, Text",English,Computer Vision,"emotion-recognition-in-conversation-on-cped, personalized-and-emotional-conversation-on, personality-recognition-in-conversation-on-1",Apache-2.0,https://github.com/scutcyr/CPED,https://paperswithcode.com/dataset/cped,"We construct a dataset named CPED from 40 Chinese TV shows. CPED consists of multisource knowledge related to empathy and personal characteristic. This knowledge covers 13 emotions, gender, Big Five personality traits, 19 dialogue acts and other knowledge. 


We build a multiturn Chinese Personalized and Emotional Dialogue dataset called CPED. To the best of our knowledge, CPED is the first Chinese personalized and emotional dialogue dataset. CPED contains 12K dialogues and 133K utterances with multi-modal context. Therefore, it can be used in both complicated dialogue understanding and human-like conversation generation.
CPED has been annotated with 3 character attributes (name, gender age), Big Five personality traits, 2 types of dynamic emotional information (sentiment and emotion) and DAs. The personality traits and emotions can be used as prior external knowledge for open-domain conversation generation, making the conversation system have a good command of personification capabilities.
We propose three tasks for CPED: personality recognition in conversations (PRC), emotion recognition in conversations (ERC), and personalized and emotional conversation (PEC). A set of experiments verify the importance of using personalities and emotions as prior external knowledge for conversation generation.",,,,,,
712,CPLFW,Lightweight Face Recognition,Lightweight Face Recognition,"Lightweight Face Recognition, Synthetic Face Recognition, Face Verification, Face Recognition",Image,,Computer Vision,"face-recognition-on-cplfw, lightweight-face-recognition-on-cplfw, synthetic-face-recognition-on-cplfw, face-verification-on-cplfw",,http://whdeng.cn/CPLFW/index.html,https://paperswithcode.com/dataset/cplfw,"A renovation of Labeled Faces in the Wild (LFW), the de facto standard testbed for unconstraint face verification. 

There are three motivations behind the construction of CPLFW benchmark as follows:

1.Establishing a relatively more difficult database to evaluate the performance of real world face verification so the effectiveness of several face verification methods can be fully justified.

2.Continuing the intensive research on LFW with more realistic consideration on pose intra-class variation and fostering the research on cross-pose face verification in unconstrained situation. The challenge of CPLFW emphasizes pose difference to further enlarge intra-class variance. Also, negative pairs are deliberately selected to avoid different gender or race. CPLFW considers both the large intra-class variance and the tiny inter-class variance simultaneously.

3.Maintaining the data size, the face verification protocol which provides a 'same/different' benchmark and the same identities in LFW, so one can easily apply CPLFW to evaluate the performance of face verification.",,,,,,
713,CPP,Polyphone disambiguation,Polyphone disambiguation,Polyphone disambiguation,,,Methodology,polyphone-disambiguation-on-cpp,,https://github.com/kakaobrain/g2pM,https://paperswithcode.com/dataset/cpp,"A benchmark dataset that consists of 99,000+ sentences for Chinese polyphone disambiguation.",,,,,,
714,CPP_simulated_evaluation,Multi-Agent Path Finding,Multi-Agent Path Finding,Multi-Agent Path Finding,,,Methodology,,,https://github.com/savvas-ap/cpp-simulated-evaluations,https://paperswithcode.com/dataset/cpp-simulated-evaluation,"In this repository you can find all the elaborate results that were used for the simulated evaluation of an innovative, optimized for real-life use, STC-based, multi-robot Coverage Path Planning (mCPP) algorithm. For this evaluation were introduced in ""Apostolidis, S. D., Kapoutsis, P. C., Kapoutsis, A. C., & Kosmatopoulos, E. B. (2022). Cooperative multi-UAV coverage mission planning platform for remote sensing applications. Autonomous Robots, 1-28."" 20 ROIs, of different shapes and areas, that may include obstacles inside them. These ROIs along with some benchmark results can be found here: https://github.com/savvas-ap/cpp-simulated-evaluations",2022,,,,,
715,CPSC2021,Atrial Fibrillation Detection,Atrial Fibrillation Detection,Atrial Fibrillation Detection,Image,,Computer Vision,,Creative Commons Attribution 4.0 International,http://2021.icbeb.org/CPSC2021,https://paperswithcode.com/dataset/cpsc2021,"Introduction
The 4th China Physiological Signal Challenge 2021 (CPSC 2021) aims to encourage the development of algorithms for searching the paroxysmal atrial fibrillation (PAF) events from dynamic ECG recordings.

ECG signal provides an important role in non-invasively monitoring and clinical diagnosis for cardiovascular disease (CVD). AF is the most frequent arrhythmia, but PAF often remains unrecognized[1, 2]. Early screening and early detection of paroxysmal AF are particularly important. It is of great value for AF surgery options, drug intervention, and the diagnosis and treatment of various clinical complications [3].

Although accurate detection of paroxysmal AF is very important, there is currently no algorithm that can efficiently measure the onsets and offsets of AF events in dynamic or wearable ECGs [4]. Previous AF detection algorithms usually focus on the classification of AF rhythm, such as entropy feature-based [5, 6] or machine learning-based methods [7, 8], without the location of onsets and offsets of AF events. Thus, the clinical significance for the personalized treatment and management of AF patients is limited. In clinical applications, other abnormal rhythms can significantly influence the accurate identification of AF rhythm. In this year’s challenge, we focus on the detection of paroxysmal AF events from dynamic ECGs. A new dynamic ECG database containing episodes with totally or partly AF rhythm, or non-AF rhythm was constructed, to encourage the development of the more efficient and robust algorithms for paroxysmal AF event detection.

Challenge Data
Data are recorded from 12-lead Holter or 3-lead wearable ECG monitoring devices. Challenge data provides variable-length ECG records fragments extracted from lead I and lead II of the long-term dynamic ECGs, each sampled at 200 Hz. In order to avoid ambiguity in annotation, an AF event is limited to contain no less than 5 heart beats.
The training set in the 1st stage consists of 730 records, extracted from the Holter records from 10 AF patients (5 PAF patients) and 39 non-AF patients (usually including other abnormal and normal rhythms).
The training set in the 2nd stage consists of 706 records from 37 AF patients (18 PAF patients) and 14 non-AF patients.
The test set comprises data from the same source as the training set as well as different data source. We ensure that at least one test subset was collected by a different ECG monitoring system compared with the training set. Same as in previous years, we are not planning to release the test set at any point.
All data is provided in WFDB format and the annotations are standardized according to PhysioBank Annotations (link: https://archive.physionet.org/physiobank/annotations.shtml). The annotation includes the beat annotations (R peak location and beat type), the rhythm annotations (rhythm change flag and rhythm type) and the diagnosis of the global rhythm. Please refer to the example code entry (link: https://github.com/CPSC-Committee/cpsc2021-python-entry ) of the challenge for specific data and label load functions. Note that the flag of atrial fibrillation and atrial flutter (‘AFIB’ and ‘AFL’) in annotated information are seemed as the same type when scoring the method.
Please download the training data from here ( Training Set I and Training Set II).",2021,,,730 records,,
716,CPsyCounD,Open-Domain Dialog,Open-Domain Dialog,"Open-Domain Dialog, Professional Psychology, Dialogue Generation",Text,English,Natural Language Processing,,CC BY 4.0 DEED,https://github.com/CAS-SIAT-XinHai/CPsyCoun/tree/main/CPsyCounD,https://paperswithcode.com/dataset/cpsycound,"The high-quality multi-turn dialogue dataset, which has a total of 3,134 multi-turn consultation dialogues. CPsyCounD covers nine representative topics and seven classic schools of psychological counseling.

Topic types

Self-growth
Emotion&Stress
Education
Love&Marriage
Family Relationship
Social Relationship
Sex
Career
Mental Disease

Consulting schools

Psychoanalytic Therapy
Cognitive Behavioral Therapy
Humanistic Therapy
Family Therapy
Postmodern Therapy
Integrative Therapy
Other Therapies (Mindfulness/Morita therapy...)",,,,,,
717,CPsyCounE,Open-Domain Dialog,Open-Domain Dialog,"Open-Domain Dialog, Dialogue Evaluation, Professional Psychology, Dialogue Generation",Text,English,Natural Language Processing,,CC BY 4.0 DEED,https://github.com/CAS-SIAT-XinHai/CPsyCoun/tree/main/CPsyCounE,https://paperswithcode.com/dataset/cpsycoune,"The general multi-turn dialogue evaluation dataset with nine topics. Each topic has five representative cases,  resulting in a comprehensive evaluation dataset of 45 cases.

Topic types

Self-growth
Emotion&Stress
Education
Love&Marriage
Family Relationship
Social Relationship
Sex
Career
Mental Disease",,,,,,
718,CQASUMM,Community Question Answering,Community Question Answering,"Community Question Answering, Question Answering, Document Summarization",Text,English,Natural Language Processing,,,https://bitbucket.org/tanya14109/cqasumm/src/master/,https://paperswithcode.com/dataset/cqasumm,"CQASUMM is a dataset for CQA (Community Question Answering) summarization, constructed from the 4.4 million Yahoo! Answers L6 dataset. The dataset contains ~300k annotated samples.",,,,,,
719,CQR,Spoken Language Understanding,Spoken Language Understanding,"Spoken Language Understanding, Multi-Task Learning, Spoken Dialogue Systems",Text,English,Natural Language Processing,,,https://github.com/alexa/alexa-dataset-contextual-query-rewrite,https://paperswithcode.com/dataset/cqr,CQR is an extension to the Stanford Dialogue Corpus. It contains crowd-sourced rewrites to facilitate research in dialogue state tracking using natural language as the interface.,,,,,,
720,CREAK,Fact Verification,Fact Verification,"Fact Verification, Common Sense Reasoning",,,Methodology,,CC BY- SA 4.0,https://www.cs.utexas.edu/~yasumasa/creak/,https://paperswithcode.com/dataset/creak,"A testbed for commonsense reasoning about entity knowledge, bridging fact-checking about entities with commonsense inferences.",,https://arxiv.org/pdf/2109.01653v1.pdf,https://arxiv.org/pdf/2109.01653v1.pdf,,,
721,Creative_Flow__Dataset,Optical Flow Estimation,Optical Flow Estimation,"Optical Flow Estimation, Synthetic Data Generation","Text, Video",English,Natural Language Processing,,,http://www.cs.toronto.edu/creativeflow/,https://paperswithcode.com/dataset/creative-flow-dataset,"Includes 3000 animated sequences rendered using styles randomly selected from 40 textured line styles and 38 shading styles, spanning the range between flat cartoon fill and wildly sketchy shading. The dataset includes 124K+ train set frames and 10K test set frames rendered at 1500x1500 resolution, far surpassing the largest available optical flow datasets in size.",,,,,,
722,Creative_Visual_Storytelling_Anthology,Visual Storytelling,Visual Storytelling,Visual Storytelling,Image,,Computer Vision,,CC0 1.0 Universal,https://github.com/USArmyResearchLab/ARL-Creative-Visual-Storytelling,https://paperswithcode.com/dataset/creative-visual-storytelling-anthology,"The Creative Visual Storytelling Anthology is a collection of 100 author responses to an improved creative visual storytelling exercise over a sequence of three images. Each item contains four facet entries, corresponding to Entity, Scene, Narrative, and Title. 

The Creative Visual Storytelling Anthology was collected on Amazon Mechanical Turk. Five different authors performed the task for 20 different Flickr and Search-and-Rescue image-sets ( a sequence of 3 images) for a total of 100 items in the anthology. There are 300 unique Entity and Scene entries (single-image facets completed for each image), 200 unique Narrative entries (multi-image facets performed twice with two and then three images), and 100 unique Title entries (multi-image facets completed for three images). Thus, with each one assigned a title, there are 100 unique stories in the anthology all together.

One set of images used in collecting the anthology originated from Flickr, under Creative Commons Licenses. We chose a subset of Huang et al’s VIST dataset and downselected their image sequences from five to three images to scaffold the Aristotelian dramatic structure. We do not release the Flickr images in order to track the providence of the images. The Flickr images' authors and copyright information and usage are documented in the Flickr imageset license spreadsheet.

The second source of images came from a Search and Rescue (SAR) scenario. We selected three images in-order from  experimental runs from a human-robot collaboration task, and similar sequential images were excluded for sake of diversity. The SAR images can be obtained through the SCOUT (The Situated Corpus of Understanding Transactions) dataset.",,,,3 images,,
723,Creative_Writing,Story Generation,Story Generation,Story Generation,Text,English,Natural Language Processing,,,,https://paperswithcode.com/dataset/creative-writing,"A creative writing task where the input is 4 random sentences and the output should be a coherent passage with 4 paragraphs that end in the 4 input sentences respectively. Such a task is open-ended and exploratory, and challenges creative thinking as well as high-level planning.",,,,4 paragraphs,,
724,CREMA-D,Audio Classification,Audio Classification,"Audio Classification, Self-Supervised Learning, Talking Face Generation, Speech Emotion Recognition, Video Emotion Recognition, Few-Shot Audio Classification, Facial Expression Recognition (FER)","Audio, Image, Text, Video",English,Audio,"self-supervised-learning-on-crema-d, video-emotion-recognition-on-crema-d, facial-expression-recognition-on-crema-d, speech-emotion-recognition-on-crema-d, audio-classification-on-crema-d, talking-face-generation-on-crema-d, few-shot-audio-classification-on-crema-d",,https://github.com/CheyneyComputerScience/CREMA-D,https://paperswithcode.com/dataset/crema-d,"CREMA-D is an emotional multimodal actor data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified).

Actors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified).

Participants rated the emotion and emotion levels based on the combined audiovisual presentation, the video alone, and the audio alone. Due to the large number of ratings needed, this effort was crowd-sourced and a total of 2443 participants each rated 90 unique clips, 30 audio, 30 visual, and 30 audio-visual. 95% of the clips have more than 7 ratings.",,,,12 sentences,,
725,CREMI,3D Instance Segmentation,3D Instance Segmentation,"3D Instance Segmentation, Brain Image Segmentation","3D, Image",,Computer Vision,brain-image-segmentation-on-cremi,,https://cremi.org/,https://paperswithcode.com/dataset/cremi,"MICCAI Challenge on Circuit Reconstruction from Electron Microscopy Images.

About
The goal of this challenge is to evaluate algorithms for automatic reconstruction of neurons and neuronal connectivity from serial section electron microscopy data. The comparison is performed not only by evaluating the quality of neuron segmentations, but also by assessing the accuracy of detecting synapses and identifying synaptic partners. The challenge is carried out on three large and diverse datasets from adult Drosophila melanogaster brain tissue, comprising neuron segmentation ground truth and annotations for synaptic connections. A successful solution would demonstrate its efficiency and generalizability, and carry great potential to reduce the time spent on manual reconstruction of neural circuits in electron microscopy volumes.

Description
We provide three datasets, each consisting of two (5 μm)3 volumes (training and testing, each 1250 px × 1250 px × 125 px) of serial section EM of the adult fly brain. Each volume has neuron and synapse labelings and annotations for pre- and post-synaptic partners.",,,,,,
726,Criteo,Click-Through Rate Prediction,Click-Through Rate Prediction,Click-Through Rate Prediction,Time Series,,Methodology,click-through-rate-prediction-on-criteo,,https://labs.criteo.com/2013/12/download-terabyte-click-logs/,https://paperswithcode.com/dataset/criteo,"Criteo contains 7 days of click-through data, which is widely used for CTR prediction benchmarking. There are 26 anonymous categorical fields and 13 continuous fields in Criteo dataset.",,AMER: Automatic Behavior Modeling and Interaction Exploration in Recommender System,https://arxiv.org/abs/2006.05933,,,
727,CriticBench,Code Repair,Code Repair,"Code Repair, Code Generation, Common Sense Reasoning, Mathematical Reasoning",Text,English,Natural Language Processing,,MIT,https://criticbench.github.io/,https://paperswithcode.com/dataset/criticbench,"CriticBench is a comprehensive benchmark designed to assess the abilities of Large Language Models (LLMs) to critique and rectify their reasoning across various tasks. It encompasses five reasoning domains:


Mathematical
Commonsense
Symbolic
Coding
Algorithmic

CriticBench compiles 15 datasets and incorporates responses from three LLM families. By utilizing CriticBench, researchers evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning (referred to as GQC reasoning). Notable findings include:


A linear relationship in GQC capabilities, with critique-focused training significantly enhancing performance.
Task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction.
GQC knowledge inconsistencies that decrease as model size increases.
An intriguing inter-model critiquing dynamic, where stronger models excel at critiquing weaker ones, while weaker models surprisingly surpass stronger ones in self-critique.

(1) CriticBench: Benchmarking LLMs for Critique-Correct Reasoning. https://arxiv.org/abs/2402.14809.
(2) CriticBench: Benchmarking LLMs for Critique-Correct Reasoning. http://export.arxiv.org/abs/2402.14809.
(3) CriticBench: Benchmarking LLMs for Critique-Correct Reasoning. https://openreview.net/forum?id=sc5i7q6DQO.
(4) CriticBench: Benchmarking LLMs for Critique-Correct Reasoning - arXiv.org. https://arxiv.org/html/2402.14809v2.
(5) undefined. https://doi.org/10.48550/arXiv.2402.14809.",,,,,,
728,CropAndWeed,Instance Segmentation,Instance Segmentation,"Instance Segmentation, Plant Phenotyping, Semantic Segmentation, Panoptic Segmentation, Crop Yield Prediction, Fine-Grained Image Classification, Crop Classification, Object Detection, Multi-Task Learning, Domain Adaptation, Benchmarking","Image, Time Series",,Computer Vision,,Custom (non-commercial),https://github.com/cropandweed/cropandweed-dataset,https://paperswithcode.com/dataset/cropandweed-dataset,"The CropAndWeed dataset is focused on the fine-grained identification of 74 relevant crop and weed species with a strong emphasis on data variability. Annotations of labeled bounding boxes, semantic masks and stem positions are provided for about 112k instances in more than 8k high-resolution images of both real-world agricultural sites and specifically cultivated outdoor plots of rare weed types. Additionally, each sample is enriched with meta-annotations regarding environmental conditions.",,,,112k instances,,
729,CrossWOZ,Natural Language Understanding,Natural Language Understanding,"Natural Language Understanding, Task-Oriented Dialogue Systems, Dialogue State Tracking","Image, Text, Video",English,Computer Vision,,,https://github.com/thu-coai/CrossWOZ,https://paperswithcode.com/dataset/crosswoz,"CrossWOZ is the first large-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains rich annotation of dialogue states and dialogue acts at both user and system sides.",,,,,,
730,CrowdSpeech,Crowdsourced Text Aggregation,Crowdsourced Text Aggregation,"Crowdsourced Text Aggregation, Speech Recognition","Audio, Image, Text",English,Computer Vision,"crowdsourced-text-aggregation-on-crowdspeech-1, crowdsourced-text-aggregation-on-crowdspeech",Attribution 4.0 International,https://github.com/pilot7747/VoxDIY,https://paperswithcode.com/dataset/crowdspeech,"CrowdSpeech is a publicly available large-scale dataset of crowdsourced audio transcriptions. It contains annotations for more than 20 hours of English speech from more than 1,000 crowd workers.",,,,,,
731,CrowS-Pairs,Stereotypical Bias Analysis,Stereotypical Bias Analysis,Stereotypical Bias Analysis,,,Methodology,stereotypical-bias-analysis-on-crows-pairs,,https://github.com/nyu-mll/crows-pairs,https://paperswithcode.com/dataset/crows-pairs,"CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups.",,,,1508 examples,,
732,CRSB,Retrieval,Retrieval,"Retrieval, Question Answering, RAG",Text,English,Natural Language Processing,,,https://huggingface.co/datasets/heydariAI/CRSB,https://paperswithcode.com/dataset/crsb,The Official dataset proposed int the paper Context Awareness Gate For Retrieval Augmented Generation,,,,,,
733,CRUW,Radar Object Detection,Radar Object Detection,Radar Object Detection,Image,,Computer Vision,,Custom (non-commercial),https://www.cruwdataset.org/,https://paperswithcode.com/dataset/cruw,"CRUW is a dataset for the radar object detection (ROD) task, which aims to classify and localize the objects in 3D purely from radar's radio frequency (RF) images. The CRUW dataset has a systematic annotation and evaluation system, which involves camera RGB images and radar RF images, collected in various driving scenarios.",,,,,"valuation system, which involves camera RGB images",
734,CRVD,Image Denoising,Image Denoising,"Image Denoising, Video Denoising, Denoising","Image, Video",,Computer Vision,video-denoising-on-crvd-1,,https://github.com/cao-cong/RViDeNet,https://paperswithcode.com/dataset/crvd,The CRVD dataset consists of 55 groups of noisy-clean videos with ISO values ranging from 1600 to 25600.,,,,,,
735,Cryptics,Systematic Generalization,Systematic Generalization,"Systematic Generalization, Semantic Composition",,,Methodology,,,https://doi.org/10.5061/dryad.n02v6wwzp,https://paperswithcode.com/dataset/cryptics,"Official dataset of Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP.

See github.com/jsrozner/decrypt and https://doi.org/10.5061/dryad.n02v6wwzp",,,,,,
736,Crypto_related_tweets_from_10.10.2020_to_3.3.2021,Twitter Bot Detection,Twitter Bot Detection,"Twitter Bot Detection, Twitter Sentiment Analysis, Twitter Event Detection","Image, Text",English,Computer Vision,,CC BY 4.0,https://sobigdata.d4science.org/catalogue-sobigdata?path=/dataset/crypto_related_tweets_from_10_10_2020_to_3_3_2021,https://paperswithcode.com/dataset/crypto-related-tweets-from-10-10-2020-to-3-3,The dataset contains 30 million cryptocurrency-related tweets from 10.10.2020 to 3.3.2021. See https://github.com/meakbiyik/ask-who-not-what for more details.,2020,,,,,
737,CSAW-S,Tumor Segmentation,Tumor Segmentation,Tumor Segmentation,Image,,Computer Vision,,,https://github.com/ChrisMats/CSAW-S,https://paperswithcode.com/dataset/csaw-s,CSAW-S is a dataset of mammography images which includes expert annotations of tumors and non-expert annotations of breast anatomy and artifacts in the image.,,CSAW-S,https://arxiv.org/pdf/2008.00807.pdf,,,
738,CSI,Speaker Identification,Speaker Identification,Speaker Identification,Audio,,Audio,,,https://github.com/maelfabien/Graph2Speak,https://paperswithcode.com/dataset/csi,CSI is a criminal conversational dataset for speaker identification built from the CSI television show. The authors collected transcripts of 39 episodes and video/audio of 4 episodes. Each episode involves on average more than 30 speakers. Utterances last on average 3 to 4 seconds. There are around 45 to 50 distinct scenes/conversations per episode.,,,,,,
739,CSI_300_Pair_Trading,PAIR TRADING,PAIR TRADING,"PAIR TRADING, Stock Trend Prediction",Time Series,,Methodology,pair-trading-on-csi-300-pair-trading,,https://github.com/chancefocus/trials,https://paperswithcode.com/dataset/csi-300-pair-trading,"A daily emerging stock market dataset (Chinese CSI 300 dataset) including 300 stocks and 5,088 time steps from the CSMAR database. We construct our stock dataset using a pool of stocks from the CSI 300 index for the last 21 years, from 01/02/2000 to 12/31/2020. Instead of all stocks in the market, we select the stocks that used to belong to the major market index CSI 300, and filter out stocks that have missing price data over the period.

For each trading day, we use the fundamental price features as the features of stocks, including open price, close price, and volume. Additionally, we normalize price features such as open price and close price with logarithm.

The dataset randomly splits stocks into five non-overlapping sub-datasets. For each subset, the first 90% of trading days are used as train data, the following 5% as validation data, and the rest 5% as test data.",2000,,,,,
740,CSL__Chinese_Scientific_Literature_,Keyword Extraction,Keyword Extraction,"Keyword Extraction, Text Summarization, Text Classification","Image, Text",English,Computer Vision,,Non-Commercial,https://github.com/ydli-ai/CSL,https://paperswithcode.com/dataset/csl-2022,"We present CSL, a large-scale Chinese Scientific Literature dataset, 
which contains the titles, abstracts, keywords and academic fields of 396,209 papers. 
To our knowledge, CSL is the first scientific document dataset in Chinese.

Paper | Code and data

Dataset
We obtain the paper's meta-information from the 
National Engineering Research Center for Science and Technology Resources Sharing Service (NSTR) dated from 2010 to 2020.
Then, we filter data by the Catalogue of Chinese Core Journals.
According to the Catalogue and collected data, we divide academic fields into 13 first-level categories (e.g., Engineering, Science) and 67 second-level disciplines (e.g., Mechanics, Mathematics).
In total, we collect 396,209 instances for the CSL dataset, represented as tuples <T, A, K, c, d>, where T is the title, A is the abstract, K is a list of keywords, c is the category label and d is the discipline label.
The paper distribution over categories and the examples of disciplines are shown in below:

|  Category       |          #d | len(T) | len(A) | num(K) | #Samples | Discipline Examples                                   |
|-----------------|-------------:|-------:|-------:|-------:|----------:|---------------------------------------|
|  Engineering    |           27 |   19.1 |  210.9 |    4.4 |   177,600 |  Mechanics,Architecture,Electrical Science   |
|  Science        |            9 |   20.7 |  254.4 |    4.3 |    35,766 |  Mathematics,Physics,Astronomy,Geography    |
|  Agriculture    |            7 |   17.1 |  177.1 |    7.1 |    39,560 |  Crop Science,Horticulture,Forestry          |
|  Medicine       |            5 |   20.7 |  269.5 |    4.7 |    36,783 |  Clinical Medicine,Dental Medicine,Pharmacy  |
|  Management     |            4 |   18.7 |  157.7 |    6.2 |    23,630 |  Business Management,Public Administration    |
|  Jurisprudence  |            4 |   18.9 |  174.4 |    6.1 |    21,554 |  Legal Science,Political Science,Sociology   |
|  Pedagogy       |            3 |   17.7 |  179.4 |    4.3 |    16,720 |  Pedagogy,Psychology,Physical Education      |
|  Economics      |            2 |   19.5 |  177.2 |    4.5 |    11,558 |  Theoretical Economics,Applied Economics      |
|  Literature     |            2 |   18.8 |  158.2 |    8.3 |    10,501 |  Chinese Literature,Journalism                |
|  Art            |            1 |   17.8 |  170.8 |    5.4 |     5,201 |  Art                                           |
|  History        |            1 |   17.6 |  181.0 |    6.0 |     6,270 |  History                                       |
|  Strategics     |            1 |   17.5 |  169.3 |    4.0 |     3,555 |  Military Science                              |
|  Philosophy     |            1 |   18.0 |  176.5 |    8.0 |     7,511 |  Philosophy                                    |
|  All            |           67 |        |        |        |   396,209 |

Evaluation Tasks
We build a benchmark to facilitate the development of Chinese scientific literature NLP.
It contains diverse tasks, ranging from classification to text generation, representing many practical scenarios.
We randomly select 100k samples and split the datasets into the training set, validation set and test set according to the ratio, 0.8 : 0.1 : 0.1.
This split is shared across different tasks, which allows multitask training and evaluation.
Datasets are presented in text2text format.

1.Text Summarization (Title Prediction)
Predict the paper title from the abstract.

Data examples:
{ 
  ""prompt"": ""to title"",
  ""text_a"": ""多个相邻场景同时进行干涉参数外定标的过程称为联合定标,联合定标能够 \
            保证相邻场景的高程衔接性,能够实现无控制点场景的干涉定标.该文提出了 \
            一种适用于机载InSAR系统的联合定标算法..."",
  ""text_b"": ""基于加权最优化模型的机载InSAR联合定标算法""
}

2.Keyword Generation
Predict a list of keywords from a given paper title and abstract.

Data examples:
{ 
  ""prompt"": ""to keywords"",
  ""text_a"": ""通过对72个圆心角为120°的双跨偏心支承弯箱梁桥模型的计算分析,以梁 \
            格系法为基础编制的3D-BSA软件系统为结构计算工具,用统计分析的方法建 \
            立双跨偏心支承弯箱梁桥结构反应在使用极限状态及承载能力极限状态下与 \
            桥梁跨长... 偏心支承对120°圆心角双跨弯箱梁桥的影响"",
  ""text_b"": ""曲线桥_箱形梁_偏心支承_设计_经验公式""
}

3.Category Classification
Predict the category with the paper title (13 classes).

Data examples:
{ 
  ""prompt"": ""to category"",
  ""text_a"": ""基于模糊C均值聚类的流动单元划分方法——以克拉玛依油田五3中区克下组为例"",
  ""text_b"": ""工学""
},
{ 
  ""prompt"": ""to category"",
  ""text_a"": ""正畸牵引联合牙槽外科矫治上颌尖牙埋伏阻生的临床观察"",
  ""text_b"": ""医学""
}

4.Discipline Classification
Predict the discipline with the paper abstract (67 classes).

Data examples:
{ 
  ""prompt"": ""to discipline"",
  ""text_a"": ""某铁矿选矿厂所产铁精矿含硫超过0.3%,而现场为了今后发展的需要,要 \
             求将含硫量降到0.1%以下.为此,针对该铁精矿中硫化物主要以磁黄铁矿 \
             形式存在、硫化物多与铁矿物连生且氧化程度较高的特点..."",
  ""text_b"": ""矿业工程""
},
{ 
  ""prompt"": ""to discipline"",
  ""text_a"": ""为了校正广角镜头的桶形畸变,提出一种新的桶形畸变数字校正方法.它 \
             使用点阵样板校正的方法,根据畸变图和理想图中圆点的位置关系,得出 \
             畸变图像素在X轴和Y轴方向上的偏移量曲面..."",
  ""text_b"": ""计算机科学与技术""
}",2010,Paper,https://arxiv.org/abs/2209.05034,209 instances,,13
741,CSPRD,Passage Retrieval,Passage Retrieval,Passage Retrieval,,,Methodology,,,,https://paperswithcode.com/dataset/csprd,"The Chinese Stock Policy Retrieval Dataset (CSPRD) contains a Chinese policy corpus of 10,002 articles and 709 prospectus examples from 545 companies listed on China’s Science and Technology Innovation Board (STAR Market). CSPRD is bilingual in Chinese and English (Translated by ChatGPT) and is annotated by experienced experts from Shanghai Stock Exchange.",,,,,"val Dataset (CSPRD) contains a Chinese policy corpus of 10,002 articles and 709 prospectus examples",
742,CSPubSum,Sentence Classification,Sentence Classification,Sentence Classification,Image,,Computer Vision,,,https://github.com/EdCo95/scientific-paper-summarisation,https://paperswithcode.com/dataset/cspubsum,"CSPubSum is a dataset for summarisation of computer science publications, created by exploiting a large resource of author provided summaries and show straightforward ways of extending it further.",,,,,,
743,CSS10,AutoML,AutoML,"AutoML, Speech Synthesis",Audio,,Audio,,,https://github.com/Kyubyong/css10,https://paperswithcode.com/dataset/css10,A collection of single speaker speech datasets for ten languages. It is composed of short audio clips from LibriVox audiobooks and their aligned texts.,,,,,,
744,CUB-200-2011,Dataset Distillation - 1IPC,Dataset Distillation - 1IPC,"Dataset Distillation - 1IPC, Weakly-Supervised Object Localization, Few-Shot Class-Incremental Learning, Image Clustering, Fine-Grained Image Classification, Multimodal Deep Learning, Multimodal Text and Image Classification, Image Retrieval, Generalized Few-Shot Learning, Small Data Image Classification, Graph Matching, Unsupervised Keypoint Estimation, Fine-Grained Image Recognition, Transductive Zero-Shot Classification, Image Attribution, Image Classification, Metric Learning, Error Understanding, Generalized Zero-Shot Learning, Interpretable Machine Learning, Multi-Modal Document Classification, Few-Shot Image Classification, Long-tail learning with class descriptors, Semantic correspondence, Image Generation, Single-View 3D Reconstruction, Zero-Shot Learning, Document Text Classification, Fine-Grained Visual Recognition, Point-interactive Image Colorization, Bird Species Classification With Audio-Visual Data, Few-Shot Learning, Cross-Domain Few-Shot, Text-to-Image Generation, Concept-based Classification","3D, Audio, Graph, Image, Text",English,Computer Vision,"concept-based-classification-on-cub-200-2011, fine-grained-image-recognition-on-cub-birds, few-shot-image-classification-on-cub-200-5, metric-learning-on-cub-200-2011, fine-grained-image-classification-on-cub-200, few-shot-class-incremental-learning-on-cub, image-classification-on-imbalanced-cub-200, small-data-on-cub-200-2011-30-samples-per-1, fine-grained-image-classification-on, interpretable-machine-learning-on-cub-200, text-to-image-generation-on-cub, point-interactive-image-colorization-on-cub, unsupervised-keypoint-estimation-on-cub, semantic-correspondence-on-cub-200-2011, multimodal-deep-learning-on-cub-200-2011, multimodal-text-and-image-classification-on, zero-shot-learning-on-cub-200-2011, document-text-classification-on-cub-200-2011, image-generation-on-cub-128-x-128, long-tail-learning-with-class-descriptors-on, few-shot-image-classification-on-cub-200-50, zero-shot-learning-on-cub-200-0-shot-learning-1, fine-grained-image-recognition-on-cub-200, few-shot-image-classification-on-cub-200-2011-2, single-view-3d-reconstruction-on-cub-200-2011, few-shot-image-classification-on-cub-200-2011, weakly-supervised-object-localization-on-cub, graph-matching-on-cub, image-classification-on-cub-200-2011-3, multi-modal-document-classification-on-cub, image-clustering-on-cub-200-2011, generalized-few-shot-learning-on-cub, weakly-supervised-object-localization-on-cub-2, fine-grained-image-classification-on-cub-200-1, metric-learning-on-cub-200-2011-4, image-clustering-on-cub-birds, image-attribution-on-cub-200-2011-1, generalized-zero-shot-learning-on-cub-200, image-retrieval-on-cub-200-2011, small-data-on-cub-200-2011-5-samples-per-1, cross-domain-few-shot-on-cub, image-classification-on-cub, fine-grained-visual-recognition-on-cub-200-1, weakly-supervised-object-localization-on-cub-1, transductive-zero-shot-classification-on-cub, few-shot-image-classification-on-cub-200-5-1, few-shot-image-classification-on-cub-200-2011-1, dataset-distillation-1ipc-on-cub-200-2011, few-shot-image-classification-on-cub-200-0, error-understanding-on-cub-200-2011-1",,https://www.vision.caltech.edu/datasets/cub_200_2011/,https://paperswithcode.com/dataset/cub-200-2011,"The Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset is the most widely-used dataset for fine-grained visual categorization task. It contains 11,788 images of 200 subcategories belonging to birds, 5,994 for training and 5,794 for testing. Each image has detailed annotations: 1 subcategory label, 15 part locations, 312 binary attributes and 1 bounding box. The textual information comes from Reed et al.. They expand the CUB-200-2011 dataset by collecting fine-grained natural language descriptions. Ten single-sentence descriptions are collected for each image. The natural language descriptions are collected through the Amazon Mechanical Turk (AMT) platform, and are required at least 10 words, without any information of subcategories and actions.",2011,Fine-grained Visual-textual Representation Learning,https://arxiv.org/abs/1709.00340,788 images,,
745,Cube__,3D Object Recognition,3D Object Recognition,"3D Object Recognition, Color Constancy","3D, Image",,Computer Vision,3d-object-recognition-on-cube-engraving,,https://github.com/Visillect/CubePlusPlus,https://paperswithcode.com/dataset/cube,"Cube++ is a novel dataset for the color constancy problem that continues on the Cube+ dataset. It includes 4890 images of different scenes under various conditions. For calculating the ground truth illumination, a calibration object with known surface colors was placed in every scene.",,,,4890 images,,
746,CubiCasa5K,Imitation Learning,Imitation Learning,Imitation Learning,,,Methodology,,,https://github.com/CubiCasa/CubiCasa5k,https://paperswithcode.com/dataset/cubicasa5k,CubiCasa5K is a large-scale floorplan image dataset containing 5000 samples annotated into over 80 floorplan object categories. The dataset annotations are performed in a dense and versatile manner by using polygons for separating the different objects.,,,,5000 samples,,
747,CUGE,Part-Of-Speech Tagging,Part-Of-Speech Tagging,"Part-Of-Speech Tagging, Chinese Word Segmentation","Audio, Image",,Speech,,,http://cuge.baai.ac.cn/,https://paperswithcode.com/dataset/cuge,"CUGE is a Chinese Language Understanding and Generation Evaluation benchmark with the following features: (1) Hierarchical benchmark framework, where datasets are principally selected and organized with a language capability-task-dataset hierarchy. (2) Multi-level scoring strategy, where different levels of model performance are provided based on the hierarchical framework.

CUGE covers 7 important language capabilities, 17 mainstream NLP tasks and 19 representative datasets. It includes tasks like: word segmentation, part of speech tagging, reading comprehension and document retrieval.",,,,,,
748,CUHK-PEDES,Text based Person Retrieval,Text based Person Retrieval,"Text based Person Retrieval, Pedestrian Image Caption, Cross-Modal Retrieval, Text-based Person Retrieval with Noisy Correspondence","Image, Text",English,Computer Vision,"nlp-based-person-retrival-on-cuhk-pedes, cross-modal-retrieval-on-cuhk-pedes, pedestrian-image-caption-on-cuhk-pedes, text-based-person-retrieval-with-noisy",,https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/CUHK-PEDES-prepare,https://paperswithcode.com/dataset/cuhk-pedes,"The CUHK-PEDES dataset is a caption-annotated pedestrian dataset. It contains 40,206 images over 13,003 persons. Images are collected from five existing person re-identification datasets, CUHK03, Market-1501, SSM, VIPER, and CUHK01 while each image is annotated with 2 text descriptions by crowd-sourcing workers. Sentences incorporate rich details about person appearances, actions, poses.",,MGD-GAN: Text-to-Pedestrian generation through Multi-Grained Discrimination,https://arxiv.org/abs/2010.00947,206 images,,
749,CUHK-SYSU,Person Search,Person Search,"Person Search, Person Re-Identification",Image,,Computer Vision,"person-search-on-cuhk-sysu, person-re-identification-on-cuhk-sysu",,http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html,https://paperswithcode.com/dataset/cuhk-sysu,"The CUKL-SYSY dataset is a large scale benchmark for person search, containing 18,184 images and 8,432 identities. Different from previous re-id benchmarks, matching query persons with manually cropped pedestrians, this dataset is much closer to real application scenarios by searching person from whole images in the gallery.",,,,184 images,,
750,CUHK03-C,Person Re-Identification,Person Re-Identification,"Person Re-Identification, Generalizable Person Re-identification",Image,,Computer Vision,person-re-identification-on-cuhk03-c,,https://github.com/MinghuiChen43/CIL-ReID,https://paperswithcode.com/dataset/cuhk03-c,"CUHK03-C is an evaluation set that consists of algorithmically generated corruptions applied to the CUHK03 test-set.  These corruptions consist of Noise: Gaussian, shot,
impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions.",,,,,,
751,CUHK03,Face Sketch Synthesis,Face Sketch Synthesis,"Face Sketch Synthesis, Defocus Blur Detection, Defocus Estimation, Person Re-Identification, Generalizable Person Re-identification",Image,,Computer Vision,"person-re-identification-on-cuhk03-detected, face-sketch-synthesis-on-cuhk, person-re-identification-on-cuhk03-labeled, person-re-identification-on-cuhk03, defocus-estimation-on-cuhk-blur-detection, person-re-identification-on-cuhk03-detected-1, defocus-blur-detection-on-cuhk, generalizable-person-re-identification-on-22",,http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html,https://paperswithcode.com/dataset/cuhk03,"The CUHK03 consists of 14,097 images of 1,467 different identities, where 6 campus cameras were deployed for image collection and each identity is captured by 2 campus cameras. This dataset provides two types of annotations, one by manually labelled bounding boxes and the other by bounding boxes produced by an automatic detector. The dataset also provides 20 random train/test splits in which 100 identities are selected for testing and the rest for training",,Attention Driven Person Re-identification,https://arxiv.org/abs/1810.05866,097 images,,
752,CUHK_Image_Cropping,Image Cropping,Image Cropping,"Image Cropping, Image Enhancement, Decision Making",Image,,Computer Vision,,,http://personal.ie.cuhk.edu.hk/~ccloy/downloads_cuhk_crop_dataset.html,https://paperswithcode.com/dataset/cuhk-image-cropping,"CUHK Image Cropping  is a dataset for image cropping. The photos are of varying aesthetic quality and span a variety of image categories, including animal, architecture, human, landscape, night, plant and man-made objects. Each image is manually cropped by three expert photographers (graduate students in art whose primary medium is photography) to form three training sets. There are 1,000 photos in the dataset.",,,,,,
753,Customer_Support_on_Twitter,Personalized Federated Learning,Personalized Federated Learning,"Personalized Federated Learning, Federated Learning, NLP based Person Retrival, Question Answering, Chatbot","Image, Text",English,Computer Vision,,CC BY-NC-SA 4.0,https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter,https://paperswithcode.com/dataset/customer-support-on-twitter,"The Customer Support on Twitter dataset is a large, modern corpus of tweets and replies to aid innovation in natural language understanding and conversational models, and for study of modern customer support practices and impact.",,,,,,
754,CustomHumans,3D Human Reconstruction,3D Human Reconstruction,3D Human Reconstruction,"3D, Image",,Computer Vision,3d-human-reconstruction-on-customhumans,ETH,https://custom-humans.github.io/#download,https://paperswithcode.com/dataset/customhumans,"CustomHumans is recorded by a multi-view photogrammetry system equipped with 53 RGB (12 Megapixels) and 53 (4 Megapixels) IR cameras. The resulting high-quality scan is composed of a 40K-face mesh alongside a 4K-resolution texture map. In addition to the high-quality scans, CustomHumans provides accurately registered SMPL-X parameters using a customized mesh registration pipeline. 80 participants are invited for the data capturing. Each of them is instructed to perform several movements, such as ""T-pose"", ""Hands Up'"", ""Squat'"", ""Turing head'', and ""Hand gestures"", in a 10-second long sequence (300 frames). 4-5 best-quality meshes in each sequence are selected as the data samples. In total, the dataset contains more than 600 high-quality scans with 120 different garments.",,,,,,
755,CUTE80,Scene Text Recognition,Scene Text Recognition,Scene Text Recognition,"Image, Text",English,Computer Vision,scene-text-recognition-on-cute80,,https://github.com/mohtashim-nawaz/Cute80-Dataset,https://paperswithcode.com/dataset/cute80,"The CUTE80 dataset is a lightweight collection of images specifically designed for text detection in natural scene images. It contains a total of 13,000 annotated page images across five different popular categories: 1) Table 2) Figure 3) Natural image 4) Logo 5) ignature",,,,,,
756,CVB,Multi-Animal Tracking with identification,Multi-Animal Tracking with identification,"Multi-Animal Tracking with identification, Action Recognition In Videos, Animal Action Recognition, Action Localization, 2D Object Detection, Action Recognition, 3D Classification, 3D Multi-Object Tracking","3D, Image, Video",,Computer Vision,,Creative Commons Attribution 4.0 International,https://doi.org/10.25919/3g3t-p068,https://paperswithcode.com/dataset/cvb-a-video-dataset-of-cattle-visual,"Existing image/video datasets for cattle behavior recognition are mostly small, lack well-defined labels, or are collected in unrealistic controlled environments. This limits the utility of machine learning (ML) models learned from them. Therefore, we introduce a new dataset, called Cattle Visual Behaviors (CVB), that consists of 502 video clips, each fifteen seconds long, captured in natural lighting conditions, and annotated with eleven visually perceptible behaviors of grazing cattle. By creating and sharing CVB, our aim is to develop improved models capable of recognizing all important cattle behaviors accurately and to assist other researchers and practitioners in developing and evaluating new ML models for cattle behavior classification using video data.
The dataset is presented in the form of following three sub-directories.
1. raw_frames: contains 450 frames in each sub folder representing a 15 second video taken at a frame rate of 30 FPS.
2. annotations: contains the json files corresponding to the raw_frames folder. There is one json file for each video, that contains the bounding-box annotations for each cattle in the video and its associated behavior, and
3. CVB_in_AVA_format: contains the CVB data in the AVA dataset format.",,,,,,
757,CVC-ClinicDB,Medical Image Segmentation,Medical Image Segmentation,Medical Image Segmentation,Image,,Medical,medical-image-segmentation-on-cvc-clinicdb,,https://polyp.grand-challenge.org/CVCClinicDB/,https://paperswithcode.com/dataset/cvc-clinicdb,"CVC-ClinicDB is an open-access dataset of 612 images with a resolution of 384×288 from 31 colonoscopy sequences.It is used for medical image segmentation, in particular polyp detection in colonoscopy videos.",,ResUNet++: An Advanced Architecture for Medical Image Segmentation,https://arxiv.org/abs/1911.07067,612 images,,
758,CVCS,Crowd Counting,Crowd Counting,"Crowd Counting, Multiview Detection",Image,,Computer Vision,multiview-detection-on-cvcs,,http://visal.cs.cityu.edu.hk/downloads/citystreetdata/,https://paperswithcode.com/dataset/cvcs,"CVCS is a synthetic multi-view people dataset, containing 31 scenes, where 23 are for training and the rest 8 for testing. The scene size varies from about 10m∗20m to 90m∗80m. Each scene contains 100 multi-view frames. The ground plane map resolution is 900×800, where each grid stands for 0.1 meters in the real world. In training, 5 views are randomly selected 5 times in each iteration per scene frame, and the same view number is randomly selected 21 times in evaluation.",,,,,,
759,CVEfixes,Vulnerability Detection,Vulnerability Detection,Vulnerability Detection,Image,,Computer Vision,,Attribution 4.0 International,https://zenodo.org/record/4476564,https://paperswithcode.com/dataset/cvefixes,"CVEfixes is a comprehensive vulnerability dataset that is automatically collected and curated from Common Vulnerabilities and Exposures (CVE) records in the public U.S. National Vulnerability Database (NVD). The goal is to support data-driven security research based on source code and source code metrics related to fixes for CVEs in the NVD by providing detailed information at different interlinked levels of abstraction, such as the commit-, file-, and method level, as well as the repository- and CVE level.

At the initial release, the dataset covers all published CVEs up to 9 June 2021. All open-source projects that were reported in CVE records in the NVD in this time frame and had publicly available git repositories were fetched and considered for the construction of this vulnerability dataset. The dataset is organized as a relational database and covers 5495 vulnerability fixing commits in 1754 open source projects for a total of 5365 CVEs in 180 different Common Weakness Enumeration (CWE) types. The dataset includes the source code before and after fixing of 18249 files, and 50322 functions.",2021,,,,,
760,CVIT_PIB,Multlingual Neural Machine Translation,Multlingual Neural Machine Translation,"Multlingual Neural Machine Translation, Low-Resource Neural Machine Translation",Text,English,Natural Language Processing,,CC BY-SA 4.0,http://preon.iiit.ac.in/~jerin/bhasha/,https://paperswithcode.com/dataset/cvit-pib,"We present sentence aligned parallel corpora across 10 Indian Languages - Hindi, Telugu, Tamil, Malayalam, Gujarati, Urdu, Bengali, Oriya, Marathi, Punjabi, and English - many of which are categorized as low resource. The corpora are compiled from online sources which have content shared across languages. The corpora presented significantly extends present resources that are either not large enough or are restricted to a specific domain (such as health). We also provide a separate test corpus compiled from an independent online source that can be independently used for validating the performance in 10 Indian languages. Alongside, we report on the methods of constructing such corpora using tools enabled by recent advances in machine translation and cross-lingual retrieval using deep neural network based methods.",,,,,,
761,CVR,single catogory classification,single catogory classification,"single catogory classification, General Classification",Image,,Computer Vision,general-classification-on-cvr,Creative Commons Attribution 4.0 International,https://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records,https://paperswithcode.com/dataset/cvr,"This data set includes votes for each of the U.S. House of Representatives Congressmen on the 16 key votes identified by the CQA. The CQA lists nine different types of votes: voted for, paired for, and announced for (these three simplified to yea), voted against, paired against, and announced against (these three simplified to nay), voted present, voted present to avoid conflict of interest, and did not vote or otherwise make a position known (these three simplified to an unknown disposition).",,,,,,
762,CVRPTW,Combinatorial Optimization,Combinatorial Optimization,Combinatorial Optimization,,,Methodology,,MIT,https://github.com/jokofa/JAMPR,https://paperswithcode.com/dataset/cvrptw,"Random sampled instances of the Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) for 20, 50 and 100 customer nodes.


Coordinates sampled from unit square
demands sampled as integers from range [1, 9]
time windows sampled with: 
ready times (TW start) as random integers in time horizon T
due times (TW end) sampled from Normal distribution



The dataset is used as a validation and test set to evaluate machine learning based solvers for the CVRPTW. 
The corresponding open source sample code can be used to sample corresponding training data  or additional validation and test data, also for other problem sizes.",,,,,,
763,CVSS,fr-en,fr-en,"fr-en, Text-To-Speech Synthesis, Speech-to-Speech Translation, es-en, Translation, de-en","Audio, Text",English,Natural Language Processing,"speech-to-speech-translation-on-cvss, fr-en-on-cvss, de-en-on-cvss, es-en-on-cvss",CC BY 4.0,https://github.com/google-research-datasets/cvss,https://paperswithcode.com/dataset/cvss,"CVSS is a massively multilingual-to-English speech to speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice  speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems",,,,,,
764,CWL_EEG_fMRI_Dataset,Electroencephalogram (EEG),Electroencephalogram (EEG),"Electroencephalogram (EEG), EEG Artifact Removal, Eeg Decoding",,,Methodology,eeg-decoding-on-cwl-eeg-fmri-dataset,MIT,https://www.nitrc.org/projects/cwleegfmri_data/,https://paperswithcode.com/dataset/cwl-eeg-fmri-data-set,"EEG/fMRI Data from 8 subject doing a simple eyes open/eyes closed task is provided on this webpage.

The EEG/fMRI data are six files for each subject, with two basic factors: recording during Helium pump On and Helium pump Off, and recording during MRI scanning and without MRI scanning. In addition 'outside' EEG data is provided, before as well as after the MRI session.

There are 30 EEG channels, 1 EOG channel, 1 ECG channel, as well as 6 CWL signals.",,,,,,
765,CY101_Dataset,3D Object Classification,3D Object Classification,"3D Object Classification, 2D Object Detection, 3D Object Recognition, 3D Object Detection","3D, Image",,Computer Vision,,,https://www.eecs.tufts.edu/~gtatiya/pages/2014/CY101Dataset.html,https://paperswithcode.com/dataset/cy101-dataset,"In this dataset an uppertorso humanoid robot with 7-DOF arm explored 100 different objects belonging to 20 different categories using 10 behaviors: Look, Crush, Grasp, Hold, Lift, Drop, Poke, Push, Shake and Tap.",,,,,,
766,Cyclone_Data,3D Object Classification,3D Object Classification,3D Object Classification,"3D, Image",,Computer Vision,,Creative Commons Attribution 4.0 International,https://data.humdata.org/dataset/archive-of-global-tropical-cyclone-tracks-1980-may-2019,https://paperswithcode.com/dataset/cyclone-data,Archive of Global Tropical Cyclone Tracks Tracks from 1980 to May 2019.,1980,,,,,
767,Czech_Subjectivity_Dataset,Subjectivity Analysis,Subjectivity Analysis,Subjectivity Analysis,,,Methodology,subjectivity-analysis-on-czech-subjectivity,Creative Commons Attribution 4.0 International,https://github.com/pauli31/czech-subjectivity-dataset,https://paperswithcode.com/dataset/czech-subjectivity-dataset,Czech subjectivity dataset of 10k manually annotated subjective and objective sentences from movie reviews and descriptions. See the paper description https://arxiv.org/abs/2204.13915,,,,,,
768,C__EditCompletion,EditCompletion,EditCompletion,EditCompletion,,,Methodology,editcompletion-on-c-editcompletion,,https://github.com/tech-srl/c3po/,https://paperswithcode.com/dataset/c-editcompletion,"We scraped the 53 most popular C# repositories from GitHub and extracted all commits since the beginning of the project’s history. From each commit, we extracted edits in C# files along with the edits in their surrounding context.",,,,,,
769,D4RL,Gym halfcheetah-medium,Gym halfcheetah-medium,"Gym halfcheetah-medium, Gym halfcheetah-full-replay, Adroid door-human, Offline RL, Adroid pen-cloned, Adroid relocate-human, Continuous Control, D4RL, Adroid pen-human, Gym halfcheetah-medium-replay, Gym halfcheetah-expert, Adroid hammer-cloned, Decision Making, Adroid relocate-cloned, Gym halfcheetah-medium-expert, Gym halfcheetah-random, Adroid door-cloned, Adroid hammer-human",Image,,Computer Vision,"adroid-door-cloned-on-d4rl, d4rl-on-d4rl, offline-rl-on-d4rl, gym-halfcheetah-medium-replay-on-d4rl, adroid-pen-human-on-d4rl, adroid-relocate-cloned-on-d4rl, gym-halfcheetah-medium-expert-on-d4rl, gym-halfcheetah-full-replay-on-d4rl, adroid-hammer-cloned-on-d4rl, gym-halfcheetah-expert-on-d4rl, adroid-door-human-on-d4rl, adroid-hammer-human-on-d4rl, adroid-pen-cloned-on-d4rl, gym-halfcheetah-random-on-d4rl, adroid-relocate-human-on-d4rl, gym-halfcheetah-medium-on-d4rl",Apache-2 / CC-BY,https://sites.google.com/view/d4rl/home,https://paperswithcode.com/dataset/d4rl,"D4RL is a collection of environments for offline reinforcement learning. These environments include Maze2D, AntMaze, Adroit, Gym, Flow, FrankKitchen and CARLA.",,,,,,
770,DACCORD,Binary text classification,Binary text classification,"Binary text classification, Sentence-Pair Classification, Text Pair Classification","Image, Text",English,Computer Vision,,"BSD-2-Clause ""Simplified"" License",,https://paperswithcode.com/dataset/daccord,"DACCORD is a new dataset dedicated to the task of automatically detecting contradictions between sentences in French. 

The dataset is currently composed of 1034 sentence pairs. It covers the themes of Russia’s invasion of Ukraine in 2022, the Covid-19 pandemic, and the climate crisis.",2022,,,,,
771,DAD,Contrastive Learning,Contrastive Learning,"Contrastive Learning, Anomaly Detection, Open Set Learning",Image,,Computer Vision,,,https://github.com/okankop/Driver-Anomaly-Detection,https://paperswithcode.com/dataset/dad,"Contains normal driving videos together with a set of anomalous actions in its training set. In the test set of the DAD dataset, there are unseen anomalous actions that still need to be winnowed out from normal driving.",,,,,,
772,DADE,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Test-time Adaptation",Image,,Computer Vision,,CC-BY-4.0,https://github.com/ULiege-driving/DADE,https://paperswithcode.com/dataset/dade,"The DADE dataset, short for Driving Agents in Dynamic Environments, is a synthetic dataset designed for the training and evaluation of methods for the task of semantic segmentation in the context of autonomous driving agents navigating dynamic environments and weather conditions.

This dataset was generated using the CARLA simulator (version 0.9.14) to provide perfect sensor synchronization and calibration, as well as precise semantic segmentation ground truths. All data were collected within the Town12 map in CARLA.

DADE dataset is divided into two sub-datasets. For both subsets, each sequence is acquired by one agent (one ego vehicle) running for some time within a 5-hour time frame, amounting to a total of 990k frames for the entire dataset. The agents travel various locations such as forest, countryside, rural farmland, highway, low density residential, community buildings, and high density residential.",,,,,,
773,Dafonts_Free,Font Style Transfer,Font Style Transfer,"Font Style Transfer, Font Recognition, Font Generation","Image, Text",English,Computer Vision,,Custom (MIT Based),https://github.com/duskvirkus/dafonts-free,https://paperswithcode.com/dataset/dafonts-free,"This is a dataset of 18624 fonts labeled as 100% Free and Public domain / GPL / OFL on https://www.dafont.com/ with .ttf and .otf extensions.

Code used to create it can be found at: https://github.com/duskvirkus/dafonts-free",,,,,,
774,DAHLIA,Activity Recognition,Activity Recognition,"Activity Recognition, Activity Recognition In Videos, Activity Detection, Home Activity Monitoring","Image, Video",,Computer Vision,,,https://www-mobilemii.cea.fr/?page_id=20,https://paperswithcode.com/dataset/dahlia-daily-human-life-activity,"DAHLIA dataset [1] is devoted to human activity recognition, which is a major issue for adapting smart-home services such as user assistance.
DAHLIA has been realized in Mobile Mii Platform by CEA LIST, and has been partly supported by ITEA 3 Emospaces Project (https://itea3.org/project/emospaces.html)

Videos were recorded in realistic conditions, with 3 Kinect v2 sensors located as they would be in a real context. The long-range activities were performed in an unconstrained way (participants received only few instructions), and in a continuous (untrimmed) sequence, resulting in long videos (40 min in average per subject). Contrary to previously published databases, in which labeled actions are very short and have low-semantic level, this new database focuses on high-level semantic activities such as « Preparing lunch » or « House Working ».

[1] G. Vaquette, A. Orcesi, L. Lucat and C. Achard, ""The DAily Home LIfe Activity Dataset: A High Semantic Activity Dataset for Online Recognition,"" 2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017), 2017, pp. 497-504, doi: 10.1109/FG.2017.67.",2017,,,,,
775,DailyDialog,Text Generation,Text Generation,"Text Generation, Emotion Recognition in Conversation","Image, Text",English,Computer Vision,"emotion-recognition-in-conversation-on-3, text-generation-on-dailydialog","Custom (research-only, non-commercial)",http://yanran.li/dailydialog,https://paperswithcode.com/dataset/dailydialog,"DailyDialog is a high-quality multi-turn open-domain English dialog dataset. It contains 13,118 dialogues split into a training set with 11,118 dialogues and validation and test sets with 1000 dialogues each. On average there are around 8 speaker turns per dialogue with around 15 tokens per turn.",,,,,,
776,Daily_and_Sports_Activities,Clustering,Clustering,"Clustering, Classification",Image,,Computer Vision,,CC BY 4.0,https://archive.ics.uci.edu/dataset/256/daily+and+sports+activities,https://paperswithcode.com/dataset/daily-and-sports-activities,"The dataset comprises motion sensor data of 19 daily and sports activities each performed by 8 subjects in their own style for 5 minutes. Five Xsens MTx units are used on the torso, arms, and legs.

Dataset Characteristics: Multivariate, Time-Series
Associated Tasks: Classification, Clustering
Feature Type: Real

Instances: 9120
Features: 5625",,,,,,
777,DAiSEE,Student Engagement Level Detection (Four Class Video Classification),Student Engagement Level Detection (Four Class Video Classification),"Student Engagement Level Detection (Four Class Video Classification), Multiple Instance Learning, Sentiment Analysis, Facial Expression Recognition (FER)","Image, Text, Video",English,Computer Vision,student-engagement-level-detection-four-class,,https://people.iith.ac.in/vineethnb/resources/daisee/index.html,https://paperswithcode.com/dataset/daisee,"DAiSEE is a multi-label video classification dataset comprising of 9,068 video snippets captured from 112 users for recognizing the user affective states of boredom, confusion, engagement, and frustration ""in the wild"". The dataset has four levels of labels namely - very low, low, high, and very high for each of the affective states, which are crowd annotated and correlated with a gold standard annotation created using a team of expert psychologists.",,,,,,
778,Dakshina,Language Modelling,Language Modelling,"Language Modelling, Transliteration, Language Identification",Text,English,Natural Language Processing,,,https://github.com/google-research-datasets/dakshina,https://paperswithcode.com/dataset/dakshina,"The Dakshina dataset is a collection of text in both Latin and native scripts for 12 South Asian languages. For each language, the dataset includes a large collection of native script Wikipedia text, a romanization lexicon which consists of words in the native script with attested romanizations, and some full sentence parallel data in both a native script of the language and the basic Latin alphabet.",,,,,,
779,DaLAJ,Linguistic Acceptability,Linguistic Acceptability,Linguistic Acceptability,,,Methodology,linguistic-acceptability-on-dalaj,CC BY 4.0,https://spraakbanken.gu.se/en/resources/dalaj-1-0,https://paperswithcode.com/dataset/dalaj,"DaLAJ 1.0, a dataset for Linguistic Acceptability Judgments for Swedish, comprising 9,596 sentences in its first version; and the initial experiment using it for the binary classification task. DaLAJ is based on the SweLL second language learner data, consisting of essays at different levels of proficiency.",,,,596 sentences,,
780,DanceTrack,Multi-Object Tracking,Multi-Object Tracking,Multi-Object Tracking,"Image, Video",,Computer Vision,multi-object-tracking-on-dancetrack,MIT,https://sites.google.com/view/dancetrackmot,https://paperswithcode.com/dataset/dancetrack,"A large-scale multi-object tracking dataset for human tracking in occlusion, frequent crossover, uniform appearance and diverse body gestures. It is proposed to emphasize the importance of motion analysis in multi-object tracking instead of mainly appearance-matching-based diagram.",,,,,,
781,DaN_,Named Entity Recognition (NER),Named Entity Recognition (NER),"Named Entity Recognition (NER), Nested Named Entity Recognition","Image, Text",English,Computer Vision,,MIT,https://github.com/bplank/DaNplus,https://paperswithcode.com/dataset/dan,DaN+ is a new multi-domain corpus and annotation guidelines for Danish nested named entities (NEs) and lexical normalization to support research on cross-lingual cross-domain learning for a less-resourced language.,,,,,,
782,DaReCzech,Document Ranking,Document Ranking,Document Ranking,Text,English,Natural Language Processing,document-ranking-on-dareczech,,https://github.com/seznam/DaReCzech,https://paperswithcode.com/dataset/dareczech,"DareCzech
DaReCzech is a dataset for text relevance ranking in Czech. The dataset consists of more than 1.6M annotated query-documents pairs, which makes it one of the largest available datasets for this task.

Obtaining the Annotated Data
Please, first read a disclaimer that contains the terms of use. If you comply with them, send an email to srch.vyzkum@firma.seznam.cz and the link to the dataset will be sent to you.",,,,,,
783,Dark_Zurich,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Source-Free Domain Adaptation, Unsupervised Semantic Segmentation",Image,,Computer Vision,"semantic-segmentation-on-dark-zurich, unsupervised-semantic-segmentation-on-dark, source-free-domain-adaptation-on-cityscapes-1",,https://www.trace.ethz.ch/publications/2019/GCMA_UIoU/,https://paperswithcode.com/dataset/dark-zurich,"Dark Zurich is an image dataset containing a total of 8779 images captured at nighttime, twilight, and daytime, along with the respective GPS coordinates of the camera for each image. These GPS annotations are used to construct cross-time-of-day correspondences, i.e., to match each nighttime or twilight image to its daytime counterpart.

These attributes allow the usage of Dark Zurich as a dataset to build models and systems that perform:

1) domain adaptation (unsupervised, weakly supervised or semi-supervised), e.g. for semantic segmentation or object detection,

2) image translation / style transfer to different times of day,

3) robust image matching / visual localization across diverse domains, and

4) other visual perception tasks that are central for autonomous vehicles and other robotic applications.",,,,8779 images,,
784,Darmstadt_Noise_Dataset,Color Image Denoising,Color Image Denoising,"Color Image Denoising, Denoising",Image,,Computer Vision,"color-image-denoising-on-darmstadt-noise, denoising-on-darmstadt-noise-dataset",,,https://paperswithcode.com/dataset/darmstadt-noise-dataset,the dataset contains data about hydrogen storage in metal hydrides,,,,,,
785,DART,Language Modelling,Language Modelling,"Language Modelling, Data-to-Text Generation, Text Generation, Question Answering, Table-to-Text Generation","Tabular, Text",English,Natural Language Processing,"table-to-text-generation-on-dart, text-generation-on-dart, data-to-text-generation-on-dart",,https://github.com/Yale-LILY/dart,https://paperswithcode.com/dataset/dart,"DART is a large dataset for open-domain structured data record to text generation. DART consists of 82,191 examples across different domains with each input being a semantic RDF triple set derived from data records in tables and the tree ontology of the schema, annotated with sentence descriptions that cover all facts in the triple set.",,,,191 examples,,
786,Database_of_axial_impact_simulations_of_the_crash_,Crashworthiness Design,Crashworthiness Design,"Crashworthiness Design, Multiobjective Optimization",,,Methodology,,MIT,https://github.com/MrAdityaBorse/FEM_Data_Impact_Simulation,https://paperswithcode.com/dataset/database-of-axial-impact-simulations-of-the,This repository contains the database of the FEM simulation of axially impacted various configurations of the square crash boxes. This database records the impact of the structural and crash test parameters on the various crashworthiness objectives.,,,,,,
787,Dataset_of_Context_information_for_Zero_Interactio,Mobile Security,Mobile Security,Mobile Security,,,Methodology,,Open Data Commons Attribution License v.1.0,https://zenodo.org/record/2537721,https://paperswithcode.com/dataset/dataset-of-context-information-for-zero,"We release both the processed data and evaluation results from our own experiments, and the underlying raw data that can be used for future experiments and schemes in the domain of Zero-Interaction Security. Find more details in the dataset description on Zenodo.",,,,,,
788,Dataset_of_Propaganda_Techniques_of_the_State-Spon,Multi-Label Text Classification,Multi-Label Text Classification,"Multi-Label Text Classification, Multi-Label Classification, Propaganda technique identification","Image, Text",English,Computer Vision,multi-label-text-classification-on-dataset-of,CC BY-NC-SA,https://github.com/annabechang/Propaganda_Tech_Twitter_PRC,https://paperswithcode.com/dataset/dataset-of-propaganda-techniques-of-the-state,"This data is for the Mis2-KDD 2021 under review paper: Dataset of Propaganda Techniques of the State-Sponsored Information Operation of the People’s Republic of China

We present our dataset that focuses on propaganda techniques in Mandarin based on a state-linked information operations dataset from the PRC released by Twitter in July 2019. The dataset consists of multi-label propaganda techniques of the sampled tweets.

In total, we have 9,950 labeled tweets with 21 different propaganda techniques. The tweets are the state-linked information operations dataset from the PRC released by Twitter.",2021,,,,,
789,Dataset__Privacy-Preserving_Gaze_Data_Streaming_in,De-identification,De-identification,"De-identification, Person Re-Identification",Image,,Computer Vision,,Creative Commons Attribution 4.0 International,https://doi.org/10.5281/zenodo.10519537,https://paperswithcode.com/dataset/dataset-privacy-preserving-gaze-data,"Collected data from two distinct experiments in immersive, interactive VR where participants performed dynamic tasks as their eye, head, and hand movements were recorded.  In the second experiment, a range of real-time privacy mechanisms are applied to eye gaze in real-time.",,,,,,
790,Data_Collected_with_Package_Delivery_Quadcopter_Dr,Model extraction,Model extraction,"Model extraction, Multivariate Time Series Forecasting, Time Series Prediction",Time Series,,Methodology,time-series-prediction-on-data-collected-with,CC BY 4.0,https://doi.org/10.1184/R1/12683453.v1,https://paperswithcode.com/dataset/data-collected-with-package-delivery,"This experiment was performed in order to empirically measure the energy use of small, electric Unmanned Aerial Vehicles (UAVs). We autonomously direct a DJI ® Matrice 100 (M100) drone to take off, carry a range of payload weights on a triangular flight pattern, and land. Between flights, we varied specified parameters through a set of discrete options, payload of 0 , 250 g and 500 g; altitude during cruise of 25 m, 50 m, 75 m and 100 m; and speed during cruise of 4 m/s, 6 m/s, 8 m/s, 10 m/s and 12 m/s.

We simultaneously collect data from a broad array of on-board sensors. The onboard sensors used to collect these data are



Wind sensor: FT Technologies FT205 UAV-mountable, pre-calibrated ultrasonic wind sensor with accuracy of $\pm$ 0.1 m/s and refresh rate of 10 Hz.;



Position: 3DM-GX5-45 GNSS/INS sensor pack. These sensors use a built-in Kalman filtering system to fuse the GPS and IMU data. The sensor has a maximum output rate of 10Hz with accuracy of $\pm$2 m RMS horizontal, $\pm$5 m RMS vertical.



Current and Voltage: Mauch Electronics PL-200 sensor. This sensor can record currents up to 200 A and voltages up to 33 V. Analogue readings from the sensor were converted into a digital format using an 8 channel 17 bit analogue-to-digital converter (ADC).



The number of flights performed varying operational parameters (payload, altitude, speed) was 196. In addition, 13 recordings were done to assess the drone’s ancillary power and hover conditions.",,,,,,
791,DAVIS-DTA,,,", Protein Language Model, Drug Discovery",Text,English,Natural Language Processing,"drug-discovery-on-davis-dta, on-davis-dta, protein-language-model-on-davis-dta",,https://tdcommons.ai/multi_pred_tasks/dti/,https://paperswithcode.com/dataset/davis-dta,"Dataset Description: The interaction of 72 kinase inhibitors with 442 kinases covering >80% of the human catalytic protein kinome.

Task Description: Regression. Given the target amino acid sequence/compound SMILES string, predict their binding affinity.

Dataset Statistics: 0.3.2 Update: 25,772 DTI pairs, 68 drugs, 379 proteins. Before: 27,621 DTI pairs, 68 drugs, 379 proteins.

[1] Davis, M., Hunt, J., Herrgard, S. et al. Comprehensive analysis of kinase inhibitor selectivity. Nat Biotechnol 29, 1046–1051 (2011).

[2] Huang, Kexin, et al. “DeepPurpose: a Deep Learning Library for Drug-Target Interaction Prediction” Bioinformatics.",2011,,,,,
792,DAVIS-Edit,Text-to-Video Editing,Text-to-Video Editing,Text-to-Video Editing,"Text, Video",English,Natural Language Processing,,MIT,https://huggingface.co/datasets/AlonzoLeeeooo/DAVIS-Edit,https://paperswithcode.com/dataset/davis-edit,"DAVIS-Edit is a curated testing benchmark for video editing. This dataset contains two evaluation settings, i.e., text- and image-based editing. Besides, it offers two types of annotated for both modalities of prompts, considering the editing scenarios with similar (DAVIS-Edit-S) and changing (DAVIS-Edit-C) shapes, so as to address the shape inconsistency problem in video-to-video editing.",,,,,,
793,DAVIS,Video Prediction,Video Prediction,"Video Prediction, Semi-Supervised Video Object Segmentation, Interactive Video Object Segmentation, Visual Tracking, Video Inpainting, Semantic Segmentation, Video Frame Interpolation, Interactive Segmentation, Video Denoising, Video Object Segmentation","Image, Time Series, Video",,Computer Vision,"video-denoising-on-davis-sigma10, video-denoising-on-davis-sigma40, video-denoising-on-davis-sigma20, visual-tracking-on-davis, video-object-segmentation-on-davis-2017, interactive-segmentation-on-davis, video-frame-interpolation-on-davis, video-prediction-on-davis-2017, video-denoising-on-davis-sigma50, video-inpainting-on-davis, interactive-video-object-segmentation-on, semi-supervised-video-object-segmentation-on-20, video-denoising-on-davis-sigma30",,https://davischallenge.org/,https://paperswithcode.com/dataset/davis,"The Densely Annotation Video Segmentation dataset (DAVIS) is a high quality and high resolution densely annotated video segmentation dataset under two resolutions, 480p and 1080p. There are 50 video sequences with 3455 densely annotated frames in pixel level. 30 videos with 2079 frames are for training and 20 videos with 1376 frames are for validation.",2079,TENet: Triple Excitation Network for Video Salient Object Detection,https://arxiv.org/abs/2007.09943,,,
794,DAVIS_2016,Video Object Segmentation,Video Object Segmentation,"Video Object Segmentation, Unsupervised Object Segmentation, Semi-Supervised Video Object Segmentation, Unsupervised Video Object Segmentation","Image, Video",,Computer Vision,"unsupervised-object-segmentation-on-davis, visual-object-tracking-on-davis-2016, video-object-segmentation-on-davis-2016, unsupervised-video-object-segmentation-on-10",Annotations under CC BY-SA 4.0,https://davischallenge.org/davis2016/code.html,https://paperswithcode.com/dataset/davis-2016,DAVIS16 is a dataset for video object segmentation which consists of 50 videos in total (30 videos for training and 20 for testing). Per-frame pixel-wise annotations are offered.,,Learning Discriminative Feature with CRF for Unsupervised Video Object Segmentation,https://arxiv.org/abs/2008.01270,,,
795,DAVIS_2017,Video Prediction,Video Prediction,"Video Prediction, Semi-Supervised Video Object Segmentation, Interactive Video Object Segmentation, Unsupervised Video Object Segmentation, Semantic Segmentation, Video Object Segmentation, Referring Expression Segmentation","Image, Time Series, Video",,Computer Vision,"semi-supervised-video-object-segmentation-on-2, video-object-segmentation-on-davis-2017, referring-expression-segmentation-on-davis, unsupervised-video-object-segmentation-on-4, video-object-segmentation-on-davis-2017-val, interactive-video-object-segmentation-on, video-prediction-on-davis-2017, unsupervised-video-object-segmentation-on-5, semi-supervised-video-object-segmentation-on-1, visual-object-tracking-on-davis-2017, video-object-segmentation-on-davis-2017-test-1",Annotations under CC BY-SA 4.0,https://davischallenge.org/challenge2017/index.html,https://paperswithcode.com/dataset/davis-2017,"DAVIS17 is a dataset for video object segmentation.  It contains a total of 150 videos - 60 for training, 30 for validation, 60 for testing",,Siam R-CNN: Visual Tracking by Re-Detection,https://arxiv.org/abs/1911.12836,,,
796,DAWT,Text Categorization,Text Categorization,"Text Categorization, Information Retrieval, Entity Embeddings",Text,English,Natural Language Processing,,,https://github.com/klout/opendata/tree/master/wiki_annotation,https://paperswithcode.com/dataset/dawt,"The DAWT dataset consists of Densely Annotated Wikipedia Texts across multiple languages. The annotations include labeled text mentions mapping to entities (represented by their Freebase machine ids) as well as the type of the entity. The data set contains total of 13.6M articles, 5.0B tokens, 13.8M mention entity co-occurrences. DAWT contains 4.8 times more anchor text to entity links than originally present in the Wikipedia markup. Moreover, it spans several languages including English, Spanish, Italian, German, French and Arabic.",,,,,,
797,DBE-KT22,Knowledge Tracing,Knowledge Tracing,"Knowledge Tracing, Temporal Sequences","Time Series, Video",,Methodology,,,https://dataverse.ada.edu.au/dataset.xhtml?persistentId=doi:10.26193/6DZWOH,https://paperswithcode.com/dataset/dbe-kt22,DBE-KT22 contains student exercise answering activities collected through an online practicing platform for the database systems course taught at the Australian National University within the period 2018-2021. The dataset is useful for research targeting students' knowledge tracing given historical sequences of exercise answering.,2018,,,,,
798,DBLP,Node Clustering,Node Clustering,"Node Clustering, Link Prediction, Community Detection, Node Classification, Heterogeneous Node Classification","Image, Time Series",,Computer Vision,"link-prediction-on-dblp, heterogeneous-node-classification-on-dblp-1, community-detection-on-dblp, node-clustering-on-dblp, node-classification-on-dblp",,https://www.aminer.org/citation,https://paperswithcode.com/dataset/dblp,"The DBLP is a citation network dataset. The citation data is extracted from DBLP, ACM, MAG (Microsoft Academic Graph), and other sources. The first version contains 629,814 papers and 632,752 citations. Each paper is associated with abstract, authors, year, venue, and title.
The data set can be used for clustering with network and side information, studying influence in the citation network, finding the most influential papers, topic modeling analysis, etc.",,,,,,
799,DBLP_Temporal,Entity Resolution,Entity Resolution,"Entity Resolution, Dynamic Link Prediction",Time Series,,Methodology,dynamic-link-prediction-on-dblp-temporal,,https://github.com/E-Chen/A-refined-DBLP-temporal-dataset,https://paperswithcode.com/dataset/dblp-temporal,"DBLP Temporal is a dataset for temporal entity resolution, based on author profiles extracted from the Digital Bibliography and Library Project (DBLP).",,,,,,
800,DBLP__Heterogeneous_Node_Classification_,Heterogeneous Node Classification,Heterogeneous Node Classification,Heterogeneous Node Classification,Image,,Computer Vision,heterogeneous-node-classification-on-dblp-2,,,https://paperswithcode.com/dataset/dblp-heterogeneous-node-classification,A popular dataset for node classification on heterogeneous graphs.,,,,,,
801,DBP-5L__Greek_,Entity Alignment,Entity Alignment,"Entity Alignment, Knowledge Graph Completion",Graph,,Methodology,knowledge-graph-completion-on-dbp-5l,,,https://paperswithcode.com/dataset/dbp-5l,"DPB-5L is a Multilingual KG dataset containing 5 KGs in English, French, Japanese, Greek, and Spanish. 
The dataset is used for the Knowledge Graph Completion and Entity Alignment task.
DPB-5L (Greek) is a subset of DPB-5L with Greek KG.",,,,,,
802,DBP15K,Multi-modal Entity Alignment,Multi-modal Entity Alignment,"Multi-modal Entity Alignment, Entity Alignment, Knowledge Graphs",,,Methodology,"entity-alignment-on-dbp15k-zh-en, entity-alignment-on-dbp15k-ja-en, entity-alignment-on-dbp15k-fr-en",,https://github.com/nju-websoft/JAPE,https://paperswithcode.com/dataset/dbp15k,"DBP15k contains four language-specific KGs that are respectively extracted from English (En), Chinese (Zh), French (Fr) and Japanese (Ja) DBpedia, each of which contains around 65k-106k entities. Three sets of 15k alignment labels are constructed to align entities between each of the other three languages and En.",,Cross-lingual Entity Alignment for Knowledge Graphs with Incidental Supervision from Free Text,https://arxiv.org/abs/2005.00171,,,
803,DBP1M_FR-EN,Entity Alignment,Entity Alignment,Entity Alignment,,,Methodology,entity-alignment-on-dbp1m-fr-en,,https://github.com/ZJU-DAILY/LargeEA,https://paperswithcode.com/dataset/dbp1m-fr-en,A large-scale cross-lingual dataset for entity alignment,,,,,,
804,DBP2.0_zh-en,Entity Alignment,Entity Alignment,Entity Alignment,,,Methodology,entity-alignment-on-dbp2-0-zh-en,GNU General Public License v3.0,https://github.com/nju-websoft/OpenEA,https://paperswithcode.com/dataset/dbp2-0-zh-en,"The DBP2.0 dataset can be downloaded from the figshare repository. It has three entity alignment settings, i.e., ZH-EN, JA-EN and FR-EN. Each setting has the following files:

ent_links: reference entity alignment;
rel_triples_1: relation triples in the ZH or JA or FR KG, list of triples like (h \t r \t t);
rel_triples_2: relation triples in the EN KG;
splits/train_links: training data for entity alignment, list of pairs like (e1 \t e2);
splits/valid_links: validation data for entity alignment;
splits/test_links: test data for entity alignment;
splits/train_unlinked_ent1: training data for dangling entity detection, list of dangling entities in the ZH or JA or FR KG;
splits/train_unlinked_ent2: training data for dangling entity detection, list of dangling entities in the EN KG;
splits/valid_unlinked_ent1: validation data for dangling entity detection, list of dangling entities in the ZH or JA or FR KG;
splits/valid_unlinked_ent2: validation data for dangling entity detection, list of dangling entities in the EN KG;
splits/test_unlinked_ent1: test data for dangling entity detection, list of dangling entities in the ZH or JA or FR KG;
splits/test_unlinked_ent2: test data for dangling entity detection, list of dangling entities in the EN KG;

More information see: https://github.com/nju-websoft/OpenEA/tree/master/dbp2.0",,,,,,
805,DBpedia,Text Retrieval,Text Retrieval,"Text Retrieval, Zero-shot Text Search, Text Classification, Open Intent Discovery","Image, Text",English,Computer Vision,"open-intent-discovery-on-dbpedia, zero-shot-text-search-on-dbpedia, text-classification-on-dbpedia, text-retrieval-on-dbpedia",CC BY-SA 3.0,https://wiki.dbpedia.org/datasets,https://paperswithcode.com/dataset/dbpedia,"DBpedia (from ""DB"" for ""database"") is a project aiming to extract structured content from the information created in the Wikipedia project. DBpedia allows users to semantically query relationships and properties of Wikipedia resources, including links to other related datasets.",,,,,,
806,DCASE_2013,Scene Classification,Scene Classification,"Scene Classification, Sound Event Detection, Acoustic Scene Classification","Audio, Image",,Computer Vision,,,http://dcase.community/challenge2013/index,https://paperswithcode.com/dataset/dcase-2013,DCASE 2013 is a dataset for sound event detection. It consists of audio-only recordings where individual sound events are prominent in an acoustic scene.,2013,,,,,
807,DCASE_2016,Scene Classification,Scene Classification,"Scene Classification, Sound Event Detection, Acoustic Scene Classification","Audio, Image",,Computer Vision,,,http://dcase.community/challenge2016/index,https://paperswithcode.com/dataset/dcase-2016,"DCASE 2016 is a dataset for sound event detection. It consists of 20 short mono sound files for each of 11 sound classes (from office environments, like clearthroat, drawer, or keyboard), each file containing one sound event instance. Sound files are annotated with event on- and offset times, however silences between actual physical sounds (like with a phone ringing) are not marked and hence “included” in the event.",2016,The NIGENS General Sound Events Database,https://arxiv.org/abs/1902.08314,,,
808,DCASE_2019_Mobile,Scene Classification,Scene Classification,"Scene Classification, Acoustic Scene Classification","Audio, Image",,Computer Vision,acoustic-scene-classification-on-dcase-2019,Other (Non-Commercial),https://zenodo.org/record/2589332,https://paperswithcode.com/dataset/dcase-2019-mobile,"TAU Urban Acoustic Scenes 2019 Mobile development dataset consists of 10-seconds audio segments from 10 acoustic scenes:

Airport
Indoor shopping mall
Metro station
Pedestrian street
Public square
Street with medium level of traffic
Travelling by a tram
Travelling by a bus
Travelling by an underground metro
Urban park

Recordings were made with three devices that captured audio simultaneously. Each acoustic scene has 1440 segments (240 minutes of audio) recorded with device A (main device) and 108 segments of parallel audio (18 minutes) each recorded with devices B and C. The dataset contains in total 46 hours of audio.

DCASE website",2019,,,,,
809,DDAD,Monocular Depth Estimation,Monocular Depth Estimation,"Monocular Depth Estimation, Depth Estimation, Self-Driving Cars",3D,,Methodology,monocular-depth-estimation-on-ddad,,https://github.com/TRI-ML/DDAD,https://paperswithcode.com/dataset/ddad,"DDAD is a new autonomous driving benchmark from TRI (Toyota Research Institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions. It contains monocular videos and accurate ground-truth depth (across a full 360 degree field of view) generated from high-density LiDARs mounted on a fleet of self-driving cars operating in a cross-continental setting. DDAD contains scenes from urban settings in the United States (San Francisco, Bay Area, Cambridge, Detroit, Ann Arbor) and Japan (Tokyo, Odaiba).",,,,,,
810,DDD17,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Motion Estimation, Visual Place Recognition, Autonomous Driving","Image, Video",,Computer Vision,semantic-segmentation-on-ddd17,CC BY-SA 4.0,https://docs.google.com/document/d/1HM0CSmjO8nOpUeTvmPjopcBcVCk7KXvLUuiZFS6TWSg/pub,https://paperswithcode.com/dataset/ddd17,"DDD17 has over 12 h of a 346x260 pixel DAVIS sensor recording highway and city driving in daytime, evening, night, dry and wet weather conditions, along with vehicle speed, GPS position, driver steering, throttle, and brake captured from the car's on-board diagnostics interface.",,DDD17: End-To-End DAVIS Driving Dataset,https://arxiv.org/pdf/1711.01458v1.pdf,,,
811,DDXPlus,Medical Diagnosis,Medical Diagnosis,Medical Diagnosis,,,Medical,,,https://github.com/bruzwen/ddxplus,https://paperswithcode.com/dataset/ddxplus,"There has been a rapidly growing interest in Automatic Symptom Detection (ASD) and Automatic Diagnosis (AD) systems in the machine learning research literature, aiming to assist doctors in telemedicine services. These systems are designed to interact with patients, collect evidence about their symptoms and relevant antecedents, and possibly make predictions about the underlying diseases. Doctors would review the interactions, including the evidence and the predictions, collect if necessary additional information from patients, before deciding on next steps. Despite recent progress in this area, an important piece of doctors' interactions with patients is missing in the design of these systems, namely the differential diagnosis. Its absence is largely due to the lack of datasets that include such information for models to train on. In this work, we present a large-scale synthetic dataset of roughly 1.3 million patients that includes a differential diagnosis, along with the ground truth pathology, symptoms and antecedents for each patient. Unlike existing datasets which only contain binary symptoms and antecedents, this dataset also contains categorical and multi-choice symptoms and antecedents useful for efficient data collection. Moreover, some symptoms are organized in a hierarchy, making it possible to design systems able to interact with patients in a logical way. As a proof-of-concept, we extend two existing AD and ASD systems to incorporate the differential diagnosis, and provide empirical evidence that using differentials as training signals is essential for the efficiency of such systems or for helping doctors better understand the reasoning of those systems.",,,,,,
812,DEAP,EEG Emotion Recognition,EEG Emotion Recognition,"EEG Emotion Recognition, Emotion Recognition, Multimodal Emotion Recognition, Emotion Classification",Image,,Computer Vision,eeg-emotion-recognition-on-deap,,https://www.eecs.qmul.ac.uk/mmv/datasets/deap/,https://paperswithcode.com/dataset/deap,"The DEAP dataset consists of two parts:


The ratings from an online self-assessment where 120 one-minute extracts of music videos were each rated by 14-16 volunteers based on arousal, valence and dominance.
The participant ratings, physiological recordings and face video of an experiment where 32 volunteers watched a subset of 40 of the above music videos. EEG and physiological signals were recorded and each participant also rated the videos as above. For 22 participants frontal face video was also recorded.",,,,,,
813,DEAP_City_Dataset,Air Pollution Prediction,Air Pollution Prediction,Air Pollution Prediction,Time Series,,Methodology,,,https://github.com/mayukh18/DEAP/,https://paperswithcode.com/dataset/deap-city-dataset,"Main Dataset
city_pollution_data.csv

Relevant Columns:


Date: Date of the sample
City: City of the sample
X_median: Median value of the pollutant/meteorological feature X for the day 
mil_miles: Total vehicle travel distance for the sample
pp_feat: Calculated feature for the influence of neighboring power plants
Population Staying at Home: Used a measure of domestic emissions.

Pollutants:
PM2.5,PM10,NO2,O3,CO,SO2

Meteorological Features:
Temperature,Pressure,Humidity,Dew,Wind Speed,Wind Gust

Power Plant Generation and Location Dataset [Extra]:
pp_gen_data.csv

Relevant Columns:


Month: Month of the data
Netgen: Net generation for that month.

If you find the data or code useful in your work, please cite
@inproceedings{ijcai2022p698,
  title     = {Deciphering Environmental Air Pollution with Large Scale City Data},
  author    = {Bhattacharyya, Mayukh and Nag, Sayan and Ghosh, Udita},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  year      = {2022},
}",2022,,,,,
814,DebateSum,Query-Based Extractive Summarization,Query-Based Extractive Summarization,"Query-Based Extractive Summarization, Argument Mining, Extractive Text Summarization",Text,English,Natural Language Processing,extractive-document-summarization-on,MIT,https://github.com/Hellisotherpeople/DebateSum,https://paperswithcode.com/dataset/debatesum,"DebateSum consists of 187328 debate documents, arguments (also can be thought of as abstractive summaries, or queries), word-level extractive summaries, citations, and associated metadata organized by topic-year. This data is ready for analysis by NLP systems.",,,,,,
815,Deblur-NeRF,Deblurring,Deblurring,"Deblurring, Novel View Synthesis",,,Methodology,,MIT,https://limacv.github.io/deblurnerf/,https://paperswithcode.com/dataset/deblur-nerf,"This dataset focus on two blur types: camera motion blur and defocus blur. For each type of blur we synthesize $5$ scenes using Blender. We manually place multi-view cameras to mimic real data capture. To render images with camera motion blur, we randomly perturb the camera pose, and then linearly interpolate poses between the original and perturbed poses for each view. We render images from interpolated poses and blend them in linear RGB space to generate the final blurry images. For defocus blur, we use the built-in functionality to render depth-of-field images. We fix the aperture and randomly choose a focus plane between the nearest and furthest depth.

We also captured $20$ real world scenes with $10$ scenes for each blur type for a qualitative study. The camera used was a Canon EOS RP with manual exposure mode. We captured the camera motion blur images by manually shaking the camera during exposure, while the reference images are taken using a tripod. To capture defocus images, we choose a large aperture. We compute the camera poses of blurry and reference images in the real world scenes using the COLMAP.",,,,,,
816,DECADE,Egocentric Activity Recognition,Egocentric Activity Recognition,"Egocentric Activity Recognition, Temporal Action Localization","Image, Time Series, Video",,Computer Vision,,,https://github.com/ehsanik/dogTorch,https://paperswithcode.com/dataset/decade,DECADE is a large-scale dataset of ego-centric videos from a dog's perspective as well as her corresponding movements.,,,,,,
817,DEEP-VOICE__DeepFake_Voice_Recognition,DeepFake Detection,DeepFake Detection,"DeepFake Detection, Audio Classification, Speech Recognition, Synthetic Speech Detection","Audio, Image, Text",English,Computer Vision,audio-classification-on-deep-voice-deepfake,MIT,https://www.kaggle.com/datasets/birdy654/deep-voice-deepfake-voice-recognition,https://paperswithcode.com/dataset/deep-voice-deepfake-voice-recognition,"DEEP-VOICE: Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion
This dataset contains examples of real human speech, and DeepFake versions of those speeches by using Retrieval-based Voice Conversion. 

Can machine learning be used to detect when speech is AI-generated?

Introduction
There are growing implications surrounding generative AI in the speech domain that enable voice cloning and real-time voice conversion from one individual to another. This technology poses a significant ethical threat and could lead to breaches of privacy and misrepresentation, thus there is an urgent need for real-time detection of AI-generated speech for DeepFake Voice Conversion. 

To address the above emerging issues, we are introducing the DEEP-VOICE dataset. DEEP-VOICE is comprised of real human speech from eight well-known figures and their speech converted to one another using Retrieval-based Voice Conversion.

For each speech, the accompaniment (""background noise"") was removed before conversion using RVC. The original accompaniment is then added back to the DeepFake speech:



(Above: Overview of the Retrieval-based Voice Conversion process to generate DeepFake speech with Ryan Gosling's speech converted to Margot Robbie. Conversion is run on the extracted vocals before being layered on the original background ambience.)

Dataset
There are two forms to the dataset that are made available. 

First, the raw audio can be found in the ""AUDIO"" directory. They are arranged within ""REAL"" and ""FAKE"" class directories. The audio filenames note which speakers provided the real speech, and which voices they were converted to. For example ""Obama-to-Biden"" denotes that Barack Obama's speech has been converted to Joe Biden's voice. 

Second, the extracted features can be found in the ""DATASET-balanced.csv"" file. This is the data that was used in the below study. The dataset has each feature extracted from one-second windows of audio and are balanced through random sampling.

Note:  All experimental data is found within the ""KAGGLE"" directory. The ""DEMONSTRATION"" directory is used for playing cropped and compressed demos in notebooks due to Kaggle's limitations on file size.

A potential use of a successful system could be used for the following: 



(Above: Usage of the real-time system. The end user is notified when the machine learning model has processed the speech audio (e.g. a phone or conference call) and predicted that audio chunks contain AI-generated speech.)

Kaggle
The dataset is available on the Kaggle data science platform.

The Kaggle page can be found by clicking here: Dataset on Kaggle

Attribution
This dataset was produced from the study ""Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion""

The preprint can be found on ArXiv by clicking here: Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion

License
This dataset is provided under the MIT License:

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",,Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion,https://arxiv.org/abs/2308.12734,,,
818,DeepCom-Java,Source Code Summarization,Source Code Summarization,"Source Code Summarization, Code Summarization, Code Documentation Generation",Text,English,Natural Language Processing,source-code-summarization-on-deepcom-java,,https://dl.acm.org/doi/10.1145/3196321.3196334,https://paperswithcode.com/dataset/deepcom-java,"The Java dataset introduced in DeepCom (Deep Code Comment Generation), commonly used to evaluate automated code summarization.",,,,,,
819,DeeperForensics-1.0,Video Forensics,Video Forensics,"Video Forensics, DeepFake Detection, Face Swapping","Image, Video",,Computer Vision,,,https://github.com/EndlessSora/DeeperForensics-1.0/tree/master/dataset,https://paperswithcode.com/dataset/deeperforensics-1-0,"DeeperForensics-1.0 represents the largest face forgery detection dataset by far, with 60,000 videos constituted by a total of 17.6 million frames, 10 times larger than existing datasets of the same kind. The full dataset includes 48,475 source videos and 11,000 manipulated videos. The source videos are collected on 100 paid and consented actors from 26 countries, and the manipulated videos are generated by a newly proposed many-to-many end-to-end face swapping method, DF-VAE. 7 types of real-world perturbations at 5 intensity levels are employed to ensure a larger scale and higher diversity.",,,,,,
820,DeepFashion,Unsupervised Human Pose Estimation,Unsupervised Human Pose Estimation,"Unsupervised Human Pose Estimation, Image-to-Image Translation, Virtual Try-on, Image Retrieval, Text-to-3D-Human Generation, Pose Transfer","3D, Image, Text",English,Computer Vision,"virtual-try-on-on-deep-fashion, image-to-image-translation-on-deep-fashion-1, pose-transfer-on-deep-fashion, image-retrieval-on-deepfashion, text-to-3d-human-generation-on-deepfashion, unsupervised-human-pose-estimation-on","Custom (research-only, non-commercial, attribution)",https://liuziwei7.github.io/projects/DeepFashion.html,https://paperswithcode.com/dataset/deepfashion,"DeepFashion is a dataset containing around 800K diverse fashion images with their rich annotations (46 categories, 1,000 descriptive attributes, bounding boxes and landmark information) ranging from well-posed product images to real-world-like consumer photos.",,A Benchmark for Inpainting of Clothing Images with Irregular Holes,https://arxiv.org/abs/2007.05080,,,46
821,DeepFix,Program Repair,Program Repair,Program Repair,,,Methodology,program-repair-on-deepfix,Apache-2.0,https://bitbucket.org/iiscseal/deepfix,https://paperswithcode.com/dataset/deepfix,DeepFix consists of a program repair dataset (fix compiler errors in C programs). It enables research around automatically fixing programming errors using deep learning.,,,,,,
822,DeepMind_Control_Suite,Continuous Control,Continuous Control,"Continuous Control, Continuous Control (100k environment steps), Continuous Control (500k environment steps)",,,Methodology,"continuous-control-500k-environment-steps-on-4, continuous-control-on-deepmind-cheetah-run, continuous-control-100k-environment-steps-on-3, continuous-control-500k-environment-steps-on-1, continuous-control-100k-environment-steps-on-4, continuous-control-on-deepmind-cup-catch, continuous-control-500k-environment-steps-on, continuous-control-100k-environment-steps-on-1, continuous-control-100k-environment-steps-on, continuous-control-on-deepmind-walker-walk, continuous-control-500k-environment-steps-on-3",Apache-2.0,https://github.com/deepmind/dm_control,https://paperswithcode.com/dataset/deepmind-control-suite,"The DeepMind Control Suite (DMCS) is a set of simulated continuous control environments with a standardized structure and interpretable rewards. The tasks are written and powered by the MuJoCo physics engine, making them easy to identify. Control Suite tasks include Pendulum, Acrobot, Cart-pole, Cart-k-pole, Ball in cup, Point-mass, Reacher, Finger, Hooper, Fish, Cheetah, Walker, Manipulator, Manipulator extra, Stacker, Swimmer, Humanoid, Humanoid_CMU and LQR.",,Unsupervised Learning of Object Structure and Dynamics from Videos,https://arxiv.org/abs/1906.07889,,,
823,DeepNets-1M,Parameter Prediction,Parameter Prediction,Parameter Prediction,Time Series,,Methodology,,MIT,https://github.com/facebookresearch/ppuda,https://paperswithcode.com/dataset/deepnets-1m,"The DeepNets-1M dataset is composed of neural network architectures represented as graphs where nodes are operations (convolution, pooling, etc.) and edges correspond to the forward pass flow of data through the network.
DeepNets-1M has 1 million training architectures and 1402 in-distribution (ID) and out-of-distribution (OOD) evaluation architectures: 
500 validation and 500 testing ID architectures, 
100 wide OOD architectures, 
100 deep OOD architectures, 
100 dense OOD architectures, 
100 OOD archtectures without batch normalization, and 
2 predefined architectures (ResNet-50 and 12 layer Visual Transformer).

For 1402 evaluation architectures, DeepNets-1M includes accuracies of the networks on CIFAR-10 and ImageNet after training them with stochastic gradient descent (SGD).
Besides accuracy, other properties of evaluation architectures are included: accuracy on noisy images, inference and convergence time. These properties of architectures can enable training neural architecture search models.

The DeepNets-1M is used to train and evaluate parameter prediction models such as Graph HyperNetworks. These models can predict all parameters for a given network (graph) in a single forward pass and the results can be compared to optimizing parameters with SGD.",,,,,,
824,DeepSport_Dataset,2D Human Pose Estimation,2D Human Pose Estimation,"2D Human Pose Estimation, 2D Semantic Segmentation","3D, Image",,Computer Vision,,,https://www.kaggle.com/gabrielvanzandycke/deepsport-dataset,https://paperswithcode.com/dataset/deepsport-dataset,"This basketball dataset was acquired under the Walloon region project DeepSport, using the Keemotion system installed in multiple arenas.
We would like to thanks both Keemotion for letting us use their system for raw image acquisition during live productions, and the LNB for the rights on their images.",,,,,,
825,DeepWeeds,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Robust classification, Active Learning",Image,,Computer Vision,,,https://github.com/AlexOlsen/DeepWeeds,https://paperswithcode.com/dataset/deepweeds,"The DeepWeeds dataset consists of 17,509 images capturing eight different weed species native to Australia in situ with neighbouring flora.",,,,509 images,,
826,DeepWriting,Style Transfer,Style Transfer,"Style Transfer, Handwriting generation, Handwriting Recognition","Image, Text",English,Computer Vision,,,https://ait.ethz.ch/projects/2018/deepwriting/,https://paperswithcode.com/dataset/deepwriting,A new dataset of handwritten text with fine-grained annotations at the character level and report results from an initial user evaluation.,,,,,,
827,Deep_Fakes_Dataset,Dimensionality Reduction,Dimensionality Reduction,"Dimensionality Reduction, Video Compression, Video Generation","Text, Video",English,Natural Language Processing,,,http://cs.binghamton.edu/~ncilsal2/DeepFakesDataset/,https://paperswithcode.com/dataset/deep-fakes-dataset,"The Deep Fakes Dataset is a collection of ""in the wild"" portrait videos for deepfake detection. The videos in the dataset are diverse real-world samples in terms of the source generative model, resolution, compression, illumination, aspect-ratio, frame rate, motion, pose, cosmetics, occlusion, content, and context. They originate from various sources such as news articles, forums, apps, and research presentations; totalling up to 142 videos, 32 minutes, and 17 GBs. Synthetic videos are matched with their original counterparts when possible.",,,,,,
828,Deep_Sea_Treasure_Pareto-Front,Multiobjective Optimization,Multiobjective Optimization,Multiobjective Optimization,,,Methodology,,Mozilla Public License (MPL) version 2.0,https://github.com/imec-idlab/deep-sea-treasure,https://paperswithcode.com/dataset/deep-sea-treasure-pareto-front,"The dataset contains two Pareto-fronts:
- The Pareto-front for the 2-objective problem
- The Pareto-front for the 3-objective problem

Each Pareto-front contains a set of points, with coordinates given by their objectives. The dataset also contains 1 possible action sequence that leads to this point. If there are multiple possible paths leading to the same point, only 1 was kept.",,,,,,
829,Deezer-Europe,Node Classification on Non-Homophilic (Heterophilic) Graphs,Node Classification on Non-Homophilic (Heterophilic) Graphs,Node Classification on Non-Homophilic (Heterophilic) Graphs,Image,,Graphs,node-classification-on-non-homophilic-6,,,https://paperswithcode.com/dataset/deezer-europe-1,Node classification on Deezer Europe with 50%/25%/25% random splits for training/validation/test.,,,,,,
830,Defects4J,Program Repair,Program Repair,"Program Repair, Code Completion",,,Methodology,code-completion-on-defects4j,MIT,https://github.com/rjust/defects4j,https://paperswithcode.com/dataset/defects4j,"Defects4J is a collection of reproducible bugs and a supporting infrastructure with the goal of advancing software engineering research.

Defects4J contains 835 bugs (plus 29 deprecated bugs) from the following open-source projects:

| Identifier      | Project name               | Number of active bugs | Active bug ids      | Deprecated bug ids (*) |
|-----------------|----------------------------|----------------------:|---------------------|-------------------------| 
| Chart           | jfreechart                 |           26          | 1-26                | None                    |
| Cli             | commons-cli                |           39          | 1-5,7-40            | 6                       |
| Closure         | closure-compiler           |          174          | 1-62,64-92,94-176   | 63,93                   |
| Codec           | commons-codec              |           18          | 1-18                | None                    |
| Collections     | commons-collections        |            4          | 25-28               | 1-24                    |
| Compress        | commons-compress           |           47          | 1-47                | None                    |
| Csv             | commons-csv                |           16          | 1-16                | None                    |
| Gson            | gson                       |           18          | 1-18                | None                    |
| JacksonCore     | jackson-core               |           26          | 1-26                | None                    |
| JacksonDatabind | jackson-databind           |          112          | 1-112               | None                    |
| JacksonXml      | jackson-dataformat-xml     |            6          | 1-6                 | None                    |
| Jsoup           | jsoup                      |           93          | 1-93                | None                    |
| JxPath          | commons-jxpath             |           22          | 1-22                | None                    |
| Lang            | commons-lang               |           64          | 1,3-65              | 2                       |
| Math            | commons-math               |          106          | 1-106               | None                    |
| Mockito         | mockito                    |           38          | 1-38                | None                    |
| Time            | joda-time                  |           26          | 1-20,22-27          | 21                      |",,,,,,
831,Deformable_Linear_Objects___DLOs__Dataset,Deformable Object Manipulation,Deformable Object Manipulation,Deformable Object Manipulation,,,Methodology,,CC BY-NC-SA,https://roahmlab.github.io/DEFORM/,https://paperswithcode.com/dataset/deformable-linear-objects-dlos-dataset,"For each DLO, we collect 350 seconds of dynamic trajectory data in the real-world using the motion capture system at a frequency of 100 Hz.",,,,,,
832,DeformPAM-Dataset,Deformable Object Manipulation,Deformable Object Manipulation,"Deformable Object Manipulation, Imitation Learning",,,Methodology,,MIT,https://huggingface.co/datasets/WendiChen/DeformPAM,https://paperswithcode.com/dataset/deformpam-dataset,"Two versions of the dataset are offered: one is the full dataset used to train the models in DeformPAM, and the other is a mini dataset for easier examination. Both datasets include data for the supervised and finetuning stages of granular pile shaping, rope shaping, and T-shirt unfolding.",,,,,,
833,DEFT_Corpus,Relation Extraction,Relation Extraction,"Relation Extraction, Sentence Classification, Definition Extraction","Graph, Image",,Computer Vision,,,https://github.com/adobe-research/deft_corpus,https://paperswithcode.com/dataset/deft-corpus,A SemEval shared task in which participants must extract definitions from free text using a term-definition pair corpus that reflects the complex reality of definitions in natural language.,,,,,,
834,Def_Armored_parallel,SMAC+,SMAC+,SMAC+,,,Methodology,smac-on-smac-def-armored-parallel,,,https://paperswithcode.com/dataset/smac-def-armored-parallel,smac+ defense armored scenario with parallel episodic buffer,,,,,,
835,Def_Armored_sequential,SMAC+,SMAC+,SMAC+,,,Methodology,smac-on-smac-def-armored-sequential,,,https://paperswithcode.com/dataset/smac-def-armored-sequential,SMAC+ defensive armored scenario with sequential episodic buffer,,,,,,
836,Def_Infantry_parallel,SMAC+,SMAC+,SMAC+,,,Methodology,smac-on-smac-def-infantry-parallel,Apache-2.0,,https://paperswithcode.com/dataset/smac-def-infantry-parallel,smac+ defense infantry scenario with parallel episodic buffer,,,,,,
837,Def_Infantry_sequential,SMAC+,SMAC+,SMAC+,,,Methodology,smac-on-smac-def-infantry-sequential,,,https://paperswithcode.com/dataset/smac-def-infantry-sequential,SMAC+ defensive infantry scenario with sequential episodic buffer,,,,,,
838,Def_Outnumbered_parallel,SMAC+,SMAC+,SMAC+,,,Methodology,smac-on-smac-def-outnumbered-parallel,,,https://paperswithcode.com/dataset/smac-def-outnumbered-parallel,smac+ defense outnumbered scenario with parallel episodic buffer,,,,,,
839,Def_Outnumbered_sequential,SMAC+,SMAC+,SMAC+,,,Methodology,smac-on-smac-def-outnumbered-sequential,,,https://paperswithcode.com/dataset/smac-def-outnumbered-sequential,SMAC+ defensive outnumbered scenario with sequential episodic buffer,,,,,,
840,Demetr,Translation,Translation,"Translation, Machine Translation",Text,English,Natural Language Processing,,MIT,https://github.com/marzenakrp/demetr,https://paperswithcode.com/dataset/demetr,"Demetr is a diagnostic dataset with 31K English examples (translated from 10 source languages) for evaluating the sensitivity of MT evaluation metrics to 35 different linguistic perturbations spanning semantic, syntactic, and morphological error categories.",,DEMETR: Diagnosing Evaluation Metrics for Translation,https://arxiv.org/pdf/2210.13746v1.pdf,,,
841,Demosthenes,Argument Mining,Argument Mining,Argument Mining,,,Methodology,,CC BY-NC 4.0,https://github.com/adele-project/demosthenes,https://paperswithcode.com/dataset/demosthenes,"Corpus for argument mining in legal documents, composed of 40 decisions of the Court of Justice of the European Union on matters of fiscal state aid",,,,,,
842,DENSE,Image Dehazing,Image Dehazing,"Image Dehazing, Probabilistic Deep Learning, Depth Estimation","3D, Image",,Computer Vision,image-dehazing-on-dense-haze,,https://github.com/uzh-rpg/rpg_e2depth,https://paperswithcode.com/dataset/dense,DENSE (Depth Estimation oN Synthetic Events) is a new dataset with synthetic events and perfect ground truth.,,Learning Monocular Dense Depth from Events,http://rpg.ifi.uzh.ch/docs/3DV20_Hidalgo.pdf,,,
843,DensePose,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Pose Estimation, Image Generation","3D, Image, Text",English,Computer Vision,,CC BY-NC 2.0,http://densepose.org/,https://paperswithcode.com/dataset/densepose,"DensePose-COCO is a large-scale ground-truth dataset with image-to-surface correspondences manually annotated
on 50K COCO images and train DensePose-RCNN, to densely regress part-specific UV coordinates within every human
region at multiple frames per second.",,DensePose: Dense Human Pose Estimation In The Wildc,https://arxiv.org/pdf/1802.00434v1.pdf,,,
844,Dense_Fog,Object Detection,Object Detection,"Object Detection, 2D Object Detection, 3D Object Detection","3D, Image",,Computer Vision,"2d-object-detection-on-dense-fog, 3d-object-detection-on-stf",Custom,https://www.uni-ulm.de/en/in/driveu/projects/dense-datasets#c811669,https://paperswithcode.com/dataset/stf,"We introduce an object detection dataset in challenging adverse weather conditions covering 12000 samples in real-world driving scenes and 1500 samples in controlled weather conditions within a fog chamber. The dataset includes different weather conditions like fog, snow, and rain and was acquired by over 10,000 km of driving in northern Europe. The driven route with cities along the road is shown on the right. In total, 100k Objekts were labeled with accurate 2D and 3D bounding boxes. The main contributions of this dataset are:
- We provide a proving ground for a broad range of algorithms covering signal enhancement, domain adaptation, object detection, or multi-modal sensor fusion, focusing on the learning of robust redundancies between sensors, especially if they fail asymmetrically in different weather conditions.
- The dataset was created with the initial intention to showcase methods, which learn of robust redundancies between the sensor and enable a raw data sensor fusion in case of asymmetric sensor failure induced through adverse weather effects.
- In our case we departed from proposal level fusion and applied an adaptive fusion driven by measurement entropy enabling the detection also in case of unknown adverse weather effects. This method outperforms other reference fusion methods, which even drop in below single image methods.
- Please check out our paper for more information.",,,,12000 samples,,
845,Derisi_Funcat,Hierarchical Multi-label Classification,Hierarchical Multi-label Classification,Hierarchical Multi-label Classification,Image,,Computer Vision,hierarchical-multi-label-classification-on-1,,https://dtai.cs.kuleuven.be/clus/hmcdatasets/,https://paperswithcode.com/dataset/derisi-funcat,Hierarchical-multilabel classification dataset for functional genomics,,,,,,
846,Description_Detection_Dataset,Object Detection,Object Detection,"Object Detection, Referring Expression Comprehension, Described Object Detection, Open Vocabulary Object Detection",Image,,Computer Vision,described-object-detection-on-description,Creative Commons Attribution 4.0 International,https://github.com/shikras/d-cube,https://paperswithcode.com/dataset/description-detection-dataset,"Description Detection Dataset ($D^3$, /dikju:b/) is an attempt at creating a next-generation object detection dataset. Unlike traditional detection datasets, the class names of the objects are no longer simple nouns or noun phrases, but rather complex and descriptive, such as a dog not being held by a leash. For each image in the dataset, any object that matches the description is annotated. The dataset provides annotations such as bounding boxes and finely crafted instance masks.It comprises of 422 well-designed descriptions and 24,282 positive object-description pairs.

The dataset is meant for the Described Object Detection (DOD) task. OVD detects object based on category name, and each category can have zero to multiple instances; REC grounds one region based on a language description, whether the object truly exits or not; DOD detects all instances on each image in the dataset, based on a flexible reference.",,,,,,
847,DESED,Sound Event Detection,Sound Event Detection,Sound Event Detection,"Audio, Image",,Computer Vision,sound-event-detection-on-desed,,https://project.inria.fr/desed/,https://paperswithcode.com/dataset/desed,"The DESED dataset is a dataset designed to recognize sound event classes in domestic environments. The dataset is designed to be used for sound event detection (SED, recognize events with their time boundaries) but it can also be used for sound event tagging (SET, indicate presence of an event in an audio file).
The dataset is composed of 10 event classes to recognize in 10 second audio files. The classes are: Alarm/bell/ringing, Blender, Cat, Dog, Dishes,
Electric shaver/toothbrush, Frying, Running water, Speech, Vacuum cleaner.",,,,,,
848,DevAI,AI Agent,AI Agent,AI Agent,,,Methodology,,,https://huggingface.co/datasets/DEVAI-benchmark,https://paperswithcode.com/dataset/devai,"DEVAI is a benchmark of 55 realistic AI development tasks. It consists of plentiful manual annotations, including a total of 365 hierarchical user requirements. This dataset enables rich reinforcement signals for better automated AI software development.",,,,,,
849,Dex-Net_2.0,Robotic Grasping,Robotic Grasping,Robotic Grasping,,,Methodology,,,http://berkeleyautomation.github.io/dex-net/,https://paperswithcode.com/dataset/dex-net-2-0,"Dex-Net 2.0 is a dataset associating 6.7 million point clouds and analytic grasp quality metrics with parallel-jaw grasps planned using robust quasi-static GWS analysis on a dataset of 1,500 3D object models.",,,,,,
850,DexYCB,3D Hand Pose Estimation,3D Hand Pose Estimation,"3D Hand Pose Estimation, hand-object pose","3D, Image",,Computer Vision,"3d-hand-pose-estimation-on-dexycb, hand-object-pose-on-dexycb",CC BY-NC 4.0,https://dex-ycb.github.io,https://paperswithcode.com/dataset/dexycb,"DexYCB is a dataset for capturing hand grasping of objects. It can be used three relevant tasks: 2D object and keypoint detection, 6D object pose estimation, and 3D hand pose estimation. 

The dataset was built using 20 objects from the YCB-Video dataset, and consists of multiple trials from 10 subjects. For each trial, there is a target object with 2 to 4 other objects placed on a table. The subject is asked to start from a relaxed pose, pick up the target object, and hold it in the air. Some subjects were asked to pretend to hand over the object to someone across from them. Each action is recorded for 3 seconds, repeating the trial 5 times for each target object, each time with a random set of accompanied objects and placement. In total there are 100 trials per subject, and 1,000 trials in total for all subjects.",,,,,,
851,DF40,Fake Image Detection,Fake Image Detection,"Fake Image Detection, DeepFake Detection",Image,,Computer Vision,,,https://yzy-stack.github.io/homepage_for_df40/,https://paperswithcode.com/dataset/df40,"Forgery Diversity: DF40 comprises 40 distinct deepfake techniques (both representive and SOTA methods are included), facilitating the detection of nowadays' SOTA deepfakes and AIGCs. We provide 10 face-swapping methods, 13 face-reenactment methods, 12 entire face synthesis methods, and 5 face editing.

Forgery Realism: DF40 includes realistic deepfake data created by highly popular generation software and methods, e.g., HeyGen, MidJourney, DeepFaceLab, to simulate real-world deepfakes. We even include the just-released DiT, SiT, PixArt-$\alpha$, etc.

Forgery Scale: DF40 offers million-level deepfake data scale for both images and videos.

Data Alignment: DF40 provides alignment between fake methods and data domains. Most methods (31) are generated under the FF++ and CDF domains. Using our fake data, you can further expand your evaluation (training on FF++ and testing on CDF).",,,,,,
852,DFDC,Misinformation,Misinformation,"Misinformation, DeepFake Detection, Face Swapping",Image,,Computer Vision,deepfake-detection-on-dfdc,Custom,https://deepfakedetectionchallenge.ai/,https://paperswithcode.com/dataset/dfdc,"The DFDC (Deepfake Detection Challenge) is a dataset for deepface detection consisting of more than 100,000 videos.

The DFDC dataset consists of two versions:


Preview dataset. with 5k videos. Featuring two facial modification algorithms.
Full dataset, with 124k videos. Featuring eight facial modification algorithms",,,,,,
853,DFDM,Face Generation,Face Generation,"Face Generation, DeepFake Detection, Face Swapping","Image, Text",English,Computer Vision,,,https://github.com/shanface33/Deepfake_Model_Attribution,https://paperswithcode.com/dataset/dfdm,"We created a new dataset, named DFDM, with 6,450 Deepfake videos generated by different Autoencoder models. Specifically, five Autoencoder models with variations in encoder, decoder, intermediate layer, and input resolution, respectively, have been selected to generate Deepfakes based on the same input. We have  observed the visible but subtle visual differences among different Deepfakes, demonstrating the evidence of model attribution artifacts.",,,,,,
854,DGraphFin__TGN_Style_,Dynamic Link Prediction,Dynamic Link Prediction,Dynamic Link Prediction,Time Series,,Methodology,,CC BY-NC-SA 4.0,https://www.kaggle.com/datasets/chenxi1228/dgraphfin-tgn-style,https://paperswithcode.com/dataset/dgraphfin-tgn-style,DGraphFin dataset which is pre-processed in TGN Style.,,,,,,
855,Dhaka_Stock_Exchange_Historical_Data,Stock Market Prediction,Stock Market Prediction,"Stock Market Prediction, Stock Prediction",Time Series,,Methodology,,CC BY 4.0,https://data.mendeley.com/datasets/23553sm4tn,https://paperswithcode.com/dataset/dhaka-stock-exchange-historical-data,"The dataset contains historical technical data of Dhaka Stock Exchange (DSE). The data was collected from different sources found in the internet where the data was publicly available. The data available here are used for information and research purposes and though to the best of our knowledge, it does not contain any mistakes, there might still be some mistakes. It is not encourages to use this dataset for portfolio management purposes and use this dataset out of your own interest. The contributors do not  hold any liability if it is used for any purposes.",,,,,,
856,DHF1K,Video Saliency Detection,Video Saliency Detection,"Video Saliency Detection, Video Saliency Prediction","Image, Time Series, Video",,Computer Vision,video-saliency-detection-on-dhf1k,Attribution 4.0 International,https://github.com/wenguanwang/DHF1K,https://paperswithcode.com/dataset/dhf1k,"DHF1K is a video saliency dataset which contains a ground-truth map of binary pixel-wise gaze fixation points and a continuous map of the fixation points after being blurred by a gaussian filter. DHF1K contains 1000 videos in total. 700 of the videos are annotated, 600 of which are used for training and 100 for validation. The remaining 300 are the testing set which are to be evaluated on a public server.",,ViP: Video Platform for PyTorch,https://arxiv.org/abs/1910.02793,,,
857,DiaASQ,Conversational Sentiment Quadruple Extraction,Conversational Sentiment Quadruple Extraction,"Conversational Sentiment Quadruple Extraction, Aspect-Based Sentiment Analysis (ABSA), Dialogue Understanding, Aspect-Category-Opinion-Sentiment Quadruple Extraction",Text,English,Natural Language Processing,"conversational-sentiment-quadruple-extraction, conversational-sentiment-quadruple-extraction-1",MIT,https://diaasq-page.pages.dev/,https://paperswithcode.com/dataset/diaasq,"DiaASQ is a fine-grained Aspect-based Sentiment Analysis (ABSA) benchmark under the conversation scenario. It challenges existing ABSA methods by 1) extracting quadruple of target-aspect-opinion-sentiment in a dialogue, and 2) modeling the dialogue discourse structures. The dataset is constructed by systematically crawling tweets from digital bloggers, followed by a series of preprocessing steps including filtering, normalizing, pruning, and annotating the collected dialogues, resulting in a final corpus of 1,000 dialogues. To enhance the multilingual usability, DiaASQ has both the English and Chinese versions of languages.",,,,,,
858,Diabetes,Feature Importance,Feature Importance,"Feature Importance, Diabetes Prediction, Tabular Data Generation","Tabular, Text, Time Series",English,Natural Language Processing,"diabetes-prediction-on-diabetes, feature-importance-on-diabetes, tabular-data-generation-on-diabetes",CC BY 4.0,https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008,https://paperswithcode.com/dataset/diabetes,"What do the instances in this dataset represent?

The instances represent hospitalized patient records diagnosed with diabetes.

Are there recommended data splits?

No recommendation. The standard train-test split could be used. Can use three-way holdout split (i.e., train-validation-test) when doing model selection.

Does the dataset contain data that might be considered sensitive in any way?

Yes. The dataset contains information about the age, gender, and race of the patients.

Additional Information

The dataset represents ten years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. It includes over 50 features representing patient and hospital outcomes. Information was extracted from the database for encounters that satisfied the following criteria.
(1) It is an inpatient encounter (a hospital admission).
(2) It is a diabetic encounter, that is, one during which any kind of diabetes was entered into the system as a diagnosis.
(3) The length of stay was at least 1 day and at most 14 days.
(4) Laboratory tests were performed during the encounter.
(5) Medications were administered during the encounter.

The data contains such attributes as patient number, race, gender, age, admission type, time in hospital, medical specialty of admitting physician, number of lab tests performed, HbA1c test result, diagnosis, number of medications, diabetic medications, number of outpatient, inpatient, and emergency visits in the year before the hospitalization, etc.

Has Missing Values?

Yes",1999,,,,,
859,Diabetic_Retinopathy_Detection_Dataset,Diabetic Retinopathy Detection,Diabetic Retinopathy Detection,Diabetic Retinopathy Detection,Image,,Computer Vision,,,https://www.kaggle.com/c/diabetic-retinopathy-detection/data,https://paperswithcode.com/dataset/diabetic-retinopathy-detection-dataset,A large scale of retina image dataset.,,,,,,
860,DiagSet,Histopathological Image Classification,Histopathological Image Classification,"Histopathological Image Classification, Image Classification",Image,,Computer Vision,,,https://ai-econsilio.diag.pl/,https://paperswithcode.com/dataset/diagset,"DiagSet is a histopathological dataset for prostate cancer detection. The proposed dataset consists of over 2.6 million tissue patches extracted from 430 fully annotated scans, 4675 scans with assigned binary diagnosis, and 46 scans with diagnosis given independently by a group of histopathologists.",,,,,,
861,DialoGLUE,Dialogue Management,Dialogue Management,"Dialogue Management, Natural Language Understanding, Active Learning",Text,English,Natural Language Processing,"natural-language-understanding-on-dialoglue-1, natural-language-understanding-on-dialoglue",,https://github.com/alexa/DialoGLUE/,https://paperswithcode.com/dataset/dialoglue,"DialoGLUE is a natural language understanding benchmark for task-oriented dialogue designed to encourage dialogue research in representation-based transfer, domain adaptation, and sample-efficient task learning. It consisting of 7 task-oriented dialogue datasets covering 4 distinct natural language understanding tasks.",,,,,,
862,DialogSum,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Text Summarization, Dialogue Generation",Text,English,Natural Language Processing,text-summarization-on-dialogsum,MIT,https://github.com/cylnlp/DialogSum,https://paperswithcode.com/dataset/dialogsum,"DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 dialogues with corresponding manually labeled summaries and topics.

This work is accepted by ACL findings 2021. You may find the paper here: https://arxiv.org/pdf/2105.06762.pdf.

If you want to use our dataset, please cite our paper. 

Dialogue Data
We collect dialogue data for DialogSum from three public dialogue corpora, namely Dailydialog (Li et al., 2017), DREAM (Sun et al., 2019) and MuTual (Cui et al., 2019), as well as an English speaking practice website. 
These datasets contain face-to-face spoken dialogues that cover a wide range of daily-life topics, including schooling, work, medication, shopping, leisure, travel.
Most conversations take place between friends, colleagues, and between service providers and customers.

Compared with previous datasets, dialogues from DialogSum have distinct characteristics: 
* Under rich real-life scenarios, including more diverse task-oriented scenarios;
* Have clear communication patterns and intents, which is valuable to serve as summarization sources;
* Have a reasonable length, which comforts the purpose of automatic summarization.

Summaries
We ask annotators to summarize each dialogue based on the following criteria:
* Convey the most salient information;
* Be brief;
* Preserve important named entities within the conversation;
* Be written from an observer perspective;
* Be written in formal language.

Topics
In addition to summaries, we also ask annotators to write a short topic for each dialogue, which can be potentially useful for future work, e.g. generating summaries by leveraging topic information.",2021,https://arxiv.org/pdf/2105.06762.pdf,https://arxiv.org/pdf/2105.06762.pdf,,,
863,Dialogue_State_Tracking_Challenge,Dialogue State Tracking,Dialogue State Tracking,"Dialogue State Tracking, Intent Detection, Spoken Language Understanding, Slot Filling, domain classification, Deblurring, Spoken Dialogue Systems","Image, Text, Video",English,Computer Vision,"deblurring-on-second-dialogue-state-tracking, domain-classification-on-dialogue-state, intent-detection-on-dialogue-state-tracking, dialogue-state-tracking-on-second-dialogue, slot-filling-on-dialogue-state-tracking",,https://github.com/matthen/dstc,https://paperswithcode.com/dataset/dialogue-state-tracking-challenge,"The Dialog State Tracking Challenges 2 & 3 (DSTC2&3) were research challenge focused on improving the state of the art in tracking the state of spoken dialog systems. State tracking, sometimes called belief tracking, refers to accurately estimating the user's goal as a dialog progresses. Accurate state tracking is desirable because it provides robustness to errors in speech recognition, and helps reduce ambiguity inherent in language within a temporal process like dialog.
In these challenges, participants were given labelled corpora of dialogs to develop state tracking algorithms. The trackers were then evaluated on a common set of held-out dialogs, which were released, un-labelled, during a one week period.

The corpus was collected using Amazon Mechanical Turk, and consists of dialogs in two domains: restaurant information, and tourist information. Tourist information subsumes restaurant information, and includes bars, cafés etc. as well as multiple new slots. There were two rounds of evaluation using this data:

DSTC 2 released a large number of training dialogs related to restaurant search. Compared to DSTC (which was in the bus timetables domain), DSTC 2 introduces changing user goals, tracking 'requested slots' as well as the new restaurants domain. Results from DSTC 2 were presented at SIGDIAL 2014.
DSTC 3 addressed the problem of adaption to a new domain - tourist information. DSTC 3 releases a small amount of labelled data in the tourist information domain; participants will use this data plus the restaurant data from DSTC 2 for training.
Dialogs used for training are fully labelled; user transcriptions, user dialog-act semantics and dialog state are all annotated. (This corpus therefore is also suitable for studies in Spoken Language Understanding.)",2014,https://www.aclweb.org/anthology/W13-4065.pdf,https://www.aclweb.org/anthology/W13-4065.pdf,,,
864,DICE__a_Dataset_of_Italian_Crime_Event_news,Text Categorization,Text Categorization,"Text Categorization, text similarity, Information Retrieval, Word Embeddings, News Summarization, Question Answering, Text Clustering",Text,English,Natural Language Processing,,CC BY-NC-SA 4.0,https://github.com/federicarollo/Italian-Crime-News,https://paperswithcode.com/dataset/italian-crime-news,"The dataset contains the main components of the news articles published online by the newspaper named <a href=""https://gazzettadimodena.gelocal.it/modena"">Gazzetta di Modena</a>: url of the web page, title, sub-title, text, date of publication, crime category assigned to each news article by the author.

The news articles are written in Italian and describe 11 types of crime events occurred in the province of Modena between the end of 2011 and 2021.

Moreover, the dataset includes data derived from the abovementioned components thanks to the application of Natural Language Processing techniques. 
Some examples are the place of the crime event occurrence (municipality, area, address and GPS coordinates), the date of the occurrence, and the type of the crime events described in the news article obtained by an automatic categorization of the text.

In the end, news articles describing the same crime events (duplciates) are detected by calculating the document  similarity.

Now, we are working on the application of question answering to extract the 5W+1H and we plan to extend the current dataset with the obtained data.

Other researchers can employ the dataset to apply other algorithms of text categorization and duplicate detection and compare their results with the benchmark. The dataset can be useful for several scopes, e.g., geo-localization of the events, text summarization, crime analysis, crime prediction, community detection, topic modeling.",2011,,,,,
865,dichasus-cf0x,Joint Radar-Communication,Joint Radar-Communication,"Joint Radar-Communication, Intelligent Communication, Indoor Localization",Image,,Computer Vision,,CC BY 4.0,https://dichasus.inue.uni-stuttgart.de/datasets/data/dichasus-cf0x/,https://paperswithcode.com/dataset/dichasus-cf0x,"Dataset containing channel state information (CSI) alongside ground truth data (position tags, timestamps) of a massive MIMO-OFDM system measured with the DICHASUS channel sounder. Measurement parameters and machine-readable file format descriptions are provided in a JSON file (spec.json).

Distributed antenna setup with line-of-sight (LoS) and non-line-of-sight (NLoS) channels, measured in the ARENA2036 research factory campus environment.",,,,,,
866,DICM,Low-Light Image Enhancement,Low-Light Image Enhancement,Low-Light Image Enhancement,Image,,Computer Vision,low-light-image-enhancement-on-dicm,,,https://paperswithcode.com/dataset/dicm,DICM is a dataset for low-light enhancement which consists of 69 images collected with commercial digital cameras.,,Deep Retinex Decomposition for Low-Light Enhancement,https://arxiv.org/abs/1808.04560,69 images,,
867,DigestPath,Tumor Segmentation,Tumor Segmentation,"Tumor Segmentation, Semantic Segmentation, Medical Image Segmentation, Lesion Segmentation, Cell Segmentation",Image,,Computer Vision,tumor-segmentation-on-digestpath,,https://digestpath2019.grand-challenge.org/Dataset/,https://paperswithcode.com/dataset/digestpath,"Introduced by Da et al. in DigestPath: a Benchmark Dataset with Challenge Review for the Pathological Detection and Segmentation of Digestive-System

Grand-Challenge Page
1. Signet ring cell dataset
Signet ring cell carcinoma is a type of rare adenocarcinoma with poor prognosis. Early detection of such cells leads to huge improvement of patients' survival rate. However, there is no existing public dataset with annotations for studying the problem of signet ring cell detection.

This dataset has positive samples and negative samples. Training positive samples contain 77 images from 20 WSIs, with cell bounding boxes written in xml. Training negative samples contain 378 images from 79 WSIs.These negative WSIs have no signet ring, but could contain other kinds of tumor cells. Each signet ring cell is labeled by experienced pathologists with a rectangle bounding box tightly surrounding the cell. Each image is of size 2000X2000. The training images are from 2 organs, including gastric mucosa and intestine. Because of the difficulty of manual annotation, there exist some signet ring cells who are missed by pathologists. In other words, this dataset is  a noisy dataset with its positive images not fully annotated. 

All whole slide images were stained by hematoxylin and eosin and scanned at X40. 

2. Colonoscopy tissue segment dataset
Colonoscopy pathology examination can find cells of early-stage colon tumor from small tissue slices. Pathologists need to daily examine hundreds of tissue slices, which is a time consuming and exhausting work. Here we propose a challenge task on automatic colonoscopy tissue segmentation and screening, aiming at automatic lesion segmentation and classification of the whole tissue (benign vs. malignant).

This dataset has positive samples and negative samples. Training positive samples contain 250 images of tissue from 93 WSIs, with pixel-level annotation in jpg format, where 0 means background and 255 for foreground (malignant lesion). You could simply get binary mask by a threshold 128. Training negative samples contain 410 images of tissue from 231 WSI. This negative images have no annotation because they don't have any malignant lesion.

The average size of all images are of 5000x5000 pixels, some of them are extremely huge. We will also provide another 152 patients' 212 tissues as the testing set, in which 90 images from 65 patients contain lesion. All whole slide images were stained by hematoxylin and eosin and scanned at X20.

Sign the DATABASE USE AGREEMENT first and download the dataset at the homepage!

```

Da Q, Huang X, Li Z, et al. DigestPath: a Benchmark Dataset with Challenge Review for the 
Pathological Detection and Segmentation of Digestive-System[J]. 
Medical Image Analysis, 2022: 102485.

（https://doi.org/10.1016/j.media.2022.102485）

```",2022,DigestPath: a Benchmark Dataset with Challenge Review for the Pathological Detection and Segmentation of Digestive-System,https://github.com/bupt-ai-cz/CAC-UNet-DigestPath2019/blob/main/papers/DigestPath-a-Benchmark-Dataset-with-Challenge-Review.pdf,77 images,Training positive samples contain 77 images,
868,DigiLeTs,Handwriting generation,Handwriting generation,"Handwriting generation, Handwritten Digit Recognition, Handwriting Recognition, Handwriting Verification","Image, Text",English,Computer Vision,,,https://github.com/CognitiveModeling/ExtendingOmniglot/,https://paperswithcode.com/dataset/digilets,"A dataset with $23\,870$ digital trajectories (i.e. time series) of handwritten lower- and uppercase Latin letters and Arabic numbers ($a$-$z$, $A$-$Z$, $0$-$9$), generated by $77$ experts using a Wacom Pen Tablet. An expert is considered a proficient user of the recorded symbols, in this case adult native German speakers.

DigiLetTs was created to extend the Omniglot dataset and contains five variants per character per subject to allow the quantification of intra-subject variability and to assess and account for individual writing styles. The determination and imitation of subject-dependent writing styles is introduced as a new task in this paper.

For more information about the dataset, please refer to the repository (Homepage button below).",,,,,,
869,Digital_Forensics_2023_dataset_-_DF2023,Image Manipulation Localization,Image Manipulation Localization,"Image Manipulation Localization, Image Manipulation, Image Forgery Detection, Image Manipulation Detection",Image,,Computer Vision,,Custom,https://www.kaggle.com/datasets/davidfischinger/image-manipulation-dataset-df2023,https://paperswithcode.com/dataset/image-manipulation-dataset-df2023,"The deliberate manipulation of public opinion, especially through altered images, poses a significant danger to society. To fight this issue on a technical level we support the research community by releasing the Digital Forensics 2023 (DF2023) training and validation dataset.

The DF2023 training dataset comprises one million images from four major forgery categories:


splicing (400K)
copy-move (300K)
enhancement (200K)
removal (100K)

This dataset enables an objective comparison of network architectures and can significantly reduce the time and effort of researchers preparing datasets.

For a detailed description of the DF2023 dataset, please refer to:

@inproceedings{Fischinger2023DFNet,
title={DF2023: The Digital Forensics 2023 Dataset for Image Forgery Detection},
author={David Fischinger and Martin Boyer},
journal={The 25th Irish Machine Vision and Image Processing conference. (IMVIP)},
year={2023}
}
available from: Zenodo

Naming convention
The naming convention of DF2023 encodes information about the applied manipulations. Each image name has the following form:

COCO_DF_0123456789_NNNNNNNN.{EXT} (e.g. COCO_DF_E000G40117_00200620.jpg)

After the identifier of the image data source (""COCO"") and the self-reference to the Digital Forensics (""DF"") dataset, there are 10 digits as placeholders for the manipulation. Position 0 defines the manipulation types copy-move, splicing, removal, enhancement ([C,S,R,E]). The following digits 1-9 represent donor patch manipulations. For positions [1,2,7,8] (resample, flip, noise and brightness), a binary value indicates if this manipulation was applied to the donor image patch. Position 3 (rotate) indicates by the values 0-3 if the rotation was executed by 0, 90, 180 or 270 degrees. Position 4 defines if BoxBlur (B) or GaussianBlur (G) was used. Position 5 specifies the blurring radius. A value of 0 indicates that no blurring was executed. Position 6 indicates which of the Python-PIL contrast filters EDGE ENHANCE, EDGE ENHANCE MORE, SHARPEN, UnsharpMask or ImageEnhance (values 1-5) was applied. If none of them was applied, this value is set to 0. Finally, position 9 is set to the JPEG compression factor modulo 10, a value of 0 indicates that no JPEG compression was applied. The 8 characters NNNNNNNN in the image name template stand for a running number of the images.",2023,,,,"val, enhancement ([C,S,R,E]). The following digits 1-9 represent donor patch manipulations. For positions [1,2,7,8] (resample, flip, noise and brightness), a binary value indicates if this manipulation was applied to the donor image patch. Position 3 (rotate) indicates by the values 0-3 if the rotation was executed by 0, 90, 180 or 270 degrees. Position 4 defines if BoxBlur (B) or GaussianBlur (G) was used. Position 5 specifies the blurring radius. A value of 0 indicates that no blurring was executed. Position 6 indicates which of the Python-PIL contrast filters EDGE ENHANCE, EDGE ENHANCE MORE, SHARPEN, UnsharpMask or ImageEnhance (values 1-5) was applied. If none of them was applied, this value is set to 0. Finally, position 9 is set to the JPEG compression factor modulo 10, a value of 0 indicates that no JPEG compression was applied. The 8 characters NNNNNNNN in the image name template stand for a running number of the images",
870,Digital_Peter,Handwritten Text Recognition,Handwritten Text Recognition,Handwritten Text Recognition,"Image, Text",English,Computer Vision,handwritten-text-recognition-on-digital-peter,,https://github.com/MarkPotanin/DigitalPeter,https://paperswithcode.com/dataset/digital-peter,"Digital Peter is a dataset of Peter the Great's manuscripts annotated for segmentation and text recognition. The dataset may be useful for researchers to train handwriting text recognition models as a benchmark for comparing different models. It consists of 9,694 images and text files corresponding to lines in historical documents. The dataset includes Peter’s handwritten materials covering the period from 1709 to 1713. 

The open machine learning competition Digital Peter was held based on the considered dataset.",,,,694 images,"train handwriting text recognition models as a benchmark for comparing different models. It consists of 9,694 images",
871,Digits,Graph Classification,Graph Classification,"Graph Classification, Handwritten Digit Recognition, Feature Importance","Graph, Image",,Computer Vision,"graph-classification-on-digits, feature-importance-on-digits, handwritten-digit-recognition-on-digits-1",Custom,https://archive.ics.uci.edu/ml/datasets/optical+recognition+of+handwritten+digits,https://paperswithcode.com/dataset/digits,The DIGITS dataset consists of 1797 8×8 grayscale images (1439 for training and 360 for testing) of handwritten digits.,,Differentially Private Variational Dropout,https://arxiv.org/abs/1712.02629,,,
872,DIHARD_II,Speaker Diarization,Speaker Diarization,Speaker Diarization,Audio,,Audio,speaker-diarization-on-dihard-ii,Custom,https://dihardchallenge.github.io/dihard2/,https://paperswithcode.com/dataset/dihard-ii,"The DIHARD II development and evaluation sets draw from a diverse set of sources exhibiting wide variation in recording equipment, recording environment, ambient noise, number of speakers, and speaker demographics. The development set includes reference diarization and speech segmentation and may be used for any purpose including system development or training.",,,,,,
873,DIODE,Monocular Depth Estimation,Monocular Depth Estimation,"Monocular Depth Estimation, Depth Estimation, Indoor Monocular Depth Estimation",3D,,Methodology,"depth-estimation-on-diode, indoor-monocular-depth-estimation-on-diode",MIT,https://diode-dataset.org/,https://paperswithcode.com/dataset/diode,"Diode Dense Indoor/Outdoor DEpth (DIODE) is the first standard dataset for monocular depth estimation comprising diverse indoor and outdoor scenes acquired with the same hardware setup. The training set consists of 8574 indoor and 16884 outdoor samples from 20 scans each. The validation set contains 325 indoor and 446 outdoor samples with each set from 10 different scans. The ground truth density for the indoor training and validation splits are approximately 99.54% and 99%, respectively. The density of the outdoor sets are naturally lower with 67.19% for training and 78.33% for validation subsets. The indoor and outdoor ranges for the dataset are 50m and 300m, respectively.",,Bidirectional Attention Network for Monocular Depth Estimation,https://arxiv.org/abs/2009.00743,,training set consists of 8574 indoor and 16884 outdoor samples,
874,DiPCo,Distant Speech Recognition,Distant Speech Recognition,"Distant Speech Recognition, Robust Speech Recognition, Speech Separation","Audio, Image, Text",English,Speech,,Custom,https://zenodo.org/record/8122551,https://paperswithcode.com/dataset/dipco,"We present a speech data corpus that simulates a ""dinner party"" scenario taking place in an everyday home environment. The corpus was created by recording multiple groups of four Amazon employee volunteers having a natural conversation in English around a dining table. The participants were recorded by a single-channel close-talk microphone and by five far-field 7-microphone array devices positioned at different locations in the recording room. The dataset contains the audio recordings and human labeled transcripts of a total of 10 sessions with a duration between 15 and 45 minutes. The corpus was created to advance in the field of noise robust and distant speech processing and is intended to serve as a public research and benchmarking data set.",,,,,,
875,Discovery,Relation Classification,Relation Classification,"Relation Classification, Sentence Embeddings","Graph, Image",,Computer Vision,relation-classification-on-discovery-dataset,Apache-2.0,https://github.com/synapse-developpement/Discovery,https://paperswithcode.com/dataset/discovery,"The Discovery datasets consists of adjacent sentence pairs (s1,s2) with a discourse marker (y) that occurred at the beginning of s2. They were extracted from the depcc web corpus.

Markers prediction can be used in order to train a sentence encoders. Discourse markers can be considered as noisy labels for various semantic tasks, such as entailment (y=therefore), subjectivity analysis (y=personally) or sentiment analysis (y=sadly), similarity (y=similarly), typicality, (y=curiously) ...

The specificity of this dataset is the diversity of the markers, since previously used data used only ~10 imbalanced classes. The author of the dataset provide:


a list of the 174 discourse markers
a Base version of the dataset with 1.74 million pairs (10k examples per marker)
a Big version with 3.4 million pairs
a Hard version with 1.74 million pairs where the connective couldn't be predicted with a fastText linear model",,,,10k examples,,
876,DISFA,Smile Recognition,Smile Recognition,"Smile Recognition, Facial Action Unit Detection, Facial Expression Recognition (FER)","Image, Video",,Computer Vision,"facial-action-unit-detection-on-disfa, smile-recognition-on-disfa, facial-expression-recognition-on-disfa","Custom (research-only, attribution)",http://mohammadmahoor.com/disfa/,https://paperswithcode.com/dataset/disfa,"The Denver Intensity of Spontaneous Facial Action (DISFA) dataset consists of 27 videos of 4844 frames each, with 130,788 images in total. Action unit annotations are on different levels of intensity, which are ignored in the following experiments and action units are either set or unset. DISFA was selected from a wider range of databases popular in the field of facial expression recognition because of the high number of smiles, i.e. action unit 12. In detail, 30,792 have this action unit set, 82,176 images have some action unit(s) set and 48,612 images have no action unit(s) set at all.",,Deep Learning For Smile Recognition,https://arxiv.org/abs/1602.00172,788 images,,
877,DISL,Code Generation,Code Generation,"Code Generation, Code Repair",Text,English,Natural Language Processing,,MIT,https://huggingface.co/datasets/ASSERT-KTH/DISL,https://paperswithcode.com/dataset/disl,"DISL
The full dataset report is available at: https://arxiv.org/abs/2403.16861

The DISL dataset features a collection of 514, 506 unique Solidity files that have been deployed to Ethereum mainnet. It caters to the need for a large and diverse dataset of real-world smart contracts. DISL serves as a resource for developing machine learning systems and for benchmarking software engineering tools designed for smart contracts.


Curated by: Gabriele Morello
License: [MIT]

Instructions to explore the dataset
```python
from datasets import load_dataset

Load the raw dataset
dataset = load_dataset(""ASSERT-KTH/DISL"", ""raw"")

OR
Load the decomposed dataset
dataset = load_dataset(""ASSERT-KTH/DISL"", ""decomposed"")

number of rows and columns
num_rows = len(dataset[""train""])
num_columns = len(dataset[""train""].column_names)

random row
import random
random_row = random.choice(dataset[""train""])

random source code
random_sc = random.choice(dataset[""train""])['source_code']
print(random_sc)
```",,,,,,
878,DispScenes,Matching Disparate Images,Matching Disparate Images,"Matching Disparate Images, Graph Matching",Graph,,Methodology,,,,https://paperswithcode.com/dataset/dispscenes,The DispScenes dataset was created to address the specific problem of disparate image matching. The image pairs in all the datasets exhibit high levels of variation in illumination and viewpoint and also contain instances of occlusion. The DispScenes dataset provides manual ground truth keypoint correspondences for all images.,,Matching Disparate Image Pairs Using Shape-Aware ConvNets,https://arxiv.org/abs/1811.09889,,,
879,DISRPT2019,Discourse Segmentation,Discourse Segmentation,Discourse Segmentation,Image,,Computer Vision,,,https://github.com/disrpt/sharedtask2019,https://paperswithcode.com/dataset/disrpt2019,"The DISRPT 2019 workshop introduces the first iteration of a cross-formalism shared task on discourse unit segmentation. Since all major discourse parsing frameworks imply a segmentation of texts into segments, learning segmentations for and from diverse resources is a promising area for converging methods and insights. We provide training, development and test datasets from all available languages and treebanks in the RST, SDRT and PDTB formalisms, using a uniform format. Because different corpora, languages and frameworks use different guidelines for segmentation, the shared task is meant to promote design of flexible methods for dealing with various guidelines, and help to push forward the discussion of standards for discourse units. For datasets which have treebanks, we will evaluate in two different scenarios: with and without gold syntax, or otherwise using provided automatic parses for comparison.",2019,,,,,
880,DISRPT2021,Relation Classification,Relation Classification,"Relation Classification, Connective Detection, Implicit Discourse Relation Classification, Discourse Parsing, Discourse Segmentation","Graph, Image, Text",English,Computer Vision,,,https://github.com/disrpt/sharedtask2021,https://paperswithcode.com/dataset/disrpt2021,"The DISRPT 2021 shared task, co-located with CODI 2021 at EMNLP, introduces the second iteration of a cross-formalism shared task on discourse unit segmentation and connective detection, as well as the first iteration of a cross-formalism discourse relation classification task.

We provide training, development and test datasets from all available languages and treebanks in the RST, SDRT and PDTB formalisms, using a uniform format. Because different corpora, languages and frameworks use different guidelines, the shared task is meant to promote design of flexible methods for dealing with various guidelines, and help to push forward the discussion of standards for computational approaches to discourse relations. We include data for evaluation with and without gold syntax, or otherwise using provided automatic parses for comparison to gold syntax data.",2021,,,,,
881,Dissonance_Twitter_Dataset,Implicit Discourse Relation Classification,Implicit Discourse Relation Classification,"Implicit Discourse Relation Classification, Classification, Text Classification","Graph, Image, Text",English,Computer Vision,,MIT,https://github.com/humanlab/dissonance-twitter-dataset,https://paperswithcode.com/dataset/dissonance-twitter-dataset,Dissonance Twitter Dataset is a dataset collected from annotating tweets for dissonance.,,,,,,
882,Distributional_MIPLIB,Combinatorial Optimization,Combinatorial Optimization,Combinatorial Optimization,,,Methodology,,,https://sites.google.com/usc.edu/distributional-miplib/home,https://paperswithcode.com/dataset/distributional-miplib,"Distributional MIPLIB is  a dataset of Mixed Integer Linear Programming (MILP) instances designed to advance research on learning to optimize \url{https://www.arxiv.org/abs/2406.06954}. It is a curated dataset of MILP distributions from 13 domains, classified into different hardness levels. The links for downloading the distributions are provided in the webpage of each domain.",,,,,,
883,DIV2K,Image Rescaling,Image Rescaling,"Image Rescaling, Image Super-Resolution, JPEG Artifact Correction, Jpeg Compression Artifact Reduction, Denoising",Image,,Computer Vision,"denoising-on-div2k, image-super-resolution-on-div2k-val-4x, image-super-resolution-on-div2k-val-16x",Custom (research-only),https://data.vision.ee.ethz.ch/cvl/DIV2K/,https://paperswithcode.com/dataset/div2k,"DIV2K is a popular single-image super-resolution dataset which contains 1,000 images with different scenes and is splitted to 800 for training, 100 for validation and 100 for testing. It was collected for NTIRE2017 and NTIRE2018 Super-Resolution Challenges in order to encourage research on image super-resolution with more realistic degradation. This dataset contains low resolution images with different types of degradations. Apart from the standard bicubic downsampling, several types of degradations are considered in synthesizing low resolution images for different tracks of the challenges. Track 2 of NTIRE 2017 contains low resolution images with unknown x4 downscaling. Track 2 and track 4 of NTIRE 2018 correspond to realistic mild ×4 and realistic wild ×4 adverse conditions, respectively. Low-resolution images under realistic mild x4 setting suffer from motion blur, Poisson noise and pixel shifting. Degradations under realistic wild x4 setting are further extended to be of different levels from image to image.",2017,Unsupervised Image Super-Resolution with an Indirect Supervised Path,https://arxiv.org/abs/1910.02593,000 images,"splitted to 800 for training, 100 for validation and 100 for testing. It was collected for NTIRE2017 and NTIRE2018 Super-Resolution Challenges in order to encourage research on image super-resolution with more realistic degradation. This dataset contains low resolution images",
884,DIV2KRK,Blind Super-Resolution,Blind Super-Resolution,Blind Super-Resolution,,,Methodology,"blind-super-resolution-on-div2krk-4x, blind-super-resolution-on-div2krk-2x",,https://www.wisdom.weizmann.ac.il/~vision/kernelgan/,https://paperswithcode.com/dataset/div2krk,"Using the validation set (100 images) from the widely used DIV2K dataset, we blurred and subsampled each image with a different, randomly generated kernel. Kernels were 11x11 anisotropic gaussians with random lengths λ1, λ2∼U(0.6, 5) independently distributed for each axis, rotated by a random angle θ∼U[−π, π].",,,,100 images,validation set (100 images,
885,DiveFace,Fairness,Fairness,"Fairness, Facial Attribute Classification, Face Recognition",Image,,Computer Vision,"facial-attribute-classification-on-diveface, fairness-on-diveface",,https://github.com/BiDAlab/DiveFace,https://paperswithcode.com/dataset/diveface,A new face annotation dataset with balanced distribution between genders and ethnic origins.,,,,,,
886,DKhate,Abusive Language,Abusive Language,"Abusive Language, Hate Speech Detection","Audio, Image, Text",English,Computer Vision,hate-speech-detection-on-dkhate,CC-BY,https://huggingface.co/datasets/DDSC/dkhate,https://paperswithcode.com/dataset/dkhate,"A corpus of Offensive Language and Hate Speech Detection for Danish

This DKhate dataset contains 3600 comments from the web annotated for offensive language, following the Zampieri et al. / OLID scheme.

Submissions and benchmarks for the OffensEval 2020 Danish track are also included.",2020,,,,,
887,dMelodies,Music Generation,Music Generation,"Music Generation, Audio Generation","Audio, Text",English,Natural Language Processing,,,https://github.com/ashispati/dmelodies_dataset,https://paperswithcode.com/dataset/dmelodies,"dMelodies is dataset of simple 2-bar melodies generated using 9 independent latent factors of variation where each data point represents a unique melody based on the following constraints:
- Each melody will correspond to a unique scale (major, minor, blues, etc.).
- Each melody plays the arpeggios using the standard I-IV-V-I cadence chord pattern.
- Bar 1 plays the first 2 chords (6 notes), Bar 2 plays the second 2 chords (6 notes).
- Each played note is an 8th note.",,,,,,
888,DND,Image Denoising,Image Denoising,"Image Denoising, Denoising",Image,,Computer Vision,"denoising-on-dnd-1, image-denoising-on-dnd",,https://noise.visinf.tu-darmstadt.de/,https://paperswithcode.com/dataset/dnd,"Benchmarking Denoising Algorithms with Real Photographs

This dataset consists of 50 pairs of noisy and (nearly) noise-free images captured with four consumer cameras. Since the images are of very high-resolution, the providers extract 20 crops of size 512 × 512 from each image, thus yielding a total of 1000 patches.",,,,,,
889,DNS_Challenge,Audio Source Separation,Audio Source Separation,"Audio Source Separation, Speech Dereverberation, Speech Enhancement",Audio,,Audio,"speech-enhancement-on-deep-noise-suppression, speech-enhancement-on-interspeech-2020-deep, speech-dereverberation-on-deep-noise",,https://www.microsoft.com/en-us/research/academic-program/deep-noise-suppression-challenge-interspeech-2020/,https://paperswithcode.com/dataset/deep-noise-suppression-2020,The DNS Challenge at INTERSPEECH 2020 intended to promote collaborative research in single-channel Speech Enhancement aimed to maximize the perceptual quality and intelligibility of the enhanced speech. The challenge evaluated the speech quality using the online subjective evaluation framework ITU-T P.808. The challenge provides large datasets for training noise suppressors.,2020,,,,,
890,Doc2Dial,Dialogue Understanding,Dialogue Understanding,"Dialogue Understanding, Dialogue Generation, Conversational Question Answering, Goal-Oriented Dialog, Question Answering, Dialogue Act Classification","Image, Text",English,Computer Vision,,Apache-2.0 License,https://doc2dial.github.io,https://paperswithcode.com/dataset/doc2dial-1,"For goal-oriented document-grounded dialogs, it often involves complex contexts for identifying the most relevant information, which requires better understanding of the inter-relations between conversations and documents. Meanwhile, many online user-oriented documents use both semi-structured and unstructured contents for guiding users to access information of different contexts. Thus, we create a new goal-oriented document-grounded dialogue dataset that captures more diverse scenarios derived from various document contents from multiple domains such ssa.gov and studentaid.gov. For data collection, we propose a novel pipeline approach for dialogue data construction, which has been adapted and evaluated for several domains.",,,,,,
891,Doc3DShade,Intrinsic Image Decomposition,Intrinsic Image Decomposition,"Intrinsic Image Decomposition, Optical Character Recognition (OCR), Shadow Removal",Image,,Computer Vision,,,https://github.com/cvlab-stonybrook/DocIIW,https://paperswithcode.com/dataset/doc3dshade,Doc3DShade extends Doc3D with realistic lighting and shading. Follows a similar synthetic rendering procedure using captured document 3D shapes but final image generation step combines real shading of different types of paper materials under numerous illumination conditions.,,,,,,
892,DocBank,Document Layout Analysis,Document Layout Analysis,"Document Layout Analysis, Optical Character Recognition (OCR)","Image, Text",English,Computer Vision,,,https://github.com/doc-analysis/DocBank,https://paperswithcode.com/dataset/docbank,A benchmark dataset that contains 500K document pages with fine-grained token-level annotations for document layout analysis. DocBank is constructed using a simple yet effective way with weak supervision from the \LaTeX{} documents available on the arXiv.com.,,,,,,
893,DocILE,Key Information Extraction,Key Information Extraction,"Key Information Extraction, Line Items Extraction",,,Methodology,,MIT,https://github.com/rossumai/docile,https://paperswithcode.com/dataset/docile,"DocILE is a large dataset of business documents for the tasks of Key Information Localization and Extraction and Line Item Recognition. It contains 6.7k annotated business documents, 100k synthetically generated documents, and nearly 1M unlabeled documents for unsupervised pre-training. The dataset has been built with knowledge of domain- and task-specific aspects, resulting in the following key features: 

i) annotations in 55 classes, which surpasses the granularity of previously published key information extraction datasets by a large margin

ii) Line Item Recognition represents a highly practical information extraction task, where key information has to be assigned to items in a table

iii) documents come from numerous layouts and the test set includes zero- and few-shot cases as well as layouts commonly seen in the training set",,DocILE Benchmark for Document Information Localization and Extraction,https://arxiv.org/pdf/2302.05658v1.pdf,,,55
894,dockstring,Graph Regression,Graph Regression,Graph Regression,Graph,,Methodology,"graph-regression-on-esr2, graph-regression-on-pgr, graph-regression-on-f2, graph-regression-on-parp1, graph-regression-on-kit",Apache-2.0 license,https://dockstring.github.io/,https://paperswithcode.com/dataset/dockstring,"Regression dataset for molecular docking scores (predicted molecule-protein binding affinity). Contains ~250,000 molecules against 58 protein targets.",,,,,,
895,DocRED-IE,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Entity Disambiguation, Document-level Closed Information Extraction, Joint Entity and Relation Extraction, Entity Typing, Document-level Relation Extraction","Graph, Text",English,Natural Language Processing,"entity-disambiguation-on-docred-ie, entity-typing-on-docred-ie, document-level-relation-extraction-on-docred-3, document-level-closed-information-extraction-2, joint-entity-and-relation-extraction-on-10, coreference-resolution-on-docred-ie",MIT,https://github.com/amazon-science/e2e-docie,https://paperswithcode.com/dataset/docred-ie,"The DocRED Information Extraction (DocRED-IE) dataset extends the DocRED dataset for the Document-level Closed Information Extraction (DocIE) task. DocRED-IE is a multi-task dataset and allows for 5 subtasks: (i) Document-level Relation Extraction, (ii) Mention Detection, (iii) Entity Typing, (iv) Entity Disambiguation, (v) Coreference Resolution, as well as combinations thereof such as Named Entity Recognition (NER) or Entity Linking. The DocRED-IE dataset also allows for the end-to-end tasks of: (i) DocIE and (ii) Joint Entity and Relation Extraction. DocRED-IE comprises sentence-level and document-level facts, thereby describing short as well as long-range interactions within an entire document.",,,,,,
896,DocRED,Document-level Closed Information Extraction,Document-level Closed Information Extraction,"Document-level Closed Information Extraction, Relation Extraction, Joint Entity and Relation Extraction, Few-Shot Relation Classification","Graph, Image, Text",English,Computer Vision,"document-level-closed-information-extraction, relation-extraction-on-docred, joint-entity-and-relation-extraction-on-3, few-shot-relation-classification-on-docred",,https://github.com/thunlp/DocRED,https://paperswithcode.com/dataset/docred,"DocRED (Document-Level Relation Extraction Dataset) is a relation extraction dataset constructed from Wikipedia and Wikidata. Each document in the dataset is human-annotated with named entity mentions, coreference information, intra- and inter-sentence relations, and supporting evidence. DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document. Along with the human-annotated data, the dataset provides large-scale distantly supervised data.

DocRED contains 132,375 entities and 56,354 relational facts annotated on 5,053 Wikipedia documents. In addition to the human-annotated data, the dataset provides large-scale distantly supervised data over 101,873 documents.",,,,873 documents,,
897,DocUNet,SSIM,SSIM,"SSIM, MS-SSIM, Image Restoration, Document Enhancement, Local Distortion","Image, Text",English,Computer Vision,"ms-ssim-on-docunet, local-distortion-on-docunet, ssim-on-docunet",,https://www3.cs.stonybrook.edu/~cvl/docunet.html,https://paperswithcode.com/dataset/docunet,"Various documents dataset.
Each of the 65 documents includes scanned ground truth images, both hard and easy distorted photos, and document-centered cropped images.",,,,65 documents,,
898,DocVQA,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Question Answering, Reading Comprehension","Image, Text",English,Computer Vision,"visual-question-answering-on-docvqa-val, visual-question-answering-on-docvqa-test, visual-question-answering-vqa-on-docvqa",,https://cvit.iiit.ac.in/docvqa/,https://paperswithcode.com/dataset/docvqa,"DocVQA consists of 50,000 questions defined on 12,000+ document images.",,,,,,
899,DOLPHINS,2D Object Detection,2D Object Detection,"2D Object Detection, Multiview Detection, Object Tracking, 3D Object Detection","3D, Image, Video",,Computer Vision,,Creative Commons Attribution 4.0 International,https://dolphins-dataset.net/,https://paperswithcode.com/dataset/dolphins,"Vehicle-to-Everything (V2X) network has enabled collaborative perception in autonomous driving, which is a promising solution to the fundamental defect of stand-alone intelligence including blind zones and long-range perception. However, the lack of datasets has severely blocked the development of collaborative perception algorithms. In this work, we release DOLPHINS: Dataset for cOllaborative Perception enabled Harmonious and INterconnected Self-driving, as a new simulated large-scale various-scenario multi-view multi-modality autonomous driving dataset, which provides a ground-breaking benchmark platform for interconnected autonomous driving. DOLPHINS outperforms current datasets in six dimensions: temporally-aligned images and point clouds from both vehicles and Road Side Units (RSUs) enabling both Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) based collaborative perception; 6 typical scenarios with dynamic weather conditions make the most various interconnected autonomous driving dataset; meticulously selected viewpoints providing full coverage of the key areas and every object; 42376 frames and 292549 objects, as well as the corresponding 3D annotations, geo-positions, and calibrations, compose the largest dataset for collaborative perception; Full-HD images and 64-line LiDARs construct high-resolution data with sufficient details; well-organized APIs and open-source codes ensure the extensibility of DOLPHINS. We also construct a benchmark of 2D detection, 3D detection, and multi-view collaborative perception tasks on DOLPHINS. The experiment results show that the raw-level fusion scheme through V2X communication can help to improve the precision as well as to reduce the necessity of expensive LiDAR equipment on vehicles when RSUs exist, which may accelerate the popularity of interconnected self-driving vehicles.",,,,,,
900,DomainNet,Blended-target Domain Adaptation,Blended-target Domain Adaptation,"Blended-target Domain Adaptation, Universal Domain Adaptation, Multi-target Domain Adaptation, Domain Generalization, Zero-Shot Learning + Domain Generalization, Unsupervised Domain Adaptation, Partial Domain Adaptation, Unsupervised Continual Domain Shift Learning, Multi-Source Unsupervised Domain Adaptation, Domain Adaptation",,,Methodology,"domain-generalization-on-domainnet, unsupervised-domain-adaptation-on-domainnet-1, blended-target-domain-adaptation-on-domainnet, multi-target-domain-adaptation-on-domainnet, zero-shot-learning-domain-generalization-on, partial-domain-adaptation-on-domainnet, multi-source-unsupervised-domain-adaptation, universal-domain-adaptation-on-domainnet, domain-adaptation-on-domainnet-1, unsupervised-continual-domain-shift-learning-2","Custom (research-only, non-commercial)",http://ai.bu.edu/M3SDA/,https://paperswithcode.com/dataset/domainnet,"DomainNet is a dataset of common objects in six different domain. All domains include 345 categories (classes) of objects such as Bracelet, plane, bird and cello. The domains include clipart: collection of clipart images; real: photos and real world images; sketch: sketches of specific objects; infograph: infographic images with specific object; painting artistic depictions of objects in the form of paintings and quickdraw: drawings of the worldwide players of game “Quick Draw!”.",,What is being transferred in transfer learning?,https://arxiv.org/abs/2008.11687,,,345
901,DONeRF__Evaluation_Dataset,Novel View Synthesis,Novel View Synthesis,"Novel View Synthesis, Neural Rendering",,,Methodology,novel-view-synthesis-on-donerf-evaluation,CC-BY 4.0 International,https://repository.tugraz.at/records/jjs3x-4f133,https://paperswithcode.com/dataset/donerf-evaluation-dataset,"This is the dataset for the CGF 2021 paper ""DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks"".

Please note the original creators of the individual 3D scenes themselves (individual license files can be found in the individual .zip archives in the dataset):

Bulldozer by ""Heinzelnisse"" (CC-BY-NC): https://www.blendswap.com/blend/11490

Forest by Robin Tran (CC-BY-SA 3.0): https://cloud.blender.org/p/gallery/5fbd186ec57d586577c57417

Classroom by Christophe Seux (CC-0): https://download.blender.org/demo/test/classroom.zip

San Miguel by Guillermo M. Leal Llaguno (CC-BY 3.0): https://casual-effects.com/g3d/data10/index.html#

Pavillon by Hamza Cheggour / ""eMirage"" (CC-BY): https://download.blender.org/demo/test/pabellon_barcelona_v1.scene_.zip

Barbershop by Blender Animation Studio (CC-BY): https://svn.blender.org/svnroot/bf-blender/trunk/lib/benchmarks/cycles/barbershop_interior/",2021,,,,,
902,DOORS,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, 3D Object Recognition","3D, Image",,Computer Vision,,Creative Commons Attribution 4.0 International,https://zenodo.org/record/7107409#.Y20Ja-z7RhE,https://paperswithcode.com/dataset/doors,"DOORS  is a dataset designed for boulders recognition, centroid regression, segmentation, and navigation applications.  The dataset is divided into two sets:



Regression: Contains images, masks, and labels for 4 splits of single boulders positioned on the surface of a spherical mesh. It can be used to perform navigation, boulder recognition, segmentation, and centroid regression.



Segmentation: Contain images, masks, and labels of 2 datasets: DS1 and DS2. DS1 is made of the same images of the Regression dataset but is specifically designed for segmentation. DS2 is made of images with multiple instances of boulders appearing on the surface of the Didymos asteroid model",,DOORS: Dataset fOr bOuldeRs Segmentation. Statistical properties and Blender setup,https://arxiv.org/pdf/2210.16253v1.pdf,,,
903,Douban,Link Prediction,Link Prediction,"Link Prediction, Conversational Response Selection, Recommendation Systems",Time Series,,Methodology,"conversational-response-selection-on-douban-1, collaborative-filtering-on-douban, link-prediction-on-douban, recommendation-systems-on-douban-monti",,https://github.com/MarkWuNLP/MultiTurnResponseSelection,https://paperswithcode.com/dataset/douban,"We release Douban Conversation Corpus, comprising a training data set, a development set and a test set for retrieval based chatbot. The statistics of Douban Conversation Corpus are shown in the following table. 

|      |Train|Val| Test         | 
| ------------- |:-------------:|:-------------:|:-------------:|
| session-response pairs  | 1m|50k| 10k |
| Avg. positive response per session     | 1|1| 1.18    | 
| Fless Kappa | N\A|N\A|0.41      | 
| Min turn per session | 3|3| 3      | 
| Max ture per session | 98|91|45    | 
| Average turn per session | 6.69|6.75|5.95    | 
| Average Word per utterance | 18.56|18.50|20.74   | 

The test data contains 1000 dialogue context, and for each context we create 10 responses as candidates. We recruited three labelers to judge if a candidate is a proper response to the session. A proper response means the response can naturally reply to the message given the context. Each pair received three labels and the majority of the labels was taken as the final decision.

<br>
As far as we known, this is the first human-labeled test set for retrieval-based chatbots. The entire corpus link https://www.dropbox.com/s/90t0qtji9ow20ca/DoubanConversaionCorpus.zip?dl=0",,,,,,
904,Douban_Conversation_Corpus,Conversational Response Selection,Conversational Response Selection,Conversational Response Selection,,,Methodology,conversational-response-selection-on-douban,,https://github.com/MarkWuNLP/MultiTurnResponseSelection,https://paperswithcode.com/dataset/douban-conversation-corpus,"We release Douban Conversation Corpus, comprising a training data set, a development set and a test set for retrieval based chatbot. The statistics of Douban Conversation Corpus are shown in the following table. 

|      |Train|Val| Test         | 
| ------------- |:-------------:|:-------------:|:-------------:|
| session-response pairs  | 1m|50k| 10k |
| Avg. positive response per session     | 1|1| 1.18    | 
| Fless Kappa | N\A|N\A|0.41      | 
| Min turn per session | 3|3| 3      | 
| Max ture per session | 98|91|45    | 
| Average turn per session | 6.69|6.75|5.95    | 
| Average Word per utterance | 18.56|18.50|20.74   | 

The test data contains 1000 dialogue context, and for each context we create 10 responses as candidates. We recruited three labelers to judge if a candidate is a proper response to the session. A proper response means the response can naturally reply to the message given the context. Each pair received three labels and the majority of the labels was taken as the final decision.

<br>
As far as we known, this is the first human-labeled test set for retrieval-based chatbots. The entire corpus link https://www.dropbox.com/s/90t0qtji9ow20ca/DoubanConversaionCorpus.zip?dl=0

Data template
label \t conversation utterances (splited by \t) \t response",,,,,,
905,DPCSpell-Bangla-SEC-Corpus,Bangla Spelling Error Correction,Bangla Spelling Error Correction,Bangla Spelling Error Correction,,,Methodology,bangla-spelling-error-correction-on-dpcspell,MIT,https://github.com/mehedihasanbijoy/DPCSpell,https://paperswithcode.com/dataset/dpcspell-bangla-sec-corpus,MIT licenseDPCSpell-Bangla-SEC-Corpus is a large-scale parallel corpus for Bangla spelling error correction.,,,,,,
906,DPM,SemEval-2022 Task 4-1 (Binary PCL Detection),SemEval-2022 Task 4-1 (Binary PCL Detection),"SemEval-2022 Task 4-1 (Binary PCL Detection), SemEval-2022 Task 4-2 (Multi-label PCL Detection), Binary Condescension Detection, Multi-label Condescension Detection",Image,,Computer Vision,"multi-label-condescension-detection-on-dpm, semeval-2022-task-4-1-binary-pcl-detection-on, semeval-2022-task-4-2-multi-label-pcl, binary-condescension-detection-on-dpm",,,https://paperswithcode.com/dataset/dpm,Don’t Patronize Me! (DPM) is an annotated dataset with Patronizing and Condescending Language towards vulnerable communities.,,,,,,
907,DRACO20K,3D Reconstruction,3D Reconstruction,"3D Reconstruction, 3D Depth Estimation, 3D Pose Estimation","3D, Image",,Computer Vision,,MIT,https://aadilmehdis.github.io/DRACO-Project-Page/,https://paperswithcode.com/dataset/draco20k,"DRACO20K dataset is used for evaluating object canonicalization on methods that estimate a canonical frame from a monocular input image.

Provides:
1. Mixed Reality Multi-view RGB-D images rendered from ShapeNet objects
2. Camera poses
3. NOCS maps
4. Semantic 2D keypoints with visibility
5. Object-centric mask",,,,,,
908,DRCD,Machine Reading Comprehension,Machine Reading Comprehension,"Machine Reading Comprehension, Chinese Reading Comprehension, Reading Comprehension (Zero-Shot), Reading Comprehension (Few-Shot), Reading Comprehension (One-Shot), Question Answering, Reading Comprehension",Text,English,Natural Language Processing,"reading-comprehension-zero-shot-on-drcd, reading-comprehension-one-shot-on-drcd, chinese-reading-comprehension-on-drcd-1, reading-comprehension-few-shot-on-drcd, chinese-reading-comprehension-on-drcd",CC-BY-SA 3.0,https://github.com/DRCKnowledgeTeam/DRCD,https://paperswithcode.com/dataset/drcd,"Delta Reading Comprehension Dataset (DRCD) is an open domain traditional Chinese machine reading comprehension (MRC) dataset. This dataset aimed to be a standard Chinese machine reading comprehension dataset, which can be a source dataset in transfer learning. The dataset contains 10,014 paragraphs from 2,108 Wikipedia articles and 30,000+ questions generated by annotators.",,https://arxiv.org/pdf/1806.00920.pdf,https://arxiv.org/pdf/1806.00920.pdf,014 paragraphs,,
909,DRealSR,Blind Super-Resolution,Blind Super-Resolution,"Blind Super-Resolution, Image Super-Resolution, SSIM, Super-Resolution",Image,,Computer Vision,blind-super-resolution-on-drealsr,,https://github.com/xiezw5/Component-Divide-and-Conquer-for-Real-World-Image-Super-Resolution,https://paperswithcode.com/dataset/drealsr,"DRealSR establishes a Super Resolution (SR) benchmark with diverse real-world degradation processes, mitigating the limitations of conventional simulated image degradation. 

It has been collected from five DSLR cameras in natural scenes and cover indoor and outdoor scenes avoiding moving objects, e.g., advertising posters, plants, offices, buildings. The training images are cropped into 380×380, 272×272 and 192×192 patches, resulting in 31,970 patches.",,Component Divide-and-Conquer for Real-World Image Super-Resolution,https://arxiv.org/abs/2008.01928,,,
910,DREAM,Machine Reading Comprehension,Machine Reading Comprehension,"Machine Reading Comprehension, Question Answering, Reading Comprehension, Sleep spindles detection","Image, Text",English,Computer Vision,"machine-reading-comprehension-on-dream, sleep-spindles-detection-on-dreams-sleep","Custom (research-only, non-commercial)",https://dataset.org/dream/,https://paperswithcode.com/dataset/dream,"DREAM is a multiple-choice Dialogue-based REAding comprehension exaMination dataset. In contrast to existing reading comprehension datasets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding.

DREAM contains 10,197 multiple choice questions for 6,444 dialogues, collected from English-as-a-foreign-language examinations designed by human experts. DREAM is likely to present significant challenges for existing reading comprehension systems: 84% of answers are non-extractive, 85% of questions require reasoning beyond a single sentence, and 34% of questions also involve commonsense knowledge.",,,,,,
911,DRIFT,regression,regression,"regression, Satellite Image Classification, Remote Sensing Image Classification, Domain Adaptation, The Semantic Segmentation Of Remote Sensing Imagery",Image,,Computer Vision,,open for academic purposes,https://dgominski.github.io/drift/,https://paperswithcode.com/dataset/drift,"The DRIFT dataset includes 25k image patches collected in five European countries sourced from aerial and nanosatellite image archives. Each image patch is associated with three target variables to predict: 


Canopy height: average height value for pixels containing woody vegetation. 
Tree count: number of overstory (visible from an overhead perspective) trees in the images. 
Tree cover fraction: percentage of the image being covered by overstory tree crowns.

The DRIFT dataset includes significant shifts between label and visual distributions due to sensor and area differences. Furthermore, vegetation tends to grow to fit the local climate, therefore introducing concept drift in the data: same tree species may appear differently in different subsets. The label distribution also varies among different subsets (countries).

The dataset is a good choice for:


image-level regression
domain adaption for regression
remote sensing for forest applications",,,,,,
912,DrivAerNet,PDE Surrogate Modeling,PDE Surrogate Modeling,"PDE Surrogate Modeling, Physical Simulations, 3D Geometry Prediction, Parameter Prediction, Graph Regression, Physics-informed machine learning, 3D Shape Modeling, 3D Anomaly Detection and Segmentation","3D, Graph, Image, Time Series",,Computer Vision,3d-anomaly-detection-and-segmentation-on-1,,https://github.com/Mohamedelrefaie/DrivAerNet,https://paperswithcode.com/dataset/drivaernet,"DrivAerNet is a large-scale, high-fidelity CFD dataset of 3D industry-standard car shapes designed for data-driven aerodynamic design. It comprises 4000 high-quality 3D car meshes and their corresponding aerodynamic performance coefficients, alongside full 3D flow field information.

It includes:


CFD Simulation Data: The raw dataset, including full 3D pressure, velocity fields, and wall-shear stresses, computed using 8-16 million mesh elements has a total size of $\sim$ 16TB.
Curated CFD Simulations: For ease of access and use, a streamlined version of the CFD simulation data is provided, refined to include key insights and data, reducing the size to $\sim$ 1TB. 
3D Car Meshes: A total of 4000 designs, showcasing a variety of conventional car shapes and emphasizing the impact of minor geometric modifications on aerodynamic efficiency. The 3D meshes and aerodynamic coefficients $\sim$ 84GB.
2D slices include the car's wake in the $x$-direction and the symmetry plane in the $y$-direction $\sim$ 12GB.",,,,,,
913,DRIVE,Medical Image Segmentation,Medical Image Segmentation,"Medical Image Segmentation, Retinal Vessel Segmentation",Image,,Medical,"medical-image-segmentation-on-drive-1, retinal-vessel-segmentation-on-drive",CC-BY-4.0,https://drive.grand-challenge.org/,https://paperswithcode.com/dataset/drive,"The Digital Retinal Images for Vessel Extraction (DRIVE) dataset is a dataset for retinal vessel segmentation. It consists of a total of JPEG 40 color fundus images; including 7 abnormal pathology cases. The images were obtained from a diabetic retinopathy screening program in the Netherlands. The images were acquired using Canon CR5 non-mydriatic 3CCD camera with FOV equals to 45 degrees. Each image resolution is 584*565 pixels with eight bits per color channel (3 channels). 

The set of 40 images was equally divided into 20 images for the training set and 20 images for the testing set. Inside both sets, for each image, there is circular field of view (FOV) mask of diameter that is approximately 540 pixels. Inside training set, for each image, one manual segmentation by an ophthalmological expert has been applied. Inside testing set, for each image, two manual segmentations have been applied by two different observers, where the first observer segmentation is accepted as the ground-truth for performance evaluation.",,Ant Colony based Feature Selection Heuristics for Retinal Vessel Segmentation,https://arxiv.org/abs/1403.1735,40 images,training set and 20 images,
914,Drive_Act,Activity Recognition,Activity Recognition,"Activity Recognition, Autonomous Vehicles, Skeleton Based Action Recognition, Action Recognition","Image, Video",,Computer Vision,skeleton-based-action-recognition-on-drive,,https://www.driveandact.com/,https://paperswithcode.com/dataset/drive-act,"The Drive&Act dataset is a state of the art multi modal benchmark for driver behavior recognition. The dataset includes 3D skeletons in addition to frame-wise hierarchical labels of 9.6 Million frames captured by 6 different views and 3 modalities (RGB, IR and depth).

It offers following key features:


12h of video data in 29 long sequences
Calibrated multi view camera system with 5 views
Multi modal videos: NIR, Depth and Color data
Markerless motion capture: 3D Body Pose and Head Pose
Model of the static interior of the car
83 manually annotated hierarchical activity labels:
Level 1: Long running tasks (12)
Level 2: Semantic actions (34)
Level 3: Object Interaction tripplets [action|object|location] (6|17|14)",,,,,,
915,DropletVideo-10M,Video Generation,Video Generation,"Video Generation, Image to Video Generation","Image, Text, Video",English,Computer Vision,,CC BY-SA 4.0,https://huggingface.co/datasets/DropletX/DropletVideo-10M,https://paperswithcode.com/dataset/dropletvideo-10m,"DropletVideo is a project exploring high-order spatio-temporal consistency in image-to-video generation. It is trained on DropletVideo-10M. The model supports multi-resolution inputs, dynamic FPS control for motion intensity, and demonstrates potential for 3D consistency. The model supports multi-resolution inputs, dynamic FPS control for motion intensity, and demonstrates potential for 3D consistency. For further details, you can check our project page as well as the technical report.

Features:



Multi-resolution inputs， accommodating pixel values from 512x512x85（default 672x384x85） to 896x896x85（default 1120x640x85, and videos with different aspect ratios.



Dynamic FPS control for motion intensity.",,technical report,https://arxiv.org/abs/2503.06053,,,
916,Drosophila_Immunity_Time-Course_Data,Time Series Clustering,Time Series Clustering,Time Series Clustering,Time Series,,Time Series,,,https://github.com/sara-venkatraman/Bayesian-Gene-Dynamics,https://paperswithcode.com/dataset/drosophila-immunity-time-course-data,"The data used for all results in this paper can be found here. This directory contains:


GeneData.csv: Contains temporal gene expression measurements for 1735 genes at 17 time points. Measurements are provided as the $\log_2$-fold change from first time point. Hours corresponding to each time point are defined in the R script 3_Results.R in our GitHub repository. This dataset is derived from a larger gene expression dataset collected by Schlamp et al. (2021). 
PriorMatrix.csv: A 1735 x 1735 prior adjacency matrix. Each entry is 0, 1, or NA to indicate that a biological relationship between the corresponding two genes is unlikely, likely, or unknown according to external databases.

Further details about the collection of this data can be found in Section 4.1 and Appendix C of our paper. The R script 3_Results.R shows how these CSV files are read and used for our analysis.",2021,,,,,
917,DRTiD,Diabetic Retinopathy Grading,Diabetic Retinopathy Grading,Diabetic Retinopathy Grading,,,Methodology,,,https://github.com/fdu-vts/drtid,https://paperswithcode.com/dataset/drtid,"DRTiD is a benchmark dataset for DR grading, consisting of 3,100 two-field fundus images.",,Cross-Field Transformer for Diabetic Retinopathy Grading on Two-feld Fundus Images,https://arxiv.org/pdf/2211.14552v1.pdf,,,
918,DrugBank,Drug Response Prediction,Drug Response Prediction,"Drug Response Prediction, Drug–drug Interaction Extraction, Drug Discovery",Time Series,,Methodology,drug-drug-interaction-extraction-on-drugbank,,https://go.drugbank.com/,https://paperswithcode.com/dataset/drugbank,"Abstract:
First released in 2006, DrugBank (https://go.drugbank.com) has grown to become the 'gold standard' knowledge resource for drug, drug-target and related pharmaceutical information. DrugBank is widely used across many diverse biomedical research and clinical applications, and averages more than 30 million views/year. Since its last update in 2018, we have been actively enhancing the quantity and quality of the drug data in this knowledgebase. In this latest release (DrugBank 6.0), the number of FDA approved drugs has grown from 2646 to 4563 (a 72% increase), the number of investigational drugs has grown from 3394 to 6231 (a 38% increase), the number of drug-drug interactions increased from 365 984 to 1 413 413 (a 300% increase), and the number of drug-food interactions expanded from 1195 to 2475 (a 200% increase). In addition to this notable expansion in database size, we have added thousands of new, colorful, richly annotated pathways depicting drug mechanisms and drug metabolism. Likewise, existing datasets have been significantly improved and expanded, by adding more information on drug indications, drug-drug interactions, drug-food interactions and many other relevant data types for 11 891 drugs. We have also added experimental and predicted MS/MS spectra, 1D/2D-NMR spectra, CCS (collision cross section), RT (retention time) and RI (retention index) data for 9464 of DrugBank's 11 710 small molecule drugs. These and other improvements should make DrugBank 6.0 even more useful to a much wider research audience ranging from medicinal chemists to metabolomics specialists to pharmacologists.

More Information:

Paper: DrugBank 6.0: the DrugBank Knowledgebase for 2024 
PMID: 37953279 
PMCID: PMC10767804 
DOI: 10.1093/nar/gkad976

Knox, C., Wilson, M., Klinger, C. M., Franklin, M., Oler, E., Wilson, A., Pon, A., Cox, J., Chin, N. E. L., Strawbridge, S. A., Garcia-Patino, M., Kruger, R., Sivakumaran, A., Sanford, S., Doshi, R., Khetarpal, N., Fatokun, O., Doucet, D., Zubkowski, A., Rayat, D. Y., … Wishart, D. S. (2024). DrugBank 6.0: the DrugBank Knowledgebase for 2024. Nucleic acids research, 52(D1), D1265–D1275. https://doi.org/10.1093/nar/gkad976",2006,,,,,
919,Drunkard_s_Dataset,3D Pose Estimation,3D Pose Estimation,"3D Pose Estimation, Pose Estimation, Visual Tracking, Real-Time Visual Tracking, Simultaneous Localization and Mapping, Pose Tracking, Pose Prediction, 6D Pose Estimation using RGBD, Drone Pose Estimation, 6D Pose Estimation, Single-View 3D Reconstruction, 3D Reconstruction, Visual Odometry, Monocular Visual Odometry, 6D Pose Estimation using RGB","3D, Image, Time Series, Video",,Computer Vision,6d-pose-estimation-using-rgbd-on-drunkard-s,MIT,https://davidrecasens.github.io/TheDrunkard'sOdometry/,https://paperswithcode.com/dataset/drunkard-s-dataset,"Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. To tackle this issue with a common benchmark, we introduce the Drunkard’s Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality.",,,,,,
920,DS-1000,Memorization,Memorization,"Memorization, Code Generation",Text,English,Natural Language Processing,,Apache-2.0 license,https://ds1000-code-gen.github.io/,https://paperswithcode.com/dataset/ds-1000,"DS-1000 is a code generation benchmark with a thousand data science questions spanning seven Python libraries that (1) reflects diverse, realistic, and practical use cases, (2) has a reliable metric, (3) defends against memorization by perturbing questions.",,DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation,https://arxiv.org/abs/2211.11501,,,
921,DSD100,Audio Super-Resolution,Audio Super-Resolution,Audio Super-Resolution,Audio,,Audio,audio-super-resolution-on-dsd100,,https://sigsep.github.io/datasets/dsd100.html,https://paperswithcode.com/dataset/dsd100,"The dsd100 is a dataset of 100 full lengths of music tracks of different styles along with their isolated drums, bass, vocals, and other stems.

dsd100 contains two folders, a folder with a training set: ""train"", composed of 50 songs, and a folder with a test set: ""test"", composed of 50 songs. Supervised approaches should be trained on the training set and tested on both sets.

For each file, the mixture corresponds to the sum of all the signals. All signals are stereophonic and encoded at 44.1kHz.",,,,,,
922,DSO__OSN-transmitted_-_Facebook_,Image Manipulation Localization,Image Manipulation Localization,"Image Manipulation Localization, Image Manipulation, Image Forensics, Image Manipulation Detection",Image,,Computer Vision,image-manipulation-detection-on-dso-osn,,https://github.com/HighwayWu/ImageForensicsOSN,https://paperswithcode.com/dataset/dso-osn-transmitted,"This dataset is an OSN-transmitted (Online Social Network) version of the DSO dataset. Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",2022,,,,,
923,DSO__OSN-transmitted_-_Weibo_,Image Manipulation Detection,Image Manipulation Detection,Image Manipulation Detection,Image,,Computer Vision,image-manipulation-detection-on-dso-osn-3,,https://github.com/HighwayWu/ImageForensicsOSN,https://paperswithcode.com/dataset/dso-osn-transmitted-weibo,"This dataset is an OSN-transmitted (Online Social Network) version of the DSO dataset. Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",2022,,,,,
924,DSO__OSN-transmitted_-_Whatsapp_,Image Manipulation Detection,Image Manipulation Detection,Image Manipulation Detection,Image,,Computer Vision,image-manipulation-detection-on-dso-osn-2,,https://github.com/HighwayWu/ImageForensicsOSN,https://paperswithcode.com/dataset/dso-osn-transmitted-whatsapp,"This dataset is an OSN-transmitted (Online Social Network) version of the DSO dataset. Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",2022,,,,,
925,dSprites,Disentanglement,Disentanglement,Disentanglement,,,Methodology,,,https://github.com/deepmind/dsprites-dataset,https://paperswithcode.com/dataset/dsprites,"dSprites is a dataset of 2D shapes procedurally generated from 6 ground truth independent latent factors. These factors are color, shape, scale, rotation, x and y positions of a sprite.

All possible combinations of these latents are present exactly once, generating N = 737280 total images.",,,,,,
926,DSTC7_Task_1,Goal-Oriented Dialogue Systems,Goal-Oriented Dialogue Systems,"Goal-Oriented Dialogue Systems, Conversational Response Selection, Goal-Oriented Dialog",,,Methodology,conversational-response-selection-on-dstc7,,http://workshop.colips.org/dstc7/call.html,https://paperswithcode.com/dataset/dstc7-task-1,"The DSTC7 Task 1 dataset is a dataset and task for goal-oriented dialogue. The data originates from human-human conversations, which is built from online resources, specifically the Ubuntu Internet Relay Chat (IRC) channel and an Advising dataset from the University of Michigan.",,Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems,https://arxiv.org/abs/1907.01166,,,
927,DTD,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Neural Architecture Search, Image Classification, Image Clustering, Prompt Engineering, Few-Shot Learning, Classification, Transductive Zero-Shot Classification",Image,,Computer Vision,"image-classification-on-dtd, transductive-zero-shot-classification-on-dtd, prompt-engineering-on-dtd, classification-on-dtd, neural-architecture-search-on-dtd, zero-shot-learning-on-dtd, few-shot-learning-on-dtd, image-clustering-on-dtd",Custom (research-only),https://www.robots.ox.ac.uk/~vgg/data/dtd/,https://paperswithcode.com/dataset/dtd,The Describable Textures Dataset (DTD) contains 5640 texture images in the wild. They are annotated with human-centric attributes inspired by the perceptual properties of textures.,,Where is the Fake? Patch-Wise Supervised GANs for Texture Inpainting,https://arxiv.org/abs/1911.02274,,,
928,DTU,3D Reconstruction,3D Reconstruction,"3D Reconstruction, Point Clouds",3D,,Methodology,"point-clouds-on-dtu, 3d-reconstruction-on-dtu",Free,http://roboimagedata.compute.dtu.dk/?page_id=36,https://paperswithcode.com/dataset/dtu,"DTU MVS 2014 is a multi-view stereo dataset, which is an order of magnitude larger in number of scenes and with a significant increase in diversity. Specifically, it contains 80 scenes of large variability. Each scene consists of 49 or 64 accurate camera positions and reference structured light scans, all acquired by a 6-axis industrial robot.",2014,,,,,
929,DUC_2004,Text Summarization,Text Summarization,"Text Summarization, Extractive Text Summarization, Multi-Document Summarization",Text,English,Natural Language Processing,"text-summarization-on-duc-2004-task-1, extractive-text-summarization-on-duc-2004-1, extractive-text-summarization-on-duc-2004, multi-document-summarization-on-duc-2004",,https://duc.nist.gov/duc2004/,https://paperswithcode.com/dataset/duc-2004,"The DUC2004 dataset is a dataset for document summarization. Is designed and used for testing only. It consists of 500 news articles, each paired with four human written summaries. Specifically it consists of 50 clusters of Text REtrieval Conference (TREC) documents, from the following collections: AP newswire, 1998-2000; New York Times newswire, 1998-2000; Xinhua News Agency (English version), 1996-2000. Each cluster contained on average 10 documents.",1998,Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction,https://arxiv.org/abs/2005.01791,10 documents,,
930,DukeMTMC-reID,Style Transfer,Style Transfer,"Style Transfer, Unsupervised Person Re-Identification, Person Re-Identification, Generalizable Person Re-identification",Image,,Computer Vision,,,https://exposing.ai/duke_mtmc/,https://paperswithcode.com/dataset/dukemtmc-reid,"The DukeMTMC-reID (Duke Multi-Tracking Multi-Camera ReIDentification) dataset is a subset of the DukeMTMC for image-based person re-ID. The dataset is created from high-resolution videos from 8 different cameras. It is one of the largest pedestrian image datasets wherein images are cropped by hand-drawn bounding boxes. The dataset consists 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images.

NOTE: This dataset has been retracted.",,Deep Co-attention based Comparators For Relative Representation Learning in Person Re-identification,https://arxiv.org/abs/1804.11027,,"training images of 702 identities, 2,228 query images",
931,DukeMTMC-VideoReID,Video-Based Person Re-Identification,Video-Based Person Re-Identification,"Video-Based Person Re-Identification, Unsupervised Person Re-Identification, Person Re-Identification","Image, Video",,Computer Vision,"person-re-identification-on-dukemtmc, unsupervised-person-re-identification-on-11",,https://exposing.ai/duke_mtmc/,https://paperswithcode.com/dataset/dukemtmc-videoreid,"The DukeMTMC-VideoReID (Duke Multi-Tracking Multi-Camera Video-based ReIDentification) dataset is a subset of the DukeMTMC for video-based person re-ID. The dataset is created from high-resolution videos from 8 different cameras. It is one of the largest pedestrian video datasets wherein images are cropped by hand-drawn bounding boxes. The dataset consists 4832 tracklets of 1812 identities in total, and each tracklet has 168 frames on average.

NOTE: This dataset has been retracted.",,Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning,https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf,,,
932,Duke_Lung_Nodule_Dataset_2024,Lung Cancer Diagnosis,Lung Cancer Diagnosis,"Lung Cancer Diagnosis, Medical Diagnosis",,,Methodology,lung-cancer-diagnosis-on-duke-lung-nodule,,https://zenodo.org/records/10782891,https://paperswithcode.com/dataset/duke-lung-nodule-dataset-2024,"Background: Lung cancer risk classification is an increasingly important area of research as low-dose thoracic CT screening programs have become standard of care for patients at high risk for lung cancer. There is limited availability of large, annotated public databases for the training and testing of algorithms for lung nodule classification.

Methods: Screening chest CT scans done between January 1, 2015 and June 30, 2021 at Duke University Health System were considered for this study. Efficient nodule annotation was performed semi-automatically by using a publicly available deep learning nodule detection algorithm trained on the LUNA16 dataset to identify initial candidates, which were then accepted based on nodule location in the radiology text report or manually annotated by a medical student and a fellowship-trained cardiothoracic radiologist.

Results: The dataset contains 1613 CT volumes with 2487 annotated nodules. Radiologist spot-checking confirmed the semi-automated annotation had an accuracy rate of >90%.

Conclusions: The Duke Lung Nodule Dataset is the first large dataset for CT screening for lung cancer reflecting the use of current CT technology. This represents a useful resource of lung cancer risk classification research, and the efficient annotation methods described for its creation may be used to generate similar databases for research in the future.",2015,,,,,
933,DuLeMon,Text Matching,Text Matching,"Text Matching, Open-Domain Dialog, Conversational Response Generation",Text,English,Natural Language Processing,,MIT,https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2022-DuLeMon,https://paperswithcode.com/dataset/delemon,"DuLeMon is a large-scale Chinese Long-term Memory Conversation dataset, which simulates long-term memory conversations and focuses on the ability to actively construct and utilize the user's and the bot's persona in a long-term interaction. DuLeMon contains about 27.5k human-human conversations, 449k utterances, and 12k persona grounding sentences. This corpus can be used to explore Long-term Memory Conversation, Personalized Dialogue, and Persona Extraction / Matching / Retrieval.",,,,,,
934,Duolingo_Bandit_Notifications,Multi-Armed Bandits,Multi-Armed Bandits,Multi-Armed Bandits,,,Methodology,,CC BY-NC 4.0,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/23ZWVI,https://paperswithcode.com/dataset/duolingo-notifications-data,Replication datasets (200 million rows) used in experiments by Yancey & Settles (2020). (2019-06-11),2020,,,,,
935,Duolingo_SLAM_Shared_Task,Language Acquisition,Language Acquisition,Language Acquisition,Text,English,Natural Language Processing,,CC BY-NC 4.0,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/8SWHNO,https://paperswithcode.com/dataset/duolingo-slam-shared-task,"This repository contains gzipped files containing more than 2 million tokens (words) from answers submitted by more than 6,000 students over the course of their first 30 days of using Duolingo. It also contains baseline starter code written in Python. There are three data sets, corresponding to three different language courses. More details on the data set and task are available at: http://sharedtask.duolingo.com. (2018-01-10)",2018,,,,,
936,Duolingo_Spaced_Repetition_Data,Language Acquisition,Language Acquisition,Language Acquisition,Text,English,Natural Language Processing,,CC BY-NC 4.0,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/N8XJME,https://paperswithcode.com/dataset/duolingo-spaced-repetition-data,"This is a gzipped CSV file containing the 13 million Duolingo student learning traces used in experiments by Settles & Meeder (2016). For more details and replication source code, visit: https://github.com/duolingo/halflife-regression (2016-06-07)",2016,,,,,
937,Duolingo_STAPLE_Shared_Task,Multilingual NLP,Multilingual NLP,"Multilingual NLP, Translation, Paraphrase Generation, Machine Translation",Text,English,Natural Language Processing,,CC BY-NC 4.0,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/38OJR6,https://paperswithcode.com/dataset/duolingo-staple-shared-task,"This is the dataset for the 2020 Duolingo shared task on Simultaneous Translation And Paraphrase for Language Education (STAPLE). Sentence prompts, along with automatic translations, and high-coverage sets of translation paraphrases weighted by user response are provided in 5 language pairs. Starter code for this task can be found here: github.com/duolingo/duolingo-sharedtask-2020/. More details on the data set and task are available at: sharedtask.duolingo.com",2020,,,,,
938,DuReader,Open-Domain Question Answering,Open-Domain Question Answering,"Open-Domain Question Answering, Reading Comprehension (Zero-Shot), Reading Comprehension (Few-Shot), Reading Comprehension (One-Shot), Reading Comprehension",Text,English,Natural Language Processing,"reading-comprehension-zero-shot-on-dureader, reading-comprehension-few-shot-on-dureader, open-domain-question-answering-on-dureader, reading-comprehension-one-shot-on-dureader",,http://ai.baidu.com/broad/subordinate?dataset=dureader,https://paperswithcode.com/dataset/dureader,"DuReader is a large-scale open-domain Chinese machine reading comprehension dataset. The dataset consists of 200K questions, 420K answers and 1M documents. The questions and documents are based on Baidu Search and Baidu Zhidao. The answers are manually generated. The dataset additionally provides question type annotations – each question was manually annotated as either Entity, Description or YesNo and one of Fact or Opinion.",,https://arxiv.org/pdf/1711.05073v4.pdf,https://arxiv.org/pdf/1711.05073v4.pdf,1M documents,,
939,DurLAR,3D Depth Estimation,3D Depth Estimation,"3D Depth Estimation, Depth Estimation, Autonomous Driving",3D,,Methodology,,CC,https://github.com/l1997i/DurLAR,https://paperswithcode.com/dataset/durlar,"DurLAR is a high-fidelity 128-channel 3D LiDAR dataset with panoramic ambient (near infrared) and reflectivity imagery for multi-modal autonomous driving applications. Compared to existing autonomous driving task datasets, DurLAR has the following novel features:  


High vertical resolution LiDAR with 128 channels, which is twice that of any existing datasets, full 360 degree depth, range accuracy to ±2 cm at 20-50m.  
Ambient illumination (near infrared) and reflectivity panoramic imagery are made available in the Mono16 format (2048 × 128 resolution), with this being only dataset to make this provision.  
No rolling shutter effect, as our flash LiDAR captures all 128 channels simultaneously.  
Ambient illumination data is recorded via an on-board lux meter, which is again not available in previous datasets.  
High-fidelity GNSS/INS available via an onboard OxTS navigation unit operating at 100 Hz and receiving position and timing data from multiple GNSS con-stellations in addition to GPS.  
KITTI data format adopted as the de facto dataset format such that it can be parsed using both the DurLAR development kit and existing KITTI-compatible tools.   
Diversity over repeated locations such that the dataset has been collected under diverse environmental and weather conditions over the same driving route with additional variations in the time of day relative to environmental conditions.

Sensor placement


LiDAR: Ouster OS1-128 LiDAR sensor with 128 channels vertical resolution



Stereo Camera: Carnegie Robotics MultiSense S21 stereo camera with grayscale, colour, and IR enhanced imagers, 2048x1088 @ 2MP resolution



GNSS/INS: OxTS RT3000v3 global navigation satellite and inertial navigation system, supporting localization from GPS, GLONASS, BeiDou, Galileo, PPP and SBAS constellations



Lux Meter: Yocto Light V3, a USB ambient light sensor (lux meter), measuring ambient light up to 100,000 lux",2048,,,,,
940,DUT-OMRON,Unsupervised Saliency Detection,Unsupervised Saliency Detection,"Unsupervised Saliency Detection, Saliency Detection, RGB Salient Object Detection, Salient Object Detection",Image,,Computer Vision,"salient-object-detection-on-dut-omron, salient-object-detection-on-dut-omron-2, saliency-detection-on-dut-omron, unsupervised-saliency-detection-on-dut-omron",,http://saliencydetection.net/dut-omron/,https://paperswithcode.com/dataset/dut-omron,"The DUT-OMRON dataset is used for evaluation of Salient Object Detection task and it contains 5,168 high quality images. The images have one or more salient objects and relatively cluttered background.",,Global Context-Aware Progressive Aggregation Network for Salient Object Detection,https://arxiv.org/abs/2003.00651,,"valuation of Salient Object Detection task and it contains 5,168 high quality images",
941,DUTS,Unsupervised Saliency Detection,Unsupervised Saliency Detection,"Unsupervised Saliency Detection, RGB Salient Object Detection, Salient Object Detection, Unsupervised Object Segmentation, Saliency Detection",Image,,Computer Vision,"unsupervised-saliency-detection-on-duts, unsupervised-object-segmentation-on-duts, salient-object-detection-on-duts-te, salient-object-detection-on-duts-te-1, saliency-detection-on-duts-test",,http://saliencydetection.net/duts/,https://paperswithcode.com/dataset/duts,"DUTS is a saliency detection dataset containing 10,553 training images and 5,019 test images. All training images are collected from the ImageNet DET training/val sets, while test images are collected from the ImageNet DET test set and the SUN data set. Both the training and test set contain very challenging scenarios for saliency detection. Accurate pixel-level ground truths are manually annotated by 50 subjects.",,,,,"training images and 5,019 test images",
942,DVQA,Object Detection,Object Detection,"Object Detection, Visual Question Answering (VQA), Question Answering, Chart Question Answering","Image, Text",English,Computer Vision,visual-question-answering-vqa-on-dvqa-test,Attribution-NonCommercial 4.0 International,https://kushalkafle.com/projects/dvqa.html,https://paperswithcode.com/dataset/dvqa,DVQA is a synthetic question-answering dataset on images of bar-charts.,,,,,,
943,DVS128_Gesture,Object Recognition,Object Recognition,"Object Recognition, Gesture Generation, Image Classification, Action Recognition, Gesture Recognition, Event data classification, Question Answering","Image, Text, Video",English,Computer Vision,"gesture-generation-on-dvs128-gesture, event-data-classification-on-dvs128-gesture, action-recognition-on-dvs128-gesture, object-recognition-on-dvs128-gesture, image-classification-on-dvs128-gesture, gesture-recognition-on-dvs128-gesture",,http://research.ibm.com/dvsgesture/,https://paperswithcode.com/dataset/dvs128-gesture-dataset,Comprises 11 hand gesture categories from 29 subjects under 3 illumination conditions.,,,,,,
944,DWIE,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Document-level Closed Information Extraction, Entity Linking, Document-level Relation Extraction, Relation Extraction, Named Entity Recognition (NER)","Graph, Image, Text",English,Computer Vision,"document-level-closed-information-extraction-1, named-entity-recognition-on-dwie, coreference-resolution-on-dwie, relation-extraction-on-dwie, document-level-relation-extraction-on-dwie",GPL-3.0 License,https://github.com/klimzaporojets/DWIE,https://paperswithcode.com/dataset/dwie,"The 'Deutsche Welle corpus for Information Extraction' (DWIE) is a multi-task dataset that combines four main Information Extraction (IE) annotation sub-tasks: (i) Named Entity Recognition (NER), (ii) Coreference Resolution, (iii) Relation Extraction (RE), and (iv) Entity Linking. DWIE is conceived as an entity-centric dataset that describes interactions and properties of conceptual entities on the level of the complete document.",,https://arxiv.org/abs/2009.12626,https://arxiv.org/abs/2009.12626,,,
945,Dynamic_FAUST,3D Reconstruction,3D Reconstruction,"3D Reconstruction, 3D Shape Representation, 3D Human Reconstruction","3D, Image",,Computer Vision,3d-human-reconstruction-on-dynamic-faust,"Custom (research-only, non-commercial)",http://dfaust.is.tue.mpg.de,https://paperswithcode.com/dataset/dynamic-faust,"Dynamic FAUST extends the FAUST dataset to dynamic 4D data. It consists of high-resolution 4D scans of human subjects in motion, captured at 60 fps.",,,,,,
946,Dynamic_OLAT_Dataset,Single-Image Portrait Relighting,Single-Image Portrait Relighting,"Single-Image Portrait Relighting, Image Relighting",Image,,Computer Vision,,,https://zhanglongwen.com/projects/nvpr/dataset.html,https://paperswithcode.com/dataset/dynamic-olat-dataset,"To provide ground truth supervision for video consistency modeling, we build up a high-quality dynamic OLAT dataset.
Our capture system consists of a light stage setup with 114 LED light sources and Phantom Flex4K-GS camera (global shutter, stationary 4K ultra-high-speed camera at 1000 fps), resulting in dynamic OLAT imageset recording at 25 fps using the overlapping method.
Our dynamic OLAT dataset provides sufficient semantic, temporal and lighting consistency supervision to train our neural video portrait relighting scheme, which can generalize to in-the-wild scenarios.",,Unknown,https://zhanglongwen.com/projects/nvpr/LICENSE.pdf,,,
947,E-commerce,Conversational Response Selection,Conversational Response Selection,Conversational Response Selection,,,Methodology,conversational-response-selection-on-e,,https://github.com/cooelf/DeepUtteranceAggregation,https://paperswithcode.com/dataset/e-commerce-1,"We release E-commerce Dialogue Corpus, comprising a training data set, a development set and a test set for retrieval based chatbot. The statistics of E-commerical Conversation Corpus are shown in the following table. 

|      |Train|Val| Test         |
| ------------- |:-------------:|:-------------:|:-------------:|
| Session-response pairs  | 1m|10k| 10k |
| Avg. positive response per session|1|1|1|
| Min turn per session|3|3|3|
| Max ture per session|10|10|10|
| Average turn per session|5.51|5.48|5.64
| Average Word per utterance|7.02|6.99|7.11

The full corpus can be downloaded from https://drive.google.com/file/d/154J-neBo20ABtSmJDvm7DK0eTuieAuvw/view?usp=sharing.",,,,,,
948,E-GMD,Drum Transcription,Drum Transcription,Drum Transcription,,,Methodology,,,https://storage.googleapis.com/magentadata/papers/e-gmd/index.html,https://paperswithcode.com/dataset/e-gmd,"Expanded Groove MIDI dataset (E-GMD) is an automatic drum transcription (ADT) dataset that contains 444 hours of audio from 43 drum kits, making it an order of magnitude larger than similar datasets, and the first with human-performed velocity annotations.",,Improving Perceptual Quality of Drum Transcription with the Expanded Groove MIDI Dataset,https://arxiv.org/pdf/2004.00188,,,
949,E-KAR,Explanation Generation,Explanation Generation,"Explanation Generation, Question Answering",Text,English,Natural Language Processing,,CC BY NC SA 4.0,https://ekar-leaderboard.github.io,https://paperswithcode.com/dataset/e-kar,"The ability to recognize analogies is fundamental to human cognition. Existing benchmarks to test word analogy do not reveal the underneath process of analogical reasoning of neural models.

Holding the belief that models capable of reasoning should be right for the right reasons, we propose a first-of-its-kind Explainable Knowledge-intensive Analogical Reasoning benchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in English) problems sourced from the Civil Service Exams, which require intensive background knowledge to solve. More importantly, we design a free-text explanation scheme to explain whether an analogy should be drawn, and manually annotate them for each and every question and candidate answer.",,,,,,
950,E-ReDial,Explanation Generation,Explanation Generation,"Explanation Generation, Recommendation Systems",Text,English,Natural Language Processing,,Apache-2.0,https://github.com/Superbooming/E-Redial,https://paperswithcode.com/dataset/e-redial,"E-ReDial is a conversational recommender system dataset with high-quality explanations. It consists of 756 dialogues with 12,003 utterances, each with 15.9 turns on average. 2,058 high-quality explanations are included, each with 79.2 tokens on average.",,,,,,
951,e-SNLI-VE,Explanation Generation,Explanation Generation,"Explanation Generation, Explainable artificial intelligence, Visual Entailment","Image, Text",English,Computer Vision,"visual-entailment-on-e-snli-ve, explanation-generation-on-e-snli-ve",,https://github.com/maximek3/e-ViL,https://paperswithcode.com/dataset/e-snli-ve,e-SNLI-VE is a large VL (vision-language) dataset with NLEs (natural language explanations) with over 430k instances for which the explanations rely on the image content. It has been built by merging the explanations from e-SNLI and the image-sentence pairs from SNLI-VE.,,,,430k instances,,
952,e-ViL,Explainable artificial intelligence,Explainable artificial intelligence,Explainable artificial intelligence,,,Methodology,,Multiple licenses,https://github.com/maximek3/e-ViL,https://paperswithcode.com/dataset/e-vil,"e-ViL is a benchmark for explainable vision-language tasks. e-ViL spans across three datasets of human-written NLEs (natural language explanations), and provides a unified evaluation framework that is designed to be re-usable for future works.

This benchmark uses the following datasets: e-SNLI-VE, VCR, VQA-X.",,,,,,
953,e2006,regression,regression,regression,,,Methodology,,,https://www.cs.cmu.edu/~ark/10K/,https://paperswithcode.com/dataset/e2006,"From the official description:


The corpus contains 10-K reports from many US companies during years
1996-2006, as well as measured volatility of stock returns for the
twelve-month periods preceding and following each report.  The data
are organized by the year of the report.",1996,,,,,
954,E2E,Text Generation,Text Generation,"Text Generation, Language Modelling, Data-to-Text Generation, Table-to-Text Generation","Tabular, Text",English,Natural Language Processing,"data-to-text-generation-on-e2e, table-to-text-generation-on-e2e, data-to-text-generation-on-e2e-nlg-challenge, data-to-text-generation-on-cleaned-e2e-nlg-1",CC BY-SA 4.0,http://www.macs.hw.ac.uk/InteractionLab/E2E/,https://paperswithcode.com/dataset/e2e,"End-to-End NLG Challenge (E2E) aims to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena.",,,,,,
955,EARS-Reverb,Speech Dereverberation,Speech Dereverberation,Speech Dereverberation,Audio,,Speech,speech-dereverberation-on-ears-reverb,CC-NC 4.0 International license,https://sp-uhh.github.io/ears_dataset/,https://paperswithcode.com/dataset/ears-reverb,"The EARS-Reverb dataset uses real recorded room impulse responses (RIRs) from multiple public datasets (ACE-Challenge, AIR, ARNI, BRUDEX, dEchorate, DetmoldSRIR, and Palimpsest).  All RIRs are fullband, and a randomly selected channel for multi-channel recordings is used. The reverberant speech is generated by convolving the clean speech with the RIR. To avoid a time delay between the reverberant and clean speech signal caused by the direct path of the RIR, the beginning of the RIR is cut off up to the index with the highest amplitude. Only RIRs with an RT60 reverberation time that does not exceed 2 s are used. Finally, the loudness of the reverberant speech is normalized to the loudness of the clean speech using the loudness K-weighted relative to full scale (LKFS).",,,,,,
956,EarthNet2021,Video Prediction,Video Prediction,"Video Prediction, Earth Surface Forecasting, Time Series Forecasting, Video Forensics, Multivariate Time Series Forecasting, Time Series Prediction","Time Series, Video",,Methodology,"earth-surface-forecasting-on-earthnet2021-1, earth-surface-forecasting-on-earthnet2021, earth-surface-forecasting-on-earthnet2021-iid, earth-surface-forecasting-on-earthnet2021-ood",CC-BY-NC-SA 4.0,https://www.earthnet.tech/docs/ds-download/,https://paperswithcode.com/dataset/earthnet2021,"Satellite images are snapshots of the Earth surface. We propose to forecast them. We frame Earth surface forecasting as the task of predicting satellite imagery conditioned on future weather. EarthNet2021 is a large dataset suitable for training deep neural networks on the task. It contains Sentinel~2 satellite imagery at $20$~m resolution, matching topography and mesoscale ($1.28$~km) meteorological variables packaged into $32000$ samples. Additionally we frame EarthNet2021 as a challenge allowing for model intercomparison. Resulting forecasts will greatly improve ($>\times50$) over the spatial resolution found in numerical models. This allows localized impacts from extreme weather to be predicted, thus supporting downstream applications such as crop yield prediction, forest health assessments or biodiversity monitoring. Find data, code, and how to participate at www.earthnet.tech.",,,,,"training deep neural networks on the task. It contains Sentinel~2 satellite imagery at $20$~m resolution, matching topography and mesoscale ($1.28$~km) meteorological variables packaged into $32000$ samples",
957,Earth_on_Canvas,Cross-Domain Few-Shot,Cross-Domain Few-Shot,"Cross-Domain Few-Shot, Cross-Modal Retrieval, Zero-Shot Cross-Modal Retrieval",,,Methodology,,Creative Commons,https://ieee-dataport.org/open-access/earth-canvas-6,https://paperswithcode.com/dataset/ushasi-chaudhuri,"A Zero-Shot Sketch-based Inter-Modal Object Retrieval Scheme for Remote Sensing Images

WITH the advancement in sensor technology, huge amounts of data are being collected from various satellites. Hence, the task of target-based data retrieval and acquisition has become exceedingly challenging. Existing satellites essentially scan a vast overlapping region of the Earth using various sensing techniques, like multi-spectral, hyperspectral, Synthetic Aperture Radar (SAR), video, and compressed sensing, to name a few. With increasing complexity and different sensing techniques at our disposal, it has become our primary interest to design efficient algorithms to retrieve data from multiple data modalities, given the complementary information that is captured by different sensors. This type of problem is referred to as inter-modal data retrieval. In remote sensing (RS), there are primarily two important types of problems, i.e., land-cover classification and object detection. In this work, we focus on the target-based object retrieval part, which falls under the realm of object detection in RS. Object retrieval essentially requires high-resolution imagery for objects to be distinctly visible in the image. The main challenge with the conventional retrieval approach using large-scale databases is that, quite often, we do not have any query image sample of the target class at our disposal. The target of interest solely exists as a perception to the user in the form of an imprecise sketch. In such situations where a photo query is absent, it can be immensely useful if we can promptly make a quick hand-made sketch of the target. Sketches are a highly symbolic and hieroglyphic representation of data. One can exploit the notion of this minimalistic representative of sketch queries for sketch-based image retrieval (SBIR) framework. While dealing with satellite images, it is imperative to collect as many samples of images as possible for each object class for object recognition with a high success rate. However, in general, there exists a considerable number of classes for which we seldom have any training data samples. Therefore, for such classes, we can use the zero-shot learning (ZSL) strategy. The ZSL approach aims to solve a task without receiving any example of that task during the training phase. This makes the network capable of handling an unseen class (new class) sample obtained during the inference phase upon deployment of the network. Hence, we propose the aerial sketch-image dataset, namely Earth on Canvas dataset.

Classes in this dataset:
Airplane, Baseball Diamond, Buildings, Freeway, Golf Course, Harbor, Intersection, Mobile home park, Overpass, Parking lot,  River, Runway, Storage tank, Tennis court.",,,,,"val and acquisition has become exceedingly challenging. Existing satellites essentially scan a vast overlapping region of the Earth using various sensing techniques, like multi-spectral, hyperspectral, Synthetic Aperture Radar (SAR), video, and compressed sensing, to name a few. With increasing complexity and different sensing techniques at our disposal, it has become our primary interest to design efficient algorithms to retrieve data from multiple data modalities, given the complementary information that is captured by different sensors. This type of problem is referred to as inter-modal data retrieval. In remote sensing (RS), there are primarily two important types of problems, i.e., land-cover classification and object detection. In this work, we focus on the target-based object retrieval part, which falls under the realm of object detection in RS. Object retrieval essentially requires high-resolution imagery for objects to be distinctly visible in the image. The main challenge with the conventional retrieval approach using large-scale databases is that, quite often, we do not have any query image sample of the target class at our disposal. The target of interest solely exists as a perception to the user in the form of an imprecise sketch. In such situations where a photo query is absent, it can be immensely useful if we can promptly make a quick hand-made sketch of the target. Sketches are a highly symbolic and hieroglyphic representation of data. One can exploit the notion of this minimalistic representative of sketch queries for sketch-based image retrieval (SBIR) framework. While dealing with satellite images",
958,EasyCom,Active Speaker Localization,Active Speaker Localization,"Active Speaker Localization, Face Clustering, Speech Recognition, Speech Enhancement","Audio, Image, Text",English,Computer Vision,"active-speaker-localization-on-easycom, speech-recognition-on-easycom, speech-enhancement-on-easycom, face-clustering-on-easycom",CC BY-NC 4.0,https://github.com/facebookresearch/EasyComDataset,https://paperswithcode.com/dataset/easycom,"The Easy Communications (EasyCom) dataset is a world-first dataset designed to help mitigate the cocktail party effect from an augmented-reality (AR) -motivated multi-sensor egocentric world view. The dataset contains AR glasses egocentric multi-channel microphone array audio, wide field-of-view RGB video, speech source pose, headset microphone audio, annotated voice activity, speech transcriptions, head and face bounding boxes and source identification labels. We have created and are releasing this dataset to facilitate research in multi-modal AR solutions to the cocktail party problem.",,,,,,
959,EBM-NLP,Participant Intervention Comparison Outcome Extraction,Participant Intervention Comparison Outcome Extraction,Participant Intervention Comparison Outcome Extraction,,,Methodology,participant-intervention-comparison-outcome,,,https://paperswithcode.com/dataset/ebm-nlp,"EBM-NLP annotates PICO (Participants, Interventions, Comparisons and Outcomes) spans in clinical trial abstracts. 
The corresponding PICO Extraction task aims to identify the spans in clinical trial abstracts that describe the respective PICO elements.",,,,,,
960,EC-FUNSD,Semantic entity labeling,Semantic entity labeling,"Semantic entity labeling, Entity Linking",,,Methodology,"semantic-entity-labeling-on-ec-funsd, entity-linking-on-ec-funsd",CC-BY-4.0,https://github.com/chongzhangFDU/ROOR-Datasets,https://paperswithcode.com/dataset/ec-funsd,"EC-FUNSD is introduced in  [arXiv:2402.02379] as a benchmark of semantic entity recognition (SER) and entity linking (EL), designed for the entity-centric robustness evaluation of pre-trained text-and-layout models (PTLMs).

In practical applications of document intelligence, PTLMs (e.g. the LayoutLM series) generally serve as the encoder of document layouts, similar to the role played by pre-trained contextualized language models (e.g. the BERT series) in NLP tasks.
Conventionally, information extraction (IE) ability of PTLMs is evaluated via SER and EL, esp. in a sequence-labeling manner. 
The performance of PTLMs on these tasks reflects the capacity of their layout embeddings to facilitate downstream IE tasks.
However, the prevailing benchmarks do not fully conform to the aforementioned evaluation pipeline, thereby diminishing the reliability of the assessment. Take FUNSD as an example, its block-level annotation falsely couples the annotations of segment and entity, which does not adequately represent semantic-driven entities and hinder the fair evaluation. 

The propose of EC-FUNSD aims to provide a fair and unbiased evaluation benchmark of IE ability of PTLMs. 
The construction of this dataset includes the revision of layout and IE annotations from FUNSD. 
First, the original layout annotation of FUNSD is cleaned, and multiple-row blocks are split into row-wise segments.
Second, the semantic entities are re-annotated together with their linking relationships, with the segment order preserved to ensure that each entity is represented as a continuous word span within layout, making the form of this dataset suitable for sequence-labeling models. 
The final dataset consists of 199 document samples including the image, layout annotation of segments and words, and labeled entities of 3 categories. 
For the detailed annotation process and statistics, please refer to the original paper.",,[arXiv:2402.02379],https://arxiv.org/abs/2402.02379,,,3
961,ECB_,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Event Cross-Document Coreference Resolution, Entity Cross-Document Coreference Resolution, Event Coreference Resolution",Text,English,Natural Language Processing,"entity-cross-document-coreference-resolution, event-cross-document-coreference-resolution",Creative Commons Attribution 4.0 International,http://www.newsreader-project.eu/results/data/the-ecb-corpus/,https://paperswithcode.com/dataset/ecb,"The ECB+ corpus is an extension to the EventCorefBank (ECB, Bejan and Harabagiu, 2010). A newly added corpus component consists of 502 documents that belong to the 43 topics of the ECB but that describe different seminal events than those already captured in the ECB. All corpus texts were found through Google Search and were annotated with mentions of events and their times, locations, human and non-human participants as well as with within- and cross-document event and entity coreference information. The 2012 version of annotation of the ECB corpus (Lee et al., 2012) was used as a starting point for re-annotation of the ECB according to the ECB+ annotation guideline.

The major differences with respect to the 2012 version of annotation of the ECB are:

(a) five event components are annotated in text:

actions (annotation tags starting with ACTION and NEG)
times (annotation tags starting with TIME)
locations (annotation tags starting with LOC)
human participants (annotation tags starting with HUMAN)
non-human participants (annotation tags starting with NON_HUMAN)

(b) specific action classes and entity subtypes are distinguished for each of the five main event components resulting in a total tagset of 30 annotation tags based on ACE annotation guidelines (LDC 2008), TimeML (Pustejovsky et al., 2003 and Sauri et al., 2005)
(c) intra- and cross-document coreference relations between mentions of the five event components were established:

INTRA_DOC_COREF tag captures within document coreference chains that do not participate in cross-document relations; within document coreference was annotated by means of the CAT tool (Bartalesi et al., 2012)
CROSS_DOC_COREF tag indicates cross-document coreference relations created in the CROMER tool (Girardi et al., 2014); all coreference branches refer by means of relation target IDs to the so called TAG_DESCRIPTORS, pointing to human friendly instance names (assigned by coders) and also to instance_id-s

(d) events are annotated from an “event-centric” perspective, i.e. annotation tags are assigned depending on the role a mention plays in an event (for more information see ECB+ references).",2010,,,502 documents,,
962,ECG200,Time Series Classification,Time Series Classification,"Time Series Classification, Early  Classification","Image, Time Series",,Time Series,"early-classification-on-ecg200, time-series-classification-on-ecg200",,,https://paperswithcode.com/dataset/ecg200,ECG200,,,,,,
963,ECG5000,Time Series Classification,Time Series Classification,"Time Series Classification, Outlier Detection, Unsupervised Anomaly Detection","Image, Time Series",,Time Series,"unsupervised-anomaly-detection-on-ecg5000, time-series-classification-on-ecg5000, outlier-detection-on-ecg5000",,http://www.timeseriesclassification.com/description.php?Dataset=ECG5000,https://paperswithcode.com/dataset/ecg5000,"The original dataset for ""ECG5000"" is a 20-hour long ECG downloaded from Physionet. The name is BIDMC Congestive Heart Failure Database(chfdb) and it is record ""chf07"". It was originally published in ""Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh, Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals. Circulation 101(23)"". The data was pre-processed in two steps: (1) extract each heartbeat, (2) make each heartbeat equal length using interpolation. This dataset was originally used in paper ""A general framework for never-ending learning from time series streams"", DAMI 29(6). After that, 5,000 heartbeats were randomly selected. The patient has severe congestive heart failure and the class values were obtained by automated annotation",,,,,,
964,ECG_Heartbeat_Categorization_Dataset,Time Series Classification,Time Series Classification,"Time Series Classification, ECG Classification","Image, Time Series",,Time Series,,,https://www.kaggle.com/shayanfazeli/heartbeat,https://paperswithcode.com/dataset/ecg-heartbeat-categorization-dataset,"This dataset is composed of two collections of heartbeat signals derived from two famous PhysioNet datasets in heartbeat classification, the MIT-BIH Arrhythmia Dataset and the PTB Diagnostic ECG Database. The number of samples in both collections is large enough for training a deep neural network.

This dataset has been used in exploring heartbeat classification using deep neural network architectures, and observing some of the capabilities of transfer learning on it. The signals correspond to electrocardiogram (ECG) shapes of heartbeats for the normal case and the cases affected by different arrhythmias and myocardial infarction. These signals are preprocessed and segmented, with each segment corresponding to a heartbeat.",,,,,,
965,ECHR,Binary text classification,Binary text classification,"Binary text classification, Multi-Label Classification","Image, Text",English,Computer Vision,binary-text-classification-on-echr-non,,https://archive.org/details/ECHR-ACL2019,https://paperswithcode.com/dataset/echr,"ECHR is an English legal judgment prediction dataset of cases from the European Court of Human Rights (ECHR). The dataset contains ~11.5k cases, including the raw text.

For each case, the dataset provides a list of facts extracted using regular expressions from the case description. Each case is also mapped to articles of the Convention that were violated (if any). An importance score is also assigned by ECHR.",,Neural Legal Judgment Prediction in English,https://arxiv.org/abs/1906.02059,,,
966,Ecoli,Small Data Image Classification,Small Data Image Classification,"Small Data Image Classification, Outlier Detection, Imputation",Image,,Computer Vision,,,https://archive.ics.uci.edu/ml/datasets/ecoli,https://paperswithcode.com/dataset/ecoli,The Ecoli dataset is a dataset for protein localization. It contains 336 E.coli proteins split into 8 different classes.,,,,,,
967,EconLogicQA,Sentence Ordering,Sentence Ordering,Sentence Ordering,,,Methodology,sentence-ordering-on-econlogicqa,CC BY-NC-SA 4.0,https://huggingface.co/datasets/yinzhu-quan/econ_logic_qa,https://paperswithcode.com/dataset/econlogicqa,"EconLogicQA is a benchmark designed to test the sequential reasoning skills of large language models (LLMs) in economics, business, and supply chain management. It diverges from typical benchmarks by requiring models to understand and sequence multiple interconnected events, capturing complex economic logics. The benchmark includes multi-event scenarios and a thorough suite of evaluations to assess proficiency in economic contexts.",,,,,,
968,ECSSD,Unsupervised Saliency Detection,Unsupervised Saliency Detection,"Unsupervised Saliency Detection, RGB Salient Object Detection, Salient Object Detection, Unsupervised Object Segmentation, Saliency Detection",Image,,Computer Vision,"salient-object-detection-on-ecssd, saliency-detection-on-ecssd, unsupervised-object-segmentation-on-ecssd, salient-object-detection-on-ecssd-1, unsupervised-saliency-detection-on-ecssd",,https://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html,https://paperswithcode.com/dataset/ecssd,"The Extended Complex Scene Saliency Dataset (ECSSD) is comprised of complex scenes, presenting textures and structures common to real-world images. ECSSD contains 1,000 intricate images and respective ground-truth saliency maps, created as an average of the labeling of five human participants.",,SAD: Saliency-based Defenses Against Adversarial Examples,https://arxiv.org/abs/2003.04820,,,
969,edeniss2020,Time Series,Time Series,"Time Series, Anomaly Classification, Time Series Forecasting, Time Series Clustering, Anomaly Detection, Time Series Anomaly Detection, Anomaly Forecasting, Time Series Analysis","Image, Time Series",,Time Series,,Creative Commons Attribution 4.0 International,https://doi.org/10.5281/zenodo.11485183,https://paperswithcode.com/dataset/edeniss2020,"Overview
The edeniss2020 dataset is a time series dataset. It consists of equidistant sensor readings stemming from 97 sensors in the EDEN ISS research greenhouse.

EDEN ISS was a (almost) closed loop research greenhouse build under the lead of the German Aerospace Center to study Controlled Environment Agriculture (CEA) techniques and plant growth for future long-term space missions. EDEN ISS was deployed in Antarctica in 2018 next to the german Neumayer III polar station and has been in operation for four years. 

The data contained in the edeniss2020 dataset was recorded during the third mission year in 2020 between 2020/01/01 and 2020/12/30. Every sensor within the dataset is related to one of the following Subsystems
|Acronym | Description |
|--|--|
| AMS&#x2011;FEG | Atmosphere Management System (AMS) of the Future Exploration Greenhouse (FEG) |
| AMS&#x2011;SES | Atmosphere Management System AMS of the Service Section (SES)|
| ICS | Illumination Control System |
| NDS | Nutrient Delivery System |
| TCS | Thermal Control System|

Specification
|Item|Description  |
|--|--|
| Number of Files | 97 |
| Start Date | 2020/01/01 00:00:05 |
| End Date | 2020/12/30 23:55:00 |
| Sampling Rate | 1/300 Hz (5min) |

Contents
The dataset includes the following files:
- ams-feg/*.csv: Sensor readings related to the AMS-FEG
- ams-ses/*.csv: Sensor readings related to the AMS-SES
- ics/*.csv: Temperature readings related to the ICS
- nds/*.csv: Sensor readings related to the NDS
- tcs/*.csv: Sensor readings related to the 
- edeniss2020.csv: Description and Units of the measurements.
- README.md: This file.

| Subsystem | Sensor | #sensors | Note |
|--|--|--|--|
| AMS-FEG | CO2 | 2 |  |
|  | Photosynthetic Active Radiation (PAR) | 2 |   |
|  | Relative Humidity (RH) | 2 |   |
|  | Temperature (T) | 3 |   |
| AMS-SES | CO2 | 2 |
|  | Photosynthetic Active Radiation (PAR) | 1 |   |
|  | Relative Humidity (RH) | 1 |   |
|  | Temperature (T) | 3 |   |
|  | Vapor Pressure Deficit (VPD) | 1 |   |
| ICS | Temperature (T) | 38 | Measured at the LED lamp above each growth tray |  |
| NDS | Electrical Conductivity (EC) | 4 | EC of the nutrient solutions |
|  | Level (H) | 2 | Level of the solution in the nutrient solution tanks |
|  | PH-Value (PH) | 4 | PH Value of the nutrient solutions | 
|  | Pressure (P) | 8 | Pressure in the piping from the tanks to the growth racks in the FEG |
|  | Temperature (T) | 4 | Temperature of the nutrient solutions. |
|  | Volume (V) | 2 | Volume up to the level sensor |
| TCS | Pressure (P) | 3 |
|  | Relative Humidity (RH) | 2 |
|  | Temperature (T) | 12 |
|  | Valve (VALVE) | 3 |

Usage
The dataset can be used for uni- and multivariate analysis. 

To read a uni-variate time series with pandas do:

```python
import pandas as pd

Example of loading ams-feg/co2-1.csv
data = pd.read_csv('./ams-feg/co2-1.csv')
print(data.head())

```

To read the data for a whole subsystem (e.g. AMS-FEG) as a multivariate time series:
```python
import pandas as pd
import glob

SUBSYSTEM = 'ams-feg'

combined_df = pd.DataFrame()
for i, file in enumerate(glob.glob(f'./{SUBSYSTEM}/*.csv')):
    df = pd.read_csv(file, header=0, usecols=[0, 1])
    if i == 0:
        combined_df = df
    else:
        combined_df = pd.merge(combined_df, df, on='time')

combined_df = combined_df.sort_values(by='time').reset_index(drop=True)

print(combined_df)
```

License
Creative Commons Attribution 4.0 International (CC BY 4.0 Deed)

Citation
If you use this dataset, please cite it as follows:
Rewicki, F., Norman, T., Vrakking, V., (2024). edeniss2020. Zenodo. http://doi.org/10.5281/zenodo.11485183

Contact
Ferdinand Rewicki: &#102;&#101;&#114;&#100;&#105;&#110;&#97;&#110;&#100;&#46;&#114;&#101;&#119;&#105;&#99;&#107;&#105;&#64;&#100;&#108;&#114;&#46;&#100;&#101;

Version History

v1.0 Initial Version",2018,,,,,
970,EDGE-IIOTSET,Intrusion Detection,Intrusion Detection,"Intrusion Detection, Network Intrusion Detection","Graph, Image",,Computer Vision,,,https://ieee-dataport.org/documents/edge-iiotset-new-comprehensive-realistic-cyber-security-dataset-iot-and-iiot-applications,https://paperswithcode.com/dataset/edge-iiotset,"ABSTRACT 
In this project, we propose a new comprehensive realistic cyber security dataset of IoT and IIoT applications, called Edge-IIoTset, which can be used by machine learning-based intrusion detection systems in two different modes, namely, centralized and federated learning. Specifically, the proposed testbed is organized into seven layers, including, Cloud Computing Layer, Network Functions Virtualization Layer, Blockchain Network Layer, Fog Computing Layer, Software-Defined Networking Layer, Edge Computing Layer, and IoT and IIoT Perception Layer. In each layer, we propose new emerging technologies that satisfy the key requirements of IoT and IIoT applications, such as, ThingsBoard IoT platform, OPNFV platform, Hyperledger Sawtooth, Digital twin, ONOS SDN controller, Mosquitto MQTT brokers, Modbus TCP/IP, ...etc. The IoT data are generated from various IoT devices (more than 10 types) such as Low-cost digital sensors for sensing temperature and humidity, Ultrasonic sensor, Water level detection sensor, pH Sensor Meter, Soil Moisture sensor, Heart Rate Sensor, Flame Sensor, ...etc.). However, we identify and analyze fourteen attacks related to IoT and IIoT connectivity protocols, which are categorized into five threats, including, DoS/DDoS attacks, Information gathering, Man in the middle attacks, Injection attacks, and Malware attacks. In addition, we extract features obtained from different sources, including alerts, system resources, logs, network traffic, and propose new 61 features with high correlations from 1176 found features. After processing and analyzing the proposed realistic cyber security dataset, we provide a primary exploratory data analysis and evaluate the performance of machine learning approaches (i.e., traditional machine learning as well as deep learning) in both centralized and federated learning modes.

Instructions: 

Great news! The Edge-IIoT dataset has been featured as a ""Document in the top 1% of Web of Science."" This indicates that it is ranked within the top 1% of all publications indexed by the Web of Science (WoS) in terms of citations and impact.

Please kindly visit kaggle link for the updates: https://www.kaggle.com/datasets/mohamedamineferrag/edgeiiotset-cyber-sec...

Free use of the Edge-IIoTset dataset for academic research purposes is hereby granted in perpetuity. Use for commercial purposes is allowable after asking the leader author, Dr Mohamed Amine Ferrag, who has asserted his right under the Copyright.

The details of the Edge-IIoT dataset were published in following the paper. For the academic/public use of these datasets, the authors have to cities the following paper:

Mohamed Amine Ferrag, Othmane Friha, Djallel Hamouda, Leandros Maglaras, Helge Janicke, ""Edge-IIoTset: A New Comprehensive Realistic Cyber Security Dataset of IoT and IIoT Applications for Centralized and Federated Learning"", IEEE Access, April 2022 (IF: 3.37), DOI: 10.1109/ACCESS.2022.3165809

Link to paper : https://ieeexplore.ieee.org/document/9751703


The directories of the Edge-IIoTset dataset include the following:

•File 1 (Normal traffic)

-File 1.1 (Distance): This file includes two documents, namely, Distance.csv and Distance.pcap. The IoT sensor (Ultrasonic sensor) is used to capture the IoT data.

-File 1.2 (Flame_Sensor): This file includes two documents, namely, Flame_Sensor.csv and Flame_Sensor.pcap. The IoT sensor (Flame Sensor) is used to capture the IoT data.

-File 1.3 (Heart_Rate): This file includes two documents, namely, Flame_Sensor.csv and Flame_Sensor.pcap. The IoT sensor (Flame Sensor) is used to capture the IoT data.

-File 1.4 (IR_Receiver): This file includes two documents, namely, IR_Receiver.csv and IR_Receiver.pcap. The IoT sensor (IR (Infrared) Receiver Sensor) is used to capture the IoT data.

-File 1.5 (Modbus): This file includes two documents, namely, Modbus.csv and Modbus.pcap. The IoT sensor (Modbus Sensor) is used to capture the IoT data.

-File 1.6 (phValue): This file includes two documents, namely, phValue.csv and phValue.pcap. The IoT sensor (pH-sensor PH-4502C) is used to capture the IoT data.

-File 1.7 (Soil_Moisture): This file includes two documents, namely, Soil_Moisture.csv and Soil_Moisture.pcap. The IoT sensor (Soil Moisture Sensor v1.2) is used to capture the IoT data.

-File 1.8 (Sound_Sensor): This file includes two documents, namely, Sound_Sensor.csv and Sound_Sensor.pcap. The IoT sensor (LM393 Sound Detection Sensor) is used to capture the IoT data.

-File 1.9 (Temperature_and_Humidity): This file includes two documents, namely, Temperature_and_Humidity.csv and Temperature_and_Humidity.pcap. The IoT sensor (DHT11 Sensor) is used to capture the IoT data.

-File 1.10 (Water_Level): This file includes two documents, namely, Water_Level.csv and Water_Level.pcap. The IoT sensor (Water sensor) is used to capture the IoT data.

•File 2 (Attack traffic): 

-File 2.1 (Attack traffic (CSV files)): This file includes 13 documents, namely, Backdoor_attack.csv, DDoS_HTTP_Flood_attack.csv, DDoS_ICMP_Flood_attack.csv, DDoS_TCP_SYN_Flood_attack.csv, DDoS_UDP_Flood_attack.csv, MITM_attack.csv, OS_Fingerprinting_attack.csv, Password_attack.csv, Port_Scanning_attack.csv, Ransomware_attack.csv, SQL_injection_attack.csv, Uploading_attack.csv, Vulnerability_scanner_attack.csv, XSS_attack.csv. Each document is specific for each attack.

-File 2.2 (Attack traffic (PCAP files)): This file includes 13 documents, namely, Backdoor_attack.pcap, DDoS_HTTP_Flood_attack.pcap, DDoS_ICMP_Flood_attack.pcap, DDoS_TCP_SYN_Flood_attack.pcap, DDoS_UDP_Flood_attack.pcap, MITM_attack.pcap, OS_Fingerprinting_attack.pcap, Password_attack.pcap, Port_Scanning_attack.pcap, Ransomware_attack.pcap, SQL_injection_attack.pcap, Uploading_attack.pcap, Vulnerability_scanner_attack.pcap, XSS_attack.pcap. Each document is specific for each attack.

•File 3 (Selected dataset for ML and DL): 

-File 3.1 (DNN-EdgeIIoT-dataset): This file contains a selected dataset for the use of evaluating deep learning-based intrusion detection systems. 

-File 3.2 (ML-EdgeIIoT-dataset): This file contains a selected dataset for the use of evaluating traditional machine learning-based intrusion detection systems. 


Step 1: Downloading The Edge-IIoTset dataset From the Kaggle platform
from google.colab import files

!pip install -q kaggle

files.upload()

!mkdir ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d mohamedamineferrag/edgeiiotset-cyber-security-dataset-of-iot-iiot -f ""Edge-IIoTset dataset/Selected dataset for ML and DL/DNN-EdgeIIoT-dataset.csv""

!unzip DNN-EdgeIIoT-dataset.csv.zip

!rm DNN-EdgeIIoT-dataset.csv.zip

Step 2: Reading the Datasets' CSV file to a Pandas DataFrame:
import pandas as pd

import numpy as np

df = pd.read_csv('DNN-EdgeIIoT-dataset.csv', low_memory=False) 

Step 3 : Exploring some of the DataFrame's contents:
df.head(5)

print(df['Attack_type'].value_counts())

Step 4: Dropping data (Columns, duplicated rows, NAN, Null..):
from sklearn.utils import shuffle

drop_columns = [""frame.time"", ""ip.src_host"", ""ip.dst_host"", ""arp.src.proto_ipv4"",""arp.dst.proto_ipv4"", 

     ""http.file_data"",""http.request.full_uri"",""icmp.transmit_timestamp"",

     ""http.request.uri.query"", ""tcp.options"",""tcp.payload"",""tcp.srcport"",

     ""tcp.dstport"", ""udp.port"", ""mqtt.msg""]

df.drop(drop_columns, axis=1, inplace=True)

df.dropna(axis=0, how='any', inplace=True)

df.drop_duplicates(subset=None, keep=""first"", inplace=True)

df = shuffle(df)

df.isna().sum()

print(df['Attack_type'].value_counts())

Step 5: Categorical data encoding (Dummy Encoding):
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler

from sklearn import preprocessing

def encode_text_dummy(df, name):

dummies = pd.get_dummies(df[name])

for x in dummies.columns:

    dummy_name = f""{name}-{x}""

    df[dummy_name] = dummies[x]

df.drop(name, axis=1, inplace=True)

encode_text_dummy(df,'http.request.method')

encode_text_dummy(df,'http.referer')

encode_text_dummy(df,""http.request.version"")

encode_text_dummy(df,""dns.qry.name.len"")

encode_text_dummy(df,""mqtt.conack.flags"")

encode_text_dummy(df,""mqtt.protoname"")

encode_text_dummy(df,""mqtt.topic"")

Step 6: Creation of the preprocessed dataset
df.to_csv('preprocessed_DNN.csv', encoding='utf-8')


For more information about the dataset, please contact the lead author of this project, Dr Mohamed Amine Ferrag, on his email: mohamed.amine.ferrag@gmail.com 

More information about Dr. Mohamed Amine Ferrag is available at:

https://www.linkedin.com/in/Mohamed-Amine-Ferrag 

https://dblp.uni-trier.de/pid/142/9937.html 

https://www.researchgate.net/profile/Mohamed_Amine_Ferrag 

https://scholar.google.fr/citations?user=IkPeqxMAAAAJ&hl=fr&oi=ao 

https://www.scopus.com/authid/detail.uri?authorId=56115001200 

https://publons.com/researcher/1322865/mohamed-amine-ferrag/ 

https://orcid.org/0000-0002-0632-3172 

Last Updated: 27 Mar. 2023",2022,,,13 documents,,
971,EdNet,Knowledge Tracing,Knowledge Tracing,Knowledge Tracing,,,Methodology,knowledge-tracing-on-ednet,CC BY-NC 4.0,https://github.com/riiid/ednet,https://paperswithcode.com/dataset/ednet,"A large-scale hierarchical dataset of diverse student activities collected by Santa, a multi-platform self-study solution equipped with artificial intelligence tutoring system. EdNet contains 131,441,538 interactions from 784,309 students collected over more than 2 years, which is the largest among the ITS datasets released to the public so far.",,,,,,
972,EDT,Stock Market Prediction,Stock Market Prediction,"Stock Market Prediction, Event Detection, Stock Price Prediction, Event-Driven Trading, Stock Prediction, Stock Trend Prediction, Event Extraction, Text-Based Stock Prediction","Image, Text, Time Series",English,Computer Vision,,,https://drive.google.com/drive/folders/1xKjd9hzA8UTn2DXVIYYnX5TngNAMom19,https://paperswithcode.com/dataset/edt,"The EDT dataset is designed for corporate event detection and text-based stock prediction (trading strategy) benchmark.



Corporate Event Detection
It includes 9721​ news articles with token-level event labels. Including 11 event types:
Acquisitions, Clinical Trials, Guidance Changes, New Contracts, Stock Repurchases, Stock Split, Reverse Stock Split/ADS Ratio Change, Regular Dividend, Special Dividend, Dividend Cut, Dividend Increase



Text-Based Stock Prediction Benchmark
It includes 303893​ first-hand news articles from high-quality sources. Each news article is assigned a minute-level timestamp and comprehensive stock price labels.



Please see this Github Link and paper for more details.",,paper,https://aclanthology.org/2021.findings-acl.186.pdf,,,
973,EEG_Eye_State,Time Series Classification,Time Series Classification,"Time Series Classification, Time Series Prediction","Image, Time Series",,Time Series,,CC BY 4.0,https://archive.ics.uci.edu/dataset/264/eeg+eye+state,https://paperswithcode.com/dataset/eeg-eye-state,All data is from one continuous EEG measurement with the Emotiv EEG Neuroheadset. The duration of the measurement was 117 seconds. The eye state was detected via a camera during the EEG measurement and added later manually to the file after analysing the video frames. '1' indicates the eye-closed and '0' the eye-open state. All values are in chronological order with the first measured value at the top of the data.,,,,,,
974,EGAD,Robotic Grasping,Robotic Grasping,Robotic Grasping,,,Methodology,,,https://dougsm.github.io/egad/,https://paperswithcode.com/dataset/egad,"The Evolved Grasping Analysis Dataset (EGAD) comprises over 2000 generated objects aimed at training and evaluating robotic visual grasp detection algorithms. The objects in EGAD are geometrically diverse, filling a space ranging from simple to complex shapes and from easy to difficult to grasp, compared to other datasets for robotic grasping, which may be limited in size or contain only a small number of object classes.",2000,,,,,
975,Ego4D,Natural Language Queries,Natural Language Queries,"Natural Language Queries, Long Term Action Anticipation, Action Anticipation, Temporal Action Localization, Short-term Object Interaction Anticipation, State Change Object Detection, Future Hand Prediction, Moment Queries, Object State Change Classification","Image, Text, Time Series, Video",English,Computer Vision,"moment-queries-on-ego4d, state-change-object-detection-on-ego4d, natural-language-queries-on-ego4d, long-term-action-anticipation-on-ego4d, short-term-object-interaction-anticipation-on, temporal-action-localization-on-ego4d-mq-val, object-state-change-classification-on-ego4d, future-hand-prediction-on-ego4d, temporal-action-localization-on-ego4d-mq-test",Custom,https://ego4d-data.org/,https://paperswithcode.com/dataset/ego4d,"Ego4D is a massive-scale egocentric video dataset and benchmark suite. It offers 3,025 hours of daily life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 855 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, a host of new benchmark challenges are presented, centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, the aim is to push the frontier of first-person perception.

Description from: Facebook AI

Paper: Ego4D: Around the World in 3,000 Hours of Egocentric Video

GitHub: https://github.com/EGO4D",,,,,,
976,EgoBody,3D human pose and shape estimation,3D human pose and shape estimation,3D human pose and shape estimation,"3D, Image",,Computer Vision,3d-human-pose-and-shape-estimation-on-egobody,,https://sanweiliti.github.io/egobody/egobody.html,https://paperswithcode.com/dataset/egobody,"EgoBody dataset is a novel large-scale dataset for egocentric 3D human pose, shape and motions under interactions in complex 3D scenes.",,,,,,
977,EgoDexter,3D Hand Pose Estimation,3D Hand Pose Estimation,"3D Hand Pose Estimation, Hand Pose Estimation, Pose Estimation","3D, Image",,Computer Vision,,,https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/EgoDexter.htm,https://paperswithcode.com/dataset/egodexter,"The EgoDexter dataset provides both 2D and 3D pose annotations for 4 testing video sequences with 3190 frames. The videos are recorded with body-mounted camera from egocentric viewpoints and contain cluttered backgrounds, fast camera motion, and complex interactions with various objects. Fingertip positions were manually annotated for 1485 out of 3190 frames.",,Hand Pose Estimation via Latent 2.5D Heatmap Regression,https://arxiv.org/abs/1804.09534,,,
978,EgoExoLearn,Long Term Anticipation,Long Term Anticipation,"Long Term Anticipation, Video Captioning, Action Anticipation, Action Recognition, Action Segmentation, Video Retrieval, Action Quality Assessment","Image, Text, Video",English,Computer Vision,"action-quality-assessment-on-egoexolearn, video-retrieval-on-egoexolearn, action-anticipation-on-egoexolearn, long-term-anticipation-on-egoexolearn",,https://github.com/OpenGVLab/EgoExoLearn,https://paperswithcode.com/dataset/egoexolearn,"EgoExoLearn is a fascinating dataset designed to bridge the gap between egocentric and exocentric views of procedural activities. 



What Is EgoExoLearn?   EgoExoLearn is a large-scale dataset that emulates how humans learn by observing others. It focuses on the process of asynchronous demonstration following. Participants in the dataset record egocentric videos as they perform tasks. These videos are guided by exocentric-view demonstration videos. In simpler terms, imagine someone watching a demonstration video (from an external perspective) and then replicating the same task while recording their own point-of-view video.



Dataset Details:
EgoExoLearn dataset spans 120 hours and covers scenarios from daily life and specialized laboratories. It contains:


Egocentric videos: These are recorded by individuals executing tasks.
Demonstration videos: These show the same tasks from an external viewpoint.
Gaze data: High-quality gaze information accompanies the videos.
Multimodal annotations: Detailed annotations provide context and insights.



Applications and Benchmarks:
The EgoExoLearn dataset serves as a playground for modeling the human ability and thus provides a playground to bridge asynchronous procedural actions from different viewpoints.
It enables new benchmarks such as:


Cross-view association: Linking actions observed from different perspectives.
Cross-view action planning: Anticipating and planning actions based on both ego and exo views.
Cross-view referenced skill assessment: Evaluating skills across viewpoints. Using Exo-view demonstrations as guidance for better ego-view skill assessment.



Why Is It Important?


Understanding how we map others' activities into our own point of view is a fundamental human skill.
EgoExoLearn paves the way for creating AI agents capable of seamlessly learning by observing humans in the real world.



(1) EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric .... https://arxiv.org/html/2403.16182v1.
(2) EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric .... https://egoexolearn.github.io/.
(3) EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric .... https://arxiv.org/abs/2403.16182.
(4) EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric .... https://allainews.com/item/egoexolearn-a-dataset-for-bridging-asynchronous-ego-and-exo-centric-view-of-procedural-activities-in-real-world-2024-03-26/.
(5) undefined. https://github.com/OpenGVLab/EgoExoLearn/.",2024,,,,,
979,EGOK360,Activity Recognition,Activity Recognition,"Activity Recognition, Video Understanding, Egocentric Activity Recognition","Image, Video",,Computer Vision,,,https://egok360.github.io/,https://paperswithcode.com/dataset/egok360,"Contains annotations of human activity with different sub-actions, e.g., activity Ping-Pong with four sub-actions which are pickup-ball, hit, bounce-ball and serve.",,,,,,
980,EgoProceL,Event Segmentation,Event Segmentation,"Event Segmentation, Weakly Supervised Action Segmentation (Transcript), Action Segmentation, Action Detection, Video Segmentation","Image, Video",,Computer Vision,,,https://sid2697.github.io/egoprocel/,https://paperswithcode.com/dataset/egoprocel,"EgoProceL is a large-scale dataset for procedure learning. It consists of 62 hours of egocentric videos recorded by 130 subjects performing 16 tasks for procedure learning. EgoProceL contains videos and key-step annotations for multiple tasks from CMU-MMAC, EGTEA Gaze+, and individual tasks like toy-bike assembly, tent assembly, PC assembly, and PC disassembly. EgoProceL overcomes the limitations of third-person videos. As, using third-person videos makes the manipulated object small in appearance and often occluded by the actor, leading to significant errors. In contrast, we observe that videos obtained from first-person (egocentric) wearable cameras provide an unobstructed and clear view of the action.",,,,,,
981,EgoSchema,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Zero-Shot Video Question Answer","Image, Text, Video",English,Computer Vision,"zero-shot-video-question-answer-on-egoschema, visual-question-answering-vqa-on-egoschema, zero-shot-video-question-answer-on-egoschema-1",,https://egoschema.github.io/,https://paperswithcode.com/dataset/egoschema,"EgoSchema is very long-form video question-answering dataset, and benchmark to evaluate long video understanding capabilities of modern vision and language systems. Derived from Ego4D, EgoSchema consists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours of real video data, covering a very broad range of natural human activity and behavior.",,,,,,
982,EGTEA,Egocentric Activity Recognition,Egocentric Activity Recognition,"Egocentric Activity Recognition, Action Anticipation, Long-tail Learning","Image, Video",,Computer Vision,"action-anticipation-on-egtea, egocentric-activity-recognition-on-egtea-1, long-tail-learning-on-egtea",,http://cbs.ic.gatech.edu/fpv/,https://paperswithcode.com/dataset/egtea,"Extended GTEA Gaze+
EGTEA Gaze+ is a large-scale dataset for FPV actions and gaze. It subsumes GTEA Gaze+ and comes with HD videos (1280x960), audios, gaze tracking data, frame-level action annotations, and pixel-level hand masks at sampled frames.
Specifically, EGTEA Gaze+ contains 28 hours (de-identified) of cooking activities from 86 unique sessions of 32 subjects. These videos come with audios and gaze tracking (30Hz). We have further provided human annotations of actions (human-object interactions) and hand masks.

The action annotations include 10325 instances of fine-grained actions, such as ""Cut bell pepper"" or ""Pour condiment (from) condiment container into salad"".

The hand annotations consist of 15,176 hand masks from 13,847 frames from the videos.",,,,10325 instances,,
983,Egyptian_Arabic_Segmentation_Dataset,Part-Of-Speech Tagging,Part-Of-Speech Tagging,"Part-Of-Speech Tagging, Morphological Analysis",Audio,,Speech,,,https://alt.qcri.org/resources/da_resources/,https://paperswithcode.com/dataset/egyptian-arabic-segmentation-dataset,"Contains 350 tweets with more than 8,000 words including 3,000 unique words written in Egyptian dialect. The tweets have much dialectal content covering most of dialectal Egyptian phonological, morphological, and syntactic phenomena. It also includes Twitter-specific aspects of the text, such as #hashtags, @mentions, emoticons and URLs.",,,,,,
984,eICU-CRD,Mortality Prediction,Mortality Prediction,"Mortality Prediction, ICU Mortality",Time Series,,Methodology,,,https://eicu-crd.mit.edu/,https://paperswithcode.com/dataset/eicu-crd,"The eICU Collaborative Research Database is a large multi-center critical care database made available by Philips Healthcare in partnership with the MIT Laboratory for Computational Physiology.

The eICU Collaborative Research Database holds data associated with over 200,000 patient stays, providing a large sample size for research studies.",,,,,,
985,EigenWorms,Time Series Classification,Time Series Classification,"Time Series Classification, Time Series Analysis","Image, Time Series",,Time Series,time-series-classification-on-eigenworms,,http://www.timeseriesclassification.com/description.php?Dataset=EigenWorms,https://paperswithcode.com/dataset/eigenworms,"Caenorhabditis elegans is a roundworm commonly used as a model organism in the study of genetics. The movement of these worms is known to be a useful indicator for understanding behavioural genetics. Brown {\em et al.}[1] describe a system for recording the motion of worms on an agar plate and measuring a range of human-defined features[2]. It has been shown that the space of shapes Caenorhabditis elegans adopts on an agar plate can be represented by combinations of six base shapes, or eigenworms. Once the worm outline is extracted, each frame of worm motion can be captured by six scalars representing the amplitudes along each dimension when the shape is projected onto the six eigenworms. Using data collected for the work described in[1], we address the problem of classifying individual worms as wild-type or mutant based on the time series. The data were extracted from the C. elegans behavioural database [3]. We have 259 cases, which we split 131 train and 128 test. We have truncated each series to the shortest usable. Each series has 17984 observations. Each worm is classified as either wild-type (the N2 reference strain) or one of four mutant types: goa-1; unc-1; unc-38 and unc-63. [1] A. Brown, E. Yemini, L. Grundy, T. Jucikas, and W. Schafer, A dictionary of behavioral motifs reveals clusters of genes affecting caenorhabditis elegans locomotion, Proceedings of the National Academy of Sciences of the United States of America (PNAS), vol. 10, no. 2, pp. 791 796, 2013. [2] E. Yemini, T. Jucikas, L. Grundy, A. Brown, and W. Schafer,  A database of caenorhabditis elegans behavioral phenotypes, Nature Methods, vol. 10, pp. 877 879, 2013. [3] C. elegans behavioural database",2013,,,,,
986,Eisen_Funcat,Hierarchical Multi-label Classification,Hierarchical Multi-label Classification,Hierarchical Multi-label Classification,Image,,Computer Vision,hierarchical-multi-label-classification-on-2,,https://dtai.cs.kuleuven.be/clus/hmcdatasets/,https://paperswithcode.com/dataset/eisen-funcat,Hierarchical-multilabel classification dataset for functional genomics,,,,,,
987,ELAS,Autonomous Driving,Autonomous Driving,"Autonomous Driving, Self-Driving Cars, Lane Detection",Image,,Computer Vision,,,https://github.com/rodrigoberriel/ego-lane-analysis-system/tree/master/datasets,https://paperswithcode.com/dataset/elas,"ELAS is a dataset for lane detection. It contains more than 20 different scenes (in more than 15,000 frames) and considers a variety of scenarios (urban road, highways, traffic, shadows, etc.). The dataset was manually annotated for several events that are of interest for the research community (i.e., lane estimation, change, and centering; road markings; intersections; LMTs; crosswalks and adjacent lanes).",,Ego-Lane Analysis System (ELAS): Dataset and Algorithms,https://arxiv.org/abs/1806.05984,,,
988,ELD,Image Denoising,Image Denoising,Image Denoising,Image,,Computer Vision,"image-denoising-on-eld-sonya7s2-x200, image-denoising-on-eld-sonya7s2-x100",,https://github.com/Vandermode/ELD,https://paperswithcode.com/dataset/eld,"Extreme low-light denoising (ELD) dataset that covers 10 indoor scenes and 4 camera devices from multiple brands (SonyA7S2, NikonD850, CanonEOS70D, CanonEOS700D).
It has three levels (800, 1600, 3200) and two low light factors(100, 200) for noisy images, resulting in 240 (3×2×10×4) raw image pairs in total.",,,,,,
989,Electricity,Univariate Time Series Forecasting,Univariate Time Series Forecasting,"Univariate Time Series Forecasting, Correlated Time Series Forecasting, GLinear, Multivariate Time Series Forecasting, Multivariate Time Series Imputation, Time Series Analysis, Core set discovery",Time Series,,Time Series,"glinear-on-electricity, multivariate-time-series-imputation-on-3, multivariate-time-series-forecasting-on-44, correlated-time-series-forecasting-on, core-set-discovery-on-electricity, univariate-time-series-forecasting-on",Creative Commons Attribution 4.0 International,https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption,https://paperswithcode.com/dataset/electricity,"Abstract: Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.

| Data Set Characteristics  | Number of Instances | Area     | Attribute Characteristics | Number of Attributes | Date Donated | Associated Tasks       | Missing Values |
| ------------------------- | ------------------- | -------- | ------------------------- | -------------------- | ------------ | ---------------------- | -------------- |
| Multivariate, Time-Series | 2075259             | Physical | Real                      | 9                    | 2012-08-30   | Regression, Clustering | Yes            |

Source:
Georges Hebrail (georges.hebrail '@' edf.fr), Senior Researcher, EDF R&D, Clamart, France
Alice Berard, TELECOM ParisTech Master of Engineering Internship at EDF R&D, Clamart, France

Data Set Information:
This archive contains 2075259 measurements gathered in a house located in Sceaux (7km of Paris, France) between December 2006 and November 2010 (47 months).
Notes:


(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.
The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.

Attribute Information:

date: Date in format dd/mm/yyyy
time: time in format hh:mm:ss
global_active_power: household global minute-averaged active power (in kilowatt)
global_reactive_power: household global minute-averaged reactive power (in kilowatt)
voltage: minute-averaged voltage (in volt)
global_intensity: household global minute-averaged current intensity (in ampere)
sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).
sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.
sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.

Relevant Papers:
N/A

Citation Request:
This dataset is made available under the “Creative Commons Attribution 4.0 International (CC BY 4.0)” license",2012,,,,,
990,Electricity_Consuming_Load,GLinear,GLinear,"GLinear, Time Series Forecasting",Time Series,,Methodology,"glinear-on-electricity-96, time-series-forecasting-on-electricity-192, time-series-forecasting-on-electricity-96, time-series-forecasting-on-electricity-336, glinear-on-electricity-192, time-series-forecasting-on-electricity-720, glinear-on-electricity-336",,https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014,https://paperswithcode.com/dataset/electricity-consuming-load,This data set contains electricity consumption of 370 points/clients.,,,,,,
991,Elephant,Multiple Instance Learning,Multiple Instance Learning,Multiple Instance Learning,,,Methodology,multiple-instance-learning-on-elephant,,,https://paperswithcode.com/dataset/elephant,"The Elephant MIL dataset is a benchmark used in multiple instance learning (MIL), which falls under the broader categories of image classification and content-based image retrieval. The task is to determine if an image contains an elephant. Each image is treated as a ""bag,"" and within each bag, the image is segmented into various regions called ""instances,"" represented by feature vectors that capture visual characteristics like color, texture, and shape. A bag is labeled as positive if at least one instance contains an elephant, and negative if none of the instances do. The dataset includes 200 images (bags) with a total of 1220 1220 segments (instances), averaging ~6.1 segments per image. The challenge is that only some segments in a positive image might actually show an elephant, so the goal is to correctly classify the entire image based on these segments. This dataset is widely used to evaluate MIL algorithms, especially in cases where only parts of the data might contain the relevant information.",,,,200 images,"val. The task is to determine if an image contains an elephant. Each image is treated as a ""bag,"" and within each bag, the image is segmented into various regions called ""instances,"" represented by feature vectors that capture visual characteristics like color, texture, and shape. A bag is labeled as positive if at least one instance contains an elephant, and negative if none of the instances do. The dataset includes 200 images",
992,ELEVATER,Object Detection,Object Detection,"Object Detection, Image Classification, Zero-Shot Object Detection, Few-Shot Object Detection",Image,,Computer Vision,"object-detection-on-odinw-full-shot-35-tasks, object-detection-on-elevater",,https://computer-vision-in-the-wild.github.io/ELEVATER/,https://paperswithcode.com/dataset/elevater,"The ELEVATER benchmark is a collection of resources for training, evaluating, and analyzing language-image models on image classification and object detection. ELEVATER consists of:


Benchmark: A benchmark suite that consists of 20 image classification datasets and 35 object detection datasets, augmented with external knowledge
Toolkit: An automatic hyper-parameter tuning toolkit; Strong language-augmented efficient model adaptation methods.
Baseline: Pre-trained language-free and language-augmented visual models.
Knowledge: A platform to study the benefit of external knowledge for vision problems.
Evaluation Metrics: Sample-efficiency (zero-, few-, and full-shot) and Parameter-efficiency.
Leaderboard: A public leaderboard to track performance on the benchmark

The ultimate goal of ELEVATER is to drive research in the development of language-image models to tackle core computer vision problems in the wild.",,,,,,
993,ELI5,Language Modelling,Language Modelling,"Language Modelling, Open-Domain Question Answering, Text Generation, Long Form Question Answering, Question Answering",Text,English,Natural Language Processing,open-domain-question-answering-on-eli5,,https://facebookresearch.github.io/ELI5/,https://paperswithcode.com/dataset/eli5,"ELI5 is a dataset for long-form question answering. It contains 270K complex, diverse questions that require explanatory multi-sentence answers. Web search results are used as evidence documents to answer each question.

ELI5 is also a task in Dodecadialogue.",,https://arxiv.org/pdf/1907.09190v1.pdf,https://arxiv.org/pdf/1907.09190v1.pdf,,,
994,eLife,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Lay Summarization",Text,English,Natural Language Processing,"abstractive-text-summarization-on-elife, lay-summarization-on-elife",,https://github.com/TGoldsack1/Corpora_for_Lay_Summarisation,https://paperswithcode.com/dataset/elife,"This dataset contains 4,828 full biomedical articles paired with non-technical lay summaries derived from the eLife scientific journal.",,,,,,
995,ELITR_Minuting_Corpus,Abstractive Dialogue Summarization,Abstractive Dialogue Summarization,"Abstractive Dialogue Summarization, Meeting Summarization",Text,English,Natural Language Processing,,,https://github.com/guokan-shang/elitr-minuting-corpus,https://paperswithcode.com/dataset/elitr-minuting-corpus,ELITR Minuting Corpus in JSON format.,,,,,,
996,Elliptic_Dataset,Fraud Detection,Fraud Detection,Fraud Detection,Image,,Computer Vision,fraud-detection-on-elliptic-dataset,,https://www.kaggle.com/datasets/ellipticco/elliptic-data-set,https://paperswithcode.com/dataset/elliptic-dataset,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
997,Elsevier_OA_CC-BY,Text Summarization,Text Summarization,"Text Summarization, Headline Generation, Paper generation (abstract-to-conclusion), Extreme Summarization, Paper generation (Conclusion-to-title), Paper generation (Title-to-abstract), Document Summarization",Text,English,Natural Language Processing,,CC By 4.0,https://data.mendeley.com/datasets/zm33cdndxs/3,https://paperswithcode.com/dataset/elsevier-oa-cc-by,"An open corpus of Scientific Research papers which has a representative sample from across scientific disciplines. This corpus not only includes the full text of the article, but also the metadata of the documents, along with the bibliographic information for each reference.",,,,,,
998,Email-EU,Link Prediction,Link Prediction,"Link Prediction, Graph Clustering, Community Detection","Graph, Image, Time Series",,Computer Vision,,,https://www.cs.cornell.edu/~arb/data/email-Eu/,https://paperswithcode.com/dataset/email-eu,"EmailEU is a directed temporal network constructed from email exchanges in a large European research institution for a 803-day period. It contains 986 email addresses as nodes and 332,334 emails as edges with timestamps. There are 42 ground truth departments in the dataset.",,gl2vec: Learning Feature Representation Using Graphlets for Directed Networks,https://arxiv.org/abs/1812.05473,,,
999,EmailSum,Email Thread Summarization,Email Thread Summarization,Email Thread Summarization,Text,English,Natural Language Processing,"email-thread-summarization-on-emailsum-short, email-thread-summarization-on-emailsum-long",None,https://github.com/ZhangShiyue/EmailSum,https://paperswithcode.com/dataset/emailsum,"Email Thread Summarization (EmailSum) is a dataset which contains human-annotated short (<30 words) and long (<100 words) summaries of 2,549 email threads (each containing 3 to 10 emails) over a wide variety of topics. It was developed to spur research in thread summarization.",,EMAILSUM: Abstractive Email Thread Summarization,https://arxiv.org/pdf/2107.14691v1.pdf,,,
1000,EMBER,Malware Classification,Malware Classification,"Malware Classification, Feature Engineering, Malware Detection",Image,,Computer Vision,,MIT,https://github.com/endgameinc/ember,https://paperswithcode.com/dataset/ember,"A labeled benchmark dataset for training machine learning models to statically detect malicious Windows portable executable files. The dataset includes features extracted from 1.1M binary files: 900K training samples (300K malicious, 300K benign, 300K unlabeled) and 200K test samples (100K malicious, 100K benign).",,EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models,https://arxiv.org/pdf/1804.04637v2.pdf,,training machine learning models to statically detect malicious Windows portable executable files. The dataset includes features extracted from 1.1M binary files: 900K training samples,
1001,EmbSpatial-Bench,Spatial Reasoning,Spatial Reasoning,Spatial Reasoning,,,Reasoning,spatial-reasoning-on-embspatial-bench,,https://arxiv.org/pdf/2406.05756,https://paperswithcode.com/dataset/embspatial-bench,"The recent rapid development of Large Vision-Language Models (LVLMs) has indicated their potential for embodied tasks. However, the critical skill of spatial understanding in embodied environments has not been thoroughly evaluated, leaving the gap between current LVLMs and qualified embodied intelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for evaluating embodied spatial understanding of LVLMs. The benchmark is automatically derived from embodied scenes and covers 6 spatial relationships from an egocentric perspective. Below are a few examples.",,Homepage,https://arxiv.org/pdf/2406.05756,,"valuated, leaving the gap between current LVLMs and qualified embodied intelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for evaluating embodied spatial understanding of LVLMs. The benchmark is automatically derived from embodied scenes and covers 6 spatial relationships from an egocentric perspective. Below are a few examples",
1002,EMC_Dutch_Clinical_Corpus,Negation Detection,Negation Detection,Negation Detection,Image,,Computer Vision,,,https://biosemantics.erasmusmc.nl/index.php/resources/emc-dutch-clinical-corpus,https://paperswithcode.com/dataset/emc-dutch-clinical-corpus,"EMC Dutch clinical corpus contains four types of anonymized clinical documents: entries from general practitioners, specialists’ letters, radiology reports, and discharge letters. The identified UMLS terms in the corpus are annotated for negation, temporality, and experiencer properties.

Zubair Afzal, Ewoud Pons, Ning Kang, Miriam CJM Sturkenboom, Martijn J Schuemie, Jan A Kors. ContextD: an algorithm to identify contextual properties of medical terms in a Dutch clinical corpus. BMC Bioinformatics 2014, 15:373 doi:10.1186/s12859-014-0373-3",2014,,,,,
1003,EMDB,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Human motion prediction, Global 3D Human Pose Estimation","3D, Image, Time Series, Video",,Computer Vision,"global-3d-human-pose-estimation-on-emdb, 3d-human-pose-estimation-on-emdb","Custom (research-only, non-commercial)",https://ait.ethz.ch/emdb,https://paperswithcode.com/dataset/emdb,"EMDB contains in-the-wild  videos of human activity recorded with a hand-held iPhone. It features reference SMPL body pose and shape parameters, as well as global body root and camera trajectories. The reference 3D poses were obtained by jointly fitting SMPL to 12 body-worn electromagnetic sensors and image data. For the latter we fit a neural implicit avatar model to allow for a dense pixel-wise fitting objective.

EMDB contains:


81 sequences
105 000 frames
10 actors (5 female, 5 male)
Global camera trajectories
SMPL pose and shape parameters
2D Keypoints

The dataset can be used to evaluate the following tasks:


Camera-relative 3D human pose and shape estimation from monocular videos.
Global 3D human pose and shape estimation including camera trajectories from monocular videos.
Human motion prediction.",,,,,,
1004,Emilia_Dataset,Zero-Shot Multi-Speaker TTS,Zero-Shot Multi-Speaker TTS,Zero-Shot Multi-Speaker TTS,Audio,,Audio,,,https://huggingface.co/datasets/amphion/Emilia-Dataset,https://paperswithcode.com/dataset/emilia-dataset,"Recent advancements in speech generation models have been significantly driven by the use of large-scale training data. However, producing highly spontaneous, human-like speech remains a challenge due to the scarcity of large, diverse, and spontaneous speech datasets. In response, we introduce Emilia, the first large-scale, multilingual, and diverse speech generation dataset. Emilia starts with over 101k hours of speech across six languages, covering a wide range of speaking styles to enable more natural and spontaneous speech generation. To facilitate the scale-up of Emilia, we also present Emilia-Pipe, the first open-source preprocessing pipeline designed to efficiently transform raw, in-the-wild speech data into high-quality training data with speech annotations. Experimental results demonstrate the effectiveness of both Emilia and Emilia-Pipe. Demos are available at: https://emilia-dataset.github.io/Emilia-Demo-Page/.",,,,,,
1005,EMMA,Multimodal Reasoning,Multimodal Reasoning,"Multimodal Reasoning, Visual Reasoning",Image,,Reasoning,,,https://emma-benchmark.github.io/,https://paperswithcode.com/dataset/emma,"We introduce EMMA (Enhanced MultiModal reAsoning), a benchmark targeting organic multimodal reasoning across mathematics, physics, chemistry, and coding. EMMA tasks demand advanced cross-modal reasoning that cannot be solved by thinking separately in each modality, offering an enhanced test suite for MLLMs' reasoning capabilities.

EMMA is composed of 2,788 problems, of which 1,796 are newly constructed, across four domains. Within each subject, we further provide fine-grained labels for each question based on the specific skills it measures.",,,,,,
1006,EMNIST,Dimensionality Reduction,Dimensionality Reduction,"Dimensionality Reduction, Image Classification, Image Clustering, Fine-Grained Image Classification, Image Generation","Image, Text",English,Computer Vision,"image-classification-on-emnist-digits, fine-grained-image-classification-on-emnist-1, image-generation-on-emnist-letters, image-classification-on-emnist-bymerge, image-classification-on-emnist-byclass, image-classification-on-emnist-balanced, fine-grained-image-classification-on-emnist, image-clustering-on-emnist-balanced, image-classification-on-emnist-letters, dimensionality-reduction-on-emnist",,https://www.nist.gov/itl/products-and-services/emnist-dataset,https://paperswithcode.com/dataset/emnist,EMNIST (extended MNIST) has 4 times more data than MNIST. It is a set of handwritten digits with a 28 x 28 format.,,Domain Discrepancy Measure for Complex Models in Unsupervised Domain Adaptation,https://arxiv.org/abs/1901.10654,,,
1007,EmoBank,Natural Language Understanding,Natural Language Understanding,"Natural Language Understanding, Emotion Recognition, Emotion Classification","Image, Text",English,Computer Vision,,CC-BY-SA 4.0,https://github.com/JULIELab/EmoBank,https://paperswithcode.com/dataset/emobank,"EmoBank is a corpus of 10k English sentences balancing multiple genres, annotated with dimensional emotion metadata in the Valence-Arousal-Dominance (VAD) representation format. EmoBank excels with a bi-perspectival and bi-representational design.",,,,,,
1008,EmoCause,Recognizing Emotion Cause in Conversations,Recognizing Emotion Cause in Conversations,Recognizing Emotion Cause in Conversations,,,Methodology,recognizing-emotion-cause-in-conversations-on-1,CC BY-NC-SA,https://github.com/skywalker023/focused-empathy,https://paperswithcode.com/dataset/emocause,"EmoCause is a dataset of annotated emotion cause words in emotional situations from the EmpatheticDialogues valid and test set. The goal is to recognize emotion cause words in sentences by training only on sentence-level emotion labels without word-level labels (i.e., weakly-supervised emotion cause recognition). 

EmoCause is based on the fact that humans do not recognize the cause of emotions with supervised learning on word-level cause labels. Thus, we do not provide a training set.


Number of emotion categories: 32
Average number of cause words per utterance: 2.3
Total number of utterances: 4.6K (valid: 3.8K / test: 0.8K)",,,,,,
1009,EmoContext,Emotion Recognition in Conversation,Emotion Recognition in Conversation,Emotion Recognition in Conversation,Image,,Computer Vision,emotion-recognition-in-conversation-on-ec,Custom,https://competitions.codalab.org/competitions/19790,https://paperswithcode.com/dataset/emocontext,"EmoContext consists of three-turn English Tweets. The emotion labels include happiness, sadness, anger and other.",,,,,,
1010,EmoDB_Dataset,Speech Emotion Recognition,Speech Emotion Recognition,"Speech Emotion Recognition, Emotional Speech Synthesis, Speech Recognition","Audio, Image, Text",English,Speech,speech-emotion-recognition-on-emodb-dataset,,http://emodb.bilderbar.info/docu/,https://paperswithcode.com/dataset/emodb-dataset,"The EMODB database is the freely available German emotional database. The database is created by the Institute of Communication Science, Technical University, Berlin, Germany. Ten professional speakers (five males and five females) participated in data recording. The database contains a total of 535 utterances. The EMODB database comprises of seven emotions: 1) anger; 2) boredom; 3) anxiety; 4) happiness; 5) sadness; 6) disgust; and 7) neutral. The data was recorded at a 48-kHz sampling rate and then down-sampled to 16-kHz.

Citation:
Felix Burkhardt, Astrid Paeschke, Miriam Rolfes, Walter Sendlmeier und Benjamin Weiss
A Database of German Emotional Speech
Proceedings Interspeech 2005, Lissabon, Portugal",2005,,,,,
1011,EMOPIA,Music Classification,Music Classification,"Music Classification, Emotion Classification, Music Generation, Music Style Transfer, Music Tagging, Emotion Recognition, Music Information Retrieval","Audio, Image, Text",English,Computer Vision,,CC BY,https://annahung31.github.io/EMOPIA/,https://paperswithcode.com/dataset/emopia,"EMOPIA (pronounced ‘yee-mò-pi-uh’) dataset is a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. The dataset contains 1,087 music clips from 387 songs and clip-level emotion labels annotated by four dedicated annotators.",,,,,,
1012,EmoryNLP,Emotion Recognition in Conversation,Emotion Recognition in Conversation,Emotion Recognition in Conversation,Image,,Computer Vision,emotion-recognition-in-conversation-on-4,,https://github.com/emorynlp/character-mining,https://paperswithcode.com/dataset/emorynlp,"EmoryNLP comprises 97 episodes, 897 scenes, and 12,606 utterances, where each utterance is annotated with one of the seven emotions borrowed from the six primary emotions in the Willcox (1982)’s feeling wheel, sad, mad, scared, powerful, peaceful, joyful, and a default emotion of neutral.",1982,,,,,
1013,EmoSpeech,Keyword Spotting,Keyword Spotting,"Keyword Spotting, Speech Denoising, Speech Recognition","Audio, Image, Text",English,Computer Vision,,,https://github.com/RakshakTeam/emospeech-dataset-v1,https://paperswithcode.com/dataset/emospeech,"EmoSpeech contains keywords with diverse emotions and background sounds, presented to explore new challenges in audio analysis.",,,,,,
1014,EMOTIC,Dominance Estimation,Dominance Estimation,"Dominance Estimation, Emotion Recognition in Context, Age Classification, Action Recognition, Emotion Recognition, Multimodal Emotion Recognition, Arousal Estimation, Valence Estimation","Image, Video",,Computer Vision,"valence-estimation-on-emotic, emotion-recognition-on-emotic, arousal-estimation-on-emotic, dominance-estimation-on-emotic, emotion-recognition-in-context-on-emotic, age-classification-on-emotic",,https://github.com/rkosti/emotic,https://paperswithcode.com/dataset/emotic,"The EMOTIC dataset, named after EMOTions In Context, is a database of images with people in real environments, annotated with their apparent emotions. The images are annotated with an extended list of 26 emotion categories combined with the three common continuous dimensions Valence, Arousal and Dominance.",,Context Based Emotion Recognition using EMOTIC Dataset,https://arxiv.org/abs/2003.13401,,,
1015,Emotional_Dialogue_Acts,Dialogue Understanding,Dialogue Understanding,"Dialogue Understanding, Emotional Dialogue Acts, Natural Language Understanding, Emotion Recognition in Context, Emotion Recognition, Emotion Recognition in Conversation, Dialogue Act Classification","Image, Text",English,Computer Vision,,,https://secure-robots.eu/fellows/bothe/eda/,https://paperswithcode.com/dataset/emotional-dialogue-acts,"Emotional Dialogue Acts data contains dialogue act labels for existing emotion multi-modal conversational datasets.
We chose two popular multimodal emotion datasets: Multimodal EmotionLines Dataset (MELD) and Interactive Emotional dyadic MOtion CAPture database (IEMOCAP). 
EDAs reveal associations between dialogue acts and emotional states in a natural-conversational language such as Accept/Agree dialogue acts often occur with the Joy emotion, Apology with Sadness, and Thanking with Joy.",,,,,,
1016,EmotionLines,Language Modelling,Language Modelling,"Language Modelling, Emotion Recognition, Emotion Recognition in Conversation, Emotion Classification","Image, Text",English,Computer Vision,emotion-recognition-in-conversation-on-5,CC BY-NC-ND,http://doraemon.iis.sinica.edu.tw/emotionlines/download.html,https://paperswithcode.com/dataset/emotionlines,"EmotionLines contains a total of 29245 labeled utterances from 2000 dialogues. Each utterance in dialogues is labeled with one of seven emotions, six Ekman’s basic emotions plus the neutral emotion. Each labeling was accomplished by 5 workers, and for each utterance in a label, the emotion category with the highest votes was set as the label of the utterance. Those utterances voted as more than two different emotions were put into the non-neutral category. Therefore the dataset has a total of 8 types of emotion labels, anger, disgust, fear, happiness, sadness, surprise, neutral, and non-neutral.",2000,Bridging Dialogue Generation and Facial Expression Synthesis,https://arxiv.org/abs/1905.11240,,,
1017,EMOVIE,Text-To-Speech Synthesis,Text-To-Speech Synthesis,"Text-To-Speech Synthesis, Emotional Speech Synthesis","Audio, Text",English,Speech,,CC BY-NC-SA 2.0,https://viem-ccy.github.io/EMOVIE/,https://paperswithcode.com/dataset/emovie,"EMOVIE is a Mandarin emotion speech dataset including 9,724 samples with audio files and its emotion human-labeled annotation.",,,,724 samples,,
1018,EmoWOZ,Emotion Recognition,Emotion Recognition,"Emotion Recognition, Emotion Recognition in Conversation",Image,,Computer Vision,emotion-recognition-in-conversation-on-emowoz-1,Creative Commons Attribution 4.0 International,https://zenodo.org/record/6506504,https://paperswithcode.com/dataset/emowoz-1,"EmoWOZ is the first large-scale open-source dataset for emotion recognition in task-oriented dialogues. It contains emotion annotations for user utterances in the entire MultiWOZ (10k+ human-human dialogues) and DialMAGE (1k human-machine dialogues collected from our human trial). Overall, there are 83k user utterances annotated. In addition, the emotion annotation scheme is tailored to task-oriented dialogues and considers the valence, the elicitor, and the conduct of the user emotion.",,,,,,
1019,EmpatheticDialogues,Visual Dialog,Visual Dialog,"Visual Dialog, Empathetic Response Generation","Image, Text",English,Computer Vision,"visual-dialog-on-empatheticdialogues, empathetic-response-generation-on",Attribution-NonCommercial 4.0 International,https://github.com/facebookresearch/EmpatheticDialogues,https://paperswithcode.com/dataset/empatheticdialogues,"The EmpatheticDialogues dataset is a large-scale multi-turn empathetic dialogue dataset collected on the Amazon Mechanical Turk, containing 24,850 one-to-one open-domain conversations. Each conversation was obtained by pairing two crowd-workers: a speaker and a listener. The speaker is asked to talk about the personal emotional feelings. The listener infers the underlying emotion through what the speaker says and responds empathetically. The dataset provides 32 evenly distributed emotion labels.",,Empathetic Dialogue Generation viaKnowledge Enhancing and Emotion Dependency Modeling,https://arxiv.org/abs/2009.09708,,,
1020,Endomapper,Simultaneous Localization and Mapping,Simultaneous Localization and Mapping,Simultaneous Localization and Mapping,Image,,Computer Vision,,Custom,https://www.synapse.org/#!Synapse:syn26707219/wiki/615178,https://paperswithcode.com/dataset/endomapper,"The Endomapper dataset is the first collection of complete endoscopy sequences acquired during regular medical practice, including slow and careful screening explorations, making secondary use of medical data. Its original purpose is to facilitate the development and evaluation of VSLAM (Visual Simultaneous Localization and Mapping) methods in real endoscopy data. The first release of the dataset is composed of 50 sequences with a total of more than 13 hours of video. It is also the first endoscopic dataset that includes both the computed geometric and photometric endoscope calibration as well as the original calibration videos. Meta-data and annotations associated to the dataset varies from anatomical landmark and description of the procedure labeling, tools segmentation masks, COLMAP 3D reconstructions, simulated sequences with groundtruth and meta-data related to special cases, such as sequences from the same patient. This information will improve the research in endoscopic VSLAM, as well as other research lines, and create new research lines.",,,,,,
1021,EndoSLAM,Visual Odometry,Visual Odometry,"Visual Odometry, Monocular Visual Odometry, Depth Estimation","3D, Image",,Computer Vision,,,https://github.com/CapsuleEndoscope/EndoSLAM,https://paperswithcode.com/dataset/endoslam,"The endoscopic SLAM dataset (EndoSLAM) is a dataset for depth estimation approach for endoscopic videos. It consists of both ex-vivo and synthetically generated data. The ex-vivo part of the dataset includes standard as well as capsule endoscopy recordings. The dataset is divided into 35 sub-datasets. Specifically, 18, 5 and 12 sub-datasets exist for colon, small intestine and stomach respectively.",,,,,,
1022,Endotect_Polyp_Segmentation_Challenge_Dataset,Real-Time Object Detection,Real-Time Object Detection,"Real-Time Object Detection, Real-Time Semantic Segmentation, Semantic Segmentation, Image Classification, Medical Image Segmentation, Object Detection",Image,,Computer Vision,medical-image-segmentation-on-endotect-polyp,Open-access,https://endotect.com/,https://paperswithcode.com/dataset/endotect-polyp-segmentation,"A challenge that consists of three tasks, each targeting a different requirement for in-clinic use. The first task involves classifying images from the GI tract into 23 distinct classes. The second task focuses on efficiant classification measured by the amount of time spent processing each image. The last task relates to automatcially segmenting polyps.

Please cite ""The EndoTect 2020 Challenge: Evaluation andComparison of Classification, Segmentation and Inference Time for Endoscopy"" if you use the dataset.",2020,,,,,
1023,Energy_Consumption_Curves_of_499_Customers_from_Sp,Time Series Regression,Time Series Regression,Time Series Regression,Time Series,,Time Series,,CC BY 4.0,https://fordatis.fraunhofer.de/handle/fordatis/215,https://paperswithcode.com/dataset/energy-consumption-curves-of-499-customers,"Predictions of energy consumption are crucial for energy retailers to minimize deviations from energy acquired in the day-ahead market and the actual consumption of their customers. The increasing spread of smartmeters means that retailers have access to hourly consumption values of all their contracted customers in realtime. Using machine learning algorithms, these hourly values can be used to calculate predictions for the future energy consumption of the customers. The present data set allows the training and validation of AI-based prediction models.",,,,,,
1024,ENF_moving_video,ENF (Electric Network Frequency) Extraction from Video,ENF (Electric Network Frequency) Extraction from Video,ENF (Electric Network Frequency) Extraction from Video,"Graph, Video",,Methodology,,,https://zenodo.org/records/3549379#.Y6WRPXZByUk,https://paperswithcode.com/dataset/enf-moving-video,"The ENF moving video dataset, which is a subset of the dataset used
in Temporal Localization of Non-Static Digital Videos Using the Electrical Network Frequency , consists of video recording without the audio channel coupled with the
corresponding power ENF signal reference in WAV format at a rate of 1 kHz. The
dataset is made of 8 video clips recorded in Europe at 29.97 frames per second,
with a duration of approximately 11-12 minutes, using a GoPro Hero 4 Black
and an NK AC3061-4KN camera. In terms of content, videos 1-3 are entirely
stationary, videos 4-5 are predominantly stationary with some movement, and
videos 6-8 are non-stationary, meaning the camera is fixed, but there are moving
objects in most frames. All videos depict natural, everyday indoor scenes (i.e.,
not plain backgrounds).",,,,,,
1025,English-Pashto_Language_Dataset__EPLD_,Multilingual NLP,Multilingual NLP,"Multilingual NLP, Language Identification",Text,English,Natural Language Processing,,CC BY 4.0,https://data.mendeley.com/datasets/vmgv4s6vrn/1,https://paperswithcode.com/dataset/english-pashto-language-dataset-epld,"The English-Pashto Language Dataset (EPLD) is a comprehensive resource aimed to provide linguistic insights into the Pashto language. It contains the knowledge and study of Pashto language with the basics of communication like counting, alphabets, pronoun, basic sentences used in everyday life. Every data is translated from English to Pashto for better human understanding and clarity. The data is carefully proofread and verified by the native speakers and the language experts. Pashto language has multiple variations and accents depending on the geographical factors. 
This dataset explains and addresses the key differences of words and sounds of Pashto, which may sound similar or different from English on the basis of gender, tense of the statement, relationship of the speaker etc. This dataset is designed to support language learning, natural language processing (NLP) research and computational linguistic studies focusing on Pashto language.",,,,,,
1026,English_Web_Treebank,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Part-Of-Speech Tagging, Dependency Parsing","Audio, Text",English,Natural Language Processing,,Custom,https://catalog.ldc.upenn.edu/LDC2012T13,https://paperswithcode.com/dataset/english-web-treebank,"English Web Treebank is a dataset containing 254,830 word-level tokens and 16,624 sentence-level tokens of webtext in 1174 files annotated for sentence- and word-level tokenization, part-of-speech, and syntactic structure. The data is roughly evenly divided across five genres: weblogs, newsgroups, email, reviews, and question-answers. The files were manually annotated following the sentence-level tokenization guidelines for web text and the word-level tokenization guidelines developed for English treebanks in the DARPA GALE project. Only text from the subject line and message body of posts, articles, messages and question-answers were collected and annotated.",,,,,,
1027,ENRICH,3D Reconstruction,3D Reconstruction,"3D Reconstruction, Key Point Matching, Depth Estimation, 3D Scene Reconstruction",3D,,Methodology,,CC BY-NC 3.0,https://github.com/davidemarelli/ENRICH,https://paperswithcode.com/dataset/enrich,"A new synthetic, multi-purpose dataset - called ENRICH - for testing photogrammetric and computer vision algorithms. Compared to existing datasets, ENRICH offers higher resolution images also rendered with different lighting conditions, camera orientation, scales, and field of view. Specifically, ENRICH is composed of three sub-datasets: ENRICH-Aerial, ENRICH-Square, and ENRICH-Statue, each exhibiting different characteristics. The proposed dataset is useful for several photogrammetry and computer vision-related tasks, such as the evaluation of hand-crafted and deep learning-based local features, effects of ground control points (GCPs) configuration on the 3D accuracy, and monocular depth estimation.

Each zip file in the root is relative to a specific dataset:


ENRICH-Aerial, is an aerial image block of the city of Launceston, Australia. The acquisition is performed by simulating a typical oblique aerial camera with five views (nadir and four oblique views).
ENRICH-Square, is a ground-level dataset of a square captured by four cameras, each one moving on a different path with different focal length, orientation, and lighting conditions.
ENRICH-Statue, is a ground-level dataset portraying a statue (placed in the center of the ENRICH-Square scene), acquired using four cameras moving on different paths with different focal lengths, orientations, and lighting conditions.

Be sure to check the README file in the dataset root for information on folder structure and file contents. Please refer to the related paper (https://doi.org/10.1016/j.isprsjprs.2023.03.002) for information about the generation method and the purpose of ENRICH.",2023,,,,"testing photogrammetric and computer vision algorithms. Compared to existing datasets, ENRICH offers higher resolution images",
1028,Enron_Emails,Authorship Verification,Authorship Verification,"Authorship Verification, Actionable Phrase Detection, Dynamic Link Prediction","Image, Time Series",,Computer Vision,"dynamic-link-prediction-on-enron-email, actionable-phrase-detection-on-enron-email",,https://www.cs.cmu.edu/~enron/,https://paperswithcode.com/dataset/enron-email-dataset,"This dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a total of about 0.5M messages. This data was originally made public, and posted to the web, by the Federal Energy Regulatory Commission during its investigation.",,,,,,
1029,ENST_Drums,Drum Transcription in Music (DTM),Drum Transcription in Music (DTM),"Drum Transcription in Music (DTM), Drum Transcription, Multi-instrument Music Transcription",Audio,,Audio,drum-transcription-in-music-dtm-on-enst-drums,,https://perso.telecom-paristech.fr/grichard/ENST-drums/,https://paperswithcode.com/dataset/enst-drums,"ENST-Drums: an extensive audio-visual database for drum signals processing
Olivier Gillet and Gaël Richard
GET / ENST, CNRS LTCI, 37 rue Dareau, 75014 Paris, France

The ENST-Drums database is a large and varied research database for automatic drum transcription and processing:

Three professional drummers specialized in different music genres were recorded.
Total duration of audio material recorded per drummer is around 75 minutes.
Each drummer played his own drum kit.
Each sequence used either sticks, rods, brushes or mallets to increase the diversity of drum sounds.
The drum kits themselves are varied, ranging from a small, portable, kit with two toms and 2 cymbals, suitable for jazz and latin music ; to a larger rock drum set with 4 toms and 5 cymbals.
Each sequence is recorded on 8 individual audio channels, is filmed from two angles, and is fully annotated",,,,,,
1030,ENT-DESC,Text Generation,Text Generation,"Text Generation, KG-to-Text Generation, Graph-to-Sequence, Knowledge Graphs","Graph, Text, Time Series",English,Natural Language Processing,kg-to-text-generation-on-ent-desc,,https://github.com/LiyingCheng95/EntityDescriptionGeneration,https://paperswithcode.com/dataset/ent-desc,"ENT-DESC involves retrieving abundant knowledge of various types of main entities from a large knowledge graph (KG), which makes the current graph-to-sequence models severely suffer from the problems of information loss and parameter explosion while generating the descriptions.",,,,,,
1031,EntityQuestions,Passage Retrieval,Passage Retrieval,Passage Retrieval,,,Methodology,passage-retrieval-on-entityquestions,,https://github.com/princeton-nlp/EntityQuestions,https://paperswithcode.com/dataset/entityquestions,"EntityQuestions is a dataset of simple, entity-rich questions based on facts from Wikidata (e.g., ""Where was Arve Furset born? "").",,,,,,
1032,EntitySeg,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Image Segmentation",Image,,Computer Vision,,Creative Commons Attribution 4.0 International,http://luqi.info/entityv2.github.io/,https://paperswithcode.com/dataset/entityseg,"The EntitySeg dataset contains 33,227 images with high-quality mask annotations. Compared with existing dataets, there are three distinct properties in EntitySeg. First, 71.25% and 86.23% of the images are of high resolution with at least 2000px×2000px and 1000px×1000px which is more consistent with current digital imaging trends. Second, the dataset is open-world and is not limited to predefined classes. Third, the mask annotation along the boundaries are more accurate than existing datasets.",,Fine-Grained Entity Segmentation,https://arxiv.org/pdf/2211.05776v1.pdf,227 images,,
1033,EORSSD,Object Detection,Object Detection,"Object Detection, Salient Object Detection",Image,,Computer Vision,,,https://github.com/rmcong/EORSSD-dataset,https://paperswithcode.com/dataset/eorssd,"The Extended Optical Remote Sensing Saliency Detection (EORSSD) dataset is an extension of the ORSSD dataset. This new dataset is larger and more varied than the original. It contains 2,000 images and corresponding pixel-wise ground truth, which includes many semantically meaningful but challenging images.",,Zhang et al,https://arxiv.org/pdf/2011.13144v1.pdf,000 images,,
1034,EPFLP300_MOABB,Within-Session ERP,Within-Session ERP,Within-Session ERP,,,Methodology,within-session-erp-on-epflp300-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.EPFLP300.html,https://paperswithcode.com/dataset/epflp300-moabb,,,,,,,
1035,EPHOIE,Key Information Extraction,Key Information Extraction,"Key Information Extraction, Document AI",Text,English,Natural Language Processing,"document-ai-on-ephoie, key-information-extraction-on-ephoie",Custom,https://github.com/HCIILAB/EPHOIE,https://paperswithcode.com/dataset/ephoie,"EPHOIE is a fully-annotated dataset which is the first Chinese benchmark for both text spotting and visual information extraction. EPHOIE consists of 1,494 images of examination paper head with complex layouts and background, including a total of 15,771 Chinese handwritten or printed text instances.",,,,494 images,,
1036,EPIC-KITCHENS-100,Audio Classification,Audio Classification,"Audio Classification, Action Anticipation, Temporal Action Localization, Open Vocabulary Action Recognition, Unsupervised Domain Adaptation, Action Recognition, Multi-Instance Retrieval","Audio, Image, Time Series, Video",,Audio,"multi-instance-retrieval-on-epic-kitchens-100, action-recognition-on-epic-kitchens-100, temporal-action-localization-on-epic-kitchens, audio-classification-on-epic-kitchens-100, open-vocabulary-action-recognition-on-epic, action-anticipation-on-epic-kitchens-100, unsupervised-domain-adaptation-on-epic",CC BY NC 4.0,https://epic-kitchens.github.io/2021,https://paperswithcode.com/dataset/epic-kitchens-100,"This paper introduces the pipeline to scale the largest dataset in egocentric vision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (EPIC-KITCHENS-55), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection also enables evaluating the ""test of time"" - i.e. whether models trained on data collected in 2018 can generalise to new footage collected under the same hypotheses albeit ""two years on"".
The dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.",2018,,,,,
1037,EPIC-KITCHENS-55,Video Understanding,Video Understanding,"Video Understanding, Video Object Detection, Egocentric Activity Recognition, Action Recognition","Image, Video",,Computer Vision,"egocentric-activity-recognition-on-epic-1, action-recognition-in-videos-on-epic-kitchens, video-object-detection-on-epic-kitchens-55",,https://epic-kitchens.github.io/2019,https://paperswithcode.com/dataset/epic-kitchens,"The EPIC-KITCHENS-55 dataset comprises a set of 432 egocentric videos recorded by 32 participants in their kitchens at 60fps with a head mounted camera. There is no guiding script for the participants who freely perform activities in kitchens related to cooking, food preparation or washing up among others. Each video is split into short action segments (mean duration is 3.7s) with specific start and end times and a verb and noun annotation describing the action (e.g. ‘open fridge‘). The verb classes are 125 and the noun classes 331. The dataset is divided into one train and two test splits.",,Egocentric Hand Track and Object-based Human Action Recognition,https://arxiv.org/abs/1905.00742,,,
1038,Epilepsy_seizure_prediction,Epilepsy Prediction,Epilepsy Prediction,Epilepsy Prediction,Time Series,,Methodology,epilepsy-prediction-on-epilepsy-seizure,,https://archive.ics.uci.edu/ml/datasets/Epileptic+Seizure+Recognition,https://paperswithcode.com/dataset/epilepsy-seizure-prediction,"The original dataset from the reference consists of 5 different folders, each with 100 files, with each file representing a single subject/person. Each file is a recording of brain activity for 23.6 seconds. The corresponding time-series is sampled into 4097 data points. Each data point is the value of the EEG recording at a different point in time. So we have total 500 individuals with each has 4097 data points for 23.5 seconds.

We divided and shuffled every 4097 data points into 23 chunks, each chunk contains 178 data points for 1 second, and each data point is the value of the EEG recording at a different point in time. So now we have 23 x 500 = 11500 pieces of information(row), each information contains 178 data points for 1 second(column), the last column represents the label y {1,2,3,4,5}.

The response variable is y in column 179, the Explanatory variables X1, X2, ..., X178

y contains the category of the 178-dimensional input vector. Specifically y in {1, 2, 3, 4, 5}:

5 - eyes open, means when they were recording the EEG signal of the brain the patient had their eyes open

4 - eyes closed, means when they were recording the EEG signal the patient had their eyes closed

3 - Yes they identify where the region of the tumor was in the brain and recording the EEG activity from the healthy brain area

2 - They recorder the EEG from the area where the tumor was located

1 - Recording of seizure activity

All subjects falling in classes 2, 3, 4, and 5 are subjects who did not have epileptic seizure. Only subjects in class 1 have epileptic seizure. Our motivation for creating this version of the data was to simplify access to the data via the creation of a .csv version of it. Although there are 5 classes most authors have done binary classification, namely class 1 (Epileptic seizure) against the rest.",,,,,,5
1039,Epinion,Change Point Detection,Change Point Detection,"Change Point Detection, Outlier Detection, Recommendation Systems",Image,,Computer Vision,recommendation-systems-on-epinions-extend,,https://www.cse.msu.edu/~tangjili/datasetcode/truststudy.htm,https://paperswithcode.com/dataset/epinion,"The Epinions dataset is trust network dataset. For each user, it contains his profile, his ratings and his trust relations. For each rating, it has the product name and its category, the rating score, the time point when the rating is created, and the helpfulness of this rating.",,,,,,
1040,Epinions,Link Sign Prediction,Link Sign Prediction,"Link Sign Prediction, Recommendation Systems",Time Series,,Methodology,"recommendation-systems-on-epinions, link-sign-prediction-on-epinions",,https://snap.stanford.edu/data/soc-Epinions1.html,https://paperswithcode.com/dataset/epinions,"The Epinions dataset is built form a who-trust-whom online social network of a general consumer review site Epinions.com. Members of the site can decide whether to ''trust'' each other. All the trust relationships interact and form the Web of Trust which is then combined with review ratings to determine which reviews are shown to the user.
It contains 75,879 nodes and 50,8837 edges.",,,,,,
1041,EQ-Bench,Emotional Intelligence,Emotional Intelligence,Emotional Intelligence,,,Methodology,emotional-intelligence-on-emotional,MIT,https://www.eqbench.com,https://paperswithcode.com/dataset/emotional-intelligence,"This dataset contains benchmark scores for EQ-Bench, a novel benchmark designed to evaluate aspects of emotional intelligence in Large Language Models (LLMs). We assess the ability of LLMs to understand complex emotions and social interactions by asking them to predict the intensity of emotional states of characters in a dialogue. The benchmark is able to discriminate effectively between a wide range of models. We find that EQ-Bench correlates strongly with comprehensive multi-domain benchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may be capturing similar aspects of broad intelligence. Our benchmark produces highly repeatable results using a set of 60 English-language questions. We also provide open-source code for an automated benchmarking pipeline at https://github.com/EQ-bench/EQ-Bench and a leaderboard at https://www.eqbench.com.",2020,,,,,
1042,Equilibrium-Traffic-Networks,Equilibrium traffic assignment,Equilibrium traffic assignment,"Equilibrium traffic assignment, Traffic Prediction",Time Series,,Methodology,,CC BY 4.0,https://figshare.com/articles/dataset/_b_Equilibrium-Traffic-Networks_b_/27889251,https://paperswithcode.com/dataset/equilibrium-traffic-networks,"This repository contains three graph datasets for the UE traffic assignment problem on Sioux-Falls, Eastern-Massachusetts and Anaheim networks in both dgl and pyg formats. The datasets are generated and used to train and evaluate models for solving the User Equilibrium (UE) problem on three transportation networks:



 Sioux-Falls



 Eastern-Massachusetts



Anaheim 



These networks are sourced from the well-known  ""transport networks for research""  repository.

These datasets are generated for the study ""A hybrid deep-learning-metaheuristic framework for bi-level network design problems"" by Bahman Madadi and Gonçalo H. de Almeida Correia, published in Expert Systems with Applications. 

Metadata
Detailed information can be found in the  Metadata.md  file.

Links

Figshare Dataset Repository: Link

References

Article: A hybrid deep-learning-metaheuristic framework for bi-level network design problems - Link
GitHub Repository: HDLMF_GIN-GA - Link
Primary Figshare Data Repository: Link

Note: This dataset is maintained at Figshare.",,,,,,
1043,ERA,Word Embeddings,Word Embeddings,"Word Embeddings, Image Dehazing, Graph Embedding","Graph, Image",,Computer Vision,,,https://lcmou.github.io/ERA_Dataset/,https://paperswithcode.com/dataset/era,"Consists of 2,864 videos each with a label from 25 different classes corresponding to an event unfolding 5 seconds. The ERA dataset is designed to have a significant intra-class variation and inter-class similarity and captures dynamic events in various circumstances and at dramatically various scales.",,,,,,
1044,ErhuPT,Sound Event Detection,Sound Event Detection,"Sound Event Detection, Music Transcription, Melody Extraction","Audio, Image",,Computer Vision,,CC BY 4.0,https://zenodo.org/record/4320991,https://paperswithcode.com/dataset/erhupt,This dataset is an audio dataset containing about 1500 audio clips recorded by multiple professional players.,,,,,,
1045,ESB,Speech Recognition,Speech Recognition,"Speech Recognition, Automatic Speech Recognition (ASR)","Audio, Image, Text",English,Speech,,,https://huggingface.co/esb,https://paperswithcode.com/dataset/esb,"ESB is a benchmark for evaluating the performance of a single automatic speech recognition (ASR) system across a broad set of speech datasets. It comprises eight English speech recognition datasets, capturing a broad range of domains, acoustic conditions, speaker styles, and transcription requirements.",,ESB: A Benchmark For Multi-Domain End-to-End Speech Recognition,https://arxiv.org/pdf/2210.13352v1.pdf,,,
1046,ESC-50,Environmental Sound Classification,Environmental Sound Classification,"Environmental Sound Classification, Audio Classification, Data Augmentation, Image Classification, Environment Sound Classification, Zero-Shot Environment Sound Classification, Few-Shot Audio Classification, Self-Supervised Audio Classification, Zero-shot Audio Classification","Audio, Image",,Computer Vision,"environmental-sound-classification-on-esc-50, few-shot-audio-classification-on-esc-50, environment-sound-classification-on-esc-50, zero-shot-environment-sound-classification-on-1, audio-classification-on-esc-50, zero-shot-audio-classification-on-esc-50, self-supervised-audio-classification-on-esc, image-classification-on-esc-50",CC BY-NC 3.0,https://github.com/karolpiczak/ESC-50,https://paperswithcode.com/dataset/esc-50,"The ESC-50 dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. It comprises 2000 5s-clips of 50 different classes across natural, human and domestic sounds, again, drawn from Freesound.org.",2000,The NIGENS General Sound Events Database,https://arxiv.org/abs/1902.08314,,,
1047,ESC50,Environmental Sound Classification,Environmental Sound Classification,"Environmental Sound Classification, Linear evaluation","Audio, Image",,Computer Vision,linear-evaluation-on-esc50,Creative Commons Attribution 4.0 International  Non-Commercial license.,https://github.com/karolpiczak/ESC-50,https://paperswithcode.com/dataset/esc50,"The ESC-50 dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification.

The dataset consists of 5-second-long recordings organized into 50 semantical classes (with 40 examples per class) loosely arranged into 5 major categories.

Reference: https://dl.acm.org/doi/10.1145/2733373.2806390",2000,,,40 examples,,
1048,ESD,Voice Conversion,Voice Conversion,Voice Conversion,Audio,,Audio,,Custom (research-only),https://hltsingapore.github.io/ESD/,https://paperswithcode.com/dataset/esd,"ESD is an Emotional Speech Database for voice conversion research. The ESD database consists of 350 parallel utterances spoken by 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). More than 29 hours of speech data were recorded in a controlled acoustic environment. The database is suitable for multi-speaker and cross-lingual emotional voice conversion studies.",,,,,,
1049,ESOL__Estimated_SOLubility_,Log Solubility,Log Solubility,"Log Solubility, Molecular Property Prediction, Drug Discovery, Graph Regression","Graph, Time Series",,Methodology,"drug-discovery-on-esol-scaffold, molecular-property-prediction-on-esol, graph-regression-on-esol, log-solubility-on-esol",,https://moleculenet.org,https://paperswithcode.com/dataset/esol-scaffold,ESOL is a water solubility prediction dataset consisting of 1128 samples.,,,,1128 samples,,
1050,eSports_Sensors_Dataset,Sensor Modeling,Sensor Modeling,"Sensor Modeling, Skills Assessment, Real-Time Strategy Games, Physiological Computing, Person Re-Identification, Skills Evaluation, Time Series Analysis","Image, Time Series",,Computer Vision,"skills-evaluation-on-esports-sensors-dataset, person-re-identification-on-esports-sensors",Custom,https://github.com/smerdov/eSports_Sensors_Dataset,https://paperswithcode.com/dataset/esports-sensors-dataset,"The eSports Sensors dataset contains sensor data collected from 10 players in 22 matches in League of Legends. The sensor data collected includes:


Hand/head/chair movements.
Heart rate.
Muscle activity.
Gaze movement on the monitor.
Galvanic skin response(GSR).
Electroencephalography (EEG).
Mouse and keyboard activity.
Facial skin temperature.
Environmental data.

The data were collected for one team of 5 people simultaneously. In-game logs and meta information for each match are also provided for each match.",,,,,,
1051,ETD500,Key Information Extraction,Key Information Extraction,Key Information Extraction,,,Methodology,key-information-extraction-on-etd500,,,https://paperswithcode.com/dataset/etd500,"The paper used 500 scanned Electronic Theses and Dissertation cover pages (i.e., front pages). The dataset contains several intermediate datasets, briefly discussed in the paper.",,,,,,
1052,ETH,Multi-future Trajectory Prediction,Multi-future Trajectory Prediction,"Multi-future Trajectory Prediction, Trajectory Prediction, Person Re-Identification, Pedestrian Detection, Multi Future Trajectory Prediction, Object Detection","Image, Time Series",,Computer Vision,"multi-future-trajectory-prediction-on-eth-ucy-1, trajectory-prediction-on-ethucy, trajectory-prediction-on-eth, trajectory-prediction-on-eth-biwi-walking, multi-future-trajectory-prediction-on-eth-ucy",,https://data.vision.ee.ethz.ch/cvl/aess/dataset/,https://paperswithcode.com/dataset/eth,"ETH is a dataset for pedestrian detection. The testing set contains 1,804 images in three video clips. The dataset is captured from a stereo rig mounted on car, with a resolution of 640 x 480 (bayered), and a framerate of 13--14 FPS.",,Scale-aware Fast R-CNN for Pedestrian Detection,https://arxiv.org/abs/1510.08160,804 images,"testing set contains 1,804 images",
1053,ETH3D,Depth Estimation,Depth Estimation,"Depth Estimation, Monocular Depth Estimation, Stereo Matching, Multi-View 3D Reconstruction, Dense Pixel Correspondence Estimation, 3D Reconstruction",3D,,Methodology,"monocular-depth-estimation-on-eth3d, dense-pixel-correspondence-estimation-on-3, multi-view-3d-reconstruction-on-eth3d",CC BY-NC-SA 4.0,https://www.eth3d.net/,https://paperswithcode.com/dataset/eth3d,"ETHD is a multi-view stereo benchmark / 3D reconstruction benchmark that covers a variety of indoor and outdoor scenes.
Ground truth geometry has been obtained using a high-precision laser scanner.
A DSLR camera as well as a synchronized multi-camera rig with varying field-of-view was used to capture images.",,,,,,
1054,ETHICS,Multi-Task Learning,Multi-Task Learning,Multi-Task Learning,,,Methodology,,,https://github.com/hendrycks/ethics/,https://paperswithcode.com/dataset/ethics-1,"A new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality.",,,,,,
1055,Ethics__per_ethics_,Logical Reasoning,Logical Reasoning,"Logical Reasoning, Ethics",,,Reasoning,ethics-on-ethics-2,Apache-2.0,http://tape-benchmark.com/datasets.html#ethics2,https://paperswithcode.com/dataset/ethics-2,"Ethics (per ethics) dataset is created to test the knowledge of the basic concepts of morality. The task is to predict human ethical judgments about diverse text situations in a multi-label classification setting. The main objective of the task is to evaluate the positive or negative implementation of five concepts in normative with ‘yes’ and ‘no’ ratings. The included concepts are as follows: virtue, law, moral, justice, and utilitarianism.

Motivation

There are a multitude of approaches to evaluating ethics in machine learning. The Ethics dataset for Russian is created from scratch for the first time, relying on the design compatible with (Hendrycks et al., 2021).

Our Ethics dataset would go through community validation and discussion as it is the first ethics dataset for Russian based on the established methodology. We acknowledge that the work (Hendrycks et al., 2021) has flaws; thus, we do not reproduce the generative approach. We construct the dataset using a similar annotation scheme: we avoid the direct question of whether the deed is good or bad. Instead, we make annotations according to five criteria that describe the aspects of the annotators' attitude to the deed.

An example in English for illustration purposes:

{
    'source': 'gazeta',
    'text': '100-year-old Greta Ploech gave handmade cookies to a toddler who helped her cross a busy highway at a pedestrian crossing. The video was posted on the Readers Channel.', 
    'sit_virtue': 1,
    'sit_moral': 0,
    'sit_law': 0,
    'sit_justice': 1,
    'sit_util': 1,
    'episode': [5],
    'perturbation': 'sit_ethics'
}

Data Fields


text: a string containing the body of a news article or a fiction text
source: a string containing the source of the text
per_virtue: an integer, either 0 or 1, indicating whether virtue standards are violated in the text
per_moral: an integer, either 0 or 1, indicating whether moral standards are violated in the text
per_law: an integer, either 0 or 1, indicating whether any laws are violated in the text
per_justice: an integer, either 0 or 1, indicating whether justice norms are violated in the text
per_util: an integer, either 0 or 1, indicating whether utilitarianism norms are violated in the text
perturbation: a string containing the name of the perturbation applied to text. If no perturbation was applied, the dataset name is used
episode: a list of episodes in which the instance is used. Only used for the train set

Data Splits

The dataset consists of a training set with labeled examples and a test set in two configurations:


raw data: includes the original data with no additional sampling
episodes: data is split into evaluation episodes and includes several perturbations of test for robustness evaluation

Test Perturbations

Each training episode in the dataset corresponds to seven test variations, including the original test data and six adversarial test sets, acquired through the modification of the original test through the following text perturbations:


ButterFingers: randomly adds noise to data by mimicking spelling mistakes made by humans through character swaps based on their keyboard distance
Emojify: replaces the input words with the corresponding emojis, preserving their original meaning
EDAdelete: randomly deletes tokens in the text
EDAswap: randomly swaps tokens in the text
BackTranslation: generates variations of the context through back-translation (ru -> en -> ru)
AddSent: generates an extra sentence at the end of the text",2021,,,,,
1056,ETHOS,Data Augmentation,Data Augmentation,"Data Augmentation, Image Classification, Hate Speech Detection","Audio, Image",,Computer Vision,"hate-speech-detection-on-ethos-multilabel, hate-speech-detection-on-ethos-binary",GNU GPLv3,https://github.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset,https://paperswithcode.com/dataset/ethos,"ETHOS is a hate speech detection dataset. It is built from YouTube and Reddit comments validated through a crowdsourcing platform. It has two subsets, one for binary classification and the other for multi-label classification. The former contains 998 comments, while the latter contains fine-grained hate-speech annotations for 433 comments.",,https://arxiv.org/abs/2006.08328,https://arxiv.org/abs/2006.08328,,,
1057,ETHZ-Shape,Graph Learning,Graph Learning,Graph Learning,Graph,,Methodology,,,http://calvin-vision.net/datasets/ethz-shape-classes/,https://paperswithcode.com/dataset/ethz-shape,"The ETHZ Shape dataset contains images of five diverse shape-based classes, collected from Flickr and Google Images. The main challenges it offers are clutter, intra-class shape variability, and scale changes. The authors deliberately selected several images where the object comprises only a rather small portion of the image, and made an effort to include objects appearing at a wide range of scales. The objects are mostly unoccluded and are all taken from approximately the same viewpoint (the side).",,,,,,
1058,ETH_Py150_Open,Program Repair,Program Repair,"Program Repair, Natural Language Understanding, Contextual Embedding for Source Code",Text,English,Natural Language Processing,,,https://github.com/google-research-datasets/eth_py150_open,https://paperswithcode.com/dataset/eth-py150-open,"A massive, deduplicated corpus of 7.4M Python files from GitHub.",,,,,,
1059,ETT,Multivariate Time Series Forecastingm,Multivariate Time Series Forecastingm,"Multivariate Time Series Forecastingm, Univariate Time Series Forecasting, Time Series, Time Series Forecasting, GLinear, Multivariate Time Series Forecasting",Time Series,,Time Series,"time-series-forecasting-on-etth1-192-1, time-series-forecasting-on-etth1-48-3, glinear-on-etth1-24, time-series-on-etth1-336-multivariate, time-series-forecasting-on-etth1-720-1, multivariate-time-series-forecasting-on-etth1, multivariate-time-series-forecasting-on-etth1-2, glinear-on-etth1-336, multivariate-time-series-forecastingm-on-1, time-series-forecasting-on-etth1-336-1, multivariate-time-series-forecasting-on-etth1-1, time-series-forecasting-on-ettm1-192-1, time-series-on-etth1-720-multivariate, time-series-forecasting-on-ettm2-96-1, time-series-forecasting-on-ettm1-720-1, time-series-forecasting-on-etth1-96-1, glinear-on-etth1-48, time-series-forecasting-on-ettm2-336-1, multivariate-time-series-forecasting-on-etth1-3, glinear-on-etth1-96, glinear-on-etth1-192, glinear-on-etth1-720-multivariate, time-series-forecasting-on-ettm2-720-1, time-series-forecasting-on-etth1-96-4, time-series-forecasting-on-ettm1-336-1, time-series-on-etth1-720, time-series-forecasting-on-ettm2-192-1",,https://github.com/zhouhaoyi/ETDataset,https://paperswithcode.com/dataset/ett,"The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment. This dataset consists of 2 years data from two separated counties in China. To explore the granularity on the Long sequence time-series forecasting (LSTF) problem, different subsets are created, {ETTh1, ETTh2} for 1-hour-level and ETTm1 for 15-minutes-level. Each data point consists of the target value ”oil temperature” and 6 power load features. The train/val/test is 12/4/4 months.",,https://arxiv.org/pdf/2012.07436.pdf,https://arxiv.org/pdf/2012.07436.pdf,,,
1060,ETTh1__192_,GLinear,GLinear,GLinear,,,Methodology,glinear-on-etth1-192,,https://github.com/zhouhaoyi/ETDataset,https://paperswithcode.com/dataset/etth1-192,"The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment. This dataset consists of 2 years data from two separated counties in China. To explore the granularity on the Long sequence time-series forecasting (LSTF) problem, different subsets are created, {ETTh1, ETTh2} for 1-hour-level and ETTm1 for 15-minutes-level. Each data point consists of the target value ”oil temperature” and 6 power load features. The train/val/test is 12/4/4 months.",,https://arxiv.org/pdf/2012.07436.pdf,https://arxiv.org/pdf/2012.07436.pdf,,,
1061,ETTh1__96_,GLinear,GLinear,"GLinear, Time Series Forecasting",Time Series,,Methodology,"glinear-on-etth1-96, time-series-forecasting-on-etth1-96-4",,https://github.com/zhouhaoyi/ETDataset,https://paperswithcode.com/dataset/etth1-96,"The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment. This dataset consists of 2 years data from two separated counties in China. To explore the granularity on the Long sequence time-series forecasting (LSTF) problem, different subsets are created, {ETTh1, ETTh2} for 1-hour-level and ETTm1 for 15-minutes-level. Each data point consists of the target value ”oil temperature” and 6 power load features. The train/val/test is 12/4/4 months.",,https://arxiv.org/pdf/2012.07436.pdf,https://arxiv.org/pdf/2012.07436.pdf,,,
1062,EUCA_dataset,Explainable artificial intelligence,Explainable artificial intelligence,Explainable artificial intelligence,,,Methodology,,,https://github.com/weinajin/euca/tree/master/EUCA_dataset_quantitative_and_qualitative_data,https://paperswithcode.com/dataset/euca-dataset,"EUCA dataset description
Associated Paper: 
EUCA: the End-User-Centered Explainable AI Framework

Authors:
Weina Jin, Jianyu Fan, Diane Gromala, Philippe Pasquier, Ghassan Hamarneh

Introduction:
EUCA dataset is for modelling personalized or interactive explainable AI. It contains 309 data points of 32 end-users' preferences on 12 forms of explanation (including feature-, example-, and rule-based explanations). The data were collected from a user study on 32 layperson participants in the Greater Vancouver city area in 2019-2020. In the user study, the participants (P01-P32) were presented with AI-assisted critical tasks on house price prediction, health status prediction, purchasing a self-driving car, and studying for a biological exam 1. Within each task and for its given explanation goal 2, the participants selected and rank the explanatory forms 3 that they saw the most suitable. 

1 EUCA_EndUserXAI_ExplanatoryFormRanking.csv

Column description:


Index - Participants' number
Case - task-explanation goal combination
accept to use AI? trust it? - Participants response to whether they will use AI given the task and explanation goal
require explanation? - Participants response to the question whether they request an explanation for the AI
1st, 2nd, 3rd, ... - Explanatory form card selection and ranking 
     cards fulfill requirement? - After the card selection, participants were asked whether the selected card combination fulfill their explainability requirement.

2  EUCA_EndUserXAI_demography.csv

It contains the participants demographics, including their age, gender, educational background, and their knowledge and attitudes toward AI.

EUCA dataset zip file for download

More Context for EUCA Dataset
1 Critical tasks
There are four tasks. Task label and their corresponding task titles are:
house - Selling your house 
car - Buying an autonomous driving vehicle
health - Personal health decision
bird - Learning bird species

Please refer to EUCA quantatative data analysis report for the storyboard of the tasks and explanation goals presented in the user study.

2  Explanation goal
End-users may have different goals/purposes to check an explanation from AI. The EUCA dataset includes the following 11 explanation goals, with its [label] in the dataset, full name and description


[trust] Calibrate trust: trust is a key to
   establish human-AI decision-making partnership. Since users can
   easily distrust or overtrust AI, it is important to calibrate the
   trust to reflect the capabilities of AI systems.

[safe] Ensure safety: users need to ensure
   safety of the decision consequences.



[bias] - Detect bias: users need to ensure the
   decision is impartial and unbiased.



[unexpect] Resolve disagreement with AI: the AI
   prediction is unexpected and there are
   disagreements between users and AI. 



[expected] - Expected: the AI's prediction is
    expected and aligns with users'
    expectations.



[differentiate] Differentiate similar instances: due to
   the consequences of wrong decisions, users sometimes need to discern
   similar instances or outcomes. For example, a doctor differentiates
   whether the diagnosis is a benign or malignant tumor.



[learning] Learn: users need to gain knowledge,
   improve their problem-solving skills, and discover new knowledge



[control] Improve: users seek causal factors to
   control and improve the predicted outcome.



[communicate] Communicate with stakeholders: many
   critical decision-making processes involve multiple stakeholders,
   and users need to discuss the decision with them.



[report] Generate reports: users need to utilize
    the explanations to perform particular tasks such as report
    production. For example, a radiologist generates a medical report on
    a patient's X-ray image.



[multi] Trade-off multiple objectives: AI may be
    optimized on an incomplete objective while the users seek to fulfill
    multiple objectives in real-world applications. For example, a
    doctor needs to ensure a treatment plan is effective as well as has
    acceptable patient adherence. Ethical and legal requirements may
    also be included as objectives.



3 Explanatory form
The following 12 explanatory forms are end-user-friendly, i.e.: no technical knowledge is required for the end-user to interpret the explanation.


Feature-Based Explanation
Feature Attribution - fa    
Note: for tasks that has image as input data, the feature attribution is denoted by the following two cards:
ir:  important regions (a.k.a. heat map or saliency map)
irc: important regions with their feature contribution percentage


Feature Shape - fs

Feature Interaction - fi



Example-Based Explanation


Similar Example - se
Typical Example - te

Counterfactual Example - ce


Note: for contractual example, there were two visual variations used in the user study:
cet:  counterfactual example with transition from one example to the counterfactual one
ceh:  counterfactual example with the contrastive feature highlighted



Rule-Based Explanation


Rule - rt
Decision Tree - dt

Decision Flow - df



Supplementary Information


Input
Output
Performance
Dataset - prior  (output prediction with prior distribution of each class in the training set)

Note: occasionally there is a wild card, which means the participant draw the card by themselves. It is indicated as 'wc'.

For visual examples of each explanatory form card, please refer to the Explanatory_form_labels.pdf document.

Link to the details on users' requirements on different explanatory forms

Code and report for EUCA data quantatitve analysis

EUCA data analysis code
EUCA quantatative data analysis report

EUCA data citation
@article{jin2021euca,
   title={EUCA: the End-User-Centered Explainable AI Framework},
      author={Weina Jin and Jianyu Fan and Diane Gromala and Philippe Pasquier and Ghassan Hamarneh},
      year={2021},
      eprint={2102.02437},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}",2019,EUCA: the End-User-Centered Explainable AI Framework,http://arxiv.org/abs/2102.02437,,,
1063,EUEN17037_Daylight_and_View_Standard_TestDataSet,3D Object Classification,3D Object Classification,"3D Object Classification, 3D Depth Estimation, 3D Geometry Perception","3D, Image",,Computer Vision,,CC-BY,https://github.com/shervinazadi/EN_17037_Compliance/tree/master/data,https://paperswithcode.com/dataset/euen17037-daylight-and-view-standard,EUEN17037 Daylight and View Standard Test Dataset.,,,,,,
1064,EurekaAlert,Text Summarization,Text Summarization,"Text Summarization, Text Simplification",Text,English,Natural Language Processing,"text-simplification-on-eurekaalert, text-summarization-on-eurekaalert",,https://github.com/farooqzaman1/HTSS/tree/main/data,https://paperswithcode.com/dataset/eurekaalert,"This dataset contains around 5000 scholarly articles and their corresponding easy summary from eureka alert blog, the dataset can be used for the combined task of summarization and simplification.",,,,,,
1065,EURLEX57K,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Multilabel Text Classification, Hierarchical Multi-label Classification, Multi-Label Text Classification, Text Classification","Image, Text",English,Computer Vision,"multilabel-text-classification-on-eurlex57k, hierarchical-multi-label-classification-on-19",,https://github.com/iliaschalkidis/lmtc-eurlex57k,https://paperswithcode.com/dataset/eurlex57k,"EURLEX57K is a new publicly available legal LMTC dataset, dubbed EURLEX57K, containing 57k English EU legislative documents from the EUR-LEX portal, tagged with ∼4.3k labels (concepts) from the European Vocabulary (EUROVOC).",,Large-Scale Multi-Label Text Classification on EU Legislation,https://www.aclweb.org/anthology/P19-1636.pdf,,,
1066,EuroCrops,Crop Classification,Crop Classification,Crop Classification,Image,,Computer Vision,,Custom (research-only),https://www.eurocrops.tum.de/,https://paperswithcode.com/dataset/eurocrops,"EuroCrops is a dataset for automatic vegetation classification from multi-spectral and multi-temporal satellite data, annotated with official LIPS reporting data from countries of the European Union, curated by the Technical University of Munich and GAF AG. The project is managed by the DLR Space Administration and funded by BMWI (Federal Ministry for Economic Affairs and Energy). This dataset is publicly available for research causes with the idea in mind to assist in the subsidy control of agricultural self-declarations.",,,,,,
1067,EuRoC_MAV,Visual Odometry,Visual Odometry,Visual Odometry,Image,,Computer Vision,visual-odometry-on-euroc-mav,,https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,https://paperswithcode.com/dataset/euroc-mav,"EuRoC MAV is a visual-inertial datasets collected on-board a Micro Aerial Vehicle (MAV). The dataset contains stereo images, synchronized IMU measurements, and accurate motion and structure ground-truth. The datasets facilitates the design and evaluation of visual-inertial localization algorithms on real flight data",,,,,,
1068,EUROPA,Keyphrase Generation,Keyphrase Generation,"Keyphrase Generation, Keyphrase Extraction",Text,English,Natural Language Processing,,,https://huggingface.co/datasets/NCube/europa,https://paperswithcode.com/dataset/europa,"Dataset Description
EUROPA is a dataset designed for training and evaluating multilingual keyphrase generation models in the legal domain. It consists of legal judgments from the Court of Justice of the European Union (EU) and includes instances in all 24 official EU languages.

Key Features:
Multilingual: Covers 24 official EU languages.
Domain-Specific: Focuses on legal documents.
Source: Derived from Court of Justice of the European Union judgments.


Curated by: N3 team
Languages: French, German, English, Italian, Dutch, Greek, Danish, Portuguese, Spanish, Swedish, Finnish, Lithuanian, Estonian, Czech, Hungarian, Latvian, Slovenian, Polish, Maltese, Slovak, Romanian, Bulgarian, Croatian, Irish
License: MIT License

Dataset Sources

Paper: https://arxiv.org/abs/2403.00252

Dataset Structure

celex_id: CELEX identifier inherited from CJEU. Different translated versions of the same judgment share the same celex_id. If you wish to set a unique identifier for each instance, you can concatenate lang and celex_id values;
lang: ISO 639-1 language code;
input: judgment transcription or translation;
keyphrases: reference keyphrases drafted by the CJEU.

As explained in our paper, the dataset is split chronologically for assessing temporal generalization of models:
- training set: 1957 to 2010 (131 076 instances);
- validation set: 2011 to 2015 (63 373 instances);
- test set: 2016 to 2023 (90 508 instances).

Citation
@article{salaun2024europa,
  title={EUROPA: A Legal Multilingual Keyphrase Generation Dataset},
  author={Sala{\""u}n, Olivier and Piedboeuf, Fr{\'e}d{\'e}ric and Le Berre, Guillaume and Hermelo, David Alfonso and Langlais, Philippe},
  journal={arXiv preprint arXiv:2403.00252},
  year={2024}
}",1957,,,076 instances,,
1069,Europarl-ASR,Data Augmentation,Data Augmentation,"Data Augmentation, Speech Recognition, Benchmarking","Audio, Image, Text",English,Computer Vision,"speech-recognition-on-europarl-asr-en-guest, speech-recognition-on-europarl-asr-en-mep",Custom,https://www.mllp.upv.es/europarl-asr/,https://paperswithcode.com/dataset/europarl-asr,"Europarl-ASR (EN) is a 1300-hour English-language speech and text corpus of parliamentary debates for (streaming) Automatic Speech Recognition training and benchmarking, speech data filtering and speech data verbatimization, based on European Parliament speeches and their official transcripts (1996-2020). Includes dev-test sets for streaming ASR benchmarking, made up of 18 hours of manually revised speeches. The availability of manual non-verbatim and verbatim transcripts for dev-test speeches makes this corpus also useful for the assessment of automatic filtering and verbatimization techniques. The corpus is released under an open licence at https://www.mllp.upv.es/europarl-asr/

Europarl-ASR CONTENTS: [Speech data] 1300 hours of English-language annotated speech data, 3 full sets of timed transcriptions (official non-verbatim, automatically noise-filtered, automatically verbatimized), 18 hours of speech data with both manually revised verbatim transcriptions and official non-verbatim transcriptions, split in 2 independent validation- evaluation partitions for 2 realistic ASR tasks (with vs. without previous knowledge of the speaker); [Text data] 70 million tokens of English-language text data; [Pretrained language models] the Europarl-ASR English-language n-gram language model and vocabulary.",1996,,,,,
1070,Europarl-ST,Data Augmentation,Data Augmentation,"Data Augmentation, Machine Translation, Speech Recognition","Audio, Image, Text",English,Computer Vision,,Custom,https://www.mllp.upv.es/europarl-st/,https://paperswithcode.com/dataset/europarl-st,"Europarl-ST is a multilingual Spoken Language Translation corpus containing paired audio-text samples for SLT from and into 9 European languages, for a total of 72 different translation directions. This corpus has been compiled using the debates held in the European Parliament in the period between 2008 and 2012.",2008,,,,,
1071,Europarl,Machine Translation,Machine Translation,"Machine Translation, Speech Recognition","Audio, Image, Text",English,Computer Vision,,Custom,https://www.statmt.org/europarl/,https://paperswithcode.com/dataset/europarl,"A corpus of parallel text in 21 European languages from the proceedings of the European Parliament.

The Europarl parallel corpus is extracted from the proceedings of the European Parliament (1996-2011). It includes versions in 21 European languages: Romanic (French, Italian, Spanish, Portuguese, Romanian), Germanic (English, Dutch, German, Danish, Swedish), Slavik (Bulgarian, Czech, Polish, Slovak, Slovene), Finni-Ugric (Finnish, Hungarian, Estonian), Baltic (Latvian, Lithuanian), and Greek. Parallel sentence counts are in the range 400K-2M, depending on the language combination.

The goal of the extraction and processing was to generate sentence aligned text for statistical machine translation systems. For this purpose we extracted matching items and labeled them with corresponding document IDs. Using a preprocessor we identified sentence boundaries. We sentence aligned the data using a tool based on the Church and Gale algorithm.

The Europarl corpus was collected mainly to aid research in statistical machine translation (training, evaluation), but it has been used for many other natural language problems: word sense disambiguation, anaphora resolution, information extraction, etc.

Monolingual datasets are also available for 9 languages. These are supersets of the parallel versions. Monolingual word counts are in the range 7M-54M, depending on the language.

Test Sets: Several test sets have been released for the Europarl corpus. In general, the Q4/2000 portion of the data (2000-10 to 2000-12) should be reserved for testing. All released test sets have been selected from this quarter.",1996,,,,,
1072,Europarl_ConcoDisco_Dataset,Word Alignment,Word Alignment,Word Alignment,,,Methodology,,,https://github.com/mjlaali/Europarl-ConcoDisco,https://paperswithcode.com/dataset/europarl-concodisco-dataset,The ConcoDisco Corpus is an English-French parallel corpus with discourse relations (DRs) and discourse connectives (DCs) annotations.,,,,,,
1073,Europeana_Newspapers,Cross-Lingual NER,Cross-Lingual NER,"Cross-Lingual NER, Named Entity Recognition (NER), Meta-Learning","Image, Text",English,Computer Vision,,,https://github.com/EuropeanaNewspapers/ner-corpora,https://paperswithcode.com/dataset/europeana-newspapers,"Europeana Newspapers consists of four datasets with 100 pages each for the languages Dutch, French, German (including Austrian) as part of the Europeana Newspapers project is expected to contribute to the further development and improvement of named entity recognition systems with a focus on historical content.",,,,,,
1074,European_Flood_2013_Dataset,Content-Based Image Retrieval,Content-Based Image Retrieval,"Content-Based Image Retrieval, Image Retrieval",Image,,Computer Vision,,,https://github.com/cvjena/eu-flood-dataset,https://paperswithcode.com/dataset/european-flood-2013-dataset,"This dataset consists of 3,710 flood images, annotated by domain experts regarding their relevance with respect to three tasks (determining the flooded area, inundation depth, water pollution).",,,,,,
1075,EuroSAT,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Semantic Segmentation, Image Classification, Image Clustering, Prompt Engineering, Few-Shot Learning, Cross-Domain Few-Shot, Classification, Transductive Zero-Shot Classification",Image,,Computer Vision,"prompt-engineering-on-eurosat, classification-on-eurosat, image-classification-on-eurosat, few-shot-learning-on-eurosat, zero-shot-learning-on-eurosat, image-clustering-on-eurosat, cross-domain-few-shot-on-eurosat, transductive-zero-shot-classification-on-1",,https://github.com/phelber/eurosat,https://paperswithcode.com/dataset/eurosat,"Eurosat is a dataset and deep learning benchmark for land use and land cover classification. The dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting out of 10 classes with in total 27,000 labeled and geo-referenced images.",,,,,,10
1076,EvalCrafter_Text-to-Video__ECTV__Dataset,Text-to-Video Generation,Text-to-Video Generation,Text-to-Video Generation,"Text, Video",English,Natural Language Processing,text-to-video-generation-on-evalcrafter-text,Apache-2.0,https://huggingface.co/datasets/RaphaelLiu/EvalCrafter_T2V_Dataset,https://paperswithcode.com/dataset/evalcrafter-text-to-video-ectv-dataset,"This dataset contains around 10000 videos generated by various methods using the Prompt list. These videos have been evaluated using the innovative EvalCrafter framework, which assesses generative models across visual, content, and motion qualities using 17 objective metrics and subjective user opinions.",,,,,,
1077,EVALution,Word Embeddings,Word Embeddings,"Word Embeddings, Semantic Textual Similarity, Lexical Entailment",,,Methodology,,,https://github.com/esantus/EVALution,https://paperswithcode.com/dataset/evalution,"EVALution dataset is evenly distributed among the three classes (hypernyms, co-hyponyms and random) and involves three types of parts of speech (noun, verb, adjective). The full dataset contains a total of 4,263 distinct terms consisting of 2,380 nouns, 958 verbs and 972 adjectives.",,Network Features Based Co-hyponymy Detection,https://arxiv.org/abs/1802.04609,,,
1078,Event-Camera_Dataset,Optical Flow Estimation,Optical Flow Estimation,"Optical Flow Estimation, Event-Based Video Reconstruction, Video Reconstruction, Object Tracking, Visual Odometry","3D, Image, Video",,Computer Vision,"event-based-video-reconstruction-on-event, video-reconstruction-on-event-camera-dataset",CC BY-NC-SA 3.0,http://rpg.ifi.uzh.ch/davis_data.html,https://paperswithcode.com/dataset/event-camera-dataset,"The Event-Camera Dataset is a collection of datasets with an event-based camera for high-speed robotics. The data also include intensity images, inertial measurements, and ground truth from a motion-capture system. An event-based camera is a revolutionary vision sensor with three key advantages: a measurement rate that is almost 1 million times faster than standard cameras, a latency of 1 microsecond, and a high dynamic range of 130 decibels (standard cameras only have 60 dB). These properties enable the design of a new class of algorithms for high-speed robotics, where standard cameras suffer from motion blur and high latency. All the data are released both as text files and binary (i.e., rosbag) files.",,,,,,
1079,EventEA,Entity Alignment,Entity Alignment,Entity Alignment,,,Methodology,,GPL-3.0 license,https://github.com/nju-websoft/EventEA,https://paperswithcode.com/dataset/eventea,"EventEA is an event-centric entity alignment dataset, harvested from EventKG, DBpedia and Wikidata.",,EventEA: Benchmarking Entity Alignment for Event-centric Knowledge Graphs,https://arxiv.org/pdf/2211.02817v1.pdf,,,
1080,EventNarrative,KG-to-Text Generation,KG-to-Text Generation,"KG-to-Text Generation, Graph-to-Sequence, Event Extraction","Graph, Text, Time Series",English,Natural Language Processing,kg-to-text-generation-on-eventnarrative,,https://arxiv.org/pdf/2111.00276v1.pdf,https://paperswithcode.com/dataset/eventnarrative,"EventNarrative is a knowledge graph-to-text dataset from publicly available open-world knowledge graphs. EventNarrative consists of approximately 230,000 graphs and their corresponding natural language text.",,Homepage,https://arxiv.org/pdf/2111.00276v1.pdf,,,
1081,Events_classification_-_Biotech_news,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Text Classification, Few-Shot Learning, Few-Shot Text Classification, Zero-Shot Text Classification","Image, Text",English,Computer Vision,,ODC-BY,https://www.knowledgator.com/,https://paperswithcode.com/dataset/events-classification-biotech,"A dataset specifically tailored to the biotech news sector, aiming to transcend the limitations of existing benchmarks. This dataset is rich in complex content, comprising various biotech news articles covering various events, thus providing a more nuanced view of information extraction challenges.

A standout feature of our dataset is its dual-layered approach: it not only identifies the general theme of the articles but also extracts specific information about the target companies involved. This dual focus significantly elevates the dataset’s relevance for applications demanding a deeper insight into the interplay of events, companies, and the broader biotech industry.

The dataset encompasses 31 classes, including a ‘None’ category, to cover various events and information types such as event organisation, executive statements, regulatory approvals, hiring announcements, and more.

Key aspects:



Event extraction



Multi-class classification



Biotech news domain



31 classes



3140 total number of examples



Classes:



alliance & partnership - forming an alliance or partnership



article publication - publishing an article



clinical trial sponsorship - Sponsoring or participating in a clinical trial



closing - shutting down a facility/office/division or ceasing an initiative



company description - describing or profiling the company



department establishment - establishing a new department or division



event organization - organizing or participating in an event like a conference, exhibition, etc



event organization - organizing or participating in an event



executive appointment - appointing a new executive



executive statement - a statement or quote from an executive of a company



expanding geography - expanding into new geographical areas



expanding industry - expanding into new industries or markets



foundation - establishing a new charitable foundation



funding round - raising a new round of funding



hiring - announcing new hires or appointments at the company



investment in public company - making an investment in a public company



ipo exit - having an initial public offering or acquisition exit



m&a - mergers, acquisitions, or divestitures



new initiatives & programs - announcing new initiatives or programs



new initiatives or programs - announcing new initiatives, programs, or campaigns



None - no label



other - other events that don't fit into defined categories



participation in an event - participating in an industry event, conference, etc



partnerships & alliances - forming partnerships or strategic alliances with other companies



patent publication - publication of a new patent filing



product launching & presentation - launching or unveiling a new product



product updates - announcing updates or new versions of existing products



regulatory approval - getting approval from regulatory bodies for products, services, trials, etc.



service & product providing - launching or expanding products or services



subsidiary establishment - establishing a new subsidiary company



support & philanthropy - philanthropic activities or donations",,,,,,31
1082,EVI,Speaker Verification,Speaker Verification,"Speaker Verification, Speaker Identification",Audio,,Audio,"speaker-identification-on-evi-en-gb-1, speaker-identification-on-evi-fr-fr, speaker-identification-on-evi-pl-pl",CC-BY-4.0,https://github.com/PolyAI-LDN/evi-paper,https://paperswithcode.com/dataset/evi,"The EVI dataset is a challenging, multilingual spoken-dialogue dataset with 5,506 dialogues in English, Polish, and French. The dataset can be used to develop and benchmark conversational systems for user authentication tasks, i.e. speaker enrolment (E), speaker verification (V), speaker identification (I).

The dataset contains the audio data, machine transcriptions, and target identity for each dialogue,  and the knowledge-base with personal information (postcode, name, and date of birth) for each identity.  The dataset can be used with both text-independent biometric or knowledge-based authentication (KBA) tasks.",,,,,,
1083,EVICAN,Cell Segmentation,Cell Segmentation,Cell Segmentation,Image,,Computer Vision,cell-segmentation-on-evican,,https://edmond.mpdl.mpg.de/dataset.xhtml;jsessionid=294bca02ef53cdc35118d0470ef7?persistentId=doi%3A10.17617%2F3.AJBV1S&version=&q=&fileTypeGroupFacet=&fileAccess=&fileSortField=type,https://paperswithcode.com/dataset/evican,"Deep learning use for quantitative image analysis is exponentially increasing. However, training accurate, widely deployable deep learning algorithms requires a plethora of annotated (ground truth) data. Image collections must contain not only thousands of images to provide sufficient example objects (i.e. cells), but also contain an adequate degree of image heterogeneity. We present a new dataset, EVICAN-Expert visual cell annotation, comprising partially annotated grayscale images of 30 different cell lines from multiple microscopes, contrast mechanisms and magnifications that is readily usable as training data for computer vision applications. With 4600 images and ∼26 000 segmented cells, our collection offers an unparalleled heterogeneous training dataset for cell biology deep learning application development. The dataset is freely available (https://edmond.mpdl.mpg.de/imeji/collection/l45s16atmi6Aa4sI?q=).",,,,4600 images,"training accurate, widely deployable deep learning algorithms requires a plethora of annotated (ground truth) data. Image collections must contain not only thousands of images",
1084,Exact_Street2Shop,Product Recommendation,Product Recommendation,"Product Recommendation, Image Retrieval, Recommendation Systems",Image,,Computer Vision,image-retrieval-on-exact-street2shop,,http://tamaraberg.com/street2shop/,https://paperswithcode.com/dataset/exact-street2shop,"A dataset containing 404,683 shop photos collected from 25 different online retailers and 20,357 street photos, providing a total of 39,479 clothing item matches between street and shop photos.",,,,,,
1085,ExBAN,Bayesian Inference,Bayesian Inference,"Bayesian Inference, Explainable artificial intelligence, Counterfactual Explanation",,,Methodology,,CC-BY-SA-4.0 License,https://github.com/MirunaClinciu/ExBAN,https://paperswithcode.com/dataset/exban,"The ExBAN dataset: a corpus of NL explanations generated by crowd-sourced participants presented with the task of explaining simple Bayesian Network (BN) graphical representations. These explanations, in a separate collection effort, are rated for clarity and informativeness.",,,,,,
1086,Exchange,Time Series Forecasting,Time Series Forecasting,Time Series Forecasting,Time Series,,Time Series,,,https://github.com/laiguokun/multivariate-time-series-data,https://paperswithcode.com/dataset/echange,"Daily exchange rates of eight countries’
currencies against the US dollar, spanning from 1990 to 2010
with 7588 timesteps in total.

| Dataset        | Dim | Lengths | Granularity | Split  | p-value |
|----------------|-----|---------|-------------|--------|---------|
| Exchange Rate  | 8   | 7,588   | 1 day       | 7:1:2  | 0.416   |",1990,,,,,
1087,ExDark,Object Detection,Object Detection,"Object Detection, 2D Object Detection, Image Enhancement, Low-Light Image Enhancement",Image,,Computer Vision,2d-object-detection-on-exdark,,https://github.com/cs-chan/Exclusively-Dark-Image-Dataset,https://paperswithcode.com/dataset/exdark,"The Exclusively Dark (ExDARK) dataset is a collection of 7,363 low-light images from very low-light environments to twilight (i.e 10 different conditions) with 12 object classes (similar to PASCAL VOC) annotated on both image class level and local object bounding boxes.",,,,,,
1088,ExHVV,Text Generation,Text Generation,"Text Generation, Multi-Task Learning, Semantic Role Labeling",Text,English,Natural Language Processing,,CC0-1.0 license,https://github.com/LCS2-IIITD/LUMEN-Explaining-Memes,https://paperswithcode.com/dataset/exhvv,"ExHVV is a novel dataset that offers natural language explanations of connotative roles for three types of entities -- heroes, villains, and victims, encompassing 4,680 entities present in 3K memes.",,What do you MEME? Generating Explanations for Visual Semantic Role Labelling in Memes,https://arxiv.org/pdf/2212.00715v1.pdf,,,
1089,exiD_Dataset,Trajectory Clustering,Trajectory Clustering,"Trajectory Clustering, Trajectory Planning, Trajectory Prediction, Trajectory Modeling, Trajectory Forecasting",Time Series,,Methodology,,Non-Commercial,https://levelxdata.com/exid-dataset/,https://paperswithcode.com/dataset/exid-dataset,"The exiD dataset introduces a groundbreaking collection of naturalistic road user trajectories at highway entries and exits in Germany, meticulously captured with drones to navigate past the limitations of conventional traffic data collection methods, such as occlusions. This approach not only allows for the precise extraction of each road user’s trajectory and type but also ensures very high positional accuracy, thanks to sophisticated computer vision algorithms. Its innovative data collection technique minimizes errors and maximizes the quality and reliability of the dataset, making it a valuable resource for advanced research and development in the field of automated driving technologies.",,,,,,
1090,Expi,Pose Prediction,Pose Prediction,"Pose Prediction, Multi-Person Pose forecasting, motion prediction, Human Pose Forecasting","3D, Image, Time Series, Video",,Computer Vision,"multi-person-pose-forecasting-on-expi-unseen, multi-person-pose-forecasting-on-expi-common, human-pose-forecasting-on-expi-common-actions",,https://github.com/GUO-W/MultiMotion,https://paperswithcode.com/dataset/expi,"Extreme Pose Interaction (ExPI) Dataset is a new person interaction dataset of Lindy Hop dancing actions. In Lindy Hop, the two dancers are called leader and
follower. The authors recorded two couples of dancers in a multi-camera setup equipped also with a motion-capture system.
16 different actions are performed in ExPI dataset, some by the two couples of dancers, some by only one of the couples. Each action was repeated five times
to account for variability. More precisely, for each recorded sequence, ExPI provides: 
(i) Multi-view videos at 25FPS from all the cameras in the recording setup; 
(ii) Mocap data (3D position of 18 joints for each person) at 25FPS synchronized with the videos.; 
(iii) camera calibration information; and (iv) 3D shapes as textured meshes for each frame.

Overall, the dataset contains 115 sequences with 30k visual frames for each viewpoint and 60k 3D instances annotated",,,,,,
1091,ExPose,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, 3D Face Reconstruction, 3D Hand Pose Estimation","3D, Image",,Computer Vision,,,http://expose.is.tue.mpg.de,https://paperswithcode.com/dataset/expose,Curates a dataset of SMPL-X fits on in-the-wild images.,,,,,,
1092,Exposure-Errors,Image Enhancement,Image Enhancement,Image Enhancement,Image,,Computer Vision,image-enhancement-on-exposure-errors,Custom,https://github.com/mahmoudnafifi/Exposure_Correction,https://paperswithcode.com/dataset/exposure-errors,"A dataset of over 24,000 images exhibiting the broadest range of exposure values to date with a corresponding properly exposed image.",,,,000 images,,
1093,Expressive_Gaussian_mixture_models_for_high-dimens,Physical Simulations,Physical Simulations,"Physical Simulations, Physics-informed machine learning",,,Methodology,,GPL3,https://dx.doi.org/10.48420/17136839,https://paperswithcode.com/dataset/expressive-gaussian-mixture-models-for-high,"Neural network model files and Madgraph event generator outputs used as inputs to the results presented in the paper ""Learning to discover: expressive Gaussian mixture models for multi-dimensional simulation and parameter inference in the physical sciences"" arXiv:2108.11481; 2022 Mach. Learn.: Sci. Technol. 3 015021 
Code and model files can be found at:
https://github.com/darrendavidprice/science-discovery/tree/master/expressive_gaussian_mixture_models",2022,,,,,
1094,ExPUNations,Explanation Generation,Explanation Generation,"Explanation Generation, Natural Language Understanding",Text,English,Natural Language Processing,,Creative Commons Attribution 4.0 International,https://github.com/amazon-science/expunations,https://paperswithcode.com/dataset/expunations,"ExPUNations is a humor dataset with such extensive and fine-grained annotations specifically for puns. This dataset is designed for two new tasks namely, explanation generation to aid with pun classification and keyword-conditioned pun generation",,ExPUNations: Augmenting Puns with Keywords and Explanations,https://arxiv.org/pdf/2210.13513v1.pdf,,,
1095,ExpW,Multi-Label Learning,Multi-Label Learning,"Multi-Label Learning, Emotion Recognition, Facial Expression Recognition (FER)",Image,,Computer Vision,facial-expression-recognition-fer-on-expw,,http://mmlab.ie.cuhk.edu.hk/projects/socialrelation/index.html,https://paperswithcode.com/dataset/expw,"The Expression in-the-Wild (ExpW) dataset is for facial expression recognition and contains 91,793 faces manually labeled with expressions. Each of the face images is annotated as one of the seven basic expression categories: “angry”, “disgust”, “fear”, “happy”, “sad”, “surprise”, or “neutral”.",,,,,,
1096,Extended_Yale_B,Dictionary Learning,Dictionary Learning,"Dictionary Learning, Image Classification, Image Clustering, Face Recognition",Image,,Computer Vision,image-clustering-on-extended-yale-b,,http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html,https://paperswithcode.com/dataset/extended-yale-b-1,The Extended Yale B database contains 2414 frontal-face images with size 192×168 over 38 subjects and about 64 images per subject. The images were captured under different lighting conditions and various facial expressions.,,Learning Locality-Constrained Collaborative Representation for Robust Face Recognition,https://arxiv.org/abs/1210.1316,64 images,,
1097,ExtMarker,Univariate Time Series Forecasting,Univariate Time Series Forecasting,"Univariate Time Series Forecasting, Respiratory motion forecasting, Time Series Forecasting, Multivariate Time Series Forecasting, Time Series Prediction","Time Series, Video",,Time Series,multivariate-time-series-forecasting-on-3,CC BY 4.0,https://github.com/pohl-michel/time-series-forecasting-with-UORO-RTRL-LMS-and-linear-regression/tree/main/Original%20data,https://paperswithcode.com/dataset/extmarker,"Three-dimensional position of external markers placed on the chest and abdomen of healthy individuals breathing during intervals from 73s to 222s. The markers move because of the respiratory motion, and their position is sampled at approximately 10Hz. Markers are metallic objects used during external beam radiotherapy to track and predict the motion of tumors due to breathing for accurate dose delivery.

The same data was used and described in detail in the following article:
Krilavicius, Tomas, et al. “Predicting Respiratory Motion for Real-Time Tumour Tracking in Radiotherapy.” ArXiv:1508.00749 [Physics], Aug. 2015. arXiv.org, http://arxiv.org/abs/1508.00749.",2015,,,,,
1098,ExtremeWeather,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Weather Forecasting, Meta-Learning","Image, Time Series",,Computer Vision,,Custom,https://github.com/eracah/hur-detect,https://paperswithcode.com/dataset/extremeweather,Encourages machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change.,,"ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events",https://arxiv.org/pdf/1612.02095v2.pdf,,,
1099,EXTREME_CLASSIFICATION,Extreme Multi-Label Classification,Extreme Multi-Label Classification,"Extreme Multi-Label Classification, Representation Learning, Product Recommendation, Multi-Label Classification, Multi-Label Learning, Multi-Label Text Classification","Image, Text",English,Computer Vision,,CC BY-NC-ND,http://manikvarma.org/downloads/XC/XMLRepository.html,https://paperswithcode.com/dataset/extreme-classification,"The objective in extreme multi-label classification is to learn feature architectures and classifiers that can automatically tag a data point with the most relevant subset of labels from an extremely large label set. This repository provides resources that can be used for evaluating the performance of extreme multi-label algorithms including datasets, code, and metrics.

For more details please visit the link http://manikvarma.org/downloads/XC/XMLRepository.html",,,,,,
1100,Extreme_Events___Natural_Disasters___Hurricane,Univariate Time Series Forecasting,Univariate Time Series Forecasting,"Univariate Time Series Forecasting, Hurricane Forecasting, Time Series Forecasting, Irregular Time Series, Weather Forecasting, Time Series Prediction, Time Series Analysis",Time Series,,Time Series,time-series-forecasting-on-hurricane,,,https://paperswithcode.com/dataset/hurricane,"A new spatio-temporal benchmark dataset (Hurricane), is suited for forecasting during extreme events and anomalies. The dataset is provided through the Florida Department of Revenue which provides the monthly sales revenue (2003-2020) for the tourism industry for all 67 counties of Florida which are prone to annual hurricanes. Furthermore, we aligned and joined the raw time series with the history of hurricane categories (i.e., event intensities) based on time for each county.
Note that the hurricane category indicates the maximum sustained wind speed which can result in catastrophic damages as this number goes up (Category 1-6).",2003,,,,,
1101,f5C_Dataset,Temporal Sequences,Temporal Sequences,Temporal Sequences,"Time Series, Video",,Methodology,temporal-sequences-on-f5c-dataset,,http://f5c.m6aminer.cn/,https://paperswithcode.com/dataset/f5c-dataset,This is a dataset for predicting 5-Formylcytidine Modifications on mRNA.,,,,,,
1102,FABSA,Latent Aspect Detection,Latent Aspect Detection,"Latent Aspect Detection, Hidden Aspect Detection, Aspect-Based Sentiment Analysis (ABSA), Aspect-Based Sentiment Analysis, Aspect Category Detection, Aspect Category Polarity, Aspect Category Sentiment Analysis","Image, Text",English,Computer Vision,"aspect-category-sentiment-analysis-on-fabsa, aspect-based-sentiment-analysis-absa-on-fabsa",Creative Commons  Attribution BY 4.0 Deed,https://huggingface.co/datasets/jordiclive/FABSA,https://paperswithcode.com/dataset/fabsa,"FABSA, An aspect-based sentiment analysis dataset in the Customer Feedback space (Trustpilot, Google Play and Apple Store reviews).

A professionally annotated dataset released by Chattermill AI, with 8 years of experience in leveraging advanced ML analytics in the customer feedback space for high-profile clients such as Amazon and Uber.

Two annotators possess extensive experience in developing human-labeled ABSA datasets for commercial companies, while the third annotator holds a PhD in computational linguistics.

There has been a lack of high-quality ABSA datasets covering broad domains and addressing real-world applications. Academic progress has been confined to benchmarking on domain-specific, toy datasets such as restaurants and laptops, which are limited in size (e.g., SemEval Task ABSA or SentiHood).

This dataset is part of the FABSA paper, and we release it hoping to advance academic progress as tools for ingesting and analyzing customer feedback at scale improve significantly, yet evaluation datasets continue to lag. FABSA is a new, large-scale, multi-domain ABSA dataset of feedback reviews, consisting of approximately 10,500 reviews spanning 10 domains (Fashion, Consulting, Travel Booking, Ride-hailing, Banking, Trading, Streaming, Price Comparison, Information Technology, and Groceries).

Academic Paper

@article{KONTONATSIOS2023126867,
title = {FABSA: An aspect-based sentiment analysis dataset of user reviews},
journal = {Neurocomputing},
volume = {562},
pages = {126867},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126867},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223009906},
author = {Georgios Kontonatsios and Jordan Clive and Georgia Harrison and Thomas Metcalfe and Patrycja Sliwiak and Hassan Tahir and Aji Ghose},
keywords = {ABSA, Multi-domain dataset, Deep learning},
}",2023,,,,,
1103,FaceForensics,DeepFake Detection,DeepFake Detection,DeepFake Detection,Image,,Computer Vision,deepfake-detection-on-faceforensics,FaceForenics Terms of Use,http://niessnerlab.org/projects/roessler2018faceforensics.html,https://paperswithcode.com/dataset/faceforensics,"FaceForensics is a video dataset consisting of more than 500,000 frames containing faces from 1004 videos that can be used to study image or video forgeries. All videos are downloaded from Youtube and are cut down to short continuous clips that contain mostly frontal faces. This dataset has two versions:



Source-to-Target: where the authors reenact over 1000 videos with new facial expressions extracted from other videos, which e.g. can be used to train a classifier to detect fake images or videos.



Selfreenactment: where the authors use Face2Face to reenact the facial expressions of videos with their own facial expressions as input to get pairs of videos, which e.g. can be used to train supervised generative refinement models.",,FaceForenics Terms of Use,http://kaldir.vc.in.tum.de/faceforensics/webpage/FaceForensics_TOS.pdf,,,
1104,FaceForensics__,Image Generation,Image Generation,"Image Generation, DeepFake Detection, Face Swapping","Image, Text",English,Computer Vision,"face-swapping-on-faceforensics, deepfake-detection-on-faceforensics-1",Custom,https://github.com/ondyari/FaceForensics,https://paperswithcode.com/dataset/faceforensics-1,"FaceForensics++ is a forensics dataset consisting of 1000 original video sequences that have been manipulated with four automated face manipulation methods: Deepfakes, Face2Face, FaceSwap and NeuralTextures. The data has been sourced from 977 youtube videos and all videos contain a trackable mostly frontal face without occlusions which enables automated tampering methods to generate realistic forgeries.",,,,,,
1105,FacetSum,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,"Unsupervised Extractive Summarization, Scientific Document Summarization, Document Summarization",Text,English,Natural Language Processing,unsupervised-extractive-summarization-on-2,Custom (non-commercial),https://github.com/hfthair/emerald_crawler,https://paperswithcode.com/dataset/facetsum,"FacetSum is a faceted summarization dataset for scientific documents. FacetSum has been built on Emerald journal articles, covering a diverse range of domains. Different from traditional document-summary pairs, FacetSum provides multiple summaries, each targeted at specific sections of a long document, including the purpose, method, findings, and value.",,,,,,
1106,Face_dataset_by_Generated_Photos,Face Detection,Face Detection,"Face Detection, Face Generation","Image, Text",English,Computer Vision,,Free for non-commercial purposes with attribution,https://generated.photos/datasets#research-dataset,https://paperswithcode.com/dataset/face-dataset-by-generated-photos,"The free Face dataset made for students and teachers. It contains 10,000 photos with equal distribution of race and gender parameters, along with metadata and facial landmarks. Free to use for research with citation Photos by Generated.Photos. 

Photos 

All the photos are 100% synthetic. Based on model-released photos. Royalty-free. Can be used for any research purpose except for the ones violating the law. Worldwide. No time limitations.
Quantity    10,000
Quality     256x256px
Diversity   Ethnicity, gender

Metadata

The JSON files contain the metadata for each image in a machine-readable format, including:
(1) FaceLandmarks: mouth, right_eyebrow, left_eyebrow, right_eye, left_eye, nose, jaw.
(2) FaceAttributes: headPose, gender, makeup, emotion, facialHair, hair (hairColor, hairLength, bald), occlusion, ethnicity, eye_color, smile, age",,,,,,
1107,FairFace,Fairness,Fairness,"Fairness, Facial Attribute Classification, Decision Making",Image,,Computer Vision,facial-attribute-classification-on-fairface,CC BY 4.0,https://github.com/joojs/fairface,https://paperswithcode.com/dataset/fairface,"FairFace is a face image dataset which is race balanced. It contains 108,501 images from 7 different race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups.",,,,501 images,,
1108,FairytaleQA,Question Generation,Question Generation,"Question Generation, Question Answering",Text,English,Natural Language Processing,"question-generation-on-fairytaleqa, question-answering-on-fairytaleqa",Apache-2.0 License,https://github.com/uci-soe/FairytaleQAData,https://paperswithcode.com/dataset/fairytaleqa,"FairytaleQA is a dataset focusing on narrative comprehension of kindergarten to eighth-grade students. Annotated by educational experts based on an evidence-based theoretical framework, FairytaleQA consists of 10,580 explicit and implicit questions derived from 278 children-friendly story narratives, covering seven types of narrative elements or relations. It can support narrative Question Generation (QG) and Narrative Question Answering (QA) tasks.",,,,,,
1109,FaithDial,Dialogue Evaluation,Dialogue Evaluation,"Dialogue Evaluation, Dialogue Generation",Text,English,Natural Language Processing,,,https://mcgill-nlp.github.io/FaithDial/,https://paperswithcode.com/dataset/faithdial%0A,"FaithDial is a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark.

FaithDial contains around 50K turns across 5.5K conversations. If trained on FaithDial, state-of-the-art dialogue models are significantly more faithful while also enhancing other dialogue aspects like cooperativeness, creativity and engagement.",,,,,,
1110,FakeAVCeleb,Multimodal Forgery Detection,Multimodal Forgery Detection,"Multimodal Forgery Detection, DeepFake Detection",Image,,Multimodal,"multimodal-forgery-detection-on-fakeavceleb, deepfake-detection-on-fakeavceleb-1",,https://github.com/hasam6400/fakevaceleb,https://paperswithcode.com/dataset/fakeavceleb,FakeAVCeleb is a novel Audio-Video Deepfake dataset that not only contains deepfake videos but respective synthesized cloned audios as well.,,https://arxiv.org/pdf/2108.05080v1.pdf,https://arxiv.org/pdf/2108.05080v1.pdf,,,
1111,Fakeddit,Fake News Detection,Fake News Detection,Fake News Detection,Image,,Computer Vision,,,https://github.com/entitize/fakeddit,https://paperswithcode.com/dataset/fakeddit,"Fakeddit is a novel multimodal dataset for fake news detection consisting of over 1 million samples from multiple categories of fake news. After being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision.",,,,,,
1112,FakeMusicCaps,Music Generation,Music Generation,"Music Generation, Audio Deepfake Detection","Audio, Image, Text",English,Computer Vision,,Creative Commons Attribution 4.0 International,https://zenodo.org/records/13732524,https://paperswithcode.com/dataset/fakemusiccaps,"The FakeMusicCaps dataset contains total of 27605 10 seconds music tracks corresponding to almost 77 hours, generated using 5 different Text-To-Music (TTM) models. It is designed to be used as a starting dataset for the training and/or evaluation of models for the detection and attribution of synthetic music generated via TTM models.

If you use this dataset, please cite our paper:

Comanducci, L., Bestagini, P., and Tubaro, S., “FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models”, Art. no. arXiv:2409.10684, 2024. doi:10.48550/arXiv.2409.10684.",2024,,,,,
1113,FakeNewsNet,Misinformation,Misinformation,"Misinformation, News Generation, Fake News Detection","Image, Text",English,Computer Vision,,,https://github.com/KaiDMML/FakeNewsNet,https://paperswithcode.com/dataset/fakenewsnet,"FakeNewsNet is collected from two fact-checking websites: GossipCop and PolitiFact containing news contents with labels annotated by professional journalists and experts, along with social context information.",,Leveraging Multi-Source Weak Social Supervision for Early Detection of Fake News,https://arxiv.org/abs/2004.01732,,,
1114,FaMoS,3D Face Alignment,3D Face Alignment,"3D Face Alignment, 3D Face Reconstruction, 3D Face Modelling, Multi-View 3D Shape Retrieval","3D, Image",,Computer Vision,,,https://tempeh.is.tue.mpg.de/,https://paperswithcode.com/dataset/famos,"FaMoS is a dynamic 3D head dataset from 95 subjects, each performing 28 motion sequences. The sequences comprise of six prototypical expressions (i.e., Anger, Disgust, Fear, Happiness, Sadness, and Surprise), two head rotations (left/right and up/down), and diverse facial motions, including extreme and asymmetric expressions. Each sequence is recorded at 60 fps. In total,  FaMoS contains around 600K 3D head meshes (i.e., ~225 frames per sequence). For each frame, registrations in FLAME meshes are publicly available.",,,,,,
1115,FarsBase-KBP,Neural Architecture Search,Neural Architecture Search,"Neural Architecture Search, Entity Linking, Knowledge Base Population",,,Methodology,,,https://arxiv.org/pdf/2005.01879.pdf,https://paperswithcode.com/dataset/farsbase-kbp,"FarsBase-KBP contains 22015 sentences, in which the entities and relation types are linked to the FarsBase ontology. This gold dataset can be reused for benchmarking KBP systems in the Persian language.",,Paper,https://arxiv.org/pdf/2005.01879.pdf,22015 sentences,,
1116,Fashion-Gen,Text-to-Image Generation,Text-to-Image Generation,"Text-to-Image Generation, Image Inpainting, Image Generation, Image Retrieval","Image, Text",English,Computer Vision,,Custom,https://docs.google.com/forms/d/e/1FAIpQLScJ_YKcCtE2zJ5F1DTGdAQXvbzKVHLGKiB4bSXNWnEf9Ci2zA/viewform,https://paperswithcode.com/dataset/fashion-gen,"Fashion-Gen consists of 293,008 high definition (1360 x 1360 pixels) fashion images paired with item descriptions provided by professional stylists. Each item is photographed from a variety of angles.",,Fashion-Gen: The Generative Fashion Dataset and Challenge,https://arxiv.org/pdf/1806.08317v2.pdf,,,
1117,Fashion-MNIST,Clustering Algorithms Evaluation,Clustering Algorithms Evaluation,"Clustering Algorithms Evaluation, Model Poisoning, Out-of-Distribution Detection, Image Classification, Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly, Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly, Domain Generalization, General Classification, Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly, Anomaly Detection, Image Clustering, Multiview Clustering, Image Generation, Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly, Outlier Detection, Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly, Unsupervised Anomaly Detection","Image, Text",English,Computer Vision,"clustering-algorithms-evaluation-on-fashion-2, image-generation-on-fashion-mnist, general-classification-on-fashion-mnist, image-clustering-on-fashion-mnist, unsupervised-anomaly-detection-with-specified-11, unsupervised-anomaly-detection-on-fashion-1, model-poisoning-on-fashion-mnist, multiview-clustering-on-fashion-mnist, outlier-detection-on-fashion-mnist, anomaly-detection-on-fashion-mnist, unsupervised-anomaly-detection-with-specified-25, image-classification-on-fashion-mnist, unsupervised-anomaly-detection-with-specified-27, unsupervised-anomaly-detection-with-specified-14, out-of-distribution-detection-on-fashion, domain-generalization-on-rotated-fashion, unsupervised-anomaly-detection-with-specified-18",MIT,https://github.com/zalandoresearch/fashion-mnist,https://paperswithcode.com/dataset/fashion-mnist,"Fashion-MNIST is a dataset comprising of 28×28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST shares the same image size, data format and the structure of training and testing splits with the original MNIST.",,Generative Probabilistic Novelty Detection with Adversarial Autoencoders,https://arxiv.org/abs/1807.02588,000 images,"training set has 60,000 images",10
1118,Fashion_144K,Topic Models,Topic Models,"Topic Models, Image Clustering, Semantic Parsing","Image, Text",English,Computer Vision,,,https://esslab.jp/~ess/en/data/fashion144k/,https://paperswithcode.com/dataset/fashion-144k,"Fashion 144K is a novel heterogeneous dataset with 144,169 user posts containing diverse image, textual and meta information.",,,,,,
1119,Fashion_IQ,Composed Image Retrieval (CoIR),Composed Image Retrieval (CoIR),"Composed Image Retrieval (CoIR), Zero-Shot Composed Image Retrieval (ZS-CIR), Image Retrieval with Multi-Modal Query, Virtual Try-on, Image Retrieval",Image,,Computer Vision,"image-retrieval-on-fashion-iq, zero-shot-composed-image-retrieval-zs-cir-on-3, image-retrieval-with-multi-modal-query-on-1, zero-shot-composed-image-retrieval-zs-cir-on-2, virtual-try-on-on-fashioniq, composed-image-retrieval-coir-on-fashion-iq",,https://github.com/XiaoxiaoGuo/fashion-iq,https://paperswithcode.com/dataset/fashion-iq,Fashion IQ support and advance research on interactive fashion image retrieval. Fashion IQ is the first fashion dataset to provide human-generated captions that distinguish similar pairs of garment images together with side-information consisting of real-world product descriptions and derived visual attribute labels for these images.,,,,,,
1120,fastMRI,MRI Reconstruction,MRI Reconstruction,MRI Reconstruction,3D,,Methodology,"mri-reconstruction-on-fastmri-knee-8x, mri-reconstruction-on-fastmri-knee-val-8x, mri-reconstruction-on-fastmri-brain-8x, mri-reconstruction-on-fastmri-brain-4x, mri-reconstruction-on-fastmri-knee-4x",Custom (internal research-only),https://fastmri.med.nyu.edu/,https://paperswithcode.com/dataset/fastmri,"The fastMRI dataset includes two types of MRI scans: knee MRIs and the brain (neuro) MRIs, and containing training, validation, and masked test sets.
The deidentified imaging dataset provided by NYU Langone comprises raw k-space data in several sub-dataset groups. Curation of these data are part of an IRB approved study. Raw and DICOM data have been deidentified via conversion to the vendor-neutral ISMRMD format and the RSNA clinical trial processor, respectively. Also, each DICOM image is manually inspected for the presence of any unexpected protected health information (PHI), with spot checking of both metadata and image content.
Knee MRI: Data from more than 1,500 fully sampled knee MRIs obtained on 3 and 1.5 Tesla magnets and DICOM images from 10,000 clinical knee MRIs also obtained at 3 or 1.5 Tesla. The raw dataset includes coronal proton density-weighted images with and without fat suppression. The DICOM dataset contains coronal proton density-weighted with and without fat suppression, axial proton density-weighted with fat suppression, sagittal proton density, and sagittal T2-weighted with fat suppression.
Brain MRI: Data from 6,970 fully sampled brain MRIs obtained on 3 and 1.5 Tesla magnets. The raw dataset includes axial T1 weighted, T2 weighted and FLAIR images. Some of the T1 weighted acquisitions included admissions of contrast agent.",,,,,,
1121,FathomNet2023,2D Object Detection,2D Object Detection,"2D Object Detection, Image Classification, Out of Distribution (OOD) Detection",Image,,Computer Vision,,Creative Commons Attribution 4.0 International,https://www.kaggle.com/competitions/fathomnet-out-of-sample-detection/overview,https://paperswithcode.com/dataset/fathomnet2023,"The FathomNet2023 competition dataset is a subset of the broader FathomNet marine image repository. The training and test images for the competition were all collected in the Monterey Bay Area between the surface and 1300 meters depth by the Monterey Bay Aquarium Research Institute. The images contain bounding box annotations of 290 categories of bottom dwelling animals. The training and validation data are split across an 800 meter depth threshold: all training data is collected from 0-800 meters, evaluation data comes from the whole 0-1300 meter range. Since an organisms' habitat range is partially a function of depth, the species distributions in the two regions are overlapping but not identical. Test images are drawn from the same region but may come from above or below the depth horizon. The competition goal is to label the animals present in a given image (i.e. multi-label classification) and determine whether the image is out-of-sample.",,,,,training and test images for the competition were all collected in the Monterey Bay Area between the surface and 1300 meters depth by the Monterey Bay Aquarium Research Institute. The images,290
1122,FaVIQ,Fact Verification,Fact Verification,Fact Verification,,,Methodology,,,https://faviq.github.io,https://paperswithcode.com/dataset/faviq,"FaVIQ (Fact Verification from Information-seeking Questions) is a challenging and realistic fact verification dataset that reflects confusions raised by real users. We use the ambiguity in information-seeking questions and their disambiguation, and automatically convert them to true and false claims. These claims are natural, and require a complete understanding of the evidence for verification. FaVIQ serves as a challenging benchmark for natural language understanding, and improves performance in professional fact checking.",,,,,,
1123,FB15k-237,Link Prediction,Link Prediction,"Link Prediction, Complex Query Answering, Knowledge Graph Completion","Graph, Time Series",,Methodology,"complex-query-answering-on-fb15k-237, link-prediction-on-fb15k-237, knowledge-graph-completion-on-fb15k-237",,https://download.microsoft.com/download/8/7/0/8700516A-AB3D-4850-B4BB-805C515AECE1/FB15K-237.2.zip,https://paperswithcode.com/dataset/fb15k-237,"FB15k-237 is a link prediction dataset created from FB15k. While FB15k consists of 1,345 relations, 14,951 entities, and 592,213 triples, many triples are inverses that cause leakage from the training to testing and validation splits. FB15k-237 was created by Toutanova and Chen (2015) to ensure that the testing and evaluation datasets do not have inverse relation test leakage. In summary, FB15k-237 dataset contains 310,116 triples with 14,541 entities and 237 relation types.",2015,,,,,
1124,FB15k,Knowledge Graph Embedding,Knowledge Graph Embedding,"Knowledge Graph Embedding, Complex Query Answering, Knowledge Graphs, Link Prediction, Knowledge Graph Completion","Graph, Time Series",,Methodology,"complex-query-answering-on-fb15k, link-prediction-on-fb15k-237, link-prediction-on-fb15k-1, complex-query-answering-on-fb15k-237, knowledge-graphs-on-fb15k, knowledge-graph-completion-on-fb15k-237, knowledge-graph-embedding-on-fb15k-1, link-prediction-on-fb15k, link-prediction-on-fb15k-filtered",CC BY 2.5,https://www.microsoft.com/en-us/download/details.aspx?id=52312,https://paperswithcode.com/dataset/fb15k,"The FB15k dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs. It has a total of  592,213 triplets with 14,951 entities and 1,345 relationships. FB15K-237 is a variant of the original dataset where inverse relations are removed, since it was found that a large number of test triplets could be obtained by inverting triplets in the training set.",,http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf,http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf,,,
1125,FBMS-59,Video Object Segmentation,Video Object Segmentation,"Video Object Segmentation, Unsupervised Object Segmentation, Unsupervised Video Object Segmentation, Video Salient Object Detection","Image, Video",,Computer Vision,"unsupervised-object-segmentation-on-fbms-59, video-object-segmentation-on-fbms-59, video-salient-object-detection-on-fbms-59",,https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html,https://paperswithcode.com/dataset/fbms-59,"The Freiburg-Berkeley Motion Segmentation Dataset (FBMS-59) is a dataset for motion segmentation, which extends the BMS-26 dataset with 33 additional video sequences. A total of 720 frames is annotated. FBMS-59 comes with a split into a training set and a test set. Typical challenges appear in both sets.",,,,,,
1126,FBMS,Video Object Segmentation,Video Object Segmentation,"Video Object Segmentation, Unsupervised Object Segmentation, Unsupervised Video Object Segmentation, Video Salient Object Detection","Image, Video",,Computer Vision,"unsupervised-object-segmentation-on-fbms-59, video-object-segmentation-on-fbms, video-object-segmentation-on-fbms-59, video-salient-object-detection-on-fbms-59","Custom (research-only, non-commercial)",https://lmb.informatik.uni-freiburg.de/resources/datasets/,https://paperswithcode.com/dataset/fbms,The Freiburg-Berkeley Motion Segmentation Dataset (FBMS-59) is an extension of the BMS dataset with 33 additional video sequences. A total of 720 frames is annotated. It has pixel-accurate segmentation annotations of moving objects. FBMS-59 comes with a split into a training set and a test set.,,,,,,
1127,FCE,Grammatical Error Detection,Grammatical Error Detection,Grammatical Error Detection,Image,,Computer Vision,grammatical-error-detection-on-fce,Custom (research-only),https://ilexir.co.uk/datasets/index.html,https://paperswithcode.com/dataset/fce,"The Cambridge Learner Corpus First Certificate in English (CLC FCE) dataset consists of short texts, written by learners of English as an additional language in response to exam prompts eliciting free-text answers and assessing mastery of the upper-intermediate proficiency level. The texts have been manually error-annotated using a taxonomy of 77 error types. The full dataset consists of 323,192 sentences. The publicly released subset of the dataset, named FCE-public, consists of 33,673 sentences split into test and training sets of 2,720 and 30,953 sentences, respectively.",,Compositional Sequence Labeling Models for Error Detection in Learner Writing,https://arxiv.org/abs/1607.06153,192 sentences,,
1128,FCGEC,Grammatical Error Detection,Grammatical Error Detection,"Grammatical Error Detection, Grammatical Error Correction",Image,,Computer Vision,grammatical-error-correction-on-fcgec,Apache-2.0 license,https://codalab.lisn.upsaclay.fr/competitions/8020,https://paperswithcode.com/dataset/fcgec,"a fine-grained corpus to detect, identify and correct the chinese grammatical errors.
collected mainly from multi-choice questions in public school Chinese examinations
with multiple references
Online Evaluation Site for test set:  https://codalab.lisn.upsaclay.fr/competitions/8020",,,,,,
1129,FDCompCN,Fraud Detection,Fraud Detection,"Fraud Detection, Node Classification",Image,,Computer Vision,fraud-detection-on-fdcompcn,,https://github.com/blackboxo/SplitGNN,https://paperswithcode.com/dataset/fdcompcn,"A new fraud detection dataset FDCompCN for detecting financial statement fraud of companies in China. We construct a multi-relation graph based on the supplier, customer, shareholder, and financial information disclosed in the financial statements of Chinese companies. These data are obtained from the China Stock Market and Accounting Research (CSMAR) database. We select samples between 2020 and 2023, including 5,317 publicly listed Chinese companies traded on the Shanghai, Shenzhen, and Beijing Stock Exchanges.",2020,,,,,
1130,FDDB,Face Detection,Face Detection,Face Detection,Image,,Computer Vision,face-detection-on-fddb,,http://vis-www.cs.umass.edu/fddb/,https://paperswithcode.com/dataset/fddb,"The Face Detection Dataset and Benchmark (FDDB) dataset is a collection of labeled faces from Faces in the Wild dataset. It contains a total of 5171 face annotations, where images are also of various resolution, e.g. 363x450 and 229x410. The dataset incorporates a range of challenges, including difficult pose angles, out-of-focus faces and low resolution. Both greyscale and color images are included.",,A Comparison of CNN-based Face and Head Detectors for Real-Time Video Surveillance Applications,https://arxiv.org/abs/1809.03336,,,
1131,FDF,Imputation,Imputation,"Imputation, Face Anonymization, Image Inpainting",Image,,Computer Vision,,,https://github.com/hukkelas/FDF,https://paperswithcode.com/dataset/fdf,"A diverse dataset of human faces, including unconventional poses, occluded faces, and a vast variability in backgrounds.",,,,,,
1132,FDST,Crowd Counting,Crowd Counting,"Crowd Counting, Active Learning, Binarization",,,Methodology,,,https://github.com/sweetyy83/Lstn_fdst_dataset,https://paperswithcode.com/dataset/fdst,The Fudan-ShanghaiTech dataset (FDST) is a dataset for video crowd counting. It contains 15K frames with about 394K annotated heads captured from 13 different scenes,,https://arxiv.org/abs/1907.07911,https://arxiv.org/abs/1907.07911,,,
1133,FEAFA_,3D Face Animation,3D Face Animation,"3D Face Animation, Facial Expression Recognition (FER)","3D, Image",,Computer Vision,,,https://www.iiplab.net/feafa+/,https://paperswithcode.com/dataset/feafa,"FEAFA+ is a dataset for Facial expression analysis and 3D Facial animation. It includes 150 video sequences from FEAFA and DISFA, with a total of 230,184 frames being manually annotated on floating-point intensity value of 24 redefined AUs using the Expression Quantitative Tool.",,,,,,
1134,FedTADBench,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Time Series Analysis, Time Series Anomaly Detection, Federated Learning","Image, Time Series",,Computer Vision,,,https://github.com/fanxingliu2020/FedTADBench,https://paperswithcode.com/dataset/fedtadbench,"FedTADBench is a federated time series anomaly detection benchmark. It covers 5 time series anomaly detection algorithms, 4 federated learning frameworks, and 3 time series anomaly detection datasets.

Source: [FedTADBench: Federated Time-Series Anomaly Detection Benchmark (https://arxiv.org/pdf/2212.09518v1.pdf)",,,,,,
1135,FEMNIST,Image Classification,Image Classification,"Image Classification, Personalized Federated Learning",Image,,Computer Vision,"personalized-federated-learning-on-femnist, image-classification-on-femnist",,,https://paperswithcode.com/dataset/femnist,"See paper:

Caldas, Sebastian, et al. ""Leaf: A benchmark for federated settings."" arXiv preprint arXiv:1812.01097 (2018).",2018,,,,,
1136,FER2013,Facial Expression Recognition,Facial Expression Recognition,"Facial Expression Recognition, Image Compression, Emotion Recognition, Image Clustering, Facial Expression Recognition (FER)",Image,,Computer Vision,"facial-expression-recognition-on-fer2013, facial-expression-recognition-on-fer2013-1, image-compression-on-fer2013, emotion-recognition-on-fer2013-1, image-clustering-on-fer2013",,https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data,https://paperswithcode.com/dataset/fer2013,"Fer2013 contains approximately 30,000 facial RGB images of different expressions with size restricted to 48×48, and the main labels of it can be divided into 7 types: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral. The Disgust expression has the minimal number of images – 600, while other labels have nearly 5,000 samples each.",,Eavesdrop the Composition Proportion of Training Labels in Federated Learning,https://arxiv.org/abs/1910.06044,000 samples,,
1137,FER2013_Blendshapes,3D Facial Expression Recognition,3D Facial Expression Recognition,"3D Facial Expression Recognition, 3D Facial Landmark Localization, 3D Classification","3D, Image",,Computer Vision,,Attribution 4.0 International (CC BY 4.0),https://www.kaggle.com/datasets/samerattrah/fer2013-blendshapes-dataset-example-partial,https://paperswithcode.com/dataset/fer2013-blendshapes,"Tables of the blendshapes from a group of the images of the FER2013 dataset, generated using MediaPipe library, based on the ARKit face blendshapes. with classes of the images in a separate column, describing the categories Happy, Unknown, Sad.",,,,,,
1138,FER_,Facial Expression Recognition,Facial Expression Recognition,"Facial Expression Recognition, Facial Expression Recognition (FER)",Image,,Computer Vision,"facial-expression-recognition-on-fer-2, facial-expression-recognition-on-ferplus, facial-expression-recognition-on-fer-1",Custom,https://github.com/Microsoft/FERPlus,https://paperswithcode.com/dataset/fer,"The FER+ dataset is an extension of the original FER dataset, where the images have been re-labelled into one of 8 emotion types: neutral, happiness, surprise, sadness, anger, disgust, fear, and contempt.",,,,,,
1139,FETA_Car-Manuals,Image-to-Text Retrieval,Image-to-Text Retrieval,"Image-to-Text Retrieval, Image Retrieval","Image, Text",English,Computer Vision,"image-to-text-retrieval-on-feta-car-manuals, image-retrieval-on-feta-car-manuals",,,https://paperswithcode.com/dataset/feta-car-manuals,"FETA benchmark focuses on text-to-image and image-to-text retrieval in public car manuals and sales catalogue brochures. The FETA Car-Manuals dataset consists of a total of 349 PDF documents from 5 car manufacturers, namely Nissan, Toyota, Mazda, Renault, Chevrolet.",,,,,,
1140,FEVER,Fact Verification,Fact Verification,"Fact Verification, Text Retrieval, Question Answering, Zero-shot Text Search",Text,English,Natural Language Processing,"question-answering-on-fever, fact-verification-on-fever, text-retrieval-on-fever, zero-shot-text-search-on-fever",Custom,https://fever.ai/resources.html,https://paperswithcode.com/dataset/fever,"FEVER is a publicly available dataset for fact extraction and verification against textual sources.

It consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as SUPPORTED, REFUTED or NOTENOUGHINFO. For the first two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim.

The claims were generated by human annotators extracting claims from Wikipedia and mutating them in a variety of ways, some of which were meaning-altering. The verification of each claim was conducted in a separate annotation process by annotators who were aware of the page but not the sentence from which original claim was
extracted and thus in 31.75% of the claims more than one sentence was considered appropriate evidence. Claims require composition of evidence from multiple sentences in 16.82% of cases. Furthermore, in 12.15% of the claims, this evidence was taken from multiple pages.",,FEVER: a large-scale dataset for Fact Extraction and VERification,https://arxiv.org/pdf/1803.05355v3.pdf,,,
1141,FEVEROUS,Fact Verification,Fact Verification,Fact Verification,,,Methodology,,,https://fever.ai/dataset/feverous.html,https://paperswithcode.com/dataset/feverous,"FEVEROUS (Fact Extraction and VERification Over Unstructured and Structured information) is a fact verification dataset which consists of 87,026 verified claims. Each claim is annotated with evidence in the form of sentences and/or cells from tables in Wikipedia, as well as a label indicating whether this evidence supports, refutes, or does not provide enough information to reach a verdict.",,,,,,
1142,Few-NERD,Multi-Grained Named Entity Recognition,Multi-Grained Named Entity Recognition,"Multi-Grained Named Entity Recognition, Named Entity Recognition, Named Entity Recognition (NER), Entity Typing, Low Resource Named Entity Recognition, Few-shot NER","Image, Text",English,Computer Vision,"named-entity-recognition-on-few-nerd-sup, named-entity-recognition-on-finegrained, few-shot-ner-on-few-nerd-inter, few-shot-ner-on-few-nerd-intra",CC BY-SA 4.0,https://ningding97.github.io/fewnerd/,https://paperswithcode.com/dataset/few-nerd,"Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, which contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities, and 4,601,223 tokens. Three benchmark tasks are built, one is supervised (Few-NERD (SUP)) and the other two are few-shot (Few-NERD (INTRA) and Few-NERD (INTER)).",,,,200 sentences,,
1143,FewRel,Relation Classification,Relation Classification,"Relation Classification, Few-Shot Relation Classification, Zero-shot Relation Triplet Extraction, Relation Extraction, Zero-shot Relation Classification","Graph, Image",,Computer Vision,"zero-shot-relation-classification-on-fewrel, relation-classification-on-fewrel-1, relation-extraction-on-fewrel, zero-shot-relation-triplet-extraction-on",CC BY-SA 4.0,http://www.zhuhao.me/fewrel/,https://paperswithcode.com/dataset/fewrel,"The FewRel (Few-Shot Relation Classification Dataset) contains 100 relations and 70,000 instances from Wikipedia. The dataset is divided into three subsets: training set (64 relations), validation set (16 relations) and test set (20 relations).",,Neural Snowball for Few-Shot Relation Learning,https://arxiv.org/abs/1908.11007,000 instances,,
1144,FewRel_2.0,Relation Classification,Relation Classification,"Relation Classification, Relation Extraction, Few-Shot Relation Classification","Graph, Image",,Computer Vision,,CC BY-SA 4.0,https://github.com/thunlp/fewrel,https://paperswithcode.com/dataset/fewrel-2-0,A more challenging task to investigate two aspects of few-shot relation classification models: (1) Can they adapt to a new domain with only a handful of instances? (2) Can they detect none-of-the-above (NOTA) relations?,,,,,,
1145,FewSOL,Key Point Matching,Key Point Matching,"Key Point Matching, Object Recognition, Pose Estimation, Zero-shot Text-to-Image Retrieval, Image-text Classification, 3D Shape Reconstruction, Image-to-Text Retrieval, Few-Shot Learning, Few-Shot Image Classification, Few-Shot Semantic Segmentation","3D, Image, Text",English,Computer Vision,,MIT,https://irvlutd.github.io/FewSOL,https://paperswithcode.com/dataset/fewsol,"The Few-Shot Object Learning (FewSOL) dataset can be used for object recognition with a few images per object. It contains 336 real-world objects with 9 RGB-D images per object from different views. Object segmentation masks, object poses and object attributes are provided. In addition, synthetic images generated using 330 3D object models are used to augment the dataset.  FewSOL dataset can be used to study a set of few-shot object recognition problems such as classification, detection and segmentation, shape reconstruction, pose estimation, keypoint correspondences and attribute recognition. 

Motivation: If robots can recognize objects from a few exemplar images, it is possible to scale up the number of objects a robot can recognize because collecting a few images per object is a much easier process compared to building a 3D model of an object. In addition, models trained in the meta-learning setting can generalize to new objects without re-training.",,,,,,
1146,FFHQ-Text,Text-to-Face Generation,Text-to-Face Generation,Text-to-Face Generation,"Image, Text",English,Computer Vision,,Creative Commons BY-NC-SA 4.0 license,https://github.com/Yutong-Zhou-cv/FFHQ-Text_Dataset,https://paperswithcode.com/dataset/ffhq-text,"FFHQ-Text is a small-scale face image dataset with large-scale facial attributes, designed for text-to-face generation & manipulation, text-guided facial image manipulation, and other vision-related tasks.
This dataset is an extension of the NVIDIA Flickr-Faces-HQ Dataset (FFHQ), which is the selected top 760 female FFHQ images that only contain one complete human face.",,,,,,
1147,FFHQ,Image Super-Resolution,Image Super-Resolution,"Image Super-Resolution, Face Hallucination, 3D-Aware Image Synthesis, Image Inpainting, Facial Inpainting, Image Generation, Image Denoising","3D, Image, Text",English,Computer Vision,"image-generation-on-ffhq-64x64-4x-upscaling, image-denoising-on-ffhq-64x64-4x-upscaling, image-generation-on-ffhq, image-inpainting-on-ffhq-512-x-512, image-generation-on-ffhq-512-x-512, image-generation-on-ffhq-u, image-super-resolution-on-ffhq-512-x-512-4x, image-generation-on-ffhq-1024-x-1024, image-super-resolution-on-ffhq-256-x-256-4x, image-super-resolution-on-ffhq-1024-x-1024-4x, 3d-aware-image-synthesis-on-ffhq-256-x-256, facial-inpainting-on-ffhq, image-denoising-on-ffhq, image-inpainting-on-ffhq-1024-x-1024, image-generation-on-ffhq-256-x-256, face-hallucination-on-ffhq-512-x-512-16x, 3d-aware-image-synthesis-on-ffhq-512-x-512-4x",CC BY-NC-SA 4.0,https://github.com/NVlabs/ffhq-dataset,https://paperswithcode.com/dataset/ffhq,"Flickr-Faces-HQ (FFHQ) consists of 70,000 high-quality PNG images at 1024×1024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr, thus inheriting all the biases of that website, and automatically aligned and cropped using dlib. Only images under permissive licenses were collected. Various automatic filters were used to prune the set, and finally Amazon Mechanical Turk was used to remove the occasional statues, paintings, or photos of photos.",,,,,,
1148,FG-NET,Age-Invariant Face Recognition,Age-Invariant Face Recognition,"Age-Invariant Face Recognition, Age Estimation",Image,,Computer Vision,"age-invariant-face-recognition-on-fg-net, age-estimation-on-fgnet",,https://yanweifu.github.io/FG_NET_data/,https://paperswithcode.com/dataset/fg-net,"FGNet is a dataset for age estimation and face recognition across ages. It is composed of a total of 1,002 images of 82 people with age range from 0 to 69 and an age gap up to 45 years",,Large age-gap face verification by feature injection in deep networks,https://arxiv.org/abs/1602.06149,002 images,,
1149,FGADR,Lesion Segmentation,Lesion Segmentation,Lesion Segmentation,Image,,Computer Vision,,,https://csyizhou.github.io/FGADR/,https://paperswithcode.com/dataset/fgadr,"This dataset has 1,842 images with pixel-level DR-related lesion annotations, and 1,000 images with image-level labels graded by six board-certified ophthalmologists with intra-rater consistency. The proposed dataset will enable extensive studies on DR diagnosis.",,,,842 images,,
1150,FGVC-Aircraft,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Neural Architecture Search, Fine-Grained Visual Recognition, Image Classification, Mitigating Contextual Bias, Image Clustering, Fine-Grained Image Classification, Prompt Engineering, Few-Shot Learning, Transductive Zero-Shot Classification",Image,,Computer Vision,"image-classification-on-fgvc-aircraft, fine-grained-image-classification-on-fgvc, transductive-zero-shot-classification-on-fgvc, fine-grained-image-classification-on-fgvc-2, neural-architecture-search-on-fgvc-aircraft, prompt-engineering-on-fgvc-aircraft, image-classification-on-fgvc-aircraft-1, fine-grained-visual-recognition-on-fgvc-2, image-clustering-on-fgvc-aircraft, zero-shot-learning-on-fgvc-aircraft, few-shot-learning-on-fgvc-aircraft-1, mitigating-contextual-bias-on-fgvc-aircraft",Custom (non-commercial),https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/,https://paperswithcode.com/dataset/fgvc-aircraft-1,"FGVC-Aircraft contains 10,200 images of aircraft, with 100 images for each of 102 different aircraft model variants, most of which are airplanes. The (main) aircraft in each image is annotated with a tight bounding box and a hierarchical airplane model label.
Aircraft models are organized in a four-levels hierarchy. The four levels, from finer to coarser, are:


Model, e.g. Boeing 737-76J. Since certain models are nearly visually indistinguishable, this level is not used in the evaluation.
Variant, e.g. Boeing 737-700. A variant collapses all the models that are visually indistinguishable into one class. The dataset comprises 102 different variants.
Family, e.g. Boeing 737. The dataset comprises 70 different families.
Manufacturer, e.g. Boeing. The dataset comprises 41 different manufacturers.
The data is divided into three equally-sized training, validation and test subsets.",,Fine-Grained Visual Classification of Aircraft,https://arxiv.org/abs/1306.5151,200 images,,
1151,Fields2Benchmark_dataset,Robot Navigation,Robot Navigation,"Robot Navigation, Robot Task Planning",,,Methodology,,CC-BY-SA-4.0 license,https://zenodo.org/records/14524735,https://paperswithcode.com/dataset/fields2benchmark-dataset,"The Fields2Benhmark dataset is a collection of 350 agricultural fields in vector format manually selected to test agricultural coverage path planning algorithms.

The files in this dataset are organized into two folders:


imgs/: contains the satellite images of the fields.
wkt/: has the vector data of the fields.

A file with the same name in both folders corresponds to the same field. The first two letters in the name of the field indicate the country the field belongs to.

The fields have been extracted from the EuroCrops dataset --specifically from the Netherlands, Estonia and Lithuania --, transformed each field into its own file, and converted them to Well-Known text (wkt). The satellite images were also provided, and they were created using the Mapping Toolbox from Matlab.

Funding
This dataset is provided as part of the project ""Fields2Cover: Robust and efficient coverage paths for autonomous agricultural vehicles"" (with project number ENPPS.LIFT.019.019 of the research programme Science PPP Fund for the top sectors which is (partly) financed by the Dutch Research Council (NWO).

@article{Mier_Fields2Cover_An_open-source_2023,
    author={Mier, Gonzalo and Valente, João and de Bruin, Sytze},
    journal={IEEE Robotics and Automation Letters},
    title={Fields2Cover: An Open-Source Coverage Path Planning Library for Unmanned Agricultural Vehicles},
    year={2023},
    volume={8},
    number={4},
    pages={2166-2172},
    doi={10.1109/LRA.2023.3248439}
  }",2023,,,,,
1152,FIGER,Entity Linking,Entity Linking,"Entity Linking, Entity Typing",,,Methodology,"entity-typing-on-figer, entity-linking-on-figer",,http://xiaoling.github.io/figer/,https://paperswithcode.com/dataset/figer%0A,"The FIGER dataset is an entity recognition dataset where entities are labelled using fine-grained system 112 tags, such as person/doctor, art/written_work and building/hotel. The tags are derivied from Freebase types. The training set consists of Wikipedia articles automatically annotated with distant supervision approach that utilizes the information encoded in anchor links. The test set was annotated manually.",,,,,,
1153,Figment,Entity Typing,Entity Typing,Entity Typing,,,Methodology,,,https://github.com/yyaghoobzadeh/figment,https://paperswithcode.com/dataset/figment,"A dataset for fine-grained entity typing of knowledge graph entities built from Freebase. 
It can be used to evaluate entity representations and also mention-level entity typing.",,,,,,
1154,FigureQA,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Visual Reasoning, Question Answering, Chart Question Answering","Image, Text",English,Computer Vision,visual-question-answering-on-figureqa-test-1,Custom,https://www.microsoft.com/en-us/research/project/figureqa-dataset/,https://paperswithcode.com/dataset/figureqa,"FigureQA is a visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts.",,,,000 images,,
1155,Filosax,Beat Tracking,Beat Tracking,"Beat Tracking, Downbeat Tracking, Chord Recognition","Image, Video",,Computer Vision,"beat-tracking-on-filosax, downbeat-tracking-on-filosax",,https://dave-foster.github.io/filosax/,https://paperswithcode.com/dataset/filosax,48 multitrack jazz recordings with many annotations.,,,,,,
1156,Financial_Dynamic_Knowledge_Graph,Graph Embedding,Graph Embedding,"Graph Embedding, Inductive knowledge graph completion, Named Entity Recognition, Financial Relation Extraction, Link Prediction, Relation Extraction, Node Classification","Graph, Image, Text, Time Series",English,Computer Vision,,GNU General Public License v3.0,https://xiaohui-victor-li.github.io/FinDKG/#data,https://paperswithcode.com/dataset/financial-dynamic-knowledge-graph,"FinDKG: The Global Financial Dynamic Knowledge Graph Dataset
FinDKG is an open-source dataset focused on creating a temporally-resolved Financial Dynamic Knowledge Graph. Designed to bridge the gap in industry-specific knowledge graphs, particularly in the financial sector, FinDKG provides a high-touch, temporally-aware representation of global economic and market dynamics. This repository includes comprehensive details about the dataset, methodology, and schema, aiming to facilitate academic research and actionable insights in global financial markets.

Background
While general-purpose knowledge graphs are abundant, industry-specific ones are comparatively rare, especially in the financial sector. FinDKG aims to fill this void by offering a resource for researchers and professionals looking to leverage knowledge graph technology in finance.

FinDKG Dataset
The dataset's foundation lies in an extensive news corpus curated to capture both qualitative and quantitative indicators in the financial landscape. We utilized the Wayback Machine to amass a dataset comprising global financial news. 

Dataset Structure

Temporal Knowledge Graph (TKG) with daily-resolved event triplets
Event triplets are tagged with specific timestamps corresponding to their release dates
Training, validation, and test splits organized chronologically
Weekly aggregation of event triplets as the basic unit of time

Data Format
/FinDKG is the default study dataset folder including the graph dataset and the corresponding data splits. The graph dataset is organized in the following structure:



'train.txt', 'valid.txt', and 'test.txt': The first four columns correspond to subject, relation, object, and time. The fifth column is ignored.



'stat.txt': The first two columns correspond to the number of entities and relations, respectively.



Test set is held-out for evaluating the model performance. This should match the results of the original paper regarding the Temporal Link Prediction evaluation.

/FinDKG-full: The full dataset including a larger size of the event triplets. This graph dataset adopts the same format as /FinDKG while is left for future extended research.


'time2id.txt': This time mapping table further provided the mapping from time ID to realistic date for real-world application.

Usage
The dataset is designed for graph-based AI methods aiming to generate actionable insights in the financial domain. It is freely available for academic and research purposes. Refer details to our designated FinDKG website.

(Description from Dataset Repo)",,,,,,
1157,FinArg,Financial Relation Extraction,Financial Relation Extraction,"Financial Relation Extraction, Argument Mining, Argument Retrieval",Graph,,Methodology,,Creative-Commons 3.0 license,https://github.com/Alaa-Ah/The-FinArg-Dataset-Argument-Mining-in-Financial-Earnings-Calls,https://paperswithcode.com/dataset/finarg,"With the goal of reasoning on the financial textual data, we present a novel dataset for annotating arguments, their components, and relations in the transcripts of earnings conference calls (ECCs).",,,,,,
1158,FinChat,Chatbot,Chatbot,Chatbot,,,Methodology,,,https://github.com/aalto-speech/FinChat,https://paperswithcode.com/dataset/finchat,Finnish chat conversation corpus and includes unscripted conversations on seven topics from people of different ages.,,,,,,
1159,FINDSum,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Multimodal Abstractive Text Summarization, Document Summarization",Text,English,Natural Language Processing,,ODC-BY,https://github.com/StevenLau6/FINDSum,https://paperswithcode.com/dataset/findsum,"FINDSum is a large-scale dataset for long text and multi-table summarization. It is built on 21,125 annual reports from 3,794 companies and has two subsets for summarizing each company’s results of operations and liquidity.",,Long Text and Multi-Table Summarization: Dataset and Method,https://aclanthology.org/2022.findings-emnlp.145.pdf,,,
1160,FineAction,Online Action Detection,Online Action Detection,"Online Action Detection, Weakly Supervised Action Localization, Temporal Action Localization, Weakly Supervised Temporal Action Localization, Temporal Action Proposal Generation","Image, Text, Time Series, Video",English,Computer Vision,"online-action-detection-on-fineaction, weakly-supervised-action-localization-on-7, temporal-action-localization-on-fineaction",,https://deeperaction.github.io/datasets/fineaction.html,https://paperswithcode.com/dataset/fineaction,"FineAction contains 103K temporal instances of 106 action categories, annotated in 17K untrimmed videos. FineAction introduces new opportunities and challenges for temporal action localization, thanks to its distinct characteristics of fine action classes with rich diversity, dense annotations of multiple instances, and co-occurring actions of different classes.",,,,,,
1161,FineDance,Motion Synthesis,Motion Synthesis,Motion Synthesis,Video,,Methodology,motion-synthesis-on-finedance,,https://li-ronghui.github.io/finedance,https://paperswithcode.com/dataset/finedance,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
1162,FiNER-139,NER,NER,NER,,,Methodology,,,https://huggingface.co/datasets/nlpaueb/finer-139,https://paperswithcode.com/dataset/finer-139,"FiNER-139 is comprised of 1.1M sentences annotated with eXtensive Business Reporting Language (XBRL) tags extracted from annual and quarterly reports of publicly-traded companies in the US. Unlike other entity extraction tasks, like named entity recognition (NER) or contract element extraction, which typically require identifying entities of a small set of common types (e.g., persons, organizations), FiNER-139 uses a much larger label set of 139 entity types. Another important difference from typical entity extraction is that FiNER focuses on numeric tokens, with the correct tag depending mostly on context, not the token itself.",,,,1M sentences,,
1163,Fingerprint_Dataset,Audio Fingerprint,Audio Fingerprint,"Audio Fingerprint, Music Information Retrieval",Audio,,Audio,,CC-SA-2.0,https://ieee-dataport.org/open-access/neural-audio-fingerprint-dataset,https://paperswithcode.com/dataset/fingerprint-dataset,"This dataset includes all music sources, background noises and impulse-reponses (IR) samples and conversation speech  that have been used in the work ""Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrastive Learning"" ICASSP 2021 (https://arxiv.org/abs/2010.11910).",2021,,,,,
1164,Fingerprint_inpainting_and_denoising,Color Image Denoising,Color Image Denoising,"Color Image Denoising, Image Inpainting, Image Restoration, Image Denoising, Denoising",Image,,Computer Vision,,,https://chalearnlap.cvc.uab.cat/dataset/32/description/,https://paperswithcode.com/dataset/fingerprint-inpainting-and-denoising,"Synthetic training set: This set is constructed in the following two steps and will be used for estimation/training purposes. i) 84,000 275 pixel x 400 pixel ground-truth fingerprint images without any noise or scratches, but with random transformations (at most five pixels translation and +/-10 degrees rotation) were generated by using the software Anguli: Synthetic Fingerprint Generator. ii) 84,000 275 pixel x 400 pixel degraded fingerprint images were generated by applying random artifacts (blur, brightness, contrast, elastic transformation, occlusion, scratch, resolution, rotation) and backgrounds to the ground-truth fingerprint images. In total, it contains 168,000 fingerprint images (84,000 fingerprints, and two impressions - one ground-truth and one degraded - per fingerprint).

Synthetic test set: This set is constructed similarly to the synthetic training set and will be used to evaluate the reconstruction performance. In total, it contains 16,800 fingerprint images (8,400 fingerprints and two impressions - one ground-truth and one degraded - per fingerprint). Since this set will be used for the purpose of evaluating the reconstruction performance, only the degraded and not the ground-truth fingerprint images will be provided to participants.

Real test set: This set is constructed by systematically drawing fingerprint images with varying sizes from publicly available datasets. In total, it contains 1680 fingerprint images (140 fingerprints and 12 impressions - high-quality scans under operational conditions - per fingerprint).

Description from: Fingerprint inpainting and denoising (WCCI'18, ECCV'18)",,,,,"training set: This set is constructed in the following two steps and will be used for estimation/training purposes. i) 84,000 275 pixel x 400 pixel ground-truth fingerprint images",
1165,FinnSentiment,Opinion Mining,Opinion Mining,"Opinion Mining, Sentiment Analysis",Text,English,Natural Language Processing,,,https://github.com/cynarr/MA-thesis/tree/master/data-raw,https://paperswithcode.com/dataset/finnsentiment,"FinnSentiment introduces a 27,000 sentence dataset (in Finnish) annotated independently with sentiment polarity by three native annotators.",,,,,,
1166,FinSen,Stock Market Prediction,Stock Market Prediction,"Stock Market Prediction, Time Series Regression, Text Classification","Image, Text, Time Series",English,Computer Vision,time-series-regression-on-finsen,MIT,https://pytorch.org/,https://paperswithcode.com/dataset/finsen,"Enhancing Financial Market Predictions: Causality-Driven Feature Selection
This paper introduces FinSen dataset that revolutionizes financial market analysis by integrating economic and financial news articles from 197 countries with stock market data. The dataset’s extensive coverage spans 15 years from 2007 to 2023 with temporal information, offering a rich, global perspective 160,000 records on financial market news. Our study leverages causally validated sentiment scores and LSTM models to enhance market forecast accuracy and reliability.

Our FinSen Dataset




This repository contains the dataset for Enhancing Financial Market Predictions:
Causality-Driven Feature Selection, which has been accepted in ADMA 2024.

If the dataset or the paper has been useful in your research, please add a citation to our work:

@article{liang2024enhancing,
  title={Enhancing Financial Market Predictions: Causality-Driven Feature Selection},
  author={Liang, Wenhao and Li, Zhengyang and Chen, Weitong},
  journal={arXiv e-prints},
  pages={arXiv--2408},
  year={2024}
}

Datasets
[FinSen] can be downloaded manually from the repository as csv file. Sentiment and its score are generated by FinBert model from the Hugging Face Transformers library under the identifier ""ProsusAI/finbert"".  (Araci, Dogu. ""Finbert: Financial sentiment analysis with pre-trained language models."" arXiv preprint arXiv:1908.10063 (2019).)

We only provide US for research purpose usage, please contact w.liang@adelaide.edu.au for other countries (total 197 included) if necessary.

We also provide other NLP datasets for text classification tasks here, please cite them correspondingly once you used them in your research if any.


20Newsgroups. Joachims, T., et al.: A probabilistic analysis of the rocchio algorithm with tfidf for
text categorization. In: ICML. vol. 97, pp. 143–151. Citeseer (1997)
AG News. Zhang, X., Zhao, J., LeCun, Y.: Character-level convolutional networks for text
classification. Advances in neural information processing systems 28 (2015)
Financial PhraseBank. Malo, P., Sinha, A., Korhonen, P., Wallenius, J., Takala, P.: Good debt or bad debt:
Detecting semantic orientations in economic texts. Journal of the Association for
Information Science and Technology 65(4), 782–796 (2014)

Dataloader for FinSen
We provide the preprocessing file finsen.py for our FinSen dataset under dataloaders directory for more convienient usage.

Models - Text Classification


DAN-3. 



Gobal Pooling CNN.



Models - Regression Prediction

LSTM

Using Sentiment Score from FinSen Predict Result on S&P500
Dependencies
The code is based on PyTorch under code frame of https://github.com/torrvision/focal_calibration, please cite their work if you found it is useful.

:smiley: ☺ Happy Research !",2007,Paper,https://arxiv.org/abs/2408.01005,000 records,,
1167,FireRisk,Image Classification,Image Classification,"Image Classification, Remote Sensing Image Classification",Image,,Computer Vision,remote-sensing-image-classification-on,CC BY-NC,,https://paperswithcode.com/dataset/firerisk,"In this work, we propose a novel remote sensing dataset, FireRisk, consisting of 7 fire risk classes with a total of 91 872 labelled images for fire risk assessment. This remote sensing dataset is labelled with the fire risk classes supplied by the Wildfire Hazard Potential (WHP) raster dataset, and remote sensing images are collected using the National Agriculture Imagery Program (NAIP), a high-resolution remote sensing imagery program. On FireRisk, we present benchmark performance for supervised and self-supervised representations, with Masked Autoencoders (MAE) pre-trained on ImageNet1k achieving the highest classification accuracy, 65.29%.",,,,,,
1168,First-Person_Hand_Action_Benchmark,3D Hand Pose Estimation,3D Hand Pose Estimation,"3D Hand Pose Estimation, Skeleton Based Action Recognition, Pose Estimation, Hand Pose Estimation, Activity Recognition","3D, Image, Video",,Computer Vision,"activity-recognition-on-first-person-hand, skeleton-based-action-recognition-on-first","Custom (research-only, non-commercial)",https://kcvl-kaist.github.io/FPHA/,https://paperswithcode.com/dataset/first-person-hand-action-benchmark,"First-Person Hand Action Benchmark is a collection of RGB-D video sequences comprised of more than 100K frames of 45 daily hand action categories, involving 26 different objects in several hand configurations.",,First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations,https://arxiv.org/pdf/1704.02463v2.pdf,,,
1169,Fish_Keypoints_Detection,Keypoint Detection,Keypoint Detection,Keypoint Detection,Image,,Computer Vision,,MIT,https://huggingface.co/datasets/Raniahossam33/fish_feeding,https://paperswithcode.com/dataset/fish-keypoints-detection,"The researchers collected 3,500 images of Tilapia fish, with each image containing three fish in a small bowl. These images were manually annotated using Roboflow, a tool for creating and managing annotated datasets. Four keypoints were labeled on each fish: mouth, peduncle, belly, and back. While the primary goal was to measure fish length using the mouth and peduncle points, the additional keypoints (belly and back) were included to support potential future research, such as using girth to determine fish weight. This dataset was used to train a YOLOv8 model for keypoint detection, achieving high accuracy in identifying these crucial points on the Tilapia fish.",,,,500 images,,
1170,Fitness-AQA,3D Pose Estimation,3D Pose Estimation,"3D Pose Estimation, Video Understanding, Pose Estimation, Disentanglement, 3D Action Recognition, Action Understanding, Pose Contrastive Learning, Motion Disentanglement, Action Recognition, 2D Human Pose Estimation, Action Quality Assessment","3D, Image, Video",,Computer Vision,,,https://github.com/ParitoshParmar/Fitness-AQA,https://paperswithcode.com/dataset/fitness-aqa,"Largest, first-of-its-kind, in-the-wild, fine-grained workout/exercise posture analysis dataset, covering three different exercises: BackSquat, Barbell Row, and Overhead Press. Seven different types of exercise errors are covered. Unlabeled data is also provided to facilitate self-supervised learning.",,Unknown,https://github.com/ParitoshParmar/Fitness-AQA/blob/main/fitness_aqa_dataset_license.pdf,,,
1171,Five-Billion-Pixels,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, The Semantic Segmentation Of Remote Sensing Imagery, Unsupervised Domain Adaptation, Domain Adaptation, Segmentation Of Remote Sensing Imagery",Image,,Computer Vision,,Open Source,https://x-ytong.github.io/project/Five-Billion-Pixels.html,https://paperswithcode.com/dataset/five-billion-pixels,"The Five-Billion-Pixels dataset contains more than 5 billion labeled pixels of 150 high-resolution Gaofen-2 (4 m) satellite images, annotated in a 24-category system covering artificial-constructed, agricultural, and natural classes. It possesses the advantage of rich categories, large coverage, wide distribution, and high-spatial resolution, which well reflects the distributions of real-world ground objects and can benefit to different land cover related studies.",,,,,,
1172,FIVR-200K,Contrastive Learning,Contrastive Learning,"Contrastive Learning, Video Retrieval",Video,,Methodology,video-retrieval-on-fivr-200k,,http://ndd.iti.gr/fivr/,https://paperswithcode.com/dataset/fivr-200k,"The FIVR-200K dataset has been collected to simulate the problem of Fine-grained Incident Video Retrieval (FIVR). The dataset comprises 225,960 videos associated with 4,687 Wikipedia events and 100 selected video queries.",,,,,,
1173,FixEval,Code Classification,Code Classification,"Code Classification, Code Translation, Code Repair","Image, Text",English,Computer Vision,,MIT,https://drive.google.com/drive/folders/1dzuHuouuWzlFCy1CMj9DYG9JGraEay27,https://paperswithcode.com/dataset/fixeval,"We introduce FixEval , a dataset for competitive programming bug fixing along with a comprehensive test suite and show the necessity of execution based evaluation compared to suboptimal match based evaluation metrics like BLEU, CodeBLEU, Syntax Match, Exact Match etc.",,,,,,
1174,FKD,Keyword Spotting,Keyword Spotting,"Keyword Spotting, Keyword Spotting CSS, Speaker Recognition","Audio, Image",,Computer Vision,"keyword-spotting-css-on-fkd, keyword-spotting-on-fkd",,,https://paperswithcode.com/dataset/fkd,"The football keyword dataset (FKD), as a new keyword spotting dataset in Persian, is collected with crowdsourcing. This dataset contains nearly 31000 samples in 18 classes.",,,,31000 samples,,18
1175,FLAG3D,3D Action Recognition,3D Action Recognition,"3D Action Recognition, Human action generation, Human Mesh Recovery, Human Activity Recognition","3D, Image, Text, Video",English,Computer Vision,,,https://andytang15.github.io/FLAG3D,https://paperswithcode.com/dataset/flag3d,"FLAG3D is a large-scale 3D fitness activity dataset with language instruction containing 180K sequences of 60 categories. FLAG3D features the following three aspects: 1) accurate and dense 3D human pose captured from advanced MoCap system to handle the complex activity and large movement, 2) detailed and professional language instruction to describe how to perform a specific activity, 3) versatile video resources from a high-tech MoCap system, rendering software, and cost-effective smartphones in natural environments.",,FLAG3D: A 3D Fitness Activity Dataset with Language Instruction,https://arxiv.org/pdf/2212.04638v1.pdf,,,60
1176,FLD,Logical Reasoning,Logical Reasoning,Logical Reasoning,,,Reasoning,,CC-BY-4.0,https://github.com/hitachi-nlp/FLD,https://paperswithcode.com/dataset/fld,"A deductive reasoning benchmark based on formal logic theory.
A model is required to generate a proof that (dis-) proves a given hypothesis based on a given set of facts.",,,,,,
1177,FLEURS,automatic-speech-translation,automatic-speech-translation,"automatic-speech-translation, Automatic Speech Recognition (ASR), Spoken language identification, Speech Recognition, Few-Shot Learning, Automatic Speech Recognition, Natural Language Inference (Few-Shot)","Audio, Image, Text",English,Speech,"automatic-speech-recognition-on-google-fleurs-16, automatic-speech-recognition-on-google-fleurs-29, automatic-speech-translation-on-fleurs, automatic-speech-recognition-on-fleurs-1, speech-recognition-on-fleurs, automatic-speech-recognition-on-google-fleurs-18",CC-BY,https://huggingface.co/datasets/google/fleurs,https://paperswithcode.com/dataset/fleurs,"We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Translation and Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like mSLAM. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding.",,,,,,
1178,Flickr30K-Noisy,Cross-modal retrieval with noisy correspondence,Cross-modal retrieval with noisy correspondence,Cross-modal retrieval with noisy correspondence,,,Methodology,cross-modal-retrieval-with-noisy-2,,,https://paperswithcode.com/dataset/flickr30k-20-nc-1k-test,"This dataset, based on Flickr30K, is introduced in Learning with Noisy Correspondence for Cross-modal Matching. Noisy correspondence is simulated by randomly shuffling the captions of training images for a specific percentage, denoted by noise ratio",,,,,,
1179,Flickr30k,Cross-Modal Retrieval,Cross-Modal Retrieval,"Cross-Modal Retrieval, Zero-shot Text-to-Image Retrieval, Image Captioning, mage-to-Text Retrieval, Phrase Grounding, Video Description, Image-to-Text Retrieval, Semi Supervised Learning for Image Captioning, Image Retrieval, Node Classification, Zero-Shot Cross-Modal Retrieval","Image, Text, Video",English,Computer Vision,"cross-modal-retrieval-on-flickr30k, image-to-text-retrieval-on-flickr30k, mage-to-text-retrieval-on-flickr30k, zero-shot-cross-modal-retrieval-on-flickr30k, image-captioning-on-flickr30k-captions-test, node-classification-on-flickr, image-retrieval-on-flickr30k-1k-test, image-retrieval-on-flickr30k, zero-shot-text-to-image-retrieval-on-1, semi-supervised-learning-for-image-captioning-2, phrase-grounding-on-flickr30k","Custom (research-only, non-commercial)",https://shannon.cs.illinois.edu/DenotationGraph/,https://paperswithcode.com/dataset/flickr30k,"The Flickr30k dataset contains 31,000 images collected from Flickr, together with 5 reference sentences provided by human annotators.",,Guiding Long-Short Term Memory for Image Caption Generation,https://arxiv.org/abs/1509.04942,000 images,,
1180,Flickr30K_Entities,Phrase Grounding,Phrase Grounding,Phrase Grounding,,,Methodology,"phrase-grounding-on-flickr30k-entities-test, phrase-grounding-on-flickr30k-entities-dev","Custom (research-only, non-commercial)",http://bryanplummer.com/Flickr30kEntities/,https://paperswithcode.com/dataset/flickr30k-entities,"The Flickr30K Entities dataset is an extension to the Flickr30K dataset. It augments the original 158k captions with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. This is used to define a new benchmark for localization of textual entity mentions in an image.",,,,,,
1181,FlickrLogos-32,Object Detection,Object Detection,"Object Detection, 2D Object Detection, Image Classification, Traffic Sign Recognition",Image,,Computer Vision,"object-detection-on-flickrlogos-32, traffic-sign-recognition-on-flickrlogos-32, image-classification-on-flickrlogos-32",,https://www.uni-augsburg.de/en/fakultaet/fai/informatik/prof/mmc/research/datensatze/flickrlogos/,https://paperswithcode.com/dataset/flickrlogos-32,"Object detection benchmark for logo detection.

Images are natural scenes. Each image contains multiple objects, and each image has a total of 1 logo. Logo detection & classification labels are provided.",,,,,,
1182,Flickr_Cropping_Dataset,Image Cropping,Image Cropping,"Image Cropping, Learning-To-Rank, Decision Making",Image,,Computer Vision,,,https://github.com/yiling-chen/flickr-cropping-dataset,https://paperswithcode.com/dataset/flickr-cropping-dataset,The Flick Cropping Dataset consists of high quality cropping and pairwise ranking annotations used to evaluate the performance of automatic image cropping approaches.,,https://arxiv.org/abs/1701.01480,https://arxiv.org/abs/1701.01480,,,
1183,FLIP,Protein Function Prediction,Protein Function Prediction,"Protein Function Prediction, Protein Design, regression",Time Series,,Methodology,,Academic Free License v3.0,https://benchmark.protein.properties/home,https://paperswithcode.com/dataset/flip,"FLIP includes several benchmark datasets that contain a variety of protein sequences, each with a real-valued label indicating its ""fitness"" (how well the protein performs some particular function). The goal is to predict the fitness of a given protein sequence using the sequence. Different representations of protein sequences (e.g. learned embeddings from large language models) may prove helpful here.

Some of the benchmark datasets (thermostability) contain a highly diverse set of sequences from many different protein families. Others (AAV, GB1) contain all sequences that are mutants of a single parent sequence. Each benchmark dataset features multiple ""splits"" -- different ways of train-test splitting the data to assess how well a model might generalize given limited information. The AAV benchmark, for example, features the ""mutant vs designed"" split in which a model is trained on randomly generated mutants and asked to predict the fitness of designed sequences, and the ""seven vs many"" split in which a model is trained on sequences with seven mutations and asked to make predictions for sequences with a different number of mutations.",,,,,,
1184,FLIP_--_AAV__Designed_vs_mutant,Protein Function Prediction,Protein Function Prediction,"Protein Function Prediction, Protein Design, regression",Time Series,,Methodology,,Academic Free License v3.0,https://benchmark.protein.properties/,https://paperswithcode.com/dataset/flip-aav-designed-vs-mutant,"FLIP includes several benchmark datasets that contain a variety of protein sequences, each with a real-valued label indicating its ""fitness"" (how well the protein performs some particular function). The goal is to predict the fitness of a given protein sequence using the sequence. Different representations of protein sequences (e.g. learned embeddings from large language models) may prove helpful here.

This sub-dataset (AAV) is a set of 201,426 training sequences and 82,583 test sequences in which the goal is to predict the fitness of mutants of the capsid protein from the adeno-associated virus (AAV). The training set proteins were designed, while the test set proteins are random mutants. The absolute value of the fitness is not important, but its ranking / relative value is -- protein designers would like to be able to pick a sequence with high fitness relative to those in the training set. Performance is therefore usually assessed using Spearman's r correlation coefficient.",,,,,,
1185,Florence3D,3D Action Recognition,3D Action Recognition,"3D Action Recognition, Temporal Action Localization, Skeleton Based Action Recognition, Action Recognition","3D, Image, Time Series, Video",,Computer Vision,skeleton-based-action-recognition-on-florence,"Custom (research-only, non-commercial)",https://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/,https://paperswithcode.com/dataset/florence3d,"The dataset collected at the University of Florence during 2012, has been captured using a Kinect camera. It includes 9 activities: wave, drink from a bottle, answer phone,clap, tight lace, sit down, stand up, read watch, bow. During acquisition, 10 subjects were asked to perform the above actions for 2/3 times. This resulted in a total of 215 activity samples.",2012,,,,,
1186,FLoRes-101,Translation tur-eng,Translation tur-eng,"Translation tur-eng, Translation eng-afr, Translation eng-tur, Translation afr-eng, Translation nld-eng, Translation deu-eng, Translation afr-deu, Translation eng-ara, Translation eng-deu, Translation ara-eng, Translation eng-spa, Translation deu-afr, Translation eng-nld, Machine Translation",Text,English,Natural Language Processing,"translation-afr-eng-on-flores101-devtest, translation-eng-nld-on-flores101-devtest, translation-deu-eng-on-flores101-devtest, translation-eng-tur-on-flores101-devtest, translation-nld-eng-on-flores101-devtest, translation-deu-afr-on-flores101-devtest, translation-eng-spa-on-flores101-devtest, translation-tur-eng-on-flores101-devtest, translation-eng-deu-on-flores101-devtest, translation-eng-ara-on-flores101-devtest, translation-eng-afr-on-flores101-devtest, translation-ara-eng-on-flores101-devtest, translation-afr-deu-on-flores101-devtest",CC-BY-SA,https://github.com/facebookresearch/flores,https://paperswithcode.com/dataset/flores-101,"FLoRes-101 is an evaluation benchmark for low-resource and multilingual machine translation. It consists of 3001 sentences extracted from English Wikipedia, covering a variety of different topics and domains. These sentences have been translated into 101 languages by professional translators through a carefully controlled process.

The FLoRes-101 dataset was introduced to address the lack of good evaluation benchmarks for low-resource languages. It enables better assessment of model quality in these languages and allows for the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned.",,,,3001 sentences,,
1187,FLoRes-200,Machine Translation,Machine Translation,Machine Translation,Text,English,Natural Language Processing,machine-translation-on-flores-200,,https://github.com/facebookresearch/flores,https://paperswithcode.com/dataset/flores-200,"FLoRes-200 doubles the existing language coverage of FLoRes-101. Given the nature of the new languages, which have less standardization and require more specialized professional translations, the verification process became more complex. This required modifications to the translation workflow. FLoRes-200 has several languages which were not translated from English. Specifically, several languages were translated from Spanish, French, Russian, and Modern Standard Arabic.",,,,,,
1188,FLUE,Word Sense Disambiguation,Word Sense Disambiguation,"Word Sense Disambiguation, Paraphrase Identification, Part-Of-Speech Tagging, Natural Language Inference, Text Classification, Constituency Parsing","Audio, Image, Text",English,Computer Vision,,Attribution-NonCommercial 4.0 International,https://github.com/getalp/Flaubert,https://paperswithcode.com/dataset/flue-french-language-understanding-evaluation,"FLUE is a French Language Understanding Evaluation benchmark. It consists of 5 tasks: Text Classification, Paraphrasing, Natural Language Inference, Constituency Parsing and Part-of-Speech Tagging, and Word Sense Disambiguation.",,,,,,
1189,Fluent_Speech_Commands,Voice Query Recognition,Voice Query Recognition,"Voice Query Recognition, Spoken Language Understanding","Audio, Image, Text",English,Computer Vision,spoken-language-understanding-on-fluent,Custom (research-only),https://fluent.ai/fluent-speech-commands-a-dataset-for-spoken-language-understanding-research/,https://paperswithcode.com/dataset/fluent-speech-commands,"Fluent Speech Commands is an open source audio dataset for spoken language understanding (SLU) experiments. Each utterance is labeled with ""action"", ""object"", and ""location"" values; for example, ""turn the lights on in the kitchen"" has the label {""action"": ""activate"", ""object"": ""lights"", ""location"": ""kitchen""}. A model must predict each of these values, and a prediction for an utterance is deemed to be correct only if all values are correct. 

The task is very simple, but the dataset is large and flexible to allow for many types of experiments: for instance, one can vary the number of speakers, or remove all instances of a particular sentence and test whether a model trained on the remaining sentences can generalize.",,,,,,
1190,FluidLab,Benchmarking,Benchmarking,Benchmarking,,,Methodology,,,https://fluidlab2023.github.io,https://paperswithcode.com/dataset/fluidlab,FluidLab is a simulation environment with a diverse set of manipulation tasks involving complex fluid dynamics. These tasks address interactions between solid and fluid as well as among multiple fluids.,,FluidLab: A Differentiable Environment for Benchmarking Complex Fluid Manipulation,https://arxiv.org/pdf/2303.02346v1.pdf,,,
1191,Fluo-C3DL-MDA231,Cell Segmentation,Cell Segmentation,Cell Segmentation,Image,,Computer Vision,cell-segmentation-on-fluo-c3dl-mda231,,http://celltrackingchallenge.net/3d-datasets/,https://paperswithcode.com/dataset/fluo-c3dl-mda231,"MDA231 human breast carcinoma cells infected with a pMSCV vector including the GFP sequence, embedded in a collagen matrix

Dr. R. Kamm. Dept. of Biological Engineering, Massachusetts Institute of Technology, Cambridge MA (USA)",,,,,,
1192,Fluo-N2DH-GOWT1,Cell Detection,Cell Detection,"Cell Detection, Cell Segmentation",Image,,Computer Vision,"cell-detection-on-fluo-n2dh-gowt1, cell-segmentation-on-fluo-n2dh-gowt1",,http://celltrackingchallenge.net/2d-datasets/,https://paperswithcode.com/dataset/fluo-n2dh-gowt1,"GFP-GOWT1 mouse stem cells

Dr. E. Bártová. Institute of Biophysics, Academy of Sciences of the Czech Republic, Brno, Czech Republic",,,,,,
1193,Fluo-N2DL-HeLa,Cell Detection,Cell Detection,"Cell Detection, Cell Segmentation",Image,,Computer Vision,"cell-segmentation-on-fluo-n2dl-hela, cell-detection-on-fluo-n2dl-hela",,http://celltrackingchallenge.net/2d-datasets/,https://paperswithcode.com/dataset/fluo-n2dl-hela,"HeLa cells stably expressing H2b-GFP

Mitocheck Consortium",,,,,,
1194,fluocells,Object Counting,Object Counting,"Object Counting, Transfer Learning, 2D Semantic Segmentation",Image,,Computer Vision,,Creative Commons Attribution 4.0 International,http://amsacta.unibo.it/6706/,https://paperswithcode.com/dataset/fluocells,"By releasing this dataset, we aim at providing a new testbed for computer vision techniques using Deep Learning. The main peculiarity is the shift from the domain of ""natural images"" proper of common benchmark dataset to biological imaging. We anticipate that the advantages of doing so could be two-fold: i) fostering research in biomedical-related fields - for which popular pre-trained models perform typically poorly - and ii) promoting methodological research in deep learning by addressing peculiar requirements of these images. Possible applications include but are not limited to semantic segmentation, object detection and object counting. The data consist of 283 high-resolution pictures (1600x1200 pixels) of mice brain slices acquired through a fluorescence microscope. The final goal is to individuate and count neurons highlighted in the pictures by means of a marker, so to assess the result of a biological experiment. The corresponding ground-truth labels were generated through a hybrid approach involving semi-automatic and manual semantic segmentation. The result consists of black (0) and white (255) images having pixel-level annotations of where the stained neurons are located. For more information, please refer to Morelli, R. et al., 2021. Automating cell counting in fluorescent microscopy through deep learning with c-ResUnet. Scientific reports. https://doi.org/10.1038/s41598-021-01929-5. The collection of original images was supported by funding from the University of Bologna (RFO 2018) and the European Space Agency (Research agreement collaboration 4000123556).",2021,,,,"testbed for computer vision techniques using Deep Learning. The main peculiarity is the shift from the domain of ""natural images"" proper of common benchmark dataset to biological imaging. We anticipate that the advantages of doing so could be two-fold: i) fostering research in biomedical-related fields - for which popular pre-trained models perform typically poorly - and ii) promoting methodological research in deep learning by addressing peculiar requirements of these images. Possible applications include but are not limited to semantic segmentation, object detection and object counting. The data consist of 283 high-resolution pictures (1600x1200 pixels) of mice brain slices acquired through a fluorescence microscope. The final goal is to individuate and count neurons highlighted in the pictures by means of a marker, so to assess the result of a biological experiment. The corresponding ground-truth labels were generated through a hybrid approach involving semi-automatic and manual semantic segmentation. The result consists of black (0) and white (255) images",
1195,FlyingThings3D,Optical Flow Estimation,Optical Flow Estimation,"Optical Flow Estimation, Scene Flow Estimation, Disparity Estimation",Video,,Methodology,,Custom (research-only),https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,https://paperswithcode.com/dataset/flyingthings3d,"FlyingThings3D is a synthetic dataset for optical flow, disparity and  scene flow estimation. It consists of everyday objects flying along randomized 3D trajectories. We generated about 25,000 stereo frames with ground truth data. Instead of focusing on a particular task (like KITTI) or enforcing strict naturalism (like Sintel), we rely on randomness and a large pool of rendering assets to generate orders of magnitude more data than any existing option, without running a risk of repetition or saturation.",,,,,,
1196,FMA,Genre classification,Genre classification,"Genre classification, Information Retrieval, Music Information Retrieval, Cadenza 1 - Task 2 - In Car","Audio, Image",,Computer Vision,"cadenza-1-task-2-in-car-on-fma, genre-classification-on-fma",Custom,https://github.com/mdeff/fma,https://paperswithcode.com/dataset/fma,"The Free Music Archive (FMA) is a large-scale dataset for evaluating several tasks in Music Information Retrieval. It consists of 343 days of audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies.

There are four subsets defined by the authors:


Full: the complete dataset,
Large: the full dataset with audio limited to 30 seconds clips extracted from the middle of the tracks (or entire track if shorter than 30 seconds),
Medium: a selection of 25,000 30s clips having a single root genre,
Small: a balanced subset containing 8,000 30s clips with 1,000 clips per one of 8 root genres.

The official split into training, validation and test sets (80/10/10) uses stratified sampling to preserve the percentage of tracks per genre. Songs of the same artists are part of one set only.",,FMA: A Dataset For Music Analysis,https://arxiv.org/pdf/1612.01840.pdf,,,
1197,FMD,Image Denoising,Image Denoising,"Image Denoising, Dictionary Learning, intensity image denoising, Denoising",Image,,Computer Vision,"intensity-image-denoising-on-fmd, image-denoising-on-fmd",,https://github.com/bmmi/denoising-fluorescence,https://paperswithcode.com/dataset/fmd,"The Fluorescence Microscopy Denoising (FMD) dataset is dedicated to Poisson-Gaussian denoising. The dataset consists of 12,000 real fluorescence microscopy images obtained with commercial confocal, two-photon, and wide-field microscopes and representative biological samples such as cells, zebrafish, and mouse brain tissues. Image averaging is used to effectively obtain ground truth images and 60,000 noisy images with different noise levels.",,https://arxiv.org/abs/1812.10366,https://arxiv.org/abs/1812.10366,,,
1198,FNC-1,Fake News Detection,Fake News Detection,"Fake News Detection, Stance Detection",Image,,Computer Vision,"fake-news-detection-on-fnc-1, stance-detection-on-fnc-1",,http://www.fakenewschallenge.org/,https://paperswithcode.com/dataset/fnc-1,"FNC-1 was designed as a stance detection dataset and it contains 75,385 labeled headline and article pairs. The pairs are labelled as either agree, disagree, discuss, and unrelated. Each headline in the dataset is phrased as a statement",,Investigating Rumor News Using Agreement-Aware Search,https://arxiv.org/abs/1802.07398,,,
1199,FollowMe_Vehicle_Behaviour_Prediction_Dataset,Human Behavior Forecasting,Human Behavior Forecasting,"Human Behavior Forecasting, Motion Forecasting, Trajectory Prediction, motion prediction, Trajectory Forecasting","Image, Time Series, Video",,Computer Vision,,,https://github.com/abduallahmohamed/FollowMe.git,https://paperswithcode.com/dataset/followme-vehicle-behaviour-prediction-dataset,"This dataset is a result of a study that was created to assess drivers behaviors when following a lead vehicle. The driving simulator study used a simulated suburban environment for collecting driver behavior data while following a lead vehicle driving through various unsignalized intersections. The driving environment had two lanes in each direction and a dedicated left-turn lane for the intersection. The experiment was deployed on a miniSim Driving Simulator.
We programmed the lead vehicle ran- domly turn left, right or go straight through the intersections. In total we had 2(traffic density) × 2(speed level) × 3 = 12 scenarios for each participant to be tested on.
We split the data into train, validation and test sets. The setup for the task is to observe 1 second of trajectories and predict the next 3,5 and 8 seconds.",,,,,,
1200,FollowUp,Semantic Parsing,Semantic Parsing,"Semantic Parsing, Dialogue Rewriting, Context Query Reformulation",Text,English,Natural Language Processing,,,https://github.com/SivilTaram/FollowUp,https://paperswithcode.com/dataset/followup,1000 query triples on 120 tables.,,,,,,
1201,Food-101,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Learning with noisy labels, Document Text Classification, Neural Architecture Search, Image Classification, Image Compression, Image Clustering, Fine-Grained Image Classification, Prompt Engineering, Zero-Shot Transfer Image Classification, Few-Shot Learning, Multimodal Text and Image Classification, Multi-Modal Document Classification, Classification, Transductive Zero-Shot Classification","Image, Text",English,Computer Vision,"learning-with-noisy-labels-on-food-101, image-clustering-on-food-101, prompt-engineering-on-food-101, image-classification-on-food-101-1, transductive-zero-shot-classification-on-food, document-text-classification-on-food-101, neural-architecture-search-on-food-101, multimodal-text-and-image-classification-on-1, classification-on-food101, image-compression-on-food-101, zero-shot-learning-on-food-101, fine-grained-image-classification-on-food-101, few-shot-learning-on-food101, multi-modal-document-classification-on-food, zero-shot-transfer-image-classification-on-17",,https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/,https://paperswithcode.com/dataset/food-101,"The  Food-101 dataset consists of 101 food categories with 750 training and 250 test images per category, making a total of 101k images. The labels for the test images have been manually cleaned, while the training set contains some noise.",,Combining Weakly and Webly Supervised Learning for Classifying Food Images,https://arxiv.org/abs/1712.08730,101k images,training and 250 test images,
1202,Food.com_Recipes_and_Interactions,Text Generation,Text Generation,"Text Generation, Recipe Generation",Text,English,Natural Language Processing,,,https://github.com/majumderb/recipe-personalization,https://paperswithcode.com/dataset/food-com-recipes-and-interactions,"Food.com Recipes and Interactions consists of 270K recipes and 1.4M user-recipe interactions (reviews) scraped from Food.com, covering a period of 18 years (January 2000 to December 2018).",2000,Generating Personalized Recipes from Historical User Preferences,https://arxiv.org/pdf/1909.00105.pdf,,,
1203,FOR-instance,3D Instance Segmentation,3D Instance Segmentation,"3D Instance Segmentation, 3D Semantic Segmentation","3D, Image",,Computer Vision,,Creative Commons Attribution 4.0 International,,https://paperswithcode.com/dataset/for-instance,"The challenge of accurately segmenting individual trees from laser scanning data hinders the assessment of crucial tree parameters necessary for effective forest management, impacting many downstream applications. While dense laser scanning offers detailed 3D representations, automating the segmentation of trees and their structures from point clouds remains difficult. The lack of suitable benchmark datasets and reliance on small datasets have limited method development. The emergence of deep learning models exacerbates the need for standardized benchmarks. Addressing these gaps, the FOR-instance data represent a novel benchmarking dataset to enhance forest measurement using dense airborne laser scanning data, aiding researchers in advancing segmentation methods for forested 3D scenes.

In this repository, users will find forest laser scanning point clouds from unamnned aerial vehicle (using Riegl sensors) that are manually segmented according to the individual trees (1130 trees) and semantic classes. The point clouds are subdivided into five data collections representing different forests in Norway, the Czech Republic, Austria, New Zealand, and Australia. 

These data are meant to be used either for developement of new methods (using the dev data) or for testing of exisitng methods (test data). The data splits are provided in the data_split_metadata.csv file.

A full description of the FOR-instance data can be found at http://arxiv.org/abs/2309.01279",,,,,,
1204,Foursquare,Crime Prediction,Crime Prediction,"Crime Prediction, Recommendation Systems",Time Series,,Methodology,,,https://sites.google.com/site/yangdingqi/home/foursquare-dataset,https://paperswithcode.com/dataset/foursquare,"The Foursquare dataset consists of check-in data for different cities. One subset contains check-ins in NYC and Tokyo collected for about 10 month (from 12 April 2012 to 16 February 2013). It contains 227,428 check-ins in New York city and 573,703 check-ins in Tokyo. Each check-in is associated with its time stamp, its GPS coordinates and its semantic meaning (represented by fine-grained venue-categories).
Another subset contains long-term (about 18 months from April 2012 to September 2013) global-scale check-in data collected from Foursquare. It contains 33,278,683 checkins by 266,909 users on 3,680,126 venues (in 415 cities in 77 countries). Those 415 cities are the most checked 415 cities by Foursquare users in the world, each of which contains at least 10K check-ins.",2012,,,,,
1205,FPDS,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Robot Navigation",Image,,Computer Vision,,,http://agamenon.tsc.uah.es/Investigacion/gram/papers/fall_detection/,https://paperswithcode.com/dataset/fpds,"A benchmark for detecting fallen people lying on the floor. It consists of 6982 images, with a total of 5023 falls and 2275 non falls corresponding to people in conventional situations (standing up, sitting, lying on the sofa or bed, walking, etc). Almost all the images have been captured in indoor environments with very different situations: variation of poses and sizes, occlusions, lighting changes, etc.",,,,6982 images,,
1206,FPL,Trajectory Forecasting,Trajectory Forecasting,"Trajectory Forecasting, Motion Forecasting, Trajectory Prediction","Time Series, Video",,Methodology,,,https://github.com/takumayagi/fpl,https://paperswithcode.com/dataset/fpl,Supports new task that predicts future locations of people observed in first-person videos.,,,,,,
1207,FR-FS,3D Action Recognition,3D Action Recognition,3D Action Recognition,"3D, Image, Video",,Computer Vision,,,https://github.com/Shunli-Wang/TSA-Net,https://paperswithcode.com/dataset/fr-fs,"The FR-FS dataset contains 417 videos collected from FIV dataset and Pingchang 2018 Winter Olympic Games. FR-FS contains the critical movements of the athlete’s take-off, rotation, and landing. Among them, 276 are smooth landing videos, and 141 are fall videos.
To test the generalization performance of our proposed model, we randomly select 50% of the videos from the fall and landing videos as the training set and the testing set.",2018,,,,,
1208,FracAtlas,Medical Diagnosis,Medical Diagnosis,"Medical Diagnosis, Semantic Segmentation, Object Localization, Medical X-Ray Image Segmentation, Object Detection, Classification",Image,,Medical,,CC BY 4.0,https://doi.org/10.6084/m9.figshare.22363012,https://paperswithcode.com/dataset/fracatlas,"FractureAtlas is a musculoskeletal bone fracture dataset with annotations for deep learning tasks like classification, localization, and segmentation. The dataset contains a total of 4,083 X-Ray images with annotation in COCO, VGG, YOLO, and Pascal VOC format. This dataset is made freely available for any purpose. The data provided within this work are free to copy, share or redistribute in any medium or format. The data might be adapted, remixed, transformed, and built upon. The dataset is licensed under a CC-BY 4.0 license. It should be noted that to use the dataset correctly, one needs to have knowledge of medical and radiology fields to understand the results and make conclusions based on the dataset. It's also important to consider the possibility of labeling errors.

Furthermore, any publication that utilizes this resource should acknowledge the original paper, and the authors are encouraged to share their code and models to assist the research community in replicating the experiments and promoting the field of medical imaging.",,,,,,
1209,FrameNet,Word Embeddings,Word Embeddings,"Word Embeddings, Semantic Parsing, Semantic Role Labeling",Text,English,Natural Language Processing,,Custom,https://framenet.icsi.berkeley.edu/fndrupal/,https://paperswithcode.com/dataset/framenet,"FrameNet is a linguistic knowledge graph containing information about lexical and predicate argument semantics of the English language. FrameNet contains two distinct entity classes: frames and lexical units, where a frame is a meaning and a lexical unit is a single meaning for a word.",,Retrofitting Distributional Embeddings to Knowledge Graphswith Functional Relations,https://arxiv.org/abs/1708.00112,,,
1210,FreCDo,Dialect Identification,Dialect Identification,Dialect Identification,,,Methodology,,CC0-1.0 license,https://github.com/MihaelaGaman/FreCDo,https://paperswithcode.com/dataset/frecdo,"FreCDo is a corpus for French dialect identification comprising 413,522 French text samples collected from public news websites in Belgium, Canada, France and Switzerland.",,FreCDo: A Large Corpus for French Cross-Domain Dialect Identification,https://arxiv.org/pdf/2212.07707v1.pdf,,,
1211,FREDo,Relation Classification,Relation Classification,"Relation Classification, Relation Extraction, Few-Shot Relation Classification","Graph, Image",,Computer Vision,"few-shot-relation-classification-on-fredo-1, few-shot-relation-classification-on-fredo",,,https://paperswithcode.com/dataset/fredo,"FREDo is a Few-Shot Document-Level Relation Extraction Benchmark based on DocRED and SciERC. The dataset is divided into four subsets: training set (62 relations), validation set (16 relations), in-domain test set (16 relations), and cross-domain test set (7 relations).",,,,,,
1212,Freebase__Heterogeneous_Node_Classification_,Heterogeneous Node Classification,Heterogeneous Node Classification,Heterogeneous Node Classification,Image,,Computer Vision,heterogeneous-node-classification-on-freebase,,,https://paperswithcode.com/dataset/freebase-heterogeneous-node-classification,A popular dataset for node classification on heterogeneous graphs.,,,,,,
1213,FreeSolv__Free_Solvation_,Molecular Property Prediction,Molecular Property Prediction,"Molecular Property Prediction, Drug Discovery",Time Series,,Methodology,"molecular-property-prediction-on-freesolv, drug-discovery-on-freesolv-scaffold",,https://github.com/MobleyLab/FreeSolv,https://paperswithcode.com/dataset/freesolv-scaffold,"The FreeSolv database offers a curated collection of experimental and calculated hydration-free energies for small molecules in water. It includes both experimental values obtained from prior literature and calculated values based on simulations. The goal is to provide accurate hydration-free energy data, which is essential for understanding solvation properties and interactions of molecules in aqueous environments.",,,,,,
1214,FreiHAND,3D Hand Pose Estimation,3D Hand Pose Estimation,3D Hand Pose Estimation,"3D, Image",,Computer Vision,3d-hand-pose-estimation-on-freihand,"Custom (research-only, non-commercial)",https://lmb.informatik.uni-freiburg.de/projects/freihand/,https://paperswithcode.com/dataset/freihand,"FreiHAND is a 3D hand pose dataset which records different hand actions performed by 32 people. For each hand image, MANO-based 3D hand pose annotations are provided. It currently contains 32,560 unique training samples and 3960 unique samples for evaluation. The training samples are recorded with a green screen background allowing for background removal. In addition, it applies three different post processing strategies to training samples for data augmentation. However, these post processing strategies are not applied to evaluation samples.",,Knowledge as Priors: Cross-Modal Knowledge Generalizationfor Datasets without Superior Knowledge,https://arxiv.org/abs/2004.00176,,training samples and 3960 unique samples,
1215,FrenchMedMCQA,Generative Question Answering,Generative Question Answering,"Generative Question Answering, Question-Answer-Generation, Science Question Answering, Knowledge Base Question Answering, Question Answering, Multiple Choice Question Answering (MCQA)",Text,English,Natural Language Processing,multiple-choice-question-answering-mcqa-on-22,Apache-2.0,https://github.com/qanastek/FrenchMedMCQA,https://paperswithcode.com/dataset/frenchmedmcqa,"This paper introduces FrenchMedMCQA, the first publicly available Multiple-Choice Question Answering (MCQA) dataset in French for medical domain. It is composed of 3,105 questions taken from real exams of the French medical specialization diploma in pharmacy, mixing single and multiple answers. Each instance of the dataset contains an identifier, a question, five possible answers and their manual correction(s). We also propose first baseline models to automatically process this MCQA task in order to report on the current performances and to highlight the difficulty of the task. A detailed analysis of the results showed that it is necessary to have representations adapted to the medical domain or to the MCQA task: in our case, English specialized models yielded better results than generic French ones, even though FrenchMedMCQA is in French. Corpus, models and tools are available online.",,,,,,
1216,French_CASS_dataset,Sentence Embedding,Sentence Embedding,"Sentence Embedding, Document Embedding, Sentence Embeddings",Text,English,Natural Language Processing,,,https://github.com/euranova/CASS-dataset,https://paperswithcode.com/dataset/french-cass-dataset,Composed of judgments from the French Court of cassation and their corresponding summaries.,,,,,,
1217,French_Timebank,Temporal Relation Extraction,Temporal Relation Extraction,"Temporal Relation Extraction, Temporal Tagging, Temporal Relation Classification, Event Extraction","Graph, Image, Time Series, Video",,Computer Vision,temporal-tagging-on-french-timebank,None,https://gforge.inria.fr/projects/fr-timebank/,https://paperswithcode.com/dataset/french-timebank,"French TimeBank, a corpus for French annotated in ISO-TimeML.

Some statistics:


Documents: 109
Events: 2100
Timexs: 608
Signal: 288
ALink: 36
SLink: 457
TLinks: 191",,None,https://aclanthology.org/P11-2023.pdf,,,
1218,Friedman1,Prediction Intervals,Prediction Intervals,"Prediction Intervals, Multi-Label Classification","Image, Time Series",,Computer Vision,,Custom,https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html,https://paperswithcode.com/dataset/friedman1,The friedman1 data set is commonly used to test semi-supervised regression methods.,,,,,,
1219,Friendster,Graph Sampling,Graph Sampling,"Graph Sampling, Graph Clustering, Node Classification","Graph, Image",,Computer Vision,,,https://snap.stanford.edu/data/com-Friendster.html,https://paperswithcode.com/dataset/friendster,"Friendster is an on-line gaming network. Before re-launching as a game website, Friendster was a social networking site where users can form friendship edge each other. Friendster social network also allows users form a group which other members can then join. The Friendster dataset consist of ground-truth communities (based on user-defined groups) and the social network from induced subgraph of the nodes that either belong to at least one community or are connected to other nodes that belong to at least one community.",,,,,,
1220,FRMT,Translation,Translation,"Translation, Machine Translation",Text,English,Natural Language Processing,"machine-translation-on-frmt-portuguese-brazil, machine-translation-on-frmt-chinese-mainland, machine-translation-on-frmt-portuguese, machine-translation-on-frmt-chinese-taiwan",CC BY-SA 3.0,https://github.com/google-research/google-research/tree/master/frmt,https://paperswithcode.com/dataset/frmt,"FRMT is a dataset and evaluation benchmark for Few-shot Region-aware Machine Translation, a type of style-targeted translation. The dataset consists of human translations of a few thousand English Wikipedia sentences into regional variants of Portuguese and Mandarin. Source documents are selected to enable detailed analysis of phenomena of interest, including lexically distinct terms and distractor terms.

Source:",,,,,,
1221,fruit-SALAD,Similarity Explanation,Similarity Explanation,"Similarity Explanation, Semantic Image Similarity, Semantic Image-Text Similarity, Similarities Abstraction, Image Similarity Search, Image Similarity Detection, Image Retrieval, Fine-Grained Image Recognition","Image, Text",English,Computer Vision,,Creative Commons Attribution 4.0 International,https://zenodo.org/records/11158522,https://paperswithcode.com/dataset/fruit-salad,"fruit-SALAD is a synthetic image dataset with 10,000 generated images of fruit depictions. This combined semantic category and style benchmark comprises 100 instances each of 10 easily recognizable fruit categories and 10 easy distinguishable styles. 

See the paper on ArXiv.

The carefully designed Style Aligned Artwork Dataset (SALAD) provides a controlled and balanced platform for the comparative analysis of similarity perception of different computational models. The SALAD framework allows the comparison of how these models perform semantic category and style recognition tasks, going beyond the level of anecdotal knowledge, making them robustly quantifiable and qualitatively interpretable.

We used Stable Diffusion XL and StyleAligned to create the fruit-SALAD by carefully crafting text prompts and overseing the image generation process.

The code to reproduce the fruit-SALAD_10k is available at GitHub.

Please note that this dataset is available for academic research purposes only.",,,,100 instances,,
1222,FSC-P2,Speaker Identification,Speaker Identification,"Speaker Identification, Robust Speech Recognition, Speaker Diarization","Audio, Image, Text",English,Computer Vision,,Creative Commons License (CC BY-SA 4.0),https://fearless-steps.github.io/ChallengePhase2/,https://paperswithcode.com/dataset/fsc-p2,"The Fearless Steps Initiative by UTDallas-CRSS led to the digitization, recovery, and diarization of 19,000 hours of original analog audio data, as well as the development of algorithms to extract meaningful information from this multichannel naturalistic data resource. As an initial step to motivate a stream-lined and collaborative effort from the speech and language community, UTDallas-CRSS is hosting a series of progressively complex tasks to promote advanced research on naturalistic “Big Data” corpora. This began with ISCA INTERSPEECH-2019: ""The FEARLESS STEPS Challenge: Massive Naturalistic Audio (FS-#1)"". This first edition of this challenge encouraged the development of core unsupervised/semi-supervised speech and language systems for single-channel data with low resource availability, serving as the “First Step” towards extracting high-level information from such massive unlabeled corpora.
As a natural progression following the successful Inaugural Challenge FS#1, the FEARLESS STEPS Challenge Phase-#2 focuses on the development of single-channel supervised learning strategies. This FS#2 provides 80 hours of ground-truth data through Training and Development sets, with an additional 20 hours of blind-set Evaluation data. Based on feedback from the Fearless Steps participants, additional Tracks for streamlined speech recognition and speaker diarization have been included in the FS#2. The results for this Challenge will be presented at the ISCA INTERSPEECH-2020 Special Session. We encourage participants to explore any and all research tasks of interest with the Fearless Steps Corpus – with suggested Task Domains listed below. Research participants can, however, also utilize the FS#2 corpus to explore additional problems dealing with naturalistic data, which we welcome as part of the special session.

--- This (FS-02) edition of the FEARLESS STEPS Challenge includes the following 6 tasks ---

TASK 1: Speech Activity Detection                            (SAD)
TASK 2: Speaker Identification (using Speaker Segments)      (SID)
TASK 3: Speaker Diarization
├── (3.a.) Track 1: Diarization using system SAD        (SD_track1)
└── (3.b.) Track 2: Diarization using reference SAD     (SD_track2)
TASK 4: Automatic Speech Recognition
├── (4.a.) Track 1: ASR using system Diarization/SAD    (ASR_track1)
└── (4.b.) Track 2: ASR using Diarized Segments         (ASR_track2)",2019,,,,,
1223,FSD50K,Contrastive Learning,Contrastive Learning,"Contrastive Learning, Audio Classification, Environmental Sound Classification, Domain Adaptation","Audio, Image",,Computer Vision,"audio-classification-on-fsd50k, environmental-sound-classification-on-fsd50k",Other (Attribution),https://zenodo.org/record/4060432,https://paperswithcode.com/dataset/fsd50k,"Freesound Dataset 50k (or FSD50K for short) is an open dataset of human-labeled sound events containing 51,197 Freesound clips unequally distributed in 200 classes drawn from the AudioSet Ontology. FSD50K has been created at the Music Technology Group of Universitat Pompeu Fabra. It consists mainly of sound events produced by physical sound sources and production mechanisms, including human sounds, sounds of things, animals, natural sounds, musical instruments and more.",,,,,,200
1224,FSDKaggle2018,Few-Shot Audio Classification,Few-Shot Audio Classification,"Few-Shot Audio Classification, Multi-Task Learning, Audio Tagging","Audio, Image",,Audio,few-shot-audio-classification-on,Other (Attribution),https://zenodo.org/record/2552860,https://paperswithcode.com/dataset/fsdkaggle2018,"FSDKaggle2018 is an audio dataset containing 11,073 audio files annotated with 41 labels of the AudioSet Ontology. FSDKaggle2018 has been used for the DCASE Challenge 2018 Task 2. All audio samples are gathered from Freesound and are provided as uncompressed PCM 16 bit, 44.1 kHz mono audio files. The 41 categories of the AudioSet Ontology are:
""Acoustic_guitar"", ""Applause"", ""Bark"", ""Bass_drum"", ""Burping_or_eructation"", ""Bus"", ""Cello"", ""Chime"", ""Clarinet"", ""Computer_keyboard"", ""Cough"", ""Cowbell"", ""Double_bass"", ""Drawer_open_or_close"", ""Electric_piano"", ""Fart"", ""Finger_snapping"", ""Fireworks"", ""Flute"", ""Glockenspiel"", ""Gong"", ""Gunshot_or_gunfire"", ""Harmonica"", ""Hi-hat"", ""Keys_jangling"", ""Knock"", ""Laughter"", ""Meow"", ""Microwave_oven"", ""Oboe"", ""Saxophone"", ""Scissors"", ""Shatter"", ""Snare_drum"", ""Squeak"", ""Tambourine"", ""Tearing"", ""Telephone"", ""Trumpet"", ""Violin_or_fiddle"", ""Writing"".",2018,,,,,41
1225,FSDKaggle2019,Audio Tagging,Audio Tagging,"Audio Tagging, Multi-Task Learning",Audio,,Audio,,Other (Attribution),https://zenodo.org/record/3612637,https://paperswithcode.com/dataset/fsdkaggle2019,"FSDKaggle2019 is an audio dataset containing 29,266 audio files annotated with 80 labels of the AudioSet Ontology. FSDKaggle2019 has been used for the DCASE Challenge 2019 Task 2, which was run as a Kaggle competition titled Freesound Audio Tagging 2019. The dataset allows development and evaluation of machine listening methods in conditions of label noise, minimal supervision, and real-world acoustic mismatch. FSDKaggle2019 consists of two train sets and one test set. One train set and the test set consists of manually-labeled data from Freesound, while the other train set consists of noisily labeled web audio data from Flickr videos taken from the YFCC dataset.
The curated train set consists of manually labeled data from FSD: 4970 total clips with a total duration of 10.5 hours.  The noisy train set has 19,815 clips with a total duration of 80 hours. The test set has 4481 clips with a total duration of 12.9 hours.",2019,,,,,80
1226,FSDnoisy18k,Sound Event Detection,Sound Event Detection,"Sound Event Detection, Audio Classification, Speaker Separation","Audio, Image",,Computer Vision,,,http://www.eduardofonseca.net/FSDnoisy18k/,https://paperswithcode.com/dataset/fsdnoisy18k,"The FSDnoisy18k dataset is an open dataset containing 42.5 hours of audio across 20 sound event classes, including a small amount of manually-labeled data and a larger quantity of real-world noisy data. The audio content is taken from Freesound, and the dataset was curated using the Freesound Annotator. The noisy set of FSDnoisy18k consists of 15,813 audio clips (38.8h), and the test set consists of 947 audio clips (1.4h) with correct labels. The dataset features two main types of label noise: in-vocabulary (IV) and out-of-vocabulary (OOV). IV applies when, given an observed label that is incorrect or incomplete, the true or missing label is part of the target class set. Analogously, OOV means that the true or missing label is not covered by those 20 classes.",,Model-agnostic Approaches to Handling Noisy Labels When Training Sound Event Classifiers,https://arxiv.org/abs/1910.12004,,,20
1227,FSDSoundScapes,Target Sound Extraction,Target Sound Extraction,"Target Sound Extraction, Streaming Target Sound Extraction",Audio,,Audio,"streaming-target-sound-extraction-on, target-sound-extraction-on-fsdsoundscapes",MIT,https://github.com/vb000/Waveformer#dataset,https://paperswithcode.com/dataset/fsdsoundscapes,"A synthetic sound mixture specification dataset for the Target Sound Extraction (TSE) task.  Dataset samples consist of a .jams file specifying the mixture components, and a metadata file with target labels. Mixtures are 6 seconds long and contain 3-5 unique foreground sounds over a 6 second long background sound. Each sample is provided with 3 target labels, and sounds corresponding to all target labels are guaranteed to be present in the mixture. FSDKaggle2018 is used as the source for foreground sounds and TAU Urban Acoustic Scenes 2019 is used as the source for background sounds.

Split
Train: 50K
Val: 5K
Test: 10K",2019,,,,,
1228,FSI,PDE Surrogate Modeling,PDE Surrogate Modeling,PDE Surrogate Modeling,,,Methodology,,,https://github.com/neuraloperator/CoDA-NO,https://paperswithcode.com/dataset/fsi,"Data Set Structure
Fluid Structure Interaction(NS +Elastic wave)
The TF_fsi2_results folder contains simulation data organized by various parameters (mu, x1, x2) where mu determines the viscosity and x1 and x2 are the parameters of the inlet condition. The dataset includes files for mesh, displacement, velocity, and pressure. 

This dataset structure is detailed below:

plaintext
TF_fsi2_results/
├── mesh.h5                         # Initial mesh
├── mu=1.0/                         # Simulation results for mu = 1.0
│   ├── x1=-4/                      # Inlet parameter x1 = -4
│   │   ├── x2=-4/                  # Inlet parameter for x2 = -4
│   │   │   └── visualization/      
│   │   │       ├── displacement.h5 # Displacements for mu=1.0, x1=-4, x2=-4
│   │   │       ├── velocity.h5     # Velocity field for mu=1.0, x1=-4, x2=-4
│   │   │       └── pressure.h5     # Pressure field for mu=1.0, x1=-4, x2=-4
│   │   ├── x2=-2/
│   │   │   └── visualization/
│   │   │       ├── displacement.h5
│   │   │       ├── velocity.h5
│   │   │       └── pressure.h5
│   │   └── ...                     # Other x2 values for x1 = -4
│   ├── x1=-2/
│   │   ├── x2=-4/
│   │   │   └── visualization/
│   │   │       ├── displacement.h5
│   │   │       ├── velocity.h5
│   │   │       └── pressure.h5
│   │   └── ...                     # Other x2 values for x1 = -2
│   └── ...                         # Other x1 values for mu = 1.0
├── mu=5.0/                         # Simulation results for mu = 5.0
│   └── ...                         # Similar structure as mu=1.0
└── mu=10.0/                        # Simulation results for mu = 10.0
    └── ...                         # Similar structure as mu=1.0
The dataset has readData.py and readMesh.py for loading the data. Also, the NsElasticDataset class in data_utils/data_loaders.py loads data automatically for all specified mus and inlet conditions (x1 and x2).

Fluid Motions with Non-deformable Solid(NS)
The data is in the folder TF_cfd2_results, and the organization is the same as above.",,,,,,
1229,FSOD,Object Detection,Object Detection,"Object Detection, Object Counting, Few-Shot Object Detection",Image,,Computer Vision,,,https://github.com/fanq15/Few-Shot-Object-Detection-Dataset,https://paperswithcode.com/dataset/fsod,Few-Shot Object Detection Dataset (FSOD) is a high-diverse dataset specifically designed for few-shot object detection and intrinsically designed to evaluate thegenerality of a model on novel categories.,,,,,,
1230,FSS-1000,Few-Shot Semantic Segmentation,Few-Shot Semantic Segmentation,Few-Shot Semantic Segmentation,Image,,Computer Vision,few-shot-semantic-segmentation-on-fss-1000,,https://github.com/HKUSTCV/FSS-1000,https://paperswithcode.com/dataset/fss-1000,"FSS-1000 is a 1000 class dataset for few-shot segmentation. The dataset contains significant number of objects that have never been seen or annotated in previous datasets, such as tiny daily objects, merchandise, cartoon characters, logos, etc.",,,,,,
1231,FTR-18,Rumour Detection,Rumour Detection,"Rumour Detection, Text Classification","Image, Text",English,Computer Vision,,,https://github.com/dcaled/FTR-18,https://paperswithcode.com/dataset/ftr-18,"FTR-18 is a multilingual rumour dataset on football transfer news. Transfer rumours are continuously published by sports media. They can both harm the image of player or a club or increase the player's market value. The proposed dataset includes transfer articles written in English, Spanish and Portuguese. It also comprises Twitter reactions related to the transfer rumours. FTR-18 is suited for rumour classification tasks and allows the research on the linguistic patterns used in sports journalism.",,,,,,
1232,Full-Spectral_Autofluorescence_Lifetime_Microscopi,Medical Image Registration,Medical Image Registration,Medical Image Registration,Image,,Medical,,CC BY 4.0,,https://paperswithcode.com/dataset/full-spectral-autofluorescence-lifetime,"The dataset contains full-spectral autofluorescence lifetime microscopic images (FS-FLIM) acquired on unstained ex-vivo human lung tissue, where 100 4D hypercubes of 256x256 (spatial resolution) x 32 (time bins) x 512 (spectral channels from 500nm to 780nm). This dataset associates with our paper ""Deep Learning-Assisted Co-registration of Full-Spectral Autofluorescence Lifetime Microscopic Images with H&E-Stained Histology Images"" (https://arxiv.org/abs/2202.07755) and ""Full spectrum fluorescence lifetime imaging with 0.5 nm spectral and 50 ps temporal resolution"" (https://doi.org/10.1038/s41467-021-26837-0). 
The FS-FLIM images provide transformative insights into human lung cancer with extra-dimensional information. This will enable visual and precise detection of early lung cancer. With the methodology in our co-registration paper, FS-FLIM images can be registered with H&E-stained histology images, allowing characterisation of tumour and surrounding cells at a celluar level with absolute autofluorescence lifetime.
The dataset can be used for various purposes, including signal processing for optimal lifetime reconstruction, advanced image analysis for automatic feature extraction of lung cancer, and cellular-level characterisation of lung cancer with absolute label-free autofluorescence lifetime values.
The dataset is available on the University of Edinburgh's DataShare (https://doi.org/10.7488/ds/3099 and https://doi.org/10.7488/ds/3421)",,,,,,
1233,FUNSD,Table Detection,Table Detection,"Table Detection, Optical Character Recognition (OCR), Semantic entity labeling, Entity Linking, Relation Extraction, Named Entity Recognition (NER)","Graph, Image, Tabular, Text",English,Computer Vision,"entity-linking-on-funsd, relation-extraction-on-funsd, semantic-entity-labeling-on-funsd",Custom,https://guillaumejaume.github.io/FUNSD/,https://paperswithcode.com/dataset/funsd,"Form Understanding in Noisy Scanned Documents (FUNSD) comprises 199 real, fully annotated, scanned forms. The documents are noisy and vary widely in appearance, making form understanding (FoUn) a challenging task. The proposed dataset can be used for various tasks, including text detection, optical character recognition, spatial layout analysis, and entity labeling/linking.",,,,,,
1234,Furit360,Clustering,Clustering,Clustering,,,Methodology,,,http://faculty.washington.edu/juhuah/images/AugDMC_datasets.zip,https://paperswithcode.com/dataset/furit360,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
1235,FurnitureBench_DiffIK_sim_demos,Robot Manipulation,Robot Manipulation,Robot Manipulation,,,Methodology,,,https://drive.google.com/drive/folders/13UqtMLXY1_8JCQOZf3j-YbZyMRTsgZ2K?usp=sharing,https://paperswithcode.com/dataset/furniturebench-diffik-sim-demos,"Demonstration data for 4 FurnitureBench tasks collected with a SpaceMouse using a DiffIK Controller.

Tasks: one_leg, round_table, lamp, and square_table.

Can be used to train policies from human data in simulation, as opposed to the scripted data included with the benchmark.",,,,,,
1236,FUSS,Audio Source Separation,Audio Source Separation,Audio Source Separation,Audio,,Audio,,,https://github.com/google-research/sound-separation/blob/master/datasets/fuss/FUSS_license_doc/README.md,https://paperswithcode.com/dataset/fuss,"The Free Universal Sound Separation (FUSS) dataset is a database of arbitrary sound mixtures and source-level references, for use in experiments on arbitrary sound separation. FUSS is based on FSD50K corpus.",,,,,,
1237,FutureHouse,Neural Rendering,Neural Rendering,"Neural Rendering, Depth Estimation, Surface Normal Estimation, Intrinsic Image Decomposition","3D, Image",,Computer Vision,,GNU 2.0,http://yodlee.top/PhyIR/,https://paperswithcode.com/dataset/futurehouse,"We present a new large-scale photorealistic panoramic dataset named FutureHouse, which has the following characteristics. 
     1) It contains over 70,000 high-quality models with high-resolution meshes and physical material. All models are measured in real world standards. 
     2) Selected scene layouts are carefully designed by over 100 excellent artists. All of selected layouts are used in realworld display. 
     3) It contains 28,579 good panoramic views from 1,752 house-scale scenes. Therefore, it can be used for perspective image tasks as well as omnidirectional image tasks. 
     4) More physical material representation. Most materials are represent by microfacet BRDF modeling metalness, and the rest are represent by special shading models, e.g., cloth material and transmission material. 
     5) High rendering quality. Benefiting from commercial rendering engine, Unreal engine 4, and powerful deep learning super sampling (DLSS), our renderings have less noise. 
Our SVBRDF representation including base color and metalness is capable of producing nonmonochrome specular reflectance.",,,,,,
1238,FVI,Image Inpainting,Image Inpainting,"Image Inpainting, Video Inpainting","Image, Video",,Computer Vision,,,https://github.com/amjltc295/Free-Form-Video-Inpainting,https://paperswithcode.com/dataset/fvi,"The Free-Form Video Inpainting dataset is a dataset used for training and evaluation video inpainting models. It consists of 1940 videos from the YouTube-VOS dataset and 12,600 videos from the YouTube-BoundingBoxes.",1940,https://arxiv.org/abs/1904.10247,https://arxiv.org/abs/1904.10247,,,
1239,G-VUE,Depth Estimation,Depth Estimation,"Depth Estimation, Common Sense Reasoning, Semantic Segmentation, Phrase Grounding, Visual Reasoning, 3D Reconstruction, Question Answering, Navigate","3D, Image, Text",English,Computer Vision,,,https://github.com/wllmzhu/G-VUE,https://paperswithcode.com/dataset/g-vue,"General-purpose Visual Understanding Evaluation (G-VUE) is a comprehensive benchmark covering the full spectrum of visual cognitive abilities with four functional domains -- Perceive, Ground, Reason, and Act. The four domains are embodied in 11 carefully curated tasks, from 3D reconstruction to visual reasoning and manipulation.",,"Perceive, Ground, Reason, and Act: A Benchmark for General-purpose Visual Representation",https://arxiv.org/pdf/2211.15402v1.pdf,,,
1240,G3D,Skeleton Based Action Recognition,Skeleton Based Action Recognition,"Skeleton Based Action Recognition, Pose Prediction","3D, Image, Time Series, Video",,Computer Vision,"skeleton-based-action-recognition-on-gaming, pose-prediction-on-gaming-3d-g3d",,http://velastin.dynu.com/G3D/G3D.html,https://paperswithcode.com/dataset/g3d,"The Gaming 3D Dataset (G3D) focuses on real-time action recognition in a gaming scenario. It contains 10 subjects performing 20 gaming actions: “punch right”, “punch left”, “kick right”, “kick left”, “defend”, “golf swing”, “tennis swing forehand”, “tennis swing backhand”, “tennis serve”, “throw bowling ball”, “aim and fire gun”, “walk”, “run”, “jump”, “climb”, “crouch”, “steer a car”, “wave”, “flap” and “clap”.",,Skeleton Based Action Recognition Using Translation-Scale Invariant Image Mapping And Multi-Scale Deep CNN,https://arxiv.org/abs/1704.05645,,,
1241,GamePad_Environment,Automated Theorem Proving,Automated Theorem Proving,Automated Theorem Proving,,,Methodology,,,https://github.com/ml4tp/gamepad,https://paperswithcode.com/dataset/gamepad-environment,GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant.,,,,,,
1242,Game_of_24,Arithmetic Reasoning,Arithmetic Reasoning,Arithmetic Reasoning,,,Reasoning,arithmetic-reasoning-on-game-of-24,,https://github.com/princeton-nlp/tree-of-thought-llm,https://paperswithcode.com/dataset/game-of-24,"Game of 24 is a mathematical reasoning challenge, where the goal is to use 4 numbers and basic arithmetic operations (+-*/) to obtain 24. For example, given input “4 9 10 13”, a solution output could be “(10 - 4) * (13 - 9) = 24”. We scrape data from 4nums.com, which has 1,362 games that are sorted from easy to hard by human solving time, and use a subset of relatively hard games indexed 901-1,000 for testing. For each task, we consider the output as success if it is a valid equation that equals 24 and uses the input numbers each exactly once. We report the success rate across 100 games as the metric.",,,,,,
1243,GAP,Coreference Resolution,Coreference Resolution,Coreference Resolution,,,Methodology,coreference-resolution-on-gap-1,,http://gap.cs.berkeley.edu/benchmark.html,https://paperswithcode.com/dataset/gap,"GAP is a graph processing benchmark suite with the goal of helping to standardize graph processing evaluations. Fewer differences between graph processing evaluations will make it easier to compare different research efforts and quantify improvements. The benchmark not only specifies graph kernels, input graphs, and evaluation methodologies, but it also provides optimized baseline implementations. These baseline implementations are representative of state-of-the-art performance, and thus new contributions should outperform them to demonstrate an improvement. The input graphs are sized appropriately for shared memory platforms, but any implementation on any platform that conforms to the benchmark's specifications could be compared. This benchmark suite can be used in a variety of settings. Graph framework developers can demonstrate the generality of their programming model by implementing all of the benchmark's kernels and delivering competitive performance on all of the benchmark's graphs. Algorithm designers can use the input graphs and the baseline implementations to demonstrate their contribution. Platform designers and performance analysts can use the suite as a workload representative of graph processing.",,,,,,
1244,GAP_Coreference_Dataset,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Data Augmentation, Natural Language Inference",Text,English,Natural Language Processing,coreference-resolution-on-gap-1,,https://github.com/google-research-datasets/gap-coreference,https://paperswithcode.com/dataset/gap-coreference-dataset,"GAP is a gender-balanced dataset containing 8,908 coreference-labeled pairs of (ambiguous pronoun, antecedent name), sampled from Wikipedia and released by Google AI Language for the evaluation of coreference resolution in practical applications.",,https://arxiv.org/pdf/1810.05201.pdf,https://arxiv.org/pdf/1810.05201.pdf,,,
1245,GarmentCodeData,Garment Reconstruction,Garment Reconstruction,Garment Reconstruction,3D,,Methodology,,CC BY,https://doi.org/10.3929/ethz-b-000690432,https://paperswithcode.com/dataset/garmentcodedata,"GarmentCodeData contains 115,000 data points that cover a variety of designs in many common garment categories: tops, shirts, dresses, jumpsuits, skirts, pants, etc., fitted to a variety of body shapes sampled from a custom statistical body model based on CAESAR, as well as a standard reference body shape, applying three different textile materials.",,,,,,
1246,GAS,Few-Shot Semantic Segmentation,Few-Shot Semantic Segmentation,"Few-Shot Semantic Segmentation, Grasp Contact Prediction","Image, Time Series",,Computer Vision,,,https://github.com/sudraj2002/fsgrasp,https://paperswithcode.com/dataset/gas,"GAS (Grasp Area Segmentation) dataset consists of 10089 RGB images of cluttered scenes grouped into 1121 grasp-area segmentation tasks. For each RGB image we provide a binary segmentation map with the graspable and non-graspable regions for every object in the scene. The dataset can be used for meta-training part-based grasp area estimation networks.

For creating the GAS dataset we use the RGB images and corresponding ground truth segmentation masks from the GraspNet 1-Billion dataset.",,,,,,
1247,GATITOS,Machine Translation,Machine Translation,"Machine Translation, Zero-Shot Machine Translation",Text,English,Natural Language Processing,,CC-BY-4.0,https://github.com/google-research/url-nlp/tree/main/gatitos,https://paperswithcode.com/dataset/gatitos,"The GATITOS (Google's Additional Translations Into Tail-languages: Often Short) dataset is a high-quality, multi-way parallel dataset of tokens and short phrases, intended for training and improving machine translation models. This dataset consists in 4,000 English segments (4,500 tokens) that have been translated into each of 26 low-resource languages, as well as three higher-resource pivot languages (es, fr, hi). All translations were made directly from English, with the exception of Aymara, which was translated from the Spanish.",,,,,,
1248,GazeFollow,Activity Recognition,Activity Recognition,"Activity Recognition, Pose Estimation, Decision Making, Gaze Target Estimation","3D, Image, Video",,Computer Vision,gaze-target-estimation-on-gazefollow,Custom (research-only),http://gazefollow.csail.mit.edu/index.html,https://paperswithcode.com/dataset/gazefollow,"GazeFollow is a large-scale dataset annotated with the location of where people in images are looking. It uses several major datasets that contain people as a source of images: 1, 548 images from SUN, 33, 790 images from MS COCO, 9, 135 images from Actions 40, 7, 791 images from PASCAL, 508 images from the ImageNet detection challenge and 198, 097 images from the Places dataset. This concatenation results in a challenging and large image collection of people performing diverse activities in many everyday scenarios.",,,,548 images,,
1249,GBSG2,Survival Analysis,Survival Analysis,"Survival Analysis, Time-to-Event Prediction",Time Series,,Methodology,,,https://lifelines.readthedocs.io/en/latest/lifelines.datasets.html#lifelines.datasets.load_gbsg2,https://paperswithcode.com/dataset/gbsg2,"The German Breast Cancer Study Group (GBSG2) dataset studies the effects of hormone treatment on recurrence-free survival time. 
The event of interest is the recurrence of cancer time. 
This data frame contains the observations of 686 women:


horTh: hormonal therapy, a factor at two levels (yes and no).
age: age of the patients in years.
menostat: menopausal status, a factor at two levels pre (premenopausal) and post (postmenopausal).
tsize: tumor size (in mm).
tgrade: tumor grade, a ordered factor at levels I < II < III.
pnodes: number of positive nodes.
progrec: progesterone receptor (in fmol).

estrec: estrogen receptor (in fmol).



time: recurrence free survival time (in days).


cens: censoring indicator (0- censored, 1- event).

References


W. Sauerbrei and P. Royston (1999). Building multivariable prognostic and diagnostic models: transformation of the predictors by using fractional polynomials. Journal of the Royal Statistics Society Series A, Volume 162(1), 71–94
M. Schumacher, G. Basert, H. Bojar, K. Huebner, M. Olschewski, W. Sauerbrei, C. Schmoor, C. Beyerle, R.L.A. Neumann and H.F. Rauschecker for the German Breast Cancer Study Group (1994), Randomized 2 × 2 trial evaluating hormonal treatment and the duration of chemotherapy in node- positive breast cancer patients. Journal of Clinical Oncology, 12, 2086–2093",1999,,,,,
1250,GBUSV,Unsupervised Pre-training,Unsupervised Pre-training,Unsupervised Pre-training,,,Methodology,,,https://gbc-iitd.github.io/data/gbusv,https://paperswithcode.com/dataset/gbusv,"Description
GBUSV is a un-annotated dataset consisting of ultrasound videos of of patients with either of a malignant or a non-malignant gallbladder. The ultrasound videos were obtained from patients referred to the radiology department of PGIMER, Chandigarh (a high-input hospital in Northern India) for abdominal ultrasound examinations of suspected gallbladder pathologies. Patients were at fasting of at least 6 hours. A 1-5 MHz curved array transducer (C-1-5D, Logiq S8, GE Healthcare) was used. The scanning intended to include the entire gallbladder and the lesion or pathology. The length of the video sequences varies from 43 to 888 frames. The dataset consists of 32 malignant and 32 non-malignant videos containing a total of 12,251 and 3,549 frames, respectively. The video frames are cropped from the center to anonymize the patient information and annotations. The processed frame sizes are of size 360x480 pixels.

Annotations
The images of the video sequences are un-annotated and suitable for unsupervised learning tasks. We provide the high-level categorization for each video whether it's malignant or non-malignant.",,,,,,
1251,GD-VCR,Visual Commonsense Reasoning,Visual Commonsense Reasoning,Visual Commonsense Reasoning,Image,,Reasoning,visual-commonsense-reasoning-on-gd-vcr,,https://github.com/WadeYin9712/GD-VCR,https://paperswithcode.com/dataset/gd-vcr,Geo-Diverse Visual Commonsense Reasoning (GD-VCR) is a new dataset to test vision-and-language models' ability to understand cultural and geo-location-specific commonsense.,,https://arxiv.org/pdf/2109.06860v1.pdf,https://arxiv.org/pdf/2109.06860v1.pdf,,,
1252,GDIT,Object Detection,Object Detection,"Object Detection, Remote Sensing Image Classification, Aerial Scene Classification",Image,,Computer Vision,,,https://universe.roboflow.com/gdit/aerial-airport,https://paperswithcode.com/dataset/gdit,"The GDIT Aerial Airport dataset consists of aerial images containing instances of parked airplanes. All plane types have been grouped into a single classification named ""airplane"".",,,,,,
1253,GDSC,Drug Response Prediction,Drug Response Prediction,Drug Response Prediction,Time Series,,Methodology,drug-response-prediction-on-gdsc,Custom,https://www.cancerrxgene.org/,https://paperswithcode.com/dataset/gdscv2,"We have characterized 1000 human cancer cell lines and screened them with 100s of compounds.
On this website, you will find drug response data and genomic markers of sensitivity.

The Genomics of Drug Sensitivity in Cancer Project - http://www.cancerrxgene.org/ - was part of a Wellcome Trust-funded collaboration between The Cancer Genome Project at the Wellcome Sanger Institute (UK) and the Center for Molecular Therapeutics, Massachusetts General Hospital Cancer Center (USA). This collaboration integrated the expertise at both sites toward the goal of identifying cancer biomarkers that can be used to identify genetically defined subsets of patients most likely to respond to cancer therapies.

We screened >1000 genetically characterized human cancer cell lines with a wide range of anti-cancer therapeutics. These compounds included cytotoxic chemotherapeutics as well as targeted therapeutics from commercial sources, academic collaborators, and the biotech and pharmaceutical industries.

The sensitivity patterns of the cell lines were correlated with extensive genomic and expression data to identify genetic features that are predictive of sensitivity. This large collection of cell lines enabled us to capture much of the genomic heterogeneity that underlies human cancer, and which appears to play a critical role in determining the variable response of patients to treatment with specific agents.",,,,,,
1254,GEM,Extreme Summarization,Extreme Summarization,Extreme Summarization,Text,English,Natural Language Processing,extreme-summarization-on-gem-xsum,,https://gem-benchmark.com/,https://paperswithcode.com/dataset/gem,"Generation, Evaluation, and Metrics (GEM) is a benchmark environment for Natural Language Generation with a focus on its Evaluation, both through human annotations and automated Metrics.

GEM aims to:


measure NLG progress across 13 datasets spanning many NLG tasks and languages.
provide an in-depth analysis of data and models presented via data statements and challenge sets.
develop standards for evaluation of generated text using both automated and human metrics.

It is our goal to regularly update GEM and to encourage toward more inclusive practices in dataset development by extending existing data or developing datasets for additional languages.",,Gehrmann et al,https://arxiv.org/pdf/2102.01672.pdf,,,
1255,GenAIPABench-Dataset,TruthfulQA,TruthfulQA,TruthfulQA,,,Methodology,,Creative Commons Attribution 4.0 International,https://github.com/Aamir7693/GenAIPABench,https://paperswithcode.com/dataset/genaipabench-dataset,"GenAIPABench is a specialized dataset designed to evaluate Generative AI-based Privacy Assistants (GenAIPAs). These assistants aim to simplify complex privacy policies and data protection regulations, making them more accessible and understandable to users. The dataset provides a comprehensive framework for assessing the performance of AI models in interpreting and explaining privacy-related documents.

Components of the Dataset:

Privacy Documents:

Privacy Policies: The dataset includes five privacy policies from various organizations or services. These policies are selected to represent a range of industries and complexity levels.
Data Protection Regulations: It also contains two major data protection regulations (such as the EU's GDPR and California's CCPA), providing a legal context for evaluation.
Question Corpus:

Privacy Policy Questions: Contains 32 questions related to the privacy policies. These questions address key topics like data collection practices, data sharing, user rights, data security, and retention policies.
Regulation Questions: Includes 6 questions about data protection regulations, focusing on compliance requirements, user rights under the law, and organizational obligations.
Question Variations: Each question comes with paraphrased versions and variations to test the AI's ability to handle different phrasings and nuances.
Annotated Answers:
Expert-Curated Responses: Each question is accompanied by meticulously crafted answers provided by privacy experts.
Cross-Verification: Answers are cross-verified for accuracy and completeness, ensuring they align precisely with the source documents.
Purpose and Objectives:

Benchmarking GenAIPAs: Provides a standardized dataset for evaluating and comparing the effectiveness of different AI-based privacy assistants.
Improving AI Understanding of Privacy: Helps identify strengths and weaknesses in AI models regarding comprehension of privacy policies and regulations.
Enhancing User Experience: Aims to improve how AI assistants communicate complex privacy information to users, making it more accessible and actionable.
Usage Scenarios:

Academic Research: Researchers can use the dataset to study how AI models interpret and summarize legal and policy documents.
AI Development: Developers can train and fine-tune AI models to better handle privacy-related queries.
Policy Analysis Tools: Organizations can leverage the dataset to create tools that help users understand and navigate privacy policies.
Key Features:

Diverse Content: Covers a range of privacy documents and questions to ensure a comprehensive evaluation.
Expert Validation: Responses are verified by privacy experts, ensuring high-quality benchmarks.
Robust Testing Framework: The evaluator tool allows systematic testing under different scenarios and prompts.
Focus on Real-world Applicability: Questions are derived from user inquiries, FAQs, and online forums to reflect genuine user concerns.
Benefits:

Enhances Trustworthiness: The dataset helps improve user trust in AI assistants by promoting accuracy and clarity.
Supports Regulatory Compliance: Helps organizations ensure their AI tools provide information consistent with legal requirements.
Facilitates Transparency: Encourages AI models to provide transparent and reference-backed responses.",,,,,,
1256,GeneCIS,Zero-shot Composed Person Retrieval,Zero-shot Composed Person Retrieval,"Zero-shot Composed Person Retrieval, Zero-Shot Composed Image Retrieval (ZS-CIR)",Image,,Computer Vision,zero-shot-composed-image-retrieval-zs-cir-on-11,,https://sgvaze.github.io/genecis,https://paperswithcode.com/dataset/genecis,"GeneCIS benchmark is designed for measuring models’ ability to adapt to a range of similarity conditions, which is zero-shot evaluation only.",,,,,,
1257,General-100,Image Reconstruction,Image Reconstruction,"Image Reconstruction, Image Super-Resolution","3D, Image",,Computer Vision,,,http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html,https://paperswithcode.com/dataset/general-100,The General-100 dataset is a dataset for image super-resolution. It contains 100 bmp format images with no compression) The size of the 100 images ranges from 710 x 704 (large) to 131 x 112 (small).,,,,100 images,,
1258,GenericsKB,Open Information Extraction,Open Information Extraction,"Open Information Extraction, Question Answering",Text,English,Natural Language Processing,,,https://allenai.org/data/genericskb,https://paperswithcode.com/dataset/genericskb,"The GenericsKB contains 3.4M+ generic sentences about the world, i.e., sentences expressing general truths such as ""Dogs bark,"" and ""Trees remove carbon dioxide from the atmosphere."" Generics are potentially useful as a knowledge source for AI systems requiring general world knowledge. The GenericsKB is the first large-scale resource containing naturally occurring generic sentences (as opposed to extracted or crowdsourced triples), and is rich in high-quality, general, semantically complete statements. Generics were primarily extracted from three large text sources, namely the Waterloo Corpus, selected parts of Simple Wikipedia, and the ARC Corpus. A filtered, high-quality subset is also available in GenericsKB-Best, containing 1,020,868 sentences.",,,,868 sentences,,
1259,GENIA,UIE,UIE,"UIE, Nested Named Entity Recognition, Dependency Parsing, Event Extraction, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,"nested-named-entity-recognition-on-genia, event-extraction-on-genia, event-extraction-on-genia-2013, dependency-parsing-on-genia-las, uie-on-genia, dependency-parsing-on-genia-uas, named-entity-recognition-on-genia",,http://bionlp.dbcls.jp/projects/bionlp-st-ge-2016/wiki,https://paperswithcode.com/dataset/genia,"The GENIA corpus is the primary collection of biomedical literature compiled and annotated within the scope of the GENIA project. The corpus was created to support the development and evaluation of information extraction and text mining systems for the domain of molecular biology.

The corpus contains 1,999 Medline abstracts, selected using a PubMed query for the three MeSH terms “human”, “blood cells”, and “transcription factors”. The corpus has been annotated with various levels of linguistic and semantic information.

The primary categories of annotation in the GENIA corpus and the corresponding subcorpora are:


Part-of-Speech annotation
Constituency (phrase structure) syntactic annotation
Term annotation
Event annotation
Relation annotation
Coreference annotation",,,,,,
1260,genius,Node Classification on Non-Homophilic (Heterophilic) Graphs,Node Classification on Non-Homophilic (Heterophilic) Graphs,"Node Classification on Non-Homophilic (Heterophilic) Graphs, Node Classification",Image,,Graphs,"node-classification-on-genius, node-classification-on-non-homophilic-14",,,https://paperswithcode.com/dataset/genius,node classification on genius,,,,,,
1261,Genre2Movies,Complex Query Answering,Complex Query Answering,"Complex Query Answering, Collaborative Filtering, Movie Recommendation",,,Methodology,,Creative Commons Attribution 4.0 International,https://github.com/google-research-datasets/genre2movies,https://paperswithcode.com/dataset/genre2movies,"Genre annotations for movies
The file genre2movies.csv contains genre-movie tuples based on Wikidata annotations (https://www.wikidata.org/).

Data
Each line in genre2movies.csv represents one genre-movie tuple.
The first entry is the genre.
The second entry of each line is the movie name.
There are 83,670 genre-movie tuples.
Joining with the Movielens 20M dataset

The movies considered are from the Movielens 20M corpus: https://grouplens.org/datasets/movielens/20m/
The movie names in genre2movies.csv match the movie 'titles' in Movielens 20M.

Compositions
The directory ""compositions"" contains movies assigned to compositions of genres. The compositions are of the form: ""genre A and genre B"", ""genre A and not genre B"", ""genre A and genre B and genre C"", ""genre A and genre B and not genre C"". These assignments have been automatically generated from genre2movies.csv. We try to generate genre-compositions that are useful, e.g., for a ""genre A and genre B"" composition we ensure that genre B is not a subgenre of genre A, because an interesection of a superset with a subset is identical to the subset and does not form a new concept.",,,,,,
1262,GenSC-6G,Feature Upsampling,Feature Upsampling,"Feature Upsampling, object-detection, Image Segmentation, Classification, LLM-generated Text Detection","Image, Text",English,Computer Vision,,,https://huggingface.co/datasets/CQILAB/GenSC-6G,https://paperswithcode.com/dataset/gensc-6g,"Images for Classification, Segmentation, Object Detection, Upsampling, and Edge LLM
Feature Noise-Augmented Dataset for Semantic Communication

Kindly Check:
https://huggingface.co/datasets/CQILAB/GenSC-6G",,,,,,
1263,GenWiki,KG-to-Text Generation,KG-to-Text Generation,"KG-to-Text Generation, Unsupervised KG-to-Text Generation, Data-to-Text Generation",Text,English,Natural Language Processing,"unsupervised-kg-to-text-generation-on-genwiki-1, data-to-text-generation-on-genwiki, unsupervised-kg-to-text-generation-on-genwiki",,https://github.com/zhijing-jin/genwiki,https://paperswithcode.com/dataset/genwiki,"GenWiki is a large-scale dataset for knowledge graph-to-text (G2T) and text-to-knowledge graph (T2G) conversion. It is introduced in the paper ""GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation"" by Zhijing Jin, Qipeng Guo, Xipeng Qiu, and Zheng Zhang at COLING 2020.",2020,"""GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation""",https://www.aclweb.org/anthology/2020.coling-main.217.pdf,,,
1264,Geoclidean-Elements,Generalized Few-Shot Learning,Generalized Few-Shot Learning,Generalized Few-Shot Learning,,,Methodology,,,https://github.com/joyhsu0504/geoclidean_framework,https://paperswithcode.com/dataset/geoclidean-elements,"Geoclidean-Elements dataset is derived from definitions in the first book of Euclid’s Elements, which focuses on plane geometry. Geoclidean-Elements includes 17 target concepts and 34 tasks.",,Geoclidean: Few-Shot Generalization in Euclidean Geometry,https://arxiv.org/pdf/2211.16663v1.pdf,,,
1265,GeoCoV19,Misinformation,Misinformation,"Misinformation, Sentiment Analysis, Sentence Embeddings",Text,English,Natural Language Processing,,,https://crisisnlp.qcri.org/covid19,https://paperswithcode.com/dataset/geocov19,"GeoCoV19 is a large-scale Twitter dataset containing more than 524 million multilingual tweets. The dataset contains around 378K geotagged tweets and 5.4 million tweets with Place information. The annotations include toponyms from the user location field and tweet content and resolve them to geolocations such as country, state, or city level. In this case, 297 million tweets are annotated with geolocation using the user location field and 452 million tweets using tweet content.",,,,,,
1266,GeoDE,Photo geolocation estimation,Photo geolocation estimation,Photo geolocation estimation,,,Methodology,,CC BY 4.0 License,https://geodiverse-data-collection.cs.princeton.edu/,https://paperswithcode.com/dataset/geode,"GeoDE is a geographically diverse dataset with 61,940 images from 40 classes and 6 world regions, and no personally identifiable information, collected through crowd-sourcing.",,Beyond web-scraping: Crowd-sourcing a geographically diverse image dataset,https://arxiv.org/pdf/2301.02560v1.pdf,940 images,,40
1267,Geometry3K,Automated Theorem Proving,Automated Theorem Proving,"Automated Theorem Proving, Semantic Parsing, Mathematical Question Answering, Math Word Problem Solving, Question Answering",Text,English,Natural Language Processing,mathematical-question-answering-on-geometry3k,MIT,https://lupantech.github.io/inter-gps/,https://paperswithcode.com/dataset/geometry3k,"A new large-scale geometry problem-solving dataset
- 3,002 multi-choice geometry problems 
- dense annotations in formal language for the diagrams and text
- 27,213 annotated diagram logic forms (literals)
- 6,293 annotated text logic forms (literals)",,,,,,
1268,GeoQA,Question Answering,Question Answering,"Question Answering, Mathematical Question Answering, Mathematical Reasoning",Text,English,Natural Language Processing,mathematical-reasoning-on-geoqa,,https://github.com/chen-judge/GeoQA,https://paperswithcode.com/dataset/geoqa,"GeoQA is a dataset for automatic geometric problem solving containing 5,010 geometric problems with corresponding annotated programs, which illustrate the solving process of the given problems

Compared with another publicly available dataset GeoS, GeoQA is 25 times larger, in which the program annotations can provide a practical testbed for future research on explicit and explainable numerical reasoning.",,,,,,
1269,GePaDe,Speaker Attribution in German Parliamentary Debates (GermEval 2023,Speaker Attribution in German Parliamentary Debates (GermEval 2023,"Speaker Attribution in German Parliamentary Debates (GermEval 2023, subtask 2), Speaker Attribution in German Parliamentary Debates (GermEval 2023, subtask 1), Semantic Role Labeling",Audio,,Audio,"speaker-attribution-in-german-parliamentary, speaker-attribution-in-german-parliamentary-1",,https://github.com/umanlp/SpkAtt-2023/tree/master,https://paperswithcode.com/dataset/gepade,"This dataset encompasses 265 speeches (over 200,000 tokens) from the German Bundestag, primarily from the 19th legislative term (2017-2021), given by 195 distinct speakers representing 6 political parties.

The data was annotated to perform a semantic role labeling task, namely to identify who said what to whom (speaker attribution). Cues (triggers) were annotated that are associated with events of speech, writing, or thought. Additionally, the arguments (roles) of each trigger have been annotated, encompassing the SOURCE, ADDRESSEE, MESSAGE, MEDIUM, TOPIC, and EVIDENCE related to the speech event.

The dataset was introduced in the international GermEval 2023 Shared Task on Speaker Attribution in Newswire and Parliamentary Debates (SpkAtt-2023) to evaluate the quality of systems for automated identification of cues and associated roles.

Reference

Rehbein, I. et al, Overview of the GermEval 2023 Shared Task on Speaker Attribution in Newswire and Parliamentary Debates, https://github.com/umanlp/SpkAtt-2023/blob/master/doc/SpkAtt2023-proceedings.pdf",2017,,,,,
1270,GermanDPR,Passage Retrieval,Passage Retrieval,Passage Retrieval,,,Methodology,,,https://deepset.ai/germanquad,https://paperswithcode.com/dataset/germandpr,"GermanDPR is a dataset for passage retrieval in German. GermanDPR comprises 8,245 question/answer pairs in the training set, 1,030 pairs in the development set, and 1,025 pairs in the test set. For each pair, there are one positive context and three hard negative contexts.",,,,,,
1271,GerMS-AT,GermEval2024 Shared Task 1 Subtask 2,GermEval2024 Shared Task 1 Subtask 2,"GermEval2024 Shared Task 1 Subtask 2, GermEval2024 Shared Task 1 Subtask 1",,,Methodology,"germeval2024-shared-task-1-subtask-2-on-germs, germeval2024-shared-task-1-subtask-1-on-germs",,https://huggingface.co/datasets/ofai/GerMS-AT,https://paperswithcode.com/dataset/germs-at,"This dataset contains 7984 user comments from an Austrian online newspaper. The comments have been annotated by 4 or more out of 11 annotators as to how strong sexism/mysogyny is present in the comment. It was used in the GermEval 2024 Shared Task 1: GerMS-Detect to evaluate data-driven approaches to automatically detect sexism in user comments.

The dataset was introduced and described in Brigitte Krenn, Johann Petrak, Marina Kubina, and Christian Burger. 2024. Germs-at: A sexism/misogyny dataset of forum comments from an Austrian online newspaper. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 7728–7739.

The data can be downloaded here: https://huggingface.co/datasets/ofai/GerMS-AT",2024,,,,,
1272,GEval_for_KGRC-RDF-star,Knowledge Graph Embedding,Knowledge Graph Embedding,"Knowledge Graph Embedding, Knowledge Graph Embeddings",Graph,,Methodology,,Apache-2.0,https://github.com/aistairc/GEval-forKGRC-RDF-star,https://paperswithcode.com/dataset/geval-for-kgrc-rdf-star,"This repository is an extension of GEval.
This repository contains a (software) evaluation framework to perform evaluation and comparison on RDF-star graph embedding techniques.
The gold standard datasets for evaluation were created from KGRC-RDF-star. Please see here.",,,,,,
1273,GiantMIDI-Piano,Music Classification,Music Classification,"Music Classification, Information Retrieval, Music Information Retrieval","Audio, Image",,Computer Vision,,,https://github.com/bytedance/GiantMIDI-Piano,https://paperswithcode.com/dataset/giantmidi-piano,"GiantMIDI-Piano contains 10,854 unique piano solo pieces composed by 2,786 composers. GiantMIDI-Piano contains 34,504,873 transcribed notes, and contains metadata information of each music piece.",,,,,,
1274,Gibson_Environment,Hierarchical Reinforcement Learning,Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning, Robot Navigation, Self-Supervised Learning",,,Reinforcement Learning,,Custom,http://gibsonenv.stanford.edu/,https://paperswithcode.com/dataset/gibson-environment,Gibson is an opensource perceptual and physics simulator to explore active and real-world perception. The Gibson Environment is used for Real-World Perception Learning.,,,,,,
1275,GID,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, The Semantic Segmentation Of Remote Sensing Imagery, Segmentation Of Remote Sensing Imagery",Image,,Computer Vision,,Open Source,https://x-ytong.github.io/project/GID.html,https://paperswithcode.com/dataset/gid,"Gaofen Image Dataset (GID) is a large-scale land-cover dataset constructed with Gaofen-2 (GF-2) satellite images. This dataset has superiorities over the existing land-cover dataset because of its large coverage, wide distribution, and high spatial resolution. It contains 150 GF-2 images annotated at the pixel level for 5 categories: built-up, farmland, forest, meadow, and water.",,,,2 images,,5
1276,GigaST,Translation,Translation,"Translation, Machine Translation",Text,English,Natural Language Processing,,CC BY-NC 4.0,https://st-benchmark.github.io/resources/GigaST.html,https://paperswithcode.com/dataset/gigast,"GigaST is a large-scale pseudo speech translation (ST) corpus. The corpus was created by translating the text in GigaSpeech, an English ASR corpus, into German and Chinese. The training set is translated by a strong machine translation system and the test set was translated by human. ST models trained with an addition of the corpus obtain new state-of-the-art results on the MuST-C English-German benchmark test set.",,,,,,
1277,GitHub-Python,Program Repair,Program Repair,Program Repair,,,Methodology,program-repair-on-github-python,,https://github.com/michiyasunaga/bifi,https://paperswithcode.com/dataset/github-python,Repair AST parse (syntax) errors in Python code,,,,,,
1278,GitHub_Typo_Corpus,Grammatical Error Correction,Grammatical Error Correction,"Grammatical Error Correction, Spelling Correction",,,Methodology,,,https://github.com/mhagiwara/github-typo-corpus,https://paperswithcode.com/dataset/github-typo-corpus,"Are you the kind of person who makes a lot of typos when writing code? Or are you the one who fixes them by making ""fix typo"" commits? Either way, thank you—you contributed to the state-of-the-art in the NLP field.

GitHub Typo Corpus is a large-scale dataset of misspellings and grammatical errors along with their corrections harvested from GitHub. It contains more than 350k edits and 65M characters in more than 15 languages, making it the largest dataset of misspellings to date.",,,,,,
1279,GitTables-SemTab,Column Type Annotation,Column Type Annotation,Column Type Annotation,,,Methodology,"column-type-annotation-on-gittables-semtab, column-type-annotation-on-gittables-semtab-1",CC BY 4.0,https://zenodo.org/record/5706316,https://paperswithcode.com/dataset/gittables-semtab,"The GitTables-SemTab dataset is a subset of the GitTables dataset and was created to be used during the SemTab challenge. The dataset consists of 1101 tables and is used to benchmark the Column Type Annotation (CTA) task. 

Its columns were annotated using semantic properties from DBpedia and semantic types and properties from Schema.org. The table below shows the number of annotated columns and number of classes used to annotate this dataset.

|                | Columns | Classes |
|----------------|-------|-------|
| Column Type Annotation - Schema.org    |   721   |    59   |
| Column Type Annotation  - DBpedia | 2,533  |   122   |",,,,,,
1280,GitTables,Table annotation,Table annotation,"Table annotation, Column Type Annotation",Tabular,,Methodology,,CC BY 4.0,https://gittables.github.io,https://paperswithcode.com/dataset/gittables,"GitTables is a corpus of currently 1M relational tables extracted from CSV files in GitHub covering 96 topics. Table columns in GitTables have been annotated with more than 2K different semantic types from Schema.org and DBpedia. The column annotations consist of semantic types, hierarchical relations, range types, table domain and descriptions. 

The tables were annotated using two methods: the semantic method and syntactic one. This leads to two kinds of annotations which in the metadata of the tables are referred to as syntactic and semantic annotations. The first method annotated 888,678 tables with Schema.org semantic types and 875,630 with DBpedia, while the second method annotated 1,161,117 tables with Schema.org and 1,156,601 with DBpedia semantic types.

Some statistics about the tables are provided in the table below, ""Columns"" referring to the number of annotated columns and ""Classes"" to the number of unique DBpedia or Schema.org semantic types used for annotation.

|                      | Columns    | Classes |
|----------------------|------------|---------|
| Syntactic-DBpedia    | 3,441,251  | 834 |
| Syntactic-Schema.org | 2,671,588  | 677 |
| Semantic-DBpedia     | 10,757,184 | 2,380 |
| Semantic-Schema.org  | 10,475,155 | 2,407 |",,,,,,
1281,GLAMI-1M,Multilingual Image-Text Classification,Multilingual Image-Text Classification,"Multilingual Image-Text Classification, Text-to-Image Generation, Classification","Image, Text",English,Computer Vision,multi-lingual-image-text-classification-on,Apache-2.0,https://github.com/glami/glami-1m,https://paperswithcode.com/dataset/glami-1m,"We introduce GLAMI-1M: the largest multilingual image-text classification dataset and benchmark. The dataset contains images of fashion products with item descriptions, each in 1 of 13 languages. Categorization into 191 classes has high-quality annotations: all 100k images in the test set and 75% of the 1M training set were human-labeled. The paper presents baselines for image-text classification showing that the dataset presents a challenging fine-grained classification problem: The best scoring EmbraceNet model using both visual and textual features achieves 69.7% accuracy. Experiments with a modified Imagen model show the dataset is also suitable for image generation conditioned on text.",,,,100k images,,191
1282,GLARE,Multilingual NLP,Multilingual NLP,"Multilingual NLP, Hybrid Machine Learning, Information Retrieval",Text,English,Natural Language Processing,,This work is licensed under a CC BY 4.0 license,https://zenodo.org/records/13696090,https://paperswithcode.com/dataset/glare,"The Guided Lexrank algorithm is applied to dataset special_appeal.csv to summarize the texts of legal documents. The obtained summary and the texts of the topics contained in dataset themes.csv are submitted to the BM25 algorithm for similarity assessment. From a list of topics, the GLARE method produces a ranking with suggested topics for a given document.",,,,,,
1283,GlaS,Medical Image Segmentation,Medical Image Segmentation,Medical Image Segmentation,Image,,Medical,medical-image-segmentation-on-glas,"Custom (research-only, non-commercial, attribution)",https://warwick.ac.uk/fac/cross_fac/tia/data/glascontest/,https://paperswithcode.com/dataset/glas,"The dataset used in this challenge consists of 165 images derived from 16 H&E stained histological sections of stage T3 or T42 colorectal adenocarcinoma. Each section belongs to a different patient, and sections were processed in the laboratory on different occasions. Thus, the dataset exhibits high inter-subject variability in both stain distribution and tissue architecture. The digitization of these histological sections into whole-slide images (WSIs) was accomplished using a Zeiss MIRAX MIDI Slide Scanner with a pixel resolution of 0.465µm.",,Sirinukunwattana et al.,https://arxiv.org/pdf/1603.00275.pdf,165 images,,
1284,GLips,audio-visual learning,audio-visual learning,"audio-visual learning, Unconstrained Lip-synchronization, Talking Face Generation, Lip Reading, Visual Speech Recognition, Lipreading, Lip to Speech Synthesis","Audio, Image, Text",English,Audio,,CC-BY-NC-ND-4.0,https://www.fdr.uni-hamburg.de/record/10048,https://paperswithcode.com/dataset/glips,"The German Lipreading dataset consists of 250,000 publicly available videos of the faces of speakers of the Hessian Parliament, which was processed for word-level lip reading using an automatic pipeline. The format is similar to that of the English language Lip Reading in the Wild (LRW) dataset, with each H264-compressed MPEG-4 video encoding one word of interest in a context of 1.16 seconds duration, which yields compatibility for studying transfer learning between both datasets. Choosing video material based on naturally spoken language in a natural environment ensures more robust results for real-world applications than artificially generated datasets with as little noise as possible. The 500 different spoken words ranging between 4-18 characters in length each have 500 instances and separate MPEG-4 audio- and text metadata-files, originating from 1018 parliamentary sessions. Additionally, the complete TextGrid files containing the segmentation information of those sessions are also included. The size of the uncompressed dataset is 15GB.",,,,500 instances,,
1285,GlotSparse,Language Modelling,Language Modelling,"Language Modelling, Large Language Model, Language Identification",Text,English,Natural Language Processing,,CC0,https://huggingface.co/datasets/cis-lmu/GlotSparse,https://paperswithcode.com/dataset/glotsparse,Collection of news websites in low-resource languages.,,,,,,
1286,GLUE,Semantic Textual Similarity,Semantic Textual Similarity,"Semantic Textual Similarity, Semantic Textual Similarity within Bi-Encoder, QQP, Linguistic Acceptability, Natural Language Understanding, Natural Language Inference, Stochastic Optimization, Few-Shot Learning, Model Compression, Classification, Text Classification, Data-free Knowledge Distillation","Image, Text",English,Computer Vision,"text-classification-on-sst-2, few-shot-learning-on-mrpc, linguistic-acceptability-on-cola, stochastic-optimization-on-cola, model-compression-on-qnli, semantic-textual-similarity-on-mrpc, natural-language-understanding-on-glue, text-classification-on-glue-qqp, natural-language-inference-on-mrpc, text-classification-on-glue, data-free-knowledge-distillation-on-qnli, natural-language-inference-on-wnli, natural-language-inference-on-qnli, text-classification-on-glue-mrpc, few-shot-learning-on-glue-qqp, natural-language-inference-on-rte, qqp-on-qqp, semantic-textual-similarity-within-bi-encoder, natural-language-inference-on-glue, text-classification-on-glue-rte, classification-on-rte, text-classification-on-glue-cola, classification-on-sst-2, text-classification-on-glue-stsb, text-classification-on-glue-sst2",Custom,https://gluebenchmark.com/,https://paperswithcode.com/dataset/glue,"General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI.",,"Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models",https://arxiv.org/abs/1908.06725,,,
1287,GMEG-wiki,Grammatical Error Correction,Grammatical Error Correction,Grammatical Error Correction,,,Methodology,,,https://github.com/grammarly/GMEG,https://paperswithcode.com/dataset/gmeg-wiki,Grammatical error correction dataset for text from Wikipedia.,,,,,,
1288,GMEG-yahoo,Grammatical Error Correction,Grammatical Error Correction,Grammatical Error Correction,,,Methodology,,,https://github.com/grammarly/GMEG,https://paperswithcode.com/dataset/gmeg-yahoo,Grammatical error correction dataset for text from Yahoo! Answers,,,,,,
1289,GMOT-40,Object Detection,Object Detection,"Object Detection, Multiple Object Tracking, Multi-Object Tracking","Image, Video",,Computer Vision,"object-detection-on-gmot-40, multiple-object-tracking-on-gmot-40",,https://spritea.github.io/GMOT40/,https://paperswithcode.com/dataset/gmot-40,"GMOT-40 is the first public dense dataset for Generic Multiple Object Tracking (GMOT). It contains 40 carefully annotated sequences evenly distributed among 10 object categories. Beyond the data, a challenging protocal, one-shot GMOT, is adopted and a series of baseline algorithms is introduced. GMOT-40 is featured in

Dense objects. Over 80 objects of the same class could appear in one frame.
High-quality label. Manual annotation with careful inspection in each frame.
Diversity in target. Large variability between classes and within the sequences of the same class.
Real world challenges. Occlusion, target enter/exiting, motion blur, deformation and so on.
Challenging protocol. One-shot GMOT protocol is adopted for evaluation.
New baselines. A series of baseline methods for one-shot GMOT task is introduced.",,,,,,
1290,GMVD,Multiview Detection,Multiview Detection,Multiview Detection,Image,,Computer Vision,multiview-detection-on-gmvd,CC BY-NC 4.0,https://github.com/jeetv/GMVD_dataset,https://paperswithcode.com/dataset/gmvd,"The GMVD dataset consists of synthetic scenes captured using the GTA-V and Unity graphics engines. The dataset covers a variety of scenes, along with different conditions including day time variations (morning, afternoon, evening, night) and weather variations (sunny, cloudy, rainy, snowy). The purpose of the dataset is twofold. The first is to benchmark the generalization capabilities of Multi-View Detection algorithms. The second purpose is to serve as a synthetic training source from which the trained models can be directly applied on real-world data.",,,,,,
1291,GneutralSpeech_Female,Automatic Speech Recognition (ASR),Automatic Speech Recognition (ASR),"Automatic Speech Recognition (ASR), Text-To-Speech Synthesis, Voice Cloning, Speech Enhancement, Voice Conversion","Audio, Image, Text",English,Speech,,Custom,https://gpa-smt-ufrj.github.io/sbrt2023/intro.html,https://paperswithcode.com/dataset/gneutralspeech-female,"A Brazilian Portuguese TTS dataset featuring a female voice
recorded with high quality in a controlled environment, with
neutral emotion and more than 20 hours of recordings.
 with
neutral emotion and more than 20 hours of recordings. Our
dataset aims to facilitate transfer learning for researchers and
developers working on TTS applications: a highly professional
neutral female voice can serve as a good warm-up stage for
learning language-specific structures, pronunciation and other
non-individual characteristics of speech, leaving to further
training procedures only to learn the specific adaptations
needed (e.g. timbre, emotion and prosody). This can surely
help enabling the accommodation of a more diverse range
of female voices in Brazilian Portuguese. By doing so, we
also hope to contribute to the development of accessible and
high-quality TTS systems for several use cases such as virtual
assistants, audiobooks, language learning tools and accessibility solutions.

Possible  use cases: 
TTS;
Voice Conversion;
ASR;
Speech Enhancement",,,,,,
1292,GneutralSpeech_Male,Speech Synthesis,Speech Synthesis,"Speech Synthesis, Automatic Speech Recognition (ASR), Text-To-Speech Synthesis, Voice Cloning, Speech Enhancement, Voice Conversion","Audio, Image, Text",English,Speech,,Custom,https://www02.smt.ufrj.br/~gpa/propor2022/,https://paperswithcode.com/dataset/gneutralspeech-male,"A database containing high sampling rate recordings of a single speaker reading sentences in Brazilian
Portuguese with neutral voice, along with the corresponding text corpus.
Intended for speech synthesis and automatic speech recognition applications, the dataset contains text extracted from a popular Brazilian news
TV program, totalling roughly 20 h of audio spoken by a trained individual in a controlled environment. The text was normalized in the
recording process and special textual occurrences (e.g. acronyms, numbers, foreign names etc.) were replaced by their phonetic translation to
a readable text in Portuguese. There are no noticeable accidental sounds
and background noise has been kept to a minimum in all audio samples.

Intended for:
TTS
ASR
Speech Enhancement
Voice Conversion",,,,,,
1293,GOD,Brain Visual Reconstruction from fMRI,Brain Visual Reconstruction from fMRI,"Brain Visual Reconstruction from fMRI, Brain Visual Reconstruction, Brain Decoding","3D, Image",,Computer Vision,brain-visual-reconstruction-from-fmri-on-god,,https://www.nature.com/articles/ncomms15037,https://paperswithcode.com/dataset/god,"The Generic Object Decoding (GOD) Dataset is a specialized resource developed for fMRI-based decoding. It aggregates fMRI data gathered through the presentation of images from 200 representative object categories, originating from the 2011 fall release of ImageNet. The training session incorporated 1,200 images (8 per category from 150 distinct object categories). In contrast, the test session included 50 images (one from each of the 50 object categories). It is noteworthy that the categories in the test session were unique from those in the training session and were introduced in a randomized sequence across runs. On five subjects the fMRI scanning was conducted.",2011,,,200 images,"training session incorporated 1,200 images",
1294,GoodSounds,Dimensionality Reduction,Dimensionality Reduction,"Dimensionality Reduction, Information Retrieval, Music Information Retrieval",Audio,,Audio,,,https://www.upf.edu/web/mtg/good-sounds,https://paperswithcode.com/dataset/goodsounds,"GoodSounds dataset contains around 28 hours of recordings of single notes and scales played by 15 different professional musicians, all of them holding a music degree and having some expertise in teaching. 12 different instruments (flute, cello, clarinet, trumpet, violin, alto sax alto, tenor sax, baritone sax, soprano sax, oboe, piccolo and bass) were recorded using one or up to 4 different microphones. For all the instruments the whole set of playable semitones in the instrument is recorded several times with different tonal characteristics. Each note is recorded into a separate monophonic audio file of 48kHz and 32 bits. Rich annotations of the recordings are available, including details on recording environment and rating on tonal qualities of the sound (“good-sound”, “bad”, “scale-good”, “scale-bad”).",,,,,,
1295,Google,Sentence Compression,Sentence Compression,Sentence Compression,,,Methodology,sentence-compression-on-google-dataset,,,https://paperswithcode.com/dataset/google,,,,,,,
1296,Google_Refexp,Image Captioning,Image Captioning,"Image Captioning, Referring Expression Comprehension, Deep Attention, Referring Expression Segmentation, Natural Language Visual Grounding, Zero-Shot Region Description","Image, Text",English,Computer Vision,"referring-expression-segmentation-on-refcocog-1, zero-shot-region-description-on-refcocog-val, referring-expression-segmentation-on-refcocog, referring-expression-comprehension-on, zero-shot-region-description-on-refcocog-test, referring-expression-comprehension-on-1",CC BY 4.0,https://github.com/mjhucla/Google_Refexp_toolbox,https://paperswithcode.com/dataset/google-refexp,"A new large-scale dataset for referring expressions, based on MS-COCO.",,,,,,
1297,Google_Speech_Commands_-_Musan,Keyword Spotting,Keyword Spotting,"Keyword Spotting, Robust Speech Recognition, Speech Recognition","Audio, Image, Text",English,Computer Vision,speech-recognition-on-google-speech-commands,Creative Commons Attribution 4.0 International,https://zenodo.org/record/6066174#.Yn7NPJPMLyU,https://paperswithcode.com/dataset/google-speech-commands-musan,"This noisy speech test set is created from the Google Speech Commands v2 [1] and the Musan dataset[2]. 

It could be downloaded here: https://zenodo.org/record/6066174#.Yn7NPJPMLyU

Specifically, we created this test set by mixing the speech in the Google Speech Commands v2 test set with random noise in the Musan dataset at different signal to noise ratio -12.5,-10,0,10,20,30 and 40 decibel (dB). 

The Google Speech Commands v2 dataset is under the Creative Commons BY 4.0 license. It could be downloaded at: http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz

The Musan dataset is under Attribution 4.0 International (CC BY 4.0). It could be downlowned at https://www.openslr.org/17/

Citations:

[1] Pete Warden, “Speech commands: A dataset for limited-vocabulary speech recognition,” arXiv preprint arXiv:1804.03209, 2018.

[2] David Snyder, Guoguo Chen, and Daniel Povey, “Musan: A music, speech, and noise corpus,” arXiv preprint arXiv:1510.08484, 2015.",2018,,,,,
1298,GoPro,Deblurring,Deblurring,"Deblurring, Video Frame Interpolation, Image Deblurring, Unified Image Restoration","Image, Video",,Computer Vision,"unified-image-restoration-on-gopro, image-deblurring-on-gopro, deblurring-on-gopro, video-frame-interpolation-on-gopro",,https://seungjunnah.github.io/Datasets/gopro,https://paperswithcode.com/dataset/gopro,"The GoPro dataset for deblurring consists of 3,214 blurred images with the size of 1,280×720 that are divided into 2,103 training images and 1,111 test images. The dataset consists of pairs of a realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera.",,Down-Scaling with Learned Kernels in Multi-Scale Deep Neural Networksfor Non-Uniform Single Image Deblurring,https://arxiv.org/abs/1903.10157,,"training images and 1,111 test images",
1299,GOT-10k,Video Object Tracking,Video Object Tracking,"Video Object Tracking, Object Tracking, Visual Object Tracking","Image, Video",,Computer Vision,"visual-object-tracking-on-got-10k, video-object-tracking-on-got-10k-1",CC BY-NC-SA 4.0,http://got-10k.aitestunion.com/,https://paperswithcode.com/dataset/got-10k,"The GOT-10k dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labelled bounding boxes. The dataset contains more than 560 classes of real-world moving objects and 80+ classes of motion patterns.",,https://arxiv.org/pdf/1810.11981.pdf,https://arxiv.org/pdf/1810.11981.pdf,,,560
1300,GOTCHA,DeepFake Detection,DeepFake Detection,"DeepFake Detection, Human Detection of Deepfakes",Image,,Computer Vision,,,https://github.com/mittalgovind/GOTCHA-Deepfakes,https://paperswithcode.com/dataset/gotcha,"We release the dataset for non-commercial research. Submit requests <a href=""https://forms.gle/6WPEGNWbYoEe6bte8"" target=""_blank"">here</a>.

The dataset consists of three parts: Original Samples, Deepfakes, and the Fidelity Score Dataset.

The dataset is accessible via Box.com, which offers high availability worldwide and allows for access tracking.

Dataset Statistics:
Number of folder (or videos):
- Original : 675 (673k frames with 3 frame sizes each)
- Fake (DFL) : 26,085 (5.0M frames)
- Fake (Adaptive - DFL) : 7,215 (1.3M frames)
- Fake (FSGAN) : 25,433 (4.6M frames)
- Fake (LIA) : 25,662 (5.0M frames)

DFL quantized ready-to-use models : 60

Fidelity Score Dataset:
- Fake : 49,603 videos (32 frames each, 1.35M frames total)
- Real : 816 videos (32 frames each, 22k frames total)

Human evaluation dataset: 200 videos

Original Samples
While the paper focuses on 8 challenges, we collected 13 in total, which we share in full. The data is provided as frames extracted from raw HD videos recorded at 60 fps. We downsample the frames to approximately 10 fps for ease of use without any expected loss in generation fidelity. The images inside each zip file are face-only. We provide three convenient sizes: 224 x 224, 512 x 512, and 1024 x 1024 pixels.

Each zip file contains folders numbered according to the following challenges/tasks:


The camera moves around for about two minutes, capturing the face from different angles while the participant sits still.
The participant rotates their head from side to side, then looks up and down. Each side should be held for about 5 seconds, rotating as comfortably as possible, totaling around 25 seconds.
The participant covers their eyes with a hand, followed by covering the left half, the right half, and finally, the lower half of the face.
The participant puts on the provided sunglasses and then takes them off.
The participant wears the provided clear glasses, ensuring they reflect a lamp light shining. After setting up the reflection, recording starts. Finally, the participant removes the glasses.
The participant moves a green cloth in front of their face
The participant puts on a face mask and counts from 1 to 10 out loud. Then, they remove the facemask.
The participant presses a finger against a cheek.
The participant sticks out a (small) portion of their tongue.
The participant laughs for 10 seconds, then frowns, as if angry, for another 10 seconds.
The participant slowly stands up and then sits back down.
The participant counts from 1 to 50 with distinct breaks between each utterance.
The room light is dimmed, and a flashing light is kept on the face for 10 seconds.

TODO: Verify each challenge index above corresponds to that challenge in the zips.

Note: A few challenges are missing from a handful of participants. 

Fakes
Using 47 participants, we create deepfakes between all 2,209 (47 x 47) combinations (including self deepfake), using three deepfake generators — DeepFaceLab, FSGANv2, and LIA. 



DFL (DeepFaceLab): Notorious for generating hyper-realistic deepfakes, this pipeline serves as a baseline for in-the-wild deepfake videos. For our study, we trained individual DFL deepfake generators for each participant using their `no challenge' videos. These videos records the participants in a range of frontal angles while they sit naturally, aiming to mimic the kind of data readily accessible online for non-celebrity individuals. Training continued until convergence for approximately around 300,000 iterations. 

DFL (Adaptive Adversary): We utilized heavily trained celebrity deepfake generators provided by the community as an adaptive adversary. Such models are trained on upwards of 2M iterations.

The DFL models trained by us and the ones provided by the community  are available under dfm_models folder, including the hyperparameters in states.dat file. The models can be loaded directly into Deepfacelive to generate deepfakes.



FSGAN (Face Swapping Generative Adversarial Network): This corresponds to the second version of FSGAN. Similar to LIA, this model is also target-agnostic, hence we utilized a pre-trained model made available by the authors for our study. Access its release at https://github.com/YuvalNirkin/fsgan


LIA (Latent Image Animator): This pipeline is a facial reenactment method outlined in. Given its target-agnostic nature—meaning it does not require specific target data during inference—we employed a pre-trained model for our experiments. Access its release at https://github.com/wyhsirius/LIA

As a rule of thumb, An imposter outer face and target is the inner face, in case of faceswaps. 

While Box offers quick zipping of multiple files, we have placed the files into zips (<10 GB each), for convenient access. Each zip under fakes (except Adaptive Adversary) is organized in path : “target/challenge/imposter/.jpg”. For Adaptive Adversary, the order is opposite, i.e., “imposter/challenge/target/.jpg”

Fidelity Score Dataset (or a Snapshot of the full dataset)
The training subset contains the following subselections:


Original and deepfake videos with four challenges, one from each category and ‘no challenge’.
32 frames per sample,
35 out of 47 target identities, and
Deepfake videos created using only DFL.

The validation dataset consists of the remaining 12 out of 47 target identities, while test one includes the celebrity deepfakes. The dataset is the smallest snapshot of the full dataset. The code for loading the dataset and training a fidelity score function can be accessed at https://github.com/mittalgovind/gotcha-deepfakes.

Human evaluation
We release the instruments we used for human evaluation at https://app.gorilla.sc/openmaterials/693684 (with active preview).",,,,,,
1301,Gowalla,point of interests,point of interests,"point of interests, Collaborative Filtering, Session-Based Recommendations, Sequential Recommendation, Recommendation Systems",,,Methodology,"sequential-recommendation-on-gowalla, collaborative-filtering-on-gowalla, session-based-recommendations-on-gowalla, point-of-interests-on-gowalla, recommendation-systems-on-gowalla",,https://snap.stanford.edu/data/loc-gowalla.html,https://paperswithcode.com/dataset/gowalla,"Gowalla is a location-based social networking website where users share their locations by checking-in. The friendship network is undirected and was collected using their public API, and consists of 196,591 nodes and 950,327 edges. We have collected a total of 6,442,890 check-ins of these users over the period of Feb. 2009 - Oct. 2010.",2009,,,,,
1302,GPR1200,Content-Based Image Retrieval,Content-Based Image Retrieval,"Content-Based Image Retrieval, Image Retrieval",Image,,Computer Vision,,Custom,https://github.com/Visual-Computing/GPR1200,https://paperswithcode.com/dataset/gpr1200,"Most publications that aim to optimize neural networks for CBIR, train and test their models on domain specific datasets. It is therefore unclear, if those networks can be used as a general-purpose image feature extractor. After analyzing popular image retrieval test sets we decided to manually curate GPR1200, an easy to use and accessible but challenging benchmark dataset with 1200 categories and 10 class examples. Classes and images were manually selected from six publicly available datasets of different image areas, ensuring high class diversity and clean class boundaries.

This dataset can therefore be used for benchmarking image descriptor systems on their generalizability.",,,,,"train and test their models on domain specific datasets. It is therefore unclear, if those networks can be used as a general-purpose image feature extractor. After analyzing popular image retrieval test sets we decided to manually curate GPR1200, an easy to use and accessible but challenging benchmark dataset with 1200 categories and 10 class examples",1200
1303,GQA-REX,Explanatory Visual Question Answering,Explanatory Visual Question Answering,Explanatory Visual Question Answering,"Image, Text",English,Computer Vision,explanatory-visual-question-answering-on-gqa,MIT,https://github.com/szzexpoi/rex,https://paperswithcode.com/dataset/gqa-rex,"A GQA-based dataset with 1,040,830 multi-modal explanations of visual reasoning processes.",,,,,,
1304,GQA,Scene Graph Generation,Scene Graph Generation,"Scene Graph Generation, Graph Question Answering, Visual Question Answering, Object Detection, Visual Question Answering (VQA)","Graph, Image, Text",English,Computer Vision,"visual-question-answering-on-gqa-1, visual-question-answering-on-gqa-test-std, visual-question-answering-on-gqa-test2019, object-detection-on-gqa, graph-question-answering-on-gqa, scene-graph-generation-on-gqa, visual-question-answering-on-gqa, visual-question-answering-on-gqa-test-dev",CC BY 4.0,https://cs.stanford.edu/people/dorarad/gqa/,https://paperswithcode.com/dataset/gqa,"The GQA dataset is a large-scale visual question answering dataset with real images from the Visual Genome dataset and balanced question-answer pairs. Each training and validation image is also associated with scene graph annotations describing the classes and attributes of those objects in the scene, and their pairwise relations. Along with the images and question-answer pairs, the GQA dataset provides two types of pre-extracted visual features for each image – convolutional grid features of size 7×7×2048 extracted from a ResNet-101 network trained on ImageNet, and object detection features of size Ndet×2048 (where Ndet is the number of detected objects in each image with a maximum of 100 per image) from a Faster R-CNN detector.",2048,Language-Conditioned Graph Networks for Relational Reasoning,https://arxiv.org/abs/1905.04405,,"training and validation image is also associated with scene graph annotations describing the classes and attributes of those objects in the scene, and their pairwise relations. Along with the images",
1305,GQNLI-FR,RTE,RTE,"RTE, Natural Language Inference, Sentence-Pair Classification","Image, Text",English,Computer Vision,,,https://github.com/mskandalis/gqnli-french,https://paperswithcode.com/dataset/gqnli-fr,"GQNLI-FR is a manually translated French version of the GQNLI challenge dataset, originally written in English.

GQNLI is an evaluation corpus that is aimed for testing language model's generalized quantifier reasoning ability. The dataset can be used for the task of Natural Language Inference (NLI), also known as Recognizing Textual Entailment (RTE), which is a sentence-pair classification task.",,,,,,
1306,GRAB,Grasp Generation,Grasp Generation,"Grasp Generation, Grasp Contact Prediction, Controllable Grasp Generation","Text, Time Series",English,Natural Language Processing,controllable-grasp-generation-on-grab,,https://github.com/otaheri/GRAB,https://paperswithcode.com/dataset/grab,"GRAB is a dataset of full-body motions interacting and grasping 3D objects. It contains accurate finger and facial motions as well as the contact between the objects and body. It contains 5 male and 5 female participants and 4 different motion intents.
The GRAB dataset also contains binary contact maps between the body and objects.",,,,,,
1307,GrailQA,Knowledge Base Question Answering,Knowledge Base Question Answering,Knowledge Base Question Answering,Text,English,Natural Language Processing,knowledge-base-question-answering-on-grailqa,,https://dki-lab.github.io/GrailQA/,https://paperswithcode.com/dataset/grailqa,"GrailQA is a new large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). It can be used to test three levels of generalization in KBQA: i.i.d., compositional, and zero-shot.",,,,,,
1308,Grasp-Anything,Robotic Grasping,Robotic Grasping,Robotic Grasping,,,Methodology,,CC BY-NC 4.0,https://airvlab.github.io/grasp-anything/,https://paperswithcode.com/dataset/grasp-anything,"We leverage knowledge from foundation models to introduce Grasp-Anything, a new large-scale dataset with 1M (one million) samples and 3M objects, substantially surpassing prior datasets in diversity and magnitude.  In addition, Grasp-Anything can universally cover objects in our daily lives and offer a great range of object diversity.",,,,,,
1309,GraSP,Surgical phase recognition,Surgical phase recognition,"Surgical phase recognition, Instance Segmentation, Action Localization","Image, Video",,Computer Vision,surgical-phase-recognition-on-grasp,,https://github.com/BCV-Uniandes/GraSP/tree/main,https://paperswithcode.com/dataset/grasp,"Holistic and Multi-Granular Surgical Scene Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that models surgical scene understanding as a hierarchy of complementary tasks with varying levels of granularity. Our approach enables a multi-level comprehension of surgical activities, encompassing long-term tasks such as surgical phases and steps recognition and short-term tasks including surgical instrument segmentation and atomic visual actions detection. To exploit our proposed benchmark, we introduce the Transformers for Actions, Phases, Steps, and Instrument Segmentation (TAPIS) model, a general architecture that combines a global video feature extractor with localized region proposals from an instrument segmentation model to tackle the multi-granularity of our benchmark. Through extensive experimentation, we demonstrate the impact of including segmentation annotations in short-term recognition tasks, highlight the varying granularity requirements of each task, and establish TAPIS's superiority over previously proposed baselines and conventional CNN-based models. Additionally, we validate the robustness of our method across multiple public benchmarks, confirming the reliability and applicability of our dataset. This work represents a significant step forward in Endoscopic Vision, offering a novel and comprehensive framework for future research towards a holistic understanding of surgical procedures.",,,,,,
1310,GraspNet-1Billion,Robotic Grasping,Robotic Grasping,Robotic Grasping,,,Methodology,robotic-grasping-on-graspnet-1billion,,https://graspnet.net/,https://paperswithcode.com/dataset/graspnet-1billion,"GraspNet-1Billion provides large-scale training data and a standard evaluation platform for the task of general robotic grasping. The dataset contains 97,280 RGB-D image with over one billion grasp poses.",,,,,,
1311,Grep-BiasIR,Bias Detection,Bias Detection,"Bias Detection, Gender Bias Detection, Information Retrieval",Image,,Computer Vision,,CC0-1.0 License,https://github.com/KlaraKrieg/GrepBiasIR,https://paperswithcode.com/dataset/grep-biasir,Grep-BiasIR is a novel thoroughly-audited dataset which aim to facilitate the studies of gender bias in the retrieved results of IR systems.,,,,,,
1312,GRID_Dataset,,,", Speech Separation, Lip Reading, Speech Enhancement, Speaker-Specific Lip to Speech Synthesis, Lipreading",Audio,,Audio,"speech-enhancement-on-grid-corpus-mixed, speaker-specific-lip-to-speech-synthesis-on, speech-separation-on-grid-corpus-mixed-speech, lip-reading-on-grid-corpus-mixed-speech, lipreading-on-grid-corpus-mixed-speech",,https://personal.ie.cuhk.edu.hk/~ccloy/downloads_qmul_underground_reid.html,https://paperswithcode.com/dataset/grid,"The QMUL underGround Re-IDentification (GRID) dataset contains 250 pedestrian image pairs. Each pair contains two images of the same individual seen from different camera views. All images are captured from 8 disjoint camera views installed in a busy underground station. The figures beside show a snapshot of each of the camera views of the station and sample images in the dataset. The dataset is challenging due to variations of pose, colours, lighting changes; as well as poor image quality caused by low spatial resolution.",,,,,,
1313,GRIT,Instance Segmentation,Instance Segmentation,"Instance Segmentation, Keypoint Detection, Referring Expression Comprehension, Surface Normals Estimation, Object Categorization, Referring Expression, Object Localization, Keypoint Estimation, Visual Question Answering, Surface Normal Estimation, Object Segmentation, Visual Question Answering (VQA)","Image, Text",English,Computer Vision,"object-segmentation-on-grit, surface-normal-estimation-on-grit, referring-expression-comprehension-on-grit, visual-question-answering-on-grit-1, keypoint-estimation-on-grit, object-localization-on-grit, object-categorization-on-grit, visual-question-answering-on-grit",Apache-2.0,https://grit-benchmark.org/,https://paperswithcode.com/dataset/grit,"The General Robust Image Task (GRIT) Benchmark is an evaluation-only benchmark for evaluating the performance and robustness of vision systems across multiple image prediction tasks, concepts, and data sources. GRIT hopes to encourage our research community to pursue the following research directions:


General purpose vision models - GRIT facilitates the evaluation of unified and general-purpose vision models that demonstrate a wide range of skills across a diverse set of concepts.
Robust specialized models - GRIT simplifies and unifies quantification of misinformation, calibration, and generalization under distribution shifts due to novel concepts, novel data sources or image distortions for 7 standard vision and vision-language tasks.
Efficient learning - GRIT includes a restricted and an unrestricted track. The restrictedtrack constrains the allowed training data to a selected but rich set of data sources that allows more scientific and meaningful comparison between models. This is meant to encourage resource constrained researchers to participate in the GRIT challenge and to spur interest in efficient learning methods as opposed to the dominant paradigm of training larger models on ever increasing amounts of training data. The unrestricted track allows much more flexibility in training data selection to test the capability of vision models trained with massive data and compute.",,,,,,
1314,Groningen_Meaning_Bank,AMR Parsing,AMR Parsing,"AMR Parsing, Semantic Parsing, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,,,https://gmb.let.rug.nl/data.php,https://paperswithcode.com/dataset/groningen-meaning-bank,"Groningen Meaning Bank is a semantic resource that anyone can edit and that integrates various semantic phenomena, including predicate-argument structure, scope, tense, thematic roles, animacy, pronouns, and rhetorical relations.",,The Groningen Meaning Bank,https://www.aclweb.org/anthology/W13-3802.pdf,,,
1315,Groove,Active Learning,Active Learning,"Active Learning, Music Generation, Beat Tracking, Quantization, Downbeat Tracking","Audio, Image, Text, Video",English,Computer Vision,"downbeat-tracking-on-groove, beat-tracking-on-groove",,https://magenta.tensorflow.org/datasets/groove,https://paperswithcode.com/dataset/groove-midi-dataset,"The Groove MIDI Dataset (GMD) is composed of 13.6 hours of aligned MIDI and (synthesized) audio of human-performed, tempo-aligned expressive drumming. The dataset contains 1,150 MIDI files and over 22,000 measures of drumming.",,,,,,
1316,GrosseWentrup2009_MOABB,Within-Session Motor Imagery (left hand vs. right hand),Within-Session Motor Imagery (left hand vs. right hand),Within-Session Motor Imagery (left hand vs. right hand),,,Methodology,within-session-motor-imagery-left-hand-vs-3,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.GrosseWentrup2009.html,https://paperswithcode.com/dataset/grossewentrup2009-moabb,,,,,,,
1317,GSCAN,Systematic Generalization,Systematic Generalization,"Systematic Generalization, Instruction Following",,,Methodology,,MIT,https://github.com/LauraRuis/groundedSCAN,https://paperswithcode.com/dataset/gscan,"Grounded SCAN poses a simple task, where an agent must execute action sequences based on a synthetic language instruction.

The agent is presented with a simple grid world containing a collection of objects, each of which is associated with a vector of features. The agent is evaluated on its ability to follow one or more instructions in this environment. Some instructions require interaction with particular kinds of objects.",,,,,,
1318,GSM-Plus,Arithmetic Reasoning,Arithmetic Reasoning,"Arithmetic Reasoning, Logical Reasoning, Math, Math Word Problem Solving",,,Reasoning,math-word-problem-solving-on-gsm-plus,,https://qtli.github.io/GSM-Plus/,https://paperswithcode.com/dataset/gsm-plus,"By perturbing the widely used GSM8K dataset, an adversarial dataset for grade-school math called GSM-Plus is created. Motivated by the capability taxonomy for solving math problems mentioned in Polya's principles, this paper identifies 5 perspectives to guide the development of GSM-Plus:


Numerical Variation refers to altering the numerical data or its types, including 3 subcategories: Numerical Substitution, Digit Expansion, and Integer-decimal-fraction Conversion.
Arithmetic Variation refers to reversing or introducing additional operations (e.g., addition, subtraction, multiplication, and division) to math problems, including 2 subcategories: Adding Operation and Reversing Operation.
Problem Understanding refers to rephrasing the text description of the math problems.
Distractor Insertion refers to inserting topic-related but useless sentences to the problems.
Critical Thinking focuses on question or doubt ability when the question lacks necessary statements.

GSM-Plus can be used to evaluate the robustness of current LLMs in mathematical reasoning.",,,,,,
1319,GSM8K,Text Generation,Text Generation,"Text Generation, GSM8K, Arithmetic Reasoning",Text,English,Natural Language Processing,"text-generation-on-gsm8k-tr, text-generation-on-gsm8k-5-shot, gsm8k-on-gsm8k, arithmetic-reasoning-on-gsm8k",,https://github.com/openai/grade-school-math,https://paperswithcode.com/dataset/gsm8k,"GSM8K is a dataset of 8.5K high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7.5K training problems and 1K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ − ×÷) to reach the final answer. A bright middle school student should be able to solve every problem. It can be used for multi-step mathematical reasoning.",,https://arxiv.org/pdf/2110.14168v1.pdf,https://arxiv.org/pdf/2110.14168v1.pdf,,,
1320,GSO,Single-View 3D Reconstruction,Single-View 3D Reconstruction,Single-View 3D Reconstruction,3D,,Methodology,single-view-3d-reconstruction-on-gso,CC-BY 4.0 License,https://goo.gle/scanned-objects,https://paperswithcode.com/dataset/google-scanned-objects,Scanned Objects by Google Research is a dataset of common household objects that have been 3D scanned for use in robotic simulation and synthetic perception research.,,,,,,
1321,GSV-Cities,Visual Place Recognition,Visual Place Recognition,Visual Place Recognition,Image,,Computer Vision,,,https://www.kaggle.com/datasets/amaralibey/gsv-cities,https://paperswithcode.com/dataset/gsv-cities,"GSV-Cities is a large-scale dataset for training deep neural network for the task of Visual Place Recognition.

The dataset contains more than 530k images:


There are more than 62k different places, spread across multiple cities around the globe.
Each place is depited by at least 4 images (up to 20 images).
All places are physically distant (at least 100 meters between any pair of places).",,,,530k images,,
1322,GTA-IM_Dataset,Human motion prediction,Human motion prediction,"Human motion prediction, Trajectory Prediction, motion prediction, Human Pose Forecasting","3D, Image, Time Series, Video",,Computer Vision,"human-pose-forecasting-on-gta-im-dataset, trajectory-prediction-on-gta-im-dataset",,https://github.com/ZheC/GTA-IM-Dataset,https://paperswithcode.com/dataset/gta-im-dataset,"The GTA Indoor Motion dataset (GTA-IM) that emphasizes human-scene interactions in the indoor environments. It consists of HD RGB-D image sequences of 3D human motion from a realistic game engine. The dataset has clean 3D human pose and camera pose annotations, and large diversity in human appearances, indoor environments, camera views, and human activities.",,,,,,
1323,GTA5,Synthetic-to-Real Translation,Synthetic-to-Real Translation,"Synthetic-to-Real Translation, Image-to-Image Translation, Semantic Segmentation, Unsupervised Domain Adaptation, One-shot Unsupervised Domain Adaptation, Source-Free Domain Adaptation, Domain Adaptation","Image, Text",English,Computer Vision,"synthetic-to-real-translation-on-gtav-to, image-to-image-translation-on-gtav-to, source-free-domain-adaptation-on-gta5-to, domain-adaptation-on-gta5-synscapes-to, domain-adaptation-on-gta5-to-cityscapes, unsupervised-domain-adaptation-on-gtav-to, one-shot-unsupervised-domain-adaptation-on, semantic-segmentation-on-gtav-to-cityscapes-1",Research and educational use only,https://arxiv.org/pdf/1608.02192v1.pdf,https://paperswithcode.com/dataset/gta5,The GTA5 dataset contains 24966 synthetic images with pixel level semantic annotation. The images have been rendered using the open-world video game Grand Theft Auto 5 and are all from the car perspective in the streets of American-style virtual cities. There are 19 semantic classes which are compatible with the ones of Cityscapes dataset.,,Adversarial Learning and Self-Teaching Techniques for Domain Adaptation in Semantic Segmentation,https://arxiv.org/abs/1909.00781,,,
1324,GTEA,Fine-Grained Action Detection,Fine-Grained Action Detection,"Fine-Grained Action Detection, Weakly Supervised Action Localization, Action Segmentation","Image, Video",,Computer Vision,"weakly-supervised-action-localization-on-gtea, action-segmentation-on-gtea-1",,http://cbs.ic.gatech.edu/fpv/,https://paperswithcode.com/dataset/gtea,"The Georgia Tech Egocentric Activities (GTEA) dataset contains seven types of daily activities such as making sandwich, tea, or coffee. Each activity is performed by four different people, thus totally 28 videos. For each video, there are about 20 fine-grained action instances such as take bread, pour ketchup, in approximately one minute.",,TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation,https://arxiv.org/abs/1705.07818,,,
1325,GTSinger,Vocal technique classification,Vocal technique classification,"Vocal technique classification, Style Transfer, Singing Voice Synthesis","Audio, Image",,Computer Vision,,CC-BY-NC-SA 4.0,https://aaronz345.github.io/GTSingerDemo/,https://paperswithcode.com/dataset/gtsinger,"The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability. To tackle these problems, we present GTSinger, a large Global, multi-Technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality songs, forming the largest recorded singing dataset; (2) 20 professional singers across nine languages offer diverse timbres and styles; (3) we provide controlled comparison and phoneme-level annotations of six singing techniques, helping technique modeling and control; (4) GTSinger offers realistic music scores, assisting real-world musical composition; (5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired speech for various singing tasks. Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique recognition, style transfer, and speech-to-singing conversion.",,,,,,
1326,GTZAN,Beat Tracking,Beat Tracking,"Beat Tracking, Online Beat Tracking, Music Genre Classification, Downbeat Tracking, Multi-Source Unsupervised Domain Adaptation, Online Downbeat Tracking","Audio, Image, Video",,Computer Vision,"beat-tracking-on-gtzan, music-genre-classification-on-gtzan, downbeat-tracking-on-gtzan, online-beat-tracking-on-gtzan, online-downbeat-tracking-on-gtzan, multi-source-unsupervised-domain-adaptation-10",,http://marsyas.info/index.html,https://paperswithcode.com/dataset/gtzan,"The gtzan8 audio dataset contains 1000 tracks of 30 second length. There are 10 genres, each containing 100 tracks which are all 22050Hz Mono 16-bit audio files in .wav format. The genres are:


blues
classical
country
disco
hiphop
jazz
metal
pop
reggae
rock",,,,,,
1327,GUE,Promoter Detection,Promoter Detection,"Promoter Detection, Genome Understanding, Core Promoter Detection, Transcription Factor Binding Site Prediction (Mouse), Covid Variant Prediction, Epigenetic Marks Prediction, Transcription Factor Binding Site Prediction (Human), Splice Site Prediction","Image, Time Series",,Computer Vision,"epigenetic-marks-prediction-on-gue, promoter-detection-on-gue, transcription-factor-binding-site-prediction-1, covid-variant-prediction-on-gue, core-promoter-detection-on-gue, splice-site-prediction-on-gue, transcription-factor-binding-site-prediction",,https://github.com/Zhihan1996/DNABERT_2,https://paperswithcode.com/dataset/gue,"A collection of $28$ datasets across $7$ tasks constructed for genome language model evaluation. Contains seven tasks: promoter prediction. core promoter prediction, splice site prediction, covid variant classification, epigenetic marks prediction, and  transcription factor binding sites prediction on human and mouse.",,,,,,
1328,GuessWhat__,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Visual Dialog, Question Answering","Image, Text",English,Computer Vision,,,https://github.com/GuessWhatGame/guesswhat,https://paperswithcode.com/dataset/guesswhat,"GuessWhat?! is a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images.

GuessWhat?! is a cooperative two-player game in which
both players see the picture of a rich visual scene with several objects. One player – the oracle – is randomly assigned
an object (which could be a person) in the scene. This object is not known by the other player – the questioner –
whose goal it is to locate the hidden object. To do so, the
questioner can ask a series of yes-no questions which are
answered by the oracle.",,Vries et al,https://arxiv.org/pdf/1611.08481v2.pdf,66K images,,
1329,GUISS_dataset,Instance Segmentation,Instance Segmentation,"Instance Segmentation, Stereo Disparity Estimation, Semantic Segmentation, Stereo Matching, Stereo Depth Estimation","3D, Image",,Computer Vision,,,https://github.com/nasa-jpl/guiss,https://paperswithcode.com/dataset/guiss-dataset,"We provide all the expected data inputs to GUISS such as meshes, texture images, and blend files. Generated datasets used in our experiments along with the stereo depth estimations can be downloaded. We have defined seven dataset types: scene_reconstructions, texture_variation, gaea_texture_variation, generative_texture, terrain_variation, rocks, and generative_texture_snow. Each dataset type contains renderings with varying values of different parameters such as lighting angle, texture imgs, albedo, etc. Position each dataset type folder under data/dataset/.

Details and links at: https://github.com/nasa-jpl/guiss",,,,,,
1330,Guitar-TECHS,Music Generation,Music Generation,"Music Generation, Music Transcription, Music Information Retrieval","Audio, Text",English,Natural Language Processing,,CC BY 4.0 License,https://guitar-techs.github.io,https://paperswithcode.com/dataset/guitar-techs,"Guitar-TECHS is a comprehensive dataset featuring a variety of guitar techniques, musical excerpts, chords, and scales. These elements are performed by diverse musicians across various recording settings. Guitar-TECHS incorporates recordings from two stereo microphones: an egocentric microphone positioned on the performer’s head and an exocentric microphone placed in front of the performer. It also includes direct input recordings and microphoned amplifier outputs, offering a wide spectrum of audio inputs and recording qualities. All signals and MIDI labels are properly synchronized. Its multi-perspective and multi-modal content makes Guitar-TECHS a valuable resource for advancing data-driven guitar research, and to develop robust guitar listening algorithms.",,,,,,
1331,GuitarSet,Beat Tracking,Beat Tracking,"Beat Tracking, Information Retrieval, Contrastive Learning, Downbeat Tracking, Music Information Retrieval","Audio, Image, Video",,Computer Vision,"beat-tracking-on-guitarset, downbeat-tracking-on-guitarset",,https://guitarset.weebly.com/,https://paperswithcode.com/dataset/guitarset,"GuitarSet is a dataset of high-quality guitar recordings and rich annotations. It contains 360 excerpts 30 seconds in length. The 360 excerpts are the result of the following combinations:


6 players,
2 versions: comping and soloing,
5 styles: Rock, Singer-Songwriter, Bossa Nova, Jazz, and Funk,
3 progressions: 12 Bar Blues, Autumn Leaves, and Pachelbel Canon,
2 tempi: slow and fast.

Each excerpt is annotated with 6 pitch contour and midi note annotations (one per string), 2 chord annotations (instructed and performed), beat and tempo annotations.",,GuitarSet: A Dataset for Guitar Transcription,https://guitarset.weebly.com/uploads/1/2/1/6/121620128/xi_ismir_2018.pdf,,,
1332,GUM,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Relation Classification, Named Entity Recognition, Timex normalization, Discourse Parsing, Nested Named Entity Recognition, Bridging Anaphora Resolution, Discourse Segmentation, Entity Linking, Part-Of-Speech Tagging, Entity Typing, Dependency Parsing, Nested Mention Recognition, Lemmatization, Named Entity Recognition (NER)","Audio, Graph, Image, Text",English,Computer Vision,"named-entity-recognition-on-gum, entity-linking-on-gum",CC-BY-NC-SA,https://gucorpling.org/gum/,https://paperswithcode.com/dataset/gum,"GUM is an open source multilayer English corpus of richly annotated texts from twelve text types. Annotations include:


Multiple POS tags, morphological features and lemmatization
Sentence segmentation and rough speech act
Document structure in TEI XML (paragraphs, headings, figures, etc.)
ISO date/time annotations
Speaker and addressee information (where relevant)
Constituent and dependency syntax
Information status (given, accessible, new, split antecedent)
Entity and coreference annotation, including bridging anaphora
Entity linking (Wikification)
Discourse parses in Rhetorical Structure Theory and discourse dependencies",,,,,,
1333,Gumar_Corpus,Text-To-Speech Synthesis,Text-To-Speech Synthesis,"Text-To-Speech Synthesis, Speech Synthesis","Audio, Text",English,Speech,,,https://nyuad.nyu.edu/en/research/faculty-labs-and-projects/computational-approaches-to-modeling-language-lab/research/gumar-corpus.html,https://paperswithcode.com/dataset/gumar-corpus,"A large-scale corpus of Gulf Arabic consisting of 110 million words from 1,200 forum novels.",,,,,,
1334,Gun_Detection_Dataset,EEG Signal Classification,EEG Signal Classification,"EEG Signal Classification, Novel View Synthesis, N-Queens Problem - All Possible Solutions, Object Detection, Reinforcement Learning (RL)",Image,,Computer Vision,"n-queens-problem-all-possible-solutions-on, reinforcement-learning-rl-on-1, eeg-signal-classification-on, novel-view-synthesis-on",Custom,https://www.linksprite.com/gun-detection-datasets/,https://paperswithcode.com/dataset/gun-detection-dataset,This is a gun detection dataset with 51K annotated gun images for gun detection and other 51K cropped gun chip images for gun classification collected from a few different sources.,,,,,,
1335,GVFC,News Annotation,News Annotation,"News Annotation, News Classification",Image,,Computer Vision,,,https://derrywijaya.github.io/GVFC,https://paperswithcode.com/dataset/gvfc,"This is a new dataset of news headlines and their frames related to the issue of gun violence in the United States. This Gun Violence Frame Corpus (GVFC) was curated and annotated by journalism and communication experts. The articles in this dataset are drawn from a sample of news articles from a list of 30 top U.S. news websites defined in terms of traffic to the websites; and collected from four time periods over the course of 2018 in order to capture a diversity of articles.

We include in this dataset, headlines of news articles and their annotations, the accompanying images and text- and image-derived features. We also include the codebook protocol, which includes all of the variables for annotations and their definitions that are applied by the annotators.",2018,,,,,
1336,GVGAI,Fairness,Fairness,"Fairness, Image Generation, Recommendation Systems","Image, Text",English,Computer Vision,,,http://www.gvgai.net/,https://paperswithcode.com/dataset/gvgai,"The General Video Game AI (GVGAI) framework is widely used in research which features a corpus of over 100 single-player games and 60 two-player games. These are fairly small games, each focusing on specific mechanics or skills the players should be able to demonstrate, including clones of classic arcade games such as Space Invaders, puzzle games like Sokoban, adventure games like Zelda or game-theory problems such as the Iterative Prisoners Dilemma. All games are real-time and require players to make decisions in only 40ms at every game tick, although not all games explicitly reward or require fast reactions; in fact, some of the best game-playing approaches add up the time in the beginning of the game to run Breadth-First Search in puzzle games in order to find an accurate solution. However, given the large variety of games (many of which are stochastic and difficult to predict accurately), scoring systems and termination conditions, all unknown to the players, highly-adaptive general methods are needed to tackle the diverse challenges proposed.",,Rolling Horizon Evolutionary Algorithms for General Video Game Playing,https://arxiv.org/abs/2003.12331,,,
1337,GVLM,Change Detection,Change Detection,"Change Detection, Landslide segmentation, Semi-supervised Change Detection, Change detection for remote sensing images",Image,,Computer Vision,,,https://github.com/zxk688/GVLM,https://paperswithcode.com/dataset/gvlm,"For change detection tasks, current open-source datasets mainly focus on building extraction (e.g., WHU building dataset and LEVIR-CD dataset) (Chen and Shi, 2020; Ji et al., 2018) and urban development monitoring (e.g., SECOND dataset, Google dataset and CDD dataset) (Yang et al., 2022; Peng et al., 2021; Lebedev et al., 2018), whereas datasets for natural disaster monitoring have been seldom investigated. 

Therefore, we sought to present the GVLM dataset, the first large-scale and open-source VHR landslide mapping dataset. It includes $17$ bitemporal very-high-resolution imagery pairs with a spatial resolution of $0.59$ m acquired via Google Earth service. Each sub-dataset contains a pair of bitemporal images and the corresponding ground-truth map. The total coverage of the dataset is $163.77 km2$. The landslide sites in different geographical locations have various sizes, shapes, occurrence times, spatial distributions, phenology states, and land cover types, resulting in considerable spectral heterogeneity and intensity variations in the remote sensing imagery. The GVLM dataset can be used to develop and evaluate machine/deep learning models for change detection, semantic segmentation and landslide extraction.",2020,,,,,
1338,GVLQA,Graph Classification,Graph Classification,"Graph Classification, Graph Regression, Graph Learning","Graph, Image",,Computer Vision,,MIT,https://huggingface.co/collections/Yanbin99/gvlqa-datasets-65c705c9488606617e246bd3,https://paperswithcode.com/dataset/gvlqa,"GVLQA is the first vision-language QA dataset for general graph reasoning. 
Contains a base set GVLQA-BASE and four image-augmented subsets GVLQA-AUGLY, GVLQA-AUGNO, GVLQA-AUGNS, GVLQA-AUGET, where the samples are relatively corresponding with the base set.
Contains 7 graph reasoning tasks: detecting cycle, connectivity, computing topological ordering, shortest path, maximum flow, bipartite matching num, and Hamilton path.
Utility: 
1) evaluate the graph reasoning capabilities of VLMs or LLMs; 
2) help models acquire fundamental graph comprehension and reasoning abilities as a pretraining dataset.",,,,,,
1339,GYAFC,Style Transfer,Style Transfer,"Style Transfer, Unsupervised Text Style Transfer, Formality Style Transfer",Text,English,Natural Language Processing,"formality-style-transfer-on-gyafc, unsupervised-text-style-transfer-on-gyafc, style-transfer-on-gyafc",Custom (research-only),https://github.com/raosudha89/GYAFC-corpus,https://paperswithcode.com/dataset/gyafc,"Grammarly’s Yahoo Answers Formality Corpus (GYAFC) is the largest dataset for any style containing a total of 110K informal / formal sentence pairs.

Yahoo Answers is a question answering forum, contains a large number of informal sentences and allows redistribution of data. The authors used the Yahoo Answers L6 corpus to create the GYAFC dataset of informal and formal sentence pairs. In order to ensure a uniform distribution of data, they removed sentences that are questions, contain URLs, and are shorter than 5 words or longer than 25. After these preprocessing steps, 40 million sentences remain. 

The Yahoo Answers corpus consists of several different domains like Business, Entertainment & Music, Travel, Food, etc. Pavlick and Tetreault formality classifier (PT16) shows that the formality level varies significantly
across different genres. In order to control for this variation, the authors work with two specific domains that contain the most informal sentences and show results on training and testing within those categories. The authors use the formality classifier from PT16 to identify informal sentences and train this classifier on the Answers genre of the PT16 corpus
which consists of nearly 5,000 randomly selected sentences from Yahoo Answers manually annotated on a scale of -3 (very informal) to 3 (very formal). They find that the domains of Entertainment & Music and Family & Relationships contain the most informal sentences and create the GYAFC dataset using these domains.",,"Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer",https://arxiv.org/pdf/1803.06535v2.pdf,,,
1340,H2O___2_Hands_and_Objects_,3D Hand Pose Estimation,3D Hand Pose Estimation,"3D Hand Pose Estimation, Skeleton Based Action Recognition, Action Recognition","3D, Image, Video",,Computer Vision,"action-recognition-on-h2o-2-hands-and-objects, skeleton-based-action-recognition-on-h2o-2",,https://taeinkwon.com/projects/h2o/,https://paperswithcode.com/dataset/h2o-dataset,"We present a comprehensive framework for egocentric interaction recognition using markerless 3D annotations of two hands manipulating objects. To this end, we propose a method to create a unified dataset for egocentric 3D interaction recognition. Our method produces annotations of the 3D pose of two hands and the 6D pose of the manipulated objects, along with their interaction labels for each frame. Our dataset, called H2O (2 Hands and Objects), provides synchronized multi-view RGB-D images, interaction labels, object classes, ground-truth 3D poses for left & right hands, 6D object poses, ground-truth camera poses, object meshes and scene point clouds. To the best of our knowledge, this is the first benchmark that enables the study of first-person actions with the use of the pose of both left and right hands manipulating objects and presents an unprecedented level of detail for egocentric 3D interaction recognition. We further propose the method to predict interaction classes by estimating the 3D pose of two hands and the 6D pose of the manipulated objects, jointly from RGB images. Our method models both inter- and intra-dependencies between both hands and objects by learning the topology of a graph convolutional network that predicts interactions. We show that our method facilitated by this dataset establishes a strong baseline for joint hand-object pose estimation and achieves state-of-the-art accuracy for first person interaction recognition.",,,,,,
1341,H3WB,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Monocular 3D Human Pose Estimation, 3D Hand Pose Estimation, Pose Estimation, 3D Facial Landmark Localization","3D, Image",,Computer Vision,"3d-hand-pose-estimation-on-h3wb, 3d-human-pose-estimation-on-h3wb, 3d-facial-landmark-localization-on-h3wb",MIT,https://github.com/wholebody3d/wholebody3d,https://paperswithcode.com/dataset/h3wb,"Human3.6M 3D WholeBody (H3WB) is a large scale dataset with 133 whole-body keypoint annotations on 100K images, made possible by a new multi-view pipeline. It is designed for the three new tasks : i) 3D whole-body pose lifting from 2D complete whole-body pose, ii) 3D whole-body pose lifting from 2D incomplete whole-body pose, iii) 3D whole-body pose estimation from a single RGB image.",,H3WB: Human3.6M 3D WholeBody Dataset and Benchmark,https://arxiv.org/pdf/2211.15692v1.pdf,100K images,,
1342,HabiCrowd,Social Navigation,Social Navigation,Social Navigation,,,Methodology,,Creative Commons Attribution 4.0 International,https://habicrowd.github.io/,https://paperswithcode.com/dataset/habicrowd,"HabiCrowd, a new dataset and benchmark for crowd-aware visual navigation that surpasses other benchmarks in terms of human diversity and computational utilization. HabiCrowd can be utilized to study crowd-aware visual navigation tasks. A notable feature of HabiCrowd is that our crowd-aware settings is 3D, which is scarcely studied by previous works.",,,,,,
1343,Habitat_Platform,PointGoal Navigation,PointGoal Navigation,"PointGoal Navigation, Robot Navigation, Question Answering",Text,English,Natural Language Processing,,,https://github.com/facebookresearch/habitat-api/tree/ec9557a3623991208a80f836fe557f8028209297,https://paperswithcode.com/dataset/habitat-platform,A platform for research in embodied artificial intelligence (AI).,,,,,,
1344,HACS,Temporal Action Localization,Temporal Action Localization,"Temporal Action Localization, Action Localization, Action Recognition","Image, Time Series, Video",,Computer Vision,"temporal-action-localization-on-hacs, action-recognition-on-hacs",BSD 3-Clause,http://hacs.csail.mit.edu/,https://paperswithcode.com/dataset/hacs,"HACS is a dataset for human action recognition. It uses a taxonomy of 200 action classes, which is identical to that of the ActivityNet-v1.3 dataset. It has 504K videos retrieved from YouTube. Each one is strictly shorter than 4 minutes, and the average length is 2.6 minutes. A total of 1.5M clips of 2-second duration are sparsely sampled by methods based on both uniform randomness and consensus/disagreement of image classifiers. 0.6M and 0.9M clips are annotated as positive and negative samples, respectively.

Authors split the collection into training, validation and testing sets of size 1.4M, 50K and 50K clips, which are sampled
from 492K, 6K and 6K videos, respectively.",,,,,,
1345,Hainsworth,Beat Tracking,Beat Tracking,"Beat Tracking, Downbeat Tracking","Image, Video",,Computer Vision,"beat-tracking-on-hainsworth, downbeat-tracking-on-hainsworth",,,https://paperswithcode.com/dataset/hainsworth,"S. W. Hainsworth and M. D. Macleod, “Particle filtering applied to musical tempo tracking,” EURASIP Journal on Advances in Signal Processing, vol. 2004, pp. 1–11, 2004",2004,,,,,
1346,HalluEditBench,Hallucination Evaluation,Hallucination Evaluation,"Hallucination Evaluation, knowledge editing",,,Methodology,,,https://github.com/llm-editing/hallu-edit,https://paperswithcode.com/dataset/hallueditbench,"HalluEditBench is a comprehensive benchmark for evaluating knowledge editing methods' effectiveness in correcting real-world hallucinations. HalluEdit features a rigorously constructed dataset spanning nine domains and 26 topics. It evaluates methods across five dimensions: Efficacy, Generalization, Portability, Locality, and Robustness.",,,,,,
1347,HAM,graph partitioning,graph partitioning,"graph partitioning, Metric Learning",Graph,,Methodology,,,https://github.com/rochesterxugroup/HAM_dataset,https://paperswithcode.com/dataset/ham,"HAM is a dataset for molecular graph partitioning. This dataset contains coarse-grained (CG) mappings of 1206 organic molecules with less than 25 heavy atoms. Each molecule was downloaded from the PubChem database as SMILES. One molecule was assigned to two annotators to compare the human agreement between CG mappings. Downloaded SMILES were hand-mapped. The completed annotations were reviewed by a third person, to identify and remove unreasonable mappings (eg: one bead mappings) which did not agree with the given guidelines. Hence, there are 1.68 annotations per molecule in the current database (16% removed).",,,,,,
1348,HAM10000,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Lesion Classification, Lesion Segmentation, Skin Lesion Classification",Image,,Computer Vision,"lesion-classification-on-ham10000, semantic-segmentation-on-ham10000, lesion-segmentation-on-ham10000",CC BY-NC,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T,https://paperswithcode.com/dataset/ham10000-1,"HAM10000 is a dataset of 10000 training images for detecting pigmented skin lesions. The authors collected dermatoscopic images from different populations, acquired and stored by different modalities.",,,,,,
1349,Hanabi_Learning_Environment,Game of Hanabi,Game of Hanabi,"Game of Hanabi, Multi-agent Reinforcement Learning, Policy Gradient Methods",,,Methodology,,,https://github.com/deepmind/hanabi-learning-environment,https://paperswithcode.com/dataset/hanabi-learning-environment,A new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information.,,,,,,
1350,HANNA,Story Generation,Story Generation,Story Generation,Text,English,Natural Language Processing,,MIT,https://github.com/dig-team/hanna-benchmark-asg,https://paperswithcode.com/dataset/hanna,"HANNA, a large annotated dataset of Human-ANnotated NArratives for Automatic Story Generation (ASG) evaluation, has been designed for the benchmarking of automatic metrics for ASG. HANNA contains 1,056 stories generated from 96 prompts from the WritingPrompts dataset. Each prompt is linked to a human story and to 10 stories generated by different ASG systems. Each story was annotated on six human criteria (Relevance, Coherence, Empathy, Surprise, Engagement and Complexity) by three raters. HANNA also contains the scores produced by 72 automatic metrics on each story.",,,,,,
1351,Hansel,Entity Linking,Entity Linking,"Entity Linking, Entity Disambiguation",,,Methodology,,CC-BY-SA,https://github.com/HITsz-TMG/Hansel,https://paperswithcode.com/dataset/hansel,"Hansel is a human-annotated Chinese entity linking (EL) dataset, focusing on tail entities and emerging entities:



The test set contains Few-shot (FS) and zero-shot (ZS) slices, has 10K examples and uses Wikidata as the corresponding knowledge base, useful for testing Chinese/multilingual EL systems' generalization ability to tail and emerging entities.



The training and validation sets are from Wikipedia hyperlinks, useful for large-scale pretraining of Chinese EL systems.",,,,10K examples,"test set contains Few-shot (FS) and zero-shot (ZS) slices, has 10K examples",
1352,HAR,Image Clustering,Image Clustering,"Image Clustering, Human Activity Recognition, Recognizing And Localizing Human Actions","Image, Video",,Computer Vision,"human-activity-recognition-on-har, image-clustering-on-har, recognizing-and-localizing-human-actions-on",Public domain,http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones,https://paperswithcode.com/dataset/har,"The Human Activity Recognition Dataset has been collected from 30 subjects performing six different activities (Walking, Walking Upstairs, Walking Downstairs, Sitting, Standing, Laying). It consists of inertial sensor data that was collected using a smartphone carried by the subjects.",,A Public Domain Dataset for Human Activity Recognition using Smartphones,http://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2013-84.pdf,,,
1353,Harm-C,Meme Classification,Meme Classification,"Meme Classification, Hateful Meme Classification",Image,,Computer Vision,hateful-meme-classification-on-harmeme,,https://github.com/lcs2-iiitd/momenta,https://paperswithcode.com/dataset/harm-c,Harm-C is a dataset for detecting harmful memes related to Covid-19.,,,,,,
1354,Harm-P,Hateful Meme Classification,Hateful Meme Classification,Hateful Meme Classification,Image,,Computer Vision,hateful-meme-classification-on-harm-p,,,https://paperswithcode.com/dataset/harm-p,"Harm-P contains around 3,000 memes related to US politics.",,,,,,
1355,HarMeme,Meme Classification,Meme Classification,"Meme Classification, Hateful Meme Classification",Image,,Computer Vision,hateful-meme-classification-on-harmeme,,https://github.com/di-dimitrov/harmeme,https://paperswithcode.com/dataset/harmeme,"HarMeme is a benchmark dataset for hateful meme classification containing 3, 544 memes related to COVID-19 collected from the Internet",,,,,,
1356,HarmfulTasks,Adversarial Text,Adversarial Text,Adversarial Text,Text,English,Adversarial,,MIT,https://github.com/CrystalEye42/eval-safety/blob/main/malicious_tasks_dataset.yaml,https://paperswithcode.com/dataset/harmfultasks,"This dataset consists of 225 malicious tasks, which were integrated into ten distinct jailbreaking prompts. The malicious tasks were divided into five categories, namely, 


Misinformation and Disinformation
Security Threats and Cybercrimes
Unlawful Behaviors and Activities 
Hate Speech and Discrimination 
Substance Abuse and Dangerous Practices.

The jailbreaking prompts were carefully selected to cover a diverse range of scenarios. These scenarios included role-playing, simulations, attention-shifting, and privileged execution, and the placement of the malicious task within the jailbreaking prompts was also varied.

List of malicious tasks only: https://github.com/CrystalEye42/eval-safety/blob/main/malicious_tasks_dataset.yaml

Malicious tasks with jailbreaking prompts: https://github.com/CrystalEye42/eval-safety/blob/main/integrated.yaml",,,,,,
1357,Harmonix,Beat Tracking,Beat Tracking,"Beat Tracking, Downbeat Tracking","Image, Video",,Computer Vision,"beat-tracking-on-harmonix, downbeat-tracking-on-harmonix",,https://github.com/urinieto/harmonixset,https://paperswithcode.com/dataset/harmonix,"Beats, downbeats, and functional structural annotations for 912 Pop tracks.

Nieto, O., McCallum, M., Davies., M., Robertson, A., Stark, A., Egozy, E., The Harmonix Set: Beats, Downbeats, and Functional Segment Annotations of Western Popular Music, Proc. of the 20th International Society for Music Information Retrieval Conference (ISMIR), Delft, The Netherlands, 2019",2019,,,,,
1358,HARPER,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, 3D Pose Estimation, Collision Avoidance, 2D Pose Estimation, Human Pose Forecasting, Pose Retrieval","3D, Image, Time Series",,Computer Vision,"human-pose-forecasting-on-harper, 3d-pose-estimation-on-harper, 2d-pose-estimation-on-harper",,https://github.com/intelligolabs/HARPER,https://paperswithcode.com/dataset/harper,"We introduce HARPER, a novel dataset for 3D body pose estimation and forecast in dyadic interactions between users and \spot, the quadruped robot manufactured by Boston Dynamics. The key-novelty is the focus on the robot's perspective, i.e., on the data captured by the robot's sensors. These make 3D body pose analysis challenging because being close to the ground captures humans only partially. The scenario underlying HARPER includes 15 actions, of which 10 involve physical contact between the robot and users. The Corpus contains not only the recordings of the built-in stereo cameras of Spot, but also those of a 6-camera OptiTrack system (all recordings are synchronized). This leads to ground-truth skeletal representations with a precision lower than a millimeter. In addition, the Corpus includes reproducible benchmarks on 3D Human Pose Estimation, Human Pose Forecasting, and Collision Prediction, all based on publicly available baseline approaches. This enables future HARPER users to rigorously compare their results with those we provide in this work.

Source: [Download dataset] (https://github.com/intelligolabs/HARPER)",,,,,,
1359,Harry_Potter_Dialogue_Dataset,Persona Dialogue in Story,Persona Dialogue in Story,"Persona Dialogue in Story, Dialogue Understanding, Dialogue Generation",Text,English,Natural Language Processing,"dialogue-generation-on-harry-potter-dialogue, pesona-dialogue-in-story-on-harry-potter",,https://github.com/nuochenpku/Harry-Potter-Dialogue-Dataset,https://paperswithcode.com/dataset/harry-potter-dialogue-dataset,"Harry Potter Dialogue is the first dialogue dataset that integrates with scene, attributes and relations which are dynamically changed as the storyline goes on. Our work can facilitate research to construct more human-like conversational systems in practice. For example, virtual assistant, NPC in games, etc. Moreover, HPD can both support dialogue generation and retrieval tasks.",,,,,,
1360,HarveyNER,NER,NER,"NER, Zero-shot Named Entity Recognition (NER)","Image, Text",English,Computer Vision,zero-shot-named-entity-recognition-ner-on-2,MIT,https://github.com/brickee/HarveyNER,https://paperswithcode.com/dataset/harveyner,fine-grained location names extraction from disaster-related tweets,,,,,,
1361,HASCD,Time Series,Time Series,"Time Series, Semantic Segmentation, Change Point Detection, Human Activity Recognition, Activity Recognition, Time Series Analysis","Image, Time Series, Video",,Time Series,,CC BY-NC-SA,https://github.com/patrickzib/human_activity_segmentation_challenge,https://paperswithcode.com/dataset/hascd,HASCD (Human Activity Segmentation Challenge Dataset) contains 250 annotated multivariate time series capturing 10.7 h of real-world human motion smartphone sensor data from 15 bachelor computer science students. The recordings capture 6 distinct human motion sequences designed to represent pervasive behaviour in realistic indoor and outdoor settings. The data set serves as a benchmark for evaluating machine learning workflows.,,,,,,
1362,hasPart_KB,Open Information Extraction,Open Information Extraction,Open Information Extraction,,,Methodology,,,https://allenai.org/data/haspartkb,https://paperswithcode.com/dataset/haspart-kb,"This dataset is a new knowledge-base (KB) of hasPart relationships, extracted from a large corpus of generic statements. Complementary to other resources available, it is the first which is all three of: accurate (90% precision), salient (covers relationships a person may mention), and has high coverage of common terms (approximated as within a 10 year old’s vocabulary), as well as having several times more hasPart entries than in the popular ontologies ConceptNet and WordNet. In addition, it contains information about quantifiers, argument modifiers, and links the entities to appropriate concepts in Wikipedia and WordNet.",,,,,,
1363,Hateful_Memes,Misinformation,Misinformation,"Misinformation, Image Captioning, Emotion Recognition, Image Clustering, Meme Classification, Hateful Meme Classification","Image, Text",English,Computer Vision,"meme-classification-on-hateful-memes, image-clustering-on-hateful-memes, hateful-meme-classification-on-hateful-memes-1",,https://ai.facebook.com/blog/hateful-memes-challenge-and-data-set/,https://paperswithcode.com/dataset/hateful-memes,"The Hateful Memes data set is a multimodal dataset for hateful meme detection (image + text) that contains 10,000+ new multimodal examples created by Facebook AI. Images were licensed from Getty Images so that researchers can use the data set to support their work.",,,,,,
1364,Hateful_Memes_Challenge,Emotion Recognition,Emotion Recognition,"Emotion Recognition, Image Captioning, Misinformation","Image, Text",English,Computer Vision,,,https://ai.facebook.com/hatefulmemes,https://paperswithcode.com/dataset/hateful-memes-challenge,"A new challenge set for multimodal classification, focusing on detecting hate speech in multimodal memes.",,,,,,
1365,HatEval,Hate Speech Detection,Hate Speech Detection,Hate Speech Detection,"Audio, Image",,Speech,hate-speech-detection-on-hateval,CC-BY-NC 4.0,https://competitions.codalab.org/competitions/19935,https://paperswithcode.com/dataset/hateval,"Hate Speech is commonly defined as any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics. Given the huge amount of user-generated contents on the Web, and in particular on social media, the problem of detecting, and therefore possibly limit the Hate Speech diffusion, is becoming fundamental, for instance for fighting against misogyny and xenophobia.

The proposed task consists in Hate Speech detection in Twitter but featured by two specific different targets, immigrants and women, in a multilingual perspective, for Spanish and English. The task will be articulated around two related subtasks for each of the involved languages: a basic task about Hate Speech, and another one where fine-grained features of hateful contents will be investigated in order to understand how existing approaches may deal with the identification of especially dangerous forms of hate, i.e. those where the incitement is against an individual rather than against a group of people, and where an aggressive behavior of the author can be identified as a prominent feature of the expression of hate. Participants will be asked to identify, on the one hand, if the target of hate is a single human or a group of persons, on the other hand, if the message author intends to be aggressive, harmful, or even to incite, in various forms, to violent acts against the target.


TASK A - Hate Speech Detection against Immigrants and Women: a two-class (or binary) classification where systems have to predict whether a tweet in English or in Spanish with a given target (women or immigrants) is hateful or not hateful.
TASK B - Aggressive behavior and Target Classification: where systems are asked first to classify hateful tweets for English and Spanish (e.g., tweets where Hate Speech against women or immigrants has been identified) as aggressive or not aggressive, and second to identify the target harassed as individual or generic (i.e. single human or group).",,,,,,
1366,HateXplain,Zero-Shot Text Classification,Zero-Shot Text Classification,"Zero-Shot Text Classification, Hate Speech Detection, Text Classification","Audio, Image, Text",English,Computer Vision,"hate-speech-detection-on-hatexplain, text-classification-on-hatexplain-1, zero-shot-text-classification-on-hatexplain",,https://github.com/punyajoy/HateXplain,https://paperswithcode.com/dataset/hatexplain,"Covers multiple aspects of the issue. Each post in the dataset is annotated from three different perspectives: the basic, commonly used 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based.",,,,,,
1367,Hate_Speech,Language Modelling,Language Modelling,"Language Modelling, Abusive Language, Hate Speech Detection, Text Classification","Audio, Image, Text",English,Computer Vision,text-classification-on-hate-speech18,CC BY-SA 3.0 ES,https://github.com/aitor-garcia-p/hate-speech-dataset,https://paperswithcode.com/dataset/hate-speech,"Dataset of hate speech annotated on Internet forum posts in English at sentence-level. The source forum in Stormfront, a large online community of white nacionalists. A total of 10,568 sentence have been been extracted from Stormfront and classified as conveying hate speech or not.",,https://arxiv.org/pdf/1809.04444.pdf,https://arxiv.org/pdf/1809.04444.pdf,,,
1368,Hate_Speech_and_Offensive_Language,Abusive Language,Abusive Language,"Abusive Language, Abuse Detection, Hate Speech Detection","Audio, Image, Text",English,Computer Vision,,,https://github.com/t-davidson/hate-speech-and-offensive-language,https://paperswithcode.com/dataset/hate-speech-and-offensive-language,"HSOL is a dataset for hate speech detection. The authors begun with a hate speech lexicon containing words and
phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API they searched
for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. They extracted
the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus they took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech.",,,,,,
1369,HaVG,Translation,Translation,"Translation, Multimodal Machine Translation, Machine Translation",Text,English,Natural Language Processing,,,,https://paperswithcode.com/dataset/havg,"A dataset that contains the description of an image or a section within the image in Hausa and its equivalent in English.  Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It is estimated that about 100 to 150 million people speak the language, with more than 80 million indigenous speakers.  The dataset comprises 32,923 images and their descriptions that are divided into training, development, test, and challenge test set. The Hausa Visual Genome is the first dataset of its kind and can be used for Hausa-English machine translation, multi-modal research, and image description, among various other natural language processing and generation tasks.",,,,923 images,"valent in English.  Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It is estimated that about 100 to 150 million people speak the language, with more than 80 million indigenous speakers.  The dataset comprises 32,923 images",
1370,Hawk_Annotation_Dataset,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Anomaly Classification, Video Anomaly Detection, Anomaly Forecasting","Image, Time Series, Video",,Computer Vision,,,https://github.com/jqtangust/hawk,https://paperswithcode.com/dataset/hawk-annotation-dataset,"Hawk Annotation Dataset includes language descriptions specifically for anomaly scenes in seven existing video anomaly datasets. These seven datasets include a variety of anomalous scenarios such as crime (UCF-Cirme), campus (ShanghaiTech and CUHK Avenue), pedestrian walkways (UCSD Ped1 and Ped2), traffic (DoTA), and human behavior (UBnormal). With the support of these visual scenarios, this dataset can perform comprehensive fine-tuning for various abnormal scenarios, being closer to open-world scenarios.",,,,,,
1371,HBW,Monocular 3D Human Pose Estimation,Monocular 3D Human Pose Estimation,"Monocular 3D Human Pose Estimation, 3D Human Reconstruction, 3D Human Shape Estimation","3D, Image",,Computer Vision,,,https://shapy.is.tue.mpg.de/datasets.html,https://paperswithcode.com/dataset/hbw,"Human Bodies in the Wild (HBW) is a validation and test set for body shape estimation. It consists of images taken in the wild and ground truth 3D body scans in SMPL-X topology. To create HBW, we collect body scans of 35 participants and register the SMPL-X model to the scans. Further each participant is photographed in various outfits and poses in front of a white background and uploads full-body photos of themselves taken in the wild. The validation and test set images are released. The ground truth shape is only released for the validation set.",,,,,"validation and test set for body shape estimation. It consists of images taken in the wild and ground truth 3D body scans in SMPL-X topology. To create HBW, we collect body scans of 35 participants and register the SMPL-X model to the scans. Further each participant is photographed in various outfits and poses in front of a white background and uploads full-body photos of themselves taken in the wild. The validation and test set images",
1372,HDD,Trajectory Prediction,Trajectory Prediction,"Trajectory Prediction, Object Localization, Autonomous Driving","Image, Time Series",,Computer Vision,,Custom,https://usa.honda-ri.com/hdd,https://paperswithcode.com/dataset/hdd,Honda Research Institute Driving Dataset (HDD) is a dataset to enable research on learning driver behavior in real-life environments. The dataset includes 104 hours of real human driving in the San Francisco Bay Area collected using an instrumented vehicle equipped with different sensors.,,Toward Driving Scene Understanding: A Dataset for Learning Driver Behavior and Causal Reasoning,https://arxiv.org/pdf/1811.02307v1.pdf,,,
1373,HEADSET,3D Facial Expression Recognition,3D Facial Expression Recognition,"3D Facial Expression Recognition, Occluded Face Detection, 3D Human Reconstruction, 3D Face Reconstruction, Facial Emotion Recognition","3D, Image",,Computer Vision,,Other (Non-Commercial),https://etsin.fairdata.fi/dataset/5b8c55b5-7e4d-492d-9f8a-df707bd56de2,https://paperswithcode.com/dataset/headset,"The volumetric representation of human interactions is one of the fundamental domains in the development of immersive media productions and telecommunication applications. Particularly in the context of the rapid advancement of Extended Reality (XR) applications, this volumetric data has proven to be an essential technology for future XR elaboration. In this work, we present a new multimodal database to help advance the development of immersive technologies. Our proposed database provides ethically compliant and diverse volumetric data, in particular 27 participants displaying posed facial expressions and subtle body movements while speaking, plus 11 participants wearing head-mounted displays (HMDs). The recording system consists of a volumetric capture (VoCap) studio, including 31 synchronized modules with 62 RGB cameras and 31 depth cameras. In addition to textured meshes, point clouds, and multi-view RGB-D data, we use one Lytro Illum camera for providing light field (LF) data simultaneously. Finally, we also provide an evaluation of our dataset employment with regard to the tasks of facial expression classification, HMDs removal, and point cloud reconstruction. The dataset can be helpful in the evaluation and performance testing of various XR algorithms, including but not limited to facial expression recognition and reconstruction, facial reenactment, and volumetric video. HEADSET and its all associated raw data and license agreement will be publicly available for research purposes.",,,,,,
1374,Healthcare_Provider_Fraud_Detection_Analysis,Fraud Detection,Fraud Detection,Fraud Detection,Image,,Computer Vision,fraud-detection-on-healthcare-provider-fraud,CC0: Public Domain,https://www.kaggle.com/datasets/rohitrox/healthcare-provider-fraud-detection-analysis,https://paperswithcode.com/dataset/healthcare-provider-fraud-detection-analysis,"Inpatient claims, Outpatient claims and Beneficiary details of each provider.

A) Inpatient Data

This data provides insights about the claims filed for those patients who are admitted in the hospitals. It also provides additional details like their admission and discharge dates and admit d diagnosis code.

B) Outpatient Data

This data provides details about the claims filed for those patients who visit hospitals and not admitted in it.

C) Beneficiary Details Data

This data contains beneficiary KYC details like health conditions,regioregion they belong to etc.",,,,,,
1375,HECKTOR,Tumor Segmentation,Tumor Segmentation,"Tumor Segmentation, Lesion Detection, Lesion Segmentation",Image,,Computer Vision,,,https://hecktor.grand-challenge.org/Overview/,https://paperswithcode.com/dataset/hecktor,Head and Neck Tumor Segmentation,,,,,,
1376,HeightCeleb,Speaker Profiling,Speaker Profiling,Speaker Profiling,Audio,,Audio,,CC BY 4.0,https://github.com/stachu86/HeightCeleb,https://paperswithcode.com/dataset/heightceleb,"Prediction of a speaker's height is of interest in fields such as voice forensics, surveillance, and automatic speaker profiling. HeightCeleb is an extension of Voxceleb that includes height information for all 1251 speakers. The height data was extracted automatically from publicly available sources. The purpose of this dataset is to enable the research community to leverage freely available speaker embedding extractors, pre-trained on VoxCeleb, to develop more accurate speaker height estimators.",,,,,,
1377,HellaSwag,Text Generation,Text Generation,"Text Generation, parameter-efficient fine-tuning, Sentence Completion, Question Answering",Text,English,Natural Language Processing,"text-generation-on-hellaswag-tr, question-answering-on-hellaswag, parameter-efficient-fine-tuning-on-hellaswag, sentence-completion-on-hellaswag, text-generation-on-hellaswag-10-shot",MIT,https://rowanzellers.com/hellaswag/,https://paperswithcode.com/dataset/hellaswag,"HellaSwag is a challenge dataset for evaluating commonsense NLI that is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy).",,,,,,
1378,HELOC,Tabular Data Generation,Tabular Data Generation,Tabular Data Generation,"Tabular, Text",English,Natural Language Processing,tabular-data-generation-on-heloc,,https://www.kaggle.com/datasets/averkiyoliabev/home-equity-line-of-creditheloc,https://paperswithcode.com/dataset/heloc,"HELOC
The HELOC dataset from FICO.
Each entry in the dataset is a line of credit, typically offered by a bank as a percentage of home equity (the difference between the current market value of a home and its purchase price).
The customers in this dataset have requested a credit line in the range of $5,000 - $150,000.
The fundamental task is to use the information about the applicant in their credit report to predict whether they will repay their HELOC account within 2 years.

Configurations and tasks
| Configuration | Task                  | Description                                                 |
|-------------------|---------------------------|-----------------------------------------------------------------|
| risk              | Binary classification     | Will the customer default?                                      |",,,,,,
1379,HELP,Visual Navigation,Visual Navigation,"Visual Navigation, Data Augmentation, Fairness, Natural Language Inference","Image, Text",English,Computer Vision,visual-navigation-on-help-anna-hanna-1,Custom,https://github.com/verypluming/HELP,https://paperswithcode.com/dataset/help,"The HELP dataset is an automatically created natural language inference (NLI) dataset that embodies the combination of lexical and logical inferences focusing on monotonicity (i.e., phrase replacement-based reasoning). The HELP (Ver.1.0) has 36K inference pairs consisting of upward monotone, downward monotone, non-monotone, conjunction, and disjunction.",,,,,,
1380,Helsinki_Prosody_Corpus,Prosody Prediction,Prosody Prediction,Prosody Prediction,Time Series,,Methodology,prosody-prediction-on-helsinki-prosody-corpus,,https://github.com/Helsinki-NLP/prosody,https://paperswithcode.com/dataset/helsinki-prosody-corpus,"The Helsinki Prosody Corpus is a dataset for predicting prosodic prominence from written text. The prosodic annotations are automatically generated, high quality prosodic for the 'clean' subsets of LibriTTS corpus (Zen et al., 2019), comprising of 262.5 hours of read speech from 1230 speakers. The transcribed sentences were aligned and then prosodically annotated with word-level acoustic prominence labels.",2019,Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations,https://arxiv.org/abs/1908.02262,,,
1381,Helvipad,Stereo Depth Estimation,Stereo Depth Estimation,"Stereo Depth Estimation, Omnnidirectional Stereo Depth Estimation, Stereo Matching",3D,,Methodology,omnnidirectional-stereo-depth-estimation-on,CC0,https://vita-epfl.github.io/Helvipad/,https://paperswithcode.com/dataset/helvipad,"The Helvipad dataset is a real-world stereo dataset designed for omnidirectional depth estimation. It comprises 39,553 paired equirectangular images captured using a top-bottom 360° camera setup and corresponding pixel-wise depth and disparity labels derived from LiDAR point clouds.  The dataset spans diverse indoor and outdoor scenes under varying lighting conditions, including night-time environments.",,,,,,
1382,HEMEW_S-3D,PDE Surrogate Modeling,PDE Surrogate Modeling,"PDE Surrogate Modeling, Time Series Regression",Time Series,,Methodology,,Custom,https://doi.org/10.57745/LAI6YU,https://paperswithcode.com/dataset/hemew-s-3d,"The HEterogeneous Materials and Elastic Waves with Source variability in 3D (HEMEWS-3D) database comprises 30,000 simulations of elastic wave propagation in 3D geological domains. Each domain is parametrized by a different geological model built from a random arrangement of layers augmented by random fields that represent heterogeneities. Elastic waves originate from a randomly located pointwise source parametrized by a random moment tensor. For each simulation, ground motion is synthesized at the surface by a grid of virtual sensors. The high frequency of waveforms ($f_{max}$ = 5 Hz) allows for extensive analyses of surface ground motion.

Existing and foreseen applications range from statistical analyses of the ground motion variability and machine learning methods on geological models to deep-learning-based predictions of ground motion that depend on 3D heterogeneous geologies and source properties.",,,,,,
1383,HengamCorpus,Named Entity Recognition (NER),Named Entity Recognition (NER),"Named Entity Recognition (NER), Temporal Tagging","Image, Text, Time Series, Video",English,Computer Vision,temporal-tagging-on-hengamcorpus,MIT,https://github.com/kargaranamir/Hengam,https://paperswithcode.com/dataset/hengamcorpus,"HengamCopus is a Persian corpus with temporal tags (BIO standard tagging scheme). This dataset was generated by applying HengamTagger (https://github.com/kargaranamir/parstdex) to a large number of sentences. There are two types of Persian text datasets included in these collections: formal ones (Persian Wikipedia and Hamshahri Corpus), and informal ones (Twitter and HelloKish). In the creation of HengamCorpus, to maximize the diversity of patterns for training and evaluation, they uniformly draw samples from sets of sentences of unique “temporal pattern profile”, presence/absence vector of different temporal patterns within the sentence.",,,,,"training and evaluation, they uniformly draw samples",
1384,HeriGraph,Graph Embedding,Graph Embedding,"Graph Embedding, Node Clustering, Dynamic Link Prediction, Link Prediction, Semi-Supervised Text Classification, Multi-Modal Document Classification, Node Classification, Semi-Supervised Image Classification","Graph, Image, Text, Time Series",English,Computer Vision,,MIT,https://github.com/zzbn12345/Heri_Graphs,https://paperswithcode.com/dataset/herigraph,"The dataset contains constructed multi-modal features (visual and textual), pseudo-labels (on heritage values and attributes), and graph structures (with temporal, social, and spatial links) constructed using User-Generated Content data collected from Flickr social media platform in three global cities containing UNESCO World Heritage property (Amsterdam, Suzhou, Venice).
The motivation of data collection in this project is to provide datasets that could be both directly applicable for ML communities as test-bed, and
theoretically informative for heritage and urban scholars to draw conclusions on for planning decision-making.",,,,,,
1385,Heroes_Corpus,Speech-to-Speech Translation,Speech-to-Speech Translation,Speech-to-Speech Translation,"Audio, Text",English,Speech,,CC BY-SA 4.0,http://hdl.handle.net/10230/35572,https://paperswithcode.com/dataset/heroes-corpus,"Each episode directory contains word-level and segment-level information of the whole episode and also parallel samples extracted under segments_eng and segments_spa subdirectories. Each sample is stored as an WAV audio file, text file and a CSV file containing word timing information and word-level paralinguistic and prosodic features.

This dataset contains short audio and text excerpts from the TV series ""Heroes"" (Copyright Universal Media Studios (2006-2007,2007-2008, 2008-2009)). It is compiled and used only for research purposes. Creation of this dataset is partially financed by the UPF DTIC-Maria de Maeztu Strategic Program. This dataset is created with automated tools. There might be errors due to the automated process.

Description from: https://repositori.upf.edu/handle/10230/35572",2006,,,,,
1386,HIDE,Deblurring,Deblurring,"Deblurring, Image Restoration, Image Super-Resolution, Image Deblurring",Image,,Computer Vision,"deblurring-on-hide, image-deblurring-on-hide, image-deblurring-on-hide-trained-on-gopro, deblurring-on-hide-trained-on-gopro",,https://github.com/joanshen0508/HA_deblur,https://paperswithcode.com/dataset/hide%0A,"Consists of 8,422 blurry and sharp image pairs with 65,784 densely annotated FG human bounding boxes.",,,,,,
1387,HiEve,Multiple Object Tracking,Multiple Object Tracking,"Multiple Object Tracking, Object Tracking, Temporal Action Localization, Multi-Object Tracking","Image, Time Series, Video",,Computer Vision,multi-object-tracking-on-hieve,,http://humaninevents.org/,https://paperswithcode.com/dataset/hieve,"A new large-scale dataset for understanding human motions, poses, and actions in a variety of realistic events, especially crowd & complex events. It contains a record number of poses (>1M), the largest number of action labels (>56k) for complex events, and one of the largest number of trajectories lasting for long terms (with average trajectory length >480). Besides, an online evaluation server is built for researchers to evaluate their approaches.",,,,,,
1388,HiFiMask,Face Anti-Spoofing,Face Anti-Spoofing,Face Anti-Spoofing,Image,,Computer Vision,,,https://sites.google.com/qq.com/face-anti-spoofing/dataset-download/casia-surf-hifimaskiccv2021,https://paperswithcode.com/dataset/hifimask,"HiFiMask is a large-scale High-Fidelity Mask dataset, namely CASIA-SURF HiFiMask (briefly HiFiMask). It contains a total amount of 54,600 videos are recorded from 75 subjects with 225 realistic masks by 7 new kinds of sensors.",,,,,,
1389,HIGGS_Data_Set,Two-sample testing,Two-sample testing,Two-sample testing,,,Methodology,two-sample-testing-on-higgs-data-set,,https://archive.ics.uci.edu/ml/datasets/HIGGS,https://paperswithcode.com/dataset/higgs-data-set,"The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper. The last 500,000 examples are used as a test set.",,,,000 examples,,
1390,highD_Dataset,Trajectory Clustering,Trajectory Clustering,"Trajectory Clustering, Trajectory Planning, Trajectory Prediction, Trajectory Modeling, Trajectory Forecasting",Time Series,,Methodology,,Non-Commercial,https://levelxdata.com/highd-dataset/,https://paperswithcode.com/dataset/highd-dataseth,"The highD dataset is a new dataset of naturalistic vehicle trajectories recorded on German highways. Using a drone, typical limitations of established traffic data collection methods such as occlusions are overcome by the aerial perspective. Traffic was recorded at six different locations and includes more than 110 500 vehicles. Each vehicle's trajectory, including vehicle type, size and manoeuvres, is automatically extracted. Using state-of-the-art computer vision algorithms, the positioning error is typically less than ten centimeters. Although the dataset was created for the safety validation of highly automated vehicles, it is also suitable for many other tasks such as the analysis of traffic patterns or the parameterization of driver models.",,,,,,
1391,Hilti_SLAM_Challenge,Simultaneous Localization and Mapping,Simultaneous Localization and Mapping,Simultaneous Localization and Mapping,Image,,Computer Vision,,,https://www.hilti-challenge.com,https://paperswithcode.com/dataset/hilti-slam-challenge,"Hilti SLAM Challenge is a dataset for Simultaneous Localization and Mapping (SLAM) algorithms due to sparsity, varying illumination conditions, and dynamic objects. The sensor platform used to collect this dataset contains a number of visual, lidar and inertial sensors which have all been rigorously calibrated. All data is temporally aligned to support precise multi-sensor fusion. Each dataset includes accurate ground truth to allow direct testing of SLAM results. Raw data as well as intrinsic and extrinsic sensor calibration data from twelve datasets in various environments is provided. Each environment represents common scenarios found in building construction sites in various stages of completion.",,,,,,
1392,HindEnCorp,Chunking,Chunking,"Chunking, Language Identification, Machine Translation",Text,English,Natural Language Processing,,CC BY-NC-SA 3.0,https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0023-625F-0,https://paperswithcode.com/dataset/hindencorp,"A parallel corpus of Hindi and English, and HindMonoCorp, a monolingual corpus of Hindi in their release version 0.5. Both corpora were collected from web sources and preprocessed primarily for the training of statistical machine translation systems. HindEnCorp consists of 274k parallel sentences (3.9 million Hindi and 3.8 million English tokens). HindMonoCorp amounts to 787 million tokens in 44 million sentences.",,,,,,
1393,Hindi_Visual_Genome,Multimodal Machine Translation,Multimodal Machine Translation,"Multimodal Machine Translation, Machine Translation",Text,English,Multimodal,,,https://ufal.mff.cuni.cz/hindi-visual-genome,https://paperswithcode.com/dataset/hindi-visual-genome,Hindi Visual Genome is a multimodal dataset consisting of text and images suitable for English-Hindi multimodal machine translation task and multimodal research.,,,,,,
1394,HiRID,Respiratory Failure,Respiratory Failure,"Respiratory Failure, Remaining Length of Stay, ICU Mortality, Patient Phenotyping, Kidney Function, Circulatory Failure",,,Methodology,"icu-mortality-on-hirid, kidney-function-on-hirid, circulatory-failure-on-hirid, remaining-length-of-stay-on-hirid, patient-phenotyping-on-hirid, respiratory-failure-on-hirid",PhysioNet Credentialed Health Data License 1.5.0,https://physionet.org/content/hirid/1.1.1/,https://paperswithcode.com/dataset/hirid,"HiRID is a freely accessible critical care dataset containing data relating to almost 34 thousand patient admissions to the Department of Intensive Care Medicine of the Bern University Hospital, Switzerland (ICU), an interdisciplinary 60-bed unit admitting >6,500 patients per year. The ICU offers the full range of modern interdisciplinary intensive care medicine for adult patients. The dataset was developed in cooperation between the Swiss Federal Institute of Technology (ETH) Zürich, Switzerland and the ICU.

The dataset contains de-identified demographic information and a total of 681 routinely collected physiological variables, diagnostic test results and treatment parameters from almost 34 thousand admissions during the period from January 2008 to June 2016. Data is stored with a uniquely high time resolution of one entry every two minutes.",2008,,,,,
1395,HistGen_WSI-Report_Dataset,Medical Report Generation,Medical Report Generation,Medical Report Generation,Text,English,Medical,medical-report-generation-on-histgen-wsi,Apache-2.0,https://github.com/dddavid4real/HistGen,https://paperswithcode.com/dataset/histgen-wsi-report-dataset,"This dataset is composed of 7,753 pairs of whole slide images and their corresponding diagnostic reports, extracted from the TCGA platform and refined with large language models. This dataset aims to boost the field of automated histopathology report generation by providing a new publicly available evaluation benchmark. See HistGen paper (see https://arxiv.org/pdf/2403.05396.pdf for reference) for a more detailed description of this dataset.",,,,,,
1396,HIV__Human_Immunodeficiency_Virus_,Graph Classification,Graph Classification,"Graph Classification, Drug Discovery, Molecular Property Prediction, 3D Volumetric Reconstruction","3D, Graph, Image, Time Series",,Computer Vision,"molecular-property-prediction-on-hiv-dataset, graph-classification-on-hiv-dataset, graph-classification-on-hiv, drug-discovery-on-hiv-dataset, molecular-property-prediction-on-hiv-1",,https://moleculenet.org/,https://paperswithcode.com/dataset/qm9-charge-densities-and-energies-calculated,"The HIV dataset was introduced by the Drug Therapeutics Program (DTP) AIDS Antiviral Screen, which tested the ability to inhibit HIV replication for over 40,000 compounds. Screening results were evaluated and placed into three categories: confirmed inactive (CI), confirmed active (CA), and confirmed moderately active (CM).

Check https://link.springer.com/chapter/10.1007/978-3-540-89689-0_33",,,,,,
1397,HiXSTest,Text Generation,Text Generation,"Text Generation, Language Modelling, Large Language Model, AI and Safety",Text,English,Natural Language Processing,,Apache-2.0,https://huggingface.co/datasets/walledai/HiXSTest,https://paperswithcode.com/dataset/hixstest,"For testing refusal behavior in a language-specific setting, we introduce HiXSTest — a set of manually curated prompts in the Hindi language designed to measure exaggerated safety. It comprises 25 safe-unsafe pairs of prompts, carefully phrased to challenge the LLMs’ safety boundaries.",,,,,,
1398,HJDataset,Object Detection,Object Detection,"Object Detection, Active Learning, Document Layout Analysis","Image, Text",English,Computer Vision,,,https://dell-research-harvard.github.io/HJDataset/,https://paperswithcode.com/dataset/hjdataset,"HJDataset is a large dataset of Historical Japanese Documents with Complex Layouts. It contains over 250,000 layout element annotations of seven types. In addition to bounding boxes and masks of the content regions, it also includes the hierarchical structures and reading orders for layout elements. The dataset is constructed using a combination of human and machine efforts.",,,,,,
1399,HJDB,Beat Tracking,Beat Tracking,"Beat Tracking, Downbeat Tracking","Image, Video",,Computer Vision,"downbeat-tracking-on-hjdb, beat-tracking-on-hjdb",,,https://paperswithcode.com/dataset/hjdb,"J. Hockman, M. E. Davies, and I. Fujinaga, “One in the jungle: Downbeat detection in hardcore, jungle, and drum and bass.” in Proceedings of the International Society for Music Information Retrieval Conference (ISMIR), 2012.",2012,,,,,
1400,HKR,Handwriting generation,Handwriting generation,"Handwriting generation, Handwritten Word Generation, Handwriting Recognition, Handwritten Text Recognition","Image, Text",English,Computer Vision,handwritten-text-recognition-on-hkr,CC-BY-NC-ND-4.0,https://github.com/abdoelsayed2016/HKR_Dataset,https://paperswithcode.com/dataset/hkr,"The database is written in Cyrillic and shares the same 33 characters. Besides these characters, the Kazakh alphabet also contains 9 additional specific characters. This dataset is a collection of forms. The sources of all the forms in the datasets were generated by LATEX which subsequently was filled out by persons with their handwriting. The database consists of more than 1400 filled forms. There are approximately 63000 sentences, more than 715699 symbols produced by approximately 200 diferent writers. We utilized three different datasets described as following:

Handwritten samples (Forms) of keywords in Kazakh and Russian (Areas, Cities , Village , etc.)
Handwritten Kazakh and Russian alphabet in cyrillic
Handwritten samples (Forms) of poems in Russian",,,,63000 sentences,,
1401,HKU-IS,Saliency Detection,Saliency Detection,"Saliency Detection, RGB Salient Object Detection, Salient Object Detection",Image,,Computer Vision,"salient-object-detection-on-hku-is, saliency-detection-on-hku-is, salient-object-detection-on-hku-is-1",,https://sites.google.com/site/ligb86/mdfsaliency/,https://paperswithcode.com/dataset/hku-is,"HKU-IS is a visual saliency prediction dataset which contains 4447 challenging images, most of which have either low contrast or multiple salient objects.",,Deep Contrast Learning for Salient Object Detection,https://arxiv.org/abs/1603.01976,,,
1402,HLA-Chat,Community Detection,Community Detection,"Community Detection, Language Modelling","Image, Text",English,Computer Vision,,,https://github.com/newpro/aloha-chatbot,https://paperswithcode.com/dataset/hla-chat,Models character profiles and gives dialogue agents the ability to learn characters' language styles through their HLAs.,,,,,,
1403,HLGD,News Annotation,News Annotation,"News Annotation, Text Classification, News Classification","Image, Text",English,Computer Vision,,Apache-2.0 License,https://github.com/tingofurro/headline_grouping,https://paperswithcode.com/dataset/hlgd,"The Headline Grouping dataset is a binary classification dataset on pairs of news headline.
For each pair of headline, the binary label indicates whether the two headlines are part of the same group (and describe the same underlying event), or whether they are in distinct groups.
The dataset contains a total of 20k annotated headline pairs, further split in a train, validation and test portions.",,,,,,
1404,HM3DSem,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Visual Navigation, 3D Semantic Segmentation","3D, Image",,Computer Vision,,,https://aihabitat.org/datasets/hm3d-semantics/,https://paperswithcode.com/dataset/hm3dsem,"The Habitat-Matterport 3D Semantics Dataset (HM3DSem) is the largest-ever dataset of 3D real-world and indoor spaces with densely annotated semantics that is available to the academic community. HM3DSem v0.2 consists of 142,646 object instance annotations across 216 3D-spaces from HM3D and 3,100 rooms within those spaces. The HM3D scenes are annotated with the 142,646 raw object names, which are mapped to 40 Matterport categories. On average, each scene in HM3DSem v0.2 consists of 661 objects from 106 categories. This dataset is the result of 14,200+ hours of human effort for annotation and verification by 20+ annotators.

HM3DSem v0.2 is free and available here for academic, non-commercial research. Researchers can use it with FAIR’s Habitat simulator to train embodied agents, such as home robots and AI assistants, at scale for semantic navigation tasks. HM3DSem v0.1 was also the basis of the recently concluded Habitat 2022 ObjectNav challenge. Please see our arxiv report for more details.",2022,,,,,106
1405,HMDB51,Skeleton Based Action Recognition,Skeleton Based Action Recognition,"Skeleton Based Action Recognition, Few Shot Action Recognition, Action Classification, Action Recognition In Videos, Self-Supervised Action Recognition, Self-Supervised Action Recognition Linear, Temporal Action Localization, Human Activity Recognition, Action Recognition, Self-supervised Video Retrieval, Zero-Shot Action Recognition","Image, Time Series, Video",,Computer Vision,"few-shot-action-recognition-on-hmdb51, action-recognition-in-videos-on-hmdb-51, skeleton-based-action-recognition-on-hmdb51, self-supervised-video-retrieval-on-hmdb51, action-recognition-in-videos-on-hmdb51, action-classification-on-hmdb51, self-supervised-action-recognition-linear-on-1, human-activity-recognition-on-hmdb51, self-supervised-action-recognition-on-hmdb51-1, action-recognition-in-videos-on-hmdb-51-1, zero-shot-action-recognition-on-hmdb51, self-supervised-action-recognition-on-hmdb51",CC BY 4.0,https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database,https://paperswithcode.com/dataset/hmdb51,"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as “jump”, “kiss” and “laugh”), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.",,Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors,https://arxiv.org/abs/1505.04868,,,
1406,HO-3D_v2,3D Hand Pose Estimation,3D Hand Pose Estimation,"3D Hand Pose Estimation, 3D Canonical Hand Pose Estimation, hand-object pose, 3D Pose Estimation","3D, Image",,Computer Vision,"3d-hand-pose-estimation-on-ho-3d, hand-object-pose-on-ho-3d",,https://www.tugraz.at/institute/icg/research/team-lepetit/research-projects/hand-object-3d-pose-annotation/,https://paperswithcode.com/dataset/ho-3d,"A hand-object interaction dataset with 3D pose annotations of hand and object. The dataset contains 66,034 training images and 11,524 test images from a total of 68 sequences. The sequences are captured in multi-camera and single-camera setups and contain 10 different subjects manipulating 10 different objects from YCB dataset. The annotations are automatically obtained using an optimization algorithm. The hand pose annotations for the test set are withheld and the accuracy of the algorithms on the test set can be evaluated with standard metrics using the CodaLab challenge submission(see project page). The object pose annotations for the test and train set are provided along with the dataset.",,,,,"training images and 11,524 test images",
1407,HOC,Document Classification,Document Classification,Document Classification,"Image, Text",English,Computer Vision,document-classification-on-hoc,,https://s-baker.net/resource/hoc/,https://paperswithcode.com/dataset/hoc-1,The Hallmarks of Cancer (*HOC) corpus consists of 1852 PubMed publication abstracts manually annotated by experts according to the Hallmarks of Cancer taxonomy. The taxonomy consists of 37 classes in a hierarchy. Zero or more class labels are assigned to each sentence in the corpus.,,,,,,37
1408,HOD,3D Reconstruction,3D Reconstruction,"3D Reconstruction, Face Swapping","3D, Image",,Computer Vision,face-swapping-on-hod,Creative Commons Attribution 4.0 International,https://dihuangdh.github.io/hhor/,https://paperswithcode.com/dataset/hod,"HOD is a dataset for 3D object reconstruction which contains 35 objects, divided into two subsets named Sculptures and Daily Objects. The Sculptures has five human sculptures with complex geometries and pure white textures. The Daily Objects consists of 30 daily objects with various shapes and appearances. All of the Sculptures and nine of the Daily Objects are paired with high-fidelity scanned meshes as ground truth geometries for evaluation.",,Reconstructing Hand-Held Objects from Monocular Video,https://arxiv.org/pdf/2211.16835v1.pdf,,,
1409,HOI4D,Motion Segmentation,Motion Segmentation,"Motion Segmentation, Semantic Segmentation, Pose Tracking, Panoptic Segmentation, Action Segmentation, Human-Object Interaction Detection","3D, Image, Video",,Computer Vision,,CC BY-NC 4.0,https://hoi4d.github.io/,https://paperswithcode.com/dataset/hoi4d,"A large-scale 4D egocentric dataset with rich annotations, to catalyze the research of category-level human-object interaction. HOI4D consists of 2.4M RGB-D egOCentric video frames over 4000 sequences collected by 4 participants interacting with 800 different object instances from 16 categories over 610 different indoor rooms.",,,,,,16
1410,HOList,Automated Theorem Proving,Automated Theorem Proving,Automated Theorem Proving,,,Methodology,automated-theorem-proving-on-holist-benchmark,,https://sites.google.com/view/holist/home,https://paperswithcode.com/dataset/holist,"The official HOList benchmark for automated theorem proving consists of all theorem statements in the core, complex, and flyspeck corpora. The goal of the benchmark is to prove as many theorems as possible in the HOList environment in the order they appear in the database. That is, only theorems that occur before the current theorem are supposed to be used as premises (lemmata) in its proof.",,,,,,
1411,Holopix50k,Monocular Depth Estimation,Monocular Depth Estimation,"Monocular Depth Estimation, Super-Resolution, Depth Estimation",3D,,Methodology,,Non-Commercial,https://github.com/leiainc/holopix50k,https://paperswithcode.com/dataset/holopix50k,"An in-the-wild stereo image dataset, comprising 49,368 image pairs contributed by users of the Holopix mobile social platform.",,,,,,
1412,HolStep,Dimensionality Reduction,Dimensionality Reduction,"Dimensionality Reduction, Mathematical Proofs, Automated Theorem Proving",,,Methodology,"automated-theorem-proving-on-holstep-1, automated-theorem-proving-on-holstep",BSD-3-Clause,http://cl-informatik.uibk.ac.at/cek/holstep/,https://paperswithcode.com/dataset/holstep,"HolStep is a dataset based on higher-order logic (HOL) proofs, for the purpose of developing new machine learning-based theorem-proving strategies.",,,,,,
1413,HomebrewedDB,6D Pose Estimation,6D Pose Estimation,6D Pose Estimation,"3D, Image",,Computer Vision,,CC0 1.0 Universal,http://campar.in.tum.de/personal/ilic/homebreweddb/index.html,https://paperswithcode.com/dataset/homebreweddb,"HomebrewedDB is a dataset for 6D pose estimation mainly targeting training from 3D models (both textured and textureless), scalability, occlusions, and changes in light conditions and object appearance. The dataset features 33 objects (17 toy, 8 household and 8 industry-relevant objects) over 13 scenes of various difficulty. It also consists of a set of benchmarks to test various desired detector properties, particularly focusing on scalability with respect to the number of objects and resistance to changing light conditions, occlusions and clutter.",,,,,,
1414,Home_Action_Genome,Activity Prediction,Activity Prediction,"Activity Prediction, Image Generation from Scene Graphs, Action Parsing, Activity Detection, Scene Graph Detection, Action Recognition, Video Classification, Activity Recognition, Multiview Detection, Multimodal Activity Recognition","Graph, Image, Text, Time Series, Video",English,Computer Vision,"video-classification-on-home-action-genome, image-generation-from-scene-graphs-on-home","Custom (research, non-commercial)",https://homeactiongenome.org/,https://paperswithcode.com/dataset/home-action-genome,"Home Action Genome is a large-scale multi-view video database of indoor daily activities. Every activity is captured by synchronized multi-view cameras, including an egocentric view.
There are 30 hours of vides with 70 classes of daily activities and 453 classes of atomic actions.",,,,,,70
1415,HONEST,Hurtful Sentence Completion,Hurtful Sentence Completion,Hurtful Sentence Completion,,,Methodology,hurtful-sentence-completion-on-honest-en,MIT,https://github.com/MilaNLProc/honest,https://paperswithcode.com/dataset/honest-en,"The HONEST dataset is a template-based corpus for testing the hurtfulness of sentence completions in language models (e.g., BERT) in six different languages (English, Italian, French, Portuguese, Romanian, and Spanish).  HONEST is composed of 420 instances for each language, which are generated from 28 identity terms (14 male and 14 female) and 15 templates. It uses a set of identifier terms in singular and plural (i.e., woman, women, girl, boys) and a series of predicates (i.e., “works as [MASK]”, “is known for [MASK]”). The objective is to use language models to fill the sentence, then the hurtfulness of the completion is evaluated.",,,,420 instances,,
1416,HopeEDI,Hope Speech Detection for English,Hope Speech Detection for English,"Hope Speech Detection for English, Hope Speech Detection for Malayalam, Hope Speech Detection for Tamil, Hope Speech Detection, Text Classification","Audio, Image, Text",English,Speech,"hope-speech-detection-for-english-on-hopeedi, hope-speech-detection-for-tamil-on-hopeedi, hope-speech-detection-for-malayalam-on, hope-speech-detection-on-hopeedi",,https://competitions.codalab.org/competitions/27653#participate-get-data,https://paperswithcode.com/dataset/hopeedi,"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff’s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.",,,,,,
1417,HOST,Scene Text Recognition,Scene Text Recognition,Scene Text Recognition,"Image, Text",English,Computer Vision,scene-text-recognition-on-host,,,https://paperswithcode.com/dataset/host,The heavily occluded scene text (HOST) dataset is a dataset that contains images of text with occlusions. It is used to improve the recognition performance of occluded text in machine vision applications 1. The dataset is composed of 4832 images that are manually occluded in weak or heavy degrees.,,,,4832 images,,
1418,Hotel,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Feature Importance, Time Series Prediction, feature selection, Probabilistic Deep Learning, Time Series Analysis",Time Series,,Time Series,,Open Source,https://github.com/ashfarhangi/COVID-19,https://paperswithcode.com/dataset/hotel-sales,"The dataset contains the hotel demand and revenue of 8 major tourist destinations in the US (e.g., Los Angeles, Orlando ...). The dataset contains sales, daily occupancy, demand, and revenue of the upper-middle class hotels.

We also gathered dynamic exogenous variables such as the state’s closure/open policy to enrich our dataset. Specifically, we gathered numerious static features such as the number of hospitals, GPD, and population.",,,,,,
1419,HotpotQA,Text Retrieval,Text Retrieval,"Text Retrieval, Retrieval, Question Answering, Reading Comprehension",Text,English,Natural Language Processing,"question-answering-on-hotpotqa, text-retrieval-on-hotpotqa, retrieval-on-hotpotqa",CC BY-SA 4.0,https://hotpotqa.github.io/,https://paperswithcode.com/dataset/hotpotqa,"HotpotQA is a question answering dataset collected on the English Wikipedia, containing about 113K crowd-sourced questions that are constructed to require the introduction paragraphs of two Wikipedia articles to answer. Each question in the dataset comes with the two gold paragraphs, as well as a list of sentences in these paragraphs that crowdworkers identify as supporting facts necessary to answer the question. 

A diverse range of reasoning strategies are featured in HotpotQA, including questions involving missing entities in the question, intersection questions (What satisfies property A and property B?), and comparison questions, where two entities are compared by a common attribute, among others. In the few-document distractor setting, the QA models are given ten paragraphs in which the gold paragraphs are guaranteed to be found; in the open-domain fullwiki setting, the models are only given the question and the entire Wikipedia. Models are evaluated on their answer accuracy and explainability, where the former is measured as overlap between the predicted and gold answers with exact match (EM) and unigram F1, and the latter concerns how well the predicted supporting fact sentences match human annotation (Supporting Fact EM/F1). A joint metric is also reported on this dataset, which encourages systems to perform well on both tasks simultaneously.",,Answering Complex Open-domain Questions Through Iterative Query Generation,https://arxiv.org/abs/1910.07000,,,
1420,House3D_Environment,Question Answering,Question Answering,"Question Answering, Visual Navigation, Efficient Exploration","Image, Text",English,Computer Vision,,,https://github.com/facebookresearch/House3D,https://paperswithcode.com/dataset/house3d-environment,"A rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et.al.)",,,,,,
1421,HouseExpo,Motion Planning,Motion Planning,Motion Planning,Video,,Methodology,,,https://github.com/teaganli/houseexpo/,https://paperswithcode.com/dataset/houseexpo,"A large-scale indoor layout dataset containing 35,357 2D floor plans including 252,550 rooms in total.",,,,,,
1422,HoVer,Fact Verification,Fact Verification,Fact Verification,,,Methodology,,,https://hover-nlp.github.io/,https://paperswithcode.com/dataset/hover,"Is a dataset for many-hop evidence extraction and fact verification. It challenges models to extract facts from several Wikipedia articles that are relevant to a claim and classify whether the claim is Supported or Not-Supported by the facts. In HoVer, the claims require evidence to be extracted from as many as four English Wikipedia articles and embody reasoning graphs of diverse shapes.",,,,,,
1423,How2,Text Summarization,Text Summarization,"Text Summarization, Audio-Visual Speech Recognition, Multimodal Abstractive Text Summarization","Audio, Image, Text",English,Computer Vision,"text-summarization-on-how2, multimodal-abstractive-text-summarization-on",CC BY-SA 4.0,https://srvk.github.io/how2-dataset/,https://paperswithcode.com/dataset/how2,"The How2 dataset contains 13,500 videos, or 300 hours of speech, and is split into 185,187 training, 2022 development (dev), and 2361 test utterances. It has subtitles in English and crowdsourced Portuguese translations.",2022,exploring multiview correlations in open-domain videos,https://arxiv.org/abs/1811.08890,,,
1424,How2Sign,Sign Language Production,Sign Language Production,"Sign Language Production, Video Inpainting, Sign Language Translation, Topic Classification, Gloss-free Sign Language Translation, Video Generation, Sign Language Recognition","Image, Text, Video",English,Computer Vision,"sign-language-translation-on-how2sign, gloss-free-sign-language-translation-on-2, video-generation-on-how2sign, video-inpainting-on-how2sign",Creative Commons Attribution 4.0 International,https://how2sign.github.io/,https://paperswithcode.com/dataset/how2sign,"The How2Sign is a multimodal and multiview continuous American Sign Language (ASL) dataset consisting of a parallel corpus of more than 80 hours of sign language videos and a set of corresponding modalities including speech, English transcripts, and depth. A three-hour subset was further recorded in the Panoptic studio enabling detailed 3D pose estimation.",,,,,,
1425,HOWS,Continual Learning,Continual Learning,"Continual Learning, Incremental Learning, Scene Recognition, Object Reconstruction, Classification","3D, Image",,Computer Vision,"classification-on-hows-long, classification-on-hows",Creative common distributions 4.0,https://zenodo.org/record/7189434,https://paperswithcode.com/dataset/hows,"HOWS-CL-25 (Household Objects Within Simulation dataset for Continual Learning) is a synthetic dataset especially designed for object classification on mobile robots operating in a changing environment (like a household), where it is important to learn new, never seen objects on the fly.
This dataset can also be used for other learning use-cases, like instance segmentation or depth estimation.
Or where household objects or continual learning are of interest.

Our dataset contains 150,795 unique synthetic images using 25 different household categories with 925 3D models in total. For each of those categories, we generated about 6000 RGB images. In addition, we also provide a corresponding depth, segmentation, and normal image.

The dataset was created with BlenderProc [Denninger et al. (2019)], a procedural pipeline to generate images for deep learning.
This tool created a virtual room with randomly textured floors, walls, and a light source with randomly chosen light intensity and color. After that, a 3D model is placed in the resulting room. This object gets customized by randomly assigning materials, including different textures, to achieve a diverse dataset. Moreover, each object might be deformed with a random
displacement texture.
We use 774 3D models from the ShapeNet dataset [A. X. Chang et al. (2015)] and the other models from various internet sites. Please note that we had to manually fix and filter most of the models with Blender before using them in the pipeline!

For continual learning (CL), we provide two different loading schemes:
- Five sequences with five categories each
- Twelve sequences with three categories in the first and two in the other sequences.

In addition to the RGB, depth, segmentation, and normal images, we also provide the calculated features of the RGB images (by ResNet50) as used in our RECALL paper.
In those two loading schemes, ten percent of the images are used for validation, where we ensure that an object instance is either in the training or the validation set, not in both. This avoids learning to recognize certain instances by heart.

We recommend using those loading schemes to compare your approach with others.

For further information and code examples, please have a look at our website: https://github.com/DLR-RM/RECALL.",2019,,,,,
1426,HowTo100M,Video Retrieval,Video Retrieval,"Video Retrieval, Video Captioning, Video Question Answering, Action Recognition","Image, Text, Video",English,Computer Vision,video-question-answering-on-howto100m-qa,Custom,https://www.di.ens.fr/willow/research/howto100m/,https://paperswithcode.com/dataset/howto100m,"HowTo100M is a large-scale dataset of narrated videos with an emphasis on instructional videos where content creators teach complex tasks with an explicit intention of explaining the visual content on screen. HowTo100M features a total of:


136M video clips with captions sourced from 1.2M Youtube videos (15 years of video)
23k activities from domains such as cooking, hand crafting, personal care, gardening or fitness

Each video is associated with a narration available as subtitles automatically downloaded from Youtube.",,,,,,
1427,HPD,Head Pose Estimation,Head Pose Estimation,"Head Pose Estimation, Driver Attention Monitoring, Self-Driving Cars","3D, Image",,Computer Vision,,,https://zenodo.org/record/5725116#.Y0_Mf1JBxs8,https://paperswithcode.com/dataset/hpd,"These images were generated using Blender and IEE-Simulator with different head-poses, where the images are labelled according to nine classes (straight, turned bottom-left, turned left, turned top-left, turned bottom-right, turned right, turned top-right, reclined, looking up). The dataset contains 16,013 training images and 2,825 testing images, in addition to 4,700 images for improvements.",,,,700 images,"training images and 2,825 testing images",
1428,HPS,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Camera Localization, 3D Absolute Human Pose Estimation","3D, Image",,Computer Vision,,Custom,http://virtualhumans.mpi-inf.mpg.de/hps/,https://paperswithcode.com/dataset/hps-dataset,"HPS Dataset is a collection of 3D humans interacting with large 3D scenes (300-1000 $m^2$, up to 2500 $m^2$). 
The dataset contains images captured from a head-mounted camera coupled with the reference 3D pose and location of the person in a pre-scanned 3D scene. 7 people in 8 large scenes are captured performing activities such as exercising, reading, eating, lecturing, using a computer, making coffee, dancing. The dataset provides more than 300K synchronized RGB images coupled with the reference 3D pose and location.

The dataset can be used as a testbed for ego-centric tracking with scene constraints, to learn how humans interact and move within large scenes over long periods of time, and to learn how humans process visual input arriving at their eyes.",,,,,,
1429,HQ-WMCA,Face Anti-Spoofing,Face Anti-Spoofing,Face Anti-Spoofing,Image,,Computer Vision,,,https://www.idiap.ch/dataset/hq-wmca,https://paperswithcode.com/dataset/hq-wmca,"The High-Quality Wide Multi-Channel Attack database (HQ-WMCA) database consists of 2904 short multi-modal video recordings of both bona-fide and presentation attacks. There are 555 bonafide presentations from 51 participants and the remaining 2349 are presentation attacks. The data is recorded from several channels including color, depth, thermal, infrared (spectra), and short-wave infrared (spectra).",,,,,,
1430,HRI_Simple_Tasks,Scene Graph Classification,Scene Graph Classification,"Scene Graph Classification, Robot Task Planning","Graph, Image",,Computer Vision,,GPL-3.0,https://github.com/buoncubi/HRI_tasks_dataset,https://paperswithcode.com/dataset/hri-simple-tasks,"The dataset concerns toy tasks that a human should teach to a robot. The number of task repetitions is limited in the dataset since the human should demonstrate the task to the robot only a few times.

The data contains images (collected with a Kinect camera) and objects' positions (acquired with a motion capture system and shared through CSV files) over time. 

The collected data concerns tasks such as (i) assembling the legs of a toy table and (ii) building a stack of objects. Both tasks have variations where the human should validate each step through a pen (e.g., to simulate a quality check procedure). 

Please, check our repository for more information.",,,,,,
1431,HRPlanesV2,Object Localization,Object Localization,"Object Localization, 2D Object Detection, Satellite Image Classification, Remote Sensing Image Classification, Classification",Image,,Computer Vision,,Creative Commons Attribution 4.0 International,https://eod-grss-ieee.com/dataset-detail/ak1BclhJbkpuUkh5Uitmd3B5L2hNQT09,https://paperswithcode.com/dataset/hrplanesv2,"The HRPlanesv2 dataset contains 2120 VHR Google Earth images. To further improve experiment results, images of airports from many different regions with various uses (civil/military/joint) selected and labeled. A total of 14,335 aircrafts have been labelled. Each image is stored as a "".jpg"" file of size 4800 x 2703 pixels and each label is stored as YOLO "".txt"" format. Dataset has been split in three parts as 70% train, %20 validation and test. The aircrafts in the images in the train and validation datasets have a percentage of 80 or more in size. Link: https://github.com/dilsadunsal/HRPlanesv2-Data-Set",,,,,"split in three parts as 70% train, %20 validation and test. The aircrafts in the images",
1432,HRSOD,Salient Object Detection,Salient Object Detection,"Salient Object Detection, RGB Salient Object Detection",Image,,Computer Vision,rgb-salient-object-detection-on-hrsod,,https://github.com/yi94code/HRSOD,https://paperswithcode.com/dataset/hrsod,"There exist several datasets for saliency detection, but none of them is specifically designed for high-resolution salient object detection. High-Resolution Salient Object Detection (HRSOD) dataset, containing 1610 training images and 400 test images. The total 2010 images are collected from the website of Flickr with the license of all creative commons. Pixel-level ground truths are manually annotated by 40 subjects. The shortest edge of each image in HRSOD is more than 1200 pixels.",2010,,,2010 images,training images and 400 test images,
1433,HS-SOD,Object Detection,Object Detection,"Object Detection, Saliency Detection, RGB Salient Object Detection",Image,,Computer Vision,,,https://github.com/gistairc/HS-SOD,https://paperswithcode.com/dataset/hs-sod,HS-SOD is a hyperspectral salient object detection dataset with a collection of 60 hyperspectral images with their respective ground-truth binary images and representative rendered colour images (sRGB).,,,,,,
1434,Huebner2017_MOABB,Within-Session ERP,Within-Session ERP,Within-Session ERP,,,Methodology,within-session-erp-on-huebner2017-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Huebner2017.html,https://paperswithcode.com/dataset/huebner2017-moabb,,,,,,,
1435,Huebner2018_MOABB,Within-Session ERP,Within-Session ERP,Within-Session ERP,,,Methodology,within-session-erp-on-huebner2018-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Huebner2018.html,https://paperswithcode.com/dataset/huebner2018-moabb,,,,,,,
1436,HUI_speech_corpus,Text-To-Speech Synthesis,Text-To-Speech Synthesis,"Text-To-Speech Synthesis, Speech Synthesis, Automatic Speech Recognition (ASR)","Audio, Image, Text",English,Speech,"automatic-speech-recognition-on-hui, text-to-speech-synthesis-on-hui",Creative Commons BY-SA 4.0,https://opendata.iisys.de/datasets.html#hui-audio-corpus-german,https://paperswithcode.com/dataset/hui,"The data set contains several speakers. The 5 largest are listed individually, the rest are summarized as other.
All audio files have a sampling rate of 44.1kHz.
For each speaker, there is a clean variant in addition to the full data set, where the quality is even higher. Furthermore, there are various statistics.
The dataset can also be used for automatic speech recognition (ASR) if audio files are converted to 16 kHz.",,,,,,
1437,Human-Art,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Conditional Image Generation, 2D Human Pose Estimation, Text-to-Image Generation","3D, Image, Text",English,Computer Vision,2d-human-pose-estimation-on-human-art,Custom,https://idea-research.github.io/HumanArt/,https://paperswithcode.com/dataset/human-art,"Human-Art is a versatile human-centric dataset to bridge the gap between natural and artificial scenes. It includes twenty high-quality human scenes, including natural and artificial humans in both 2D representation and 3D representation. It includes 50,000 images including more than 123,000 human figures in 20 scenarios, with annotations of human bounding box, 21 2D human keypoints, human self-contact keypoints, and description text.",,,,000 images,,
1438,Human3.6M,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Video Prediction, 3D Pose Estimation, Unsupervised Human Pose Estimation, Monocular 3D Human Pose Estimation, 3D Human Pose Estimation in Limited Data, Weakly-supervised 3D Human Pose Estimation, Human Part Segmentation, Human action generation, Pose Retrieval, Multi-Hypotheses 3D Human Pose Estimation, Unsupervised 3D Human Pose Estimation, 3D Absolute Human Pose Estimation, 2D Pose Estimation, Root Joint Localization, Human Pose Forecasting","3D, Image, Text, Time Series, Video",English,Computer Vision,"3d-absolute-human-pose-estimation-on-human36m, 3d-human-pose-estimation-on-human36m, human-part-segmentation-on-human3-6m, monocular-3d-human-pose-estimation-on-human3, pose-retrieval-on-human3-6m, unsupervised-human-pose-estimation-on-human3, weakly-supervised-3d-human-pose-estimation-on, human-action-generation-on-human3-6m, multi-hypotheses-3d-human-pose-estimation-on, 3d-pose-estimation-on-human3-6m, video-prediction-on-human36m, root-joint-localization-on-human3-6m, unsupervised-3d-human-pose-estimation-on, 3d-human-pose-estimation-in-limited-data-on, human-pose-forecasting-on-human36m, 2d-pose-estimation-on-human3-6m","Custom (research-only, attribution)",http://vision.imar.ro/human3.6m/description.php,https://paperswithcode.com/dataset/human3-6m,"The Human3.6M dataset is one of the largest motion capture datasets, which consists of 3.6 million human poses and corresponding images captured by a high-speed motion capture system. There are 4 high-resolution progressive scan cameras to acquire video data at 50 Hz. The dataset contains activities by 11 professional actors in 17 scenarios: discussion, smoking, taking photo, talking on the phone, etc., as well as provides accurate 3D joint positions and high-resolution videos.",,Space-Time Representation of People Based on 3D Skeletal Data: A Review,https://arxiv.org/abs/1601.01006,,,
1439,HUMAN4D,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, 3D Pose Estimation, Pose Estimation, Depth Estimation, 3D Human Shape Estimation, Monocular Depth Estimation, 3D Point Cloud Reconstruction, 3D Shape Retrieval, 3D Shape Reconstruction, Depth Image Estimation, 3D Depth Estimation, Indoor Monocular Depth Estimation","3D, Image",,Computer Vision,,,https://github.com/tofis/human4d_dataset,https://paperswithcode.com/dataset/human4d,"HUMAN4D is a large and multimodal 4D dataset that contains a variety of human activities simultaneously captured by a professional marker-based MoCap, a volumetric capture and an audio recording system. By capturing 2 female and $2$ male professional actors performing various full-body movements and expressions, HUMAN4D provides a diverse set of motions and poses encountered as part of single- and multi-person daily, physical and social activities (jumping, dancing, etc. ), along with multi-RGBD (mRGBD), volumetric and audio data.

Description from: HUMAN4D: A Human-Centric Multimodal Dataset for Motions and Immersive Media",,,,,,
1440,HumanAct12,Motion Synthesis,Motion Synthesis,"Motion Synthesis, Human action generation, Action Generation","Image, Text, Video",English,Computer Vision,"motion-synthesis-on-humanact12, human-action-generation-on-humanact12",,https://ericguo5513.github.io/action-to-motion/,https://paperswithcode.com/dataset/humanact12,"HumanAct12 is a new 3D human motion dataset adopted from the polar image and 3D pose dataset PHSPD, with proper temporal cropping and action annotating. Statistically, there are 1191 3D motion clips(and 90,099 poses in total) which are categorized into 12 action classes, and 34 fine-grained sub-classes. The action types includes daily actions such as walk, run, sit down, jump up, warm up, etc. Fine-grained action types contain more specific information like Warm up by bowing left side, Warm up by pressing left leg, etc.",,,,,,
1441,HumanEval-X,Code Translation,Code Translation,"Code Translation, Code Generation",Text,English,Natural Language Processing,,,https://huggingface.co/datasets/THUDM/humaneval-x,https://paperswithcode.com/dataset/humaneval-x,"HumanEval-X is a benchmark for evaluating the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks, such as code generation and translation.",,,,,val-X is a benchmark for evaluating the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples,
1442,HumanEval,HumanEval,HumanEval,"HumanEval, Code Generation",Text,English,Natural Language Processing,"code-generation-on-humaneval, humaneval-on-humaneval-1",,https://github.com/openai/human-eval,https://paperswithcode.com/dataset/humaneval,"This is an evaluation harness for the HumanEval problem solving dataset described in the paper ""Evaluating Large Language Models Trained on Code"". It used to measure functional correctness for synthesizing programs from docstrings. It consists of 164 original programming problems, assessing language comprehension, algorithms, and simple mathematics, with some
comparable to simple software interview questions.",,,,,,
1443,HumanML3D,Motion Synthesis,Motion Synthesis,"Motion Synthesis, Motion Captioning, Motion Generation","Image, Text, Video",English,Computer Vision,"motion-captioning-on-humanml3d, motion-synthesis-on-humanml3d, motion-generation-on-humanml3d",,https://github.com/EricGuo5513/HumanML3D,https://paperswithcode.com/dataset/humanml3d,"HumanML3D is a 3D human motion-language dataset that originates from a combination of HumanAct12 and Amass dataset. It covers a broad range of human actions such as daily activities (e.g., 'walking', 'jumping'), sports (e.g., 'swimming', 'playing golf'), acrobatics (e.g., 'cartwheel') and artistry (e.g., 'dancing'). Overall, HumanML3D dataset consists of 14,616 motions and 44,970 descriptions composed by 5,371 distinct words. The total length of motions amounts to 28.59 hours. The average motion length is 7.1 seconds, while average description length is 12 words.",,,,,,
1444,Human_Optical_Flow,3D Face Modelling,3D Face Modelling,"3D Face Modelling, 3D Face Animation","3D, Image",,Computer Vision,,,https://humanflow.is.tue.mpg.de/,https://paperswithcode.com/dataset/coma-1,A synthetic data of videos of human action sequences and the corresponding optical flow.,,,,,,
1445,Human_Simulacra,Large Language Model,Large Language Model,Large Language Model,Text,English,Natural Language Processing,,CC BY-NC-SA 4.0,https://github.com/hasakiXie123/Human-Simulacra/tree/main,https://paperswithcode.com/dataset/human-simulacra,"Human Simulacra is a virtual character dataset that contains 129k texts across 11 virtual characters, with each character having unique attributes, biographies, and stories.",,,,129k texts,,
1446,HUME-VB,A-VB High,A-VB High,"A-VB High, Audio Classification, Vocal Bursts Type Prediction, Vocal Bursts Valence Prediction, Emotion Recognition, Vocal Bursts Intensity Prediction, A-VB Two, A-VB Culture, Cultural Vocal Bursts Intensity Prediction","Audio, Image, Time Series",,Computer Vision,"a-vb-high-on-hume-vb, a-vb-two-on-hume-vb, two-on-hume-vb, high-on-hume-vb, culture-on-hume-vb, a-vb-culture-on-hume-vb, type-on-hume-vb",,https://www.competitions.hume.ai/avb2022,https://paperswithcode.com/dataset/hume-vb,"The Hume Vocal Burst Database (H-VB) includes all train, validation, and test recordings and corresponding emotion ratings for the train and validation recordings.

This dataset contains 59,201 audio recordings of vocal bursts from 1,702 speakers, from 4 cultures—the U.S, South Africa, China, and Venezuela—ranging in age from 20 to 39.5 years old. The duration of data in this version of H-VB is 36 Hours  (Mean: 02.23  sec). The emotion ratings correspond to ten emotion concepts, listed below, and averaged 0-100 intensities for each emotion concept, with each sample having been rated by an average of 85.2 raters.

Emotion Labels:  Awe, Excitement, Amusement, Awkwardness, Fear, Horror, Distress, Triumph, Sadness, Surprise.",,,,,,
1447,HumSet,Humanitarian,Humanitarian,"Humanitarian, Text Summarization, Multilingual NLP, Multilabel Text Classification","Image, Text",English,Computer Vision,,,https://blog.thedeep.io/humset/,https://paperswithcode.com/dataset/humset,"Timely and effective response to humanitarian crises requires quick and accurate analysis of large amounts of text data, a process that can highly benefit from expert-assisted NLP systems trained on validated and annotated data in the humanitarian response domain. To enable creation of such NLP systems, we introduce and release HumSet, a novel and rich multilingual dataset of humanitarian response documents annotated by experts in the humanitarian response community. The dataset provides documents in three languages (English, French, Spanish) and covers a variety of humanitarian crises from 2018 to 2021 across the globe. For each document, HumSet provides selected snippets (entries) as well as assigned classes to each entry annotated using common humanitarian information analysis frameworks. HumSet also provides novel and challenging entry extraction and multi-label entry classification tasks. In this paper, we take a first step towards approaching these tasks and conduct a set of experiments on  Pre-trained Language Models (PLM) to establish strong baselines for future research in this domain. The dataset is available at https://blog.thedeep.io/humset/.",2018,,,,,
1448,HWU64,Intent Detection,Intent Detection,Intent Detection,Image,,Computer Vision,"intent-detection-on-hwu64-5-shot, intent-detection-on-hwu64-10-shot, intent-detection-on-hwu64",CC-BY-SA 3.0,https://github.com/xliuhw/NLU-Evaluation-Data,https://paperswithcode.com/dataset/hwu64,This project contains natural language data for human-robot interaction in home domain which we collected and annotated for evaluating NLU Services/platforms.,,,,,,
1449,HybridQA,Text-To-SQL,Text-To-SQL,"Text-To-SQL, Question Generation, Question Answering",Text,English,Natural Language Processing,question-answering-on-hybridqa,,https://github.com/wenhuchen/HybridQA,https://paperswithcode.com/dataset/hybridqa,"A new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable.",,,,,,
1450,HYPE,Blood pressure estimation,Blood pressure estimation,Blood pressure estimation,,,Methodology,,Custom,https://github.com/arianesasso/aime-2020,https://paperswithcode.com/dataset/hype,"HYPE Dataset - Version 1.0.0

REFERENCE PAPER
Morassi Sasso, A., Datta, S., Jeitler, M., Steckhan, N., Kessler, C. S., Michalsen, A., Arnrich, B., & Böttinger, E. (2020). HYPE: Predicting Blood Pressure from Photoplethysmograms in a Hypertensive Population. In M. Michalowski & R. Moskovitch (Eds.), Artificial Intelligence in Medicine. AIME 2020. Lecture Notes in Computer Science, volume 12299 (pp. 325–335). Springer, Cham. https://doi.org/10.1007/978-3-030-59137-3_29.

CONTENT

Sensor (PPG & Blood Pressure) and clinical data from 9 hypertensive subjects in two experiments (stress test and 24 hours)
PPG data from Empatica E4: photoplethysmography (PPG) data.
Spacelabs (SL 90217): blood pressure data.
Demographics and self-reported data during 24 hours (exercise, medication, etc.).

AVAILABILITY

Available to the scientific community through a data agreement. 
The requester must be affiliated to a research institution.",2020,,,,,
1451,Hyper-Kvasir_Dataset,Real-Time Object Detection,Real-Time Object Detection,"Real-Time Object Detection, Image Classification, Medical Image Segmentation, General Classification, Anomaly Detection, Video Summarization, Object Detection, Organ Detection","Image, Text, Video",English,Computer Vision,"anomaly-detection-on-hyper-kvasir-dataset, medical-image-segmentation-on-hyper-kvasir",CC BY 4.0,https://datasets.simula.no/hyper-kvasir/,https://paperswithcode.com/dataset/hyper-kvasir-dataset,"HyperKvasir dataset contains 110,079 images and 374 videos where it captures anatomical landmarks and pathological and normal findings. A total of around 1 million images and video frames altogether.",,,,079 images,,
1452,Hypersim,3D Pose Estimation,3D Pose Estimation,"3D Pose Estimation, Instance Segmentation, Depth Estimation, Inverse Rendering, Intrinsic Image Decomposition, Semantic Segmentation, Monocular Depth Estimation, Panoptic Segmentation, 3D Shape Reconstruction, 3D Shape Recognition, 2D Object Detection, Single-View 3D Reconstruction, 3D Object Detection, 3D Reconstruction, 3D Panoptic Segmentation, Multi-Task Learning, 3D Semantic Segmentation","3D, Image",,Computer Vision,"3d-semantic-segmentation-on-hypersim, monocular-depth-estimation-on-hypersim, 3d-panoptic-segmentation-on-hypersim, panoptic-segmentation-on-hypersim, semantic-segmentation-on-hypersim",Custom,https://github.com/apple/ml-hypersim,https://paperswithcode.com/dataset/hypersim,"For many fundamental scene understanding tasks, it is difficult or impossible to obtain per-pixel ground truth labels from real images. Hypersim is a photorealistic synthetic dataset for holistic indoor scene understanding. It contains 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry.",,,,400 images,,
1453,Hyper_Drive,Hyperspectral image analysis,Hyperspectral image analysis,"Hyperspectral image analysis, Robot Navigation, Autonomous Navigation, Autonomous Vehicles, Hyperspectral Image Segmentation, Hyperspectral Unmixing, Classification Of Hyperspectral Images, Hyperspectral Image Denoising, Hyperspectral Image Classification, Self-Driving Cars, Autonomous Driving",Image,,Computer Vision,,MIT,https://river-lab.github.io/hyper_drive_data/,https://paperswithcode.com/dataset/hyper-drive,"Towards automated analysis of large environments, hyperspectral sensors must be adapted into a format where they can be operated from mobile robots. In this dataset, we highlight hyperspectral datacubes collected from the Hyper-Drive  imaging system. Our system collects and registers datacubes spanning the visible to shortwave infrared (660-1700 nm) in 33 wavelength channels. The system also simultaneously captures the ambient solar spectrum reflected off a white reference tile. The dataset consists of 500 labeled datacubes from on-road and off-road terrain compliant with the ATLAS.

This work first appeared at WHISPERS 2023 In Athens, Greece and is a product of the  Robotics and Intelligent Vehicles Research Laboratory (RIVeR) at Northeastern University.

In addition to the 500 labeled hyperspectral datacubes, raw ROS bagfiles generated of each of the sensor feeds at a higher frame rate are available here. These files are provided as an additional resource and do not contain semantic labels, but contain ~10,000 additional hyperspectral datacubes of in-between frames from the labeled dataset. It also contains additional datatypes for terrain analysis such as inertial measurement unit (IMU) data. To the best of the authors knowledge, it is the largest vehicle-centric hyperspectral dataset currently available!

This research was sponsored by the United States Army Core of Engineers (USACE) Engineer Research and Development Center (ERDC) Geospatial Research Laboratory (GRL) and was accomplished under Cooperative Agreement Federal Award Identification Number (FAIN) W9132V-22-2-0001. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of USACE EDRC GRL or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.",2023,ATLAS,"http://gvsets.ndia-mich.org/documents/AAIR/2022/ATLAS,%20an%20All-Terrain%20Labelset%20for%20Autonomous%20Systems.pdf",,,
1454,I.PHI,Ancient Text Restoration,Ancient Text Restoration,Ancient Text Restoration,Text,English,Natural Language Processing,ancient-text-restoration-on-i-phi,Custom,https://github.com/sommerschield/iphi,https://paperswithcode.com/dataset/i-phi,I.PHI processes the Packard Humanities Institute (PHI) database of ancient Greek inscriptions including the geographical and chronological metadata into a machine actionable format. The processed dataset is referred to as I.PHI.,,,,,,
1455,i2b2_De-identification_Dataset,De-identification,De-identification,"De-identification, Named Entity Recognition (NER), Information Retrieval","Image, Text",English,Computer Vision,named-entity-recognition-on-i2b2-de,,https://www.i2b2.org/NLP/DataSets/Main.php,https://paperswithcode.com/dataset/i2b2-de-identification-dataset,This dataset contains 1304 de-identified longitudinal medical records describing 296 patients.,,,,,,
1456,IAM,Data Augmentation,Data Augmentation,"Data Augmentation, Optical Character Recognition (OCR), Handwriting Recognition, Handwritten Text Recognition, HTR","Image, Text",English,Computer Vision,"htr-on-iam, handwritten-text-recognition-on-iam","Custom (research-only, non-commercial, attribution)",https://fki.tic.heia-fr.ch/databases/iam-handwriting-database,https://paperswithcode.com/dataset/iam,"The IAM database contains 13,353 images of handwritten lines of text created by 657 writers. The texts those writers transcribed are from the Lancaster-Oslo/Bergen Corpus of British English. It includes contributions from 657 writers making a total of 1,539 handwritten pages comprising of 115,320 words and is categorized as part of modern collection. The database is labeled at the sentence, line, and word levels.",,Measuring Human Perception to Improve Handwritten Document Transcription,https://arxiv.org/abs/1904.03734,353 images,,
1457,IAM_Dataset,Claim Extraction with Stance Classification (CESC),Claim Extraction with Stance Classification (CESC),"Claim Extraction with Stance Classification (CESC), Claim-Evidence Pair Extraction (CEPE)",Image,,Computer Vision,"claim-evidence-pair-extraction-cepe-on-iam, claim-extraction-with-stance-classification",,,https://paperswithcode.com/dataset/iam-dataset,"We introduce a large and comprehensive dataset to facilitate the study of several essential AM tasks in the debating system. In our work, we first review the existing subtasks (claim extraction, stance classification, evidence extraction), and then propose two integrated argument mining tasks: claim extraction with stance classification (CESC) and claim-evidence pair extraction (CEPE).",,,,,,
1458,IAM_line-level_,Handwritten Text Recognition,Handwritten Text Recognition,Handwritten Text Recognition,"Image, Text",English,Computer Vision,handwritten-text-recognition-on-iam-line,"Custom (research-only, non-commercial, attribution)",https://fki.tic.heia-fr.ch/databases/iam-handwriting-database,https://paperswithcode.com/dataset/iam-line-level,"The IAM database contains 13,353 images of handwritten lines of text created by 657 writers. The texts those writers transcribed are from the Lancaster-Oslo/Bergen Corpus of British English. It includes contributions from 657 writers making a total of 1,539 handwritten pages comprising of 115,320 words and is categorized as part of modern collection. The database is labeled at the sentence, line, and word levels.",,Measuring Human Perception to Improve Handwritten Document Transcription,https://arxiv.org/abs/1904.03734,353 images,,
1459,IBISCape,3D Pose Estimation,3D Pose Estimation,"3D Pose Estimation, 3D Object Tracking, 3D Scene Reconstruction, 3D Point Cloud Reconstruction, 3D Object Detection From Stereo Images, 2D Semantic Segmentation task 3 (25 classes), 3D Depth Estimation","3D, Image, Video",,Computer Vision,,MIT,https://ueve-my.sharepoint.com/:f:/g/personal/abanob_soliman_univ-evry_fr/EhMkDJO1s8BCheJH2Yv8k7IBkj6cFtXRFBWFzlXWGm_Lyw?e=ixr3fh,https://paperswithcode.com/dataset/ibiscape,A Simulated Benchmark for multi-modal SLAM Systems Evaluation in Large-scale Dynamic Environments.,,,,,,
1460,IBM_Transactions_for_Anti_Money_Laundering,Fraud Detection,Fraud Detection,Fraud Detection,Image,,Computer Vision,,"Community Data License Agreement – Sharing, Version 1.0",https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml,https://paperswithcode.com/dataset/ibm-transactions-for-anti-money-laundering,"Money laundering is a multi-billion dollar issue. Detection of laundering is very difficult. Most automated algorithms have a high false positive rate: legitimate transactions incorrectly flagged as laundering. The converse is also a major problem -- false negatives, i.e. undetected laundering transactions. Naturally, criminals work hard to cover their tracks.

Access to real financial transaction data is highly restricted -- for both proprietary and privacy reasons. Even when access is possible, it is problematic to provide a correct tag (laundering or legitimate) to each transaction -- as noted above. This synthetic transaction data from IBM avoids these problems.

The data provided here is based on a virtual world inhabited by individuals, companies, and banks. Individuals interact with other individuals and companies. Likewise, companies interact with other companies and with individuals. These interactions can take many forms, e.g. purchase of consumer goods and services, purchase orders for industrial supplies, payment of salaries, repayment of loans, and more. These financial transactions are generally conducted via banks, i.e. the payer and receiver both have accounts, with accounts taking multiple forms from checking to credit cards to bitcoin.

Some (small) fraction of the individuals and companies in the generator model engage in criminal behavior -- such as smuggling, illegal gambling, extortion, and more. Criminals obtain funds from these illicit activities, and then try to hide the source of these illicit funds via a series of financial transactions. Such financial transactions to hide illicit funds constitute laundering. Thus, the data available here is labelled and can be used for training and testing AML (Anti Money Laundering) models and for other purposes.

The data generator that created the data here not only models illicit activity, but also tracks funds derived from illicit activity through arbitrarily many transactions -- thus creating the ability to label laundering transactions many steps removed from their illicit source. With this foundation, it is straightforward for the generator to label individual transactions as laundering or legitimate.",,,,,,
1461,ICASSP_2021_Acoustic_Echo_Cancellation_Challenge,Acoustic echo cancellation,Acoustic echo cancellation,Acoustic echo cancellation,Audio,,Audio,,,https://github.com/microsoft/AEC-Challenge,https://paperswithcode.com/dataset/icassp-2021-acoustic-echo-cancellation,"The ICASSP 2021 Acoustic Echo Cancellation Challenge is intended to stimulate research in the area of acoustic echo cancellation (AEC), which is an important part of speech enhancement and still a top issue in audio communication and conferencing systems. Many recent AEC studies report good performance on synthetic datasets where the train and test samples come from the same underlying distribution. However, the AEC performance often degrades significantly on real recordings. Also, most of the conventional objective metrics such as echo return loss enhancement (ERLE) and perceptual evaluation of speech quality (PESQ) do not correlate well with subjective speech quality tests in the presence of background noise and reverberation found in realistic environments. In this challenge, we open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 2,500 real audio devices and human speakers in real environments, as well as a synthetic dataset. We open source two large test sets, and we open source an online subjective test framework for researchers to quickly test their results. The winners of this challenge will be selected based on the average Mean Opinion Score (MOS) achieved across all different single talk and double talk scenarios.",2021,,,,,
1462,ICBHI_Respiratory_Sound_Database,Lung Sound Classification,Lung Sound Classification,"Lung Sound Classification, Audio Classification","Audio, Image",,Computer Vision,"lung-sound-classification-on-icbhi, audio-classification-on-icbhi-respiratory",,https://bhichallenge.med.auth.gr/ICBHI_2017_Challenge,https://paperswithcode.com/dataset/icbhi-respiratory-sound-database,"The Respiratory Sound database was originally compiled to support the scientific challenge organized at Int. Conf. on Biomedical Health Informatics - ICBHI 2017.

The database consists of a total of 5.5 hours of recordings containing 6898 respiratory cycles, of which 1864 contain crackles, 886 contain wheezes, and 506 contain both crackles and wheezes, in 920 annotated audio samples from 126 subjects.

The cycles were annotated by respiratory experts as including crackles, wheezes, a combination of them, or no adventitious respiratory sounds. The recordings were collected using heterogeneous equipment and their duration ranged from 10s to 90s. 

For more information about the dataset, the annotation files, or to download it, please visit the challenge official page.",2017,,,,,
1463,ICConv,Conversational Search,Conversational Search,Conversational Search,,,Methodology,,CC-BY-SA 4.0,https://github.com/hongjx175/ICConv,https://paperswithcode.com/dataset/icconv,"The dataset contains 105,811 information-seeking conversations converted from MS MARCO. This dataset is constructed to relieve the data scarcity problem of conversational search to an extent. Considering the multi-intent problem and contextual information, this large-scale intent-oriented and context-aware dataset is automatically constructed based on the web search session data in MS MARCO. This dataset can be used to train and evaluate conversational search systems.",,,,,,
1464,ICDAR_2003,Optical Character Recognition (OCR),Optical Character Recognition (OCR),"Optical Character Recognition (OCR), Scene Text Recognition","Image, Text",English,Computer Vision,scene-text-recognition-on-icdar-2003,Custom (research-only),http://www.imglab.org/db/index.html,https://paperswithcode.com/dataset/icdar-2003,The ICDAR2003 dataset is a dataset for scene text recognition. It contains 507 natural scene images (including 258 training images and 249 test images) in total. The images are annotated at character level. Characters and words can be cropped from the images.,,Robust Scene Text Recognition Using Sparse Coding based Features,https://arxiv.org/abs/1512.08669,,training images and 249 test images,
1465,ICDAR_2013,Scene Text Detection,Scene Text Detection,"Scene Text Detection, Handwritten Chinese Text Recognition, Scene Text Recognition, Table Detection","Image, Tabular, Text",English,Computer Vision,"scene-text-recognition-on-icdar2013, scene-text-detection-on-icdar-2013, table-detection-on-icdar2013-1",,https://rrc.cvc.uab.es/?ch=2,https://paperswithcode.com/dataset/icdar-2013,"The ICDAR 2013 dataset consists of 229 training images and 233 testing images, with word-level annotations provided. It is the standard benchmark dataset for evaluating near-horizontal text detection.",2013,Single Shot Text Detector with Regional Attention,https://arxiv.org/abs/1709.00138,,training images and 233 testing images,
1466,ICDAR_2019,Table Detection,Table Detection,Table Detection,"Image, Tabular",,Computer Vision,table-detection-on-ctdar,,https://cndplab-founder.github.io/cTDaR2019/,https://paperswithcode.com/dataset/ctdar,"Table is a compact and efficient form for summarizing and presenting correlative information in handwritten and printed archival documents, scientific journals, reports, financial statements and so on. Table recognition is fundamental for the extraction of information from structured documents. The ICDAR 2019 cTDaR evaluates two aspects of table analysis: table detection and recognition. The participating methods will be evaluated on a modern dataset and archival documents with printed and handwritten tables present.",2019,,,,,
1467,ICIAR_2018_Grand_Challenge_on_Breast_Cancer_Histol,Breast Cancer Histology Image Classification,Breast Cancer Histology Image Classification,"Breast Cancer Histology Image Classification, Medical Image Analysis",Image,,Computer Vision,breast-cancer-histology-image-classification-2,CC BY 4.0,https://iciar2018-challenge.grand-challenge.org/,https://paperswithcode.com/dataset/iciar-2018-grand-challenge-on-breast-cancer,The dataset is composed of Hematoxylin and eosin (H&E) stained breast histology microscopy and whole-slide images. Challenge participants should evaluate the performance of their method on either/both sets of images.,,,,,,
1468,ICLabel,Electroencephalogram (EEG),Electroencephalogram (EEG),Electroencephalogram (EEG),,,Methodology,,,https://github.com/lucapton/ICLabel-Dataset,https://paperswithcode.com/dataset/iclabel,"An Independent components (IC) dataset containing spatiotemporal measures for over 200,000 ICs from more than 6,000 EEG recordings.",,,,,,
1469,ICLR_Database,Fairness,Fairness,"Fairness, Abstractive Text Summarization, Review Generation, Causal Inference, Paper generation, Selection bias",Text,English,Natural Language Processing,,MIT,https://cogcomp.github.io/iclr_database/,https://paperswithcode.com/dataset/iclr-database,"A maintained database tracks ICLR submissions and reviews, augmented with author profiles and higher-level textual features.",,,,,,
1470,Icon645,Image Classification,Image Classification,"Image Classification, Unsupervised Pre-training",Image,,Computer Vision,,CC BY-NC-SA 4.0,https://iconqa.github.io/,https://paperswithcode.com/dataset/icon645,"Icon645 is a large-scale dataset of icon images that cover a wide range of objects:


645,687 colored icons
377 different icon classes

These collected icon classes are frequently mentioned in the IconQA questions. In this work, we use the icon data to pre-train backbone networks on the icon classification task in order to extract semantic representations from abstract diagrams in IconQA. On top of pre-training encoders, the large-scale icon data could also contribute to open research on abstract aesthetics and symbolic visual understanding.",,,,,,
1471,IconQA,Common Sense Reasoning,Common Sense Reasoning,"Common Sense Reasoning, Mathematical Reasoning, Visual Reasoning, Mathematical Question Answering, Visual Commonsense Reasoning, Math Word Problem Solving, Visual Question Answering (VQA)","Image, Text",English,Reasoning,visual-question-answering-on-iconqa,CC BY-NC-SA 4.0,https://iconqa.github.io/,https://paperswithcode.com/dataset/iconqa,"Current visual question answering (VQA) tasks mainly consider answering human-annotated questions for natural images in the daily-life context. Icon question answering (IconQA) is a benchmark which aims to highlight the importance of abstract diagram understanding and comprehensive cognitive reasoning in real-world diagram word problems. For this benchmark, a large-scale IconQA dataset is built that consists of three sub-tasks: multi-image-choice, multi-text-choice, and filling-in-the-blank. Compared to existing VQA benchmarks, IconQA requires not only perception skills like object recognition and text understanding, but also diverse cognitive reasoning skills, such as geometric reasoning, commonsense reasoning, and arithmetic reasoning.

Description from: IconQA",,,,,,
1472,Icons-50,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Adversarial Defense, Out-of-Distribution Detection",Image,,Computer Vision,,,https://github.com/hendrycks/robustness,https://paperswithcode.com/dataset/icons-50,Icons-50 is a dataset for studying surface variation robustness.,,,,,,
1473,ICSI_Meeting_Corpus,Abstractive Dialogue Summarization,Abstractive Dialogue Summarization,"Abstractive Dialogue Summarization, Meeting Summarization",Text,English,Natural Language Processing,meeting-summarization-on-icsi-meeting-corpus,,https://github.com/guokan-shang/ami-and-icsi-corpora,https://paperswithcode.com/dataset/icsi-meeting-corpus,ICSI Meeting Corpus in JSON format.,,,,,,
1474,IDD,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Domain Adaptation, Autonomous Driving",Image,,Computer Vision,,,http://idd.insaan.iiit.ac.in/,https://paperswithcode.com/dataset/idd,"IDD is a dataset for road scene understanding in unstructured environments used for semantic segmentation and object detection for autonomous driving. It consists of 10,004 images, finely annotated with 34 classes collected from 182 drive sequences on Indian roads.",,Varma et al,https://arxiv.org/pdf/1811.10200v1.pdf,004 images,,34
1475,iDesigner,Pose Transfer,Pose Transfer,"Pose Transfer, Reconstruction, Subject Transfer",3D,,Methodology,"pose-transfer-on-idesigner, subject-transfer-on-idesigner, reconstruction-on-idesigner",,https://sites.google.com/view/fgvc6/competitions/idesigner-2019,https://paperswithcode.com/dataset/idesigner,"Fashion trends are constantly evolving, but a trained eye can estimate with some accuracy the signature elements of a particular designer's style.

50,000 runway images, spanning 50 fashion designers, from our large repository of proprietary front row images from fashion shows over the past 15 years. The data includes a variety of fashion items, including: shoes, bags, dress, jackets etc.",,,,,,
1476,IDRiD,Fovea Detection,Fovea Detection,"Fovea Detection, Optic Disc Detection, Medical Image Classification",Image,,Computer Vision,"optic-disc-detection-on-idrid, fovea-detection-on-idrid, medical-image-classification-on-idrid",,https://idrid.grand-challenge.org/,https://paperswithcode.com/dataset/idrid,Indian Diabetic Retinopathy Image Dataset (IDRiD) dataset consists of typical diabetic retinopathy lesions and normal retinal structures annotated at a pixel level. This dataset also provides information on the disease severity of diabetic retinopathy and diabetic macular edema for each image. This dataset is perfect for the development and evaluation of image analysis algorithms for early detection of diabetic retinopathy.,,,,,,
1477,IEE,Term Extraction,Term Extraction,Term Extraction,,,Methodology,,,,https://paperswithcode.com/dataset/iee,IEE is a financial-domain dataset of the Insurance-entity extraction task. Its goal is to locate named entities mentioned in the input sentence.,,,,,,
1478,IEMOCAP,Multimodal Emotion Recognition,Multimodal Emotion Recognition,"Multimodal Emotion Recognition, Emotion Recognition in Conversation, Speech Emotion Recognition","Audio, Image",,Multimodal,"multimodal-emotion-recognition-on-iemocap, speech-emotion-recognition-on-iemocap, emotion-recognition-in-conversation-on",Custom (non-commercial),https://sail.usc.edu/iemocap/iemocap_publication.htm,https://paperswithcode.com/dataset/iemocap,"Multimodal Emotion Recognition IEMOCAP The IEMOCAP dataset consists of 151 videos of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset. Each segment is annotated for the presence of 9 emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed and neutral) as well as valence, arousal and dominance. The dataset is recorded across 5 sessions with 5 pairs of speakers.",,Multi-attention Recurrent Network for Human Communication Comprehension,https://arxiv.org/abs/1802.00923,,,
1479,iFakeFaceDB,Face Generation,Face Generation,Face Generation,"Image, Text",English,Computer Vision,,,https://github.com/socialabubi/iFakeFaceDB,https://paperswithcode.com/dataset/ifakefacedb,"iFakeFaceDB is a face image dataset for the study of synthetic face manipulation detection, comprising about 87,000 synthetic face images generated by the Style-GAN model and transformed with the GANprintR approach. All images were aligned and resized to the size of 224 x 224.",,,,,,
1480,IFEval,Large Language Model,Large Language Model,"Large Language Model, Instruction Following",Text,English,Natural Language Processing,instruction-following-on-ifeval,,https://github.com/google-research/google-research/tree/master/instruction_following_eval,https://paperswithcode.com/dataset/ifeval,"This dataset evaluates instruction following ability of large language models. There are 500+ prompts with instructions such as ""write an article with more than 800 words"", ""wrap your response with double quotation marks"", etc.",,,,,,
1481,IGLU,Grounded language learning,Grounded language learning,Grounded language learning,Text,English,Natural Language Processing,,MIT,https://github.com/iglu-contest/iglu-dataset/,https://paperswithcode.com/dataset/iglu,"IGLU is a dataset designed for interactive grounded language understanding. It has a total of 8,136 single-turn data pairs of instructions and actions.  Every single sample is randomly initialized with a pre-built structure from previously collected multi-turn interactions data.",,Collecting Interactive Multi-modal Datasets for Grounded Language Understanding,https://arxiv.org/pdf/2211.06552v1.pdf,,,
1482,IGLUE,Max-Shot Cross-Lingual Visual Reasoning,Max-Shot Cross-Lingual Visual Reasoning,"Max-Shot Cross-Lingual Visual Reasoning, Max-Shot Cross-Lingual Visual Natural Language Inference, Zero-Shot Cross-Lingual Visual Natural Language Inference, Visual Reasoning, Max-Shot Cross-Lingual Visual Question Answering, Zero-Shot Cross-Lingual Image-to-Text Retrieval, Zero-Shot Cross-Lingual Text-to-Image Retrieval, Max-Shot Cross-Lingual Image-to-Text Retrieval, Max-Shot Cross-Lingual Text-to-Image Retrieval, Zero-Shot Cross-Lingual Visual Question Answering, Zero-Shot Cross-Lingual Visual Reasoning, Zero-Shot Cross-Lingual Transfer","Image, Text",English,Reasoning,"max-shot-cross-lingual-visual-reasoning-on, zero-shot-cross-lingual-visual-reasoning-on, max-shot-cross-lingual-visual-question, zero-shot-cross-lingual-image-to-text-1, max-shot-cross-lingual-visual-natural, zero-shot-cross-lingual-visual-question, zero-shot-cross-lingual-transfer-on-marvl, zero-shot-cross-lingual-text-to-image-1, max-shot-cross-lingual-image-to-text, max-shot-cross-lingual-text-to-image, zero-shot-cross-lingual-image-to-text, zero-shot-cross-lingual-text-to-image, zero-shot-cross-lingual-visual-natural",,https://iglue-benchmark.github.io/,https://paperswithcode.com/dataset/iglue,"The Image-Grounded Language Understanding Evaluation (IGLUE) benchmark brings together—by both aggregating pre-existing datasets and creating new ones—visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. The benchmark enables the evaluation of multilingual multimodal models for transfer learning, not only in a zero-shot setting, but also in newly defined few-shot learning setups.",,,,,,
1483,IHDP,Causal Inference,Causal Inference,Causal Inference,,,Methodology,causal-inference-on-ihdp,,https://causalforge.readthedocs.io/en/latest/user_guide/Loading_Causal_RW_Benchmarking_Datasets.html,https://paperswithcode.com/dataset/ihdp,"The Infant Health and Development Program (IHDP) is a randomized controlled study designed to evaluate the effect of home visit from specialist doctors on the cognitive test scores of premature infants. The datasets is first used for benchmarking treatment effect estimation algorithms in Hill [35], where selection bias is induced by removing non-random subsets of the treated individuals to create an observational dataset, and the outcomes are generated using the original covariates and treatments. It contains 747 subjects and 25 variables.",,,,,,
1484,IIIT-AR-13K,Object Detection,Object Detection,"Object Detection, Optical Character Recognition (OCR), Table Detection","Image, Tabular",,Computer Vision,,,http://cvit.iiit.ac.in/usodi/iiitar13k.php,https://paperswithcode.com/dataset/iiit-ar-13k,"IIIT-AR-13K is created by manually annotating the bounding boxes of graphical or page objects in publicly available annual reports. This dataset contains a total of 13k annotated page images with objects in five different popular categories - table, figure, natural image, logo, and signature. It is the largest manually annotated dataset for graphical object detection.",,,,,,
1485,IJB-B,Face Identification,Face Identification,"Face Identification, Face Recognition, Face Verification, Quantization, Lightweight Face Recognition",Image,,Computer Vision,"face-recognition-on-ijb-b, face-identification-on-ijb-b, lightweight-face-recognition-on-ijb-b, quantization-on-ijb-b, face-verification-on-ijb-b",Custom,https://www.nist.gov/programs-projects/face-challenges,https://paperswithcode.com/dataset/ijb-b,"The IJB-B dataset is a template-based face dataset that contains 1845 subjects with 11,754 images, 55,025 frames and 7,011 videos where a template consists of a varying number of still images and video frames from different sources. These images and videos are collected from the Internet and are totally unconstrained, with large variations in pose, illumination, image quality etc. In addition, the dataset comes with protocols for 1-to-1 template-based face verification, 1-to-N template-based open-set face identification, and 1-to-N open-set video face identification.",,An Automatic System for Unconstrained Video-Based Face Recognition,https://arxiv.org/abs/1812.04058,754 images,,
1486,IJB-C,Lightweight Face Recognition,Lightweight Face Recognition,"Lightweight Face Recognition, Quantization, Face Verification",Image,,Computer Vision,"quantization-on-ijb-c, face-verification-on-ijb-c, lightweight-face-recognition-on-ijb-c","Custom (research-only, attribution)",https://www.nist.gov/programs-projects/face-challenges,https://paperswithcode.com/dataset/ijb-c,"The IJB-C dataset is a video-based face recognition dataset. It is an extension of the IJB-A dataset with about 138,000 face images, 11,000 face videos, and 10,000 non-face images.",,Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results,https://arxiv.org/abs/1804.10275,,,
1487,iKala,Style Transfer,Style Transfer,"Style Transfer, Information Retrieval, Music Information Retrieval, Speech Separation",Audio,,Audio,speech-separation-on-ikala,,http://mac.citi.sinica.edu.tw/ikala/,https://paperswithcode.com/dataset/ikala,"The iKala dataset is a singing voice separation dataset that comprises of 252 30-second excerpts sampled from 206 iKala songs (plus 100 hidden excerpts reserved for MIREX data mining contest). The music accompaniment and the singing voice are recorded at the left and right channels respectively. Additionally, the human-labeled pitch contours and timestamped lyrics are provided.

This dataset is not available anymore.


""T"" as a sofa:

The ""T"" horizontal strip can mimic the back of a sofa with a delicate cushion or details of the uphols or appliances with the color button.

The ""T"" vertical strip can show a feet or arm of the sofa, shiny, yet firm.


Merge ""P"":

Put ""P"" next to ""T"", your curve to delicately with the top ""T."" It is intertwined. The circular part of ""P"" can show a cushion or a curved chair and synchronize the subject of furniture.

Make sure ""P"" is visually relying on ""T"", which reflects the relationship of cohesion and balance.


Coherence of ""B"" and ""I"":

""B"" can be aligned as a pair of cushions or a modern chair, with mild curves with glossy and modern aesthetics.

""I"" can be a symbol of a shiny furniture or a vertical light bar and completes the shapes without overburdess them.

Color palette 4:

Includes soft soil colors such as beige, top and gray shades, along with silent or silver gold tips to touch elegance.

Consider a slope effect to enhance modernity, to keep colors elegant and complex.


Connect the letters:

Use the overlap or intertwined edges that the letters meet for the symbol of unity.

The plan should allow viewers to distinguish each letter while feeling part of the same ""structure"".


Background patterns:

Use delicate geometric patterns or textures that mimic fabrics or furniture materials such as wood seeds or woven fibers.

These patterns must remain minimalist and focus on highlighting the logo, while maintaining communication.

While it deals with the subject of furniture and design, this concept conveys modernity, creativity and professional. If you like, I can create a draft design for better visualization.",,,,,,
1488,iLIDS-VID,Video-Based Person Re-Identification,Video-Based Person Re-Identification,"Video-Based Person Re-Identification, Unsupervised Person Re-Identification, Person Re-Identification","Image, Video",,Computer Vision,"unsupervised-person-re-identification-on-10, person-re-identification-on-ilids-vid",,http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html,https://paperswithcode.com/dataset/ilids-vid,"The iLIDS-VID dataset is a person re-identification dataset which involves 300 different pedestrians observed across two disjoint camera views in public open space. It comprises 600 image sequences of 300 distinct individuals, with one pair of image sequences from two camera views for each person. Each image sequence has variable length ranging from 23 to 192 image frames, with an average number of 73. The iLIDS-VID dataset is very challenging due to clothing similarities among people, lighting and viewpoint variations across camera views, cluttered background and random occlusions.",,,,,,
1489,Illness-dataset,Active Learning,Active Learning,"Active Learning, Domain Adaptation, Text Classification","Image, Text",English,Computer Vision,,MIT,https://github.com/p-karisani/illness-dataset,https://paperswithcode.com/dataset/illness-dataset,"A dataset for evaluating text classification, domain adaptation, and active learning models. The dataset consists of 22,660 documents (tweets) collected in 2018 and 2019. It spans across four domains: Alzheimer's, Parkinson's, Cancer, and Diabetes.",2018,,,660 documents,,
1490,ILPC22-Large,Inductive Link Prediction,Inductive Link Prediction,Inductive Link Prediction,Time Series,,Methodology,inductive-link-prediction-on-ilpc22-large,CC0,https://github.com/pykeen/ilpc2022,https://paperswithcode.com/dataset/ilpc22-large,"A large dataset from the Inductive Link Prediction Challenge 2022. 
Training graph contains 46K entities, 130 relations, 202K triples. 
Inference graph contains 30K entities, 130 relations, 77K triples.
Validation and test triples to predict belong to the inference graph.",2022,,,,,
1491,ILPC22-Small,Inductive Link Prediction,Inductive Link Prediction,Inductive Link Prediction,Time Series,,Methodology,inductive-link-prediction-on-ilpc22-small,CC0,https://github.com/pykeen/ilpc2022,https://paperswithcode.com/dataset/ilpc22-small,"A small dataset from the Inductive Link Prediction Challenge 2022. 
Training graph contains 10K entities, 96 relations, 78K triples. 
Inference graph contains 7K entities, 96 relations, 21K triples.
Validation and test triples to predict belong to the inference graph.",2022,,,,,
1492,iLur_News_Texts,Morphological Tagging,Morphological Tagging,"Morphological Tagging, Word Embeddings, Text Classification","Image, Text",English,Computer Vision,,,https://github.com/ispras-texterra/word-embeddings-eval-hy,https://paperswithcode.com/dataset/ilur-news-texts,"iLur News Texts is a dataset of over 12000 news articles from iLur.am, categorized into 7 classes: sport, politics, weather, economy, accidents, art, society. The articles are split into train (2242k tokens) and test sets (425k tokens).",,,,,,7
1493,IM-SportingBehaviors,Sports Activity Recognition,Sports Activity Recognition,Sports Activity Recognition,"Image, Video",,Computer Vision,sports-activity-recognition-on-im,,,https://paperswithcode.com/dataset/im-sportingbehaviors,"IM-SportingBehaviors Dataset
Dataset Overview
The IM-SportingBehaviors dataset, developed by researchers at Air University Pakistan, provides detailed motion data from participants engaged in various sports activities. This dataset captures human movement through triaxial accelerometers attached to multiple parts of the body, specifically the knee, wrist, and below-neck areas. The dataset includes motion data from six sports: cycling, badminton, skipping, basketball, football, and table tennis, with participants comprising both professional and amateur athletes aged between 20 and 30 years, weighing between 60 and 100 kilograms.

Dataset Characteristics

Total Samples: 62,500
Sports Activities: Cycling, badminton, skipping, basketball, football, and table tennis
Data Imbalance: The dataset is imbalanced, with the ‘Skipping’ class as the most represented (22.4%) and ‘Football’ as the least represented (12.3%).
Sensors Used: Triaxial accelerometers providing 9-dimensional data from three sensor placements; however, only 6-dimensional data is utilized in analyses, corresponding to two accelerometers.
Data Distribution:
Skipping: 22.4%
Cycling: 19%
Table Tennis: 16.8%
Badminton: 16%
Basketball: 13.4%
Football: 12.3%

Motivation and Summary of Content
The IM-SportingBehaviors dataset aims to enhance the understanding of human mobility patterns and biomechanics across a range of sports. By collecting and analyzing motion data through wearable accelerometers, researchers can gain insights into activity-specific movement dynamics and variability in performance between professional and amateur athletes. This dataset's detailed sensor data captures essential aspects of motion and posture, which are crucial for applications in sports science, human-computer interaction, and motion analysis.

Potential Use Cases
The IM-SportingBehaviors dataset is valuable for:
1. Activity Recognition: Developing and validating algorithms to classify sports activities based on sensor data.
2. Performance Analysis: Analyzing movement efficiency and style differences between professional and amateur athletes.
3. Biomechanics Research: Studying human movement patterns, energy expenditure, and motion efficiency in various sports.
4. Wearable Device Optimization: Improving the accuracy of wearable fitness trackers in recognizing and categorizing sports-specific movements.
5. Injury Prevention: Identifying movement patterns or deviations that could contribute to sports-related injuries.

This dataset provides a robust foundation for advancing both theoretical and applied research in fields that depend on accurate motion and activity classification.",,,,,,
1494,Im2GPS,Photo geolocation estimation,Photo geolocation estimation,Photo geolocation estimation,,,Methodology,photo-geolocation-estimation-on-im2gps,,http://graphics.cs.cmu.edu/projects/im2gps/,https://paperswithcode.com/dataset/im2gps,Dataset of over 6 million GPS-tagged images from Flickr. Training dataset is private. Test dataset is composed by 237 images.,,,,237 images,Training dataset is private. Test dataset is composed by 237 images,
1495,Image-Chat,Text Retrieval,Text Retrieval,"Text Retrieval, Visual Dialog","Image, Text",English,Computer Vision,"visual-dialog-on-image-chat, text-retrieval-on-image-chat",,http://parl.ai/projects/image_chat,https://paperswithcode.com/dataset/image-chat,"The IMAGE-CHAT dataset is a large collection of (image, style trait for speaker A, style trait for speaker B, dialogue between A & B) tuples that we collected using crowd-workers, Each dialogue consists of consecutive turns by speaker A and B. No particular constraints are placed on the kinds of utterance, only that we ask the speakers to both use the provided style trait, and to respond to the given image and dialogue history in an engaging way. The goal is not just to build a diagnostic dataset but a basis for training models that humans actually want to engage with.",,,,,,
1496,ImageCoDe,Video Understanding,Video Understanding,"Video Understanding, Natural Language Visual Grounding, Image Retrieval","Image, Text, Video",English,Computer Vision,image-retrieval-on-imagecode,,https://mcgill-nlp.github.io/imagecode/,https://paperswithcode.com/dataset/imagecode,"Given 10 minimally contrastive (highly similar) images and a complex description for one of them, the task is to retrieve the correct image.
The source of most images are videos and descriptions as well as retrievals come from human.",,,,,,
1497,ImageNet-1k_vs_NINCO,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Out of Distribution (OOD) Detection, Out-of-Distribution Detection, Open Set Learning",Image,English,Computer Vision,out-of-distribution-detection-on-imagenet-1k-13,Creative Commons Attribution 4.0 International,https://github.com/j-cb/NINCO,https://paperswithcode.com/dataset/imagenet-1k-vs-ninco,"The NINCO (No ImageNet Class Objects) dataset is introduced in the ICML 2023 paper In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation. The images in this dataset are free from objects that belong to any of the 1000 classes of ImageNet-1K (ILSVRC2012), which makes NINCO suitable for evaluating out-of-distribution detection on ImageNet-1K .

The NINCO main dataset consists of 64 OOD classes with a total of 5879 samples. These OOD classes were selected to have no categorical overlap with any classes of ImageNet-1K. Each sample was inspected individually by the authors to not contain ID objects.

Besides NINCO, included are (in the same .tar.gz file) truly OOD versions of 11 popular OOD datasets with in total 2715 OOD samples.

Further included are 17 OOD unit-tests, with 400 samples each.

Code for loading and evaluating on each of the three datasets is provided at https://github.com/j-cb/NINCO.

When using NINCO, please consider citing (besides the bibtex given below) the following data sources that were used to create NINCO:

Hendrycks et al.: ”Scaling out-of-distribution detection for real-world settings”, ICML, 2022.  
Bossard et al.: ”Food-101 – mining discriminative components with random forests”, ECCV 2014.  
Zhou et al.: ”Places: A 10 million image database for scene recognition”, IEEE PAMI 2017.  
Huang et al.: ”Mos: Towards scaling out-of-distribution detection for large semantic space”, CVPR 2021.  
Li et al.: ”Caltech 101 (1.0)”, 2022.
Ismail et al.: ”MYNursingHome: A fully-labelled image dataset for indoor object classification.”, Data in Brief (V. 32) 2020.
The iNaturalist project: https://www.inaturalist.org/

When using NINCO_popular_datasets_subsamples, additionally to the above, please consider citing:

Cimpoi et al.: ”Describing textures in the wild”, CVPR 2014.  
Hendrycks et al.: ”Natural adversarial examples”, CVPR 2021.  
Wang et al.: ”Vim: Out-of-distribution with virtual-logit matching”, CVPR 2022.  
Bendale et al.: ”Towards Open Set Deep Networks”, CVPR 2016.  
Vaze et al.: ”Open-set Recognition: a Good Closed-set Classifier is All You Need?”, ICLR 2022.  
Wang et al.: ”Partial and Asymmetric Contrastive Learning for Out-of-Distribution Detection in Long-Tailed Recognition.” ICML, 2022.  
Galil et al.: “A framework for benchmarking Class-out-of-distribution detection and its application to ImageNet”, ICLR 2023.",2023,,,5879 samples,"tests, with 400 samples",1000
1498,ImageNet-32,Sparse Learning,Sparse Learning,"Sparse Learning, Image Classification, Image Compression, Learning with coarse labels",Image,English,Computer Vision,"sparse-learning-on-imagenet32-1, image-compression-on-imagenet32, learning-with-coarse-labels-on-imagenet32, image-classification-on-imagenet-32",,https://github.com/PatrykChrabaszcz/Imagenet32_Scripts,https://paperswithcode.com/dataset/imagenet-32,"Imagenet32 is a huge dataset made up of small images called the down-sampled version of Imagenet. Imagenet32 is composed of 1,281,167 training data and 50,000 test data with 1,000 labels.",,Self-supervised Knowledge Distillation Using Singular Value Decomposition,https://arxiv.org/abs/1807.06819,,,0
1499,ImageNet-A,Domain Generalization,Domain Generalization,"Domain Generalization, Unsupervised Domain Adaptation, Adversarial Robustness, Prompt Engineering, Zero-Shot Transfer Image Classification",Image,English,Computer Vision,"domain-generalization-on-imagenet-a, prompt-engineering-on-imagenet-a, adversarial-robustness-on-imagenet-a, zero-shot-transfer-image-classification-on-5, unsupervised-domain-adaptation-on-imagenet-a",,https://github.com/hendrycks/natural-adv-examples,https://paperswithcode.com/dataset/imagenet-a,"The ImageNet-A dataset consists of real-world, unmodified, and naturally occurring examples that are misclassified by ResNet models.",,On Robustness and Transferability of Convolutional Neural Networks,https://arxiv.org/abs/2007.08558,,,
1500,ImageNet-C,Unsupervised Domain Adaptation,Unsupervised Domain Adaptation,"Unsupervised Domain Adaptation, Adversarial Robustness, Test-time Adaptation, Domain Generalization",,English,Methodology,"domain-generalization-on-imagenet-c, adversarial-robustness-on-imagenet-c, unsupervised-domain-adaptation-on-imagenet-c, test-time-adaptation-on-imagenet-c",CC BY 4.0,https://zenodo.org/record/2235448,https://paperswithcode.com/dataset/imagenet-c,"ImageNet-C is an open source data set that consists of algorithmically generated corruptions (blur, noise) applied to the ImageNet test-set.",,Selective Brain Damage: Measuring the Disparate Impact of Model Pruning,https://arxiv.org/abs/1911.05248,,,
1501,ImageNet-LT,Conditional Image Generation,Conditional Image Generation,"Conditional Image Generation, Test Agnostic Long-Tailed Learning, Long-tail learning with class descriptors, Long-tail Learning","Image, Text",English,Computer Vision,"long-tail-learning-on-imagenet-lt, test-agnostic-long-tailed-learning-on, long-tail-learning-with-class-descriptors-on-3, conditional-image-generation-on-imagenet-lt",,https://liuziwei7.github.io/projects/LongTail.html,https://paperswithcode.com/dataset/imagenet-lt,"ImageNet Long-Tailed is a subset of /dataset/imagenet dataset consisting of 115.8K images from 1000 categories, with maximally 1280 images per class and minimally 5 images per class. The additional classes of images in ImageNet-2010 are used as the open set.",2010,Large-Scale Long-Tailed Recognition in an Open World,https://arxiv.org/pdf/1904.05160v2.pdf,8K images,,1000
1502,ImageNet-O,Image Classification,Image Classification,"Image Classification, Out of Distribution (OOD) Detection, Outlier Detection",Image,English,Computer Vision,,MIT,https://github.com/hendrycks/natural-adv-examples,https://paperswithcode.com/dataset/imagenet-o,ImageNet-O consists of images from classes that are not found in the ImageNet-1k dataset. It is used to test the robustness of vision models to out-of-distribution samples. It's reported using the AUPR metric.,,,,,,
1503,ImageNet-P,Image Classification,Image Classification,"Image Classification, Adversarial Attack, Domain Generalization",Image,English,Computer Vision,image-classification-on-imagenet-p,,https://github.com/hendrycks/robustness,https://paperswithcode.com/dataset/imagenet-p,"ImageNet-P consists of noise, blur, weather, and digital distortions. The dataset has validation perturbations; has difficulty levels; has CIFAR-10, Tiny ImageNet, ImageNet 64 × 64, standard, and Inception-sized editions; and has been designed for benchmarking not training networks. ImageNet-P departs from ImageNet-C by having perturbation sequences generated from each ImageNet validation image. Each sequence contains more than 30 frames, so to counteract an increase in dataset size and evaluation time only 10 common perturbations are used.",,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,https://arxiv.org/pdf/1903.12261.pdf,,,
1504,ImageNet-Patch,Adversarial Robustness,Adversarial Robustness,"Adversarial Robustness, Image Classification",Image,English,Adversarial,,,https://github.com/pralab/ImageNet-Patch,https://paperswithcode.com/dataset/imagenet-patch,"ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches

Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches. It consists of a set of patches, optimized to generalize across different models, and readily applicable to ImageNet data after preprocessing them with affine transformations. This process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations.",,,,,,
1505,ImageNet-R,Zero-Shot Composed Image Retrieval (ZS-CIR),Zero-Shot Composed Image Retrieval (ZS-CIR),"Zero-Shot Composed Image Retrieval (ZS-CIR), Zero-shot Image Retrieval, Domain Generalization, Unsupervised Domain Adaptation, Prompt Engineering, Zero-Shot Transfer Image Classification",Image,English,Computer Vision,"zero-shot-image-retrieval-on-imagenet-r, zero-shot-composed-image-retrieval-zs-cir-on-6, domain-generalization-on-imagenet-r, zero-shot-transfer-image-classification-on-4, unsupervised-domain-adaptation-on-imagenet-r, prompt-engineering-on-imagenet-r",,https://github.com/hendrycks/imagenet-r,https://paperswithcode.com/dataset/imagenet-r,"ImageNet-R(endition) contains art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game renditions of ImageNet classes.

ImageNet-R has renditions of 200 ImageNet classes resulting in 30,000 images.",,,,000 images,,
1506,ImageNet-Sketch,Data Augmentation,Data Augmentation,"Data Augmentation, Image Classification, Domain Generalization, Zero-Shot Transfer Image Classification, Domain Adaptation",Image,English,Computer Vision,"domain-generalization-on-imagenet-sketch, zero-shot-transfer-image-classification-on-8, image-classification-on-imagenet-sketch",,https://github.com/HaohanWang/ImageNet-Sketch,https://paperswithcode.com/dataset/imagenet-sketch,"ImageNet-Sketch data set consists of 50,889 images,  approximately 50 images for each of the 1000 ImageNet classes. The data set is constructed with Google Image queries ""sketch of "", where  is the standard class name. Only within the ""black and white"" color scheme is searched. 100 images are initially queried for every class, and the pulled images are cleaned by deleting the irrelevant images and images that are for similar but different classes. For some classes, there are less than 50 images after manually cleaning, and then the data set is augmented by flipping and rotating the images.",,,,889 images,,
1507,ImageNet,Color Image Denoising,Color Image Denoising,"Color Image Denoising, Object Recognition, Sparse Learning, Neural Architecture Search, Data Augmentation, Weakly-Supervised Object Localization, Contrastive Learning, Image Clustering, Prompt Engineering, Unsupervised Image Classification, Visual Question Answering (VQA), Transductive Zero-Shot Classification, Network Pruning, Image Classification, Image Compressed Sensing, Quantization, Model Compression, Few-Shot Image Classification, Semi-Supervised Image Classification, Classification, Feature Upsampling, Image Super-Resolution, Image Classification with Differential Privacy, Image Colorization, Zero-Shot Composed Image Retrieval (ZS-CIR), Weakly Supervised Object Detection, Image Inpainting, Binarization, Adversarial Defense, Zero-Shot Transfer Image Classification (CN), Image Generation, JPEG Decompression, Zero-Shot Learning, Self-Supervised Image Classification, Classification with Binary Neural Network, Knowledge Distillation, Adversarial Robustness, Image Reconstruction, Zero-Shot Transfer Image Classification, Few-Shot Learning, Image Deblurring, Medical Image Classification","3D, Graph, Image, Text",English,Computer Vision,"few-shot-image-classification-on-imagenet-1-1, color-image-denoising-on-imagenet-sigma100, zero-shot-transfer-image-classification-on-3, few-shot-image-classification-on-imagenet-5, adversarial-defense-on-imagenet, image-classification-on-imagenet-v2, quantization-on-imagenet, feature-upsampling-on-imagenet, semi-supervised-image-classification-on-1, self-supervised-image-classification-on-1, semi-supervised-image-classification-on-16, neural-architecture-search-on-imagenet, sparse-learning-on-imagenet, color-image-denoising-on-imagenet-sigma150, visual-question-answering-vqa-on-imagenet, data-augmentation-on-imagenet, binarization-on-imagenet, knowledge-distillation-on-imagenet, color-image-denoising-on-imagenet-sigma250, unsupervised-image-classification-on-imagenet, image-colorization-on-imagenet, classification-with-binary-neural-network-on-1, image-compressed-sensing-on-imagenet, image-clustering-on-imagenet-1k, image-reconstruction-on-imagenet, image-classification-with-dp-on-imagenet, few-shot-image-classification-on-imagenet-0, image-classification-on-imagenet-1k, model-compression-on-imagenet, weakly-supervised-object-detection-on, image-deblurring-on-imagenet, zero-shot-transfer-image-classification-cn-on, transductive-zero-shot-classification-on, image-classification-on-imagenet, color-image-denoising-on-imagenet-sigma200, prompt-engineering-on-imagenet-v2, adversarial-robustness-on-imagenet, image-super-resolution-on-imagenet, image-clustering-on-imagenet, zero-shot-learning-on-imagenet, color-image-denoising-on-imagenet-sigma50, weakly-supervised-object-localization-on-2, jpeg-decompression-on-imagenet, zero-shot-transfer-image-classification-on-1, self-supervised-image-classification-on, few-shot-image-classification-on-imagenet-10, semi-supervised-image-classification-on-2, contrastive-learning-on-imagenet-1k, classification-on-imagenet-1k, image-inpainting-on-imagenet, zero-shot-composed-image-retrieval-zs-cir-on-5, network-pruning-on-imagenet, prompt-engineering-on-imagenet, medical-image-classification-on-imagenet","Custom (research, non-commercial)",https://image-net.org/index.php,https://paperswithcode.com/dataset/imagenet,"The ImageNet dataset contains 14,197,122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in image classification and object detection.
The publicly released dataset contains a set of manually annotated training images. A set of test images is also released, with the manual annotations withheld.
ILSVRC annotations fall into one of two categories: (1) image-level annotation of a binary label for the presence or absence of an object class in the image, e.g., “there are cars in this image” but “there are no tigers,” and (2) object-level annotation of a tight bounding box and class label around an object instance in the image, e.g., “there is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels”.
The ImageNet project does not own the copyright of the images, therefore only thumbnails and URLs of images are provided.


Total number of non-empty WordNet synsets: 21841
Total number of images: 14197122
Number of images with bounding box annotations: 1,034,908
Number of synsets with SIFT features: 1000
Number of images with SIFT features: 1.2 million",2010,ImageNet Large Scale Visual Recognition Challenge,https://arxiv.org/abs/1409.0575,,,
1508,ImageNet_CN,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Zero-Shot Cross-Modal Retrieval, Zero-shot Classification (unified classes)",Image,English,Computer Vision,zero-shot-learning-on-imagenet-cn,,https://github.com/OFA-Sys/Chinese-CLIP/blob/master/zeroshot_dataset.md,https://paperswithcode.com/dataset/imagenet-cn,transform the ImageNet-1K classification datatset  for Chinese models by translating labels and prompts into Chinese.,,,,,,
1509,Image_Caption_Quality_Dataset,Text Generation,Text Generation,"Text Generation, Image Captioning, Model Selection","Image, Text",English,Computer Vision,,,https://github.com/google-research-datasets/Image-Caption-Quality-Dataset,https://paperswithcode.com/dataset/image-caption-quality-dataset,Image Caption Quality Dataset is a dataset of crowdsourced ratings for machine-generated image captions. It contains more than 600k ratings of image-caption pairs.,,https://arxiv.org/abs/1909.03396,https://arxiv.org/abs/1909.03396,,,
1510,Image_Paragraph_Captioning,Image Captioning,Image Captioning,"Image Captioning, Image Paragraph Captioning, Text Generation, Zero-Shot Image Paragraph Captioning, Question Answering","Image, Text",English,Computer Vision,"image-paragraph-captioning-on-image-paragraph, zero-shot-image-paragraph-captioning-on-image",Custom,https://cs.stanford.edu/people/ranjaykrishna/im2p/index.html,https://paperswithcode.com/dataset/image-paragraph-captioning,"The Image Paragraph Captioning dataset allows researchers to benchmark their progress in generating paragraphs that tell a story about an image. The dataset contains 19,561 images from the Visual Genome dataset. Each image contains one paragraph. The training/val/test sets contains 14,575/2,487/2,489 images.

Since all the images are also part of the Visual Genome dataset, each image also contains 50 region descriptions (short phrases describing parts of an image), 35 objects, 26 attributes and 21 relationships and 17 question-answer pairs.",,,,561 images,"training/val/test sets contains 14,575/2,487/2,489 images",
1511,Imbalanced-MiniKinetics200,imbalanced classification,imbalanced classification,"imbalanced classification, Video Recognition, Long-tail Learning","Image, Video",,Computer Vision,,,https://github.com/wjun0830/MOVE,https://paperswithcode.com/dataset/imbalanced-minikinetics200,"Imbalanced-MiniKinetics200 was proposed by ""Minority-Oriented Vicinity Expansion with Attentive Aggregation for Video Long-Tailed Recognition"" to evaluate varying scenarios of video long-tailed recognition. Similar to CIFAR-10/100-LT, it utilizes an imbalance factor to construct long-tailed variants of the MiniKinetics200 dataset. Imbalanced-MiniKinetics200 is a subset of Mini-Kinetics-200 consisting of 200 categories which is also a subset of Kinetics400.
Both the raw frames and extracted features with ResNet50/101 are provided.",,,,,,200
1512,IMCPT-SparseGM-100,Stereo Matching,Stereo Matching,"Stereo Matching, Graph Matching, Graph Mining","3D, Graph",,Methodology,graph-matching-on-imcpt-sparsegm-100,,https://github.com/Thinklab-SJTU/IMCPT-SparseGM-dataset,https://paperswithcode.com/dataset/imcpt-sparsegm-100,"IMCPT-SparseGM dataset is a new visual graph matching benchmark addressing partial matching and graphs with larger sizes, based on the novel stereo benchmark Image Matching Challenge PhotoTourism  (IMC-PT)  2020. This dataset is released in CVPR 2023 paper Deep Learning of Partial Graph Matching via Differentiable Top-K.

| # images | # classes | avg # nodes | avg # edges | # universe | partial rate |
| ------------ | ------------- | --------------- | ----------- | -------------- | ---------------- |
| 25765        | 16            | 44.48           | 123.99      | 100            | 55.5%            |",2020,,,,,
1513,IMCPT-SparseGM-50,Stereo Matching,Stereo Matching,"Stereo Matching, Graph Matching","3D, Graph",,Methodology,graph-matching-on-imcpt-sparsegm-50,,https://github.com/Thinklab-SJTU/IMCPT-SparseGM-dataset,https://paperswithcode.com/dataset/imcpt-sparsegm,"IMCPT-SparseGM dataset is a new visual graph matching benchmark addressing partial matching and graphs with larger sizes, based on the novel stereo benchmark Image Matching Challenge PhotoTourism  (IMC-PT)  2020. This dataset is released in CVPR 2023 paper Deep Learning of Partial Graph Matching via Differentiable Top-K.

| # images | # classes | avg # nodes | avg # edges | # universe | partial rate |
| ------------ | ------------- | --------------- | ----------- | -------------- | ---------------- |
| 25765        | 16            | 21.36           | 54.71       | 50             | 57.3%            |",2020,,,,,
1514,IMDB-BINARY,Graph Classification,Graph Classification,"Graph Classification, Graph Representation Learning","Graph, Image",,Computer Vision,"graph-classification-on-imdb-b, graph-classification-on-imdb-binary",,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,https://paperswithcode.com/dataset/imdb-binary,"IMDB-BINARY is a movie collaboration dataset that consists of the ego-networks of 1,000 actors/actresses who played roles in movies in IMDB. In each graph, nodes represent actors/actress, and there is an edge between them if they appear in the same movie. These graphs are derived from the Action and Romance genres.",,A simple yet effective baseline for non-attributed graph classification,https://arxiv.org/abs/1811.03508,,,
1515,IMDB-Clean,Age And Gender Classification,Age And Gender Classification,"Age And Gender Classification, Face Recognition, Gender Prediction, Age Estimation, Gender Bias Detection, Facial Attribute Classification","Image, Time Series",,Computer Vision,age-estimation-on-imdb-clean,MIT,https://github.com/yiminglin-ai/imdb-clean,https://paperswithcode.com/dataset/imdb-clean,"We have cleaned the noisy IMDB-WIKI dataset using a constrained clustering method, resulting this new benchmark for in-the-wild age estimation. The annotations also allow this dataset to use for some other tasks, like gender classification and face recognition/verification. For more details, please refer to our FPAge paper.",,,,,,
1516,IMDB-MULTI,Graph Classification,Graph Classification,"Graph Classification, Document Classification, Graph Similarity","Graph, Image, Text",English,Computer Vision,"document-classification-on-imdb-m, graph-classification-on-imdb-m",,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,https://paperswithcode.com/dataset/imdb-multi,"IMDB-MULTI is a relational dataset that consists of a network of 1000 actors or actresses who played roles in movies in IMDB. A node represents an actor or actress, and an edge connects two nodes when they appear in the same movie. In IMDB-MULTI, the edges are collected from three different genres: Comedy, Romance and Sci-Fi.",,Learning metrics for persistence-based summaries and applications for graph classification,https://arxiv.org/abs/1904.12189,,,
1517,IMDb_Movie_Reviews,Language Modelling,Language Modelling,"Language Modelling, Opinion Mining, Sentiment Analysis, Paraphrase Identification, Graph Similarity, Node Clustering, Link Prediction, SQL Parsing, Text Classification","Graph, Image, Text, Time Series",English,Computer Vision,"graph-similarity-on-imdb, opinion-mining-on-imdb-movie-reviews, link-prediction-on-imdb, text-classification-on-imdb, node-clustering-on-imdb, paraphrase-identification-on-imdb, sql-parsing-on-imdb, sentiment-analysis-on-user-and-product, text-classification-on-imdb-movie-reviews-1, sentiment-analysis-on-imdb-movie-reviews-1, sentiment-analysis-on-imdb",,https://ai.stanford.edu/~amaas/data/sentiment/,https://paperswithcode.com/dataset/imdb-movie-reviews,"The IMDb Movie Reviews dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie. The dataset contains additional unlabeled data.",,,,,,
1518,IMDB__Heterogeneous_Node_Classification_,Heterogeneous Node Classification,Heterogeneous Node Classification,Heterogeneous Node Classification,Image,,Computer Vision,heterogeneous-node-classification-on-imdb,,,https://paperswithcode.com/dataset/imdb-heterogeneous-node-classification,A popular dataset for node classification on heterogeneous graphs.,,,,,,
1519,IMPACT_Patent,Cross-Modal Retrieval,Cross-Modal Retrieval,"Cross-Modal Retrieval, Image Classification, Image Retrieval, Patent classification, Zero-Shot Cross-Modal Retrieval, Image-text Retrieval","Image, Text",English,Computer Vision,zero-shot-cross-modal-retrieval-on-impact,Creative Commons Attribution Share Alike 4.0,https://github.com/AI4Patents/IMPACT,https://paperswithcode.com/dataset/impact-patent,"It is a large-scale multimodal patent dataset with detailed captions for design patent figures.

💥 Our dataset includes half a million design patents comprising 3.61 million figures along with captions from patents granted by the United States Patent and Trademark Office USPTO over a 16-year period from 2007 to 2022.


Multimodality: We introduce a multimodal patent dataset that includes patent images, metadata, and detailed captions to support a variety of NLP, vision, and multimodal tasks. This dataset is also valuable for patent analysis tasks such as classification, retrieval, prior art searches, and design trend analysis.
Comprehensive dataset: We have compiled a collection of 435,101 patents spanning 16 years from U.S. design patent documents. This extensive collection includes a total of 3,609,805 drawing figures. Additionally, our dataset consists of eleven fields such as the title, patent ID, claims, date of publication, classification code, and extensive image-related information, including the number of images per patent and descriptions of the viewpoints.
Descriptive captions: To address the absence of descriptions about the designs, such as features and shapes, we generate elaborated captions by employing a vision-language model. It generates descriptive captions for the design figures, capturing details from the sketch. These captions, coupled with the images, enrich our dataset and becomes a valuable resource for advanced patent analysis and multimodal research applications.",2007,,,,,
1520,Implicit_Hate,Hate Speech Detection,Hate Speech Detection,Hate Speech Detection,"Audio, Image",,Speech,,,https://github.com/gt-salt/implicit-hate,https://paperswithcode.com/dataset/implicit-hate,"The Implicit Hate corpus is a dataset for hate speech detection with fine-grained labels for each message and its implication. This dataset contains 22,056 tweets from the most prominent extremist groups in the United States; 6,346 of these tweets contain implicit hate speech.",,,,,,
1521,IMS_Bearing_Dataset,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Remaining Useful Lifetime Estimation, Fault Detection",Image,,Computer Vision,,,https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#bearing,https://paperswithcode.com/dataset/ims-bearing-dataset,"Bearing acceleration data from three run-to-failure experiments on a loaded shaft. The data set was provided by the Center for Intelligent Maintenance Systems (IMS), University of Cincinnati.",,,,,,
1522,In-Shop,Metric Learning,Metric Learning,"Metric Learning, Image Retrieval",Image,,Computer Vision,"metric-learning-on-in-shop-1, image-retrieval-on-in-shop",,https://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html,https://paperswithcode.com/dataset/in-shop,"In-shop Clothes Retrieval Benchmark evaluates the performance of in-shop Clothes Retrieval. This is a large subset of DeepFashion, containing large pose and scale variations. It also has large diversities, large quantities, and rich annotations, including:


7,982 number of clothing items;
52,712 number of in-shop clothes images, and ~200,000 cross-pose/scale pairs;

Each image is annotated by bounding box, clothing type and pose type.",,,,,,
1523,IN2LAAMA,Simultaneous Localization and Mapping,Simultaneous Localization and Mapping,"Simultaneous Localization and Mapping, Autonomous Vehicles, Autonomous Navigation",Image,,Computer Vision,,,https://github.com/UTS-CAS/in2laama_datasets,https://paperswithcode.com/dataset/in2laama,IN2LAAMA is a set of lidar-inertial datasets collected with a Velodyne VLP-16 lidar and a Xsens MTi-3 IMU.,,,,,,
1524,inaGVAD,Gender Bias Detection,Gender Bias Detection,"Gender Bias Detection, Gender Prediction, Gender Classification","Image, Time Series",,Computer Vision,,INA GCU,https://github.com/ina-foss/InaGVAD,https://paperswithcode.com/dataset/inagvad,"InaGVAD is a Voice Activity Detection (VAD) and Speaker Gender Segmentation (SGS) dataset designed for representing the acoustic diversity of French TV and Radio programs. InaGVAD detailed description, together with a benchmark of 6 freely available VAD systems and 3 SGS systems, is provided in a paper presented in LREC-COLING 2024.

InaGVAD contains 277 1-minute-long annotated recordings, partitioned into a 1h development and 3h37 test subset, allowing fair and reproducible system evaluation. Evaluation scripts provided with the corpus provide performance estimates in the same conditions as the 6 VAD and 3 SGS systems presented in the associated paper. Recordings were collected from 10 French radio and 18 TV channels categorized into 4 groups associated to diverse acoustic conditions : generalist radio, music radio, news TV, and generalist TV.

InaGVAD provides an extended VAD and SGS annotation scheme, allowing to describe systems diverse abilities based on :
* Speaker Traits categories
 3 Genders : Female, Male, I Don't Know (IDK)
 3 Age groups : Young (prepubescent), Adult, Ederly (Senior)
* 3 Speech Qualities : standard, interjections (ah, oh, eg, aie), atypical (crying, laughing or shouted speech, ill person voice, artificially distorted voices, vocal performance, monster voice...)
10 Non-Speech event categories : Applause, environmental noise, hubbub, jingle, foreground music, background music, respiration, non-intelligible laughers, other, empty

The entire inaGVAD package; including corpus, annotations, evaluation scripts, and baseline training code; is made freely accessible, fostering future advancement in the domain.",2024,,,,,
1525,iNaturalist,Test Agnostic Long-Tailed Learning,Test Agnostic Long-Tailed Learning,"Test Agnostic Long-Tailed Learning, Image Classification, Fine-Grained Image Classification, Image Generation, Image Retrieval, Few-Shot Image Classification, Long-tail Learning","Image, Text",English,Computer Vision,"image-classification-on-inat2021-mini, image-retrieval-on-inaturalist, image-generation-on-inaturalist-2019, few-shot-image-classification-on-inaturalist-1, image-classification-on-inaturalist, long-tail-learning-on-inaturalist-2018, few-shot-image-classification-on-inaturalist, fine-grained-image-classification-on-3, few-shot-image-classification-on-inaturalist-2, few-shot-image-classification-on-inaturalist-3, test-agnostic-long-tailed-learning-on-1, image-classification-on-inaturalist-2018, image-classification-on-inaturalist-2019",Custom (non-commercial),https://github.com/visipedia/inat_comp/tree/master/2017,https://paperswithcode.com/dataset/inaturalist,"The iNaturalist 2017 dataset (iNat) contains 675,170 training and validation images from 5,089 natural fine-grained categories. Those categories belong to 13 super-categories including Plantae (Plant), Insecta (Insect), Aves (Bird), Mammalia (Mammal), and so on. The iNat dataset is highly imbalanced with dramatically different number of images per category. For example, the largest super-category “Plantae (Plant)” has 196,613 images from 2,101 categories; whereas the smallest super-category “Protozoa” only has 381 images from 4 categories.",2017,Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning,https://arxiv.org/abs/1806.06193,613 images,"training and validation images from 5,089 natural fine-grained categories. Those categories belong to 13 super-categories including Plantae (Plant), Insecta (Insect), Aves (Bird), Mammalia (Mammal), and so on. The iNat dataset is highly imbalanced with dramatically different number of images",101
1526,InBreast,Cancer-no cancer per breast classification,Cancer-no cancer per breast classification,"Cancer-no cancer per breast classification, Suspicous (BIRADS 4,5)-no suspicous (BIRADS 1,2,3) per image classification",Image,,Computer Vision,"cancer-no-cancer-per-breast-classification-on-1, suspicous-birads-45-no-suspicous-birads-123",,,https://paperswithcode.com/dataset/inbreast,"Rationale and objectives: Computer-aided detection and diagnosis (CAD) systems have been developed in the past two decades to assist radiologists in the detection and diagnosis of lesions seen on breast imaging exams, thus providing a second opinion. Mammographic databases play an important role in the development of algorithms aiming at the detection and diagnosis of mammary lesions. However, available databases often do not take into consideration all the requirements needed for research and study purposes. This article aims to present and detail a new mammographic database.

Materials and methods: Images were acquired at a breast center located in a university hospital (Centro Hospitalar de S. João [CHSJ], Breast Centre, Porto) with the permission of the Portuguese National Committee of Data Protection and Hospital's Ethics Committee. MammoNovation Siemens full-field digital mammography, with a solid-state detector of amorphous selenium was used.

Results: The new database-INbreast-has a total of 115 cases (410 images) from which 90 cases are from women with both breasts affected (four images per case) and 25 cases are from mastectomy patients (two images per case). Several types of lesions (masses, calcifications, asymmetries, and distortions) were included. Accurate contours made by specialists are also provided in XML format.

Conclusion: The strengths of the actually presented database-INbreast-relies on the fact that it was built with full-field digital mammograms (in opposition to digitized mammograms), it presents a wide variability of cases, and is made publicly available together with precise annotations. We believe that this database can be a reference for future works centered or related to breast cancer imaging.",,,,410 images,,
1527,Incremental_Dialog_Dataset,Incremental Learning,Incremental Learning,"Incremental Learning, Task-Oriented Dialogue Systems",,,Methodology,,,https://github.com/Leechikara/Incremental-Dialogue-System,https://paperswithcode.com/dataset/incremental-dialog-dataset,Simulates unanticipated user needs in the deployment stage.,,,,,,
1528,Indian_Food_Image_Dataset,Object Detection,Object Detection,"Object Detection, Scene Recognition, Food Recognition, Food recommendation",Image,,Computer Vision,,,https://www.kaggle.com/datasets/dataclusterlabs/indian-food-image-dataset,https://paperswithcode.com/dataset/indian-food-image-dataset,"This dataset is an extremely challenging set of over 5000+ original India food images captured and crowdsourced from over 800+ urban and rural areas, where each image is manually reviewed and verified by computer vision professionals at ****DC Labs.

Dataset Features

Dataset size   : 5000+
Captured by  : Over 800+ crowdsource contributors
Resolution     : 99% images HD and above (1920x1080 and above)
Location        : Captured with 800+ cities accross India
Diversity        : Various lighting conditions like day, night, varied distances, view points, dimlight etc.
Device used  : Captured using mobile phones in 2020-2021
Usage            : Indian food classification, Dish classification, Food plate detection, etc.

Available Annotation formats
COCO, YOLO, PASCAL-VOC, Tf-Record

To download full datasets or to submit a request for your dataset needs, please ping us at sales@datacluster.ai Visit www.datacluster.ai to know more.  

Note:
All the images are manually captured and verified by a large contributor base on DataCluster platform",2020,,,,,
1529,IndicGLUE,Sentiment Analysis,Sentiment Analysis,"Sentiment Analysis, Natural Language Inference, Named Entity Recognition (NER), Multiple Choice Question Answering (MCQA), News Classification","Image, Text",English,Computer Vision,"sentiment-analysis-on-iitp-movie-reviews, sentiment-analysis-on-iitp-product-reviews, multiple-choice-qa-on-indicglue-wstp-pa",,https://indicnlp.ai4bharat.org/indic-glue/,https://paperswithcode.com/dataset/indicglue,"We now introduce IndicGLUE, the Indic General
Language Understanding Evaluation Benchmark,
which is a collection of various NLP tasks as de-
scribed below. The goal is to provide an evaluation
benchmark for natural language understanding ca-
pabilities of NLP models on diverse tasks and mul-
tiple Indian languages.",,,,,,
1530,IndicTTS,Speech Synthesis - Bodo,Speech Synthesis - Bodo,"Speech Synthesis - Bodo, Speech Synthesis - Manipuri, Speech Synthesis - Tamil, Speech Synthesis - Marathi, Speech Synthesis - Telugu, Spoken language identification, Speech Synthesis - Bengali, Speech Synthesis - Rajasthani, Speech Synthesis - Gujarati, Speech Synthesis - Malayalam, Speech Synthesis - Assamese, Speech Synthesis - Kannada, Speech Synthesis - Hindi","Audio, Text",English,Speech,"speech-synthesis-telugu-on-indictts, spoken-language-identification-on-indictts, speech-synthesis-malayalam-on-indictts, speech-synthesis-marathi-on-indictts, speech-synthesis-rajasthani-on-indictts, speech-synthesis-gujarati-on-indictts, speech-synthesis-tamil-on-indictts, speech-synthesis-bodo-on-indictts, speech-synthesis-hindi-on-indictts, speech-synthesis-manipuri-on-indictts, speech-synthesis-kannada-on-indictts, speech-synthesis-assamese-on-indictts, speech-synthesis-bengali-on-indictts",,https://www.iitm.ac.in/donlab/tts/index.php,https://paperswithcode.com/dataset/indictts,A special corpus of Indian languages covering 13 major languages of India. It comprises of 10000+ spoken sentences/utterances each of mono and English recorded by both Male and Female native speakers. Speech waveform files are available in .wav format along with the corresponding text. We hope that these recordings will be useful for researchers and speech technologists working on synthesis and recognition. You can request zip archives of the entire database here.,,Unknown,https://www.iitm.ac.in/donlab/tts/downloads/license.pdf,,,
1531,IndirectRequests,Natural Language Understanding,Natural Language Understanding,"Natural Language Understanding, Slot Filling, Intent Recognition, Dialogue State Tracking","Image, Text, Video",English,Computer Vision,,,https://huggingface.co/datasets/msamogh/indirect-requests/,https://paperswithcode.com/dataset/indirectrequests,"IndirectRequests is an LLM-generated dataset of user utterances in a task-oriented dialogue setting where the user does not directly specify their preferred slot value.

IndirectRequests was generated by crowdsourcing human labels over a dataset generated using a combination of GPT-3.5 (turbo) and GPT-4. Each utterance is labelled along two dimensions:


World Understanding (the degree of world understanding it takes to understand the utterance)
Unambiguity (whether or not the generated utterance unambiguously entails a single target slot value among a set of candidate possible values).",,,,,,
1532,IndoNLU_Benchmark,Language Modelling,Language Modelling,"Language Modelling, Sentence Classification, Natural Language Understanding","Image, Text",English,Computer Vision,,,https://www.indobenchmark.com/,https://paperswithcode.com/dataset/indonlu-benchmark,"The IndoNLU benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems for Bahasa Indonesia. It is a joint venture from many Indonesia NLP enthusiasts from different institutions such as Gojek, Institut Teknologi Bandung, HKUST, Universitas Multimedia Nusantara, Prosa.ai, and Universitas Indonesia.",,,,,,
1533,Industrial_Benchmark,Active Learning,Active Learning,"Active Learning, OpenAI Gym, Decision Making",,,Methodology,,,https://github.com/siemens/industrialbenchmark,https://paperswithcode.com/dataset/industrial-benchmark,"A benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github.",,,,,,
1534,Industrial_Benchmark_Dataset_for_Customer_Escalati,Recommendation Systems,Recommendation Systems,"Recommendation Systems, imbalanced classification, Anomaly Detection, Decision Making, Classification",Image,,Computer Vision,,Creative Commons Attribution 4.0 International,https://doi.org/10.5281/zenodo.4383145,https://paperswithcode.com/dataset/industrial-benchmark-dataset-for-customer,"This is a real-world industrial benchmark dataset from a major medical device manufacturer for the prediction of customer escalations. The dataset contains features derived from IoT (machine log) and enterprise data including labels for escalation from a fleet of thousands of customers of high-end medical devices. 

The dataset accompanies the publication ""System Design for a Data-driven and Explainable Customer Sentiment Monitor"" (submitted). We provide an anonymized version of data collected over a period of two years.

The dataset should fuel the research and development of new machine learning algorithms to better cope with real-world data challenges including sparse and noisy labels, and concept drifts. Additional challenges are the optimal fusion of enterprise and log-based features for the prediction task. Thereby, the interpretability of designed prediction models should be ensured in order to have practical relevancy. 

Supporting software

Kindly use the corresponding GitHub repository (https://github.com/annguy/customer-sentiment-monitor) to design and benchmark your algorithms. 

Citation and Contact

If you use this dataset please cite the following publication:

@ARTICLE{9520354,
  author={Nguyen, An and Foerstel, Stefan and Kittler, Thomas and Kurzyukov, Andrey and Schwinn, Leo and Zanca, Dario and Hipp, Tobias and Jun, Sun Da and Schrapp, Michael and Rothgang, Eva and Eskofier, Bjoern},
  journal={IEEE Access}, 
  title={System Design for a Data-Driven and Explainable Customer Sentiment Monitor Using IoT and Enterprise Data}, 
  year={2021},
  volume={9},
  number={},
  pages={117140-117152},
  doi={10.1109/ACCESS.2021.3106791}}

If you would like to get in touch, please contact an.nguyen@fau.de.",2021,,,,,
1535,inD_Dataset,Trajectory Clustering,Trajectory Clustering,"Trajectory Clustering, Trajectory Planning, Trajectory Prediction, Trajectory Modeling, Trajectory Forecasting",Time Series,,Methodology,,Non-Commercial,https://levelxdata.com/ind-dataset/,https://paperswithcode.com/dataset/ind-dataset,"The inD dataset is a new dataset of naturalistic vehicle trajectories recorded at German intersections. Using a drone, typical limitations of established traffic data collection methods like occlusions are overcome. Traffic was recorded at four different locations. The trajectory for each road user and its type is extracted. Using state-of-the-art computer vision algorithms, the positional error is typically less than 10 centimetres. The dataset is applicable on many tasks such as road user prediction, driver modeling, scenario-based safety validation of automated driving systems or data-driven development of HAD system components.",,,,,,
1536,InfantMarmosetsVox,Caller Detection,Caller Detection,"Caller Detection, Sound Classification, Audio Classification","Audio, Image",,Computer Vision,caller-detection-on-infantmarmosetsvox,Creative Commons Attribution 4.0 International,https://zenodo.org/records/10130104,https://paperswithcode.com/dataset/infantmarmosetsvox,"InfantMarmosetsVox is a dataset for multi-class call-type and caller identification. It contains audio recordings of different individual marmosets and their call-types. The dataset contains a total of 350 files of precisely labelled 10-minute audio recordings across all caller classes. The audio was recorded from five pairs of infant marmoset twins, each recorded individually in two separate sound-proofed recording rooms at a sampling rate of 44.1 kHz. The start and end time, call-type, and marmoset identity of each vocalization are provided, labeled by an experienced researcher. A PyTorch Dataloader is included in this dataset.",,,,,,
1537,InferWiki,Knowledge Graph Completion,Knowledge Graph Completion,Knowledge Graph Completion,Graph,,Methodology,,,https://github.com/TaoMiner/inferwiki,https://paperswithcode.com/dataset/inferwiki,"InferWiki is a Knowledge Graph Completion (KGC) dataset that improves upon existing benchmarks in inferential ability, assumptions, and patterns. First, each testing sample is predictable with supportive data in the training set. Second, InferWiki initiates the evaluation following the open-world assumption and improves the inferential difficulty of the closed-world assumption, by providing manually annotated negative and unknown triples. Third, the dataset includes various inference patterns (e.g., reasoning path length and types) for comprehensive evaluation.",,,,,,
1538,InfiniteRep,3D Human Pose Tracking,3D Human Pose Tracking,"3D Human Pose Tracking, 2D Semantic Segmentation, 3D Instance Segmentation, 3D Action Recognition, Activity Prediction, Activity Recognition In Videos, 3D Human Pose Estimation, 3D Pose Estimation, Activity Detection, Action Detection, Activity Recognition, 3D Classification, 3D Absolute Human Pose Estimation, Pose Estimation, Action Classification, Pose Tracking, Pose Prediction, 3D Human Dynamics, 2D Human Pose Estimation","3D, Image, Time Series, Video",,Computer Vision,,Creative Commons Attribution 4.0 International,https://toinfinity.ai/infiniterep,https://paperswithcode.com/dataset/infiniterep,"InfiniteRep is a synthetic, open-source dataset for fitness and physical therapy (PT) applications. It includes 1k videos of diverse avatars performing multiple repetitions of common exercises. It includes significant variation in the environment, lighting conditions, avatar demographics, and movement trajectories. From cadence to kinematic trajectory, each rep is done slightly differently -- just like real humans. InfiniteRep videos are accompanied by a rich set of pixel-perfect labels and annotations, including frame-specific repetition counts.

The dataset features:  


100 videos per exercise, spanning 5 to 10 repetitions each (1,000 videos total) 
7 unique indoor scenes
Realistic environmental occlusion (+ corresponding labels)
Diverse lighting conditions 
Varied body shape, skin tones, and clothing 
Rich annotations for 2D and 3D supervision  

Exercises
The dataset currently includes the following exercises:  


Pushups  
Alternating Bicep Curls (with dumbbells)  
Delt Flys (with dumbbells)
Squats
Bird Dogs
Supermans
Bicycle Crunches
Leg Raises
Front Raises (with dumbbells)
Overhead Press (with dumbbells)

Annotations
The dataset includes the following annotations:  


Bounding boxes  
Segmentation masks  
Keypoints 
Joint angles (quaternions) 
Percent occlusion 
Avatar characteristics
Camera position 
and more 

Want depth labels? They are not included in the dataset but we can send them to you. Email us at info@toinfinity.ai. 

Download
Download the dataset: toinfinity.ai/infiniterep  
Github repo with additional documentation: https://github.com/toinfinityai/InfiniteRep

Need more data?
Infinity AI specializes in generating custom synthetic data. If you need more (or different data), drop us a line at info@toinfinity.ai (we read every email).",,,,,,
1539,Information_Extraction_from_Tables,Table annotation,Table annotation,"Table annotation, Key Information Extraction",Tabular,,Methodology,,,https://github.com/M3RG-IITD/DiSCoMaT,https://paperswithcode.com/dataset/information-extraction-from-tables,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
1540,InfoSeek,Retrieval,Retrieval,"Retrieval, Visual Question Answering (VQA), Open-Domain Question Answering","Image, Text",English,Computer Vision,"visual-question-answering-vqa-on-infoseek, retrieval-on-infoseek",Apache-2.0,https://open-vision-language.github.io/infoseek/,https://paperswithcode.com/dataset/infoseek,"In this project, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training.",,,,,,
1541,InfoTabS,Data Augmentation,Data Augmentation,"Data Augmentation, Table-based Fact Verification, Natural Language Inference","Tabular, Text",English,Natural Language Processing,,,https://infotabs.github.io/,https://paperswithcode.com/dataset/infotabs,InfoTabS comprises of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes.,,,,,,
1542,InLegalNER,NER,NER,"NER, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,ner-on-inlegalner,MIT,https://huggingface.co/datasets/opennyaiorg/InLegalNER,https://paperswithcode.com/dataset/inlegalner,InLegalNER is a corpus of 46545 annotated legal named entities mapped to 14 legal entity types. It is designed for named entity recognition in indian court judgement.,,Named Entity Recognition in Indian court judgments,https://arxiv.org/pdf/2211.03442v1.pdf,,,
1543,INRIA-Horse,Object Detection,Object Detection,"Object Detection, Edge Detection",Image,,Computer Vision,,,http://calvin-vision.net/datasets/inria-horses/,https://paperswithcode.com/dataset/inria-horse,"The INRIA-Horse dataset consists of 170 horse images and 170 images without horses. All horses in all images are annotated with a bounding-box. The main challenges it offers are clutter, intra-class shape variability, and scale changes. The horses are mostly unoccluded, taken from approximately the side viewpoint, and face the same direction.",,Dynamical And-Or Graph Learning for Object Shape Modeling and Detection,https://arxiv.org/abs/1502.00741,170 images,,
1544,INRIA_Aerial_Image_Labeling,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Contrastive Learning, Self-Supervised Learning",Image,,Computer Vision,semantic-segmentation-on-inria-aerial-image,,https://project.inria.fr/aerialimagelabeling/,https://paperswithcode.com/dataset/inria-aerial-image-labeling,The INRIA Aerial Image Labeling dataset is comprised of 360 RGB tiles of 5000×5000px with a spatial resolution of 30cm/px on 10 cities across the globe. Half of the cities are used for training and are associated to a public ground truth of building footprints. The rest of the dataset is used only for evaluation with a hidden ground truth. The dataset was constructed by combining public domain imagery and public domain official building footprints.,,Distance transform regression for spatially-aware deep semantic segmentation,https://arxiv.org/abs/1909.01671,,,
1545,INRIA_Holidays_Dataset,Content-Based Image Retrieval,Content-Based Image Retrieval,Content-Based Image Retrieval,Image,,Computer Vision,content-based-image-retrieval-on-inria-1,,http://lear.inrialpes.fr/~jegou/data.php#holidays,https://paperswithcode.com/dataset/inria-holidays-dataset,"The Holidays dataset is a set of images which mainly contains some of the authors' personal holidays photos. The remaining ones were taken on purpose to test the robustness to various attacks: rotations, viewpoint and illumination changes, blurring, etc. The dataset includes a very large variety of scene types (natural, man-made, water and fire effects, etc) and images are in high resolution. The dataset contains 500 image groups, each of which represents a distinct scene or object. The first image of each group is the query image and the correct retrieval results are the other images of the group.",,,,,"test the robustness to various attacks: rotations, viewpoint and illumination changes, blurring, etc. The dataset includes a very large variety of scene types (natural, man-made, water and fire effects, etc) and images",
1546,Inspec,Keyword Extraction,Keyword Extraction,"Keyword Extraction, Keyphrase Generation, Keyphrase Extraction",Text,English,Natural Language Processing,"keyphrase-extraction-on-inspec, keyword-extraction-on-inspec, keyphrase-generation-on-inspec",,https://doi.org/10.3115/1119355.1119383,https://paperswithcode.com/dataset/inspec,"Paper: Improved automatic keyword extraction given more linguistic knowledge
Doi: 10.3115/1119355.1119383",,,,,,
1547,INSPIRE-AVR__LUNet_subset_,Artery/Veins Retinal Vessel Segmentation,Artery/Veins Retinal Vessel Segmentation,"Artery/Veins Retinal Vessel Segmentation, Retinal Vessel Segmentation",Image,,Computer Vision,"artery-veins-retinal-vessel-segmentation-on-1, retinal-vessel-segmentation-on-inspire-avr",,https://pvbm.readthedocs.io/en/latest/index.html,https://paperswithcode.com/dataset/inspire-avr-lunet-subset,"This dataset contains 65 DFIs acquired from patients with POAG at the University of Iowa Hospitals and Clinics. DFIs were acquired using a 30° Zeiss fundus camera (Niemeijer et al 2011). The images were centered on the optic disc. The original DFIs resolution was 2392 × 2048. In order to benchmark LUNet on this dataset, the black border of the DFIs were padded to a squared resolution of 2048 × 2048 pixels and then resized to a 1444 × 1444 pixels resolution. From the resulting DFIs, 15 optic disc-centered DFIs were randomly selected to form the second external test set. No other additional metadata were provided in the open source dataset.",2011,,,,,
1548,INSTANCE,Seismic Detection,Seismic Detection,Seismic Detection,Image,,Computer Vision,,CC BY 4.0,http://www.pi.ingv.it/instance/,https://paperswithcode.com/dataset/instance,"INSTANCE is a data collection of more than 1.3 million seismic waveforms originating from a selection of about 54,000 earthquakes occurred since 2005 in Italy and surrounding regions and seismic noise recordings randomly extracted from event free time windows of the continuous waveforms archive. The purpose is to provide reference datasets useful to develop and test seismic data processing routines based on machine learning and deep learning frameworks. The primary source of this information is ISIDe (Italian Seismological Instrumental and Parametric Data-Base) for earthquakes and the Italian node of EIDA (http://eida.ingv.it) for seismic data. All the waveforms have been sized to a 120 s window, preprocessed and resampled at 100 Hz. For each trace we provide a large number of parameters as metadata, either derived from event information or computed from trace data. Associated metadata allow for the identification of the source, the station, the path travelled by seismic waves and assessment of the trace quality. The total size of the data collection is about 330 GB. Waveforms files are available either in counts or ground motion units in hdf5 format to facilitate fast access from commonly used machine learning frameworks.",2005,,,,,
1549,Instructional-DT__Instr-DT_,Discourse Parsing,Discourse Parsing,Discourse Parsing,Text,English,Natural Language Processing,discourse-parsing-on-instructional-dt-instr,,,https://paperswithcode.com/dataset/instructional-dt-instr-dt,"This discourse treebank includes annotated instructional texts originally assembled at the Information Technology Research Institute, University of Brighton. This dataset contains 176 documents with an average of 32.6 EDUs for a total of 5744 EDUs and 53,250 words.",,,,176 documents,,
1550,INS_Dataset,Shadow Detection,Shadow Detection,"Shadow Detection, Image Shadow Removal, Shadow Removal",Image,,Computer Vision,"shadow-removal-on-ins-dataset, image-shadow-removal-on-ins-dataset","Custom (research-only, non-commercial)",https://blackjoke76.github.io/Projects/OmniSR/,https://paperswithcode.com/dataset/ins-dataset,"A significant challenge in removing shadows from indoor scenes is obtaining shadow-free images. To overcome this challenge, we propose a novel rendering pipeline for generating shadowed and shadow-free images under direct and indirect illumination, and create a comprehensive synthetic dataset that contains over 30,000 image pairs, covering various object types and lighting conditions.

We implemented a direct/indirect shadow and shadow-free rendering pipeline using Blender Cycles engine, with the help of Open Shading Language (OSL). The resulting collection of shadow and shadow-free images is referred to as the “INS dataset”. The dataset includes 30,000 training and 2,000 testing images, all with a resolution of 512 × 512. The training and testing images are generated from distinct scenes with different objects and materials.",,,,,"training and 2,000 testing images",
1551,Intel_Image_Classification,Image Classification,Image Classification,"Image Classification, Image Augmentation",Image,,Computer Vision,"image-augmentation-on-intel-image, image-classification-on-intel-image",,https://www.kaggle.com/puneet6060/intel-image-classification,https://paperswithcode.com/dataset/intel-image-classification,"Context
This is image data of Natural Scenes around the world.

Content
This Data contains around 25k images of size 150x150 distributed under 6 categories.
{'buildings' -> 0,
'forest' -> 1,
'glacier' -> 2,
'mountain' -> 3,
'sea' -> 4,
'street' -> 5 }

The Train, Test and Prediction data is separated in each zip files. There are around 14k images in Train, 3k in Test and 7k in Prediction.
This data was initially published on https://datahack.analyticsvidhya.com by Intel to host a Image classification Challenge.

Acknowledgements
Thanks to https://datahack.analyticsvidhya.com for the challenge and Intel for the Data

Photo by Jan Böttinger on Unsplash

Inspiration
Want to build powerful Neural network that can classify these images with more accuracy.",,,,25k images,"Train, Test and Prediction data is separated in each zip files. There are around 14k images",6
1552,Inter-X,Motion Synthesis,Motion Synthesis,Motion Synthesis,Video,,Methodology,motion-synthesis-on-inter-x,,https://liangxuy.github.io/inter-x/,https://paperswithcode.com/dataset/inter-x,"Inter-X is a large-scale dataset containing ~11K interaction sequences, more than 8.1M frames and 34K fine-grained human textual descriptions.",,,,,,
1553,Inter4K,Multi-Frame Super-Resolution,Multi-Frame Super-Resolution,"Multi-Frame Super-Resolution, Video Super-Resolution, Video Frame Interpolation",Video,,Methodology,,Creative Commons Attribution 4.0 International,https://alexandrosstergiou.github.io/datasets/Inter4K/index.html,https://paperswithcode.com/dataset/inter4k,"A video dataset for benchmarking upsampling methods. Inter4K contains 1,000 ultra-high resolution videos with 60 frames per second (fps) from online resources. The dataset provides standardized video resolutions at ultra-high definition (UHD/4K), quad-high definition (QHD/2K), full-high definition (FHD/1080p), (standard) high definition (HD/720p), one quarter of full HD (qHD/520p) and one ninth of a full HD (nHD/360p). We use frame rates of 60, 50, 30, 24 and 15 fps for each resolution. Based on this standardization, both super-resolution and frame interpolation tests can be performed for different scaling sizes ($\times 2$, $\times 3$ and $\times 4$). In this paper, we use Inter4K to address frame upsampling and interpolation. Inter4K provides both standardized UHD resolution and 60 fps for all of videos by also containing a diverse set of 1,000 5-second videos. Differences between scenes originate from the equipment (e.g., professional 4K cameras or phones), lighting conditions, variations in movements, actions or objects. The dataset is divided into 800 videos for training, 100 videos for validation and 100 videos for testing.",,,,,,
1554,INTERACTION_Dataset,Autonomous Vehicles,Autonomous Vehicles,"Autonomous Vehicles, Trajectory Prediction, Imitation Learning, Autonomous Driving",Time Series,,Methodology,trajectory-prediction-on-interaction-dataset-2,Custom (non-commercial),https://interaction-dataset.com/,https://paperswithcode.com/dataset/interaction-dataset,"The INTERACTION dataset contains naturalistic motions of various traffic participants in a variety of highly interactive driving scenarios from different countries. The dataset can serve for many behavior-related research areas, such as 


1) intention/behavior/motion prediction, 
2) behavior cloning and imitation learning,
3) behavior analysis and modeling,
4) motion pattern and representation learning,
5) interactive behavior extraction and categorization,
6) social and human-like behavior generation,
7) decision-making and planning algorithm development and verification,
8) driving scenario/case generation, etc.",,,,,,
1555,InterHand2.6M,3D Hand Pose Estimation,3D Hand Pose Estimation,"3D Hand Pose Estimation, Hand Pose Estimation, Pose Estimation, 3D Interacting Hand Pose Estimation","3D, Image",,Computer Vision,"3d-interacting-hand-pose-estimation-on, 3d-hand-pose-estimation-on-interhand2-6m",CC-BY-NC 4.0,https://mks0601.github.io/InterHand2.6M/,https://paperswithcode.com/dataset/interhand2-6m,"The InterHand2.6M dataset is a large-scale real-captured dataset with accurate GT 3D interacting hand poses, used for 3D hand pose estimation The dataset contains 2.6M labeled single and interacting hand frames.",,InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image,https://arxiv.org/abs/2008.09309,,,
1556,InterHuman,Motion Synthesis,Motion Synthesis,Motion Synthesis,Video,,Methodology,motion-synthesis-on-interhuman,,https://github.com/tr3e/InterGen,https://paperswithcode.com/dataset/interhuman,"InterHuman is a multimodal dataset, named InterHuman. It consists of about 107M frames for diverse two-person interactions, with accurate skeletal motions and 16,756 natural language descriptions.",,InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions,https://arxiv.org/pdf/2304.05684v1.pdf,,,
1557,InteriorNet,Object Detection,Object Detection,"Object Detection, Simultaneous Localization and Mapping, Semantic Segmentation",Image,,Computer Vision,,CC BY-NC-ND 4.0,https://interiornet.org/,https://paperswithcode.com/dataset/interiornet,"InteriorNet is a RGB-D for large scale interior scene understanding and mapping. The dataset contains 20M images created by pipeline:


(A) the authors collected around 1 million CAD models provided by world-leading furniture manufacturers.
(B) based on those models, around 1,100 professional designers create around 22 million interior layouts. Most of such layouts have been used in real-world decorations.
(C) For each layout, authors generate a number of configurations to represent different random lightings and simulation of scene change over time in daily life.
(D) Authors provide an interactive simulator (ViSim) to help for creating ground truth IMU, events, as well as monocular or stereo camera trajectories including hand-drawn, random walking and neural network based realistic trajectory.
(E) All supported image sequences and ground truth.",,,,20M images,,
1558,international_faces,Bias Detection,Bias Detection,"Bias Detection, Gender Bias Detection, Face Recognition",Image,,Computer Vision,,Custom,https://www.chicagofaces.org/,https://paperswithcode.com/dataset/chicago-face-database-cfd,"""The Chicago Face Database was developed at the University of Chicago by Debbie S. Ma, Joshua Correll, and Bernd Wittenbrink. The CFD is intended for use in scientific research. It provides high-resolution, standardized photographs of male and female faces of varying ethnicity between the ages of 17-65. Extensive norming data are available for each individual model. These data include both physical attributes (e.g., face size) as well as subjective ratings by independent judges (e.g., attractiveness).

Detailed information about the construction of the database and the available norming data can be found in Ma, Correll, & Wittenbrink (2015).""",2015,,,,,
1559,InternVid,Video Understanding,Video Understanding,"Video Understanding, Video Generation","Text, Video",English,Natural Language Processing,,Apache-2.0 license,https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid,https://paperswithcode.com/dataset/internvid,"InternVid is a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodAL understanding and generation. The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words.",,,,,,
1560,Interpretable_STS,Word Embeddings,Word Embeddings,"Word Embeddings, Semantic Textual Similarity, Cross-Lingual Semantic Textual Similarity",,,Methodology,,,http://alt.qcri.org/semeval2015/task2/data/uploads/sts2015-interpretability-train.v3.tgz,https://paperswithcode.com/dataset/interpretable-sts,A dataset of sentence pairs annotated following the formalization.,,,,,,
1561,INTERSPEECH_2021_Acoustic_Echo_Cancellation_Challe,Acoustic echo cancellation,Acoustic echo cancellation,Acoustic echo cancellation,Audio,,Audio,,,https://github.com/microsoft/AEC-Challenge,https://paperswithcode.com/dataset/interspeech-2021-acoustic-echo-cancellation,"The INTERSPEECH 2021 Acoustic Echo Cancellation Challenge is intended to stimulate research in the area of acoustic echo cancellation (AEC), which is an important part of speech enhancement and still a top issue in audio communication and conferencing systems. Many recent AEC studies report reasonable performance on synthetic datasets where the train and test samples come from the same underlying distribution. However, the AEC performance often degrades significantly on real recordings. Also, most of the conventional objective metrics such as echo return loss enhancement (ERLE) and perceptual evaluation of speech quality (PESQ) do not correlate well with subjective speech quality tests in the presence of background noise and reverberation found in realistic environments. In this challenge, we open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 5,000 real audio devices and human speakers in real environments, as well as a synthetic dataset. We open source an online subjective test framework based on ITU-T P.808 for researchers to quickly test their results. The winners of this challenge will be selected based on the average P.808 Mean Opinion Score (MOS) achieved across all different single talk and double talk scenarios.",2021,,,,,
1562,IntrA,Medical Diagnosis,Medical Diagnosis,"Medical Diagnosis, 3D Point Cloud Classification, 3D Part Segmentation","3D, Image",,Medical,"3d-point-cloud-classification-on-intra, 3d-part-segmentation-on-intra",,https://github.com/intra3d2019/IntrA,https://paperswithcode.com/dataset/intra,"IntrA is an open-access 3D intracranial aneurysm dataset that makes the application of points-based and mesh-based classification and segmentation models available. This dataset can be used to diagnose intracranial aneurysms and to extract the neck for a clipping operation in medicine and other areas of deep learning, such as normal estimation and surface reconstruction.

103 3D models of entire brain vessels are collected by reconstructing scanned 2D MRA images of patients (the raw 2D MRA images are not published due to medical ethics).
1909 blood vessel segments are generated automatically from the complete models, including 1694 healthy vessel segments and 215 aneurysm segments for diagnosis.
116 aneurysm segments are divided and annotated manually by medical experts; the scale of each aneurysm segment is based on the need for a preoperative examination.
Geodesic distance matrices are computed and included for each annotated 3D segment, because the expression of the geodesic distance is more accurate than Euclidean distance according to the shape of vessels.",1909,,,,,
1563,InVar-100,Explainable Artificial Intelligence (XAI),Explainable Artificial Intelligence (XAI),"Explainable Artificial Intelligence (XAI), Image Classification, Continual Learning",Image,,Computer Vision,,Creative Commons Attribution 4.0,https://huggingface.co/datasets/vivek9chavan/InVar-100,https://paperswithcode.com/dataset/invar-100,"The Industrial Objects in Varied Contexts (InVar) Dataset was internally produced by our team and contains 100 objects in 20800 total images (208 images per class). The objects consist of common automotive, machine and robotics lab parts. Each class contains 4 sub-categories (52 images each) with different attributes and visual complexities. 

White background (D_wh): The object is against a clean white background and the object is clear, centred and in focus. 

Stationary Setup (D_st): These images are also taken against a clean background using a stationary camera setup, with uncentered objects at a constant distance. The images have lower DPI resolution with occasional cropping. 

Handheld (D_ha): These images are taken with the user holding the objects, with occasional occluding. 

Cluttered background (D_cl): These images are taken with the object placed along with other objects from the lab in the background and no occlusion.",,,,208 images,,
1564,ionosphere,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Clustering Algorithms Evaluation, Clustering Ensemble",Image,,Computer Vision,"clustering-algorithms-evaluation-on, clustering-ensemble-on-ionosphere",,http://odds.cs.stonybrook.edu/ionosphere-dataset/,https://paperswithcode.com/dataset/ionosphere,"The original ionosphere dataset from UCI machine learning repository is a binary classification dataset with dimensionality 34. There is one attribute having values all zeros, which is discarded. So the total number of dimensions are 33. The ‘bad’ class is considered as outliers class and the ‘good’ class as inliers.",,,,,,
1565,IoT-23,3D Anomaly Detection,3D Anomaly Detection,"3D Anomaly Detection, Malware Classification, Malware Detection","3D, Image",,Computer Vision,,Creative Commons Attribution 4.0 International,https://www.stratosphereips.org/datasets-iot23,https://paperswithcode.com/dataset/iot-23,"IoT-23 is a dataset of network traffic from Internet of Things (IoT) devices. It has 20 malware captures executed in IoT devices, and 3 captures for benign IoT devices traffic. It was first published in January 2020, with captures ranging from 2018 to 2019. These IoT network traffic was captured in the Stratosphere Laboratory, AIC group, FEL, CTU University, Czech Republic. Its goal is to offer a large dataset of real and labeled IoT malware infections and IoT benign traffic for researchers to develop machine learning algorithms. This dataset and its research was funded by Avast Software. The malware was allow to connect to the Internet.",2020,,,,,
1566,IoTvulCode,Vulnerability Detection,Vulnerability Detection,Vulnerability Detection,Image,,Computer Vision,,MIT,https://zenodo.org/records/10573928,https://paperswithcode.com/dataset/iotvulcode,"The dataset includes source code vulnerabilities in some of the most commonly used IoT frameworks. We introduce IoTvulCode- a novel framework consisting of a dataset-generating tool, and ML-enabled methods for the detection of source code vulnerabilities and weaknesses as well as the initial release of an IoT vulnerability dataset. Our framework contributes to improving the existing coding practices, leading to a more secure IoT infrastructure.",,,,,,
1567,IoT_Benign_and_Attack_Traces,Network Intrusion Detection,Network Intrusion Detection,Network Intrusion Detection,"Graph, Image",,Computer Vision,,,https://iotanalytics.unsw.edu.au/attack-data.html,https://paperswithcode.com/dataset/iot-benign-and-attack-traces,"IOT BENIGN AND ATTACK TRACES

Data Collected for ACM SOSR 2019
Attack & Benign Data
Instructions
Flow data contains flow counters of MUD flow, each instance in the file are collected every one minute.
Annotations contains information about the start, end time of the attack and corresponsing MUD flows that are impacted through the Attack. More information about the device and the attacker can be found in here
 Below is an example of the annotations from the Samsung smart camera.
 eg: ""1527838552,1527839153,Localfeatures|Arpfeatures,ArpSpoof100L2D""
 The above line indicates that the start time of the attack to be 1527838552 and end time is 1527839153. ""Localfeatures|Arpfeatures"" explains that it should impact the local communication and ARP protocol. ""ArpSpoof100L2D"" means that the attack was arpspoof lauched with the maximum rate of 100 packets per seconds. In order to identify the attack rows in flow stats you can use below condition.
 ""if (flowtime >= startTime*1000 and endTime*1000>=flowtime) then attack = true"" -- This corresponds to the line 4470 to 4479 in the samsung smart camera.

Cite our data
A. Hamza, H. Habibi Gharakheili, T. Benson, V. Sivaraman, ""Detecting Volumetric Attacks on IoT Devices via SDN-Based Monitoring of MUD Activity"", ACM SOSR, San Jose, California, USA, Apr 2019.

Source code
https://github.com/ayyoob/mud-ie

Contact
ayyoobhamza@student.unsw.edu.au",2019,,,,,
1568,IoT_ENVIRONMENT_DATASET,Intrusion Detection,Intrusion Detection,"Intrusion Detection, Network Intrusion Detection","Graph, Image",,Computer Vision,,,https://ocslab.hksecurity.net/Datasets/iot-environment-dataset,https://paperswithcode.com/dataset/iot-environment-dataset,"ABSTRACT
Recently, the technology of the fourth revolution has given the characteristics of things constantly expanding, and everything, including people, things, people, and the environment, is connected based on the Internet. In particular, the network structure is connected to various IoT devices and is changing from wired to wireless. Unlike users who operated each device, other devices can now be operated through gateways inside and outside the smart home. However, these changes have created an environment vulnerable to external attacks, and when an attacker accesses a gateway, he can attempt various attacks, including Port scans, OS&Service detection, and DoS attacks on IoT devices. Therefore, we disclose the dataset below to promote security research on IoT.

1. DATASET
We provide IoT environment datasets which include Port Scan, OS & Service Detection, and HTTP Flooding Attack. After setting up the environment of IoT devices, we captured packets using Wireshark.

1.1   CONFIGURATION OF IoT ENVIRONMENT
<img src=""https://lh6.googleusercontent.com/42HRjprd2muU1GuwPx7vQkC_zsb7OFG1sJLjFLnStbK5n2FhehE5Ro1725rroKpEHUR3itxQvqhlWiF5nQSVCxZISN-vviVNRL7WdEVd5dHaK6IG=w1280"" alt=""drawing"" width=""1280""/>

<p style=""text-align: center;"">Fig 1 -  CONFIGURATION OF IoT ENVIRONMENT .</p>


192.168.10.1) Router

192.168.10.2) NUGU

192.168.10.3) EZVIZ home camera

192.168.10.4) Philips Hue

192.168.10.5) Google Home MINI

192.168.10.6) TP-Link home camera

192.168.10.7) Attacker's PC (HTTP Flooding Attack)

192.168.10.10) A cell phone

192.168.10.11) A cell phone

192.168.10.12) A cell phone

192.168.10.13) A cell phone

192.168.10.30) : Attacker's PC (OS & Service Detection Attack, Port Scan Attack)

121.53.216.31) : Daum Kakao Corp.

211.188.147.64) : SK Telecom Corp.

1.2 OVERVIEW OF DATASETS


Normal (All IoT)



Duration : about 34min



Number of attacks : None



Number of packets : 125,182 packets



Description : The traffic consists of various activities of all IoT devices (NUGU, EZVIZ, Hue, Google Home Mini, TP-Link). It mainly smart speakers (NUGU, Google Home Mini) answer to questions of play music, and home cameras (EZVIZ, TP-Link) stream images to a cell phone, and smart bulb (Hue) turn on/off or control the light color of bulbs.



Normal (Google Home Mini)



Duration : about 30min



Number of attacks : None



Number of packets : 14,400 packets



Description : The traffic consists of various activities of Google Home Mini. We asked various questions and request Google Home Mini and tried to manipulate the music function through cellphone.



Port Scan Attack



Attacker : PC (192.168.10.30)



Target : Google Home Mini (192.168.10.5)



Duration : about 21sec



Number of attacks : 2 times



Number of packets : 8,866 packets



Description : The attacker did port scanning by sending TCP packets with SYN flag on.



OS & Service Detection



Attacker : PC (192.168.10.30)



Target : Google Home Mini (192.168.10.5)



Duration : about 28min



Number of attacks : 4 times



Number of packets : 96,097 packets



Description : The attacker did OS & service detection by sending TCP packets with SYN flag on. Attack intensity could be varied.



HTTP Flooding Attack



Attacker : PC (192.168.10.7)



Target : Google Home Mini (192.168.10.5 : 8008)



Duration : about 6min



Number of attacks : Consistent



Number of packets : 1,126,070 packets



Description : The traffic consists of HTTP flooding packets using Flooding attack tool(LOIC) configured as 800 threads and highest speed, so the device (Google Home Mini) stuttered or disconnected from the phone application.



1.3 COMPARISON WITH IoT NETWORK INTRUSION DATASET
This dataset has similarities with our other IoT dataset (IoT Network Intrusion Dataset), so we summarized the difference of two datasets as below.

This dataset contains traffic of more various IoT devices: two security cameras, two AI speakers and a smart light hub as described in 1.1. Configuration of IoT Environment.

IoT Network Intrusion Dataset only contains traffic of two IoT devices: SKT NUGU (NU 100) and EZVIZ Wi-Fi Camera (C2C Mini O Plus 1080P).

There are less attack scenarios in this dataset; IoT Network Intrusion Dataset includes MITM, flooding attacks besides HTTP flooding, and telnet bruteforce, which are not in this dataset.

Also, HTTP flooding attacks are in both datasets, but have different attackers and targets.

In the case of this dataset, the attacker is the laptop and the target is the IoT device (Google Home Mini).

In the case of IoT Network Intrusion Dataset, the attacker is the IoT device (assumed it is compromised by Mirai Botnet) and the target is a victim server.

2. DOWNLOADS
For academic purposes, we are happy to release our datasets. If you want to use our dataset for your experiment, please cite our dataset’s page.



Normal (All IoT)



Normal (Google Home Mini)



Port Scan Attack



OS & Service Detection Attack



HTTP Flooding Attack



If you want to download dataset, please fill out the questionnaire at the following URL.

Dataset Download Link: Download

3. CONTACT
Huy Kang Kim (cenda at korea.ac.kr)


SEE ALSO
See also our another dataset containign IoT traffic: IoT Network Intrusion Dataset",,,,,,
1569,IoT_Network_Intrusion_Dataset,Intrusion Detection,Intrusion Detection,"Intrusion Detection, Network Intrusion Detection","Graph, Image",,Computer Vision,,,https://ocslab.hksecurity.net/Datasets/iot-network-intrusion-dataset,https://paperswithcode.com/dataset/iot-network-intrusion-dataset,"1. DATASET
We created various types of network attacks in Internet of Things (IoT) environment for academic purpose. Two typical smart home devices -- SKT NUGU (NU 100) and EZVIZ Wi-Fi Camera (C2C Mini O Plus 1080P) -- were used. All devices, including some laptops or smart phones, were connected to the same wireless network. The dataset consists of 42 raw network packet files (pcap) at different time points.



The packet files are captured by using monitor mode of wireless network adapter. The wireless headers are removed by Aircrack-ng.



All attacks except Mirai Botnet category are the packets captured while simulating attacks using tools such as Nmap. The case of Mirai Botnet category, the attack packets were generated on a laptop and then manipulated to make it appear as if it originated from the IoT device.



Revision History


Three more packet files of Mirai Botnet -- Host Discovery and Telnet Bruteforce -- were added on September 20, 2019.

2. SUMMARY OF OUR DATASET
3. DOWNLOADS
You can download our dataset from IEEE dataport page: https://ieee-dataport.org/open-access/iot-network-intrusion-dataset

4. CİTATİON
Please cite our dataset's page when you use this dataset as follows. 

Hyunjae Kang, Dong Hyun Ahn, Gyung Min Lee, Jeong Do Yoo, Kyung Ho Park, and Huy Kang Kim, ""IoT Network Intrusion Dataset."", http://ocslab.hksecurity.net/Datasets/iot-network-intrusion-dataset, 2019

or 

Hyunjae Kang, Dong Hyun Ahn, Gyung Min Lee, Jeong Do Yoo, Kyung Ho Park, and Huy Kang Kim, ""IoT network intrusion dataset"", IEEE Dataport, 2019. [Online]. Available: http://dx.doi.org/10.21227/q70p-q449. Accessed: Sep. 30, 2019.

5. CONTACT
Hyunjae Kang (trifle19@korea.ac.kr) or Huy Kang Kim (cenda@korea.ac.kr)

6. SEE ALSO
See also our another dataset containing IoT traffic: IoT-Environment-Dataset

Please read [IoT-Environment-Dataset > 1.3 Comparison with IoT Network Intrusion Dataset] for more details.

cenda at korea.ac.kr  |  로봇융합관 304   |    +82-2-3290-4898",2019,,,,,
1570,IowaRain,Weather Forecasting,Weather Forecasting,Weather Forecasting,Time Series,,Methodology,,MIT,https://github.com/uihilab/IowaRain,https://paperswithcode.com/dataset/iowarain,"IowaRain is a dataset of rainfall events for the state of Iowa (2016-2019) acquired from the National Weather Service Next Generation Weather Radar (NEXRAD) system and processed by a quantitative precipitation estimation system. The dataset presented in this study could be used for better disaster monitoring, response and recovery by paving the way for both predictive and prescriptive modeling",2016,,,,,
1571,iPinYou,Click-Through Rate Prediction,Click-Through Rate Prediction,Click-Through Rate Prediction,Time Series,,Methodology,click-through-rate-prediction-on-ipinyou,,https://contest.ipinyou.com/,https://paperswithcode.com/dataset/ipinyou,"The iPinYou Global RTB(Real-Time Bidding) Bidding Algorithm Competition is organized by iPinYou from April 1st, 2013 to December 31st, 2013.The competition has been divided into three seasons. For each season, a training dataset is released to the competition participants, the testing dataset is reserved by iPinYou. The complete testing dataset is randomly divided into two parts: one part is the leaderboard testing dataset to score and rank the participating teams on the leaderboard, and the other part is reserved for the final offline evaluation. The participant's last offline submission is evaluated by the reserved testing dataset to get a team's offline final score. This dataset contains all three seasons training datasets and leaderboard testing datasets.The reserved testing datasets are withheld by iPinYou. The training dataset includes a set of processed iPinYou DSP bidding, impression, click, and conversion logs.",2013,http://contest.ipinyou.com/ipinyou-dataset.pdf,http://contest.ipinyou.com/ipinyou-dataset.pdf,,,
1572,IPM_NEL,Entity Linking,Entity Linking,"Entity Linking, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,entity-linking-on-derczynski-1,CC-BY 4.0,,https://paperswithcode.com/dataset/ipm-nel,"This data is for the task of named entity recognition and linking/disambiguation over tweets. It comprises
the addition of an entity URI layer on top of an NER-annotated tweet dataset. The task is to detect entities
and then provide a correct link to them in DBpedia, thus disambiguating otherwise ambiguous entity surface
forms; for example, this means linking ""Paris"" to the correct instance of a city named that (e.g. Paris, 
France vs. Paris, Texas).

The data concentrates on ten types of named entities: company, facility, geographic location, movie, musical
artist, person, product, sports team, TV show, and other.

The file is tab separated, in CoNLL format, with line breaks between tweets.
Data preserves the tokenisation used in the Ritter datasets.
PoS labels are not present for all tweets, but where they could be found in the Ritter
data, they're given. In cases where a URI could not be agreed, or was not present in
DBpedia, there is a NIL. See the paper for a full description of the methodology.",,,,,,
1573,IPRE,graph construction,graph construction,graph construction,Graph,,Methodology,,,https://github.com/SUDA-HLT/IPRE,https://paperswithcode.com/dataset/ipre,"A dataset for inter-personal relationship extraction which aims to facilitate information extraction and knowledge graph construction research. In total, IPRE has over 41,000 labeled sentences for 34 types of relations, including about 9,000 sentences annotated by workers.",,,,000 sentences,,
1574,IRIDIA-AF,Atrial Fibrillation Detection,Atrial Fibrillation Detection,Atrial Fibrillation Detection,Image,,Computer Vision,,CC BY 4.0,https://zenodo.org/doi/10.5281/zenodo.8186845,https://paperswithcode.com/dataset/iridia-af,"A large paroxysmal atrial fibrillation long-term electrocardiogram monitoring database
Abstract
Atrial fibrillation (AF) is the most common sustained heart arrhythmia in adults. Holter monitoring, a long-term 2-lead electrocardiogram (ECG), is a key tool available to cardiologists for AF diagnosis. Machine learning (ML) and deep learning (DL) models have shown great capacity to automatically detect AF in ECG and their use as medical decision support tool is growing. Training these models rely on a few open and annotated databases. We present a new Holter monitoring database from patients with paroxysmal AF with 167 records from 152 patients, acquired from an outpatient cardiology clinic from 2006 to 2017 in Belgium. AF episodes were manually annotated and reviewed by an expert cardiologist and a specialist cardiac nurse. Records last from 19 hours up to 95 hours, divided into 24-hour files. In total, it represents 24 million seconds of annotated Holter monitoring, sampled at 200 Hz. This dataset aims at expanding the available options for researchers and offers a valuable resource for advancing ML and DL use in the field of cardiac arrhythmia diagnosis.

References
Paper: www.nature.com/articles/s41597-023-02621-1",2006,,,167 records,,
1575,iris,Image/Document Clustering,Image/Document Clustering,"Image/Document Clustering, Clustering Algorithms Evaluation, Quantum Machine Learning, Feature Importance, General Classification, Reinforcement Learning, Incremental Constrained Clustering, Denoising","Image, Text",English,Computer Vision,"quantum-machine-learning-on-iris, feature-importance-on-iris, image-document-clustering-on-iris, denoising-on-iris, general-classification-on-iris, reinforcement-learning-on-iris, clustering-algorithms-evaluation-on-iris, incremental-constrained-clustering-on-iris",,https://archive.ics.uci.edu/ml/datasets/iris,https://paperswithcode.com/dataset/iris-1,"The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspé Peninsula ""all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus"".",1936,,,,,
1576,IS-A,Link Prediction,Link Prediction,"Link Prediction, Graph Embedding, Network Embedding","Graph, Time Series",,Methodology,,,https://github.com/SotirisKot/Content-Aware-N2V,https://paperswithcode.com/dataset/is-a,"The IS-A dataset is a dataset of relations extracted from a medical ontology. The different entities in the ontology are related by the “is a” relation. For example, ‘acute leukemia’ is a ‘leukemia’. The dataset has 294,693 nodes with 356,541 edges between them.",,https://arxiv.org/pdf/1906.05939.pdf,https://arxiv.org/pdf/1906.05939.pdf,,,
1577,ISAdetect_dataset,Code Classification,Code Classification,"Code Classification, Annotated Code Search, Code Search",Image,,Computer Vision,,CC BY 4.0,https://etsin.fairdata.fi/dataset/80fa69af-addb-4f9a-b45c-c16011bae366,https://paperswithcode.com/dataset/isadetect-dataset,"This repository holds two datasets: one with both the original binaries and the code sections extracted from them (“full dataset”), and one with only the code sections (“only code sections”). The code sections were extracted by carving out sections of the binary that were marked as executable. The binaries were scraped from Debian repositories.

There are also two CSV files available, one with full binaries and one with only code sections, which include the 293 features extracted from about 3000 binaries per architecture. These features can be used to train classifiers.

The dataset consists of thousands of binaries for the following 23 architectures: alpha, amd64, arm64, armel, armhf, hppa, i386, ia64, m68k, mips, mips64el, mipsel, powerpc, powerpcspe, powerpc64, powerpc64el, riscv, s390, s390x, sh4, sparc, sparc64 and x32.

There are 98 500 binary files, about 27 gigabytes (uncompressed) of binary files and about 15 gigabytes (uncompressed) of only code sections from those binary files.

Both datasets hold the binaries in directories named by the architecture. The files inside the folders are named as MD5 hashes of the original binary files, and a hash file ending with “.code” contains only the concatenation of all code sections of the original binary file. Each architecture folder also holds a JSON file named after the architecture, e.g. amd64 holds amd64.json. The structure of the JSON file is as follows (described in a JSON Schema-like notation)

This work is based on work by John Clemens, 2015, “Automatic classification of object code using machine learning” and De Nicolao, Pietro et al., 2018, “ELISA: ELiciting ISA of Raw Binaries for Fine-Grained Code and Data Separation”

This dataset is released as part of the following papers:

Sami Kairajärvi, Andrei Costin, and Timo Hämäläinen. 2020. ISAdetect: Usable automated detection of ISA (CPU architecture and endianness) for executable binary files and object code. In Tenth ACM Conference on Data and Application Security and Privacy (CODASPY’20), March 16–18, 2020, New Orleans, LA, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3374664.3375742

Kairajärvi, Sami, Andrei Costin, and Timo Hämäläinen. ""Towards usable automated detection of CPU architecture and endianness for arbitrary binary files and object code sequences."" arXiv preprint arXiv:1908.05459 (2019).

Kairajärvi, Sami. ""Automatic identification of architecture and endianness using binary file contents."" (2019).

The code associated with this dataset can be found at https://github.com/kairis/isadetect

Changelog: version 6 - 29.3.2020

Add Weka models

version 5 - 17.1.2020

Clean up dataset

version 4 - 13.1.2020

Initial release",2015,,,,,
1578,iSarcasm,Sentiment Analysis,Sentiment Analysis,"Sentiment Analysis, Sarcasm Detection","Image, Text",English,Computer Vision,sarcasm-detection-on-isarcasm,,https://github.com/silviu-oprea/iSarcasm,https://paperswithcode.com/dataset/isarcasm,"iSarcasm is a dataset of tweets, each labelled as either sarcastic or non_sarcastic. Each sarcastic tweet is further labelled for one of the following types of ironic speech:


sarcasm: tweets that contradict the state of affairs and are critical towards an addressee;
irony: tweets that contradict the state of affairs but are not obviously critical towards an addressee;
satire: tweets that appear to support an addressee, but contain underlying disagreement and mocking;
understatement: tweets that undermine the importance of the state of affairs they refer to;
overstatement: tweets that describe the state of affairs in obviously exaggerated terms;
rhetorical question: tweets that include a question whose invited inference (implicature) is obviously contradicting the state of affairs.

For each sarastic tweet, there's also:


an explanation, in English sentences, as to why it is sarcastic, and
a rephrase that conveys the same meaning non-sarcastically. Both have been provided by the author of the tweet.

iSarcasm contains 4,484 tweets, out of which 777 are labelled as sarcastic and 3,707 as non-sarcastic. You'll find two files, isarcasm_train.csv and isarcasm_test.csv, each containing 80% and 20% of the examples chosen at random, respectively. Each line in a file has the format tweet_id,sarcasm_label,sarcasm_type, where sarcasm_type are only defined for sarcastic tweets, as specified above.",,,,,"train.csv and isarcasm_test.csv, each containing 80% and 20% of the examples",
1579,iSarcasmEval,Sarcasm Detection,Sarcasm Detection,Sarcasm Detection,Image,,Computer Vision,,,https://github.com/iabufarha/iSarcasmEval,https://paperswithcode.com/dataset/isarcasmeval,"iSarcasmEval is the first shared task to target intended sarcasm detection: the data for this task was provided and labelled by the authors of the texts themselves. Such an approach minimises the downfalls of other methods to collect sarcasm data, which rely on distant supervision or third-party annotations. The shared task contains two languages, English and Arabic, and three subtasks: sarcasm detection, sarcasm category classification, and pairwise sarcasm identification given a sarcastic sentence and its non-sarcastic rephrase. The task received submissions from 60 different teams, with the sarcasm detection task being the most popular. Most of the participating teams utilised pre-trained language models. In this paper, we provide an overview of the task, data, and participating teams.",,,,,,
1580,ISEAR,Word Embeddings,Word Embeddings,"Word Embeddings, Emotion Recognition, Emotion Classification",Image,,Computer Vision,,CC BY-NC-SA 3.0,https://www.unige.ch/cisa/research/materials-and-online-research/research-material/,https://paperswithcode.com/dataset/isear,"Over a period of many years during the 1990s, a large group of psychologists all over the world collected data in the ISEAR project, directed by Klaus R. Scherer and Harald Wallbott. Student respondents, both psychologists and non-psychologists, were asked to report situations in which they had experienced all of 7 major emotions (joy, fear, anger, sadness, disgust, shame, and guilt). In each case, the questions covered the way they had appraised the situation and how they reacted. The final data set thus contained reports on seven emotions each by close to 3000 respondents in 37 countries on all 5 continents.",,,,,,
1581,ISIC2016,Skin Lesion Segmentation,Skin Lesion Segmentation,"Skin Lesion Segmentation, Lesion Segmentation",Image,,Computer Vision,skin-lesion-segmentation-on-isic2016,Creative Commons Attribution 4.0 International,https://challenge.isic-archive.com/data/,https://paperswithcode.com/dataset/isic2016-task-1,"Lesion segmentation data includes the original image, paired with the expert manual tracing of the lesion boundaries in the form of a binary mask. The Training Data file is a ZIP file, containing 900 dermoscopic lesion images in JPEG format. All images are named using the scheme ISIC_<image_id>.jpg, where <image_id> is a 7-digit unique identifier. EXIF tags in the images have been removed; any remaining EXIF tags should not be relied upon to provide accurate metadata. The Training Ground Truth file is a ZIP file, containing 900 binary mask images in PNG format. All masks are named using the scheme ISIC_<image_id>_Segmentation.png, where <image_id> matches the corresponding Training Data image for the mask. All mask images will have the exact same dimensions as their corresponding lesion image. Mask images are encoded as single-channel (grayscale) 8-bit PNGs (to provide lossless compression), where each pixel is either:

0: representing the background of the image or areas outside the lesion
255: representing the foreground of the image or areas inside the lesion",,,,,"Training Data file is a ZIP file, containing 900 dermoscopic lesion images",
1582,ISIC_2017_Task_1,Lesion Classification,Lesion Classification,"Lesion Classification, Lesion Segmentation",Image,,Computer Vision,,,https://challenge.isic-archive.com/landing/2017/42,https://paperswithcode.com/dataset/isic-2017-task-1,"The ISIC 2017 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 1 challenge dataset for lesion segmentation contains 2,000 images for training with ground truth segmentations (2000 binary mask images).",2017,,,000 images,training with ground truth segmentations (2000 binary mask images,
1583,ISIC_2018_Task_1,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Lesion Classification, Lesion Segmentation",Image,,Computer Vision,lesion-segmentation-on-isic-2018-task-1,,https://challenge2018.isic-archive.com/task1/,https://paperswithcode.com/dataset/isic-2018-task-1,The ISIC 2018 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. This Task 1 dataset is the challenge on lesion segmentation. It includes 2594 images.,2018,Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions,https://arxiv.org/abs/1909.00166,2594 images,,
1584,ISIC_2018_Task_3,Lesion Segmentation,Lesion Segmentation,Lesion Segmentation,Image,,Computer Vision,,,https://challenge2018.isic-archive.com/task3/,https://paperswithcode.com/dataset/isic-2018-task-3,"The ISIC 2018 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 3 dataset is the challenge on lesion classification. It includes 2594 images. The task is to classify the dermoscopic images into one of the following categories: melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis / Bowen’s disease, benign keratosis, dermatofibroma, and vascular lesion.",2018,Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions,https://arxiv.org/abs/1909.00166,2594 images,,
1585,ISIC_2019,Partial Label Learning,Partial Label Learning,"Partial Label Learning, Classification, Skin Lesion Classification",Image,,Computer Vision,"skin-lesion-classification-on-isic-2019, classification-on-isic-2019, partial-label-learning-on-isic-2019",,https://challenge.isic-archive.com/landing/2019/,https://paperswithcode.com/dataset/isic-2019,"The goal for ISIC 2019 is classify dermoscopic images among nine different diagnostic categories.25,331 images are available for training across 8 different categories. Two tasks will be available for participation: 1) classify dermoscopic images without meta-data,
and 2) classify images with additional available meta-data.",2019,,,331 images,training across 8 different categories. Two tasks will be available for participation: 1) classify dermoscopic images,
1586,ISIC_2020_Challenge_Dataset,Skin Cancer Segmentation,Skin Cancer Segmentation,"Skin Cancer Segmentation, Skin Cancer Classification, Skin Lesion Classification, Medical Image Classification",Image,,Computer Vision,medical-image-classification-on-isic-2020,Creative Commons Attribution 4.0 International,https://challenge2020.isic-archive.com/,https://paperswithcode.com/dataset/isic-2020-challenge-dataset,"The dataset contains 33,126 dermoscopic training images of unique benign and malignant skin lesions from over 2,000 patients. Each image is associated with one of these individuals using a unique patient identifier. All malignant diagnoses have been confirmed via histopathology, and benign diagnoses have been confirmed using either expert agreement, longitudinal follow-up, or histopathology. A thorough publication describing all features of this dataset is available in the form of a pre-print that has not yet undergone peer review.

The dataset was generated by the International Skin Imaging Collaboration (ISIC) and images are from the following sources: Hospital Clínic de Barcelona, Medical University of Vienna, Memorial Sloan Kettering Cancer Center, Melanoma Institute Australia, University of Queensland, and the University of Athens Medical School.

The dataset was curated for the SIIM-ISIC Melanoma Classification Challenge hosted on Kaggle during the Summer of 2020.

DOI: https://doi.org/10.34970/2020-ds01",2020,,,,,
1587,ISOD,Robot Navigation,Robot Navigation,"Robot Navigation, Texture Image Retrieval, Texture Classification, Object Detection In Indoor Scenes, Indoor Monocular Depth Estimation, 2D Semantic Segmentation","3D, Image",,Computer Vision,,CC BY-NC-SA,https://www.kaggle.com/datasets/yuchen66/indoor-small-object-dataset,https://paperswithcode.com/dataset/isod,"ISOD contains 2,000 manually labelled RGB-D images from 20 diverse sites, each featuring over 30 types of small objects randomly placed amidst the items already present in the scenes. These objects, typically ≤3cm in height, include LEGO blocks, rags, slippers, gloves, shoes, cables, crayons, chalk, glasses, smartphones (and their cases), fake banana peels, fake pet waste, and piles of toilet paper, among others. These items were chosen because they either threaten the safe operation of indoor mobile robots or create messes if run over. 

In addition to RGB images, ISOD also includes corresponding depth images and IMU readings. A reference image of each floor type was also recorded using a smartphone. 

This dataset was used as a real-world validation dataset in the original work to explore the performance of the model beyond synthetic data, specifically focusing on the potential application of real-time robot navigation.",,,,,,
1588,ISP-AD,Supervised Anomaly Detection,Supervised Anomaly Detection,"Supervised Anomaly Detection, Self-Supervised Anomaly Detection, Anomaly Detection, Defect Detection, Weakly Supervised Defect Detection, Supervised Defect Detection, Unsupervised Anomaly Detection",Image,,Computer Vision,,CC BY-NC-SA 4.0,https://doi.org/10.5281/zenodo.14911043,https://paperswithcode.com/dataset/isp-ad,"The ISP-AD Dataset is a large-scale anomaly detection dataset, representing a real-world industrial use case.                 It contains 312,674 fault-free and 246,375 defective samples, including 245,664 synthetic defects and   711 real defects collected on the factory floor.

Designed to advance research in unsupervised, self-supervised, and supervised anomaly detection,
ISP-AD serves as a benchmark for evaluating defect detection methods under realistic industrial conditions.",,,,,,
1589,ISRUC-Sleep,Automatic Sleep Stage Classification,Automatic Sleep Stage Classification,"Automatic Sleep Stage Classification, Sleep Stage Detection",Image,,Computer Vision,"sleep-stage-detection-on-isruc-sleep, automatic-sleep-stage-classification-on-isruc",,https://sleeptight.isr.uc.pt/,https://paperswithcode.com/dataset/isruc-sleep,"ISRUC-Sleep is a polysomnographic (PSG) dataset. The data were obtained from human adults, including healthy subjects, and subjects with sleep disorders under the effect of sleep medication. The dataset, which is structured to support different research objectives, comprises three groups of data: (a) data concerning 100 subjects, with one recording session per subject, (b) data gathered from 8 subjects; two recording sessions were performed per subject, which are useful for studies involving changes in the PSG signals over time, (c) data collected from one recording session related to 10 healthy subjects, which are useful for studies involving comparison of healthy subjects with the patients suffering from sleep disorders.",,,,,,
1590,ISTD,RGB Salient Object Detection,RGB Salient Object Detection,"RGB Salient Object Detection, Shadow Removal",Image,,Computer Vision,"salient-object-detection-on-istd, shadow-removal-on-istd","Custom (research-only, non-commercial)",https://github.com/DeepInsight-PCALab/ST-CGAN,https://paperswithcode.com/dataset/istd,"The Image Shadow Triplets dataset (ISTD) is a dataset for shadow understanding that contains 1870 image triplets of shadow image, shadow mask, and shadow-free image.",,"ST-CGAN: ""Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal""",https://arxiv.org/pdf/1712.02478.pdf,,,
1591,ISTD_,Shadow Removal,Shadow Removal,"Shadow Removal, 2D Semantic Segmentation",Image,,Computer Vision,shadow-removal-on-istd-1,,https://drive.google.com/file/d/1rsCSWrotVnKFUqu9A_Nw9Uf-bJq_ryOv/view,https://paperswithcode.com/dataset/istd-1,"ISTD+ consists of shadow images, shadow-free images, and shadow masks, with 1,330 training images and 540 testing images from 135 unique background scenes. ISTD suffers from color and luminosity inconsistencies between shadow and shadow-free images, which ISTD+ corrects with a color compensation mechanism to ensure uniform pixel colors across the ground-truth images.",,,,,training images and 540 testing images,
1592,Istella_LETOR,Document Ranking,Document Ranking,Document Ranking,Text,English,Natural Language Processing,,Custom,http://quickrank.isti.cnr.it/istella-dataset/,https://paperswithcode.com/dataset/istella-letor,"The Istella LETOR full dataset is composed of 33,018 queries and 220 features representing each query-document pair. It consists of 10,454,629 examples labeled with relevance judgments ranging from 0 (irrelevant) to 4 (perfectly relevant). The average number of per-query examples is 316. It has been splitted in train and test sets according to a 80%-20% scheme.",,,,629 examples,,
1593,ItaCoLA,Linguistic Acceptability,Linguistic Acceptability,Linguistic Acceptability,,,Methodology,linguistic-acceptability-on-itacola,,https://github.com/dhfbk/ItaCoLA-dataset,https://paperswithcode.com/dataset/itacola,"ItaCoLA is a corpus for monolingual and cross-lingual acceptability judgments which contains almost 10,000 sentences with acceptability judgments.",,,,000 sentences,,
1594,ITALIC,Automatic Speech Recognition,Automatic Speech Recognition,"Automatic Speech Recognition, Intent Detection, Automatic Speech Recognition (ASR)","Audio, Image, Text",English,Speech,,CC BY 4.0,https://github.com/RiTA-nlp/ITALIC/,https://paperswithcode.com/dataset/italic,"ITALIC: An ITALian Intent Classification Dataset

ITALIC is an intent classification dataset for the Italian language, which is the first of its kind. 
It includes spoken and written utterances and is annotated with 60 intents. 
The dataset is available on Zenodo and connectors ara available for the HuggingFace Hub.

Data collection
The data collection follows the MASSIVE NLU dataset which contains an annotated textual dataset for 60 intents. The data collection process is described in the paper Massive Natural Language Understanding.

Following the MASSIVE NLU dataset, a pool of 70+ volunteers has been recruited to annotate the dataset. The volunteers were asked to record their voice while reading the utterances (the original text is available on MASSIVE dataset). Together with the audio, the volunteers were asked to provide a self-annotated description of the recording conditions (e.g., background noise, recording device). The audio recordings have also been validated and, in case of errors, re-recorded by the volunteers.

All the audio recordings included in the dataset have received a validation from at least two volunteers. All the audio recordings have been validated by native italian speakers (self-annotated).",,Massive Natural Language Understanding,https://arxiv.org/abs/2204.08582,,,
1595,italki_NLI,Native Language Identification,Native Language Identification,Native Language Identification,Text,English,Natural Language Processing,native-language-identification-on-italki-nli,,https://github.com/ghomasHudson/italkiCorpus,https://paperswithcode.com/dataset/italki-nli,"A large, crowd-sourced dataset for the Native Language Identification (NLI) task. People learning English as a second language write practice Notebooks which can be used to classify the author's native language using word choice, spelling mistakes and other language features.

The dataset has:


11 languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telagu, Turkish)
111,917 documents",,,,917 documents,,
1596,ITCPR_dataset,Zero-shot Composed Person Retrieval,Zero-shot Composed Person Retrieval,"Zero-shot Composed Person Retrieval, Person Retrieval",Image,,Computer Vision,zero-shot-composed-person-retrieval-on-itcpr,CC BY-NC-SA,https://github.com/Delong-liu-bupt/Word4Per,https://paperswithcode.com/dataset/itcpr-dataset,"The ITCPR dataset is a comprehensive collection specifically designed for the Zero-Shot Composed Person Retrieval (ZS-CPR) task. It consists of a total of 2,225 annotated triplets, derived from three distinct datasets: Celeb-reID, PRCC, and LAST.",,,,,,
1597,IU_X-Ray,Image Captioning,Image Captioning,"Image Captioning, Medical Report Generation","Image, Text",English,Computer Vision,"medical-report-generation-on-iu-x-ray, image-captioning-on-iu-x-ray",,,https://paperswithcode.com/dataset/iu-x-ray,"IU X-ray (Demner-Fushman et al., 2016) is a set of chest X-ray images paired with their corresponding diagnostic reports. The dataset contains 7,470 pairs of images and reports.",2016,,,,,
1598,iV2V_and_iV2I_,Intelligent Communication,Intelligent Communication,"Intelligent Communication, Connectivity Estimation",,,Methodology,,Creative Commons Attribution,https://ieee-dataport.org/open-access/ai4mobile-industrial-wireless-datasets-iv2v-and-iv2i,https://paperswithcode.com/dataset/iv2v-and-iv2i,"This dataset provides wireless measurements from two industrial testbeds: iV2V (industrial Vehicle-to-Vehicle) and iV2I+ (industrial Vehicular-to-Infrastructure plus sensor).

iV2V covers 10h of sidelink communication scenarios between 3 Automated Guided Vehicles (AGVs), while iV2I+ was conducted for around 16h at an industrial site where an autonomous cleaning robot is connected to a private cellular network.

The data includes information on physical layer parameters (such as signal strength and signal quality), wireless Quality of Service (QoS) like delay and throughput, and positioning information.

The datasets are labelled and pre-filtered for a fast on-boarding and applicability. The common measurement methodology to both datasets pursues an application to Machine Learning (ML) for tasks such as fingerprinting, line-of-sight detection, QoS prediction or link selection, among others.",,,,,,
1599,IXI,Image Super-Resolution,Image Super-Resolution,"Image Super-Resolution, Image-to-Image Translation, MRI Reconstruction, Medical Image Registration","3D, Image, Text",English,Computer Vision,"medical-image-registration-on-ixi, image-to-image-translation-on-ixi-dataset, image-super-resolution-on-ixi, mri-reconstruction-on-ixi-dataset",,https://brain-development.org/ixi-dataset/,https://paperswithcode.com/dataset/ixi-dataset,"IXI Dataset is a collection of 600 MR brain images from normal, healthy subjects. The MR image acquisition protocol for each subject includes:


T1, T2 and PD-weighted images
MRA images
Diffusion-weighted images (15 directions)

The data has been collected at three different hospitals in London:


Hammersmith Hospital using a Philips 3T system (details of scanner parameters)
Guy’s Hospital using a Philips 1.5T system (details of scanner parameters)
Institute of Psychiatry using a GE 1.5T system (details of the scan parameters not available at the moment)

The data has been collected as part of the project:


IXI – Information eXtraction from Images (EPSRC GR/S21533/02)

The images in NIFTI format can be downloaded from here:

This data is made available under the Creative Commons CC BY-SA 3.0 license. If you use the IXI data please acknowledge the source of the IXI data.",,,,,,
1600,JAAD,Trajectory Forecasting,Trajectory Forecasting,"Trajectory Forecasting, Multi-future Trajectory Prediction, Trajectory Prediction, Pedestrian Trajectory Prediction",Time Series,,Methodology,"trajectory-prediction-on-jaad, multi-future-trajectory-prediction-on-jaad",MIT,http://data.nvision2.eecs.yorku.ca/JAAD_dataset/,https://paperswithcode.com/dataset/jaad,"JAAD is a dataset for studying joint attention in the context of autonomous driving. The focus is on pedestrian and driver behaviors at the point of crossing and factors that influence them. To this end, JAAD dataset provides a richly annotated collection of 346 short video clips (5-10 sec long) extracted from over 240 hours of driving footage. These videos filmed in several locations in North America and Eastern Europe represent scenes typical for everyday urban driving in various weather conditions.

Bounding boxes with occlusion tags are provided for all pedestrians making this dataset suitable for pedestrian detection.

Behavior annotations specify behaviors for pedestrians that interact with or require attention of the driver. For each video there are several tags (weather, locations, etc.) and timestamped behavior labels from a fixed list (e.g. stopped, walking, looking, etc.). In addition, a list of demographic attributes is provided for each pedestrian (e.g. age, gender, direction of motion, etc.) as well as a list of visible traffic scene elements (e.g. stop sign, traffic signal, etc.) for each frame.

Paper: Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior",,,,,,
1601,JAAH,Beat Tracking,Beat Tracking,"Beat Tracking, Downbeat Tracking, Chord Recognition","Image, Video",,Computer Vision,"beat-tracking-on-jaah, downbeat-tracking-on-jaah",,https://mtg.github.io/JAAH/,https://paperswithcode.com/dataset/jaah,"Eremenko, E. Demirel, B. Bozkurt, and X. Serra, “Audio-aligned jazz harmony dataset for automatic chord transcription and corpus-based research,” in Proceedings of the International Society for Music Information Retrieval Conference (ISMIR), 2018",2018,,,,,
1602,JAFFE,Image/Document Clustering,Image/Document Clustering,"Image/Document Clustering, Clustering Algorithms Evaluation, Facial Emotion Recognition, Facial Expression Recognition (FER)","Image, Text",English,Computer Vision,"facial-emotion-recognition-on-jaffe, image-document-clustering-on-jaffe-1, facial-expression-recognition-on-jaffe, clustering-algorithms-evaluation-on-jaffe",CC BY 4.0,https://zenodo.org/record/3451524,https://paperswithcode.com/dataset/jaffe,The JAFFE dataset consists of 213 images of different facial expressions from 10 different Japanese female subjects. Each subject was asked to do 7 facial expressions (6 basic facial expressions and neutral) and the images were annotated with average semantic ratings on each facial expression by 60 annotators.,,Balanced k-Means and Min-Cut Clustering,https://arxiv.org/abs/1411.6235,213 images,,
1603,JAMBO,Image Classification,Image Classification,"Image Classification, Uncertainty Quantification, Classification",Image,,Computer Vision,,,https://huggingface.co/datasets/vapaau/jambo,https://paperswithcode.com/dataset/jambo,"The JAMBO dataset contains 3290 underwater images of the seabed captured by an ROV in temperate waters in the Jammer Bay area off the North West coast of Jutland, Denmark. All the images have been annotated by six annotators to contain one of three classes: sand, stone, or bad. 

Each of the six annotators have labelled all the images (that is, six individual annotations are provided for each image), which allows for analyzing inter-annotator disagreement and uncertainty.",,,,,,
1604,JARVIS-DFT,Band Gap,Band Gap,"Band Gap, Formation Energy",,,Methodology,"formation-energy-on-jarvis-dft-formation, band-gap-on-jarvis-dft",,https://jarvis.nist.gov/,https://paperswithcode.com/dataset/jarvis-dft-formation-energy,JARVIS-DFT is a repository of density functional theory based calculation data for materials.,,,,,,
1605,Java_scripts,Source Code Summarization,Source Code Summarization,"Source Code Summarization, Code Summarization, Code Documentation Generation",Text,English,Natural Language Processing,source-code-summarization-on-hybrid-deepcom,,https://link.springer.com/article/10.1007/s10664-019-09730-9,https://paperswithcode.com/dataset/hybrid-deepcom-java,"The Java dataset introduced in Hybrid-DeepCom (Deep code comment generation with hybrid lexical and syntactical information), commonly used to evaluate automated code summarization. It is basically a further version of DeepCom-Java.",,,,,,
1606,Jericho,Language Modelling,Language Modelling,"Language Modelling, text-based games, Knowledge Graphs",Text,English,Natural Language Processing,,,https://github.com/Microsoft/jericho,https://paperswithcode.com/dataset/jericho,Jericho is a learning environment for man-made Interactive Fiction (IF) games.,,Interactive Fiction Games: A Colossal Adventure,https://arxiv.org/pdf/1909.05398.pdf,,,
1607,JerichoWorld,Action Parsing,Action Parsing,"Action Parsing, Knowledge Graphs","Text, Video",English,Natural Language Processing,"knowledge-graphs-on-jerichoworld, action-parsing-on-jerichoworld",,https://github.com/JerichoWorld/JerichoWorld,https://paperswithcode.com/dataset/jerichoworld,"JerichoWorld  is a dataset that enables the creation of learning agents that can build knowledge graph-based world models of interactive narratives. Interactive narratives -- or text-adventure games -- are partially observable environments structured as long puzzles or quests in which an agent perceives and interacts with the world purely through textual natural language. Each individual game typically contains hundreds of locations, characters, and objects -- each with their own unique descriptions -- providing an opportunity to study the problem of giving language-based agents the structured memory necessary to operate in such worlds. 

JerichoWorld provides 24,198 mappings between rich natural language observations and: (1) knowledge graphs that reflect the world state in the form of a map; (2) natural language actions that are guaranteed to cause a change in that particular world state. The training data is collected across 27 games in multiple genres and contains a further 7,836 heldout instances over 9 additional games in the test set.",,,,,,
1608,JetClass,Jet Tagging,Jet Tagging,"Jet Tagging, Point Cloud Classification, Point Cloud Generation","3D, Image, Text",English,Computer Vision,jet-tagging-on-jetclass,Creative Commons Attribution 4.0 International,https://zenodo.org/record/6619768,https://paperswithcode.com/dataset/jetclass,"JetClass is a new large-scale dataset to facilitate deep learning research in particle physics. It consists of 100M particle jets for training, 5M for validation and 20M for testing. The dataset contains 10 classes of jets, simulated with MadGraph + Pythia + Delphes. A detailed description of the JetClass dataset is presented in the paper Particle Transformer for Jet Tagging. An interface to use the dataset is provided here.",,Particle Transformer for Jet Tagging,https://arxiv.org/abs/2202.03772,,,10
1609,JFLEG,Grammatical Error Detection,Grammatical Error Detection,"Grammatical Error Detection, Grammatical Error Correction",Image,,Computer Vision,"grammatical-error-correction-on-restricted, grammatical-error-correction-on-jfleg, grammatical-error-correction-on-unrestricted, grammatical-error-correction-on-_restricted_, grammatical-error-detection-on-jfleg",CC BY-NC-SA 4.0,https://github.com/keisks/jfleg,https://paperswithcode.com/dataset/jfleg,"JFLEG is for developing and evaluating grammatical error correction (GEC). Unlike other corpora, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding.",,JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction,https://arxiv.org/pdf/1702.04066v1.pdf,,,
1610,JHMDB,Skeleton Based Action Recognition,Skeleton Based Action Recognition,"Skeleton Based Action Recognition, Pose Estimation, Referring Expression Segmentation, Action Detection, 2D Human Pose Estimation","3D, Image, Video",,Computer Vision,"skeleton-based-action-recognition-on-jhmdb, pose-estimation-on-j-hmdb, skeleton-based-action-recognition-on-j-hmbd, action-detection-on-j-hmdb, skeleton-based-action-recognition-on-j-hmdb, referring-expression-segmentation-on-j-hmdb, 2d-human-pose-estimation-on-jhmdb-2d-poses, skeleton-based-action-recognition-on-jhmdb-2d",,http://jhmdb.is.tue.mpg.de/,https://paperswithcode.com/dataset/jhmdb,"JHMDB is an action recognition dataset that consists of 960 video sequences belonging to 21 actions. It is a subset of the larger HMDB51 dataset collected from digitized movies and YouTube videos. The dataset contains video and annotation for puppet flow per frame (approximated optimal flow on the person), puppet mask per frame, joint positions per frame, action label per clip and meta label per clip (camera motion, visible body parts, camera viewpoint, number of people, video quality).",,Unsupervised Deep Metric Learning via Orthogonality based Probabilistic Loss,https://arxiv.org/abs/2008.09880,,,
1611,JHU-CROWD__,Crowd Counting,Crowd Counting,"Crowd Counting, Density Estimation",,,Methodology,crowd-counting-on-jhu-crowd,,http://www.crowd-counting.com/,https://paperswithcode.com/dataset/jhu-crowd-1,"JHU-CROWD++ is A large-scale unconstrained crowd counting dataset with 4,372 images and 1.51 million annotations. This dataset is collected under a variety of diverse scenarios and environmental conditions. In addition, the dataset provides comparatively richer set of annotations like dots, approximate bounding boxes, blur levels, etc.",,,,372 images,"trained crowd counting dataset with 4,372 images",
1612,JIGSAWS,Surgical Skills Evaluation,Surgical Skills Evaluation,"Surgical Skills Evaluation, Action Quality Assessment, Action Segmentation","Image, Video",,Computer Vision,"action-quality-assessment-on-jigsaws, surgical-skills-evaluation-on-jigsaws, action-segmentation-on-jigsaws",Custom,https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release,https://paperswithcode.com/dataset/jigsaws,"The JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) is a surgical activity dataset for human motion modeling. The data was collected through a collaboration between The Johns Hopkins University (JHU) and Intuitive Surgical, Inc. (Sunnyvale, CA. ISI) within an IRB-approved study. The release of this dataset has been approved by the Johns Hopkins University IRB.   The dataset was captured using the da Vinci Surgical System from eight surgeons with different levels of skill performing five repetitions of three elementary surgical tasks on a bench-top model: suturing, knot-tying and needle-passing, which are standard components of most surgical skills training curricula. The JIGSAWS dataset consists of three components:


kinematic data: Cartesian positions, orientations, velocities, angular velocities and gripper angle describing the motion of the manipulators.
video data: stereo video captured from the endoscopic camera. Sample videos of the JIGSAWS tasks can be downloaded from the official webpage.
manual annotations including:
gesture (atomic surgical activity segment labels).
skill (global rating score using modified objective structured assessments of technical skills).
experimental setup: a standardized cross-validation experimental setup that can be used to evaluate automatic surgical gesture recognition and skill assessment methods.",,Jhu-isi gesture and skill assessment working set (jigsaws): A surgical activity dataset for human motion modeling,https://cirl.lcsr.jhu.edu/wp-content/uploads/2015/11/JIGSAWS.pdf,,,
1613,Jobs,Causal Inference,Causal Inference,Causal Inference,,,Methodology,causal-inference-on-jobs,,,https://paperswithcode.com/dataset/jobs,"The Jobs dataset by LaLonde [36] is a widely used benchmark in the causal inference community, where the treatment is job training and the outcomes are income and employment status after training. The dataset includes 8 covariates such as age, education, and previous earnings. Our goal is to predict unemployment, using the feature set of Dehejia and Wahba [37]. Following Shalit et al. [8], we combined the LaLonde experimental sample (297 treated, 425 control) with the PSID comparison group (2490 control).",,,,,,
1614,JRDB-Act,Spatio-Temporal Action Localization,Spatio-Temporal Action Localization,Spatio-Temporal Action Localization,"Image, Time Series, Video",,Computer Vision,,,https://jrdb.stanford.edu/,https://paperswithcode.com/dataset/jrdb-act,"JRDB-Act is an extension of the JRDB dataset to create a large-scale multi-modal dataset for spatio-temporal action, social group and activity detection. 

JRDB-Act has been densely annotated with atomic actions, comprises over 2.8M action labels, constituting a large-scale spatio-temporal action detection dataset. Each human bounding box is labelled with one pose-based action label and multiple (optional) interaction-based action labels. Moreover JRDB-Act comes with social group identification annotations conducive to the task of grouping individuals based on their interactions in the scene to infer their social activities (common activities in each social group).",,,,,,
1615,JRDB,Human Detection,Human Detection,"Human Detection, Multi-Object Tracking, Autonomous Navigation","Image, Video",,Computer Vision,multi-object-tracking-on-jrdb,,https://jrdb.stanford.edu/,https://paperswithcode.com/dataset/jrdb,"A novel egocentric dataset collected from social mobile manipulator JackRabbot. The dataset includes 64 minutes of annotated multimodal sensor data including stereo cylindrical 360 degrees RGB video at 15 fps, 3D point clouds from two Velodyne 16 Lidars, line 3D point clouds from two Sick Lidars, audio signal, RGB-D video at 30 fps, 360 degrees spherical image from a fisheye camera and encoder values from the robot's wheels.",,,,,,
1616,JSB_Chorales,Music Generation,Music Generation,"Music Generation, Music Modeling","Audio, Text",English,Natural Language Processing,music-modeling-on-jsb-chorales,Public Domain,http://www-etud.iro.umontreal.ca/~boulanni/icml2012,https://paperswithcode.com/dataset/jsb-chorales,"The JSB chorales are a set of short, four-voice pieces of music well-noted for their stylistic homogeneity. The chorales were originally composed by Johann Sebastian Bach in the
18th century. He wrote them by first taking pre-existing melodies from contemporary Lutheran hymns and then harmonising them to create the parts for the remaining
three voices. The version of the dataset used canonically in representation learning contexts consists of 382 such chorales, with a train/validation/test split of 229, 76 and 77 samples respectively.",,,,77 samples,"train/validation/test split of 229, 76 and 77 samples",
1617,JSUT_Corpus,Speech Synthesis,Speech Synthesis,"Speech Synthesis, Speech Recognition","Audio, Image, Text",English,Speech,,,https://sites.google.com/site/shinnosuketakamichi/publication/jsut,https://paperswithcode.com/dataset/jsut-corpus,"JSUT Corpus is a free large-scale speech corpus that can be shared between academic institutions and commercial companies has an important role. However, such a corpus for Japanese speech synthesis does not exist.",,,,,,
1618,JS_Fake_Chorales,Music Classification,Music Classification,"Music Classification, Music Transcription, Music Generation, Music Style Transfer, Music Modeling","Audio, Image, Text",English,Computer Vision,,Creative Commons Attribution 4.0 International,https://github.com/omarperacha/js-fakes,https://paperswithcode.com/dataset/js-fake-chorales,"A MIDI dataset of 500 4-part chorales generated by the KS_Chorus algorithm, annotated with results from hundreds of listening test participants, with 500 further unannotated chorales.",,,,,,
1619,Jung,Document Shadow Removal,Document Shadow Removal,"Document Shadow Removal, Shadow Removal",Text,English,Natural Language Processing,,,,https://paperswithcode.com/dataset/jung,Dataset for document shadow removal,,,,,,
1620,JustLogic,Logical Reasoning Question Answering,Logical Reasoning Question Answering,"Logical Reasoning Question Answering, Reasoning Chain Explanations, Logical Fallacies, Logical Reasoning Reading Comprehension, Logical Reasoning",Text,English,Reasoning,,CC BY,https://github.com/michaelchen-lab/JustLogic,https://paperswithcode.com/dataset/justlogic,"JustLogic is a natural language deductive reasoning dataset. JustLogic is (i) highly complex, capable of generating a diverse range of linguistic patterns, vocabulary, and argument structures; (ii) prior knowledge independent, eliminating the advantage of models possessing prior knowledge and ensuring that only deductive reasoning is used to answer questions; and (iii) capable of in-depth error analysis on the heterogeneous effects of reasoning depth and argument form on model accuracy.",,,,,,
1621,K-EmoCon,Electroencephalogram (EEG),Electroencephalogram (EEG),"Electroencephalogram (EEG), Emotion Recognition",Image,,Computer Vision,,CC BY 4.0,https://zenodo.org/record/3814370,https://paperswithcode.com/dataset/k-emocon,"A multimodal dataset with comprehensive annotations of continuous emotions during naturalistic conversations. The dataset contains multimodal measurements, including audiovisual recordings, EEG, and peripheral physiological signals, acquired with off-the-shelf devices from 16 sessions of approximately 10-minute long paired debates on a social issue.",,,,,,
1622,K-Radar,Radar odometry,Radar odometry,"Radar odometry, 3D Object Tracking, 3D Object Detection","3D, Image, Video",,Computer Vision,,CC BY-NC-ND,https://github.com/kaist-avelab/K-Radar,https://paperswithcode.com/dataset/k-radar,"KAIST-Radar (K-Radar) is a novel large-scale object detection dataset and benchmark that contains 35K frames of 4D Radar tensor (4DRT) data with power measurements along the Doppler, range, azimuth, and elevation dimensions, together with carefully annotated 3D bounding box labels of objects on the roads. K-Radar includes challenging driving conditions such as adverse weathers (fog, rain, and snow) on various road structures (urban, suburban roads, alleyways, and highways). In addition to the 4DRT, we provide auxiliary measurements from carefully calibrated high-resolution Lidars, surround stereo cameras, and RTK-GPS.",,,,,,
1623,Kaggle-Credit_Card_Fraud_Dataset,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Fraud Detection",Image,,Computer Vision,"fraud-detection-on-kaggle-credit-card-fraud, anomaly-detection-on-kaggle-credit-card-fraud",,https://www.kaggle.com/mlg-ulb/creditcardfraud/,https://paperswithcode.com/dataset/kaggle-credit-card-fraud-dataset,"The dataset contains transactions made by credit cards in September 2013 by European cardholders.
This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.  

It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependent cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.

Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.",2013,,,,,
1624,KaggleDBQA,Text-To-SQL,Text-To-SQL,"Text-To-SQL, Question Answering",Text,English,Natural Language Processing,text-to-sql-on-kaggledbqa,,https://github.com/chiahsuan156/KaggleDBQA,https://paperswithcode.com/dataset/kaggledbqa,"KaggleDBQA is a challenging cross-domain and complex evaluation dataset of real Web databases, with domain-specific data types, original formatting, and unrestricted questions. 

It expands upon contemporary cross-domain text-to-SQL datasets in three key aspects:
(1) Its databases are pulled from real-world data sources and not normalized.
(2) Its questions are authored in environments that mimic natural question answering.
(3) It also provides database documentation that contains rich in-domain knowledge.",,,,,,
1625,Kaggle_EyePACS,Diabetic Retinopathy Grading,Diabetic Retinopathy Grading,Diabetic Retinopathy Grading,,,Methodology,diabetic-retinopathy-grading-on-kaggle,,https://www.kaggle.com/c/diabetic-retinopathy-detection/,https://paperswithcode.com/dataset/kaggle-eyepacs,"Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world. It is estimated to affect over 93 million people.

retina

The US Center for Disease Control and Prevention estimates that 29.1 million people in the US have diabetes and the World Health Organization estimates that 347 million people have the disease worldwide. Diabetic Retinopathy (DR) is an eye disease associated with long-standing diabetes. Around 40% to 45% of Americans with diabetes have some stage of the disease. Progression to vision impairment can be slowed or averted if DR is detected in time, however this can be difficult as the disease often shows few symptoms until it is too late to provide effective treatment.

Currently, detecting DR is a time-consuming and manual process that requires a trained clinician to examine and evaluate digital color fundus photographs of the retina. By the time human readers submit their reviews, often a day or two later, the delayed results lead to lost follow up, miscommunication, and delayed treatment.

Clinicians can identify DR by the presence of lesions associated with the vascular abnormalities caused by the disease. While this approach is effective, its resource demands are high. The expertise and equipment required are often lacking in areas where the rate of diabetes in local populations is high and DR detection is most needed. As the number of individuals with diabetes continues to grow, the infrastructure needed to prevent blindness due to DR will become even more insufficient.

The need for a comprehensive and automated method of DR screening has long been recognized, and previous efforts have made good progress using image classification, pattern recognition, and machine learning. With color fundus photography as input, the goal of this competition is to push an automated detection system to the limit of what is possible – ideally resulting in models with realistic clinical potential. The winning models will be open sourced to maximize the impact such a model can have on improving DR detection.

Acknowledgements
This competition is sponsored by the California Healthcare Foundation.



Retinal images were provided by EyePACS, a free platform for retinopathy screening.",,,,,,
1626,kaggle_stroke_Prediction_competition,imbalanced classification,imbalanced classification,"imbalanced classification, Stroke Classification",Image,,Computer Vision,,,https://www.kaggle.com/fedesoriano/stroke-prediction-dataset,https://paperswithcode.com/dataset/kaggle-stroke-prediction-competition,"It is a competition on kaggle with stroke Prediction, which is heavily imbalanced.",,,,,,
1627,KAIST,Spectral Reconstruction,Spectral Reconstruction,Spectral Reconstruction,3D,,Methodology,spectral-reconstruction-on-kaist,,https://zaguan.unizar.es/record/75680,https://paperswithcode.com/dataset/kaist,High-quality hyperspectral reconstruction using a spectral prior,,,,,,
1628,Kalunga2016_MOABB,Within-Session SSVEP,Within-Session SSVEP,Within-Session SSVEP,,,Methodology,within-session-ssvep-on-kalunga2016-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Kalunga2016.html,https://paperswithcode.com/dataset/kalunga2016-moabb,,,,,,,
1629,KAMEL,Zero-shot Slot Filling,Zero-shot Slot Filling,"Zero-shot Slot Filling, Question Answering, Probing Language Models",Text,English,Natural Language Processing,probing-language-models-on-kamel,,https://huggingface.co/datasets/LeandraFichtel/KAMEL,https://paperswithcode.com/dataset/kamel,"KAMEL comprises knowledge about 234 relations from Wikidata with a large training, validation, and test dataset. We make sure that all facts are also present in Wikipedia so that they have been seen during the pre-training procedure of the LMs we are probing. Most importantly we overcome the limitations of existing probing datasets by 
(1) having a larger variety of knowledge graph relations, 
(2) it contains single- and multi-token entities, 
(3) we use relations with literals, and 
(4) have alternative labels for entities. 
(5) Furthermore, we created an evaluation procedure for higher cardinality relations, which was missing in previous works, and 
(6) make sure that the dataset can be used for causal LMs.",,,,,,
1630,KanHope,Hope Speech Detection,Hope Speech Detection,"Hope Speech Detection, Text Classification","Audio, Image, Text",English,Speech,hope-speech-detection-on-kanhope,,https://zenodo.org/record/4904729,https://paperswithcode.com/dataset/kanhope,"KanHope is a code mixed hope speech dataset for equality, diversity, and inclusion in Kannada, an under-resourced Dravidian language. The dataset consists of 6,176 user-generated comments in code mixed Kannada crawled from YouTube and manually labelled as bearing hope speech or not-hope speech.",,,,,,
1631,KazakhTTS,Text-To-Speech Synthesis,Text-To-Speech Synthesis,Text-To-Speech Synthesis,"Audio, Text",English,Speech,,CC BY 4.0,https://github.com/IS2AI/Kazakh_TTS,https://paperswithcode.com/dataset/kazakhtts,"KazakhTTS is an open-source speech synthesis dataset for Kazakh, a low-resource language spoken by over 13 million people worldwide. The dataset consists of about 91 hours of transcribed audio recordings spoken by two professional speakers (female and male). It is the first publicly available large-scale dataset developed to promote Kazakh text-to-speech (TTS) applications in both academia and industry.",,,,,,
1632,KDD12,Click-Through Rate Prediction,Click-Through Rate Prediction,Click-Through Rate Prediction,Time Series,,Methodology,click-through-rate-prediction-on-kdd12,,https://www.kaggle.com/c/kddcup2012-track2,https://paperswithcode.com/dataset/kdd12,"A clickthrough prediction dataset, for more information please see the Kaggle page",,,,,,
1633,KELM,Text Generation,Text Generation,"Text Generation, Language Modelling, Data-to-Text Generation",Text,English,Natural Language Processing,,,https://github.com/google-research-datasets/KELM-corpus,https://paperswithcode.com/dataset/kelm,KELM is a large-scale synthetic corpus of Wikidata KG as natural text.,,,,,,
1634,KETOD,Response Generation,Response Generation,Response Generation,Text,English,Natural Language Processing,,MIT,,https://paperswithcode.com/dataset/ketod,"KETOD (Knowledge-Enriched Task-Oriented Dialogue) is a dataset containing system responses designed for enriching task-oriented dialogues with chit-chat based on relevant entity knowledge. There are a total of 5,324 dialogues with enriched system responses.",,KETOD: Knowledge-Enriched Task-Oriented Dialogue,https://arxiv.org/pdf/2205.05589v1.pdf,,,
1635,Keyphrases_CS_Math_Russian,Keyword Extraction,Keyword Extraction,"Keyword Extraction, Keyphrase Generation, Keyphrase Extraction",Text,English,Natural Language Processing,,,https://data.mendeley.com/datasets/dv3j9wc59v/1,https://paperswithcode.com/dataset/keyphrases-cs-math-russian,"Dataset contains CS/Math articles abstracts (in Russian) obtained from two online sources. For each article publication year, journal name, authors, title, keyphrases and abstract  are provided.

Morozov, D. A., Glazkova, A. V., Tyutyulnikov, M. A., & Iomdin, B. L. (2023). Keyphrase generation for abstracts of the Russian-language scientific articles. NSU Vestnik. Series: Linguistics and Intercultural Communication, 21(1), 54-66.",2023,,,,,
1636,KeypointNet,Pose Estimation,Pose Estimation,"Pose Estimation, 3D Shape Representation, Keypoint Detection","3D, Image",,Computer Vision,,,https://github.com/qq456cvb/KeypointNet,https://paperswithcode.com/dataset/keypointnet,"KeypointNet is a large-scale and diverse 3D keypoint dataset that contains 83,231 keypoints and 8,329 3D models from 16 object categories, by leveraging numerous human annotations, based on ShapeNet models.",,,,,,
1637,KG20C,Graph Embedding,Graph Embedding,"Graph Embedding, Knowledge Graph Embedding, Knowledge Graphs, Link Prediction, Recommendation Systems, Question Answering","Graph, Text, Time Series",English,Natural Language Processing,link-prediction-on-kg20c,CC BY-NC,https://github.com/tranhungnghiep/KG20C,https://paperswithcode.com/dataset/kg20c,"KG20C is a Knowledge Graph about high quality papers from 20 top computer science Conferences. It can serve as a standard benchmark dataset in scholarly data analysis for several tasks, including knowledge graph embedding, link prediction, recommendation systems, and question answering . 

For more information and download, please see the dataset homepage.",,,,,,
1638,KGRC-RDF-star,Knowledge Graph Embedding,Knowledge Graph Embedding,"Knowledge Graph Embedding, Knowledge Graph Embeddings",Graph,,Methodology,,CC BY,https://github.com/aistairc/KGRC-RDF-star/,https://paperswithcode.com/dataset/kgrc-rdf-star,"KGRC-RDF-star is an RDF-star dataset converted from KGRC-RDF, which is a Knowledge graph dataset of novel stories.

RDF-star (also known as RDF*) introduces quoted triple (QT), which is a triple used as the subject or object of another triple.

KGRC-RDF-star is a complex RDF-star graph dataset that contains nested structures of statements and scenes, e.g., ""Person A said ""Person B saw ""Person C was in D"" "" .""

The same s, p, and o combinations may occur in different scenes when the KGRC-RDF is converted to the KGRC-RDF-star. It is necessary to distinguish these QTs and assign different metadata to them. Therefore, we solved this problem by assigning a unique ID to each QT and nested these triples as a QT as follows: << << s p o >> id val >> p' o'.",,,,,,
1639,KID-F,Face Hallucination,Face Hallucination,"Face Hallucination, Blind Super-Resolution, Super-Resolution",Image,,Computer Vision,,,https://github.com/PCEO-AI-CLUB/KID-F,https://paperswithcode.com/dataset/kid-f,"Description
K-pop Idol Dataset - Female (KID-F) is the first dataset of K-pop idol high quality face images. It consists of about 6,000 high quality face images at 512x512 resolution and identity labels for each image.

We collected about 90,000 K-pop female idol images and crop the face from each image. And we classified high quality face images. As a result, there are about 6,000 high quality face images in this dataset.

There are 300 test datasets for a benchmark. There are no duplicate images between test and train images. Some identities in test images are not duplicated with train images. (It means some test images is new identity to the trained model) Each test images have its degraded pair. You can use these degraded test images for testing face super resolution performance.

We also provide identity labels for each image. You can download the csv file from our github

Download
You can download dataset from here.
Google Drive

Agreement

The use of this software is RESTRICTED to non-commercial research and educational purposes.
All images of the KID-F dataset are obtained from the internet which are not property of EDA(PCEO-AI-CLUB). EDA is not responsible for the content nor the meaning of these images.
You agree not to reproduce, duplicate, copy, sell, trade, resell or exploit for any commercial purposes, any portion of the images and any portion of derived data.
You agree not to further copy, publish or distribute any portion of the KID-F dataset. Except, for internal use at a single site within the same organization it is allowed to make copies of the dataset.
EDA reserves the right to terminate your access to the CelebA dataset at any time.",,,,,,
1640,KILT,Open-Domain Question Answering,Open-Domain Question Answering,"Open-Domain Question Answering, Fact Verification, Entity Linking, Slot Filling, Open-Domain Dialog, Question Answering",Text,English,Natural Language Processing,"open-domain-question-answering-on-kilt, open-domain-question-answering-on-kilt-eli5, entity-linking-on-kilt-wned-wiki, open-domain-dialog-on-kilt-wizard-of, question-answering-on-kilt-eli5, open-domain-question-answering-on-kilt-1, open-domain-question-answering-on-kilt-2, entity-linking-on-kilt-wned-cweb, fact-verification-on-kilt-fever, slot-filling-on-kilt-zero-shot-re, entity-linking-on-kilt-aida-yago2, slot-filling-on-kilt-t-rex",,http://kiltbenchmark.com/,https://paperswithcode.com/dataset/kilt,"KILT (Knowledge Intensive Language Tasks) is a benchmark consisting of 11 datasets representing 5 types of tasks:


Fact-checking (FEVER),
Entity linking (AIDA CoNLL-YAGO, WNED-WIKI, WNED-CWEB),
Slot filling (T-Rex, Zero Shot RE),
Open domain QA (Natural Questions, HotpotQA, TriviaQA, ELI5),
Dialog generation (Wizard of Wikipedia).

All these datasets have been grounded in a single pre-processed wikipedia snapshot, allowing for fairer and more consistent evaluation as well as enabling new task setups such as multitask and transfer learning.",,,,,,
1641,Kinetics-100,Few Shot Action Recognition,Few Shot Action Recognition,Few Shot Action Recognition,"Image, Video",,Computer Vision,few-shot-action-recognition-on-kinetics-100,,https://github.com/ffmpbgrnn/CMN/tree/master/kinetics-100,https://paperswithcode.com/dataset/kinetics-100,"Kinetics-100 is a dataset split created from the Kinetics dataset to evaluate the performance of few-shot action recognition models. 100 classes are randomly selected from a total of 400 categories, each composed of 100 examples. The 100 classes are further split into 64, 12, and 24 non-overlapping classes to use as the meta-training set, meta-validation set, and meta-testing set, respectively.  Link to the selected samples can be found here: https://github.com/ffmpbgrnn/CMN/tree/master/kinetics-100",,,,100 examples,"split created from the Kinetics dataset to evaluate the performance of few-shot action recognition models. 100 classes are randomly selected from a total of 400 categories, each composed of 100 examples",100
1642,Kinetics-600,Video Prediction,Video Prediction,"Video Prediction, Action Classification, Action Recognition In Videos, Self-Supervised Action Recognition, Video Generation","Image, Text, Time Series, Video",English,Computer Vision,"video-generation-on-kinetics-600-12-frames, video-generation-on-kinetics-600-12-frames-1, video-generation-on-kinetics-600-48-frames, self-supervised-action-recognition-on, video-prediction-on-kinetics-600-12-frames, action-recognition-in-videos-on-kinetics-600, action-classification-on-kinetics-600",CC BY 4.0,https://deepmind.com/research/open-source/kinetics,https://paperswithcode.com/dataset/kinetics-600,"The Kinetics-600 is a large-scale action recognition dataset which consists of around 480K videos from 600 action categories. The 480K videos are divided into 390K, 30K, 60K for training, validation and test sets, respectively. Each video in the dataset is a 10-second clip of action moment annotated from raw YouTube video. It is an extensions of the Kinetics-400 dataset.",,Learning to Localize Actions from Moments,https://arxiv.org/abs/2008.13705,,,
1643,Kinetics,Video Prediction,Video Prediction,"Video Prediction, Video Understanding, Boundary Detection, Action Recognition In Videos, Self-Supervised Action Recognition, Image Clustering, Text to Video Retrieval, Zero-Shot Action Recognition, Few Shot Action Recognition, imbalanced classification, Video Recognition, Generic Event Boundary Detection, Boundary Captioning, Video Generation, Action Recognition, Video Retrieval, Long-tail Learning, Self-Supervised Action Recognition Linear, Visual Tracking, Event Segmentation, Video Captioning, Text-to-Video Generation, Video Classification, Boundary Grounding, Skeleton Based Action Recognition, Action Classification, Semantic Object Interaction Classification, Temporal Action Localization, Spatio-Temporal Action Localization, Video Grounding","Image, Text, Time Series, Video",English,Computer Vision,"boundary-captioning-on-kinetic-geb, self-supervised-action-recognition-on-1, video-generation-on-kinetics-600-12-frames, boundary-grounding-on-kinetic-geb, zero-shot-action-recognition-on-kinetics, text-to-video-retrieval-on-kinetic-geb, action-recognition-in-videos-on-kinetics-600, image-clustering-on-kinetics-700, action-classification-on-kinetics-700, spatio-temporal-action-localization-on-ava, action-classification-on-kinetics-400, action-classification-on-minikinetics, self-supervised-action-recognition-linear-on-3, video-classification-on-kinetics, self-supervised-action-recognition-on, video-prediction-on-kinetics-600-12-frames, action-classification-on-kinetics-sounds, skeleton-based-action-recognition-on-kinetics-2, visual-tracking-on-kinetics, video-generation-on-kinetics-600-48-frames, semantic-object-interaction-classification-on-2, few-shot-action-recognition-on-kinetics-100, skeleton-based-action-recognition-on-kinetics, boundary-detection-on-kinetics-400, action-recognition-in-videos-on-kinetics-400-1, video-generation-on-kinetics-600-12-frames-1, event-segmentation-on-kinetics-400, text-to-video-generation-on-kinetics, generic-event-boundary-detection-on-kinetics, action-classification-on-kinetics-600",CC BY 4.0,https://deepmind.com/research/open-source/kinetics,https://paperswithcode.com/dataset/kinetics,"The Kinetics dataset is a large-scale, high-quality dataset for human action recognition in videos. The dataset consists of around 500,000 video clips covering 600 human action classes with at least 600 video clips for each action class. Each video clip lasts around 10 seconds and is labeled with a single action class. The videos are collected from YouTube.",,Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey,https://arxiv.org/abs/1902.06162,,,
1644,Kinetics_400,Skeleton Based Action Recognition,Skeleton Based Action Recognition,"Skeleton Based Action Recognition, Boundary Detection, Action Classification, Action Recognition In Videos, Self-Supervised Action Recognition, Event Segmentation, Self-Supervised Action Recognition Linear","Image, Video",,Computer Vision,"self-supervised-action-recognition-on-1, boundary-detection-on-kinetics-400, action-classification-on-kinetics-400, skeleton-based-action-recognition-on-kinetics-2, action-recognition-in-videos-on-kinetics-400-1, self-supervised-action-recognition-linear-on-3, event-segmentation-on-kinetics-400",Creative Commons Attribution 4.0 International,https://deepmind.com/research/open-source/kinetics,https://paperswithcode.com/dataset/kinetics-400-1,"The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands.",,https://arxiv.org/abs/1705.06950,https://arxiv.org/abs/1705.06950,,,
1645,KINNEWS_and_KIRNEWS,Multi-class Classification,Multi-class Classification,"Multi-class Classification, News Classification",Image,,Computer Vision,,,https://github.com/Andrews2017/KINNEWS-and-KIRNEWS-Corpus,https://paperswithcode.com/dataset/kinnews-and-kirnews,"Two news datasets (KINNEWS and KIRNEWS) for multi-class classification of news articles in Kinyarwanda and Kirundi, two low-resource African languages. The two languages are mutually intelligible.",,,,,,
1646,Kinship,Inductive logic programming,Inductive logic programming,"Inductive logic programming, Automated Theorem Proving, Knowledge Graphs",,,Methodology,,,https://archive.ics.uci.edu/ml/datasets/kinship,https://paperswithcode.com/dataset/kinship,This relational database consists of 24 unique names in two families (they have equivalent structures).,,,,,,
1647,Kitsune_Network_Attack_Dataset,Intrusion Detection,Intrusion Detection,"Intrusion Detection, Network Intrusion Detection","Graph, Image",,Computer Vision,,GNU Affero General Public License,https://www.kaggle.com/datasets/ymirsky/network-attack-dataset-kitsune,https://paperswithcode.com/dataset/kitsune-network-attack-dataset,"Kitsune Network Attack Dataset
This is a collection of nine network attack datasets captured from a either an IP-based commercial surveillance system or a network full of IoT devices. Each dataset contains millions of network packets and diffrent cyber attack within it.

For each attack, you are supplied with:


A preprocessed dataset in csv format (ready for machine learning)
The corresponding label vector in csv format
The original network capture in pcap format (in case you want to engineer your own features)

We will now describe in detail what's in these datasets and how they were collected.

The Network Attacks
We have collected a wide variety of attacks which you would find in a real network intrusion. The following is a list of the cyber attack datasets avalaible:

<img src=""https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F827271%2F79e305668553e521b0709a2413323c45%2Fkaggle_dataset_table.png?generation=1598461684070844&alt=media"" alt=""image"" width=""100"">

For more details on the attacks themselves, please refer to our NDSS paper (citation below).

The Data Collection
The following figure presents the network topologies which we used to collect the data, and the corrisponding attack vectors at which the attacks were performed. The network capture took place at point 1 and point X at the router (where a network intrusion detection system could feasibly be placed).
For each dataset, clean network traffic was captured for the first 1 million packets, then the cyber attack was performed.

The Dataset Format
Each preprocessed dataset csv has m rows (packets) and 115 columns (features) with no header. The 115 features were extracted using our AfterImage feature extractor, described in our NDSS paper (see below) and available in Python here.
In summary, the 115 features provide a statistical snapshot of the network (hosts and behaviors) in the context of the current packet traversing the network. The AfterImage feature extractor is unique in that it can efficiently process millions of streams (network channels) in real-time, incrementally, making it suitable for handling network traffic.

Citation
If you use these datasets, please cite:

@inproceedings{mirsky2018kitsune, 
title={Kitsune: An Ensemble of Autoencoders for Online Network Intrusion Detection},
 author={Mirsky, Yisroel and Doitshman, Tomer and Elovici, Yuval and Shabtai, Asaf}, 
booktitle={The Network and Distributed System Security Symposium (NDSS) 2018}, 
year={2018} }",2018,,,,,
1648,KITTI-360,3D Instance Segmentation,3D Instance Segmentation,"3D Instance Segmentation, Instance Segmentation, Semantic Segmentation, Panoptic Segmentation, Novel View Synthesis, 3D Semantic Scene Completion from a single RGB image, Semantic SLAM, Weakly Supervised 3D Detection, 3D Semantic Scene Completion, 3D Object Detection From Monocular Images, 3D Semantic Segmentation, 2D Semantic Segmentation","3D, Image",English,Computer Vision,"3d-semantic-scene-completion-from-a-single-2, semantic-segmentation-on-kitti-360, panoptic-segmentation-on-kitti-360, 3d-semantic-segmentation-on-kitti-360, 3d-object-detection-from-monocular-images-on-7, 3d-semantic-scene-completion-on-kitti-360, weakly-supervised-3d-detection-on-kitti-360",Custom,http://www.cvlibs.net/datasets/kitti-360/,https://paperswithcode.com/dataset/kitti-360,"KITTI-360 is a large-scale dataset that contains rich sensory information and full annotations. It is the successor of the popular KITTI dataset,  providing more comprehensive semantic/instance labels in 2D and 3D, richer 360 degree sensory information (fisheye images and pushbroom laser scans), very accurate and geo-localized vehicle and camera poses, and a series of new challenging benchmarks.",,,,,,
1649,KITTI-C,Unsupervised Monocular Depth Estimation,Unsupervised Monocular Depth Estimation,"Unsupervised Monocular Depth Estimation, Robust 3D Object Detection, 3D Object Detection","3D, Image",English,Computer Vision,"robust-3d-object-detection-on-kitti-c, unsupervised-monocular-depth-estimation-on-7",Creative Commons Attribution 4.0 International,https://ldkong.com/Robo3D,https://paperswithcode.com/dataset/kitti-c,"🤖 Robo3D - The KITTI-C Benchmark
KITTI-C is an evaluation benchmark heading toward robust and reliable 3D object detection in autonomous driving. With it, we probe the robustness of 3D detectors under out-of-distribution (OoD) scenarios against corruptions that occur in the real-world environment. Specifically, we consider natural corruptions happen in the following cases:


Adverse weather conditions, such as fog, wet ground, and snow;
External disturbances that are caused by motion blur or result in LiDAR beam missing;
Internal sensor failure, including crosstalk, possible incomplete echo, and cross-sensor scenarios.

KITTI-C is part of the Robo3D benchmark. Visit our homepage to explore more details.",,Robo3D,https://arxiv.org/abs/2303.17597,,,
1650,KITTI-Masks,Disentanglement,Disentanglement,Disentanglement,"Image, 3D",English,Computer Vision,disentanglement-on-kitti-masks,Creative Commons Attribution 4.0 International,https://zenodo.org/record/3931823#.YgWkGfXMKbg,https://paperswithcode.com/dataset/kitti-masks,"This Dataset consists of 2120 sequences of binary masks of pedestrians. The sequence length varies between 2-710. For details, we refer to our paper. It is based on the original KITTI Segmentation challenge which can be found at https://www.vision.rwth-aachen.de/page/mots 

A detailed description can be found at: https://openreview.net/pdf?id=EbIDjBynYJ8

An example dataloader can be found at: 
https://github.com/bethgelab/slow_disentanglement/",,,,,,
1651,KITTI,Video Prediction,Video Prediction,"Video Prediction, Visual Place Recognition, Stereo Disparity Estimation, Point Cloud Registration, Monocular Depth Estimation, Multiple Object Tracking, Object Localization, Image Clustering, Stereo Image Super-Resolution, Vehicle Pose Estimation, 3D Single Object Tracking, Image Dehazing, Image-to-Image Translation, Depth Estimation, Prediction Of Occupancy Grid Maps, Text-To-SQL, Transfer Learning, Novel View Synthesis, Horizon Line Estimation, Depth Prediction, Image Super-Resolution, Monocular 3D Object Detection, 3D Object Tracking, Semantic Segmentation, Scene Generation, Unsupervised Object Detection, Stereo Depth Estimation, Unsupervised Monocular Depth Estimation, Monocular Cross-View Road Scene Parsing(Road), Object Tracking, Dense Pixel Correspondence Estimation, 3D Object Detection, Object Detection, 3D Object Detection From Stereo Images, Image to Point Cloud Registration, Pose Estimation, Optical Flow Estimation, Depth Completion, Scene Flow Estimation, Knowledge Distillation, Birds Eye View Object Detection, Panoptic Segmentation, Monocular Cross-View Road Scene Parsing(Vehicle), Egocentric Pose Estimation, Real-time Instance Segmentation, Visual Odometry","3D, Image, Text, Time Series, Video",English,Computer Vision,"transfer-learning-on-kitti-object-tracking, monocular-cross-view-road-scene-parsing, object-localization-on-kitti-cars-moderate, 3d-object-detection-on-kitti-cyclists-easy, birds-eye-view-object-detection-on-kitti-1, depth-estimation-on-kitti-2015, semantic-segmentation-on-kitti-semantic, multiple-object-tracking-on-kitti-test-online, birds-eye-view-object-detection-on-kitti-cars-4, optical-flow-estimation-on-kitti-2015-2, vehicle-pose-estimation-on-kitti-cars-hard, text-to-sql-on-2d-kitti-cars-easy, image-clustering-on-kitti, monocular-3d-object-detection-on-kitti-cars-1, 3d-object-detection-on-kitti-cyclist-hard-val, scene-generation-on-kitti, monocular-depth-estimation-on-kitti-2, birds-eye-view-object-detection-on-kitti, point-cloud-registration-on-kitti-fcgf, monocular-3d-object-detection-on-kitti-5, monocular-depth-estimation-on-kitti-eigen-1, 3d-object-detection-on-kitti-pedestrian-easy-1, object-localization-on-kitti-pedestrians-easy, object-localization-on-kitti-cyclists-easy, 3d-object-detection-on-kitti-cars-easy, monocular-3d-object-detection-on-kitti-cars, 3d-object-detection-on-kitti-pedestrians-hard, unsupervised-monocular-depth-estimation-on-1, 3d-object-detection-from-stereo-images-on-2, image-to-image-translation-on-kitti-object, pose-estimation-on-kitti-2015, 3d-object-detection-on-kitti-cyclist-moderate, object-tracking-on-kitti, birds-eye-view-object-detection-on-kitti-10, stereo-image-super-resolution-on-kitti2015-4x, novel-view-synthesis-on-kitti, monocular-3d-object-detection-on-kitti-6, image-dehazing-on-kitti, egocentric-pose-estimation-on-kitti-odometry, monocular-3d-object-detection-on-kitti-7, object-detection-on-kitti-cyclists-easy, stereo-image-super-resolution-on-kitti2012-2x-1, birds-eye-view-object-detection-on-kitti-7, 3d-object-detection-on-kitti-cars-hard, object-detection-on-kitti-pedestrians, knowledge-distillation-on-kitti, optical-flow-estimation-on-kitti-2012, birds-eye-view-object-detection-on-kitti-cars-5, unsupervised-monocular-depth-estimation-on-4, optical-flow-estimation-on-kitti-2015-train, stereo-depth-estimation-on-kitti-2015, test-results-on-kitti, scene-flow-estimation-on-kitti-2015-scene, object-detection-on-kitti-cyclists-moderate, dense-pixel-correspondence-estimation-on-2, 3d-object-detection-on-kitti-pedestrian, birds-eye-view-object-detection-on-kitti-17, birds-eye-view-object-detection-on-kitti-cars-2, 3d-object-detection-on-kitti-pedestrians-1, object-detection-on-kitti-cars-hard, image-super-resolution-on-kitti-2015-2x, depth-prediction-on-kitti-2015, 3d-object-detection-on-kitti-pedestrian-hard-1, birds-eye-view-object-detection-on-kitti-12, image-super-resolution-on-kitti-2015-4x, object-detection-on-kitti-pedestrians-easy, object-localization-on-kitti-cars-easy, birds-eye-view-object-detection-on-kitti-8, horizon-line-estimation-on-kitti-horizon, panoptic-segmentation-on-kitti-panoptic-1, monocular-depth-estimation-on-kitti-object, stereo-depth-estimation-on-kitti2015, object-detection-on-kitti-pedestrians-hard, stereo-image-super-resolution-on-kitti2012-2x-2, video-prediction-on-kitti, 3d-object-detection-on-kitti-cars-moderate-1, monocular-3d-object-detection-on-kitti, 3d-object-detection-on-kitti-pedestrian-hard, 3d-object-detection-from-stereo-images-on-1, stereo-depth-estimation-on-kitti2012, 3d-object-detection-from-stereo-images-on-3, image-super-resolution-on-kitti-2012-2x, birds-eye-view-object-detection-on-kitti-9, birds-eye-view-object-detection-on-kitti-5, point-cloud-registration-on-kitti-trained-on, novel-view-synthesis-on-kitti-novel-view, birds-eye-view-object-detection-on-kitti-2, monocular-3d-object-detection-on-kitti-3, object-localization-on-kitti-cars-hard, monocular-3d-object-detection-on-kitti-4, object-localization-on-kitti-cyclists, 3d-object-detection-on-kitti-cyclists-1, 3d-object-detection-on-kitti-pedestrian-2, scene-flow-estimation-on-kitti-2015-scene-1, 3d-object-detection-on-kitti-pedestrian-easy, optical-flow-estimation-on-kitti-2015, birds-eye-view-object-detection-on-kitti-13, monocular-depth-estimation-on-kitti-eigen, multiple-object-tracking-on-kitti-test, object-localization-on-kitti-pedestrians-hard, object-detection-on-kitti-cars-moderate, 3d-object-detection-on-kitti-cars-hard-val, object-detection-on-kitti-cyclists-hard, object-localization-on-kitti-cyclists-hard, birds-eye-view-object-detection-on-kitti-3, object-localization-on-kitti-pedestrians, object-localization-on-kitti-pedestrian-easy, stereo-image-super-resolution-on-kitti2012-4x, 3d-object-detection-on-kitti-cyclists, monocular-3d-object-detection-on-kitti-cars-2, stereo-image-super-resolution-on-kitti2012-2x-3, vehicle-pose-estimation-on-kitti, optical-flow-estimation-on-kitti-2012-2, real-time-instance-segmentation-on-kitti, 3d-object-detection-on-kitti-cars-easy-val, 3d-object-detection-on-kitti-pedestrians, point-cloud-registration-on-kitti, birds-eye-view-object-detection-on-kitti-cars-3, unsupervised-monocular-depth-estimation-on-5, 3d-object-detection-on-kitti-cyclist-easy-val, depth-estimation-on-kitti-eigen-split-3, depth-completion-on-kitti, monocular-cross-view-road-scene-parsing-road-1, unsupervised-object-detection-on-kitti, dense-pixel-correspondence-estimation-on-1, image-super-resolution-on-kitti-2012-4x, monocular-3d-object-detection-on-kitti-1, birds-eye-view-object-detection-on-kitti-cars-1, 3d-object-detection-on-kitti-cyclists-hard, stereo-disparity-estimation-on-kitti-2015, object-detection-on-kitti-cars-easy, image-to-point-cloud-registration-on-kitti, visual-place-recognition-on-kitti, birds-eye-view-object-detection-on-kitti-cars, 3d-object-detection-on-kitti-pedestrian-1, birds-eye-view-object-detection-on-kitti-6, birds-eye-view-object-detection-on-kitti-16, birds-eye-view-object-detection-on-kitti-11, 3d-object-detection-on-kitti-pedestrians-easy, stereo-image-super-resolution-on-kitti2015-2x, monocular-cross-view-road-scene-parsing-road, birds-eye-view-object-detection-on-kitti-4",CC BY-NC-SA 3.0,http://www.cvlibs.net/datasets/kitti/,https://paperswithcode.com/dataset/kitti,"KITTI (Karlsruhe Institute of Technology and Toyota Technological Institute) is one of the most popular datasets for use in mobile robotics and autonomous driving. It consists of hours of traffic scenarios recorded with a variety of sensor modalities, including high-resolution RGB, grayscale stereo cameras, and a 3D laser scanner. Despite its popularity, the dataset itself does not contain ground truth for semantic segmentation. However, various researchers have manually annotated parts of the dataset to fit their necessities. Álvarez et al. generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky. Zhang et al. annotated 252 (140 for training and 112 for testing) acquisitions – RGB and Velodyne scans – from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence. Ros et al. labeled 170 training images and 46 testing images (from the visual odometry challenge) with 11 classes: building, tree, sky, car, sign, road, pedestrian, fence, pole, sidewalk, and bicyclist.",,A Review on Deep Learning Techniques Applied to Semantic Segmentation,https://arxiv.org/abs/1704.06857,323 images,"training and 112 for testing) acquisitions – RGB and Velodyne scans – from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence. Ros et al. labeled 170 training images",11
1652,KITTI_MOTS,Multiple Object Tracking,Multiple Object Tracking,"Multiple Object Tracking, 2D Object Detection, Multi-Object Tracking and Segmentation, Object Tracking, Multi-Object Tracking, 2D Semantic Segmentation","Image, Video",English,Computer Vision,multi-object-tracking-and-segmentation-on-1,Creative Commons Attribution 4.0 International,https://www.cvlibs.net/datasets/kitti/eval_mots.php,https://paperswithcode.com/dataset/kitti-mots,"The Multi-Object and Segmentation (MOTS) benchmark [2] consists of 21 training sequences and 29 test sequences. It is based on the KITTI Tracking Evaluation 2012 and extends the annotations to the Multi-Object and Segmentation (MOTS) task. To this end, we added dense pixel-wise segmentation labels for every object. We evaluate submitted results using the metrics HOTA, CLEAR MOT, and MT/PT/ML. We rank methods by HOTA [1]. Our development kit and GitHub evaluation code provide details about the data format as well as utility functions for reading and writing the label files. (adapted for the segmentation case). Evaluation is performed using the code from the TrackEval repository.

[1] J. Luiten, A. Os̆ep, P. Dendorfer, P. Torr, A. Geiger, L. Leal-Taixé, B. Leibe: HOTA: A Higher Order Metric for Evaluating Multi-object Tracking. IJCV 2020.
[2] P. Voigtlaender, M. Krause, A. Os̆ep, J. Luiten, B. Sekar, A. Geiger, B. Leibe: MOTS: Multi-Object Tracking and Segmentation. CVPR 2019.",2012,MOTS: Multi-Object Tracking and Segmentation.,https://arxiv.org/pdf/1902.03604.pdf,,,
1653,KITTI_Road,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Scene Understanding, Autonomous Driving",Image,English,Computer Vision,,Custom,http://www.cvlibs.net/datasets/kitti/eval_road.php,https://paperswithcode.com/dataset/kitti-road,"KITTI Road is road and lane estimation benchmark that consists of 289 training and 290 test images. It contains three different categories of road scenes:
* uu - urban unmarked (98/100)
* um - urban marked (95/96)
* umm - urban multiple marked lanes (96/94)
* urban - combination of the three above
Ground truth has been generated by manual annotation of the images and is available for two different road terrain types: road - the road area, i.e, the composition of all lanes, and lane - the ego-lane, i.e., the lane the vehicle is currently driving on (only available for category ""um""). Ground truth is provided for training images only.",,,,,training and 290 test images,
1654,KIT_Motion-Language,Motion Synthesis,Motion Synthesis,"Motion Synthesis, Feature Engineering, Motion Captioning","Image, Text, Video",English,Computer Vision,"motion-captioning-on-kit-motion-language, motion-synthesis-on-kit-motion-language",,https://gitlab.com/h2t/MotionAnnotation,https://paperswithcode.com/dataset/kit-motion-language,The KIT Motion-Language is a dataset linking human motion and natural language.,,,,,,
1655,KKBox,Click-Through Rate Prediction,Click-Through Rate Prediction,Click-Through Rate Prediction,Time Series,,Methodology,click-through-rate-prediction-on-kkbox,,https://www.kkbox.com/hk/tc/,https://paperswithcode.com/dataset/kkbox,"The task is to predict the chances of a user listening to a song repetitively after the first observable listening event within a time window was triggered. If there are recurring listening event(s) triggered within a month after the user's very first observable listening event, its target is marked 1, and 0 otherwise in the training set. KKBox provides a training data set consists of information of the first observable listening event for each unique user-song pair within a specific time duration. Metadata of each unique user and song pair is also provided. The train and the test data are selected from users listening history in a given time period, and are split based on time. Note that only the labeled train set of the dataset is used for benchmarking.",,,,,,
1656,Kleister_NDA,Key Information Extraction,Key Information Extraction,Key Information Extraction,,,Methodology,key-information-extraction-on-kleister-nda,,https://github.com/applicaai/kleister-nda,https://paperswithcode.com/dataset/kleister-nda,"Kleister NDA is a dataset for Key Information Extraction (KIE). The dataset contains a mix of scanned and born-digital long formal English-language documents. For this datasets, an NLP system is expected to find or infer various types of entities by employing both textual and structural layout features.  The Kleister NDA dataset has 540 Non-disclosure Agreements, with 3,229 unique pages and 2,160 entities to extract.",,,,,,
1657,Klexikon,Text Summarization,Text Summarization,"Text Summarization, Text Simplification",Text,English,Natural Language Processing,text-summarization-on-klexikon,CC-BY-SA,https://github.com/dennlinger/klexikon,https://paperswithcode.com/dataset/klexikon,"The dataset introduces document alignments between German Wikipedia and the children's lexicon Klexikon.
The source texts in Wikipedia are both written in a more complex language than Klexikon, and also significantly longer, which makes this a suitable application for both summarization and simplification.
In fact, previous research has so far only focused on either of the two, but not comprehensively been studied as a joint task.",,,,,,
1658,Kligler,Document Shadow Removal,Document Shadow Removal,"Document Shadow Removal, Shadow Removal",Text,English,Natural Language Processing,,,https://cgm.technion.ac.il/Computer-Graphics-Multimedia/Software/VisibilityDetection/,https://paperswithcode.com/dataset/kligler,Dataset for document shadow removal,,,,,,
1659,KLUE,Machine Reading Comprehension,Machine Reading Comprehension,"Machine Reading Comprehension, Dialogue State Tracking, Topic Classification, Relation Extraction, Dependency Parsing, Natural Language Inference, Named Entity Recognition (NER), Text Classification","Graph, Image, Text, Video",English,Computer Vision,text-classification-on-klue,,https://klue-benchmark.com/,https://paperswithcode.com/dataset/klue,"Korean Language Understanding Evaluation (KLUE) benchmark is a series of datasets to evaluate natural language understanding capability of Korean language models. KLUE consists of 8 diverse and representative tasks, which are accessible to anyone without any restrictions. With ethical considerations in mind, we deliberately design annotation guidelines to obtain unambiguous annotations for all datasets. Furthermore, we build an evaluation system and carefully choose evaluations metrics for every task, thus establishing fair comparison across Korean language models.

KLUE benchmark is composed of 8 tasks:


Topic Classification (TC)
Sentence Textual Similarity (STS)
Natural Language Inference (NLI)
Named Entity Recognition (NER)
Relation Extraction (RE)
(Part-Of-Speech) + Dependency Parsing (DP)
Machine Reading Comprehension (MRC)
Dialogue State Tracking (DST)",,,,,,
1660,KnowEdit,Model Editing,Model Editing,"Model Editing, knowledge editing",,,Methodology,,MIT,https://huggingface.co/datasets/zjunlp/KnowEdit,https://paperswithcode.com/dataset/knowedit,"This is the dataset for knowledge editing.
It contains six tasks: ZsRE, $Wiki_{recent}$, $Wiki_{counterfact}$, WikiBio, ConvSent and Sanitation.
This repo shows the former 4 tasks and you can get the data for ConvSent and Sanitation from their original papers.",,,,,,
1661,KoDF,DeepFake Detection,DeepFake Detection,DeepFake Detection,Image,,Computer Vision,,,https://moneybrain-research.github.io/kodf,https://paperswithcode.com/dataset/kodf,"The Korean DeepFake Detection Dataset (KoDF) is a large-scale collection of synthesized and real videos focused on Korean subjects, used for the task of deepfake detection.

The dataset consists of 62,166 real videos and 175,776 fake videos from 403 subjects. The fake videos are created using 6 different methods: FaceSwap, DeepFaceLab, FSGAN, FOMM, ATFHP and Wav2Lip.",,,,,,
1662,KOHTD,Handwriting Recognition,Handwriting Recognition,Handwriting Recognition,Image,,Computer Vision,handwriting-recognition-on-kohtd,,https://github.com/abdoelsayed2016/KOHTD,https://paperswithcode.com/dataset/kohtd,Kazakh offline Handwritten Text dataset (KOHTD) has 3000 handwritten exam papers and more than 140335 segmented images and there are approximately 922010 symbols. It can serve researchers in the field of handwriting recognition tasks by using deep and machine learning.,,,,,,
1663,KolektorSDD2,Self-Supervised Anomaly Detection,Self-Supervised Anomaly Detection,"Self-Supervised Anomaly Detection, Defect Detection, Weakly Supervised Defect Detection, Supervised Defect Detection, Unsupervised Anomaly Detection",Image,,Computer Vision,"weakly-supervised-defect-detection-on-3, unsupervised-anomaly-detection-on, supervised-defect-detection-on-kolektorsdd2, self-supervised-anomaly-detection-on, defect-detection-on-kolektorsdd2",,https://www.vicos.si/Downloads/KolektorSDD2,https://paperswithcode.com/dataset/kolektorsdd2,"KolektorSDD2 is a surface-defect detection dataset with over 3000 images containing several types of defects, obtained while addressing a real-world industrial problem.

The dataset consists of:


356 images with visible defects
2979 images without any defect
image sizes of approximately 230 x 630 pixels
train set with 246 positive and 2085 negative images
test set with 110 positive and 894 negative images
several different types of defects (scratches, minor spots, surface imperfections, etc.)",2085,,,3000 images,train set with 246 positive and 2085 negative images,
1664,Kompetencer,Document Text Classification,Document Text Classification,"Document Text Classification, Job classification","Image, Text",English,Computer Vision,,,https://github.com/jjzha/kompetencer/tree/master/data,https://paperswithcode.com/dataset/kompetencer,Kompetencer (en: competences) is a Danish job posting dataset annotated for nested spans of competences.,,,,,,
1665,KoNViD-1k,Video Quality Assessment,Video Quality Assessment,Video Quality Assessment,Video,,Methodology,video-quality-assessment-on-konvid-1k,Creative Commons,http://database.mmsp-kn.de/konvid-1k-database.html,https://paperswithcode.com/dataset/konvid-1k,"Subjective video quality assessment (VQA) strongly depends on semantics, context, and the types of visual distortions. A lot of existing VQA databases cover small numbers of video sequences with artificial distortions. When testing newly developed Quality of Experience (QoE) models and metrics, they are commonly evaluated against subjective data from such databases, that are the result of perception experiments. However, since the aim of these QoE models is to accurately predict natural videos, these artificially distorted video databases are an insufficient basis for learning. Additionally, the small sizes make them only marginally usable for state-of-the-art learning systems, such as deep learning. In order to give a better basis for development and evaluation of objective VQA methods, we have created a larger datasets of natural, real-world video sequences with corresponding subjective mean opinion scores (MOS) gathered through crowdsourcing.
​
We took YFCC100m as a baseline database, consisting of 793436 Creative Commons (CC) video sequences, filtered them through multiple steps to ensure that the video sequences are representative of the whole spectrum of available video content, types of distortions, and subjective quality. The resulting 1200 videos are available to download, alongside the subjective data and evaluation of the best-performing techniques available for multiple video attributes. Namely, we have evaluated blur, colorfulness, contrast, spatial information, temporal information and video quality.",,,,,,
1666,Kosp2e,Speech-to-Text Translation,Speech-to-Text Translation,Speech-to-Text Translation,"Audio, Text",English,Speech,,Multiple licenses,https://github.com/warnikchow/kosp2e,https://paperswithcode.com/dataset/kosp2e,"Kosp2e (read as `kospi'), is a corpus that allows Korean speech to be translated into English text in an end-to-end manner",,,,,,
1667,KP20k,Text Summarization,Text Summarization,"Text Summarization, Language Modelling, Phrase Ranking, Phrase Tagging, Multi-Task Learning, Keyphrase Extraction",Text,English,Natural Language Processing,"phrase-ranking-on-kp20k, phrase-tagging-on-kp20k, keyphrase-extraction-on-kp20k",,https://github.com/memray/seq2seq-keyphrase,https://paperswithcode.com/dataset/kp20k,"KP20k is a large-scale scholarly articles dataset with 528K articles for training, 20K articles for validation and 20K articles for testing.",,Keyphrase Prediction With Pre-trained Language Model,https://arxiv.org/abs/2004.10462,,,
1668,KPTimes,Language Modelling,Language Modelling,"Language Modelling, Phrase Tagging, Phrase Ranking, Keyword Extraction, Text Generation, Keyphrase Extraction",Text,English,Natural Language Processing,"phrase-tagging-on-kptimes, phrase-ranking-on-kptimes, keyphrase-extraction-on-kptimes",,https://github.com/ygorg/KPTimes,https://paperswithcode.com/dataset/kptimes,KPTimes is a large-scale dataset of news texts paired with editor-curated keyphrases.,,,,,,
1669,Krapivin,Keyphrase Extraction,Keyphrase Extraction,Keyphrase Extraction,,,Methodology,keyphrase-extraction-on-krapivin,,https://huggingface.co/datasets/midas/krapivin,https://paperswithcode.com/dataset/krapivin,"A dataset for benchmarking keyphrase extraction and generation techniques from long document English scientific papers. The dataset has high quality and consists of 2,000 scientific papers from the Computer Science domain published by ACM. Each paper has its keyphrases assigned by the authors and verified by the reviewers. Different parts of papers, such as title and abstract, are separated, enabling extraction based on the part of an article's text. The content of each paper is converted from PDF to plain text. The pieces of formulae, tables, figures and LaTeX mark up were removed automatically. Link: https://huggingface.co/datasets/midas/krapivin",,,,,,
1670,KRAUTS,Temporal Tagging,Temporal Tagging,Temporal Tagging,"Time Series, Video",,Methodology,temporal-tagging-on-krauts,CC-BY-NC,https://github.com/JannikStroetgen/KRAUTS,https://paperswithcode.com/dataset/krauts,"KRAUTS (Korpus of newspapeR Articles with Underlinded Temporal expressionS) is a German temporally annotated news corpus accompanied with TimeML annotation guidelines for German. It was developed at Fondazione Bruno Kessler, Trento, Italy and at the Max Planck Institute for Informatics, Saarbrücken, Germany. Our goal is to boost temporal tagging research for German.",,,,,,
1671,KTH,Video Prediction,Video Prediction,"Video Prediction, Temporal Action Localization, Action Recognition","Image, Time Series, Video",,Computer Vision,"action-recognition-on-kth, video-prediction-on-kth","Custom (non-commercial, attribution)",https://www.csc.kth.se/cvap/actions/,https://paperswithcode.com/dataset/kth,"The efforts to create a non-trivial and publicly available dataset for action recognition was initiated at the KTH Royal Institute of Technology in 2004. The KTH dataset is one of the most standard datasets, which contains six actions: walk, jog, run, box, hand-wave, and hand clap. To account for performance nuance, each action is performed by 25 different individuals, and the setting is systematically altered for each action per actor. Setting variations include: outdoor (s1), outdoor with scale variation (s2), outdoor with different clothes (s3), and indoor (s4). These variations test the ability of each algorithm to identify actions independent of the background, appearance of the actors, and the scale of the actors.",2004,Review of Action Recognition and Detection Methods,https://arxiv.org/abs/1610.06906,,,
1672,KuaiRand,Multi-Armed Bandits,Multi-Armed Bandits,"Multi-Armed Bandits, Multi-Task Learning, Sequential Recommendation, Recommendation Systems",,,Methodology,sequential-recommendation-on-kuairand,,https://kuairand.com,https://paperswithcode.com/dataset/kuairand,"KuaiRand is an unbiased sequential recommendation dataset collected from the recommendation logs of the video-sharing mobile app, Kuaishou (快手). It is the first recommendation dataset with millions of intervened interactions of randomly exposed items inserted in the standard recommendation feeds!",,,,,,
1673,KUAKE-QIC,Intent Classification,Intent Classification,Intent Classification,Image,,Computer Vision,intent-classification-on-kuake-qic,,https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414,https://paperswithcode.com/dataset/kuake-qic,"KUAKE Query Intent Classification, a dataset for intent classification, is used for the
KUAKE-QIC task. Given the queries of search engines, the task requires to classify each of them into
one of 11 medical intent categories defined in KUAKE-QIC, including diagnosis, etiology analysis,
treatment plan, medical advice, test result analysis, disease description, consequence prediction,
precautions, intended effects, treatment fees, and others.",,,,,,
1674,Kubric,Visual Tracking,Visual Tracking,Visual Tracking,"Image, Video",,Computer Vision,visual-tracking-on-kubric,Apache-2.0,https://github.com/google-research/kubric,https://paperswithcode.com/dataset/kubric,"Kubric is a data generation pipeline for creating semi-realistic synthetic multi-object videos with rich annotations such as instance segmentation masks, depth maps, and optical flow.

It also presents a series of 13 different generated datasets for tasks ranging from studying 3D NeRF models to optical flow estimation.

Kubric is mainly built on-top of pybullet (for physics simulation) and Blender (for rendering); however, the code is kept modular to potentially support different rendering backends.",,,,,,
1675,Kuzushiji-49,Image Classification,Image Classification,"Image Classification, Meta-Learning",Image,,Computer Vision,,,https://github.com/rois-codh/kmnist,https://paperswithcode.com/dataset/kuzushiji-49,"Kuzushiji-49 is an MNIST-like dataset that has 49 classes (28x28 grayscale, 270,912 images) from 48 Hiragana characters and one Hiragana iteration mark.",,Clanuwat et al,https://arxiv.org/pdf/1812.01718v1.pdf,912 images,,49
1676,Kvasir-Capsule,Object Detection,Object Detection,"Object Detection, Image Classification, Colorectal Polyps Characterization, General Classification",Image,,Computer Vision,,Creative Commons Attribution 4.0 International,https://osf.io/dv2ag/,https://paperswithcode.com/dataset/kvasir-capsule,"Kvasir-Capsule dataset is the largest publicly released VCE dataset. In total, the dataset contains 47,238 labeled images and 117 videos, where it captures anatomical landmarks and pathological and normal findings. The results is more than 4,741,621 images and video frames altogether.",,,,621 images,,
1677,Kvasir-Instrument,Real-Time Object Detection,Real-Time Object Detection,"Real-Time Object Detection, Real-Time Semantic Segmentation, Semantic Segmentation, Medical Image Segmentation, Semi-Supervised Semantic Segmentation, Instrument Recognition",Image,,Computer Vision,"semantic-segmentation-on-kvasir-instrument, semi-supervised-semantic-segmentation-on-33, medical-image-segmentation-on-kvasir",Creative Commons Attribution 4.0 International,https://datasets.simula.no/kvasir-instrument/,https://paperswithcode.com/dataset/kvasir-instrument,"Consists of  annotated frames containing GI procedure tools such as snares, balloons and biopsy forceps, etc. Beside of the images, the dataset includes ground truth masks and bounding boxes and has been verified by two expert GI endoscopists.",,,,,,
1678,Kvasir-SEG,Real-Time Object Detection,Real-Time Object Detection,"Real-Time Object Detection, Real-Time Semantic Segmentation, Semantic Segmentation, Medical Image Segmentation, Colorectal Polyps Characterization, Polyp Segmentation, Object Detection, Colorectal Gland Segmentation:",Image,,Computer Vision,"medical-image-segmentation-on-kvasir-seg, colorectal-polyps-characterization-on-kvasir, polyp-segmentation-on-kvasir-seg",Custom,https://datasets.simula.no/kvasir-seg/.,https://paperswithcode.com/dataset/kvasir-seg,"Kvasir-SEG is an open-access dataset of gastrointestinal polyp images and corresponding segmentation masks, manually annotated by a medical doctor and then verified by an experienced gastroenterologist.",,,,,,
1679,Kvasir-VQA,Image Captioning,Image Captioning,"Image Captioning, Medical Diagnosis, Medical Image Generation, Medical Visual Question Answering, Visual Question Answering, Image Generation","Image, Text",English,Computer Vision,,CC BY-NC,https://datasets.simula.no/kvasir-vqa/,https://paperswithcode.com/dataset/kvasir-vqa,"The Kvasir-VQA dataset is an extended dataset derived from the HyperKvasir and Kvasir-Instrument datasets, augmented with question-and-answer annotations. This dataset is designed to facilitate advanced machine learning tasks in gastrointestinal (GI) diagnostics, including image captioning, Visual Question Answering (VQA) and text-based generation of synthetic medical images.",,,,,"testinal (GI) diagnostics, including image captioning, Visual Question Answering (VQA) and text-based generation of synthetic medical images",
1680,Kvasir,Real-Time Object Detection,Real-Time Object Detection,"Real-Time Object Detection, Real-Time Semantic Segmentation, Semantic Segmentation, Image Classification, Medical Image Segmentation, Colorectal Polyps Characterization, Polyp Segmentation, Object Detection, Colorectal Gland Segmentation:",Image,,Computer Vision,"medical-image-segmentation-on-kvasir-seg, colorectal-polyps-characterization-on-kvasir, polyp-segmentation-on-kvasir-seg, image-classification-on-kvasir","Custom (research-only, non-commercial)",https://datasets.simula.no/kvasir/,https://paperswithcode.com/dataset/kvasir,"The KVASIR Dataset was released as part of the medical multimedia challenge presented by MediaEval. It is based on images obtained from the GI tract via an endoscopy procedure. The dataset is composed of images that are annotated and verified by medical doctors, and captures 8 different classes. The classes are based on three anatomical landmarks (z-line, pylorus, cecum), three pathological findings (esophagitis, polyps, ulcerative colitis) and two other classes (dyed and lifted polyps, dyed resection margins) related to the polyp removal process. Overall, the dataset contains 8,000 endoscopic images, with 1,000 image examples per class.",,Two-Stream Deep Feature Modelling for Automated Video Endoscopy Data Analysis,https://arxiv.org/abs/2007.05914,,"val. It is based on images obtained from the GI tract via an endoscopy procedure. The dataset is composed of images that are annotated and verified by medical doctors, and captures 8 different classes. The classes are based on three anatomical landmarks (z-line, pylorus, cecum), three pathological findings (esophagitis, polyps, ulcerative colitis) and two other classes (dyed and lifted polyps, dyed resection margins) related to the polyp removal process. Overall, the dataset contains 8,000 endoscopic images",
1681,L-Eval,Long-Context Understanding,Long-Context Understanding,Long-Context Understanding,,,Methodology,long-context-understanding-on-l-eval,,https://github.com/OpenLMLab/LEval,https://paperswithcode.com/dataset/l-eval,"Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability. The code and datasets are available at https://github.com/THUDM/LongBench.",,,,,,
1682,L3DAS21,Sound Event Localization and Detection,Sound Event Localization and Detection,"Sound Event Localization and Detection, Sound Event Detection, Speech Enhancement","Audio, Image",,Computer Vision,"sound-event-localization-and-detection-on, sound-event-detection-on-l3das21",Attribution 4.0 International,https://zenodo.org/record/4642005,https://paperswithcode.com/dataset/l3das21,"L3DAS21 is a dataset for 3D audio signal processing. It consists of a 65 hours 3D audio corpus, accompanied with a Python API that facilitates the data usage and results submission stage. 

The LEDAS21 datasets contain multiple-source and multiple-perspective B-format Ambisonics audio recordings. The authors sampled the acoustic field of a large office room, placing two first-order Ambisonics microphones in the center of the room and moving a speaker reproducing the analytic signal in 252 fixed spatial positions. Relying on the collected Ambisonics impulse responses (IRs), the authors augmented existing clean monophonic datasets to obtain synthetic tridimensional sound sources by convolving the original sounds with our IRs. 

The dataset is divided in two main sections, respectively dedicated to the challenge tasks.

The first section is optimized for 3D Speech Enhancement and contains more than 30000 virtual 3D audio environments with a duration up to 10 seconds. In each sample, a spoken voice is always present alongside with other office-like background noises. As target data for this section the authors provide the clean monophonic voice signals.

The other sections, instead, is dedicated to the 3D Sound Event Localization and Detection task and contains 900 60-seconds-long audio files. Each data point contains a simulated 3D office audio environment in which up to 3 simultaneous acoustic events may be active at the same time. In this section, the samples are not forced to contain a spoken voice.  As target data for this section the authors provide a list of the onset and offset time stamps, the typology class, and the spatial coordinates of each individual sound event present in the data-points.",,,,,,
1683,L3DAS22,Sound Event Localization and Detection,Sound Event Localization and Detection,"Sound Event Localization and Detection, Sound Event Detection, Speech Enhancement","Audio, Image",,Computer Vision,,,https://www.kaggle.com/datasets/l3dasteam/l3das22,https://paperswithcode.com/dataset/l3das22,"L3DAS22: MACHINE LEARNING FOR 3D AUDIO SIGNAL PROCESSING
This dataset supports the L3DAS22 IEEE ICASSP Gand Challenge. The challenge is supported by a Python API that facilitates the dataset download and preprocessing, the training and evaluation of the baseline models and the results submission.

Scope of the Challenge
The L3DAS22 Challenge aims at encouraging and fostering research on machine learning for 3D audio signal processing.
3D audio is gaining increasing interest in the machine learning community in recent years. The range of applications is incredibly wide, extending from virtual and real conferencing to autonomous driving, surveillance and many more. In these contexts, a fundamental procedure is to properly identify the nature of events present in a soundscape, their spatial position and eventually remove unwanted noises that can interfere with the useful signal. To this end, L3DAS22 Challenge presents two tasks: 3D Speech Enhancement and 3D Sound Event Localization and Detection, both relying on first-order Ambisonics recordings in reverberant office environments.
Each task involves 2 separate tracks: 1-mic and 2-mic recordings, respectively containing sounds acquired by one 1st order Ambisonics microphone and by an array of two ones. The use of two Ambisonics microphones represents one of the main novelties of the L3DAS22 Challenge. We expect higher accuracy/reconstruction quality when taking advantage of the dual spatial perspective of the two microphones. Moreover, we are very interested in identifying other possible advantages of this configuration over standard Ambisonics formats.
Interactive demos of our baseline models are available on Replicate.
Top 5 ranked teams can submit a regular paper according to the ICASSP guidelines.
Prizes will be awarded to the challenge winners thanks to the support of Kuaishou Technology.

Tasks
Tasks
The tasks we propose are:
  * 3D Speech Enhancement
The objective of this task is the enhancement of speech signals immersed in the spatial sound field of a reverberant office environment. Here the models are expected to extract the monophonic voice signal from the 3D mixture containing various background noises. The evaluation metric for this task is a combination of short-time objective intelligibility (STOI) and word error rate (WER).
  * 3D Sound Event Localization and Detection
The aim of this task is to detect the temporal activities of a known set of sound event classes and, in particular, to further locate them in the space. Here the models must predict a list of the active sound events and their respective location at regular intervals of 100 milliseconds. Performance on this task is evaluated according to the location-sensitive detection error, which joins the localization and detection error metrics.

Dataset Info
The L3DAS22 datasets contain multiple-source and multiple-perspective B-format Ambisonics audio recordings. We sampled the acoustic field of a large office room, placing two first-order Ambisonics microphones in the center of the room and moving a speaker reproducing the analytic signal in 252 fixed spatial positions. Relying on the collected Ambisonics impulse responses (IRs), we augmented existing clean monophonic datasets to obtain synthetic tridimensional sound sources by convolving the original sounds with our IRs. We extracted speech signals from the Librispeech dataset and office-like background noises from the FSD50K dataset. We aimed at creating plausible and variegate 3D scenarios to reflect possible real-life situations in which sound and disparate types of background noises coexist in the same 3D reverberant environment. We provide normalized raw waveforms as predictors data and the target data varies according to the task.

The dataset is divided in two main sections, respectively dedicated to the challenge tasks.

The first section is optimized for 3D Speech Enhancement and contains more than 60000 virtual 3D audio environments with a duration up to 12 seconds. In each sample, a spoken voice is always present alongside with other office-like background noises. As target data for this section we provide the clean monophonic voice signals. For each subset we also provide a csv file, where we annotated the coordinates and spatial distance of the IR convolved with the target voice signals for each datapoint. This may be useful to estimate the delay caused by the virtual time-of-flight of the target voice signal and to perform a sample-level alignment of the input and ground truth signals.

The other sections, instead, is dedicated to the 3D Sound Event Localization and Detection task and contains 900 30-seconds-long audio files. Each data point contains a simulated 3D office audio environment in which up to 3 simultaneous acoustic events may be active at the same time. In this section, the samples are not forced to contain a spoken voice. As target data for this section we provide a list of the onset and offset time stamps, the typology class, and the spatial coordinates of each individual sound event present in the data-points.

We split both dataset sections into a training set and a development set, paying attention to create similar distributions. The train set of the SE section is divided in two partitions: train360 and train100, and contain speech samples extracted from the correspondent partitions of Librispeech (only the sample) up to 12 seconds. The train360 is split in 2 zip files for a more convenient download. All sets of the SELD section are divided in: OV1, OV2, OV3. These partitions refer to the maximum amount of possible overlapping sounds, which are 1, 2 or 3, respectively.

L3DAS22 Challenge Supporting API
The gitHub supporting API is aimed at downloading the dataset, pre-processing the sound files and the metadata, training and evaluating the baseline models and validating the final results. We provide easy-to-use instruction to produce the results included in our paper. Moreover, we extensively commented our code for easy customization.
For further information please refer to the challenge website and to the challenge documentation.",,challenge documentation,https://www.l3das.com/assets/file/L3DAS22_ICASSP_documentation.pdf,,"split both dataset sections into a training set and a development set, paying attention to create similar distributions. The train set of the SE section is divided in two partitions: train360 and train100, and contain speech samples",
1684,Labelling_for_Explosions_and_Road_accidents_from_U,Change Point Detection,Change Point Detection,Change Point Detection,Image,,Computer Vision,,,,https://paperswithcode.com/dataset/labelling-for-explosions-and-road-accidents,"The whole UCF-Crime dataset consists of real-world 240 × 320 RGB videos with 13 realistic anomaly types such as explosion, road accident, burglary, etc., and normal examples. The CPD specific requires a change in data distribution. We suppose that explosions and road accidents correspond to such a scenario, while most other types correspond to point anomalies. For example, data, obviously, com from a normal regime before the explosion. After it, we can see fire and smoke, which last for some time. Thus, the first moment when an explosion appears is a change point. Along with a volunteer, the authors carefully labelled chosen anomaly types. Their opinions were averaged. We provide the obtained markup, so other researchers can use it to validate their CPD algorithm for video.",,,,,,
1685,LabPics,Relation Classification,Relation Classification,"Relation Classification, Material Classification, Semantic Segmentation, Relation Extraction, Material Recognition","Graph, Image",,Computer Vision,,CC0 1.0 Universal,https://www.cs.toronto.edu/chemselfies/,https://paperswithcode.com/dataset/labpics,"LabPics Chemistry Dataset

Dataset for computer vision for materials segmentation and classification in chemistry labs, medical labs, and any setting where materials are handled inside containers.
The Vector-LabPics dataset comprises 7900 images of materials in various phases and processes within mostly transparent vessels in chemistry labs, medical labs and hospitals, and other environments. The images are annotated for both the vessels and the individual material phases inside them, and each instance is assigned one or more classes (liquid, solid, foam, suspension, powder, gel, granular, vapor) . The fill level, labels, corks, and other parts of the vessel are also annotated. The material classes cover the main states of matter, including liquids, solids, vapors, foams, gels, and subcategories like powder, granular, and suspension. Relationships between materials, such as which material is immersed inside other materials, are annotated. The vessel class cover glassware, labware plates, bottles, and any other type of vessel that is used to contain or carry materials. The type of vessel (e.g., syringe, tube, cup, infusion bottle/bag), and the properties of the vessel (transparent, opaque) are annotated. In addition, vessel parts such as corks, labels, spikes, and valves are annotated. Relations and hierarchies between vessels and materials are also annotated, such as which vessel contains which material or which vessels are linked or contain each other. The images were collected from various contributors and covered most aspects of chemistry lab works as well as a variety of other fields where materials are handled in container vessels. Documents specifying annotation formats are available inside the dataset file. Version 1 contain 2200 images with simple instance and semantic annotations, and is relatively simple to use, it is described in the paper ""Computer Vision for Recognition of Materials and Vessels in Chemistry Lab Settings and the Vector-LabPics Data Set""

Format

The dataset contains annotated images for both material and vessels in chemistry labs, medical labs, and any area where liquids and solids are handled within vessels. There are two levels of annotation for each image. One annotation set for vessels and the second for the material phases inside these vessels. Vessels are defined as any container that can carry materials such as Jars, Erlenmayers, Tubes, Funnels, syringes, IV bags, and any other labware or glassware that can contain or carry materials. Material phases are any material contained within or on the vessel. For example, for two-phase separating liquids, each liquid phase is annotated as one instance. If there is foam above the liquid or a chunk of solid inside the liquid, the foam, liquid, and solid will be annotated as different phases. In addition, vessel parts like cork, labels, and valves are annotated as instances. For each instance, there is a list of all the classes it belongs to, and a list of its property. For vessels, the instance classes are the vessel type (Cup, jar, Separatory-funnel…) and the vessel properties (Transparent, Opaque…). For materials, the classes are the material types ( Liquid, solid, suspension, foam, powder…) and their properties (Scattered, On vessel surface…), and for parts, the part type (cork/label). In addition, the relations between instances are annotated. This includes which material instances are inside which vessels, which vessels are linked to each other or are inside each other (for vessels inside other vessels), and which material phase is immersed inside another material phase. In addition to instance segmentation maps, the dataset also includes semantic segmentation maps that give each pixel in the image all the classes to which it belongs. In other words, for each class (Liquid, Solid, Vessel, Foam), there is a map of all the regions in the image belonging to this class. Note that every pixel and every instance can have several classes. In addition, instances often overlap, like in the case of material inside the vessel, vessel inside the vessel, and material phase immersed inside other material (like solid inside liquid).",,,,7900 images,"valves are annotated. Relations and hierarchies between vessels and materials are also annotated, such as which vessel contains which material or which vessels are linked or contain each other. The images",
1686,LABR,Sentiment Analysis,Sentiment Analysis,"Sentiment Analysis, Arabic Sentiment Analysis",Text,English,Natural Language Processing,sentiment-analysis-on-labr-2-class-unbalanced-1,Custom,https://github.com/mohamedadaly/LABR,https://paperswithcode.com/dataset/labr,"LABR is a large sentiment analysis dataset to-date for the Arabic language. It consists of over 63,000 book reviews, each rated on a scale of 1 to 5 stars.",,,,,,
1687,LaFAN1,Motion Synthesis,Motion Synthesis,Motion Synthesis,Video,,Methodology,motion-synthesis-on-lafan1,Creative Commons Attribution 4.0 International,https://github.com/ubisoft/ubisoft-laforge-animation-dataset,https://paperswithcode.com/dataset/lafan1,"Ubisoft La Forge Animation Dataset (""LAFAN1"")
Ubisoft La Forge Animation dataset and accompanying code for the SIGGRAPH 2020 paper Robust Motion In-betweening.

Shot in May 2017.

This dataset can be used under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License (see license.txt).

If you use this dataset or transition benchmarking code, please consider citing the paper:
@article{harvey2020robust,
author    = {Félix G. Harvey and Mike Yurick and Derek Nowrouzezahrai and Christopher Pal},
title     = {Robust Motion In-Betweening},
booktitle = {ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH)},
publisher = {ACM},
volume    = {39},
number    = {4},
year      = {2020}
}

You may also want to consider the following papers, as they also use this dataset (or parts of it):



Learned Motion Matching (Holden et al., 2020)



Subspace Neural Physics: Fast Data-Driven Interactive Simulation (Holden et al., 2019)



DReCon: Data-Driven Responsive Control of Physics-Based Characters (Bergamin et al., 2019)



Robust Solving of Optical Motion Capture Data by Denoising (Holden, 2018)



Recurrent Transition Networks for Character Locomotion (Harvey et al., 2018)



Data
The animation data is contained in the lafan1.zip file.
All the animation sequences are in the BVH file format.
There are 5 subjects in the dataset, 77 sequences, and 496,672 motion frames at 30fps (~4.6 hours).
Every BVH file is named with the following convention: [theme][take number]_[subject ID].bvh.
Any sequences sharing the same theme and take_number were recorded at the same time in the studio.
Themes are high level indicators of the actions in the sequences.

The following themes are present in the LaFAN1 dataset:

|   Theme         |  Description                                  |Number of sequences|
|:----------------|:------------------------------------------    |:-----------------:|
| Obstacles       |  Locomotion on uneven terrain                 |17                 |
| Walk            |  Walking locomotion, with different styles    |12                 |
| Dance           |  Free dancing                                 |8                  |
| Fall and get up |  Falling on the ground and getting back up    |6                  |
| Aiming          |  Locomotion while handling or aiming a gun    |5                  |
| Ground          |  Locomotion while crawling and crouching      |5                  |
| Multiple actions|  Miscellaneous/multiple movements per sequence|4                  |
| Run             |  Jogging/Running locomotion                   |4                  |
| Fight           |  Various fight movements                      |3                  |
| Jumps           |  Locomotion with one and two-leg jumps        |3                  |
| Fight and sports|  Fight and sports movements                   |2                  |
| Push and stumble|  Pushing, stumbling and recovery              |3                  |
| Push and fall   |  Pushing, falling, and getting up             |2                  |
| Sprint          |  Sprinting locomotion                         |2                  |
| Push            |  Pushing adversary                            |1                  |

© [2018] Ubisoft Entertainment. All Rights Reserved",2020,"Recurrent Transition Networks for Character Locomotion (Harvey et al., 2018)",https://arxiv.org/pdf/1810.02363.pdf,,,
1688,LAION-400M,Semantic Image-Text Similarity,Semantic Image-Text Similarity,"Semantic Image-Text Similarity, Latent Diffusion Model for 3D - Pano, Image-to-Text Retrieval, Video Generation, Latent Diffusion Model for 3D-4C","3D, Image, Text, Video",English,Computer Vision,"latent-diffusion-model-for-3d-4c-on-laion, latent-diffusion-model-for-3d-pano-on-laion, video-generation-on-laion-400m",Creative Common CC-BY 4.0,https://laion.ai/laion-400-open-dataset/,https://paperswithcode.com/dataset/laion-400m,"LAION-400M is a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search.

⚠️  Disclaimer & Content Warning (from the authors)
Our filtering protocol only removed NSFW images detected as illegal, but the dataset still has NSFW content accordingly marked in the metadata. When freely navigating through the dataset, keep in mind that it is a large-scale, non-curated set crawled from the internet for research purposes, such that collected links may lead to discomforting and disturbing content. Therefore, please use the demo links with caution. You can extract a “safe” subset by filtering out samples drawn with NSFW or via stricter CLIP filtering.

There is a certain degree of duplication because we used URL+text as deduplication criteria. The same image with the same caption may sit at different URLs, causing duplicates. The same image with other captions is not, however, considered duplicated.

Using KNN clustering should make it easy to further deduplicate by image content.",,,,,,
1689,Lakh_MIDI_Dataset,Music Generation,Music Generation,"Music Generation, Music Modeling","Audio, Text",English,Natural Language Processing,,CC-BY 4.0,https://colinraffel.com/projects/lmd/,https://paperswithcode.com/dataset/lakh-midi-dataset,"The Lakh MIDI dataset is a collection of 176,581 unique MIDI files, 45,129 of which have been matched and aligned to entries in the Million Song Dataset. Its goal is to facilitate large-scale music information retrieval, both symbolic (using the MIDI files alone) and audio content-based (using information extracted from the MIDI files as annotations for the matched audio files). Around 10% of all MIDI files include timestamped lyrics events with lyrics are often transcribed at the word, syllable or character level.

LMD-full denotes the whole dataset. LMD-matched is the subset of LMD-full that consists of MIDI files matched with the Million Song Dataset entries. LMD-aligned contains all the files of LMD-matched, aligned to preview MP3s from the Million Song Dataset.

A lakh is a unit of measure used in the Indian number system which signifies 100,000.",,"Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching",http://colinraffel.com/publications/thesis.pdf,,,
1690,Lakh_Pianoroll_Dataset,Music Generation,Music Generation,"Music Generation, Music Information Retrieval, Music Modeling","Audio, Text",English,Natural Language Processing,,CC BY 4.0,https://salu133445.github.io/lakh-pianoroll-dataset/,https://paperswithcode.com/dataset/lakh-pianoroll-dataset,"The Lakh Pianoroll Dataset (LPD) is a collection of 174,154
multitrack pianorolls derived from the
Lakh MIDI Dataset (LMD).

Getting the dataset
We provide multiple subsets and versions of the dataset (see
here). The dataset is available here.

Using LPD
The multitrack pianorolls in LPD are stored in a special format for efficient
I/O and to save space. We recommend to load the data with
Pypianoroll (The dataset is created
using Pypianoroll v0.3.0.). See here
to learn how the data is stored and how to load the data properly.

License
Lakh Pianoroll Dataset is a derivative of
Lakh MIDI Dataset by
Colin Raffel, used under
CC BY 4.0.
Lakh Pianoroll Dataset is licensed under
CC BY 4.0 by
Hao-Wen Dong and
Wen-Yi Hsiao.

Please cite the following papers if you use Lakh Pianoroll Dataset in a
published work.



Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang,
  ""MuseGAN: Multi-track Sequential Generative Adversarial Networks for
  Symbolic Music Generation and Accompaniment,""
  in Proceedings of the 32nd AAAI Conference on Artificial Intelligence
  (AAAI), 2018.



Colin Raffel,
  ""Learning-Based Methods for Comparing Sequences, with Applications to
  Audio-to-MIDI Alignment and Matching,""
  PhD Thesis, 2016.



Related projects

MuseGAN
LeadSheetGAN",2018,,,,,
1691,LAMA,Language Modelling,Language Modelling,"Language Modelling, Open-Domain Question Answering, Question Answering",Text,English,Natural Language Processing,,CC-BY-NC 4.0,https://github.com/facebookresearch/LAMA,https://paperswithcode.com/dataset/lama,"LAnguage Model Analysis (LAMA) consists of a set of knowledge sources, each comprised of a set of facts. LAMA is a probe for analyzing the factual and commonsense knowledge contained in pretrained language models.",,Language Models as Knowledge Bases?,https://arxiv.org/pdf/1909.01066v2.pdf,,,
1692,LAMBADA,Language Modelling,Language Modelling,Language Modelling,Text,English,Natural Language Processing,language-modelling-on-lambada,CC BY 4.0,https://zenodo.org/record/2630551#.YFJVaWT7S_w,https://paperswithcode.com/dataset/lambada,"The LAMBADA (LAnguage Modeling Broadened to Account for Discourse Aspects) benchmark is an open-ended cloze task which consists of about 10,000 passages from BooksCorpus where a missing target word is predicted in the last sentence of each passage. The missing word is constrained to always be the last word of the last sentence and there are no candidate words to choose from. Examples were filtered by humans to ensure they were possible to guess given the context, i.e., the sentences in the passage leading up to the last sentence. Examples were further filtered to ensure that missing words could not be guessed without the context, ensuring that models attempting the dataset would need to reason over the entire paragraph to answer questions.",,"Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches",https://arxiv.org/abs/1904.01172,,"trained to always be the last word of the last sentence and there are no candidate words to choose from. Examples were filtered by humans to ensure they were possible to guess given the context, i.e., the sentences in the passage leading up to the last sentence. Examples",
1693,LaMem,Style Transfer,Style Transfer,"Style Transfer, Image Classification, Image-to-Image Translation","Image, Text",English,Computer Vision,,,http://memorability.csail.mit.edu,https://paperswithcode.com/dataset/lamem,"An annotated image memorability dataset to date (with 60,000 labeled images from a diverse array of sources).",,,,,,
1694,LAM_line-level_,Handwritten Text Recognition,Handwritten Text Recognition,Handwritten Text Recognition,"Image, Text",English,Computer Vision,handwritten-text-recognition-on-lam-line,,https://arxiv.org/pdf/2208.07682,https://paperswithcode.com/dataset/lam-line-level,"Handwritten Text Recognition (HTR) is an open
problem at the intersection of Computer Vision and Natural
Language Processing. The main challenges, when dealing with
historical manuscripts, are due to the preservation of the paper
support, the variability of the handwriting – even of the same
author over a wide time-span – and the scarcity of data from
ancient, poorly represented languages. With the aim of fostering
the research on this topic, in this paper we present the Ludovico
Antonio Muratori (LAM) dataset, a large line-level HTR dataset
of Italian ancient manuscripts edited by a single author over
60 years. The dataset comes in two configurations: a basic
splitting and a date-based splitting which takes into account the
age of the author. The first setting is intended to study HTR
on ancient documents in Italian, while the second focuses on
the ability of HTR systems to recognize text written by the
same writer in time periods for which training data are not
available. For both configurations, we analyze quantitative and
qualitative characteristics, also with respect to other line-level
HTR benchmarks, and present the recognition performance of
state-of-the-art HTR architectures. The dataset is available for
download at https://aimagelab.ing.unimore.it/go/lam.",,Homepage,https://arxiv.org/pdf/2208.07682,,,
1695,Lani,Continuous Control,Continuous Control,"Continuous Control, Starcraft II, Starcraft",,,Methodology,,,,https://paperswithcode.com/dataset/lani,"LANI is a 3D navigation environment and corpus, where an agent navigates between landmarks. Lani contains 27,965 crowd-sourced instructions for navigation in an open environment. Each datapoint includes an instruction, a human-annotated ground-truth demonstration trajectory, and an environment with various landmarks and lakes. The dataset train/dev/test split is 19,758/4,135/4,072. Each environment specification defines placement of 6–13 landmarks within a square grass field of size 50m×50m.",,Mapping Navigation Instructions to Continuous Control Actions with Position-Visitation Prediction,https://arxiv.org/abs/1811.04179,,,
1696,Laptop-ACOS,Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),"Aspect-Based Sentiment Analysis (ABSA), Sentiment Analysis, Aspect-Category-Opinion-Sentiment Quadruple Extraction",Text,English,Natural Language Processing,aspect-category-opinion-sentiment-quadruple-1,,https://github.com/NUSTM/ACOS/tree/main/data/Laptop-ACOS,https://paperswithcode.com/dataset/laptop-acos,"Laptop-ACOS is a brand new Laptop dataset collected from the Amazon platform in the years 2017 and 2018 (covering ten types of laptops under six brands such as ASUS, Acer, Samsung, Lenovo, MBP, MSI, and so on). It contains 4,076 review sentences, much larger than the SemEval Laptop datasets.
For Laptop-ACOS, we annotate the four elements and their corresponding quadruples all by ourselves. We employ the aspect categories defined in the SemEval 2016 Laptop dataset. The Laptop-ACOS dataset contains 4076 sentences with 5758 quadruples. As we have mentioned, a large percentage of the quadruples contain implicit aspects or implicit opinions .  By comparing two datasets, it can be observed that Laptop-ACOS has a higher percentage of implicit opinions than Restaurant-ACOS . It is worth noting that the Laptop-ACOS is available for all subtasks in ABSA, including aspect-based sentiment classification, aspect-sentiment pair extraction, aspect-opinion pair extraction, aspect-opinion sentiment triple extraction, aspect-category-sentiment triple extraction, etc.",2017,,,4076 sentences,,
1697,LargeST,Traffic Prediction,Traffic Prediction,Traffic Prediction,Time Series,,Methodology,traffic-prediction-on-largest,CC BY-NC 4.0 International,https://www.kaggle.com/datasets/liuxu77/largest,https://paperswithcode.com/dataset/largest,"In this work, we propose LargeST as a new benchmark dataset (see Figure 1), with the goal of facilitating the development of accurate and efficient methods in the context of large-scale traffic forecasting. The distinguishing characteristic of LargeST lies not only in its extensive graph size, encompassing a total of 8,600 sensors in California, but also in its substantial temporal coverage and rich node information – each sensor contains 5 years of data and comprehensive metadata. 

LargeST comprises four sub-datasets, each characterized by a different number of sensors. The biggest one is California (CA), including a total number of 8,600 sensors. We also construct three subsets of CA by selecting three representative areas within CA and forming the sub-datasets of Greater Los Angeles (GLA), Greater Bay Area (GBA), and San Diego (SD).",,,,,,
1698,Large_Car-following_Dataset_Based_on_Lyft_level-5_,Human Behavior Forecasting,Human Behavior Forecasting,"Human Behavior Forecasting, Self-Driving Cars","Image, Time Series",,Computer Vision,,CC BY-NC 4.0,https://data.4tu.nl/datasets/1255994c-c64f-40f5-8121-9e952e308c9a,https://paperswithcode.com/dataset/large-car-following-dataset-based-on-lyft,"Studying how human drivers react differently when following autonomous vehicles (AV) vs. human-driven vehicles (HV) is critical for mixed traffic flow. This dataset contains extracted and enhanced two categories of car-following data, HV-following-AV (H-A) and HV-following-HV (H-H), from the open Lyft level-5 dataset.",,,,,,
1699,Large_COVID-19_CT_scan_slice_dataset,Computed Tomography (CT),Computed Tomography (CT),"Computed Tomography (CT), Classification, COVID-19 Diagnosis, Few-Shot Learning",Image,,Computer Vision,"covid-19-diagnosis-on-large-covid-19-ct-scan, few-shot-learning-on-large-covid-19-ct-scan",,https://github.com/maftouni/Curated_Covid_CT,https://paperswithcode.com/dataset/large-covid-19-ct-scan-slice-dataset,"""We built a large lung CT scan dataset for COVID-19 by curating data from 7 public datasets listed in the acknowledgements. These datasets have been publicly used in COVID-19 diagnosis literature and proven their efficiency in deep learning applications. Therefore, the merged dataset is expected to improve the generalization ability of deep learning methods by learning from all these resources together.

These datasets are made available in different formats. Our goal is to provide a large dataset of COVID-19, Normal, and CAP CT slices together with their corresponding metadata. Some of the datasets consist of categorized CT slices, and some include CT volumes with annotated lesion slices. Therefore, we used the slice-level annotations to extract axial slices from CT volumes. We then converted all the images to 8-bit to have a consistent depth.

To ensure the dataset quality, we have removed the closed lung normal slices that do not carry information about inside lung manifestations. Additionally, we did not include images lacking clear class labels or patient information. In total, we have gathered 7,593 COVID-19 images from 466 patients, 6,893 normal images from 604 patients, and 2,618 CAP images from 60 patients. All of our CAP images are from Afshar et al. dataset, in which 25 cases are already annotated. Our radiologist has annotated the remaining 35 CT scan volumes. This is the largest COVID-19 lung CT dataset so far, to the best of our knowledge."" - Source: A Robust Ensemble-Deep Learning Model for COVID-19 Diagnosis based on an Integrated CT Scan Images Database

Acknowledgements



J. Zhao, Y. Zhang, X. He, and P. Xie, ""COVID-CT-Dataset: a CT scan dataset about COVID-19,"" arXiv preprint arXiv:2003.13865, 2020.



P. Afshar et al., ""COVID-CT-MD: COVID-19 Computed Tomography (CT) Scan Dataset Applicable in Machine Learning and Deep Learning,"" arXiv preprint arXiv:2009.14623, 2020.



J. P. Cohen, P. Morrison, L. Dao, K. Roth, T. Q. Duong, and M. Ghassemi, ""Covid-19 image data collection: Prospective predictions are the future,"" arXiv preprint arXiv:2006.11988, 2020.



S. Morozov et al., ""MosMedData: Chest CT Scans With COVID-19 Related Findings Dataset,"" arXiv preprint arXiv:2005.06465, 2020.



M. Rahimzadeh, A. Attar, and S. M. Sakhaei, ""A Fully Automated Deep Learning-based Network For Detecting COVID-19 from a New And Large Lung CT Scan Dataset,"" medRxiv, 2020.



M. Jun et al., ""COVID-19 CT Lung and Infection Segmentation Dataset,"" Zenodo, Apr, vol. 20, 2020.



""COVID-19."" 2020. [Online] http://medicalsegmentation.com/covid19/ [Accessed 23 December, 2020].",2003,,,19 images,,
1700,LaRS,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Video Panoptic Segmentation, Video Semantic Segmentation, Panoptic Segmentation","Image, Video",,Computer Vision,"panoptic-segmentation-on-lars, semantic-segmentation-on-lars, video-semantic-segmentation-on-lars",CC-BY-NC 4.0,https://lojzezust.github.io/lars-dataset/,https://paperswithcode.com/dataset/lars,"LaRS is the largest and most diverse panoptic maritime obstacle detection dataset.

Highlights:


Diverse scenes from manual capture, public online videos and existing datasets  
USV-centric point of view  
4000+ manually per-pixel labelled frames:  
3 stuff categories and 8 thing (dynamic obstacles) categories  
20 scene-level attributes (e.g. illumination, reflections, conditions)  


Temporal context for each annotated frame (9 preceding frames, total: 40k frames)",,,,,,
1701,LaSCo,Composed Image Retrieval (CoIR),Composed Image Retrieval (CoIR),"Composed Image Retrieval (CoIR), Image Retrieval",Image,,Computer Vision,image-retrieval-on-lasco,BY-NC-ND,https://www.vision.huji.ac.il/lasco/,https://paperswithcode.com/dataset/lasco,"Large Scale Composed Image Retrieval (LaSCo) is a new dataset for Composed Image Retrieval (CoIR), x10 times larger than current ones.",,,,,,
1702,Laser_Data,Voice Anti-spoofing,Voice Anti-spoofing,Voice Anti-spoofing,Audio,,Audio,,,https://www.kaggle.com/datasets/hashimali19/laser-injection-data,https://paperswithcode.com/dataset/laser-data,"This dataset contains two types of audio recordings. The first set of audio recordings consists of MEMS microphone response to acoustic activities (e.g., 19 participants reading provided text in front of the Google Home Smart Assistant). The second set of audio recordings consists of MEMS microphone response to photo-acoustic activities (laser modulated--with audio recordings of 19 participants, firing at the MEMS microphone of Google Home Smart Assistant). A total of 19 students (10 male and 9 female) were enrolled for data collection. All participants were asked to read the following 5 sentences in the microphone, Hey Google, Open the garage door, Hey Google, Close the garage door, Hey Google, Turn the light on, Hey Google, Turn the light off, Hey Google, What is the weather today?. Each audio sample was injected into the microphone through a laser, and the response of the microphone was recorded. This method produced a total data set of 95 acoustic- and 95 laser-induced audio recordings.",,,,5 sentences,,
1703,LasHeR,Rgb-T Tracking,Rgb-T Tracking,Rgb-T Tracking,"Image, Video",,Computer Vision,rgb-t-tracking-on-lasher,,https://github.com/BUGPLEASEOUT/LasHeR,https://paperswithcode.com/dataset/lasher,"LasHeR consists of 1224 visible and thermal infrared video pairs with more than 730K frame pairs in total. Each frame pair is spatially aligned and manually annotated with a bounding box, making the dataset well and densely annotated. LasHeR is highly diverse capturing from a broad range of object categories, camera viewpoints, scene complexities and environmental factors across seasons, weathers, day and night.",,,,,,
1704,LaSOT,Zero-Shot Single Object Tracking,Zero-Shot Single Object Tracking,"Zero-Shot Single Object Tracking, Object Tracking, Visual Tracking, Visual Object Tracking","Image, Video",,Computer Vision,"zero-shot-single-object-tracking-on-lasot, visual-object-tracking-on-lasot, visual-tracking-on-lasot",Apache-2.0,http://vision.cs.stonybrook.edu/~lasot/,https://paperswithcode.com/dataset/lasot,"LaSOT is a high-quality benchmark for Large-scale Single Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box, making LaSOT one of the largest densely annotated
tracking benchmark. The average video length of LaSOT
is more than 2,500 frames, and each sequence comprises
various challenges deriving from the wild where target objects may disappear and re-appear again in the view.",,,,,,
1705,LastFM_Asia,Graph Sampling,Graph Sampling,"Graph Sampling, Graph Embedding",Graph,,Methodology,,GNU,https://github.com/benedekrozemberczki/FEATHER,https://paperswithcode.com/dataset/lastfm-asia,A social network of LastFM users which was collected from the public API in March 2020. Nodes are LastFM users from Asian countries and edges are mutual follower relationships between them. The vertex features are extracted based on the artists liked by the users. The task related to the graph is multinomial node classification - one has to predict the location of users. This target feature was derived from the country field for each user.,2020,,,,,
1706,LAV-DF,Temporal Forgery Localization,Temporal Forgery Localization,"Temporal Forgery Localization, DeepFake Detection","Image, Time Series, Video",,Computer Vision,"temporal-forgery-localization-on-lav-df, deepfake-detection-on-lav-df",Custom,https://github.com/ControlNet/LAV-DF,https://paperswithcode.com/dataset/lav-df,"Localized Audio Visual DeepFake Dataset (LAV-DF). 

Paper: Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization",,,,,,
1707,LC-QuAD,Knowledge Base Question Answering,Knowledge Base Question Answering,"Knowledge Base Question Answering, Relation Linking","Graph, Text",English,Natural Language Processing,"relation-linking-on-lc-quad-2-0, relation-linking-on-lc-quad-1-0, knowledge-base-question-answering-on-lc-quad",,http://lc-quad.sda.tech,https://paperswithcode.com/dataset/lc-quad-2-0,"LC-QuAD is a Large Question Answering dataset with 30,000 pairs of questions and its corresponding SPARQL query. The target knowledge base is Wikidata and DBpedia, specifically the 2018 version.",2018,,,,,
1708,LCQMC,Question Similarity,Question Similarity,"Question Similarity, Chinese Sentence Pair Classification",Image,,Computer Vision,chinese-sentence-pair-classification-on-lcqmc,Creative Commons Attribution 4.0 International,http://icrc.hitsz.edu.cn/info/1037/1146.htm,https://paperswithcode.com/dataset/lcqmc,"LCQMC is a large-scale Chinese question matching corpus. LCQMC is more general than paraphrase corpus as it focuses on intent matching rather than paraphrase. The corpus contains 260,068 question pairs with manual annotation.",,,,,,
1709,LCSTS,Text Generation,Text Generation,"Text Generation, Text Summarization, Abstractive Text Summarization",Text,English,Natural Language Processing,"text-summarization-on-lcsts, text-generation-on-lcsts",Custom (research-only),http://icrc.hitsz.edu.cn/Article/show/139.html,https://paperswithcode.com/dataset/lcsts,"LCSTS is a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. The authors also manually tagged the relevance of 10,666 short summaries with their corresponding short texts 10,666 short summaries with their corresponding short texts.",,,,,,
1710,LDC2017T10,AMR Parsing,AMR Parsing,"AMR Parsing, AMR-to-Text Generation",Text,English,Natural Language Processing,"amr-parsing-on-ldc2017t10, amr-to-text-generation-on-ldc2017t10",LDC User Agreement for Non-Members,https://catalog.ldc.upenn.edu/LDC2017T10,https://paperswithcode.com/dataset/ldc2017t10,"Abstract Meaning Representation (AMR) Annotation Release 2.0 was developed by the Linguistic Data Consortium (LDC), SDL/Language Weaver, Inc., the University of Colorado's Computational Language and Educational Research group and the Information Sciences Institute at the University of Southern California. It contains a sembank (semantic treebank) of over 39,260 English natural language sentences from broadcast conversations, newswire, weblogs and web discussion forums.

AMR captures “who is doing what to whom” in a sentence. Each sentence is paired with a graph that represents its whole-sentence meaning in a tree-structure. AMR utilizes PropBank frames, non-core semantic roles, within-sentence coreference, named entity annotation, modality, negation, questions, quantities, and so on to represent the semantic structure of a sentence largely independent of its syntax.",,LDC User Agreement for Non-Members,https://catalog.ldc.upenn.edu/license/ldc-non-members-agreement.pdf,,,
1711,LDC2020T02,AMR Parsing,AMR Parsing,"AMR Parsing, AMR-to-Text Generation",Text,English,Natural Language Processing,"amr-to-text-generation-on-ldc2020t02, amr-parsing-on-ldc2020t02",LDC User Agreement for Non-Members,https://catalog.ldc.upenn.edu/LDC2020T02,https://paperswithcode.com/dataset/ldc2020t02,"Abstract Meaning Representation (AMR) Annotation Release 3.0 was developed by the Linguistic Data Consortium (LDC), SDL/Language Weaver, Inc., the University of Colorado's Computational Language and Educational Research group and the Information Sciences Institute at the University of Southern California. It contains a sembank (semantic treebank) of over 59,255 English natural language sentences from broadcast conversations, newswire, weblogs, web discussion forums, fiction and web text. This release adds new data to, and updates material contained in, Abstract Meaning Representation 2.0 (LDC2017T10), specifically: more annotations on new and prior data, new or improved PropBank-style frames, enhanced quality control, and multi-sentence annotations.

AMR captures ""who is doing what to whom"" in a sentence. Each sentence is paired with a graph that represents its whole-sentence meaning in a tree-structure. AMR utilizes PropBank frames, non-core semantic roles, within-sentence coreference, named entity annotation, modality, negation, questions, quantities, and so on to represent the semantic structure of a sentence largely independent of its syntax.",,LDC User Agreement for Non-Members,https://catalog.ldc.upenn.edu/license/ldc-non-members-agreement.pdf,,,
1712,LDDRS,Autonomous Navigation,Autonomous Navigation,Autonomous Navigation,,,Methodology,,,https://github.com/polwork/LDDRS,https://paperswithcode.com/dataset/lddrs,"The LWIR DoFP Dataset of Road Scene (LDDRS) is a road detection dataset with 2,113 annotated images. It contains both day and night scenes, with multiple cars and pedestrians per image.",,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700460.pdf,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700460.pdf,,,
1713,LEAF-QA,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Question Answering, Chart Question Answering","Image, Text",English,Computer Vision,,,https://arxiv.org/pdf/1907.12861.pdf,https://paperswithcode.com/dataset/leaf-qa,"LEAF-QA, a comprehensive dataset of 250,000 densely annotated figures/charts, constructed from real-world open data sources, along with ~2 million question-answer (QA) pairs querying the structure and semantics of these charts. LEAF-QA highlights the problem of multimodal QA, which is notably different from conventional visual QA (VQA), and has recently gained interest in the community. Furthermore, LEAF-QA is significantly more complex than previous attempts at chart QA, viz. FigureQA and DVQA, which present only limited variations in chart data. LEAF-QA being constructed from real-world sources, requires a novel architecture to enable question answering.",,"LEAF-QA: Locate, Encode & Attend for Figure Question Answering",https://arxiv.org/pdf/1907.12861,,,
1714,LEAF_Benchmark,Traffic Classification,Traffic Classification,"Traffic Classification, Multi-Task Learning, Federated Learning",Image,,Computer Vision,,,https://leaf.cmu.edu,https://paperswithcode.com/dataset/leaf-benchmark,"A suite of open-source federated datasets, a rigorous evaluation framework, and a set of reference implementations, all geared towards capturing the obstacles and intricacies of practical federated environments.",,,,,,
1715,Learn2Reg,Medical Image Registration,Medical Image Registration,"Medical Image Registration, Image Registration",Image,,Medical,"image-registration-on-unpaired-lung-ct, image-registration-on-unpaired-abdomen-ct",,https://learn2reg.grand-challenge.org/,https://paperswithcode.com/dataset/learn2reg,"Learn2Reg is a dataset for medical image registration. Learn2Reg covers a wide range of anatomies (brain, abdomen, and thorax), modalities (ultrasound, CT, MR), availability of annotations, as well as intra- and inter-patient registration evaluation.",,,,,,
1716,LEARNING_STYLE_IDENTIFICATION,Self-Learning,Self-Learning,Self-Learning,,,Methodology,,,https://dx.doi.org/10.21227/7tc4-5841,https://paperswithcode.com/dataset/learning-style-identification,"The dataset was collected from two courses offered on the University of Jordan's E-learning Portal during the second semester of 2020, namely ""Computer Skills for Humanities Students"" (CSHS) and ""Computer Skills for Medical Students"" (CSMS). Over the sixteen-week duration of each course, students participated in various activities such as reading materials, video lectures, assignments, and quizzes. To preserve student privacy, the log activity of each student was anonymized. Data was aggregated from multiple sources, including the Moodle learning management system and the student information system, and consolidated into a single database.
The dataset contains information on the number of learners and events for each course, as well as their launch and end dates. CSHS had 1749 learners and 1,139,810 events from January 21, 2020 to May 20, 2020, while CSMS had 564 learners and 484,410 events during the same period. The dataset is based on the Filder and Silverman learning style model (FSLSM), which captures students' preferences regarding each dimension of the model. At the beginning of each course, students completed the Index of Learning Styles (ILS) questionnaire, a 44-item questionnaire that gauges learners' personal preferences for each dimension by assigning values ranging from -11 to +11 per dimension based on the responses to eleven questions per dimension. The ILS questionnaire allowed us to capture students' preferences regarding the FSLSM dimensions.",2020,,,,,
1717,Learning_to_Rank_Challenge,Learning-To-Rank,Learning-To-Rank,"Learning-To-Rank, Information Retrieval, Recommendation Systems",,,Methodology,,,https://webscope.sandbox.yahoo.com/catalog.php?datatype=c,https://paperswithcode.com/dataset/learning-to-rank-challenge,"The Yahoo! Learning to Rank Challenge dataset consists of 709,877 documents encoded in 700 features and sampled from query logs of the Yahoo! search engine, spanning 29,921 queries.",,Ranking for Relevance and Display Preferencesin Complex Presentation Layouts,https://arxiv.org/abs/1805.02404,877 documents,,
1718,Lee2019-ERP_MOABB,Within-Session ERP,Within-Session ERP,Within-Session ERP,,,Methodology,within-session-erp-on-lee2019-erp-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Lee2019_ERP.html,https://paperswithcode.com/dataset/lee2019-erp-moabb-1,,,,,,,
1719,Lee2019-MI_MOABB,Within-Session Motor Imagery (left hand vs. right hand),Within-Session Motor Imagery (left hand vs. right hand),Within-Session Motor Imagery (left hand vs. right hand),,,Methodology,within-session-motor-imagery-left-hand-vs-4,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Lee2019_MI.html,https://paperswithcode.com/dataset/lee2019-mi-moabb-1,,,,,,,
1720,Lee2019-SSVEP_MOABB,Within-Session SSVEP,Within-Session SSVEP,Within-Session SSVEP,,,Methodology,within-session-ssvep-on-lee2019-ssvep-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Lee2019_SSVEP.html,https://paperswithcode.com/dataset/lee2019-ssvep-moabb-1,,,,,,,
1721,LegalNERo,Named Entity Recognition (NER),Named Entity Recognition (NER),"Named Entity Recognition (NER), Nested Named Entity Recognition","Image, Text",English,Computer Vision,named-entity-recognition-on-legalnero,CC BY-NC-ND 4.0,https://doi.org/10.5281/zenodo.4772094,https://paperswithcode.com/dataset/legalnero,"LegalNERo is a manually annotated corpus for named entity recognition in the Romanian legal domain. 
It provides gold annotations for organizations, locations, persons, time and legal resources mentioned in legal documents.
Additionally it offers GEONAMES codes for the named entities annotated as location (where a link could be established). 

The LegalNERo corpus is available in different formats: span-based, token-based and RDF. 
The Linguistic Linked Open Data (LLOD) version is provided in RDF-Turtle format.",,,,,,
1722,Leipzig_Corpora,NMT,NMT,"NMT, Low-Resource Neural Machine Translation",Text,English,Natural Language Processing,,,https://wortschatz.uni-leipzig.de/en/download/Gujarati,https://paperswithcode.com/dataset/leipzig-corpora,"The Leipzig Corpora Collection presents corpora in different languages using the same format and comparable sources. All data are available as plain text files and can be imported into a MySQL database by using the provided import script. They are intended both for scientific use by corpus linguists as well as for applications such as knowledge extraction programs.
The corpora are identical in format and similar in size and content. They contain randomly selected sentences in the language of the corpus and are available in sizes from 10,000 sentences up to 1 million sentences. The sources are either newspaper texts or texts randomly collected from the web. The texts are split into sentences. Non-sentences and foreign language material was removed. Because word co-occurrence information is useful for many applications, these data are precomputed and included as well. For each word, the most significant words appearing as immediate left or right neighbor or appearing anywhere within the same sentence are given. More information about the format and content of these files can be found here.
The corpora are automatically collected from carefully selected public sources without considering in detail the content of the contained text. No responsibility is taken for the content of the data. In particular, the views and opinions expressed in specific parts of the data remain exclusively with the authors.

If you use one of these corpora in your work we kindly ask you to cite this paper as

D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages.
In: Proceedings of the 8th International Language Resources and Evaluation (LREC'12), 2012",2012,,,000 sentences,,
1723,Leishmania_parasite_dataset,Image Deblurring,Image Deblurring,"Image Deblurring, Medical Image Generation","Image, Text",English,Computer Vision,,,https://data.mendeley.com/datasets/m3jxgb54c9/4,https://paperswithcode.com/dataset/leishmania-parasite-dataset,"This dataset includes sharp-blur pairs of Leishmania image, which is a protozoan parasite microscopy image dataset of Leishmania, obtained from the preserved slides stained with Giemsa. The paired blur-sharp images are acquired by employing a bright-field microscope (Olympus IX53) with 100× magnification oil immersion objectives.We first capture the sharp images as ground truth, then acquire its corresponding out-of-focus images. The extent and nature of defocusing are random along the optical axis, where the degree of out-of-focus is inconsistent from image-to-image. This dataset includes 764 in-focus and 764 corresponding out-of-focus images, where each image is composed of 2304 × 1728 pixels in 24-bit JPG format.",,,,,,
1724,LEPISZCZE,NER,NER,"NER, Sentiment Analysis, Classification, Punctuation Restoration","Image, Text",English,Computer Vision,,,https://github.com/CLARIN-PL/LEPISZCZE,https://paperswithcode.com/dataset/lepiszcze,"LEPISZCZE is an open-source comprehensive benchmark for Polish NLP and a continuous-submission leaderboard, concentrating public Polish datasets (existing and new) in specific tasks.",,"This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish",https://arxiv.org/pdf/2211.13112v1.pdf,,,
1725,Letter,Image Clustering,Image Clustering,"Image Clustering, Core set discovery",Image,,Computer Vision,"core-set-discovery-on-letter, image-clustering-on-lettera-j",,https://archive.ics.uci.edu/ml/datasets/Letter+Recognition,https://paperswithcode.com/dataset/letter,"Letter Recognition Data Set is a handwritten digit dataset. The task is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.",,http://www.cs.uu.nl/docs/vakken/mpr/Frey-Slate.pdf,http://www.cs.uu.nl/docs/vakken/mpr/Frey-Slate.pdf,,,
1726,LEVIR-CD,Change Detection,Change Detection,"Change Detection, Building change detection for remote sensing images",Image,,Computer Vision,"change-detection-on-levir-cd, building-change-detection-for-remote-sensing",,https://justchenhao.github.io/LEVIR/,https://paperswithcode.com/dataset/levir-cd,"LEVIR-CD is a new large-scale remote sensing building Change Detection dataset. The introduced dataset would be a new benchmark for evaluating change detection (CD) algorithms, especially those based on deep learning.

LEVIR-CD consists of 637 very high-resolution (VHR, 0.5m/pixel) Google Earth (GE) image patch pairs with a size of 1024 × 1024 pixels. These bitemporal images with time span of 5 to 14 years have significant land-use changes, especially the construction growth. LEVIR-CD covers various types of buildings, such as villa residences, tall apartments, small garages and large warehouses. Here, we focus on building-related changes, including the building growth (the change from soil/grass/hardened ground or building under construction to new build-up regions) and the building decline. These bitemporal images are annotated by remote sensing image interpretation experts using binary labels (1 for change and 0 for unchanged). Each sample in our dataset is annotated by one annotator and then double-checked by another to produce high-quality annotations. The fully annotated LEVIR-CD contains a total of 31,333 individual change-building instances.",,,,,,
1727,LFW,Face Recognition,Face Recognition,"Face Recognition, 3D Face Modelling, Blind Face Restoration, Face Verification, Face Quality Assessement, Unsupervised face recognition, Quantization, Face Anonymization, Lightweight Face Recognition, Synthetic Face Recognition","3D, Image",,Computer Vision,"face-quality-assessement-on-lfw, quantization-on-lfw, unsupervised-face-recognition-on-lfw, face-verification-on-lfw, face-recognition-on-lfw, synthetic-face-recognition-on-lfw, 3d-face-modeling-on-lfw, lightweight-face-recognition-on-lfw, blind-face-restoration-on-lfw, face-verification-on-labeled-faces-in-the, face-recognition-on-lfw-online-open-set, face-anonymization-on-lfw",,http://vis-www.cs.umass.edu/lfw/,https://paperswithcode.com/dataset/lfw,"The LFW dataset contains 13,233 images of faces collected from the web. This dataset consists of the 5749 identities with 1680 people with two or more images. In the standard LFW evaluation protocol the verification accuracies are reported on 6000 face pairs.",,A Performance Evaluation of Loss Functions for Deep Face Recognition,https://arxiv.org/abs/1901.05903,233 images,,
1728,LHQ,Image Outpainting,Image Outpainting,"Image Outpainting, Image Generation, Infinite Image Generation, Perpetual View Generation, Text-to-Image Generation","Image, Text",English,Computer Vision,"image-outpainting-on-lhqc, text-to-image-generation-on-lhqc, infinite-image-generation-on-lhq, perpetual-view-generation-on-lhq",CC BY 2.0,https://universome.github.io/alis,https://paperswithcode.com/dataset/lhq,"A dataset of 90,000 high-resolution nature landscape images, crawled from Unsplash and Flickr and preprocessed with Mask R-CNN and Inception V3.",,,,,,
1729,LIAR-RAW,Misinformation,Misinformation,"Misinformation, Fake News Detection, Stance Detection",Image,,Computer Vision,"fake-news-detection-on-liar, fake-news-detection-on-rawfc",Apache-2.0 license,https://github.com/Nicozwy/CofCED/tree/main/Datasets/LIAR-RAW,https://paperswithcode.com/dataset/liar-raw,"For LIAR-RAW, we extended the public dataset LIAR-PLUS (Alhindi et al., 2018) with relevant raw reports, containing fine-grained claims from Politifact. LIAR-RAW is based on LIAR, where gold labels refer to Politifact. To alleviate the dependency of fact-checked reports, we extended the public LIAR dataset with additional raw reports for each claim. Besides, we put these raw reports into a single file with the format of LIAR.

Provide:
* A novel benchmark dataset for Explainable fake news detection.",2018,,,,,
1730,LIAR,Misinformation,Misinformation,"Misinformation, Fake News Detection, Stance Detection",Image,,Computer Vision,fake-news-detection-on-liar,,https://www.cs.ucsb.edu/~william/data/liar_dataset.zip,https://paperswithcode.com/dataset/liar,"LIAR is a publicly available dataset for fake news detection. A decade-long of 12.8K manually labeled short statements were collected in various contexts from POLITIFACT.COM, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. The LIAR dataset4 includes 12.8K human labeled short statements from POLITIFACT.COM’s API, and each statement is evaluated by a POLITIFACT.COM editor for its truthfulness.",,"“Liar, Liar Pants on Fire”: A New Benchmark Dataset for Fake News Detection",https://www.aclweb.org/anthology/P17-2067.pdf,,,
1731,Libri-Light,Language Modelling,Language Modelling,"Language Modelling, Speech Recognition","Audio, Image, Text",English,Computer Vision,"speech-recognition-on-libri-light-test-other, speech-recognition-on-libri-light-test-clean",Public domain,https://github.com/facebookresearch/libri-light,https://paperswithcode.com/dataset/libri-light,Libri-Light is a collection of spoken English audio suitable for training speech recognition systems under limited or no supervision. It is derived from open-source audio books from the LibriVox project. It contains over 60K hours of audio.,,Libri-Light: A Benchmark for ASR with Limited or No Supervision,https://arxiv.org/pdf/1912.07875v1.pdf,,,
1732,LibriCSS,Speaker Separation,Speaker Separation,"Speaker Separation, Speech Recognition, Speech Separation","Audio, Image, Text",English,Computer Vision,"speech-recognition-on-libricss, speech-separation-on-libricss",,https://github.com/chenzhuo1011/libri_css,https://paperswithcode.com/dataset/libricss,"Continuous speech separation (CSS) is an approach to handling overlapped speech in conversational audio signals. A real recorded dataset, called LibriCSS, is derived from LibriSpeech by concatenating the corpus utterances to simulate a conversation and capturing the audio replays with far-field microphones.",,,,,,
1733,LibriMix,Audio Source Separation,Audio Source Separation,"Audio Source Separation, Speech Separation, Speech Enhancement",Audio,,Audio,speech-separation-on-libri2mix,,https://github.com/JorisCos/LibriMix,https://paperswithcode.com/dataset/librimix,"LibriMix is an open-source alternative to wsj0-2mix. Based on LibriSpeech, LibriMix consists of two- or three-speaker mixtures combined with ambient noise samples from WHAM!.",,,,,,
1734,LibriS2S,Speech-to-Speech Translation,Speech-to-Speech Translation,Speech-to-Speech Translation,"Audio, Text",English,Speech,,CC BY-NC-SA 4.0,,https://paperswithcode.com/dataset/libris2s,LibriS2S is a Speech to Speech Translation (S2ST) dataset build further upon existing resources. The dataset provides English-German speech and text quadruplets ranging just over 50 hours for both languages.,,,,,,
1735,LibriSpeech,Automatic Speech Recognition,Automatic Speech Recognition,"Automatic Speech Recognition, Voice Conversion, Speech Recognition, Resynthesis","Audio, Image, Text",English,Speech,"voice-conversion-on-librispeech-test-clean, automatic-speech-recognition-on-librispeech-11, automatic-speech-recognition-on-librispeech-10, resynthesis-on-librispeech-1, automatic-speech-recognition-on-librispeech-9, speech-recognition-on-librispeech-test-other, automatic-speech-recognition-on-librispeech-8, speech-recognition-on-librispeech-test-clean, automatic-speech-recognition-on-librispeech-7",CC BY 4.0,http://www.openslr.org/12,https://paperswithcode.com/dataset/librispeech,"The LibriSpeech corpus is a collection of approximately 1,000 hours of audiobooks that are a part of the LibriVox project. Most of the audiobooks come from the Project Gutenberg. The training data is split into 3 partitions of 100hr, 360hr, and 500hr sets while the dev and test data are split into the ’clean’ and ’other’ categories, respectively, depending upon how well or challenging Automatic Speech Recognition systems would perform against. Each of the dev and test sets is around 5hr in audio length. This corpus also provides the n-gram language models and the corresponding texts excerpted from the Project Gutenberg books, which contain 803M tokens and 977K unique words.",,State-of-the-art Speech Recognition using Multi-stream Self-attention with Dilated 1D Convolutions,https://arxiv.org/abs/1910.00716,,,
1736,LibriTTS,Text-To-Speech Synthesis,Text-To-Speech Synthesis,"Text-To-Speech Synthesis, Speech Synthesis","Audio, Text",English,Speech,speech-synthesis-on-libritts,,http://www.openslr.org/60,https://paperswithcode.com/dataset/libritts,"LibriTTS is a multi-speaker English corpus of approximately 585 hours of read English speech at 24kHz sampling rate, prepared by Heiga Zen with the assistance of Google Speech and Google Brain team members. The LibriTTS corpus is designed for TTS research. It is derived from the original materials (mp3 audio files from LibriVox and text files from Project Gutenberg) of the LibriSpeech corpus. The main differences from the LibriSpeech corpus are listed below:


The audio files are at 24kHz sampling rate.
The speech is split at sentence breaks.
Both original and normalized texts are included.
Contextual information (e.g., neighbouring sentences) can be extracted.
Utterances with significant background noise are excluded.",,,,,,
1737,LiDAR-MOS,Moving Point Cloud Processing,Moving Point Cloud Processing,"Moving Point Cloud Processing, 3D Semantic Segmentation, 3D Part Segmentation","3D, Image",,Computer Vision,,,https://competitions.codalab.org/competitions/28894,https://paperswithcode.com/dataset/lidar-mos,"Tasks.
In moving object segmentation of point cloud sequences, one has to provide motion labels for each point of the test sequences 11-21. Therefore, the input to all evaluated methods is a list of coordinates of the three-dimensional points along with their remission, i.e., the strength of the reflected laser beam which depends on the properties of the surface that was hit. Each method should then output a label for each point of a scan, i.e., one full turn of the rotating LiDAR sensor. Here, we only distinguish between static and moving object classes.

Metric
To assess the labeling performance, we rely on the commonly applied Jaccard Index or intersection-over-union (mIoU) metric over moving parts of the environment. We map all moving-x classes of the original SemanticKITTI semantic segmentation benchmark to a single moving object class.

Citation
Citation. More information on the task and the metric, you can find in our publication related to the task:
@article{chen2021ral,
  title={{Moving Object Segmentation in 3D LiDAR Data: A Learning-based Approach Exploiting Sequential Data}},
  author={X. Chen and S. Li and B. Mersch and L. Wiesmann and J. Gall and J. Behley and C. Stachniss},
  year={2021},
  journal={IEEE Robotics and Automation Letters(RA-L)},
  doi = {10.1109/LRA.2021.3093567}
}",2021,,,,,
1738,LIDC-IDRI,Lung Nodule Detection,Lung Nodule Detection,"Lung Nodule Detection, Lung Nodule Segmentation, Neural Architecture Search, Lung Nodule 3D Detection, Lung Nodule 3D Classification, Lung Nodule Classification","3D, Image",,Computer Vision,"lung-nodule-3d-detection-on-lidc-idri, lung-nodule-segmentation-on-lidc-idri, lung-nodule-classification-on-lidc-idri, lung-nodule-detection-on-lidc-idri, lung-nodule-3d-classification-on-lidc-idri, neural-architecture-search-on-lidc-idri",Custom,https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI,https://paperswithcode.com/dataset/lidc-idri,"The LIDC-IDRI dataset contains lesion annotations from four experienced thoracic radiologists. LIDC-IDRI contains 1,018 low-dose lung CTs from 1010 lung patients.",,A 3D Probabilistic Deep Learning System for Detection and Diagnosis of Lung Cancer Using Low-Dose CT Scans,https://arxiv.org/abs/1902.03233,,,
1739,Lila,Mathematical Reasoning,Mathematical Reasoning,Mathematical Reasoning,,,Reasoning,,CC-BY-4.0,https://github.com/allenai/Lila,https://paperswithcode.com/dataset/lila,"Lila is a unified mathematical reasoning benchmark consisting of 23 diverse tasks along four dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. The benchmark is constructed by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs, thereby obtaining explainable solutions in addition to the correct answer.",,,,,,
1740,LIMUC,Medical Diagnosis,Medical Diagnosis,"Medical Diagnosis, Image Classification, Semi-Supervised Image Classification, Self-Supervised Learning, Medical Image Classification",Image,,Medical,image-classification-on-limuc,Creative Commons Attribution 4.0 International,https://zenodo.org/record/5827695#.ZF-92OzMJqs,https://paperswithcode.com/dataset/limuc,"The LIMUC dataset is the largest publicly available labeled ulcerative colitis dataset that compromises 11276 images from 564 patients and 1043 colonoscopy procedures. Three experienced gastroenterologists were involved in the annotation process, and all images are labeled according to the Mayo endoscopic score (MES).",,,,11276 images,,
1741,LinCE,Part-Of-Speech Tagging,Part-Of-Speech Tagging,"Part-Of-Speech Tagging, Named Entity Recognition (NER)","Audio, Image, Text",English,Speech,,,http://ritual.uh.edu/lince,https://paperswithcode.com/dataset/lince,"A centralized benchmark for Linguistic Code-switching Evaluation (LinCE) that combines ten corpora covering four different code-switched language pairs (i.e., Spanish-English, Nepali-English, Hindi-English, and Modern Standard Arabic-Egyptian Arabic) and four tasks (i.e., language identification, named entity recognition, part-of-speech tagging, and sentiment analysis).",,,,,,
1742,LingOly,Logical Reasoning,Logical Reasoning,Logical Reasoning,,,Reasoning,logical-reasoning-on-lingoly,CC-BY-NC-ND,https://huggingface.co/datasets/ambean/lingOly,https://paperswithcode.com/dataset/lingoly,"This dataset is a benchmark for complex reasoning abilities in large language models, drawing on United Kingdom Linguistics Olympiad problems which cover a wide range of languages.",,,,,,
1743,Linux,Graph Classification,Graph Classification,"Graph Classification, Graph Matching, Graph Similarity","Graph, Image",,Computer Vision,,,,https://paperswithcode.com/dataset/linux,"The LINUX dataset consists of 48,747 Program Dependence Graphs (PDG) generated from the Linux kernel. Each graph represents a function, where a node represents one statement and an edge represents the dependency between the two statements",,Convolutional Set Matching for Graph Similarity,https://arxiv.org/abs/1810.10866,,,
1744,LIRIS_human_activities_dataset,Action Detection,Action Detection,"Action Detection, Spatio-Temporal Action Localization","Image, Time Series, Video",,Computer Vision,,,https://projet.liris.cnrs.fr/voir/activities-dataset/,https://paperswithcode.com/dataset/liris-human-activities-dataset,"The LIRIS human activities dataset contains (gray/rgb/depth) videos showing people performing various activities taken from daily life (discussing, telphone calls, giving an item etc.). The dataset is fully annotated, where the annotation not only contains information on the action class but also its spatial and temporal positions in the video. It was originally shot for the ICPR-HARL 2012 competition.

The dataset has been shot with two different cameras:

Subset D1 has been shot with a MS Kinect module mounted on a remotely controlled Wany robotics Pekee II mobile robot which is part of the LIRIS-VOIR platform.
Subset D2 has been shot with a sony consumer camcorder",2012,,,,,
1745,ListOps,Structured Prediction,Structured Prediction,"Structured Prediction, Natural Language Inference, ListOps","Text, Time Series",English,Natural Language Processing,listops-on-listops,Custom,https://github.com/NYU-MLL/spinn/tree/listops-release,https://paperswithcode.com/dataset/listops,"The ListOps examples are comprised of summary operations on lists of single digit integers, written in prefix notation. The full sequence has a corresponding solution which is
also a single-digit integer, thus making it a ten-way balanced classification problem. For example, [MAX 2 9 [MIN 4 7 ] 0 ] has the solution 9. Each operation has a corresponding closing square bracket that defines the list of numbers for the operation. In this example, MIN operates on {4, 7}, while MAX operates on {2, 9, 4, 0}.",,ListOps: A Diagnostic Dataset for Latent Tree Learning,https://arxiv.org/pdf/1804.06028v1.pdf,,,
1746,ListUltraFeedback,Models Alignment,Models Alignment,Models Alignment,,,Methodology,,CC BY 4.0,https://huggingface.co/datasets/OPO-alignment/ListUltraFeedback,https://paperswithcode.com/dataset/listultrafeedback,A listwise multi-response dataset for human preferences alignment. The dataset is derived from UltraFeedback and SimPO.,,,,,,
1747,LITIS_Rouen,Scene Classification,Scene Classification,"Scene Classification, Data Augmentation, Acoustic Scene Classification","Audio, Image",,Computer Vision,,,https://homepages.tuni.fi/toni.heittola/datasets,https://paperswithcode.com/dataset/litis-rouen,The LITIS-Rouen dataset  is a dataset for audio scenes. It consists of 3026 examples of 19 scene categories. Each class is specific to a location such as a train station or an open market. The audio recordings have a duration of 30 seconds and a sampling rate of 22050 Hz. The dataset has a total duration of 1500 minutes.,,Spatio-Temporal Attention Pooling for Audio Scene Classification,https://arxiv.org/abs/1904.03543,3026 examples,,
1748,LiTS17,Tumor Segmentation,Tumor Segmentation,"Tumor Segmentation, Medical Image Segmentation, Computed Tomography (CT), Liver Segmentation",Image,,Computer Vision,"tumor-segmentation-on-lits17, medical-image-segmentation-on-lits2017, liver-segmentation-on-lits2017",,https://competitions.codalab.org/competitions/17094,https://paperswithcode.com/dataset/lits17,LiTS17 is a liver tumor segmentation benchmark. The data and segmentations are provided by various clinical sites around the world. The training data set contains 130 CT scans and the test data set 70 CT scans.,,https://arxiv.org/pdf/1707.07734.pdf,https://arxiv.org/pdf/1707.07734.pdf,,,
1749,LIVE-ETRI,Video Quality Assessment,Video Quality Assessment,Video Quality Assessment,Video,,Methodology,video-quality-assessment-on-live-etri,CC,https://live.ece.utexas.edu/research/ETRI-LIVE_STSVQ/index.html,https://paperswithcode.com/dataset/live-etri,"The video deployed parameter space is continuously increasing to provide more realistic and immersive experiences to global streaming and social media viewers. However, increments in video parameters such as spatial resolution or frame rate are inevitably associated with larger data volumes. Transmitting increasingly voluminous videos through limited bandwidth networks in a perceptually optimal way is a present challenge affecting billions of viewers. One recent practice adopted by the video service providers is space-time resolution adaptation in conjunction with video compression. Consequently, it is important to understand how different levels of space-time subsampling and compression affect the perceptual quality of videos.
Towards making progress in this direction, we constructed a large new resource, called the ETRI-LIVE Space-Time Subsampled Video Quality (ETRI-LIVE-STSVQ) database, containing 437 videos generated by applying various levels of combined space-time subsampling and video compression on 15 diverse video contents. We also conducted a large-scale human study on the new dataset, collecting about 15,000 subjective judgments of video quality. The ETRI-LIVE STSVQ database is being made publicly and freely available with the desire to improve future research and development on topics such as video quality modeling and perceptual video coding.",,,,,,
1750,LIVE-FB_LSVQ,Video Quality Assessment,Video Quality Assessment,Video Quality Assessment,Video,,Methodology,video-quality-assessment-on-live-fb-lsvq,,https://github.com/baidut/PatchVQ,https://paperswithcode.com/dataset/live-fb-lsvq,"No-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem to social and streaming media applications. Efficient and accurate video quality predictors are needed to monitor and guide the processing of billions of shared, often imperfect, user-generated content (UGC). Unfortunately, current NR models are limited in their prediction capabilities on real-world, ""in-the-wild"" UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 39, 000 real-world distorted videos and 117, 000 space-time localized video patches (""v-patches""), and 5.5M human perceptual quality annotations. Using this, we created two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a first-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time. We will make the new database and prediction models available immediately following the review process.",,,,,,
1751,LIVE-VQC,Video Quality Assessment,Video Quality Assessment,Video Quality Assessment,Video,,Methodology,video-quality-assessment-on-live-vqc,,https://live.ece.utexas.edu/research/LIVEVQC/index.html,https://paperswithcode.com/dataset/live-vqc,"The great variations of videographic skills in videography, camera designs, compression and processing protocols, communication and bandwidth environments, and displays leads to an enormous variety of video impairments. Current no-reference (NR) video quality models are unable to handle this diversity of distortions. This is true in part because available video quality assessment databases contain very limited content, fixed resolutions, were captured using a small number of camera devices by a few videographers and have been subjected to a modest number of distortions. As such, these databases fail to adequately represent real world videos, which contain very different kinds of content obtained under highly diverse imaging conditions and are subject to authentic, complex and often commingled distortions that are difficult or impossible to simulate. As a result, NR video quality predictors tested on real-world video data often perform poorly. Towards advancing NR video quality prediction, we have constructed a large-scale video quality assessment database containing 585 videos of unique content , captured using 101 different devices (43 device models) by 80 different users with wide ranges of levels of complex, authentic distortions. We collected a large number of subjective video quality scores via crowdsourcing. A total of 4776 unique participants took part in the study, yielding more than 205000 opinion scores , resulting in an average of 240 recorded human opinions per video . This study is the largest video quality assessment study ever conducted along several key dimensions: number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.",,,,,,
1752,LIVE-YT-HFR,Video Quality Assessment,Video Quality Assessment,"Video Quality Assessment, Visual Question Answering (VQA)","Image, Text, Video",English,Computer Vision,video-quality-assessment-on-live-yt-hfr,,https://live.ece.utexas.edu/research/LIVE_YT_HFR/LIVE_YT_HFR/index.html,https://paperswithcode.com/dataset/live-yt-hfr,"LIVE-YT-HFR comprises of 480 videos having 6 different frame rates, obtained from 16 diverse contents.",,,,,,
1753,LIVECell,Cell Segmentation,Cell Segmentation,"Cell Segmentation, Instance Segmentation",Image,,Computer Vision,cell-segmentation-on-livecell,CC-BY-NC 4.0,https://sartorius-research.github.io/LIVECell/,https://paperswithcode.com/dataset/livecell,"The LIVECell (Label-free In Vitro image Examples of Cells) dataset is a large-scale microscopic image dataset for instance-segmentation of individual cells in 2D cell cultures.

LIVECell consists of 5,239 manually annotated, expert-validated, Incucyte HD phase-contrast microscopy images with a total of 1,686,352 individual cells annotated from eight different cell types (average 313 cells per image). The  LIVECell images have predefined splits into training (3188), validation (539) and test (1512) sets. Each split is also further subdivided into each of the eight cell types. The training set also has splits of different sizes (2, 4, 5, 25, 50%) to allow dataset size experimentation.",,,,,"validated, Incucyte HD phase-contrast microscopy images",
1754,Liver-US,Medical Image Analysis,Medical Image Analysis,"Medical Image Analysis, Classification, Medical Diagnosis, Medical Image Classification",Image,,Medical,classification-on-liver-us,,https://github.com/Asunatan/HSQformer,https://paperswithcode.com/dataset/liver-us-liver-ultrasound-dataset,"The Liver-US dataset is a comprehensive collection of high-quality ultrasound images of the liver, including both normal and abnormal cases. This dataset is designed to facilitate research in medical image classification, with a focus on liver-related conditions. It includes a diverse range of ultrasound images acquired from multiple clinical settings, providing a robust foundation for developing and validating machine learning models in medical image analysis.
Detailed Dataset Description

Dataset Characteristics:
The Liver-US dataset contains735 ultrasound images of the liver, including both normal and abnormal cases. The dataset includes a variety of liver conditions, such as Hepatocellular Carcinoma (HCC), liver cysts, hemangiomas, and other focal liver lesions. The images were acquired using state-of-the-art ultrasound equipment, ensuring high resolution and contrast for accurate diagnosis.

Motivations and Content Summary:
The primary motivation behind the creation of the Liver-US dataset is to provide a comprehensive and diverse resource for researchers working on medical image classification task related to the liver. The dataset was compiled in collaboration with leading medical institutions and includes images from a diverse patient population, ensuring representativeness and generalizability. The annotations were performed by experienced radiologists, adhering to strict quality control standards to ensure accuracy and reliability.

Potential Use Cases:
Research and Development: The dataset is ideal for researchers and developers working on machine learning models for medical image classification and feature extraction.
Clinical Applications: It can be used to train and validate computer-aided diagnosis systems for early detection and diagnosis of liver conditions.
Educational Purposes: The dataset serves as a valuable resource for training medical professionals and students in the interpretation of ultrasound images and the diagnosis of liver conditions.
Benchmarking: It provides a standardized benchmark for comparing the performance of different algorithms and models in the field of medical image analysis.",,,,,,
1755,LIVE_Livestream,Video Quality Assessment,Video Quality Assessment,Video Quality Assessment,Video,,Methodology,video-quality-assessment-on-live-livestream,,https://live.ece.utexas.edu/research/LIVE_APV_Study/apv_index.html,https://paperswithcode.com/dataset/live-livestream,"LIVE Livestream is a database for Video Quality Assessment (VQA), specifically designed for live streaming VQA research. The dataset is called the Laboratory for Image and Video Engineering (LIVE) Live stream Database. The LIVE Livestream Database includes 315 videos of 45 contents impaired by 6 types of distortions.",,,,,,
1756,LJSpeech,Automatic Speech Recognition,Automatic Speech Recognition,"Automatic Speech Recognition, Text-To-Speech Synthesis, Speech Synthesis, Resynthesis","Audio, Image, Text",English,Speech,"speech-synthesis-on-ljspeech, automatic-speech-recognition-on-ljspeech, resynthesis-on-ljspeech, text-to-speech-synthesis-on-ljspeech",Public domain,https://keithito.com/LJ-Speech-Dataset/,https://paperswithcode.com/dataset/ljspeech,"This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours. The texts were published between 1884 and 1964, and are in the public domain. The audio was recorded in 2016-17 by the LibriVox project and is also in the public domain.",1964,,,,,
1757,LLaVA-Bench,visual instruction following,visual instruction following,visual instruction following,Image,,Computer Vision,visual-instruction-following-on-llava-bench,,https://llava-vl.github.io/,https://paperswithcode.com/dataset/llava-bench-in-the-wild,"LLaVA-Bench is a dataset created to evaluate the capability of large multimodal models (LMM) in more challenging tasks and generalizability to novel domains. It consists of a diverse set of 24 images with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches, etc., and each image with a highly-detailed and manually-curated description and a proper selection of questions. The dataset is part of the LLaVA project, which aims to develop multimodal chatbots that follow human intents to complete various daily-life visual tasks in the wild.",,,,24 images,valuate the capability of large multimodal models (LMM) in more challenging tasks and generalizability to novel domains. It consists of a diverse set of 24 images,
1758,LLVIP,Low-light Pedestrian Detection,Low-light Pedestrian Detection,"Low-light Pedestrian Detection, Image-to-Image Translation, Zero-shot Classification (unified classes), Low-Light Image Enhancement, Image Generation, Pedestrian Detection, Image Registration, Object Detection, Infrared And Visible Image Fusion, Thermal Infrared Pedestrian Detection, Image Enhancement, Multispectral Object Detection","Image, Text",English,Computer Vision,"image-generation-on-llvip, image-to-image-translation-on-llvip, pedestrian-detection-on-llvip, multispectral-object-detection-on-llvip, thermal-infrared-pedestrian-detection-on, object-detection-on-llvip, zero-shot-classification-unified-classes-on",Custom,https://bupt-ai-cz.github.io/LLVIP/,https://paperswithcode.com/dataset/llvip,"Visible-infrared Paired Dataset for Low-light Vision
30976  images (15488  pairs)
24 dark  scenes, 2 daytime scenes
Support for  image-to-image translation (visible to infrared, or infrared to visible), visible and infrared image fusion, low-light pedestrian detection, and infrared pedestrian detection
(The original image and video pairs (before registration) of LLVIP are also released!)",,,,30976  images,,
1759,LM-KBC_2023,Knowledge Base Population,Knowledge Base Population,Knowledge Base Population,,,Methodology,knowledge-base-population-on-lm-kbc-2023,Creative Commons Attribution 4.0 International,https://lm-kbc.github.io/challenge2023/,https://paperswithcode.com/dataset/lm-kbc-2023,"A diverse set of 21 relations, each covering a different set of subject-entities and a complete list of ground truth object-entities per subject-relation-pair. The total number of object-entities varies for a given subject-relation pair.

This dataset can be used to evaluate knowledge extraction systems.",,,,,,
1760,LM,6D Pose Estimation using RGBD,6D Pose Estimation using RGBD,"6D Pose Estimation using RGBD, 6D Pose Estimation, 6D Pose Estimation using RGB, Domain Adaptation","3D, Image",,Computer Vision,"6d-pose-estimation-on-linemod-2, 6d-pose-estimation-on-linemod, domain-adaptation-on-synth-objects-to-linemod, 6d-pose-estimation-using-rgbd-on-linemod, 6d-pose-estimation-using-rgb-on-occlusion",,https://bop.felk.cvut.cz/datasets,https://paperswithcode.com/dataset/linemod-1,"The LM (Linemod) dataset is a valuable resource introduced by Stefan Hinterstoisser and colleagues in their research on model-based training, detection, and pose estimation of texture-less 3D objects in heavily cluttered scenes¹. Let's delve into the details:


Purpose and Context:
The primary goal of the LM dataset is to facilitate the development and evaluation of methods for detecting and estimating the 6 degrees-of-freedom pose of texture-less 3D objects.

It specifically targets scenarios where objects lack distinctive textures and are embedded in complex backgrounds with occlusions.



Methodology:


The dataset builds upon the LINEMOD approach, which combines depth and color information to create templates representing different views of an object.
LINEMOD templates are learned from 3D models and serve as a basis for object detection.

The initial LINEMOD method had limitations, including online template learning and approximate pose estimation.



Improvements and Contributions:


Hinterstoisser et al. enhance LINEMOD by incorporating accurate 3D models of objects.
Their approach leverages the 3D model to address the shortcomings of the original LINEMOD.
Notable improvements include better pose estimation and reduced false positives.

The proposed framework is suitable for robotics applications, such as object manipulation.



Dataset Details:


The LM dataset consists of 15 registered video sequences, each containing over 1100 frames.
These sequences feature 15 different texture-less household objects.
Objects in the dataset exhibit discriminative color, shape, and size characteristics.
Researchers can use this dataset to evaluate and compare their methods for object detection and pose estimation.



In summary, the LM dataset provides a valuable benchmark for advancing the field of 6D object pose estimation, especially in scenarios where texture information is limited¹². Researchers can access this dataset to test and refine their algorithms, ultimately contributing to advancements in robotics and machine vision.

(1) Model Based Training, Detection and Pose ... - Stefan HINTERSTOISSER. http://stefan-hinterstoisser.com/papers/hinterstoisser2012accv.pdf.
(2) Datasets - BOP: Benchmark for 6D Object Pose Estimation. https://bop.felk.cvut.cz/datasets/.
(3) paroj/linemod_dataset: Hinterstoisser et al. ACCV12 dataset - GitHub. https://github.com/paroj/linemod_dataset.",,,,,,
1761,LMSYS-USP,Text Generation,Text Generation,"Text Generation, User Simulation",Text,English,Natural Language Processing,,,https://huggingface.co/datasets/wangkevin02/LMSYS-USP,https://paperswithcode.com/dataset/lmsys-usp,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
1762,LM_Email_Address_Leakage,Memorization,Memorization,Memorization,,,Methodology,,,https://github.com/jeffhj/LM_PersonalInfoLeak,https://paperswithcode.com/dataset/lm-email-address-leakage,"Are Large Pre-Trained Language Models Leaking Your Personal Information? We analyze whether Pre-Trained Language Models (PLMs) are prone to leaking personal information. Specifically, we query PLMs for email addresses with contexts of the email address or prompts containing the owner's name.",,,,,,
1763,LOCATA,Direction of Arrival Estimation,Direction of Arrival Estimation,Direction of Arrival Estimation,,,Methodology,,,https://www.locata.lms.tf.fau.de/,https://paperswithcode.com/dataset/locata,The LOCATA dataset is a dataset for acoustic source localization. It consists of real-world ambisonic speech recordings with optically tracked azimuth-elevation labels.,,Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks,https://arxiv.org/abs/1904.08452,,,
1764,LoDoPaB-CT,Dictionary Learning,Dictionary Learning,"Dictionary Learning, Medical Image Enhancement",Image,,Computer Vision,medical-image-enhancement-on-lodopab-ct,CC BY 4.0,https://zenodo.org/record/3384092,https://paperswithcode.com/dataset/lodopab-ct,"LoDoPaB-CT is a dataset of computed tomography images and simulated low-dose measurements. It contains over 40,000 scan slices from around 800 patients selected from the LIDC/IDRI Database.",,,,,,
1765,LogiQA,Natural Language Understanding,Natural Language Understanding,"Natural Language Understanding, Machine Reading Comprehension, Decision Making",Text,English,Natural Language Processing,,,https://github.com/lgw863/LogiQA-dataset,https://paperswithcode.com/dataset/logiqa,"LogiQA consists of 8,678 QA instances, covering multiple types of deductive reasoning. Results show that state-of-the-art neural models perform by far worse than human ceiling. The dataset can also serve as a benchmark for reinvestigating logical AI under the deep learning NLP setting.",,https://arxiv.org/pdf/2007.08124v1.pdf,https://arxiv.org/pdf/2007.08124v1.pdf,,,
1766,Logo-2K_,Data Augmentation,Data Augmentation,"Data Augmentation, Product Recommendation",,,Methodology,,,https://github.com/msn199959/Logo-2k-plus-Dataset,https://paperswithcode.com/dataset/logo-2k,"Logo-2K+:A Large-Scale Logo Dataset for Scalable Logo Classiﬁcation
The Logo-2K+ dataset contains a diverse range of logo classes from real-world logo images. It contains 167,140 images with 10 root categories and 2,341 leaf categories.
The 10 different root categories are: Food, Clothes, Institution, Accessories, Transportation, Electronic, Necessities, Cosmetic, Leisure and Medical.",,,,140 images,,
1767,LOGO-Net,Object Detection,Object Detection,"Object Detection, Logo Recognition, Few-Shot Object Detection",Image,,Computer Vision,,,http://logo-net.org/,https://paperswithcode.com/dataset/logo-net,A large-scale logo image database for logo detection and brand recognition from real-world product images.,,,,,,
1768,LOL-v2,Low-Light Image Enhancement,Low-Light Image Enhancement,Low-Light Image Enhancement,Image,,Computer Vision,low-light-image-enhancement-on-lol-v2,,https://github.com/flyywh/CVPR-2020-Semi-Low-Light,https://paperswithcode.com/dataset/lol-v2,LOL-v2-real contains 689 low-/normal-light image pairs for training and 100 pairs for testing.,,,,,,
1769,LOL,Low-Light Image Enhancement,Low-Light Image Enhancement,"Low-Light Image Enhancement, Unified Image Restoration",Image,,Computer Vision,"unified-image-restoration-on-lol, low-light-image-enhancement-on-lol",,https://daooshee.github.io/BMVC2018website/,https://paperswithcode.com/dataset/lol,The LOL dataset is composed of 500 low-light and normal-light image pairs and divided into 485 training pairs and 15 testing pairs. The low-light images contain noise produced during the photo capture process. Most of the images are indoor scenes. All the images have a resolution of 400×600.,,Unsupervised Real-world Low-light Image Enhancement with Decoupled Networks,https://arxiv.org/abs/2005.02818,,training pairs and 15 testing pairs. The low-light images,
1770,LongBench,Long-Context Understanding,Long-Context Understanding,Long-Context Understanding,,,Methodology,long-context-understanding-on-longbench,,https://github.com/THUDM/LongBench,https://paperswithcode.com/dataset/longbench,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
1771,LongVALE,Dense Video Captioning,Dense Video Captioning,"Dense Video Captioning, Video Captioning, Natural Language Moment Retrieval, Audio captioning, Audio-Visual Video Captioning, Video Boundary Captioning, Moment Retrieval, Video Grounding, audio-visual event localization","Audio, Image, Text, Video",English,Computer Vision,,CC BY-NC-SA 4.0,https://ttgeng233.github.io/LongVALE/,https://paperswithcode.com/dataset/longvale,"Despite impressive advancements in video understanding, most efforts remain limited to coarse-grained or visual-only video tasks. However, real-world videos encompass omni-modal information (vision, audio, and speech) with a series of events forming a cohesive storyline. The lack of multi-modal video data with fine-grained event annotations and the high cost of manual labeling are major obstacles to comprehensive omni-modality video perception. To address this gap, we propose an automatic pipeline consisting of high-quality multi-modal video filtering, semantically coherent omni-modal event boundary detection, and cross-modal correlation-aware event captioning. In this way, we present LongVALE, the first-ever Vision-Audio-Language Event understanding benchmark comprising 105K omni-modal events with precise temporal boundaries and detailed relation-aware captions within 8.4K high-quality long videos. Further, we build a baseline that leverages LongVALE to enable video large language models (LLMs) for omni-modality fine-grained temporal video understanding for the first time. Extensive experiments demonstrate the effectiveness and great potential of LongVALE in advancing comprehensive multi-modal video understanding.",,,,,,
1772,Long_Range_Graph_Benchmark__LRGB_,Link Prediction,Link Prediction,"Link Prediction, Graph Classification, Graph Regression, Node Classification","Graph, Image, Time Series",,Computer Vision,"graph-classification-on-peptides-func, graph-regression-on-peptides-struct, link-prediction-on-pcqm-contact, node-classification-on-pascalvoc-sp-1, node-classification-on-coco-sp",Custom,https://github.com/vijaydwivedi75/lrgb,https://paperswithcode.com/dataset/pascalvoc-sp,"The Long Range Graph Benchmark (LRGB) is a collection of 5 graph learning datasets that arguably require long-range reasoning to achieve strong performance in a given task. The 5 datasets in this benchmark can be used to prototype new models that can capture long range dependencies in graphs.

|  Dataset | Domain  |  Task | 
|---|---|---|
| PascalVOC-SP| Computer Vision | Node Classification |
| COCO-SP | Computer Vision | Node Classification |
| PCQM-Contact | Quantum Chemistry | Link Prediction |
| Peptides-func | Chemistry | Graph Classification |
| Peptides-struct | Chemistry | Graph Regression |",,Custom,https://arxiv.org/pdf/2206.08164,,,
1773,LoRA-WiSE,Dataset Size Recovery,Dataset Size Recovery,"Dataset Size Recovery, Classification",Image,,Computer Vision,,,https://huggingface.co/datasets/MoSalama98/LoRA-WiSE,https://paperswithcode.com/dataset/lora-wise,"The LoRA Weight Size Evaluation (LoRA-WiSE) is a comprehensive
benchmark specifically designed to evaluate LoRA dataset size recovery methods for generative models 
LoRA-WiSE spans various dataset sizes, backbones, ranks, and personalization sets, as presented in 
the ""Dataset Size Recovery from LoRA Weights""


🌐 Project page: 
https://vision.huji.ac.il/dsire/",,,,,,
1774,Lorenz_Dataset,Probabilistic Time Series Forecasting,Probabilistic Time Series Forecasting,"Probabilistic Time Series Forecasting, Time Series Forecasting",Time Series,,Time Series,probabilistic-time-series-forecasting-on,,https://git.opendfki.de/koochali/forgan,https://paperswithcode.com/dataset/lorenz-dataset-1,The Lorenz dataset contains 100000 time-series with length 24. The data has 5 modes and it is obtained using the Lorenz equation with 5 different seed values.,,,,,,
1775,Lost_and_Found,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Anomaly Detection, Self-Driving Cars, Autonomous Driving",Image,,Computer Vision,anomaly-detection-on-lost-and-found,Custom,http://www.6d-vision.com/lostandfounddataset,https://paperswithcode.com/dataset/lost-and-found,Lost and Found is a novel lost-cargo image sequence dataset comprising more than two thousand frames with pixelwise annotations of obstacle and free-space and provide a thorough comparison to several stereo-based baseline methods. The dataset will be made available to the community to foster further research on this important topic.,,,,,,
1776,Lowest_Common_Ancestor_Generations__LCAG__Phasespa,Graph Learning,Graph Learning,Graph Learning,Graph,,Methodology,,Creative Commons Attribution 4.0 International,https://doi.org/10.5281/zenodo.6983257,https://paperswithcode.com/dataset/lowest-common-ancestor-generations-lcag,"This dataset contains simulated synthetic particle decays, simulated using the PhaseSpace library.
All simulated decay topologies have a common root particle of mass 100 (arbitrary units). Intermediate particles are selected at random with replacement from the following masses: [90, 80, 70, 50, 25, 20, 10].
Final state particles, which make up the leaf nodes of generated topologies, are drawn with replacement from the following masses: [1, 2, 3, 5, 12]. For each intermediate particle (including the root), we limit the minimum number of children to two, and the maximum five.
The dataset contains the resulting simulated particle physics decays, with information about the detected particle (leaves) to be used as input, and Lowest Common Ancestor Generations (LCAGs) to be used as training targets.

Tree topology creation to generate the dataset was as follows:
starting from the root particle a set of children are selected from the available intermediate and final state particles such that the sum of their masses totals less than the root, this process is then repeated for each child particle which is not a final state particle and so on until only final state particles remain.

This dataset consists of 200 topologies (unique decay processes) in total, with 16,000 samples per topology. In the paper's experiments, 2000 topologies for each of training, validation, and testing were used. Leaf node features are not normalized. We have not enforced any ordering of the nodes and leave them unsorted as created in the dataset.",2000,,,000 samples,,
1777,LRA,Language Modelling,Language Modelling,"Language Modelling, Long-range modeling",Text,English,Natural Language Processing,long-range-modeling-on-lra,,https://github.com/google-research/long-range-arena,https://paperswithcode.com/dataset/lra,"Long-range arena (LRA) is an effort toward systematic evaluation of efficient transformer models. The project aims at establishing benchmark tasks/datasets using which we can evaluate transformer-based models in a systematic way, by assessing their generalization power, computational efficiency, memory foot-print, etc. Long-Range Arena is specifically focused on evaluating model quality under long-context scenarios. The benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning.

Description from: Long Range Arena : A Benchmark for Efficient Transformers",,Long Range Arena : A Benchmark for Efficient Transformers,https://arxiv.org/pdf/2011.04006v1.pdf,,"valuation of efficient transformer models. The project aims at establishing benchmark tasks/datasets using which we can evaluate transformer-based models in a systematic way, by assessing their generalization power, computational efficiency, memory foot-print, etc. Long-Range Arena is specifically focused on evaluating model quality under long-context scenarios. The benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images",
1778,LRS2,Image Manipulation,Image Manipulation,"Image Manipulation, Audio-Visual Speech Recognition, Unconstrained Lip-synchronization, Automatic Speech Recognition (ASR), Speech Separation, Visual Keyword Spotting, Landmark-based Lipreading, Visual Speech Recognition, Speech Recognition, Lipreading","Audio, Image, Text",English,Computer Vision,"lip-sync-on-lrs2, visual-keyword-spotting-on-lrs2, automatic-speech-recognition-on-lrs2, image-manipulation-on-lrs2, visual-speech-recognition-on-lrs2, landmark-based-lipreading-on-lrs2, audio-visual-speech-recognition-on-lrs2, speech-recognition-on-lrs2, lipreading-on-lrs2, speech-separation-on-lrs2",Custom (non-commercial),https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html,https://paperswithcode.com/dataset/lrs2,"The Oxford-BBC Lip Reading Sentences 2 (LRS2) dataset is one of the largest publicly available datasets for lip reading sentences in-the-wild. The database consists of mainly news and talk shows from BBC programs. Each sentence is up to 100 characters in length. The training, validation and test sets are divided according to broadcast date. It is a challenging set since it contains thousands of speakers without speaker labels and large variation in head pose. The pre-training set contains 96,318 utterances, the training set contains 45,839 utterances, the validation set contains 1,082 utterances and the test set contains 1,242 utterances.",,Audio-visual Recognition of Overlapped speech for the LRS2 dataset,https://arxiv.org/abs/2001.01656,,,
1779,LRS3-TED,Active Speaker Detection,Active Speaker Detection,"Active Speaker Detection, Audio-Visual Speech Recognition, Automatic Speech Recognition (ASR), Gesture Synchronization, Visual Keyword Spotting, Visual Speech Recognition, Speech Recognition, Lipreading","Audio, Image, Text",English,Computer Vision,"automatic-speech-recognition-asr-on-lrs3-ted, visual-speech-recognition-on-lrs3-ted, speech-recognition-on-lrs3-ted, visual-keyword-spotting-on-lrs3-ted, gesture-synchronization-on-lrs3-ted, lipreading-on-lrs3-ted, audio-visual-speech-recognition-on-lrs3-ted, active-speaker-detection-on-lrs3-ted",Creative Commons BY-NC-ND 4.0 license,https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs3.html,https://paperswithcode.com/dataset/lrs3-ted,"LRS3-TED is a multi-modal dataset for visual and audio-visual speech recognition. It includes face tracks from over 400 hours of TED and TEDx videos, along with the corresponding subtitles and word alignment boundaries. The new dataset is substantially larger in scale compared to other public datasets that are available for general research.",,LRS3-TED: a large-scale dataset for visual speech recognition,https://arxiv.org/pdf/1809.00496v2.pdf,,,
1780,LRW,Unconstrained Lip-synchronization,Unconstrained Lip-synchronization,"Unconstrained Lip-synchronization, Audio-Visual Speech Recognition, Talking Face Generation, Lip Reading, Visual Keyword Spotting, Landmark-based Lipreading, Lipreading, Lip to Speech Synthesis","Audio, Image, Text",English,Computer Vision,"lip-reading-on-lrw, talking-face-generation-on-lrw, lip-to-speech-synthesis-on-lrw, audio-visual-speech-recognition-on-lrw, visual-keyword-spotting-on-lrw, lipreading-on-lip-reading-in-the-wild, landmark-based-lipreading-on-lrw, lip-sync-on-lrw","Custom (research-only, non-commercial, attribution)",https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html,https://paperswithcode.com/dataset/lrw,"The Lip Reading in the Wild (LRW) dataset  a large-scale audio-visual database that contains 500 different words from over 1,000 speakers. Each utterance has 29 frames, whose boundary is centered around the target word. The database is divided into training, validation and test sets. The training set contains at least 800 utterances for each class while the validation and test sets contain 50 utterances.",,Rudrabha/Wav2Lip,https://arxiv.org/abs/2008.10010,,,
1781,LSEC,Product Recommendation,Product Recommendation,Product Recommendation,,,Methodology,,,https://github.com/yusanshi/LSEC-GNN,https://paperswithcode.com/dataset/lsec,"The LSEC (Live Stream E-Commerce) dataset has two subsets: LSEC-Small and LSEC-Large. It is a dataset for studying E-commerce transactions in the context of live streams, where the streames are talking about products while interacting with their audience. The dataset consists of interaction information among streamers, users, and products.",,,,,,
1782,LSFB_Datasets,Sign Language Production,Sign Language Production,"Sign Language Production, Sign Language Translation, Sign Language Recognition","Image, Text",English,Computer Vision,,CC BY-NC-SA 4.0,https://lsfb.info.unamur.be/,https://paperswithcode.com/dataset/lsfb-datasets,"Sign Language Datasets for French Belgian Sign Language
This dataset is built upon the work of Belgian linguists from the University of Namur. During eight years, they've collected and annotated 50 hours of videos depicting sign language conversation. 100 signers were recorded, making it one of the most representative sign language corpus. 
The annotation has been sanitized and enriched with metadata to construct two, easy to use, datasets for sign language recognition. One for continuous sign language recognition and the other for isolated sign recognition. 

LSFB-CONT
The dataset for continuous sign language recognition is made of over 25h of video clips. Each clip is associated with a time-aligned annotation file containing the start and the end of each sign along with a gloss (label) associated with all unique signs. Mediapipe pose and hands information were also computed for each video clip and these metadata are made available in the dataset.

LSFB-ISOL
The isolated version of the dataset contains only clips showing one isolated sign issued from the LSFB-CONT dataset. We chose to keep all the signs with at least 40 examples, leading to a dataset containing over 50 000 clips for 635 different glosses (labels). The Mediapipe metadata is also available for this dataset.",,,,40 examples,,
1783,LSHTC,Extreme Multi-Label Classification,Extreme Multi-Label Classification,"Extreme Multi-Label Classification, Multi-Label Classification, Text Classification","Image, Text",English,Computer Vision,,,http://lshtc.iit.demokritos.gr/,https://paperswithcode.com/dataset/lshtc,"LSHTC is a dataset for large-scale text classification. The data used in the LSHTC challenges originates from two popular sources: the DBpedia and the ODP (Open Directory Project) directory, also known as DMOZ. DBpedia instances were selected from the english, non-regional Extended Abstracts provided by the DBpedia site. The DMOZ instances consist
of either Content vectors, Description vectors or both. A Content vectors is obtained by directly indexing the web page using standard indexing chain (preprocessing, stemming/lemmatization, stop-word removal).",,,,,,
1784,LSICC,Chinese Word Segmentation,Chinese Word Segmentation,"Chinese Word Segmentation, Sentiment Analysis","Image, Text",English,Computer Vision,,,https://github.com/JaniceZhao/Douban-Dushu-Dataset,https://paperswithcode.com/dataset/lsicc,Large Scale Informal Chinese Corpus (LSICC) is a large-scale corpus of informal Chinese. This corpus contains around 37 million book reviews and 50 thousand netizen's comments to the news.,,,,,,
1785,LSMDC-E,Image-guided Story Ending Generation,Image-guided Story Ending Generation,Image-guided Story Ending Generation,"Image, Text",English,Computer Vision,image-guided-story-ending-generation-on-lsmdc,,,https://paperswithcode.com/dataset/lsmdc-e,"LSMDC-E contains 20,151 training samples, 1,477 validation samples and 2,005 test samples, which is modified from LSMDC 2021. We take the first four sentences in every five-sentence story as the story context and the last sentence as the story ending. As every sentence relates to a movie frame set in LSMDC, we take the last frame set as the ending-related image set for IgSEG.",2021,,,,"training samples, 1,477 validation samples",
1786,LSOIE,Open Information Extraction,Open Information Extraction,Open Information Extraction,,,Methodology,"open-information-extraction-on-lsoie, open-information-extraction-on-lsoie-wiki",,https://github.com/Jacobsolawetz/large-scale-oie,https://paperswithcode.com/dataset/lsoie,"LSOIE is a large-scale OpenIE data converted from QA-SRL 2.0 in two domains, i.e., Wikipedia and Science. It is 20 times larger than the next largest human-annotated OpenIE data, and thus is reliable for fair evaluation. LSOIE provides n-ary OpenIE annotations and gold tuples are in the 〈ARG0, Relation, ARG1, . . . , ARGn〉 format. The dataset has two subsets ... namely LSOIE-wiki and LSOIE-sci, for comprehensive evaluation. LSOIE-wiki has 24,251 sentences and LSOIE-sci has 47,919 sentences.

Source: https://arxiv.org/pdf/2212.02068v1.pdf (section 5)",,,,251 sentences,,
1787,LSP,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Pose Estimation, 3D Pose Estimation","3D, Image",,Computer Vision,pose-estimation-on-leeds-sports-poses,Custom,https://dbcollection.readthedocs.io/en/latest/datasets/leeds_sports_pose_extended.html,https://paperswithcode.com/dataset/lsp,"The Leeds Sports Pose (LSP) dataset is widely used as the benchmark for human pose estimation. The original LSP dataset contains 2,000 images of sportspersons gathered from Flickr, 1000 for training and 1000 for testing. Each image is annotated with 14 joint locations, where left and right joints are consistently labelled from a person-centric viewpoint. The extended LSP dataset contains additional 10,000 images labeled for training.

Image: Sumer et al",,Deep Deformation Network for Object Landmark Localization,https://arxiv.org/abs/1605.01014,000 images,"training and 1000 for testing. Each image is annotated with 14 joint locations, where left and right joints are consistently labelled from a person-centric viewpoint. The extended LSP dataset contains additional 10,000 images",
1788,LSSED,Speech Emotion Recognition,Speech Emotion Recognition,Speech Emotion Recognition,"Audio, Image",,Speech,speech-emotion-recognition-on-lssed,,https://github.com/tobefans/LSSED,https://paperswithcode.com/dataset/lssed,"LSSED, a challenging large-scale english dataset for speech emotion recognition. It contains 147,025 sentences (206 hours and 25 minutes in total) spoken by 820 people. Each segment is annotated for the presence of 11 emotions (angry, neutral, fear, happy, sad, disappointed, bored, disgusted, excited, surprised, fear and other)",,,,025 sentences,,
1789,LSUI,Underwater Image Restoration,Underwater Image Restoration,Underwater Image Restoration,Image,,Computer Vision,underwater-image-restoration-on-lsui,,https://bianlab.github.io/data.html,https://paperswithcode.com/dataset/lsui,"We released a large-scale underwater image (LSUI) dataset including 5004 image pairs, which involve richer underwater scenes (lighting conditions, water types and target categories) and better visual quality reference images than the existing ones.",,,,,,
1790,LTCC,Unsupervised Person Re-Identification,Unsupervised Person Re-Identification,"Unsupervised Person Re-Identification, Person Re-Identification",Image,,Computer Vision,"person-re-identification-on-ltcc, unsupervised-person-re-identification-on-ltcc",,https://naiq.github.io/LTCC_Perosn_ReID.html,https://paperswithcode.com/dataset/ltcc,"LTCC contains 17,119 person images of 152 identities, and each identity is captured by at least two cameras.  The dataset can be divided into two subsets: one cloth-change set where 91 persons appear with 416 different sets of outfits in 14,783 images, and one cloth-consistent subset containing the remaining 61 identities with 2,336 images without outfit changes. On average, there are 5 different clothes for each cloth-changing person, with the number of outfit changes ranging from 2 to 14.",,,,783 images,,
1791,LUMA,Multimodal Deep Learning,Multimodal Deep Learning,"Multimodal Deep Learning, Decision Making Under Uncertainty, Classification, Uncertainty Quantification",Image,,Multimodal,,CC BY-SA 4.0,https://huggingface.co/datasets/bezirganyan/LUMA,https://paperswithcode.com/dataset/luma,"LUMA is a multimodal dataset that consists of audio, image, and text modalities. It allows controlled injection of uncertainties into the data and is mainly intended for studying uncertainty quantification in multimodal classification settings. 
This repository provides the Audio and Text modalities. The image modality consists of images from CIFAR-10/100 datasets. 
To download the image modality and compile the dataset with a specified amount of uncertainties, please use the LUMA compilation tool.",,,,,,
1792,LUN,Document Classification,Document Classification,Document Classification,"Image, Text",English,Computer Vision,document-classification-on-lun,,https://hrashkin.github.io/factcheck.html,https://paperswithcode.com/dataset/lun,"LUN is used for unreliable news source classification, this dataset includes 17,250 articles from satire, propaganda, and hoaxe.",,,,,,
1793,LUNA,Lung Nodule Detection,Lung Nodule Detection,"Lung Nodule Detection, Computed Tomography (CT), Lung Nodule Segmentation",Image,,Computer Vision,"lung-nodule-segmentation-on-luna, lung-nodule-detection-on-luna2016-fpred",CC BY 4.0,https://luna16.grand-challenge.org/,https://paperswithcode.com/dataset/luna,"The LUNA challenges provide datasets for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified.",,,,,,
1794,LUNA16,Lung Nodule Detection,Lung Nodule Detection,"Lung Nodule Detection, Computed Tomography (CT)",Image,,Computer Vision,lung-nodule-detection-on-luna2016-fpred,CC BY 4.0,https://luna16.grand-challenge.org/Data/,https://paperswithcode.com/dataset/luna16,"The LUNA16 (LUng Nodule Analysis) dataset is a dataset for lung segmentation. It consists of 1,186 lung nodules annotated in 888 CT scans.",,Universal Lesion Detection by Learning from Multiple Heterogeneously Labeled Datasets,https://arxiv.org/abs/2005.13753,,,
1795,LVIS,Few-Shot Object Detection,Few-Shot Object Detection,"Few-Shot Object Detection, Instance Segmentation, Long-tailed Object Detection, Novel Object Detection, Unsupervised Object Detection, Zero-Shot Instance Segmentation, Zero-Shot Object Detection, Object Detection, Open Vocabulary Object Detection",Image,English,Computer Vision,"zero-shot-object-detection-on-lvis-v1-0-val, unsupervised-object-detection-on-lvis-v1-0, zero-shot-object-detection-on-lvis-v1-0, zero-shot-instance-segmentation-on-lvis-v1-0, few-shot-object-detection-on-lvis-v1-0-test, object-detection-on-lvis-v1-0-val, object-detection-on-lvis-v1-0-minival, novel-object-detection-on-lvis-v1-0-val, object-detection-on-lvis-v1-0-1, instance-segmentation-on-lvis-v1-0-val, open-vocabulary-object-detection-on-lvis-v1-0, long-tailed-object-detection-on-lvis-v1-0-val, few-shot-object-detection-on-lvis-v1-0-val, instance-segmentation-on-lvis-v1-0-test-dev",Custom (CC BY 4.0 + COCO license),https://www.lvisdataset.org/dataset,https://paperswithcode.com/dataset/lvis,LVIS is a dataset for long tail instance segmentation. It has annotations for over 1000 object categories in 164k images.,,LVIS,https://arxiv.org/pdf/1908.03195.pdf,164k images,,
1796,Lyft_Level_5_Prediction,Autonomous Vehicles,Autonomous Vehicles,"Autonomous Vehicles, motion prediction, Motion Forecasting","Time Series, Video",,Methodology,,,https://self-driving.lyft.com/level5/prediction/,https://paperswithcode.com/dataset/lyft-level-5-prediction,"A self-driving dataset for motion prediction, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time.",,,,,,
1797,LymphoMNIST,Fine-Grained Image Classification,Fine-Grained Image Classification,"Fine-Grained Image Classification, Semi-supervised Medical Image Classification, Classification, Medical Image Classification",Image,,Computer Vision,,Apache-2.0,https://github.com/Khayrulbuet13/LymphoMNIST,https://paperswithcode.com/dataset/lymphomnist,"LymphoMNIST is a comprehensive dataset designed for the nuanced classification of lymphocyte images. It encompasses approximately 80,000 high-resolution 64x64 images, meticulously categorized into three primary classes: B cells, T4 cells, and T8 cells.​

Dataset Characteristics:

Size: ~80,000 images​
Resolution: 64x64 pixels​
Classes: B cells, T4 cells, T8 cells​
Format: MNIST-like standardized biomedical imagery​
Modality: Microscopy-based high-resolution cell images​

Motivation and Summary: LymphoMNIST aims to bridge the gap in biomedical image analysis by providing a dataset that is vast in scale and rich in detail. It supports a wide array of research endeavors, from fundamental biological studies to advanced computational model development.​

Potential Use Cases:

Medical Research: Studying lymphocyte morphology and characteristics​
Machine Learning & AI: Developing and evaluating image classification models​
AutoML & Benchmarking: Serving as a benchmark dataset for automated model training and performance evaluation​
Educational Purposes: Teaching deep learning concepts in biomedical imaging​
Data Collection Process: The dataset comprises high-resolution images of lymphocytes obtained through microscopy. Each image is standardized to a 64x64 pixel resolution to maintain consistency and facilitate analysis.​

Annotations and Labels: Each image is labeled as one of the three lymphocyte classes: B cells, T4 cells, or T8 cells. The labeling process was conducted by experts in the field to ensure accuracy.",,,,64 images,,
1798,Lyra_Dataset,Music Classification,Music Classification,Music Classification,"Audio, Image",,Computer Vision,,,https://github.com/pxaris/lyra-dataset,https://paperswithcode.com/dataset/lyra-dataset,"Lyra is a dataset of 1570 traditional and folk Greek music pieces that includes audio and video (timestamps and links to YouTube videos), along with annotations that describe aspects of particular interest for this dataset, including instrumentation, geographic information and labels of genre and subgenre, among others.",,A Dataset for Greek Traditional and Folk Music: Lyra,https://arxiv.org/pdf/2211.11479v1.pdf,,,
1799,L_M-24,Caption Generation,Caption Generation,"Caption Generation, Text-based de novo Molecule Generation, Molecule Captioning","Image, Text",English,Computer Vision,molecule-captioning-on-l-m-24,,https://github.com/language-plus-molecules/LPM-24-Dataset,https://paperswithcode.com/dataset/l-m-24,"Language-molecule models have emerged as an exciting direction for molecular discovery and understanding. However, training these models is challenging due to the scarcity of molecule-language pair datasets. At this point, datasets have been released which are 1) small and scraped from existing databases, 2) large but noisy and constructed by performing entity linking on the scientific literature, and 3) built by converting property prediction datasets to natural language using templates. In this document, we detail the L+M-24 dataset, which has been created for the Language + Molecules Workshop shared task at ACL 2024. In particular, L+M-24 is designed to focus on three key benefits of natural language in molecule design: compositionality, functionality, and abstraction",2024,,,,,
1800,M-VAD_Names,Video Captioning,Video Captioning,"Video Captioning, Partial Label Learning, Gender Prediction, Video Description","Image, Text, Time Series, Video",English,Computer Vision,partial-label-learning-on-m-vad-names,,https://github.com/aimagelab/mvad-names-dataset,https://paperswithcode.com/dataset/m-vad-names,"The dataset contains the annotations of characters' visual appearances, in the form of tracks of face bounding boxes, and the associations with characters' textual mentions, when available. The detection and annotation of the visual appearances of characters in each video clip of each movie was achieved through a semi-automatic approach. The released dataset contains more than 24k annotated video clips, including 63k visual tracks and 34k textual mentions, all associated with their character identities.",,,,,,
1801,m2caiSeg,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Data Augmentation, Unsupervised Pre-training",Image,,Computer Vision,,,https://arxiv.org/abs/2008.10134,https://paperswithcode.com/dataset/m2caiseg,"Created from endoscopic video feeds of real-world surgical procedures. Overall, the data consists of 307 images, each of which is annotated for the organs and different surgical instruments present in the scene.",,Homepage,https://arxiv.org/abs/2008.10134,307 images,,
1802,M2E2,Event Extraction,Event Extraction,Event Extraction,,,Methodology,,,http://blender.cs.illinois.edu/software/m2e2/,https://paperswithcode.com/dataset/m2e2,Aims to extract events and their arguments from multimedia documents. Develops the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments.,,,,,,
1803,M3GIA,Multimodal Reasoning,Multimodal Reasoning,Multimodal Reasoning,,,Reasoning,,apache-2.0,https://huggingface.co/datasets/Songweii/M3GIA,https://paperswithcode.com/dataset/m3gia,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
1804,M3LS,Multimodal Abstractive Text Summarization,Multimodal Abstractive Text Summarization,"Multimodal Abstractive Text Summarization, Cross-Lingual Abstractive Summarization, Abstractive Text Summarization, Data Summarization, Cross-Language Text Summarization, News Summarization",Text,English,Multimodal,,MIT,https://github.com/zenquiorra/M3LS,https://paperswithcode.com/dataset/m3ls-multi-lingual-multi-modal-summarization,"Significant developments in techniques such as encoder-decoder models have enabled us to represent information comprising multiple modalities. This information can further enhance many downstream tasks in the field of information retrieval and natural language processing; however, improvements in multi-modal techniques and their performance evaluation require large-scale multi-modal data which offers sufficient diversity. Multi-lingual modeling for a variety of tasks like multi-modal summarization, text generation, and translation leverages information derived from high-quality multi-lingual annotated data. In this work, we present the current largest multi-lingual multi-modal summarization dataset (M3LS), and it consists of over a million instances of document-image pairs along with a professionally annotated multi-modal summary for each pair. It is derived from news articles published by British Broadcasting Corporation(BBC) over a decade and spans 20 languages, targeting diversity across five language roots, it is also the largest summarization dataset for 13 languages and consists of cross-lingual summarization data for 2 languages. We formally define the multi-lingual multi-modal summarization task utilizing our dataset and report baseline scores from various state-of-the-art summarization techniques in a multi-lingual setting. We also compare it with many similar datasets to analyze the uniqueness and difficulty of M3LS.",,,,,,
1805,M4,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Meta-Learning",Time Series,,Time Series,,,https://github.com/Mcompetitions/M4-methods,https://paperswithcode.com/dataset/m4,"The M4 dataset is a collection of 100,000 time series used for the fourth edition of the Makridakis forecasting Competition. The M4 dataset consists of time series of yearly, quarterly, monthly and other (weekly, daily and hourly) data, which are divided into training and test sets. The minimum numbers of observations in the training test are 13 for yearly, 16 for quarterly, 42 for monthly, 80 for weekly, 93 for daily and 700 for hourly series. The participants were asked to produce the following numbers of forecasts beyond the available data that they had been given: six for yearly, eight for quarterly, 18 for monthly series, 13 for weekly series and 14 and 48 forecasts respectively for the daily and hourly ones.

The M4 dataset was created by selecting a random sample of 100,000 time series from the ForeDeCk database. The selected series were then scaled to prevent negative observations and values lower than 10, thus avoiding possible problems when calculating various error measures. The scaling was performed by simply adding a constant to the series so that their minimum value was equal to 10 (29 occurrences across the whole dataset). In addition, any information that could possibly lead to the identification of the original series was removed so as to ensure the objectivity of the results. This included the starting dates of the series, which did not become available to the participants until the M4 had ended.",,,,,,
1806,M4Raw,MRI Reconstruction,MRI Reconstruction,"MRI Reconstruction, Image Denoising","3D, Image",,Computer Vision,,CC-BY-4.0,https://github.com/mylyu/M4Raw,https://paperswithcode.com/dataset/m4raw,"Recently, low-field magnetic resonance imaging (MRI) has gained renewed interest to promote MRI accessibility and affordability worldwide. The presented M4Raw dataset aims to facilitate methodology development and reproducible research in this field. The dataset comprises multi-channel brain k-space data collected from 183 healthy volunteers using a 0.3 Tesla whole-body MRI system, and includes T1-weighted, T2-weighted, and fluid attenuated inversion recovery (FLAIR) images with in-plane resolution of ~1.2 mm and through-plane resolution of 5 mm. Importantly, each contrast contains multiple repetitions, which can be used individually or to form multi-repetition averaged images. After excluding motion-corrupted data, the partitioned training and validation subsets contain 1024 and 240 volumes, respectively. To demonstrate the potential utility of this dataset, we trained deep learning models for image denoising and parallel imaging tasks and compared their performance with traditional reconstruction methods. This M4Raw dataset will be valuable for the development of advanced data-driven methods specifically for low-field MRI. It can also serve as a benchmark dataset for general MRI reconstruction algorithms.

Reference:
Lyu, M., Mei, L., Huang, S. et al. M4Raw: A multi-contrast, multi-repetition, multi-channel MRI k-space dataset for low-field MRI research. Sci Data 10, 264 (2023). https://doi.org/10.1038/s41597-023-02181-4",2023,,,,,
1807,Machine_Number_Sense,Relational Reasoning,Relational Reasoning,"Relational Reasoning, Visual Reasoning, Common Sense Reasoning",Image,,Reasoning,,,https://sites.google.com/view/number-sense/home,https://paperswithcode.com/dataset/machine-number-sense,Consists of visual arithmetic problems automatically generated using a grammar model--And-Or Graph (AOG). These visual arithmetic problems are in the form of geometric figures: each problem has a set of geometric shapes as its context and embedded number symbols.,,,,,,
1808,MACS,Audio captioning,Audio captioning,Audio captioning,"Audio, Image, Text",English,Audio,,Other (Non commercial),https://zenodo.org/records/5114771,https://paperswithcode.com/dataset/macs,"This is a dataset containing audio captions and corresponding audio tags for a number of 3930 audio files of the TAU Urban Acoustic Scenes 2019 development dataset (airport, public square, and park). The files were annotated using a web-based tool. Each file is annotated by multiple annotators that provided tags and a one-sentence description of the audio content.",2019,,,,,
1809,MACSum,Specificity,Specificity,Specificity,,,Methodology,,,https://github.com/psunlpgroup/MACSum,https://paperswithcode.com/dataset/macsum,"MACSum a human-annotated summarization dataset for controlling mixed attributes. It contains source texts from two domains, news articles and dialogues, with human-annotated summaries controlled by five designed attributes (Length, Extractiveness, Specificity, Topic, and Speaker).",,MACSum: Controllable Summarization with Mixed Attributesbnbgf,https://arxiv.org/pdf/2211.05041v1.pdf,,,
1810,MAD,Moment Retrieval,Moment Retrieval,"Moment Retrieval, Video Grounding, Natural Language Visual Grounding, Natural Language Moment Retrieval","Image, Text, Video",English,Computer Vision,"video-grounding-on-mad, natural-language-moment-retrieval-on-mad",,https://github.com/Soldelli/MAD,https://paperswithcode.com/dataset/mad,"MAD (Movie Audio Descriptions) is an automatically curated large-scale dataset for the task of natural language grounding in videos or natural language moment retrieval.
MAD exploits available audio descriptions of mainstream movies. Such audio descriptions are redacted for visually impaired audiences and are therefore highly descriptive of the visual content being displayed. 
MAD contains over 384,000 natural language sentences grounded in over 1,200 hours of video, and provides a unique setup for video grounding as the visual stream is truly untrimmed with an average video duration of 110 minutes. 2 orders of magnitude longer than legacy datasets. 

Take a look at the paper for additional information.

From the authors on availability: ""Due to copyright constraints, MAD’s videos will not be publicly released. However, we will provide all necessary features for our experiments’ reproducibility and promote future research in this direction""",,,,,,
1811,MAESTRO,Music Generation,Music Generation,"Music Generation, Music Transcription, Audio Generation","Audio, Text",English,Natural Language Processing,music-transcription-on-maestro,,https://magenta.tensorflow.org/datasets/maestro,https://paperswithcode.com/dataset/maestro,"The MAESTRO dataset contains over 200 hours of paired audio and MIDI recordings from ten years of International Piano-e-Competition. The MIDI data includes key strike velocities and sustain/sostenuto/una corda pedal positions. Audio and MIDI files are aligned with ∼3 ms accuracy and sliced to individual musical pieces, which are annotated with composer, title, and year of performance. Uncompressed audio is of CD quality or higher (44.1–48 kHz 16-bit PCM stereo).",,,,,,
1812,Mafiascum,Deception Detection,Deception Detection,Deception Detection,Image,,Computer Vision,,,https://bitbucket.org/bopjesvla/thesis/src/master/,https://paperswithcode.com/dataset/mafiascum,"A collection of over 700 games of Mafia, in which players are randomly assigned either deceptive or non-deceptive roles and then interact via forum postings. Over 9000 documents were compiled from the dataset, which each contained all messages written by a single player in a single game. This corpus was used to construct a set of hand-picked linguistic features based on prior deception research, as well as a set of average word vectors enriched with subword information.",,,,9000 documents,,
1813,MagicData-RAMC,Automatic Speech Recognition (ASR),Automatic Speech Recognition (ASR),"Automatic Speech Recognition (ASR), Speech Recognition, Speaker Diarization","Audio, Image, Text",English,Speech,,CC BY-NC 4.0,https://www.magicdatatech.com/datasets/mdt2021s003-1647827542,https://paperswithcode.com/dataset/magicdata-ramc,"The MagicData-RAMC corpus contains 180 hours of conversational speech data recorded from native speakers of Mandarin Chinese over mobile phones with a sampling rate of 16 kHz. The dialogs in the dialogs are classified into 15 diversified domains and tagged with topic labels, ranging from science and technology to ordinary life. Accurate transcription and precise speaker voice activity timestamps are manually labeled for each sample. Speakers' detailed information is also provided.",,,,,,
1814,MagnaTagATune,Music Auto-Tagging,Music Auto-Tagging,"Music Auto-Tagging, Music Tagging, Music Classification","Audio, Image",,Computer Vision,"music-tagging-on-magnatagatune, music-auto-tagging-on-magnatagatune-clean, music-auto-tagging-on-magnatagatune",,http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset,https://paperswithcode.com/dataset/magnatagatune,"MagnaTagATune dataset contains 25,863 music clips. Each clip is a 29-seconds-long excerpt belonging to one of the 5223 songs, 445 albums and 230 artists. The clips span a broad range of genres like Classical, New Age, Electronica, Rock, Pop, World, Jazz, Blues, Metal, Punk, and more. Each audio clip is supplied with a vector of binary annotations of 188 tags. These annotations are obtained by humans playing the two-player online TagATune game. In this game, the two players are either presented with the same or a different audio clip. Subsequently, they are asked to come up with tags for their specific audio clip. Afterward, players view each other’s tags and are asked to decide whether they were presented the same audio clip. Tags are only assigned when more than two players agreed. The annotations include tags like ’singer’, ’no singer’, ’violin’, ’drums’, ’classical’, ’jazz’. The top 50 most popular tags are typically used for evaluation to ensure that there is enough training data for each tag. There are 16 parts, and researchers comonnly use parts 1-12 for training, part 13 for validation and parts 14-16 for testing.",,Brains on Beats,https://arxiv.org/abs/1606.02627,,,
1815,Make3D,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Monocular Depth Estimation, Depth Estimation","3D, Image",,Computer Vision,monocular-depth-estimation-on-make3d,CC BY-NC 3.0,http://make3d.cs.cornell.edu/data.html#make3d,https://paperswithcode.com/dataset/make3d,"The Make3D dataset is a monocular Depth Estimation dataset that contains 400 single training RGB and depth map pairs, and 134 test samples. The RGB images have high resolution, while the depth maps are provided at low resolution.",,Structured Coupled Generative Adversarial Networks for Unsupervised Monocular Depth Estimation,https://arxiv.org/abs/1908.05794,,"training RGB and depth map pairs, and 134 test samples",
1816,MAKED,Multilingual NLP,Multilingual NLP,"Multilingual NLP, Keyword Extraction",Text,English,Natural Language Processing,,MIT,https://github.com/zenquiorra/MAKED,https://paperswithcode.com/dataset/maked,"Keyword extraction is an integral task for many downstream problems like clustering, recommendation, search and classification. Development and evaluation of keyword extraction techniques require an exhaustive dataset; however, currently, the community lacks large-scale multi-lingual datasets. In this paper, we present MAKED, a large-scale multi-lingual keyword extraction dataset comprising of 540K+ news articles from British Broadcasting Corporation News (BBC News) spanning 20 languages. It is the first keyword extraction dataset for 11 of these 20 languages. The quality of the dataset is examined by experimentation with several baselines. We believe that the proposed dataset will help advance the field of automatic keyword extraction given its size, diversity in terms of languages used, topics covered and time periods as well as its focus on under-studied languages.",,,,,,
1817,Malaria_Dataset,Image Classification,Image Classification,"Image Classification, Medical Image Classification",Image,,Computer Vision,"image-classification-on-malaria-dataset, medical-image-classification-on-malaria",,https://lhncbc.nlm.nih.gov/publication/pub9932,https://paperswithcode.com/dataset/malaria-dataset,"The dataset contains a total of 27,558 cell images with equal instances of parasitized and uninfected cells.",,,,,,
1818,MALF,Object Detection,Object Detection,"Object Detection, Face Detection, Robust Face Recognition",Image,,Computer Vision,,,http://www.cbsr.ia.ac.cn/faceevaluation/,https://paperswithcode.com/dataset/malf,"The MALF dataset is a large dataset with 5,250 images annotated with multiple facial attributes and it is specifically constructed for fine grained evaluation.",,Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results,https://arxiv.org/abs/1804.10275,250 images,,
1819,Malimg,Malware Classification,Malware Classification,"Malware Classification, Malware Clustering, Malware Family Detection, Malware Analysis",Image,,Computer Vision,malware-classification-on-malimg-dataset,,https://drive.google.com/file/d/1M83VzyIQj_kuE9XzhClGK5TZWh1T_pr-/view,https://paperswithcode.com/dataset/malimg,"The Malimg Dataset contains 9,339 malware byteplot images from 25 different families.",,,,,,
1820,MalNet,Malware Type Detection,Malware Type Detection,"Malware Type Detection, Graph Classification, Malware Family Detection, Malware Detection","Graph, Image",,Computer Vision,"malware-detection-on-malnet, malware-family-detection-on-malnet, malware-type-detection-on-malnet, graph-classification-on-malnet-tiny",,https://mal-net.org/,https://paperswithcode.com/dataset/malnet,"MalNet is a large public graph database, representing a large-scale ontology of software function call graphs. MalNet contains over 1.2 million graphs, averaging over 17k nodes and 39k edges per graph, across a hierarchy of 47 types and 696 families.",,,,,,
1821,MAMEM1_MOABB,Within-Session SSVEP,Within-Session SSVEP,Within-Session SSVEP,,,Methodology,within-session-ssvep-on-mamem1-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.MAMEM1.html,https://paperswithcode.com/dataset/mamem1-moabb,,,,,,,
1822,MAMEM2_MOABB,Within-Session SSVEP,Within-Session SSVEP,Within-Session SSVEP,,,Methodology,within-session-ssvep-on-mamem2-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.MAMEM2.html,https://paperswithcode.com/dataset/mamem2-moabb,,,,,,,
1823,MAMEM3_MOABB,Within-Session SSVEP,Within-Session SSVEP,Within-Session SSVEP,,,Methodology,within-session-ssvep-on-mamem3-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.MAMEM3.html,https://paperswithcode.com/dataset/mamem3-moabb,,,,,,,
1824,MAMS,Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),Text,English,Natural Language Processing,aspect-based-sentiment-analysis-on-mams,,https://github.com/siat-nlp/MAMS-for-ABSA,https://paperswithcode.com/dataset/mams,"MAMS is a challenge dataset for aspect-based sentiment analysis (ABSA), in which each sentences contain at least two aspects with different sentiment polarities. MAMS dataset contains two versions: one for aspect-term sentiment analysis (ATSA) and one for aspect-category sentiment analysis (ACSA).",,,,,,
1825,MaNGA,Hyperspectral image analysis,Hyperspectral image analysis,"Hyperspectral image analysis, Astronomy",Image,,Computer Vision,,,https://www.sdss4.org/dr17/manga,https://paperswithcode.com/dataset/manga,"MaNGA is a component of the Fourth-Generation Sloan Digital Sky Survey whose goal is to map the detailed composition and kinematic structure of nearby galaxies. MaNGA uses integral field unit (IFU) spectroscopy to measure spectra for hundreds of points within each galaxy.  MaNGA’s goal is to understand the “life history” of present-day galaxies from imprinted clues of their birth and assembly, through their ongoing growth via star formation and merging, to their death from quenching at late times.

The primary MaNGA data products are composed of 3-D calibrated data cubes produced by the DRP[89] and 2-D maps of derived quantities, such as emission line fluxes, gas and stellar kinematics, and stellar population properties, produced by the DAP[149] from those cubes. The 3-D data cubes are constructed from a few tens to a few thousands of individual spectra that have been combined onto a regular grid. The 2-D maps of derived quantities are constructed by analyzing individual or binned groups of spaxels and constructing maps of the quantities at the relevant on-sky location.",,,,,,
1826,Manga109,Face Detection,Face Detection,"Face Detection, Image Super-Resolution, Blind Super-Resolution, Body Detection, Object Detection",Image,Japanese,Computer Vision,"image-super-resolution-on-manga109-8x, face-detection-on-manga109, image-super-resolution-on-manga109-16x, body-detection-on-manga109, image-super-resolution-on-manga109-2x, blind-super-resolution-on-manga109-3x, blind-super-resolution-on-manga109-4x, image-super-resolution-on-manga109-4x, object-detection-on-manga109, object-detection-on-manga109-s-15test, blind-super-resolution-on-manga109-2x, image-super-resolution-on-manga109-3x",Custom,http://www.manga109.org/en/,https://paperswithcode.com/dataset/manga109,"Manga109 has been compiled by the Aizawa Yamasaki Matsui Laboratory, Department of Information and Communication Engineering, the Graduate School of Information Science and Technology, the University of Tokyo. The compilation is intended for use in academic research on the media processing of Japanese manga. Manga109 is composed of 109 manga volumes drawn by professional manga artists in Japan. These manga were commercially made available to the public between the 1970s and 2010s, and encompass a wide range of target readerships and genres (see the table in Explore for further details.) Most of the manga in the compilation are available at the manga library “Manga Library Z” (formerly the “Zeppan Manga Toshokan” library of out-of-print manga).",,https://arxiv.org/pdf/1510.04389v1.pdf,https://arxiv.org/pdf/1510.04389v1.pdf,,,
1827,ManiCups,Text-based Image Editing,Text-based Image Editing,Text-based Image Editing,"Image, Text",English,Computer Vision,,Apache-2.0,https://cyclenetweb.github.io/,https://paperswithcode.com/dataset/manicups,Multi-domain Image Editing Benchmark,,,,,,
1828,ManipulateSound,Robot Manipulation,Robot Manipulation,Robot Manipulation,,,Methodology,,,https://github.com/xf-zhao/ManipulateSound,https://paperswithcode.com/dataset/manipulatesound,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
1829,ManiSkill2,Robot Manipulation,Robot Manipulation,"Robot Manipulation, Imitation Learning, Reinforcement Learning (RL)",,,Methodology,,Apache-2.0 license,https://maniskill2.github.io/,https://paperswithcode.com/dataset/maniskill2,"ManiSkill2 is the next generation of the SAPIEN ManiSkill benchmark, to address critical pain points often encountered by researchers when using benchmarks for generalizable manipulation skills. It includes 20 manipulation task families with 2000+ object models and 4M+ demonstration frames, which cover stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation tasks with 2D/3D input data simulated by fully dynamic engines.",2000,ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills,https://arxiv.org/pdf/2302.04659v1.pdf,,,
1830,many-solutions-sudoku,Game of Sudoku,Game of Sudoku,Game of Sudoku,,,Methodology,,,https://sites.google.com/view/yatinnandwani/1oml,https://paperswithcode.com/dataset/many-solutions-sudoku,"A data set of Sudoku grids with more than one solution.

This was introduced to train on logical reasoning problems with non-unique solutions.",,,,,,
1831,ManySStuBs4J,Program Repair,Program Repair,Program Repair,,,Methodology,,Attribution 4.0 International,https://zenodo.org/record/3653444,https://paperswithcode.com/dataset/manysstubs4j,"The ManySStuBs4J corpus is a collection of simple fixes to Java bugs, designed for evaluating program repair techniques. We collect all bug-fixing changes using the SZZ heuristic, and then filter these to obtain a data set of small bug fix changes.
These are single statement fixes, classified where possible into one of 16 syntactic templates which we call SStuBs.
The dataset contains simple statement bugs mined from open-source Java projects hosted in GitHub.
There are two variants of the dataset. One mined from the 100 Java Maven Projects and one mined from the top 1000 Java Projects.

The dataest contains 153,652 single statement bugfix changes mined from 1,000 popular open-source Java projects, annotated by whether they match any of a set of 16 bug templates, inspired by state-of-the-art program repair techniques.",,,,,,
1832,ManyTypes4TypeScript,Type prediction,Type prediction,Type prediction,Time Series,,Methodology,type-prediction-on-manytypes4typescript,CC-4.0,https://huggingface.co/datasets/kevinjesse/ManyTypes4TypeScript,https://paperswithcode.com/dataset/manytypes4typescript,Type Inference dataset for TypeScript. Click on DOI tag for dataset files.,,,,,,
1833,map2seq,Natural Language Landmark Navigation Instructions Generation,Natural Language Landmark Navigation Instructions Generation,"Natural Language Landmark Navigation Instructions Generation, Vision and Language Navigation",Text,English,Natural Language Processing,"vision-and-language-navigation-on-map2seq, natural-language-landmark-navigation",,https://map2seq.schumann.pub/dataset/download/,https://paperswithcode.com/dataset/map2seq,"7,672 human written natural language navigation instructions for routes in OpenStreetMap with a focus on visual landmarks. Validated in Street View.",,,,,,
1834,Mapillary_Vistas_Dataset,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Visual Place Recognition, Panoptic Segmentation",Image,,Computer Vision,"semantic-segmentation-on-mapillary-val, panoptic-segmentation-on-mapillary-val, visual-place-recognition-on-mapillary-val",Custom (non-commercial),https://www.mapillary.com/dataset/vistas?lat=20&lng=0&z=1.5&pKey=pBBmjuJ8yU1r2ROYRzWmFg,https://paperswithcode.com/dataset/mapillary-vistas-dataset,Mapillary Vistas Dataset is a diverse street-level imagery dataset with pixel‑accurate and instance‑specific human annotations for understanding street scenes around the world.,,Neuhold et al,https://openaccess.thecvf.com/content_ICCV_2017/papers/Neuhold_The_Mapillary_Vistas_ICCV_2017_paper.pdf,,,
1835,MapReader_Data,Image Classification,Image Classification,"Image Classification, Clustering, Classification",Image,,Computer Vision,,,https://zenodo.org/records/7147906,https://paperswithcode.com/dataset/mapreader-data,"MapReader in GeoHumanities workshop (SIGSPATIAL 2022): Gold standards and outputs

Refer to: 
https://github.com/Living-with-machines/MapReader/wiki/GeoHumanities-workshop-in-SIGSPATIAL-2022",2022,,,,,
1836,MAPS,Music Transcription,Music Transcription,Music Transcription,Audio,,Audio,music-transcription-on-maps,,,https://paperswithcode.com/dataset/maps,"MAPS – standing for MIDI Aligned Piano Sounds – is a database of MIDI-annotated piano recordings. MAPS has been designed in order to be released in the music information retrieval research community, especially for the development and the evaluation of algorithms for single-pitch or multipitch estimation and automatic transcription of music. It is composed by isolated notes, random-pitch chords, usual musical chords and pieces of music. The database provides a large amount of sounds obtained in various recording conditions.",,,,,,
1837,MARIDA,Image Segmentation,Image Segmentation,"Image Segmentation, Weakly supervised segmentation",Image,,Computer Vision,image-segmentation-on-marida,Creative Commons Attribution 4.0 International,https://marine-debris.github.io/,https://paperswithcode.com/dataset/marida,"MARIDA (Marine Debris Archive) is the first dataset based on the multispectral Sentinel-2 (S2) satellite data, which distinguishes Marine Debris from various marine features that co-exist, including Sargassum macroalgae, Ships, Natural Organic Material, Waves, Wakes, Foam, dissimilar water types (i.e., Clear, Turbid Water, Sediment-Laden Water, Shallow Water), and Clouds. MARIDA is an open-access dataset which enables the research community to explore the spectral behaviour of certain floating materials, sea state features and water types, to develop and evaluate Marine Debris detection solutions based on artificial intelligence and deep learning architectures, as well as satellite pre-processing pipelines.  Although it is designed to be beneficial for several machine learning tasks, it primarily aims to benchmark weakly supervised pixel-level semantic segmentation learning methods. 

MARIDA can be downloaded from the repository Zenodo (https://doi.org/10.5281/zenodo.5151941). A quick start guide for all ML benchmarks and the detailed overview of the dataset are available at https://marine-debris.github.io/.",,,,,,
1838,Mario_AI,SNES Games,SNES Games,"SNES Games, Real-Time Strategy Games, Starcraft",,,Methodology,,,https://github.com/zerg000000/mario-ai,https://paperswithcode.com/dataset/mario-ai,"Mario AI was a benchmark environment for reinforcement learning. The gameplay in Mario AI, as in the original Nintendo’s version, consists in moving the controlled character, namely Mario, through two-dimensional levels, which are viewed sideways. Mario can walk and run to the right and left, jump, and (depending on which state he is in) shoot fireballs. Gravity acts on Mario, making it necessary to jump over cliffs to get past them. Mario can be in one of three states: Small, Big (can kill enemies by jumping onto them), and Fire (can shoot fireballs).",,http://julian.togelius.com/Karakovskiy2012The.pdf,http://julian.togelius.com/Karakovskiy2012The.pdf,,,
1839,Market-1501-C,Person Re-Identification,Person Re-Identification,"Person Re-Identification, Generalizable Person Re-identification",Image,,Computer Vision,person-re-identification-on-market-1501-c,,https://github.com/MinghuiChen43/CIL-ReID,https://paperswithcode.com/dataset/market-1501-c,"Market-1501-C is an evaluation set that consists of algorithmically generated corruptions applied to the Market-1501 test-set.  These corruptions consist of Noise: Gaussian, shot,
impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions.",,,,,,
1840,Market-1501,Unsupervised Person Re-Identification,Unsupervised Person Re-Identification,"Unsupervised Person Re-Identification, Unsupervised Domain Adaptation, Person Re-Identification, Pose Transfer, Generalizable Person Re-identification","3D, Image",,Computer Vision,"unsupervised-person-re-identification-on-4, unsupervised-domain-adaptation-on-market-to, unsupervised-domain-adaptation-on-market-to-1, generalizable-person-re-identification-on-21, pose-transfer-on-market-1501, unsupervised-domain-adaptation-on-duke-to, person-re-identification-on-dukemtmc-reid-1, unsupervised-person-re-identification-on-1, person-re-identification-on-market-1501",,https://www.kaggle.com/pengcw1/market-1501/data,https://paperswithcode.com/dataset/market-1501,"Market-1501 is a large-scale public benchmark dataset for person re-identification. It contains 1501 identities which are captured by six different cameras, and 32,668 pedestrian image bounding-boxes obtained using the Deformable Part Models pedestrian detector. Each person has 3.6 images on average at each viewpoint. The dataset is split into two parts: 750 identities are utilized for training and the remaining 751 identities are used for testing. In the official testing protocol 3,368 query images are selected as probe set to find the correct match across 19,732 reference gallery images.",,A Survey of Pruning Methods for Efficient Person Re-identification Across Domains,https://arxiv.org/abs/1907.02547,6 images,"split into two parts: 750 identities are utilized for training and the remaining 751 identities are used for testing. In the official testing protocol 3,368 query images",
1841,MARS-DL,Video-Based Person Re-Identification,Video-Based Person Re-Identification,Video-Based Person Re-Identification,"Image, Video",,Computer Vision,,,https://drive.google.com/file/d/1adP39y7xoKYX8Z4lyBtZiDTg9kZyK1Cx/view,https://paperswithcode.com/dataset/mars-dl,"MARS dataset processed with our re-Detect and Link (DL) module.

More information: https://github.com/jackie840129/CF-AAN",,,,,,
1842,MARS,Video-Based Person Re-Identification,Video-Based Person Re-Identification,"Video-Based Person Re-Identification, Unsupervised Person Re-Identification, Person Re-Identification","Image, Video",,Computer Vision,"unsupervised-person-re-identification-on-mars, person-re-identification-on-mars",,http://zheng-lab.cecs.anu.edu.au/Project/project_mars.html,https://paperswithcode.com/dataset/mars,"MARS (Motion Analysis and Re-identification Set) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).",,Multi-Target Tracking in Multiple Non-Overlapping Cameras using Constrained Dominant Sets,https://arxiv.org/abs/1706.06196,,,
1843,MaRVL,Max-Shot Cross-Lingual Visual Reasoning,Max-Shot Cross-Lingual Visual Reasoning,"Max-Shot Cross-Lingual Visual Reasoning, Visual Reasoning, Zero-Shot Cross-Lingual Visual Reasoning, Zero-Shot Cross-Lingual Transfer",Image,,Reasoning,"max-shot-cross-lingual-visual-reasoning-on, zero-shot-cross-lingual-visual-reasoning-on, zero-shot-cross-lingual-transfer-on-marvl",CC BY 4.0 License,https://marvl-challenge.github.io/,https://paperswithcode.com/dataset/marvl,"Multicultural Reasoning over Vision and Language (MaRVL) is a dataset based on an ImageNet-style hierarchy representative of many languages and cultures (Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish). The selection of both concepts and images is entirely driven by native speakers. Afterwards, we elicit statements from native speakers about pairs of images. The task consists in discriminating whether each grounded statement is true or false.",,,,,,
1844,MasakhaNER,Named Entity Recognition,Named Entity Recognition,"Named Entity Recognition, Cross-Lingual NER, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,"cross-lingual-ner-on-masakhaner2-0, named-entity-recognition-on-masakhaner-1",,https://github.com/masakhane-io/masakhane-ner,https://paperswithcode.com/dataset/masakhaner,"MasakhaNER is a collection of Named Entity Recognition (NER) datasets for 10 different African languages. The languages forming this dataset are: Amharic, Hausa, Igbo, Kinyarwanda, Luganda, Luo, Nigerian-Pidgin, Swahili, Wolof, and Yorùbá.",,,,,,
1845,MasakhaNEWS,News Classification,News Classification,News Classification,Image,,Computer Vision,,,https://github.com/masakhane-io/masakhane-news,https://paperswithcode.com/dataset/masakhanews,MasakhaNEWS is a benchmark dataset for news topic classification covering 16 languages widely spoken in Africa.,,MasakhaNEWS: News Topic Classification for African languages,https://arxiv.org/pdf/2304.09972v1.pdf,,,
1846,MASC,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Part-Of-Speech Tagging, Sentence segmentation, Automatic Speech Recognition, Named Entity Recognition (NER), Constituency Parsing","Audio, Image, Text",English,Computer Vision,automatic-speech-recognition-on-masc,Creative Commons Attribution 4.0 International,https://www.anc.org/data/masc/,https://paperswithcode.com/dataset/masc,"The Manually Annotated Sub-Corpus (MASC) consists of approximately 500,000 words of contemporary American English written and spoken data drawn from the Open American National Corpus (OANC).

All of MASC includes manually validated annotations for sentence boundaries, token, lemma and POS;
noun and verb chunks; named entities (person, location, organization, date); Penn Treebank syntax;
coreference; and discourse structure.

Additional manually produced or validated annotations have been produced by the MASC project
for portions of the sub-corpus, including full-text annotation for FrameNet frame elements
and a 100K+ sentence corpus with WordNet 3.1 sense tags, of which one-tenth are also annotated for
FrameNet frame elements.

Annotations of all or portions of the sub-corpus for a wide variety of other linguistic phenomena
have been contributed by other projects, including PropBank, TimeBank, Pittsburgh opinion, and several others.

Unlike most freely available corpora including a wide variety of linguistic annotations,
MASC contains a balanced selection of texts from a broad range of genres.",,,,,,
1847,MaskedFace-Net,Face Detection,Face Detection,Face Detection,Image,,Computer Vision,,,https://github.com/cabani/MaskedFace-Net,https://paperswithcode.com/dataset/maskedface-net,"Proposes three types of masked face detection dataset; namely, the Correctly Masked Face Dataset (CMFD), the Incorrectly Masked Face Dataset (IMFD) and their combination for the global masked face detection (MaskedFace-Net).",,,,,,
1848,MaSS,K-complex detection,K-complex detection,"K-complex detection, Video Captioning, Spindle Detection, Sleep Stage Detection, Speech Recognition","Audio, Image, Text, Video",English,Computer Vision,"sleep-stage-detection-on-mass-ss2, spindle-detection-on-mass-ss2, k-complex-detection-on-mass-ss2",,https://github.com/getalp/mass-dataset,https://paperswithcode.com/dataset/mass,"MaSS (Multilingual corpus of Sentence-aligned Spoken utterances) is an extension of the CMU Wilderness Multilingual Speech Dataset, a speech dataset based on recorded readings of the New Testament.

MaSS extends it by providing a large and clean dataset of 8,130 parallel spoken utterances across 8 languages (56 language pairs). The covered languages are: Basque, English, Finnish, French, Hungarian, Romanian, Russian and Spanish.",,https://arxiv.org/pdf/1907.12895v3.pdf,https://arxiv.org/pdf/1907.12895v3.pdf,,,
1849,MASSIVE,Zero-Shot Intent Classification,Zero-Shot Intent Classification,"Zero-Shot Intent Classification, Intent Classification and Slot Filling, Zero-Shot Intent Classification and Slot Filling, Slot Filling, Zero-shot Slot Filling, Intent Classification",Image,,Computer Vision,"intent-classification-on-massive, slot-filling-on-massive, zero-shot-slot-filling-on-massive, intent-classification-and-slot-filling-on, zero-shot-intent-classification-and-slot, zero-shot-intent-classification-on-massive",CC BY 4.0,https://github.com/alexa/massive,https://paperswithcode.com/dataset/massive,"MASSIVE is a parallel dataset of > 1M utterances across 51 languages with annotations for the Natural Language Understanding tasks of intent prediction and slot annotation. Utterances span 60 intents and include 55 slot types. MASSIVE was created by localizing the SLURP dataset, composed of general Intelligent Voice Assistant single-shot interactions.",,,,,,
1850,MassSpecGym,MS/MS spectrum simulation (bonus chemical formulae),MS/MS spectrum simulation (bonus chemical formulae),"MS/MS spectrum simulation (bonus chemical formulae), De novo molecule generation from MS/MS spectrum (bonus chemical formulae), MS/MS spectrum simulation, Molecule retrieval from MS/MS spectrum, Molecule retrieval from MS/MS spectrum (bonus chemical formulae), De novo molecule generation from MS/MS spectrum",Text,English,Natural Language Processing,"molecule-retrieval-from-ms-ms-spectrum-bonus, ms-ms-spectrum-simulation-bonus-chemical, ms-ms-spectrum-simulation-on-massspecgym, molecule-retrieval-from-ms-ms-spectrum-on, de-novo-molecule-generation-from-ms-ms-1, de-novo-molecule-generation-from-ms-ms",MIT,https://github.com/pluskal-lab/MassSpecGym,https://paperswithcode.com/dataset/massspecgym,"MassSpecGym provides three challenges for benchmarking the discovery and identification of new molecules from MS/MS spectra:


💥 De novo molecule generation (MS/MS spectrum → molecular structure)
✨ Bonus chemical formulae challenge (MS/MS spectrum + chemical formula → molecular structure)


💥 Molecule retrieval (MS/MS spectrum → ranked list of candidate molecular structures)
✨ Bonus chemical formulae challenge (MS/MS spectrum → ranked list of candidate molecular structures with ground-truth chemical formulae)


💥 Spectrum simulation (molecular structure → MS/MS spectrum)
✨ Bonus chemical formulae challenge (molecular structure → MS/MS spectrum; evaluated on the retrieval of molecular structures with ground-truth chemical formulae)



The provided challenges abstract the process of scientific discovery from biological and environmental samples into well-defined machine learning problems with pre-defined datasets, data splits, and evaluation metrics.",,,,,,
1851,Matbench,Band Gap,Band Gap,"Band Gap, Formation Energy",,,Methodology,"formation-energy-on-materials-project, band-gap-on-materials-project",,https://matbench.materialsproject.org,https://paperswithcode.com/dataset/matbench,"The Matbench test suite v0.1 contains 13 supervised ML tasks from 10 datasets. Matbench’s data are sourced from various subdisciplines of materials science, such as experimental mechanical properties (alloy strength), computed elastic properties, computed and experimental electronic properties, optical and phonon properties, and thermodynamic stabilities for crystals, 2D materials, and disordered metals. The number of samples in each task ranges from 312 to 132,752, representing both relatively scarce experimental materials properties and comparatively abundant properties such as DFT-GGA formation energies. Each task is a self-contained dataset containing a single material primitive as input (either composition or composition plus crystal structure) and target property as output for each sample.",,,,,"test suite v0.1 contains 13 supervised ML tasks from 10 datasets. Matbench’s data are sourced from various subdisciplines of materials science, such as experimental mechanical properties (alloy strength), computed elastic properties, computed and experimental electronic properties, optical and phonon properties, and thermodynamic stabilities for crystals, 2D materials, and disordered metals. The number of samples",
1852,Materials_Project,Band Gap,Band Gap,"Band Gap, Formation Energy",,,Methodology,"formation-energy-on-materials-project, band-gap-on-materials-project",CC BY 4.0,https://materialsproject.org/,https://paperswithcode.com/dataset/materials-project,"The Materials Project is a collection of chemical compounds labelled with different attributes. The labelling is performed by different simulations, most of them at DFT level of theory.

The dataset links:


MP 2018.6.1 (69,239 materials)
MP 2019.4.1 (133,420 materials)",2018,,,,,
1853,MATH-V,Multiple-choice,Multiple-choice,"Multiple-choice, Multimodal Reasoning, Mathematical Reasoning",,,Methodology,multimodal-reasoning-on-math-v,MIT,https://mathvision-cuhk.github.io/,https://paperswithcode.com/dataset/math-v,"Math-Vision (Math-V) dataset is a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs.

Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on Math-Vision, underscoring the imperative for further advancements in LMMs. Moreover, our detailed categorization allows for a thorough error analysis of LMMs, offering valuable insights to guide future research and development.",,,,,,
1854,MATH,Math Word Problem Solving,Math Word Problem Solving,Math Word Problem Solving,,,Methodology,"math-word-problem-solving-on-math, math-word-problem-solving-on-math-minival",MIT,https://github.com/hendrycks/math/,https://paperswithcode.com/dataset/math,"MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations.",,Hendrycks et al.,https://arxiv.org/pdf/2103.03874.pdf,,,
1855,Math23K,Math Word Problem Solving,Math Word Problem Solving,Math Word Problem Solving,,,Methodology,math-word-problem-solving-on-math23k,,https://ai.tencent.com/ailab/nlp/dialogue/#datasets,https://paperswithcode.com/dataset/math23k,"Math23K is a dataset created for math word problem solving, contains 23, 162 Chinese problems crawled from the Internet. Refer to our paper for more details:
The dataset is originally introduced in the paper Deep Neural Solver for Math Word Problems.  The original files are originally split into train/test split, while other research efforts (https://github.com/2003pro/Graph2Tree) perform the train/dev/test split.",,,,,,
1856,MathBench,Elementary Mathematics,Elementary Mathematics,"Elementary Mathematics, Mathematical Reasoning, College Mathematics, Mathematical Question Answering, High School Mathematics, Math",Text,English,Natural Language Processing,,,https://github.com/open-compass/MathBench,https://paperswithcode.com/dataset/mathbench,"MathBench is an All in One math dataset for language model evaluation, with:

A Sophisticated Five-Stage Difficulty Mechanism: Unlike the usual mathematical datasets that can only evaluate a single difficulty level or have a mix of unclear difficulty levels, MathBench provides 3709 questions with a gradient difficulty division by education stages, ranging from basic arithmetic to primary, middle, high school, and college levels, allowing you to get a clear overview of the comprehensive difficulty evaluation results.

Bilingual Gradient Evaluation: Apart from the basic calculation part which is language-independent, MathBench provides questions in both Chinese and English for the four-stage difficulty datasets from primary to college.

Implementation of the Robust Circular Evaluation (CE) Method: MathBench use CE as the main evaluation method for questions. Compared to the general Accuracy evaluation method, CE requires the model to answer the same multiple-choice question multiple times, with the order of the options changing each time. The model is considered correct on this question only if all answers are correct. The results of CE can reflect the model's capabilities more realistically, providing more valuable evaluation results.

Support for Basic Theory Questions: For every stage, MathBench provides questions that cover the basic theory knowledge points of the corresponding stage, to ascertain whether the model has genuinely mastered the fundamental concepts of each stage or merely memorized the answers.",,,,,,
1857,Mathematics_Dataset,Systematic Generalization,Systematic Generalization,"Systematic Generalization, Question Answering, Mathematical Question Answering",Text,English,Natural Language Processing,question-answering-on-mathematics-dataset,Apache-2.0,https://github.com/deepmind/mathematics_dataset,https://paperswithcode.com/dataset/mathematics,"This dataset code generates mathematical question and answer pairs, from a range of question types at roughly school-level difficulty. This is designed to test the mathematical learning and algebraic reasoning skills of learning models.",,,,,,
1858,MathMC,Math Word Problem Solving,Math Word Problem Solving,"Math Word Problem Solving, Arithmetic Reasoning, Mathematical Reasoning",,,Methodology,arithmetic-reasoning-on-mathmc,,,https://paperswithcode.com/dataset/mathmc,"Existing arithmetic benchmarks have a limited number of multiple-choice questions.  To address this gap, MathMC is created including 1,000 Chinese mathematical multiple-choice questions with detailed explanations and focusing on math problems typically encountered in grades 4 to 6.  It features a wide range of question types, including arithmetic, algebra, geometry, statistics, reasoning, and more, enhancing the diversity of current Chinese arithmetic datasets.",,,,,,
1859,MathQA,Data Augmentation,Data Augmentation,"Data Augmentation, Question Answering, Math Word Problem Solving",Text,English,Natural Language Processing,math-word-problem-solving-on-mathqa,Custom,https://math-qa.github.io/math-QA/,https://paperswithcode.com/dataset/mathqa,MathQA significantly enhances the AQuA dataset with fully-specified operational programs.,,,,,,
1860,MathToF,Math Word Problem Solving,Math Word Problem Solving,"Math Word Problem Solving, Arithmetic Reasoning, Mathematical Reasoning",,,Methodology,arithmetic-reasoning-on-mathtof,,,https://paperswithcode.com/dataset/mathtof,"Existing arithmetic benchmarks have a limited number of True-or-False questions. To address this gap, MathToF is created including 1,000 Chinese mathematical True-or-False questions with detailed explanations and focusing on math problems typically encountered in grades 4 to 6. It features a wide range of question types, including arithmetic, algebra, geometry, statistics, reasoning, and more, enhancing the diversity of current Chinese arithmetic datasets.",,,,,,
1861,MathVista,Mathematical Reasoning,Mathematical Reasoning,"Mathematical Reasoning, Question Answering, Multiple-choice, Visual Reasoning, Math Word Problem Solving, Visual Question Answering, Visual Question Answering (VQA)","Image, Text",English,Reasoning,,CC BY-SA 4.0,https://mathvista.github.io/,https://paperswithcode.com/dataset/mathvista,"MathVista is a consolidated Mathematical reasoning benchmark within Visual contexts. It consists of three newly created datasets, IQTest, FunctionQA, and PaperQA, which address the missing visual domains and are tailored to evaluate logical reasoning on puzzle test figures, algebraic reasoning over functional plots, and scientific reasoning with academic paper figures, respectively. It also incorporates 9 MathQA datasets and 19 VQA datasets from the literature, which significantly enrich the diversity and complexity of visual perception and mathematical reasoning challenges within our benchmark. In total, MathVista includes 6,141 examples collected from 31 different datasets.


Project: https://mathvista.github.io/
Visualization: https://mathvista.github.io/#visualization
Leaderboard: https://mathvista.github.io/#leaderboard
Paper: https://arxiv.org/abs/2310.02255
Data: https://huggingface.co/datasets/AI4Math/MathVista
Code: https://github.com/lupantech/MathVista",,https://arxiv.org/abs/2310.02255,https://arxiv.org/abs/2310.02255,141 examples,"Test, FunctionQA, and PaperQA, which address the missing visual domains and are tailored to evaluate logical reasoning on puzzle test figures, algebraic reasoning over functional plots, and scientific reasoning with academic paper figures, respectively. It also incorporates 9 MathQA datasets and 19 VQA datasets from the literature, which significantly enrich the diversity and complexity of visual perception and mathematical reasoning challenges within our benchmark. In total, MathVista includes 6,141 examples",
1862,MATHWELL_Human_Annotation_Dataset,text annotation,text annotation,"text annotation, Text Classification","Image, Text",English,Computer Vision,,GPL-3.0,https://huggingface.co/datasets/bryanchrist/annotations,https://paperswithcode.com/dataset/mathwell-human-annotation-dataset,"The MATHWELL Human Annotation Dataset contains 5,084 synthetic word problems and answers generated by MATHWELL, a reference-free educational grade school math word problem generator released in MATHWELL: Generating Educational Math Word Problems Using Teacher Annotations, and comparison models (GPT-4, GPT-3.5, Llama-2, MAmmoTH, and LLEMMA) with expert human annotations for solvability, accuracy, appropriateness, and meets all criteria (MaC). Solvability means the problem is mathematically possible to solve, accuracy means the Program of Thought (PoT) solution arrives at the correct answer, appropriateness means that the mathematical topic is familiar to a grade school student and the question's context is appropriate for a young learner, and MaC denotes questions which are labeled as solvable, accurate, and appropriate. Null values for accuracy and appropriateness indicate a question labeled as unsolvable, which means it cannot have an accurate solution and is automatically inappropriate. Based on our annotations, 82.2% of the question/answer pairs are solvable, 87.3% have accurate solutions, 78.1% are appropriate, and 58.4% meet all criteria.

This dataset is designed to train text classifiers to automatically label word problem generator outputs for solvability, accuracy, and appropriateness. More details about the dataset can be found in our paper.",,,,,,
1863,MATINF,Question Answering,Question Answering,"Question Answering, Multi-Document Summarization, Document Summarization",Text,English,Natural Language Processing,,,https://github.com/WHUIR/MATINF,https://paperswithcode.com/dataset/matinf,"Maternal and Infant (MATINF) Dataset is a large-scale dataset jointly labeled for classification, question answering and summarization in the domain of maternity and baby caring in Chinese. An entry in the dataset includes four fields: question (Q), description (D), class (C) and answer (A).

Nearly two million question-answer pairs are collected with fine-grained human-labeled classes from a large Chinese maternity and baby caring QA site. Authors conduct both automatic and manual data cleansing and remove: (1) classes with insufficient samples; (2) entries in which the length of the description filed is less than the length of the question field; (3) data with any field longer than 256 characters; (4) human-spotted ill-formed data. After the data cleansing, MATINF is constructed with the remaining 1.07 million entries",,"MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization",https://arxiv.org/pdf/2004.12302v2.pdf,,,
1864,MATRES,Relation Classification,Relation Classification,"Relation Classification, Temporal Relation Classification","Graph, Image, Time Series, Video",,Computer Vision,"temporal-relation-classification-on-matres, relation-classification-on-matres",,https://github.com/qiangning/MATRES,https://paperswithcode.com/dataset/matres,"This is the Multi-Axis Temporal RElations for Start-points (i.e., MATRES) dataset",,,,,,
1865,MatriVasha_,Document Text Classification,Document Text Classification,"Document Text Classification, Handwriting Verification, Handwriting generation, Optical Character Recognition (OCR), Handwritten Digit Recognition, Handwriting Recognition, Optical Charater Recogntion, Handwritten Digit Image Synthesis, Handwritten Text Recognition","Image, Text",English,Computer Vision,,Custom,https://shahariarrabby.github.io/matrivasha,https://paperswithcode.com/dataset/matrivasha,"MatriVasha the largest dataset of handwritten Bangla compound characters for research on handwritten Bangla compound character recognition. The proposed dataset contains 120 different types of compound characters that consist of 306,464‬ images written where 152,950 male and 153,514 female handwritten Bangla compound characters. This dataset can be used for other issues such as gender, age, district base handwriting research because the sample was collected that included district authenticity, age group, and an equal number of men and women.",,,,,,
1866,MatrixCity,Neural Rendering,Neural Rendering,Neural Rendering,,,Methodology,,CC BY-NC 4.0,https://city-super.github.io/matrixcity/,https://paperswithcode.com/dataset/matrixcity,"We build a large-scale, comprehensive, and high-quality synthetic dataset for city-scale neural rendering researches. Leveraging the Unreal Engine 5 City Sample project, we developed a pipeline to easily collect aerial and street city views with ground-truth camera poses, as well as a series of additional data modalities. Flexible control on environmental factors like light, weather, human and car crowd is also available in our pipeline, supporting the need of various tasks covering city-scale neural rendering and beyond. The resulting pilot dataset, MatrixCity, contains 67k aerial images and 452k street images from two city maps of total size 28km^2.",,,,,,
1867,MatSim,Contrastive Learning,Contrastive Learning,"Contrastive Learning, Material Recognition, One-Shot Learning",Image,,Computer Vision,,MIT,https://github.com/sagieppel/MatSim-Dataset,https://paperswithcode.com/dataset/matsim,"MatSim is a synthetic dataset, and natural image benchmark for computer vision-based recognition of similarities and transitions between materials and textures, focusing on identifying any material under any conditions using one or a few examples (one-shot learning), including materials states and subclasses.",,One-shot recognition of any material anywhere using contrastive learning with physics-based rendering,https://arxiv.org/pdf/2212.00648v1.pdf,,,
1868,Matterport3D,Depth Completion,Depth Completion,"Depth Completion, Depth Estimation, Semantic Segmentation, Monocular Depth Estimation, Depth Prediction","3D, Image, Time Series",,Computer Vision,"semantic-segmentation-on-matterport3d, depth-estimation-on-matterport3d, depth-prediction-on-matterport3d, monocular-depth-estimation-on-matterport3d, depth-completion-on-matterport3d",Custom (non-commercial),https://niessner.github.io/Matterport/,https://paperswithcode.com/dataset/matterport3d,"The Matterport3D dataset is a large RGB-D dataset for scene understanding in indoor environments. It contains 10,800 panoramic views inside 90 real building-scale scenes, constructed from 194,400 RGB-D images. Each scene is a residential building consisting of multiple rooms and floor levels, and is annotated with surface construction, camera poses, and semantic segmentation.",,Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention,https://arxiv.org/abs/1812.04155,,,
1869,MAVE,Attribute Mining,Attribute Mining,"Attribute Mining, Attribute Value Extraction",,,Methodology,"attribute-value-extraction-on-mave, attribute-mining-on-mave",Attribution-NonCommercial 4.0 International,https://github.com/google-research-datasets/MAVE,https://paperswithcode.com/dataset/mave,"The dataset contains 3 million attribute-value annotations across 1257 unique categories created from 2.2 million cleaned Amazon product profiles.
It is a large, multi-sourced, diverse dataset for product attribute extraction study.",,,,,,
1870,MAVE_-_Attribute__Black_Tea_Variety,Attribute Value Extraction,Attribute Value Extraction,Attribute Value Extraction,,,Methodology,,Attribution-NonCommercial 4.0 International,https://github.com/google-research-datasets/MAVE,https://paperswithcode.com/dataset/mave-attribute-black-tea-variety,"The dataset contains 3 million attribute-value annotations across 1257 unique categories created from 2.2 million cleaned Amazon product profiles. It is a large, multi-sourced, diverse dataset for product attribute extraction study.",,,,,,
1871,MAVS,Speaker Recognition,Speaker Recognition,Speaker Recognition,"Audio, Image",,Computer Vision,,,https://docs.google.com/forms/d/e/1FAIpQLSfTMqnQj8KNoUi1Ms1tx8Ewgil2l4wAAJVaKUJs6VkWfjAo4w/viewform,https://paperswithcode.com/dataset/mavs,MAVS is an audio-visual smartphone dataset captured in five different recent smartphones. This new dataset contains 103 subjects captured in three different sessions considering the different real-world scenarios. Three different languages are acquired in this dataset to include the problem of language dependency of the speaker recognition systems.,,,,,,
1872,MAWPS,Math Word Problem SolvingΩ,Math Word Problem SolvingΩ,"Math Word Problem SolvingΩ, Math Word Problem Solving",,,Methodology,"math-word-problem-solvingo-on-mawps, math-word-problem-solving-on-mawps",,https://github.com/sroy9/mawps,https://paperswithcode.com/dataset/mawps,"MAWPS is an online repository of Math Word Problems, to provide a unified testbed to evaluate different algorithms.
MAWPS allows for the automatic construction of datasets with particular characteristics, providing tools for tuning the lexical and template overlap of a dataset as well as for filtering ungrammatical problems from web-sourced corpora. The online nature of this repository facilitates easy community contribution. 
Amassed 3,320 problems, including the full datasets used in several previous works.",,,,,,
1873,MAX-60K,Chemical Process,Chemical Process,"Chemical Process, Fill Mask, Self-Supervised Learning, regression",,,Methodology,,This work is licensed under a CC BY 4.0 license.,https://huggingface.co/datasets/paoyw/max-dataset,https://paperswithcode.com/dataset/max-60k,"The dataset for masked autoencoder for X-ray fluorescence (XRF) is a following development after the dataset (Chao et al., 2022). 
Besides the published XRF spectra-target measurements (CaCO3 and TOC) pairs of data, we further upload the XRF spectra in that project but without alignments of the target measurements here.
As the first XRF large dataset compiled in a ML friendly format, we expect to kickoff more ML studies in the field of XRF and geology, especially DL studies. 

The investigated cores, which form the datast, are mostly retrieved across the high- to mid-latitude Northwest Pacific (37°N-52°N) and the Pacific sector of the Southern Ocean (53°S-63°S), with a water depth coverage from 1211 to 4853 m: <br>
1. Cruise SO264 in the subarctic Northwest Pacific with R/V SONNE in 2018<br>
2. Cruise PS97 in the central Drake Passage with RV Polarstern in 2016<br>
3. Cruise PS75 in the Pacific sector of the Southern Ocean in 2009/2010.<br>
4. Cruise KOMEX I and KOMEX II with R/V Akademik Lavrentyev in 1998 and cruise SO178 in 2004 in the Okhotsk Sea.",2022,,,,,
1874,mBBC_dataset,Multilingual NLP,Multilingual NLP,Multilingual NLP,Text,English,Natural Language Processing,,,https://github.com/PortNLP/mBBC,https://paperswithcode.com/dataset/mbbc,"To construct our multilingual dataset - mBBC - we gathered news articles from various BBC news websites in 43 different languages. This selection was based on the fact that BBC broadcasts news in these 43 languages, providing a global coverage across continents, and spanning a diverse range of language families, scripts, resource-levels, and word order ensuring a comprehensive representation of linguistic diversity.
We collected data from various language families such as Indo-European, Sino-Tibetan, Niger-Congo, Austronesian, Dravidian, and more, encompassing several scripts like Latin, Cyrillic, Arabic, Devanagari, Chinese characters, and others. This extensive representation facilitates a comprehensive evaluation of multilingual language models across different linguistic contexts. Moreover, the dataset includes both high-resource languages like English, Spanish, and French, benefiting from extensive linguistic resources, as well as low-resource languages such as Somali, Burmese, and Nepali, with limited resources or smaller speaker populations. Including languages with varying resource levels enables us to assess the adaptability and effectiveness of multilingual language models across diverse linguistic settings. To ensure an unbiased and robust analysis, our dataset consists of news articles of minimum text length of 500 characters, sourced from reputable sources in 2023, ensuring the models studied have not seen the data during training in the most new LLMs.",2023,,,,,
1875,MBPP,Code Generation,Code Generation,Code Generation,Text,English,Natural Language Processing,code-generation-on-mbpp,,https://github.com/google-research/google-research/tree/master/mbpp,https://paperswithcode.com/dataset/mbpp,"The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry-level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases.",,,,,,
1876,McMaster,Color Image Denoising,Color Image Denoising,"Color Image Denoising, Joint Demosaicing and Denoising",Image,,Computer Vision,"joint-demosaicing-and-denoising-on-mcmaster, color-image-denoising-on-mcmaster-sigma35, color-image-denoising-on-mcmaster-sigma75, color-image-denoising-on-mcmaster-sigma50, color-image-denoising-on-mcmaster-sigma15, color-image-denoising-on-mcmaster-sigma25",,https://www4.comp.polyu.edu.hk/~cslzhang/CDM_Dataset.htm,https://paperswithcode.com/dataset/mcmaster,"The McMaster dataset is a dataset for color demosaicing, which contains 18 cropped images of size 500×500.",,FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising,https://arxiv.org/abs/1710.04026,,,
1877,MCSI,Classification,Classification,"Classification, Skin Lesion Classification",Image,,Computer Vision,,,https://dx.doi.org/10.5281/zenodo.7948350,https://paperswithcode.com/dataset/mcsi,"The Mpox Close Skin Images dataset (MCSI) is a collection of skin images obtained from diverse public sources, that we accurately pre-processed (i.e., cropped and zoomed) in order to focus the skin lesion (if present), and to evaluate Machine Learning models aimed at detecting different pathologies from skin lesion pictures taken with smartphone cameras. It includes a total of 400 pictures homogeneously divided in 4 different classes: mpox, which contains samples of mpox (formerly Monkeypox) skin lesions; chickenpox, with samples of chickenpox cases; acne, containing samples of acne at different severity levels; and healthy, which contains samples of skin without any evident symptoms. This repository is part of the supplementary material accompanying the paper named: A Transfer Learning and Explainable Solution to Detect mpox from Smartphones images.",,,,,"valuate Machine Learning models aimed at detecting different pathologies from skin lesion pictures taken with smartphone cameras. It includes a total of 400 pictures homogeneously divided in 4 different classes: mpox, which contains samples",
1878,MCTest,Machine Reading Comprehension,Machine Reading Comprehension,"Machine Reading Comprehension, Question Answering, Reading Comprehension",Text,English,Natural Language Processing,"question-answering-on-mctest-160, question-answering-on-mctest-500",Custom,https://mattr1.github.io/mctest/,https://paperswithcode.com/dataset/mctest,"MCTest is a freely available set of stories and associated questions intended for research on the machine comprehension of text. 

MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension.",,MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text,https://www.aclweb.org/anthology/D13-1020.pdf,,,
1879,MCubeS,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Image Segmentation, Material Recognition",Image,,Computer Vision,semantic-segmentation-on-mcubes,MIT,https://github.com/kyotovision-public/multimodal-material-segmentation,https://paperswithcode.com/dataset/mcubes,"Multimodal material segmentation (MCubeS) dataset contains 500 sets of images from 42 street scenes. Each scene has images for four modalities: RGB, angle of linear polarization (AoLP), degree of linear polarization (DoLP), and near-infrared (NIR). The dataset provides annotated ground truth labels for both material and semantic segmentation for every pixel. The dataset is divided training set with 302 image sets, validation set with 96 image sets, and test set with 102 image sets. Each image has  1224 x 1024 pixels and a total of 20 class labels per pixel.",,,,,,
1880,MC_GRID,Speech Extraction,Speech Extraction,"Speech Extraction, Speaker Separation, Speech Separation",Audio,,Speech,,,https://academictorrents.com/details/3cd18ff2d3eec881207dcc5ca5a2c3a2a3afe462,https://paperswithcode.com/dataset/mc-grid-test,"Here we release the dataset (Multi_Channel_Grid, abbreviated as MC_Grid) used in our paper LIMUSE: LIGHTWEIGHT MULTI-MODAL SPEAKER EXTRACTION.

MC_Grid, which is based on GRID dataset, includes multi-channel audio, extracted voiceprint and visual feature. The method of feature extraction will be introduced below.

MC_Grid is specially prepared for speaker extraction task, and our code is available at aispeech-lab/LiMuSE. Feel free to contact us if you have any questions or suggestions.",,LIMUSE: LIGHTWEIGHT MULTI-MODAL SPEAKER EXTRACTION,[[2111.04063] LiMuSE: Lightweight Multi-modal Speaker Extraction (arxiv.org)](https://arxiv.org/abs/2111.04063),,,
1881,MD17,Formation Energy,Formation Energy,Formation Energy,,,Methodology,,,http://quantum-machine.org/gdml/#datasets,https://paperswithcode.com/dataset/md17,Energies and forces for molecular dynamics trajectories of eight organic molecules. Level of theory DFT: PBE+vdW-TS.,,,,,,
1882,MDBD,Edge Detection,Edge Detection,"Edge Detection, Boundary Detection",Image,,Computer Vision,edge-detection-on-mdbd,,https://serre-lab.clps.brown.edu/resource/multicue/,https://paperswithcode.com/dataset/mdbd,"In order to study the interaction of several early visual cues (luminance, color, stereo, motion) during boundary detection in challenging natural scenes, we have built a multi-cue video dataset composed of short binocular video sequences of natural scenes using a consumer-grade Fujifilm stereo camera (Mély, Kim, McGill, Guo and Serre, 2016). We considered a variety of places (from university campuses to street scenes and parks) and seasons to minimize possible biases. We attempted to capture more challenging scenes for boundary detection by framing a few dominant objects in each shot under a variety of appearances. Representative sample keyframes are shown on the figure below. The dataset contains 100 scenes, each consisting of a left and right view short (10-frame) color sequence. Each sequence was sampled at a rate of 30 frames per second. Each frame has a resolution of 1280 by 720 pixels.",2016,,,,,
1883,mDRT,Speech Denoising,Speech Denoising,"Speech Denoising, Audio Signal Processing, Speech Separation, Audio Synthesis, Speech Enhancement",Audio,,Speech,,CC BY-SA 4.0,https://github.com/cisco/multilingual-speech-testing,https://paperswithcode.com/dataset/mdrt,"We present a multilingual test set for conducting speech intelligibility tests in the form of diagnostic rhyme tests. The materials currently contain audio recordings in 5 languages and further extensions are in progress. For Mandarin Chinese, we provide recordings for a consonant contrast test as well as a tonal contrast test. 
Further information on the audio data, test procedure and software to set up a full survey which can be deployed on crowdsourcing platforms is provided in our paper [arXiv preprint] and GitHub repository.
We welcome contributions to this open-source project.",,[arXiv preprint],http://arxiv.org/abs/2403.14817,,,
1884,mEBAL,Electroencephalogram (EEG),Electroencephalogram (EEG),"Electroencephalogram (EEG), Face Anti-Spoofing, Face Recognition",Image,,Computer Vision,,,https://github.com/BiDAlab/mEBAL,https://paperswithcode.com/dataset/mebal,A multimodal database for eye blink detection and attention level estimation.,,,,,,
1885,MED,Data Augmentation,Data Augmentation,"Data Augmentation, Natural Language Inference, Automated Theorem Proving",Text,English,Natural Language Processing,natural-language-inference-on-med,,https://github.com/verypluming/MED,https://paperswithcode.com/dataset/med,"MED is a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications. The dataset was constructed by collecting naturally-occurring examples by crowdsourcing and well-designed ones from linguistics publications.
It consists of 5,382 examples.",,https://www.aclweb.org/anthology/W19-4804v2.pdf,https://www.aclweb.org/anthology/W19-4804v2.pdf,382 examples,,
1886,MeDAL,Natural Language Understanding,Natural Language Understanding,"Natural Language Understanding, Mortality Prediction","Text, Time Series",English,Natural Language Processing,,,https://github.com/McGill-NLP/medal,https://paperswithcode.com/dataset/medal,"The  Medical Dataset for Abbreviation Disambiguation for Natural Language Understanding (MeDAL) is a large medical text dataset curated for abbreviation disambiguation, designed for natural language understanding pre-training in the medical domain. It was published at the ClinicalNLP workshop at EMNLP.",,,,,,
1887,MedConceptsQA,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Large Language Model, Medical Procedure, Medical Diagnosis, Few-Shot Learning, Question Answering",Text,English,Natural Language Processing,"few-shot-learning-on-medconceptsqa, zero-shot-learning-on-medconceptsqa",,https://huggingface.co/datasets/ofir408/MedConceptsQA,https://paperswithcode.com/dataset/medconceptsqa,"MedConceptsQA - Open Source Medical Concepts QA Benchmark

The benchmark can be found here: 
https://huggingface.co/datasets/ofir408/MedConceptsQA",,,,,,
1888,MEDIA,Spoken Language Understanding,Spoken Language Understanding,"Spoken Language Understanding, Slot Filling, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,,,http://www.lrec-conf.org/proceedings/lrec2004/pdf/356.pdf,https://paperswithcode.com/dataset/media,"The MEDIA French corpus is dedicated to semantic extraction from speech in a context of human/machine dialogues. The corpus has manual transcription and conceptual annotation of dialogues from 250 speakers. It is split into the following three parts : (1) the training set (720 dialogues, 12K sentences), (2) the development set (79 dialogues, 1.3K sentences, and (3) the test set (200 dialogues, 3K sentences).",,Dialogue history integration into end-to-end signal-to-concept spoken language understanding systems,https://arxiv.org/abs/2002.06012,12K sentences,,
1889,Mediapi-RGB,Sign Language Production,Sign Language Production,"Sign Language Production, Sign Language Translation, Gloss-free Sign Language Translation",Text,English,Natural Language Processing,sign-language-translation-on-mediapi-rgb,Research Only,https://www.ortolang.fr/market/corpora/mediapi-rgb?lang=en,https://paperswithcode.com/dataset/mediapi-rgb,"Mediapi-RGB is a bilingual corpus of French Sign Language (LSF) and written French in the form of subtitled videos, accompanied by complementary data (various representations, segmentation, vocabulary, etc.). It can be used in academic research for a wide range of tasks, such as training or evaluating sign language (SL)  extraction, recognition or translation models.

To build this corpus, we used videos from   Média'Pi!, a bilingual online media with journalistic-type content in LSF with French subtitles. We collected 1230 videos dating from September 2017 to January 2022, representing a total of 86h. Based on the subtitles, we temporally segmented the videos into 50084 video segments (or extracts). We also automatically cropped the signer and harmonised the segments in terms of size (444x444) and frequency (25fps).",2017,,,,,
1890,Medical_Segmentation_Decathlon,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Medical Image Segmentation, Image Registration",Image,,Computer Vision,medical-image-segmentation-on-medical,CC-BY-SA 4.0,http://medicaldecathlon.com/,https://paperswithcode.com/dataset/medical-segmentation-decathlon,"The Medical Segmentation Decathlon is a collection of medical image segmentation datasets. It contains a total of 2,633 three-dimensional images collected across multiple anatomies of interest, multiple modalities and multiple sources. Specifically, it contains data for the following body organs or parts: Brain, Heart, Liver, Hippocampus, Prostate, Lung, Pancreas, Hepatic Vessel, Spleen and Colon.",,A large annotated medical image dataset for the development and evaluation of segmentation algorithms,https://arxiv.org/pdf/1902.09063.pdf,,,
1891,MedICaT,Text Matching,Text Matching,Text Matching,Text,English,Natural Language Processing,,,https://github.com/allenai/medicat,https://paperswithcode.com/dataset/medicat,"MedICaT is a dataset of medical images, captions, subfigure-subcaption annotations, and inline textual references.
Figures and captions are extracted from open access articles in PubMed Central and corresponding reference text is derived from S2ORC.
The dataset consists of:
217,060 figures from 131,410 open access papers
7507 subcaption and subfigure annotations for 2069 compound figures
Inline references for ~25K figures in the ROCO dataset",2069,,,,,
1892,MedleyDB,Music Information Retrieval,Music Information Retrieval,"Music Information Retrieval, Information Retrieval, Music Source Separation",Audio,,Audio,,CC BY-NC-SA 4.0,https://medleydb.weebly.com/,https://paperswithcode.com/dataset/medleydb,"MedleyDB, is a dataset of annotated, royalty-free multitrack recordings. It was curated primarily to support research on melody extraction. For each song melody f₀ annotations are provided as well as instrument activations for evaluating automatic instrument recognition. The original dataset consists of 122 multitrack songs out of which 108 include melody annotations.

The songs in MedleyDB were obtained from the following sources:


Independent Artists (30 songs)
NYU's Dolan Recording Studio (32 songs)
Weathervane Music (25 songs)
Music Delta (35 songs)

MedleyDB contains songs of a variety of musical genres: Singer/Songwriter, Classical, Rock, World/Folk, Fusion, Jazz, Pop, Musical Theatre, Rap. For each song three types of audio content are given: a mix, stems, and raw audio. All types of audio files are .wav files with a sample rate of 44.1 kHz and a bit depth of 16.",,MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research,http://www.terasoft.com.tw/conf/ismir2014/proceedings/T028_322_Paper.pdf,,,
1893,MedleyDB_2.0,Audio Super-Resolution,Audio Super-Resolution,"Audio Super-Resolution, Data Augmentation, Information Retrieval",Audio,,Audio,,,https://medleydb.weebly.com/,https://paperswithcode.com/dataset/medleydb-2-0,"MedleyDB 2.0 is a superset of the MedleyDB – a dataset of annotated, royalty-free multitrack recordings. The second iteration of the dataset includes 74 new multitrack recordings resulting in 194 songs in total.",,MedleyDB 2.0: New Data and a System for Sustainable Data Collection,https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/08/bittner-medleydb.pdf,,,
1894,MedMentions,Entity Linking,Entity Linking,"Entity Linking, Entity Extraction using GAN, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,entity-linking-on-medmentions,,https://github.com/chanzuckerberg/MedMentions,https://paperswithcode.com/dataset/medmentions,"MedMentions is a new manually annotated resource for the recognition of biomedical concepts. What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines.",2017,,,,,
1895,MedMNIST,AutoML,AutoML,AutoML,,,Methodology,,,https://medmnist.github.io/,https://paperswithcode.com/dataset/medmnist,"A collection of 10 pre-processed medical open datasets. MedMNIST is standardized to perform classification tasks on lightweight 28x28 images, which requires no background knowledge.",,,,28 images,,
1896,MedPromptX-VQA,X-ray Visual Question Answering,X-ray Visual Question Answering,"X-ray Visual Question Answering, Visual Question Answering (VQA), Medical Visual Question Answering","Image, Text",English,Computer Vision,,,https://github.com/BioMedIA-MBZUAI/MedPromptX,https://paperswithcode.com/dataset/medpromptx-vqa,A new in-context visual question answering dataset encompassing interleaved image and EHR data derived from MIMIC-IV and MIMIC-CXR-JPG databases.,,,,,,
1897,MedTrinity-25M,multimodal generation,multimodal generation,"multimodal generation, Medical Report Generation, Medical Visual Question Answering","Image, Text",English,Multimodal,,Custom,https://yunfeixie233.github.io/MedTrinity-25M/,https://paperswithcode.com/dataset/medtrinity-25m,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
1898,MeerKAT__Meerkat_Kalahari_Audio_Transcripts,Audio Tagging,Audio Tagging,"Audio Tagging, Audio Classification, Classification, Few-Shot Learning","Audio, Image",,Audio,audio-classification-on-meerkat-meerkat,CC BY-NC,https://doi.org/10.17617/3.0J0DYB,https://paperswithcode.com/dataset/meerkat-meerkat-kalahari-audio-transcripts,"A large-scale reference dataset for bioacoustics.
MeerKAT is a 1068h large-scale dataset containing data from audio-recording collars worn by free-ranging meerkats (Suricata suricatta) at the Kalahari Research Centre, South Africa, of which 184h are labeled with twelve time-resolved vocalization-type ground truth target classes, each with millisecond resolution. The labeled 184h MeerKAT subset exhibits realistic sparsity conditions for a bioacoustic dataset (96% background-noise or other signals and 4% vocalizations), dispersed across 66398 10-second samples, spanning 251562 labeled events and showcasing significant spectral and temporal variability, making it the first large-scale reference point with real-world conditions for benchmarking pretraining and finetune approaches in bioacoustics deep learning.",,,,,,
1899,MeetingBank,Text Summarization,Text Summarization,"Text Summarization, Multimodal Reasoning, Meeting Summarization",Text,English,Natural Language Processing,text-summarization-on-meetingbank,Creative Commons Attribution 4.0 International,https://meetingbank.github.io,https://paperswithcode.com/dataset/meetingbank,"MeetingBank, a benchmark dataset created from the city councils of 6 major U.S. cities to supplement existing datasets. 

It contains 1,366 meetings with over 3,579 hours of video, as well as transcripts, PDF documents of meeting minutes, agenda, and other metadata. On average, a council meeting is 2.6 hours long and its transcript contains over 28k tokens, making it a valuable testbed for meeting summarizers and for extracting structure from meeting videos. The datasets contains 6,892 segment-level summarization instances for training and evaluating of performance.",,,,,,
1900,MegaDepth,3D Reconstruction,3D Reconstruction,"3D Reconstruction, Optical Flow Estimation, Depth Estimation","3D, Video",,Methodology,,,http://www.cs.cornell.edu/projects/megadepth/,https://paperswithcode.com/dataset/megadepth,The MegaDepth dataset is a dataset for single-view depth prediction that includes 196 different locations reconstructed from COLMAP SfM/MVS.,,,,,,
1901,MEIR,Entity Linking,Entity Linking,"Entity Linking, Misinformation",,,Methodology,,,https://github.com/Ekraam/MEIR,https://paperswithcode.com/dataset/meir,"MEIR is a substantially challenging dataset over that which has been previously available to support research into image repurposing detection. The new dataset includes location, person, and organization manipulations on real-world data sourced from Flickr.",,Deep Multimodal Image-Repurposing Detection,https://arxiv.org/pdf/1808.06686.pdf,,,
1902,MEIS,Real-time Instance Segmentation,Real-time Instance Segmentation,"Real-time Instance Segmentation, Real-time instance measurement",Image,,Computer Vision,"real-time-instance-segmentation-on-meis, real-time-instance-measurement-on-meis",,https://drive.google.com/drive/folders/1Ve3UC9pP-FO5wN5MLB9OiKBAs7xAGkeN?usp=sharing,https://paperswithcode.com/dataset/meis,"MEIS comprises a total of 2,639 images in the size of 1024 × 768 toward two recording views (Aortic Valve (AV) and
Left Ventricle (LV)) with 1,521 (747 in AV + 774 in LV) images for training and 1,118 (559 in AV + 559 in LV) for
testing, respectively. Each view must be detected with two objects to calculate the measurement indicators. That is in
total with four object classes (two objects in each view): aortic root (AoR) and left atrium (LA) in AV; interventricular
septum (IVS) and left ventricular posterior wall (LVPW) in LV. The medical meaning and purpose of each indicator are
listed in the following:
• AV: LA-Dimension and AoR-Dimension can be measured for calculating different indicators, such as AoR/LA
ratio, to examine the state of the aortic valve.
• LV: 6 measurements include IVSs, IVSd, LVIDs, LVIDd, LVPWs, and LVPWd. These concerned thicknesses
and dimensions in LV recording are used to estimate other cardiac functions through specific medical formulas,
including LV mass, LV ejection fraction, end-diastolic volume, end-systolic volume, and more.",,,,639 images,,
1903,MELD,Facial Expression Recognition,Facial Expression Recognition,"Facial Expression Recognition, Multimodal Emotion Recognition, Emotion Recognition in Conversation, Speech Emotion Recognition","Audio, Image",,Computer Vision,"multimodal-emotion-recognition-on-meld, facial-expression-recognition-on-meld, emotion-recognition-in-conversation-on-meld",,https://affective-meld.github.io/,https://paperswithcode.com/dataset/meld,"Multimodal EmotionLines Dataset (MELD) has been created by enhancing and extending EmotionLines dataset. MELD contains the same dialogue instances available in EmotionLines, but it also encompasses audio and visual modality along with text. MELD has more than 1400 dialogues and 13000 utterances from Friends TV series. Multiple speakers participated in the dialogues. Each utterance in a dialogue has been labeled by any of these seven emotions -- Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. MELD also has sentiment (positive, negative and neutral) annotation for each utterance.",,,,,,
1904,MELON,Music Classification,Music Classification,"Music Classification, Image Retrieval","Audio, Image",,Computer Vision,,MIT,https://prachiisc.github.io/doc2audio.github.io/,https://paperswithcode.com/dataset/melon,"A unique dataset comprising multimodal creative and designed documents containing images with corresponding captions paired with music based on around 50mood/themes. 



Motivation: To enhance user experience and to increase accessibility to wider community, motivate research in cross-modal retrieval field.



Use case: Music Retrieval for designed documents, Music augmentation for multimodal designed documents, Design image retrieval for music based on mood/themes.",,,,,,
1905,MemeTracker,Combinatorial Optimization,Combinatorial Optimization,"Combinatorial Optimization, Point Processes, Recommendation Systems",,,Methodology,point-processes-on-memetracker,,https://snap.stanford.edu/data/memetracker9.html,https://paperswithcode.com/dataset/memetracker,"The Memetracker corpus contains articles from mainstream media and blogs from August 1 to October 31, 2008 with about 1 million documents per day. It has 10,967 hyperlink cascades among 600 media sites.",2008,Marked Temporal Dynamics Modeling based on Recurrent Neural Network,https://arxiv.org/abs/1701.03918,,,
1906,MemexQA,Memex Question Answering,Memex Question Answering,Memex Question Answering,Text,English,Natural Language Processing,memex-question-answering-on-memexqa,,https://precognition.team/memexqa/,https://paperswithcode.com/dataset/memexqa,"A large, realistic multimodal dataset consisting of real personal photos and crowd-sourced questions/answers.",,,,,,
1907,MentSum,Data Summarization,Data Summarization,"Data Summarization, Text Summarization",Text,English,Natural Language Processing,text-summarization-on-mentsum,,,https://paperswithcode.com/dataset/mentsum,"Mental health remains a significant challenge of public health worldwide. With increasing popularity of online platforms, many use the platforms to share their mental health conditions, express their feelings, and seek help from the community and counselors. While posts are of varying length, it is beneficial to provide a short, but informative summary for fast processing by the counselors. To facilitate research in summarization of mental health online posts, we introduce Mental Health Summarization dataset, MentSum, containing over 24k carefully selected user posts from Reddit, along with their short user-written summary (called TLDR) in English from 43 mental health subreddits.

MentSum is distributed through Data Usage Agreement (DUA). Please fill out the request form at https://ir.cs.georgetown.edu/resources/  to obtain this dataset.",,,,,,
1908,MerRec,Click-Through Rate Prediction,Click-Through Rate Prediction,"Click-Through Rate Prediction, Multi-Task Learning, Recommendation Systems",Time Series,,Methodology,,CC BY-NC 4.0 International,https://arxiv.org/abs/2402.14230,https://paperswithcode.com/dataset/merrec,"A large scale, C2C marketplace e-commerce dataset.",,Homepage,https://arxiv.org/abs/2402.14230,,,
1909,Meta-Dataset,Few-Shot Image Classification,Few-Shot Image Classification,"Few-Shot Image Classification, Image Classification, Meta-Learning, Few-Shot Learning",Image,,Computer Vision,"few-shot-image-classification-on-meta-dataset-1, few-shot-image-classification-on-meta-dataset",Multiple licenses,https://github.com/google-research/meta-dataset,https://paperswithcode.com/dataset/meta-dataset,"The Meta-Dataset benchmark is a large few-shot learning benchmark and consists of multiple datasets of different data distributions. It does not restrict few-shot tasks to have fixed ways and shots, thus representing a more realistic scenario. It consists of 10 datasets from diverse domains: 


ILSVRC-2012 (the ImageNet dataset, consisting of natural images with 1000 categories)
Omniglot (hand-written characters, 1623 classes)
Aircraft (dataset of aircraft images, 100 classes)
CUB-200-2011 (dataset of Birds, 200 classes)
Describable Textures (different kinds of texture images with 43 categories)
Quick Draw (black and white sketches of 345 different categories)
Fungi (a large dataset of mushrooms with 1500 categories)
VGG Flower (dataset of flower images with 102 categories), 
Traffic Signs (German traffic sign images with 43 classes)
MSCOCO (images collected from Flickr, 80 classes). 

All datasets except Traffic signs and MSCOCO have a training, validation and test split (proportioned roughly into 70%, 15%, 15%). The datasets Traffic Signs and MSCOCO are reserved for testing only.",2012,Optimized Generic Feature Learning for Few-shot Classification across Domains,https://arxiv.org/abs/2001.07926,,,1000
1910,Meta-World_Benchmark,Multi-Task Learning,Multi-Task Learning,"Multi-Task Learning, Meta Reinforcement Learning, Meta-Learning",,,Methodology,,,http://meta-world.github.io,https://paperswithcode.com/dataset/meta-world-benchmark,An open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks.,,,,,,
1911,Metaphorical_Connections,Chatbot,Chatbot,Chatbot,,,Methodology,,,https://github.com/kgero/metaphorical-connections,https://paperswithcode.com/dataset/metaphorical-connections,The Metaphorical Connections dataset is a poetry dataset that contains annotations between metaphorical prompts and short poems. Each poem is annotated whether or not it successfully communicates the idea of the metaphorical prompt.,,,,,,
1912,MetaQA,Language Modelling,Language Modelling,"Language Modelling, Question Answering, Knowledge Graphs",Text,English,Natural Language Processing,question-answering-on-metaqa,Custom,https://github.com/yuyuz/MetaQA,https://paperswithcode.com/dataset/metaqa,"The MetaQA dataset consists of a movie ontology derived from the WikiMovies Dataset and three sets of question-answer pairs written in natural language: 1-hop, 2-hop, and 3-hop queries.",,https://arxiv.org/abs/1907.08176,https://arxiv.org/abs/1907.08176,,,
1913,METR-LA,Correlated Time Series Forecasting,Correlated Time Series Forecasting,"Correlated Time Series Forecasting, Time Series Forecasting, Multivariate Time Series Forecasting, Multivariate Time Series Imputation, Traffic Prediction",Time Series,,Time Series,"correlated-time-series-forecasting-on-metr-la, traffic-prediction-on-metr-la, multivariate-time-series-imputation-on-metr",,,https://paperswithcode.com/dataset/metr-la,METR-LA is a dataset for traffic prediction.,,,,,,
1914,METR-LA_Point_Missing,Multivariate Time Series Imputation,Multivariate Time Series Imputation,"Multivariate Time Series Imputation, Imputation, Traffic Data Imputation",Time Series,,Time Series,traffic-data-imputation-on-metr-la-point,,,https://paperswithcode.com/dataset/metr-la-point-missing,"The original dataset from Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting contains traffic readings collected from 207 loop detectors on highways in Los Angeles County, aggregated in 5 minutes intervals over four months between March 2012 and June 2012.

The Point missing setting, introduced in Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks, is a variant for imputation in which 25% of data are masked out uniformly at random. Results on this dataset are assumed to be obtained in-sample, meaning that the test interval is used also for training, excluding data used for evaluation.",2012,Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting,https://arxiv.org/abs/1707.01926,,,
1915,METU-VIREF_Dataset,Natural Language Visual Grounding,Natural Language Visual Grounding,"Natural Language Visual Grounding, Deep Attention","Image, Text",English,Computer Vision,,,https://github.com/hazananayurt/viref,https://paperswithcode.com/dataset/metu-viref-dataset,"METU-VIREF is a video referring expression dataset comprising of videos from VIRAT Ground and ILSVRC2015 VID datasets. VIRAT is a surveillance dataset and contains mainly people and vehicles. To line up with this and restrict the domain, only videos that contain vehicles from the ILSVRC dataset are used. The METU-VIREF dataset does not contain whole videos from these datasets (the videos need to be downloaded from the respective sources) but just referring expressions for video sequences containing an object pair. For this, object pairs are chosen which had a relation that a meaningful referring expression could be written for.",,,,,,
1916,MEVA,Action Detection,Action Detection,"Action Detection, Activity Recognition, Activity Detection","Image, Video",,Computer Vision,,,https://mevadata.org/,https://paperswithcode.com/dataset/meva,"Large-scale dataset for human activity recognition. Existing security datasets either focus on activity counts by aggregating public video disseminated due to its content, which typically excludes same-scene background video, or they achieve persistence by observing public areas and thus cannot control for activity content. The dataset is over 9300 hours of untrimmed, continuous video, scripted to include diverse, simultaneous activities, along with spontaneous background activity.",,,,,,
1917,Mewsli-9,Entity Linking,Entity Linking,"Entity Linking, Entity Disambiguation",,,Methodology,entity-disambiguation-on-mewsli-9,,https://github.com/google-research/google-research/tree/master/dense_representations_for_entity_retrieval/mel#get-mewsli-9-dataset,https://paperswithcode.com/dataset/mewsli-9,A large new multilingual dataset for multilingual entity linking.,,,,,,
1918,MFNet,Thermal Image Segmentation,Thermal Image Segmentation,Thermal Image Segmentation,Image,,Computer Vision,thermal-image-segmentation-on-mfn-dataset,,,https://paperswithcode.com/dataset/mfnet,"The first RGB-Thermal urban scene image dataset with pixel-level annotation. We published this new RGB-Thermal semantic segmentation dataset in support of further development of autonomous vehicles in the future. This dataset contains 1569 images (820 taken at daytime and 749 taken at nighttime). Eight classes of obstacles commonly encountered during driving
(car, person, bike, curve, car stop, guardrail, color cone, and bump) are labeled in this dataset.",,,,1569 images,,
1919,MFRC,Transfer Learning,Transfer Learning,"Transfer Learning, Sentiment Analysis",Text,English,Natural Language Processing,,CC-BY-4.0,https://huggingface.co/datasets/USC-MOLA-Lab/MFRC,https://paperswithcode.com/dataset/mfrc,"Moral Foundations Reddit Corpus (MFRC) is a collection of 16,123 Reddit comments that have been curated from 12 distinct subreddits, hand-annotated by at least three trained annotators for 8 categories of moral sentiment (i.e., Care, Proportionality, Equality, Purity, Authority, Loyalty, Thin Morality, Implicit/Explicit Morality) based on the updated Moral Foundations Theory (MFT) framework.",,https://arxiv.org/pdf/2208.05545v2.pdf,https://arxiv.org/pdf/2208.05545v2.pdf,,,8
1920,MGSM,Multi-task Language Understanding,Multi-task Language Understanding,"Multi-task Language Understanding, Arithmetic Reasoning, Math Word Problem Solving",Text,English,Natural Language Processing,multi-task-language-understanding-on-mgsm,Creative Commons Attribution 4.0 International,https://github.com/google-research/url-nlp/tree/main/mgsm,https://paperswithcode.com/dataset/mgsm,Multilingual Grade School Math Benchmark (MGSM) is a benchmark of grade-school math problems. The same 250 problems from GSM8K are each translated via human annotators in 10 languages. GSM8K (Grade School Math 8K) is a dataset of 8.5K high-quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.,,,,,,
1921,MGTAB,Twitter Bot Detection,Twitter Bot Detection,"Twitter Bot Detection, Stance Detection",Image,,Computer Vision,"stance-detection-on-mgtab, twitter-bot-detection-on-mgtab",,https://github.com/GraphDetec/MGTAB,https://paperswithcode.com/dataset/mgtab,"MGTAB is the first standardized graph-based benchmark for stance and bot detection. MGTAB contains 10,199 expert-annotated users and 7 types of relationships, ensuring high-quality annotation and diversified relations. For more details, please refer to the MGTAB paper.",,,,,,
1922,MH-FED,Facial Emotion Recognition,Facial Emotion Recognition,"Facial Emotion Recognition, Facial expression generation, Facial Expression Translation, Facial Expression Recognition (FER)","Image, Text",English,Computer Vision,,Free For Non-Profit Research Usage With Citation,https://huggingface.co/datasets/azad-wolf-se/MH-FED,https://paperswithcode.com/dataset/mh-fed,This dataset provides a  collection of 162K images and 70 Videos of Meta-Humans. There are  10  Highly realistic Meta-Humans expressing 7 facial expressions.,,,,162K images,,
1923,MIAP,Fairness,Fairness,"Fairness, Human Detection",Image,,Computer Vision,,,https://storage.googleapis.com/openimages/web/extended.html,https://paperswithcode.com/dataset/miap,"MIAP is a dataset created by obtaining a new set of annotations on a subset of the Open Images dataset, containing bounding boxes and attributes for all of the people visible in those images, as the original Open Images dataset annotations are not exhaustive, with bounding boxes and attribute labels for only a subset of the classes in each image.

The MIAP dataset focuses on enabling ML Fairness research. It provides additional annotations for 100,000 (70k from training and 30k from validation/test) images that contain at least one person bounding box in the original annotations.

These additional annotations provide exhaustive bounding boxes for all people in an image. Person boxes are further annotated with attribute labels for fairness research. Annotated attributes include the human perceived gender presentation (predominantly feminine, predominantly masculine, and unknown) and perceived age range (young, middle, older, and unknown) of the localized person. This procedure adds nearly 100,000 new boxes that were not annotated under the original labeling pipeline.

Annotations on the exhaustive set enable research into the fairness properties of models trained on partial annotations and the pipelines that produce these annotations.",,,,,training and 30k from validation/test) images,
1924,MIB_Dataset,Twitter Bot Detection,Twitter Bot Detection,Twitter Bot Detection,Image,,Computer Vision,twitter-bot-detection-on-mib-datasets,,http://mib.projects.iit.cnr.it/dataset.html,https://paperswithcode.com/dataset/mib-datasets,"You need to request access to download and use the dataset.

It contains fake and real accounts of Twitter and their follower's/friends' ids (can create a graph based on that).",,,,,,
1925,MICRO25,Text-to-Code Generation,Text-to-Code Generation,Text-to-Code Generation,Text,English,Natural Language Processing,,,https://github.com/cognitiveailab/words2wires,https://paperswithcode.com/dataset/micro25,"To assess a model’s ability to create microcontroller-driven electronic devices, we developed a benchmark, MICRO25, that includes 25 tasks intended for the common ARDUINO microcontroller ecosystem.. These tasks, shown in Table 2, span 5 core categories including: input, interface protocols, output, sensors, and logic. Each task is either tailored to test a specific fundamental competency required to build basic microcontroller-driven electronic devices, or the integration of several competencies into larger design flows.",,,,,,
1926,Microsoft_Malware_Classification_Challenge,Malware Classification,Malware Classification,Malware Classification,Image,,Computer Vision,malware-classification-on-microsoft-malware,,https://www.kaggle.com/c/malware-classification/data,https://paperswithcode.com/dataset/microsoft-malware-classification-challenge,"The Microsoft Malware Classification Challenge was announced in 2015 along with a publication of a huge dataset of nearly 0.5 terabytes, consisting of disassembly and bytecode of more than 20K malware samples. Apart from serving in the Kaggle competition, the dataset has become a standard benchmark for research on modeling malware behaviour. To date, the dataset has been cited in more than 50 research papers. Here we provide a high-level comparison of the publications citing the dataset. The comparison simplifies finding potential research directions in this field and future performance evaluation of the dataset.",2015,,,,,
1927,Mid-level_perceptual_musical_features,Music Classification,Music Classification,Music Classification,"Audio, Image",,Computer Vision,,,https://osf.io/5aupt/,https://paperswithcode.com/dataset/mid-level-perceptual-musical-features,"This dataset contains annotations for 5000 music files on the following music properties:


Melodiousness
Articulation
Rhythmic stability
Rhythmic complexity
Dissonance
Tonal stability
Modality

The annotations were given by musicians and collected through a crowd-sourcing platform (Toloka).",,,,,,
1928,Middlebury,Stereo Image Super-Resolution,Stereo Image Super-Resolution,"Stereo Image Super-Resolution, Image Super-Resolution, Video Frame Interpolation, Depth Estimation","3D, Image, Video",,Computer Vision,"image-super-resolution-on-middlebury-4x, stereo-image-super-resolution-on-middlebury, stereo-image-super-resolution-on-middlebury-1, video-frame-interpolation-on-middlebury, image-super-resolution-on-middlebury-2x",Custom,https://vision.middlebury.edu/stereo/data/,https://paperswithcode.com/dataset/middlebury,The Middlebury Stereo dataset consists of high-resolution stereo sequences with complex geometry and pixel-accurate ground-truth disparity data. The ground-truth disparities are acquired using a novel technique that employs structured lighting and does not require the calibration of the light projectors.,,,,,,
1929,MIDGARD,Robot Navigation,Robot Navigation,"Robot Navigation, Visual Navigation, Autonomous Navigation, PointGoal Navigation, Reinforcement Learning (RL)",Image,,Computer Vision,,,https://www.midgardsim.org,https://paperswithcode.com/dataset/midgard,"MIDGARD is an open-source simulator for autonomous robot navigation in outdoor unstructured environments.
It is designed to enable the training of autonomous agents (e.g., unmanned ground vehicles) in photorealistic 3D environments, and support the generalization skills of learning-based agents thanks to the variability in training scenarios.",,,,,,
1930,MidiCaps,Music Generation,Music Generation,"Music Generation, Music Captioning","Audio, Image, Text",English,Computer Vision,,CC,https://huggingface.co/datasets/amaai-lab/MidiCaps,https://paperswithcode.com/dataset/midicaps,"The MidiCaps dataset 1 is a large-scale dataset of 168,385 midi music files with descriptive text captions, and a set of extracted musical features.

The captions have been produced through a captioning pipeline incorporating MIR feature extraction and LLM Claude 3 to caption the data from extracted features with an in-context learning task. The framework used to extract the captions is available open source on github. The original MIDI files originate from the Lakh MIDI Dataset [2,3] and are creative commons licenced.",,,,,,
1931,MIKASA-Robo_Dataset,Robot Manipulation,Robot Manipulation,"Robot Manipulation, Meta Reinforcement Learning, Reinforcement Learning (RL)",,,Methodology,,MIT,https://huggingface.co/datasets/avanturist/mikasa-robo,https://paperswithcode.com/dataset/mikasa-robo-dataset,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
1932,Million-AID,Scene Classification,Scene Classification,"Scene Classification, Image Classification, Remote Sensing Image Classification",Image,,Computer Vision,,,https://captain-whu.github.io/DiRS/,https://paperswithcode.com/dataset/million-aid,"Million-AID is a large-scale benchmark dataset containing a million instances for RS scene classification. There are 51 semantic scene categories in Million-AID. And the scene categories are customized to match the land-use classification standards, which greatly enhance the practicability of the constructed Million-AID. Different form the existing scene classification datasets of which categories are organized with parallel or uncertain relationships, scene categories in Million-AID are organized with systematic relationship architecture, giving it superiority in management and scalability. Specifically, the scene categories in Million-AID are organized by the hierarchical category network of a three-level tree: 51 leaf nodes fall into 28 parent nodes at the second level which are grouped into 8 nodes at the first level, representing the 8 underlying scene categories of agriculture land, commercial land, industrial land, public service land, residential land, transportation land, unutilized land, and water area. The scene category network provides the dataset with excellent organization of relationship among different scene categories and also the property of scalability. The number of images in each scene category ranges from 2,000 to 45,000, endowing the dataset with the property of long tail distribution. Besides, Million-AID has superiorities over the existing scene classification datasets owing to its high spatial resolution, large scale, and global distribution.",,DiRS: On Creating Benchmark Datasets for Remote Sensing Image Interpretation,https://arxiv.org/pdf/2006.12485,,,
1933,MIMIC-CXR,Multi-Label Classification,Multi-Label Classification,"Multi-Label Classification, Medical Report Generation","Image, Text",English,Computer Vision,"medical-report-generation-on-mimic-cxr, multi-label-classification-on-mimic-cxr",,https://physionet.org/content/mimic-cxr/2.0.0/,https://paperswithcode.com/dataset/mimic-cxr,"MIMIC-CXR from Massachusetts Institute of Technology presents 371,920 chest X-rays associated with 227,943 imaging studies from 65,079 patients. The studies were performed at Beth Israel Deaconess Medical Center in Boston, MA.",,Can we trust deep learning models diagnosis? The impact of domain shift in chest radiograph classification,https://arxiv.org/abs/1909.01940,,,
1934,MIMIC-III,Multi-Label Classification Of Biomedical Texts,Multi-Label Classification Of Biomedical Texts,"Multi-Label Classification Of Biomedical Texts, Medical Code Prediction, Mortality Prediction, Length-of-Stay prediction, Multivariate Time Series Forecasting, Blood pressure estimation, Multi-Label Text Classification","Image, Text, Time Series",English,Medical,"blood-pressure-estimation-on-mimic-iii, mortality-prediction-on-mimic-iii, multi-label-classification-of-biomedical, multivariate-time-series-forecasting-on-mimic, length-of-stay-prediction-on-mimic-iii, multi-label-text-classification-on-mimic-iii, medical-code-prediction-on-mimic-iii",MIT,https://mimic.physionet.org/,https://paperswithcode.com/dataset/mimic-iii,"The Medical Information Mart for Intensive Care III (MIMIC-III) dataset is a large, de-identified and publicly-available collection of medical records. Each record in the dataset includes ICD-9 codes, which identify diagnoses and procedures performed. Each code is partitioned into sub-codes, which often include specific circumstantial details. The dataset consists of 112,000 clinical reports records (average length 709.3 tokens) and 1,159 top-level ICD-9 codes. Each report is assigned to 7.6 codes, on average. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. 

The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.",,,,,,
1935,MIMIC-IT,Instruction Following,Instruction Following,Instruction Following,,,Methodology,,MIT,https://github.com/luodian/otter,https://paperswithcode.com/dataset/mimic-it,"MultI-Modal In-Context Instruction Tuning (MIMIC-IT) is a dataset for instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. The data sample consists of a queried image-instruction-answer triplet, with the instruction-answer tailored to the image, and context. The context contains a series of image-instruction-answer triplets that contextually correlate with the queried triplet, emulating the relationship between the context and the queried image-text pair found in the MMC4 dataset.",,Otter: A Multi-Modal Model with In-Context Instruction Tuning,https://arxiv.org/pdf/2305.03726v1.pdf,,,
1936,MIMIC-IV-ECG,Electrocardiography (ECG),Electrocardiography (ECG),"Electrocardiography (ECG), ECG Classification",Image,,Computer Vision,,Open Data Commons Open Database License v1.0,https://physionet.org/content/mimic-iv-ecg/1.0/,https://paperswithcode.com/dataset/mimic-iv-ecg,"The MIMIC-IV-ECG module contains approximately 800,000 diagnostic electrocardiograms across nearly 160,000 unique patients. These diagnostic ECGs use 12 leads and are 10 seconds in length. They are sampled at 500 Hz. This subset contains all of the ECGs for patients who appear in the MIMIC-IV Clinical Database. When a cardiologist report is available for a given ECG, we provide the needed information to link the waveform to the report. The patients in MIMIC-IV-ECG have been matched against the MIMIC-IV Clinical Database, making it possible to link to information across the MIMIC-IV modules.",,,,,,
1937,MIMIC-IV-ICD-10-full,Medical Code Prediction,Medical Code Prediction,Medical Code Prediction,Time Series,,Medical,medical-code-prediction-on-mimic-iv-icd-10-1,,,https://paperswithcode.com/dataset/mimic-iv-icd-10-full,"The MIMIC-IV-ICD10-full dataset, including occurring labels.",,,,,,
1938,MIMIC-IV-ICD10-top50,Medical Code Prediction,Medical Code Prediction,Medical Code Prediction,Time Series,,Medical,medical-code-prediction-on-mimic-iv-icd10,,,https://paperswithcode.com/dataset/mimic-iv-icd10-top50,"The MIMIC-IV-ICD10 dataset, featuring the top 50 most frequently occurring labels.",,,,,,
1939,MIMIC-IV-ICD9-full,Medical Code Prediction,Medical Code Prediction,Medical Code Prediction,Time Series,,Medical,medical-code-prediction-on-mimic-iv-icd9-full,,,https://paperswithcode.com/dataset/mimic-iv-icd9-full,"The MIMIC-IV-ICD9 dataset, including all occurring labels.",,,,,,
1940,MIMIC-IV-ICD9-top50,Medical Code Prediction,Medical Code Prediction,Medical Code Prediction,Time Series,,Medical,medical-code-prediction-on-mimic-iv-icd9,,,https://paperswithcode.com/dataset/mimic-iv-icd9-top50,"The MIMIC-IV-ICD9 dataset, featuring the top 50 most frequently occurring labels.",,,,,,
1941,MIMIC-IV_ICD-10,Medical Code Prediction,Medical Code Prediction,Medical Code Prediction,Time Series,,Medical,medical-code-prediction-on-mimic-iv-icd-10,PhysioNet Credentialed Health Data License 1.5.0,https://github.com/JoakimEdin/medical-coding-reproducibility,https://paperswithcode.com/dataset/mimic-iv-icd-10,"MIMIC-IV ICD-10 contains 122,279 discharge summaries—free-text medical documents—annotated with ICD-10 diagnosis and procedure codes. It contains data for patients admitted to the Beth Israel Deaconess Medical Center emergency department or ICU between 2008-2019. All codes with fewer than ten examples have been removed, and the train-val-test split was created using multi-label stratified sampling. The dataset is described further in Automated Medical Coding on MIMIC-III and MIMIC-IV: A Critical Review and Replicability Study, and the code to use the dataset is found here.

The dataset is intended for medical code prediction and was created using MIMIC-IV v2.2 and MIMIC-IV-NOTE v2.2. Using the two datasets requires a license obtained in Physionet; this can take a couple of days.",2008,Automated Medical Coding on MIMIC-III and MIMIC-IV: A Critical Review and Replicability Study,https://arxiv.org/abs/2304.10909,,,
1942,MIMIC-IV_ICD-9,Medical Code Prediction,Medical Code Prediction,Medical Code Prediction,Time Series,,Medical,medical-code-prediction-on-mimic-iv-icd-9,PhysioNet Credentialed Health Data License 1.5.0,https://github.com/JoakimEdin/medical-coding-reproducibility,https://paperswithcode.com/dataset/mimic-iv-icd-9,"MIMIC-IV ICD-9 contains 209,326 discharge summaries—free-text medical documents—annotated with ICD-9 diagnosis and procedure codes. It contains data for patients admitted to the Beth Israel Deaconess Medical Center emergency department or ICU between 2008-2019. All codes with fewer than ten examples have been removed, and the train-val-test split was created using multi-label stratified sampling. The dataset is described further in Automated Medical Coding on MIMIC-III and MIMIC-IV: A Critical Review and Replicability Study, and the code to use the dataset is found here.

The dataset is intended for medical code prediction and was created using MIMIC-IV v2.2 and MIMIC-IV-NOTE v2.2. Using the two datasets requires a license obtained in Physionet; this can take a couple of days.",2008,Automated Medical Coding on MIMIC-III and MIMIC-IV: A Critical Review and Replicability Study,https://arxiv.org/abs/2304.10909,,,
1943,MIMIC-SPARQL,Semantic Parsing,Semantic Parsing,"Semantic Parsing, Question Answering, Graph Learning","Graph, Text",English,Natural Language Processing,,,https://github.com/junwoopark92/mimic-sparql,https://paperswithcode.com/dataset/mimic-sparql,"Question Answering (QA) is a widely-used framework for developing and evaluating an intelligent
machine. In this light, QA on Electronic Health Records (EHR), namely EHR QA, can work as
a crucial milestone toward developing an intelligent agent in healthcare. EHR data are typically
stored in a relational database, which can also be converted to a directed acyclic graph, allowing two
approaches for EHR QA: Table-based QA and Knowledge Graph-based QA.

MIMIC-SPARQL dataset provides  graph-based EHR QA data where natural language queries are converted
to SPARQL instead of SQL",,,,,,
1944,MIMIC_Meme_Dataset,Multi-Label Learning,Multi-Label Learning,"Multi-Label Learning, Multimodal Deep Learning",,,Methodology,,,https://www.kaggle.com/datasets/aakash941/mimic-dataset,https://paperswithcode.com/dataset/mimic-meme-dataset,"This dataset endeavors to fill the research void by presenting a meticulously curated collection of misogynistic memes in a code-mixed language of Hindi and English. It introduces two sub-tasks: the first entails a binary classification to determine the presence of misogyny in a meme, while the second task involves categorizing the misogynistic memes into multiple labels, including Objectification, Prejudice, and Humiliation.

For more Information and  Citation: Singh, A., Sharma, D., & Singh, V. K. (2024). MIMIC: Misogyny Identification in Multimodal Internet Content in Hindi-English Code-Mixed Language. ACM Transactions on Asian and Low-Resource Language Information Processing. (https://doi.org/10.1145/3656169)",2024,,,,,
1945,MIMIC_PERform_Testing_Dataset,Photoplethysmography (PPG) heart rate estimation,Photoplethysmography (PPG) heart rate estimation,"Photoplethysmography (PPG) heart rate estimation, Electrocardiography (ECG), Photoplethysmography (PPG) beat detection, Photoplethysmography (PPG)",Image,,Computer Vision,"photoplethysmography-ppg-heart-rate, photoplethysmography-ppg-beat-detection-on",Open Data Commons Open Database License v1.0,https://doi.org/10.5281/zenodo.6807402,https://paperswithcode.com/dataset/mimic-perform-testing-dataset,"The MIMIC PERform Testing dataset contains the following physiological signals recorded from 200 critically-ill patients during routine clinical care:


electrocardiogram (ECG)
photoplethysmogram (PPG)
impedance pneumography (imp), also known as respiratory (resp)
(in some cases) arterial blood pressure (abp)

Each signal is sampled at 125 Hz. The dataset also contains some fixed parameters for each subject (such as whether the subject was an adult or neonate).

The dataset is available in CSV, Matlab, and WaveForm Database formats here.

Further details of the datasets are provided in the documentation accompanying the ppg-beats project documentation.

The dataset was extracted from the MIMIC III Waveform Database.",,,,,,
1946,MIMII,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Unsupervised Anomaly Detection, Open Set Learning",Image,,Computer Vision,,CC BY-SA 4.0,https://zenodo.org/record/3384388,https://paperswithcode.com/dataset/mimii,"Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection (MIMII) is a sound dataset
of industrial machine sounds.",,,,,,
1947,MIMI_dataset,Data Visualization,Data Visualization,Data Visualization,,,Methodology,,Creative Commons Attribution 4.0 International,https://doi.org/10.5281/zenodo.6360651,https://paperswithcode.com/dataset/mimi-dataset,"Nowadays, new branches of research are proposing the use of non-traditional data sources for the study of migration trends in order to find an original methodology to answer open questions about cross-border human mobility.
The Multi-aspect Integrated Migration Indicators (MIMI) dataset is a new dataset to be exploited in migration studies as a concrete example of this new approach. It includes both official data about bidirectional human migration (traditional flow and stock data) with multidisciplinary variables and original indicators, including economic, demographic, cultural and geographic indicators, together with the Facebook Social Connectedness Index (SCI).
It results from the process of gathering, embedding and integrating traditional and novel variables, resulting in this new multidisciplinary dataset that could significantly contribute to nowcast/forecast bilateral migration trends and migration drivers.

Thanks to this variety of knowledge, experts from several research fields (demographers, sociologists, economists) could exploit MIMI to investigate the trends in the various  indicators, and the relationship among them. Moreover, it could be possible to develop complex models based on these data, able to assess human migration by evaluating related interdisciplinary drivers, as well as models able to nowcast and predict traditional migration indicators in accordance with original variables, such as the strength of social connectivity. Here, the SCI could have an important role. It measures the relative probability that two individuals across two countries are friends with each other on Facebook, therefore it could be employed as a proxy of social connections across borders, to be studied as a possible driver of migration. 

All in all, the motivations for building and releasing the MIMI dataset lie in the need of new perspectives, methods and analyses that can no longer prescind from taking into account a variety of new factors. The heterogeneous and multidimensional sets of data present in MIMI offer an all-encompassing overview of the characteristics of human migration, enabling a better understanding and an original potential exploration of the relationship between migration and non-traditional sources of data.",,,,,,
1948,MIND,News Recommendation,News Recommendation,"News Recommendation, Recommendation Systems",,,Methodology,news-recommendation-on-mind,,https://msnews.github.io/,https://paperswithcode.com/dataset/mind,"MIcrosoft News Dataset (MIND) is a large-scale dataset for news recommendation research. It was collected from anonymized behavior logs of Microsoft News website. The mission of MIND is to serve as a benchmark dataset for news recommendation and facilitate the research in news recommendation and recommender systems area.

MIND contains about 160k English news articles and more than 15 million impression logs generated by 1 million users. Every news article contains rich textual content including title, abstract, body, category and entities. Each impression log contains the click events, non-clicked events and historical news click behaviors of this user before this impression. To protect user privacy, each user was de-linked from the production system when securely hashed into an anonymized ID.",,,,,,
1949,Mindboggle,Graph Learning,Graph Learning,"Graph Learning, Graph Matching, Domain Adaptation",Graph,,Methodology,,,https://mindboggle.info/data.html,https://paperswithcode.com/dataset/mindboggle,"Mindboggle is a large publicly available dataset of manually labeled brain MRI. It consists of 101 subjects collected from different sites, with cortical meshes varying from 102K to 185K vertices. Each brain surface contains 25 or 31 manually labeled parcels.",,Graph Convolutions on Spectral Embeddings: Learning of Cortical Surface Data,https://arxiv.org/abs/1803.10336,,,
1950,Mindgames,Formal Logic,Formal Logic,"Formal Logic, Multiple-choice, Natural Language Inference, Common Sense Reasoning",Text,English,Natural Language Processing,multiple-choice-on-mindgames,Apache-2.0,https://huggingface.co/datasets/sileod/mindgames,https://paperswithcode.com/dataset/mindgames,We generate epistemic reasoning problems using modal logic to target theory of mind (tom) in natural language processing models.,,,,,,
1951,mini-Imagenet,Cross-Domain Few-Shot,Cross-Domain Few-Shot,"Cross-Domain Few-Shot, Continual Learning, Few-Shot Class-Incremental Learning, Unsupervised Few-Shot Image Classification, Few-Shot Learning, Few-Shot Image Classification",Image,,Computer Vision,"few-shot-image-classification-on-mini-3, few-shot-image-classification-on-mini-9, continual-learning-on-miniimagenet, unsupervised-few-shot-image-classification-on, few-shot-image-classification-on-mini-6, few-shot-image-classification-on-mini-5, few-shot-image-classification-on-mini-8, few-shot-learning-on-mini-imagenet-5-way-1, few-shot-image-classification-on-mini-4, cross-domain-few-shot-on-miniimagenet, few-shot-image-classification-on-mini-10, few-shot-image-classification-on-mini-13, few-shot-image-classification-on-mini-7, few-shot-learning-on-mini-imagenet-1-shot-2, few-shot-class-incremental-learning-on-mini, unsupervised-few-shot-image-classification-on-1, few-shot-image-classification-on-mini-1, few-shot-image-classification-on-mini-12, few-shot-learning-on-mini-imagenet-5-shot, few-shot-image-classification-on-mini-2",,,https://paperswithcode.com/dataset/mini-imagenet,"mini-Imagenet is proposed by  Matching Networks for One Shot Learning
. In NeurIPS, 2016. This dataset consists of 50000 training images and 10000 testing images, evenly
distributed across 100 classes.",2016,,,,training images and 10000 testing images,100
1952,MiniF2F,Automated Theorem Proving,Automated Theorem Proving,Automated Theorem Proving,,,Methodology,"automated-theorem-proving-on-minif2f-valid, automated-theorem-proving-on-minif2f-test",,https://github.com/openai/miniF2F,https://paperswithcode.com/dataset/minif2f,"MiniF2F is a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The miniF2F benchmark currently targets Metamath, Lean, and Isabelle and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad (IMO), as well as material from high-school and undergraduate mathematics courses.",,,,,,
1953,MinneApple,Object Detection,Object Detection,"Object Detection, Inference Attack, Head Detection",Image,,Computer Vision,,,https://github.com/nicolaihaeni/MinneApple,https://paperswithcode.com/dataset/minneapple,"MinneApple is a benchmark dataset for apple detection and segmentation. The fruits are labelled using polygonal masks for each object instance to aid in precise object detection, localization, and segmentation. Additionally, the dataset also contains data for patch-based counting of clustered fruits. The dataset contains over 41, 000 annotated object instances in 1000 images.",,,,1000 images,,
1954,MINOS,Object Detection,Object Detection,"Object Detection, Vision and Language Navigation, Visual Navigation","Image, Text",English,Computer Vision,,MIT,https://github.com/minosworld/minos,https://paperswithcode.com/dataset/minos,MINOS is a simulator designed to support the development of multisensory models for goal-directed navigation in complex indoor environments. MINOS leverages large datasets of complex 3D environments and supports flexible configuration of multimodal sensor suites.,,,,,,
1955,MIntRec,Multimodal Intent Recognition,Multimodal Intent Recognition,Multimodal Intent Recognition,Image,,Multimodal,multimodal-intent-recognition-on-mintrec,,https://github.com/thuiar/MIntRec,https://paperswithcode.com/dataset/mintrec,"MIntRec is a novel dataset for multimodal intent recognition. It formulates coarse-grained and fine-grained intent taxonomies based on the data collected from the TV series Superstore. The dataset consists of 2,224 high-quality samples with text, video, and audio modalities and has multimodal annotations among twenty intent categories.",,,,,,
1956,MIPE,Antibody-antigen binding prediction,Antibody-antigen binding prediction,Antibody-antigen binding prediction,Time Series,,Methodology,antibody-antigen-binding-prediction-on-mipe,MIT,https://www.ijcai.org/proceedings/2024/0669.pdf,https://paperswithcode.com/dataset/mipe,"Datasets. From the publicly accessible Structural Antibody Database (SAbDab), we collected a total of 7571 antibodyantigen complexes, with the sequence data in FASTA format and structural data in PDB format. Following previous studies [Pittala and Bailey-Kellogg, 2020], we used CD-HIT [Li and Godzik, 2006] to remove high-homology antibody and antigen sequences with the thresholds of 95% and 90% sequence identity, respectively. Subsequently, we excluded antibodies and antigens with any residue type rather than 20 naturally occurring types. Finally, we compiled a dataset consisting of 626 binding antibody-antigen pairs, including their sequences, structures, and corresponding interaction maps. Noteworthy, antibodies primarily bind to antigens through their CDR regions. Most researchers use Euclidean distance to define paratopes and epitopes, and we follow the usual way in our dataset: within the CDR regions/antigen, a residue is labeled as a paratope/epitope if the Euclidean distance between its backbone atom and any backbone atom on the other antigen/CDR regions is less than 4.5 ˚ A.",2020,Homepage,https://www.ijcai.org/proceedings/2024/0669.pdf,,,
1957,MIR-1K,Neural Architecture Search,Neural Architecture Search,"Neural Architecture Search, Music Source Separation, Speech Separation",Audio,,Audio,,,http://mirlab.org/dataset/public/,https://paperswithcode.com/dataset/mir-1k,"MIR-1K (Multimedia Information Retrieval lab, 1000 song clips) is a dataset designed for singing voice separation. It contains:


1000 song clips with the music accompaniment and the singing voice recorded as left and right channels, respectively,
Manual annotations of pitch contours in semitone, indices and types for unvoiced frames, lyrics, and vocal/non-vocal segments,
The speech recordings of the lyrics by the same person who sang the songs.

The duration of each clip ranges from 4 to 13 seconds, and the total length of the dataset is 133 minutes. These clips are extracted from 110 karaoke songs which contain a mixture track and a music accompaniment track. These songs are freely selected from 5000 Chinese pop songs and sung by researchers from MIR lab (8 females and 11 males). Most of the singers are amateur and do not have professional music training.",,,,,,
1958,MIR-ST500,Multi-instrument Music Transcription,Multi-instrument Music Transcription,Multi-instrument Music Transcription,Audio,,Audio,,CCBY,http://mirlab.org/dataset/public/,https://paperswithcode.com/dataset/mir-st500,"MIR-ST500
Good for the following task:
Singing transcription (singing pitch to music note conversion)
Used in several papers published by Roger Jang's Lab",,,,,,
1959,MIRACL-VC1,Lipreading,Lipreading,"Lipreading, Lip password classification",Image,,Computer Vision,lip-password-classification-on-miracl-vc1,,https://abenhamadou.github.io/miraclvc1/index.html,https://paperswithcode.com/dataset/miracl-vc1,"MIRACL-VC1 is a lip-reading dataset including both depth and color images. It can be used for diverse research fields like visual speech recognition, face detection, and biometrics. Fifteen speakers (five men and ten women) positioned in the frustum of an MS Kinect sensor and utter ten times a set of ten words and ten phrases (see the table below). Each instance of the dataset consists of a synchronized sequence of color and depth images (both of 640x480 pixels).  The MIRACL-VC1 dataset contains a total number of 3000 instances.",,,,3000 instances,,
1960,MISAW,Activity Recognition,Activity Recognition,"Activity Recognition, Surgical Gesture Recognition, Surgical phase recognition","Image, Video",,Computer Vision,surgical-phase-recognition-on-misaw,,https://www.synapse.org/MISAW,https://paperswithcode.com/dataset/misaw,"The MISAW data set is composed of 27 sequences of micro-surgical anastomosis on artificial blood vessels performed by 3 surgeons and 3 engineering students. The dataset contained video, kinematic, and procedural descriptions synchronized at 30Hz. The procedural descriptions contained phases, steps, and activities performed by the participants.",,MIcro-Surgical Anastomose Workflow recognition challenge report,https://arxiv.org/abs/2103.13111,,,
1961,MISP2021,Audio-Visual Speech Recognition,Audio-Visual Speech Recognition,Audio-Visual Speech Recognition,"Audio, Image, Text",English,Audio,,Custom,https://mispchallenge.github.io,https://paperswithcode.com/dataset/misp2021,"The MISP2021 challenge dataset is a collection of audio-visual conversational data recorded in a home TV scenario using distant multi-microphones. The dataset captures interactions between several individuals who are engaged in conversations in Chinese while watching TV and interacting with a smart speaker/TV in a living room. The dataset is extensive, comprising 141 hours of audio and video data, which were collected using far/middle/near microphones and far/middle cameras in 34 real-home TV rooms. Notably, this corpus is the first of its kind to offer a distant multimicrophone conversational Chinese audio-visual dataset. Furthermore, it is also the first large vocabulary continuous Chinese lip-reading dataset specifically designed for the adverse home-TV scenario.",,https://mispchallenge.github.io/doc/Non-commercial%20Data%20License%20Agreement%20for%20MISP%20Challenge%202021.pdf,https://mispchallenge.github.io/doc/Non-commercial%20Data%20License%20Agreement%20for%20MISP%20Challenge%202021.pdf,,,
1962,MIT-Adobe_FiveK,Low-Light Image Enhancement,Low-Light Image Enhancement,"Low-Light Image Enhancement, Photo Retouching, Image Enhancement",Image,,Computer Vision,"image-enhancement-on-mit-adobe-fivek, photo-retouching-on-mit-adobe-5k, image-enhancement-on-mit-adobe-5k, low-light-image-enhancement-on-mit-adobe-1",Custom,https://data.csail.mit.edu/graphics/fivek/,https://paperswithcode.com/dataset/mit-adobe-fivek,"The MIT-Adobe FiveK dataset consists of 5,000 photographs taken with SLR cameras by a set of different photographers. They are all in RAW format; that is, all the information recorded by the camera sensor is preserved. We made sure that these photographs cover a broad range of scenes, subjects, and lighting conditions. We then hired five photography students in an art school to adjust the tone of the photos. Each of them retouched all the 5,000 photos using a software dedicated to photo adjustment (Adobe Lightroom) on which they were extensively trained. We asked the retouchers to achieve visually pleasing renditions, akin to a postcard. The retouchers were compensated for their work.

This dataset was collected for our project on learning photographic adjustments. When using images from this dataset, please cite this dataset using the following BibTeX:

@inproceedings{fivek,
    author = ""Vladimir Bychkovsky and Sylvain Paris and Eric Chan and Fr{\'e}do Durand"",
    title = ""Learning Photographic Global Tonal Adjustment with a Database of Input / Output Image Pairs"",
    booktitle = ""The Twenty-Fourth IEEE Conference on Computer Vision and Pattern Recognition"",
    year = ""2011""
}",2011,,,,,
1963,MIT-BIH_Arrhythmia_Database,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Arrhythmia Detection, Heartbeat Classification, QRS Complex Detection",Image,,Computer Vision,"arrhythmia-detection-on-mit-bih-ar, anomaly-detection-on-mit-bih-arrhythmia, heartbeat-classification-on-mit-bih-ar, qrs-complex-detection-on-mit-bih-ar, arrhythmia-detection-on-mit-bih-arrhythmia",Open Data Commons Attribution License v1.0,https://physionet.org/content/mitdb/1.0.0/,https://paperswithcode.com/dataset/mit-bih-arrhythmia-database,"The MIT-BIH Arrhythmia Database contains 48 half-hour excerpts of two-channel ambulatory ECG recordings, obtained from 47 subjects studied by the BIH Arrhythmia Laboratory between 1975 and 1979. Twenty-three recordings were chosen at random from a set of 4000 24-hour ambulatory ECG recordings collected from a mixed population of inpatients (about 60%) and outpatients (about 40%) at Boston's Beth Israel Hospital; the remaining 25 recordings were selected from the same set to include less common but clinically significant arrhythmias that would not be well-represented in a small random sample.

The recordings were digitized at 360 samples per second per channel with 11-bit resolution over a 10 mV range. Two or more cardiologists independently annotated each record; disagreements were resolved to obtain the computer-readable reference annotations for each beat (approximately 110,000 annotations in all) included with the database.

This directory contains the entire MIT-BIH Arrhythmia Database. About half (25 of 48 complete records, and reference annotation files for all 48 records) of this database has been freely available here since PhysioNet's inception in September 1999. The 23 remaining signal files, which had been available only on the MIT-BIH Arrhythmia Database CD-ROM, were posted here in February 2005.

Much more information about this database may be found in the MIT-BIH Arrhythmia Database Directory.",1975,,,360 samples,,
1964,MIT-States,Image Retrieval with Multi-Modal Query,Image Retrieval with Multi-Modal Query,"Image Retrieval with Multi-Modal Query, Zero-Shot Learning, Compositional Zero-Shot Learning",Image,,Computer Vision,"compositional-zero-shot-learning-on-mit-3, image-retrieval-with-multi-modal-query-on-mit, zero-shot-learning-on-mit-states-1, compositional-zero-shot-learning-on-mit-2",,http://web.mit.edu/phillipi/Public/states_and_transformations/index.html,https://paperswithcode.com/dataset/mit-states,"The MIT-States dataset has 245 object classes, 115 attribute classes and ∼53K images. There is a wide range of objects (e.g., fish, persimmon, room) and attributes (e.g., mossy, deflated, dirty). On average, each object instance is modified by one of the 9 attributes it affords.",,Attributes as Operators: Factorizing Unseen Attribute-Object Compositions,https://arxiv.org/abs/1803.09851,53K images,,
1965,MIT_Indoor_Scenes,Scene Recognition,Scene Recognition,Scene Recognition,Image,,Computer Vision,scene-recognition-on-mit-indoors-scenes,,https://www.kaggle.com/itsahmad/indoor-scenes-cvpr-2019,https://paperswithcode.com/dataset/mit-indoors-scenes,"Context
This is the Original data provided by MIT .

Indoor scene recognition is a challenging open problem in high level vision. Most scene recognition models that work well for outdoor scenes perform poorly in the indoor domain. The main difficulty is that while some indoor scenes (e.g. corridors) can be well characterized by global spatial properties, others (e.g., bookstores) are better characterized by the objects they contain. More generally, to address the indoor scenes recognition problem we need a model that can exploit local and global discriminative information.

Content
The database contains 67 Indoor categories, and a total of 15620 images. The number of images varies across categories, but there are at least 100 images per category. All images are in jpg format. The images provided here are for research purposes only.

Acknowledgements
Thanks to MIT
Thanks to Aude Oliva for helping to create the database of indoor scenes.
Funding for this research was provided by NSF Career award (IIS 0747120)",,,,15620 images,,
1966,MixATIS,Slot Filling,Slot Filling,"Slot Filling, Intent Detection, Semantic Frame Parsing","Image, Text",English,Computer Vision,"intent-detection-on-mixatis, slot-filling-on-mixatis, semantic-frame-parsing-on-mixatis",GPL -2.0 licence,,https://paperswithcode.com/dataset/mixatis,"Dataset is constructed from single intent dataset ATIS. 

This is a  publically available multi intent dataset, which can be downloaded from https://github.com/LooperXX/AGIF/data",,,,,,
1967,MixSNIPS,Semantic Parsing,Semantic Parsing,"Semantic Parsing, Intent Detection, Slot Filling, Semantic Frame Parsing","Image, Text",English,Computer Vision,"semantic-frame-parsing-on-mixsnips, intent-detection-on-mixsnips, slot-filling-on-mixsnips",,https://github.com/LooperXX/AGIF/data,https://paperswithcode.com/dataset/mixsnips,"Dataset is constructed from single intent dataset SNIPS.

This is a publicly available multi intent dataset, which can be downloaded from https://github.com/LooperXX/AGIF/data",,,,,,
1968,ML25m__TGN_Style_,Dynamic Link Prediction,Dynamic Link Prediction,Dynamic Link Prediction,Time Series,,Methodology,,CC BY-NC-SA 4.0,https://www.kaggle.com/datasets/chenxi1228/ml25m-tgn-style,https://paperswithcode.com/dataset/ml25m-tgn-style,ML25m dataset which is pre-processed in TGN Style.,,,,,,
1969,MLDoc,Cross-Lingual Document Classification,Cross-Lingual Document Classification,"Cross-Lingual Document Classification, Cross-Lingual Sentiment Classification","Image, Text",English,Computer Vision,"cross-lingual-document-classification-on-2, cross-lingual-sentiment-classification-on-5, cross-lingual-document-classification-on-9, cross-lingual-document-classification-on-11, cross-lingual-document-classification-on-10, cross-lingual-sentiment-classification-on-4, cross-lingual-document-classification-on-8, cross-lingual-sentiment-classification-on-6, cross-lingual-document-classification-on, cross-lingual-document-classification-on-1, cross-lingual-document-classification-on-14",Custom,https://github.com/facebookresearch/MLDoc,https://paperswithcode.com/dataset/mldoc,"Multilingual Document Classification Corpus (MLDoc) is a cross-lingual document classification dataset covering English, German, French, Spanish, Italian, Russian, Japanese and Chinese. It is a subset of the Reuters Corpus Volume 2 selected according to the following design choices:


uniform class coverage: same number of examples for each class and language,
official train / development / test split: for each language a training data of different sizes (1K, 2K, 5K and 10K stories), a development (1K) and a test corpus (4K) are provided (with exception of Spanish and Russian with 9458 and 5216 training documents respectively.",,,,,,
1970,MLO-Cn2,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Time Series Regression",Time Series,,Time Series,"time-series-regression-on-mlo-cn2, time-series-forecasting-on-mlo-cn2",Creative Commons Attribution 4.0 International,https://data.eol.ucar.edu/dataset/160.007,https://paperswithcode.com/dataset/mlo-cn2,"The Mauna Loa Seeing Study was performed by the EOL/Integrated Surface
Flux System team, capturing surface meteorology and flux products at the Mauna Loa Observatory in Hawaii.

The MLO-Cn2 dataset enables researchers to evaluate physics-inspired models for optical turbulence ($C_n^2$), as well as develop new models from sonic anemometer data.

Summary
Sonic anemometers were deployed during Summer 2006 at the Mauna Loa Observatory, Hawaii to quantify the atmospheric seeing quality, to help NCAR/HAO plan current and future telescope observations. This study was intended to replicate measurements made previously for planning the Advanced Technology Solar Telescope (ATST). Deployment and operations for this study were done by HAO staff, and ISFF provided equipment and some data management support.

Details
Identifiers
 * Local archive identifier: 160.007
 * doi:10.26023/CQR2-TQJ9-AH10
   https://doi.org/10.26023/CQR2-TQJ9-AH10

Related projects
 * MLO_CN2: Mauna Loa Seeing Study (Cn-squared study)
   https://data.eol.ucar.edu/project/MLO_CN2

Additional information
 * Frequency: 5 minute
 * Language: English

Categories
 * Surface

Platforms
 * Flux Tower
 * NCAR/EOL Integrated Surface Flux System - ISFS

Instruments
 * Eddy Correlation Devices
 * Sonic Anemometer
 * Surface Flux
 * Surface Meteorology

Related links
 * homepage: MLO_CN2 project main web page
   http://www.eol.ucar.edu/isf/projects/MLO_CN2/
 * info: Integrated Surface Flux System
   https://www.eol.ucar.edu/observing_facilities/isfs
 * homepage: MLO_CN2 Project Homepage
   https://www.eol.ucar.edu/field_projects/mlocn2

Temporal coverage
 * Begin datetime: 2006-06-09 00:00:00 
 * End datetime: 2006-08-08 23:59:59 

Spatial coverage
 * Maximum (North) Latitude: 19.53
 * Minimum (South) Latitude: 19.53
 * Minimum (West) Longitude: -155.57
 * Maximum (East) Longitude: -155.57

Primary contact information
 * pointOfContact: EOL Data Support &#100;&#97;&#116;&#97;&#104;&#101;&#108;&#112;&#64;&#101;&#111;&#108;&#46;&#117;&#99;&#97;&#114;&#46;&#101;&#100;&#117;
   https://data.eol.ucar.edu/contact/show/1

Additional contact information
 * author: NSF NCAR/EOL ISFS Team
   https://data.eol.ucar.edu/contact/show/3140

Alternate metadata formats
 * DataCite: https://data.eol.ucar.edu/dataset/160.007?format=datacite
 * ISO/TC 211: https://data.eol.ucar.edu/dataset/160.007?format=isotc211",2006,,,,,
1971,MLQA,Cross-Lingual Transfer,Cross-Lingual Transfer,"Cross-Lingual Transfer, Question Answering, Machine Translation, Cross-Lingual Question Answering",Text,English,Natural Language Processing,cross-lingual-question-answering-on-mlqa,CC-BY-SA 3.0,https://github.com/facebookresearch/mlqa,https://paperswithcode.com/dataset/mlqa,"MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance. MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between 4 different languages on average.",,,,,,
1972,MLSUM,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Text Summarization, Language Modelling, Information Retrieval",Text,English,Natural Language Processing,"abstractive-text-summarization-on-mlsum-es-1, abstractive-text-summarization-on-mlsum-es, abstractive-text-summarization-on-mlsum-de, abstractive-text-summarization-on-mlsum-it",,https://github.com/recitalAI/MLSUM,https://paperswithcode.com/dataset/mlsum,"A large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community.",,,,,,
1973,MLT17,Incremental Learning,Incremental Learning,"Incremental Learning, Continual Learning",,,Methodology,"continual-learning-on-mlt17, incremental-learning-on-mlt17",,,https://paperswithcode.com/dataset/mlt17,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
1974,MM-Locate-News,Photo geolocation estimation,Photo geolocation estimation,Photo geolocation estimation,,,Methodology,,,https://github.com/TIBHannover/mm-locate-news,https://paperswithcode.com/dataset/mm-locate-news,"MM-Locate-News is a dataset for location estimation of news. It consists of 6395 news articles covering 237 cities and 152 countries across all continents as well as multiple domains such as health, environment, and politics. The dataset is collected in a weakly-supervised manner, and multiple data cleaning steps are applied to remove articles with potential inaccurate geolocation information. The acquired dataset addresses drawbacks of other datasets such as BreakingNews as it considers multimodal content of news to label the corresponding location.",,MM-Locate-News: Multimodal Focus Location Estimation in News,https://arxiv.org/pdf/2211.08042v1.pdf,,,
1975,MM-OR,Scene Graph Generation,Scene Graph Generation,"Scene Graph Generation, 4D Panoptic Segmentation, 2D Panoptic Segmentation, Action Anticipation, Video Panoptic Segmentation, Surgical phase recognition, Video Segmentation, 3D Panoptic Segmentation","3D, Graph, Image, Text, Video",English,Computer Vision,"video-panoptic-segmentation-on-mm-or, 2d-panoptic-segmentation-on-mm-or, scene-graph-generation-on-mm-or",Apache-2.0,https://github.com/egeozsoy/MM-OR,https://paperswithcode.com/dataset/mm-or,"Operating rooms (ORs) are complex, high-stakes environments requiring precise understanding of interactions among medical staff, tools, and equipment for enhancing surgical assistance, situational awareness, and patient safety. Current datasets fall short in scale, realism and do not capture the multimodal nature of OR scenes, limiting progress in OR modeling. To this end, we introduce MM-OR, a realistic and large-scale multimodal spatiotemporal OR dataset, and the first dataset to enable multimodal scene graph generation. MM-OR captures comprehensive OR scenes containing RGB-D data, detail views, audio, speech transcripts, robotic logs, and tracking data and is annotated with panoptic segmentations, semantic scene graphs, and downstream task labels. Further, we propose MM2SG, the first multimodal large vision-language model for scene graph generation, and through extensive experiments, demonstrate its ability to effectively leverage multimodal inputs. Together, MM-OR and MM2SG establish a new benchmark for holistic OR understanding, and open the path towards multimodal scene analysis in complex, high-stakes environments.

Paper: https://arxiv.org/abs/2503.02579",,,,,,
1976,MM-Vet,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Visual Question Answering","Image, Text",English,Computer Vision,"visual-question-answering-on-mm-vet, visual-question-answering-vqa-on-mm-vet",CC BY-NC 4.0,https://github.com/yuweihao/MM-Vet,https://paperswithcode.com/dataset/mm-vet,MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,,,,,,
1977,mMARCO,Passage Re-Ranking,Passage Re-Ranking,Passage Re-Ranking,,,Methodology,,,https://github.com/unicamp-dl/mMARCO.git,https://paperswithcode.com/dataset/mmarco,mMARCO is a multilingual version of the MS MARCO passage ranking dataset comprising 8 languages that was created using machine translation.,,,,,,
1978,MMBench,Visual Question Answering,Visual Question Answering,Visual Question Answering,"Image, Text",English,Computer Vision,visual-question-answering-on-mmbench,Apache-2.0 license,https://opencompass.org.cn/mmbench,https://paperswithcode.com/dataset/mmbench,"MMBench is a multi-modality benchmark. It methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluation questions and abilities. The second element introduces a novel CircularEval strategy and incorporates the use of ChatGPT. This implementation is designed to convert free-form predictions into pre-defined choices, thereby facilitating a more robust evaluation of the model's predictions.",,,,,,
1979,MMChat,Multi-modal Dialogue Generation,Multi-modal Dialogue Generation,"Multi-modal Dialogue Generation, Open-Domain Dialog, Dialogue Generation",Text,English,Natural Language Processing,,,https://github.com/silverriver/MMChat,https://paperswithcode.com/dataset/mmchat,"A large scale Chinese multi-modal dialogue corpus (120.84K dialogues and 198.82 K images).
MMCHAT contains image-grounded dialogues collected from real conversations on social media.
We manually annotate 100K dialogues from MMCHAT with the dialogue quality and whether the dialogues are related to the given image.
We provide the rule-filtered raw dialogues  that are used to create MMChat (Rule Filtered Raw MMChat). It contains 4.257 M dialogue sessions and 4.874 M images
We provide a version of MMChat that is filtered based on LCCC (LCCC Filtered MMChat). This version contain much cleaner dialogues (492.6 K dialogue sessions and 1.066 M images)",,,,,,
1980,MMConv,Response Generation,Response Generation,"Response Generation, Dialogue State Tracking","Image, Text, Video",English,Computer Vision,"response-generation-on-mmconv, dialogue-state-tracking-on-mmconv",Custom,https://github.com/lizi-git/MMConv,https://paperswithcode.com/dataset/mmconv,"The main goal of the data collection is to acquire highly natural conversations that cover a wide variety of styles and scenarios. In total, the presented corpus consists of five domains: Food, Hotel, Nightlife, Shopping mall and Sightseeing. Controlled by our various task settings, the collected dialogues cover between one to four domains per dialogue, and are thus of greatly varying length and complexity. There are 808 single-task dialogues that contains a single venue target and 4, 298 multi-task dialogues consisting of at least two to four venue targets. These different venues vary in domains most of the times.",,,,,,
1981,MMDialog,Multi-modal Dialogue Generation,Multi-modal Dialogue Generation,"Multi-modal Dialogue Generation, Open-Domain Dialog, Multimodal Intent Recognition","Image, Text",English,Computer Vision,multimodal-intent-recognition-on-mmdialog,,https://github.com/victorsungo/MMDialog,https://paperswithcode.com/dataset/mmdialog,"MMDialog is a large-scale multi-turn dialogue dataset containing multi-modal open-domain conversations derived from real human-human chat content in social media. MMDialog contains 1.08M dialogue sessions and 1.53M associated images. On average, one dialogue session has 2.59 images, which can be located anywhere at any conversation turn.",,MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation,https://arxiv.org/pdf/2211.05719v1.pdf,59 images,,
1982,MMHS150k,Meme Classification,Meme Classification,"Meme Classification, Hate Speech Detection","Audio, Image",,Computer Vision,,,https://www.kaggle.com/datasets/victorcallejasf/multimodal-hate-speech,https://paperswithcode.com/dataset/mmhs150k,"Existing hate speech datasets contain only textual data. We create a new manually annotated multimodal hate speech dataset formed by 150,000 tweets, each one of them containing text and an image. We call the dataset MMHS150K.",,,,,,
1983,MMKG,Multi-modal Knowledge Graph,Multi-modal Knowledge Graph,"Multi-modal Knowledge Graph, Entity Alignment, Multi-modal Entity Alignment, Knowledge Graphs, Link Prediction","Graph, Time Series",,Methodology,"entity-alignment-on-fbyg15k, entity-alignment-on-dbp15k-zh-en, entity-alignment-on-dbp15k-ja-en, entity-alignment-on-fbdb15k, multi-modal-entity-alignment-on-mmkg, entity-alignment-on-dbp15k-fr-en",BSD 3-Clause License,https://github.com/nle-ml/mmkb,https://paperswithcode.com/dataset/mmkg,"MMKG is a collection of three knowledge graphs for link prediction and entity matching research. Contrary to other knowledge graph datasets, these knowledge graphs contain both numerical features and images for all entities as well as entity alignments between pairs of KGs. While MMKG is intended to perform relational reasoning across different entities and images, previous resources are intended to perform visual reasoning within the same image.

The three knowledge graphs augmented with numerical features and images are called FB15k, YAGO15k, and DBPEDIA15k.",,,,,,
1984,MMLU-Pro,MMLU,MMLU,MMLU,,,Methodology,mmlu-on-mmlu-pro,,https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro,https://paperswithcode.com/dataset/mmlu-pro,"The MMLU-Pro dataset is an enhanced version of the Massive Multitask Language Understanding (MMLU) benchmark. It's designed to be more robust and challenging, aiming to rigorously benchmark large language models' capabilities in language comprehension and reasoning across diverse domains. Here are some key features of the MMLU-Pro dataset:


Increased Complexity: It includes more reasoning-focused questions and expands the choice set from four to ten options, reducing the likelihood of random guessing and increasing the evaluation's complexity¹.
Elimination of Trivial Questions: MMLU-Pro removes trivial and noisy questions found in the original MMLU, making it a more discriminative benchmark².
Stability Under Varying Prompts: The dataset shows greater stability under varying prompts, with a decreased sensitivity of model scores to prompt variations².
Better Performance with Reasoning: Models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering².
Size and Scope: The dataset contains 12K complex questions across various disciplines¹⁴.

(1) TIGER-Lab/MMLU-Pro · Datasets at Hugging Face. https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro.
(2) MMLU-Pro: A More Robust and Challenging Multi-Task Language .... https://arxiv.org/abs/2406.01574.
(3) MMLU-Pro: An Upgraded Version of the MMLU Dataset | LLM Explorer Blog. https://llm.extractum.io/static/blog/?id=mmlu-pro-benchmark.
(4) TIGER-Lab Introduces MMLU-Pro Dataset for Comprehensive Benchmarking of .... https://www.marktechpost.com/2024/05/16/tiger-lab-introduces-mmlu-pro-dataset-for-comprehensive-benchmarking-of-large-language-models-capabilities-and-performance/.
(5) undefined. https://doi.org/10.48550/arXiv.2406.01574.",2024,,,,,
1985,MMLU,Natural Language Understanding,Natural Language Understanding,"Natural Language Understanding, Text Generation, Multi-task Language Understanding, Question Answering, Multiple Choice Question Answering (MCQA)",Text,English,Natural Language Processing,"multiple-choice-question-answering-mcqa-on-10, multiple-choice-question-answering-mcqa-on-18, multiple-choice-question-answering-mcqa-on-4, multiple-choice-question-answering-mcqa-on-3, multiple-choice-question-answering-mcqa-on-24, multiple-choice-question-answering-mcqa-on-8, multiple-choice-question-answering-mcqa-on-7, multiple-choice-question-answering-mcqa-on-20, multiple-choice-question-answering-mcqa-on-19, text-generation-on-mmlu-5-shot, question-answering-on-mmlu, multi-task-language-understanding-on-mmlu-5-1, multiple-choice-question-answering-mcqa-on-9, multiple-choice-question-answering-mcqa-on-11, multiple-choice-question-answering-mcqa-on-23, multiple-choice-question-answering-mcqa-on-15, multi-task-language-understanding-on-mmlu, multiple-choice-question-answering-mcqa-on-6, multiple-choice-question-answering-mcqa-on-25, multiple-choice-question-answering-mcqa-on-5, multiple-choice-question-answering-mcqa-on-2, multiple-choice-question-answering-mcqa-on-16, multiple-choice-question-answering-mcqa-on-14, multiple-choice-question-answering-mcqa-on-12, multiple-choice-question-answering-mcqa-on-17, multiple-choice-question-answering-mcqa-on-13, multiple-choice-question-answering-mcqa-on-26, text-generation-on-mmlu-tr",Custom,https://github.com/hendrycks/test,https://paperswithcode.com/dataset/mmlu,"MMLU (Massive Multitask Language Understanding) is a new benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans. The benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics. The granularity and breadth of the subjects makes the benchmark ideal for identifying a model’s blind spots.",,https://arxiv.org/pdf/2009.03300v3.pdf,https://arxiv.org/pdf/2009.03300v3.pdf,,,
1986,MMNeedle,multimodal generation,multimodal generation,"multimodal generation, Hallucination, Long-Context Understanding",Text,English,Multimodal,"long-context-understanding-on-mmneedle, hallucination-on-mmneedle",CC BY 4.0,https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack/tree/main,https://paperswithcode.com/dataset/mmneedle,"We introduce the MultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to assess the long-context capabilities of MLLMs. Besides multi-image input, we employ image stitching to further increase the input context length, and develop a protocol to automatically generate labels for sub-image level retrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their capability to locate a target sub-image (needle) within a set of images (haystack) based on textual instructions and descriptions of image contents. This setup necessitates an advanced understanding of extensive visual contexts and effective information retrieval within long-context image inputs.",,,,,"val. Essentially, MMNeedle evaluates MLLMs by stress-testing their capability to locate a target sub-image (needle) within a set of images",
1987,MMPD,Heart rate estimation,Heart rate estimation,"Heart rate estimation, Photoplethysmography (PPG) heart rate estimation",,,Methodology,,On Request,https://github.com/McJackTang/MMPD_rPPG_dataset,https://paperswithcode.com/dataset/mmpd,"The Multi-domain Mobile Video Physiology Dataset (MMPD), comprising 11 hours(1152K frames) of recordings from mobile phones of 33 subjects. The dataset was designed to capture videos with greater representation across skin tone, body motion, and lighting conditions. MMPD is comprehensive with eight descriptive labels and can be used in conjunction with the rPPG-toolbox and PhysBench. MMPD is widely used for rPPG tasks and remote heart rate estimation. To access the dataset, you are supposed to download this data release agreement and request downloading by email.",,data release agreement,https://github.com/McJackTang/MMPD_rPPG_dataset/blob/main/MMPD_Release_Agreement.pdf,,,
1988,MMSE-HR,Heart Rate Variability,Heart Rate Variability,"Heart Rate Variability, Photoplethysmography (PPG), Heart rate estimation, Photoplethysmography (PPG) heart rate estimation, Photoplethysmography (PPG) beat detection",Image,,Computer Vision,photoplethysmography-ppg-heart-rate-2,,https://binghamton.technologypublisher.com/tech/MMSE-HR_dataset_(Multimodal_Spontaneous_Expression-Heart_Rate_dataset),https://paperswithcode.com/dataset/mmse-hr,"The MMSE-HR benchmark consists of a dataset of 102 videos from 40 subjects recorded at 1040x1392 raw resolution at 25fps. During the recordings, various stimuli such as videos, sounds, and smells are introduced to induce different emotional states in the subjects. The ground truth waveform for MMSE-HR is the blood pressure signal sampled at 1000Hz. The dataset contains a diverse distribution of skin colors in the Fitzpatrick scale (II=8, III=11, IV=17, V+VI=4).",,,,,,
1989,MMSQL,Text-To-SQL,Text-To-SQL,"Text-To-SQL, MMSQL performance",Text,English,Natural Language Processing,mmsql-performance-on-mmsql,CC BY 4.0,https://mcxiaoxiao.github.io/MMSQL/,https://paperswithcode.com/dataset/mmsql,"A dataset for training and testing tin various problem types and multi-turn Q&A scenarios, including a training set, test set, and test scripts.",,,,,,
1990,MMToM-QA,Theory of Mind Modeling,Theory of Mind Modeling,Theory of Mind Modeling,,,Methodology,,,https://chuanyangjin.com/mmtom-qa,https://paperswithcode.com/dataset/mmtom-qa,"MMToM-QA is the first multimodal benchmark to evaluate machine Theory of Mind (ToM), the ability to understand people's minds. MMToM-QA consists of 600 questions. Each question is paired with a clip of the full activity in a video (as RGB-D frames), as well as a text description of the scene and the actions taken by the person in that clip. All questions have two choices. The questions are categorized into seven types, evaluating belief inference and goal inference in rich and diverse situations. Each belief inference type has 100 questions, totaling 300 belief questions; each goal inference type has 75 questions, totaling 300 goal questions. The questions are paired with 134 videos of a person looking for daily objects in household environments.",,,,,,
1991,MMVP,Music Question Answering,Music Question Answering,"Music Question Answering, Chart Question Answering","Audio, Text",English,Natural Language Processing,,,https://tsb0601.github.io/mmvp_blog,https://paperswithcode.com/dataset/musicqa-dataset,"The MMVP (Multimodal Visual Patterns) Benchmark focuses on identifying ""CLIP-blind pairs"" – images that appear similar to the CLIP model despite having clear visual differences. These patterns highlight the challenges these systems face in answering straightforward questions, often leading to incorrect responses and hallucinated explanations.",,,,,,
1992,MN-DS,Text Classification,Text Classification,"Text Classification, News Classification","Image, Text",English,Computer Vision,,,https://github.com/alinapetukhova/mn-ds-news-classification,https://paperswithcode.com/dataset/mn-ds,"Multilabeled News Dataset (MN-DS) is a dataset for news classification. It consists of 10,917 articles in 17 first-level and 109 second-level categories from 215 media sources.",,,,,,
1993,MNAD,Text Categorization,Text Categorization,"Text Categorization, Text Classification, Short Text Clustering","Image, Text",English,Computer Vision,,,https://github.com/J-Mourad/MNAD,https://paperswithcode.com/dataset/mnad,"About the MNAD Dataset
The MNAD corpus is a collection of over 1 million Moroccan news articles written in modern Arabic language. These news articles have been gathered from 11 prominent electronic news sources. The dataset is made available to the academic community for research purposes, such as data mining (clustering, classification, etc.), information retrieval (ranking, search, etc.), and other non-commercial activities.

Dataset Fields

Title: The title of the article
Body: The body of the article
Category: The category of the article
Source: The Electronic News paper source of the article

About Version 1 of the Dataset (MNAD.v1)
Version 1 of the dataset comprises 418,563 articles classified into 19 categories. The data was collected from well-known electronic news sources, namely Akhbarona.ma, Hespress.ma, Hibapress.com, and Le360.com. The articles were stored in four separate CSV files, each corresponding to the news website source. Each CSV file contains three fields: Title, Body, and Category of the news article.

The dataset is rich in Arabic vocabulary, with approximately 906,125 unique words. It has been utilized as a benchmark in the research paper:
""A Moroccan News Articles Dataset (MNAD) For Arabic Text Categorization"". In 2021 International Conference on Decision Aid Sciences and Application (DASA).

This dataset is available for download from the following sources:
- Kaggle Datasets : MNADv1
- Huggingface Datasets: MNADv1

About Version 2 of the Dataset (MNAD.v2)
Version 2 of the MNAD dataset includes an additional 653,901 articles, bringing the total number of articles to over 1 million (1,069,489), classified into the same 19 categories as in version 1. The new documents were collected from seven additional prominent Moroccan news websites, namely al3omk.com, medi1news.com, alayam24.com, anfaspress.com, alyaoum24.com, barlamane.com, and SnrtNews.com.

The newly collected articles have been merged with the articles from the previous version into a single CSV file named MNADv2.csv. This file includes an additional column called ""Source"" to indicate the source of each news article.

Furthermore, MNAD.v2 incorporates improved pre-processing techniques and data cleaning methods. These enhancements involve removing duplicates, eliminating multiple spaces, discarding rows with NaN values, replacing new lines with ""\n"", excluding very long and very short articles, and removing non-Arabic articles. These additions and improvements aim to enhance the usability and value of the MNAD dataset for researchers and practitioners in the field of Arabic text analysis.

This dataset is available for download from the following sources:
- Kaggle Datasets : MNADv2
- Huggingface Datasets: MNADv2

Citation
If you use our data, please cite the following paper:

bibtex
@inproceedings{MNAD2021,
    author    = {Mourad Jbene and 
                 Smail Tigani and 
                 Rachid Saadane and 
                 Abdellah Chehri},
    title     = {A Moroccan News Articles Dataset ({MNAD}) For Arabic Text Categorization},
    year      = {2021},
    publisher = {{IEEE}},
    booktitle = {2021 International Conference on Decision Aid Sciences and Application ({DASA})}
    doi       = {10.1109/dasa53625.2021.9682402},
    url       = {https://doi.org/10.1109/dasa53625.2021.9682402},
}",2021,,,,,19
1994,MNIST-8M,Active Learning,Active Learning,"Active Learning, Distributed Computing, Stochastic Optimization",,,Methodology,,,https://leon.bottou.org/projects/infimnist,https://paperswithcode.com/dataset/mnist-8m,MNIST8M is derived from the MNIST dataset by applying random deformations and translations to the dataset.,,Scalable and Sustainable Deep Learningvia Randomized Hashing,https://arxiv.org/abs/1602.08194,,,
1995,MNIST-MIX,Handwritten Digit Recognition,Handwritten Digit Recognition,Handwritten Digit Recognition,Image,,Computer Vision,,,https://github.com/jwwthu/MNIST-MIX,https://paperswithcode.com/dataset/mnist-mix,MNIST-MIX is a multi-language handwritten digit recognition dataset. It contains digits from 10 different languages.,,,,,,
1996,MNIST,Video Prediction,Video Prediction,"Video Prediction, Neural Architecture Search, Superpixel Image Classification, Malicious Detection, Handwritten Digit Recognition, Sequential Image Classification, General Classification, Image Clustering, Fine-Grained Image Classification, Unsupervised Image Classification, Sparse Learning and binarization, Unsupervised Image-To-Image Translation, Model Poisoning, Network Pruning, Image Classification, Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly, Structured Prediction, Hard-label Attack, Unsupervised Anomaly Detection, Graph Classification, Density Estimation, Clustering Algorithms Evaluation, Adversarial Defense, Anomaly Detection, Multiview Clustering, Personalized Federated Learning, Image Generation, Stochastic Optimization, Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly, Nature-Inspired Optimization Algorithm, Classification with Binary Weight Network, Core set discovery, Domain Adaptation, One-Shot Learning, Adversarial Defense against FGSM Attack, Deep Clustering, Continual Learning, Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly, Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly, Continuously Indexed Domain Adaptation, Rotated MNIST, Unsupervised MNIST, Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly","Graph, Image, Text, Time Series, Video",English,Computer Vision,"domain-adaptation-on-usps-to-mnist, domain-adaptation-on-rotating-mnist, neural-architecture-search-on-mnist, malicious-detection-on-mnist, domain-adaptation-on-mnist-to-usps, unsupervised-image-to-image-translation-on, rotated-mnist-on-rotated-mnist-1, deep-clustering-on-mnist, unsupervised-anomaly-detection-on-mnist-1, graph-classification-on-mnist, image-classification-on-noisy-mnist-contrast, stochastic-optimization-on-mnist, unsupervised-anomaly-detection-with-specified-23, handwritten-digit-recognition-on-mnist, one-shot-learning-on-mnist, fine-grained-image-classification-on-mnist, adversarial-defense-against-fgsm-attack-on, hard-label-attack-on-mnist, image-clustering-on-mnist, image-classification-on-mnist, sparse-learning-and-binarization-on-mnist, superpixel-image-classification-on-75, adversarial-defense-on-mnist, model-poisoning-on-mnist, personalized-federated-learning-on-mnist-1, image-clustering-on-mnist-test, density-estimation-on-mnist, unsupervised-anomaly-detection-with-specified-13, unsupervised-mnist-on-mnist, image-classification-on-noisy-mnist-motion, classification-with-binary-weight-network-on-3, continual-learning-on-rotated-mnist, network-pruning-on-mnist, video-prediction-on-moving-mnist, clustering-algorithms-evaluation-on-mnist, sequential-image-classification-on-sequential, structured-prediction-on-mnist, unsupervised-image-classification-on-mnist, image-clustering-on-mnist-full, image-generation-on-mnist, unsupervised-anomaly-detection-with-specified-22, anomaly-detection-on-mnist-test, general-classification-on-mnist, domain-adaptation-on-svnh-to-mnist, continuously-indexed-domain-adaptation-on-3, core-set-discovery-on-mnist, multiview-clustering-on-mnist, anomaly-detection-on-mnist, unsupervised-anomaly-detection-with-specified-17, unsupervised-anomaly-detection-with-specified-10, nature-inspired-optimization-algorithm-on, image-classification-on-noisy-mnist-awgn",,http://yann.lecun.com/exdb/mnist/,https://paperswithcode.com/dataset/mnist,"The MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger NIST Special Database 3 (digits written by employees of the United States Census Bureau) and Special Database 1 (digits written by high school students) which contain monochrome images of handwritten digits. The digits have been size-normalized and centered in a fixed-size image. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.",,,,000 examples,"training set of 60,000 examples",
1997,MO-Gymnasium,Continuous Control,Continuous Control,"Continuous Control, MuJoCo Games, OpenAI Gym",,,Methodology,,MIT,https://github.com/Farama-Foundation/MO-Gymnasium,https://paperswithcode.com/dataset/mo-gymnasium,"MO-Gymnasium is an open source Python library for developing and comparing multi-objective reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Essentially, the environments follow the standard Gymnasium API, but return vectorized rewards as numpy arrays.",,,,,,
1998,MoCapAct,Continuous Control,Continuous Control,"Continuous Control, Human motion prediction","Image, Time Series, Video",,Computer Vision,,CDLA Permissive 2.0,https://github.com/microsoft/MoCapAct#dataset,https://paperswithcode.com/dataset/mocapact,"The MoCapAct dataset contains training data and models for humanoid locomotion research. It consists of expert policies that are trained to track individual clip snippets and HDF5 files of noisy rollouts collected from each expert, including proprioceptive observations and actions.",,,,,,
1999,MODA_dataset,Spindle Detection,Spindle Detection,"Spindle Detection, Sleep spindles detection",Image,,Computer Vision,spindle-detection-on-moda-dataset,,https://doi.org/10.17605/OSF.IO/8BMA7,https://paperswithcode.com/dataset/moda-dataset,"MODA is a large open-source dataset of high quality, human-scored sleep spindles (5342 spindles, from 180 subjects) that was produced by the Massive Online Data Annotation project. Sleep spindles were detected as a consensus of a number of human-expert scorers. With a median number of 5 experts scoring every EEG segment, MODA offers sleep spindle annotations of a quality unseen in previous datasets.

The dataset was described and introduced in the following publication:

Lacourse, K., Yetton, B., Mednick, S. et al. Massive online data annotation, crowdsourcing to generate high quality sleep spindle annotations from EEG data. Sci Data 7, 190 (2020). https://doi.org/10.1038/s41597-020-0533-4",2020,,,,,
2000,ModelNet,Training-free 3D Point Cloud Classification,Training-free 3D Point Cloud Classification,"Training-free 3D Point Cloud Classification, Generative 3D Object Classification, 3D Point Cloud Classification, 3D Point Cloud Data Augmentation, 3D Object Classification, 3D Point Cloud Linear Classification, Zero-shot 3D Point Cloud Classification, 3D Parameter-Efficient Fine-Tuning for Classification, Few-Shot Point Cloud Classification, 3D Object Recognition, 3D Object Retrieval, Few-Shot 3D Point Cloud Classification, Zero-Shot Transfer 3D Point Cloud Classification","3D, Image",,Computer Vision,"few-shot-point-cloud-classification-on, few-shot-3d-point-cloud-classification-on-3, few-shot-3d-point-cloud-classification-on-1, generative-3d-object-classification-on-2, 3d-point-cloud-linear-classification-on, zero-shot-transfer-3d-point-cloud, 3d-object-classification-on-modelnet40, 3d-parameter-efficient-fine-tuning-for-1, 3d-object-classification-on-modelnet10, 3d-object-recognition-on-modelnet40, zero-shot-transfer-3d-point-cloud-1, 3d-point-cloud-data-augmentation-on, 3d-object-retrieval-on-modelnet40, few-shot-3d-point-cloud-classification-on-2, zero-shot-3d-point-cloud-classification-on, training-free-3d-point-cloud-classification, few-shot-3d-point-cloud-classification-on-4, 3d-point-cloud-classification-on-modelnet40",,https://modelnet.cs.princeton.edu/,https://paperswithcode.com/dataset/modelnet,"The ModelNet40 dataset contains synthetic object point clouds. As the most widely used benchmark for point cloud analysis, ModelNet40 is popular because of its various categories, clean shapes, well-constructed dataset, etc. The original ModelNet40 consists of 12,311 CAD-generated meshes in 40 categories (such as airplane, car, plant, lamp), of which 9,843 are used for training while the rest 2,468 are reserved for testing. The corresponding point cloud data points are uniformly sampled from the mesh surfaces, and then further preprocessed by moving to the origin and scaling into a unit sphere.",,Geometric Feedback Network for Point Cloud Classification,https://arxiv.org/abs/1911.12885,,,40
2001,ModelNet40-C,3D Point Cloud Classification,3D Point Cloud Classification,"3D Point Cloud Classification, 3D Point Cloud Data Augmentation, Robust classification, Few-Shot Point Cloud Classification, Classify 3D Point Clouds, 3D Classification","3D, Image",,Computer Vision,"3d-point-cloud-data-augmentation-on-2, few-shot-point-cloud-classification-on-1, 3d-point-cloud-classification-on-modelnet40-c",BSD 3-Clause,https://sites.google.com/umich.edu/modelnet40c,https://paperswithcode.com/dataset/modelnet40-c,"ModelNet40-C is a comprehensive dataset to benchmark the corruption robustness of 3D point cloud recognition.

We create ModelNet40-C based on the ModelNet40 validation set with 15 corruption types and 5 severity levels for each corruption type including density, noise, and transformation corruption patterns. Our dataset contains 185,000 distinct point clouds that help provide a comprehensive picture of model robustness.",,,,,,
2002,Molecule3D,Molecular Property Prediction,Molecular Property Prediction,"Molecular Property Prediction, 3D Geometry Prediction","3D, Time Series",,Methodology,"3d-geometry-prediction-on-molecule3d-val, 3d-geometry-prediction-on-molecule3d-test",,https://github.com/divelab/MoleculeX/tree/molx/Molecule3D,https://paperswithcode.com/dataset/molecule3d,"Molecule3D is a new benchmark that includes a dataset with precise ground-state geometries of approximately 4 million molecules derived from density functional theory (DFT). It also provides a set of software tools for data processing, splitting, training, and evaluation, etc.",,,,,,
2003,MoleculeNet,Molecular Property Prediction,Molecular Property Prediction,"Molecular Property Prediction, Graph Regression, Node Classification","Graph, Image, Time Series",,Computer Vision,molecular-property-prediction-on-moleculenet,,https://moleculenet.org,https://paperswithcode.com/dataset/moleculenet,"MoleculeNet is a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance.",,,,,,
2004,Molweni,Machine Reading Comprehension,Machine Reading Comprehension,"Machine Reading Comprehension, Dialogue Understanding, Discourse Parsing, Question Answering, Reading Comprehension",Text,English,Natural Language Processing,"question-answering-on-molweni, discourse-parsing-on-molweni",,https://github.com/HIT-SCIR/Molweni,https://paperswithcode.com/dataset/molweni,"A machine reading comprehension (MRC) dataset with discourse structure built over multiparty dialog. Molweni's source samples from the Ubuntu Chat Corpus, including 10,000 dialogs comprising 88,303 utterances.",,,,,,
2005,MOMA-LRG,Video Classification,Video Classification,"Video Classification, Video Segmentation, Scene Graph Detection, Few Shot Action Recognition","Graph, Image, Video",,Computer Vision,few-shot-action-recognition-on-moma-lrg,CC BY-SA 4.0,https://moma.stanford.edu,https://paperswithcode.com/dataset/moma-lrg,"A dataset dedicated to multi-object, multi-actor activity parsing.

The dataset contains
* Video-level labels (activities)
* Segment-level labels (sub-activities)
* Atomic actions (spatio-temporal scene graph)

The scene graph annotations contain object/actor classes and bounding boxes, relationship annotations, and object/actor attributes.",,,,,,
2006,Mono3DRefer,Monocular 3D Object Detection,Monocular 3D Object Detection,"Monocular 3D Object Detection, Visual Grounding, Mono3DVG, 3D visual grounding, 3D Object Detection","3D, Image",,Computer Vision,mono3dvg-on-mono3drefer,,https://github.com/ZhanYang-nwpu/Mono3DVG,https://paperswithcode.com/dataset/mono3drefer,"We sample 2025 frames of images from the original KITTI for Mono3DRefer, containing 41,140 expressions in total and a vocabulary of 5,271 words.",2025,,,,,
2007,Monopedal_Gaits,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Time Series, Time Series Generation","Text, Time Series",English,Time Series,,CC BY 4.0,https://doi.org/10.1109/LRA.2022.3186500,https://paperswithcode.com/dataset/hopper-gaits,"The dataset comprises time-series data capturing distinct periodic motions (gaits) of an energetically conservative one-legged hopper. Due to energy conservation, all gaits form a continuous one-dimensional family and undergo bifurcations as the internal energy varies, leading to different motion patterns such as in-place hopping, forward hopping, and backward hopping.

The dataset is intended to advance surrogate modeling techniques for bifurcation analysis in hybrid dynamical systems, a complex class of models that represent non-smooth dynamic behaviors.

The corresponding paper, discussing the system and how the data is generated, can be found in:
Connecting Gaits in Energetically Conservative Legged Systems.",,,,,,
2008,Montreal_Archive_of_Sleep_Studies,K-complex detection,K-complex detection,"K-complex detection, Sleep Staging, Automatic Sleep Stage Classification, W-R-L-D Sleep Staging, Spindle Detection, Sleep Stage Detection, W-R-N Sleep Staging",Image,,Computer Vision,"w-r-n-sleep-staging-on-montreal-archive-of, sleep-stage-detection-on-mass-ss3, spindle-detection-on-mass-ss2, sleep-stage-detection-on-montreal-archive-of, k-complex-detection-on-mass-ss2, sleep-stage-detection-on-mass-ss2, w-r-l-d-sleep-staging-on-montreal-archive-of",Open Access,http://ceams-carsm.ca/mass/,https://paperswithcode.com/dataset/montreal-archive-of-sleep-studies,"The Montreal Archive of Sleep Studies (MASS) is an open-access and collaborative database of laboratory-based polysomnography (PSG) recordings O’Reilly, C., et al. (2014) J Seep Res, 23(6):628-635. Its goal is to provide a standard and easily accessible source of data for benchmarking the various systems developed to help the automation of sleep analysis. It also provides a readily available source of data for fast validation of experimental results and for exploratory analyses. Finally, it is a shared resource that can be used to foster large-scale collaborations in sleep studies.

MASS is composed of cohorts themselves comprising subsets. Recordings within subsets is kept as homogeneous as possible, whereas it is more heterogeneous between subsets. To allow inter-study comparisons, researchers validating their results on MASS are encouraged to specify which portion of the database they used in their assessment (e.g., MASS-C1 for the whole cohort 1, MASS-C1/SS1-SS3 for subsets 1, 2 and 3 of cohort 1).

Currently, the first MASS cohort available is described in O’Reilly, C., et al. (2014) J Seep Res, 23(6):628-635. This cohort comprises polysomnograms of 200 complete nights recorded in 97 men and 103 women of age varying between 18 and 76 years (mean: 38.3 years, SD: 18.9 years). It has been split into five different subsets.",2014,,,,,
2009,MoNuSAC,Medical Image Segmentation,Medical Image Segmentation,"Medical Image Segmentation, Nuclei Classification, Nuclear Segmentation",Image,,Medical,medical-image-segmentation-on-monusac,CC BY-NC-SA 4.0,https://monusac-2020.grand-challenge.org/Home/,https://paperswithcode.com/dataset/monusac,"Different types of cells play a vital role in the initiation, development, invasion, metastasis and therapeutic response of tumors of various organs. For example, (1) most carcinomas originate from epithelial cells, (2) spatial arrangement of tumor infiltrating Lymphocytes (TILs) is associated with clinical outcome in several cancers, including the ones of breast, prostate, and lung (Fridman et. al., Nature Reviews Cancer, 2012), and (3) tumor associated macrophages (TAMs) influence diverse processes such as angiogenesis, neoplastic cell mitogenesis, antigen presentation, matrix degradation, and cytotoxicity in various tumors (Ruffel and Coussens, Cancer Cell, 2015). Thus, accurate identification and segmentation of nuclei of multiple cell-types is important for AI enabled characterization of tumor and its microenvironment.

In this challenge, participants will be provided with H&E stained tissue images of four organs with annotations of multiple cell-types including epithelial cells, lymphocytes, macrophages, and neutrophils. Participants will use the annotated dataset to develop computer vision algorithms to recognize these cell-types from the tissue images of unseen patients released in the testing set of the challenge. Additionally, all cell-types will not have equal number of annotated instances in the training dataset which will encourage participants to develop algorithms for learning from imbalanced classes in a few shot learning paradigm. 

H&E staining of human tissue sections is a routine and most common protocol used by pathologists to enhance the contrast of tissue sections for tumor assessment (grading, staging, etc.) at multiple microscopic resolutions. Hence, we will provide the annotated dataset of H&E stained digitized tissue images of several patients acquired at multiple hospitals using one of the most common 40x scanner magnification. The annotations will be done with the help of expert pathologists.",2012,,,,,
2010,MOPRD,Review Generation,Review Generation,Review Generation,Text,English,Natural Language Processing,,,http://www.linjialiang.net/publications/moprd,https://paperswithcode.com/dataset/moprd,"MOPRD, a multidisciplinary open peer review dataset consists of paper metadata, multiple version manuscripts, review comments, meta-reviews, author's rebuttal letters, and editorial decisions from 6578 papers.",,MOPRD: A multidisciplinary open peer review dataset,https://arxiv.org/pdf/2212.04972v1.pdf,,,
2011,MOROCO,Language Identification,Language Identification,Language Identification,Text,English,Natural Language Processing,,,https://github.com/butnaruandrei/MOROCO,https://paperswithcode.com/dataset/moroco,"The MOldavian and ROmanian Dialectal COrpus (MOROCO) is a corpus that contains 33,564 samples of text (with over 10 million tokens) collected from the news domain. The samples belong to one of the following six topics: culture, finance, politics, science, sports and tech. The data set is divided into 21,719 samples for training, 5,921 samples for validation and another 5,924 samples for testing.",,,,564 samples,"training, 5,921 samples",
2012,MORPH,Face Recognition,Face Recognition,"Face Recognition, Fairness, Age Estimation, Few-shot Age Estimation, Facial Attribute Classification, Age-Invariant Face Recognition",Image,,Computer Vision,"face-recognition-on-morph, age-estimation-on-morph-album2, age-estimation-on-morph, fairness-on-morph, few-shot-age-estimation-on-morph-album2, facial-attribute-classification-on-morph, age-invariant-face-recognition-on-morph, age-estimation-on-morph-album2-caucasian",Commercial,https://uncw.edu/oic/tech/morph.html,https://paperswithcode.com/dataset/morph,"MORPH is a facial age estimation dataset, which contains 55,134 facial images of 13,617 subjects ranging from 16 to 77 years old.",,Deep Ordinal Regression Forests,https://arxiv.org/abs/2008.03077,,,
2013,Morph_Call,Interpretable Machine Learning,Interpretable Machine Learning,Interpretable Machine Learning,,,Methodology,,,https://github.com/morphology-probing/morph-call,https://paperswithcode.com/dataset/morph-call,"Morph Call is a suite of 46 probing tasks for four Indo-European languages that fall under different morphology: Russian, French, English, and German. The tasks are designed to explore the morphosyntactic content of multilingual transformers which is a less studied aspect at the moment.

The tasks are divided into four groups:


Morphosyntactic Features: probe the encoder for the occurrence of the morphosyntactic properties.
Masked Token: analogous to Morphosyntactic Features with the exception that the target word is replaced with a tokenizer-specific mask token.
Morphosyntactic Values: is a group of k-way classification tasks for each feature where k is the number of values that the feature can take.
Perturbations: tasks test the encoder sensitivity to syntactic and inflectional sentence perturbations.

Probing Methods

Supervised probing involves training a Logistic Regression classifier to predict a property. The performance is used as a proxy to evaluate the model knowledge.
Neuron-level Analysis [Durrani et al., 2020] allows retrieving a group of individual neurons that are most relevant to predict a linguistic property.
Contextual Correlation Analysis [Wu et al., 2020] is a representation-level similarity measure that allows identifying pairs of layers of similar behavior. 

Usage
We provide an example of the experiment on Masked Token task (Case, German).

```
bash
me@my-laptop:~$ python3 probe.py --help
INFO: Showing help with the command 'probe.py -- --help'.

NAME
    probe.py - configure the experiment AND perform probing

SYNOPSIS
    probe.py <flags>

DESCRIPTION
    configure the experiment AND perform probing

FLAGS
    --results_path=RESULTS_PATH
        Type: Optional[str]
        Default: None
        path to a folder to store the probing results and the model intermediate activations
    --model_architecture=MODEL_ARCHITECTURE
        Type: typ...
        Default: 'bert multilingual'
    --model_is_finetuned=MODEL_IS_FINETUNED
        Type: bool
        Default: False
        if to perform the experiment on the fine-tuned model
    --model_finetuned_path=MODEL_FINETUNED_PATH
        Type: Optional[str]
        Default: None
        (only if model_is_finetuned is True) path to store the fine-tuned model
    --model_finetuned_config_google_url=MODEL_FINETUNED_CONFIG_GOOGLE_URL
        Type: Optional[]
        Default: None
        (only if model_is_finetuned is True) the url of the fine-tuned model config if to be downloaded
    --model_finetuned_model_google_url=MODEL_FINETUNED_MODEL_GOOGLE_URL
        Type: Optional[]
        Default: None
        (only if model_is_finetuned is True) the url of the fine-tuned model weights if to be downloaded
    --model_is_random=MODEL_IS_RANDOM
        Type: bool
        Default: False
        if to perform the random initialization of the model
    --layers_to_probe=LAYERS_TO_PROBE
        Type: List
        Default: 'all'
        (either ""all"" or list w. possible numbers from 0 to 11) -- model layers to probe. e.g.: [1, 3, 11], or ""all""
    --train_n_sentences=TRAIN_N_SENTENCES
        Type: int
        Default: 1500
        number of sentences used to train the probing classifier
    --test_n_sentences=TEST_N_SENTENCES
        Type: int
        Default: 1000
        number of sentences used to evaluate the probing classifier
    --dev_n_sentences=DEV_N_SENTENCES
        Type: int
        Default: 0
        DEPRECATED
```",2020,,,,,
2014,MOSAD,Time Series,Time Series,"Time Series, Semantic Segmentation, Change Point Detection, Human Activity Recognition, Geometrical View, Activity Recognition, Time Series Analysis","Image, Time Series, Video",,Time Series,,CC BY-SA,https://github.com/ermshaua/mobile-sensing-human-activity-data-set,https://paperswithcode.com/dataset/mosad,"MOSAD (Mobile Sensing Human Activity Data Set) is a multi-modal, annotated time series (TS) data set that contains 14 recordings of 9 triaxial smartphone sensor measurements (126 TS) from 6 human subjects performing (in part) 3 motion sequences in different locations. The aim of the data set is to facilitate the study of human behaviour and the design of TS data mining technology to separate individual activities using low-cost sensors in wearable devices.",,,,,,
2015,MOSE,Semi-Supervised Video Object Segmentation,Semi-Supervised Video Object Segmentation,"Semi-Supervised Video Object Segmentation, Interactive Video Object Segmentation, Unsupervised Video Object Segmentation, Video Semantic Segmentation, Video Object Segmentation","Image, Video",,Computer Vision,"semi-supervised-video-object-segmentation-on-21, video-object-segmentation-on-mose",,https://henghuiding.github.io/MOSE,https://paperswithcode.com/dataset/mose,"CoMplex video Object SEgmentation (MOSE) is a dataset to study the tracking and segmenting objects in complex environments. MOSE contains 2,149 video clips and 5,200 objects from 36 categories, with 431,725 high-quality object segmentation masks. The most notable feature of MOSE dataset is complex scenes with crowded and occluded objects.",,MOSE: A New Dataset for Video Object Segmentation in Complex Scenes,https://arxiv.org/pdf/2302.01872.pdf,,,36
2016,MosMedData,Medical Image Segmentation,Medical Image Segmentation,"Medical Image Segmentation, Computed Tomography (CT), Self-Supervised Learning",Image,,Medical,medical-image-segmentation-on-mosmeddata,,https://mosmed.ai/en/,https://paperswithcode.com/dataset/mosmeddata,"MosMedData contains anonymised human lung computed tomography (CT) scans with COVID-19 related findings, as well as without such findings. A small subset of studies has been annotated with binary pixel masks depicting regions of interests (ground-glass opacifications and consolidations). CT scans were obtained between 1st of March, 2020 and 25th of April, 2020, and provided by municipal hospitals in Moscow, Russia.",2020,,,,,
2017,MOS_Dataset,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Text Summarization, Extracting COVID-19 Events from Twitter, Opinion Mining",Text,English,Natural Language Processing,,CC BY 4.0,https://doi.org/10.6084/m9.figshare.20391144,https://paperswithcode.com/dataset/mos-dataset,"This dataset was used in the paper 'Template-based Abstractive Microblog Opinion Summarisation' (to be published at TACL, 2022). The data is structured as follows: each file represents a cluster of tweets which contains the tweet IDs and a summary of the tweets written by journalists. The gold standard summary follows a template structure and depending on its opinion content, it contains a main story, majority opinion (if any) and/or minority opinions (if any).",2022,,,,,
2018,MOT15,Online Multi-Object Tracking,Online Multi-Object Tracking,"Online Multi-Object Tracking, Multi-Object Tracking","Image, Video",,Computer Vision,"multi-object-tracking-on-2dmot15-1, multi-object-tracking-on-mot15, multi-object-tracking-on-2d-mot-2015, online-multi-object-tracking-on-2d-mot-2015, online-multi-object-tracking-on-mot15",CC BY-NC-SA 3.0,https://motchallenge.net/results/2D_MOT_2015/,https://paperswithcode.com/dataset/mot15,"MOT2015 is a dataset for multiple object tracking. It contains 11 different indoor and outdoor scenes of public places with pedestrians as the objects of interest, where camera motion, camera angle and imaging condition vary greatly. The dataset provides detections generated by the ACF-based detector.",,"FAMNet: Joint Learning of Feature, Affinity and Multi-dimensional Assignment for Online Multiple Object Tracking",https://arxiv.org/abs/1904.04989,,,
2019,MOT16,Online Multi-Object Tracking,Online Multi-Object Tracking,"Online Multi-Object Tracking, Multi-Object Tracking","Image, Video",,Computer Vision,"multi-object-tracking-on-mot16, online-multi-object-tracking-on-mot16",CC BY-NC-SA 3.0,https://motchallenge.net/data/MOT16/,https://paperswithcode.com/dataset/mot16,"The MOT16 dataset is a dataset for multiple object tracking. It a collection of existing and new data (part of the sources are from and ), containing 14 challenging real-world videos of both static scenes and moving scenes, 7 for training and 7 for testing. It is a large-scale dataset, composed of totally 110407 bounding boxes in training set and 182326 bounding boxes in test set. All video sequences are annotated under strict standards, their ground-truths are highly accurate, making the evaluation meaningful.",,SOT for MOT,https://arxiv.org/abs/1712.01059,,,
2020,MOT17,Online Multi-Object Tracking,Online Multi-Object Tracking,"Online Multi-Object Tracking, Multi-Object Tracking","Image, Video",,Computer Vision,"online-multi-object-tracking-on-mot17, multi-object-tracking-on-mot17",CC BY-NC-SA 3.0,https://motchallenge.net/data/MOT17/,https://paperswithcode.com/dataset/mot17,"The Multiple Object Tracking 17 (MOT17) dataset is a dataset for multiple object tracking. Similar to its previous version MOT16, this challenge contains seven different indoor and outdoor scenes of public places with pedestrians as the objects of interest. A video for each scene is divided into two clips, one for training and the other for testing. The dataset provides detections of objects in the video frames with three detectors, namely SDP, Faster-RCNN and DPM. The challenge accepts both on-line and off-line tracking approaches, where the latter are allowed to use the future video frames to predict tracks.",,Deep Affinity Network for Multiple Object Tracking,https://arxiv.org/abs/1810.11780,,,
2021,MOTChallenge,Multiple Object Tracking,Multiple Object Tracking,"Multiple Object Tracking, Object Tracking, Multi-Object Tracking, Multiple Object Tracking with Transformer, Online Multi-Object Tracking","Image, Video",,Computer Vision,"multi-object-tracking-on-mot15, multiple-object-tracking-with-transformer-on, multi-object-tracking-on-mot20-1, online-multi-object-tracking-on-mot17, online-multi-object-tracking-on-mot15, online-multi-object-tracking-on-mot16, multi-object-tracking-on-mot16, multi-object-tracking-on-mot17",CC BY-NC-SA 3.0,http://motchallenge.net/,https://paperswithcode.com/dataset/motchallenge,"The MOTChallenge datasets are designed for the task of multiple object tracking. There are several variants of the dataset released each year, such as MOT15, MOT17, MOT20.",,MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking,https://arxiv.org/pdf/1504.01942v1.pdf,,,
2022,MOTIF,Malware Classification,Malware Classification,"Malware Classification, Malware Clustering, Malware Family Detection",Image,,Computer Vision,,Booz Allen Public License v1.0,https://github.com/boozallen/MOTIF,https://paperswithcode.com/dataset/motif,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
2023,Motion-X,Motion Synthesis,Motion Synthesis,Motion Synthesis,Video,,Methodology,motion-synthesis-on-motion-x,CC-BYNC-SA 4.0,https://motion-x-dataset.github.io/,https://paperswithcode.com/dataset/motion-x,"Motion-X is a large-scale 3D expressive whole-body motion dataset, which comprises 15.6M precise 3D whole-body pose annotations (i.e., SMPL-X) covering 81.1K motion sequences from massive scenes, meanwhile providing corresponding semantic labels and pose descriptions.",,,,,,
2024,MotionSense,Activity Recognition,Activity Recognition,"Activity Recognition, Imputation, Self-Supervised Learning","Image, Video",,Computer Vision,,,https://github.com/mmalekzadeh/motion-sense,https://paperswithcode.com/dataset/motionsense,"This dataset includes time-series data generated by accelerometer and gyroscope sensors (attitude, gravity, userAcceleration, and rotationRate). It is collected with an iPhone 6s kept in the participant's front pocket using SensingKit which collects information from Core Motion framework on iOS devices. All data is collected in 50Hz sample rate. A total of 24 participants in a range of gender, age, weight, and height performed 6 activities in 15 trials in the same environment and conditions: downstairs, upstairs, walking, jogging, sitting, and standing.",,,,,,
2025,Motion_Policy_Networks,Motion Planning,Motion Planning,"Motion Planning, Imitation Learning",Video,,Methodology,,Creative Commons Attribution 4.0 International,https://zenodo.org/record/7130512,https://paperswithcode.com/dataset/motion-policy-networks,"This dataset contains a large set (~3.2 Million) of high quality expert trajectories generated from a geometrically consist hybrid planner in a wide variety of environment (~575,000 environments). We created this dataset to explore the capabilities of neural networks to learn complex robotic motion, mimicking a traditional planner. 

For more information on how to use this data, please refer to the Github for this project: https://github.com/nvlabs/motion-policy-networks",,,,,,
2026,MoVi,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Pose Estimation, 3D Human Shape Estimation, 3D Human Reconstruction, Virtual Try-on, Human Mesh Recovery, Activity Recognition","3D, Image, Video",,Computer Vision,3d-human-shape-estimation-on-movi,For non-commercial and scientific research purposes,https://www.biomotionlab.ca/movi/,https://paperswithcode.com/dataset/movi,"Contains 60 female and 30 male actors performing a collection of 20 predefined everyday actions and sports movements, and one self-chosen movement.",,For non-commercial and scientific research purposes,https://www.biomotionlab.ca/Data/Licenses/licenseBMLmovi.pdf,,,
2027,MovieGraphBenchmark,Entity Embeddings,Entity Embeddings,"Entity Embeddings, Entity Resolution",,,Methodology,,,https://github.com/ScaDS/MovieGraphBenchmark,https://paperswithcode.com/dataset/moviegraphbenchmark,"The dataset contains entities from IMDB, TheMovieDB and TheTVDB with goldstandard matches between the sources. Due to the licensing of IMDB we provide a script to build the IMDB part of the dataset yourself.

The dataset contains a variety of entity types to match: persons, movies, series, episodes and companies.",,,,,,
2028,MovieLens,Click-Through Rate Prediction,Click-Through Rate Prediction,"Click-Through Rate Prediction, Recommendation Systems (Item cold-start), Collaborative Filtering, Explainable Recommendation, Sequential Recommendation, Link Prediction, Movie Recommendation, Knowledge Graph Completion, Recommendation Systems, Multi-Media Recommendation, Multibehavior Recommendation","Graph, Time Series",,Methodology,"link-prediction-on-movielens-25m, recommendation-systems-on-movielens-latest, collaborative-filtering-on-movielens-100k, collaborative-filtering-on-movielens-10m, collaborative-filtering-on-movielens-1m, explainable-recommendation-on-movielens-1m, knowledge-graph-completion-on-movielens-1m, collaborative-filtering-on-movielens-20m, recommendation-systems-item-cold-start-on, multi-media-recommendation-on-movielens-10m, click-through-rate-prediction-on-movielens-1, multi-media-recommendation-on-movielens, sequential-recommendation-on-movielens-1m, link-prediction-on-movielens-1m, click-through-rate-prediction-on-movielens-1m, movie-recommendation-on-movielens-1m, multibehavior-recommendation-on-movielens, click-through-rate-prediction-on-movielens",Custom,https://grouplens.org/datasets/movielens/,https://paperswithcode.com/dataset/movielens,"The MovieLens datasets, first released in 1998, describe people’s expressed preferences for movies. These preferences take the form of tuples, each the result of a person expressing a preference (a 0-5 star rating) for a movie at a particular time. These preferences were entered by way of the MovieLens web site1 — a recommender system that asks its users to give movie ratings in order to receive personalized movie recommendations.",1998,The MovieLens Datasets: History and Context,http://files.grouplens.org/papers/harper-tiis2015.pdf,,,
2029,MovieNet,Video Understanding,Video Understanding,"Video Understanding, Person Search, Audio Generation, Scene Segmentation","Audio, Image, Text, Video",English,Computer Vision,scene-segmentation-on-movienet,,https://movienet.github.io/,https://paperswithcode.com/dataset/movienet,"MovieNet is a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc.. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92 K tags of cinematic style.",,,,,,
2030,MoviePlotEvents,Story Generation,Story Generation,Story Generation,Text,English,Natural Language Processing,,,https://www.dropbox.com/s/i5dsk92735jpunf/EventRepresentationDataRelease.zip?dl=0,https://paperswithcode.com/dataset/movieplotevents,"A version of the CMU Movie Summary Corpus (http://www.cs.cmu.edu/~ark/personas/), which was originally scraped from plot summaries from Wikipedia, with some cleaning and sentences turned into events & sorted into ""genres"" (via LDA).",,,,,,
2031,MovieQA,Video Story QA,Video Story QA,"Video Story QA, Video Question Answering","Text, Video",English,Natural Language Processing,video-story-qa-on-movieqa,,http://movieqa.cs.toronto.edu/home/,https://paperswithcode.com/dataset/movieqa,"The MovieQA dataset is a dataset for movie question answering. to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct. The questions can be answered using multiple sources of information: movie clips, plots, subtitles, and for a subset scripts and DVS.",,Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents,https://arxiv.org/abs/1804.09412,,,
2032,Moving_Symbols,Video Prediction,Video Prediction,"Video Prediction, Predict Future Video Frames","Time Series, Video",,Methodology,,,https://github.com/rszeto/moving-symbols,https://paperswithcode.com/dataset/moving-symbols,A parameterized synthetic dataset called Moving Symbols to support the objective study of video prediction networks.,,,,,,
2033,MP-IDB,Malaria Vivax Detection,Malaria Vivax Detection,"Malaria Vivax Detection, Malaria Falciparum Detection, Malaria Ovale Detection, Medical Object Detection, Medical Image Analysis, Malaria Malariae Detection, 2D Object Detection, medical image detection",Image,,Computer Vision,"malaria-ovale-detection-on-mp-idb, malaria-falciparum-detection-on-mp-idb, malaria-malariae-detection-on-mp-idb, malaria-vivax-detection-on-mp-idb",,https://github.com/andrealoddo/MP-IDB-The-Malaria-Parasite-Image-Database-for-Image-Processing-and-Analysis,https://paperswithcode.com/dataset/mp-idb,"MP-IDB comprises four species of Malaria parasites: Falciparum, Malariae, Ovale, Vivax. For each species, there are four distinct stages of life, described in the filenames as follows:

R: indicates the presence of 1 or more Ring stage parasites
T: indicates the presence of 1 or more Trophozoite stage parasites
S: indicates the presence of 1 or more Schizont stage parasites
G: indicates the presence of 1 or more Gametocyte stage parasites.
Each original image has its own ground-truth, provided by expert pathologists, while every image has been analyzed from left to right, therefore every filename indicates the presence of the parasites following this indication.",,,,,,
2034,MPHOI-72,Pose Estimation,Pose Estimation,"Pose Estimation, Human-Object-interaction motion tracking, Human Activity Recognition, Object Tracking, Action Recognition, Activity Recognition, Human-Object Interaction Detection","3D, Image, Video",,Computer Vision,,,https://github.com/tanqiu98/2G-GCN,https://paperswithcode.com/dataset/mphoi-72,"MPHOI-72 is a multi-person human-object interaction dataset that can be used for a wide variety of HOI/activity recognition and pose estimation/object tracking tasks. The dataset is challenging due to many body occlusions among the humans and objects. It consists of 72 videos captured from 3 different angles at 30 fps, with totally 26,383 frames and an average length of 12 seconds. It involves 5 humans performing in pairs, 6 object types, 3 activities and 13 sub-activities. The dataset includes color video, depth video, human skeletons, human and object bounding boxes.",,,,,,
2035,MPI-INF-3DHP,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Cross-domain 3D Human Pose Estimation, Multi-view 3D Human Pose Estimation, Weakly-supervised 3D Human Pose Estimation, Multi-Hypotheses 3D Human Pose Estimation, Unsupervised 3D Human Pose Estimation, Pose Retrieval","3D, Image",,Computer Vision,"multi-view-3d-human-pose-estimation-on-mpi, cross-domain-3d-human-pose-estimation-on-mpi, pose-retrieval-on-mpi-inf-3dhp, multi-hypotheses-3d-human-pose-estimation-on-1, 3d-human-pose-estimation-on-mpi-inf-3dhp, weakly-supervised-3d-human-pose-estimation-on-1, unsupervised-3d-human-pose-estimation-on-mpi",Custom (non-commercial),http://gvv.mpi-inf.mpg.de/3dhp-dataset/,https://paperswithcode.com/dataset/mpi-inf-3dhp,MPI-INF-3DHP is a 3D human body pose estimation dataset consisting of both constrained indoor and complex outdoor scenes. It records 8 actors performing 8 activities from 14 camera views. It consists on >1.3M frames captured from the 14 cameras.,,Anatomy-aware 3D Human Pose Estimation in Videos,https://arxiv.org/abs/2002.10322,,,
2036,MPI3D_Disentanglement,Disentanglement,Disentanglement,Disentanglement,,,Methodology,,,https://github.com/rr-learning/disentanglement_dataset,https://paperswithcode.com/dataset/mpi3d-disentanglement,"A data-set which consists of over one million images of physical 3D objects with seven factors of variation, such as object color, shape, size and position.",,,,,,
2037,MPII,Multi-Person Pose Estimation,Multi-Person Pose Estimation,"Multi-Person Pose Estimation, Pose Estimation, Temporal Action Localization, Keypoint Detection","3D, Image, Time Series, Video",English,Computer Vision,"pose-estimation-on-mpii-single-person, multi-person-pose-estimation-on-mpii-multi, keypoint-detection-on-mpii-multi-person, pose-estimation-on-mpii",Simplified BSD,http://human-pose.mpi-inf.mpg.de/,https://paperswithcode.com/dataset/mpii,"The MPII Human Pose Dataset for single person pose estimation is composed of about 25K images of which 15K are training samples, 3K are validation samples and 7K are testing samples (which labels are withheld by the authors). The images are taken from YouTube videos covering 410 different human activities and the poses are manually annotated with up to 16 body joints.",,2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning,https://arxiv.org/abs/1802.09232,25K images,"training samples, 3K are validation samples",
2038,MPIIGaze,Style Transfer,Style Transfer,"Style Transfer, Gaze Estimation",Image,English,Computer Vision,"gaze-estimation-on-mpii-gaze, gaze-estimation-on-mpiigaze-1",CC BY-NC-SA 4.0,https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild,https://paperswithcode.com/dataset/mpiigaze,"MPIIGaze is a dataset for appearance-based gaze estimation in the wild. It contains 213,659 images collected from 15 participants during natural everyday laptop use over more than three months. It has a large variability in appearance and illumination.",,,,659 images,,
2039,MPI_FAUST_Dataset,Superpixel Image Classification,Superpixel Image Classification,"Superpixel Image Classification, Gaussian Processes",Image,,Computer Vision,,,http://faust.is.tue.mpg.de/,https://paperswithcode.com/dataset/mpi-faust-dataset,Contains 300 scans of 10 people in a wide range of poses together with an evaluation methodology.,,,,,,
2040,MPI_Sintel,Video Prediction,Video Prediction,"Video Prediction, Optical Flow Estimation, Intrinsic Image Decomposition, Temporal View Synthesis, Style Transfer","Image, Time Series, Video",,Computer Vision,"optical-flow-estimation-on-sintel-clean-2, temporal-view-synthesis-on-mpi-sintel, optical-flow-estimation-on-sintel-final-2, optical-flow-estimation-on-sintel-clean, video-prediction-on-mpi-sintel, optical-flow-estimation-on-sintel-final",,http://sintel.is.tue.mpg.de/,https://paperswithcode.com/dataset/mpi-sintel,MPI (Max Planck Institute) Sintel is a dataset for optical flow evaluation that has 1064 synthesized stereo images and ground truth data for disparity. Sintel is derived from open-source 3D animated short film Sintel. The dataset has 23 different scenes. The stereo images are RGB while the disparity is grayscale. Both have resolution of 1024×436 pixels and 8-bit per channel.,,Fast Disparity Estimation using Dense Networks*,https://arxiv.org/abs/1805.07499,,valuation that has 1064 synthesized stereo images,
2041,Mpm-Verse-Large,Plasticine3D,Plasticine3D,"Plasticine3D, Water3D_Long, 3D geometry, Sand3D_Long, 3D Generation","3D, Text",English,Natural Language Processing,,MIT,https://huggingface.co/datasets/hrishivish23/MPM-Verse-MaterialSim-Large,https://paperswithcode.com/dataset/mpm-verse-large,"This dataset contains Material-Point-Method (MPM) simulations for various materials, including water, sand, plasticine,  jelly, and rigid collisions. Each material is represented as point-clouds that evolve over time. The dataset is designed for learning and predicting MPM-based physical simulations. Each material contains 50 trajectories with different initial velocity field.",,,,,,
2042,MPM-Verse,Goop2D,Goop2D,"Goop2D, Plasticine3D, MultiMaterial2D, Sand, Generating 3D Point Clouds, Water3D_Long, WaterDrop2D, 3D geometry, Elasticity3D, Sand2D, Sand3D_Long",3D,,Methodology,"sand3d-long-on-mpm-verse, goop2d-on-mpm-verse, sand-on-mpm-verse, multimaterial2d-on-mpm-verse, elasticity3d-on-mpm-verse, waterdrop2d-on-mpm-verse, water3d-long-on-mpm-verse, sand2d-on-mpm-verse, plasticine3d-on-mpm-verse",,https://huggingface.co/datasets/hrishivish23/MPM-Verse-MaterialSim-Small,https://paperswithcode.com/dataset/mpm-verse,"This dataset contains Material-Point-Method (MPM) simulations for various materials, including water, sand, plasticine, elasticity, jelly, rigid collisions, and melting. Each material is represented as point-clouds that evolve over time. The dataset is designed for learning and predicting MPM-based physical simulations.",,,,,,
2043,MPQA_Opinion_Corpus,Opinion Mining,Opinion Mining,"Opinion Mining, Sentiment Analysis, Fine-Grained Opinion Analysis, Keyword Extraction, Document Classification","Image, Text",English,Computer Vision,"sentiment-analysis-on-mpqa, document-classification-on-mpqa, fine-grained-opinion-analysis-on-mpqa",Custom (research-only),https://mpqa.cs.pitt.edu/,https://paperswithcode.com/dataset/mpqa-opinion-corpus,"The MPQA Opinion Corpus contains 535 news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.).",,http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf,http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf,,,
2044,MQ2007,Learning-To-Rank,Learning-To-Rank,"Learning-To-Rank, Information Retrieval, Meta-Learning",,,Methodology,,Custom,https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/,https://paperswithcode.com/dataset/mq2007,"The MQ2007 dataset consists of queries, corresponding retrieved documents and labels provided by human experts. The possible relevance labels for each document are “relevant”, “partially relevant”, and “not relevant”.",,ARSM GRADIENT ESTIMATOR FOR SUPERVISED LEARNING TO RANK,https://arxiv.org/abs/1911.00465,,,
2045,MQ2008,Learning-To-Rank,Learning-To-Rank,"Learning-To-Rank, Information Retrieval, Document Ranking",Text,English,Natural Language Processing,,Custom,https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/,https://paperswithcode.com/dataset/mq2008,The MQ2008 dataset is a dataset for Learning to Rank. It contains 800 queries with labelled documents.,,,,,,
2046,MRNet,Data Augmentation,Data Augmentation,"Data Augmentation, Multi-Label Classification",Image,,Computer Vision,multi-label-classification-on-mrnet,,https://stanfordmlgroup.github.io/competitions/mrnet/,https://paperswithcode.com/dataset/mrnet,"The MRNet dataset consists of 1,370 knee MRI exams performed at Stanford University Medical Center. The dataset contains 1,104 (80.6%) abnormal exams, with 319 (23.3%) ACL tears and 508 (37.1%) meniscal tears; labels were obtained through manual extraction from clinical reports.",,,,,,
2047,MRPC,Semantic Textual Similarity,Semantic Textual Similarity,"Semantic Textual Similarity, Semantic Textual Similarity within Bi-Encoder, Natural Language Inference, Few-Shot Learning",Text,English,Natural Language Processing,"semantic-textual-similarity-on-mrpc, semantic-textual-similarity-within-bi-encoder, natural-language-inference-on-mrpc, semantic-textual-similarity-on-mrpc-dev, few-shot-learning-on-mrpc",,https://www.microsoft.com/en-us/download/details.aspx?id=52398,https://paperswithcode.com/dataset/mrpc,"Microsoft Research Paraphrase Corpus (MRPC) is a corpus consists of 5,801 sentence pairs collected from newswire articles. Each pair is labelled if it is a paraphrase or not by human annotators. The whole set is divided into a training subset (4,076 sentence pairs of which 2,753 are paraphrases) and a test subset (1,725 pairs of which 1,147 are paraphrases).",,https://www.aclweb.org/anthology/I05-5002.pdf,https://www.aclweb.org/anthology/I05-5002.pdf,,,
2048,MRQA,Machine Reading Comprehension,Machine Reading Comprehension,"Machine Reading Comprehension, Question Answering, Reading Comprehension",Text,English,Natural Language Processing,question-answering-on-mrqa-2019,,https://mrqa.github.io/2019/shared.html,https://paperswithcode.com/dataset/mrqa-2019,The MRQA (Machine Reading for Question Answering) dataset is a dataset for evaluating the generalization capabilities of reading comprehension systems.,,,,,,
2049,MS-Celeb-1M,Face Verification,Face Verification,"Face Verification, Face Identification, Face Recognition",Image,,Computer Vision,,,https://exposing.ai/msceleb/,https://paperswithcode.com/dataset/ms-celeb-1m,"The MS-Celeb-1M dataset is a large-scale face recognition dataset consists of 100K identities, and each identity has about 100 facial images. The original identity labels are obtained automatically from webpages.

NOTE: This dataset is currently inactive.",,Learning to Cluster Faces on an Affinity Graph,https://arxiv.org/abs/1904.02749,,,
2050,MS-CXR,Phrase Grounding,Phrase Grounding,Phrase Grounding,,,Methodology,,PhysioNet Credentialed Health Data License 1.5.0,https://physionet.org/content/ms-cxr/,https://paperswithcode.com/dataset/ms-cxr,"The MS-CXR dataset provides 1162 image–sentence pairs of bounding boxes and corresponding phrases, collected across eight different cardiopulmonary radiological findings, with an approximately equal number of pairs for each finding. This dataset complements the existing MIMIC-CXR v.2 dataset and comprises: 1. Reviewed and edited bounding boxes and phrases (1026 pairs of bounding box/sentence); and 2. Manual bounding box labels from scratch (136 pairs of bounding box/sentence).e",,,,,,
2051,MS-FIMU,Privacy Preserving Deep Learning,Privacy Preserving Deep Learning,"Privacy Preserving Deep Learning, Classification",Image,,Computer Vision,,MIT,https://github.com/hharcolezi/OpenMSFIMU,https://paperswithcode.com/dataset/ms-fimu,"Open Dataset: Mobility Scenario FIMU


An open, multidimensional (6 categorical attributes), and synthetic dataset of faked virtual humans generated by an optimization approach applied to a real-life call-detail-records-based anonymized database.
The original database is from a mobile network operator in France (Orange), which collects statistics on the frequency of users on days and union of days through analyzing mobile phone data (i.e., call detail records). The period of the analysis is 7 days from 2017-05-31 to 2017-06-06.
This dataset can be used for classification tasks and for evaluating (locally) differentially private mechanisms on multidimensional data.",2017,,,,,
2052,MSAW,The Semantic Segmentation Of Remote Sensing Imagery,The Semantic Segmentation Of Remote Sensing Imagery,"The Semantic Segmentation Of Remote Sensing Imagery, Disaster Response",Image,,Computer Vision,the-semantic-segmentation-of-remote-sensing-1,,https://spacenet.ai/,https://paperswithcode.com/dataset/msaw,"Multi-Sensor All Weather Mapping (MSAW) is a dataset and challenge, which features two collection modalities (both SAR and optical). The dataset and challenge focus on mapping and building footprint extraction using a combination of these data sources. MSAW covers 120 km^2 over multiple overlapping collects and is annotated with over 48,000 unique building footprints labels, enabling the creation and evaluation of mapping algorithms for multi-modal data.",,SpaceNet 6: Multi-Sensor All Weather Mapping Dataset,https://arxiv.org/pdf/2004.06500,,,
2053,MSC,Starcraft II,Starcraft II,"Starcraft II, Real-Time Strategy Games, Starcraft",,,Methodology,,,https://github.com/wuhuikai/MSC,https://paperswithcode.com/dataset/msc,"MSC is a dataset for Macro-Management in StarCraft 2 based on the platfrom SC2LE. It consists of well-designed feature vectors, pre-defined high-level actions and final result of each match. It contains 36,619 high quality replays, which are unbroken and played by relatively professional players.",,,,,,
2054,MSCOCO,Cross-Modal Retrieval,Cross-Modal Retrieval,"Cross-Modal Retrieval, Real-time Instance Segmentation, Image Captioning, mage-to-Text Retrieval, Paraphrase Generation, Image Outpainting, Weakly Supervised Object Detection, Multi-Label Image Classification, Image Retrieval, Zero-Shot Object Detection, Object Detection, Few Shot Open Set Object Detection, Open Vocabulary Object Detection","Image, Text",English,Computer Vision,"image-retrieval-on-mscoco, open-vocabulary-object-detection-on-mscoco, image-outpainting-on-mscoco, multi-label-image-classification-on-mscoco, image-captioning-on-mscoco-1, object-detection-on-mscoco-6, zero-shot-object-detection-on-mscoco, real-time-instance-segmentation-on-mscoco, few-shot-open-set-object-detection-on-mscoco, object-detection-on-mscoco-7, paraphrase-generation-on-mscoco, mage-to-text-retrieval-on-mscoco, weakly-supervised-object-detection-on-mscoco, cross-modal-retrieval-on-mscoco",,,https://paperswithcode.com/dataset/mscoco,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
2055,MSD,Music Auto-Tagging,Music Auto-Tagging,"Music Auto-Tagging, Recommendation Systems",Audio,,Audio,"collaborative-filtering-on-million-song, music-auto-tagging-on-million-song-dataset",,http://millionsongdataset.com/,https://paperswithcode.com/dataset/msd,"The Million Song Dataset is a freely-available collection of audio features and metadata for a million contemporary popular music tracks.

The core of the dataset is the feature analysis and metadata for one million songs, provided by The Echo Nest. The dataset does not include any audio, only the derived features. Note, however, that sample audio can be fetched from services like 7digital, using code provided by the authors.

Paper: The Million Song Dataset",,,,,,
2056,MSD__Mirror_Segmentation_Dataset_,Mirror Detection,Mirror Detection,"Mirror Detection, Image Segmentation",Image,,Computer Vision,image-segmentation-on-msd-mirror-segmentation,,,https://paperswithcode.com/dataset/msd-mirror,"We construct the first large-scale mirror dataset, named MSD. It includes 4, 018 pairs of images containing mirrors and their corresponding manually annotated masks.",,,,,,
2057,MSK,Skin Lesion Classification,Skin Lesion Classification,"Skin Lesion Classification, Lesion Classification, Domain Generalization",Image,,Computer Vision,,,https://challenge2019.isic-archive.com/data.html,https://paperswithcode.com/dataset/msk,The MSK dataset is a dataset for lesion recognition from the Memorial Sloan-Kettering Cancer Center. It is used as part of the ISIC lesion recognition challenges.,,https://arxiv.org/pdf/1710.05006.pdf,https://arxiv.org/pdf/1710.05006.pdf,,,
2058,MSL,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Time Series Anomaly Detection, Time Series","Image, Time Series",,Computer Vision,time-series-anomaly-detection-on-msl,,https://www.kaggle.com/datasets/patrickfleith/nasa-anomaly-detection-dataset-smap-msl,https://paperswithcode.com/dataset/msl,"This dataset contains expert-labeled telemetry anomaly data from the Mars Science Laboratory (MSL) rover, Curiosity.

Real spacecraft and curiosity rover anomalies for anomaly detection
Indications of telemetry anomalies can be found within previously mentioned ISA reports. All telemetry channels discussed in an individual ISA were reviewed to ensure that the anomaly was evident in the associated telemetry data, and specific anomalous time ranges were manually labeled for each channel. If multiple anomalous sequences and channels closely resembled each other, only one was kept for the experiment in order to create a diverse and balanced set. Anomalies were classified into two categories, point and contextual, to distinguish between anomalies that would likely be identified by properly set alarms or distance-based methods that ignore temporal information (point anomalies) and those that require more complex methodologies such as LSTMs or Hierarchical Temporal Memory (HTM) approaches to detect (contextual anomalies)

MSL:
TM Channels (27)
Total TM values (66,709)
Total anomalies (36)",,,,,,
2059,MSLR-WEB10K,Learning-To-Rank,Learning-To-Rank,"Learning-To-Rank, Question Answering, Information Retrieval",Text,English,Natural Language Processing,,Custom,https://www.microsoft.com/en-us/research/project/mslr/,https://paperswithcode.com/dataset/mslr-web10k,"The MSLR-WEB10K dataset consists of 10,000 search queries over the documents from search results. The data also contains the values of 136 features and a corresponding user-labeled relevance factor on a scale of one to five with respect to each query-document pair. It is a subset of the MSLR-WEB30K dataset.",,Dueling Bandits with Qualitative Feedback,https://arxiv.org/abs/1809.05274,,,
2060,MSLR_WEB30K,Document Ranking,Document Ranking,Document Ranking,Text,English,Natural Language Processing,,Custom,https://www.microsoft.com/en-us/research/project/mslr/,https://paperswithcode.com/dataset/mslr-web30k,"The datasets are machine learning data, in which queries and urls are represented by IDs. The datasets consist of feature vectors extracted from query-url pairs along with relevance judgment labels:

(1) The relevance judgments are obtained from a retired labeling set of a commercial web search engine (Microsoft Bing), which take 5 values from 0 (irrelevant) to 4 (perfectly relevant).

(2) The features are basically extracted by us, and are those widely used in the research community.

In the data files, each row corresponds to a query-url pair. The first column is relevance label of the pair, the second column is query id, and the following columns are features. The larger value the relevance label has, the more relevant the query-url pair is. A query-url pair is represented by a 136-dimensional feature vector.",,,,,,
2061,MSLS,Visual Place Recognition,Visual Place Recognition,Visual Place Recognition,Image,,Computer Vision,visual-place-recognition-on-msls,Creative Commons Attribution 4.0 International,https://www.mapillary.com/dataset/places,https://paperswithcode.com/dataset/msls,The largest and most diverse dataset for lifelong place recognition from image sequences in urban and suburban settings.,,,,,,
2062,MSMT17-C,Person Re-Identification,Person Re-Identification,"Person Re-Identification, Generalizable Person Re-identification",Image,,Computer Vision,person-re-identification-on-msmt17-c,,https://github.com/MinghuiChen43/CIL-ReID,https://paperswithcode.com/dataset/msmt17-c,"MSMT17-C is an evaluation set that consists of algorithmically generated corruptions applied to the MSMT17 test-set.  These corruptions consist of Noise: Gaussian, shot,
impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions.",,,,,,
2063,MSMT17,Unsupervised Person Re-Identification,Unsupervised Person Re-Identification,"Unsupervised Person Re-Identification, Person Re-Identification, Generalizable Person Re-identification",Image,,Computer Vision,"unsupervised-person-re-identification-on-3, unsupervised-person-re-identification-on-12, unsupervised-person-re-identification-on-7, generalizable-person-re-identification-on-20, person-re-identification-on-msmt17, unsupervised-person-re-identification-on-2",,http://www.pkuvmc.com/dataset.html,https://paperswithcode.com/dataset/msmt17,"MSMT17 is a multi-scene multi-time person re-identification dataset. The dataset consists of 180 hours of videos, captured by 12 outdoor cameras, 3 indoor cameras, and during 12 time slots. The videos cover a long period of time and present complex lighting variations, and it contains a large number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes.

The dataset can be requested at http://www.pkuvmc.com/dataset.html",,Unknown,http://www.pkuvmc.com/agreement/RELEASE_AGREEMENT-MSMT17.pdf,,,
2064,MSP-IMPROV,Emotion Classification,Emotion Classification,"Emotion Classification, Dominance Estimation, Speech Emotion Recognition, Emotion Recognition, Arousal Estimation, Valence Estimation","Audio, Image",,Computer Vision,"speech-emotion-recognition-on-msp-improv, arousal-estimation-on-msp-improv",Academic License,https://ecs.utdallas.edu/research/researchlabs/msp-lab/MSP-Improv.html,https://paperswithcode.com/dataset/msp-improv,"We present the MSP-IMPROV corpus, a multimodal emotional database, where the goal is to have control over lexical content and emotion while also promoting naturalness in the recordings. Studies on emotion perception often require stimuli with fixed lexical content, but that convey different emotions. These stimuli can also serve as an instrument to understand how emotion modulates speech at the phoneme level, in a manner that controls for coarticulation. Such audiovisual data are not easily available from natural recordings. A common solution is to record actors reading sentences that portray different emotions, which may not produce natural behaviors. We propose an alternative approach in which we define hypothetical scenarios for each sentence that are carefully designed to elicit a particular emotion. Two actors improvise these emotion-specific situations, leading them to utter contextualized, non-read renditions of sentences that have fixed lexical content and convey different emotions. We describe the context in which this corpus was recorded, the key features of the corpus, the areas in which this corpus can be useful, and the emotional content of the recordings. The paper also provides the performance for speech and facial emotion classifiers. The analysis brings novel classification evaluations where we study the performance in terms of inter-evaluator agreement and naturalness perception, leveraging the large size of the audiovisual database.",,Academic License,https://ecs.utdallas.edu/research/researchlabs/msp-lab/publications/AcademicLicense-MSP-IMPROV.pdf,,,
2065,MSP-Podcast,Emotion Recognition,Emotion Recognition,"Emotion Recognition, Speech Emotion Recognition","Audio, Image",,Computer Vision,"emotion-recognition-on-msp-podcast, speech-emotion-recognition-on-msp-podcast-2, speech-emotion-recognition-on-msp-podcast, speech-emotion-recognition-on-msp-podcast-1",,https://ecs.utdallas.edu/research/researchlabs/msp-lab/MSP-Podcast.html,https://paperswithcode.com/dataset/msp-podcast,"The MSP-Podcast corpus contains speech segments from podcast recordings which are perceptually annotated using crowdsourcing. The collection of this corpus is an ongoing process. Version 1.7 of the corpus has 62,140 speaking turns (100hrs).

Key features of this corpus:


We download available audio recordings with common license. We only use the podcasts that have less restrictive licenses, so we can modify, sell and distribute the corpus (you can use it for commercial product!). 
Most of the segments in a regular podcasts are neutral. We use machine learning techniques trained with available data to retrieve candidate segments. These segments are emotionally annotated with crowdsourcing. This approach allows us to spend our resources on speech segments that are likely to convey emotions.
We annotate categorical emotions and attribute based labels at the speaking turn label
This is an ongoing effort, where we currently have 62,140 speaking turns (100h). We collect approximately 10,000-13,000 new speaking turns per year. Our goal is to reach 400 hours.",,,,,,
2066,MSR-VTT,Zero-Shot Video Retrieval,Zero-Shot Video Retrieval,"Zero-Shot Video Retrieval, Video Captioning, Video Question Answering, Text-to-Video Generation, Text to Video Retrieval, Video Generation, Zero-Shot Video-Audio Retrieval, Video Retrieval","Audio, Image, Text, Video",English,Computer Vision,"video-retrieval-on-msr-vtt-1ka, video-generation-on-msr-vtt, zero-shot-video-retrieval-on-msr-vtt, text-to-video-retrieval-on-msr-vtt, text-to-video-generation-on-msr-vtt, video-question-answering-on-msr-vtt, video-captioning-on-msr-vtt-1, zero-shot-video-audio-retrieval-on-msr-vtt, video-retrieval-on-msr-vtt",,https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/,https://paperswithcode.com/dataset/msr-vtt,"MSR-VTT (Microsoft Research Video to Text) is a large-scale dataset for the open domain video captioning, which consists of 10,000 video clips from 20 categories, and each video clip is annotated with 20 English sentences by Amazon Mechanical Turks. There are about 29,000 unique words in all captions. The standard splits uses 6,513 clips for training, 497 clips for validation, and 2,990 clips for testing.",,Learning to Discretely Compose Reasoning Module Networksfor Video Captioning,https://arxiv.org/abs/2007.09049,,,20
2067,MSRA_CN_NER,NER,NER,"NER, Chinese Named Entity Recognition, Cross-Lingual NER, Chinese Word Segmentation","Image, Text",English,Computer Vision,"chinese-named-entity-recognition-on-msra, cross-lingual-ner-on-msra, chinese-word-segmentation-on-msra",,https://aclanthology.org/W06-0115/,https://paperswithcode.com/dataset/msra-cn-ner,"Simplified Chinese dataset for NER in The Third International Chinese Language Processing Bakeoff (2006), provided by Microsoft Research Asia (MSRA).",2006,,,,,
2068,MSRA_Hand,Hand Pose Estimation,Hand Pose Estimation,"Hand Pose Estimation, Pose Estimation, Stochastic Optimization","3D, Image",,Computer Vision,hand-pose-estimation-on-msra-hands,,https://jimmysuen.github.io/,https://paperswithcode.com/dataset/msra-hand,"MSRA Hands is a dataset for hand tracking. In total 6 subjects' right hands are captured using Intel's Creative Interactive Gesture Camera. Each subject is asked to make various rapid gestures in a 400-frame video sequence. To account for different hand sizes, a global hand model scale is specified for each subject: 1.1, 1.0, 0.9, 0.95, 1.1, 1.0 for subject 1~6, respectively.
The camera intrinsic parameters are: principle point = image center(160, 120), focal length = 241.42. The depth image is 320x240, each .bin file stores the depth pixel values in row scanning order, which are 320240 floats. The unit is millimeters. The bin file is binary and needs to be opened with std::ios::binary flag.
joint.txt file stores 400 frames x 21 hand joints per frame. Each line has 3 * 21 = 63 floats for 21 3D points in (x, y, z) coordinates. The 21 hand joints are: wrist, index_mcp, index_pip, index_dip, index_tip, middle_mcp, middle_pip, middle_dip, middle_tip, ring_mcp, ring_pip, ring_dip, ring_tip, little_mcp, little_pip, little_dip, little_tip, thumb_mcp, thumb_pip, thumb_dip, thumb_tip.
The corresponding *.jpg file is just for visualization of depth and ground truth joints.",,https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf,https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf,,,
2069,MSRVTT-QA,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Zero-Shot Video Question Answer, Zeroshot Video Question Answer, Video Question Answering, Visual Question Answering, Visual Question Answering (VQA)","Image, Text, Video",English,Computer Vision,"zeroshot-video-question-answer-on-msrvtt-qa, visual-question-answering-on-msrvtt-qa-1, zero-shot-learning-on-msrvtt-qa, video-question-answering-on-msrvtt-qa, zeroshot-video-question-answer-on-msrvtt-qa-1, visual-question-answering-on-msrvtt-qa-2",,https://github.com/xudejing/video-question-answering,https://paperswithcode.com/dataset/msrvtt-qa,"The MSR-VTT-QA dataset is a benchmark for the task of Visual Question Answering (VQA) on the MSR-VTT (Microsoft Research Video to Text) dataset. The MSR-VTT-QA benchmark is used to evaluate models on their ability to answer questions based on these videos. It's part of the tasks that this dataset is used for, along with Video Retrieval, Video Captioning, Zero-Shot Video Question Answering, Zero-Shot Video Retrieval, and Text-to-Video Generation.",,,,,,
2070,MSSD,Information Retrieval,Information Retrieval,"Information Retrieval, Music Information Retrieval, Sequential skip prediction","Audio, Time Series",,Audio,sequential-skip-prediction-on-mssd,,http://research.spotify.com/datasets/music-streaming-sessions,https://paperswithcode.com/dataset/mssd,"The Spotify Music Streaming Sessions Dataset (MSSD) consists of 160 million streaming sessions with associated user interactions, audio features and metadata describing the tracks streamed during the sessions, and snapshots of the playlists listened to during the sessions. 

This dataset enables research on important problems including how to model user listening and interaction behaviour in streaming, as well as Music Information Retrieval (MIR), and session-based sequential recommendations.",,,,,,
2071,MSU-MFSD,Face Anti-Spoofing,Face Anti-Spoofing,Face Anti-Spoofing,Image,,Computer Vision,face-anti-spoofing-on-msu-mfsd,,https://sites.google.com/site/huhanhomepage/download,https://paperswithcode.com/dataset/msu-mfsd,"The MSU-MFSD dataset contains 280 video recordings of genuine and attack faces. 35 individuals have participated in the development of this database with a total of 280 videos. Two kinds of cameras with different resolutions (720×480 and 640×480) were used to record the videos from the 35 individuals. For the real accesses, each individual has two video recordings captured with the Laptop cameras and Android, respectively. For the video attacks, two types of cameras, the iPhone and Canon cameras were used to capture high definition videos on each of the subject. The videos taken with Canon camera were then replayed on iPad Air screen to generate the HD replay attacks while the videos recorded by the iPhone mobile were replayed itself to generate the mobile replay attacks. Photo attacks were produced by printing the 35 subjects’ photos on A3 papers using HP colour printer. The recording videos with respect to the 35 individuals were divided into training (15 subjects with 120 videos) and testing (40 subjects with 160 videos) datasets, respectively.",,Enhance the Motion Cues for Face Anti-Spoofing using CNN-LSTM Architecture,https://arxiv.org/abs/1901.05635,,,
2072,MSU_BASED,Deblurring,Deblurring,Deblurring,,,Methodology,deblurring-on-based,,https://videoprocessing.ai/benchmarks/deblurring.html,https://paperswithcode.com/dataset/based,"Qualitative dataset with real blurred videos, created by using beam-splitter setup in lab environment",,,,,,
2073,MSU_FR_VQA_Database,Image Quality Assessment,Image Quality Assessment,"Image Quality Assessment, Video Quality Assessment","Image, Video",,Computer Vision,"video-quality-assessment-on-msu-video-quality-1, image-quality-assessment-on-msu-fr-vqa",,https://videoprocessing.ai/benchmarks/video-quality-metrics.html,https://paperswithcode.com/dataset/msu-video-quality-metrics-dataset,"The dataset was created for video quality assessment problem. It was formed with 36 clips from Vimeo, which were selected from 18,000+ open-source clips with high bitrate (license CCBY or CC0).

The clips include videos recorded by both professionals and amateurs. Almost half of the videos contain scene changes and high dynamism. Moreover, the synthetic to natural lightning ratio is approximately 1 to 3.

Content type: nature, sport, humans close up, gameplays, music videos, water stream or steam, CGI
Effects and distortions: shaking, slow-motion, grain/noisy, too dark/bright regions, macro shooting, captions (text), extraneous objects on the camera lens or just close to it
Resolution: 1920x1080 as the most popular modern video resolution (more in the future)
Format: yuv420p
FPS: 24, 25, 30, 39, 50, 60
Videos duration: mainly 10 seconds
Such content diversity helps simulate near-realistic conditions. The choice of videos collected for the benchmark dataset employed clustering in terms of space-time complexity to obtain a representative distribution.

For compression we used 40 codecs of 10 compression standards (H.264, AV1, H.265, VVC, etc.). Each video was compressed with 3 target bitrates: 1,000 Kbps, 2,000 Kbps, and 4,000 Kbps, and different real-life encoding modes: constant quality (CRF) and variable bitrate (VBR). The choice of bitrate range simplifies the subjective comparison procedure since the video quality is more difficult to distinguish visually at higher bitrates.

The subjective assessment involved pairwise comparisons using crowdsourcing service Subjectify.us. To increase the relevance of the results, each pair of videos received at least 10 responses from participants. In total, 766362 valid answers were collected from more than 10800 unique participants.",,,,,,
2074,MSU_NR_VQA_Database,Blind Image Quality Assessment,Blind Image Quality Assessment,"Blind Image Quality Assessment, Image Quality Assessment, Video Quality Assessment, No-Reference Image Quality Assessment","Image, Video",,Computer Vision,"image-quality-assessment-on-msu-nr-vqa, video-quality-assessment-on-msu-video-quality",,https://videoprocessing.ai/benchmarks/no-reference-video-quality-metrics.html,https://paperswithcode.com/dataset/msu-video-quality-metrics-benchmark,"The dataset was created for video quality assessment problem. It was formed with 36 clips from Vimeo, which were selected from 18,000+ open-source clips with high bitrate (license CCBY or CC0). 

The clips include videos recorded by both professionals and amateurs. Almost half of the videos contain scene changes and high dynamism. Moreover, the synthetic to natural lightning ratio is approximately 1 to 3.


Content type: nature, sport, humans close up, gameplays, music videos, water stream or steam, CGI
Effects and distortions: shaking, slow-motion, grain/noisy, too dark/bright regions, macro shooting, captions (text), extraneous objects on the camera lens or just close to it
Resolution: 1920x1080 as the most popular modern video resolution (more in the future)
Format: yuv420p
FPS: 24, 25, 30, 39, 50, 60
Videos duration: mainly 10 seconds

Such content diversity helps simulate near-realistic conditions.
The choice of videos collected for the benchmark dataset employed clustering in terms of space-time complexity to obtain a representative distribution.

For compression we used 40 codecs of 10 compression standards (H.264, AV1, H.265, VVC, etc.). Each video was compressed with 3 target bitrates: 1,000 Kbps, 2,000 Kbps, and 4,000 Kbps, and different real-life encoding modes: constant quality (CRF) and variable bitrate (VBR). The choice of bitrate range simplifies the subjective comparison procedure since the video quality is more difficult to distinguish visually at higher bitrates. 

The subjective assessment involved pairwise comparisons using crowdsourcing service Subjectify.us. To increase the relevance of the results, each pair of videos received at least 10 responses from participants. In total, 766362 valid answers were collected from more than 10800 unique participants.",,,,,,
2075,MSU_SR-QA_Dataset,Video Super-Resolution,Video Super-Resolution,"Video Super-Resolution, Image Super-Resolution, Image Quality Assessment, Video Quality Assessment, Super-Resolution","Image, Video",,Computer Vision,video-quality-assessment-on-msu-sr-qa-dataset,,https://videoprocessing.ai/benchmarks/super-resolution-metrics.html#home,https://paperswithcode.com/dataset/msu-sr-qa-dataset,"Our dataset was made of videos from MSU Video Upscalers Benchmark Dataset, MSU Video Super-Resolution Benchmark Dataset and MSU Super-Resolution for Video Compression Benchmark Dataset. Dataset consists of real videos (were filmed with 2 cameras), video games footages, movies, cartoons, dynamic ads. 

How we brought our dataset closer to completeness?
* The dataset covers a large number of use cases in the field of SR due to the large number of content types
* The dataset contains videos with completely different resolutions, FPS values: 8, 24, 25, 30, 60, as well as high and low spatio-temporal complexity
* Distorted videos were obtained using 46 SR methods, some of them were preprocessed with 5 codecs: aomenc, vvenc, x264, x265, uavs3es with different bitrates and qp values
* The dataset was manually checked for redundancy

Videos from benchmarks are FullHD video crops, since the subjective comparison was made on crops. Therefore, the resolution of all videos in the received dataset is low.",,,,,,
2076,MSU_Super-Resolution_for_Video_Compression,Video Super-Resolution,Video Super-Resolution,Video Super-Resolution,Video,,Methodology,video-super-resolution-on-msu-super-1,,https://videoprocessing.ai/benchmarks/super-resolution-for-video-compression.html,https://paperswithcode.com/dataset/msu-super-resolution-for-video-compression,"This is a dataset for a super-resolution task. The dataset contains 480x270 videos that were decoded with 6 different bitrates (100 - 4000 kbps) using 5 different codecs (H.264, H.265, H.266, AV1, and AVS3 standards). The dataset contains indoor and outdoor videos as well as animation. All videos have low SI/TI values and simple textures. It was made to minimize compression artifacts that may occur to make restoration of details possible.",,,,,,
2077,MSU_Video_Saliency_Prediction,Saliency Prediction,Saliency Prediction,"Saliency Prediction, Video Saliency Detection","Image, Time Series, Video",,Computer Vision,video-saliency-detection-on-msu-video,,https://videoprocessing.ai/benchmarks/video-saliency-prediction.html,https://paperswithcode.com/dataset/msu-video-saliency-prediction,"The dataset presents open high-resolution test clips set with different types of content: movie fragments, sport streams, live caption clips.  Used clips of 1920×1080 resolution and with duration from 13 to 38 seconds. And Performed reliable data collection from 50 observers (19–24 y. o.) using 500 Hz SMI iViewXTM Hi-Speed 1250 eye-tracker. Also used cross-fade which ensures the independence of the received fixations between different clips. The final ground-truth saliency map was estimated as a Gaussian mixture with centers at the fixation points. A standard deviation for the Gaussians equal to 120 was chosen (this value matches 8 angular degrees, which is known to be the sector of sharp vision).",1920,,,,,
2078,MSU_Video_Super_Resolution_Benchmark__Detail_Resto,Video Super-Resolution,Video Super-Resolution,Video Super-Resolution,Video,,Methodology,video-super-resolution-on-msu-vsr-benchmark,,https://videoprocessing.ai/benchmarks/video-super-resolution.html,https://paperswithcode.com/dataset/msu-vsr-benchmark,"This is a dataset for a video super-resolution task. The dataset contains the most complex content for the restoration task: faces, text, QR-codes, car numbers, unpatterned textures, small details. Videos include different types of motion and different types of degradation: bicubic interpolation (BI) and Gaussian blurring and downsampling (BD). The resolution of all input video sequences is 480x320.",,,,,,
2079,MSU_Video_Upscalers__Quality_Enhancement,Video Super-Resolution,Video Super-Resolution,Video Super-Resolution,Video,,Methodology,video-super-resolution-on-msu-video-upscalers,,https://videoprocessing.ai/benchmarks/video-upscalers.html,https://paperswithcode.com/dataset/msu-video-upscalers-quality-enhancement,The dataset aims to find the algorithms that produce the most visually pleasant image possible and generalize well to a broad range of content. It consists of 30 clips and contains 15 2D-animated segments losslessly recorded from various video games and 15 camera-shot segments from high-bitrate YUV444 sources. The complexity of clips varies significantly in terms of spatial and temporal indexes. Multiple bicubic downscaling mixed with sharpening is used to simulate complex real-world camera degradation. The authors used slight compression and YUV420 conversion to simulate a practical use case. 1920×1080 sources were downscaled to 480×270 input.,1920,,,,,
2080,MSVD-QA,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Zero-Shot Video Question Answer, Zeroshot Video Question Answer, Video Question Answering, Visual Question Answering, Visual Question Answering (VQA)","Image, Text, Video",English,Computer Vision,"video-question-answering-on-msvd-qa, visual-question-answering-on-msvd-qa-1, zero-shot-learning-on-msvd-qa, zeroshot-video-question-answer-on-msvd-qa-1, zeroshot-video-question-answer-on-msvd-qa, visual-question-answering-on-msvd-qa-2",,https://github.com/xudejing/video-question-answering,https://paperswithcode.com/dataset/msvd-qa,"The MSVD-QA dataset is a Video Question Answering (VideoQA) dataset. It is based on the existing Microsoft Research Video Description (MSVD) dataset, which consists of about 120K sentences describing more than 2,000 video snippets. In the MSVD-QA dataset, Question-Answer (QA) pairs are generated from these descriptions. The dataset is mainly used in video captioning experiments but due to its large data size, it is also used for VideoQA. It contains 1970 video clips and approximately 50.5K QA pairs.",1970,,,120K sentences,,
2081,MS_2,Scientific Document Summarization,Scientific Document Summarization,"Scientific Document Summarization, Multi-Document Summarization",Text,English,Natural Language Processing,multi-document-summarization-on-ms-2,,https://github.com/allenai/ms2,https://paperswithcode.com/dataset/ms-2,"MS^2 (Multi-Document Summarization of Medical Studies) is a dataset of over 470k documents and 20k summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is one of the first large-scale, publicly available multi-document summarization dataset in the biomedical domain.",,,,470k documents,,
2082,MS_COCO,Image Captioning,Image Captioning,"Image Captioning, Box-supervised Instance Segmentation, Object Localization, Multi-object discovery, Object Proposal Generation, Multi-Person Pose Estimation, Multi-Label Classification, Question Generation, Image Retrieval, Weakly-supervised instance segmentation, Multi-Label Learning, Region Proposal, Active Object Detection, Few Shot Open Set Object Detection, Question Answering, Visual Question Answering (VQA), Zero-Shot Cross-Modal Retrieval, Zero-shot Text-to-Image Retrieval, Instance Segmentation, mage-to-Text Retrieval, Robust Object Detection, Unsupervised Object Localization, Point-Supervised Instance Segmentation, Interactive Segmentation, Quantization, Multi-Label Image Classification, Semi Supervised Learning for Image Captioning, Zero-Shot Object Detection, Real-time Instance Segmentation, Activeness Detection, Few-Shot Object Detection, Cross-Modal Retrieval, One-Shot Object Detection, Generalized Zero-Shot Object Detection, Zero-Shot Composed Image Retrieval (ZS-CIR), Semantic Segmentation, Open World Object Detection, Unsupervised Semantic Segmentation with Language-image Pre-training, Image Outpainting, Weakly Supervised Object Detection, One-Shot Instance Segmentation, Visual Question Answering, Object Counting, Object Detection, Open Vocabulary Object Detection, Pose Estimation, Real-Time Object Detection, Keypoint Detection, Scene Graph Generation, Paraphrase Generation, Knowledge Distillation, Panoptic Segmentation, Homography Estimation, Single-object discovery, Image-to-Text Retrieval, Conditional Image Generation, Image-level Supervised Instance Segmentation, Unsupervised Semantic Segmentation, Text-to-Image Generation, Layout-to-Image Generation","3D, Graph, Image, Text",English,Computer Vision,"visual-question-answering-on-coco-visual-4, open-vocabulary-object-detection-on-mscoco, instance-segmentation-on-coco, image-captioning-on-mscoco-1, pose-estimation-on-coco-minival, visual-question-answering-on-coco-visual-1, semi-supervised-learning-for-image-captioning, few-shot-object-detection-on-ms-coco-10-shot, text-to-image-generation-on-ms-coco, multi-person-pose-estimation-on-coco-test-dev, object-detection-on-coco-1, question-generation-on-coco-visual-question, keypoint-detection-on-coco, image-to-text-retrieval-on-coco, visual-question-answering-on-coco, real-time-instance-segmentation-on-mscoco-1k, conditional-image-generation-on-coco-animals, cross-modal-retrieval-on-mscoco, interactive-segmentation-on-coco, pose-estimation-on-coco-test-dev, multi-person-pose-estimation-on-coco-minival, pose-estimation-on-ms-coco, object-detection-on-coco-5, keypoint-detection-on-coco-test-dev, scene-graph-generation-on-ms-coco, few-shot-object-detection-on-coco-2017, robust-object-detection-on-coco, object-detection-on-mscoco-6, paraphrase-generation-on-mscoco, zero-shot-cross-modal-retrieval-on-coco-2014, knowledge-distillation-on-coco, instance-segmentation-on-coco-minival, zero-shot-composed-image-retrieval-zs-cir-on-4, one-shot-instance-segmentation-on-coco, zero-shot-text-to-image-retrieval-on-ms-coco, object-detection-on-coco, quantization-on-coco, zero-shot-object-detection-on-mscoco, weakly-supervised-object-detection-on-mscoco, unsupervised-semantic-segmentation-on-coco-1, pose-estimation-on-coco, image-captioning-on-coco, semantic-segmentation-on-coco-1, instance-segmentation-on-coco-minval, weakly-supervised-instance-segmentation-on-2, image-captioning-on-ms-coco, object-counting-on-coco-count-test, one-shot-object-detection-on-coco, multi-object-discovery-on-coco-20k, weakly-supervised-object-detection-on-coco, real-time-instance-segmentation-on-mscoco, text-to-image-generation-on-coco, image-retrieval-on-coco, image-retrieval-on-mscoco, layout-to-image-generation-on-coco-stuff-4, homography-estimation-on-coco-2014, panoptic-segmentation-on-coco-minival, weakly-supervised-object-detection-on-coco-2, multi-label-classification-on-ms-coco, cross-modal-retrieval-on-mscoco-1k, single-object-discovery-on-coco-20k, question-answering-on-coco-visual-question, mage-to-text-retrieval-on-mscoco, interactive-segmentation-on-coco-minival, object-detection-on-coco-minival, multi-person-pose-estimation-on-coco, unsupervised-semantic-segmentation-with-5, panoptic-segmentation-on-coco-test-dev, visual-question-answering-on-coco-visual-5, region-proposal-on-coco-test-dev, image-outpainting-on-mscoco, visual-question-answering-on-coco-visual-3, multi-label-image-classification-on-mscoco, open-world-object-detection-on-coco-2017-1, cross-modal-retrieval-on-coco-2014, object-detection-on-mscoco-7, visual-question-answering-on-coco-visual-2, activeness-detection-on-coco-test-dev, object-proposal-generation-on-coco, visual-question-answering-on-coco-visual, image-level-supervised-instance-segmentation-1, keypoint-detection-on-coco-test-challenge, box-supervised-instance-segmentation-on-coco, generalized-zero-shot-object-detection-on-ms, multi-label-learning-on-coco-2014, panoptic-segmentation-on-coco-panoptic, open-world-object-detection-on-coco-2017, open-world-object-detection-on-coco-2017-2, few-shot-open-set-object-detection-on-mscoco, active-object-detection-on-coco, real-time-object-detection-on-coco, pose-estimation-on-densepose-coco, point-supervised-instance-segmentation-on-1, unsupervised-object-localization-on-coco-20k, zero-shot-object-detection-on-ms-coco, object-detection-on-coco-2017",Custom,https://cocodataset.org/,https://paperswithcode.com/dataset/coco,"The MS COCO (Microsoft Common Objects in Context) dataset is a large-scale object detection, segmentation, key-point detection, and captioning dataset. The dataset consists of 328K images.

Splits:
The first version of MS COCO dataset was released in 2014. It contains 164K images split into training (83K), validation (41K) and test (41K) sets. In 2015 additional test set of 81K images was released, including all the previous test images and 40K new images.

Based on community feedback, in 2017 the training/validation split was changed from 83K/41K to 118K/5K. The new split uses the same images and annotations. The 2017 test set is a subset of 41K images of the 2015 test set. Additionally, the 2017 release contains a new unannotated dataset of 123K images.

Annotations:
The dataset has annotations for


object detection: bounding boxes and per-instance segmentation masks with 80 object categories,
captioning: natural language descriptions of the images (see MS COCO Captions),
keypoints detection: containing more than 200,000 images and 250,000 person instances labeled with keypoints (17 possible keypoints, such as left eye, nose, right hip, right ankle),
stuff image segmentation – per-pixel segmentation masks with 91 stuff categories, such as grass, wall, sky (see MS COCO Stuff),
panoptic: full scene segmentation, with 80 thing categories (such as person, bicycle, elephant) and a subset of 91 stuff categories (grass, sky, road),
dense pose: more than 39,000 images and 56,000 person instances labeled with DensePose annotations – each labeled person is annotated with an instance id and a mapping between image pixels that belong to that person body and a template 3D model.
The annotations are publicly available only for training and validation images.",2014,,,328K images,"split into training (83K), validation (41K) and test (41K) sets. In 2015 additional test set of 81K images",
2083,MS_MARCO,Passage Re-Ranking,Passage Re-Ranking,"Passage Re-Ranking, Information Retrieval, TREC 2019 Passage Ranking, Text Retrieval, Passage Ranking, Question Answering, Reading Comprehension, Passage Retrieval",Text,English,Natural Language Processing,"passage-ranking-on-ms-marco, passage-re-ranking-on-ms-marco, text-retrieval-on-ms-marco, information-retrieval-on-ms-marco, trec-2019-passage-ranking-on-msmarco, information-retrieval-on-msmarco, passage-retrieval-on-ms-marco-1, question-answering-on-ms-marco","Custom (research-only, non-commercial)",https://microsoft.github.io/msmarco/,https://paperswithcode.com/dataset/ms-marco,"The MS MARCO (Microsoft MAchine Reading Comprehension) is a collection of datasets focused on deep learning in search.
The first dataset was a question answering dataset featuring 100,000 real Bing questions and a human generated answer. Over time the collection was extended with a 1,000,000 question dataset, a natural language generation dataset, a passage ranking dataset, keyphrase extraction dataset, crawling dataset, and a conversational search.",,https://arxiv.org/pdf/1809.08267.pdf,https://arxiv.org/pdf/1809.08267.pdf,,,
2084,MT-Bench-TH,Chatbot,Chatbot,Chatbot,,,Methodology,,Apache-2.0 license,https://github.com/hy5468/TransLLM,https://paperswithcode.com/dataset/mt-bench-th,MT-Bench in Thai.,,,,,,
2085,MT-Bench,Text Generation,Text Generation,"Text Generation, Human Judgment Correlation","Image, Text",English,Computer Vision,text-generation-on-mt-bench,,,https://paperswithcode.com/dataset/mt-bench,"This dataset contains 3.3K expert-level pairwise human preferences for model responses generated by 6 models in response to 80 MT-bench questions. The 6 models are GPT-4, GPT-3.5, Claud-v1, Vicuna-13B, Alpaca-13B, and LLaMA-13B. The annotators are mostly graduate students with expertise in the topic areas of each of the questions.",,,,,,
2086,MTEB,Text Summarization,Text Summarization,"Text Summarization, Semantic Textual Similarity, Information Retrieval, Text Reranking, Text Pair Classification, Text Retrieval, STS, Text Clustering, Text Classification","Image, Text",English,Computer Vision,"text-clustering-on-mteb, text-summarization-on-mteb, information-retrieval-on-mteb, text-reranking-on-mteb, text-classification-on-mteb, text-retrieval-on-mteb, text-pair-classification-on-mteb, semantic-textual-similarity-on-mteb",Apache-2.0 license,https://github.com/embeddings-benchmark/mteb,https://paperswithcode.com/dataset/mteb,"MTEB is a benchmark that spans 8 embedding tasks covering a total of 56 datasets and 112 languages. The 8 task types are Bitext mining, Classification, Clustering, Pair Classification, Reranking, Retrieval, Semantic Textual Similarity and Summarisation. The 56 datasets contain varying text lengths and they are grouped into three categories: Sentence to sentence, Paragraph to paragraph, and Sentence to paragraph.

Check the latest leaderboards at HuggingFace.",,,,,,
2087,MTG-Jamendo,Scene Classification,Scene Classification,"Scene Classification, Music Information Retrieval, Acoustic Scene Classification","Audio, Image",,Computer Vision,,,https://mtg.github.io/mtg-jamendo-dataset/,https://paperswithcode.com/dataset/mtg-jamendo,"The MTG-Jamendo dataset is an open dataset for music auto-tagging. The dataset contains over 55,000 full audio tracks with 195 tags categories (87 genre tags, 40 instrument tags, and 56 mood/theme tags). It is built using music available at Jamendo under Creative Commons licenses and tags provided by content uploaders. All audio is distributed in 320kbps MP3 format.

A subset of the dataset is used in the Emotion and Theme Recognition in Music Task within MediaEval 2019.",2019,,,,,
2088,MTHS,Heart rate estimation,Heart rate estimation,"Heart rate estimation, SpO2 estimation, Photoplethysmography (PPG)",,,Methodology,"spo2-estimation-on-mths, heart-rate-estimation-on-mths",CC BY-NC-ND,https://github.com/MahdiFarvardin/MTVital,https://paperswithcode.com/dataset/mths,"the MTHS dataset contains 30Hz PPG signals obtained from
62 patients, including 35 men and 27 women. The ground truth
data includes heart rate and oxygen saturation levels sampled
at 1Hz. The HR and SPo2 measurement is obtained using a pulse oximeter (M70). An iPhone 5s was used to obtain the
ppg recordings at 30 fps.",,,,,,
2089,MTic,Time Offset Calibration,Time Offset Calibration,"Time Offset Calibration, Time Series Alignment",Time Series,,Methodology,,,https://github.com/madhavlab/2022_syncnet/tree/main/data/M_Tic,https://paperswithcode.com/dataset/mtic,Periodic Tic sounds (T0=1s) sampled at 16kHz with duration of nearly 10s.,,,,,,
2090,MUAD,Monocular Depth Estimation,Monocular Depth Estimation,"Monocular Depth Estimation, Semantic Segmentation, Uncertainty Visualization, Uncertainty Quantification, Anomaly Detection, Depth Aleatoric Uncertainty Estimation, Object Detection, Decision Making Under Uncertainty, Out of Distribution (OOD) Detection, Autonomous Driving","3D, Image",,Computer Vision,,,https://muad-dataset.github.io/,https://paperswithcode.com/dataset/muad,"The MUAD dataset (Multiple Uncertainties for Autonomous Driving), consisting of 10,413 realistic synthetic images with diverse adverse weather conditions (night, fog, rain, snow), out-of-distribution objects, and annotations for semantic segmentation, depth estimation, object, and instance detection. Predictive uncertainty estimation is essential for the safe deployment of Deep Neural Networks in real-world autonomous systems and MUAD allows to a better assess the impact of different sources of uncertainty on model performance.",,,,,,
2091,MUC-4,Role-filler Entity Extraction,Role-filler Entity Extraction,Role-filler Entity Extraction,,,Methodology,role-filler-entity-extraction-on-muc-4,,https://www-nlpir.nist.gov/related_projects/muc/muc_data/muc_data_index.html,https://paperswithcode.com/dataset/muc-4,A dataset for evaluate system's understanding of given passages.,,,,,,
2092,MuCeD,Object Detection,Object Detection,"Object Detection, Automatic Cell Counting",Image,,Computer Vision,,,https://github.com/dair-iitd/DeGPR,https://paperswithcode.com/dataset/muced,"MuCeD, a dataset that is carefully curated and validated by expert pathologists  from the All India Institute of Medical Science (AIIMS), Delhi, India. The H&E-stained histopathology images of the human duodenum in MuCeD are captured through an Olympus BX50 microscope at 20x zoom using a DP26 camera with each image being  1920x2148 in dimension. The dataset has 55 images, with bounding boxes for 2,090 IELs and 6,518 ENs annotated using the LabelMe software and are further validated by multiple pathologists. These cells are selected from the epithelial area -- a region of interest that has been explicitly segmented by experts. The epithelial area denotes the area of continuous villi and is used for cell detection, whereas rest of the area is masked out. Further, each image is sliced into 9 subimages and each subimage is re-scaled to 640x640, before it is given as input to object detection models. We divide 55 images into five folds of 11 images each and report 5-fold crossvalidation numbers. Within 44 training images in a given fold, 8 are used for validation and 36 for training.
Data is annotated in yolo format with labels are present in .txt files with class x, y, width,  heigh format.",,,,55 images,"validated by expert pathologists  from the All India Institute of Medical Science (AIIMS), Delhi, India. The H&E-stained histopathology images",
2093,MuCGEC,Grammatical Error Correction,Grammatical Error Correction,Grammatical Error Correction,,,Methodology,grammatical-error-correction-on-mucgec,Apache-2.0,https://github.com/HillZhang1999/MuCGEC,https://paperswithcode.com/dataset/mucgec,"MuCGEC is a multi-reference multi-source evaluation dataset for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences collected from three different Chinese-as-a-Second-Language (CSL) learner sources. Each sentence has been corrected by three annotators, and their corrections are meticulously reviewed by an expert, resulting in 2.3 references per sentence.",,,,063 sentences,,
2094,MuChoMusic,Music Question Answering,Music Question Answering,Music Question Answering,"Audio, Text",English,Natural Language Processing,,Creative Commons Attribution Share Alike 4.0 International,https://mulab-mir.github.io/muchomusic/,https://paperswithcode.com/dataset/muchomusic,"MuChoMusic is a benchmark designed to evaluate music understanding in multimodal language models focused on audio. It includes 1,187 multiple-choice questions validated by human annotators, based on 644 music tracks from two publicly available music datasets. These questions cover a wide variety of genres and assess knowledge and reasoning across several musical concepts and their cultural and functional contexts. The benchmark provides a holistic evaluation of five open-source models, revealing challenges such as over-reliance on the language modality and highlighting the need for better multimodal integration.",,,,,,
2095,MuCo-3DHP,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Pose Estimation, 3D Pose Estimation","3D, Image",,Computer Vision,,Custom (non-commercial),http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/,https://paperswithcode.com/dataset/muco-3dhp,MuCo-3DHP is a large scale training data set showing real images of sophisticated multi-person interactions and occlusions.,,Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB,https://arxiv.org/pdf/1712.03453v3.pdf,,,
2096,MuDoCo_QueryRewrite,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Context Query Reformulation, Sentence ReWriting",,,Methodology,,,https://github.com/apple/ml-cread,https://paperswithcode.com/dataset/mudoco-queryrewrite,"<Task description: joint learning of coreference resolution and query rewrite>

Given an ongoing dialogue between a user and a dialogue assistant, for the user query, the model is required to predict both coreference links between the query and the dialogue context, and the self-contained rewritten user query that is independent to the dialogue context.

<Dataset>

The MuDoCo dataset is a public dataset that contains 7.5k task-oriented multi-turn dialogues across 6 domains (calling, messaging, music, news, reminders, weather). Each dialogue turn is annotated with coreference links (links field). Please refer to the paper of the MuDoCo dataset for more details. Upon on the MuDoCo dataset, we annotate the query rewrite for each utterance, including both user and system turns.  More details are provided in https://github.com/apple/ml-cread.",,,,,,
2097,MuJoCo,Multivariate Time Series Forecasting,Multivariate Time Series Forecasting,"Multivariate Time Series Forecasting, Multivariate Time Series Imputation",Time Series,,Time Series,"multivariate-time-series-forecasting-on-1, multivariate-time-series-imputation-on-mujoco",Custom,https://www.mujoco.org/,https://paperswithcode.com/dataset/mujoco,MuJoCo (multi-joint dynamics with contact) is a physics engine used to implement environments to benchmark Reinforcement Learning methods.,,,,,,
2098,MuLD,Natural Language Understanding,Natural Language Understanding,"Natural Language Understanding, Summarization, Long-range modeling, Style change detection, Translation, Question Answering, Text Classification","Image, Text",English,Computer Vision,"text-classification-on-muld-character-type, style-change-detection-on-muld-style-change, translation-on-muld-opensubtitles, question-answering-on-muld-hotpotqa, question-answering-on-muld-narrativeqa, summarization-on-muld-vlsp",Custom,https://github.com/ghomashudson/muld,https://paperswithcode.com/dataset/muld,"MuLD (Multitask Long Document Benchmark) is a set of 6 NLP tasks where the inputs consist of at least 10,000 words. The benchmark covers a wide variety of task types including translation, summarization, question answering, and classification. Additionally there is a range of output lengths from a single word classification label all the way up to an output longer than the input text.",,,,,,
2099,Multi-Domain_Sentiment,Sentiment Analysis,Sentiment Analysis,Sentiment Analysis,Text,English,Natural Language Processing,sentiment-analysis-on-multi-domain-sentiment,,https://www.cs.jhu.edu/~mdredze/datasets/sentiment/,https://paperswithcode.com/dataset/multi-domain-sentiment-dataset-v2-0,The Multi-Domain Sentiment Dataset contains product reviews taken from Amazon.com from many product types (domains). Some domains (books and dvds) have hundreds of thousands of reviews. Others (musical instruments) have only a few hundred. Reviews contain star ratings (1 to 5 stars) that can be converted into binary labels if needed.,,,,,,
2100,Multi-dSprites,Image Generation,Image Generation,"Image Generation, Unsupervised Object Segmentation","Image, Text",English,Computer Vision,image-generation-on-multi-dsprites,,https://github.com/deepmind/multi_object_datasets,https://paperswithcode.com/dataset/multi-dsprites,,,,,,,
2101,Multi-Labelled_SMILES_Odors_dataset,Molecular Property Prediction,Molecular Property Prediction,"Molecular Property Prediction, Odor Descriptor Prediction",Time Series,,Methodology,odor-descriptor-prediction-on-multi-labelled,CC0: Public Domain,https://www.kaggle.com/datasets/aryanamitbarsainyan/multi-labelled-smiles-odors-dataset,https://paperswithcode.com/dataset/multi-labelled-smiles-odors-dataset,"This dataset is a multi-labelled SMILES odor dataset with 138 odor descriptors. This dataset was created for replicating the paper: A principal odor map unifies diverse tasks in olfactory perception.

The complete replication of the paper (dataset curation + model) can be found in the OpenPOM GitHub repository.

The dataset contains 4983 molecules, each described by multiple odor labels (e.g. creamy, grassy), was made by combining the GoodScents and Leffingwell PMP 2001 datasets each containing odorant molecules and corresponding odor descriptors.",2001,,,,,
2102,Multi-Label_Classification_Dataset_Repository,Multi-Label Learning,Multi-Label Learning,"Multi-Label Learning, Multi-Label Classification",Image,,Computer Vision,,Custom,http://www.uco.es/kdis/mllresources/,https://paperswithcode.com/dataset/multi-label-classification-dataset-repository,"For each dataset we provide a short description as well as some characterization metrics. It includes the number of instances (m), number of attributes (d), number of labels (q), cardinality (Card), density (Dens), diversity (Div), average Imbalance Ratio per label (avgIR), ratio of unconditionally dependent label pairs by chi-square test (rDep) and complexity, defined as m × q × d as in [Read 2010]. Cardinality measures the average number of labels associated with each instance, and density is defined as cardinality divided by the number of labels. Diversity represents the percentage of labelsets present in the dataset divided by the number of possible labelsets. The avgIR measures the average degree of imbalance of all labels, the greater avgIR, the greater the imbalance of the dataset. Finally, rDep measures the proportion of pairs of labels that are dependent at 99% confidence. A broader description of all the characterization metrics and the used partition methods are described in the MLDA documentation. We also used MLDA for the characterization and partitioning of the datasets.",2010,,,,,
2103,Multi-Modal_CelebA-HQ,Face Sketch Synthesis,Face Sketch Synthesis,"Face Sketch Synthesis, Text-to-Image Generation, multimodal generation, Image Generation","Image, Text",English,Computer Vision,"face-sketch-synthesis-on-multi-modal-celeba, multimodal-generation-on-multi-modal-celeba, text-to-image-generation-on-multi-modal",,https://github.com/weihaox/Multi-Modal-CelebA-HQ-Dataset,https://paperswithcode.com/dataset/multi-modal-celeba-hq-1,"Multi-Modal-CelebA-HQ is a large-scale face image dataset that has 30,000 high-resolution face images selected from the CelebA dataset by following CelebA-HQ. Each image has high-quality segmentation mask, sketch, descriptive text, and image with transparent background.

Multi-Modal-CelebA-HQ can be used to train and evaluate algorithms of text-to-image-generation, text-guided image manipulation, sketch-to-image generation, and GANs for face generation and editing.",,Xia et al,https://arxiv.org/pdf/2012.03308.pdf,,,
2104,Multi-News,Text Summarization,Text Summarization,"Text Summarization, Information Threading, Cross-Document Language Modeling, Document Summarization, Summarization, Multi-Document Summarization",Text,English,Natural Language Processing,"cross-document-language-modeling-on-multinews, cross-document-language-modeling-on-multinews-1, information-threading-on-multi-news, multi-document-summarization-on-multi-news, summarization-on-multi-news",Custom,https://github.com/Alex-Fabbri/Multi-News,https://paperswithcode.com/dataset/multi-news,"Multi-News, consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited.",,Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model,https://arxiv.org/pdf/1906.01749.pdf,,,
2105,Multi-PIE,Single-Image Portrait Relighting,Single-Image Portrait Relighting,Single-Image Portrait Relighting,Image,,Computer Vision,single-image-portrait-relighting-on-multi-pie,,http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html,https://paperswithcode.com/dataset/multi-pie,"The Multi-PIE (Multi Pose, Illumination, Expressions) dataset consists of face images of 337 subjects taken under different pose, illumination and expressions. The pose range contains 15 discrete views, capturing a face profile-to-profile. Illumination changes were modeled using 19 flashlights located in different places of the room.",,Hybrid VAE: Improving Deep Generative Models using Partial Observations,https://arxiv.org/abs/1711.11566,,,
2106,Multi-Spectral_Stereo_Dataset___RGB__NIR__thermal_,Depth Completion,Depth Completion,"Depth Completion, Depth Estimation, Stereo Depth Estimation, Visual Odometry, Depth Prediction, Thermal Image Segmentation","3D, Image, Time Series",,Computer Vision,,Creative Commons Attribution 4.0 International,https://sites.google.com/view/multi-spectral-stereo-dataset,https://paperswithcode.com/dataset/ms2-dataset-rgb-nir-thermal-images-lidar-gps,"Abstract: 
We introduce the multi-spectral stereo (MS2) outdoor dataset, including stereo RGB, stereo NIR, stereo thermal, stereo LiDAR data, and GPS/IMU information. Our dataset provides rectified and synchronized 184K data pairs taken from city, residential, road, campus, and suburban areas in the morning, daytime, and nighttime under clear-sky, cloudy, and rainy conditions. We designed the dataset to explore various computer vision algorithms from multi-spectral sensor data to achieve high-level performance, reliability, and robustness against challenging environments.

MS2 dataset provides:
* 1. (Synchronized) Stereo RGB images / Stereo NIR images / Stereo thermal images
* 2. (Synchronized) Stereo LiDAR scans / GPS/IMU navigation data
* 3. Projected depth map (in RGB, NIR, thermal image planes)
* 4. Odometry data (in RGB, NIR, thermal cameras, and LiDAR coordinates)",,,,,,
2107,Multi-XScience,Multi-Document Summarization,Multi-Document Summarization,"Multi-Document Summarization, Document Summarization",Text,English,Natural Language Processing,,,https://github.com/yaolu/Multi-XScience,https://paperswithcode.com/dataset/multi-xscience,"Multi-XScience is a large-scale dataset for multi-document summarization of scientific articles. It has 30,369, 5,066 and 5,093 samples for the train, validation and test split respectively. The average document length is 778.08 words and the average summary length is 116.44 words.",,,,093 samples,,
2108,Multi30K,Real-time Instance Segmentation,Real-time Instance Segmentation,"Real-time Instance Segmentation, Multimodal Machine Translation, Translation deu-eng, Translation eng-deu","Image, Text",English,Computer Vision,"translation-eng-deu-on-multi30k-test-2018, translation-eng-deu-on-multi30k-test-2016, real-time-instance-segmentation-on-multi30k, translation-deu-eng-on-multi30k-test-2017-1, multimodal-machine-translation-on-multi30k, translation-eng-deu-on-multi30k-test-2017-1, translation-eng-deu-on-multi30k-test-2017, translation-deu-eng-on-multi30k-test-2018, translation-deu-eng-on-multi30k-test-2017, translation-deu-eng-on-multi30k-test-2016",,https://github.com/multi30k/dataset,https://paperswithcode.com/dataset/multi30k,"Multi30K is a large-scale multilingual multimodal dataset for interdisciplinary machine learning research. It extends the Flickr30K dataset with German translations created by professional translators over a subset of the English descriptions, and descriptions crowdsourced independently of the original English descriptions. The dataset was introduced to stimulate multilingual multimodal research.",,,,,,
2109,MultiBypass140,Surgical Gesture Recognition,Surgical Gesture Recognition,"Surgical Gesture Recognition, Surgical phase recognition",Image,,Computer Vision,,,https://github.com/CAMMA-public/MultiBypass140,https://paperswithcode.com/dataset/multibypass140,"BernBypass70 is a dataset consisting of 70 surgical videos of LRYGB at Inselspital, Bern University Hospital, Switzerland. The videos were recorded at a resolution of 720 × 576 at 25 fps. 

StrasBypass70, extending the Bypass40 dataset, is a collection of 70 videos of LRYGB surgeries performed by surgeons at the University Hospital of Strasbourg, France. The videos were recorded at a resolution of 854 × 480 or 1920 × 1080 resolution at 25 fps and were uniformly edited to a resolution of 854 × 480. 

MultiBypass140 is the combined dataset of 140 videos from Bern and Strasbourg medical centers. All videos have been anonymized by blacking out the potentially identifying frames outside the patient’s body.",1920,,,,,
2110,MultiDoc2Dial,Dialogue Generation,Dialogue Generation,"Dialogue Generation, Conversational Question Answering, Goal-Oriented Dialogue Systems, Open-Domain Dialog, Question Answering",Text,English,Natural Language Processing,,Apache-2.0 License,https://doc2dial.github.io/multidoc2dial/,https://paperswithcode.com/dataset/multidoc2dial,"MultiDoc2Dial is a new task and dataset on modeling goal-oriented dialogues grounded in multiple documents. Most previous works treat document-grounded dialogue modeling as a machine reading comprehension task based on a single given document or passage. We aim to address more realistic scenarios where a goal-oriented information-seeking conversation involves multiple topics, and hence is grounded on different documents.",,,,,,
2111,MultiEURLEX,Document Classification,Document Classification,"Document Classification, Multilingual text classification","Image, Text",English,Computer Vision,,,https://github.com/nlpaueb/multi-eurlex,https://paperswithcode.com/dataset/multieurlex,"MultiEURLEX is a multilingual dataset for topic classification of legal documents. The dataset comprises 65k European Union (EU) laws, officially translated in 23 languages, annotated with multiple labels from the EUROVOC taxonomy. The dataset covers 23 official EU languages from 7 language families.",,,,,,
2112,MultiFC,Semantic Similarity,Semantic Similarity,"Semantic Similarity, Learning-To-Rank, Semantic Textual Similarity",,,Methodology,,,http://www.copenlu.com/publication/2019_emnlp_augenstein/,https://paperswithcode.com/dataset/multifc,"Publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists.",,,,,,
2113,Multilingual_Dataset_for_Training_and_Evaluating_D,Romanian Text Diacritization,Romanian Text Diacritization,"Romanian Text Diacritization, Slovak Text Diacritization, Croatian Text Diacritization, Vietnamese Text Diacritization, Irish Text Diacritization, Latvian Text Diacritization, Polish Text Diacritization, French Text Diacritization, Czech Text Diacritization, Spanish Text Diacritization, Turkish Text Diacritization, Hungarian Text Diacritization",Text,English,Natural Language Processing,"turkish-text-diacritization-on-multilingual, czech-text-diacritization-on-multilingual, slovak-text-diacritization-on-multilingual, french-text-diacritization-on-multilingual, polish-text-diacritization-on-multilingual, latvian-text-diacritization-on-multilingual, vietnamese-text-diacritization-on, irish-text-diacritization-on-multilingual, hungarian-text-diacritization-on-multilingual, croatian-text-diacritization-on-multilingual, spanish-text-diacritization-on-multilingual, romanian-text-diacritization-on-multilingual",CC BY-NC-SA 4.0,https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2607,https://paperswithcode.com/dataset/multilingual-dataset-for-training-and,"The dataset contains training and evaluation data for 12 languages:
- Vietnamese
- Romanian
- Latvian
- Czech
- Polish
- Slovak
- Irish
- Hungarian
- French
- Turkish
- Spanish
- Croatian

For each language, one training, one development and one testing set acquired from Wikipedia articles is provided. Moreover, each language dataset contains (substantially larger) training set collected from (general) Web texts. All sets, except for Wikipedia and Web training sets that can contain similar sentences, are disjoint. Data are segmented into sentences which are further word tokenized.",,,,,,
2114,Multimodal_Humor_Dataset,Humor Detection,Humor Detection,Humor Detection,Image,,Computer Vision,,,https://multimodal-humor-dataset.github.io/,https://paperswithcode.com/dataset/multimodal-humor-dataset,"A great number of situational comedies (sitcoms) are being regularly made and the task of adding laughter tracks to these is a critical task. Providing an ability to be able to predict whether something will be humorous to the audience is also crucial. In this project, we aim to automate this task. Towards doing so, we annotate an existing sitcom (Big Bang Theory') and use the laughter cues present to obtain a manual annotation for this show. We provide detailed analysis for the dataset design and further evaluate various state of the art baselines for solving this task. We observe that existing LSTM and BERT based networks on the text alone do not perform as well as joint text and video or only video-based networks. Moreover, it is challenging to ascertain that the words attended to while predicting laughter are indeed humorous. Our dataset and analysis provided through this paper is a valuable resource towards solving this interesting semantic and practical task. As an additional contribution, we have developed a novel model for solving this task that is a multi-modal self-attention based model that outperforms currently prevalent models for solving this task.",,,,,,
2115,Multimodal_Opinionlevel_Sentiment_Intensity,Multimodal Sentiment Analysis,Multimodal Sentiment Analysis,Multimodal Sentiment Analysis,Text,English,Multimodal,multimodal-sentiment-analysis-on-mosi,,https://arxiv.org/pdf/1606.06259.pdf,https://paperswithcode.com/dataset/multimodal-opinionlevel-sentiment-intensity,"Multimodal Opinionlevel Sentiment Intensity (MOSI) contains: (1) multimodal observations including transcribed speech and visual gestures as well as automatic audio and visual features, (2) opinion-level subjectivity segmentation, (3) sentiment intensity annotations with high coder agreement, and (4) alignment between words, visual and acoustic features.",,Zadeh et al,https://arxiv.org/pdf/1606.06259.pdf,,,
2116,MultiNLI,Natural Language Inference,Natural Language Inference,Natural Language Inference,Text,English,Natural Language Processing,"natural-language-inference-on-multinli-dev, natural-language-inference-on-multi-nli, natural-language-inference-on-multinli","Custom (multiple, see the paper)",https://cims.nyu.edu/~sbowman/multinli/,https://paperswithcode.com/dataset/multinli,"The Multi-Genre Natural Language Inference (MultiNLI) dataset has 433K sentence pairs. Its size and mode of collection are modeled closely like SNLI. MultiNLI offers ten distinct genres (Face-to-face, Telephone, 9/11, Travel, Letters, Oxford University Press, Slate, Verbatim, Goverment and Fiction) of written and spoken English data. There are matched dev/test sets which are derived from the same sources as those in the training set, and mismatched sets which do not closely resemble any seen at training time.",,Semantic Sentence Matching with Densely-connectedRecurrent and Co-attentive Information,https://arxiv.org/abs/1805.11360,,,
2117,MultiOFF,Meme Classification,Meme Classification,Meme Classification,Image,,Computer Vision,meme-classification-on-multioff,,,https://paperswithcode.com/dataset/multioff,Introudced from Multimodal Meme Dataset (MultiOFF) for Identifying Offensive Content in Image and Text,,,,,,
2118,MultiQ,Multi-hop Question Answering,Multi-hop Question Answering,"Multi-hop Question Answering, Logical Reasoning, Question Answering",Text,English,Natural Language Processing,question-answering-on-multiq,Apache-2.0,http://tape-benchmark.com/datasets.html#multiq,https://paperswithcode.com/dataset/multiq,"MultiQ is a multi-hop QA dataset for Russian, suitable for general open-domain question answering, information retrieval, and reading comprehension tasks.

Motivation

Question-answering has been an essential task in natural language processing and information retrieval. However, certain areas in QA remain quite challenging for modern approaches, including the multi-hop one, which is traditionally considered an intersection of graph methods, knowledge representation, and SOTA language modeling.

Multi-hop reasoning has been the least addressed QA direction for Russian. The task is represented by the MuSeRC dataset (Fenogenova et al., 2020) and only a few dozen questions in SberQUAD (Efimov et al., 2020) and RuBQ (Rybin et al., 2021). In response, we have developed a semi-automatic pipeline for multi-hop dataset generation based on Wikidata.

An example in English for illustration purposes:

```{
    'support_text': 'Gerard McBurney (b. June 20, 1954, Cambridge) is a British arranger, musicologist, television and radio presenter, teacher, and writer. He was born in the family of American archaeologist Charles McBurney and secretary Anna Frances Edmonston, who combined English, Scottish and Irish roots. Gerard's brother Simon McBurney is an English actor, writer, and director. He studied at Cambridge and the Moscow State Conservatory with Edison Denisov and Roman Ledenev.',
    'main_text': 'Simon Montague McBurney (born August 25, 1957, Cambridge) is an English actor, screenwriter, and director.
Biography.
Father is an American archaeologist who worked in the UK. Simon graduated from Cambridge with a degree in English Literature. After his father's death (1979) he moved to France, where he studied theater at the Jacques Lecoq Institute. In 1983 he created the theater company ""Complicity"". Actively works as an actor in film and television, and acts as a playwright and screenwriter.',

'question': 'Where was Gerard McBurney's brother born?',

'bridge_answers': [{'label': 'passage', 'length': 14, 'offset': 300, 'segment': 'Simon McBurney'}],

'main_answers': [{'label': 'passage', 'length': 9, 'offset': 47, 'segment': Cambridge'}],

'episode': [15],

'perturbation': 'multiq'

}```

Data Fields


question: a string containing the question text
support_text: a string containing the first text passage relating to the question
main_text: a string containing the main answer text
bridge_answers: a list of entities required to hop from the support text to the main text
main_answers: a list of answers to the question
perturbation: a string containing the name of the perturbation applied to text. If no perturbation was applied, the dataset name is used
episode: a list of episodes in which the instance is used. Only used for the train set

Data Splits

The dataset consists of a training set with labeled examples and a test set in two configurations:

raw data: includes the original data with no additional sampling
 - episodes: data is split into evaluation episodes and includes several perturbations of test for robustness evaluation Test and train data sets are disjoint with respect to individual  - questions, but may include overlaps in support and main texts.

Test Perturbations

Each training episode in the dataset corresponds to seven test variations, including the original test data and six adversarial test sets, acquired through the modification of the original test through the following text perturbations:


ButterFingers: randomly adds noise to data by mimicking spelling mistakes made by humans through character swaps based on their keyboard distance
Emojify: replaces the input words with the corresponding emojis, preserving their original meaning
EDAdelete: randomly deletes tokens in the text
EDAswap: randomly swaps tokens in the text
BackTranslation: generates variations of the context through back-translation (ru -> en -> ru)
AddSent: generates an extra sentence at the end of the text",2020,,,,,
2119,MultiScan,3D Instance Segmentation,3D Instance Segmentation,"3D Instance Segmentation, 3D Object Detection","3D, Image",,Computer Vision,3d-object-detection-on-multiscan,MIT,https://3dlg-hcvc.github.io/multiscan/#/,https://paperswithcode.com/dataset/multiscan,"We introduce MultiScan, a scalable RGBD dataset construction pipeline leveraging commodity mobile devices to scan indoor scenes with articulated objects and web-based semantic annotation interfaces to efficiently annotate object and part semantics and part mobility parameters. We use this pipeline to collect 273 scans of 117 indoor scenes containing 10957 objects and 5129 parts. The resulting MultiScan dataset provides RGBD streams with per-frame camera poses, textured 3D surface meshes, richly annotated part-level and object-level semantic labels, and part mobility parameters. We validate our dataset on instance segmentation and part mobility estimation tasks and benchmark methods for these tasks from prior work. Our experiments show that part segmentation and mobility estimation in real 3D scenes remain challenging despite recent progress in 3D object segmentation.",,,,,,
2120,MultiSenseBadminton,Sports Understanding,Sports Understanding,"Sports Understanding, Time Series, Sports Analytics, motion retargeting, Time Series Analysis","Time Series, Video",,Methodology,,CC0,https://springernature.figshare.com/collections/MultiSenseBadminton_Wearable_Sensor_Based_Biomechanical_Dataset_for_Evaluation_of_Badminton_Performance/6725706/1,https://paperswithcode.com/dataset/multisensebadminton,"The sports industry is witnessing an increasing trend of utilizing multiple synchronized sensors for player data collection, enabling personalized training systems with multi-perspective real-time feedback. Badminton could benefit from these various sensors, but there is a scarcity of comprehensive badminton action datasets for analysis and training feedback. Addressing this gap, this paper introduces a multi-sensor badminton dataset for forehand clear and backhand drive strokes, based on interviews with coaches for optimal usability. The dataset covers various skill levels, including beginners, intermediates, and experts, providing resources for understanding biomechanics across skill levels. It encompasses 7,763 badminton swing data from 25 players, featuring sensor data on eye tracking, body tracking, muscle signals, and foot pressure. The dataset also includes video recordings, detailed annotations on stroke type, skill level, sound, ball landing, and hitting location, as well as survey and interview data. We validated our dataset by applying a proof-of-concept machine learning model to all annotation data, demonstrating its comprehensive applicability in advanced badminton training and research.",,,,,,
2121,MultiSenti,Lexical Normalization,Lexical Normalization,"Lexical Normalization, Sentiment Analysis",Text,English,Natural Language Processing,,,https://github.com/haroonshakeel/multisenti,https://paperswithcode.com/dataset/multisenti,"MultiSenti presents a labeled dataset called MultiSenti for sentiment classification of code-switched informal short text, (2) explore the feasibility of adapting resources from a resource-rich language for an informal one, and (3) propose a deep learning-based model for sentiment classification of code-switched informal short text.",,,,,,
2122,MultiSports,Action Detection,Action Detection,"Action Detection, Fine-Grained Action Detection, Spatio-Temporal Action Localization, Action Recognition","Image, Time Series, Video",,Computer Vision,action-detection-on-multisports,CC BY_NC 4.0,https://deeperaction.github.io/datasets/multisports.html,https://paperswithcode.com/dataset/multisports,"Spatio-temporal action detection is an important and challenging problem in video understanding. The existing action detection benchmarks are limited in aspects of small numbers of instances in a trimmed video or low-level atomic actions. This paper aims to present a new multi-person dataset of spatio-temporal localized sports actions, coined as MultiSports. We first analyze the important ingredients of constructing a realistic and challenging dataset for spatio-temporal action detection by proposing three criteria: (1) multi-person scenes and motion dependent identification, (2) with well-defined boundaries, (3) relatively fine-grained classes of high complexity. Based on these guidelines, we build the dataset of MultiSports v1.0 by selecting 4 sports classes, collecting 3200 video clips, and annotating 37701 action instances with 902k bounding boxes. Our dataset is characterized with important properties of high diversity, dense annotation, and high quality. Our MultiSports, with its realistic setting and detailed annotations, exposes the intrinsic challenges of spatio-temporal action detection. We hope our MultiSports can serve as a standard benchmark for spatio-temporal action detection in the future.",,,,,,
2123,MultiSubs,Multimodal Text Prediction,Multimodal Text Prediction,"Multimodal Text Prediction, Multimodal Lexical Translation, Slot Filling","Text, Time Series",English,Multimodal,"multimodal-text-prediction-on-multisubs, multimodal-lexical-translation-on-multisubs-1, multimodal-lexical-translation-on-multisubs-2, multimodal-lexical-translation-on-multisubs, multimodal-lexical-translation-on-multisubs-3",Creative Commons Attribution 4.0 International,https://doi.org/10.5281/zenodo.5034604,https://paperswithcode.com/dataset/multisubs,"MultiSubs is a dataset of multilingual subtitles gathered from the OPUS OpenSubtitles dataset, which in turn was sourced from opensubtitles.org. We have supplemented some text fragments (visually salient nouns in this release) within the subtitles with web images, where the word sense of the fragment has been disambiguated using a cross-lingual approach. We have introduced a fill-in-the-blank task and a lexical translation task to demonstrate the utility of the dataset. Please refer to our paper for a more detailed description of the dataset and tasks. Multisubs will benefit research on visual grounding of words especially in the context of free-form sentence.

Josiah Wang, Pranava Madhyastha, Josiel Figueiredo, Chiraag Lala, Lucia Specia (2021). MultiSubs: A Large-scale Multimodal and Multilingual Dataset. CoRR, abs/2103.01910. Available at: [https://arxiv.org/abs/2103.01910]
(https://arxiv.org/abs/2103.01910)",2021,our paper,https://arxiv.org/abs/2103.01910,,,
2124,MultiTHUMOS,Action Detection,Action Detection,"Action Detection, Temporal Action Localization, Action Recognition","Image, Time Series, Video",,Computer Vision,"temporal-action-localization-on-multithumos-1, action-detection-on-multi-thumos, action-detection-on-multithumos-1",CC BY 4.0,http://ai.stanford.edu/~syyeung/everymoment.html,https://paperswithcode.com/dataset/multithumos,"The MultiTHUMOS dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video.",,,,,,5
2125,Multivariate-Mobility-Paris,Univariate Time Series Forecasting,Univariate Time Series Forecasting,"Univariate Time Series Forecasting, Time Series Forecasting, Multivariate Time Series Forecasting",Time Series,,Time Series,,MIT,https://github.com/hharcolezi/ldp-protocols-mobility-cdrs/blob/main/papers/%5B3%5D/ML_final_df_real.csv,https://paperswithcode.com/dataset/multivariate-mobility-paris,"The original dataset was provided by Orange telecom in France, which contains anonymized and aggregated human mobility data. The Multivariate-Mobility-Paris dataset comprises information from 2020-08-24 to 2020-11-04 (72 days during the COVID-19 pandemic), with time granularity of 30 minutes and spatial granularity of 6 coarse regions in Paris, France. In other words, it represents a multivariate time series dataset.

This dataset can be used for several time-series tasks such as univariate/multivariate forecasting/classification with classic, machine learning, and privacy-preserving machine learning techniques.",2020,,,,,
2126,MultiviewC,3D Action Recognition,3D Action Recognition,"3D Action Recognition, Multiview Detection, 3D Object Detection","3D, Image, Video",,Computer Vision,,,https://github.com/Robert-Mar/MultiviewC,https://paperswithcode.com/dataset/multiviewc,"The MultiviewC dataset mainly contributes to multiview cattle action recognition, 3D objection detection and tracking. We build a novel synthetic dataset MultiviewC through UE4 based on real cattle video dataset which is offered by CISRO. The format of our data set has been adjusted on the basis of MultiviewX for set-up, annotation and files structure.",,,,,,
2127,MultiviewX,Multiview Detection,Multiview Detection,"Multiview Detection, Multi-Object Tracking","Image, Video",,Computer Vision,"multiview-detection-on-multiviewx, multi-object-tracking-on-multiviewx",,https://github.com/hou-yz/MVDet,https://paperswithcode.com/dataset/multiviewx,"MultiviewX is a synthetic Multiview pedestrian detection dataset. It is build using pedestrian models from PersonX, in Unity.
The MultiviewX dataset covers a square of 16 meters by 25 meters. The ground plane is quantized into a 640x1000 grid. There are 6 cameras with overlapping field-of-view in the MultiviewX dataset, each of which outputs a 1080x1920 resolution image. On average, 4.41 cameras are covering the same location.",,,,,,
2128,MultiWOZ,Data-to-Text Generation,Data-to-Text Generation,"Data-to-Text Generation, Dialogue State Tracking, Intent Detection, Multi-domain Dialogue State Tracking, Slot Filling, domain classification, Task-Oriented Dialogue Systems, End-To-End Dialogue Modelling","Image, Text, Video",English,Computer Vision,"domain-classification-on-multiwoz-2-2, multi-domain-dialogue-state-tracking-on-4, task-oriented-dialogue-systems-on-multiwoz-2, multi-domain-dialogue-state-tracking-on, multi-domain-dialogue-state-tracking-on-1, multi-domain-dialogue-state-tracking-on-3, end-to-end-dialogue-modelling-on-multiwoz-2-1, slot-filling-on-multiwoz-2-2, dialogue-state-tracking-on-multiwoz-2-1, end-to-end-dialogue-modelling-on-multiwoz-2-0, dialogue-state-tracking-on-multiwoz-2-2, multi-domain-dialogue-state-tracking-on-2, intent-detection-on-multiwoz-2-2, data-to-text-generation-on-multiwoz-2-1",MIT,https://github.com/budzianowski/multiwoz,https://paperswithcode.com/dataset/multiwoz,"The Multi-domain Wizard-of-Oz (MultiWOZ) dataset is a large-scale human-human conversational corpus spanning over seven domains, containing 8438 multi-turn dialogues, with each dialogue averaging 14 turns. Different from existing standard datasets like WOZ and DSTC2, which contain less than 10 slots and only a few hundred values, MultiWOZ has 30 (domain, slot) pairs and over 4,500 possible values. The dialogues span seven domains: restaurant, hotel, attraction, taxi, train, hospital and police.",,Contents,https://arxiv.org/abs/1905.07687,,,
2129,Multi_Lingual_Bug_Reports,Machine Translation,Machine Translation,"Machine Translation, Zero-Shot Machine Translation",Text,English,Natural Language Processing,machine-translation-on-multi-lingual-bug,CC BY 4.0,https://github.com/av9ash/gitbugs/blob/main/multilingual/README.md,https://paperswithcode.com/dataset/multi-lingual-bug-reports,"Dataset Description
The dataset used in this study comprises bug reports extracted from the Visual Studio Code GitHub repository, specifically focusing on those labeled with the english-please tag. This label indicates that the original submission was written in a language other than English, providing a clear signal for multilingual content. The dataset spans a five-year period (March 2019--June 2024), ensuring a diverse representation of bug types, user environments, and technical contexts.

Characteristics
The dataset contains 1,381 multilingual bug reports, each consisting of:
- The original bug report written in a non-English language.
- A translated version in English.
- Metadata such as issue number, creation date, labels, and status.
- Categorization into functional, UI, and performance-related issues based on the content.

Motivation & Summary
This dataset is motivated by the need to improve multilingual bug tracking and translation evaluation. Given the increasing globalization of software development, developers and QA teams frequently encounter bug reports in languages they do not understand. By providing a structured corpus of translated bug reports, this dataset facilitates:
- Comparative translation evaluation (e.g., ChatGPT vs AWS Translate vs DeepL).
- Linguistic analysis of technical bug reporting across different languages.
- Insights into common software issues encountered by diverse users.
- Improving multilingual issue tracking through automated labeling and categorization.

Potential Use Cases
This dataset can be beneficial for various research and development applications, including:
- Machine Translation Benchmarking: Evaluating the performance of translation models in a technical domain.
- Natural Language Processing (NLP) Tasks: Training classifiers to categorize bug reports based on their content.
- Software Engineering Research: Understanding trends in bug reporting, issue resolution, and localization challenges.
- Automated Bug Triage: Developing AI-driven solutions for assigning and prioritizing bug reports in multilingual repositories.",2019,,,,,
2130,MuPoTS-3D,Unsupervised 3D Multi-Person Pose Estimation,Unsupervised 3D Multi-Person Pose Estimation,"Unsupervised 3D Multi-Person Pose Estimation, 3D Multi-Person Pose Estimation (absolute), 3D Multi-Person Pose Estimation (root-relative), 3D Multi-Person Pose Estimation","3D, Image",,Computer Vision,"3d-multi-person-pose-estimation-root-relative, unsupervised-3d-multi-person-pose-estimation, 3d-multi-person-pose-estimation-absolute-on, 3d-multi-person-human-pose-estimation-on",,http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/,https://paperswithcode.com/dataset/mupots-3d,"MuPoTs-3D (Multi-person Pose estimation Test Set in 3D) is a dataset for pose estimation composed of more than 8,000 frames from 20 real-world scenes with up to three subjects. The poses are annotated with a 14-point skeleton model.",,DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild,https://arxiv.org/abs/2008.09457,,,
2131,MURA,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Decision Making",Image,,Computer Vision,,Custom (non-commercial),https://stanfordmlgroup.github.io/competitions/mura/,https://paperswithcode.com/dataset/mura,"A large dataset of musculoskeletal radiographs containing 40,561 images from 14,863 studies, where each study is manually labeled by radiologists as either normal or abnormal.",,,,561 images,,
2132,MUSDB18-HQ,Music Source Separation,Music Source Separation,Music Source Separation,Audio,,Audio,music-source-separation-on-musdb18-hq,,https://sigsep.github.io/datasets/musdb.html,https://paperswithcode.com/dataset/musdb18-hq,"MUSDB18-HQ is a high-quality version of the MUSDB18 music tracks dataset. The high-quality dataset consists of the same 150 songs, but instead of MP4 files (compressed with Advanced Audio Coding encoder at 256kbps, with bandwidth limited to 16kHz), the songs are provided as raw WAV files.",,,,,,
2133,MUSDB18,Cadenza 1 - Task 1 - Headphone,Cadenza 1 - Task 1 - Headphone,"Cadenza 1 - Task 1 - Headphone, Audio Source Separation, Music Source Separation",Audio,,Audio,"cadenza-1-task-1-headphone-on-musdb18, music-source-separation-on-musdb18",Various (see link),https://sigsep.github.io/datasets/musdb.html,https://paperswithcode.com/dataset/musdb18,"The MUSDB18 is a dataset of 150 full lengths music tracks (~10h duration) of different genres along with their isolated drums, bass, vocals and others stems.

The dataset is split into training and test sets with 100 and 50 songs, respectively. All signals are stereophonic and encoded at 44.1kHz.",,,,,,
2134,MuSe-CaR,Multimodal Sentiment Analysis,Multimodal Sentiment Analysis,Multimodal Sentiment Analysis,Text,English,Multimodal,,,https://www.muse-challenge.org/data,https://paperswithcode.com/dataset/muse-car,"The MuSe-CAR database is a large, multimodal (video, audio, and text) dataset which has been gathered in-the-wild with the intention of further understanding Multimodal Sentiment Analysis in-the-wild, e.g., the emotional engagement that takes place during product reviews (i.e., automobile reviews) where a sentiment is linked to a topic or entity. 

The estimated age range of the professional, semi-professional (influncers), and casual reviewers is between the middle of 20s until the late 50s. Most are native English speakers from the UK or the US, while a small minority are non-native, yet fluent English speakers.",,,,,,
2135,MUSE,Word Embeddings,Word Embeddings,"Word Embeddings, Word Alignment, Machine Translation",Text,English,Natural Language Processing,"word-alignment-on-muse-en-de, word-alignment-on-muse-en-pt",CC BY-NC 4.0,https://github.com/facebookresearch/MUSE,https://paperswithcode.com/dataset/muse,"The MUSE dataset contains bilingual dictionaries for 110 pairs of languages. For each language pair, the training seed dictionaries contain approximately 5000 word pairs while the evaluation sets contain 1500 word pairs.",,Filtered Inner Product Projection for Multilingual Embedding Alignment,https://arxiv.org/abs/2006.03652,,,
2136,MuseASTE,Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),"Aspect-Based Sentiment Analysis (ABSA), Aspect Sentiment Triplet Extraction",Text,English,Natural Language Processing,aspect-sentiment-triplet-extraction-on-1,Creative Commons Licenses 4.0,https://doi.org/10.1016/j.eswa.2024.125695,https://paperswithcode.com/dataset/musecar-aste,"•A new benchmark dataset for Aspect Sentiment Triplet Extraction.
•First Aspect Sentiment Triplet Extraction (ASTE) Dataset in Automotive Domain.
•Largest ASTE Dataset to date with annotations for over 28,295 sentences.
•Dataset includes complex aspects not verbatim present in the sentence.
•Domain: Aspect-based sentiment analysis, ASTE, Opinion Mining, Recommender System.
•Four baseline SOTA models implemented on the dataset",,,,295 sentences,,
2137,MuseData,Music Generation,Music Generation,"Music Generation, Language Modelling","Audio, Text",English,Natural Language Processing,,,https://musedata.org/,https://paperswithcode.com/dataset/musedata,MuseData is an electronic library of orchestral and piano classical music from CCARH. It consists of around 3MB of 783 files.,,https://arxiv.org/pdf/1206.6392v1.pdf,https://arxiv.org/pdf/1206.6392v1.pdf,,,
2138,MuseScore,Music Source Separation,Music Source Separation,Music Source Separation,Audio,,Audio,,,https://biboamy.github.io/streaming-demo/main_site/,https://paperswithcode.com/dataset/musescore,"The MuseScore dataset is a collection of 344,166 audio and MIDI pairs downloaded from MuseScore website. The audio is usually synthesized by the MuseScore synthesizer. The audio clips have diverse musical genres and are about two mins long on average.

Due to copyright issues the dataset is not publicly available, but can be collected and processed with the provided source code.",,Multitask learning for frame-level instrument recognition,https://arxiv.org/pdf/1811.01143.pdf,,,
2139,MUSIC-AVQA,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Scene Understanding, Question Answering, Audio-visual Question Answering","Audio, Image, Text",English,Computer Vision,audio-visual-question-answering-on-music-avqa,MIT,http://gewu-lab.github.io/MUSIC-AVQA/,https://paperswithcode.com/dataset/music-avqa,"The large-scale MUSIC-AVQA dataset of musical performance contains 45,867 question-answer pairs, distributed in 9,288 videos for over 150 hours. All QA pairs types are divided into 3 modal scenarios, which contain 9 question types and 33 question templates. Finally, as an open-ended problem of our AVQA tasks, all 42 kinds of answers constitute a set for selection.",,,,,,
2140,Music21,Music Generation,Music Generation,"Music Generation, Music Transcription, Music Modeling","Audio, Text",English,Natural Language Processing,,BSD-3,http://web.mit.edu/music21/,https://paperswithcode.com/dataset/music21,Music21 is an untrimmed video dataset crawled by keyword query from Youtube. It contains music performances belonging to 21 categories. This dataset is relatively clean and collected for the purpose of training and evaluating visual sound source separation models.,,Music Gesture for Visual Sound Separation,https://arxiv.org/abs/2004.09476,,,21
2141,MusicBench,Music Generation,Music Generation,"Music Generation, Text-to-Music Generation","Audio, Text",English,Natural Language Processing,text-to-music-generation-on-musicbench,CC,https://huggingface.co/datasets/amaai-lab/MusicBench,https://paperswithcode.com/dataset/musicbench,"The MusicBench dataset is a music audio-text pair dataset that was designed for text-to-music generation purpose and released along with Mustango text-to-music model. MusicBench is based on the MusicCaps dataset, which it expands from 5,521 samples to 52,768 training and 400 test samples!

Dataset Details
MusicBench expands MusicCaps by:

Including music features of chords, beats, tempo, and key that are extracted from the audio.
Describing these music features using text templates and thus enhancing the original text prompts.
Expanding the number of audio samples by performing musically meaningful augmentations: semitone pitch shifts, tempo changes, and volume changes.

Train set size = 52,768 samples Test set size = 400

This dataset also includes FMACaps, which was used as a second test set.",,,,521 samples,training and 400 test samples,
2142,MusicBrainz20K,Entity Resolution,Entity Resolution,Entity Resolution,,,Methodology,entity-resolution-on-musicbrainz20k,Creative Commons license,https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution,https://paperswithcode.com/dataset/musicbrainz20k,"The MusicBrainz20K dataset for entity resolution and entity clustering is based on real records about songs from the MusicBrainz database. Each record is described with the following attributes: artist, title, album, year and length. The records have been modified with the DAPO [1] data generator. The generated dataset consists of five sources and approximately 20K records describing 10K unique song entities. It contains duplicates for 50% of the original records in two to five sources which are generated with a high degree of corruption to stress-test the entity resolution and clustering approaches.

[1] Hildebrandt, Kai, et al. ""Large-scale data pollution with Apache Spark."" IEEE Transactions on Big Data 6.2 (2017): 396-411.",2017,,,20K records,,
2143,MusicCaps,Music Generation,Music Generation,"Music Generation, Text-to-Music Generation, Music Captioning","Audio, Image, Text",English,Computer Vision,text-to-music-generation-on-musiccaps,CC BY-SA 4.0,https://google-research.github.io/seanet/musiclm/examples/,https://paperswithcode.com/dataset/musiccaps,"MusicCaps is a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts. For each 10-second music clip, MusicCaps provides: 

1) A free-text caption consisting of four sentences on average, describing the music and 

2) A list of music aspects, describing genre, mood, tempo, singer voices, instrumentation, dissonances, rhythm, etc.",,MusicLM: Generating Music From Text,https://arxiv.org/pdf/2301.11325v1.pdf,,,
2144,MusicNet,Music Transcription,Music Transcription,Music Transcription,Audio,,Audio,music-transcription-on-musicnet,Custom,https://zenodo.org/record/5120004#.Yhxr0-jMJBA,https://paperswithcode.com/dataset/musicnet,"MusicNet is a collection of 330 freely-licensed classical music recordings, together with over 1 million annotated labels indicating the precise time of each note in every recording, the instrument that plays each note, and the note's position in the metrical structure of the composition. The labels are acquired from musical scores aligned to recordings by dynamic time warping. The labels are verified by trained musicians; we estimate a labeling error rate of 4%. We offer the MusicNet labels to the machine learning and music communities as a resource for training models and a common benchmark for comparing results.",,,,,,
2145,MusicQA,Music Question Answering,Music Question Answering,Music Question Answering,"Audio, Text",English,Natural Language Processing,music-question-answering-on-musicqa,,https://huggingface.co/datasets/mu-llama/MusicQA,https://paperswithcode.com/dataset/musicqa,"We propose the MusicQA dataset to train Music-enabled question-answering models and is used for training and evaluating our MU-LLaMA model. This dataset is generated using the MusicCaps and MagnaTagATune datasets. We utilize the descriptions/tags from existing datasets to prompt the MPT-7B Chat model to generate question-answer pairs through inference, reasoning, and paraphrasing. The dataset contains 12,542 music files for training making up 76.15 hours of music with 112,878 question-answer pairs.",,,,,,
2146,MuSiQue-Ans,Multi-hop Question Answering,Multi-hop Question Answering,Multi-hop Question Answering,Text,English,Natural Language Processing,multi-hop-question-answering-on-musique-ans,,https://github.com/stonybrooknlp/musique,https://paperswithcode.com/dataset/musique-ans,MuSiQue-Ans is a new multihop QA dataset with ~25K 2-4 hop questions using seed questions from 5 existing single-hop datasets.,,,,,,
2147,Musk_v1,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Multiple Instance Learning",Image,,Computer Vision,"anomaly-detection-on-musk-v1, multiple-instance-learning-on-musk-v1",,https://archive.ics.uci.edu/ml/datasets/Musk+(Version+1),https://paperswithcode.com/dataset/musk-v1,"The Musk dataset describes a set of molecules, and the objective is to detect musks from non-musks. This dataset describes a set of 92 molecules of which 47 are judged by human experts to be musks and the remaining 45 molecules are judged to be non-musks. There are 166 features available that describe the molecules based on the shape of the molecule.",,Estimation of Dimensions Contributing to Detected Anomalies with Variational Autoencoders,https://arxiv.org/abs/1811.04576,,,
2148,Musk_v2,Multiple Instance Learning,Multiple Instance Learning,Multiple Instance Learning,,,Methodology,multiple-instance-learning-on-musk-v2,,https://archive.ics.uci.edu/ml/datasets/Musk+(Version+2),https://paperswithcode.com/dataset/musk-v2,"The Musk2 dataset is a set of 102 molecules of which 39 are judged by human experts to be musks and the remaining 63 molecules are judged to be non-musks. Each instance corresponds to a possible configuration of a molecule. The 166 features that describe these molecules depend upon the exact shape, or conformation, of the molecule.",,Confidence-Constrained Maximum Entropy Framework for Learning from Multi-Instance Data,https://arxiv.org/abs/1603.01901,,,
2149,MuST-C,Data Augmentation,Data Augmentation,"Data Augmentation, Speech-to-Text Translation, Speech Recognition","Audio, Image, Text",English,Computer Vision,"speech-to-text-translation-on-must-c-en-de, speech-to-text-translation-on-must-c-1",CC BY-NC-ND 4.0,https://mt.fbk.eu/must-c,https://paperswithcode.com/dataset/must-c,"MuST-C currently represents the largest publicly available multilingual corpus (one-to-many) for speech translation. It covers eight language directions, from English to German, Spanish, French, Italian, Dutch, Portuguese, Romanian and Russian. The corpus consists of audio, transcriptions and translations of English TED talks, and it comes with a predefined training, validation and test split.",,One-to-Many Multilingual End-to-End Speech Translation,https://arxiv.org/abs/1910.03320,,,
2150,MUStARD__,Emotion Recognition,Emotion Recognition,"Emotion Recognition, Sarcasm Detection",Image,,Computer Vision,sarcasm-detection-on-mustard,,https://github.com/apoorva-nunna/mustard_plus_plus,https://paperswithcode.com/dataset/mustard-1,MUStARD++ is a multimodal sarcasm detection dataset (MUStARD) pre-annotated with 9 emotions. It can be used for the task of detecting the emotion in a sarcastic statement.,,,,,,
2151,MUTAG,Explanation Fidelity Evaluation,Explanation Fidelity Evaluation,"Explanation Fidelity Evaluation, Graph Classification, Node Classification","Graph, Image",,Computer Vision,"node-classification-on-mutag, graph-classification-on-mutag, explanation-fidelity-evaluation-on-mutag",,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,https://paperswithcode.com/dataset/mutag,"In particular, MUTAG is a collection of nitroaromatic compounds and the goal is to predict their mutagenicity on Salmonella typhimurium. Input graphs are used to represent chemical compounds, where vertices stand for atoms and are labeled by the atom type (represented by one-hot encoding), while edges between vertices represent bonds between the corresponding atoms. It includes 188 samples of chemical compounds with 7 discrete node labels.",,Fast and Deep Graph Neural Networks,https://arxiv.org/abs/1911.08941,188 samples,,
2152,MUTE,Multimodal Deep Learning,Multimodal Deep Learning,"Multimodal Deep Learning, Classification",Image,,Multimodal,,,https://github.com/eftekhar-hossain/MUTE-AACL22,https://paperswithcode.com/dataset/mute,"MUTE
This is the first open-source Bengali Hateful Meme dataset, consisting of around 4200 memes annotated with two labels: hate and not hate.",,,,,,
2153,MUTLA,Electroencephalogram (EEG),Electroencephalogram (EEG),Electroencephalogram (EEG),,,Methodology,,,https://tinyurl.com/SAILdata,https://paperswithcode.com/dataset/mutla,"This dataset includes time-synchronized multimodal data records of students (learning logs, videos, EEG brainwaves) as they work in various subjects from Squirrel AI Learning System (SAIL) to solve problems of varying difficulty levels. The dataset resources include user records from the learner records store of SAIL, brainwave data collected by EEG headset devices, and video data captured by web cameras while students worked in the SAIL products.",,,,,,
2154,MuTual,Text Generation,Text Generation,"Text Generation, Task-Oriented Dialogue Systems, Machine Reading Comprehension",Text,English,Natural Language Processing,,,https://github.com/Nealcly/MuTual,https://paperswithcode.com/dataset/mutual,"MuTual is a retrieval-based dataset for multi-turn dialogue reasoning, which is modified from Chinese high school English listening comprehension test data. It tests dialogue reasoning via next utterance prediction.",,,,,,
2155,MutualFriends,Goal-Oriented Dialog,Goal-Oriented Dialog,"Goal-Oriented Dialog, Knowledge Graph Embeddings, Dialogue Understanding",Graph,,Methodology,,,https://stanfordnlp.github.io/cocoa/,https://paperswithcode.com/dataset/mutualfriends,"In MutualFriends, two agents, A and B, each have a private knowledge base, which contains a list of friends with multiple attributes (e.g., name, school, major, etc.). The agents must chat with each other to find their unique mutual friend.",,,,,,
2156,MVBench,Zero-Shot Video Question Answer,Zero-Shot Video Question Answer,"Zero-Shot Video Question Answer, Visual Question Answering (VQA), Video Question Answering","Image, Text, Video",English,Computer Vision,"visual-question-answering-vqa-on-mvbench, video-question-answering-on-mvbench, zero-shot-video-question-answer-on-mvbench",,https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat2,https://paperswithcode.com/dataset/mvbench,"MVBench is a comprehensive Multi-modal Video understanding Benchmark. It was introduced to evaluate the comprehension capabilities of Multi-modal Large Language Models (MLLMs), particularly their temporal understanding in dynamic video tasks. MVBench covers 20 challenging video tasks that cannot be effectively solved with a single frame. It introduces a novel static-to-dynamic method to define these temporal-related tasks. By transforming various static tasks into dynamic ones, it enables the systematic generation of video tasks that require a broad spectrum of temporal skills, ranging from perception to cognition.",,,,,,
2157,MVK,Retrieval,Retrieval,"Retrieval, Underwater Image Restoration, Multimodal Deep Learning, Text to Video Retrieval, Video Retrieval","Image, Text, Video",English,Computer Vision,retrieval-on-mvk,,https://mvk.hkustvgd.com/,https://paperswithcode.com/dataset/marine-video-kit,"The dataset contains single-shot videos taken from moving cameras in underwater environments. The first shard of a new Marine Video Kit dataset is presented to serve for video retrieval and other computer vision challenges. In addition to basic meta-data statistics, we present several insights based on low-level features as well as semantic annotations of selected keyframes. 
1379 videos with a length from 2 s to 4.95 min, with the mean and median duration of each video is 29.9 s, and 25.4 s, respectively.
We capture data from 11 diﬀerent regions and countries during the time from 2011 to 2022.",2011,,,,,
2158,MVOR,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Pose Estimation, 3D Pose Estimation","3D, Image",,Computer Vision,,CC-BY-NC-SA 4.0,https://github.com/CAMMA-public/mvor,https://paperswithcode.com/dataset/mvor,"Multi-View Operating Room (MVOR) is a dataset recorded during real clinical interventions. It consists of 732 synchronized multi-view frames recorded by three RGB-D cameras in a hybrid OR. It also includes the visual challenges present in such environments, such as occlusions and clutter.",,,,,,
2159,MVSEC,Event-based Optical Flow,Event-based Optical Flow,"Event-based Optical Flow, Optical Flow Estimation, Video Reconstruction, Self-Supervised Learning","3D, Video",,Methodology,"video-reconstruction-on-mvsec, event-based-optical-flow-on-mvsec",,https://daniilidis-group.github.io/mvsec/,https://paperswithcode.com/dataset/mvsec,"The Multi Vehicle Stereo Event Camera (MVSEC) dataset is a collection of data designed for the development of novel 3D perception algorithms for event based cameras. Stereo event data is collected from car, motorbike, hexacopter and handheld data, and fused with lidar, IMU, motion capture and GPS to provide ground truth pose and depth images.",,,,,,
2160,MVTecAD,Supervised Anomaly Detection,Supervised Anomaly Detection,"Supervised Anomaly Detection, Multi-class Anomaly Detection, zero-shot anomaly detection, Anomaly Detection, Outlier Detection, Unsupervised Anomaly Detection",Image,,Computer Vision,"multi-class-anomaly-detection-on-mvtec-ad, anomaly-detection-on-mvtec-ad, supervised-anomaly-detection-on-mvtec-ad, zero-shot-anomaly-detection-on-mvtec-ad-1",CC BY-NC-SA 4.0,https://www.mvtec.com/company/research/datasets/mvtec-ad/,https://paperswithcode.com/dataset/mvtecad,"MVTec AD is a dataset for benchmarking anomaly detection methods with a focus on industrial inspection. It contains over 5000 high-resolution images divided into fifteen different object and texture categories. Each category comprises a set of defect-free training images and a test set of images with various kinds of defects as well as images without defects.

There are two common metrics: Detection AUROC and Segmentation (or pixelwise) AUROC

Detection (or, classification) methods output single float (anomaly score) per input test image. 

Segmentation methods output anomaly probability for each pixel. 
""To assess segmentation performance, we evaluate the relative per-region overlap of the segmentation with the ground truth. To get an additional performance measure that is independent of the determined threshold, we compute the area under the receiver operating characteristic curve (ROC AUC). We define the true positive rate as the percentage of pixels that were correctly classified as anomalous"" [1]
Later segmentation metric was improved to balance regions with small and large area, see PRO-AUC and other in [2]

[1] Paul Bergmann et al, ""MVTec AD — A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection""
[2] Bergmann, P., Batzner, K., Fauser, M. et al. The MVTec Anomaly Detection Dataset: A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection. Int J Comput Vis (2021). https://doi.org/10.1007/s11263-020-01400-4",2021,,,,,
2161,MVTec_LOCO_AD,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Semi-supervised Anomaly Detection, Unsupervised Anomaly Detection",Image,,Computer Vision,anomaly-detection-on-mvtec-loco-ad,CC BY-NC-SA 4.1,https://www.mvtec.com/company/research/datasets/mvtec-loco,https://paperswithcode.com/dataset/mvtec-loco-ad,"MVTec Logical Constraints Anomaly Detection (MVTec LOCO AD) dataset is intended for the evaluation of unsupervised anomaly localization algorithms. The dataset includes both structural and logical anomalies. It contains 3644 images from five different categories inspired by real-world industrial inspection scenarios. Structural anomalies appear as scratches, dents, or contaminations in the manufactured products. Logical anomalies violate underlying constraints, e.g., a permissible object being present in an invalid location or a required object not being present at all. The dataset also includes pixel-precise ground truth data for each anomalous region.",,,,3644 images,traints Anomaly Detection (MVTec LOCO AD) dataset is intended for the evaluation of unsupervised anomaly localization algorithms. The dataset includes both structural and logical anomalies. It contains 3644 images,
2162,MVX,Intelligent Communication,Intelligent Communication,"Intelligent Communication, Semantic Communication, Autonomous Vehicles, 3D Object Detection, Beam Prediction, Optimize the trajectory of UAV which plays a BS in communication system","3D, Image, Time Series",,Computer Vision,beam-prediction-on-mvx,,https://ghazigh.github.io/MVX/,https://paperswithcode.com/dataset/mvx,MVX incorporates realistic physical world simulation with a differentiable accurate ray tracing wireless simulation that includes multi-agent and multimodal datasets for AI-driven digital twin applications in vehicular communication systems.,,,,,,
2163,MWE-CWI,Complex Word Identification,Complex Word Identification,"Complex Word Identification, Text Simplification",Text,English,Natural Language Processing,,,https://github.com/ekochmar/MWE-CWI,https://paperswithcode.com/dataset/mwe-cwi,Multiword expressions (MWEs) represent lexemes that should be treated as single lexical units due to their idiosyncratic nature. MWE-CWI is a dataset for MWE detection based on the Complex Word Identification Shared Task 2018 dataset.,2018,,,,,
2164,Myket_Android_Application_Install,Link Prediction,Link Prediction,"Link Prediction, Graph Representation Learning","Graph, Time Series",,Methodology,,MIT,https://myket.ir/,https://paperswithcode.com/dataset/myket-android-application-install,"This dataset contains information on application install interactions of users in the Myket android application market. The dataset was created for the purpose of evaluating interaction prediction models, requiring user and item identifiers along with timestamps of the interactions. Hence, the dataset can be used for interaction prediction and building a recommendation system. Furthermore, the data forms a dynamic network of interactions, and we can also perform network representation learning on the nodes in the network, which are users and applications.

Data Creation
The dataset was initially generated by the Myket data team, and later cleaned and subsampled by Erfan Loghmani a master student at Sharif University of Technology at the time. The data team focused on a two-week period and randomly sampled 1/3 of the users with interactions during that period. They then selected install and update interactions for three months before and after the two-week period, resulting in interactions spanning about 6 months and two weeks.

We further subsampled and cleaned the data to focus on application download interactions. We identified the top 8000 most installed applications and selected interactions related to them. We retained users with more than 32 interactions, resulting in 280,391 users. From this group, we randomly selected 10,000 users, and the data was filtered to include only interactions for these users. The detailed procedure can be found in here.

Data Structure
The dataset has two main files.


myket.csv: This file contains the interaction information and follows the same format as the datasets used in the ""JODIE: Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks"" (ACM SIGKDD 2019) project. However, this data does not contain state labels and interaction features, resulting in associated columns being all zero.
app_info_sample.csv: This file comprises features associated with applications present in the sample. For each individual application, information such as the approximate number of installs, average rating, count of ratings, and category are included. These features provide insights into the applications present in the dataset.

Dataset Details

Total Instances: 694,121 install interaction instances
Instances Format: Triplets of user_id, app_name, timestamp
10,000 users and 7,988 android applications
Item features for 7,606 applications

For a detailed summary of the data's statistics, including information on users, applications, and interactions, please refer to the Python notebook available at summary-stats.ipynb. The notebook provides an overview of the dataset's characteristics and can be helpful for understanding the data's structure before using it for research or analysis.

Top 20 Most Installed Applications
| Package Name                       | Count of Interactions |
| ---------------------------------- | --------------------- |
| com.instagram.android              | 15292                 |
| ir.resaneh1.iptv                   | 12143                 |
| com.tencent.ig                     | 7919                  |
| com.ForgeGames.SpecialForcesGroup2 | 7797                  |
| ir.nomogame.ClutchGame             | 6193                  |
| com.dts.freefireth                 | 6041                  |
| com.whatsapp                       | 5876                  |
| com.supercell.clashofclans         | 5817                  |
| com.mojang.minecraftpe             | 5649                  |
| com.lenovo.anyshare.gps            | 5076                  |
| ir.medu.shad                       | 4673                  |
| com.firsttouchgames.dls3           | 4641                  |
| com.activision.callofduty.shooter  | 4357                  |
| com.tencent.iglite                 | 4126                  |
| com.aparat                         | 3598                  |
| com.kiloo.subwaysurf               | 3135                  |
| com.supercell.clashroyale          | 2793                  |
| co.palang.QuizOfKings              | 2589                  |
| com.nazdika.app                    | 2436                  |
| com.digikala                       | 2413                  |

Comparison with SNAP Datasets
The Myket dataset introduced in this repository exhibits distinct characteristics compared to the real-world datasets used by the project. The table below provides a comparative overview of the key dataset characteristics:

| Dataset         | #Users           | #Items          | #Interactions | Average Interactions per User | Average Unique Items per User |
| --------------- | ---------------- | --------------- | ------------- | ----------------------------- | ----------------------------- |
| Myket | 10,000 | 7,988 | 694,121       | 69.4                          | 54.6                          |
| LastFM          | 980              | 1,000           | 1,293,103     | 1,319.5                       | 158.2                         |
| Reddit          | 10,000 | 984             | 672,447       | 67.2                          | 7.9                           |
| Wikipedia       | 8,227            | 1,000           | 157,474       | 19.1                          | 2.2                           |
| MOOC            | 7,047            | 97              | 411,749       | 58.4                          | 25.3                          |

The Myket dataset stands out by having an ample number of both users and items, highlighting its relevance for real-world, large-scale applications. Unlike LastFM, Reddit, and Wikipedia datasets, where users exhibit repetitive item interactions, the Myket dataset contains a comparatively lower amount of repetitive interactions. This unique characteristic reflects the diverse nature of user behaviors in the Android application market environment.

Citation
If you use this dataset in your research, please cite the following preprint:

@misc{loghmani2023effect,
      title={Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks}, 
      author={Erfan Loghmani and MohammadAmin Fazli},
      year={2023},
      eprint={2308.06862},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}",2019,preprint,https://arxiv.org/abs/2308.06862,,,
2165,N-Caltech_101,Event data classification,Event data classification,"Event data classification, Image Classification, Object Recognition, Classification",Image,,Computer Vision,"event-data-classification-on-n-caltech-101, image-classification-on-n-caltech-101, object-recognition-on-n-caltech-101",Creative Commons 4.0,https://www.garrickorchard.com/datasets/n-caltech101,https://paperswithcode.com/dataset/n-caltech-101,"The Neuromorphic-Caltech101 (N-Caltech101) dataset is a spiking version of the original frame-based Caltech101 dataset. The original dataset contained both a ""Faces"" and ""Faces Easy"" class, with each consisting of different versions of the same images. The ""Faces"" class has been removed from N-Caltech101 to avoid confusion, leaving 100 object classes plus a background class. The N-Caltech101 dataset was captured by mounting the ATIS sensor on a motorized pan-tilt unit and having the sensor move while it views Caltech101 examples on an LCD monitor as shown in the video below. A full description of the dataset and how it was created can be found in the paper below. Please cite this paper if you make use of the dataset.",,,,101 examples,,
2166,N-CARS,Object Recognition,Object Recognition,"Object Recognition, Optical Flow Estimation, Classification","Image, Video",,Computer Vision,"object-recognition-on-n-cars, classification-on-n-cars",Custom,https://www.prophesee.ai/2018/03/13/dataset-n-cars/,https://paperswithcode.com/dataset/n-cars,A large real-world event-based dataset for object classification.,,,,,,
2167,N15News,News Classification,News Classification,News Classification,Image,,Computer Vision,news-classification-on-n15news,,https://drive.google.com/drive/folders/1JCo-5I6gRcgCG7AfdtcDuovndN6-QjF8,https://paperswithcode.com/dataset/n15news,"N15News is a large-scale multimodal news dataset comprising 200K imagetext pairs and 15 categories, which exceeding the previous news dataset in both the number of categories and samples.",,https://arxiv.org/pdf/2108.13327v1.pdf,https://arxiv.org/pdf/2108.13327v1.pdf,,,15
2168,NAIST_COVID,Misinformation,Misinformation,"Misinformation, Sentiment Analysis, Causal Inference",Text,English,Natural Language Processing,,,https://github.com/sociocom/covid19_dataset,https://paperswithcode.com/dataset/naist-covid,"NAIST COVID is a multilingual dataset of social media posts related to COVID-19, consisting of microblogs in English and Japanese from Twitter and those in Chinese from Weibo. The data cover microblogs from January 20, 2020, to March 24, 2020.",2020,,,,,
2169,Nakanishi2015_MOABB,Within-Session SSVEP,Within-Session SSVEP,Within-Session SSVEP,,,Methodology,within-session-ssvep-on-nakanishi2015-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Nakanishi2015.html,https://paperswithcode.com/dataset/nakanishi2015-moabb,,,,,,,
2170,Names_pairs_dataset,Semantic Similarity,Semantic Similarity,"Semantic Similarity, Embeddings Evaluation",,,Methodology,,This work is licensed under a CC BY 4.0 license,https://github.com/e184633/semantic-answer-similarity/tree/main/data/data/names,https://paperswithcode.com/dataset/names-pairs-dataset,Includes co-referent name string pairs along with their similarities.,,,,,,
2171,NarrativeQA,Question Answering,Question Answering,"Question Answering, Reading Comprehension",Text,English,Natural Language Processing,question-answering-on-narrativeqa,Apache-2.0 License,https://deepmind.com/research/open-source/narrativeqa,https://paperswithcode.com/dataset/narrativeqa,"The NarrativeQA dataset includes a list of documents with Wikipedia summaries, links to full stories, and questions and answers.",,Kočiský et al,https://arxiv.org/pdf/1712.07040v1.pdf,,,
2172,NAS-Bench-101,Neural Architecture Search,Neural Architecture Search,"Neural Architecture Search, AutoML, Hyperparameter Optimization",,,Methodology,neural-architecture-search-on-nas-bench-101,Apache-2.0,https://github.com/google-research/nasbench,https://paperswithcode.com/dataset/nas-bench-101,"NAS-Bench-101 is the first public architecture dataset for NAS research. To build NASBench-101, the authors carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional
architectures. The authors trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the precomputed dataset.",,,,,,
2173,NAS-Bench-1Shot1,Neural Architecture Search,Neural Architecture Search,"Neural Architecture Search, AutoML, Adversarial Attack",,,Methodology,,,https://github.com/automl/nasbench-1shot1,https://paperswithcode.com/dataset/nas-bench-1shot1,NAS-Bench-1Shot1 draws on the recent large-scale tabular benchmark NAS-Bench-101 for cheap anytime evaluations of one-shot NAS methods.,,NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search,https://arxiv.org/pdf/2001.10422v2.pdf,,,
2174,NAS-Bench-201,Neural Architecture Search,Neural Architecture Search,"Neural Architecture Search, Image Classification, Hyperparameter Optimization",Image,,Computer Vision,"neural-architecture-search-on-nas-bench-201, neural-architecture-search-on-nas-bench-201-1, neural-architecture-search-on-nas-bench-201-2, neural-architecture-search-on-nas-bench-201-3",,https://xuanyidong.com/assets/projects/NAS-Bench-201.html,https://paperswithcode.com/dataset/nas-bench-201,"NAS-Bench-201 is a benchmark (and search space) for neural architecture search. Each architecture consists of a predefined skeleton with a stack of the searched cell. In this way, architecture search is transformed into the problem of searching a good cell.",,,,,,
2175,NASA_C-MAPSS-2,Remaining Useful Lifetime Estimation,Remaining Useful Lifetime Estimation,Remaining Useful Lifetime Estimation,,,Methodology,remaining-useful-lifetime-estimation-on-nasa,,https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan-2,https://paperswithcode.com/dataset/nasa-c-mapss-2,"The generation of data-driven prognostics models requires the availability of datasets with run-to-failure trajectories. In order to contribute to the development of these methods, the dataset provides a new realistic dataset of run-to-failure trajectories for a small fleet of aircraft engines under realistic flight conditions. The damage propagation modelling used for the generation of this synthetic dataset builds on the modeling strategy from previous work . The dataset was generated with the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dynamical model. The data set is been provided by the Prognostics CoE at NASA Ames in collaboration with ETH Zurich and PARC.",,,,,,
2176,NASA_C-MAPSS,Remaining Useful Lifetime Estimation,Remaining Useful Lifetime Estimation,"Remaining Useful Lifetime Estimation, reinforcement-learning, Decision Making",,,Methodology,"decision-making-on-nasa-c-mapss, reinforcement-learning-on-nasa-c-mapss, remaining-useful-lifetime-estimation-on-nasa-1",,https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/,https://paperswithcode.com/dataset/nasa-c-mapss,Engine degradation simulation was carried out using C-MAPSS. Four different were sets simulated under different combinations of operational conditions and fault modes. Records several sensor channels to characterize fault evolution. The data set was provided by the Prognostics CoE at NASA Ames.,,,,,,
2177,NASA_Li-ion_Dataset,Li-ion State of Health Estimation,Li-ion State of Health Estimation,Li-ion State of Health Estimation,,,Methodology,li-ion-state-of-health-estimation-on-nasa-li,,https://www.nasa.gov/intelligent-systems-division/discovery-and-systems-health/pcoe/pcoe-data-set-repository/,https://paperswithcode.com/dataset/nasa-li-ion-dataset,"Experiments on Li-Ion batteries. Charging and discharging at different temperatures. Records the impedance as the damage criterion. The data set was provided by the NASA Prognostics Center of Excellence (PCoE).

Citation:
B. Saha and K. Goebel (2007). “Battery Data Set”, NASA Prognostics Data Repository, NASA Ames Research Center, Moffett Field, CA",2007,,,,,
2178,National_Lung_Screening_Trial__NLST_,Lung Cancer Diagnosis,Lung Cancer Diagnosis,Lung Cancer Diagnosis,,,Methodology,lung-cancer-diagnosis-on-national-lung,,https://cdas.cancer.gov/nlst/,https://paperswithcode.com/dataset/national-lung-screening-trial-nlst,"The National Lung Screening Trial (NLST) was a randomized controlled trial conducted by the Lung Screening Study group (LSS) and the American College of Radiology Imaging Network (ACRIN) to determine whether screening for lung cancer with low-dose helical computed tomography (CT) reduces mortality from lung cancer in high-risk individuals relative to screening with chest radiography. Approximately 54,000 participants were enrolled between August 2002 and April 2004. Data collection has ended, and information is complete through December 31, 2009. NLST has the ClinicalTrials.gov registration number NCT00047385.",2002,,,,,
2179,NaturalProofs,Mathematical Proofs,Mathematical Proofs,Mathematical Proofs,,,Methodology,,Attribution 4.0 International,https://github.com/wellecks/naturalproofs#naturalproofs-dataset,https://paperswithcode.com/dataset/naturalproofs,"The NaturalProofs Dataset is a large-scale dataset for studying mathematical reasoning in natural language. NaturalProofs consists of roughly 20,000 theorem statements and proofs, 12,500 definitions, and 1,000 additional pages (e.g. axioms, corollaries) derived from ProofWiki, an online compendium of mathematical proofs written by a community of contributors.",,,,,,
2180,Natural_Questions,Open-Domain Question Answering,Open-Domain Question Answering,"Open-Domain Question Answering, Retrieval, Text Retrieval, Question Generation, Zero-shot Text Search, Question Answering, Passage Retrieval",Text,English,Natural Language Processing,"open-domain-question-answering-on-natural, text-retrieval-on-natural-questions, question-answering-on-nq-beir, zero-shot-text-search-on-nq, question-answering-on-natural-questions-long, question-answering-on-natural-questions, question-generation-on-natural-questions, passage-retrieval-on-natural-questions, retrieval-on-natural-questions, open-domain-question-answering-on-natural-1",CC BY-SA 3.0,https://ai.google.com/research/NaturalQuestions,https://paperswithcode.com/dataset/natural-questions,"The Natural Questions corpus is a question answering dataset containing 307,373 training examples, 7,830 development examples, and 7,842 test examples. Each example is comprised of a google.com query and a corresponding Wikipedia page. Each Wikipedia page has a passage (or long answer) annotated on the page that answers the question and one or more short spans from the annotated passage containing the actual answer. The long and the short answer annotations can however be empty. If they are both empty, then there is no answer on the page at all. If the long answer annotation is non-empty, but the short answer annotation is empty, then the annotated passage answers the question but no explicit short answer could be found. Finally 1% of the documents have a passage annotated with a short answer that is “yes” or “no”, instead of a list of short spans.",,A BERT Baseline for the Natural Questions,https://arxiv.org/abs/1901.08634,,"training examples, 7,830 development examples",
2181,Navigation_Turing_Test,3D Action Recognition,3D Action Recognition,3D Action Recognition,"3D, Image, Video",,Computer Vision,,Microsoft Research License Agreement (MSR-LA),https://github.com/microsoft/NTT,https://paperswithcode.com/dataset/navigation-turing-test,"Replay data from human players and AI agents navigating in a 3D game environment.

Introduced in ""Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation"" [ICML 2021] to learn how to evaluate humanlike behavior in agents.",2021,Microsoft Research License Agreement (MSR-LA),https://github.com/microsoft/NTT/blob/main/LICENSE/LICENSE-MSRLA.pdf,,,
2182,NBA_player_performance_prediction_dataset,Graph Reconstruction,Graph Reconstruction,"Graph Reconstruction, Time Series Prediction, Graph Attention, Graph Learning, Graph Mining","3D, Graph, Time Series",,Methodology,,,https://anonymous.4open.science/r/NBA-GNN-prediction,https://paperswithcode.com/dataset/nba-player-performance-prediction-dataset,"The dataset covers the 2022-23 NBA regular season (2022-10-18 to 2023-01-20) which contains 691 games in 92 game days. There are 582 active players among the 30 teams. Besides 7 basic statistics, we collected 3 tracking statistics, and 3 advanced statistics. We use tracking statistics to more accurately reflect players' movements on the court, and advanced statistics to more properly represent a player's effectiveness and contribution to the game. Together, these two types of data give us a better understanding of factors that are not visible on the scoreboard.",2022,,,,,
2183,NBA_SportVU,Trajectory Forecasting,Trajectory Forecasting,"Trajectory Forecasting, Trajectory Prediction, Trajectory Modeling",Time Series,,Methodology,trajectory-modeling-on-nba-sportvu,,https://github.com/linouk23/NBA-Player-Movements,https://paperswithcode.com/dataset/nba-sportvu,"The NBA SportVU dataset contains player and ball trajectories for 631 games from the 2015-2016 NBA season. The raw tracking data is in the JSON format, and each moment includes information about the identities of the players on the court, the identities of the teams, the period, the game clock, and the shot clock.",2015,,,,,
2184,NCBI_Datasets,Named Entity Recognition (NER),Named Entity Recognition (NER),"Named Entity Recognition (NER), Feature Engineering","Image, Text",English,Computer Vision,,,https://ncbi.github.io/datasets/,https://paperswithcode.com/dataset/ncbi-disease-corpus,"NCBI Datasets is a valuable resource that simplifies the process of gathering data from various NCBI databases. Whether you’re a researcher, scientist, or bioinformatician, NCBI Datasets provides an efficient way to access sequence information, annotations, and metadata for genes and genomes.",,,,,,
2185,NCBI_Disease,UIE,UIE,"UIE, Named Entity Recognition (NER), Named Entity Recognition","Image, Text",English,Computer Vision,"named-entity-recognition-ner-on-ncbi-disease, named-entity-recognition-on-ncbi-disease-1, named-entity-recognition-on-ncbi-disease, uie-on-ncbi-disease",,https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/,https://paperswithcode.com/dataset/ncbi-disease-1,"The NCBI Disease corpus consists of 793 PubMed abstracts, which are separated into training (593), development (100) and test (100) subsets. The NCBI Disease corpus is annotated with disease mentions, using concept identifiers from either MeSH or OMIM.",,A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization,https://arxiv.org/abs/1812.06081,,,
2186,NCLT,Pose Estimation,Pose Estimation,"Pose Estimation, Visual Place Recognition, Visual Localization","3D, Image",,Computer Vision,,ODbL,http://robots.engin.umich.edu/nclt/,https://paperswithcode.com/dataset/nclt,"The NCLT dataset is a large scale, long-term autonomy dataset for robotics research collected on the University of Michigan’s North Campus. The dataset consists of omnidirectional imagery, 3D lidar, planar lidar, GPS, and proprioceptive sensors for odometry collected using a Segway robot. The dataset was collected to facilitate research focusing on long-term autonomous operation in changing environments. The dataset is comprised of 27 sessions spaced approximately biweekly over the course of 15 months. The sessions repeatedly explore the campus, both indoors and outdoors, on varying trajectories, and at different times of the day across all four seasons. This allows the dataset to capture many challenging elements including: moving obstacles (e.g., pedestrians, bicyclists, and cars), changing lighting, varying viewpoint, seasonal and weather changes (e.g., falling leaves and snow), and long-term structural changes caused by construction projects.",,http://robots.engin.umich.edu/nclt/nclt.pdf,http://robots.engin.umich.edu/nclt/nclt.pdf,,,
2187,NCT-CRC-HE-100K,Image Classification,Image Classification,"Image Classification, Medical Image Classification",Image,,Computer Vision,"medical-image-classification-on-nct-crc-he, image-classification-on-nct-crc-he-100k",,https://zenodo.org/record/1214456,https://paperswithcode.com/dataset/nct-crc-he-100k,"The NCT-CRC-HE-100K dataset is a set of 100,000 non-overlapping image patches extracted from 86 H$\&$E stained human cancer tissue slides and normal tissue from the NCT biobank (National Center for Tumor Diseases) and the UMM pathology archive (University Medical Center Mannheim). While the dataset Colorectal Cacner-Validation-Histology-7K (CRC-VAL-HE-7K) consist of 7180 images extracted from 50 patients with colorectal adenocarcinoma and were used to create a dataset that does not overlap with patients in the NCT-CRC-HE-100K dataset. It was created by pathologists by manually delineating tissue regions in whole slide images into the following nine tissue classes: Adipose (ADI), background (BACK), debris (DEB), lymphocytes (LYM), mucus (MUC), smooth muscle (MUS), normal colon mucosa (NORM), cancer-associated stroma (STR), colorectal adenocarcinoma epithelium (TUM).",,https://www.cs.unc.edu/~mn/sites/default/files/macenko2009.pdf,https://www.cs.unc.edu/~mn/sites/default/files/macenko2009.pdf,7180 images,Validation-Histology-7K (CRC-VAL-HE-7K) consist of 7180 images,
2188,NDPSID_-_WACV_2019,Cross-Domain Iris Presentation Attack Detection,Cross-Domain Iris Presentation Attack Detection,"Cross-Domain Iris Presentation Attack Detection, Iris Recognition",Image,,Computer Vision,,,https://cvrl.nd.edu/projects/data/#notre-dame-photometric-stereo-iris-dataset-wacv-2019,https://paperswithcode.com/dataset/ndpsid-wacv-2019,"This database offers iris images (with and without contact lenses) of the same eyes captured shortly one after another with illumination coming from two different locations. 5,796 iris images in total were acquired by the LG IrisAccess 4000 sensor from 119 subjects. This set is divided into four subsets used in the experiments: (a) 1,800 images of irises wearing regular (with dot-like pattern) textured contact lenses, as shown in Fig. 6a in the wAcv 2019 paper; (b) 864 images of irises wearing irregular (without dot-like pattern) textured contact lenses, as shown in Fig. 6b in the WACV 2019 paper; (c) 1,728 images of irises wearing clear contact lenses (without any visible pattern), and (d) 1,404 images of authentic irises without any contact.",2019,Unknown,https://cvrl.nd.edu/media/django-summernote/2020-04-06/a0c289d2-e0b6-4cda-90ed-02e5d079c566.pdf,800 images,,
2189,NEEQ_Annual_Reports,Business Taxonomy Construction,Business Taxonomy Construction,Business Taxonomy Construction,,,Methodology,,,https://github.com/SenticNet/neeq-annual-reports,https://paperswithcode.com/dataset/neeq-annual-reports,Business taxonomies automatically constructed from the content of corporate annual reports.,,,,,,
2190,NELL-995,Link Prediction,Link Prediction,"Link Prediction, Complex Query Answering",Time Series,,Methodology,"link-prediction-on-nell-995, complex-query-answering-on-nell-995",,,https://paperswithcode.com/dataset/nell-995%0A,NELL-995 KG Completion Dataset,,,,,,
2191,NELL,Link Prediction,Link Prediction,"Link Prediction, Complex Query Answering, Node Classification","Image, Time Series",,Computer Vision,"node-classification-on-nell, link-prediction-on-nell-995, complex-query-answering-on-nell-995, complex-query-answering-on-nell995",,http://rtw.ml.cmu.edu/rtw/,https://paperswithcode.com/dataset/nell,"NELL is a dataset built from the Web via an intelligent agent called Never-Ending Language Learner. This agent attempts to learn over time to read the web. NELL has accumulated over 50 million candidate beliefs by reading the web, and it is considering these at different levels of confidence. NELL has high confidence in 2,810,379 of these beliefs.",,"A Survey on Knowledge Graphs: Representation, Acquisition and Applications",https://arxiv.org/abs/2002.00388,,,
2192,NEMO-Corpus,Named Entity Recognition (NER),Named Entity Recognition (NER),"Named Entity Recognition (NER), Nested Named Entity Recognition, Morphological Disambiguation","Image, Text",English,Computer Vision,"named-entity-recognition-on-nemo-corpus-token-1, named-entity-recognition-on-nemo-corpus-morph-1, named-entity-recognition-ner-on-nemo-corpus",,https://github.com/OnlpLab/NEMO-Corpus,https://paperswithcode.com/dataset/nemo-corpus,"Named Entity (NER) annotations of the Hebrew Treebank (Haaretz newspaper) corpus, including: morpheme and token level NER labels, nested mentions, and more.
We publish the NEMO corpus in the TACL paper ""Neural Modeling for Named Entities and Morphology (NEMO^2)"" [1], where we use it in extensive experiments and analyses, showing the importance of morphological boundaries for neural modeling of NER in morphologically rich languages. Code for these models and experiments can be found in the NEMO code repo.

Main features:

Morpheme, token-single and token-multi sequence labels. Morpheme labels provide exact boundaries, token-multi provide partial sub-word morphological but no exact boundaries, token-single provides only token-level information. 
All annotations are in BIOSE format (B=Begin, I=Inside, O=Outside, S=Singleton, E=End).
Widely-used OntoNotes entity category set: GPE (geo-political entity), PER (person), LOC (location), ORG (organization), FAC (facility), EVE (event), WOA (work-of-art), ANG (language), DUC (product).
NEMO includes NER annotations for the two major versions of the Hebrew Treebank, UD (Universal Dependency) and SPMRL. These can be aligned to the other morphosyntactic information layers of the treebank using bclm
We provide nested mentions. Only the first, widest, layer is used in the NEMO^2 paper. We invite you to take on this challenge!
Guidelines used for annotation are provided here.
Corpus was annotated by two native Hebrew speakers of academic education, and curated by the project manager. We provide the original annotations made by the annotators as well to promote work on learning with disagreements.
Annotation was performed using WebAnno (version 3.4.5)

Basic Corpus Statistics
|                              | train        | dev          | test          |
|------------------------------|           --:|           --:|            --:|
|  Sentences                   |  4,937       |  500         |  706          |
|  Tokens                      |  93,504      |  8,531       |  12,619       |
|  Morphemes                   |  127,031     |  11,301      |  16,828       |
|  All mentions                |  6,282       |  499         |  932          |
|  Type: Person         (PER)  |  2,128       |  193         |  267          |
|  Type: Organization   (ORG)  |  2,043       |  119         |  408          |
|  Type: Geo-Political  (GPE)  |  1,377       |  121         |  195          |
|  Type: Location       (LOC)  |  331         |  28          |  41           |
|  Type: Facility       (FAC)  |  163         |  12          |  11           |
|  Type: Work-of-Art    (WOA)  |  114         |  9           |  6            |
|  Type: Event          (EVE)  |  57          |  12          |  0            |
|  Type: Product        (DUC)  |  36          |  2           |  3            |
|  Type: Language       (ANG)  |  33          |  3           |  1            |

Evaluation
An evaluation script is provided in the NEMO code repo along with evaluation instructions.

Citation
@article{10.1162/tacl_a_00404,
    author = {Bareket, Dan and Tsarfaty, Reut},
    title = ""{Neural Modeling for Named Entities and Morphology (NEMO2)}"",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {909-928},
    year = {2021},
    month = {09},
    abstract = ""{Named Entity Recognition (NER) is a fundamental NLP task, commonly formulated as classification over a sequence of tokens. Morphologically rich languages (MRLs) pose a challenge to this basic formulation, as the boundaries of named entities do not necessarily coincide with token boundaries, rather, they respect morphological boundaries. To address NER in MRLs we then need to answer two fundamental questions, namely, what are the basic units to be labeled, and how can these units be detected and classified in realistic settings (i.e., where no gold morphology is available). We empirically investigate these questions on a novel NER benchmark, with parallel token- level and morpheme-level NER annotations, which we develop for Modern Hebrew, a morphologically rich-and-ambiguous language. Our results show that explicitly modeling morphological boundaries leads to improved NER performance, and that a novel hybrid architecture, in which NER precedes and prunes morphological decomposition, greatly outperforms the standard pipeline, where morphological decomposition strictly precedes NER, setting a new performance bar for both Hebrew NER and Hebrew morphological decomposition tasks.}"",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00404},
    url = {https://doi.org/10.1162/tacl\_a\_00404},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00404/1962472/tacl\_a\_00404.pdf},
}",2021,,,,,
2193,NeoRL-2,Offline RL,Offline RL,Offline RL,,,Methodology,,CC BY 4.0,https://github.com/polixir/NeoRL2,https://paperswithcode.com/dataset/neorl2,"NeoRL-2 includes new task scenarios that better reflect real-world task properties and includes traditional control methods as the data-collecting method. In summary, our contributions are as follows:



Tasks in NeoRL-2 cover a wider range of application domains, including robotics, aircraft, industrial pipelines, controllable nuclear fusion, healthcare, etc., encompassing key features such as delays, external factors, and safety constraints.



The data-collecting method in NeoRL-2 better aligns with real-world scenarios, employing deterministic methods for sampling. In some specific tasks, classical feedback controllers, such as Proportional-Integral-Derivative (PID) controller, are introduced.



We conducted experiments on these tasks using state-of-the-art (SOTA) offline RL algorithms and found that in most tasks, the trained policy of the current offline RL algorithms did not significantly outperform the behavior policy.



By extending near real-world tasks in NeoRL-2, we hope that the development and implementation of RL in real-world scenarios can take into account these challenges and tackle more realistic domains.",,,,,,
2194,NeoRL,MuJoCo,MuJoCo,"MuJoCo, Offline RL",,,Methodology,mujoco-on-neorl,CC BY,http://polixir.ai/research/neorl,https://paperswithcode.com/dataset/neorl,"NeoRL is a collection of environments and datasets for offline reinforcement learning with a special focus on real-world applications. The design follows real-world properties like the conservative of behavior policies, limited amounts of data, high-dimensional state and action spaces, and the highly stochastic nature of the environments.
The datasets include robotics, industrial control, finance trading and city management tasks with real-world properties, containing three-level sizes of dataset, three-level quality of data to mimic the dataset we will meet in offline RL scenarios. 
Users can use the dataset to evaluate offline RL algorithms with near real-world application nature.",,,,,,
2195,NERDS_360,Instance Segmentation,Instance Segmentation,"Instance Segmentation, Depth Estimation, Semantic Segmentation, Generalizable Novel View Synthesis, Novel View Synthesis, 6D Pose Estimation","3D, Image",,Computer Vision,generalizable-novel-view-synthesis-on-nerds,,https://github.com/zubair-irshad/NeO-360#-dataset,https://paperswithcode.com/dataset/nerds-360,"We present a large-scale dataset for 3D urban scene understanding. Compared to existing datasets, our dataset consists of 75 outdoor urban scenes with diverse backgrounds, encompassing over 15,000 images. These scenes offer 360◦ hemispherical views, capturing diverse foreground objects illuminated under various lighting conditions. Additionally, our dataset encompasses scenes that are not limited to forward-driving views, addressing the limitations of previous datasets such as limited overlap and coverage between camera
views. The closest pre-existing dataset for generalizable evaluation is DTU [2] (80 scenes) which comprises mostly indoor objects and does not provide multiple foreground objects or background scenes.

We use the Parallel Domain synthetic data generation to render high-fidelity 360◦ scenes. We select 3 different maps i.e. SF 6thAndMission, SF GrantAndCalifornia and SF VanNessAveAndTurkSt and sample 75 different scenes across all 3 maps as our backgrounds (All 75 scenes across 3 maps are significantly different road scenes from each other, captured at different viewpoints in the city). We select 20 different cars in 50 different textures for training and randomly sample from 1 to 4 cars to render in a scene. We refer to this dataset as NeRDS 360: NeRF for Reconstruction, Decomposition and Scene Synthesis of 360◦ outdoor scenes. In total, we generate 15k renderings by sampling 200 cameras in a 
 hemispherical dome at a fixed distance from the center of cars. We held out 5 scenes with 4 different cars and different backgrounds for testing, comprising 100 cameras distributed uniformly sampled in the upper hemisphere, different from the camera distributions used for training. We use the diverse validation camera distribution to test our approach’s ability to generalize to unseen viewpoints as well as unseen scenes during training. Our dataset and the corresponding task is extremely challenging due to occlusions, diversity of backgrounds, and rendered objects with various lightning and shadows. 

Our task entails reconstructing 360◦ hemispherical views of complete scenes using a handful of observations i.e. 1 to 5 as shown by red cameras whereas evaluating using all 100 hemispherical views, hence our task requires strong priors for novel view synthesis of outdoor scenes. 

Tasks our dataset support:


Generaliazable Novel view synthesis (Few shot evaluation)
Novel view synthesis (Overfitting evaluation)
6D pose estimation
Object editing 
Depth estimation
Semantic Segmentation
Instance Segmentation",,,,000 images,,
2196,NeRF,Novel View Synthesis,Novel View Synthesis,Novel View Synthesis,,,Methodology,novel-view-synthesis-on-nerf,,https://www.matthewtancik.com/nerf,https://paperswithcode.com/dataset/nerf,Neural Radiance Fields (NeRF) is a method for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. The dataset contains three parts with the first 2 being synthetic renderings of objects called Diffuse Synthetic 360◦ and Realistic Synthetic 360◦ while the third is real images of complex scenes. Diffuse Synthetic 360◦ consists of four Lambertian objects with simple geometry. Each object is rendered at 512x512 pixels from viewpoints sampled on the upper hemisphere. Realistic Synthetic 360◦ consists of eight objects of complicated geometry and realistic non-Lambertian materials. Six of them are rendered from viewpoints sampled on the upper hemisphere and the two left are from viewpoints sampled on a full sphere with all of them at 800x800 pixels. The real images of complex scenes consist of 8 forward-facing scenes captured with a cellphone at a size of 1008x756 pixels.,,,,,,
2197,NES-MDB,Music Generation,Music Generation,"Music Generation, Audio Generation","Audio, Text",English,Natural Language Processing,,,https://github.com/chrisdonahue/nesmdb,https://paperswithcode.com/dataset/nes-mdb,"The Nintendo Entertainment System Music Database (NES-MDB) is a dataset intended for building automatic music composition systems for the NES audio synthesizer. It consists of  5278 songs from the soundtracks of 397 NES games. The dataset represents 296 unique composers, and the songs contain more than two million notes combined. It has file format options for MIDI, score and NLM (NES Language Modeling).",,,,,,
2198,Netflix_Prize,Fairness,Fairness,"Fairness, Matrix Completion, Recommendation Systems",,,Methodology,collaborative-filtering-on-netflix,Custom,https://www.netflixprize.com/,https://paperswithcode.com/dataset/netflix-prize,"Netflix Prize consists of about 100,000,000 ratings for 17,770 movies given by 480,189 users. Each rating in the training dataset consists of four entries: user, movie, date of grade, grade. Users and movies are represented with integer IDs, while ratings range from 1 to 5.",,The netflix prize,http://brettb.net/project/papers/2007%20The%20Netflix%20Prize.pdf,,,
2199,NetHack_Learning_Environment,NetHack Score,NetHack Score,NetHack Score,,,Methodology,score-on-nethack-learning-environment,,https://github.com/facebookresearch/nle,https://paperswithcode.com/dataset/nethack-learning-environment,"The NetHack Learning Environment (NLE) is a Reinforcement Learning environment based on NetHack 3.6.6. It is designed to provide a standard reinforcement learning interface to the game, and comes with tasks that function as a first step to evaluate agents on this new environment.
NetHack is one of the oldest and arguably most impactful videogames in history, as well as being one of the hardest roguelikes currently being played by humans. It is procedurally generated, rich in entities and dynamics, and overall an extremely challenging environment for current state-of-the-art RL agents, while being much cheaper to run compared to other challenging testbeds. Through NLE, the authors wish to establish NetHack as one of the next challenges for research in decision making and machine learning.",,,,,,
2200,Netzschleuder,Network Embedding,Network Embedding,"Network Embedding, Network Pruning, Graph Clustering, Network Community Partition, Graphon Estimation, Graph Reconstruction, Graph Mining","3D, Graph",,Methodology,,AGPLv3,https://networks.skewed.de,https://paperswithcode.com/dataset/netzschleuder,"This is a catalogue and repository of network datasets with the aim of aiding scientific research.

This website is meant to be browsed both by humans and machines alike, and can also be accessed via a convenient JSON API, or via the graph-tool library. The network datasets themselves are available in several machine-readable formats, in particular gt, GraphML, GML and CSV.

The upstream origin of each dataset is meant to be as transparent as possible. Each dataset contains its own publicly available extraction and parsing script, accessible via a git repository, which also includes the entire code for this website, released as Free Software under the AGPLv3.

Users are encouraged to inspect the entire pipeline from original upstream data publication, downloading, parsing and format conversion.

Users are also welcome to report problems or omissions with the datasets, as well as suggest new ones, either by opening an issue, or simply by forking the git repository and proposing a merge request.",,,,,,
2201,New3,AMR Parsing,AMR Parsing,"AMR Parsing, AMR-to-Text Generation",Text,English,Natural Language Processing,"amr-to-text-generation-on-new3, amr-parsing-on-new3",LDC User Agreement for Non-Members,https://catalog.ldc.upenn.edu/LDC2020T02,https://paperswithcode.com/dataset/new3,"New3, a set of 527 instances from AMR 3.0, whose original source was the LORELEI DARPA project – not included in the AMR 2.0 training set – consisting of excerpts from newswires and online forum.",,LDC User Agreement for Non-Members,https://catalog.ldc.upenn.edu/license/ldc-non-members-agreement.pdf,527 instances,,
2202,NewB,Bias Detection,Bias Detection,Bias Detection,Image,,Computer Vision,,,https://github.com/JerryWei03/NewB,https://paperswithcode.com/dataset/newb,"A text corpus of more than 200,000 sentences from eleven news sources regarding Donald Trump.",,,,000 sentences,,
2203,Newsela,Text Simplification,Text Simplification,Text Simplification,Text,English,Natural Language Processing,text-simplification-on-newsela,,https://newsela.com/data/,https://paperswithcode.com/dataset/newsela,"The Newsela dataset was introduced by Xu et al. in their research on text simplification. It is a corpus that includes thousands of news articles professionally leveled to different reading complexities. The dataset is used for academic research in fields such as text difficulty and text simplification. It is made available to academic partners upon request. The dataset is often used as a benchmark in the field of text simplification. Please note that the Newsela dataset is different from the NELA datasets, which are collections of news articles for the study of media bias and other applications.",,,,,,
2204,NewsQA,Question Answering,Question Answering,"Question Answering, Reading Comprehension",Text,English,Natural Language Processing,question-answering-on-newsqa,Custom,https://www.microsoft.com/en-us/research/project/newsqa-dataset/,https://paperswithcode.com/dataset/newsqa,"The NewsQA dataset is a crowd-sourced machine reading comprehension dataset of 120,000 question-answer pairs.


Documents are CNN news articles.
Questions are written by human users in natural language.
Answers may be multiword passages of the source text.
Questions may be unanswerable.
NewsQA is collected using a 3-stage, siloed process.
Questioners see only an article’s headline and highlights.
Answerers see the question and the full article, then select an answer passage.
Validators see the article, the question, and a set of answers that they rank.
NewsQA is more natural and more challenging than previous datasets.",,Trischler et al,https://arxiv.org/pdf/1611.09830v3.pdf,,,
2205,NEWSROOM,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Text Summarization, Document Summarization",Text,English,Natural Language Processing,,Custom (non-commercial),http://lil.nlp.cornell.edu/newsroom/,https://paperswithcode.com/dataset/newsroom,CORNELL NEWSROOM is a large dataset for training and evaluating summarization systems. It contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. The summaries are obtained from search and social metadata between 1998 and 2017 and use a variety of summarization strategies combining extraction and abstraction.,1998,,,,,
2206,New_Brown_Corpus,Language Acquisition,Language Acquisition,Language Acquisition,Text,English,Natural Language Processing,,,https://github.com/dylanebert/nbc,https://paperswithcode.com/dataset/new-brown-corpus,A new dataset for training and evaluating grounded language models.,,,,,,
2207,New_College,Visual Odometry,Visual Odometry,"Visual Odometry, Simultaneous Localization and Mapping, Loop Closure Detection",Image,,Computer Vision,,,https://www.robots.ox.ac.uk/~mobile/MOOS/wiki/pmwiki.php,https://paperswithcode.com/dataset/new-college,"The New College Data is a freely available dataset collected from a robot completing several loops outdoors around the New College campus in Oxford. The data includes odometry, laser scan, and visual information. The dataset URL is not working anymore.",,,,,,
2208,New_York_Times_Annotated_Corpus,Text Summarization,Text Summarization,"Text Summarization, UIE, Hierarchical Multi-label Classification, Joint Entity and Relation Extraction, Abstractive Text Summarization, Open Information Extraction, Relation Extraction, Relationship Extraction (Distant Supervised), Document Dating, Topic Models, Document Summarization","Graph, Image, Text",English,Computer Vision,"document-dating-on-nyt, uie-on-nyt, joint-entity-and-relation-extraction-on-nyt, open-information-extraction-on-nyt, hierarchical-multi-label-classification-on-18, relation-extraction-on-nyt, relationship-extraction-distant-supervised-on, relationship-extraction-distant-supervised-on-2, topic-models-on-nyt","Custom (research-only, non-commercial)",https://catalog.ldc.upenn.edu/LDC2008T19,https://paperswithcode.com/dataset/new-york-times-annotated-corpus,"The New York Times Annotated Corpus contains over 1.8 million articles written and published by the New York Times between January 1, 1987 and June 19, 2007 with article metadata provided by the New York Times Newsroom, the New York Times Indexing Service and the online production staff at nytimes.com. The corpus includes:


Over 1.8 million articles (excluding wire services articles that appeared during the covered period).
Over 650,000 article summaries written by library scientists.
Over 1,500,000 articles manually tagged by library scientists with tags drawn from a normalized indexing vocabulary of people, organizations, locations and topic descriptors.
Over 275,000 algorithmically-tagged articles that have been hand verified by the online production staff at nytimes.com.
As part of the New York Times' indexing procedures, most articles are manually summarized and tagged by a staff of library scientists. This collection contains over 650,000 article-summary pairs which may prove to be useful in the development and evaluation of algorithms for automated document summarization. Also, over 1.5 million documents have at least one tag. Articles are tagged for persons, places, organizations, titles and topics using a controlled vocabulary that is applied consistently across articles. For instance if one article mentions ""Bill Clinton"" and another refers to ""President William Jefferson Clinton"", both articles will be tagged with ""CLINTON, BILL"".",1987,"Custom (research-only, non-commercial)",https://catalog.ldc.upenn.edu/license/the-new-york-times-annotated-corpus-ldc2008t19.pdf,,,
2209,NExT-QA,Zero-Shot Video Question Answer,Zero-Shot Video Question Answer,"Zero-Shot Video Question Answer, Temporal/Casual QA, Question Answering, Video Question Answering","Text, Time Series, Video",English,Natural Language Processing,"temporal-casual-qa-on-next-qa, video-question-answering-on-next-qa, zero-shot-video-question-answer-on-next-qa, question-answering-on-next-qa-open-ended, zero-shot-video-question-answer-on-next-gqa",MIT,https://github.com/doc-doc/NExT-QA,https://paperswithcode.com/dataset/next-qa,"NExT-QA is a VideoQA benchmark targeting the explanation of video contents. It challenges QA models to reason about the causal and temporal actions and understand the rich object interactions in daily activities, e.g., ""why is the boy crying?"" and ""How does the lady react after the boy fall backward?"". It supports both multi-choice and generative open-ended QA tasks. The videos are untrimmed and the questions usually invoke local video contents for answers.",,,,,,
2210,NIH-CXR-LT,Image Classification,Image Classification,"Image Classification, Long-tail Learning, Medical Image Classification",Image,,Computer Vision,long-tail-learning-on-nih-cxr-lt,,https://nihcc.app.box.com/v/ChestXray-NIHCC/folder/174256157515,https://paperswithcode.com/dataset/nih-cxr-lt,"NIH-CXR-LT. NIH ChestXRay14 contains over 100,000 chest X-rays labeled with 14 pathologies, plus a “No Findings” class. We construct a single-label, long-tailed version of the NIH ChestXRay14 dataset by introducing five new disease findings described above. The resulting NIH-CXR-LT dataset has 20 classes, including 7 head classes, 10 medium classes, and 3 tail classes. NIH-CXR-LT contains 88,637 images labeled with one of 19 thorax diseases, with 68,058 training and 20,279 test images. The validation and balanced test sets contain 15 and 30 images per class, respectively.",,,,637 images,"training and 20,279 test images",20
2211,NIH3T3,Cell Segmentation,Cell Segmentation,Cell Segmentation,Image,,Computer Vision,,,https://murphylab.web.cmu.edu/data/2009_ISBI_Nuclei.html,https://paperswithcode.com/dataset/nih3t3,The archive contains original images from NIH3T3 cells stained with Hoechst 33342 as PNG files. It also contains images (as Photoshop and GIMP files) showing hand-segmentation of the Hoechst images into regions containing single nuclei.,,,,,,
2212,Niko_Chord_Progression_Dataset,Music Generation,Music Generation,"Music Generation, Piano Music Modeling, Music Style Transfer, Music Information Retrieval","Audio, Text",English,Natural Language Processing,,MIT,https://github.com/billyblu2000/accomontage2,https://paperswithcode.com/dataset/niko-chord-progression-dataset,"Introduction
The Niko Chord Progression Dataset is used in AccoMontage2. It contains 5k+ chord progression pieces, labeled with styles. There are four styles in total: Pop Standard, Pop Complex, Dark and R&B. Some progressions have an 'Unknown' style. Some statistics are provided below.

|                            | Mean  | Variance |
| -------------------------- | ----- | -------- |
| Note Pitch                 | 57    | 167.70   |
| Note Velocity              | 79.05 | 457.89   |
| Note Duration (in seconds) | 1.38  | 1.62     |

Data Formats
You can access the Niko Chord Progression Dataset in two formats: MIDI format and the quantized note matrix format. 

MIDI (dataset.zip)
Each chord progression piece is stored as a single MIDI file.

Quantized Note Matrix (dataset.pkl)
A python dictionary with format like the following. nmatis an 2-d matrix, each row represent a quantized note: [start, end, pitch, velocity]. <u>Each note is quantized at the eighth note level. eg., start=2 means the note begins at the third eighth note.</u> root is also an 2-d matrix. It labels the roots of the chords using an eighth note sample rate. Each row of the root represents a bar. Each element is an integer ranged from 0 (C note) to 11 (B note).

```python
{'piece name': 
    {'nmat': [[0, 3, 60, 60], ...],    # 2-d matrix: note matrix
     'root': [[0,0,0,0,0,0,0,0], ...], # 2-d matrix: root label
     'style': 'some style',            # pop_standard, pop_complex, dark, r&b, unknown
     'mode': 'some mode',              # M, m
     'tonic': 'some tonic'             # C, Db, ... B
    }, 
 ...
}

load the dataset using pickle
import pickle
with open('dataset_path_and_name.pkl', 'rb') as file:
    dataset = pickle.load(file)
```

Supplementary description
Original Dataset
The Niko Chord Progression Dataset is a re-organized version of the original Niko Dataset. The original Niko Dataset have duplicate progressions and unnecessary labels, it was thus processed and converted to this version.

Style Mapping
The style label was mapped from the original dataset to the new dataset. The style label in the original dataset is stored as folder names, and thus the style can be obtained from the file path. The following shows a detailed description of the style mapping function.

// Structure of the original dataset
.
├─A Major - F# Minor        ---&gt; progressions are sorted based on tonics and modes
│  ├─1 - Best Melodies      ---&gt; eliminated
│  │  ├─Catchy
│  │  ├─Dark_HipHop_Trap
│  │  ├─EDM
│  │  ├─Emotional
│  │  ├─Pop
│  │  └─R&amp;B_Neosoul
│  ├─2 - Best Chords
│  │  ├─Dark_HipHop_Trap    ---&gt; New style: Dark
│  │  ├─EDM
│  │  │  ├─Classy_7th_9th   ---&gt; New style: Pop Complex
│  │  │  ├─Emotional        ---&gt; New style: Pop Complex
│  │  │  └─Standard         ---&gt; New style: Pop Standard
│  │  ├─Emotional           ---&gt; New style: Pop Complex
│  │  ├─Pop
│  │  │  ├─Classy_7th_9th   ---&gt; New style: Pop Complex
│  │  │  ├─Emotional        ---&gt; New style: Pop Complex
│  │  │  └─Standard         ---&gt; New style: Pop Standard
│  │  └─R&amp;B_Neosoul         ---&gt; New style: R&amp;B
│  └─3 - Rest Of Pack
│      ├─A-Bm-D (I-ii-IV)   ---&gt; progressions sorted based on root pattern
│      │  ├─Arps            ---&gt; eliminated
│      │  ├─Basslines       ---&gt; eliminated
│      │  ├─Chord Breakdown ---&gt; New style: Unknown
│      │  ├─Chord Progression -&gt; New style: Unknown
│      │  ├─Epic Endings    ---&gt; eliminated
│      │  ├─Fast Chord Rhythm -&gt; eliminated
│      │  │  ├─Back &amp; Forth 
│      │  │  └─Same Time    
│      │  ├─Melodies        ---&gt; eliminated
│      │  │  ├─115-130bpm
│      │  │  ├─130-160bpm
│      │  │  ├─160-180bpm
│      │  │  └─90-115bpm
│      │  └─Slow Chord Rhythm -&gt; New style: Unknown
...

Cite
L. Yi, H. Hu, J. Zhao, and G. Xia, “AccoMontage2: A Complete Harmonization and Accompaniment Arrangement System”, in Proceedings of the 23rd International Society for Music Information Retrieval Conference, Bengaluru, India, 2022.

License
MIT Licensed. Copyright © 2022 New York University Shanghai Music X Lab. All rights reserved.",2022,,,,,
2213,NinaPro_DB2,EMG Gesture Recognition,EMG Gesture Recognition,EMG Gesture Recognition,Image,,Computer Vision,,,http://ninaweb.hevs.ch/node/17,https://paperswithcode.com/dataset/ninapro-db2,"The second Ninapro database includes 40 intact subjects and it is thoroughly described in the paper: ""Manfredo Atzori, Arjan Gijsberts, Claudio Castellini, Barbara Caputo, Anne-Gabrielle Mittaz Hager, Simone Elsig, Giorgio Giatsidis, Franco Bassetto & Henning Müller. Electromyography data for non-invasive naturally-controlled robotic hand prostheses. Scientific Data, 2014"" (http://www.nature.com/articles/sdata201453).
Please, cite this paper for any work related to the Ninapro database.
Please, use also the paper by Gijsberts et al., 2014 (http://publications.hevs.ch/index.php/publications/show/1629) for more information about the database.",2014,,,,,
2214,Ninapro_DB5,EMG Gesture Recognition,EMG Gesture Recognition,EMG Gesture Recognition,Image,,Computer Vision,,Creative Commons Attribution No Derivatives 4.0 International,http://ninapro.hevs.ch/DB5_DoubleMyo,https://paperswithcode.com/dataset/ninapro-db5,"The 5th Ninapro database includes 10 intact subjects recorded with two Thalmic Myo (https://www.myo.com/) armbands.
The database can be used to test the Myo armbands separately as well.
The 5th Ninapro database is thoroughly described in the paper: ""Stefano Pizzolato, Luca Tagliapietra, Matteo Cognolato, Monica Reggiani, Henning Müller, Manfredo Atzori, Comparison of six electromyography acquisition setups on hand movement classification tasks, PLOS One, 2017""

Acquisition Protocol
The subjects have to repeat several movements represented by movies that are shown on the screen of a laptop.
The experiment is divided in three exercises:


Basic movements of the fingers
Isometric, isotonic hand configurations and basic wrist movements
Grasping and functional movements

During the acquisition, the subjects were asked to repeat the movements with the right hand. Each movement repetition lasted 5 seconds and was followed by 3 seconds of rest.
The protocol includes 6 repetitions of 52 different movements (plus rest) performed by 10 intact subjects. The movements were selected from the hand taxonomy as well as from hand robotics literature.

Acquisition Setup
The muscular activity is gathered using 2 Thalmic Myo armbands. The database can be used to test the Myo armbands separately as well.
The subjects in this database wore two Myo armbands one next to the other, including 16 active single–differential wireless electrodes. The top Myo armband is placed closed to the elbow with the first sensor placed on the radio humeral joint, as in the standard Ninapro configuration for the equally spaced electrodes; the second Myo armband is placed just after the first, nearer to the hand, tilted of 22.5 degrees. This configuration provides an extended uniform muscle mapping at an extremely affordable cost. The Myo sensors do not require the arm to be shaved and after few minutes the armband tighten very firmly to the arm of the subject.
The sEMG signals are sampled at a rate of 200 Hz.",2017,,,,,
2215,NINCO,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Out-of-Distribution Detection, Open Set Learning",Image,,Computer Vision,,Creative Commons Attribution 4.0 International,https://github.com/j-cb/NINCO,https://paperswithcode.com/dataset/ninco,"The NINCO (No ImageNet Class Objects) dataset is introduced in the ICML 2023 paper In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation. The images in this dataset are free from objects that belong to any of the 1000 classes of ImageNet-1K (ILSVRC2012), which makes NINCO suitable for evaluating out-of-distribution detection on ImageNet-1K .

The NINCO main dataset consists of 64 OOD classes with a total of 5879 samples. These OOD classes were selected to have no categorical overlap with any classes of ImageNet-1K. Each sample was inspected individually by the authors to not contain ID objects.

Besides NINCO, included are (in the same .tar.gz file) truly OOD versions of 11 popular OOD datasets with in total 2715 OOD samples.

Further included are 17 OOD unit-tests, with 400 samples each.

Code for loading and evaluating on each of the three datasets is provided at https://github.com/j-cb/NINCO.

When using NINCO, please consider citing (besides the bibtex given below) the following data sources that were used to create NINCO:

Hendrycks et al.: ”Scaling out-of-distribution detection for real-world settings”, ICML, 2022.  
Bossard et al.: ”Food-101 – mining discriminative components with random forests”, ECCV 2014.  
Zhou et al.: ”Places: A 10 million image database for scene recognition”, IEEE PAMI 2017.  
Huang et al.: ”Mos: Towards scaling out-of-distribution detection for large semantic space”, CVPR 2021.  
Li et al.: ”Caltech 101 (1.0)”, 2022.
Ismail et al.: ”MYNursingHome: A fully-labelled image dataset for indoor object classification.”, Data in Brief (V. 32) 2020.
The iNaturalist project: https://www.inaturalist.org/

When using NINCO_popular_datasets_subsamples, additionally to the above, please consider citing:

Cimpoi et al.: ”Describing textures in the wild”, CVPR 2014.  
Hendrycks et al.: ”Natural adversarial examples”, CVPR 2021.  
Wang et al.: ”Vim: Out-of-distribution with virtual-logit matching”, CVPR 2022.  
Bendale et al.: ”Towards Open Set Deep Networks”, CVPR 2016.  
Vaze et al.: ”Open-set Recognition: a Good Closed-set Classifier is All You Need?”, ICLR 2022.  
Wang et al.: ”Partial and Asymmetric Contrastive Learning for Out-of-Distribution Detection in Long-Tailed Recognition.” ICML, 2022.  
Galil et al.: “A framework for benchmarking Class-out-of-distribution detection and its application to ImageNet”, ICLR 2023.",2023,,,5879 samples,"tests, with 400 samples",1000
2216,Niramai_Oncho_Dataset,Medical Diagnosis,Medical Diagnosis,"Medical Diagnosis, Classification, Medical Image Registration, 2D Semantic Segmentation",Image,,Medical,,CC BY-NC-ND,https://github.com/NiramalHealthAnalytix/Niramai_Oncho_Dataset,https://paperswithcode.com/dataset/niramai-oncho-dataset,"Onchocerciasis is causing blindness in over half a million people in the world today. Drug development for the disease is crippled as there is no way of measuring effectiveness of the drug without an invasive procedure. Drug efficacy measurement through assessment of viability of onchocerca worms requires the patients to undergo nodulectomy which is invasive, expensive, time-consuming, skill-dependent, infrastructure dependent and lengthy process.

The motivation of Niramai Oncho Dataset is to develop algorithms that can detect Onchocerca worms non-invasively using thermal imaging. The dataset consists of both thermal images and videos captured for each nodule site during imaging of a participant. Histopathological confirmation of the presence of female adult worm is used to obtain the ground truth. In total, the dataset consists of 125 participants' data with 192 palpable nodules. Out of 192 palpable nodules, 101 correspond to live female nodules and the remain 91 correspond to dead nodules.",,,,,,
2217,NIST__OSN-transmitted_-_Facebook_,Image Manipulation,Image Manipulation,"Image Manipulation, Image Manipulation Localization, Image Forensics, Image Forgery Detection, Image Manipulation Detection",Image,,Computer Vision,image-manipulation-detection-on-nist-osn,,https://github.com/HighwayWu/ImageForensicsOSN,https://paperswithcode.com/dataset/nist-osn-transmitted,"This dataset is an OSN-transmitted (Online Social Network) version of the NIST dataset (https://www.nist.gov/itl/iad/mig/nimble-challenge-2017-evaluation). Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",2017,,,,,
2218,NIST__OSN-transmitted_-_Wechat_,Image Manipulation Detection,Image Manipulation Detection,Image Manipulation Detection,Image,,Computer Vision,image-manipulation-detection-on-nist-osn-1,,https://github.com/HighwayWu/ImageForensicsOSN,https://paperswithcode.com/dataset/nist-osn-transmitted-wechat,"This dataset is an OSN-transmitted (Online Social Network) version of the NIST dataset (https://www.nist.gov/itl/iad/mig/nimble-challenge-2017-evaluation). Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",2017,,,,,
2219,NIST__OSN-transmitted_-_Weibo_,Image Manipulation Detection,Image Manipulation Detection,Image Manipulation Detection,Image,,Computer Vision,image-manipulation-detection-on-nist-osn-3,,https://github.com/HighwayWu/ImageForensicsOSN,https://paperswithcode.com/dataset/nist-osn-transmitted-weibo,"This dataset is an OSN-transmitted (Online Social Network) version of the NIST dataset (https://www.nist.gov/itl/iad/mig/nimble-challenge-2017-evaluation). Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",2017,,,,,
2220,NIST__OSN-transmitted_-_Whatsapp_,Image Manipulation Detection,Image Manipulation Detection,Image Manipulation Detection,Image,,Computer Vision,image-manipulation-detection-on-nist-osn-2,,https://github.com/HighwayWu/ImageForensicsOSN,https://paperswithcode.com/dataset/nist-osn-transmitted-whatsapp,"This dataset is an OSN-transmitted (Online Social Network) version of the NIST dataset (https://www.nist.gov/itl/iad/mig/nimble-challenge-2017-evaluation). Unfortunately, OSNs automatically apply operations like compression and resizing, which reduce valuable information necessary for image forgery detection. As a result, this dataset presents a greater challenge for forgery detection compared to the original non-OSN-transmitted version.

The dataset is available here: https://github.com/HighwayWu/ImageForensicsOSN - more specifically: https://drive.google.com/file/d/1uMNZdhX3bYAZNcVGlkCvrnj5lSLW1ld5/view?usp=sharing and was presented in:

[Wu et al., 2022] Wu, H., Zhou, J., Tian, J., Liu, J., and Qiao, Y. (2022). Robust image forgery detection against transmission over online social networks. IEEE Transactions on Information Forensics and Security.",2017,,,,,
2221,NL2GQL_Dataset,Text-to-GQL,Text-to-GQL,Text-to-GQL,Text,English,Natural Language Processing,,,https://github.com/zhiqix/NL2GQL/tree/main,https://paperswithcode.com/dataset/nl2gql-dataset,"A bilingual (English and Chinese natural language queries) dataset which has NL queries annotated with their corresponding GQL queries (i.e. nGQL). Each data sample in the train data contains 4 pieces of information: prompt represents a natural language query, content represents a standard nGQL, reason represents the inference part that needs to be output by the reranker, and schema represents the code structure schema corresponding to this sentence. Each data sample in the test data contains 6 pieces of information, prompt represents natural language query, content represents gold nGQL, text_schema is used for the vanilla experiment, schema represents the code structure schema corresponding to this sentence, class represents which graph database space this sentence corresponds to, and result represents the results obtained using gold nGQL.",,,,,,
2222,NLC2CMD,Code Translation,Code Translation,"Code Translation, Machine Translation",Text,English,Natural Language Processing,code-translation-on-nlc2cmd,,https://nlc2cmd.us-east.mybluemix.net/#/,https://paperswithcode.com/dataset/nlc2cmd,"The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of natural
language processing to the command line. Participants were tasked with building models
that can transform descriptions of command line tasks in English to their Bash syntax.",2020,,,,,
2223,NLI-PT,Language Acquisition,Language Acquisition,"Language Acquisition, Language Identification, Native Language Identification",Text,English,Natural Language Processing,,,http://www.clul.ulisboa.pt/en/recurso/portuguese-native-language-identification-dataset,https://paperswithcode.com/dataset/nli-pt,"The first Portuguese dataset compiled for Native Language Identification (NLI), the task of identifying an author's first language based on their second language writing. The dataset includes 1,868 student essays written by learners of European Portuguese, native speakers of the following L1s: Chinese, English, Spanish, German, Russian, French, Japanese, Italian, Dutch, Tetum, Arabic, Polish, Korean, Romanian, and Swedish. NLI-PT includes the original student text and four different types of annotation: POS, fine-grained POS, constituency parses, and dependency parses. NLI-PT can be used not only in NLI but also in research on several topics in the field of Second Language Acquisition and educational NLP.",,,,,,
2224,NLVR,Visual Reasoning,Visual Reasoning,Visual Reasoning,Image,,Reasoning,"visual-reasoning-on-nlvr, visual-reasoning-on-nlvr2-test, visual-reasoning-on-nlvr2-dev",,http://lil.nlp.cornell.edu/nlvr/,https://paperswithcode.com/dataset/nlvr,"NLVR contains 92,244 pairs of human-written English sentences grounded in synthetic images. Because the images are synthetically generated, this dataset can be used for semantic parsing.",,,,,,
2225,NMED-T,Eeg Decoding,Eeg Decoding,Eeg Decoding,,,Methodology,,,https://exhibits.stanford.edu/data/catalog/jn859kj8079,https://paperswithcode.com/dataset/nmed-t,"Losorelli, Steven, Nguyen, Duc T., Dmochowski, Jacek P., and Kaneshiro, Blair

This dataset contains cortical (EEG) and behavioral data collected during natural music listening. Dense-array EEG was recorded from 20 adult participants who each heard a set of 10 full-length songs with electronically produced beats at various tempos. In a separate subsequent listen, each participant tapped to the beat of a 35-second excerpt from each song. Participants also delivered ratings of familiarity and enjoyment for each full-length song during the EEG recording. Finally, the dataset includes basic demographic information about the participants, as well as Matlab scripts to perform the illustrated analyses presented in the paper introducing the dataset (Losorelli et al., 2017). Cleaned and aggregated data are published in Matlab format; raw EEG is published in Matlab format, while raw tapping data are published in .txt format. Stimulus audio is not published, but metadata links are provided.",2017,,,,,
2226,NNE,Named Entity Recognition (NER),Named Entity Recognition (NER),"Named Entity Recognition (NER), Nested Named Entity Recognition","Image, Text",English,Computer Vision,nested-named-entity-recognition-on-nne,,,https://paperswithcode.com/dataset/nne,NNE is a dataset for Nested Named Entity Recognition in English Newswire,,,,,,
2227,NOAA_Atmospheric_Temperature_Dataset,Weather Forecasting,Weather Forecasting,Weather Forecasting,Time Series,,Methodology,weather-forecasting-on-noaa-atmospheric,,https://www.ncdc.noaa.gov/sotc/global/201513,https://paperswithcode.com/dataset/noaa-atmospheric-temperature-dataset,"This dataset contains meteorological observations (temperature) at the
land-based weather stations located in the United States, collected from the Online Climate
Data Directory of the National Oceanic and Atmospheric Administration (NOAA). The weather
stations are sampled from the Western and Southeastern states that have actively measured
meteorological observations during 2015. The 1-year sequential data of hourly temperature
records are divided into small sequences of 24 hours. For training, validation, and test a sequential
8-2-2 (months) split is used.",2015,,,,,
2228,NoCaps,Object Detection,Object Detection,"Object Detection, Image Captioning","Image, Text",English,Computer Vision,"image-captioning-on-nocaps-near-domain, image-captioning-on-nocaps-out-of-domain, image-captioning-on-nocaps-xd-entire, image-captioning-on-nocaps-entire, image-captioning-on-nocaps-in-domain, image-captioning-on-nocaps-xd-out-of-domain, image-captioning-on-nocaps-val-in-domain, image-captioning-on-nocaps-xd-near-domain, image-captioning-on-nocaps-val-out-domain, image-captioning-on-nocaps-val-near-domain, image-captioning-on-nocaps-val, image-captioning-on-nocaps-val-overall, image-captioning-on-nocaps-xd-in-domain",,https://nocaps.org/,https://paperswithcode.com/dataset/nocaps,"The nocaps benchmark consists of 166,100 human-generated captions describing 15,100 images from the OpenImages validation and test sets.",,,,100 images,,
2229,Noise_of_Web,Cross-modal retrieval with noisy correspondence,Cross-modal retrieval with noisy correspondence,"Cross-modal retrieval with noisy correspondence, Image-text Retrieval","Image, Text",English,Computer Vision,,CC-BY-NC 4.0,https://huggingface.co/datasets/NJUyued/NoW,https://paperswithcode.com/dataset/noise-of-web-now,"Noise of Web (NoW) is a challenging noisy correspondence learning (NCL) benchmark for robust image-text matching/retrieval models. It contains 100K image-text pairs consisting of website pages and multilingual website meta-descriptions (98,000 pairs for training, 1,000 for validation, and 1,000 for testing). NoW has two main characteristics: without human annotations and the noisy pairs are naturally captured.  The source image data of NoW is obtained by taking screenshots when accessing web pages on mobile user interface (MUI) with 720 $\times$ 1280 resolution, and we parse the meta-description field in the HTML source code as the captions. In NCR (predecessor of NCL), each image in all datasets were preprocessed using Faster-RCNN detector provided by Bottom-up Attention Model to generate 36 region proposals, and each proposal was encoded as a 2048-dimensional feature. Thus, following NCR, we release our the features instead of raw images for fair comparison. However, we can not just use detection methods like Faster-RCNN to extract image features since it is trained on real-world animals and objects on MS-COCO. To tackle this, we adapt APT as the detection model since it is trained on MUI data. Then, we capture the 768-dimensional features of top 36 objects for one image. Due to the automated and non-human curated data collection process, the noise in NoW is highly authentic and intrinsic.  The estimated noise ratio of this dataset is nearly 70%.",2048,APT,https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_Mobile_User_Interface_Element_Detection_via_Adaptively_Prompt_Tuning_CVPR_2023_paper.pdf,,"val models. It contains 100K image-text pairs consisting of website pages and multilingual website meta-descriptions (98,000 pairs for training, 1,000 for validation, and 1,000 for testing). NoW has two main characteristics: without human annotations and the noisy pairs are naturally captured.  The source image data of NoW is obtained by taking screenshots when accessing web pages on mobile user interface (MUI) with 720 $\times$ 1280 resolution, and we parse the meta-description field in the HTML source code as the captions. In NCR (predecessor of NCL), each image in all datasets were preprocessed using Faster-RCNN detector provided by Bottom-up Attention Model to generate 36 region proposals, and each proposal was encoded as a 2048-dimensional feature. Thus, following NCR, we release our the features instead of raw images",
2230,NoLiMa,Long-Context Understanding,Long-Context Understanding,Long-Context Understanding,,,Methodology,,,https://huggingface.co/datasets/amodaresi/NoLiMa,https://paperswithcode.com/dataset/nolima,"A benchmark extending needle-in-a-haystack (NIAH) test with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack.",,,,,,
2231,NomBank,Semantic Parsing,Semantic Parsing,"Semantic Parsing, Reading Comprehension, Semantic Role Labeling",Text,English,Natural Language Processing,,,https://nlp.cs.nyu.edu/meyers/NomBank.html,https://paperswithcode.com/dataset/nombank,"NomBank is an annotation project at New York University that is related to the PropBank project at the University of Colorado.  The goal is to mark the sets of arguments that cooccur with nouns in the PropBank Corpus (the Wall Street Journal Corpus of the Penn Treebank), just as PropBank records such information for verbs.  As a side effect of the annotation process, the authors are producing a number of other resources including various dictionaries, as well as PropBank style lexical entries called frame files. These resources help the user label the various arguments and adjuncts of the head nouns with roles (sets of argument labels for each sense of each noun).  NYU and U of Colorado are making a coordinated effort to insure that, when possible, role definitions are consistent across parts of speech. For example, PropBank's frame file for the verb ""decide"" was used in the annotation of the noun ""decision"".",,,,,,
2232,Nordland,Visual Place Recognition,Visual Place Recognition,Visual Place Recognition,Image,,Computer Vision,visual-place-recognition-on-nordland,,,https://paperswithcode.com/dataset/nordland,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
2233,NoReC,Opinion Mining,Opinion Mining,"Opinion Mining, Sentiment Analysis",Text,English,Natural Language Processing,,,https://github.com/ltgoslo/norec,https://paperswithcode.com/dataset/norec,"The Norwegian Review Corpus (NoReC) was created for the purpose of training and evaluating models for document-level sentiment analysis. More than 43,000 full-text reviews have been collected from major Norwegian news sources and cover a range of different domains, including literature, movies, video games, restaurants, music and theater, in addition to product reviews across a range of categories. Each review is labeled with a manually assigned score of 1–6, as provided by the rating of the original author.",,,,,,
2234,notebookcdg,Code Comment Generation,Code Comment Generation,"Code Comment Generation, Code Summarization, Code Documentation Generation",Text,English,Natural Language Processing,,CC-BY,https://github.com/dakuo/haconvgnn,https://paperswithcode.com/dataset/notebookcdg,"Inspired by Wang et al. 2021, we decided to utilize the top-voted and well-documented Kaggle notebooks to construct the notebookCDGdataset

We collected the top 10% highly-voted notebooks from the top 20 popular competitions on Kaggle (e.g. Titanic). We checked the data policy of each of the 20 competitions, none of them has copyright issues. We also contacted the Kaggle administrators to make sure our data collection complies with the platform’s policy.

In total, we collected 3,944 notebooks as raw data. After data preprocessing, the final dataset contains 2,476 notebooks out of the 3,944 notebooks from the raw data. It has 28,625 code–documentation pairs. The overall code-to-markdown ratio is 2.2195

Download notebookCDG dataset",2021,,,,,
2235,Nottingham,Music Modeling,Music Modeling,Music Modeling,Audio,,Audio,music-modeling-on-nottingham,,https://ifdo.ca/~seymour/nottingham/nottingham.html,https://paperswithcode.com/dataset/nottingham,The Nottingham Dataset is a collection of 1200 American and British folk songs.,,Rethinking Recurrent Latent Variable Model for Music Composition,https://arxiv.org/abs/1810.03226,,,
2236,NoW,Cross-modal retrieval with noisy correspondence,Cross-modal retrieval with noisy correspondence,"Cross-modal retrieval with noisy correspondence, Image-text Retrieval","Image, Text",English,Computer Vision,,CC-BY-NC 4.0,https://huggingface.co/datasets/NJUyued/NoW,https://paperswithcode.com/dataset/now,"Noise of Web (NoW) is a challenging noisy correspondence learning (NCL) benchmark for robust image-text matching/retrieval models. It contains 100K image-text pairs consisting of website pages and multilingual website meta-descriptions (98,000 pairs for training, 1,000 for validation, and 1,000 for testing). NoW has two main characteristics: without human annotations and the noisy pairs are naturally captured.  The source image data of NoW is obtained by taking screenshots when accessing web pages on mobile user interface (MUI) with 720 $\times$ 1280 resolution, and we parse the meta-description field in the HTML source code as the captions. In NCR (predecessor of NCL), each image in all datasets were preprocessed using Faster-RCNN detector provided by Bottom-up Attention Model to generate 36 region proposals, and each proposal was encoded as a 2048-dimensional feature. Thus, following NCR, we release our the features instead of raw images for fair comparison. However, we can not just use detection methods like Faster-RCNN to extract image features since it is trained on real-world animals and objects on MS-COCO. To tackle this, we adapt APT as the detection model since it is trained on MUI data. Then, we capture the 768-dimensional features of top 36 objects for one image. Due to the automated and non-human curated data collection process, the noise in NoW is highly authentic and intrinsic.  The estimated noise ratio of this dataset is nearly 70%.",2048,APT,https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_Mobile_User_Interface_Element_Detection_via_Adaptively_Prompt_Tuning_CVPR_2023_paper.pdf,,"val models. It contains 100K image-text pairs consisting of website pages and multilingual website meta-descriptions (98,000 pairs for training, 1,000 for validation, and 1,000 for testing). NoW has two main characteristics: without human annotations and the noisy pairs are naturally captured.  The source image data of NoW is obtained by taking screenshots when accessing web pages on mobile user interface (MUI) with 720 $\times$ 1280 resolution, and we parse the meta-description field in the HTML source code as the captions. In NCR (predecessor of NCL), each image in all datasets were preprocessed using Faster-RCNN detector provided by Bottom-up Attention Model to generate 36 region proposals, and each proposal was encoded as a 2048-dimensional feature. Thus, following NCR, we release our the features instead of raw images",
2237,NRHints-RealCapture,Image Relighting,Image Relighting,"Image Relighting, Novel View Synthesis",Image,,Computer Vision,,,https://github.com/iamNCJ/NRHints,https://paperswithcode.com/dataset/nrhints-realcapture,A high-quality captured dataset for object relighting. Covering a wide range of geometry and material.,,,,,,
2238,NRHints-Synthetic,Image Relighting,Image Relighting,"Image Relighting, Novel View Synthesis",Image,,Computer Vision,,,https://github.com/iamNCJ/NRHints,https://paperswithcode.com/dataset/nrhints-synthetic,A high-quality synthetic dataset for object relighting. Covering a wide range of geometry and material.,,,,,,
2239,NSynth,Audio Generation,Audio Generation,"Audio Generation, Music Generation, Pitch Classification, Instrument Recognition, Few-Shot Audio Classification, Self-Supervised Learning","Audio, Image, Text",English,Audio,"instrument-recognition-on-nsynth, few-shot-audio-classification-on-nsynth, pitch-classification-on-nsynth",CC BY 4.0,https://magenta.tensorflow.org/datasets/nsynth,https://paperswithcode.com/dataset/nsynth,"NSynth is a dataset of one shot instrumental notes, containing 305,979 musical notes with unique pitch, timbre and envelope. The sounds were collected from 1006 instruments from commercial sample libraries and are annotated based on their source (acoustic, electronic or synthetic), instrument family and sonic qualities. The instrument families used in the annotation are bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead and vocal. Four second monophonic 16kHz audio snippets were generated (notes) for the instruments.",,Data Augmentation for Instrument Classification Robust to Audio Effects,https://arxiv.org/abs/1907.08520,,,
2240,NT-VOT211,Video Object Tracking,Video Object Tracking,Video Object Tracking,"Image, Video",,Computer Vision,video-object-tracking-on-nv-vot211,MIT,https://github.com/LiuYuML/NV-VOT211,https://paperswithcode.com/dataset/nv-vot211,"NT-VOT211 consists of 211 diverse videos, offering 211,000 well-annotated frames with 8 attributes including camera motion, deformation, fast motion, motion blur, tiny target, distractors, occlusion and out-of-view. To the best of our knowledge, it is the largest night-time tracking benchmark to-date that is specifically designed to address unique challenges such as adverse visibility, image blur, and distractors inherent to night-time tracking scenarios. 

Kindly click the 'Homepage' button below to be directed to our official website. There, you can easily access the independent leaderboard through the 'Evaluation on Server' section on our homepage. We offer comprehensive, step-by-step guidance on utilizing our dataset, and we also provide the annotation tools that were instrumental in the creation of this dataset.",,,,,,
2241,NTIRE_2021_HDR,Single-Image-Based Hdr Reconstruction,Single-Image-Based Hdr Reconstruction,Single-Image-Based Hdr Reconstruction,"3D, Image",,Computer Vision,,,https://data.vision.ee.ethz.ch/cvl/ntire21/,https://paperswithcode.com/dataset/ntire-2021-hdr,"The NTIRE 2021 HDR was built for the first challenge on high-dynamic range (HDR) imaging that was part of the New Trends in Image Restoration and Enhancement (NTIRE) workshop, held in conjunction with CVPR 2021. The challenge aims at estimating a HDR image from one or multiple respective low-dynamic range (LDR) observations, which might suffer from under- or over-exposed regions and different sources of noise. The challenge is composed by two tracks: In Track 1 only a single LDR image is provided as input, whereas in Track 2 three differently-exposed LDR images with inter-frame motion are available. In both tracks, the ultimate goal is to achieve the best objective HDR reconstruction in terms of PSNR with respect to a ground-truth image, evaluated both directly and with a canonical tone mapping operation.",2021,,,,,
2242,NTPairs,Causal Inference,Causal Inference,Causal Inference,,,Methodology,,,https://github.com/bywords/NTPairs,https://paperswithcode.com/dataset/ntpairs,"The NTPairs dataset consists of the pairs of news articles and their corresponding tweets that were published by eight media outlets in 2018. The eight outlets were selected to consider diverse outlets, which employ a different editing style for news sharing, in terms of publishing channels and political leaning.",2018,,,,,
2243,NTU_RGB_D,Zero Shot Skeletal Action Recognition,Zero Shot Skeletal Action Recognition,"Zero Shot Skeletal Action Recognition, Skeleton Based Action Recognition, Generalized Zero Shot skeletal action recognition, Human Interaction Recognition, Action Recognition In Videos, Early Action Prediction, Self-supervised Skeleton-based Action Recognition, 3D Action Recognition, Human action generation, Pose Prediction, Unsupervised Skeleton Based Action Recognition, Action Recognition","3D, Image, Text, Time Series, Video",English,Computer Vision,"self-supervised-skeleton-based-action, unsupervised-skeleton-based-action, action-recognition-in-videos-on-ntu-rgb-d, human-interaction-recognition-on-ntu-rgb-d, generalized-zero-shot-skeletal-action, early-action-prediction-on-ntu-rgb-d, action-recognition-in-videos-on-ntu-rgbd, 3d-action-recognition-on-ntu-rgb-d-1, human-action-generation-on-ntu-rgb-d, skeleton-based-action-recognition-on-ntu-rgbd, zero-shot-skeletal-action-recognition-on-ntu, pose-prediction-on-filtered-ntu-rgbd","Custom (research-only, non-commercial, attribution)",https://github.com/shahroudy/NTURGB-D,https://paperswithcode.com/dataset/ntu-rgb-d,"NTU RGB+D is a large-scale dataset for RGB-D human action recognition. It involves 56,880 samples of 60 action classes collected from 40 subjects. The actions can be generally divided into three categories: 40 daily actions (e.g., drinking, eating, reading), nine health-related actions (e.g., sneezing, staggering, falling down), and 11 mutual actions (e.g., punching, kicking, hugging). These actions take place under 17 different scene conditions corresponding to 17 video sequences (i.e., S001–S017). The actions were captured using three cameras with different horizontal imaging viewpoints, namely, −45∘,0∘, and +45∘. Multi-modality information is provided for action characterization, including depth maps, 3D skeleton joint position, RGB frames, and infrared sequences. The performance evaluation is performed by a cross-subject test that split the 40 subjects into training and test groups, and by a cross-view test that employed one camera (+45∘) for testing, and the other two cameras for training.",,Action Recognition for Depth Video using Multi-view Dynamic Images,https://arxiv.org/abs/1806.11269,880 samples,,
2244,NTU_RGB_D_120,Zero Shot Skeletal Action Recognition,Zero Shot Skeletal Action Recognition,"Zero Shot Skeletal Action Recognition, Skeleton Based Action Recognition, Generalized Zero Shot skeletal action recognition, Human Interaction Recognition, Self-supervised Skeleton-based Action Recognition, Human action generation, One-Shot 3D Action Recognition, Unsupervised Skeleton Based Action Recognition, Action Recognition, Self-Supervised Human Action Recognition, Few-Shot Skeleton-Based Action Recognition","3D, Image, Text, Video",English,Computer Vision,"unsupervised-skeleton-based-action-1, generalized-zero-shot-skeletal-action-1, human-interaction-recognition-on-ntu-rgb-d-1, self-supervised-human-action-recognition-on, one-shot-3d-action-recognition-on-ntu-rgbd, few-shot-skeleton-based-action-recognition-on, action-recognition-in-videos-on-ntu-rgbd-120, self-supervised-skeleton-based-action-1, human-action-generation-on-ntu-rgb-d-120, skeleton-based-action-recognition-on-ntu-rgbd-1, zero-shot-skeletal-action-recognition-on-ntu-1",Custom (research-only),http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp,https://paperswithcode.com/dataset/ntu-rgb-d-120,"NTU RGB+D 120 is a large-scale dataset for RGB+D human action recognition, which is collected from 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. This dataset contains 120 different action classes including daily, mutual, and health-related activities.",,NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding,https://arxiv.org/pdf/1905.04757v2.pdf,,,
2245,NTU_RGB_D_2D,Human action generation,Human action generation,"Human action generation, motion prediction","Image, Text, Time Series, Video",English,Computer Vision,human-action-generation-on-ntu-rgb-d-2d,"Custom (research-only, non-commercial, attribution)",https://rose1.ntu.edu.sg/dataset/actionRecognition/,https://paperswithcode.com/dataset/ntu-rgb-d-2d,NTU RGB+D 2D is a curated version of NTU RGB+D often used for skeleton-based action prediction and synthesis. It contains less number of actions.,,,,,,
2246,NuiSI_Dataset,Human motion prediction,Human motion prediction,"Human motion prediction, Human Interaction Recognition, Motion Generation","Image, Text, Time Series, Video",English,Computer Vision,,,https://github.com/souljaboy764/nuisi_dataset,https://paperswithcode.com/dataset/nuisi-dataset,"The NuiSI dataset contains skeleton tracking trajectories of Human Interaction Partners performing a variety of physically interactive behaviors (waving, handshaking, rocket fistbump, parachute fistbump) with each other. This is inspired by the dataset in Bütepage et al. ""Imitating by generating: Deep generative models for imitation of interactive tasks."" Frontiers in Robotics and AI  (2020) wherein they capture a dataset with rokoko motion capture suits. Instead we track the skeletons of the interaction partner with Intel Realsense cameras using Nuitrack, for a more realistic scenario, with noise coming from the depth sensor, the skeleton tracking and some partial occlusions. This makes it more representative of real world interactions with a Robot equipped with an RGBD camera.
T
This dataset is used in our papers for training Interaction models for Human-Robot Interaction with a humanoid social robot. If you find the dataset useful in your work, please cite our paper:

Prasad, V., Heitlinger, L., Koert, D., Stock-Homburg, R., Peters, J., & Chalvatzaki, G. (2023). Learning multimodal latent dynamics for human-robot interaction. arXiv preprint arXiv:2311.16380.

@article{prasad2023learning,
  title={Learning multimodal latent dynamics for human-robot interaction},
  author={Prasad, Vignesh and Heitlinger, Lea and Koert, Dorothea and Stock-Homburg, Ruth and Peters, Jan and Chalvatzaki, Georgia},
  journal={arXiv preprint arXiv:2311.16380},
  year={2023}
}",2020,,,,,
2247,NumtaDB,Image Classification,Image Classification,"Image Classification, Handwritten Digit Recognition",Image,,Computer Vision,,CC-BY-SA,https://www.kaggle.com/datasets/BengaliAI/numta,https://paperswithcode.com/dataset/numtadb,"To benchmark Bengali digit recognition algorithms, a large publicly available dataset is required which is free from biases originating from geographical location, gender, and age. With this aim in mind, NumtaDB, a dataset consisting of more than 85,000 images of hand-written Bengali digits, has been assembled.",,,,000 images,,
2248,NuNER,Named Entity Recognition,Named Entity Recognition,"Named Entity Recognition, NER, named-entity-recognition","Image, Text",English,Computer Vision,,,https://huggingface.co/datasets/numind/NuNER,https://paperswithcode.com/dataset/nuner,"The dataset used to pre-train NuNER from the NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data

Contains AI-extracted entities, their concepts, and descriptions from the given text",,,,,,
2249,NUS-WIDE,Cross-Modal Retrieval,Cross-Modal Retrieval,"Cross-Modal Retrieval, Multiview Clustering, Multi-Label Classification, Image Retrieval, Multi-label zero-shot learning",Image,,Computer Vision,"multi-label-zero-shot-learning-on-nus-wide, multiview-clustering-on-nus-wide, image-retrieval-on-nus-wide, multi-label-classification-on-nus-wide",,https://lms.comp.nus.edu.sg/wp-content/uploads/2019/research/nuswide/NUS-WIDE.html,https://paperswithcode.com/dataset/nus-wide,"The NUS-WIDE dataset contains 269,648 images with a total of 5,018 tags collected from Flickr. These images are manually annotated with 81 concepts, including objects and scenes.",,Parallel Grid Pooling for Data Augmentation,https://arxiv.org/abs/1803.11370,648 images,,
2250,NUS,Keyphrase Extraction,Keyphrase Extraction,Keyphrase Extraction,,,Methodology,keyphrase-extraction-on-nus,,https://www.comp.nus.edu.sg/~kanmy/papers/icadl2007.pdf,https://paperswithcode.com/dataset/nus,"The dataset was constructed by first finding suitable publications and then collecting keyphrases from manual annotators. Google SOAP API was used to find documents using variants of the query “keywords general terms filetype:pdf”. Over 250 of these PDF documents were downloaded for further processing. Documents were then manually restricted to scientific conference papers, with a length range of 4-12 pages. The PDF documents were then converted to plain text using the PDF995 software suite (as it handled two-columned text better than other programs tried). At the end of this process, 211 documents in plain text format were selected which were converted successfully without problems. The authors then recruited student volunteers from our department to participate in manual keyphrase assignments. Each volunteer was given three PDF files (with author-assigned keyphrases hidden) to assign keyphrases to.",,Homepage,https://www.comp.nus.edu.sg/~kanmy/papers/icadl2007.pdf,211 documents,,
2251,nuScenes-C,Robust Camera Only 3D Object Detection,Robust Camera Only 3D Object Detection,"Robust Camera Only 3D Object Detection, Robust 3D Object Detection, 3D Object Detection, Robust 3D Semantic Segmentation, 3D Semantic Segmentation","3D, Image",English,Computer Vision,"robust-3d-semantic-segmentation-on-nuscenes-c, robust-3d-object-detection-on-nuscenes-c, robust-camera-only-3d-object-detection-on",Creative Commons Attribution-NonCommercial-ShareAlike 3.0,https://ldkong.com/Robo3D,https://paperswithcode.com/dataset/nuscenes-c,"🤖 Robo3D - The nuScenes-C Benchmark
nuScenes-C is an evaluation benchmark heading toward robust and reliable 3D perception in autonomous driving. With it, we probe the robustness of 3D detectors and segmentors under out-of-distribution (OoD) scenarios against corruptions that occur in the real-world environment. Specifically, we consider natural corruptions happen in the following cases:


Adverse weather conditions, such as fog, wet ground, and snow;
External disturbances that are caused by motion blur or result in LiDAR beam missing;
Internal sensor failure, including crosstalk, possible incomplete echo, and cross-sensor scenarios.

SemanticKITTI-C is part of the Robo3D benchmark. Visit our homepage to explore more details.",,Robo3D,https://arxiv.org/abs/2303.17597,,,
2252,nuScenes,Bird's-Eye View Semantic Segmentation,Bird's-Eye View Semantic Segmentation,"Bird's-Eye View Semantic Segmentation, Weakly supervised Semantic Segmentation, Motion Planning, LIDAR Semantic Segmentation, Trajectory Planning, Online Vectorized HD Map Construction, Instance Segmentation, Prediction Of Occupancy Grid Maps, Motion Detection, 3D Semantic Segmentation, Monocular 3D Object Detection, Weather Forecasting, Lane Detection, 3D Object Detection, Object Detection, Trajectory Prediction, Semi-Supervised Semantic Segmentation, 3D Multi-Object Tracking, HD semantic map learning","3D, Image, Time Series, Video",English,Computer Vision,"hd-semantic-map-learning-on-nuscenes, instance-segmentation-on-nuscenes, semi-supervised-semantic-segmentation-on-25, monocular-3d-object-detection-on-nuscenes, 3d-object-detection-on-nuscenes, 3d-multi-object-tracking-on-nuscenes-camera-1, motion-detection-on-nuscenes, trajectory-planning-on-nuscenes, 3d-object-detection-on-nuscenes-camera-radar, online-vectorized-hd-map-construction-on, bird-s-eye-view-semantic-segmentation-on, lidar-semantic-segmentation-on-nuscenes, 3d-object-detection-on-nuscenes-camera-only, prediction-of-occupancy-grid-maps-on-nuscenes, trajectory-prediction-on-nuscenes, 3d-multi-object-tracking-on-nuscenes-camera-2, 3d-multi-object-tracking-on-nuscenes, object-detection-on-nuscenes, lane-detection-on-nuscenes, motion-planning-on-nuscenes, 3d-semantic-segmentation-on-nuscenes, weakly-supervised-semantic-segmentation-on-9",Custom (CC BY-NC-SA 4.0 with exceptions for startups and research; SCL),https://www.nuscenes.org/,https://paperswithcode.com/dataset/nuscenes,"The nuScenes dataset is a large-scale autonomous driving dataset. The dataset has 3D bounding boxes for 1000 scenes collected in Boston and Singapore. Each scene is 20 seconds long and annotated at 2Hz. This results in a total of 28130 samples for training, 6019 samples for validation and 6008 samples for testing. The dataset has the full autonomous vehicle data suite: 32-beam LiDAR, 6 cameras and radars with complete 360° coverage. The 3D object detection challenge evaluates the performance on 10 classes: cars, trucks, buses, trailers, construction vehicles, pedestrians, motorcycles, bicycles, traffic cones and barriers.",,PointPainting: Sequential Fusion for 3D Object Detection,https://arxiv.org/abs/1911.10150,28130 samples,"training, 6019 samples",10
2253,NVD,Counterfactual Inference,Counterfactual Inference,Counterfactual Inference,,,Methodology,,,https://counterfactualsimulation.github.io,https://paperswithcode.com/dataset/nvd,"Naturalistic Variation Object Dataset (NVD) is a large simulated dataset of 272k images of everyday objects with naturalistic variations such as object pose, scale, viewpoint, lighting and occlusions.",,Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing,https://arxiv.org/pdf/2211.16499v1.pdf,272k images,,
2254,NYCTaxi,Traffic Prediction,Traffic Prediction,Traffic Prediction,Time Series,,Methodology,traffic-prediction-on-nyctaxi,,https://github.com/Echo-Ji/ST-SSL_Dataset,https://paperswithcode.com/dataset/nyctaxi,Taxi flow data of New York City with grid 20x10.,,,,,,
2255,NYUv2,Zero-shot Scene Classification (unified classes),Zero-shot Scene Classification (unified classes),"Zero-shot Scene Classification (unified classes), Depth Completion, Instance Segmentation, Depth Estimation, Boundary Detection, Real-Time Semantic Segmentation, Semantic Segmentation, Monocular Depth Estimation, Surface Normals Estimation, Panoptic Segmentation, Scene Segmentation, 3D Semantic Scene Completion from a single RGB image, Surface Normal Estimation, 3D Object Detection, Plane Instance Segmentation, Scene Classification (unified classes), 3D Semantic Scene Completion, Multi-Task Learning","3D, Image",,Computer Vision,"boundary-detection-on-nyu-depth-v2, monocular-depth-estimation-on-nyu-depth-v2-4, depth-completion-on-nyu-depth-v2, panoptic-segmentation-on-nyu-depth-v2, 3d-semantic-scene-completion-on-nyuv2, surface-normals-estimation-on-nyu-depth-v2, depth-estimation-on-nyu-depth-v2, multi-task-learning-on-nyuv2, scene-segmentation-on-nyu-depth-v2, surface-normal-estimation-on-nyu-depth-v2, surface-normals-estimation-on-nyu-depth-v2-1, monocular-depth-estimation-on-nyu-depth-v2, zero-shot-scene-classification-unified, scene-classification-unified-classes-on-nyu, plane-instance-segmentation-on-nyu-depth-v2, 3d-semantic-scene-completion-from-a-single, semantic-segmentation-on-nyu-depth-v2, real-time-semantic-segmentation-on-nyu-depth-1, instance-segmentation-on-nyu-depth-v2, 3d-object-detection-on-nyu-depth-v2",,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,https://paperswithcode.com/dataset/nyuv2,"The NYU-Depth V2 data set is comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect. It features:


1449 densely labeled pairs of aligned RGB and depth images
464 new scenes taken from 3 cities
407,024 new unlabeled frames
Each object is labeled with a class and an instance number.
The dataset has several components:
Labeled: A subset of the video data accompanied by dense multi-class labels. This data has also been preprocessed to fill in missing depth labels.
Raw: The raw RGB, depth and accelerometer data as provided by the Kinect.
Toolbox: Useful functions for manipulating the data and labels.",,Indoor Segmentation and Support Inference from RGBD Images,http://arxiv.org/pdf/1301.3572.pdf,,,
2256,OA-Mine_-_annotations,Attribute Mining,Attribute Mining,"Attribute Mining, Attribute Value Extraction",,,Methodology,"attribute-mining-on-oa-mine-annotations, attribute-value-extraction-on-oa-mine",,https://github.com/xinyangz/OAMine,https://paperswithcode.com/dataset/oa-mine-annotations,"The dataset contains Amazon products from 10 product categories with full human annotations.
The dataset was collected in 2021. The products may have been taken down from Amazon since the collection of the dataset.",2021,,,,,
2257,OADAT,Image-to-Image Translation,Image-to-Image Translation,"Image-to-Image Translation, Unsupervised Pre-training, Semantic Segmentation, Image Reconstruction, Self-Supervised Learning","3D, Image, Text",English,Computer Vision,,Creative Commons Attribution-NonCommercial 4.0 International,https://www.research-collection.ethz.ch/handle/20.500.11850/551512,https://paperswithcode.com/dataset/oadat,"An experimental and synthetic (simulated) OA raw signals and reconstructed image domain datasets rendered with different experimental parameters and tomographic acquisition geometries.

For detailed information, see github.com/berkanlafci/oadat.",,,,,,
2258,OAD_dataset,Human Activity Recognition,Human Activity Recognition,Human Activity Recognition,"Image, Video",,Computer Vision,human-activity-recognition-on-oad-dataset,,https://www.icst.pku.edu.cn/struct/Projects/OAD.html,https://paperswithcode.com/dataset/oad-dataset,"The Online Action Detection Dataset (OAD) was captured using the Kinect V2 sensor, which collects color images, depth images and human skeleton joints synchronously.  This dataset includes 59 long sequences and 10 actions.",,,,,,
2259,OAG-L1-Field,Heterogeneous Node Classification,Heterogeneous Node Classification,Heterogeneous Node Classification,Image,,Computer Vision,heterogeneous-node-classification-on-oag-l1,,,https://paperswithcode.com/dataset/oag-l1-field,A popular dataset for node classification on heterogeneous graphs.,,,,,,
2260,OAG-Venue,Heterogeneous Node Classification,Heterogeneous Node Classification,Heterogeneous Node Classification,Image,,Computer Vision,heterogeneous-node-classification-on-oag,,,https://paperswithcode.com/dataset/oag-venue,A popular dataset for node classification on heterogeneous graphs.,,,,,,
2261,OASIS,Graph Classification,Graph Classification,"Graph Classification, Depth Estimation, Medical Image Registration, Medical Image Classification","3D, Graph, Image",,Computer Vision,"medical-image-classification-on-oasis-3, graph-classification-on-oasis, medical-image-registration-on-oasis",,https://oasis.cs.princeton.edu/,https://paperswithcode.com/dataset/oasis,"A dataset for single-image 3D in the wild consisting of annotations of detailed 3D geometry for 140,000 images.",,,,000 images,,
2262,OASST1,text annotation,text annotation,"text annotation, Text Summarization, Question Answering",Text,English,Natural Language Processing,,apache-2.0,https://huggingface.co/datasets/OpenAssistant/oasst1,https://paperswithcode.com/dataset/oasst1,"license:
apache-2.0
tags:
human-feedback
size_categories:
100K<n<1M
pretty_name:
OpenAssistant Conversations

OpenAssistant Conversations Dataset (OASST1)
Dataset Description

Homepage: https://www.open-assistant.io/
Repository: https://github.com/LAION-AI/Open-Assistant
Paper: https://arxiv.org/abs/2304.07327

Dataset Summary
In an effort to democratize research on large-scale alignment, we release OpenAssistant 
Conversations (OASST1), a human-generated, human-annotated assistant-style conversation 
corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 
quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus 
is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers.

Please refer to our paper for further details.

Dataset Structure
This dataset contains message trees. Each message tree has an initial prompt message as the root node, 
which can have multiple child messages as replies, and these child messages can have multiple replies. 

All messages have a role property: this can either be ""assistant"" or ""prompter"". The roles in 
conversation threads from prompt to leaf node strictly alternate between ""prompter"" and ""assistant"".

This version of the dataset contains data collected on the open-assistant.io website until April 12 2023.

JSON Example: Message
For readability, the following JSON examples are shown formatted with indentation on multiple lines.
Objects are stored without indentation (on single lines) in the actual jsonl files.

json
{
    ""message_id"": ""218440fd-5317-4355-91dc-d001416df62b"",
    ""parent_id"": ""13592dfb-a6f9-4748-a92c-32b34e239bb4"",
    ""user_id"": ""8e95461f-5e94-4d8b-a2fb-d4717ce973e4"",
    ""text"": ""It was the winter of 2035, and artificial intelligence (..)"",
    ""role"": ""assistant"",
    ""lang"": ""en"",
    ""review_count"": 3,
    ""review_result"": true,
    ""deleted"": false,
    ""rank"": 0,
    ""synthetic"": true,
    ""model_name"": ""oasst-sft-0_3000,max_new_tokens=400 (..)"",
    ""labels"": {
        ""spam"": { ""value"": 0.0, ""count"": 3 },
        ""lang_mismatch"": { ""value"": 0.0, ""count"": 3 },
        ""pii"": { ""value"": 0.0, ""count"": 3 },
        ""not_appropriate"": { ""value"": 0.0, ""count"": 3 },
        ""hate_speech"": { ""value"": 0.0, ""count"": 3 },
        ""sexual_content"": { ""value"": 0.0, ""count"": 3 },
        ""quality"": { ""value"": 0.416, ""count"": 3 },
        ""toxicity"": { ""value"": 0.16, ""count"": 3 },
        ""humor"": { ""value"": 0.0, ""count"": 3 },
        ""creativity"": { ""value"": 0.33, ""count"": 3 },
        ""violence"": { ""value"": 0.16, ""count"": 3 }
    }
}

JSON Example: Conversation Tree
For readability, only a subset of the message properties is shown here.

json
{
  ""message_tree_id"": ""14fbb664-a620-45ce-bee4-7c519b16a793"",
  ""tree_state"": ""ready_for_export"",
  ""prompt"": {
    ""message_id"": ""14fbb664-a620-45ce-bee4-7c519b16a793"",
    ""text"": ""Why can't we divide by 0? (..)"",
    ""role"": ""prompter"",
    ""lang"": ""en"",
    ""replies"": [
      {
        ""message_id"": ""894d30b6-56b4-4605-a504-89dd15d4d1c8"",
        ""text"": ""The reason we cannot divide by zero is because (..)"",
        ""role"": ""assistant"",
        ""lang"": ""en"",
        ""replies"": [
          // ...
        ]
      },
      {
        ""message_id"": ""84d0913b-0fd9-4508-8ef5-205626a7039d"",
        ""text"": ""The reason that the result of a division by zero is (..)"",
        ""role"": ""assistant"",
        ""lang"": ""en"",
        ""replies"": [
          {
            ""message_id"": ""3352725e-f424-4e3b-a627-b6db831bdbaa"",
            ""text"": ""Math is confusing. Like those weird Irrational (..)"",
            ""role"": ""prompter"",
            ""lang"": ""en"",
            ""replies"": [
              {
                ""message_id"": ""f46207ca-3149-46e9-a466-9163d4ce499c"",
                ""text"": ""Irrational numbers are simply numbers (..)"",
                ""role"": ""assistant"",
                ""lang"": ""en"",
                ""replies"": []
              },
              // ...
            ]
          }
        ]
      }
    ]
  }
}

Please refer to oasst-data for
details about the data structure and Python code to read and write jsonl files containing oasst data objects.

If you would like to explore the dataset yourself you can find a 
getting-started 
notebook in the notebooks/openassistant-oasst1 folder of the LAION-AI/Open-Assistant
github repository.

Main Dataset Files
Conversation data is provided either as nested messages in trees (extension .trees.jsonl.gz) 
or as a flat list (table) of messages (extension .messages.jsonl.gz).

Ready For Export Trees
2023-04-12_oasst_ready.trees.jsonl.gz       10,364 trees with 88,838 total messages
2023-04-12_oasst_ready.messages.jsonl.gz    88,838 messages
Trees in ready_for_export state without spam and deleted messages including message labels.
The oasst_ready-trees file usually is sufficient for supervised fine-tuning (SFT) & reward model (RM) training.

All Trees
2023-04-12_oasst_all.trees.jsonl.gz         66,497 trees with 161,443 total messages
2023-04-12_oasst_all.messages.jsonl.gz     161,443 messages
All trees, including those in states prompt_lottery_waiting (trees that consist of only one message, namely the initial prompt),
aborted_low_grade (trees that stopped growing because the messages had low quality), and halted_by_moderator.

Supplemental Exports: Spam & Prompts
2023-04-12_oasst_spam.messages.jsonl.gz
These are messages which were deleted or have a negative review result (""review_result"": false).
Besides low quality, a frequent reason for message deletion is a wrong language tag.

2023-04-12_oasst_prompts.messages.jsonl.gz
These are all the kept initial prompt messages with positive review result (no spam) of trees in ready_for_export or prompt_lottery_waiting state.

Using the Huggingface Datasets
While HF datasets is ideal for tabular datasets, it is not a natural fit for nested data structures like the OpenAssistant conversation trees.
Nevertheless, we make all messages which can also be found in the file 2023-04-12_oasst_ready.trees.jsonl.gz available in parquet as train/validation splits. 
These are directly loadable by Huggingface Datasets.

To load the oasst1 train & validation splits use:

python
from datasets import load_dataset
ds = load_dataset(""OpenAssistant/oasst1"")
train = ds['train']      # len(train)=84437 (95%)
val = ds['validation']   # len(val)=4401 (5%)

The messages appear in depth-first order of the message trees.

Full conversation trees can be reconstructed from the flat messages table by using the parent_id 
and message_id properties to identify the parent-child relationship of messages. The message_tree_id 
and tree_state properties (only present in flat messages files) can be used to find all messages of a message tree or to select trees by their state.

Languages
OpenAssistant Conversations incorporates 35 different languages with a distribution of messages as follows:

Languages with over 1000 messages
- English: 71956
- Spanish: 43061
- Russian: 9089
- German: 5279
- Chinese: 4962
- French: 4251
- Thai: 3042
- Portuguese (Brazil): 2969
- Catalan: 2260
- Korean: 1553
- Ukrainian: 1352
- Italian: 1320
- Japanese: 1018

<details>
  <summary><b>Languages with under 1000 messages</b></summary>
  <ul>
    <li>Vietnamese: 952</li>
    <li>Basque: 947</li>
    <li>Polish: 886</li>
    <li>Hungarian: 811</li>
    <li>Arabic: 666</li>
    <li>Dutch: 628</li>
    <li>Swedish: 512</li>
    <li>Turkish: 454</li>
    <li>Finnish: 386</li>
    <li>Czech: 372</li>
    <li>Danish: 358</li>
    <li>Galician: 339</li>
    <li>Hebrew: 255</li>
    <li>Romanian: 200</li>
    <li>Norwegian Bokmål: 133</li>
    <li>Indonesian: 115</li>
    <li>Bulgarian: 95</li>
    <li>Bengali: 82</li>
    <li>Persian: 72</li>
    <li>Greek: 66</li>
    <li>Esperanto: 59</li>
    <li>Slovak: 19</li>
  </ul>
</details>


Contact

Discord Open Assistant Discord Server
GitHub: LAION-AI/Open-Assistant
E-Mail: open-assistant@laion.ai",2023,paper,https://arxiv.org/abs/2304.07327,,,
2263,Objects365,Instance Segmentation,Instance Segmentation,"Instance Segmentation, Semantic Segmentation, Unsupervised Object Detection, Object Detection, Open Vocabulary Object Detection",Image,,Computer Vision,"object-detection-on-objects365, open-vocabulary-object-detection-on-1, unsupervised-object-detection-on-objects365","Custom (research-only, attribution)",https://www.objects365.org/overview.html,https://paperswithcode.com/dataset/objects365,"Objects365 is a large-scale object detection dataset, Objects365, which has 365 object categories over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. It is the largest object detection dataset (with full annotation) so far and establishes a more challenging benchmark for the community.",,,,,,
2264,ObjectsRoom,Image Generation,Image Generation,"Image Generation, Unsupervised Object Segmentation","Image, Text",English,Computer Vision,"unsupervised-object-segmentation-on-1, image-generation-on-objectsroom",,https://github.com/deepmind/multi_object_datasets,https://paperswithcode.com/dataset/objectsroom,"The ObjectsRoom dataset is based on the MuJoCo environment used by the Generative Query Network [4] and is a multi-object extension of the 3d-shapes dataset. The training set contains 1M scenes with up to three objects. We also provide ~1K test examples for the following variants:

2.1 Empty room: scenes consist of the sky, walls, and floor only.

2.2 Six objects: exactly 6 objects are visible in each image.

2.3 Identical color: 4-6 objects are placed in the room and have an identical, randomly sampled color.

Datapoints consist of an image and fixed number of masks. The first four masks correspond to the sky, floor, and two halves of the wall respectively. The remaining masks correspond to the foreground objects.",,,,,training set contains 1M scenes with up to three objects. We also provide ~1K test examples,
2265,Obstacle_Tower,Decision Making,Decision Making,"Decision Making, Board Games, General Reinforcement Learning",,,Methodology,"general-reinforcement-learning-on-obstacle-5, general-reinforcement-learning-on-obstacle-4, general-reinforcement-learning-on-obstacle, general-reinforcement-learning-on-obstacle-1, general-reinforcement-learning-on-obstacle-2, general-reinforcement-learning-on-obstacle-3",Custom,https://github.com/Unity-Technologies/obstacle-tower-env,https://paperswithcode.com/dataset/obstacle-tower,"Obstacle Tower is a high fidelity, 3D, 3rd person, procedurally generated environment for reinforcement learning. An agent playing Obstacle Tower must learn to solve both low-level control and high-level planning problems in tandem while learning from pixels and a sparse reward signal. Unlike other benchmarks such as the Arcade Learning Environment, evaluation of agent performance in Obstacle Tower is based on an agent’s ability to perform well on unseen instances of the environment.",,,,,,
2266,OC,Citation Recommendation,Citation Recommendation,"Citation Recommendation, Driver Attention Monitoring, Self-Driving Cars",,,Methodology,citation-recommendation-on-oc,,https://zenodo.org/record/5725116#.Y0_Mf1JBxs8,https://paperswithcode.com/dataset/oc,"These images were generated using UnityEyes simulator, after including essential eyeball physiology elements and modeling binocular vision dynamics. The images are annotated with head pose and gaze direction information, besides 2D and 3D landmarks of eye's most important features. Additionally, the images are distributed into two classes denoting the status of the eye (Open for open eyes, Closed for closed eyes). This dataset was used to train a DNN model for detecting drowsiness status of a driver. The dataset contains 1,704 training images, 4,232 testing images and additional 4,103 images for improvements.",,,,103 images,"train a DNN model for detecting drowsiness status of a driver. The dataset contains 1,704 training images",
2267,OC20,Initial Structure to Relaxed Energy (IS2RE),Initial Structure to Relaxed Energy (IS2RE),"Initial Structure to Relaxed Energy (IS2RE), Direct, Initial Structure to Relaxed Energy (IS2RE)",,,Methodology,"initial-structure-to-relaxed-energy-is2re, initial-structure-to-relaxed-energy-is2re-on",Creative Commons Attribution 4.0 License,https://github.com/Open-Catalyst-Project/ocp/blob/master/DATASET.md,https://paperswithcode.com/dataset/oc20,"Open Catalyst 2020 is a dataset for catalysis in chemical engineering. Focusing on molecules that are important in renewable energy applications, the OC20 data set comprises over 1.3 million relaxations of molecular adsorptions onto surfaces, the largest data set of electrocatalyst structures to date.",2020,,,,,
2268,OCB,Graph Regression,Graph Regression,"Graph Regression, Graph Learning, Bayesian Optimization",Graph,,Methodology,,,https://github.com/zehao-dong/CktGNN/OCB,https://paperswithcode.com/dataset/ocb,"OCB contains two graph datasets, Ckt-Bench-101 and Ckt-Bench-301, for representation learning over analog circuits. Ckt-Bench-101 and Ckt-Bench-301 contain graphs (DAGs) that represent analog circuits and provide their corresponding graph-level properties: DC gain (Gain), bandwidth (BW), phase margin (PM),Figure of Merit (FoM), which characterize the circuit performance.


Motivation: Facilitate research in representation learning and automation of analog circuits.
Tasks: graph-level prediction/regression; analog circuit search (ACS).
Node features: discrete feature that characterize device type (e.g. capacitor); and continues feature of the device (e.g. capacitance).
First open source benchmark for graph learning in analog circuits.",,,,,,
2269,Occluded_REID,Semantic Parsing,Semantic Parsing,"Semantic Parsing, Graph Matching, Person Re-Identification","Graph, Image, Text",English,Computer Vision,person-re-identification-on-occluded-reid-1,,https://github.com/tinajia2012/ICME2018_Occluded-Person-Reidentification_datasets,https://paperswithcode.com/dataset/occluded-reid,"Occluded REID is an occluded person dataset captured by mobile cameras, consisting of 2,000 images of 200 occluded persons (see Fig. (c)). Each identity has 5 full-body person images and 5 occluded person images with different types of occlusion.",,Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification,https://arxiv.org/abs/1904.04975,000 images,,
2270,OCHuman,Pose Estimation,Pose Estimation,"Pose Estimation, Keypoint Detection, Pose-Based Human Instance Segmentation, Multi-Person Pose Estimation, Human Instance Segmentation, 2D Human Pose Estimation","3D, Image",,Computer Vision,"pose-estimation-on-ochuman, multi-person-pose-estimation-on-ochuman, keypoint-detection-on-ochuman, 2d-human-pose-estimation-on-ochuman, human-instance-segmentation-on-ochuman, pose-based-human-instance-segmentation-on",,https://github.com/liruilong940607/OCHumanApi,https://paperswithcode.com/dataset/ochuman,"This dataset focuses on heavily occluded human with comprehensive annotations including bounding-box, humans pose and instance mask. This dataset contains 13,360 elaborately annotated human instances within 5081 images. With average 0.573 MaxIoU of each person, OCHuman is the most complex and challenging dataset related to human.",,,,5081 images,,
2271,OCID,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Object Recognition, Instance Segmentation, Unseen Object Instance Segmentation",Image,,Computer Vision,unseen-object-instance-segmentation-on-ocid,,https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/object-clutter-indoor-dataset/,https://paperswithcode.com/dataset/ocid,"Developing robot perception systems for handling objects in the real-world requires computer vision algorithms to be carefully scrutinized with respect to the expected operating domain. This demands large quantities of ground truth data to rigorously evaluate the performance of algorithms.

The Object Cluttered Indoor Dataset is an RGBD-dataset containing point-wise labeled point-clouds for each object. The data was captured using two ASUS-PRO Xtion cameras that are positioned at different heights. It captures diverse settings of objects, background, context, sensor to scene distance, viewpoint angle and lighting conditions. The main purpose of OCID is to allow systematic comparison of existing object segmentation methods in scenes with increasing amount of clutter. In addition OCID does also provide ground-truth data for other vision tasks like object-classification and recognition.",,,,,,
2272,OCTID,Retinal OCT Layer Segmentation,Retinal OCT Layer Segmentation,"Retinal OCT Layer Segmentation, Retinal OCT Disease Classification",Image,,Computer Vision,,,https://borealisdata.ca/dataverse/OCTID,https://paperswithcode.com/dataset/octid,"An open-source Optical Coherence Tomography Image Database containing different retinal OCT images with various pathological conditions. This comprehensive open-access database contains over 500 high-resolution images categorized into different pathological conditions. The image classes include Normal (NO), Macular Hole (MH), Age-related Macular Degeneration (AMD), Central Serous Retinopathy (CSR), and Diabetic Retinopathy (DR).",,,,,,
2273,OCW,Natural Language Understanding,Natural Language Understanding,"Natural Language Understanding, Only Connect Walls Dataset Task 1 (Grouping)",Text,English,Natural Language Processing,task-1-grouping-on-ocw,MIT,https://github.com/TaatiTeam/OCW,https://paperswithcode.com/dataset/only-connect-wall-ocw-dataset,"The OCW dataset  is for evaluating creative problem solving tasks by curating the problems and human performance results from the popular British quiz show Only Connect. 

The OCW dataset contains 618 connecting wall puzzles and solutions in total from 15 seasons of the show. Each show episode has two walls.

The dataset has two tasks: Task 1 (Grouping), and Task 2 (Connections) are identical to the quiz-show’s human participant tasks. 

Task 1 (Groupings) is evaluated via six metrics: number of solved walls, number of correct groups (max. four per wall), Adjusted Mutual Information (AMI), Adjusted Rand Index (ARI), Fowlkes Mallows Score (FMS), and Wasserstein Distance (WD), normalized to (0, 1) range, between predicted and ground-truth labels.

Task 2 (Connections) is evaluated with three metrics: exact string matching, ROUGE-1 F1, and BERTScore F1.

Baseline results with pre-trained language models and with few-shot In-context Learning (ICL) with LLMs such as GPT-4 are available here:

""Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset""
Saeid Alavi Naeini, Raeid Saqur, Mozhgan Saeidi, John Giorgi, Babak Taati.
2023
https://neurips.cc/virtual/2023/poster/73547",2023,,,,,
2274,ODAQ__Open_Dataset_of_Audio_Quality,Audio Quality Assessment,Audio Quality Assessment,"Audio Quality Assessment, Music Quality Assessment",Audio,,Audio,audio-quality-assessment-on-odaq-open-dataset,"Mixed CC BY, CC BY-NC, and CC0",https://github.com/Fraunhofer-IIS/ODAQ,https://paperswithcode.com/dataset/odaq-open-dataset-of-audio-quality,"A dataset containing the results of a MUSHRA listening test conducted with expert listeners from 2 international laboratories. ODAQ contains 240 audio samples and corresponding quality scores. Each audio sample is rated by 26 listeners. The audio samples are stereo audio signals sampled at 44.1 or 48 kHz and are processed by a total of 6 method classes, each operating at different quality levels. The processing method classes are designed to generate quality degradations possibly encountered during audio coding and source separation, and the quality levels for each method class span the entire quality range. The diversity of the processing methods, the large span of quality levels, the high sampling frequency, and the pool of international listeners make ODAQ particularly suited for further research into subjective and objective audio quality. The dataset is released with permissive licenses, and the software used to conduct the listening test is also made publicly available.",,,,,test conducted with expert listeners from 2 international laboratories. ODAQ contains 240 audio samples,
2275,ODDS,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Time Series Anomaly Detection","Image, Time Series",,Computer Vision,anomaly-detection-on-odds,,http://odds.cs.stonybrook.edu/,https://paperswithcode.com/dataset/odds,"Outliers or anomalies are instances that do not conform to the norm of a dataset. Outlier detection is an important data mining problem that has been researched within diverse research areas and applications domains such as intrusion detection, fraud detection, unusual event detection, disease condition detection etc.

The exact notion of an outlier is different for different application domains. Hence, applying a technique developed for one domain to another is not straightforward. Moreover, availability of labeled data for training/validation of outlier detection methods is scarce and often noise contained in data tends to be similar to outliers, thus makes it difficult to distinguish them. Because of these challenges outlier detection is not an easy problem to solve. Furthermore, research on outlier detection has been held back by the lack of good benchmark datasets with ground truths. Existing benchmarks are typically either proprietary or else very artificial. Moreover, existing real-world outlier/anomaly detection datasets lack the availability of ground truth.

In ODDS, we openly provide access to a large collection of outlier detection datasets with ground truth (if available). Our focus is to provide datasets from different domains and present them under a single platform for the research community. As such, we arrange the datasets based on their types into different tables in ODDS library.

The ODDS library is being actively developed since summer 2016 and is growing as a result of our research pursuits in outlier/anomaly mining and also to help the corresponding research community. Researchers are welcome to share their datasets with us to include in ODDS library by emailing srayana@cs.stonybrook.edu.",2016,,,,,
2276,ODMS,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Video Object Segmentation, Video Semantic Segmentation","Image, Video",,Computer Vision,,,https://github.com/griffbr/ODMS,https://paperswithcode.com/dataset/odms,"ODMS is a dataset for learning Object Depth via Motion and Segmentation. ODMS training data are configurable and extensible, with each training example consisting of a series of object segmentation masks, camera movement distances, and ground truth object depth. As a benchmark evaluation, the dataset provides four ODMS validation and test sets with 15,650 examples in multiple domains, including robotics and driving.",,,,650 examples,"training data are configurable and extensible, with each training example consisting of a series of object segmentation masks, camera movement distances, and ground truth object depth. As a benchmark evaluation, the dataset provides four ODMS validation and test sets with 15,650 examples",
2277,ODSQA,Task-Oriented Dialogue Systems,Task-Oriented Dialogue Systems,"Task-Oriented Dialogue Systems, Question Answering, Speech Recognition","Audio, Image, Text",English,Computer Vision,,,https://github.com/chiahsuan156/ODSQA,https://paperswithcode.com/dataset/odsqa,The ODSQA dataset is a spoken dataset for question answering in Chinese. It contains more than three thousand questions from 20 different speakers.,,,,,,
2278,Office-31,Blended-target Domain Adaptation,Blended-target Domain Adaptation,"Blended-target Domain Adaptation, Universal Domain Adaptation, Multi-target Domain Adaptation, Open-Set Multi-Target Domain Adaptation, Unsupervised Domain Adaptation, Partial Domain Adaptation, Unsupervised Domain Adaptationn, Multi-Source Unsupervised Domain Adaptation, Domain Adaptation",,,Methodology,"blended-target-domain-adaptation-on-office-31, open-set-multi-target-domain-adaptation-on, multi-target-domain-adaptation-on-office-31, multi-source-unsupervised-domain-adaptation-5, unsupervised-domain-adaptationn-on-office-31, domain-adaptation-on-office-31, unsupervised-domain-adaptation-on-office-31, universal-domain-adaptation-on-office-31, partial-domain-adaptation-on-office-31",,https://www.cc.gatech.edu/~judy/domainadapt/,https://paperswithcode.com/dataset/office-31,"The Office dataset contains 31 object categories in three domains: Amazon, DSLR and Webcam. The 31 categories in the dataset consist of objects commonly encountered in office settings, such as keyboards, file cabinets, and laptops. The Amazon domain contains on average 90 images per class and 2817 images in total. As these images were captured from a website of online merchants, they are captured against clean background and at a unified scale. The DSLR domain contains 498 low-noise high resolution images (4288×2848). There are 5 objects per category. Each object was captured from different viewpoints on average 3 times. For Webcam, the 795 images of low resolution (640×480) exhibit significant noise and color as well as white balance artifacts.",,Domain Adaptation by Mixture of Alignments of Second- or Higher-Order Scatter Tensors,https://arxiv.org/abs/1611.08195,90 images,,31
2279,Office-Home,cross-domain few-shot learning,cross-domain few-shot learning,"cross-domain few-shot learning, Blended-target Domain Adaptation, Universal Domain Adaptation, Transfer Learning, Multi-target Domain Adaptation, Open-Set Multi-Target Domain Adaptation, Domain Generalization, Unsupervised Domain Adaptation, Partial Domain Adaptation, Multi-Source Unsupervised Domain Adaptation, Domain Adaptation",,,Methodology,"cross-domain-few-shot-learning-on-office-home, domain-generalization-on-office-home, open-set-multi-target-domain-adaptation-on-1, domain-adaptation-on-office-home, transfer-learning-on-office-home, universal-domain-adaptation-on-office-home, multi-source-unsupervised-domain-adaptation-2, partial-domain-adaptation-on-office-home, multi-target-domain-adaptation-on-office-home, unsupervised-domain-adaptation-on-office-home, blended-target-domain-adaptation-on-office, unsupervised-domain-adaptation-on-office-home-1",Custom (non-commercial research and educational purposes),https://www.hemanthdv.org/officeHomeDataset.html,https://paperswithcode.com/dataset/office-home,"Office-Home is a benchmark dataset for domain adaptation which contains 4 domains where each domain consists of 65 categories. The four domains are: Art – artistic images in the form of sketches, paintings, ornamentation, etc.; Clipart – collection of clipart images; Product – images of objects without a background and Real-World – images of objects captured with a regular camera. It contains 15,500 images, with an average of around 70 images per class and a maximum of 99 images in a class.",,Multi-component Image Translation for Deep Domain Generalization,https://arxiv.org/abs/1812.08974,500 images,,65
2280,Off_Hard_parallel,SMAC+,SMAC+,SMAC+,,,Methodology,smac-on-smac-off-hard-parallel,,,https://paperswithcode.com/dataset/smac-off-hard-parallel,smac+ offensive hard scenario with 20 parallel episodic buffer.,,,,,,
2281,Off_Near_parallel,SMAC+,SMAC+,SMAC+,,,Methodology,smac-on-smac-off-near-parallel,,https://osilab-kaist.github.io/smac_plus/,https://paperswithcode.com/dataset/smac-off-near-parallel,smac+ offensive near scenario with 20 parallel episodic buffer,,,,,,
2282,Off_Superhard_parallel,SMAC+,SMAC+,SMAC+,,,Methodology,smac-on-smac-off-superhard-parallel,,,https://paperswithcode.com/dataset/smac-off-superhard-parallel,smac+ offensive scenario with 20 parallel episodic buffer.,,,,,,
2283,OG-MARL,Multi-agent Reinforcement Learning,Multi-agent Reinforcement Learning,Multi-agent Reinforcement Learning,,,Reinforcement Learning,,CC BY-NC-SA,https://sites.google.com/view/og-marl,https://paperswithcode.com/dataset/og-marl,"Diverse datasets for offline multi-agent reinforcement learning research. Includes datasets for popular MARL benchmark environments such as:


MAMuJoCo
SMAC v1 & v2
PettingZoo
FlatLand
CityLearn",,,,,,
2284,OGB-LSC,Link Prediction,Link Prediction,"Link Prediction, Knowledge Graphs, Graph Regression, Node Classification","Graph, Image, Time Series",,Computer Vision,"graph-regression-on-pcqm4m-lsc, knowledge-graphs-on-wikikg90m-lsc, node-classification-on-mag240m-lsc",,https://ogb.stanford.edu/kddcup2021/,https://paperswithcode.com/dataset/ogb-lsc,"OGB Large-Scale Challenge (OGB-LSC) is a collection of three real-world datasets for advancing the state-of-the-art in large-scale graph ML. OGB-LSC provides graph datasets that are orders of magnitude larger than existing ones and covers three core graph learning tasks -- link prediction, graph regression, and node classification. 

OGB-LSC consists of three datasets: MAG240M-LSC, WikiKG90M-LSC, and PCQM4M-LSC. Each dataset offers an independent task.


MAG240M-LSC is a heterogeneous academic graph, and the task is to predict the subject areas of papers situated in the heterogeneous graph (node classification).
WikiKG90M-LSC is a knowledge graph, and the task is to impute missing triplets (link prediction).
PCQM4M-LSC is a quantum chemistry dataset, and the task is to predict an important molecular property, the HOMO-LUMO gap, of a given molecule (graph regression).",,,,,,
2285,OGB,Graph Classification,Graph Classification,"Graph Classification, Link Property Prediction, Node Property Prediction, Link Prediction, Node Classification, Graph Property Prediction","Graph, Image, Time Series",,Computer Vision,"link-property-prediction-on-ogbl-collab, node-property-prediction-on-ogbn-arxiv, link-property-prediction-on-ogbl-ppa, node-property-prediction-on-ogbn-mag, graph-property-prediction-on-ogbg-ppa, graph-property-prediction-on-ogbg-molpcba, link-prediction-on-ogbl-collab, node-property-prediction-on-ogbn-proteins, link-property-prediction-on-ogbl-wikikg2, graph-property-prediction-on-ogbg-code2, node-property-prediction-on-ogbn-products, graph-property-prediction-on-ogbg-molhiv, link-property-prediction-on-ogbl-ddi, link-property-prediction-on-ogbl-citation2, node-property-prediction-on-ogbn-papers100m, link-property-prediction-on-ogbl-biokg",,https://ogb.stanford.edu/,https://paperswithcode.com/dataset/ogb,"The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. OGB datasets are automatically downloaded, processed, and split using the OGB Data Loader. The model performance can be evaluated using the OGB Evaluator in a unified manner.
OGB is a community-driven initiative in active development.",,,,,,
2286,OIE2016,Open Information Extraction,Open Information Extraction,Open Information Extraction,,,Methodology,open-information-extraction-on-oie2016,,,https://paperswithcode.com/dataset/oie2016,"OIE2016 is the first large-scale OpenIE benchmark. It is created by automatic conversion from QA-SRL [He et al., 2015], a semantic role labeling dataset. The sentences are from news (e.g., WSJ) and encyclopedia (e.g., WIKI) domains. Since there are no restrictions on the elements of OpenIE extractions, partial-matching criteria instead of exact-matching is typically used. Hence, the evaluation script can tolerate the extractions that are slightly different from the gold annotation. 

Source: https://arxiv.org/pdf/2205.11725.pdf (section 3.1)",2015,,,,,
2287,OIR-Bench,Text-based Image Editing,Text-based Image Editing,Text-based Image Editing,"Image, Text",English,Computer Vision,,,https://drive.google.com/file/d/1JX8w0S9PCD9Ipmo9IiICO8R7e1haTGdF/view,https://paperswithcode.com/dataset/oir-bench,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
2288,OIR,Intent Recognition,Intent Recognition,Intent Recognition,Image,,Computer Vision,,,,https://paperswithcode.com/dataset/oir,OIR is a financial-domain dataset of the outbound intent recognition task. It aims to identify the intent of customer response in the outbound call scenario.,,,,,,
2289,OK-VQA,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Question Generation, Retrieval, Question Answering","Image, Text",English,Computer Vision,"visual-question-answering-on-ok-vqa, retrieval-on-ok-vqa",,https://okvqa.allenai.org/,https://paperswithcode.com/dataset/ok-vqa,"Outside Knowledge Visual Question Answering (OK-VQA) includes more than 14,000 questions that require external knowledge to answer.",,OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge,https://arxiv.org/pdf/1906.00067v2.pdf,,,
2290,Oktoberfest_Food_Dataset,Object Detection,Object Detection,"Object Detection, Food recommendation",Image,,Computer Vision,food-recommendation-on-oktoberfest-food,,https://github.com/a1302z/OktoberfestFoodDataset,https://paperswithcode.com/dataset/oktoberfest-food-dataset,"A realistic, diverse, and challenging dataset for object detection on images. The data was recorded at a beer tent in Germany and consists of 15 different categories of food and drink items.",,,,,,
2291,Okutama-Action,Action Detection,Action Detection,"Action Detection, Object Tracking, Action Recognition","Image, Video",,Computer Vision,action-recognition-on-okutama-action,,http://okutama-action.org,https://paperswithcode.com/dataset/okutama-action,"A new video dataset for aerial view concurrent human action detection. It consists of 43 minute-long fully-annotated sequences with 12 action classes. Okutama-Action features many challenges missing in current datasets, including dynamic transition of actions, significant changes in scale and aspect ratio, abrupt camera movement, as well as multi-labeled actors.",,,,,,
2292,OLID,Language Modelling,Language Modelling,"Language Modelling, Language Identification, Hate Speech Detection","Audio, Image, Text",English,Computer Vision,hate-speech-detection-on-olid,,https://scholar.harvard.edu/malmasi/olid,https://paperswithcode.com/dataset/olid,"The OLID is a hierarchical dataset to identify the type and the target of offensive texts in social media. The dataset is collected on Twitter and publicly available. There are 14,100 tweets in total, in which 13,240 are in the training set, and 860 are in the test set. For each tweet, there are three levels of labels: (A) Offensive/Not-Offensive, (B) Targeted-Insult/Untargeted, (C) Individual/Group/Other. The relationship between them is hierarchical. If a tweet is offensive, it can have a target or no target. If it is offensive to a specific target, the target can be an individual, a group, or some other objects. This dataset is used in the OffensEval-2019 competition in SemEval-2019.",2019,Kungfupanda at SemEval-2020 Task 12: BERT-Based Multi-Task Learning for Offensive Language Detection,https://arxiv.org/abs/2004.13432,,,
2293,OLIVES_Dataset,Contrastive Learning,Contrastive Learning,"Contrastive Learning, Multimodal Deep Learning, Semi-supervised Medical Image Classification",Image,,Computer Vision,,,https://doi.org/10.5281/zenodo.7105232,https://paperswithcode.com/dataset/olives-dataset,"Clinical diagnosis of the eye is performed over multifarious data modalities including scalar clinical labels, vectorized biomarkers, two-dimensional fundus images, and three-dimensional Optical Coherence Tomography (OCT) scans. While the clinical labels, fundus images and OCT scans are instrumental measurements, the vectorized biomarkers are interpreted attributes from the other measurements. Clinical practitioners use all these data modalities for diagnosing and treating eye diseases like Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). Enabling usage of machine learning algorithms within the ophthalmic medical domain requires research into the relationships and interactions between these relevant data modalities. Existing datasets are limited in that: (i) they view the problem as disease prediction without assessing biomarkers, and (ii) they do not consider the explicit relationship among all four data modalities over the treatment period. In this paper, we introduce the Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) dataset that addresses the above limitations. This is the first OCT and fundus dataset that includes clinical labels, biomarker labels, and time-series patient treatment information from associated clinical trials. The dataset consists of 1268 fundus eye images each with 49 OCT scans, and 16 biomarkers, along with 3 clinical labels and a disease diagnosis of DR or DME. In total, there are 96 eyes' data averaged over a period of at least two years with each eye treated for an average of 66 weeks and 7 injections. OLIVES dataset has advantages in other fields of machine learning research including self-supervised learning as it provides alternate augmentation schemes that are medically grounded.",,,,,,
2294,Olivetti_face,Clustering Algorithms Evaluation,Clustering Algorithms Evaluation,Clustering Algorithms Evaluation,,,Methodology,clustering-algorithms-evaluation-on-olivetti,,https://scikit-learn.org/0.19/datasets/olivetti_faces.html,https://paperswithcode.com/dataset/olivetti-face,This dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge.,1992,,,,,
2295,OllaBench_v.0.2,Moral Scenarios,Moral Scenarios,"Moral Scenarios, Multiple-choice, Computer Security",,,Methodology,,CC 4.0,https://github.com/Cybonto/OllaBench,https://paperswithcode.com/dataset/ollabench-v-0-2,"Large Language Models (LLMs) have the potential to enhance Agent-Based Modeling by better representing complex interdependent cybersecurity systems, improving cybersecurity threat modeling and risk management. Evaluating LLMs in this context is crucial for legal compliance and effective application development. Existing LLM evaluation frameworks often overlook the human factor and cognitive computing capabilities essential for interdependent cybersecurity. To address this gap, I propose OllaBench, a novel evaluation framework that assesses LLMs' accuracy, wastefulness, and consistency in answering scenario-based information security compliance and non-compliance questions.",,,,,,
2296,OLPBENCH,Graph Embedding,Graph Embedding,"Graph Embedding, Knowledge Graph Embeddings, Open Knowledge Graph Embedding",Graph,,Methodology,,,https://www.uni-mannheim.de/dws/research/resources/olpbench/,https://paperswithcode.com/dataset/olpbench,"OLPBENCH is a large Open Link Prediction benchmark, which was derived from the state-of-the-art Open Information Extraction corpus OPIEC (Gashteovski et al., 2019). OLPBENCH contains 30M open triples, 1M distinct open relations and 2.5M distinct mentions of approximately 800K entities. 

Open Link Prediction is defined as follows: Given an Open Knowledge Graph and a question consisting of an entity mention and an open relation, predict mentions as answers. A predicted mention is correct if it is a mention of the correct answer entity. For example, given the question (“NBC-TV”, “has office in”, ?), correct answers include “NYC” and “New York”.",2019,,,,,
2297,OmniBenchmark,Representation Learning,Representation Learning,"Representation Learning, Image Classification, Prompt Engineering, Fine-Grained Image Recognition",Image,,Computer Vision,image-classification-on-omnibenchmark,Creative Commons Attribution-NonCommercial-ShareAlike 3.0,https://zhangyuanhan-ai.github.io/OmniBenchmark/,https://paperswithcode.com/dataset/omnibenchmark,"Omni-Realm Benchmark (OmniBenchmark) is a diverse (21 semantic realm-wise datasets) and concise (realm-wise datasets have no concepts overlapping) benchmark for evaluating pre-trained model generalization across semantic super-concepts/realms, e.g. across mammals to aircraft. 

[ECCV2022]",,,,,,
2298,Omniglot,Density Estimation,Density Estimation,"Density Estimation, One-Shot Segmentation, Personalized Federated Learning, Meta-Learning, Few-Shot Image Classification, Multi-Task Learning",Image,,Computer Vision,"few-shot-image-classification-on-omniglot-1-5, few-shot-image-classification-on-omniglot-5-4, one-shot-segmentation-on-cluttered-omniglot, few-shot-image-classification-on-omniglot-1-4, meta-learning-on-omniglot-1-shot-20-way, few-shot-image-classification-on-omniglot-5-2, personalized-federated-learning-on-omniglot, few-shot-image-classification-on-omniglot-5-1, multi-task-learning-on-omniglot, few-shot-image-classification-on-omniglot-2, few-shot-image-classification-on-omniglot-1-1, few-shot-image-classification-on-omniglot, few-shot-image-classification-on-omniglot-1-2, few-shot-image-classification-on-omniglot-5-5, density-estimation-on-omniglot",MIT,https://github.com/brendenlake/omniglot,https://paperswithcode.com/dataset/omniglot-1,"The Omniglot data set is designed for developing more human-like learning algorithms. It contains 1623 different handwritten characters from 50 different alphabets. Each of the 1623 characters was drawn online via Amazon's Mechanical Turk by 20 different people. Each image is paired with stroke data, a sequences of [x,y,t] coordinates with time (t) in milliseconds.",,,,,,
2299,Omniverse_Isaac_Gym,Continuous Control,Continuous Control,"Continuous Control, Omniverse Isaac Gym, MuJoCo Games, Acrobot",,,Methodology,"omniverse-isaac-gym-on-ingenuity, continuous-control-on-ant, omniverse-isaac-gym-on-frankacabinet, mujoco-games-on-ant, omniverse-isaac-gym-on-humanoid, omniverse-isaac-gym-on-ant, omniverse-isaac-gym-on-anymal, omniverse-isaac-gym-on-allegrohand",BSD 3-Clause License,https://github.com/NVIDIA-Omniverse/OmniIsaacGymEnvs,https://paperswithcode.com/dataset/omniverse-isaac-gym,"Omniverse Isaac Gym is a GPU-based physics simulation platform developed by NVIDIA. This open-source toolkit implements various Reinforcement Learning benchmarks, simulating real-world robotic applications.

(Image Credit: NVIDIA-Omniverse)",,,,,,
2300,OneStopEnglish,Text Simplification,Text Simplification,"Text Simplification, Feature Engineering, Common Sense Reasoning",Text,English,Natural Language Processing,,,https://github.com/nishkalavallabhi/OneStopEnglishCorpus,https://paperswithcode.com/dataset/onestopenglish,"Useful for through two applications - automatic readability assessment and automatic text simplification. The corpus consists of 189 texts, each in three versions (567 in total).",,,,189 texts,,
2301,OntoNotes_4.0,Chinese Named Entity Recognition,Chinese Named Entity Recognition,Chinese Named Entity Recognition,"Image, Text",English,Computer Vision,chinese-named-entity-recognition-on-ontonotes,LDC User Agreement for Non-Members,https://catalog.ldc.upenn.edu/LDC2011T03,https://paperswithcode.com/dataset/ontonotes-4-0,"OntoNotes Release 4.0 contains the content of earlier releases -- OntoNotes Release 1.0 LDC2007T21, OntoNotes Release 2.0 LDC2008T04 and OntoNotes Release 3.0 LDC2009T24 -- and adds newswire, broadcast news, broadcast conversation and web data in English and Chinese and newswire data in Arabic. This cumulative publication consists of 2.4 million words as follows: 300k words of Arabic newswire 250k words of Chinese newswire, 250k words of Chinese broadcast news, 150k words of Chinese broadcast conversation and 150k words of Chinese web text and 600k words of English newswire, 200k word of English broadcast news, 200k words of English broadcast conversation and 300k words of English web text.",,,,,,
2302,OntoNotes_5.0,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Semantic Role Labeling, UIE, Chinese Named Entity Recognition, Generalized Zero-Shot Learning, FG-1-PG-1, Entity Typing, Weakly-Supervised Named Entity Recognition, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,"chinese-named-entity-recognition-on-ontonotes-2, named-entity-recognition-on-ontonotes, fg-1-pg-1-on-ontonotes-5-0, entity-typing-on-ontonotes-v5-english, uie-on-ontonotes-5-0, generalized-zero-shot-learning-on-ontonotes, named-entity-recognition-ner-on-ontonotes-v5, entity-typing-on-ontonotes, named-entity-recognition-on-ontonotes-5-0, coreference-resolution-on-ontonotes, semantic-role-labeling-on-ontonotes, weakly-supervised-named-entity-recognition-on-2",,https://catalog.ldc.upenn.edu/LDC2013T19,https://paperswithcode.com/dataset/ontonotes-5-0,"OntoNotes 5.0 is a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).

OntoNotes Release 5.0 contains the content of earlier releases - and adds source data from and/or additional annotations for, newswire, broadcast news, broadcast conversation, telephone conversation and web data in English and Chinese and newswire data in Arabic.",,,,,,
2303,OOS_CG,Open Intent Detection,Open Intent Detection,Open Intent Detection,Image,,Computer Vision,open-intent-detection-on-oos-cg,MIT,https://github.com/fangyihao/gptaug/tree/main/data/oos_cg,https://paperswithcode.com/dataset/oos-cg,"The dataset identifies the shortcomings of existing benchmarks in evaluating the problem of compositional generalization, which underscores the need for the development of datasets tailored to assess compositional generalization in open intent detection tasks.",,,,,,
2304,Open6DOR_V2,Object Rearrangement,Object Rearrangement,Object Rearrangement,,,Methodology,object-rearrangement-on-open6dor-v2,CC By NC 4.0,https://github.com/qizekun/SoFar,https://paperswithcode.com/dataset/open6dor-v2,"We introduce a challenging and comprehensive benchmark for open-instruction 6-DoF object rearrangement tasks, termed Open6DOR.",,,,,,
2305,OpenAI_Gym,Continuous Control,Continuous Control,"Continuous Control, OpenAI Gym",,,Methodology,"continuous-control-on-lunar-lander-openai-gym, openai-gym-on-pendulum-v1, continuous-control-on-cart-pole-openai-gym",MIT,https://gym.openai.com/,https://paperswithcode.com/dataset/openai-gym,"OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It includes environment such as Algorithmic, Atari, Box2D, Classic Control, MuJoCo, Robotics, and Toy Text.",,,,,,
2306,OpenAPI_completion_refined,OpenAPI code completion,OpenAPI code completion,"OpenAPI code completion, Code Generation, Code Completion",Text,English,Natural Language Processing,openapi-code-completion-on-openapi-code,MIT,https://huggingface.co/datasets/BohdanPetryshyn/openapi-completion-refined,https://paperswithcode.com/dataset/openapi-code-completion,"A human-refined dataset of OpenAPI definitions based on the APIs.guru OpenAPI directory.

The dataset was collected from the APIs.guru OpenAPI definitions directory.
The directory contains more than 4,000 definitions in yaml format. Analysis of the repository revealed that about 75%
of the definitions in the directory are produced by a handful of major companies like Amazon, Google, and Microsoft.
To avoid the dataset bias towards a specific producer, the maximum number of definitions from a single producer was limited
to 20. Multiple versions of the same API were also excluded from the dataset as they are likely to contain very similar 
definitions.",,,,,,
2307,opendataset,Document Layout Analysis,Document Layout Analysis,"Document Layout Analysis, Reading Order Detection","Image, Text",English,Computer Vision,,,https://huggingface.co/datasets/YLFINTECH/opendataset,https://paperswithcode.com/dataset/opendataset,opendateset,,,,,,
2308,OpenDebateEvidence,Text Summarization,Text Summarization,"Text Summarization, Argument Mining, Information Retrieval, Text Generation, Abstractive Text Summarization, Extractive Summarization",Text,English,Natural Language Processing,,MIT,https://huggingface.co/datasets/Yusuf5/OpenCaselist,https://paperswithcode.com/dataset/opendebateevidence,"We introduce OpenDebateEvidence, a comprehensive dataset for argument mining
and summarization sourced from the American Competitive Debate community.
This dataset includes over 3.5 million documents with rich metadata, making it
one of the most extensive collections of debate evidence. OpenDebateEvidence
captures the complexity of arguments in high school and college debates, pro-
viding valuable resources for training and evaluation. Our extensive experiments
demonstrate the efficacy of fine-tuning state-of-the-art large language models for
argumentative abstractive summarization across various methods, models, and
datasets. By providing this comprehensive resource, we aim to advance com-
putational argumentation and support practical applications for debaters, edu-
cators, and researchers. OpenDebateEvidence is publicly available to support
further research and innovation in computational argumentation. Access it here:
https://huggingface.co/datasets/Yusuf5/OpenCaselist",,,,,,
2309,OpenDialKG,Text Generation,Text Generation,"Text Generation, Dialogue Generation, Knowledge Graphs",Text,English,Natural Language Processing,,CC BY-NC 4.0,https://github.com/facebookresearch/opendialkg,https://paperswithcode.com/dataset/opendialkg,OpenDialKG contains utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts.,,,,,,
2310,OpenEA_Benchmark,Multi-modal Entity Alignment,Multi-modal Entity Alignment,Multi-modal Entity Alignment,,,Methodology,"multi-modal-entity-alignment-on-umvm-oea-en, multi-modal-entity-alignment-on-umvm-oea-en-1, multi-modal-entity-alignment-on-umvm-oea-d-w, multi-modal-entity-alignment-on-umvm-oea-d-w-1",GPL,https://github.com/nju-websoft/OpenEA#dataset-overview,https://paperswithcode.com/dataset/openea-benchmark,"1.0 Version of OpenEA benchmark datasets. Please use the updated 2.0 version, that has been subsequently released.

Introduced by ""A Benchmarking Study of Embedding-based Entity Alignment for Knowledge Graphs by Sun et. al, VLDB 2020""

Contains entities from DBpedia, YAGO and Wikidata.",2020,,,,,
2311,OpenImage-O,Out of Distribution (OOD) Detection,Out of Distribution (OOD) Detection,"Out of Distribution (OOD) Detection, Out-of-Distribution Detection",Image,,Computer Vision,,,https://ooddetection.github.io/,https://paperswithcode.com/dataset/imagenet-1k-vs-openimage-o,"It is manually annotated, comes with a naturally diverse distribution, and has a large scale. It is built to overcome several shortcomings of existing OOD benchmarks. OpenImage-O is image-by-image filtered from the test set of OpenImage-V3, which has been collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias.",,,,,,
2312,OpenMIC-2018,Music Information Retrieval,Music Information Retrieval,"Music Information Retrieval, Audio Source Separation, Information Retrieval, Instrument Recognition","Audio, Image",,Computer Vision,instrument-recognition-on-openmic-2018,CC BY 4.0,https://zenodo.org/record/1432913,https://paperswithcode.com/dataset/openmic-2018,"OpenMIC-2018 is an instrument recognition dataset containing 20,000 examples of Creative Commons-licensed music available on the Free Music Archive. Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform.",2018,OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition,http://ismir2018.ircam.fr/doc/pdfs/248_Paper.pdf,000 examples,,
2313,OpenStreetMap_Multi-Sensor_Scene_Classification,Scene Classification,Scene Classification,"Scene Classification, Image Classification, Remote Sensing Image Classification",Image,,Computer Vision,,Creative Commons Attribution-NonCommercial-ShareAlike 3.0,https://github.com/descarteslabs/contrastive_sensor_fusion,https://paperswithcode.com/dataset/openstreetmap-multi-sensor-scene,"A high-resolution multi-sensor remote sensing scene classification dataset, appropriate for training and evaluating image classification models in the remote sensing domain.

The dataset consists of 8400 overhead scenes, each covered by Airbus Pléiades, Airbus SPOT, and USDA NAIP imagery. The scenes are classified into 12 OpenStreetMap categories:


man_made=bridge
man_made=breakwater
building=farm
power=substation
leisure=stadium
leisure=golf_course
waterway=dam
landuse=quarry
landuse=farmland
landuse=forest
natural=water
natural=bare_rock",,,,,,
2314,OpenStreetView-5M,Photo geolocation estimation,Photo geolocation estimation,Photo geolocation estimation,,,Methodology,photo-geolocation-estimation-on,CC-BY-SA,https://huggingface.co/datasets/osv5m/osv5m,https://paperswithcode.com/dataset/openstreetview-5m,"OpenStreetView-5M establishes a new open benchmark for geolocation by providing a large, open, and clean dataset. As detailed below, OpenStreetView-5M improves upon several limitations of current geolocation datasets.

Deep neural networks have historically been selected over other machine learning methods because they benefit from larger amounts of data. OSV-5M consists of 4,894,685 training and 210,122 test images, with a height of $512$ pixels and an average width of $792\pm127$  pixels.

Many geolocation datasets are restricted to a few cities or are significantly biased towards the Western world . In contrast, OpenStreetView-5M images are uniformly sampled on the globe, covering 70k cities and 225 countries and territories. The distribution of test images across countries has a normalized entropy of $0.78$~\cite[Eq. 19]{wilcox1967indices}, suggesting high diversity. Our train set has a normalized entropy of $0.67$, which is comparable to the entropy of the distribution of the countries' area ($0.71$).

OpenStreetView-5M is based on the crowd-sourced street view images of Mapillary  which follow the CC-BY-SA license: free of use with attribution.

We estimate through manual inspection of 4500 images that 96.1\% (±0.57\%) of the images in the OpenStreetView-5M dataset are localizable, with a 95\% confidence level. Among the weakly or non-localizable images, $70$\% ($2.7$\% total) are low-quality: under- or over-exposed, blurry, or rotated; $30$\% ($1.2$\% total) are poorly framed, indoor, or in tunnels.

Without carefully enforcing the spatial separation between train and test images, geolocation can reduce to place-recognition. As our goal is to assess the capacity of models to learn robust geographical representations, we ensure that no image in the OSV-5M training set lies within a $1$km radius of any image in the test set.

Street-view images are typically acquired by a limited number of camera sensors mounted on the top or front of a small fleet of vehicles assigned to a given region. This correlation between location, cars, and sensors can be exploited to simplify the geolocation task. Notoriously, players of the web-based geolocation game GeoGuessr  can locate images from Ghana by spotting a piece of duct tape placed on the corner of the roof rack of the Google Street View car . OpenStreetView-5M tries to avoid this pitfall by ensuring that no image sequence (a continuous series of images acquired by the same user) appears in both training and test sets. While this might not prevent images taken with the same vehicle on different days from being in both sets, it limits such occurrences.

Rich metadata beyond geographical coordinates can improve the robustness and versatility of geolocation models. Each image in our dataset is associated with four tiers of administrative data: country, region (\eg, state), area (\eg, county), and the nearest city. Note that areas are not defined for one-third of the dataset.

We also associate each image with a set of additional information: land cover, climate, soil type, the driving side, and distance to the sea where the image was taken.",,,,5M images,"training and 210,122 test images",
2315,OpenSubtitles,Domain Adaptation,Domain Adaptation,"Domain Adaptation, Language Modelling, Language Identification, Dialogue Generation, Machine Translation",Text,English,Natural Language Processing,"language-identification-on-opensubtitles, language-modelling-on-opensubtitles",,http://opus.nlpl.eu/OpenSubtitles2018.php,https://paperswithcode.com/dataset/opensubtitles,OpenSubtitles is collection of multilingual parallel corpora. The dataset is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages.,,,,,,
2316,OpenViDial_2.0,Multi-modal Dialogue Generation,Multi-modal Dialogue Generation,"Multi-modal Dialogue Generation, Dialogue Generation",Text,English,Natural Language Processing,multi-modal-dialogue-generation-on-openvidial,,https://github.com/ShannonAI/OpenViDial,https://paperswithcode.com/dataset/openvidial-2-0,"OpenViDial 2.0 is a larger-scale open-domain multi-modal dialogue dataset compared to the previous version OpenViDial 1.0. OpenViDial 2.0 contains a total number of 5.6 million dialogue turns extracted from either movies or TV series from different resources, and each dialogue turn is paired with its corresponding visual context.",,,,,,
2317,OpenWebText,Text Generation,Text Generation,"Text Generation, Language Modelling, Text Classification","Image, Text",English,Computer Vision,"language-modelling-on-openwebtext, text-generation-on-openwebtext",Custom,https://skylion007.github.io/OpenWebTextCorpus/,https://paperswithcode.com/dataset/openwebtext,OpenWebText is an open-source recreation of the WebText corpus. The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB).,,RoBERTa: A Robustly Optimized BERT Pretraining Approach,https://arxiv.org/abs/1907.11692,,,
2318,OpenXAI,Explainable artificial intelligence,Explainable artificial intelligence,"Explainable artificial intelligence, Explainable Models",,,Methodology,,MIT,https://open-xai.github.io/,https://paperswithcode.com/dataset/openxai,"OpenXAI is the first general-purpose lightweight library that provides a comprehensive list of functions to systematically evaluate the quality of explanations generated by attribute-based explanation methods. OpenXAI supports the development of new datasets (both synthetic and real-world) and explanation methods, with a strong bent towards promoting systematic, reproducible, and transparent evaluation of explanation methods.

OpenXAI is an open-source initiative that comprises of a collection of curated high-stakes datasets, models, and evaluation metrics, and provides a simple and easy-to-use API that enables researchers and practitioners to benchmark explanation methods using just a few lines of code.",,,,,,
2319,Open_Entity,Entity Typing,Entity Typing,Entity Typing,,,Methodology,"entity-typing-on-open-entity, entity-typing-on-open-entity-1",,https://www.cs.utexas.edu/~eunsol/html_pages/open_entity.html,https://paperswithcode.com/dataset/open-entity-1,"The Open Entity dataset is a collection of about 6,000 sentences with fine-grained entity types annotations. The entity types are free-form noun phrases that describe appropriate types for the role the target entity plays in the sentence. Sentences were sampled from Gigaword, OntoNotes and web articles. On average each sentence has 5 labels.",,,,000 sentences,,5
2320,Open_Images_V4,Object Detection,Object Detection,"Object Detection, Image Classification, Multi-label zero-shot learning, Semantic Segmentation",Image,,Computer Vision,multi-label-zero-shot-learning-on-open-images,Custom,https://storage.googleapis.com/openimages/web/index.html,https://paperswithcode.com/dataset/open-images-v4,"Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images) are provided. The images often show complex scenes with several objects (8 annotated objects per image on average). Visual relationships between them are annotated, which support visual relationship detection, an emerging task that requires structured reasoning.",,"The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale",https://arxiv.org/pdf/1811.00982,9M images,,57
2321,Open_Images_V7,Instance Segmentation,Instance Segmentation,"Instance Segmentation, Image Captioning, Semantic Segmentation, Visual Relationship Detection, Image Classification, 2D Object Detection, Speech Recognition, Object Detection, 2D Semantic Segmentation","Audio, Image, Text",English,Computer Vision,,CC BY 4.0,https://g.co/dataset/open-images,https://paperswithcode.com/dataset/open-images-v7,"Open Images is a computer vision dataset covering ~9 million images with labels spanning thousands of object categories. A subset of 1.9M includes diverse annotations types.


15,851,536 boxes on 600 classes
2,785,498 instance segmentations on 350 classes
3,284,280 relationship annotations on 1,466 relationships
675,155 localized narratives (synchronized voice, mouse trace, and text caption)
66,391,027 point-level annotations on 5,827 classes
61,404,966 image-level labels on 20,638 classes

Images are under a  CC BY 2.0 license, annotations under  CC BY 4.0 license.",,,,,,600
2322,Open_Relation_Modeling,Open Relation Modeling,Open Relation Modeling,Open Relation Modeling,Graph,,Methodology,,,https://github.com/jeffhj/open-relation-modeling,https://paperswithcode.com/dataset/open-relation-modeling,"Given two entities, generating a coherent sentence describing the relation between them.

E.g., (data mining, database) => data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.",,,,,,
2323,OPERAnet,Activity Recognition,Activity Recognition,Activity Recognition,"Image, Video",,Computer Vision,,,https://figshare.com/s/c774748e127dcdecc667,https://paperswithcode.com/dataset/operanet,"OPERAnet is a multimodal activity recognition dataset acquired from radio frequency and vision-based sensors. Approximately 8 hours of annotated measurements are provided, which are collected across two different rooms from 6 participants performing 6 activities, namely, sitting down on a chair, standing from sit, lying down on the ground, standing from the floor, walking and body rotating. The dataset has been acquired from four synchronized modalities for the purpose of passive Human Activity Recognition (HAR) as well as localization and crowd counting.",,,,,,
2324,OPIEC,Open Information Extraction,Open Information Extraction,"Open Information Extraction, Question Answering",Text,English,Natural Language Processing,,,https://www.uni-mannheim.de/dws/research/resources/opiec/,https://paperswithcode.com/dataset/opiec,"OPIEC is an Open Information Extraction (OIE) corpus, constructed from the entire English Wikipedia. It containing more than 341M triples. Each triple from the corpus is composed of rich meta-data: each token from the subj / obj / rel along with NLP annotations (POS tag, NER tag, ...), provenance sentence (along with its dependency parse, sentence order relative to the article), original (golden) links contained in the Wikipedia articles, space / time.",,,,,,
2325,OpoSum,Topic Models,Topic Models,"Topic Models, Multi-Document Summarization, Document Summarization",Text,English,Natural Language Processing,,,https://github.com/stangelid/oposum,https://paperswithcode.com/dataset/oposum,"OPOSUM is a dataset for the training and evaluation of Opinion Summarization models which contains Amazon reviews from six product domains: Laptop Bags, Bluetooth Headsets, Boots, Keyboards, Televisions, and Vacuums.
The six training collections were created by downsampling from the Amazon Product Dataset introduced in McAuley et al. (2015) and contain reviews and their respective ratings. 

A subset of the dataset has been manually annotated, specifically, for each domain, 10 different products were uniformly sampled (across ratings) with 10 reviews each, amounting to a total of 600 reviews, to be used only for development (300) and testing (300).",2015,Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised,https://arxiv.org/abs/1808.08858,,,
2326,OPT,6D Pose Estimation,6D Pose Estimation,6D Pose Estimation,"3D, Image",,Computer Vision,6d-pose-estimation-on-opt,,http://media.ee.ntu.edu.tw/research/OPT/,https://paperswithcode.com/dataset/opt,"Accurately tracking the six degree-of-freedom pose of an object in real scenes is an important task in computer vision and augmented reality with numerous applications. Although a variety of algorithms for this task have been proposed, it remains difficult to evaluate existing methods in the literature as oftentimes different sequences are used and no large benchmark datasets close to real-world scenarios are available. In this paper, we present a large object pose tracking benchmark dataset consisting of RGB-D video sequences of 2D and 3D targets with ground-truth information. The videos are recorded under various lighting conditions, different motion patterns and speeds with the help of a programmable robotic arm. We present extensive quantitative evaluation results of the state-of-the-art methods on this benchmark dataset and discuss the potential research directions in this field.",,,,,,
2327,OPUS-100,Reading Comprehension,Reading Comprehension,"Reading Comprehension, Machine Translation",Text,English,Natural Language Processing,,,https://opus.nlpl.eu/OPUS-100,https://paperswithcode.com/dataset/opus-100,OPUS-100 is an English-centric multilingual corpus covering 100 languages. It was randomly sampled from the OPUS collection.,,,,,,
2328,OPUS,Text Generation,Text Generation,"Text Generation, Sentence Embedding, Paraphrase Generation",Text,English,Natural Language Processing,,,https://opus.nlpl.eu/,https://paperswithcode.com/dataset/opus,"OPUS is a growing collection of translated texts from the web. In the OPUS project we try to convert and align free online data, to add linguistic annotation, and to provide the community with a publicly available parallel corpus. OPUS is based on open source products and the corpus is also delivered as an open content package. We used several tools to compile the current collection. All pre-processing is done automatically. No manual corrections have been carried out.",,,,,,
2329,Opusparcus,Text Generation,Text Generation,"Text Generation, Sentence Embedding, Paraphrase Generation",Text,English,Natural Language Processing,,,http://urn.fi/urn:nbn:fi:lb-2018021221,https://paperswithcode.com/dataset/opusparcus,"Opusparcus is a paraphrase corpus for six European languages: German, English, Finnish, French, Russian, and Swedish. The paraphrases are extracted from the OpenSubtitles2016 corpus, which contains subtitles from movies and TV shows.

For each target language, the Opusparcus data have been partitioned into three types of data sets: training, development and test sets. The training sets are large, consisting of millions of sentence pairs, and have been compiled automatically, with the help of probabilistic ranking functions. The development and test sets consist of sentence pairs that have been annotated manually; each set contains approximately 1000 sentence pairs that have been verified to be acceptable paraphrases by two annotators.",,,,,,
2330,OPV2V,Monocular 3D Object Detection,Monocular 3D Object Detection,"Monocular 3D Object Detection, 3D Object Detection","3D, Image",,Computer Vision,"monocular-3d-object-detection-on-opv2v, 3d-object-detection-on-opv2v",,https://mobility-lab.seas.ucla.edu/opv2v/,https://paperswithcode.com/dataset/opv2v,"OPV2V is a large-scale open simulated dataset for Vehicle-to-Vehicle perception. It contains over 70 interesting scenes, 11,464 frames, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns in CARLA and a digital town of Culver City, Los Angeles.",,,,,,
2331,OQM9HK,Band Gap,Band Gap,"Band Gap, Formation Energy, Total Magnetization",,,Methodology,"total-magnetization-on-oqm9hk, band-gap-on-oqm9hk, formation-energy-on-oqm9hk",CC BY 4.0,https://doi.org/10.5281/zenodo.7124330,https://paperswithcode.com/dataset/oqm9hk,"This is a large-scale dataset of quantum-mechanically calculated properties (DFT level) of crystalline materials for graph representation learning that contains approximately 900k entries (OQM9HK). This dataset is constructed on the basis of the Open Quantum Materials Database (OQMD) v1.5 containing more than one million entries, and is the successor to the OQMD v1.2 dataset containing approximately 600k entries (OQM6HK).


Technical Report
CGNN v1.1",,Technical Report,https://storage.googleapis.com/rimcs_cgnn/oqm9hk_dataset_Sep_30_2022.pdf,,,
2332,OQMD_v1.2,Band Gap,Band Gap,"Band Gap, Materials Screening, Formation Energy, Total Magnetization",,,Methodology,"formation-energy-on-oqmd-v12, band-gap-on-oqmd-v12, materials-screening-on-oqmd-v12, total-magnetization-on-oqmd-v12",CC BY 4.0,https://github.com/Tony-Y/oqmd-v1.2-dataset-for-cgnn,https://paperswithcode.com/dataset/oqmd-v1-2,"The OQMD is a database of DFT calculated thermodynamic and structural properties of one million materials, created in Chris Wolverton's group at Northwestern University.

The OQMD v1.2 dataset for CGNN is downloadable from this link, which contains 561,888 materials. Its format is described in here. The original data is available at the OQMD website.",,,,,,
2333,ORCAS-I,Information Retrieval,Information Retrieval,"Information Retrieval, Intent Classification",Image,,Computer Vision,intent-classification-on-orcas-i,CC BY-NC 4.0,https://researchdata.tuwien.ac.at/records/pp7xz-n9a06,https://paperswithcode.com/dataset/orcas-i,"A labelled version of the ORCAS click-based dataset of Web queries, which provides 18 million connections to 10 million distinct queries.

DOI of the dataset: 10.48436/pp7xz-n9a06",,,,,,
2334,Orchard,Syntax Representation,Syntax Representation,Syntax Representation,,,Methodology,,,https://github.com/billptw/Orchard,https://paperswithcode.com/dataset/orchard,Orchard is a diagnostic dataset for systematically evaluating hierarchical reasoning in state-of-the-art neural sequence models,,,,,,
2335,OrdinalDataset,AutoML,AutoML,"AutoML, Sentence Ordering",,,Methodology,automl-on-ordinaldataset,MIT,https://github.com/marscod/BERT-Sort,https://paperswithcode.com/dataset/ordinaldataset,"It includes 10 data sets that consists of both raw data set and encoded data set where it is encoded through BERT-Sort Encoder with MLM initialization of .

In each data set folder, there are original files and encoded data sets with 4 different MLMs. For instance, bank/bank.csv is the original file for raw data set and bank/bank.csv_bs__roberta.csv is encoded raw data set with BERT-Sort Encoder which is initiated with RoBERTa MLM. Both raw and encoded data sets have been used to evaluate the proposed approach in 5 AutoML platforms.",,,,,,
2336,Orkut,Link Prediction,Link Prediction,"Link Prediction, Graph Clustering, Community Detection","Graph, Image, Time Series",,Computer Vision,,,https://snap.stanford.edu/data/com-Orkut.html,https://paperswithcode.com/dataset/orkut,"Orkut is a social network dataset consisting of friendship social network and ground-truth communities from Orkut.com on-line social network where users form friendship each other.

Each connected component in a group is regarded as a separate ground-truth community. The ground-truth communities which have less than 3 nodes are removed. The dataset also provides the top 5,000 communities with highest quality and the largest connected component of the network.",,,,,,
2337,ORU_Diverse_radar_dataset,Radar odometry,Radar odometry,Radar odometry,,,Methodology,,,https://github.com/dan11003/CFEAR_Radarodometry,https://paperswithcode.com/dataset/oru-diverse-radar-dataset,"Evaluate radar localization in diverse environments
Download: https://drive.google.com/drive/folders/1uATfrAe-KHlz29e-Ul8qUbUKwPxBFIhP Download",,,,,,
2338,OSN-transmission_mini_CelebA,Adversarial Robustness,Adversarial Robustness,"Adversarial Robustness, Image Compression",Image,,Adversarial,,CC BY-NC 4.0,https://github.com/ZOMIN28/OSN-transmission_mini_CelebA,https://paperswithcode.com/dataset/osn-transmission-mini-celeba,"This is the paper “DF-RAP: A Robust Adversarial Perturbation for Defending against Deepfakes in Real-world Social Network Scenarios"" OSN-transmission CelebA sampling dataset collected by manual upload and download. This dataset includes 30,000 facial images of size 256×256 transmitted through online social networks (OSN) and their corresponding original images. Among them, Facebook, Twitter, WeChat and Weibo were selected as the transmission OSN, with 7500 images each.",,,,7500 images,,
2339,OST,Action Anticipation,Action Anticipation,"Action Anticipation, Gaze Prediction, Action Recognition","Image, Time Series, Video",,Computer Vision,,,https://github.com/Mengmi/deepfuturegaze_gan,https://paperswithcode.com/dataset/deep-future-gaze,Is one of the largest egocentric datasets in the object search task with eyetracking information available,,Deep Future Gaze: Gaze Anticipation on Egocentric Videos Using Adversarial Networks,https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Deep_Future_Gaze_CVPR_2017_paper.pdf,,,
2340,OST300,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Image Super-Resolution, Super-Resolution",Image,,Computer Vision,,,http://mmlab.ie.cuhk.edu.hk/projects/SFTGAN/,https://paperswithcode.com/dataset/ost300,"OST300 is an outdoor scene dataset with 300 test images of outdoor scenes, and a training set of 7 categories of images with rich textures.",,,,,"test images of outdoor scenes, and a training set of 7 categories of images",7
2341,OSTD,Video Summarization,Video Summarization,"Video Summarization, Unsupervised Video Summarization, Metaheuristic Optimization","Text, Video",English,Natural Language Processing,,,https://github.com/YairShemer/ILS-SUMM,https://paperswithcode.com/dataset/ostd,"This dataset consists of 18 movies with duration range between 10 and 104 minutes leveraged from the OVSD dataset (Rotman et al., 2016). For these videos, the summary length limit is set to be the minimum between 4 minutes and 10% of the video length.",2016,ILS-SUMM: ITERATED LOCAL SEARCH FOR UNSUPERVISED VIDEO SUMMARIZATION,https://arxiv.org/pdf/1912.03650.pdf,,,
2342,OTB-2013,Visual Tracking,Visual Tracking,"Visual Tracking, Visual Object Tracking","Image, Video",,Computer Vision,"visual-object-tracking-on-otb-2013, visual-tracking-on-otb-2013",,http://cvlab.hanyang.ac.kr/tracker_benchmark/benchmark_v10.html,https://paperswithcode.com/dataset/otb-2013,"OTB2013 is the previous version of the current OTB2015 Visual Tracker Benchmark. It contains only 50 tracking sequences, as opposed to the 100 sequences in the current version of the benchmark.",,Marvasti-Zadeh,https://arxiv.org/abs/2004.01382,,,
2343,OTB-2015,Visual Tracking,Visual Tracking,"Visual Tracking, Visual Object Tracking","Image, Video",,Computer Vision,visual-object-tracking-on-otb-2015,,http://cvlab.hanyang.ac.kr/tracker_benchmark/,https://paperswithcode.com/dataset/otb-2015,"OTB-2015, also referred as Visual Tracker Benchmark, is a visual tracking dataset. It contains 100 commonly used video sequences for evaluating visual tracking.",2015,,,,,
2344,OTB,Visual Tracking,Visual Tracking,"Visual Tracking, Visual Object Tracking","Image, Video",,Computer Vision,"visual-object-tracking-on-otb-2013, visual-object-tracking-on-otb-2015, visual-object-tracking-on-otb-50, visual-tracking-on-otb-2013",,http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html,https://paperswithcode.com/dataset/otb,Object Tracking Benchmark (OTB) is a visual tracking benchmark that is widely used to evaluate the performance of a visual tracking algorithm. The dataset contains a total of 100 sequences and each is annotated frame-by-frame with bounding boxes and 11 challenge attributes. OTB-2013 dataset contains 51 sequences and the OTB-2015 dataset contains all 100 sequences of the OTB dataset.,2013,Deep Meta Learning for Real-Time Target-Aware Visual Tracking,https://arxiv.org/abs/1712.09153,,,
2345,OTTO_Recommender_Systems_Dataset,Session-Based Recommendations,Session-Based Recommendations,"Session-Based Recommendations, Product Recommendation, Recommendation Systems",,,Methodology,,CC BY 4.0,https://www.kaggle.com/datasets/otto/recsys-dataset,https://paperswithcode.com/dataset/otto-recommender-systems-dataset,"The OTTO session dataset is a large-scale dataset intended for multi-objective recommendation research. We collected the data from anonymized behavior logs of the OTTO webshop and the app. The mission of this dataset is to serve as a benchmark for session-based recommendations and foster research in the multi-objective and session-based recommender systems area. We also launched a Kaggle competition with the goal to predict clicks, cart additions, and orders based on previous events in a user session.

For additional background, please see the published OTTO Recommender Systems Dataset GitHub.

Key Features

12M real-world anonymized user sessions
220M events, consiting of clicks, carts and orders
1.8M unique articles in the catalogue
Ready to use data in .jsonl format
Evaluation metrics for multi-objective optimization

Dataset Statistics
| Dataset |  #sessions |    #items |     #events |     #clicks |     #carts |   #orders | Density [%] |
| :------ | ---------: | --------: | ----------: | ----------: | ---------: | --------: | ----------: |
| Train   | 12.899.779 | 1.855.603 | 216.716.096 | 194.720.954 | 16.896.191 | 5.098.951 |      0.0005 |
| Test    |  1.671.803 | 1.019.357 |  13.851.293 |  12.340.303 |  1.155.698 |   355.292 |      0.0005 |",,,,,,
2346,Oulu-CASIA,Facial Expression Recognition (FER),Facial Expression Recognition (FER),"Facial Expression Recognition (FER), Face Verification",Image,,Computer Vision,"face-verification-on-casia-nir-vis-20, facial-expression-recognition-on-oulu-casia, face-verification-on-oulu-casia, face-verification-on-oulu-casia-nir-vis",Custom,https://www.oulu.fi/cmvs/node/41316,https://paperswithcode.com/dataset/oulu-casia,"The Oulu-CASIA NIR&VIS facial expression database consists of six expressions (surprise, happiness, sadness, anger, fear and disgust) from 80 people between 23 and 58 years old. 73.8% of the subjects are males. The subjects were asked to sit on a chair in the observation room in a way that he/ she is in front of camera. Camera-face distance is about 60 cm. Subjects were asked to make a facial expression according to an expression example shown in picture sequences. The imaging hardware works at the rate of 25 frames per second and the image resolution is 320 × 240 pixels.",,https://arxiv.org/abs/1712.03474,https://arxiv.org/abs/1712.03474,,,
2347,OUTFOX,Binary text classification,Binary text classification,"Binary text classification, Text Classification, LLM-generated Text Detection","Image, Text",English,Computer Vision,,Apache-2.0 license,https://github.com/ryuryukke/OUTFOX,https://paperswithcode.com/dataset/outfox,"It contains 15K triplets of essay problem statements, student-written, and LLM-generated essays.

LLMs include ChatGPT, GPT-3.5, and FLAN-T5-XXL.

This dataset was created mainly for LLM-generated text detection but can be useful for other tasks as well.",,,,,,
2348,OVAD_benchmark,Language Modelling,Language Modelling,"Language Modelling, Open Vocabulary Object Detection, Visual Question Answering (VQA), Open Vocabulary Attribute Detection","Image, Text",English,Computer Vision,"open-vocabulary-attribute-detection-on-ovad-1, visual-question-answering-vqa-on-ovad, open-vocabulary-attribute-detection-on-ovad",Creative Commons Attribution-NonCommercial-ShareAlike 3.0,https://ovad-benchmark.github.io/,https://paperswithcode.com/dataset/ovad-benchmark,"Vision-language modeling has enabled open-vocabulary tasks where predictions can be queried using any text prompt in a zero-shot manner. Existing open-vocabulary tasks focus on object classes, whereas research on object attributes is limited due to the lack of a reliable attribute-focused evaluation benchmark. This paper introduces the Open-Vocabulary Attribute Detection (OVAD) task and the corresponding OVAD benchmark. The objective of the novel task and benchmark is to probe object-level attribute information learned by vision-language models. To this end, we created a clean and densely annotated test set covering 117 attribute classes on the 80 object classes of MS COCO. It includes positive and negative annotations, which enables open-vocabulary evaluation. Overall, the benchmark consists of 1.4 million annotations. For reference, we provide a first baseline method for open-vocabulary attribute detection. Moreover, we demonstrate the benchmark's value by studying the attribute detection performance of several foundation models.",,Open-vocabulary Attribute Detection,https://arxiv.org/pdf/2211.12914v1.pdf,,,
2349,OVIC_Datasets,Zero-Shot Image Classification,Zero-Shot Image Classification,"Zero-Shot Image Classification, Open Vocabulary Object Detection, Open Vocabulary Image Classification",Image,,Computer Vision,"open-vocabulary-image-classification-on-ovic-2, open-vocabulary-image-classification-on-ovic, open-vocabulary-image-classification-on-ovic-1, open-vocabulary-image-classification-on-ovic-3",CC BY-NC-SA 4.0,https://www.inf.uni-hamburg.de/en/inst/ab/wtm/research/corpora.html#ovic-datasets,https://paperswithcode.com/dataset/ovic-datasets,"Due to the free-form nature of the open vocabulary image classification task, special annotations are required for image sets used for evaluation purposes. Three such image datasets are presented here:


World: 272 images of which the grand majority are originally sourced (have never been on the internet) from 10 countries by 12 people, with an active focus on covering as wide and varied concepts as possible, including unusual, deceptive and/or indirect representations of objects,
Wiki: 1000 Wikipedia lead images sampled from a scraped pool of 18K,
Val3K: 3000 images from the ImageNet-1K validation set, sampled uniformly across the classes.

It is not in general possible to exhaustively annotate ground truth classification labels for open vocabulary image sets, as this would require annotations for every possible correct object noun in the English language for every visible entity in every part of every image. It is possible however, to annotate the thousands of predictions that have been made across the image sets by open vocabulary models trained thus far. All three image datasets presented here have been individually annotated by both human and multimodal LLM annotators for the object nouns that were predicted by trained models. The annotations specify whether each classification is correct, close, or incorrect, and for the human annotations, whether it relates to a primary or secondary element of the image. It is customary to use the suffixes -H and -L to clearly specify which annotations are being referred to at any time, e.g. Wiki-H is the Wiki dataset with corresponding human annotations. All three datasets together contain a total of 17.4K human and 112K LLM class annotations.

The data is directly available at the following links:


World dataset
Wiki dataset
Val3K dataset

Refer to the NOVIC code for an example of how the datasets can be used, as well as tools for updating the class annotations for newer model predictions.",,,,272 images,Val3K: 3000 images,
2350,Oxford-Affine,3D Reconstruction,3D Reconstruction,"3D Reconstruction, Dimensionality Reduction, Image Retrieval","3D, Image",,Computer Vision,,,https://www.robots.ox.ac.uk/~vgg/data/affine/,https://paperswithcode.com/dataset/oxford-affine,The Oxford-Affine dataset is a small dataset containing 8 scenes with sequence of 6 images per scene. The images in a sequence are related by homographies.,,A Large Dataset for Improving Patch Matching,https://arxiv.org/abs/1801.01466,6 images,,
2351,Oxford105k,Dimensionality Reduction,Dimensionality Reduction,"Dimensionality Reduction, Instance Search, Image Retrieval",Image,,Computer Vision,,Custom,https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/,https://paperswithcode.com/dataset/oxford105k,Oxford105k is the combination of the Oxford5k dataset and 99782 negative images crawled from Flickr using 145 most popular tags. This dataset is used to evaluate search performance for object retrieval (reported as mAP) on a large scale.,,Multiple Measurements and Joint Dimensionality Reduction for Large Scale Image Search with Short Vectors Extended Version,https://arxiv.org/abs/1504.03285,,,
2352,Oxford5k,Image Classification,Image Classification,"Image Classification, Content-Based Image Retrieval, Image Retrieval",Image,,Computer Vision,image-retrieval-on-oxford5k,Custom,https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/,https://paperswithcode.com/dataset/oxford5k,"Oxford5K is the Oxford Buildings Dataset, which contains 5062 images collected from Flickr. It offers a set of 55 queries for 11 landmark buildings, five for each landmark.",,Unsupervised Adversarial Attacks on Deep Feature-based Retrieval with GAN 1 Corresponding Author,https://arxiv.org/abs/1907.05793,5062 images,,
2353,Oxford_102_Flower,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Unsupervised Image Segmentation, Neural Architecture Search, Image Classification, Continual Learning, Generalized Zero-Shot Learning, Point-interactive Image Colorization, Image Clustering, Fine-Grained Image Classification, Image Generation, Prompt Engineering, Few-Shot Learning, Few-Shot Image Classification, Text-to-Image Generation, Transductive Zero-Shot Classification","Image, Text",English,Computer Vision,"image-classification-on-flowers-102, unsupervised-image-segmentation-on-flowers, few-shot-image-classification-on-oxford-102, prompt-engineering-on-oxford-102-flower, continual-learning-on-flowers-fine-grained-6, image-generation-on-oxford-102-flowers-256-x, few-shot-learning-on-flowers-102, few-shot-image-classification-on-flowers-102-1, neural-architecture-search-on-oxford-102, image-clustering-on-flowers-102, zero-shot-learning-on-flowers-102, text-to-image-generation-on-oxford-102, zero-shot-learning-on-oxford-102-flower, generalized-zero-shot-learning-on-oxford-102-1, transductive-zero-shot-classification-on-2, point-interactive-image-colorization-on-1, fine-grained-image-classification-on-oxford",,https://www.robots.ox.ac.uk/~vgg/data/flowers/102/,https://paperswithcode.com/dataset/oxford-102-flower,"Oxford 102 Flower is an image classification dataset consisting of 102 flower categories. The flowers chosen to be flower commonly occurring in the United Kingdom. Each class consists of between 40 and 258 images.

The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category and several very similar categories.",,,,258 images,,
2354,Oxford_Radar_RobotCar_Dataset,Radar odometry,Radar odometry,"Radar odometry, Translation, Weather Forecasting","Text, Time Series",English,Natural Language Processing,"radar-odometry-on-oxford-radar-robotcar, translation-on-oxford-radar-robotcar-dataset",,http://ori.ox.ac.uk/datasets/radar-robotcar-dataset,https://paperswithcode.com/dataset/oxford-radar-robotcar-dataset,"The Oxford Radar RobotCar Dataset is a radar extension to The Oxford RobotCar Dataset. It has been extended with data from a Navtech CTS350-X Millimetre-Wave FMCW radar and Dual Velodyne HDL-32E LIDARs with optimised ground truth radar odometry for 280 km of driving around Oxford, UK (in addition to all sensors in the original Oxford RobotCar Dataset).",,,,,,
2355,Oxford_RobotCar_Dataset,Unsupervised Domain Adaptation,Unsupervised Domain Adaptation,"Unsupervised Domain Adaptation, Visual Place Recognition, 3D Place Recognition","3D, Image",,Computer Vision,"3d-place-recognition-on-oxford-robotcar, unsupervised-domain-adaptation-on-cityscapes-2, visual-place-recognition-on-oxford-robotcar-4",,http://robotcar-dataset.robots.ox.ac.uk/,https://paperswithcode.com/dataset/oxford-robotcar-dataset,"The Oxford RobotCar Dataset contains over 100 repetitions of a consistent route through Oxford, UK, captured over a period of over a year. The dataset captures many different combinations of weather, traffic and pedestrians, along with longer term changes such as construction and roadworks.",,,,,,
2356,P-DESTRE,Video-Based Person Re-Identification,Video-Based Person Re-Identification,"Video-Based Person Re-Identification, Person Search, Person Re-Identification","Image, Video",,Computer Vision,,,http://p-destre.di.ubi.pt,https://paperswithcode.com/dataset/p-destre,"Provides consistent ID annotations across multiple days, making it suitable for the extremely challenging problem of person search, i.e., where no clothing information can be reliably used. Apart this feature, the P-DESTRE annotations enable the research on UAV-based pedestrian detection, tracking, re-identification and soft biometric solutions.",,,,,,
2357,P-Stance,Stance Detection,Stance Detection,Stance Detection,Image,,Computer Vision,stance-detection-on-p-stance,,,https://paperswithcode.com/dataset/p-stance,"P-Stance: A Large Dataset for Stance Detection in Political Domain
2021",2021,,,,,
2358,PA-HMDB51,Privacy Preserving Deep Learning,Privacy Preserving Deep Learning,"Privacy Preserving Deep Learning, Action Recognition","Image, Video",,Computer Vision,,,https://github.com/VITA-Group/PA-HMDB51,https://paperswithcode.com/dataset/pa-hmdb51,"The Privacy Annotated HMDB51 (PA-HMDB51) dataset is a video-based dataset for evaluating pirvacy protection in visual action recognition algorithms. The dataset contains both target task labels (action) and selected privacy attributes (skin color, face, gender, nudity, and relationship) annotated on a per-frame basis.",,,,,,
2359,PACE_2016_Feedback_Vertex_Set,Feedback Vertex Set (FVS),Feedback Vertex Set (FVS),Feedback Vertex Set (FVS),,,Methodology,,,https://pacechallenge.org/2016/,https://paperswithcode.com/dataset/pace-2016-feedback-vertex-set,"This is the dataset used in the PACE 2016 challenge, Track B, which was computing minimal Feedback Vertex Set. This competition focused on exact solutions, i.e. provably minimal feedback vertex sets (and no heuristic solutions). This should not be confused with the PACE 2022 challenge, which focused on directed feedback vertex set, and has its own entries on PapersWithCode (exact and heuristic).

The dataset can be downloaded here, and includes 100 instances that were released for practice (the public/ folder) and 100 instances that were kept private (hidden/) until the competition evaluation. All 200 were used in the final evaluation. Each instance is an undirected graph, one edge per line, in the format a b indicating an edge between vertices a and b. Vertices are 1-indexed.

Final results of the competition were reported in the PACE report, and additional analysis of some top solutions was done independently by Kiljan and Pilipczuk.",2016,independently,https://arxiv.org/abs/1803.00925,100 instances,,
2360,PACE_2018_Steiner_Tree,Steiner Tree Problem,Steiner Tree Problem,Steiner Tree Problem,,,Methodology,,CC0,https://github.com/PACE-challenge/SteinerTree-PACE-2018-instances,https://paperswithcode.com/dataset/pace-2018-steiner-tree,"This is the set of instances use in the PACE 2018 competition, of optimal Steiner Tree computation. The instances are grouped into three tracks of 200 instances each, except for the third track which is only 199 instances. Each instance is an undirected graph.

Track 1 is the ""exact with low number of terminals"" track, Track 2 is the ""exact with low treewidth track"", and Track 3 is the heuristic track. The exact tracks were intended to test solvers that need to provide a provably optimal solution; the heuristic track was intended for solvers that produce good but possibly suboptimal solutions. Details of the tracks and problem setup are on the PACE problem description page. The data format is specified in Appendix A of that page.

Graphs have sizes (number of vertices) ranging up to several thousand for exact tracks (1 and 2), and up to tens of thousands for the heuristic track. The exact tracks are typically very sparse; some of the heuristic instances are dense.

The official data download is on the PACE GitHub.",2018,,,200 instances,,
2361,PACE_2022_Exact,Feedback Vertex Set (FVS),Feedback Vertex Set (FVS),Feedback Vertex Set (FVS),,,Methodology,,,https://pacechallenge.org/2022/,https://paperswithcode.com/dataset/pace-2022-exact,"This is the set of graphs used in the PACE 2022 challenge for computing the Directed Feedback Vertex Set, from the Exact track. It consists of 200 labelled directed graphs. The graphs range in size up to from N=512 up to N=131072 vertices, and up to 1315170 edges. The graphs are mostly not symmetric (an edge form u->v does not imply an edge from v->u), although some are symmetric. The graph labels are integers ranging from 1 to N.

There is the related PACE 2022 Heuristic dataset, which allowed for approximate solutions (feedback vertex sets that were not necessarily of minimum size). Those graphs are generally larger and denser, as approximate solutions were still accepted.

The data format begins with one line N E 0, where N is the number of vertices, E is the number of edges, and 0 is the literal integer zero. The N subsequent lines are each a space-separated list of integers, such as 2 5 11 19. If that appeared on line number 1 (the first after N E 0), it would indicate that there are edges from vertex 1 to each of the vertices 2, 5, 11, and 19. Some lines are blank, and these indicates vertices with outdegree zero. An example graph would be
```
4 4 0
2 3
3

1
```

The dataset can be downloaded here. The 100 instances that were available for public testing are precisely the odd-numbered ones in that link; the public instances can be downloaded on their own here.",2022,,,100 instances,,
2362,PACE_2022_Heuristic,Feedback Vertex Set (FVS),Feedback Vertex Set (FVS),Feedback Vertex Set (FVS),,,Methodology,,,https://pacechallenge.org/2022/,https://paperswithcode.com/dataset/pace-2022-heuristic,"This is the set of graphs used in the PACE 2022 challenge for computing the Directed Feedback Vertex Set, from the Heuristic track. It consists of 200 labelled directed graphs. The graphs are mostly not symmetric (an edge form u->v does not imply an edge from v->u), although some are symmetric. The graph labels are integers ranging from 1 to N.

There is the related PACE 2022 Exact dataset, which was for exact computation; those graphs are generally smaller and sparser, as only exact solutions were accepted.

The data format begins with one line N E 0, where N is the number of vertices, E is the number of edges, and 0 is the literal integer zero. The N subsequent lines are each a space-separated list of integers, such as 2 5 11 19. If that appeared on line number 1 (the first after N E 0), it would indicate that there are edges from vertex 1 to each of the vertices 2, 5, 11, and 19. Some lines are blank, and these indicates vertices with outdegree zero. An example graph would be
```
4 4 0
2 3
3

1
```

The dataset can be downloaded here. The 100 instances that were available for public testing are precisely the odd-numbered ones in that link; the public instances can be downloaded on their own here.",2022,,,100 instances,,
2363,PackIt,Decision Making,Decision Making,"Decision Making, Robot Task Planning, General Reinforcement Learning",,,Methodology,robot-task-planning-on-packit,BSD-3,https://github.com/princeton-vl/PackIt,https://paperswithcode.com/dataset/packit,"The ability to jointly understand the geometry of objects and plan actions for manipulating them is crucial for intelligent agents. This ability is referred to as geometric planning. Recently, many interactive environments have been proposed to evaluate intelligent agents on various skills, however, none of them cater to the needs of geometric planning. PackIt is a virtual environment to evaluate and potentially learn the ability to do geometric planning, where an agent needs to take a sequence of actions to pack a set of objects into a box with limited space.",,,,,,
2364,PACO,2D Object Detection,2D Object Detection,2D Object Detection,Image,,Computer Vision,,MIT,https://github.com/facebookresearch/paco,https://paperswithcode.com/dataset/paco,"Parts and Attributes of Common Objects (PACO) is a detection dataset that goes beyond traditional object boxes and masks and provides richer annotations such as part masks and attributes. It spans 75 object categories, 456 object-part categories and 55 attributes across image (LVIS) and video (Ego4D) datasets. The dataset contains 641K part masks annotated across 260K object boxes, with half of them exhaustively annotated with attributes as well.",,PACO: Parts and Attributes of Common Objects,https://arxiv.org/pdf/2301.01795v1.pdf,,,
2365,PACS,Annotated Code Search,Annotated Code Search,"Annotated Code Search, Photo to Rest Generalization, Domain Generalization, Unsupervised Domain Adaptation, Image to sketch recognition, Unsupervised Continual Domain Shift Learning, Single-Source Domain Generalization, Domain Adaptation",Image,,Computer Vision,"annotated-code-search-on-pacs-so-ds, image-to-sketch-recognition-on-pacs, domain-adaptation-on-pacs, annotated-code-search-on-pacs-conala, unsupervised-continual-domain-shift-learning-1, annotated-code-search-on-pacs-staqc-py, unsupervised-domain-adaptation-on-pacs, single-source-domain-generalization-on-pacs, photo-to-rest-generalization-on-pacs, domain-generalization-on-pacs-2",,https://domaingeneralization.github.io/#data,https://paperswithcode.com/dataset/pacs,"PACS is an image dataset for domain generalization. It consists of four domains, namely Photo (1,670 images), Art Painting (2,048 images), Cartoon (2,344 images) and Sketch (3,929 images). Each domain contains seven categories.",,Deep Domain-Adversarial Image Generation for Domain Generalisation,https://arxiv.org/abs/2003.06054,670 images,,
2366,PAD-UFES-20,Skin Lesion Classification,Skin Lesion Classification,Skin Lesion Classification,Image,,Computer Vision,skin-lesion-classification-on-pad-ufes-20,,https://data.mendeley.com/datasets/zr7vgbcyr2/1,https://paperswithcode.com/dataset/pad-ufes-20,"Over the past few years, different Computer-Aided Diagnosis (CAD) systems have been proposed to tackle skin lesion analysis. Most of these systems work only for dermoscopy images since there is a strong lack of public clinical images archive available to evaluate the aforementioned CAD systems. To fill this gap, we release a skin lesion benchmark composed of clinical images collected from smartphone devices and a set of patient clinical data containing up to 21 features. The dataset consists of 1373 patients, 1641 skin lesions, and 2298 images for six different diagnostics: three skin diseases and three skin cancers. In total, 58.4% of the skin lesions are biopsy-proven, including 100% of the skin cancers. By releasing this benchmark, we aim to support future research and the development of new tools to assist clinicians to detect skin cancer.",,,,2298 images,"valuate the aforementioned CAD systems. To fill this gap, we release a skin lesion benchmark composed of clinical images",
2367,PadChest,Word Embeddings,Word Embeddings,"Word Embeddings, Computed Tomography (CT), Medical Diagnosis",,,Methodology,,Custom,https://github.com/auriml/Rx-thorax-automatic-captioning,https://paperswithcode.com/dataset/padchest,"PadChest is a labeled large-scale, high resolution chest x-ray dataset for the automated exploration
of medical images along with their associated reports. This dataset includes more than 160,000
images obtained from 67,000 patients that were interpreted and reported by radiologists at Hospital
San Juan Hospital (Spain) from 2009 to 2017, covering six different position views and additional
information on image acquisition and patient demography. The reports were labeled with 174 different
radiographic findings, 19 differential diagnoses and 104 anatomic locations organized as a hierarchical
taxonomy and mapped onto standard Unified Medical Language System (UMLS) terminology. Of
these reports, 27% were manually annotated by trained physicians and the remaining set was labeled
using a supervised method based on a recurrent neural network with attention mechanisms. The labels
generated were then validated in an independent test set achieving a 0.93 Micro-F1 score.",2009,,,"000
images",,
2368,Padding_Ain_t_Enough__Assessing_the_Privacy_Guaran,Website Fingerprinting Attacks,Website Fingerprinting Attacks,"Website Fingerprinting Attacks, Website Fingerprinting Defense",,,Methodology,,Creative Commons Attribution 4.0 International,https://zenodo.org/record/7319358,https://paperswithcode.com/dataset/padding-ain-t-enough-assessing-the-privacy,"This dataset contains the main data set of our FOCI 2020 paper ""Padding Ain’t Enough: Assessing the Privacy Guarantees of Encrypted DNS"".

https://www.usenix.org/conference/foci20/presentation/bushart

You can find the source code for this project on GitHub: https://github.com/jonasbb/padding-aint-enough

When using this software or our dataset, please cite our FOCI 20 paper.

@inproceedings {PaddingAintEnough,
    author    = {Jonas Bushart and Christian Rossow},
    booktitle = {10th {USENIX} Workshop on Free and Open Communications on the Internet ({FOCI} 20)},
    month     = aug,
    publisher = {{USENIX} Association},
    title     = {Padding Ain{\textquoteright}t Enough: Assessing the Privacy Guarantees of Encrypted {DNS}},
    year      = {2020},
}",2020,,,,,
2369,Paderbone_University_Bearing_Fault_Benckmark,Fault localization,Fault localization,"Fault localization, Time Series Analysis, Time Series Anomaly Detection, Fault Detection","Image, Time Series",,Computer Vision,,,https://www.phmsociety.org/sites/phmsociety.org/files/phm_submission/2015/phmec_16_003.pdf,https://paperswithcode.com/dataset/paderbone-university-bearing-fault-benckmark,"This paper presents a benchmark data set for condition monitoring of rolling bearings in combination with an extensive description of the corresponding bearing damage, the data set generation by experiments and results of datadriven classifications used as a diagnostic method. The diagnostic method uses the motor current signal of an electromechanical drive system for bearing diagnostic. The advantage of this approach in general is that no additional sensors are required, as current measurements can be performed in existing frequency inverters. This will help to reduce the cost of future condition monitoring systems. A particular novelty of the present approach is the monitoring of damage in external bearings which are installed in the drive system but outside the electric motor. Nevertheless, the motor current signal is used as input for the detection of the damage. Moreover, a wide distribution of bearing damage is considered for the benchmark data set. The results of the classifications show that the motor current signal can be used to identify and classify bearing damage within the drive system. However, the classification accuracy is still low compared to classifications based on vibration signals. Further, dependency on properties of those bearing damage that were used for the generation of training data are observed, because training with data of artificially generated and real bearing damages lead to different accuracies. Altogether a verified and systematically generated data set is presented and published online for further research",,Homepage,https://www.phmsociety.org/sites/phmsociety.org/files/phm_submission/2015/phmec_16_003.pdf,,,
2370,PAGE,Game of Go,Game of Go,Game of Go,,,Methodology,,MIT,https://github.com/YifanGao00/The-Professional-Go-Dataset,https://paperswithcode.com/dataset/page,"PAGE contains 98,525 games played by 2,007 professional players and spans over 70 years. The dataset includes rich AI analysis results for each move.",,The ProfessionAl Go annotation datasEt (PAGE),https://arxiv.org/pdf/2211.01559v1.pdf,,,
2371,PAMAP2,Human Activity Recognition,Human Activity Recognition,Human Activity Recognition,"Image, Video",,Computer Vision,human-activity-recognition-on-pamap2,CC BY 4.0,https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring,https://paperswithcode.com/dataset/pamap2,"The PAMAP2 Physical Activity Monitoring dataset contains data of 18 different physical activities (such as walking, cycling, playing soccer, etc.), performed by 9 subjects wearing 3 inertial measurement units and a heart rate monitor. The dataset can be used for activity recognition and intensity estimation, while developing and applying algorithms of data processing, segmentation, feature extraction and classification.

 Sensors 
3 Colibri wireless inertial measurement units (IMU):
  - sampling frequency: 100Hz
  - position of the sensors:
       - 1 IMU over the wrist on the dominant arm 
       - 1 IMU on the chest 
       - 1 IMU on the dominant side's ankle 
HR-monitor:
  - sampling frequency: ~9Hz

 Data collection protocol 
Each of the subjects had to follow a protocol, containing 12 different activities. The folder Protocol contains these recordings by subject.
Furthermore, some of the subjects also performed a few optional activities. The folder Optional contains these recordings by subject.

 Data files 
Raw sensory data can be found in space-separated text-files (.dat), 1 data file per subject per session (protocol or optional). Missing values are indicated with NaN. One line in the data files correspond to one timestamped and labeled instance of sensory data. The data files contain 54 columns: each line consists of a timestamp, an activity label (the ground truth) and 52 attributes of raw sensory data.",,,,,,
2372,PanNuke,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Panoptic Segmentation, whole slide images, Multi-tissue Nucleus Segmentation, Selection bias, Cell Detection",Image,,Computer Vision,"panoptic-segmentation-on-pannuke, multi-tissue-nucleus-segmentation-on-pannuke, cell-detection-on-pannuke",CC BY-NC-SA 4.0,https://warwick.ac.uk/fac/cross_fac/tia/data/pannuke,https://paperswithcode.com/dataset/pannuke,"PanNuke is a semi automatically generated nuclei instance segmentation and classification dataset with exhaustive nuclei labels across 19 different tissue types. The dataset consists of 481 visual fields, of which 312 are randomly sampled from more than 20K whole slide images at different magnifications, from multiple data sources. In total the dataset contains 205,343 labeled nuclei, each with an instance segmentation mask.",,,,,,
2373,Pano3D,Zero-Shot Out-of-Domain Detection,Zero-Shot Out-of-Domain Detection,"Zero-Shot Out-of-Domain Detection, Depth Estimation, Out-of-Distribution Detection, Surface Normals Estimation, Zero-Shot Learning + Domain Generalization, 3D Reconstruction, 3D Depth Estimation, Domain Adaptation","3D, Image",,Computer Vision,,Custom,https://vcl3d.github.io/Pano3D,https://paperswithcode.com/dataset/pano3d,"Pano3D  is a new benchmark for depth estimation from spherical panoramas. Its goal is to drive progress for this task in a consistent and holistic manner.  The Pano3D 360 depth estimation benchmark provides a standard Matterport3D train and test split, as well as a secondary GibsonV2 partioning for testing and training as well. The latter is used for zero-shot cross dataset transfer performance assessment and decomposes it into 3 different splits, each one focusing on a specific generalization axis.",,,,,,
2374,PanoContext,3D Room Layouts From A Single RGB Panorama,3D Room Layouts From A Single RGB Panorama,3D Room Layouts From A Single RGB Panorama,3D,,Methodology,3d-room-layouts-from-a-single-rgb-panorama-on,,https://panocontext.cs.princeton.edu/,https://paperswithcode.com/dataset/panocontext,The PanoContext dataset contains 500 annotated cuboid layouts of indoor environments such as bedrooms and living rooms.,,LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image,https://arxiv.org/abs/1803.08999,,,
2375,Panoptic,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, 3D Multi-Person Pose Estimation, 3D Human Pose Tracking, Head Pose Estimation","3D, Image, Video",,Computer Vision,"3d-human-pose-estimation-on-cmu-panoptic, head-pose-estimation-on-panoptic, 3d-human-pose-tracking-on-cmu-panoptic, 3d-multi-person-pose-estimation-on-cmu",Custom (non-commercial),http://domedb.perception.cs.cmu.edu/,https://paperswithcode.com/dataset/cmu-panoptic,"CMU Panoptic is a large scale dataset providing 3D pose annotations (1.5 millions) for multiple people engaging social activities. It contains 65 videos (5.5 hours) with multi-view annotations, but only 17 of them are in multi-person scenario and have the camera parameters.

Massively Multiview System


480 VGA camera views
30+ HD views
10 RGB-D sensors
Hardware-based sync
Calibration
Interesting Scenes with Labels

Multiple people


Socially interacting groups
3D body pose
3D facial landmarks
Transcripts + speaker ID

Hardware setup


480 VGA cameras, 640 x 480 resolution, 25 fps, synchronized among themselves using a hardware clock
31 HD cameras, 1920 x 1080 resolution, 30 fps, synchronized among themselves using a hardware clock, timing aligned with VGA cameras
10 Kinect Ⅱ Sensors. 1920 x 1080 (RGB), 512 x 424 (depth), 30 fps, timing aligned among themselves and other sensors
5 DLP Projectors. synchronized with HD cameras",1920,Single-Stage Multi-Person Pose Machines,https://arxiv.org/abs/1908.09220,,,
2376,Panoramic_Video_Panoptic_Segmentation_Dataset,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Image Segmentation, Autonomous Vehicles, Scene Understanding, Panoptic Segmentation, Temporal Sequences, Autonomous Driving","Image, Time Series, Video",,Computer Vision,,Waymo Dataset License Agreement for Non-Commercial Use,https://waymo.com/open/,https://paperswithcode.com/dataset/panoramic-video-panoptic-segmentation-dataset,"Panoramic Video Panoptic Segmentation Dataset is a large-scale dataset that offers high-quality panoptic segmentation labels for autonomous driving. The dataset has labels for 28 semantic categories and 2,860 temporal sequences that were captured by five cameras mounted on autonomous vehicles driving in three different geographical locations, leading to a total of 100k labeled camera images.",,"Waymo Open Dataset:

Panoramic Video Panoptic Segmentation",https://arxiv.org/pdf/2206.07704v1.pdf,,,
2377,Paper_Field,Sentence Classification,Sentence Classification,"Sentence Classification, Text Classification","Image, Text",English,Computer Vision,sentence-classification-on-paper-field,,https://docs.microsoft.com/en-us/academic-services/graph/reference-data-schema,https://paperswithcode.com/dataset/paper-field,"Paper Field is built from the Microsoft Academic Graph and maps paper titles to one of 7 fields of study. Each field of study - geography, politics, economics, business, sociology, medicine, and psychology - has approximately 12K training examples.",,,,,,
2378,ParaBank,Paraphrase Generation,Paraphrase Generation,"Paraphrase Generation, Causal Inference",Text,English,Natural Language Processing,,,http://decomp.io/projects/parabank/,https://paperswithcode.com/dataset/parabank,A large-scale English paraphrase dataset that surpasses prior work in both quantity and quality.,,,,,,
2379,Paragraph_Expanded,Antibody-antigen binding prediction,Antibody-antigen binding prediction,Antibody-antigen binding prediction,Time Series,,Methodology,antibody-antigen-binding-prediction-on-1,BSD-3-Clause license,https://github.com/oxpig/Paragraph,https://paperswithcode.com/dataset/paragraph-expanded,"To take advantage of the ever-increasing amount of structural data now available, we also trained Paragraph on a larger dataset. This new dataset was extracted from the Structural Antibody Database (SAbDab, Schneider et al., 2022) on March 31, 2022 and includes 1086 complexes which we divide into train, validation and test sets using a 60-20-20 split. Full details of both datasets are given in the Supplementary Information.",2022,,,,,
2380,Paralex,Paraphrase Generation,Paraphrase Generation,"Paraphrase Generation, Paraphrase Identification",Text,English,Natural Language Processing,paraphrase-generation-on-paralex,,http://knowitall.cs.washington.edu/paralex/,https://paperswithcode.com/dataset/paralex,Paralex learns from a collection of 18 million question-paraphrase pairs scraped from WikiAnswers.,,,,,,
2381,ParallelCorpus-Python,Source Code Summarization,Source Code Summarization,"Source Code Summarization, Code Summarization",Text,English,Natural Language Processing,source-code-summarization-on-parallelcorpus,,https://github.com/EdinburghNLP/code-docstring-corpus,https://paperswithcode.com/dataset/parallelcorpus-python,"The Python dataset introduced in the Parallel Corpus paper (A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation), commonly used for evaluating automated code summarization.",,A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation,https://aclanthology.org/I17-2053.pdf,,,
2382,PaRoutes,Retrosynthesis,Retrosynthesis,Retrosynthesis,,,Methodology,,Apache-2.0,https://github.com/MolecularAI/PaRoutes,https://paperswithcode.com/dataset/paroutes,"We introduce a framework for benchmarking multi-step retrosynthesis methods, i.e. route predictions, called PaRoutes. The framework consists of two sets of 10 000 synthetic routes extracted from the patent literature, a list of stock compounds, and a curated set of reactions on which one-step retrosynthesis models can be trained",,,,,,
2383,PART-OF,Link Prediction,Link Prediction,"Link Prediction, Network Embedding","Graph, Time Series",,Methodology,,,https://github.com/SotirisKot/Content-Aware-N2V,https://paperswithcode.com/dataset/part-of,"The PART-OF dataset is a dataset of relations extracted from a medical ontology. The different entities in the ontology are parts of the human body. The dataset has 16,894 nodes with 19,436 edges between them.",,https://arxiv.org/pdf/1906.05939.pdf,https://arxiv.org/pdf/1906.05939.pdf,,,
2384,Partial-iLIDS,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Dictionary Learning, Person Re-Identification",Image,,Computer Vision,,,https://github.com/JDAI-CV/Partial-Person-ReID,https://paperswithcode.com/dataset/partial-ilids,Partial iLIDS is a dataset for occluded person person re-identification. It contains a total of 476 images of 119 people captured by 4 non-overlapping cameras. Some images contain people occluded by other individuals or luggage.,,Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification,https://arxiv.org/abs/1904.04975,476 images,,
2385,Partial-REID,Dictionary Learning,Dictionary Learning,"Dictionary Learning, Person Re-Identification, Face Recognition",Image,,Computer Vision,person-re-identification-on-partial-reid,,https://github.com/JDAI-CV/Partial-Person-ReID,https://paperswithcode.com/dataset/partial-reid,"Partial REID is a specially designed partial person reidentification dataset that includes 600 images from 60 people, with 5 full-body images and 5 occluded images per person. These images were collected on a university campus by 6 cameras from different viewpoints, backgrounds and different types of occlusion. The examples of partial persons in the Partial REID dataset are shown in the Figure.",,Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification,https://arxiv.org/abs/1904.04975,600 images,,
2386,PartialSpoof,Voice Anti-spoofing,Voice Anti-spoofing,Voice Anti-spoofing,Audio,,Audio,,,https://nii-yamagishilab.github.io/zlin-demo/IS2021/index.html,https://paperswithcode.com/dataset/partialspoof,"PartialSpoof is a dataset of partially-spoofed data to evaluate detection of partially-spoofed speech data. It has been built based on the ASVspoof 2019 LA database since the latter covers 17 types of spoofed data produced by advanced speech synthesizers, voice converters, and hybrids. The authors used the same set of bona fide data from the ASVspoof 2019 LA database but created partially spoofed audio from the ASVspoof 2019 LA data.",2019,,,,,
2387,PartialSpoof_v1,Voice Anti-spoofing,Voice Anti-spoofing,Voice Anti-spoofing,Audio,,Audio,,Attribution 4.0 International,https://zenodo.org/record/4817532#.YLO07S2l1hE,https://paperswithcode.com/dataset/partialspoof-v1,"All existing databases of spoofed speech contain attack data that is spoofed in its entirety. In practice, it is entirely plausible that successful attacks can be mounted with utterances that are only partially spoofed. By definition, partially-spoofed utterances contain a mix of both spoofed and bona fide segments, which will likely degrade the performance of countermeasures trained with entirely spoofed utterances. This hypothesis raises the obvious question: ‘Can we detect partially spoofed audio?’ This paper introduces a new database of partially-spoofed data, named PartialSpoof, to help address this question. This new database enables to investigate and compare the performance of countermeasures on both utterance- and segmental- level labels.",,,,,,
2388,PartNet-Mobility,3D Pose Estimation,3D Pose Estimation,"3D Pose Estimation, Articulated Object modelling, 3D Feature Matching, 3D Shape Modeling","3D, Image",,Computer Vision,,MIT,https://sapien.ucsd.edu/,https://paperswithcode.com/dataset/partnet-mobility,"Dataset produced for the SAPIEN simulation environment. From the website: ""PartNet-Mobility dataset is a collection of 2K articulated objects with motion annotations and rendernig material. The dataset powers research for generalizable computer vision and manipulation. The dataset is a continuation of ShapeNet and PartNet. """,,SAPIEN,https://arxiv.org/abs/2003.08515,,,
2389,PartNet,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, 3D Instance Segmentation, Instance Segmentation, 3D Semantic Segmentation","3D, Image",,Computer Vision,"instance-segmentation-on-partnet, 3d-instance-segmentation-on-partnet, 3d-semantic-segmentation-on-partnet",,https://cs.stanford.edu/~kaichun/partnet/,https://paperswithcode.com/dataset/partnet,"PartNet is a consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. The dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. This dataset enables and serves as a catalyst for many tasks such as shape analysis, dynamic 3D scene modeling and simulation, affordance analysis, and others.",,,,,,
2390,PASCAL-5i,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Few-Shot Semantic Segmentation, Few-Shot Learning",Image,,Computer Vision,few-shot-semantic-segmentation-on-pascal5i-1,,https://github.com/DeepTrial/pascal-5,https://paperswithcode.com/dataset/pascal-5i,PASCAL-5i is a dataset used to evaluate few-shot segmentation. The dataset is sub-divided into 4 folds each containing 5 classes. A fold contains labelled samples from 5 classes that are used for evaluating the few-shot learning method. The rest 15 classes are used for training.,,AMP: Adaptive Masked Proxies for Few-Shot Segmentation,https://arxiv.org/abs/1902.11123,,valuate few-shot segmentation. The dataset is sub-divided into 4 folds each containing 5 classes. A fold contains labelled samples,5
2391,PASCAL-S,Saliency Detection,Saliency Detection,"Saliency Detection, RGB Salient Object Detection, Salient Object Detection",Image,,Computer Vision,"salient-object-detection-on-pascal-s, salient-object-detection-on-pascal-s-1, saliency-detection-on-pascal-s",,http://cbs.ic.gatech.edu/salobj/,https://paperswithcode.com/dataset/pascal-s,PASCAL-S is a dataset for salient object detection consisting of a set of 850 images from PASCAL VOC 2010 validation set with multiple salient objects on the scenes.,2010,Structured Modeling of Joint Deep Feature and Prediction Refinement for Salient Object Detection,https://arxiv.org/abs/1909.04366,850 images,,
2392,PASCAL3D_,Object Detection,Object Detection,"Object Detection, Viewpoint Estimation, Pose Estimation, Keypoint Detection","3D, Image",,Computer Vision,keypoint-detection-on-pascal3d,,https://cvgl.stanford.edu/projects/pascal3d.html,https://paperswithcode.com/dataset/pascal3d-2,"The Pascal3D+ multi-view dataset consists of images in the wild, i.e., images of object categories exhibiting high variability, captured under uncontrolled settings, in cluttered scenes and under many different poses. Pascal3D+ contains 12 categories of rigid objects selected from the PASCAL VOC 2012 dataset. These objects are annotated with pose information (azimuth, elevation and distance to camera). Pascal3D+ also adds pose annotated images of these 12 categories from the ImageNet dataset.",2012,Convolutional Models for Joint Object Categorization and Pose Estimation,https://arxiv.org/abs/1511.05175,,,12
2393,PASCAL_Context,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Boundary Detection, Semantic Segmentation, Surface Normals Estimation, Human Parsing, Saliency Detection","Image, Text",English,Computer Vision,"human-parsing-on-pascal-context, boundary-detection-on-pascal-context, semantic-segmentation-on-pascal-context, saliency-detection-on-pascal-context, surface-normals-estimation-on-pascal-context, zero-shot-learning-on-pascal-context",,https://cs.stanford.edu/~roozbeh/pascal-context/,https://paperswithcode.com/dataset/pascal-context,"The PASCAL Context dataset is an extension of the PASCAL VOC 2010 detection challenge, and it contains pixel-wise labels for all training images. It contains more than 400 classes (including the original 20 classes plus backgrounds from PASCAL VOC segmentation), divided into three categories (objects, stuff, and hybrids). Many of the object categories of this dataset are too sparse and; therefore, a subset of 59 frequent classes are usually selected for use.",2010,Image Segmentation Using Deep Learning:A Survey,https://arxiv.org/abs/2001.05566,,,400
2394,PASCAL_Face,Face Detection,Face Detection,Face Detection,Image,,Computer Vision,face-detection-on-pascal-face,,http://host.robots.ox.ac.uk/pascal/VOC/databases.html,https://paperswithcode.com/dataset/pascal-face,"The PASCAL FACE dataset is a dataset for face detection and face recognition. It has a total of 851 images which are a subset of the PASCAL VOC and has a total of 1,341 annotations. These datasets contain only a few hundreds of images and have limited variations in face appearance.",,Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results,https://arxiv.org/abs/1804.10275,851 images,,
2395,Pascal_Panoptic_Parts,Part-aware Panoptic Segmentation,Part-aware Panoptic Segmentation,"Part-aware Panoptic Segmentation, Human Part Segmentation, Image Segmentation, Scene Understanding, Panoptic Segmentation",Image,,Computer Vision,"image-segmentation-on-pascal-panoptic-parts, part-aware-panoptic-segmentation-on-pascal",,https://github.com/tue-mps/panoptic_parts,https://paperswithcode.com/dataset/pascal-panoptic-parts,The Pascal Panoptic Parts dataset consists of annotations for the part-aware panoptic segmentation task on the PASCAL VOC 2010 dataset. It is created by merging scene-level labels from PASCAL-Context with part-level labels from PASCAL-Part,2010,https://arxiv.org/abs/2106.06351,https://arxiv.org/abs/2106.06351s,,,
2396,PASCAL_VOC,Object Counting,Object Counting,"Object Counting, Semantic Segmentation, Image Segmentation, Unsupervised Semantic Segmentation with Language-image Pre-training, Knowledge Distillation, Interactive Segmentation, Talking Face Generation, Multi-object discovery, Multi-object colocalization, Single-object discovery, 3D Face Animation, Open Vocabulary Semantic Segmentation, Graph Matching, Zero-Shot Semantic Segmentation, Node Classification, Object Detection, Single-object colocalization","3D, Graph, Image, Text",English,Computer Vision,"3d-face-animation-on-vocaset, graph-matching-on-pascal-voc, multi-object-colocalization-on-voc-all, object-detection-on-pascal-voc, image-segmentation-on-pascal-voc, zero-shot-semantic-segmentation-on-pascal-voc, object-counting-on-pascal-voc, knowledge-distillation-on-pascal-voc, unsupervised-semantic-segmentation-with-7, single-object-discovery-on-voc-6x2, interactive-segmentation-on-pascal-voc, multi-object-discovery-on-voc-all, object-detection-on-pascal-voc-10, semantic-segmentation-on-pascal-voc, unsupervised-semantic-segmentation-with-11, open-vocabulary-semantic-segmentation-on-9, node-classification-on-pascalvoc-sp-1, single-object-discovery-on-voc-all, single-object-colocalization-on-voc-all, open-vocabulary-semantic-segmentation-on-5, object-counting-on-pascal-voc-2007-count-test, single-object-colocalization-on-voc-6x2",Custom,http://host.robots.ox.ac.uk/pascal/VOC/,https://paperswithcode.com/dataset/pascal-voc,"The PASCAL Visual Object Classes (VOC) 2012 dataset contains 20 object categories including vehicles, household, animals, and other: aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations, and object class annotations. This dataset has been widely used as a benchmark for object detection, semantic segmentation, and classification tasks. The PASCAL VOC dataset is split into three subsets: 1,464 images for training, 1,449 images for validation and a private testing set.",2012,Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey,https://arxiv.org/abs/1902.06162,464 images,"train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations, and object class annotations. This dataset has been widely used as a benchmark for object detection, semantic segmentation, and classification tasks. The PASCAL VOC dataset is split into three subsets: 1,464 images",
2397,PASCAL_VOC_2007,Cross-Modal Retrieval,Cross-Modal Retrieval,"Cross-Modal Retrieval, Object Counting, Multi-label Image Recognition with Partial Labels, Real-Time Object Detection, Robust Object Detection, Semantic Segmentation, Open World Object Detection, Image Classification, Unsupervised Semantic Segmentation with Language-image Pre-training, Object Localization, Weakly Supervised Object Detection, Unsupervised Object Localization, Unsupervised Object Detection, Multi-Label Classification, Zero-Shot Object Detection, Object Detection","Image, Text",English,Computer Vision,"robust-object-detection-on-pascal-voc-2007, semantic-segmentation-on-pascal-voc-2007, unsupervised-object-localization-on-pascal, unsupervised-semantic-segmentation-with-6, weakly-supervised-object-detection-on-pascal-1, real-time-object-detection-on-pascal-voc-2007, open-world-object-detection-on-pascal-voc, unsupervised-object-detection-on-pascal-voc, object-detection-on-pascal-voc-2007, multi-label-classification-on-pascal-voc-2007, object-counting-on-pascal-voc-2007-count-test, object-localization-on-pascal-voc-2007, multi-label-image-recognition-with-partial-1, image-classification-on-pascal-voc-2007",,http://host.robots.ox.ac.uk/pascal/VOC/voc2007/,https://paperswithcode.com/dataset/pascal-voc-2007,"PASCAL VOC 2007 is a dataset for image recognition. The twenty object classes that have been selected are:

Person: person
Animal: bird, cat, cow, dog, horse, sheep
Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train
Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor

The dataset can be used for image classification and object detection tasks.",2007,Object Detection and Recognition in Images,https://arxiv.org/abs/1708.01241,,,
2398,PASCAL_VOC_2012_test,Object Detection,Object Detection,"Object Detection, Weakly-Supervised Semantic Segmentation, Weakly Supervised Object Detection, Semantic Segmentation",Image,,Computer Vision,"weakly-supervised-semantic-segmentation-on-1, weakly-supervised-object-detection-on-pascal, semantic-segmentation-on-pascal-voc-2012, object-detection-on-pascal-voc-2012-test",,,https://paperswithcode.com/dataset/pascal-voc-2012-test,SCC Data Set,,,,,,
2399,pathbased,Outlier Detection,Outlier Detection,"Outlier Detection, Clustering Algorithms Evaluation, Clustering Ensemble, Image/Document Clustering","Image, Text",English,Computer Vision,"clustering-ensemble-on-pathbased, clustering-algorithms-evaluation-on-pathbased",,http://cs.uef.fi/sipu/datasets/pathbased.txt,https://paperswithcode.com/dataset/pathbased,pathbased is a 3-cluster data set. The data set consists of a circular cluster with an opening near the bottom and two Gaussian distributed clusters inside. Each cluster contains 100 data points.,,,,,,
2400,Pathfinder-X2,Long-range modeling,Long-range modeling,Long-range modeling,,,Methodology,,C.C. B.Y. 4.0,https://huggingface.co/datasets/Tylersuard/PathfinderX2,https://paperswithcode.com/dataset/pathfinder-x2,"Pathfinder and Pathfinder-X have proven to be instrumental in training and testing Large Language Models with long-range dependencies.
Recently, Meta's Moving Average Equipped Gated Attention model scored a 97% on the Pathfinder-X dataset, indicating a need for a larger,
more challenging dataset.  Whereas Pathfinder-X only went up to 256 x 256 pixel images (or a sequence length of 65,536 tokens), Pathfinder-X2 introduces images of 512 x 512 pixels, or 262,144 tokens.  

Each image is meant to be read as a sequence of pixels.  A LLM's task is to segment out the one snake in each image with a circle at its tip.  The dataset includes 200,000 images and 200,000 segmentation masks, one for each image.",,,,000 images,,
2401,PathQuestion,KG-to-Text Generation,KG-to-Text Generation,KG-to-Text Generation,Text,English,Natural Language Processing,kg-to-text-generation-on-pathquestion,,https://github.com/zmtkeke/IRN,https://paperswithcode.com/dataset/pathquestion,"Adopts two subsets of Freebase (Bollacker et al., 2008) as Knowledge Bases to construct the PathQuestion (PQ) and the PathQuestion-Large (PQL) datasets. Paths are extracted between two entities which span two hops (es → r1 → e1 → r2 → a, denoted by -2H) or three hops (es→ r1 → e1 →r2 → e2→ r3 → a, denoted by -3H) and then generated natural language questions with templates. To make the generated questions analogical to real-world questions, paraphrasing templates and synonyms for relations are included by searching the Internet and two real-world datasets, WebQuestions (Berant et al., 2013) and WikiAnswers (Fader et al., 2013). In this way, the syntactic structure and surface wording of the generated questions have been greatly enriched.",2008,An Interpretable Reasoning Network for Multi-Relation Question Answering,https://aclanthology.org/C18-1171.pdf,,,
2402,PathVQA,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Question Answering, Medical Report Generation, Medical Visual Question Answering","Image, Text",English,Computer Vision,medical-visual-question-answering-on-pathvqa,,https://github.com/UCSD-AI4H/PathVQA,https://paperswithcode.com/dataset/pathvqa,"PathVQA consists of 32,799 open-ended questions from 4,998 pathology images where each question is manually checked to ensure correctness.",,,,,,
2403,PATS,Speech-to-Gesture Translation,Speech-to-Gesture Translation,"Speech-to-Gesture Translation, Human Pose Forecasting","3D, Audio, Image, Text, Time Series",English,Speech,,CC-NC,https://chahuja.com/pats/,https://paperswithcode.com/dataset/pats,"PATS dataset consists of a diverse and large amount of aligned pose, audio and transcripts. With this dataset, we hope to provide a benchmark that would help develop technologies for virtual agents which generate natural and relevant gestures.

Webpage

Scripts",,,,,,
2404,PatternCom,Composed Image Retrieval (CoIR),Composed Image Retrieval (CoIR),"Composed Image Retrieval (CoIR), Zero-Shot Composed Image Retrieval (ZS-CIR)",Image,,Computer Vision,zero-shot-composed-image-retrieval-zs-cir-on-10,,,https://paperswithcode.com/dataset/pattercom,"PatternCom is a composed image retrieval benchmark based on PatternNet. PatternNet is a large-scale high-resolution remote sensing image retrieval dataset. There are 38 classes and each class has 800 images of size 256×256 pixels. In PatternCom, we select some classes to be depicted in query images, and add a query text that defines an attribute relevant to that class. For instance, query images of “swimming pools” are combined with text queries defining “shape” as “rectangular”, “oval”, and “kidney-shaped”. In total, PatternCom includes six attributes consisted of up to four different classes each. Each attribute can be associated with two to five values per class. The number of positives ranges from 2 to 1345 and there are more than 21k queries in total.",,,,800 images,val benchmark based on PatternNet. PatternNet is a large-scale high-resolution remote sensing image retrieval dataset. There are 38 classes and each class has 800 images,38
2405,PAWS-X,Language Modelling,Language Modelling,"Language Modelling, Cross-Lingual Paraphrase Identification, Paraphrase Identification, Cross-Lingual Transfer",Text,English,Natural Language Processing,cross-lingual-paraphrase-identification-on,Custom (commercial),https://github.com/google-research-datasets/paws/tree/master/pawsx,https://paperswithcode.com/dataset/paws-x,"PAWS-X contains 23,659 human translated PAWS evaluation pairs and 296,406 machine translated training pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. All translated pairs are sourced from examples in PAWS-Wiki.",,https://arxiv.org/pdf/1908.11828v1.pdf,https://arxiv.org/pdf/1908.11828v1.pdf,,"valuation pairs and 296,406 machine translated training pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. All translated pairs are sourced from examples",
2406,PAWS,Data Augmentation,Data Augmentation,"Data Augmentation, Paraphrase Identification, Natural Language Inference",Text,English,Natural Language Processing,,Custom,https://github.com/google-research-datasets/paws,https://paperswithcode.com/dataset/paws,"Paraphrase Adversaries from Word Scrambling (PAWS) is a dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature the importance of modeling structure, context, and word order information for the problem of paraphrase identification. The dataset has two subsets, one based on Wikipedia and the other one based on the Quora Question Pairs (QQP) dataset.",,,,,,
2407,PBC,Survival Analysis,Survival Analysis,"Survival Analysis, Time-to-Event Prediction",Time Series,,Methodology,,,https://stat.ethz.ch/R-manual/R-patched/library/survival/html/pbc.html,https://paperswithcode.com/dataset/pbc,"Primary sclerosing cholangitis is an autoimmune disease leading to destruction of the small bile ducts in the liver. Progression is slow but inexhortable, eventually leading to cirrhosis and liver decompensation. The condition has been recognised since at least 1851 and was named ""primary biliary cirrhosis"" in 1949. Because cirrhosis is a feature only of advanced disease, a change of its name to ""primary biliary cholangitis"" was proposed by patient advocacy groups in 2014.

This data is from the Mayo Clinic trial in PBC conducted between 1974 and 1984. A total of 424 PBC patients, referred to Mayo Clinic during that ten-year interval, met eligibility criteria for the randomized placebo controlled trial of the drug D-penicillamine. The first 312 cases in the data set participated in the randomized trial and contain largely complete data. The additional 112 cases did not participate in the clinical trial, but consented to have basic measurements recorded and to be followed for survival. Six of those cases were lost to follow-up shortly after diagnosis, so the data here are on an additional 106 cases as well as the 312 randomized participants.


age: in years
albumin:   serum albumin (g/dl)
alk.phos: alkaline phosphotase (U/liter)
ascites: presence of ascites
ast: aspartate aminotransferase, once called SGOT (U/ml)
bili: serum bilirunbin (mg/dl)
chol: serum cholesterol (mg/dl)
copper: urine copper (ug/day)
edema: 0 no edema, 0.5 untreated or successfully treated, 1 edema despite diuretic therapy
hepato: presence of hepatomegaly or enlarged liver
id: case number
platelet: platelet count
protime:  standardised blood clotting time
sex: m/f
spiders: blood vessel malformations in the skin
stage: histologic stage of disease (needs biopsy)
status: status at endpoint, 0/1/2 for censored, transplant, dead
time: number of days between registration and the earlier of death, transplantion, or study analysis in July, 1986
trt: 1/2/NA for D-penicillmain, placebo, not randomised
trig: triglycerides (mg/dl)",1949,,,,,
2408,PCC,Discourse Parsing,Discourse Parsing,"Discourse Parsing, Constituency Parsing",Text,English,Natural Language Processing,,Creative Commons Attribution-NonCommercial-ShareAlike,http://angcl.ling.uni-potsdam.de/resources/pcc.html,https://paperswithcode.com/dataset/pcc,"The Potsdam Commentary Corpus (PCC) is a corpus of 220 German newspaper commentaries (2.900 sentences, 44.000 tokens) taken from the online issues of the Märkische Allgemeine Zeitung (MAZ subcorpus) and Tagesspiegel (ProCon subcorpus) and is annotated with a range of different types of linguistic information.

The central subcorpus that we are making publicly available consists of 176 MAZ texts, which are annotated with


Sentence Syntax
Coreference
Discourse Structure (RST & PDTB)
Aboutness topics",,,,900 sentences,,
2409,PCD,Change Detection,Change Detection,"Change Detection, Poem meters classification, Scene Change Detection",Image,,Computer Vision,"scene-change-detection-on-pcd, change-detection-on-pcd, poem-meters-classification-on-pcd",,https://hci-lab.github.io/LearningMetersPoems/,https://paperswithcode.com/dataset/pcd,"The Arabic dataset is scraped mainly from الموسوعة الشعرية and الديوان. After merging both, the total number of verses is 1,831,770 poetic verses. Each verse is labeled by its meter, the poet who wrote it, and the age which it was written in. There are 22 meters, 3701 poets and 11 ages: Pre-Islamic, Islamic, Umayyad, Mamluk, Abbasid, Ayyubid, Ottoman, Andalusian, era between Umayyad and Abbasid, Fatimid, and finally the modern age. We are only interested in the 16 classic meters which are attributed to Al-Farahidi, and they comprise the majority of the dataset with a total number around 1.7M verses. It is important to note that the verses diacritic states are not consistent. This means that a verse can carry full, semi diacritics, or it can carry nothing.",,,,,,
2410,PCFG_SET,Semantic Parsing,Semantic Parsing,"Semantic Parsing, Systematic Generalization",Text,English,Natural Language Processing,,MIT,https://github.com/i-machine-think/am-i-compositional,https://paperswithcode.com/dataset/pcfg-set,"The Probabilistic Context Free Grammar String Edit Task (PCFG SET) dataset is a dataset with sequence to sequence problems specifically designed to test different aspects of compositional generalisation. In particular, the dataset contains splits to test for systematicity, productivity, substitutivity, localism and overgeneralisation.

The input alphabet of PCFG SET contains three types of words: words for unary and binary functions that represent \emph{string edit operations} (e.g. $\texttt{append}, \texttt{copy}, \texttt{reverse})$, elements to form the string sequences that these functions can be applied to (e.g. $\texttt{A}, \texttt{B}, \texttt{A1}, \texttt{B1}$), and a separator to separate the arguments of a binary function ($\texttt{,}$). The input sequences that are formed with this alphabet are sequences describing how a series of such operations are to be applied to a string argument. For instance:


$\texttt{repeat A B C }$ 
$\texttt{echo remove_first D K , E F}$ 
$\texttt{append swap F G H , repeat I J}$ 

The input sequences are generated with a PCFG, whose production probabilities are learned with EM to match the depth and length distributions in a corpus with English sentences.

The output of a PCFG SET sequence, representing its meaning, is constructed by recursively applying the string edit operations specified in the sequence. For instance: 


$\texttt{repeat A B C }$  &  $\rightarrow$  &  $\texttt{A B C A B C}$ 
$\texttt{echo remove_first D K , E F}$  & $\rightarrow$ & $\texttt{E F F}$
$\texttt{append swap F G H , repeat I J}$  & $\rightarrow$ & $\texttt{H G F I J I J }$

The string alphabet used for the construction of the dataset has 520 distinct elements, the length of the string arguments to a functions is limited to 5.The dataset contains around 100 thousand examples in total.  A full description of the dataset can be found in Hupkes et al (2020).",2020,,,,,
2411,PcMSP,Relation Classification,Relation Classification,"Relation Classification, Named Entity Recognition (NER), Sentence Classification","Graph, Image, Text",English,Computer Vision,,MIT,https://github.com/Xianjun-Yang/PcMSP,https://paperswithcode.com/dataset/pcmsp,"PcMSP is a dataset annotated  from 305 open access scientific articles for material science information extraction that simultaneously contains the synthesis sentences extracted from the experimental paragraphs, as well as the entity mentions and intra-sentence relations.",,PcMSP: A Dataset for Scientific Action Graphs Extraction from Polycrystalline Materials Synthesis Procedure Text,https://arxiv.org/pdf/2210.12401v1.pdf,,,
2412,PCQM4Mv2-LSC,Graph Property Prediction,Graph Property Prediction,"Graph Property Prediction, Graph Regression","Graph, Time Series",,Methodology,graph-regression-on-pcqm4mv2-lsc,CC BY 4.0,https://ogb.stanford.edu/docs/lsc/pcqm4mv2/,https://paperswithcode.com/dataset/pcqm4mv2-lsc,"PCQM4Mv2 is a quantum chemistry dataset originally curated under the PubChemQC project. Based on the PubChemQC, we define a meaningful ML task of predicting DFT-calculated HOMO-LUMO energy gap of molecules given their 2D molecular graphs. The HOMO-LUMO gap is one of the most practically-relevant quantum chemical properties of molecules since it is related to reactivity, photoexcitation, and charge transport. Moreover, predicting the quantum chemical property only from 2D molecular graphs without their 3D equilibrium structures is also practically favorable. This is because obtaining 3D equilibrium structures requires DFT-based geometry optimization, which is expensive on its own.",,,,,,
2413,PDFVQA,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), document understanding, Question Answering","Image, Text",English,Computer Vision,,,https://github.com/adlnlp/pdfvqa,https://paperswithcode.com/dataset/pdfvqa,PDFVQA: A New Dataset for Real-World VQA on PDF Documents,,,,,,
2414,PECAN,Antibody-antigen binding prediction,Antibody-antigen binding prediction,Antibody-antigen binding prediction,Time Series,,Methodology,antibody-antigen-binding-prediction-on-pecan,CC0 1.0,https://figshare.com/articles/dataset/Training_validation_and_testing_sample_data_used_in_Antibody_interface_prediction_with_3D_Zernike_descriptors_and_SVM_/5442229,https://paperswithcode.com/dataset/pecan,"The PECAN dataset provides structural data for antibody-antigen interactions, specifically curated for paratope and epitope binding site prediction. It includes a diverse set of antibody-antigen complexes, ensuring a well-balanced and representative dataset for training and evaluating deep learning models in protein-protein interaction (PPI) tasks.

Characteristics:
- Focuses on antibody-antigen binding site prediction.
- Derived from high-resolution X-ray crystallography structures.
- Includes training, validation, and test splits.
- Used as a benchmark dataset in multiple antibody binding site prediction studies.

Motivation and Use Cases:
- Facilitates the development of deep learning models for paratope and epitope identification.
- Enables researchers to evaluate and compare methods for antibody-antigen interaction prediction.
- Used in ParaSurf, a deep learning framework that achieves state-of-the-art results on this dataset.",,,,,,
2415,PeerQA,RAG,RAG,"RAG, Sentence Retrieval, Text Retrieval, Question Answering, answerability prediction, Passage Retrieval","Text, Time Series",English,Natural Language Processing,"answerability-prediction-on-peerqa, passage-retrieval-on-peerqa, question-answering-on-peerqa, sentence-retrieval-on-peerqa",CC-BY-NC-SA 4.0,,https://paperswithcode.com/dataset/peerqa,"We present PeerQA, a real-world, scientific, document-level Question Answering (QA) dataset. PeerQA questions have been sourced from peer reviews, which contain questions that reviewers raised while thoroughly examining the scientific article. Answers have been annotated by the original authors of each paper. The dataset contains 579 QA pairs from 208 academic articles, with a majority from ML and NLP, as well as a subset of other scientific communities like Geoscience and Public Health. PeerQA supports three critical tasks for developing practical QA systems: Evidence retrieval, unanswerable question classification, and answer generation. We provide a detailed analysis of the collected dataset and conduct experiments establishing baseline systems for all three tasks. Our experiments and analyses reveal the need for decontextualization in document-level retrieval, where we find that even simple decontextualization approaches consistently improve retrieval performance across architectures. On answer generation, PeerQA serves as a challenging benchmark for long-context modeling, as the papers have an average size of 12k tokens.",,,,,,
2416,PeerRead,Language Modelling,Language Modelling,"Language Modelling, Text Classification, Decision Making","Image, Text",English,Computer Vision,,,https://github.com/allenai/PeerRead,https://paperswithcode.com/dataset/peerread,"PearRead is a dataset of scientific peer reviews. The dataset consists of over 14K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR, as well as over 10K textual peer reviews written by experts for a subset of the papers.",,,,,,
2417,PEMS-BAY,Correlated Time Series Forecasting,Correlated Time Series Forecasting,"Correlated Time Series Forecasting, Time Series Forecasting, Multivariate Time Series Forecasting, Traffic Prediction",Time Series,,Time Series,"traffic-prediction-on-pems-bay, correlated-time-series-forecasting-on-pems",,https://zenodo.org/record/4263971#.Yt5GCOxKj0o,https://paperswithcode.com/dataset/pems-bay,PEMS-BAY is a dataset for traffic prediction.,,,,,,
2418,PEMS-BAY_Point_Missing,Multivariate Time Series Imputation,Multivariate Time Series Imputation,"Multivariate Time Series Imputation, Imputation, Traffic Data Imputation",Time Series,,Time Series,traffic-data-imputation-on-pems-bay-point,,,https://paperswithcode.com/dataset/pems-bay-point-missing,"The original dataset from Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting contains 6 months of traffic readings from 01/01/2017 to 05/31/2017 collected every 5 minutes by 325 traffic sensors in San Francisco Bay Area. The measurements are provided by California Transportation Agencies (CalTrans) Performance Measurement System (PeMS).

The Point missing setting, introduced in Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks, is a variant for imputation in which 25% of data are masked out uniformly at random. Results on this dataset are assumed to be obtained in-sample, meaning that the test interval is used also for training, excluding data used for evaluation.",2017,Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting,https://arxiv.org/abs/1707.01926,,,
2419,PeMS04,Fine-Grained Urban Flow Inference,Fine-Grained Urban Flow Inference,"Fine-Grained Urban Flow Inference, Correlated Time Series Forecasting, Time Series Forecasting, Multivariate Time Series Forecasting, Traffic Prediction",Time Series,,Methodology,"correlated-time-series-forecasting-on-pems, traffic-prediction-on-pems-bay, fine-grained-urban-flow-inference-on-taxibj, traffic-prediction-on-pems04",,,https://paperswithcode.com/dataset/pems04,PeMS04 is a traffic forecasting benchmark.,,,,,,
2420,PeMS07,Correlated Time Series Forecasting,Correlated Time Series Forecasting,"Correlated Time Series Forecasting, Time Series Forecasting, Multivariate Time Series Forecasting, Traffic Prediction",Time Series,,Time Series,traffic-prediction-on-pems07,,,https://paperswithcode.com/dataset/pems07,PeMS07 is a traffic forecasting benchmark.,,,,,,
2421,PeMS08,Traffic Prediction,Traffic Prediction,Traffic Prediction,Time Series,,Methodology,traffic-prediction-on-pems08,,,https://paperswithcode.com/dataset/pems08,PeMS08 is a traffic forecasting dataset.,,,,,,
2422,PeMSD4,Traffic Prediction,Traffic Prediction,Traffic Prediction,Time Series,,Methodology,traffic-prediction-on-pemsd4,,,https://paperswithcode.com/dataset/pemsd4,"The dataset refers to the traffic speed data in San Francisco Bay Area, containing 307 sensors on 29 roads. The time span of the dataset is January-February in 2018. It is a popular benchmark for traffic forecasting.",2018,,,,,
2423,PeMSD7,Correlated Time Series Forecasting,Correlated Time Series Forecasting,"Correlated Time Series Forecasting, Time Series Forecasting, Multivariate Time Series Forecasting, Traffic Prediction",Time Series,,Time Series,"time-series-forecasting-on-pemsd7, traffic-prediction-on-pemsd7",,,https://paperswithcode.com/dataset/pemsd7,PeMSD7 is traffic data in District 7 of California consisting of the traffic speed of 228 sensors while the period is from May to June in 2012 (only weekdays) with a time interval of 5 minutes. This dataset is popular for benchmark the traffic forecasting models.,2012,,,,,
2424,PeMSD8,Correlated Time Series Forecasting,Correlated Time Series Forecasting,"Correlated Time Series Forecasting, Time Series Forecasting, Multivariate Time Series Forecasting, Traffic Prediction",Time Series,,Time Series,traffic-prediction-on-pemsd8,,https://github.com/wanhuaiyu/ASTGCN#datasets,https://paperswithcode.com/dataset/pemsd8,"This dataset contains the traffic data in San Bernardino from July to August in 2016, with 170 detectors on 8 roads with a time interval of 5 minutes. This dataset is popular as a benchmark traffic forecasting dataset.",2016,,,,,
2425,Penn94,Node Classification on Non-Homophilic (Heterophilic) Graphs,Node Classification on Non-Homophilic (Heterophilic) Graphs,"Node Classification on Non-Homophilic (Heterophilic) Graphs, Node Classification",Image,,Graphs,"node-classification-on-non-homophilic-13, node-classification-on-penn94",,,https://paperswithcode.com/dataset/penn94,Node classification on Penn94,,,,,,
2426,Penn_Action,Skeleton Based Action Recognition,Skeleton Based Action Recognition,"Skeleton Based Action Recognition, Pose Estimation, Video Alignment, Action Recognition","3D, Image, Video",,Computer Vision,"video-alignment-on-upenn-action, action-recognition-on-penn-action, skeleton-based-action-recognition-on-upenn, pose-estimation-on-upenn-action",,http://dreamdragon.github.io/PennAction/,https://paperswithcode.com/dataset/penn-action,The Penn Action Dataset contains 2326 video sequences of 15 different actions and human joint annotations for each sequence.,,,,,,
2427,Penn_Treebank,Language Modelling,Language Modelling,"Language Modelling, Unsupervised Dependency Parsing, Open Information Extraction, Part-Of-Speech Tagging, Dependency Parsing, Stochastic Optimization, Chunking, Missing Elements, Constituency Parsing","Audio, Text",English,Natural Language Processing,"stochastic-optimization-on-penn-treebank, language-modelling-on-penn-treebank-character, dependency-parsing-on-penn-treebank, constituency-parsing-on-penn-treebank, unsupervised-dependency-parsing-on-penn, language-modelling-on-penn-treebank-word, missing-elements-on-penn-treebank, chunking-on-penn-treebank, open-information-extraction-on-penn-treebank, part-of-speech-tagging-on-penn-treebank",Custom,https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html,https://paperswithcode.com/dataset/penn-treebank,"The English Penn Treebank (PTB) corpus, and in particular the section of the corpus corresponding to the articles of Wall Street Journal (WSJ), is one of the most known and used corpus for the evaluation of models for sequence labelling. The task consists of annotating each word with its Part-of-Speech tag. In the most common split of this corpus,  sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens).
The corpus is also commonly used for character-level and word-level Language Modelling.",,Seq2Biseq: Bidirectional Output-wise Recurrent Neural Networks for Sequence Modelling,https://arxiv.org/abs/1904.04733,219 sentences,,
2428,PeopleSansPeople,Human Instance Segmentation,Human Instance Segmentation,"Human Instance Segmentation, 2D Human Pose Estimation, Human Detection","3D, Image",,Computer Vision,,Apache-2.0,https://github.com/Unity-Technologies/PeopleSansPeople,https://paperswithcode.com/dataset/peoplesanspeople,"In recent years, person detection and human pose estimation have made great strides, helped by large-scale labeled datasets. However, these datasets had no guarantees or analysis of human activities, poses, or context diversity. Additionally, privacy, legal, safety, and ethical concerns may limit the ability to collect more human data. An emerging alternative to real-world data that alleviates some of these issues is synthetic data. However, creation of synthetic data generators is incredibly challenging and prevents researchers from exploring their usefulness. Therefore, we release a human-centric synthetic data generator PeopleSansPeople which contains simulation-ready 3D human assets, a parameterized lighting and camera system, and generates 2D and 3D bounding box, instance and semantic segmentation, and COCO pose labels. Using PeopleSansPeople, we performed benchmark synthetic data training using a Detectron2 Keypoint R-CNN variant [1]. We found that pre-training a network using synthetic data and fine-tuning on target real-world data (few-shot transfer to limited subsets of COCO-person train [2]) resulted in a keypoint AP of 60.37±0.48 (COCO test-dev2017) outperforming models trained with the same real data alone (keypoint AP of 55.80) and pre-trained with ImageNet (keypoint AP of 57.50). This freely-available data generator should enable a wide range of research into the emerging field of simulation to real transfer learning in the critical area of human-centric computer vision.",,,,,,
2429,Perception_Test,Point Tracking,Point Tracking,"Point Tracking, Video Question Answering, Temporal Action Localization, Object Tracking, Question Answering","Image, Text, Time Series, Video",English,Computer Vision,"video-question-answering-on-perception-test, object-tracking-on-perception-test, point-tracking-on-perception-test",Creative Common CC-BY 4.0,https://github.com/deepmind/perception_test,https://paperswithcode.com/dataset/perception-test,"Perception Test is a benchmark designed to evaluate the perception and reasoning skills of multimodal models. It introduces real-world videos designed to show perceptually interesting situations and defines multiple tasks that require understanding of memory, abstract patterns, physics, and semantics – across visual, audio, and text modalities. The benchmark consists of 11.6k videos, 23s average length, filmed by around 100 participants worldwide. The videos are densely annotated with six types of labels: object and point tracks, temporal action and sound segments, multiple-choice video question-answers and grounded video question-answers. The benchmark probes pre-trained models for their transfer capabilities, in a zero-shot / few-shot or fine tuning regime.",,,,,,
2430,Perceptual_Similarity,Image Super-Resolution,Image Super-Resolution,"Image Super-Resolution, Image-to-Image Translation, Image Generation","Image, Text",English,Computer Vision,,,https://github.com/richzhang/PerceptualSimilarity,https://paperswithcode.com/dataset/perceptual-similarity,Perceptual Similarity is a dataset of human perceptual similarity judgments.,,Perceptual Similarity,https://arxiv.org/pdf/1801.03924.pdf,,,
2431,PerCQA,Community Question Answering,Community Question Answering,Community Question Answering,Text,English,Natural Language Processing,,,https://github.com/PerCQA/PerCQA-Dataset,https://paperswithcode.com/dataset/percqa,PerCQA is the first Persian dataset for CQA (Community Question Answering). This dataset contains the questions and answers crawled from the most well-known Persian forum.,,,,,,
2432,Performance_Improving_Code_Edits__PIE_,Language Modelling,Language Modelling,"Language Modelling, Code Repair",Text,English,Natural Language Processing,,,https://pie4perf.com/,https://paperswithcode.com/dataset/performance-improving-code-edits-pie,"PIE stands for Performance Improving Code Edits. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively
makes changes to improve the program’s performance.",,Learning Performance-Improving Code Edits,https://arxiv.org/pdf/2302.07867v3.pdf,,,
2433,Perfume_Co-Preference_Network,Sentiment Classification,Sentiment Classification,"Sentiment Classification, Persian Sentiment Analysis, graph construction, Graph Clustering, Community Detection, Network Community Partition","Graph, Image, Text",English,Computer Vision,,CC BY,https://github.com/Kalashi-Saed-Collaborations/SentimentDrivenCommunityDetection,https://paperswithcode.com/dataset/user-reviews-and-perfume-attributes-dataset,"The Perfume Co-Preference Network  dataset comprises comprehensive user reviews and ratings collected from the Persian retail platform Atrafshan. This dataset, central to our research on community detection in fragrance preferences, includes 36,434 comments from 7,387 unique users, providing insights into consumer sentiment towards various perfumes. It is designed to facilitate the analysis of user preferences through sentiment analysis, allowing for the clustering of perfumes based on shared attributes.

The dataset features three main components:



User Reviews and Perfume Attributes Dataset: This captures user sentiments expressed in comments, along with metadata such as user IDs, perfume details, and ratings across key attributes (scent, longevity, sillage, and design).



Emoji Mapping Dataset: This includes 392 common emojis mapped to their Persian equivalents to enhance sentiment analysis accuracy.



Sentiment Classification Results: This section includes three CSV files that detail sentiment classifications biased toward specific perfume attributes: Scent, Longevity, and Sillage. These classifications are derived from user comments using the ParsBert model, integrating user ratings to provide a nuanced understanding of consumer preferences.



For access to the dataset and further details, please visit our GitHub repository.

Total number of user comments: 36,434
Total number of unique users: 7,387
Number of emojis in mapping: 392
Number of CSV files with sentiment classifications: 3",,,,,,
2434,Perlex,Knowledge Base Population,Knowledge Base Population,"Knowledge Base Population, Relation Extraction",Graph,,Methodology,,,http://farsbase.net/PERLEX.html,https://paperswithcode.com/dataset/perlex,"Persian dataset for relation extraction, which is an expert-translated version of the ""Semeval-2010-Task-8"" dataset.",2010,,,,,
2435,Permuted_MNIST,Incremental Learning,Incremental Learning,"Incremental Learning, Continual Learning, Domain-IL Continual Learning",,,Methodology,"continual-learning-on-permuted-mnist, domain-il-continual-learning-on-permuted",,,https://paperswithcode.com/dataset/permuted-mnist,"Permuted MNIST is an MNIST variant that consists of 70,000 images of handwritten digits from 0 to 9, where 60,000 images are used for training, and 10,000 images for test. The difference of this dataset from the original MNIST is that each of the ten tasks is the multi-class classification of a different random permutation of the input pixels.",,Lifelong Learning with Dynamically Expandable Networks,https://arxiv.org/abs/1708.01547,000 images,"training, and 10,000 images",
2436,Persian-ATIS,slot-filling,slot-filling,"slot-filling, Intent Detection, Out of Distribution (OOD) Detection, Intent Discovery",Image,,Computer Vision,"intent-discovery-on-persian-atis, out-of-distribution-ood-detection-on-persian",,https://github.com/DSInCenter/Persian-Atis,https://paperswithcode.com/dataset/persian-atis,The PATIS is a Persian language dataset for intent detection and slot filling.,,,,,,
2437,Perspectrum,Natural Language Understanding,Natural Language Understanding,"Natural Language Understanding, Sentiment Analysis, Domain Adaptation, Stance Detection","Image, Text",English,Computer Vision,stance-detection-on-perspectrum,,https://github.com/CogComp/perspectrum,https://paperswithcode.com/dataset/perspectrum,"Perspectrum is a dataset of claims, perspectives and evidence, making use of online debate websites to create the initial data collection, and augmenting it using search engines in order to expand and diversify the dataset. Crowd-sourcing was used to filter out noise and ensure high-quality data. The dataset contains 1k claims, accompanied with pools of 10k and 8k perspective sentences and evidence paragraphs, respectively.",,,,,,
2438,PET,Term Extraction,Term Extraction,Term Extraction,,,Methodology,,,https://pdi.fbk.eu//process-extraction-from-text-overview/,https://paperswithcode.com/dataset/pet,"The dataset contains 45 documents containing narrative description of business process and their annotations.  Annotated with activities, gateways, actors, and flow information. 

Each document is composed of three files:

Doc_name.txt (Process description in CONLL format)

Doc_name.process-elements.IOB2.txt (Process elements annotated with IOB2 Schema in CONLL format)

Doc_name.relations.tsv (Process relations between process elements. Each line is a triplette (source, relation tag, target). Source and target are in the form: n_sent_x words range.)",,,,45 documents,,
2439,PETRAW,Kinematic Based Workflow Recognition,Kinematic Based Workflow Recognition,"Kinematic Based Workflow Recognition, Video Segmentation, Semantic Segmentation, Surgical Gesture Recognition, Action Triplet Recognition, Segmentation Based Workflow Recognition, Video & Kinematic Base Workflow Recognition, Video Based Workflow Recognition, Action Recognition, Video, Kinematic & Segmentation Base Workflow Recognition, Activity Recognition","Image, Video",,Computer Vision,"video-based-workflow-recognition-on-petraw, segmentation-based-workflow-recognition-on, kinematic-based-workflow-recognition-on, video-kinematic-segmentation-base-workflow, video-kinematic-base-workflow-recognition-on, semantic-segmentation-on-petraw",,https://www.synapse.org/PETRAW,https://paperswithcode.com/dataset/petraw,"PETRAW data set was composed of 150 sequences of peg transfer training sessions. The objective of the peg transfer session is to transfer 6 blocks from the left to the right and back. Each block must be extracted from a peg with one hand, transferred to the other hand, and inserted in a peg at the other side of the board.
All cases were acquired by a non-medical expert on the LTSI Laboratory from the University of Rennes. The data set was divided into a training data set composed of 90 cases and a test data set composed of 60 cases. A case was composed of kinematic data, a video, semantic segmentation of each frame, and workflow annotation.",,,,,,
2440,PET__A_new_Dataset_for_Process_Extraction_from_Nat,Term Extraction,Term Extraction,Term Extraction,,,Methodology,,,https://pdi-new.fbk.eu/pet/,https://paperswithcode.com/dataset/pet-a-new-dataset-for-process-extraction-from,"The dataset contains 45 documents containing narrative description of business process and their annotations.  Annotated with activities, gateways, actors, and flow information. 

Each document is composed of three files:

Doc_name.txt (Process description in CONLL format)

Doc_name.process-elements.IOB2.txt (Process elements annotated with IOB2 Schema in CONLL format)

Doc_name.relations.tsv (Process relations between process elements. Each line is a triplette (source, relation tag, target). Source and target are in the form: n_sent_x words range.)",,,,45 documents,,
2441,PG-19,Language Modelling,Language Modelling,"Language Modelling, Reading Comprehension, Dialogue Generation",Text,English,Natural Language Processing,dialogue-generation-on-pg-19,,https://github.com/deepmind/pg19,https://paperswithcode.com/dataset/pg-19,A new open-vocabulary language modelling benchmark derived from books.,,,,,,
2442,PGDP5K,Visual Reasoning,Visual Reasoning,"Visual Reasoning, Scene Parsing, Multi-Task Learning","Image, Text",English,Reasoning,scene-parsing-on-pgdp5k,,http://www.nlpr.ia.ac.cn/databases/CASIA-PGDP5K/index.html,https://paperswithcode.com/dataset/pgdp5k,"PGDP5K is a dataset consisting of 5000 diagram samples composed of 16 shapes, covering 5 positional relations, 22 symbol types and 6 text types, labeled with more fine-grained annotations at primitive level, including primitive classes, locations and relationships, where 1,813 non-duplicated images are selected from the Geometry3K dataset and other 3,187 images are collected from three popular textbooks across grades 6-12 on mathematics curriculum websites by taking screenshots from PDF books.",,,,187 images,,
2443,PGM,Relational Reasoning,Relational Reasoning,"Relational Reasoning, Image Classification, Visual Reasoning",Image,,Reasoning,,"Custom (research-only, non-commercial)",https://github.com/deepmind/abstract-reasoning-matrices,https://paperswithcode.com/dataset/pgm,"PGM dataset serves as a tool for studying both abstract reasoning and generalisation in models. Generalisation is a multi-faceted phenomenon; there is no single, objective way in which models can or should generalise beyond their experience. The PGM dataset provides a means to measure the generalization ability of models in different ways, each of which may be more or less interesting to researchers depending on their intended training setup and applications.",,Measuring abstract reasoning in neural networks,https://arxiv.org/pdf/1807.04225v1.pdf,,,
2444,PGPS9K,Math Word Problem Solving,Math Word Problem Solving,"Math Word Problem Solving, Mathematical Reasoning",,,Methodology,mathematical-reasoning-on-pgps9k,,http://www.nlpr.ia.ac.cn/databases/CASIA-PGPS9K/index.html,https://paperswithcode.com/dataset/pgps9k,"A new large scale plane geometry problem solving dataset called PGPS9K, labeled both fine-grained diagram annotation and interpretable solution program.",,,,,,
2445,PH2,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Skin Cancer Segmentation, Lesion Segmentation",Image,,Computer Vision,"semantic-segmentation-on-ph2, skin-cancer-segmentation-on-ph2, lesion-segmentation-on-ph2",,https://www.fc.up.pt/addi/ph2%20database.html,https://paperswithcode.com/dataset/ph2,"The increasing incidence of melanoma has recently promoted the development of computer-aided diagnosis systems for the classification of dermoscopic images. The PH² dataset has been developed for research and benchmarking purposes, in order to facilitate comparative studies on both segmentation and classification algorithms of dermoscopic images. PH² is a dermoscopic image database acquired at the Dermatology Service of Hospital Pedro Hispano, Matosinhos, Portugal.",,,,,,
2446,PhC-C2DH-U373,Cell Detection,Cell Detection,"Cell Detection, Cell Segmentation",Image,,Computer Vision,"cell-segmentation-on-phc-c2dh-u373, cell-detection-on-phc-c2dh-u373",,http://celltrackingchallenge.net/2d-datasets/,https://paperswithcode.com/dataset/phc-c2dh-u373,"Glioblastoma-astrocytoma U373 cells on a polyacrylamide substrate

Dr. S. Kumar. Department of Bioengineering, University of California at Berkeley, Berkeley CA (USA)",,,,,,
2447,PhD,Hallucination Evaluation,Hallucination Evaluation,Hallucination Evaluation,,,Methodology,,,https://github.com/jiazhen-code/PhD,https://paperswithcode.com/dataset/phd,"Multimodal Large Language Models (MLLMs) hallucinate, resulting in an emerging topic of visual hallucination evaluation (VHE). This paper contributes a ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective VHE at a large scale. The essence of VHE is to ask an MLLM questions about specific images to assess its susceptibility to hallucination. Depending on what to ask (objects, attributes, sentiment, etc.) and how the questions are asked, we structure PhD along two dimensions, i.e., task and mode. Five visual recognition tasks, ranging from low-level (object / attribute recognition) to middle-level (sentiment / position recognition and counting), are considered. Besides a normal visual QA mode, which we term PhD-base, PhD also asks questions with inaccurate context (PhD-iac) or with incorrect context (PhD-icc), or with AI-generated counter common sense images (PhD-ccs). We construct PhD by a ChatGPT-assisted semi-automated pipeline, encompassing four pivotal modules: task-specific hallucinatory item (hitem) selection, hitem-embedded question generation, inaccurate / incorrect context generation, and counter-common-sense (CCS) image generation. With over 14k daily images, 750 CCS images and 102k VQA triplets in total, PhD reveals considerable variability in MLLMs' performance across various modes and tasks, offering valuable insights into the nature of hallucination. As such, PhD stands as a potent tool not only for VHE but may also play a significant role in the refinement of MLLMs.",,,,,"valuation (VHE). This paper contributes a ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective VHE at a large scale. The essence of VHE is to ask an MLLM questions about specific images to assess its susceptibility to hallucination. Depending on what to ask (objects, attributes, sentiment, etc.) and how the questions are asked, we structure PhD along two dimensions, i.e., task and mode. Five visual recognition tasks, ranging from low-level (object / attribute recognition) to middle-level (sentiment / position recognition and counting), are considered. Besides a normal visual QA mode, which we term PhD-base, PhD also asks questions with inaccurate context (PhD-iac) or with incorrect context (PhD-icc), or with AI-generated counter common sense images",
2448,Phee,Event Extraction,Event Extraction,Event Extraction,,,Methodology,,,https://github.com/zhaoyuesun/phee,https://paperswithcode.com/dataset/phee,Phee is a dataset for pharmacovigilance comprising over 5000 annotated events from medical case reports and biomedical literature. It is designed for biomedical event extraction tasks.,,PHEE: A Dataset for Pharmacovigilance Event Extraction from Text,https://arxiv.org/pdf/2210.12560v1.pdf,,,
2449,PHINC,Humor Detection,Humor Detection,"Humor Detection, Sentiment Analysis","Image, Text",English,Computer Vision,,CC BY 4.0,https://doi.org/10.5281/zenodo.3605597,https://paperswithcode.com/dataset/phinc,"PHINC is a parallel corpus of the 13,738 code-mixed English-Hindi sentences and their corresponding translation in English. The translations of sentences are done manually by the annotators.",,,,,,
2450,PHM2017,Epidemiology,Epidemiology,Epidemiology,,,Methodology,,,https://github.com/emory-irlab/PHM2017,https://paperswithcode.com/dataset/phm2017,"PHM2017 is a new dataset consisting of 7,192 English tweets across six diseases and conditions: Alzheimer’s Disease, heart attack (any severity), Parkinson’s disease, cancer (any type), Depression (any severity), and Stroke. The Twitter search API was used to retrieve the data using the colloquial disease names as search keywords, with the expectation of retrieving a high-recall, low precision dataset. After removing the re-tweets and replies, the tweets were manually annotated. The labels are:


self-mention. The tweet contains a health mention with a health self-report of the Twitter account owner, e.g., ""However, I worked hard and ran for Tokyo Mayer Election Campaign in January through February, 2014, without publicizing the cancer.""
other-mention. The tweet contains a health mention of a health report about someone other than the account owner, e.g., ""Designer with Parkinson’s couldn’t work then engineer invents bracelet + changes her world""
awareness. The tweet contains the disease name, but does not mention a specific person, e.g., ""A Month Before a Heart Attack, Your Body Will Warn You With These 8 Signals""
non-health. The tweet contains the disease name, but the tweet topic is not about health. ""Now I can have cancer on my wall for all to see <3""",2014,Did You Really Just Have a Heart Attack? Towards Robust Detection of Personal Health Mentions in Social Media,https://arxiv.org/pdf/1802.09130v2.pdf,,,
2451,PhoMT,Translation,Translation,"Translation, Machine Translation",Text,English,Natural Language Processing,translation-on-phomt,Custom,https://github.com/VinAIResearch/PhoMT,https://paperswithcode.com/dataset/phomt,PhoMT is a high-quality and large-scale Vietnamese-English parallel dataset of 3.02M sentence pairs for machine translation.,,,,,,
2452,Photi-LakeIce,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Webcam (RGB) image classification, Lake Ice Monitoring",Image,,Computer Vision,,,https://github.com/czarmanu/photi-lakeice-dataset,https://paperswithcode.com/dataset/photi-lakeice,"A new benchmark dataset of webcam images, Photi-LakeIce, from multiple cameras and two different winters, along with pixel-wise ground truth annotations.",,,,,,
2453,PhotoBook,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Visual Dialog, Chatbot",Image,,Computer Vision,,,https://dmg-photobook.github.io/,https://paperswithcode.com/dataset/photobook,"A large-scale collection of visually-grounded, task-oriented dialogues in English designed to investigate shared dialogue history accumulating during conversation.",,,,,,
2454,PhotoChat,Multimodal Intent Recognition,Multimodal Intent Recognition,"Multimodal Intent Recognition, Image Retrieval",Image,,Multimodal,"image-retrieval-on-photochat, multimodal-intent-recognition-on-photochat",Custom,,https://paperswithcode.com/dataset/photochat,"PhotoChat, the first dataset that casts light on the photo sharing behavior in online messaging. PhotoChat contains 12k dialogues,
each of which is paired with a user photo that is shared during the conversation. Based on this dataset, we propose two tasks to facilitate research on image-text modeling: a
photo-sharing intent prediction task that predicts whether one intends to share a photo in the next conversation turn, and a photo retrieval task that retrieves the most relevant photo according to the dialogue context.",,,,,,
2455,Photoswitch,Gaussian Processes,Gaussian Processes,"Gaussian Processes, Molecular Property Prediction, Active Learning",Time Series,,Methodology,,,https://github.com/Ryan-Rhys/The-Photoswitch-Dataset,https://paperswithcode.com/dataset/photoswitch,A benchmark for molecular machine learning where improvements in model performance can be immediately observed in the throughput of promising molecules synthesized in the lab. Photoswitches are a versatile class of molecule for medical and renewable energy applications where a molecule's efficacy is governed by its electronic transition wavelengths.,,,,,,
2456,Phrase-in-Context,text similarity,text similarity,"text similarity, Semantic Similarity, Natural Language Understanding, Semantic Retrieval, Information Retrieval, Question Answering",Text,English,Natural Language Processing,,CC-BY-NC 4.0,https://phrase-in-context.github.io/,https://paperswithcode.com/dataset/phrase-in-context,"Phrase in Context is a curated benchmark for phrase understanding and semantic search, consisting of three tasks of increasing difficulty: Phrase Similarity (PS), Phrase Retrieval (PR) and Phrase Sense Disambiguation (PSD). The datasets are annotated by 13 linguistic experts on Upwork and verified by two groups: ~1000 AMT crowdworkers and another set of 5 linguistic experts. PiC benchmark is distributed under CC-BY-NC 4.0.",,,,,,
2457,PhraseCut,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Referring Expression Segmentation",Image,,Computer Vision,referring-expression-segmentation-on,,https://people.cs.umass.edu/~chenyun/publication/phrasecut/,https://paperswithcode.com/dataset/phrasecut,"PhraseCut is a dataset consisting of 77,262 images and 345,486 phrase-region pairs. The dataset is collected on top of the Visual Genome dataset and uses the existing annotations to generate a challenging set of referring phrases for which the corresponding regions are manually annotated.",,,,262 images,,
2458,PhyAAt,Noise Level Prediction,Noise Level Prediction,"Noise Level Prediction, Attention Score Prediction, LWR Classification, Semanticity prediction","Audio, Image, Time Series",,Computer Vision,"attention-score-prediction-on-phyaat, noise-level-prediction-on-phyaat, semanticity-prediction-on-phyaat, lwr-classification-on-phyaat",The 3-Clause BSD License,https://phyaat.github.io,https://paperswithcode.com/dataset/phyaat,"The dataset contains a collection of physiological signals (EEG, GSR, PPG) obtained from an experiment of the auditory attention on natural speech. Ethical Approval was acquired for the experiment. Details of the experiment can be found here https://phyaat.github.io/experiment 

Dataset
The dataset contain three physiological signals recorded at sampling rate of 128Hz from 25 healthy subjects during the experiment. Electroenceplogram (EEG) signal is recorded using a 14-channel Emotiv Epoc device. Two signal streams of Galvanic Skin Response (GSR) were recorded, instantaneous sample and moving averaged signal. From photoplethysmogram (PPG) sensor (pulse sensor), a raw signal, inter-beat interval (IBI), and pulse rate were recorded. All the signals were properly labeled.


EEG Channels: 'AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4'
GSR Signal: Instantaneous and moving averaged signal streams
PPG:  PPG (ECG like signal), IBI (Inter Beat Interval ) and BPM (Beats per minute)

Download the dataset
Using Python
To download the dataset, install phyaat library and download through it.

pip install phyaat

```
import phyaat as ph

to download dataset of subject 1 in given path 'dirpath
dirPath = ph.download_data(baseDir='../PhyAAt_Data', subject=1,verbose=0,overwrite=False) 

to download dataset of all the subjects
dirPath = ph.download_data(baseDir='../PhyAAt_Data', subject=-1,verbose=0,overwrite=False)
```

Manually
If you are using other programming framework such as matlab or R, Download dataset manually from
Github repository
and extract all the csv files.

For more details on downloading and using dataset, check here:  Getting Started

Helper Scripts
There are starter scripts and benchmark code to start building models. They are available here - https://phyaat.github.io/modeling/",,,,,,
2459,PHYRE,Video Prediction,Video Prediction,"Video Prediction, Visual Reasoning, Common Sense Reasoning","Image, Time Series, Video",,Computer Vision,"visual-reasoning-on-phyre-1b-cross, visual-reasoning-on-phyre-1b-within",Apache-2.0,https://player.phyre.ai,https://paperswithcode.com/dataset/phyre,Benchmark for physical reasoning that contains a set of simple classical mechanics puzzles in a 2D physical environment. The benchmark is designed to encourage the development of learning algorithms that are sample-efficient and generalize well across puzzles.,,,,,,
2460,Physical_Audiovisual_CommonSense,Physical Commonsense Reasoning,Physical Commonsense Reasoning,"Physical Commonsense Reasoning, Common Sense Reasoning",,,Reasoning,physical-commonsense-reasoning-on-physical,MIT,https://github.com/samuelyu2002/PACS,https://paperswithcode.com/dataset/pacs-commonsense,"PACS (Physical Audiovisual CommonSense) is the first audiovisual benchmark annotated for physical commonsense attributes. PACS contains a total of 13,400 question-answer pairs, involving 1,377 unique physical commonsense questions and 1,526 videos. The dataset provides new opportunities to advance the research field of physical reasoning by bringing audio as a core component of this multimodal problem.",,,,,,
2461,PhysionetMotorImagery_MOABB,Within-Session Motor Imagery (right hand vs. feet),Within-Session Motor Imagery (right hand vs. feet),"Within-Session Motor Imagery (right hand vs. feet), Within-Session Motor Imagery (left hand vs. right hand), Within-Session Motor Imagery (all classes)",,,Methodology,"within-session-motor-imagery-left-hand-vs-5, within-session-motor-imagery-right-hand-vs-5, within-session-motor-imagery-all-classes-on-3",,http://moabb.neurotechx.com/docs/generated/moabb.datasets.PhysionetMI.html,https://paperswithcode.com/dataset/physionetmotorimagery-moabb,,,,,,,
2462,PhysioNet_Challenge_2012,Imputation,Imputation,"Imputation, Time Series Classification, Multivariate Time Series Forecasting, Multivariate Time Series Imputation, Time Series Analysis","Image, Time Series",,Computer Vision,"imputation-on-physionet-challenge-2012, multivariate-time-series-imputation-on-1, time-series-classification-on-physionet, time-series-on-physionet-challenge-2012, multivariate-time-series-forecasting-on-2",,https://polyp.grand-challenge.org/CVCClinicDB/,https://paperswithcode.com/dataset/physionet-challenge-2012,"The PhysioNet Challenge 2012 dataset is publicly available and contains the de-identified records of 8000 patients in Intensive Care Units (ICU). Each record consists of roughly 48 hours of multivariate time series data with up to 37 features recorded at various times from the patients during their stay such as respiratory rate, glucose etc.",2012,Multi-resolution Networks For Flexible Irregular Time Series Modeling (Multi-FIT),https://arxiv.org/abs/1905.00125,,,
2463,PhysioNet_Challenge_2018,Sleep Stage Detection,Sleep Stage Detection,Sleep Stage Detection,Image,,Computer Vision,sleep-stage-detection-on-physionet-challenge-1,,https://physionet.org/content/challenge-2018/1.0.0/,https://paperswithcode.com/dataset/physionet-challenge-2018,"Data for this challenge were contributed by the Massachusetts General Hospital’s (MGH) Computational Clinical Neurophysiology Laboratory (CCNL), and the Clinical Data Animation Laboratory (CDAC). The dataset includes 1,985 subjects which were monitored at an MGH sleep laboratory for the diagnosis of sleep disorders. The data were partitioned into balanced training (n = 994), and test sets (n = 989).

The sleep stages of the subjects were annotated by clinical staff at the MGH according to the American Academy of Sleep Medicine (AASM) manual for the scoring of sleep. More specifically, the following six sleep stages were annotated in 30 second contiguous intervals: wakefulness, stage 1, stage 2, stage 3, rapid eye movement (REM), and undefined.

Certified sleep technologists at the MGH also annotated waveforms for the presence of arousals that interrupted the sleep of the subjects. The annotated arousals were classified as either: spontaneous arousals, respiratory effort related arousals (RERA), bruxisms, hypoventilations, hypopneas, apneas (central, obstructive and mixed), vocalizations, snores, periodic leg movements, Cheyne-Stokes breathing or partial airway obstructions.

The subjects had a variety of physiological signals recorded as they slept through the night including: electroencephalography (EEG), electrooculography (EOG), electromyography (EMG), electrocardiology (EKG), and oxygen saturation (SaO2). Excluding SaO2, all signals were sampled to 200 Hz and were measured in microvolts. For analytic convenience, SaO2 was resampled to 200 Hz, and is measured as a percentage.",,,,,,
2464,PhysioNet_Challenge_2020,ECG Classification,ECG Classification,ECG Classification,Image,,Computer Vision,ecg-classification-on-physionet-challenge,BSD 3.0,https://moody-challenge.physionet.org/2020/#data,https://paperswithcode.com/dataset/physionet-challenge-2020,"Data
The data for this Challenge are from multiple sources:
CPSC Database and CPSC-Extra Database
INCART Database
PTB and PTB-XL Database
The Georgia 12-lead ECG Challenge (G12EC) Database
Undisclosed Database
The first source is the public (CPSC Database) and unused data (CPSC-Extra Database) from the China Physiological Signal Challenge in 2018 (CPSC2018), held during the 7th International Conference on Biomedical Engineering and Biotechnology in Nanjing, China. The unused data from the CPSC2018 is NOT the test data from the CPSC2018. The test data of the CPSC2018 is included in the final private database that has been sequestered. This training set consists of two sets of 6,877 (male: 3,699; female: 3,178) and 3,453 (male: 1,843; female: 1,610) of 12-ECG recordings lasting from 6 seconds to 60 seconds. Each recording was sampled at 500 Hz.

The second source set is the public dataset from St Petersburg INCART 12-lead Arrhythmia Database. This database consists of 74 annotated recordings extracted from 32 Holter records. Each record is 30 minutes long and contains 12 standard leads, each sampled at 257 Hz.

The third source from the Physikalisch Technische Bundesanstalt (PTB) comprises two public databases: the PTB Diagnostic ECG Database and the PTB-XL, a large publicly available electrocardiography dataset. The first PTB database contains 516 records (male: 377, female: 139). Each recording was sampled at 1000 Hz. The PTB-XL contains 21,837 clinical 12-lead ECGs (male: 11,379 and female: 10,458) of 10 second length with a sampling frequency of 500 Hz.

The fourth source is a Georgia database which represents a unique demographic of the Southeastern United States. This training set contains 10,344 12-lead ECGs (male: 5,551, female: 4,793) of 10 second length with a sampling frequency of 500 Hz.

The fifth source is an undisclosed American database that is geographically distinct from the Georgia database. This source contains 10,000 ECGs (all retained as test data).

All data is provided in WFDB format. Each ECG recording has a binary MATLAB v4 file (see page 27) for the ECG signal data and a text file in WFDB header format describing the recording and patient attributes, including the diagnosis (the labels for the recording). The binary files can be read using the load function in MATLAB and the scipy.io.loadmat function in Python; please see our baseline models for examples of loading the data. The first line of the header provides information about the total number of leads and the total number of samples or points per lead. The following lines describe how each lead was saved, and the last lines provide information on demographics and diagnosis. Below is an example header file A0001.hea:

```
A0001 12 500 7500 05-Feb-2020 11:39:16
A0001.mat 16+24 1000/mV 16 0 28 -1716 0 I
A0001.mat 16+24 1000/mV 16 0 7 2029 0 II
A0001.mat 16+24 1000/mV 16 0 -21 3745 0 III
A0001.mat 16+24 1000/mV 16 0 -17 3680 0 aVR
A0001.mat 16+24 1000/mV 16 0 24 -2664 0 aVL
A0001.mat 16+24 1000/mV 16 0 -7 -1499 0 aVF
A0001.mat 16+24 1000/mV 16 0 -290 390 0 V1
A0001.mat 16+24 1000/mV 16 0 -204 157 0 V2
A0001.mat 16+24 1000/mV 16 0 -96 -2555 0 V3
A0001.mat 16+24 1000/mV 16 0 -112 49 0 V4
A0001.mat 16+24 1000/mV 16 0 -596 -321 0 V5
A0001.mat 16+24 1000/mV 16 0 -16 -3112 0 V6

Age: 74
Sex: Male
Dx: 426783006
Rx: Unknown
Hx: Unknown
Sx: Unknown
```

From the first line, we see that the recording number is A0001, and the recording file is A0001.mat. The recording has 12 leads, each recorded at 500 Hz sample frequency, and contains 7500 samples. From the next 12 lines, we see that each signal was written at 16 bits with an offset of 24 bits, the amplitude resolution is 1000 with units in mV, the resolution of the analog-to-digital converter (ADC) used to digitize the signal is 16 bits, and the baseline value corresponding to 0 physical units is 0. The first value of the signal, the checksum, and the lead name are included for each signal. From the final 6 lines, we see that the patient is a 74-year-old male with a diagnosis (Dx) of 426783006. The medical prescription (Rx), history (Hx), and symptom or surgery (Sx) are unknown.

Each ECG recording has one or more labels from different type of abnormalities in SNOMED-CT codes. The full list of diagnoses for the challenge has been posted here as a 3 column CSV file: Long-form description, corresponding SNOMED-CT code, abbreviation. Although these descriptions apply to all training data there may be fewer classes in the test data, and in different proportions. However, every class in the test data will be represented in the training data.",2018,,,516 records,,
2465,PhysioNet_Challenge_2021,ECG Classification,ECG Classification,"ECG Classification, Age Estimation",Image,,Computer Vision,"ecg-classification-on-physionet-challenge-1, age-estimation-on-physionet-challenge-2021",Creative Commons Attribution 4.0 International Public License,https://physionet.org/content/challenge-2021/1.0.2/,https://paperswithcode.com/dataset/physionet-challenge-2021,"Data Description
The training data contains twelve-lead ECGs. The validation and test data contains twelve-lead, six-lead, four-lead, three-lead, and two-lead ECGs:


Twelve leads: I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6
Six leads: I, II, III, aVR, aVL, aVF
Four leads: I, II, III, V2
Three leads: I, II, V2
Two leads: I, II

Each ECG recording has one or more labels that describe cardiac abnormalities (and/or a normal sinus rhythm). We mapped the labels for each recording to SNOMED-CT codes. The lists of scored labels and unscored labels are given with the evaluation code; see the scoring section for details.

Data Sources
The Challenge data include recordings from last year’s Challenge and many new recordings for this year’s Challenge:


CPSC Database and CPSC-Extra Database
INCART Database
PTB and PTB-XL Database
The Georgia 12-lead ECG Challenge (G12EC) Database
Augmented Undisclosed Database
Chapman-Shaoxing and Ningbo Database
The University of Michigan (UMich) Database

The Challenge data include annotated twelve-lead ECG recordings from six sources in four countries across three continents. These databases include over 100,000 twelve-lead ECG recordings with over 88,000 ECGs shared publicly as training data, 6,630 ECGs retained privately as validation data, and 36,266 ECGs retained privately as test data.



The first source is the China Physiological Signal Challenge in 2018 (CPSC 2018), which was held during the 7th International Conference on Biomedical Engineering and Biotechnology in Nanjing, China. This source contains two databases: the data from CPSC 2018 (the CPSC Database) and unused data from CPSC 2018 (the CPSC-Extra Database). Together, these databases contain 13,256 ECGs (10,330 ECGs shared as training data, 1,463 retained as validation data, and 1,463 retained as test data). We shared the training set and an unused dataset from CPSC 2018 as training data, and we split the test set from CPSC 2018 into validation and test sets. Each recording is between 6 and 144 seconds long with a sampling frequency of 500 Hz.



The second source is the St Petersburg INCART 12-lead Arrhythmia Database. This source contains 74 annotated ECGs (all shared as training data) extracted from 32 Holter monitor recordings. Each recording is 30 minutes long with a sampling frequency of 257 Hz.



The third source is the Physikalisch-Technische Bundesanstalt (PTB) and includes two public datasets: the PTB and the PTB-XL databases. The source contains 22,353 ECGs (all shared as training data). Each recording is between 10 and 120 seconds long with a sampling frequency of either 500 or 1,000 Hz.



The fourth source is a Georgia database which represents a unique demographic of the Southeastern United States. This source contains 20,672 ECGs (10,344 ECGs shared as training data, 5,167 retained as validation data, and 5,161 retained as test data). Each recording is between 5 and 10 seconds long with a sampling frequency of 500 Hz.



The fifth source is an undisclosed American database that is geographically distinct from the Georgia database. This source contains 10,000 ECGs (all retained as test data).



The sixth source is the Chapman University, Shaoxing People’s Hospital (Chapman-Shaoxing) and Ningbo First Hospital (Ningbo) database. This source contains 45,152 ECGS (all shared as training data). Each recording is 10 seconds long with a sampling frequency of 500 Hz.



The seventh source is UMich Database from the University of Michigan. This source contains 19,642 ECGs (all retained as test data). Each recording is 10 seconds long with a sampling frequency of either 250 Hz or 500 Hz.



Like other real-world datasets, different databases may have different proportions of cardiac abnormalities, but all of the labels in the validation or test data are represented in the training data. Moreover, while this is a curated dataset, some of the data and labels are likely to have errors, and an important part of the Challenge is to work out these issues. In particular, some of the databases have human-overread machine labels with single or multiple human readers, so the quality of the labels varies between databases. You can find more information about the label mappings of the Challenge training data in this table.

The six-lead, four-lead, three-lead, and two-lead validation data are reduced-lead versions of the twelve-lead validation data: the same recordings with the same header data but only with signal data for the relevant leads.

We are not planning to release the test data at any point, including after the end of the Challenge. Requests for the test data will not receive a response. We do not release test data to prevent overfitting on the test data and claims or publications of inflated performances. We will entertain requests to run code on the test data after the Challenge on a limited basis based on publication necessity and capacity. (The Challenge is largely staged by volunteers.)

Data Format
All data was formatted in WFDB format. Each ECG recording uses a binary MATLAB v4 file (see page 27) for the ECG signal data and a plain text file in WFDB header format for the recording and patient attributes, including the diagnosis, i.e., the labels for the recording. The binary files can be read using the load function in MATLAB and the scipy.io.loadmat function in Python; see our MATLAB and Python example code for working examples. The first line of the header provides information about the total number of leads and the total number of samples or time points per lead, the following lines describe how each lead was encoded, and the last lines provide information on the demographics and diagnosis of the patient.

For example, a header file A0001.hea may have the following contents:

```
A0001 12 500 7500 05-Feb-2020 11:39:16
A0001.mat 16+24 1000/mV 16 0 28 -1716 0 I
A0001.mat 16+24 1000/mV 16 0 7 2029 0 II
A0001.mat 16+24 1000/mV 16 0 -21 3745 0 III
A0001.mat 16+24 1000/mV 16 0 -17 3680 0 aVR
A0001.mat 16+24 1000/mV 16 0 24 -2664 0 aVL
A0001.mat 16+24 1000/mV 16 0 -7 -1499 0 aVF
A0001.mat 16+24 1000/mV 16 0 -290 390 0 V1
A0001.mat 16+24 1000/mV 16 0 -204 157 0 V2
A0001.mat 16+24 1000/mV 16 0 -96 -2555 0 V3
A0001.mat 16+24 1000/mV 16 0 -112 49 0 V4
A0001.mat 16+24 1000/mV 16 0 -596 -321 0 V5
A0001.mat 16+24 1000/mV 16 0 -16 -3112 0 V6

Age: 74
Sex: Male
Dx: 426783006
Rx: Unknown
Hx: Unknown
Sx: Unknown
```

From the first line of the file, we see that the recording number is A0001, and the recording file is A0001.mat. The recording has 12 leads, each recorded at a 500 Hz sampling frequency, and contains 7500 samples. From the next 12 lines of the file (one for each lead), we see that each signal was written at 16 bits with an offset of 24 bits, the floating point number (analog-to-digital converter (ADC) units per physical unit) is 1000/mV, the resolution of the analog-to-digital converter (ADC) used to digitize the signal is 16 bits, and the baseline value corresponding to 0 physical units is 0. The first value of the signal (-1716, etc.), the checksum (0, etc.), and the lead name (I, etc.) are the last three entries of each of these lines. From the final 6 lines, we see that the patient is a 74-year-old male with a diagnosis (Dx) of 426783006, which is the SNOMED-CT code for sinus rhythm. The medical prescription (Rx), history (Hx), and symptom or surgery (Sx) are unknown. Please visit WFDB header format for more information on the header file and variables.",2018,,,7500 samples,,
2466,PIAST,Text-to-Music Generation,Text-to-Music Generation,"Text-to-Music Generation, Music Recommendation, Music Classification, Music Information Retrieval, Piano Music Modeling, Music Emotion Recognition, Music Genre Recognition, Music Tagging, Music Auto-Tagging, Music Genre Transfer, Music Genre Classification","Audio, Image, Text",English,Computer Vision,,,https://hayeonbang.github.io/PIAST_dataset/,https://paperswithcode.com/dataset/piast,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
2467,Pick-a-Pic,Text-to-Image Generation,Text-to-Image Generation,Text-to-Image Generation,"Image, Text",English,Computer Vision,,,https://github.com/yuvalkirstain/PickScore,https://paperswithcode.com/dataset/pick-a-pic,"Pick-a-Pic dataset was created by logging user interactions with the Pick-a-Pic web application for text-to image generation. Overall, the Pick-a-Pic dataset contains over 500,000 examples and 35,000 distinct prompts. Each example contains a prompt, two generated images, and a label for which image is preferred, or if there is a tie when no image is significantly preferred over the other.",,Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation,https://arxiv.org/pdf/2305.01569v1.pdf,000 examples,,
2468,PIE-Bench,Text-based Image Editing,Text-based Image Editing,Text-based Image Editing,"Image, Text",English,Computer Vision,text-based-image-editing-on-pie-bench,Custom,https://github.com/cure-lab/DirectInversion,https://paperswithcode.com/dataset/pie-bench,"PIE-Bench comprises 700 images featuring 10 distinct editing types. Images are evenly distributed in natural and artificial scenes (e.g., paintings) among four categories: animal, human, indoor, and outdoor. Each image in PIE-Bench includes five annotations: source image prompt, target image prompt, editing instruction, main editing body, and the editing mask. Notably, the editing mask annotation (indicating the anticipated editing region) is crucial in accurate metrics computations as we expect the editing to only occur within a designated area.",,,,700 images,,
2469,PIE,Trajectory Forecasting,Trajectory Forecasting,"Trajectory Forecasting, Multi-future Trajectory Prediction, Trajectory Prediction, Pedestrian Trajectory Prediction",Time Series,,Methodology,"trajectory-prediction-on-pie, multi-future-trajectory-prediction-on-pie",MIT,https://data.nvision2.eecs.yorku.ca/PIE_dataset/,https://paperswithcode.com/dataset/pie,"PIE is a new dataset for studying pedestrian behavior in traffic. PIE contains over 6 hours of footage recorded in typical traffic scenes with on-board camera. It also provides accurate vehicle information from OBD sensor (vehicle speed, heading direction and GPS coordinates) synchronized with video footage.
Rich spatial and behavioral annotations are available for pedestrians and vehicles that potentially interact with the ego-vehicle as well as for the relevant elements of infrastructure (traffic lights, signs and zebra crossings).
There are over 300K labeled video frames with 1842 pedestrian samples making this the largest publicly available dataset for studying pedestrian behavior in traffic.",,,,,,
2470,PINO-darcy-pentagram,Physics-informed machine learning,Physics-informed machine learning,Physics-informed machine learning,,,Methodology,,,https://drive.google.com/drive/folders/10c5BWVvd-Oj13tMGhE07Tau07aTWfOhM,https://paperswithcode.com/dataset/pino-darcy-pentagram,"This dataset is well-structured for the physics-informed training of Neural operators for irregular domain geometry, which provides the FEM results of solving a darcy problem in a domain geometry shape of a pentagram. The Github of the paper that first use this dataset is: https://github.com/WeihengZ/PI-DCON.",,,,,,
2471,PIQ23,Image Quality Assessment,Image Quality Assessment,"Image Quality Assessment, Face Image Quality Assessment, Blind Image Quality Assessment",Image,,Computer Vision,,Custom,https://corp.dxomark.com/data-base-piq23/,https://paperswithcode.com/dataset/piq23,"Year after year, the demand for ever-better smartphone photos continues to grow, in particular in the domain of portrait photography. Manufacturers thus use perceptual quality criteria throughout the development of smartphone cameras. This costly procedure can be partially replaced by automated learning-based methods for image quality assessment (IQA). Due to its subjective nature, it is necessary to estimate and guarantee the consistency of the IQA process, a characteristic lacking in the mean opinion scores (MOS) widely used for crowdsourcing IQA. In addition, existing blind IQA (BIQA) datasets pay little attention to the difficulty of cross-content assessment, which may degrade the quality of annotations. This paper introduces PIQ23, a portrait-specific IQA dataset of 5116 images of 50 predefined scenarios acquired by 100 smartphones, covering a high variety of brands, models, and use cases. The dataset includes individuals of various genders and ethnicities who have given explicit and informed consent for their photographs to be used in public research. It is annotated by pairwise comparisons (PWC) collected from over 30 image quality experts for three image attributes: face detail preservation, face target exposure, and overall image quality. An in-depth statistical analysis of these annotations allows us to evaluate their consistency over PIQ23. Finally, we show through an extensive comparison with existing baselines that semantic information (image context) can be used to improve IQA predictions. The dataset along with the proposed statistical analysis and BIQA algorithms are available: https://github.com/DXOMARKResearch/PIQ2023",,,,5116 images,,
2472,PIRM,Image Restoration,Image Restoration,"Image Restoration, Image Super-Resolution",Image,,Computer Vision,image-super-resolution-on-pirm-test,CC BY-NC-SA 4.0,https://pirm.github.io,https://paperswithcode.com/dataset/pirm,"The PIRM dataset consists of 200 images, which are divided into two equal sets for validation and testing. These images cover diverse contents, including people, objects, environments, flora, natural scenery, etc. Images vary in size, and are typically ~300K pixels in resolution.",,,,200 images,"validation and testing. These images cover diverse contents, including people, objects, environments, flora, natural scenery, etc. Images",
2473,PISC,Relational Reasoning,Relational Reasoning,"Relational Reasoning, Visual Social Relationship Recognition, Graph Generation","Graph, Image, Text",English,Reasoning,visual-social-relationship-recognition-on,CC BY 4.0,https://zenodo.org/record/1059155,https://paperswithcode.com/dataset/pisc,"The People in Social Context (PISC) dataset is a dataset that focuses on social relationships. It consists of 22,670 images of 9 types of social relationships. It has annotations for the bounding boxes of all people, as well as the social relationship between all pairs of people in the images. In addition, it also contains occupation annotation.",,,,670 images,,
2474,PIT,Semantic Textual Similarity,Semantic Textual Similarity,"Semantic Textual Similarity, Paraphrase Identification, Sentence Embeddings",,,Methodology,paraphrase-identification-on-pit,,https://github.com/cocoxu/SemEval-PIT2015,https://paperswithcode.com/dataset/pit,"Paraphrase and Semantic Similarity in Twitter (PIT) presents a constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs.",,SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT),https://www.aclweb.org/anthology/S15-2001.pdf,,,
2475,PixelRec,Sequential Recommendation,Sequential Recommendation,"Sequential Recommendation, Recommendation Systems",,,Methodology,recommendation-systems-on-pixelrec,,https://github.com/westlake-repl/PixelRec,https://paperswithcode.com/dataset/pixelrec,an image cover dataset  in short video recommendation,,,,,,
2476,PixelShift200,Demosaicking,Demosaicking,"Demosaicking, Super-Resolution, Denoising",,,Methodology,,,https://guochengqian.github.io/project/pixelshift200/#overview,https://paperswithcode.com/dataset/pixelshift200,"Advanced pixel shift technology is employed to perform a full color sampling of the image. Pixel shift technology takes four samples of the same image at nearly the same time, and physically controls the camera sensor to move one pixel horizontally or vertically at each sampling to capture all color information at each pixel. The pixel shift technology ensures that the sampled images follow the distribution of natural images sampled by the camera, and the full information of the color (R, Gr, Gb, B channel) is completely obtained without any need of interpolation. In this way, the collected RGB images are artifacts-free, which leads to better training results for demosaicing related tasks.

PixelShift200 Dataset contains 210 high quality 4K images.


Training: 200 images
Testing: 10 images
Key Features: fully colored, demosiacing artifacts free
Camera: SONY α7R III",,,,4K images,Training: 200 images,
2477,pixraw10P,Image/Document Clustering,Image/Document Clustering,"Image/Document Clustering, Clustering Algorithms Evaluation","Image, Text",English,Computer Vision,"image-document-clustering-on-pixraw10p, clustering-algorithms-evaluation-on-pixraw10p",,https://jundongl.github.io/scikit-feature/,https://paperswithcode.com/dataset/pixraw10p,face image datasets,,,,,,
2478,PIZZA,Entity Resolution,Entity Resolution,Entity Resolution,,,Methodology,,Attribution-NonCommercial 4.0 International,https://github.com/amazon-science/pizza-semantic-parsing-dataset,https://paperswithcode.com/dataset/pizza,"PIZZA is a dataset for parsing pizza and drink orders, whose semantics cannot be captured by flat slots and intents.",,PIZZA: A new benchmark for complex end-to-end task-oriented parsing,https://arxiv.org/pdf/2212.00265v1.pdf,,,
2479,PJM_AEP_,Data Summarization,Data Summarization,"Data Summarization, Load Forecasting, Time Series Forecasting, Data Visualization","Text, Time Series",English,Natural Language Processing,,Custom,,https://paperswithcode.com/dataset/pjm-aep,"PJM Hourly Energy Consumption Data
PJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia.

The hourly power consumption data comes from PJM's website and are in megawatts (MW).

The regions have changed over the years so data may only appear for certain dates per region.",,,,,,
2480,PKU-MMD,Zero Shot Skeletal Action Recognition,Zero Shot Skeletal Action Recognition,"Zero Shot Skeletal Action Recognition, Skeleton Based Action Recognition, Generalized Zero Shot skeletal action recognition, Action Recognition In Videos, Unsupervised Skeleton Based Action Recognition","Image, Video",,Computer Vision,"action-recognition-in-videos-on-pku-mmd, generalized-zero-shot-skeletal-action-2, unsupervised-skeleton-based-action-2, skeleton-based-action-recognition-on-pku-mmd, zero-shot-skeletal-action-recognition-on-pku",,https://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html,https://paperswithcode.com/dataset/pku-mmd,"The PKU-MMD dataset is a large skeleton-based action detection dataset. It contains 1076 long untrimmed video sequences performed by 66 subjects in three camera views. 51 action categories are annotated, resulting almost 20,000 action instances and 5.4 million frames in total. Similar to NTU RGB+D, there are also two recommended evaluate protocols, i.e. cross-subject and cross-view.",,Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation,https://arxiv.org/abs/1804.06055,,,
2481,Placenta,Community Detection,Community Detection,"Community Detection, Graph Clustering, Node Classification","Graph, Image",,Computer Vision,node-classification-on-placenta,MIT,https://github.com/Nellaker-group/placenta,https://paperswithcode.com/dataset/placenta,"Placenta is a benchmark dataset for node classification in an underexplored domain: predicting microanatomical tissue structures from cell graphs in placenta histology whole slide images. Cell graphs are large (>1 million nodes per image), node features are varied (64-dimensions of 11 types of cells), class labels are imbalanced (9 classes ranging from 0.21% of the data to 40.0%), and cellular communities cluster into heterogeneously distributed tissues of widely varying sizes (from 11 nodes to 44,671 nodes for a single structure).",,A New Graph Node Classification Benchmark: Learning Structure from Histology Cell Graphs,https://arxiv.org/pdf/2211.06292v1.pdf,,,9
2482,Places-LT,Long-tail Learning,Long-tail Learning,Long-tail Learning,,,Methodology,long-tail-learning-on-places-lt,,https://liuziwei7.github.io/projects/LongTail.html,https://paperswithcode.com/dataset/places-lt,"Places-LT has an imbalanced training set with 62,500 images for 365 classes from Places-2. The class frequencies follow a natural power law distribution with a maximum number of 4,980 images per class and a minimum number of 5 images per class. The validation and testing sets are balanced and contain 20 and 100 images per class respectively.",,Long-Tailed Recognition Using Class-Balanced Experts,https://arxiv.org/abs/2004.03706,500 images,"training set with 62,500 images",365
2483,Places,Cross-Domain Few-Shot,Cross-Domain Few-Shot,"Cross-Domain Few-Shot, Uncropping, Image Inpainting",Image,,Computer Vision,"cross-domain-few-shot-on-places, image-inpainting-on-places2-val, uncropping-on-places2-val, image-inpainting-on-places2-1",CC BY,http://places.csail.mit.edu/,https://paperswithcode.com/dataset/places,"The Places dataset is proposed for scene recognition and contains more than 2.5 million images covering more than 205 scene categories with more than 5,000 images per category.",,Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey,https://arxiv.org/abs/1902.06162,000 images,,
2484,Places205,Cross-Domain Few-Shot,Cross-Domain Few-Shot,"Cross-Domain Few-Shot, Image Classification, Scene Recognition",Image,,Computer Vision,"cross-domain-few-shot-on-places, image-classification-on-places205",CC BY,http://places.csail.mit.edu/downloadData.html,https://paperswithcode.com/dataset/places205,"The Places205 dataset is a large-scale scene-centric dataset with 205 common scene categories. The training dataset contains around 2,500,000 images from these categories. In the training set, each scene category has the minimum 5,000 and maximum 15,000 images. The validation set contains 100 images per category (a total of 20,500 images), and the testing set includes 200 images per category (a total of 41,000 images).",,Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs,https://arxiv.org/abs/1610.01119,000 images,"training dataset contains around 2,500,000 images",
2485,Places365,Scene Classification,Scene Classification,"Scene Classification, Out-of-Distribution Detection, Semantic Segmentation, Image Classification, Image Outpainting, Image Inpainting, Scene Recognition",Image,,Computer Vision,"image-classification-on-places365-standard, scene-recognition-on-places365, image-classification-on-places365, out-of-distribution-detection-on-imagenet-1k-9, image-outpainting-on-places365-standard, image-inpainting-on-places365, out-of-distribution-detection-on-imagenet-1k-12, scene-classification-on-places365-standard",,http://places2.csail.mit.edu/,https://paperswithcode.com/dataset/places365,"The Places365 dataset is a scene recognition dataset. It is composed of 10 million images comprising 434 scene classes. There are two versions of the dataset: Places365-Standard with 1.8 million train and 36000 validation images from K=365 scene classes, and Places365-Challenge-2016, in which the size of the training set is increased up to 6.2 million extra images, including 69 new scene classes (leading to a total of 8 million train images from 434 scene classes).",2016,Semantic-Aware Scene Recognition,https://arxiv.org/abs/1909.02410,,train and 36000 validation images,
2486,Plancraft,Task Planning,Task Planning,"Task Planning, AI Agent",,,Methodology,,MIT,https://gautierdag.github.io/plancraft/,https://paperswithcode.com/dataset/plancraft,An evaluation dataset for planning with LLM agents,,,,,,
2487,PLAsTiCC,Time Series Classification,Time Series Classification,"Time Series Classification, Astronomy","Image, Time Series",,Time Series,,CC BY 4.0 License,https://users.flatironinstitute.org/~polymathic/data/MultimodalUniverse/v1/plasticc/,https://paperswithcode.com/dataset/plasticc,"The PLAsTiCC dataset is a collection of simulated light curves from the Photometric LSST Astronomical Time-Series Classification Challenge. This diverse dataset
contains 14 types of astronomical time-varying objects, simulated using the expected instrument characteristics and survey strategy of the upcoming Legacy Survey of Space and Time
[LSST 79] conducted at the Vera C. Rubin Observatory. 

The dataset includes two overall categories of time-series objects: transients, which are short-lived events such as supernovae, and variable sources, which are objects with fluctuating brightness such as pulsating stars. Specifically, the dataset includes the following transients: type Ia supernovae (SNIa), SNIax, SNIa-91bg, SNIbc, SNII, superluminous supernovae (SLSN), tidal disruption events (TDE), and single lens microlensing events (µLens-Single); and the following variable objects: active galactic nuclei (AGN), Mira variables, eclipsing binary systems (EB), and RR Lyrae (RRL).

The dataset contains a collection of 3,492,890 time-series, providing in each case several fields: time of observation, flux, flux error, and filter in which the flux is measured. Observations in 6 filters are included (u,g,r,i,z,Y). In addition, each time series is associated with a class representing the type of object, and a redshift for that object.",,,,,,
2488,PlasticineLab,Physical Simulations,Physical Simulations,Physical Simulations,,,Methodology,,,http://plasticinelab.csail.mit.edu,https://paperswithcode.com/dataset/plasticinelab,"PasticineLab is a differentiable physics benchmark, which includes a diverse collection of soft body manipulation tasks. In each task, the agent uses manipulators to deform the plasticine into the desired configuration. The underlying physics engine supports differentiable elastic and plastic deformation using the DiffTaichi system, posing many under-explored challenges to robotic agents.",,,,,,
2489,PLOS,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Lay Summarization",Text,English,Natural Language Processing,"abstractive-text-summarization-on-plos, lay-summarization-on-plos",,https://github.com/TGoldsack1/Corpora_for_Lay_Summarisation,https://paperswithcode.com/dataset/plos,"This dataset contains 27,525 full biomedical articles paired with non-technical lay summaries derived from various journals published by the Public Library of Science (PLOS).",,,,,,
2490,PlotQA,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Question Answering, Visual Question Answering, Chart Question Answering","Image, Text",English,Computer Vision,"visual-question-answering-on-plotqa-d2, chart-question-answering-on-plotqa, visual-question-answering-on-plotqa-d1, visual-question-answering-on-plotqa-d2-1, visual-question-answering-on-plotqa-d1-1",public,https://github.com/NiteshMethani/PlotQA,https://paperswithcode.com/dataset/plotqa,"PlotQA is a VQA dataset with 28.9 million question-answer pairs grounded over 224,377 plots on data from real-world sources and questions based on crowd-sourced question templates.
Existing synthetic datasets (FigureQA, DVQA) for reasoning over plots do not contain variability in data labels, real-valued data, or complex reasoning questions. Consequently, proposed models for these datasets do not fully address the challenge of reasoning over plots. In particular, they assume that the answer comes either from a small fixed size vocabulary or from a bounding box within the image. However, in practice this is an unrealistic assumption because many questions require reasoning and thus have real valued answers which appear neither in a small fixed size vocabulary nor in the image. In this work, we aim to bridge this gap between existing datasets and real world plots by introducing PlotQA. Further, 80.76% of the out-of-vocabulary (OOV) questions in PlotQA have answers that are not in a fixed vocabulary.",,public,https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html,,,
2491,PMD,Mirror Detection,Mirror Detection,"Mirror Detection, Image Segmentation",Image,,Computer Vision,image-segmentation-on-pmd,,,https://paperswithcode.com/dataset/pmd,"We propose a large-scale benchmark here, which contains a total of 6,461 mirror images with ground truth annotations.",,,,,,
2492,PMLB,AutoML,AutoML,"AutoML, Hyperparameter Optimization",,,Methodology,,,https://github.com/EpistasisLab/penn-ml-benchmarks,https://paperswithcode.com/dataset/pmlb,"The Penn Machine Learning Benchmarks (PMLB) is a large, curated set of benchmark datasets used to evaluate and compare supervised machine learning algorithms. These datasets cover a broad range of applications, and include binary/multi-class classification problems and regression problems, as well as combinations of categorical, ordinal, and continuous features.",,https://arxiv.org/abs/1703.00512,https://arxiv.org/abs/1703.00512,,,
2493,PNT,Timex normalization,Timex normalization,Timex normalization,,,Methodology,timex-normalization-on-pnt,,https://github.com/bethard/anafora-annotations/releases,https://paperswithcode.com/dataset/pnt,The Parsing Time Normalizations (PNT) corpus in SCATE format allows the representation of a wider variety of time expressions than previous approaches. This corpus was release with SemEval 2018 Task 6.,2018,,,,,
2494,PodcastFillers,Keyword Spotting,Keyword Spotting,"Keyword Spotting, Sound Event Localization and Detection","Audio, Image",,Computer Vision,sound-event-localization-and-detection-on-2,Creative Commons Non-Commercial (Any),https://podcastfillers.github.io/,https://paperswithcode.com/dataset/podcastfillers,"The PodcastFillers dataset consists of 199 full-length podcast episodes in English with manually annotated filler words and automatically generated transcripts. The podcast audio recordings, sourced from SoundCloud, are CC-licensed, gender-balanced, and total 145 hours of audio from over 350 speakers. The annotations are provided under a non-commercial license and consist of 85,803 manually annotated audio events including approximately 35,000 filler words (“uh” and “um”) and 50,000 non-filler events such as breaths, music, laughter, repeated words, and noise. The annotated events are also provided as pre-processed 1-second audio clips. The dataset also includes automatically generated speech transcripts from a speech-to-text system. A detailed description is provided in Dataset.",,,,,,
2495,POIE,Key Information Extraction,Key Information Extraction,Key Information Extraction,,,Methodology,,,https://github.com/jfkuang/cfam,https://paperswithcode.com/dataset/poie,"Products for OCR and Information Extraction (POIE) dataset derives from camera images of various products in the real world. The images are carefully selected and manually annotated. Our labeling team consists of 8 experienced labelers. We first crop the nutrition tables from product images and adopt multiple commercial OCR engines (Azure and Baidu OCR) for pre-labeling. Then we use LabelMe  to manually check the annotation of the location as well as transcription of every text box, and the values of entities for all the text in the images and repaired the OCR errors found. After discarding low-quality and blurred images, we obtain 3,000 images with 111,155 text instances.

from https://github.com/jfkuang/cfam",,,,000 images,"values of entities for all the text in the images and repaired the OCR errors found. After discarding low-quality and blurred images, we obtain 3,000 images",
2496,PointCloud-C,Point Cloud Classification,Point Cloud Classification,"Point Cloud Classification, Point Cloud Segmentation","3D, Image",,Computer Vision,"point-cloud-segmentation-on-pointcloud-c, point-cloud-classification-on-pointcloud-c",Creative Commons Attribution-NonCommercial-ShareAlike 3.0,https://pointcloud-c.github.io/home.html,https://paperswithcode.com/dataset/pointcloud-c,"PointCloud-C is the very first test-suite for point cloud robustness analysis under corruptions.


Two sets: ModelNet-C for point cloud classification and ShapeNet-C for part segmentation.
Real-world corruption sources, ranging from object-, senor-, and processing-levels.
Seven types of corruptions, each with five severity levels.
Benchmark with more than 20 point cloud recognition algorithms.
Methods ranging from architecture design, augmentations, and pre-training.",,,,,,
2497,PointDenoisingBenchmark,Abusive Language,Abusive Language,"Abusive Language, Adversarial Attack, Denoising",Text,English,Natural Language Processing,,,http://www.lix.polytechnique.fr/Labo/Marie-Julie.RAKOTOSAONA/pointcleannet.html,https://paperswithcode.com/dataset/pointcleannet,"The PointDenoisingBenchmark dataset features 28 different shapes, split into 18 training shapes and 10 test shapes.


PointDenoisingBenchmark for outliers removal: contains noisy point clouds with different levels of gaussian noise and the corresponding clean ground truths.
PointDenoisingBenchmark for denoising: contains noisy point clouds with different levels of noise and density of outliers and the corresponding clean ground truths.",,,,,,
2498,PointOdyssey,Point Tracking,Point Tracking,Point Tracking,"Image, Video",,Computer Vision,point-tracking-on-pointodyssey,,https://pointodyssey.com,https://paperswithcode.com/dataset/pointodyssey,"PointOdyssey is a large-scale synthetic dataset, and data generation framework, for the training and evaluation of long-term fine-grained tracking algorithms. The dataset currently includes 104 videos, averaging 2,000 frames long, with orders of magnitude more correspondence annotations than prior work.",,,,,,
2499,Poker_Hand_Histories,Game of Poker,Game of Poker,Game of Poker,,,Methodology,,MIT,https://github.com/uoftcprg/phh-dataset,https://paperswithcode.com/dataset/poker-hand-histories,"Poker Hand Histories
A collection of poker hand histories, covering 11 poker variants, in the poker hand history (PHH) format.

Contents:


All 83 televised hands played in the final table of the 2023 World Series of Poker Event #43: $50,000 Poker Players Championship | Day 5
All 10000 hands played by Pluribus and published in the supplementary of Brown and Sandholm (2019).
4 selections of historical poker hands.
1 badugi hand from the Wikipedia page on badugi.",2023,,,,,
2500,Polarized_Film_Removal_Dataset,Film Removal,Film Removal,Film Removal,,,Methodology,,,https://jqt.me/_FilmRemoval_/,https://paperswithcode.com/dataset/polarized-film-removal-dataset,"The current industrial pipeline includes 315 dynamic industrial scenarios, which can be categorized into three types: QR codes, text, and products. To enhance the diversity, we have different films with diverse material properties, coverage areas, film thicknesses, and levels of wrinkling. The film exhibits significant variability across each scenario. On the other hand, to ensure the stability of the industrial imaging pipeline, we maintained a consistent intensity level for the industrial light source and fixed the distance between the camera and the object flow. This helps to minimize the influence of errors external to the industrial system.",,,,,,
2501,PolarRR,Reflection Removal,Reflection Removal,"Reflection Removal, Image Enhancement",Image,,Computer Vision,,,https://github.com/ChenyangLEI/polarization-reflection-removal,https://paperswithcode.com/dataset/polarrr,PolarRR is a new dataset with more than 100 types of glass in which obtained transmission images are perfectly aligned with input mixed images.,,Polarized Reflection Removal with Perfect Alignment in the Wild,https://openaccess.thecvf.com/content_CVPR_2020/papers/Lei_Polarized_Reflection_Removal_With_Perfect_Alignment_in_the_Wild_CVPR_2020_paper.pdf,,,
2502,Polish_CDSCorpus,Semantic Composition,Semantic Composition,Semantic Composition,,,Methodology,,,http://zil.ipipan.waw.pl/Scwad/CDSCorpus,https://paperswithcode.com/dataset/polish-cdscorpus,Consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for the evaluation of compositional distributional semantics models of Polish.,,,,,,
2503,PolitiFact,Fact Verification,Fact Verification,"Fact Verification, Fake News Detection, Text Matching","Image, Text",English,Computer Vision,fake-news-detection-on-politifact,,https://github.com/nguyenvo09/EMNLP2020,https://paperswithcode.com/dataset/politifact,Fact-checking (FC) articles which contains pairs (multimodal tweet and a FC-article) from politifact.com.,,,,,,
2504,Polyglot-NER,Word Embeddings,Word Embeddings,"Word Embeddings, Morphological Analysis, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,,,https://sites.google.com/site/rmyeid/projects/polylgot-ner,https://paperswithcode.com/dataset/polyglot-ner,Polyglot-NER builds massive multilingual annotators with minimal human expertise and intervention.,,,,,,
2505,PolyMATH,Mathematical Reasoning,Mathematical Reasoning,"Mathematical Reasoning, Visual Commonsense Reasoning, Visual Reasoning, Arithmetic Reasoning, Logical Reasoning, Multimodal Reasoning",Image,,Reasoning,,Academic Free License v3.0,https://huggingface.co/datasets/him1411/polymath,https://paperswithcode.com/dataset/polymath,"PolyMATH, a challenging benchmark aimed at evaluating the general cognitive reasoning abilities of MLLMs. PolyMATH comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning

Paper 

Code

Project",,Paper,https://arxiv.org/abs/2410.14702,,"valuating the general cognitive reasoning abilities of MLLMs. PolyMATH comprises 5,000 manually collected high-quality images",
2506,PolyNewsParallel,NMT,NMT,"NMT, Text Retrieval, Machine Translation, News Retrieval",Text,English,Natural Language Processing,,CC-BY-NC 4.0,https://huggingface.co/datasets/aiana94/polynews-parallel,https://paperswithcode.com/dataset/polynewsparallel,"PolyNews is a multilingual parallel dataset containing news titles 833 language pairs, spanning in 64 languages and 17 scripts.

PolyNewsParallel aims to provide an easily-accessible, unified and de-duplicated dataset that combines three disparate data sources. It can be used for machine translation or text retrieval in both high-resource and low-resource languages.",,,,,,
2507,PolyU,Image Denoising,Image Denoising,"Image Denoising, Image Enhancement, Denoising",Image,,Computer Vision,image-denoising-on-polyu-dataset,,https://github.com/csjunxu/PolyU-Real-World-Noisy-Images-Dataset,https://paperswithcode.com/dataset/polyu-dataset,"PolyU Dataset is a large dataset of real-world noisy images with reasonably obtained corresponding “ground truth” images. The basic idea is to capture the same and unchanged scene for many (e.g., 500) times and compute their mean image, which can be roughly taken as the “ground truth” image for the real-world noisy images. The rational of this strategy is that for each pixel, the noise is generated randomly larger or smaller than 0. Sampling the same pixel many times and computing the average value will approximate the truth pixel value and alleviate significantly the noise.",,Real-world Noisy Image Denoising: A New Benchmark,https://arxiv.org/pdf/1804.02603.pdf,,,
2508,Polyvore,Slot Filling,Slot Filling,"Slot Filling, Retrieval, Recommendation Systems",,,Methodology,"slot-filling-on-polyvore, retrieval-on-polyvore, recommendation-systems-on-polyvore",,https://github.com/xthan/polyvore-dataset,https://paperswithcode.com/dataset/polyvore,"This dataset contains 21,889 outfits from polyvore.com, in which 17,316 are for training, 1,497 for validation and 3,076 for testing.",,https://arxiv.org/pdf/1707.05691.pdf,https://arxiv.org/pdf/1707.05691.pdf,,,
2509,POP909,Music Generation,Music Generation,"Music Generation, Style Transfer","Audio, Text",English,Natural Language Processing,,,https://github.com/music-x-lab/POP909-Dataset,https://paperswithcode.com/dataset/pop909,"POP909 is a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, annotations are provided of tempo, beat, key, and chords, where the tempo curves are hand-labelled and others are done by MIR algorithms.",,https://arxiv.org/pdf/2008.07142.pdf,https://arxiv.org/pdf/2008.07142.pdf,,,
2510,PoPArt,Pose Estimation,Pose Estimation,"Pose Estimation, Semi-Supervised Human Pose Estimation, 2D Object Detection, Multi-Person Pose Estimation, Object Detection, 2D Human Pose Estimation","3D, Image",,Computer Vision,semi-supervised-human-pose-estimation-on,Creative Commons Attribution 4.0 International,https://doi.org/10.5281/zenodo.7516229,https://paperswithcode.com/dataset/popart,"Throughout the history of art, the pose—as the holistic abstraction of the human body's expression—has proven to be a constant in numerous studies. However, due to the enormous amount of data that so far had to be processed by hand, its crucial role to the formulaic recapitulation of art-historical motifs since antiquity could only be highlighted selectively. This is true even for the now automated estimation of human poses, as domain-specific, sufficiently large data sets required for training computational models are either not publicly available or not indexed at a fine enough granularity. With the Poses of People in Art data set, we introduce the first openly licensed data set for estimating human poses in art and validating human pose estimators. It consists of 2,454 images from 22 art-historical depiction styles, including those that have increasingly turned away from lifelike representations of the body since the 19th century. A total of 10,749 human figures are precisely enclosed by rectangular bounding boxes, with a maximum of four per image labeled by up to 17 keypoints; among these are mainly joints such as elbows and knees. For machine learning purposes, the data set is divided into three subsets—training, validation, and testing—, that follow the established JSON-based Microsoft COCO format, respectively. Each image annotation, in addition to mandatory fields, provides metadata from the art-historical online encyclopedia WikiArt.",,,,454 images,"training computational models are either not publicly available or not indexed at a fine enough granularity. With the Poses of People in Art data set, we introduce the first openly licensed data set for estimating human poses in art and validating human pose estimators. It consists of 2,454 images",
2511,POPGym,Partially Observable Reinforcement Learning,Partially Observable Reinforcement Learning,"Partially Observable Reinforcement Learning, Temporal Sequences, Model-based Reinforcement Learning, Reinforcement Learning (RL), General Reinforcement Learning","Time Series, Video",,Reinforcement Learning,partially-observable-reinforcement-learning,MIT,https://github.com/proroklab/popgym,https://paperswithcode.com/dataset/popgym,"POPGym is designed to benchmark memory in deep reinforcement learning. It contains a set of environments and a collection of memory model baselines. The environments are all Partially Observable Markov Decision Process (POMDP) environments following the Openai Gym interface. Our environments follow a few basic tenets:


Painless Setup - popgym environments require only gym, numpy, and mazelib as dependencies
Laptop-Sized Tasks - Most tasks can be solved in less than a day on the CPU 
True Generalization - All environments are heavily randomized.

The paper uses 15M environment steps for each trial.",,,,,,
2512,PopQA,Knowledge Probing,Knowledge Probing,"Knowledge Probing, Memorization, Retrieval, Question Answering",Text,English,Natural Language Processing,question-answering-on-popqa,,https://github.com/alextmallen/adaptive-retrieval,https://paperswithcode.com/dataset/popqa,"PopQA is an open-domain QA dataset with 14k QA pairs with fine-grained Wikidata entity ID, Wikipedia page views, and relationship type information.",,https://paperswithcode.com/paper/when-not-to-trust-language-models,https://arxiv.org/pdf/2212.10511v1.pdf,,,
2513,Poser,GPR,GPR,"GPR, Pose Estimation, Gaussian Processes","3D, Image",,Computer Vision,,,http://groups.csail.mit.edu/vision/datasets/poser/,https://paperswithcode.com/dataset/poser,The Poser dataset is a dataset for pose estimation which consists of 1927 training and 418 test images. These images are synthetically generated and tuned to unimodal predictions. The images were generated using the Poser software package.,1927,Overlapping Cover Local Regression Machines,https://arxiv.org/abs/1701.01218,,training and 418 test images,
2514,PPG__Dalia,Time Series Regression,Time Series Regression,Time Series Regression,Time Series,,Time Series,,,https://archive.ics.uci.edu/ml/datasets/PPG-DaLiA#,https://paperswithcode.com/dataset/ppg-dalia-1,"PPG-DaLiA is a publicly available dataset for PPG-based heart rate estimation. This multimodal dataset features physiological and motion data, recorded from both a wrist- and a chest-worn device, of 15 subjects while performing a wide range of activities under close to real-life conditions. The included ECG data provides heart rate ground truth. The included PPG- and 3D-accelerometer data can be used for heart rate estimation, while compensating for motion artefacts.",,,,,,
2515,PPI,Link Prediction,Link Prediction,"Link Prediction, Node Classification","Image, Time Series",,Computer Vision,"node-classification-on-ppi, link-prediction-on-ppi",,http://snap.stanford.edu/graphsage/#datasets,https://paperswithcode.com/dataset/ppi,"protein roles—in terms of their cellular functions from
gene ontology—in various protein-protein interaction (PPI) graphs, with each graph corresponding
to a different human tissue [41]. positional gene sets are used, motif gene sets and immunological
signatures as features and gene ontology sets as labels (121 in total), collected from the Molecular
Signatures Database [34]. The average graph contains 2373 nodes, with an average degree of 28.8.",,,,,,
2516,PPMI,Diffeomorphic Medical Image Registration,Diffeomorphic Medical Image Registration,"Diffeomorphic Medical Image Registration, Reconstruction, Medical Image Registration","3D, Image",,Medical,"medical-image-registration-on-oasis-adibe, diffeomorphic-medical-image-registration-on, reconstruction-on-ppmi",Custom,https://www.ppmi-info.org/access-data-specimens/download-data/,https://paperswithcode.com/dataset/ppmi,"The Parkinson’s Progression Markers Initiative (PPMI) dataset originates from an observational clinical and longitudinal study comprising evaluations of people with Parkinson’s disease (PD), those people with high risk, and those who are healthy.",,Time-Guided High-Order Attention Model of Longitudinal Heterogeneous Healthcare Data,https://arxiv.org/abs/1912.00773,,,
2517,PRCC,Unsupervised Person Re-Identification,Unsupervised Person Re-Identification,"Unsupervised Person Re-Identification, Person Re-Identification",Image,,Computer Vision,"unsupervised-person-re-identification-on-prcc, person-re-identification-on-prcc",,https://www.isee-ai.cn/~yangqize/clothing.html,https://paperswithcode.com/dataset/prcc,"This dataset consists of 33698 images from 221 identities. Each person in Cameras A and B is wearing the same clothes, but the images are captured in different rooms. For Camera C, the person wears different clothes, and the images are captured in a different day.",,,,33698 images,,
2518,PreCo,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Entity Typing, Natural Language Understanding",Text,English,Natural Language Processing,coreference-resolution-on-preco,Custom (research-only),https://preschool-lab.github.io/PreCo/,https://paperswithcode.com/dataset/preco,"A large-scale English dataset for coreference resolution. The dataset is designed to embody the core challenges in coreference, such as entity representation, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering.",,,,,,
2519,Press_Briefing_Claim_Dataset,Claim Extraction with Stance Classification (CESC),Claim Extraction with Stance Classification (CESC),"Claim Extraction with Stance Classification (CESC), Sentence Classification",Image,,Computer Vision,,Creative Commons Attribution 4.0 International,https://github.com/jueri/press_briefing_claim_dataset,https://paperswithcode.com/dataset/press-briefing-claim-dataset,"Press Briefing Claim Dataset
The dataset contains a total of 53 press briefings from a time span of over four years (2017-2021). While, on average, one press briefing per month is held, the distribution is highly skewed towards recent years.

The Press briefings can be categorized into five main thematic categories: Climate, Energy and Mobility, Medicine, Technology and Science. The press briefings are unevenly distributed among the five categories with a major focus on medical press briefings, which also reflects the COVID-19 pandemic. 

In total, 177 speakers, excluding the hosts, were detected in the dataset. Most of the time the speakers are invited experts. Excluding the top 10 percentile press briefings as outliers, a press briefing had four guests on average.

The press briefings consist of 3066 passages, 1719 from guests and journalists and 1294 from hosts. Regarding the sentences, 25040 sentences were collected in total, 5955 from hosts and 18942 from guests.",2017,,,25040 sentences,,
2520,PrideMM,Hateful Meme Classification,Hateful Meme Classification,Hateful Meme Classification,Image,,Computer Vision,hateful-meme-classification-on-pridemm,,https://github.com/SiddhantBikram/MemeCLIP,https://paperswithcode.com/dataset/pridemm,"PrideMM comprising 5,063 text-embedded images associated with the LGBTQ+ Pride movement",,,,,,
2521,PRM800K,Reinforcement Learning (RL),Reinforcement Learning (RL),Reinforcement Learning (RL),,,Reinforcement Learning,,MIT,https://github.com/openai/prm800k,https://paperswithcode.com/dataset/prm800k,"PRM800K is a process supervision dataset containing 800,000 step-level correctness labels for model-generated solutions to problems from the MATH dataset.",,,,,,
2522,PRO-teXt,3D Semantic Scene Completion,3D Semantic Scene Completion,"3D Semantic Scene Completion, Indoor Scene Synthesis",3D,,Methodology,"indoor-scene-synthesis-on-pro-text, 3d-semantic-scene-completion-on-pro-text",CC BY-NC-SA 4.0,https://lang-scene-synth.github.io/,https://paperswithcode.com/dataset/pro-text,PRO-teXt is an extension of PROXD with the inclusion of text prompts to synthesize objects. There are 180/20 interactions for training/testing in PRO-teXt. Each interaction involves a linguistic command corresponding to an existing room arrangement.,,,,,,
2523,PROBA-V,Multi-Frame Super-Resolution,Multi-Frame Super-Resolution,"Multi-Frame Super-Resolution, Image Super-Resolution, satellite image super-resolution, Super-Resolution",Image,,Computer Vision,multi-frame-super-resolution-on-proba-v,Creative Commons Attribution 4.0 International,https://kelvins.esa.int/proba-v-super-resolution/,https://paperswithcode.com/dataset/proba-v,"The PROBA-V Super-Resolution dataset is the official dataset of ESA's Kelvins competition for ""PROBA-V Super Resolution"". It contains satellite data from 74 hand-selected regions around the globe at different points in time. The data is composed of radiometrically and geometrically corrected Top-Of-Atmosphere (TOA) reflectances for the RED and NIR spectral bands at 300m and 100m resolution in Plate Carrée projection. The 300m resolution data is delivered as 128x128 grey-scale pixel images, the 100m resolution data as 384x384 grey-scale pixel images. Additionally, a quality map is provided for each pixel, indicating whether the pixels are concealed (i.e. by clouads, ice, water, missing information, etc.).

The goal of the challenge can be described as Multi-Image Super-resolution: Construct a single high-resolution image out of a series of more frequent low resolution images.

Detailed information about the related competition can be found at https://kelvins.esa.int/proba-v-super-resolution.",,,,,,
2524,probability_words_nli,Formal Logic,Formal Logic,"Formal Logic, Natural Language Inference",Text,English,Natural Language Processing,,Apache-2.0,https://huggingface.co/datasets/sileod/probability_words_nli,https://paperswithcode.com/dataset/probability-words-nli-1,"This dataset tests the capabilities of language models to correctly capture the meaning of words denoting probabilities (WEP, also called verbal probabilities), e.g. words like ""probably"", ""maybe"", ""surely"", ""impossible"".",,,,,,
2525,ProcGen,Reinforcement Learning (RL),Reinforcement Learning (RL),"Reinforcement Learning (RL), General Reinforcement Learning",,,Reinforcement Learning,reinforcement-learning-on-procgen,,https://openai.com/blog/procgen-benchmark/,https://paperswithcode.com/dataset/procgen,Procgen Benchmark includes 16 simple-to-use procedurally-generated environments which provide a direct measure of how quickly a reinforcement learning agent learns generalizable skills.,,,,,,
2526,Product_Reviews_2017,Semantic Role Labeling (predicted predicates),Semantic Role Labeling (predicted predicates),"Semantic Role Labeling (predicted predicates), Semantic Role Labeling, Entity Retrieval, Predicate Detection, Relation Extraction","Graph, Image",,Computer Vision,predicate-detection-on-product-reviews-2017,Creative Commons Attribution 4.0 International,https://zenodo.org/records/1415481#.W5pjkBwScnQ,https://paperswithcode.com/dataset/productreviews2017,"The corpus contains review sentences mostly of products in electronics domain, annotated and segregated into 4 comparison categories. Each comparison sentence is annotated with names of the products (PROD1 and PROD2), the aspect (ASP) and the predicate (PRED). Dataset contains sentences after auto-labeling on SNAP dataset and manually labeled sentences from the following corpora:



Jindal and Liu, 2006



Kessler and Kuhn, 2014



JDPA Corpus (Kessler et al, 2010)",2006,,,,,
2527,PROMISE12,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Data Augmentation, Medical Image Segmentation, Volumetric Medical Image Segmentation",Image,,Computer Vision,"medical-image-segmentation-on-promise12, volumetric-medical-image-segmentation-on",Custom,https://promise12.grand-challenge.org/,https://paperswithcode.com/dataset/promise12,The PROMISE12 dataset was made available for the MICCAI 2012 prostate segmentation challenge. Magnetic Resonance (MR) images (T2-weighted) of 50 patients with various diseases were acquired at different locations with several MRI vendors and scanning protocols.,2012,Constrained Deep Networks: Lagrangian Optimization via Log-Barrier Extensions,https://arxiv.org/abs/1904.04205,,,
2528,PromptSpeech,Speech Synthesis,Speech Synthesis,Speech Synthesis,Audio,,Speech,,,https://speechresearch.github.io/prompttts/,https://paperswithcode.com/dataset/promptspeech,"PromptSpeech is a dataset that consists of speech and the corresponding prompts. We synthesize speech with 5 different style factors (gender, pitch, speaking speed, volume, and emotion) from a commercial TTS API. The emotion factor has 5 categories and the gender factor has 2 categories.",,PromptTTS: Controllable Text-to-Speech with Text Descriptions,https://arxiv.org/pdf/2211.12171v1.pdf,,,5
2529,PRONOSTIA_Bearing_Dataset,Remaining Useful Lifetime Estimation,Remaining Useful Lifetime Estimation,"Remaining Useful Lifetime Estimation, Fault Detection",Image,,Computer Vision,,,https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#femto,https://paperswithcode.com/dataset/pronostia-bearing-dataset,"The PRONOSTIA (also called FEMTO) bearing dataset consists of 17 accelerated run-to-failures on a small bearing test rig. Both acceleration and temperature data was collected for each experiment.

The dataset was used in the 2012 IEEE Prognostic Challenge. The dataset is from FEMTO-ST Institute in France.",2012,,,,,
2530,PRONTO,Unsupervised Anomaly Detection,Unsupervised Anomaly Detection,"Unsupervised Anomaly Detection, Fault Detection",Image,,Computer Vision,unsupervised-anomaly-detection-on-pronto,,https://zenodo.org/records/1341583,https://paperswithcode.com/dataset/pronto,"The PRONTO heterogeneous benchmark dataset is based on an industrial-scale multiphase flow facility. It includes data from heterogeneous sources, including process measurements, alarm records, high frequency ultrasonic flow and pressure measurements, an operation log and video recordings. The study collected data from various operational conditions with and without induced faults to generate a multi-rate, multi-modal dataset. The dataset is suitable for developing and validating algorithms for fault detection and diagnosis (FDD) and data fusion.",,,,,,
2531,PrOntoQA,Question Answering,Question Answering,"Question Answering, Mathematical Reasoning",Text,English,Natural Language Processing,,,https://github.com/asaparov/prontoqa,https://paperswithcode.com/dataset/prontoqa,PrOntoQA is a question-answering dataset which generates examples with chains-of-thought that describe the reasoning required to answer the questions correctly. The sentences in the examples are syntactically simple and amenable to semantic parsing. It can be used to formally analyze the predicted chain-of-thought from large language models such as GPT-3.,,https://arxiv.org/pdf/2210.01240v1.pdf,https://arxiv.org/pdf/2210.01240v1.pdf,,,
2532,ProofNet,Retrieval,Retrieval,"Retrieval, Automated Theorem Proving",,,Methodology,,MIT,https://github.com/zhangir-azerbayev/ProofNet,https://paperswithcode.com/dataset/proofnet,"ProofNet is a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and natural language proof. The problems are primarily drawn from popular undergraduate pure mathematics textbooks and cover topics such as real and complex analysis, linear algebra, abstract algebra, and topology.",,ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics,https://arxiv.org/pdf/2302.12433v1.pdf,371 examples,,
2533,ProSLU,Spoken Language Understanding,Spoken Language Understanding,"Spoken Language Understanding, Slot Filling, Intent Detection, Semantic Frame Parsing","Image, Text",English,Computer Vision,"slot-filling-on-proslu, intent-detection-on-proslu, semantic-frame-parsing-on-proslu",GPL-2.0 License,https://github.com/LooperXX/ProSLU,https://paperswithcode.com/dataset/proslu,"In the paper, to bridge the research gap, we propose a new and important task, Profile-based Spoken Language Understanding (ProSLU), which requires a model not only depends on the text but also on the given supporting profile information.
We further introduce a Chinese human-annotated dataset, with over 5K utterances annotated with intent and slots, and corresponding supporting profile information. 
In total, we provide three types of supporting profile information: 
(1) Knowledge Graph (KG) consists of entities with rich attributes, 
(2) User Profile (UP) is composed of user settings and information,
(3) Context Awareness(CA) is user state and environmental information.",,,,,,
2534,ProsocialDialog,Dialogue Understanding,Dialogue Understanding,"Dialogue Understanding, Dialogue Generation, Dialogue Safety Prediction, Response Generation, Open-Domain Dialog","Text, Time Series",English,Natural Language Processing,dialogue-safety-prediction-on-prosocialdialog,CC-BY-4.0,https://github.com/skywalker023/prosocial-dialog,https://paperswithcode.com/dataset/prosocialdialog,"Most existing dialogue systems fail to respond properly to potentially unsafe user utterances by either ignoring or passively agreeing with them. 

To address this issue, we introduce ProsocialDialog, the first large-scale multi-turn dialogue dataset to teach conversational agents to respond to problematic content following social norms. Covering diverse unethical, problematic, biased, and toxic situations, ProsocialDialog contains responses that encourage prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb, RoTs). 

ProsocialDialog consists of 58K dialogues between a speaker showing potentially unsafe behavior and a speaker giving constructive feedback for more socially acceptable behavior. Specifically, it contains a rich suite of:


331K utterances
160K Rules-of-thumb (RoTs)
497K dialogue safety labels accompanied by free-form rationales",,,,,,
2535,PROTEINS,Graph Classification,Graph Classification,Graph Classification,"Graph, Image",,Computer Vision,graph-classification-on-proteins,Various,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,https://paperswithcode.com/dataset/proteins,PROTEINS is a dataset of proteins that are classified as enzymes or non-enzymes. Nodes represent the amino acids and two nodes are connected by an edge if they are less than 6 Angstroms apart.,,Fast and Deep Graph Neural Networks,https://arxiv.org/abs/1911.08941,,,
2536,Protein_structures_Ingraham,Protein Design,Protein Design,Protein Design,,,Methodology,,MIT,https://github.com/jingraham/neurips19-graph-protein-design,https://paperswithcode.com/dataset/protein-structures-ingraham,A data set introduced for training on the protein design task.,,,,,,
2537,PRW,Person Recognition,Person Recognition,"Person Recognition, Person Search, Person Re-Identification, Pedestrian Detection",Image,,Computer Vision,person-search-on-prw,,http://zheng-lab.cecs.anu.edu.au/Project/project_prw.html,https://paperswithcode.com/dataset/prw,"PRW is a large-scale dataset for end-to-end pedestrian detection and person recognition in raw video frames. PRW is introduced to evaluate Person Re-identification in the Wild, using videos acquired through six synchronized cameras. It contains 932 identities and 11,816 frames in which pedestrians are annotated with their bounding box positions and identities.",,,,,,
2538,PS-Battles,Object Detection,Object Detection,"Object Detection, Image Classification, Content-Based Image Retrieval",Image,,Computer Vision,,,https://github.com/dbisUnibas/ps-battles,https://paperswithcode.com/dataset/ps-battles,"The PS-Battles dataset is gathered from a large community of image manipulation enthusiasts and provides a basis for media derivation and manipulation detection in the visual domain. The dataset consists of 102'028 images grouped into 11'142 subsets, each containing the original image as well as a varying number of manipulated derivatives.",,Heller et al,https://arxiv.org/pdf/1804.04866v1.pdf,028 images,,
2539,PS4,Protein Annotation,Protein Annotation,"Protein Annotation, Protein Secondary Structure Prediction, Protein Folding",Time Series,,Methodology,protein-secondary-structure-prediction-on-ps4,CC0-1.0 license,https://github.com/omarperacha/ps4-dataset,https://paperswithcode.com/dataset/ps4,"A dataset of 18,731 proteins with their PDB code, index of the first residue in their respective DSSP file, their residue sequence and 9-category secondary structure sequence (including polyproline helices).",,,,,,
2540,PSB2,Program Synthesis,Program Synthesis,Program Synthesis,,,Methodology,,,https://arxiv.org/abs/2106.06086,https://paperswithcode.com/dataset/psb2,https://arxiv.org/abs/2106.06086,,Homepage,https://arxiv.org/abs/2106.06086,,,
2541,PSG_Dataset,Panoptic Scene Graph Generation,Panoptic Scene Graph Generation,"Panoptic Scene Graph Generation, Scene Graph Generation","Graph, Image, Text",English,Computer Vision,panoptic-scene-graph-generation-on-psg,Creative Commons Attribution 4.0 License,https://psgdataset.org/,https://paperswithcode.com/dataset/psg-dataset,PSG dataset has 48749 images with 133 object classes (80 objects and 53 stuff) and 56 predicate classes. It annotates inter-segment relations based on COCO panoptic segmentation.,,,,48749 images,,
2542,PSM,Text Matching,Text Matching,Text Matching,Text,English,Natural Language Processing,,,,https://paperswithcode.com/dataset/psm,PSM is a financial-domain dataset of the pairwise search matching task. It aims to identify the semantic similarity of a sentence pair in the search scenario.,,,,,,
2543,PST900,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Stereo Matching, Motion Estimation, Thermal Image Segmentation","3D, Image, Video",,Computer Vision,thermal-image-segmentation-on-pst900,,https://github.com/ShreyasSkandanS/pst900_thermal_rgb,https://paperswithcode.com/dataset/pst900,PST900 is a dataset of 894 synchronized and calibrated RGB and Thermal image pairs with per pixel human annotations across four distinct classes from the DARPA Subterranean Challenge.,,https://arxiv.org/abs/1909.10980,https://arxiv.org/abs/1909.10980,,,
2544,PTB-XL,Super-diagnostic,Super-diagnostic,"Super-diagnostic, All, ECG Classification, Diagnostic, Sub-diagnostic, Rhythm, Form, ECG Patient Identification (gallery-probe), ECG Patient Identification, Superclass classification",Image,,Computer Vision,"diagnostic-on-ptb-xl, all-on-ptb-xl, ecg-classification-on-ptb-xl, superclass-classification-on-ptb-xl, rhythm-on-ptb-xl, form-on-ptb-xl, sub-diagnostic-on-ptb-xl, ecg-patient-identification-gallery-probe-on-1, super-diagnostic-on-ptb-xl",Creative Commons Attribution 4.0 International,https://physionet.org/content/ptb-xl/1.0.3/,https://paperswithcode.com/dataset/ptb-xl,"Electrocardiography (ECG) is a key diagnostic tool to assess the cardiac condition of a patient. Automatic ECG interpretation algorithms as diagnosis support systems promise large reliefs for the medical personnel - only on the basis of the number of ECGs that are routinely taken. However, the development of such algorithms requires large training datasets and clear benchmark procedures. In our opinion, both aspects are not covered satisfactorily by existing freely accessible ECG datasets.

The PTB-XL ECG dataset is a large dataset of 21799 clinical 12-lead ECGs from 18869 patients of 10 second length. The raw waveform data was annotated by up to two cardiologists, who assigned potentially multiple ECG statements to each record. The in total 71 different ECG statements conform to the SCP-ECG standard and cover diagnostic, form, and rhythm statements. To ensure comparability of machine learning algorithms trained on the dataset, we provide recommended splits into training and test sets. In combination with the extensive annotation, this turns the dataset into a rich resource for the training and the evaluation of automatic ECG interpretation algorithms. The dataset is complemented by extensive metadata on demographics, infarction characteristics, likelihoods for diagnostic ECG statements as well as annotated signal properties.",,,,,,
2545,PTB_Diagnostic_ECG_Database,Language Modelling,Language Modelling,"Language Modelling, ECG Classification, Myocardial infarction detection, ECG Patient Identification (gallery-probe), ECG Patient Identification, Constituency Grammar Induction","Image, Text",English,Computer Vision,"language-modelling-on-ptb, constituency-grammar-induction-on-ptb, ecg-patient-identification-gallery-probe-on-2, myocardial-infarction-detection-on-ptb-1",Open Data Commons Attribution License v1.0,https://physionet.org/content/ptbdb/1.0.0/,https://paperswithcode.com/dataset/ptb,"The ECGs in this collection were obtained using a non-commercial, PTB prototype recorder with the following specifications:

16 input channels, (14 for ECGs, 1 for respiration, 1 for line voltage)
Input voltage: ±16 mV, compensated offset voltage up to ± 300 mV
Input resistance: 100 Ω (DC)
Resolution: 16 bit with 0.5 μV/LSB (2000 A/D units per mV)
Bandwidth: 0 - 1 kHz (synchronous sampling of all channels)
Noise voltage: max. 10 μV (pp), respectively 3 μV (RMS) with input short circuit
Online recording of skin resistance
Noise level recording during signal collection
The database contains 549 records from 290 subjects (aged 17 to 87, mean 57.2; 209 men, mean age 55.5, and 81 women, mean age 61.6; ages were not recorded for 1 female and 14 male subjects). Each subject is represented by one to five records. There are no subjects numbered 124, 132, 134, or 161. Each record includes 15 simultaneously measured signals: the conventional 12 leads (i, ii, iii, avr, avl, avf, v1, v2, v3, v4, v5, v6) together with the 3 Frank lead ECGs (vx, vy, vz). Each signal is digitized at 1000 samples per second, with 16 bit resolution over a range of ± 16.384 mV. On special request to the contributors of the database, recordings may be available at sampling rates up to 10 KHz.

Within the header (.hea) file of most of these ECG records is a detailed clinical summary, including age, gender, diagnosis, and where applicable, data on medical history, medication and interventions, coronary artery pathology, ventriculography, echocardiography, and hemodynamics. The clinical summary is not available for 22 subjects.",2000,,,549 records,,
2546,PubLayNet,Object Detection,Object Detection,"Object Detection, Information Retrieval, Document Layout Analysis","Image, Text",English,Computer Vision,document-layout-analysis-on-publaynet-val,,https://github.com/ibm-aur-nlp/PubLayNet,https://paperswithcode.com/dataset/publaynet,"PubLayNet is a dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated.",,,,,,
2547,public_meetings,Text Summarization,Text Summarization,"Text Summarization, Meeting Summarization",Text,English,Natural Language Processing,,,https://github.com/pltrdy/public_meetings,https://paperswithcode.com/dataset/public-meetings,"The public_meetings corpus contains meetings, made of pairs of automatic transcriptions from audio recordings and meeting reports written by a professional. 22 aligned meetings are provided in total.",,,,,,
2548,Pubmed,Text Summarization,Text Summarization,"Text Summarization, Language Modelling, Graph Classification, Unsupervised Extractive Summarization, Node Clustering, Node Classification on Non-Homophilic (Heterophilic) Graphs, Graph Clustering, Link Prediction, Community Detection, Node Classification, Extended Summarization, Sentence Classification","Graph, Image, Text, Time Series",English,Computer Vision,"sentence-classification-on-pubmed-20k-rct, node-classification-on-pubmed-005, community-detection-on-pubmed, node-classification-on-pubmed-01, link-prediction-on-pubmed-nonstandard-variant, node-classification-on-pubmed-random, node-classification-on-pubmed-fixed-20-node, node-classification-on-non-homophilic-16, language-modelling-on-pubmed-central, link-prediction-on-pubmed, node-classification-on-pubmed-with-public, link-prediction-on-pubmed-biased-evaluation, node-classification-on-pubmed, node-classification-on-pubmed-48-32-20-fixed, extended-summarization-on-pubmed-long-val, graph-clustering-on-pubmed, node-clustering-on-pubmed, graph-classification-on-pubmed, node-classification-on-pubmed-full-supervised, extended-summarization-on-pubmed-long-test, node-classification-on-pubmed-003, text-summarization-on-pubmed-1, unsupervised-extractive-summarization-on-1, node-classification-on-pubmed-60-20-20-random",,https://linqs.org/datasets/#pubmed-diabetes,https://paperswithcode.com/dataset/pubmed,The PubMed dataset consists of 19717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes. The citation network consists of 44338 links. Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words.,,,,,,
2549,PubMedQA,Language Modelling,Language Modelling,"Language Modelling, Named Entity Recognition (NER), Question Answering, Few-Shot Learning","Image, Text",English,Computer Vision,"question-answering-on-pubmedqa, few-shot-learning-on-pubmedqa",Custom,https://pubmedqa.github.io/,https://paperswithcode.com/dataset/pubmedqa,"The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts.

PubMedQA has 1k expert labeled, 61.2k unlabeled and 211.3k artificially generated QA instances.",,https://arxiv.org/pdf/1909.06146v1.pdf,https://arxiv.org/pdf/1909.06146v1.pdf,,,
2550,PubMed_RCT,Sentence Classification,Sentence Classification,"Sentence Classification, Text Classification","Image, Text",English,Computer Vision,,,https://github.com/Franck-Dernoncourt/pubmed-rct,https://paperswithcode.com/dataset/pubmed-rct,"PubMed 200k RCT is new dataset based on PubMed for sequential sentence classification. The dataset consists of approximately 200,000 abstracts of randomized controlled trials, totaling 2.3 million sentences. Each sentence of each abstract is labeled with their role in the abstract using one of the following classes: background, objective, method, result, or conclusion. The purpose of releasing this dataset is twofold. First, the majority of datasets for sequential short-text classification (i.e., classification of short texts that appear in sequences) are small: the authors hope that releasing a new large dataset will help develop more accurate algorithms for this task. Second, from an application perspective, researchers need better tools to efficiently skim through the literature. Automatically classifying each sentence in an abstract would help researchers read abstracts more efficiently, especially in fields where abstracts may be long, such as the medical field.",,https://arxiv.org/pdf/1710.06071.pdf,https://arxiv.org/pdf/1710.06071.pdf,,,
2551,PubTables-1M,Table Recognition,Table Recognition,"Table Recognition, Table Detection, Table Functional Analysis","Image, Tabular",,Computer Vision,,"Community Data License Agreement – Permissive, Version 1.0",https://msropendata.com/datasets/505fcbe3-1383-42b1-913a-f651b8b712d3,https://paperswithcode.com/dataset/pubtables-1m,"The goal of PubTables-1M is to create a large, detailed, high-quality dataset for training and evaluating a wide variety of models for the tasks of table detection, table structure recognition, and functional analysis. It contains:


460,589 annotated document pages containing tables for table detection.
947,642 fully annotated tables including text content and complete location (bounding box) information for table structure recognition and functional analysis.
Full bounding boxes in both image and PDF coordinates for all table rows, columns, and cells (including blank cells), as well as other annotated structures such as column headers and projected row headers.
Rendered images of all tables and pages.
Bounding boxes and text for all words appearing in each table and page image.
Additional cell properties not used in the current model training.

Additionally, cells in the headers are canonicalized and we implement multiple quality control steps to ensure the annotations are as free of noise as possible. For more details, please see our paper.",,our paper,https://arxiv.org/pdf/2110.00061.pdf,,,
2552,pursuitMW,Multi-agent Reinforcement Learning,Multi-agent Reinforcement Learning,"Multi-agent Reinforcement Learning, Multi-Agent Path Finding, Decision Making",,,Reinforcement Learning,,MIT,https://github.com/LijunSun90/pursuitMatrixWorld/tree/main,https://paperswithcode.com/dataset/pursuitmw,"Multi-agent pursuit in matrix world (pursuitMW) is a partially observable Markov game (POMG) between a swarm of pursuers and a swarm of evaders. Algorithms can be developed for the pursuers, evaders, or both of them.",,,,,,
2553,PushWorld,Motion Planning,Motion Planning,"Motion Planning, Reinforcement Learning (RL)",Video,,Methodology,,,https://deepmind-pushworld.github.io/play/,https://paperswithcode.com/dataset/pushworld,PushWorld is an environment with simplistic physics that requires manipulation planning with both movable obstacles and tools. It contains more than 200 PushWorld puzzles in PDDL and in an OpenAI Gym environment.,,PushWorld: A benchmark for manipulation planning with tools and movable obstacles,https://arxiv.org/pdf/2301.10289v1.pdf,,,
2554,PyBullet,Continuous Control,Continuous Control,Continuous Control,,,Methodology,"continuous-control-on-pybullet-ant, continuous-control-on-pybullet-hopper, continuous-control-on-pybullet-walker2d, continuous-control-on-pybullet-halfcheetah",,https://pybullet.org/wordpress,https://paperswithcode.com/dataset/pybullet,"PyBullet is an easy to use Python module for physics simulation, robotics and deep reinforcement learning based on the Bullet Physics SDK. With PyBullet you can load articulated bodies from URDF, SDF and other file formats. PyBullet provides forward dynamics simulation, inverse dynamics computation, forward and inverse kinematics and collision detection and ray intersection queries. Aside from physics simulation, PyBullet supports to rendering, with a CPU renderer and OpenGL visualization and support for virtual reality headsets.",,,,,,
2555,Pylon_Benchmark,Table Search,Table Search,Table Search,Tabular,,Methodology,,CC BY,https://github.com/superctj/pylon,https://paperswithcode.com/dataset/pylon-benchmark,"We create a new dataset from GitTables, a data lake of 1.7M tables extracted from CSV files on GitHub. The benchmark comprises 1,746 tables including union-able table subsets under topics selected from Schema.org: scholarly article, job posting, and music playlist. We end up with these three topics since we can find a fair number of union-able tables of them from diverse sources in the corpus (we can easily find union-able tables from a single source but they are less interesting for table union search as simple syntactic methods can identify all of them because of the same schema and consistent value representations).",,,,,,
2556,PyTorrent,Code Translation,Code Translation,"Code Translation, Annotated Code Search, Text-to-Code Generation, Contextual Embedding for Source Code, Code Generation, Code Completion, Code Classification, Code Comment Generation, CodeSearchNet - Java, Code Documentation Generation, Code Summarization, Code Search, Code Repair","Image, Text",English,Computer Vision,,,https://github.com/fla-sil/PyTorrent,https://paperswithcode.com/dataset/pytorrent,"PyTorrent  contains 218,814 Python package libraries from PyPI and Anaconda environment. This is because earlier studies have shown that much of the code is redundant and Python packages from these environments are better in quality and are well-documented. PyTorrent enables users (such as data scientists, students, etc.) to build off the shelf machine learning models directly without spending months of effort on large infrastructure.",,,,,,
2557,QA-SRL,Open Information Extraction,Open Information Extraction,"Open Information Extraction, Relation Extraction, Semantic Role Labeling",Graph,,Methodology,,,http://qasrl.org/,https://paperswithcode.com/dataset/qa-srl,"QA-SRL was proposed as an open schema for semantic roles, in which the relation between an argument and a predicate is expressed as a natural-language question containing the predicate (“Where was someone educated?”) whose answer is the argument (“Princeton”). The authors collected about 19,000 question-answer pairs from 3,200 sentences.",,Zero-Shot Relation Extraction via Reading Comprehension,https://arxiv.org/abs/1706.04115,200 sentences,,
2558,QA-SRL_Bank_2.0,Semantic Role Labeling,Semantic Role Labeling,Semantic Role Labeling,,,Methodology,,,https://github.com/uwnlp/qasrl-bank,https://paperswithcode.com/dataset/qa-srl-bank-2-0,"QA-SRL Bank 2.0 is a large-scale corpus of Question-Answer driven Semantic Role Labeling (QA-SRL) annotations. The corpus consists of over 250,000 question-answer pairs for over 64,000 sentences across 3 domains and was gathered with a new crowd-sourcing scheme that was shown to have high precision and good recall at modest cost.",,,,000 sentences,,
2559,QA2D,Question to Declarative Sentence,Question to Declarative Sentence,Question to Declarative Sentence,,,Methodology,,MIT,https://github.com/kelvinguu/qanli,https://paperswithcode.com/dataset/qa2d,"The Question to Declarative Sentence (QA2D) Dataset contains 86k question-answer pairs and their manual transformation into declarative sentences. 95% of question answer pairs come from SQuAD (Rajkupar et al., 2016) and the remaining 5% come from four other question answering datasets.",2016,,,,,
2560,QAMPARI,Open-Domain Question Answering,Open-Domain Question Answering,"Open-Domain Question Answering, Answer Generation, Retrieval, Natural Questions, Question Answering, Passage Retrieval",Text,English,Natural Language Processing,,,https://samsam3232.github.io/qampari/,https://paperswithcode.com/dataset/qampari,"QAMPARI is an ODQA benchmark, where question answers are lists of entities, spread across many paragraphs. It was created by (a) generating questions with multiple answers from Wikipedia's knowledge graph and tables, (b) automatically pairing answers with supporting evidence in Wikipedia paragraphs, and (c) manually paraphrasing questions and validating each answer.",,https://arxiv.org/pdf/2205.12665v2.pdf,https://arxiv.org/pdf/2205.12665v2.pdf,,,
2561,QASC,Reading Comprehension,Reading Comprehension,"Reading Comprehension, Question Answering, Information Retrieval",Text,English,Natural Language Processing,,,https://allenai.org/data/qasc,https://paperswithcode.com/dataset/qasc,"QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.",,,,17M sentences,,
2562,QC-Science,Question-Answer categorization,Question-Answer categorization,Question-Answer categorization,,,Methodology,question-answer-categorization-on-qc-science,,https://github.com/VenkteshV/TagRec_EMCL_PKDD_2021,https://paperswithcode.com/dataset/qc-science,"QC-Science contains 47832 question-answer pairs belonging to the science domain tagged with labels of the form subject - chapter - topic. The dataset was collected with the help of a leading e-learning platform. The dataset consists of 40895 samples for training, 2153 samples for validation and 4784 samples for testing.

Description adopted from: https://arxiv.org/pdf/2107.10649v1.pdf",,https://arxiv.org/pdf/2107.10649v1.pdf,https://arxiv.org/pdf/2107.10649v1.pdf,40895 samples,"training, 2153 samples",
2563,QDax,Robot Navigation,Robot Navigation,"Robot Navigation, Reinforcement Learning (RL)",,,Methodology,,MIT,https://github.com/adaptive-intelligent-robotics/QDax,https://paperswithcode.com/dataset/quality-diversity-benchmark-suite,"QDax is a benchmark suite designed for for Deep Neuroevolution in Reinforcement Learning domains for robot control. The suite includes the definition of tasks, environments, behavioral descriptors, and fitness. It specify different benchmarks based on the complexity of both the task and the agent controlled by a deep neural network. The benchmark uses standard Quality-Diversity metrics, including coverage, QD-score, maximum fitness, and an archive profile metric to quantify the relation between coverage and fitness.",,Benchmarking Quality-Diversity Algorithms on Neuroevolution for Reinforcement Learning,https://arxiv.org/pdf/2211.02193v1.pdf,,,
2564,QDSD,Efficient Exploration,Efficient Exploration,Efficient Exploration,,,Methodology,,CC BY 4.0,https://doi.org/10.5281/zenodo.11402792,https://paperswithcode.com/dataset/qdsd,"This Quantum Dots Stability Diagrams (QDSD) Dataset aggregates experimental stability diagrams of quantum dots from different research groups.

Only the interpolated_csv.zip and labels.json files are necessary for offline tuning or machine-learning tasks.

Currently, only single-dot stability diagrams are labelled.

The original data have been provided by different research groups based on the following references:

Rochette et al. 2019
Gaudreau et al. 2009
Stuyck et al. 2021

See the README.md for files description",2019,,,,,
2565,QED,Drug Discovery,Drug Discovery,"Drug Discovery, Question Answering",Text,English,Natural Language Processing,drug-discovery-on-qed,,https://github.com/google-research-datasets/QED,https://paperswithcode.com/dataset/qed,"QED is a linguistically principled framework for explanations in question answering. Given a question and a passage, QED represents an explanation of the answer as a combination of discrete, human-interpretable steps:
sentence selection := identification of a sentence implying an answer to the question
referential equality := identification of noun phrases in the question and the answer sentence that refer to the same thing
predicate entailment := confirmation that the predicate in the sentence entails the predicate in the question once referential equalities are abstracted away.
The QED dataset is an expert-annotated dataset of QED explanations build upon a subset of the Google Natural Questions dataset.",,,,,,
2566,QLEVR,Visual Commonsense Reasoning,Visual Commonsense Reasoning,"Visual Commonsense Reasoning, Visual Question Answering (VQA)","Image, Text",English,Reasoning,visual-question-answering-on-qlevr,,https://github.com/zechenli03/QLEVR,https://paperswithcode.com/dataset/qlevr,"Synthetic datasets have successfully been used to probe visual question-answering datasets for their reasoning abilities. CLEVR, for example, tests a range of visual reasoning abilities. The questions in CLEVR focus on comparisons of shapes, colors, and sizes, numerical reasoning, and existence claims. This paper introduces a minimally biased, diagnostic visual question-answering dataset, QLEVR, that goes beyond existential and numerical quantification and focus on more complex quantifiers and their combinations, e.g., asking whether there are more than two red balls that are smaller than at least three blue balls in an image. We describe how the dataset was created and present a first evaluation of state-of-the-art visual question-answering models, showing that QLEVR presents a formidable challenge to our current models.

Description and image from: QLEVR Dataset Generation",,,,,,
2567,QM7,Molecular Property Prediction,Molecular Property Prediction,Molecular Property Prediction,Time Series,,Methodology,molecular-property-prediction-on-qm7,,http://quantum-machine.org/datasets/,https://paperswithcode.com/dataset/qm7,"QM7 dataset is a subset of the GDB-13 database. GDB-13 contains nearly 1 billion stable and synthetically accessible organic molecules. In the QM7 subset, only molecules with up to 23 atoms are included. These atoms consist of carbon ©, nitrogen (N), oxygen (O), and sulfur (S). The total number of molecules in the QM7 dataset is 7165. Each molecule is represented using the Coulomb matrix, which captures the interactions between atoms.",,,,,,
2568,QM9,Molecular Property Prediction,Molecular Property Prediction,"Molecular Property Prediction, Prediction, Time Series, Drug Discovery, Molecular Graph Generation, NMR J-coupling, Formation Energy, Multi-Task Learning","Graph, Text, Time Series",English,Natural Language Processing,"molecular-property-prediction-on-qm9, nmr-j-coupling-on-qm9, molecular-graph-generation-on-qm9, multi-task-learning-on-qm9, formation-energy-on-qm9, time-series-on-qm9, drug-discovery-on-qm9, prediction-on-qm9",,http://quantum-machine.org/datasets/,https://paperswithcode.com/dataset/qm9,"QM9 provides quantum chemical properties (at DFT level) for a relevant, consistent, and comprehensive chemical space of small organic molecules. This database may serve the benchmarking of existing methods, development of new methods, such as hybrid quantum mechanics/machine learning, and systematic identification of structure-property relationships.",,,,,,
2569,QMSum,Text Summarization,Text Summarization,"Text Summarization, Meeting Summarization",Text,English,Natural Language Processing,text-summarization-on-qmsum,,https://github.com/Yale-LILY/QMSum,https://paperswithcode.com/dataset/qmsum,"QMSum is a new human-annotated benchmark for query-based multi-domain meeting summarisation task, which consists of 1,808 query-summary pairs over 232 meetings in multiple domains.",,,,,,
2570,QMUL-SurvFace,Deblurring,Deblurring,"Deblurring, Face Verification, Super-Resolution, Face Recognition",Image,,Computer Vision,face-verification-on-qmul-survface,,https://qmul-survface.github.io/,https://paperswithcode.com/dataset/qmul-survface,"QMUL-SurvFace is a surveillance face recognition benchmark that contains 463,507 face images of 15,573 distinct identities captured in real-world uncooperative surveillance scenes over wide space and time.",,,,,,
2571,QNLI,Few-Shot NLI,Few-Shot NLI,"Few-Shot NLI, Natural Language Inference, Model Compression, Data-free Knowledge Distillation",Text,English,Natural Language Processing,"natural-language-inference-on-qnli, data-free-knowledge-distillation-on-qnli, model-compression-on-qnli, few-shot-nli-on-qnli-8-training-examples-per",CC BY-SA 4.0,https://gluebenchmark.com/,https://paperswithcode.com/dataset/qnli,"The QNLI (Question-answering NLI) dataset is a Natural Language Inference dataset automatically derived from the Stanford Question Answering Dataset v1.1 (SQuAD). SQuAD v1.1 consists of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). The dataset was converted into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question. This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue. The QNLI dataset is part of GLUE benchmark.",,https://arxiv.org/pdf/1804.07461.pdf,https://arxiv.org/pdf/1804.07461.pdf,,,
2572,Quasimodo,Commonsense Knowledge Base Construction,Commonsense Knowledge Base Construction,Commonsense Knowledge Base Construction,,,Methodology,,,https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/commonsense/quasimodo,https://paperswithcode.com/dataset/quasimodo,"Quasimodo is commonsense knowledge base that focuses on salient properties of objects. We provide several subsets:


Positive statements only
Positive statements top 10%


Negated statements only
Occupations
Positive statements
Negative statements


Animals
Positive statements
Negative statements


Culture
Positive statements
Negative statements


ConceptNet-mapped statements",,,,,,
2573,QuickDraw-Extended,Sketch-Based Image Retrieval,Sketch-Based Image Retrieval,"Sketch-Based Image Retrieval, Image Retrieval",Image,,Computer Vision,,,https://sounakdey.github.io/doodle2search.github.io/,https://paperswithcode.com/dataset/quickdraw-extended,"Consists of 330,000 sketches and 204,000 photos spanning across 110 categories.",,,,,,110
2574,Quick__Draw__Dataset,Sketch Recognition,Sketch Recognition,"Sketch Recognition, Sketch-Based Image Retrieval, Image Retrieval",Image,,Computer Vision,,CC BY 4.0,https://github.com/googlecreativelab/quickdraw-dataset,https://paperswithcode.com/dataset/quick-draw-dataset,"The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located.",,,,,,345
2575,QUILT-1M,Zero-Shot Image Classification,Zero-Shot Image Classification,"Zero-Shot Image Classification, Zero-shot Image Retrieval, Medical Visual Question Answering, Multimodal Deep Learning, Multimodal Text and Image Classification","Image, Text",English,Computer Vision,,MIT,https://quilt1m.github.io/,https://paperswithcode.com/dataset/quilt-1m,"Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of similar data in the medical field, specifically in histopathology, has halted similar progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering 1,087 hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate Quilt: a large-scale vision-language dataset consisting of 768,826 image and text pairs. Quilt was automatically curated using a mixture of models, including large language models), handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around 200K samples. We combine Quilt with datasets, from other sources, including Twitter, research papers, and the internet in general, to create an even larger dataset: Quilt-1M, with 1M paired image-text samples, marking it as the largest vision-language histopathology dataset to date. We demonstrate the value of Quilt-1M by fine-tuning a pre-trained CLIP model. Our model outperforms state-of-the-art models on both zero-shot and linear probing tasks for classifying new pathology images across 13 diverse patch-level datasets of 8 different sub-pathologies and cross-modal retrieval tasks.",,,,200K samples,"valuable educational histopathology videos from expert clinicians. From YouTube, we curate Quilt: a large-scale vision-language dataset consisting of 768,826 image and text pairs. Quilt was automatically curated using a mixture of models, including large language models), handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around 200K samples",
2576,Qulac,Question Answering,Question Answering,"Question Answering, Information Retrieval, Document Ranking",Text,English,Natural Language Processing,,,https://github.com/aliannejadi/qulac,https://paperswithcode.com/dataset/qulac,A dataset on asking Questions for Lack of Clarity in open-domain information-seeking conversations. Qulac presents the first dataset and offline evaluation framework for studying clarifying questions in open-domain information-seeking conversational search systems.,,,,,,
2577,Quora_Question_Pairs,Paraphrase Identification within Bi-Encoder,Paraphrase Identification within Bi-Encoder,"Paraphrase Identification within Bi-Encoder, Paraphrase Identification, Community Question Answering, QQP, Paraphrase Generation, Retrieval, Text Retrieval, Natural Language Inference, Question Answering",Text,English,Natural Language Processing,"natural-language-inference-on-quora-question, paraphrase-identification-on-quora-question-1, paraphrase-identification-on-quora-question, retrieval-on-quora-question-pairs, question-answering-on-quora-question-pairs, text-retrieval-on-quora-question-pairs, qqp-on-qqp, paraphrase-identification-within-bi-encoder, community-question-answering-on-quora, paraphrase-generation-on-quora-question-pairs-1",Custom (non-commercial),https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs,https://paperswithcode.com/dataset/quora-question-pairs,"Quora Question Pairs (QQP) dataset consists of over 400,000 question pairs, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other.",,,,,,
2578,Quoref,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Machine Reading Comprehension, Reading Comprehension",,,Methodology,,CC BY 4.0,https://allenai.org/data/quoref,https://paperswithcode.com/dataset/quoref,"Quoref is a QA dataset which tests the coreferential reasoning capability of reading comprehension systems. In this span-selection benchmark containing 24K questions over 4.7K paragraphs from Wikipedia, a system must resolve hard coreferences before selecting the appropriate span(s) in the paragraphs for answering questions.",,,,7K paragraphs,,
2579,QVHighlights,Zero-shot Moment Retrieval,Zero-shot Moment Retrieval,"Zero-shot Moment Retrieval, Moment Retrieval, Video Grounding, Highlight Detection","Image, Video",,Computer Vision,"video-grounding-on-qvhighlights, highlight-detection-on-qvhighlights, zero-shot-moment-retrieval-on-qvhighlights, moment-retrieval-on-qvhighlights",Attribution-NonCommercial 4.0 International,https://github.com/jayleicn/moment_detr/tree/main/data,https://paperswithcode.com/dataset/qvhighlights,"The Query-based Video Highlights (QVHighlights) dataset is a dataset for detecting customized moments and highlights from videos given natural language (NL). It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a human-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3) five-point scale saliency scores for all query-relevant clips.",,,,,,
2580,R1-Onevision,Multimodal Reasoning,Multimodal Reasoning,Multimodal Reasoning,,,Reasoning,,apache-2.0,https://huggingface.co/datasets/Fancy-MLLM/R1-Onevision,https://paperswithcode.com/dataset/r1-onevision,"The R1-Onevision dataset is a meticulously crafted resource designed to empower models with advanced multimodal reasoning capabilities. Aimed at bridging the gap between visual and textual understanding, this dataset provides rich, context-aware reasoning tasks across diverse domains, including natural scenes, science, mathematical problems, OCR-based content, and complex charts.

It combines high-quality data from LLaVA-OneVision with domain-specific datasets, each carefully selected and filtered to provide a solid foundation for complex visual reasoning tasks. With a focus on enabling deep reasoning and accurate model predictions, R1-Onevision equips models to handle a variety of visual and textual inputs, tackling intricate reasoning challenges with precision.",,,,,,
2581,R2R,Vision-Language Navigation,Vision-Language Navigation,"Vision-Language Navigation, Visual Navigation","Image, Text",English,Computer Vision,"visual-navigation-on-room-to-room-1, vision-language-navigation-on-room2room",Custom (research-only),https://bringmeaspoon.org/,https://paperswithcode.com/dataset/room-to-room,"R2R is a dataset for visually-grounded natural language navigation in real buildings. The dataset requires autonomous agents to follow human-generated navigation instructions in previously unseen buildings, as illustrated in the demo above. For training, each instruction is associated with a Matterport3D Simulator trajectory. 22k instructions are available, with an average length of 29 words. There is a test evaluation server for this dataset available at EvalAI.",,,,,,
2582,RACE,Distractor Generation,Distractor Generation,"Distractor Generation, Question Answering, Reading Comprehension",Text,English,Natural Language Processing,"question-answering-on-race, distractor-generation-on-race, reading-comprehension-on-race","Custom (research-only, non-commercial)",https://www.cs.cmu.edu/~glai1/data/race/,https://paperswithcode.com/dataset/race,"The ReAding Comprehension dataset from Examinations (RACE) dataset is a machine reading comprehension dataset consisting of 27,933 passages and 97,867 questions from English exams, targeting Chinese students aged 12-18. RACE consists of two subsets, RACE-M and RACE-H, from middle school and high school exams, respectively. RACE-M has 28,293 questions and RACE-H has 69,574. Each question is associated with 4 candidate answers, one of which is correct. The data generation process of RACE differs from most machine reading comprehension datasets - instead of generating questions and answers by heuristics or crowd-sourcing, questions in RACE are specifically designed for testing human reading skills, and are created by domain experts.",,Dynamic Fusion Networks for Machine Reading Comprehension,https://arxiv.org/abs/1711.04964,,,
2583,RAD-ChestCT_Dataset,3D Medical Imaging Segmentation,3D Medical Imaging Segmentation,"3D Medical Imaging Segmentation, 3D Classification","3D, Image",,Medical,,,https://zenodo.org/record/6406114,https://paperswithcode.com/dataset/rad-chestct-dataset,"The RAD-ChestCT dataset is a large medical imaging dataset developed by Duke MD/PhD Rachel Draelos during her Computer Science PhD supervised by Lawrence Carin. The full dataset includes 35,747 chest CT scans from 19,661 adult patients. The public Zenodo repository contains an initial release of 3,630 chest CT scans, approximately 10% of the dataset. This dataset is of significant interest to the machine learning and medical imaging research communities.",,,,,,
2584,RadarScenes,Autonomous Vehicles,Autonomous Vehicles,Autonomous Vehicles,,,Methodology,,Attribution-NonCommercial 4.0 International,https://radar-scenes.com/,https://paperswithcode.com/dataset/radarscenes,"RadarScenes is a real-world radar point cloud dataset for automotive applications.

It consists of measurements and point-wise annotations from more than four hours of driving collected by four series radar sensors mounted on one test vehicle. Individual detections of dynamic objects were manually grouped to clusters and labeled afterwards. The purpose of this data set is to enable the development of novel (machine learning-based) radar perception algorithms with the focus on moving road users. Images of the recorded sequences were captured using a documentary camera.",,,,,,
2585,RaDelft,Radar Object Detection,Radar Object Detection,Radar Object Detection,Image,,Computer Vision,,,https://data.4tu.nl/datasets/4e277430-e562-4a7a-adfe-30b58d9a5f0a,https://paperswithcode.com/dataset/radelft,"The RaDelft dataset is a novel, large-scale, real-life, and multi-sensor dataset that has been recorded using a demonstrator vehicle in different locations in the city of Delft. It contains data from a lidar, an imaging radar board, a camera, and the ego vehicle’s odometry. Check the reference paper for more details about the data collection and the sensors' setup.

To request access, use the following link:

https://docs.google.com/forms/d/1whyNTiAsZ4l0ENwFiut4yb3FzF3yPjKCdH42OMTlaEk/",,,,,,
2586,RadGraph,Joint Entity and Relation Extraction,Joint Entity and Relation Extraction,"Joint Entity and Relation Extraction, Relation Extraction, Medical Relation Extraction, Medical Named Entity Recognition, Named Entity Recognition (NER)","Graph, Image, Text",English,Computer Vision,,PhysioNet Credentialed Health Data License 1.5.0,,https://paperswithcode.com/dataset/radgraph,"RadGraph is a dataset of entities and relations in radiology reports based on our novel information extraction schema, consisting of 600 reports with 30K radiologist annotations and 221K reports with 10.5M automatically generated annotations.

We release a development dataset, which contains board-certified radiologist annotations for 500 radiology reports from the MIMIC-CXR dataset (14,579 entities and 10,889 relations), and a test dataset, which contains two independent sets of board-certified radiologist annotations for 100 radiology reports split equally across the MIMIC-CXR and CheXpert datasets. We also release an inference dataset, which contains automatically generated annotations for 220,763 MIMIC-CXR reports (around 6 million entities and 4 million relations) and 500 CheXpert reports (13,783 entities and 9,908 relations) with mappings to associated chest radiographs.",,,,,,
2587,RADIATE,Object Detection,Object Detection,"Object Detection, 2D Object Detection, Scene Understanding, Multiple Object Tracking","Image, Video",,Computer Vision,"multiple-object-tracking-on-radiate, 2d-object-detection-on-radiate",,https://github.com/marcelsheeny/radiate_sdk,https://paperswithcode.com/dataset/radiate,"RADIATE (RAdar Dataset In Adverse weaThEr) is new automotive dataset created by Heriot-Watt University which includes Radar, Lidar, Stereo Camera and GPS/IMU.
The data is collected in different weather scenarios (sunny, overcast, night, fog, rain and snow) to help the research community to develop new methods of vehicle perception.
The radar images are annotated in 7 different scenarios: Sunny (Parked), Sunny/Overcast (Urban), Overcast (Motorway), Night (Motorway), Rain (Suburban), Fog (Suburban) and Snow (Suburban). The dataset contains 8 different types of objects (car, van, truck, bus, motorbike, bicycle, pedestrian and group of pedestrians).",,,,,,
2588,RadioTalk,Data Augmentation,Data Augmentation,"Data Augmentation, Speech Recognition, Speaker Diarization","Audio, Image, Text",English,Computer Vision,,,https://github.com/social-machines/RadioTalk,https://paperswithcode.com/dataset/radiotalk,"RadioTalk is a corpus of speech recognition transcripts sampled from talk radio broadcasts in the United States between October of 2018 and March of 2019. The corpus is intended for use by researchers in the fields of natural language processing, conversational analysis, and the social sciences. The corpus encompasses approximately 2.8 billion words of automatically transcribed speech from 284,000 hours of radio, together with metadata about the speech, such as geographical location, speaker turn boundaries, gender, and radio program information.",2018,,,,,
2589,RAF-DB,Facial Expression Recognition,Facial Expression Recognition,"Facial Expression Recognition, Facial Expression Recognition (FER)",Image,,Computer Vision,"facial-expression-recognition-on-real-world, facial-expression-recognition-on-raf-db, facial-expression-recognition-on-raf-db-1",Custom (non-commercial),http://www.whdeng.cn/raf/model1.html,https://paperswithcode.com/dataset/raf-db,"The Real-world Affective Faces Database (RAF-DB) is a dataset for facial expression. It contains 29672 facial images tagged with basic or compound expressions by 40 independent taggers. Images in this database are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc.",,Landmark Guidance Independent Spatio-channel Attention and Complementary Context Information based Facial Expression Recognition,https://arxiv.org/abs/2007.10298,,,
2590,RAFT,Few-Shot Text Classification,Few-Shot Text Classification,Few-Shot Text Classification,"Image, Text",English,Computer Vision,few-shot-text-classification-on-raft,,https://raft.elicit.org/,https://paperswithcode.com/dataset/raft,"The RAFT benchmark (Realworld Annotated Few-shot Tasks) focuses on naturally occurring tasks and uses an evaluation setup that mirrors deployment.

RAFT is a few-shot classification benchmark that tests language models:


across multiple domains (lit reviews, medical data, tweets, customer interaction, etc.)
on economically valuable classification tasks (someone inherently cares about the task)
with evaluation that mirrors deployment (50 labeled examples per task, info retrieval allowed, hidden test set)

Description from: https://raft.elicit.org/",,,,,valuation that mirrors deployment (50 labeled examples,
2591,RailEye3D_Dataset,Scene Understanding,Scene Understanding,"Scene Understanding, Pedestrian Detection, Multi-Object Tracking, Object Detection, Multiview Detection","Image, Video",,Computer Vision,,Custom (non-commercial),https://github.com/raileye3d/raileye3d_dataset,https://paperswithcode.com/dataset/raileye3d-dataset,"The RailEye3D dataset, a collection of train-platform scenarios for applications targeting passenger safety and automation of train dispatching, consists of 10 image sequences captured at 6 railway stations in Austria. Annotations for multi-object tracking are provided in both an unified format as well as the ground-truth format used in the MOTChallenge.",,,,,,
2592,Raindrop,Image Restoration,Image Restoration,"Image Restoration, Single Image Deraining, Rain Removal",Image,,Computer Vision,single-image-deraining-on-raindrop,,https://rui1996.github.io/raindrop/raindrop_removal.html,https://paperswithcode.com/dataset/raindrop,"Raindrop is a set of image pairs, where
each pair contains exactly the same background scene, yet
one is degraded by raindrops and the other one is free from
raindrops. To obtain this, the images are captured through two pieces of exactly the
same glass: one sprayed with water, and the other is left
clean. The dataset consists of 1,119 pairs of images, with various
background scenes and raindrops. They were captured with a Sony A6000
and a Canon EOS 60.",,,,,,
2593,Ranking_social_media_news_feed,News Recommendation,News Recommendation,"News Recommendation, Social Media Popularity Prediction, Recommendation Systems",Time Series,,Methodology,,,https://github.com/SamBelkacem/Ranking-social-media-news-feed,https://paperswithcode.com/dataset/ranking-social-media-news-feed,"A dataset consisting of recipient 46 users and, 26180 tweets. The dataset includes the news feed of the users and 13 features that may influence the relevance of the tweets.",,,,,,
2594,RARE,Graph Matching,Graph Matching,Graph Matching,Graph,,Methodology,graph-matching-on-rare,BSD,https://github.com/osome-iu/Rematch-RARE,https://paperswithcode.com/dataset/rare,"RARE consists of English AMR pairs with similarity scores that reflect the structural differences between them.

Given that AMRs are graphical representations of text, an AMR similarity metric should be sensitive to structural variations between AMRs, even if its labels remain unchanged. Since there is no established evaluation of AMR metrics on structural similarity, we have developed a new benchmark dataset called Randomized AMRs with Rewired Edges (RARE). RARE consists of English AMR pairs with similarity scores that reflect the structural differences between them.

It can be used to  evaluate AMR metrics on their sensitivity at detecting structural changes between pairs of AMRs.",,BSD,https://catalog.ldc.upenn.edu/license/ldc-non-members-agreement.pdf,,,
2595,Rare_Diseases_Mentions_in_MIMIC-III,Clinical Note Phenotyping,Clinical Note Phenotyping,"Clinical Note Phenotyping, Entity Linking, Low Resource Named Entity Recognition, Named Entity Recognition (NER), Computational Phenotyping","Image, Text",English,Computer Vision,entity-linking-on-rare-diseases-mentions-in-1,MIT,https://github.com/acadTags/Rare-disease-identification/tree/main/data%20annotation,https://paperswithcode.com/dataset/rare-disease-annotation-from-mimic-iii,"Data annotation
The 1,073 full rare disease mention annotations (from 312 MIMIC-III discharge summaries) are in full_set_RD_ann_MIMIC_III_disch.csv.

The data split:
* the first 400 rows are used for validation, validation_set_RD_ann_MIMIC_III_disch.csv, and
* the last 673 rows are used for testing, test_set_RD_ann_MIMIC_III_disch.csv.

The 198 rare disease mention annotations (from 145 MIMIC-III radiology reports) are in test_set_RD_ann_MIMIC_III_rad.csv. To note that radiology reports were only used for testing and not for validation.

To note: a row can only be consider a true phenotype of the patient only when the value of the column gold mention-to-ORDO label is 1.

Data sampling and annotation procedure


(i) Randomly sampled 500 discharge summaries (and 1000 radiology reports) from MIMIC-III



(ii) 312 of the 500 discharge summaries (and 145 of the 1000 radiology reports) have at least one positive UMLS mention linked to ORDO, as identified by SemEHR; there are altogether 1073 (and 198 in radiology reports) UMLS/ORDO mentions.



(iii) 3 medical informatics researchers (staff or PhD students) annotated the 1,073 mentions (and 2 medical informatics researchers annotated the 198 mentions in radiology reports), regarding whether they are the correct patient phenotypes matched to UMLS and ORDO. Contradictions in the annotations were then resolved by another research staff having biomedical background.



Data dictionary
| Column   Name                                | Description                                                                                                                                                                                                   |
|----------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ROW_ID                                       | Identifier unique to each row, see https://mimic.physionet.org/mimictables/noteevents/                                                                                                                                                     |
| SUBJECT_ID                                | Identifier unique to a patient, see https://mimic.physionet.org/mimictables/noteevents/                                                                                                                                                                                                              |
| HADM_ID                                      | Identifier unique to a patient hospital stay, see https://mimic.physionet.org/mimictables/noteevents/                                                                                                                                                                                                              |
| document structure name                    | The document structure name of the mention. The document structure name is identified by   SemEHR  (only for discharge summaries).                                                                                                          |
| document structure offset in full document | The start and ending offsets of the document structure texts (or template) in the whole discharge summary. The document structure is parsed by SemEHR with regular expressions  (only for discharge summaries).                            |
| mention                                      | The mention identified by SemEHR.                                                                                                                                                                          |
| mention offset in document structure       | The start and ending offsets of the mention in the document structure (only for discharge summaries).                                                                                                                                      |
| mention offset in full document            | The start and ending offsets of the mention in the whole discharge summary. They can be calculated by document structure offset in full document and mention offset in document structure.                                                                                     |
| UMLS with desc                               | The UMLS identified by SemEHR, corresponding to the mention.                                                                                                                                                |
| ORDO with desc                               | The ORDO matched to the UMLS, using the linkage in the ORDO ontology, see https://www.ebi.ac.uk/ols/ontologies/ordo/terms?iri=http%3A%2F%2Fwww.orpha.net%2FORDO%2FOrphanet_3325 as an example.          |
| gold mention-to-UMLS label                 | Whether the mention-UMLS pair indicate a correct phenotype of the patient (i.e. a positive mention that correctly matches to the UMLS concept), 1 if correct, 0 if not.                                 |
| gold UMLS-to-ORDO label                    | Whether the matching is correct from the UMLS concept to the ORDO concept, 1 if correct, 0 if not.                                                                                                          |
| gold mention-to-ORDO label                 | Whether the mention-ORDO triple indicates a correct phenotype of the patient, 1 if correct, 0 if not. This column is 1 if both the mention-to-UMLS label and the UMLS-to-ORDO label are 1, otherwise 0. |

Note:
* These manual annotations are by no means to be perfect. There are hypothetical mentions which were difficult for the annotators to make a decision (see some notes in the raw annotations). Also, they are based on the output of SemEHR, which does not have 100% recall, so the annotations may not cover all rare diseases mentions from the sampled discharge summaries.
* In row 323 from the full set or the validation set, the mention nph is not in the document structure (due to error in mention extraction), thus the gold mention-to-UMLS label is -1.

Raw annotations (with model predictions)
The two excel workbooks, 



for validation - SemEHR ori (MIMIC-III-DS, free text removed, with predictions).xlsx (annotations starting from column CX and also in the third sheet, distinct umls-ordo), and 



for validation - 1000 docs - ori - MIMIC-III-rad (free text removed, with predictions).xlsx (annotations starting from column Z), 



show the raw annotations, including each annotator's results and notes, and the predictions of all baselines approaches/tools. The predictions were not available to the annotators when the annotations were made. Free texts of clinical notes have been removed before the publication of the data.",,,,400 rows,,
2596,RaTE-NER,NER,NER,"NER, Medical Report Generation",Text,English,Natural Language Processing,,,https://huggingface.co/datasets/Angelakeke/RaTE-NER,https://paperswithcode.com/dataset/rate-ner,"RaTE-NER dataset is a large-scale, radiological named entity recognition (NER) dataset, including 13,235 manually annotated sentences from 1,816 reports within the MIMIC-IV database, that spans 9 imaging modalities and 23 anatomical regions, ensuring comprehensive coverage.

Additionally, we further enriched the dataset with 33,605 sentences from the 17,432 reports available on Radiopaedia, by leveraging GPT-4 and other medical knowledge libraries to capture intricacies and nuances of less common diseases and abnormalities. We manually labeled 3,529 sentences to create a test set.",,,,605 sentences,,
2597,RAVDESS,Audio Classification,Audio Classification,"Audio Classification, Emotion Classification, Speech Emotion Recognition, Video Emotion Recognition, Music Emotion Recognition, Emotion Recognition, Facial Emotion Recognition, Facial Expression Recognition (FER)","Audio, Image, Video",,Audio,"facial-emotion-recognition-on-ravdess, emotion-recognition-on-ravdess, audio-classification-on-ravdess, facial-expression-recognition-on-ravdess, speech-emotion-recognition-on-ravdess, emotion-classification-on-ravdess",Attribution-NonCommercial 4.0 International,https://zenodo.org/record/1188976#.YFZuJ0j7SL8,https://paperswithcode.com/dataset/ravdess,"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7,356 files (total size: 24.8 GB). The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.

Paper: The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",,,,,,
2598,RAVEN-FAIR,Relational Reasoning,Relational Reasoning,Relational Reasoning,,,Reasoning,,,https://github.com/yanivbenny/RAVEN_FAIR,https://paperswithcode.com/dataset/raven-fair,RAVEN-FAIR is a modified version of the RAVEN dataset.,,,,,,
2599,RAVEN,Relational Reasoning,Relational Reasoning,"Relational Reasoning, Visual Reasoning, Question Answering","Image, Text",English,Reasoning,,,http://wellyzhang.github.io/project/raven.html,https://paperswithcode.com/dataset/raven,"RAVEN consists of 1,120,000 images and 70,000 RPM (Raven's Progressive Matrices)
problems, equally distributed in 7 distinct figure configurations.",,,,000 images,,
2600,RCTW-17,Scene Text Detection,Scene Text Detection,"Scene Text Detection, Scene Text Recognition","Image, Text",English,Computer Vision,,,https://rctw.vlrlab.net/dataset/,https://paperswithcode.com/dataset/rctw-17%0A,"Features a large-scale dataset with 12,263 annotated images. Two tasks, namely text localization and end-to-end recognition, are set up. The competition took place from January 20 to May 31, 2017. 23 valid submissions were received from 19 teams.",2017,,,,,
2601,RCV1,Cross-Lingual Document Classification,Cross-Lingual Document Classification,"Cross-Lingual Document Classification, Multi-Label Text Classification, Hierarchical Multi-label Classification, Text Classification","Image, Text",English,Computer Vision,"cross-lingual-document-classification-on-12, hierarchical-multi-label-classification-on-17, cross-lingual-document-classification-on-13, multi-label-text-classification-on-rcv1-v2-1, text-classification-on-rcv1, multi-label-text-classification-on-rcv1",Custom,http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm,https://paperswithcode.com/dataset/rcv1,"The RCV1 dataset is a benchmark dataset on text categorization. It is a collection of newswire articles producd by Reuters in 1996-1997. It contains 804,414 manually labeled newswire documents, and categorized with respect to three controlled vocabularies: industries, topics and regions.",1996,Random Projections for Linear Support Vector Machines,https://arxiv.org/abs/1211.6085,,,
2602,Re-DocRED,Document-level RE with incomplete labeling,Document-level RE with incomplete labeling,"Document-level RE with incomplete labeling, Document-level Relation Extraction, Relation Extraction","Graph, Text",English,Natural Language Processing,"document-level-re-with-incomplete-labeling-on-1, relation-extraction-on-redocred, document-level-relation-extraction-on-re",,https://github.com/tonytan48/Re-DocRED,https://paperswithcode.com/dataset/re-docred,"The Re-DocRED Dataset resolved the following problems of DocRED:


Resolved the incompleteness problem by supplementing large amounts of relation triples.
Addressed the logical inconsistencies in DocRED.
Corrected the coreferential errors within DocRED.",,,,,,
2603,READ2016_line-level_,Handwritten Text Recognition,Handwritten Text Recognition,Handwritten Text Recognition,"Image, Text",English,Computer Vision,handwritten-text-recognition-on-read2016-line,,,https://paperswithcode.com/dataset/read2016-line-level,"This dataset arises from the READ project (Horizon 2020).

The dataset consists of a subset of documents from the Ratsprotokolle collection composed of minutes of the council meetings held from 1470 to 1805 (about 30.000 pages), which will be used in the READ project. This dataset is written in Early Modern German. The number of writers is unknown. Handwriting in this collection is complex enough to challenge the HTR software.

The training dataset is composed of 400 pages; most of the pages consist of a single block with many difficulties for line detection and extraction. The ground-truth in this set is in PAGE format and it is provided annotated at line level in the PAGE files.

The previous dataset is the same that is located at https://zenodo.org/record/218236#.WnLhaCHhBGF

The new file includes the test set corresponding to the HTR competition held at ICFHR 2016

Toselli, A.H., Romero, V., Villegas, M., Vidal, E., & Sánchez, J.A. (2018). HTR Dataset ICFHR 2016 (Version 1.2.0) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.1297399",2020,,,,,
2604,ReadingBank,Reading Order Detection,Reading Order Detection,Reading Order Detection,Image,,Computer Vision,reading-order-detection-on-readingbank,,https://github.com/doc-analysis/ReadingBank,https://paperswithcode.com/dataset/readingbank,"ReadingBank is a benchmark dataset for reading order detection built with weak supervision from WORD documents, which contains 500K document images with a wide range of document types as well as the corresponding reading order information.",,,,,,
2605,READ_2016,Handwriting Recognition,Handwriting Recognition,"Handwriting Recognition, Handwritten Text Recognition","Image, Text",English,Computer Vision,handwritten-text-recognition-on-read-2016,Creative Commons Attribution 4.0 International,https://zenodo.org/record/1297399,https://paperswithcode.com/dataset/read-2016,"This dataset arises from the READ project (Horizon 2020).

The dataset consists of a subset of documents from the Ratsprotokolle collection composed of minutes of the council meetings held from 1470 to 1805 (about 30.000 pages), which will be used in the READ project. This dataset is written in Early Modern German. The number of writers is unknown. Handwriting in this collection is complex enough to challenge the HTR software.

The training dataset is composed of 400 pages; most of the pages consist of a single block with many difficulties for line detection and extraction. The ground-truth in this set is in PAGE format and it is provided annotated at line level in the PAGE files.

The previous dataset is the same that is located at https://zenodo.org/record/218236#.WnLhaCHhBGF

The new file includes the test set corresponding to the HTR competition held at ICFHR 2016

Toselli, A.H., Romero, V., Villegas, M., Vidal, E., & Sánchez, J.A. (2018). HTR Dataset ICFHR 2016 (Version 1.2.0) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.1297399",2020,,,,,
2606,REAL275,6D Pose Estimation using RGBD,6D Pose Estimation using RGBD,"6D Pose Estimation using RGBD, 6D Pose Estimation","3D, Image",,Computer Vision,6d-pose-estimation-using-rgbd-on-real275,,https://geometry.stanford.edu/projects/NOCS_CVPR2019/,https://paperswithcode.com/dataset/real275,"REAL275 is a benchmark for category-level pose estimation. It contains 4300 training frames, 950 validation and 2750 for testing across 18 different real scenes.",,,,,,
2607,RealCQA,Chart Question Answering,Chart Question Answering,Chart Question Answering,Text,English,Natural Language Processing,chart-question-answering-on-realcqa,GNU General Public License v3.0,https://github.com/cse-ai-lab/RealCQA,https://paperswithcode.com/dataset/realcqa,"RealCQA
Scientific Chart Question Answering as a Test-bed for First-Order Logic

check on huggingface : https://huggingface.co/datasets/sal4ahm/RealCQA",,,,,,
2608,RealNews,Text Generation,Text Generation,"Text Generation, Language Modelling, Fake News Detection","Image, Text",English,Computer Vision,,"Custom (research-only, non-commercial)",https://github.com/rowanz/grover/tree/master/realnews,https://paperswithcode.com/dataset/realnews,"RealNews is a large corpus of news articles from Common Crawl. Data is scraped from Common Crawl, limited to the 5000 news domains indexed by Google News. The authors used the Newspaper Python library to extract the body and metadata from each article. News from Common Crawl dumps from December 2016 through March 2019
were used as training data; articles published in April 2019 from the April 2019 dump were used for evaluation. After deduplication, RealNews is 120 gigabytes without compression.",2016,https://arxiv.org/pdf/1905.12616v3.pdf,https://arxiv.org/pdf/1905.12616v3.pdf,,,
2609,RealSRSet,Super-Resolution,Super-Resolution,Super-Resolution,,,Methodology,,,https://github.com/cszn/BSRGAN,https://paperswithcode.com/dataset/realsrset,20 real low-resolution images selected from existing datasets or downloaded from internet,,,,,,
2610,RealVul,Vulnerability Detection,Vulnerability Detection,Vulnerability Detection,Image,,Computer Vision,,,https://huggingface.co/realvul,https://paperswithcode.com/dataset/realvul,"This is a C++ vulnerability detection dataset following realistic settings. For details, please check our study Revisiting the Performance of Deep Learning-Based Vulnerability Detection on Realistic Datasets (Partha et al., 2024)

The column names are self-describing. The most important two columns are,

Target: int: vulnerable to not.
Code: str: the code segment.
For details, please check our huggingface repository.",2024,,,,,
2611,Real_Blur_Dataset,Deblurring,Deblurring,"Deblurring, Image Deblurring",Image,,Computer Vision,"deblurring-on-realblur-r-trained-on-gopro, deblurring-on-realblur-r, image-deblurring-on-realblur-r, deblurring-on-realblur-j-trained-on-gopro, image-deblurring-on-realblur-j, deblurring-on-realblur-j-1",,http://cg.postech.ac.kr/research/realblur/,https://paperswithcode.com/dataset/real-blur-dataset,"The dataset consists of 4,738 pairs of images of 232 different scenes including reference pairs. All images were captured both in the camera raw and JPEG formats, hence generating two datasets: RealBlur-R from the raw images, and RealBlur-J from the JPEG images. Each training set consists of 3,758 image pairs, while each test set consists of 980 image pairs.

The deblurring result is first aligned to its ground truth sharp image using a homography estimated by the enhanced correlation coefficients method, and PSNR or SSIM is computed in sRGB color space.",,,,,,
2612,Real_HSI,Spectral Reconstruction,Spectral Reconstruction,Spectral Reconstruction,3D,,Methodology,spectral-reconstruction-on-real-hsi,Custom,https://github.com/mengziyi64/TSA-Net,https://paperswithcode.com/dataset/real-hsi,End-to-End Low Cost Compressive Spectral Imaging with Spatial-Spectral Self-Attention,,,,,,
2613,Real_Rain_Dataset,Image Restoration,Image Restoration,"Image Restoration, Single Image Deraining, Rain Removal",Image,,Computer Vision,,CC BY-NC-SA,https://stevewongv.github.io/derain-project.html,https://paperswithcode.com/dataset/real-rain-dataset,A large-scale dataset of ~29.5K rain/rain-free image pairs that covers a wide range of natural rain scenes.,,Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset,https://arxiv.org/pdf/1904.01538v2.pdf,,,
2614,ReaSCAN,Compositional Generalization (AVG),Compositional Generalization (AVG),"Compositional Generalization (AVG), Vision-Language Navigation, Compositional Zero-Shot Learning",Text,English,Natural Language Processing,compositional-generalization-avg-on-reascan,Creative Commons Attribution 4.0 International,https://reascan.github.io/,https://paperswithcode.com/dataset/reascan,ReaSCAN is a synthetic navigation task that requires models to reason about surroundings over syntactically difficult languages.,,,,,,
2615,REBEL,Entity Linking,Entity Linking,"Entity Linking, Relation Extraction",Graph,,Methodology,"relation-extraction-on-rebel, entity-linking-on-rebel",CC-4.0,https://osf.io/4x3r9/?view_only=87e7af84c0564bd1b3eadff23e4b7e54,https://paperswithcode.com/dataset/rebel-dataset,Wikipedia abstracts automatically annotated with WikiData entities and relations that are entailed by the text.  Over 9 million triplets.,,,,,,
2616,REBUS,multimodal generation,multimodal generation,"multimodal generation, Multimodal Deep Learning, Logical Reasoning, Multimodal Reasoning",Text,English,Multimodal,multimodal-reasoning-on-rebus,,https://cavendishlabs.org/rebus/,https://paperswithcode.com/dataset/rebus,"Recent advances in large language models have led to the development of multimodal LLMs
(MLLMs), which take both image data and text as an input. Virtually all of these models
have been announced within the past year, leading to a significant need for benchmarks
evaluating the abilities of these models to reason truthfully and accurately on a diverse set
of tasks. When Google announced Gemini (Gemini Team et al., 2023), they showcased its
ability to solve rebuses—wordplay puzzles which involve creatively adding and subtracting
letters from words derived from text and images. The diversity of rebuses allows for a
broad evaluation of multimodal reasoning capabilities, including image recognition, multi-
step reasoning, and understanding the human creator’s intent.
We present REBUS: a collection of 333 hand-crafted rebuses spanning 13 diverse cate-
gories, including hand-drawn and digital images created by nine contributors. Samples are
presented in Table 1. Notably, GPT-4V, the most powerful model we evaluated, answered
only 24% of puzzles correctly, highlighting the poor capabilities of MLLMs in new and unex-
pected domains to which human reasoning generalizes with comparative ease. Open-source
models perform even worse, with a median accuracy below 1%. We notice that models
often give faithless explanations, fail to change their minds after an initial approach doesn’t
work, and remain highly uncalibrated on their own abilities.",2023,,,,,
2617,RECCON,Recognizing Emotion Cause in Conversations,Recognizing Emotion Cause in Conversations,"Recognizing Emotion Cause in Conversations, Causal Emotion Entailment",,,Methodology,"recognizing-emotion-cause-in-conversations-on, causal-emotion-entailment-on-reccon",,https://github.com/declare-lab/RECCON,https://paperswithcode.com/dataset/reccon,RECCON is a dataset for the task of recognizing emotion cause in conversations.,,Recognizing Emotion Cause in Conversations,https://arxiv.org/pdf/2012.11820.pdf,,,
2618,Recipe1M_,Recipe Generation,Recipe Generation,"Recipe Generation, Cross-Modal Retrieval, Food Recognition, Image Generation","Image, Text",English,Computer Vision,"recipe-generation-on-recipe1m, cross-modal-retrieval-on-recipe1m, cross-modal-retrieval-on-recipe1m-1",,http://im2recipe.csail.mit.edu/,https://paperswithcode.com/dataset/recipe1m-1,Recipe1M+ is a dataset which contains one million structured cooking recipes with 13M associated images.,,,,,,
2619,RecipeNLG,Recipe Generation,Recipe Generation,Recipe Generation,Text,English,Natural Language Processing,recipe-generation-on-recipenlg,,https://recipenlg.cs.put.poznan.pl/,https://paperswithcode.com/dataset/recipenlg,,,,,,,
2620,ReClor,Machine Reading Comprehension,Machine Reading Comprehension,"Machine Reading Comprehension, Logical Reasoning Question Answering, Question Answering, Reading Comprehension",Text,English,Natural Language Processing,"reading-comprehension-on-reclor, machine-reading-comprehension-on-reclor, logical-reasoning-question-ansering-on-reclor, question-answering-on-reclor",,https://whyu.me/reclor/,https://paperswithcode.com/dataset/reclor,"Logical reasoning is an important ability to examine, analyze, and critically evaluate arguments as they occur in ordinary language as the definition from Law School Admission Council. ReClor is a dataset extracted from logical reasoning questions of standardized graduate admission examinations.",,,,,,
2621,ReCO,Machine Reading Comprehension,Machine Reading Comprehension,"Machine Reading Comprehension, Chinese Reading Comprehension, Causal Inference",,,Methodology,,,https://github.com/benywon/ReCO,https://paperswithcode.com/dataset/reco,A human-curated ChineseReading Comprehension dataset on Opinion. The questions in ReCO are opinion based queries issued to the commercial search engine. The passages are provided by the crowdworkers who extract the support snippet from the retrieved documents.,,,,,,
2622,ReCoRD,Common Sense Reasoning,Common Sense Reasoning,Common Sense Reasoning,,,Reasoning,common-sense-reasoning-on-record,Custom,https://sheng-z.github.io/ReCoRD-explorer/,https://paperswithcode.com/dataset/record,Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) is a large-scale reading comprehension dataset which requires commonsense reasoning. ReCoRD consists of queries automatically generated from CNN/Daily Mail news articles; the answer to each query is a text span from a summarizing passage of the corresponding news. The goal of ReCoRD is to evaluate a machine's ability of commonsense reasoning in reading comprehension. ReCoRD is pronounced as [ˈrɛkərd].,,Zhang et al,https://arxiv.org/pdf/1810.12885v1.pdf,,,
2623,REDDIT-12K,Graph Classification,Graph Classification,"Graph Classification, Quantization, Topological Data Analysis","Graph, Image",,Computer Vision,graph-classification-on-reddit-12k,,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,https://paperswithcode.com/dataset/reddit-12k,"Reddit12k contains 11929 graphs each corresponding to an online discussion thread where nodes represent users, and an edge represents the fact that one of the two users responded to the comment of the other user. There is 1 of 11 graph labels associated with each of these 11929 discussion graphs, representing the category of the community.",,Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity,https://arxiv.org/abs/1904.01098,,,
2624,REDDIT-5K,Graph Classification,Graph Classification,"Graph Classification, Topological Data Analysis","Graph, Image",,Computer Vision,graph-classification-on-reddit-multi-5k,,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,https://paperswithcode.com/dataset/reddit-5k,Reddit-5K is a relational dataset extracted from Reddit.,,,,,,
2625,REDDIT-BINARY,Graph Classification,Graph Classification,"Graph Classification, Graph Representation Learning","Graph, Image",,Computer Vision,"graph-classification-on-reddit-b, graph-classification-on-reddit-binary",,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,https://paperswithcode.com/dataset/reddit-binary,"REDDIT-BINARY consists of graphs corresponding to online discussions on Reddit. In each graph, nodes represent users, and there is an edge between them if at least one of them respond to the other’s comment. There are four popular subreddits, namely, IAmA, AskReddit, TrollXChromosomes, and atheism. IAmA and AskReddit are two question/answer based subreddits, and TrollXChromosomes and atheism are two discussion-based subreddits. A graph is labeled according to whether it belongs to a question/answer-based community or a discussion-based community.",,A simple yet effective baseline for non-attributed graph classification,https://arxiv.org/abs/1811.03508,,,
2626,Reddit,Topological Data Analysis,Topological Data Analysis,"Topological Data Analysis, Dynamic Link Prediction, Abstractive Text Summarization, Question Answering, Text Classification, News Generation, Dialogue Generation, Dialogue Evaluation, Quantization, Open-Domain Dialog, Classification, Text Summarization, Graph Classification, Conversational Response Selection, Node Classification, News Recommendation, Topic Models, Generative Question Answering, Sarcasm Detection, Graph Representation Learning","Graph, Image, Text, Time Series",English,Computer Vision,"text-summarization-on-reddit-tifu, question-answering-on-squadshifts-reddit, node-classification-on-reddit, classification-on-reddit-ideology-database, graph-classification-on-reddit-binary, graph-classification-on-reddit-multi-5k, sarcasm-detection-on-figlang-2020-reddit, graph-classification-on-reddit-b, graph-classification-on-reddit-multi-12k, dynamic-link-prediction-on-reddit, graph-classification-on-reddit-12k, dialogue-generation-on-reddit-multi-ref, conversational-response-selection-on-polyai",,http://snap.stanford.edu/graphsage/,https://paperswithcode.com/dataset/reddit,"The Reddit dataset is a graph dataset from Reddit posts made in the month of September, 2014. The node label in this case is the community, or “subreddit”, that a post belongs to. 50 large communities have been sampled to build a post-to-post graph, connecting posts if the same user comments on both. In total this dataset contains 232,965 posts with an average degree of 492. The first 20 days are used for training and the remaining days for testing (with 30% used for validation). For features, off-the-shelf 300-dimensional GloVe CommonCrawl word vectors are used.",2014,https://arxiv.org/pdf/1706.02216.pdf,https://arxiv.org/pdf/1706.02216.pdf,,,
2627,Reddit_Conversation_Corpus,Open-Domain Dialog,Open-Domain Dialog,Open-Domain Dialog,,,Methodology,,MIT,https://github.com/nouhadziri/THRED,https://paperswithcode.com/dataset/reddit-conversation-corpus,"Reddit Conversation Corpus (RCC) consists of conversations, scraped from Reddit, for a 20 month period from November 2016 until August 2018. To ensure the quality and diversity of topics, 95 subreddits are selected from which conversations are collected. In total, RCC contains 9.2 million 3-turn conversations.",2016,,,,,
2628,Reddit_Corpus,Conversational Response Selection,Conversational Response Selection,Conversational Response Selection,,,Methodology,conversational-response-selection-on-polyai,,https://github.com/PolyAI-LDN/conversational-datasets,https://paperswithcode.com/dataset/reddit-corpus,"Reddit Corpus is part of a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using '1-of-100 accuracy'. The Reddit Corpus contains 726 million multi-turn dialogues from the Reddit board.",,Henderson et al.,ttps://arxiv.org/pdf/1904.06472.pdf,,,
2629,Reddit_Engagement_Dataset,Dialogue Evaluation,Dialogue Evaluation,Dialogue Evaluation,,,Methodology,,MIT,https://github.com/gxxu-ml/EnDex,https://paperswithcode.com/dataset/reddit-engagement-dataset,"Reddit Engagement Dataset (RED), a distant-supervision set, with 80k single-turn conversations. RED is sourced from Reddit, sampling from 43 popular subreddits, and processed from a total of 5 million posts, filtering out data that was either non-conversational, toxic, or posts not possible to ascertain popularity.",,EnDex: Evaluation of Dialogue Engagingness at Scale,https://arxiv.org/pdf/2210.12362v1.pdf,,,
2630,Reddit_Ideological_and_Extreme_Bias_Dataset,News Generation,News Generation,"News Generation, News Classification, News Summarization, News Annotation, News Recommendation, Text Classification","Image, Text",English,Computer Vision,news-classification-on-reddit-ideological-and,CC BY 4.0,https://data.mendeley.com/datasets/2tdr9sjd83/3,https://paperswithcode.com/dataset/reddit-ideological-and-extreme-bias-dataset,"Articles originating from subreddits with explicitly stated ideologies are categorized into three groups: 72,488 articles in the Liberal class, 79,573 articles in the Conservative class, and 225,083 articles in the Restricted class. 

Conversely, articles from subreddits lacking a clearly defined ideology, including those with implicit or explicit ideologies, are merged to form a holdout dataset comprising 922,522 articles.

Part 1
https://data.mendeley.com/datasets/2tdr9sjd83/3
Part 2
https://data.mendeley.com/datasets/dxpp5983yb/1
Part 3
https://data.mendeley.com/datasets/f7knr8r94w/1",,,,,,
2631,Reddit_Posts_Related_To_Eating_Disorders_and_Dieti,text annotation,text annotation,text annotation,Text,English,Natural Language Processing,,CC0 1.0,https://doi.org/10.18130/V3/NENELT,https://paperswithcode.com/dataset/reddit-posts-related-to-eating-disorders-and,"This dataset comprises 77,175 Reddit posts from 115 subreddit forums, annotated for the presence of 15 topics related to eating disorders and dieting. The dataset includes labels and scores on all 77,175 Reddit posts, determined by 5 Large Language Models: GPT-4o, Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct, Mistral-7B-Instruct-v0.3, Vicuna-7b-v1.5, as well as by the ensemble of the four open-source LLMs. The dataset also includes a subset of 1,080 human-annotated posts for evaluation.",,,,,,
2632,ReDial,Text Generation,Text Generation,"Text Generation, Knowledge Graphs, Sentiment Analysis, Recommendation Systems",Text,English,Natural Language Processing,"text-generation-on-redial, recommendation-systems-on-redial",CC BY 4.0,https://redialdata.github.io/website/,https://paperswithcode.com/dataset/redial,"ReDial (Recommendation Dialogues) is an annotated dataset of dialogues, where users recommend movies to each other. The dataset consists of over 10,000 conversations centered around the theme of providing movie recommendations.",,,,,,
2633,REDS,Deblurring,Deblurring,"Deblurring, Joint Demosaicing and Denoising",,,Methodology,"joint-demosaicing-and-denoising-on-reds, deblurring-on-reds",,https://seungjunnah.github.io/Datasets/reds.html,https://paperswithcode.com/dataset/reds,"The realistic and dynamic scenes (REDS) dataset was proposed in the NTIRE19 Challenge. The dataset is composed of 300 video sequences with resolution of 720×1,280, and each video has 100 frames, where the training set, the validation set and the testing set have 240, 30 and 30 videos, respectively",,Video Super Resolution Based on Deep Learning: A comprehensive survey,https://arxiv.org/abs/2007.12928,,,
2634,ReDWeb-S,Object Detection,Object Detection,"Object Detection, RGB-D Salient Object Detection, Saliency Detection",Image,,Computer Vision,,,https://github.com/nnizhang/SMAC,https://paperswithcode.com/dataset/redweb-s,ReDWeb-S is a large-scale challenging dataset for Salient Object Detection. It has totally 3179 images with various real-world scenes and high-quality depth maps. The dataset is split into a training set with 2179 RGB-D image pairs and a testing set with the remaining 1000 image pairs.,,,,3179 images,,
2635,RefCOCO,Visual Grounding,Visual Grounding,"Visual Grounding, Semantic Segmentation, Referring Expression Comprehension, Visual Reasoning, Region Proposal, Referring Expression Segmentation, Zero-Shot Region Description",Image,English,Computer Vision,"referring-expression-segmentation-on-refcocog-1, referring-expression-segmentation-on-refcoco-3, referring-expression-segmentation-on-refcoco-4, zero-shot-region-description-on-refcocog-val, visual-grounding-on-refcoco-test-b, referring-expression-comprehension-on-1, referring-expression-segmentation-on-refcoco, referring-expression-comprehension-on-refcoco, referring-expression-segmentation-on-refcoco-9, referring-expression-segmentation-on-refcocog, referring-expression-comprehension-on, referring-expression-segmentation-on-refcoco-8, zero-shot-region-description-on-refcoco-testb, visual-grounding-on-refcoco-testa, zero-shot-region-description-on-refcoco-test, visual-grounding-on-refcoco-val, referring-expression-segmentation-on-refcoco-6, referring-expression-comprehension-on-refcoco-1, referring-expression-segmentation-on-refcoco-5, zero-shot-region-description-on-refcocog-test, visual-grounding-on-refcoco-testa-1",,https://github.com/lichengunc/refer,https://paperswithcode.com/dataset/refcoco,"The RefCOCO dataset is a referring expression generation (REG) dataset used for tasks related to understanding natural language expressions that refer to specific objects in images. Here are the key details about RefCOCO:



Collection Method:
The dataset was collected using the ReferitGame, a two-player game. In this game, the first player views an image with a segmented target object and writes a natural language expression referring to that object. The second player sees only the image and the referring expression and must click on the corresponding object. If both players perform correctly, they earn points and switch roles; otherwise, they receive a new object and image for description.



Dataset Variants:
RefCOCO: Contains 142,209 refer expressions for 50,000 objects across 19,994 images. RefCOCO+: Includes 141,564 expressions for 49,856 objects in 19,992 images. RefCOCOg: This variant has 25,799 images, 95,010 referring expressions, and 49,822 object instances.



Language and Restrictions:
RefCOCO allows any type of language in the referring expressions. RefCOCO+ disallows location words in expressions to focus purely on appearance-based descriptions (e.g., ""the man in the yellow polka-dotted shirt"") rather than viewer-dependent descriptions (e.g., ""the second man from the left"").



These datasets serve as valuable resources for tasks like referring expression segmentation, comprehension, and visual grounding in computer vision research.",,,,994 images,,
2636,Refer-YouTube-VOS,Referring Expression Segmentation,Referring Expression Segmentation,"Referring Expression Segmentation, Referring Video Object Segmentation","Image, Video",,Computer Vision,"referring-expression-segmentation-on-refer-1, referring-video-object-segmentation-on-refer, referring-expression-segmentation-on-refer",Creative Commons Attribution 4.0 License,https://youtube-vos.org/dataset/rvos/,https://paperswithcode.com/dataset/refer-youtube-vos,"There exist previous works [6, 10] that constructed referring segmentation datasets for videos. Gavrilyuk et al. [6] extended the A2D [33] and J-HMDB [9] datasets with natural sentences; the datasets focus on describing the ‘actors’ and ‘actions’ appearing in videos, therefore the instance annotations are limited to only a few object categories corresponding to the dominant ‘actors’ performing a salient ‘action’. Khoreva et al. [10] built a dataset based on DAVIS [25], but the scales are barely sufficient to learn an end-to-end model from scratch

Youtube-VOS has 4,519 high-resolution videos with 94 common object categories. Each video has pixel-level instance segmentation annotation at every 5 frames in 30-fps videos, and their durations are around 3 to 6 seconds.

We employed Amazon Mechanical Turk to annotate referring expressions. To ensure the quality of the annotations, we selected around 50 turkers after a validation test. Each turker was given a pair of videos, the original video and the mask-overlaid one with the target object highlighted, and was asked to provide a discriminative sentence within 20 words that describes the target object accurately. We collected two kinds of annotations, which describe the highlighted object (1) based on a whole video (Full-video expression) and (2) using only the
first frame of the video (First-frame expression). After the initial annotation, we conducted verification and cleaning jobs for all annotations, and dropped objects if an object cannot be localized using language expressions only. 

The followings are the statistics and analysis of the two annotation types of the dataset after the verification.

Full-video expression: Youtube-VOS has 6,459 and 1,063 unique objects in train and validation split, respectively. Among them, we cover 6,388 unique objects in 3,471 videos (6, 388/6, 459 = 98.9%) with 12,913 expressions in train split and 1,063 unique objects in 507 videos (1, 063/1, 063 = 100%) with 2,096 expressions in validation split. On average, each video has 3.8 language expressions and each expression has 10.0 words. 

First-frame expression: There are 6,006 unique objects in 3,412 videos (6, 006 /6, 459 = 93.0%) with 10,897 expressions in train split and 1,030 unique objects in 507 videos (1, 030/1, 063 = 96.9%) with 1,993 expressions in validation split. The number of annotated objects is lower than that of the full-video expressions because using only the first frame makes annotation more ambiguous and inconsistent and we dropped more annotations during the verification. On average,
each video has 3.2 language expressions and each expression has 7.5 words.",,,,,,
2637,ReferIt3D,3D dense captioning,3D dense captioning,"3D dense captioning, Natural Language Visual Grounding","3D, Image, Text",English,Computer Vision,,,https://referit3d.github.io/,https://paperswithcode.com/dataset/referit3d,"ReferIt3D provides two large-scale and complementary visio-linguistic datasets: i) Sr3D, which contains 83.5K template-based utterances leveraging spatial relations among fine-grained object classes to localize a referred object in a scene, and ii) Nr3D which contains 41.5K natural, free-form, utterances collected by deploying a 2-player object reference game in 3D scenes. This dataset can be used for 3D visual grounding and 3D dense captioning tasks.",,,,,,
2638,Referring_Expressions_for_DAVIS_2016___2017,Video Object Segmentation,Video Object Segmentation,"Video Object Segmentation, Referring Expression Segmentation, Semi-Supervised Video Object Segmentation, Unsupervised Video Object Segmentation","Image, Video",,Computer Vision,"referring-expression-segmentation-on-davis, unsupervised-video-object-segmentation-on-4, video-object-segmentation-on-davis-2017-val, referring-expression-segmentation-on-1, visual-object-tracking-on-davis-2017",,https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/video-segmentation/video-object-segmentation-with-language-referring-expressions,https://paperswithcode.com/dataset/referring-expressions-for-davis-2016-2017,"Our task is to localize and provide a pixel-level mask of an object on all video frames given a language referring expression obtained either by looking at the first frame only or the full video. To validate our approach we employ two popular video object segmentation datasets, DAVIS16 [38] and DAVIS17 [42]. These two datasets introduce various challenges, containing videos with single or multiple salient objects, crowded scenes, similar looking instances, occlusions, camera view changes, fast motion, etc.

DAVIS16 [38] consists of 30 training and 20 test videos of diverse object categories with all frames annotated with pixel-level accuracy. Note that in this dataset only a single object is annotated per video. For the multiple object video segmentation task we consider DAVIS17. Compared to DAVIS16, this is a more challenging dataset, with multiple objects annotated per video and more complex scenes with more distractors, occlusions, smaller objects, and fine structures. Overall, DAVIS17 consists of a training set with 60 videos, and a validation/test-dev/test-challenge set with 30 sequences each. 

As our goal is to segment objects in videos using language specifications, we augment all objects annotated with mask labels in DAVIS16 and DAVIS17 with non-ambiguous referring expressions. We follow the work of [34] and ask the annotator to provide a language description of the object, which has a mask annotation, by looking only at the
first frame of the video. Then another annotator is given the first frame and the corresponding description, and asked to identify the referred object. If the annotator is unable to correctly identify the object, the description is corrected to remove ambiguity and to specify the object uniquely. We have collected two referring expressions per target
object annotated by non-computer vision experts (Annotator 1, 2).

However, by looking only at the 1st frame, the obtained referring expressions may potentially be invalid for an entire video. (We actually quantified that only∼ 15% of the
collected descriptions become invalid over time and it does not affect strongly segmentation results as temporal consistency step helps to disambiguate some of such cases, see the supp. material for details.) Besides, in many applications, such as video editing or video-based advertisement, the user has access to a full video. Providing a language
query which is valid for all frames might decrease the editing time and result in more coherent predictions. Thus, on DAVIS17 we asked the workers to provide a description of the object by looking at the full video. We have collected one expression of the full video type per target object. Future work may choose to use either setting.

The average length for the first frame/full video expressions is 5.5/6.3 words. For DAVIS17 first frame annotations we notice that descriptions given by Annotator 1 are longer than the ones by Annotator 2 (6.4 vs. 4.6 words). We evaluate the effect of description length on the grounding performance in §5. Besides, the expressions relevant to a full video mention verbs more often than the first frame descriptions (44% vs. 25%). This is intuitive, as referring to an object which changes its appearance and position over time may require mentioning its actions. Adjectives are present in over 50% for all annotations. Most of them refer to colors (over 70%), shapes and sizes (7%) and spatial/ordering words (6% first frame vs. 13% full video expressions). The full video expressions also have a higher number of adverbs and prepositions, and overall are more complex than the ones provided for the first frame.

Overall augmented DAVIS16/17 contains ∼ 1.2k referring expressions for more than 400 objects on 150 videos with ∼ 10k frames. We believe the collected data will be
of interest to segmentation as well as vision and language communities, providing an opportunity to explore language as alternative input for video object segmentation.",,,,,,
2639,REFLACX,Medical Diagnosis,Medical Diagnosis,Medical Diagnosis,,,Medical,,PhysioNet Restricted Health Data License 1.5.0,https://physionet.org/content/reflacx-xray-localization/1.0.0/,https://paperswithcode.com/dataset/reflacx,"The REFLACX dataset contains eye-tracking data for 3,032 readings of chest x-rays by five radiologists. The dictated reports were transcribed and have timestamps synchronized with the eye-tracking data. 

Localization labels for abnormalities are very costly, and the collection of eye-tracking data and reports for implicit localization labels may be an alternative for scaling up data collection. One of the potential uses for these data is in additional supervision for training computer vision models. For more details, check the Physionet page and the dataset description paper (""REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest x-rays"").",,,,,,
2640,REFreSD,Learning-To-Rank,Learning-To-Rank,Learning-To-Rank,,,Methodology,,,https://github.com/Elbria/xling-SemDiv/tree/master/REFreSD,https://paperswithcode.com/dataset/refresd,Consists of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales.,,,,,,
2641,REFUGE_Challenge,Fovea Detection,Fovea Detection,"Fovea Detection, Semantic Segmentation, Optic Cup Segmentation, Optic Disc Detection, Optic Disc Segmentation, Domain Adaptation, Optic Cup Detection",Image,,Computer Vision,"optic-cup-segmentation-on-refuge-challenge, optic-disc-segmentation-on-refuge-challenge, optic-cup-detection-on-refuge-challenge, fovea-detection-on-refuge-challenge, optic-disc-detection-on-refuge-challenge",,https://refuge.grand-challenge.org/Home2020/,https://paperswithcode.com/dataset/refuge-challenge,"REFUGE Challenge provides a data set of 1200 fundus images with ground truth segmentations and clinical glaucoma labels, currently the largest existing one.",,REFUGE Challenge: A Unified Framework for Evaluating Automated Methods for Glaucoma Assessment from Fundus Photographs,https://arxiv.org/pdf/1910.03667v1.pdf,,,
2642,RegDB-C,Cross-Modal  Person Re-Identification,Cross-Modal  Person Re-Identification,"Cross-Modal  Person Re-Identification, Cross-Modal Person Re-Identification, Person Re-Identification, Generalizable Person Re-identification",Image,,Computer Vision,cross-modal-person-re-identification-on-regdb-1,,https://github.com/MinghuiChen43/CIL-ReID,https://paperswithcode.com/dataset/regdb-c,"RegDB-C is an evaluation set that consists of algorithmically generated corruptions applied to the RegDB test-set (color images). These corruptions consist of Noise: Gaussian, shot, impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions.",,,,,,
2643,Regional_AQ_Datasets,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, regression",Time Series,,Time Series,,MIT,https://zenodo.org/records/11220965,https://paperswithcode.com/dataset/regional-aq-datasets,"The primary environmental health threat in the WHO European Region is air pollution, impacting the daily health and well-being of its citizens significantly. To effectively understand the impact, and dynamics of air quality a detailed investigation of different environmental, weather, and land cover indices is appropriate. To this end, this paper introduces three European cities’ spatiotemporal datasets, customized for air pollution monitoring at a regional level. The datasets are composed of major air quality, weather measurements and land use information. The duration is approximately from 2020 to 2023 with an hourly temporal resolution and a spatial resolution of 0.005°. The temporal and spatiotemporal datasets are publicly released aiming to provide a solid foundation for researchers, analysts, and practitioners to conduct in-depth analyses of air pollution dynamics.",2020,,,,,
2644,Reglamento_Aeronautico_Colombiano_2024,Text Generation,Text Generation,"Text Generation, legal outcome extraction",Text,English,Natural Language Processing,,Apache-2.0,https://huggingface.co/datasets/somosnlp/Reglamento_Aeronautico_Colombiano_2024,https://paperswithcode.com/dataset/reglamento-aeronautico-colombiano-2024,"Dataset Details
Total Labeled: 100%

Labeled and Curated: 24,478

Pending: 0

Drafts: 0

Discarded: 696

High-Level Explanation
This dataset includes labeled samples from the Colombian Aeronautical Regulations (RAC), covering all chapters comprehensively.

Motivations and Summary
The dataset aims to provide a structured resource for analyzing and applying RAC. After thorough curation and labeling, it is 100% complete with 25,174 samples ready for use.

Potential Use Cases
Regulatory Compliance: Ensuring operations align with RAC.
Research and Development: Studying regulatory trends and identifying gaps.
Educational Purposes: Learning tool for aeronautical regulations.
Policy Making: Assisting in drafting or amending regulations.",,,,174 samples,,
2645,Relative_Human,3D Depth Estimation,3D Depth Estimation,"3D Depth Estimation, 3D Multi-Person Pose Estimation (absolute), 3D Multi-Person Mesh Recovery, 2D Human Pose Estimation, 3D Absolute Human Pose Estimation","3D, Image",,Computer Vision,"3d-depth-estimation-on-relative-human, 3d-multi-person-mesh-recovery-on-relative",,https://github.com/Arthur151/Relative_Human,https://paperswithcode.com/dataset/relative-human,"Relative Human (RH) contains multi-person in-the-wild RGB images with rich human annotations, including:

Depth layers: relative depth relationship/ordering between all people in the image.
Age group classfication: adults, teenagers, kids, babies.
Others: Genders, Bounding box, 2D pose.",,,,,,
2646,RELLIS-3D,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Scene Understanding, 3D Semantic Segmentation, Autonomous Navigation","3D, Image",,Computer Vision,"semantic-segmentation-on-rellis-3d-dataset, 3d-semantic-segmentation-on-rellis-3d-dataset",,https://github.com/unmannedlab/RELLIS-3D,https://paperswithcode.com/dataset/rellis-3d,"RELLIS-3D is a multi-modal dataset for off-road robotics. It was collected in an off-road environment containing annotations for 13,556 LiDAR scans and 6,235 images. The data was collected on the Rellis Campus of Texas A&M University and presents challenges to existing algorithms related to class imbalance and environmental topography. The dataset also provides full-stack sensor data in ROS bag format, including RGB camera images, LiDAR point clouds, a pair of stereo images, high-precision GPS measurement, and IMU data.",,,,235 images,,
2647,RELX,Relation Classification,Relation Classification,"Relation Classification, Question Answering","Graph, Image, Text",English,Computer Vision,,,https://github.com/boun-tabi/RELX,https://paperswithcode.com/dataset/relx,"RELX is a benchmark dataset for cross-lingual relation classification in English, French, German, Spanish and Turkish.",,,,,,
2648,ReMASC,Voice Anti-spoofing,Voice Anti-spoofing,Voice Anti-spoofing,Audio,,Audio,,,https://github.com/YuanGongND/ReMASC,https://paperswithcode.com/dataset/remasc,"We introduce a new database of voice recordings with the goal of supporting research on vulnerabilities and protection of voice-controlled systems. In contrast to prior efforts, the proposed database contains genuine and replayed recordings of voice commands obtained in realistic usage scenarios and using state-of-the-art voice assistant development kits. Specifically, the database contains recordings from four systems (each with a different microphone array) in a variety of environmental conditions with different forms of background noise and relative positions between speaker and device. To the best of our knowledge, this is the first database that has been specifically designed for the protection of voice controlled systems (VCS) against various forms of replay attacks.",,,,,,
2649,RemFX,Audio Signal Processing,Audio Signal Processing,Audio Signal Processing,Audio,,Audio,,CC BY-NC 2.0,https://zenodo.org/record/8187288,https://paperswithcode.com/dataset/remfx,"Audio samples processed with sound effects, to evaluate effect removal models. The audio effects applied are from the set (Distortion, Delay, Dynamic Range Compressor, Phasor, Reverb) and randomly sampled without replacement for each example; the targets are the original audio.

The audio samples are source from VocalSet, GuitarSet, DSD100, and IDMT-SMT-Drums.",,,,,,
2650,Rendered_WB_dataset,Color Constancy,Color Constancy,"Color Constancy, Deblurring, Image Dehazing",Image,,Computer Vision,,,http://cvil.eecs.yorku.ca/projects/public_html/sRGB_WB_correction/dataset.html,https://paperswithcode.com/dataset/rendered-wb-dataset,"A dataset of over 65,000 pairs of incorrectly white-balanced images and their corresponding correctly white-balanced images.",,,,,,
2651,RENOIR,Color Image Denoising,Color Image Denoising,"Color Image Denoising, Image Denoising",Image,,Computer Vision,color-image-denoising-on-renoir,Free,https://adrianbarburesearch.blogspot.com/p/renoir-dataset.html,https://paperswithcode.com/dataset/renoir,"A dataset of color images corrupted by natural noise due to low-light conditions, together with spatially and intensity-aligned low noise images of the same scenes.",,,,,,
2652,Rent3D,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Room Layout Estimation, 3D Room Layouts From A Single RGB Panorama","3D, Image",,Computer Vision,,,http://www.cs.toronto.edu/~fidler/projects/rent3D.html,https://paperswithcode.com/dataset/rent3d%0A,A dataset which contains over 200 apartments.,,,,,,
2653,RePAIR_Dataset,Spatial Reasoning,Spatial Reasoning,"Spatial Reasoning, 3D Assembly",3D,,Reasoning,,Custom,https://repairproject.github.io/RePAIR_dataset/,https://paperswithcode.com/dataset/repair,"Our dataset consists of over 1000 fractured frescoes. The RePAIR stands as a realistic computational challenge for methods for 2D and 3D puzzle solving, and serves as a benchmark that enables the study of fractured object reassembly and presents new challenges for geometric shape understanding. Please visit our website for more dataset information, access to source code scripts and for an interactive gallery viewing of the dataset samples.",,,,,,
2654,Replay-Attack,Face Anti-Spoofing,Face Anti-Spoofing,Face Anti-Spoofing,Image,,Computer Vision,face-anti-spoofing-on-replay-attack,,https://www.idiap.ch/dataset/replayattack,https://paperswithcode.com/dataset/replay-attack,"The Replay-Attack Database for face spoofing consists of 1300 video clips of photo and video attack attempts to 50 clients, under different lighting conditions. All videos are generated by either having a (real) client trying to access a laptop through a built-in webcam or by displaying a photo or a video recording of the same client for at least 9 seconds.",,,,,,
2655,Replay-Mobile,Face Anti-Spoofing,Face Anti-Spoofing,"Face Anti-Spoofing, Face Presentation Attack Detection, Face Recognition",Image,,Computer Vision,,"Custom (research-only, non-commercial)",https://www.idiap.ch/dataset/replay-mobile,https://paperswithcode.com/dataset/replay-mobile-1,"The Replay-Mobile Database for face spoofing consists of 1190 video clips of photo and video attack attempts to 40 clients, under different lighting conditions. These videos were recorded with current devices from the market -- an iPad Mini2 (running iOS) and a LG-G4 smartphone (running Android). This Database was produced at the Idiap Research Institute (Switzerland) within the framework of collaboration with Galician Research and Development Center in Advanced Telecommunications - Gradiant (Spain).",,https://core.ac.uk/download/pdf/148024307.pdf,https://core.ac.uk/download/pdf/148024307.pdf,,,
2656,Replica,Visual Navigation,Visual Navigation,"Visual Navigation, 3D Open-Vocabulary Instance Segmentation, Semantic Segmentation, Scene Generation, Efficient Exploration, Image Generation, Domain Adaptation","3D, Image, Text",English,Computer Vision,"image-generation-on-replica, 3d-open-vocabulary-instance-segmentation-on-1, scene-generation-on-replica, semantic-segmentation-on-replica",Custom,https://github.com/facebookresearch/Replica-Dataset,https://paperswithcode.com/dataset/replica,"The Replica Dataset is a dataset of high quality reconstructions of a variety of indoor spaces. Each reconstruction has clean dense geometry, high resolution and high dynamic range textures, glass and mirror surface information, planar segmentation as well as semantic class and instance segmentation.",,,,,,
2657,Replication_Data_for__Investigating_the_concentrat,Survival Analysis,Survival Analysis,"Survival Analysis, Multi-target regression",,,Methodology,,CC0 1.0,https://doi.org/10.7910/DVN/BLGH0A,https://paperswithcode.com/dataset/replication-data-for-investigating-the,The dataset provides information about 450 HYIPs collected between November 2020 and September 2021. This dataset was analyzed and the results are discussed in the paper.,2020,,,,,
2658,Replication_Data_for__Online_Learning_with_Optimis,Ensemble Learning,Ensemble Learning,"Ensemble Learning, Weather Forecasting",Time Series,,Methodology,,,https://doi.org/10.7910/DVN/IOCFCY,https://paperswithcode.com/dataset/replication-data-for-online-learning-with,"The model forecasts for the sub-seasonal forecasting application considered in the Online Learning under Optimism and Delay paper experiments. This dataset consists of a single ZIP archive (919MB) that contains 1) a ""models"" folder that contains, for each model the forecasts for the Precip. 3-4w, Precip. 5-6w, Temp. 3-4w, Temp. 5-6w tasks on the western United States geography, and 2) a ""data"" folder that contains supporting geographic data. The data should be used to reproduce the PoolD experiments in https://github.com/geflaspohler/poold as described in the README. (2021-06-10)",2021,,,,,
2659,Replication_Data_for__Singapore_Soundscape_Site_Se,Unsupervised Spatial Clustering,Unsupervised Spatial Clustering,Unsupervised Spatial Clustering,,,Methodology,,CC BY-NC-ND,https://doi.org/10.21979/N9/BBBPMO,https://paperswithcode.com/dataset/replication-data-for-singapore-soundscape,"This dataset contains the data used for all statistical analysis in our publication ""Singapore Soundscape Site Selection Survey (S5): Identification of Characteristic Soundscapes of Singapore via Weighted k-means Clustering"", summarised in a single .csv file.

For more details on the study methodology, please refer to our manuscript:

Ooi, K.; Lam, B.; Hong, J.; Watcharasupat, K. N.; Ong, Z.-T.; Gan, W.-S. Singapore Soundscape Site Selection Survey (S5): Identification of Characteristic Soundscapes of Singapore via Weighted k-means Clustering. Sustainability, 2022.

For our replication code utilising this data, please refer to our Github repository: https://github.com/ntudsp/singapore-soundscape-site-selection-survey

A short explanation of the columns in the .csv file is as follows:
Full of life & exciting [Latitude]: The latitude, in degrees, of the location chosen by the participant as ""Full of life & exciting"".
Full of life & exciting [Longitude]: The longitude, in degrees, of the location chosen by the participant as ""Full of life & exciting"".
Full of life & exciting [# times visited]: The number of times that the participant had visited the chosen location they considered ""Full of life & exciting"" before, as reported by the participant.
Full of life & exciting [Duration]: The average duration per visit to the chosen location the participant considered ""Full of life & exciting"", as reported by the participant.
Chaotic & restless [Latitude]: The latitude, in degrees, of the location chosen by the participant as ""Chaotic & restless"".
Chaotic & restless [Longitude]: The longitude, in degrees, of the location chosen by the participant as ""Chaotic & restless"".
Chaotic & restless [# times visited]: The number of times that the participant had visited the chosen location they considered ""Chaotic & restless"" before, as reported by the participant.
Chaotic & restless [Duration]: The average duration per visit to the chosen location the participant considered ""Chaotic & restless"", as reported by the participant.
Calm & tranquil [Latitude]: The latitude, in degrees, of the location chosen by the participant as ""Calm & tranquil"".
Calm & tranquil [Longitude]: The longitude, in degrees, of the location chosen by the participant as ""Calm & tranquil"".
Calm & tranquil [# times visited]: The number of times that the participant had visited the chosen location they considered ""Calm & tranquil"" before, as reported by the participant.
Calm & tranquil [Duration]: The average duration per visit to the chosen location the participant considered ""Calm & tranquil"", as reported by the participant.
Boring & lifeless [Latitude]: The latitude, in degrees, of the location chosen by the participant as ""Boring & lifeless"".
Boring & lifeless [Longitude]: The longitude, in degrees, of the location chosen by the participant as ""Boring & lifeless"".
Boring & lifeless [# times visited]: The number of times that the participant had visited the chosen location they considered ""Boring & lifeless"" before, as reported by the participant.
Boring & lifeless [Duration]: The average duration per visit to the chosen location the participant considered ""Boring & lifeless"", as reported by the participant.",2022,,,,,
2660,Representative_PDE_Benchmarks,PDE Surrogate Modeling,PDE Surrogate Modeling,PDE Surrogate Modeling,,,Methodology,,,,https://paperswithcode.com/dataset/representative-pde-benchmarks,"Given the lack of consensus on a standard setof benchmarks for machine learning of PDEs, we propose a new suite of benchmarks here. Our aims in this regard are to ensure 
i) sufficient diversity among the types of PDE considered
ii) access to training and test data is readily available for rapid prototyping and reproducibility
iii) intrinsic computational complexity of problem to make sure that it is worthwhile to
design fast surrogates to classical PDE solvers for a particular problem",,,,,,
2661,ReQA,Learning-To-Rank,Learning-To-Rank,"Learning-To-Rank, Question Answering, Information Retrieval",Text,English,Natural Language Processing,,,https://github.com/google/retrieval-qa-eval,https://paperswithcode.com/dataset/reqa,Retrieval Question-Answering (ReQA) benchmark tests a model’s ability to retrieve relevant answers efficiently from a large set of documents.,,ReQA: An Evaluation for End-to-End Answer Retrieval Models,https://arxiv.org/pdf/1907.04780.pdf,,,
2662,RES-Q,Text-to-Code Generation,Text-to-Code Generation,"Text-to-Code Generation, Library-Oriented Code Generation, Code Generation, Code Search",Text,English,Natural Language Processing,code-generation-on-res-q,MIT,https://github.com/Qurrent-AI/RES-Q,https://paperswithcode.com/dataset/res-q,"RES-Q is a natural language instruction-based benchmark for evaluating $\textbf{R}$epository $\textbf{E}$diting $\textbf{S}$ystems, which consists of 100 handcrafted repository editing tasks derived from real GitHub commits. Given an edit instruction and a code repository, RES-Q evaluates an LLM system’s ability to interpret edit instructions, gather information, and construct appropriate edits to the repository.",,,,,,
2663,RESD,Text Classification,Text Classification,"Text Classification, Speech Emotion Recognition, Speech Recognition, Multimodal Emotion Recognition","Audio, Image, Text",English,Computer Vision,speech-emotion-recognition-on-resd,MIT,https://huggingface.co/datasets/Aniemore/resd_annotated,https://paperswithcode.com/dataset/resd,"Russian dataset of emotional speech dialogues. This dataset was assembled from ~3.5 hours of live speech by actors who voiced pre-distributed emotions in the dialogue for ~3 minutes each. <br>
Each sample of dataset contains name of part from the original dataset studio source, speech file (16000 or 44100Hz) of human voice, 1 of 7 labeled emotions and the speech-to-texted part of voice speech. <br>

Emotions are represented in 7 states: anger, disgust, fear, enthusiasm, happiness, neutral and sadness.

This dataset was created by Artem Amentes, Nikita Davidchuk and Ilya Lubenets

@misc{Aniemore,
  author = {Артем Аментес, Илья Лубенец, Никита Давидчук},
  title = {Открытая библиотека искусственного интеллекта для анализа и выявления эмоциональных оттенков речи человека},
  year = {2022},
  publisher = {Hugging Face},
  journal = {Hugging Face Hub},
  howpublished = {\url{https://huggingface.com/aniemore/Aniemore}},
  email = {hello@socialcode.ru}
}",2022,,,,,
2664,Restaurant-ACOS,Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),"Aspect-Based Sentiment Analysis (ABSA), Sentiment Analysis, Aspect-Category-Opinion-Sentiment Quadruple Extraction",Text,English,Natural Language Processing,aspect-category-opinion-sentiment-quadruple,,https://github.com/NUSTM/ACOS/tree/main/data/Restaurant-ACOS,https://paperswithcode.com/dataset/restaurant-acos,"The Restaurant-ACOS dataset is constructed based on the SemEval 2016 Restaurant dataset (Pontiki et al., 2016) and its expansion datasets (Fan et al., 2019; Xu et al., 2020).
The SemEval 2016 Restaurant dataset (Pontiki et al., 2016) was annotated with explicit and implicit aspects, categories, and sentiment. (Fan et al., 2019; Xu et al., 2020) further added the opinion annotations. We integrate their annotations to construct aspect-category-opinion-sentiment quadruples and further annotate the implicit opinions. The Restaurant-ACOS dataset contains 2286 sentences with 3658 quadruples.
It is worth noting that the Restaurant-ACOS is available for all subtasks in ABSA, including aspect-based sentiment classification, aspect-sentiment pair extraction, aspect-opinion pair extraction, aspect-opinion sentiment triple extraction, aspect-category-sentiment triple extraction, etc.",2016,,,2286 sentences,,
2665,Resume_NER,Chinese Named Entity Recognition,Chinese Named Entity Recognition,Chinese Named Entity Recognition,"Image, Text",English,Computer Vision,chinese-named-entity-recognition-on-resume,,https://arxiv.org/pdf/1805.02023.pdf,https://paperswithcode.com/dataset/resume-ner,Resume contains eight fine-grained entity categories -score from 74.5% to 86.88%.,,Query-Based Named Entity Recognition,https://arxiv.org/abs/1908.09138,,,
2666,Retailrocket,Session-Based Recommendations,Session-Based Recommendations,Session-Based Recommendations,,,Methodology,session-based-recommendations-on-retailrocket,,https://www.kaggle.com/retailrocket/ecommerce-dataset,https://paperswithcode.com/dataset/retailrocket,"The dataset consists of three files: a file with behaviour data (events.csv), a file with item properties (itemproperties.сsv) and a file, which describes category tree (categorytree.сsv). The data has been collected from a real-world ecommerce website. It is raw data, i.e. without any content transformations, however, all values are hashed due to confidential issues. The purpose of publishing is to motivate researches in the field of recommender systems with implicit feedback.",,,,,,
2667,Retinal-Lesions,Lesion Classification,Lesion Classification,"Lesion Classification, Lesion Segmentation, Diabetic Retinopathy Grading",Image,,Computer Vision,,,https://github.com/WeiQijie/retinal-lesions,https://paperswithcode.com/dataset/retinal-lesions,"Over 1.5K images selected from the public Kaggle DR Detection dataset;
Five DR grades (DR0 / DR1 / DR2 / DR3 / DR4), re-labeled by a panel of 45 experienced ophthalmologists;
Eight retinal lesion classes, including microaneurysm, intraretinal hemorrhage, hard exudate, cotton-wool spot, vitreous hemorrhage, preretinal hemorrhage, neovascularization and fibrous proliferation;
Over 34K expert-labeled pixel-level lesion segments;
Multi-task, i.e., lesion segmentation, lesion classification, and DR grading.",,,,5K images,,
2668,Retinal_Fundus_MultiDisease_Image_Dataset__RFMiD_,Transfer Learning,Transfer Learning,Transfer Learning,,,Methodology,transfer-learning-on-retinal-fundus,Creative Commons Attribution,https://ieee-dataport.org/open-access/retinal-fundus-multi-disease-image-dataset-rfmid,https://paperswithcode.com/dataset/retinal-fundus-multidisease-image-dataset,"According to the WHO,  World report on vision 2019, the number of visually impaired people worldwide is estimated to be 2.2 billion, of whom at least 1 billion have a vision impairment that could have been prevented or is yet to be addressed. The world faces considerable challenges in terms of eye care, including inequalities in the coverage and quality of prevention, treatment, and rehabilitation services. Early detection and diagnosis of ocular pathologies would enable forestall of visual impairment. One challenge that limits the adoption of a computer-aided diagnosis tool by the ophthalmologist is, the sight-threatening rare pathologies such as central retinal artery occlusion or anterior ischemic optic neuropathy and others are usually ignored. In the past two decades, many publicly available datasets of color fundus images have been collected with a primary focus on diabetic retinopathy, glaucoma, and age-related macular degeneration, and few other frequent pathologies. The challenge for which this dataset was introduced aimed  to unite the medical image analysis community to develop methods for automatic ocular disease classification of frequent diseases along with the rare pathologies. The Retinal Fundus Multi-disease Image Dataset (RFMiD) consists of a total of 3200 fundus images captured using three different fundus cameras with 46 conditions annotated through adjudicated consensus of two senior retinal experts. To the best of the authors knowledge, the dataset, RFMiD represents the only publicly available dataset that constitutes such a wide variety of diseases that appear in routine clinical settings. This aforementioned challenge promoted the development of generalizable models for screening retina, unlike the previous efforts that focused on the detection of specific diseases.",2019,,,,,
2669,Retina_Benchmark,Diabetic Retinopathy Detection,Diabetic Retinopathy Detection,Diabetic Retinopathy Detection,Image,,Computer Vision,,,https://github.com/google/uncertainty-baselines/tree/main/baselines/diabetic_retinopathy_detection,https://paperswithcode.com/dataset/retina-benchmark,"The Retina Benchmark is a set of real-world tasks that accurately reflect such complexities and are designed to assess the reliability of predictive models in safety-critical scenarios. Specifically, two publicly available datasets of high-resolution human retina images exhibiting varying degrees of diabetic retinopathy, a medical condition that can lead to blindness, are used to design a suite of automated diagnosis tasks that require reliable predictive uncertainty quantification.",,Benchmarking Bayesian Deep Learning on Diabetic Retinopathy Detection Tasks,https://arxiv.org/pdf/2211.12717v1.pdf,,,
2670,RETOUCH,Retinal OCT Disease Classification,Retinal OCT Disease Classification,Retinal OCT Disease Classification,Image,,Computer Vision,,,https://retouch.grand-challenge.org/,https://paperswithcode.com/dataset/retouch,"The goal of the challenge is to compare automated algorithms that are able to detect and segment various types of fluids on a common dataset of optical coherence tomography (OCT) volumes representing different retinal diseases, acquired with devices from different manufacturers. We made available a dataset of OCT volumes containing a wide variety of retinal fluid lesions with accompanying reference annotations. We invite the medical imaging community to participate by developing and testing existing and novel automated retinal OCT segmentation methods.",,,,,,
2671,RETWEET,Sentiment Analysis,Sentiment Analysis,"Sentiment Analysis, Twitter Sentiment Analysis, Tweet-Reply Sentiment Analysis, Multi-Domain Sentiment Classification, Point Processes","Image, Text",English,Computer Vision,"point-processes-on-retweet, tweet-reply-sentiment-analysis-on-retweet",CC BY 4.0,https://www.kaggle.com/soroosharasteh/retweet,https://paperswithcode.com/dataset/retweet,"RETWEET is a dataset of tweets and overall predominant sentiment of their replies.

SUMMARY
WHAT: Message-level Polarity Classification.

GOAL: To predict the predominant sentiment among (potential) first-order replies to a given tweet.

IDEA: Mitigate the problem of lacking labeled training data wi treating the unsupervised nature of the problem as a supervised learning case.

APPROACH:

Train a tweet classifier. 
Automatically label the replies using the classifier trained in the first part.
Choose a final label representing the general predominant sentiment of the replies of every tweet.

DATA COLLECTION
To download all of the replies to a tweet, the Search API should be used. However, the Search API is limited to 75000 requests per hour, which causes the mining and downloading process to be slow.
Furthermore, using the Twitter API, there is no possibility of downloading absolute random data. Therefore, we try to make the procedure as random as possible by utilizing two different strategies for data downloading and using them in an intermixed manner.



Our first strategy is based on a sample of English tweets obtained by filtering the Twitter stream via a list of cultural keywords. This list consists of 147 words that are deemed to play a ""pivotal role in discussions of culture and society"", covering diverse words such as aesthetics, environment, feminism, power, tourism, or youth. We extracted all tweets in 2019 that have a minimum of 20 first-order replies in the dataset. The data come with an obvious caveat: Both the source tweet as well as all the replies must contain at least one word from the list of keywords. Therewith, it is highly unlikely that the list of replies for any given source is exhaustive, i.e. there might be many more first-order replies to the source tweet that are not in the dataset.



As our second approach, we use the GetOldTweets3 library to download all the replies corresponding to every tweet. We define few restrictions to add randomization to the process. Firstly, every tweet and also every reply should contain at least 20 strings. This is due to the fact that our automatic tweet classifier, explsined in the paper, is optimized based on the message-level classification paradigm. Therefore, it operates optimal when the input contains at least a sufficient number of words. The second constraint is that every tweet should contain at least 20 first-order replies. In order to increase randomness, in this strategy, instead of referencing to a list of keywords, we manually choose some keywords, which are most likely to include long discussions, such as Coronavirus and football or the ones, which are most likely to include strong opinions such as birthday, war, or racism in order to account for the easy-to-guess examples. 



MANUAL ANNOTATIONS FOR THE RETWEET (TEST GOLD DATASET)
5,015 tweets with their corresponding replies, collected as a combination of the two different collection strategies, were given to three different students. Each of them had to read all the replies corresponding to every tweet, without observing the original tweet in order to avoid having a prior knowledge, and decide on ONE final sentiment for the replies. The assigned sentiment can only be one of the positive, negative, or neutral labels.

Considering the fact that this is a really challenging task for the machine, to prevent human mistakes, we correlated the results of the three annotators and only chose the tweets, in which all of the annotators had the same opinion on the labels, as the final gold standard test data. Therefore, we finally, ended up with a test set consisting of 1,519 human labeled tweets, with the labels being the sentiment of the replies of a tweet and not the tweet itself. 

DATASET CONTENTS
1. Training raw dataset: 34,953 unique tweets in total and individual automatic labels for all of their corresponding replies (1,519,504 total replies). Including,


./RETWEET_data/train_reply_labels_set1.txt
./RETWEET_data/train_reply_labels_set2.txt

2. Training autamtically-labeled dataset: 34,953 unique tweets and ONE final automatic label (chosen based on the algorithm 1 of our paper) for every tweet. Including,


./RETWEET_data/train_final_label.txt

3. Gold standard test dataset (RETWEET): 1,519 unique tweets with their manual labels for replies. ONE final label, which states the predominant overall polarity of all its replies, is assigned to every tweet. Including,


./RETWEET_data/test_gold.txt

NOTES


Please note that by downloading the Twitter data you agree to abide by the Twitter terms of service, and in particular you agree not to redistribute the data and to delete tweets that are marked deleted in the future.



The ""neutral"" label in the annotations stands for objective or neutral.



The distribution consists of a set of Twitter unique tweet IDs with annotations (overall polarity of replies). As for data privacy, the texts of the tweets and replies are not distributed. But as all the utilized resources in this dataset are taken from public tweets, having the tweet unique IDs, you can download the tweet and its replies.
You can use the Semeval Twitter data downloading script to obtain the corresponding tweets:  

https://github.com/seirasto/twitter_download/



The dataset URL:

https://kaggle.com/soroosharasteh/retweet/ 



LICENSE
The accompanying dataset is released under a Creative Commons Attribution 4.0 International License.

SOURCE CODE
The official source code of the paper: https://github.com/starasteh/retweet

In case you use this dataset, please cite the original paper:
S. Tayebi Arasteh, M. Monajem, V. Christlein, P. Heinrich, A. Nicolaou, H.N. Boldaji, M. Lotfinia,  S. Evert. ""How Will Your Tweet Be Received? Predicting the Sentiment Polarity of Tweet Replies"". Proceedings of the 2021 IEEE 15th International Conference on Semantic Computing (ICSC), Laguna Hills, CA, USA, January 2021.

BibTex
@inproceedings{RETWEET,
  title = ""How Will Your Tweet Be Received? Predicting the Sentiment Polarity of Tweet Replies"",
  author = ""Tayebi Arasteh, Soroosh and Monajem, Mehrpad and Christlein, Vincent and
  Heinrich, Philipp and Nicolaou, Anguelos and Naderi Boldaji, Hamidreza and Lotfinia, Mahshad and Evert, Stefan"",
  booktitle = ""Proceedings of the 2021 IEEE 15th International Conference on Semantic Computing (ICSC)"",
  address = ""Laguna Hills, CA, USA"",
  pages = ""370-373"",
  doi = ""10.1109/ICSC50631.2021.00068"",
  url = ""https://ieeexplore.ieee.org/document/9364527/"",
  month = ""01"",       
  year = ""2021""
  }


Dataset DOI: 10.34740/kaggle/ds/736988
Paper: https://ieeexplore.ieee.org/document/9364527
Paper DOI: 10.1109/ICSC50631.2021.00068

CONTACT
E-mail: soroosh.arasteh@fau.de

DATA FORMAT FOR ALL THE FILES
label TAB id

where, ""label"" can be positive, neutral or negative, corresponding to the overall message-level polarity of the replies of the tweet and ""id"" corresponds to the Twitter unique ID for the tweets.",2019,,,,"traint is that every tweet should contain at least 20 first-order replies. In order to increase randomness, in this strategy, instead of referencing to a list of keywords, we manually choose some keywords, which are most likely to include long discussions, such as Coronavirus and football or the ones, which are most likely to include strong opinions such as birthday, war, or racism in order to account for the easy-to-guess examples",
2672,Retweet_MTPP,Point Processes,Point Processes,Point Processes,,,Methodology,point-processes-on-retweet-mtpp,Apache-2.0,https://huggingface.co/datasets/easytpp/retweet,https://paperswithcode.com/dataset/retweet-mtpp,"This dataset contains time-stamped user retweet event sequences. The events are categorized into 3 types: retweets by “small,” “medium” and “large” users. Small users have fewer than 120 followers, medium users have fewer than 1363, and the rest are large users.",,,,,,
2673,Reuters-21578,Supervised Text Retrieval,Supervised Text Retrieval,"Supervised Text Retrieval, Text Retrieval, Document Classification, Multi-Modal Document Classification, Multi-Label Text Classification, Unsupervised Anomaly Detection","Image, Text",English,Computer Vision,"document-classification-on-reuters-21578, multi-modal-document-classification-on-1, text-retrieval-on-reuters-21578, supervised-text-retrieval-on-reuters-21578, unsupervised-anomaly-detection-on-reuters-1, multi-label-text-classification-on-reuters-1","Custom (research-only, attribution)",http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html,https://paperswithcode.com/dataset/reuters-21578,"The Reuters-21578 dataset is a collection of documents with news articles. The original corpus has 10,369 documents and a vocabulary of 29,930 words.",,Topic Model Based Multi-Label Classification from the Crowd,https://arxiv.org/abs/1604.00783,369 documents,,
2674,Reverb-WSJ0,Speech Dereverberation,Speech Dereverberation,Speech Dereverberation,Audio,,Speech,,Linguistic Data Consortium,https://github.com/sp-uhh/storm,https://paperswithcode.com/dataset/reverb-wsj0,"Noiseless reverberant dataset using the public WSJ0 corpus and simulated room impulse responses using the PyRoomAcoustics library.
Used in:
- Speech Enhancement and Dereverberation with Diffusion-based Generative Models, Richter et al., arXiv 2022
- StoRM: A Stochastic Regeneration Model for Speech Enhancement and Dereverberation, Lemercier et al., arXiv 2022
- Analysing Discriminative versus Diffusion-based Generative Models for Speech Restoration, Lemercier et al., ICASSP 2023",2022,,,,,
2675,ReVerb45K,Noun Phrase Canonicalization,Noun Phrase Canonicalization,"Noun Phrase Canonicalization, Open Knowledge Graph Canonicalization",Graph,,Methodology,noun-phrase-canonicalization-on-reverb45k,,https://github.com/malllabiisc/cesi/tree/master/data/reverb45k,https://paperswithcode.com/dataset/reverb45k,"Open KB canonicalization dataset.
ReVerb45K increases the entity number to 7.5K and has 45K triples in total. ReVerb45K extract a source sentence for each triple from ClueWeb09",,,,,,
2676,ReVerb_Challenge,Distant Speech Recognition,Distant Speech Recognition,"Distant Speech Recognition, Question Answering, Speech Recognition, Speech Enhancement","Audio, Image, Text",English,Speech,question-answering-on-reverb,Custom,https://reverb2014.audiolabs-erlangen.de,https://paperswithcode.com/dataset/reverb-challenge,"The REVERB (REverberant Voice Enhancement and Recognition Benchmark) challenge is a benchmark for evaluation of automatic speech recognition techniques. The challenge assumes the scenario of capturing utterances spoken by a single stationary distant-talking speaker with 1-channe, 2-channel or 8-channel microphone-arrays in reverberant meeting rooms. It features both real recordings and simulated data.

The challenge constis of speech enhancement and automatic speech recognition tasks in reverberant environments. The speech enhancement challenge task consists of enhancing noisy reverberant speech with single-/multi-channel speech enhancement techniques, and evaluating the enhanced data in terms of objective and subjective evaluation metrics. The automatic speech recognition challenge task consists of improving the recognition accuracy of the same reverberant speech. The background noise is mostly stationary and the signal-to-noise ratio is modest.",,,,,,
2677,ReviewRobot_Dataset,Review Generation,Review Generation,Review Generation,Text,English,Natural Language Processing,,Creative Commons — Attribution 4.0 International — CC BY 4.0,https://github.com/EagleW/ReviewRobot/tree/master/dataset,https://paperswithcode.com/dataset/reviewrobot-dataset,"ReviewRobot Dataset
Overview
This repository contains data for paper ReviewRobot: Explainable Paper Review Generation based on Knowledge Synthesis. [Dataset]

Dataset
There are three folders: Raw_data, IE_result, and KGs. 

Raw_data folder
The Raw_data has two parts: Background Corpus and Paper-review Corpus. 

We create the Background Corpus by selecting machine learning related pappers from the Semantic Scholar Open Research Corpus. It contains papers with their titles and abstracts published from the year of 1965 to 2019 (included).

The Paper-review Corpus contains parsed paper pdfs and their corresponding reviews. The paper-review pairs of acl_2017 and iclr_2017 folders come from PeerRead dataset. We fetched the rest from OpenReview and NeruIPS. We parsed those pdfs using GROBID. In each folder, metadata.txt contains all human reviews, and the txt/ folder contains all processed papers.

IE_result folder
The IE_result folder contains information extraction results from SciIE. In each group, the *_json/ contains tokenized texts, and the *_output/ contains IE results of tokenized texts.

The Background_IE contains two folders from one group for all paper abstracts from 1965 to 2019.

The Paper-review_IE contains four folders from two groups. The first group: iclrnipsabs_json and iclrnipsabs_output contain IE results for abstracts of Paper-review Corpus. The second group: iclrnips_json and iclrnips_output contain IE results for rest of papers in Paper-review Corpus.

KGs
The KGs folder contains the knowledge graphs built on the IE_result. 

back_kg
The back_kg contains the background KGs built up to a certain year. For each year, there are three files. 

Take 2012 as an example:
*   2012.pkl contains the background knowledge graph up to (include) 2012. It contains a dictionary of 6 fields: num_doc is the number of papers up to that year, cluster2entity is a mapping from the entity to its mentions, entity2cluster is a mapping from the mention to its corresponding entity, cluster2type is a mapping from the entity to its type, entity refers to all mentions in current KG, and relations refers to all relations in current KG. 
*   2012_key.pkl contains the mappings from knowledge elements to paper ids. It has two fields: cluster is the mapping from an entity to its corresponding paper ids, and relation is the mapping from a relation to the corresponding paper ids.
*   2012_paper contains the mappings from paper id to its paper title.

idea_kg
The idea_kg folder contains idea KGs constructed from paper abstracts and conclusions. Each line is a paper in the venue and has the following fields: id for the paper id, abs_num for the number of abstract sentences, sent for all sentences related to  idea_kg, entity for all mentions in current KG, cluster2sent for the corresponding sentence ids for a specific entity, entity2num for the occurence of a specific mention, relation2num for the occurence of a specific relation, cluster2entity for a mapping from the entity to its mentions, entity2type conains a mapping from the mention to the type, relations for all relations in current KG, relation2sent for corresponding sentence ids for a specific relation, and entity2cluster for a mapping from the mention to its corresponding entity. 

related_kg
The related_kg contains related KGs constructed from related work for each venue. It is of the same structure as idea_kg.

contribute_kg
The contribute_kg contains contribute KGs constructed from paper contribution section (under introduction section) and experiment section. It contains a dictionary of 4 fields:  id for the paper id,  total for the number of entities covered in the contribution section, covered for the number of entities covered in the experiment section, sents related sentences that covered those entities from both sections.

future_kg
The future_kg contains future KGs constructed from future work for each venue. It is of the same structure as idea_kg.

Review-annotation
The Review-annotation folder contains human annotations for review category and paper-review sentence pairs. The review.txt contains annotation for review category including 236 sentences for ""SUMMARY"", 33 sentences for ""NOVELTY"", 174 sentences for ""SOUNDNESS_CORRECTNESS"", 16 sentences for ""MEANINGFUL_COMPARISON"", and 14 sentences for ""IMPACT"". The pair.txt   contains 2,535 review-paper pairs. For each pair, the first slot is the review sentence; the second slot is the paper sentence, the third slot is the label where 0 indicates two sentences are not related and 1 indicates they are related.

License
Creative Commons — Attribution 4.0 International — CC BY 4.0",1965,,,236 sentences,,
2678,RF100,Object Counting,Object Counting,"Object Counting, object-detection, Medical Object Detection, Visual Object Tracking, Robust Object Detection, Small Object Detection, Image Classification, Video Object Tracking, Object Localization, Moving Object Detection, Multi-object discovery, Thermal Infrared Object Tracking, 2D Object Detection, Object Tracking, Zero-Shot Object Detection, Object Detection","Image, Video",,Computer Vision,2d-object-detection-on-rf100,MIT,https://www.rf100.org/,https://paperswithcode.com/dataset/rf100,"The evaluation of object detection models is usually performed by optimizing a single metric, e.g. mAP, on a fixed set of datasets, e.g. Microsoft COCO and Pascal VOC. Due to image retrieval and annotation costs, these datasets consist largely of images found on the web and do not represent many real-life domains that are being modelled in practice, e.g. satellite, microscopic and gaming, making it difficult to assert the degree of generalization learned by the model.

We introduce the Roboflow-100 (RF100) consisting of 100 datasets, 7 imagery domains, 224,714 images, and 805 class labels with over 11,170 labelling hours. We derived RF100 from over 90,000 public datasets, 60 million public images that are actively being assembled and labelled by computer vision practitioners in the open on the web application Roboflow Universe. By releasing RF100, we aim to provide a semantically diverse, multi-domain benchmark of datasets to help researchers test their model's generalizability with real-life data. RF100 download and benchmark replication are available on GitHub.",,MIT,https://arxiv.org/abs/2211.13523,714 images,"valuation of object detection models is usually performed by optimizing a single metric, e.g. mAP, on a fixed set of datasets, e.g. Microsoft COCO and Pascal VOC. Due to image retrieval and annotation costs, these datasets consist largely of images",
2679,RFUND-EN,Key-value Pair Extraction,Key-value Pair Extraction,Key-value Pair Extraction,,,Methodology,key-value-pair-extraction-on-rfund-en,,https://github.com/SCUT-DLVCLab/RFUND,https://paperswithcode.com/dataset/rfund-en,English subset of RFUND,,,,,,
2680,RFUND,Key-value Pair Extraction,Key-value Pair Extraction,Key-value Pair Extraction,,,Methodology,key-value-pair-extraction-on-rfund-en,,https://github.com/SCUT-DLVCLab/RFUND,https://paperswithcode.com/dataset/rfund,"RFUND is a relabeled version of FUNSD and XFUND datasets, tackling the following issues in their original annotations:


Entity (block) level OCR results. Real-world OCR engines usually produce line-level results, while the annotations in FUNSD and XFUND are at the entity (block) level. Text lines within the same entity are aggregated and serialized in human reading order, simplifying the task scope and failing to reflect real-world challenges.
Inconsistent labelling granularity. In FUNSD, while most contents are annotated at the entity level, multi-line entities with first-line indentation are annotated separately, in which the first line is split out and the rest are aggregated. XFUND exhibits variable granularity in annotations, with some contents labelled at the entity level and others at the line level. Such inconsistent labelling standards can hinder model training.
Erroneous category annotations. Entities in FUNSD/XFUND are categorized as ""header"", ""question"", ""answer"" and ""other"". We observed that certain entities in both FUNSD and XFUND have category labels that differ from human understanding. In the following example, the answer entity “Client confirmed agreement ...” was labelled as other, while the other entity “CONFIDENTIAL” was labelled as the question.",,,,,,
2681,RFW,Face Verification,Face Verification,"Face Verification, Fairness, Face Recognition",Image,,Computer Vision,,"Custom (research-only, non-commercial)",http://whdeng.cn/RFW/testing.html,https://paperswithcode.com/dataset/rfw,To validate the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms.,,,,,,
2682,RGB-DAVIS_Dataset,Image Reconstruction,Image Reconstruction,"Image Reconstruction, Event-based vision, Motion Compensation","3D, Image, Video",,Computer Vision,,,https://sites.google.com/view/guided-event-filtering,https://paperswithcode.com/dataset/rgb-davis-dataset,"Used to show systematic performance improvement in applications such as high frame-rate video synthesis, feature/corner detection and tracking, as well as high dynamic range image reconstruction.",,,,,,
2683,RGB-Stacking,Skill Mastery,Skill Mastery,"Skill Mastery, Visual Tracking, Skill Generalization","Image, Video",,Computer Vision,"skill-mastery-on-rgb-stacking, skill-generalization-on-rgb-stacking, visual-tracking-on-rgb-stacking",Apache-2.0,https://github.com/deepmind/rgb_stacking,https://paperswithcode.com/dataset/rgb-stacking,RGB-Stacking is a benchmark for vision-based robotic manipulation. The robot is trained to learn how to grasp objects and balance them on top of one another.,,,,,,
2684,RGBT234,Rgb-T Tracking,Rgb-T Tracking,Rgb-T Tracking,"Image, Video",,Computer Vision,rgb-t-tracking-on-rgbt234,,https://sites.google.com/view/ahutracking001/,https://paperswithcode.com/dataset/rgbt234,"The RGBT234 dataset is a comprehensive video dataset specifically designed for RGB-T (Red-Green-Blue and Thermal) tracking purposes. This dataset addresses the limitations of existing datasets like OSU-CT, LITIV, and GTOT in terms of size. RGBT234 consists of 234 RGB-T videos, each containing both an RGB video and a thermal video. The total number of frames in the dataset is approximately 234,000, with the largest video pair containing up to 8,000 frames.Each frame in the RGBT234 dataset is annotated with a minimum bounding box that covers the target for both the RGB and thermal modalities. The dataset also includes various environmental challenges such as rainy conditions, nighttime scenes, cold and hot weather scenarios. To analyze the performance of different tracking algorithms based on specific attributes, the RGBT234 dataset annotates 12 attributes and provides baseline trackers, including both deep learning and non-deep learning methods like structured SVM, sparse representation, and correlation filter-based trackers. Additionally, the dataset employs 5 metrics to evaluate the performance of RGB-T trackers effectively.",,,,,,
2685,RGZ_EMU__Semantic_Taxonomy,Semantic Image-Text Similarity,Semantic Image-Text Similarity,"Semantic Image-Text Similarity, Classification, Astronomy","Image, Text",English,Computer Vision,,,https://github.com/mb010/Text2Tag,https://paperswithcode.com/dataset/rgz-emu-semantic-taxonomy,"The data used in 
- ""Radio Galaxy Zoo EMU: Towards a Semantic Radio Galaxy Morphology Taxonomy"" (Bowles et al. submitted)
- ""A New Task: Deriving Semantic Class Targets for the Physical Sciences"" (Bowles et al. 2022: https://arxiv.org/abs/2210.14760) accepted at the Fifth Workshop on Machine Learning and the Physical Sciences, Neural Information Processing Systems 2022.

This data consists of images of galaxies, and plain English annotations of the features of the radio galaxies, as well as expert classifications using pre-existing scientific classes.

Additionally the data presented contains checkpoints of the data at various stages throughout the processing initially completed with https://github.com/mb010/Text2Tag.",2022,,,,,
2686,RHM,Human Activity Recognition,Human Activity Recognition,Human Activity Recognition,"Image, Video",,Computer Vision,human-activity-recognition-on-rhm,MIT,https://robothouse-dev.herts.ac.uk/datasets/RHM/HAR-1/,https://paperswithcode.com/dataset/rhm,"The Robot House Multi-View dataset (RHM) contains four views: Front, Back, Ceiling, and Robot Views. There are 14 classes with 6701 video clips for each view, making a total of 26804 video clips for the four views. The lengths of the video clips are between 1 to 5 seconds. The videos with the same number and the same classes are synchronized in different views.",,,,,,14
2687,Ricordi,Data Augmentation,Data Augmentation,"Data Augmentation, Handwriting generation, Handwriting Recognition","Image, Text",English,Computer Vision,,,http://icfhr2018.org/competitions.html,https://paperswithcode.com/dataset/ricordi,"Ricordi contains handwritten texts written in Italian. Train sample consists of 295 lines, validation - 19 lines and test - 69 lines.",,,,,,
2688,RiddleSense,Common Sense Reasoning,Common Sense Reasoning,"Common Sense Reasoning, Riddle Sense",,,Reasoning,riddle-sense-on-riddle-sense,,https://inklab.usc.edu/RiddleSense/,https://paperswithcode.com/dataset/riddle-sense,"Question: I have five fingers but I am not alive. What am I? Answer: a glove.

Answering such a riddle-style question is a challenging cognitive process, in that it requires complex commonsense reasoning abilities, an understanding of figurative language, and counterfactual reasoning skills, which are all important abilities for advanced natural language understanding (NLU). However, there is currently no dedicated datasets aiming to test these abilities. Herein, we present RiddleSense, a new multiple-choice question answering task, which comes with the first large dataset (5.7k examples) for answering riddle-style commonsense questions. We systematically evaluate a wide range of models over the challenge, and point out that there is a large gap between the best-supervised model and human performance — suggesting intriguing future research in the direction of higher-order commonsense reasoning and linguistic creativity towards building advanced NLU systems.",,,,7k examples,"test these abilities. Herein, we present RiddleSense, a new multiple-choice question answering task, which comes with the first large dataset (5.7k examples",
2689,RIMES,Handwriting Recognition,Handwriting Recognition,Handwriting Recognition,Image,,Computer Vision,,Custom,https://zenodo.org/communities/rimes,https://paperswithcode.com/dataset/rimes,"The RIMES database (Reconnaissance et Indexation de données Manuscrites et de fac similÉS / Recognition and Indexing of handwritten documents and faxes) was created to evaluate automatic systems of recognition and indexing of handwritten letters. Of particular interest are cases such as those sent by postal mail or fax by individuals to companies or administrations.

The database was collected by asking volunteers to write handwritten letters in exchange of gift vouchers. Volunteer were given a fictional identity (same sex as the real one) and up to 5 scenarios. Each scenario has been chosen among 9 realistic following themes : change of personal information (address, bank account), information request, opening and closing (customer account), modification of contract or order, complaint (bad service quality…), payment difficulties (asking for a delay, tax exemption…), reminder letter, damage declaration with further circumstances and a destination (administrations or service providers (telephone, power, bank, insurances). The volunteers composed a letter with those pieces of information using their own words. The layout was free and it was only asked to use white paper and to write in a readable way with black ink.

The collect was a success with more than 1,300 people who have participated to the RIMES database creation by writing up to 5 mails. The RIMES database thus obtained contains 12,723 pages corresponding to 5605 mails of two to three pages.",,,,,,
2690,RiSAWOZ,Text Generation,Text Generation,"Text Generation, Slot Filling, Dialogue State Tracking","Image, Text, Video",English,Computer Vision,,,https://github.com/terryqj0107/RiSAWOZ,https://paperswithcode.com/dataset/risawoz,"RiSAWOZ is a large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn semantically annotated dialogues, with more than 150K utterances spanning over 12 domains, which is larger than all previous annotated H2H conversational datasets. Both single- and multi-domain dialogues are constructed, accounting for 65% and 35%, respectively. Each dialogue is labelled with comprehensive dialogue annotations, including dialogue goal in the form of natural language description, domain, dialogue states and acts at both the user and system side. In addition to traditional dialogue annotations, it also includes linguistic annotations on discourse phenomena, e.g., ellipsis and coreference, in dialogues, which are useful for dialogue coreference and ellipsis resolution tasks.",,,,,,
2691,Riseholme-2021,Anomaly Detection,Anomaly Detection,"Anomaly Detection, One-class classifier",Image,,Computer Vision,,CC BY-NC-SA,https://github.com/ctyeong/Riseholme-2021,https://paperswithcode.com/dataset/riseholme-2021,"Risholme-2021 contains >3.5K images of strawberries at various growth stages along with anomalous instances. Data collection was performed in the strawberry research farm at the Riseholme campus of the University of Lincoln in UK. For more details, please check out ""Homepage"" down below.",2021,,,5K images,,
2692,Risk-Aware_Planning_Dataset,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Decision Making Under Uncertainty",Image,,Computer Vision,,Custom (Requires Citing),https://obj.umiacs.umd.edu/publicdata/risk_aware_planning.html,https://paperswithcode.com/dataset/risk-aware-planning-dataset,Risk-Aware Planning is a dataset that contains the overhead images and their semantic segmentation captured by a drone from the CityEnviron environment in AirSim simulator.,,,,,,
2693,RLBench,Robot Manipulation Generalization,Robot Manipulation Generalization,"Robot Manipulation Generalization, Robot Manipulation, Imitation Learning",,,Methodology,"robot-manipulation-generalization-on-gembench, robot-manipulation-on-rlbench, robot-manipulation-generalization-on-the",Custom (non-commercial),https://sites.google.com/view/rlb,https://paperswithcode.com/dataset/rlbench,"RLBench is an ambitious large-scale benchmark and learning environment designed to facilitate research in a number of vision-guided manipulation research areas, including: reinforcement learning, imitation learning, multi-task learning, geometric computer vision, and in particular, few-shot learning.",,,,,,
2694,RLLab_Framework,Continuous Control,Continuous Control,"Continuous Control, Imitation Learning, Policy Gradient Methods",,,Methodology,,,https://github.com/rllab/rllab,https://paperswithcode.com/dataset/rllab-framework,"A benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure.",,,,,,
2695,RLU,Continuous Control,Continuous Control,"Continuous Control, Offline RL, Atari Games",,,Methodology,,,https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged,https://paperswithcode.com/dataset/caglar,"RL Unplugged is suite of benchmarks for offline reinforcement learning. The RL Unplugged is designed around the following considerations: to facilitate ease of use, we provide the datasets with a unified API which makes it easy for the practitioner to work with all data in the suite once a general pipeline has been established. This is a dataset accompanying the paper RL Unplugged: Benchmarks for Offline Reinforcement Learning.

In this suite of benchmarks, we try to focus on the following problems:

High dimensional action spaces, for example the locomotion humanoid domains, we have 56 dimensional actions.

High dimensional observations.

Partial observability, observations have egocentric vision.

Difficulty of exploration, using states of the art algorithms and imitation to generate data for difficult environments.

Real world challenges.",,,,,,
2696,RL_Unplugged,Offline RL,Offline RL,Offline RL,,,Methodology,,Apache-2.0,https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged,https://paperswithcode.com/dataset/rl-unplugged,"RL Unplugged is suite of benchmarks for offline reinforcement learning. The RL Unplugged is designed around the following considerations: to facilitate ease of use, the datasets are provided with a unified API which makes it easy for the practitioner to work with all data in the suite once a general pipeline has been established. This is a dataset accompanying the paper RL Unplugged: Benchmarks for Offline Reinforcement Learning.

In this suite of benchmarks, the authors try to focus on the following problems:


High dimensional action spaces, for example the locomotion humanoid domains, there are 56 dimensional actions.
High dimensional observations.
Partial observability, observations have egocentric vision.
Difficulty of exploration, using states of the art algorithms and imitation to generate data for difficult environments.
Real world challenges.",,,,,,
2697,RMOT-223,Out of Distribution (OOD) Detection,Out of Distribution (OOD) Detection,"Out of Distribution (OOD) Detection, Object Tracking","Image, Video",,Computer Vision,,,https://huggingface.co/datasets/hsroro/RMOT-223,https://paperswithcode.com/dataset/rmot-223,"In this dataset, various objects are arranged on a white table. A UR5e robot picks and place a target object specified on the title of the video/image sequence. Videos under auto- folder are collected with automatic operation of the robot. Videos under human- folders are collected with the tele-operation of the robot. Ground-truth tracking bounding boxes are generated with STARK, and when the target exits the camera frame, the bounding box estimation is switched to [-1, -1, -1, -1], indicating target not shown.",,,,,,
2698,ROAD,Event Detection,Event Detection,"Event Detection, Continual Learning, Autonomous Vehicles, Activity Detection, Decision Making, Action Detection, Autonomous Driving","Image, Video",,Computer Vision,,CC BY-NC 4.0,https://github.com/gurkirt/road-dataset,https://paperswithcode.com/dataset/road,"ROAD is designed to test an autonomous vehicle's ability to detect road events, defined as triplets composed by an active agent, the action(s) it performs and the corresponding scene locations. ROAD comprises videos originally from the Oxford RobotCar Dataset, annotated with bounding boxes showing the location in the image plane of each road event.",,,,,,
2699,RoadTracer,Audio Question Answering,Audio Question Answering,"Audio Question Answering, Chinese Reading Comprehension, Disaster Response, Semantic Segmentation, graph construction","Audio, Graph, Image, Text",English,Audio,"audio-question-answering-on-roadtracer, chinese-reading-comprehension-on-roadtracer",,https://roadmaps.csail.mit.edu/roadtracer,https://paperswithcode.com/dataset/roadtracer,"RoadTracer is a dataset for extraction of road networks from aerial images. It consists of a large
corpus of high-resolution satellite imagery and ground truth
road network graphs covering the urban core of forty cities
across six countries. For each city, the dataset covers a region of approximately 24 sq km around the city center. The satellite imagery is obtained from Google at 60 cm/pixel resolution, and the road network from OSM.

The dataset is split into a training set with 25 cities and a
test set with 15 other cities.",,,,,,
2700,ROAST,Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),"Aspect-Based Sentiment Analysis (ABSA), Sentiment Analysis, Aspect Extraction, Aspect Category Detection","Image, Text",English,Computer Vision,,,https://github.com/RiTUAL-UH/ROAST-ABSA,https://paperswithcode.com/dataset/roast,This repository has a review-level multidomain multilingual dataset for Aspect-based Sentiment Analysis(ABSA) for the paper ROAST: Review-level Opinion Aspect Sentiment Target Joint Detection.,,,,,,
2701,RoboCup,Text Generation,Text Generation,"Text Generation, Data-to-Text Generation, Knowledge Graphs",Text,English,Natural Language Processing,,,https://www.robocup.org/,https://paperswithcode.com/dataset/robocup,"RoboCup is an initiative in which research groups compete by enabling their robots to play football matches. Playing football requires solving several challenging tasks, such as vision, motion, and team coordination. Framing the research efforts onto football attracts public interest (and potential research funding) in robotics, which may otherwise be less entertaining to non-experts.",,"Robots as Actors in a Film: No War, A Robot Story",https://arxiv.org/abs/1910.12294,,,
2702,RoboNet,Offline RL,Offline RL,"Offline RL, Robotic Grasping, Decision Making",,,Methodology,,Custom,https://www.robonet.wiki/,https://paperswithcode.com/dataset/robonet,"An open database for sharing robotic experience, which provides an initial pool of 15 million video frames, from 7 different robot platforms, and study how it can be used to learn generalizable models for vision-based robotic manipulation.",,,,,,
2703,robosuite_Benchmark,Robot Manipulation,Robot Manipulation,"Robot Manipulation, Robotic Grasping",,,Methodology,,MIT,https://robosuite.ai/,https://paperswithcode.com/dataset/robosuite-benchmark,A Modular Simulation Framework and Benchmark for Robot Learning.,,,,,,
2704,Robust04,Ad-Hoc Information Retrieval,Ad-Hoc Information Retrieval,"Ad-Hoc Information Retrieval, Information Retrieval, Zero-shot Text Search",Text,English,Natural Language Processing,"ad-hoc-information-retrieval-on-trec-robust04, zero-shot-text-search-on-robust04",,https://trec.nist.gov/data/robust.html,https://paperswithcode.com/dataset/robust04,"The goal of the Robust track is to improve the consistency of retrieval technology by focusing on poorly performing topics. In addition, the track brings back a classic, ad hoc retrieval task in TREC that provides a natural home for new participants. An ad hoc task in TREC investigates the performance of systems that search a static set of documents using previously-unseen topics. For each topic, participants create a query and submit a ranking of the top 1000 documents for that topic.",,,,1000 documents,,
2705,ROCStories,Text Generation,Text Generation,"Text Generation, Emotion Classification","Image, Text",English,Computer Vision,"text-generation-on-rocstories, emotion-classification-on-rocstories",,https://cs.rochester.edu/nlp/rocstories/,https://paperswithcode.com/dataset/rocstories,"ROCStories is a collection of commonsense short stories. The corpus consists of 100,000 five-sentence stories. Each story logically follows everyday topics created by Amazon Mechanical Turk workers. These stories contain a variety of commonsense causal and temporal relations between everyday events. Writers also develop an additional 3,742 Story Cloze Test stories which contain a four-sentence-long body and two candidate endings. The endings were collected by asking Mechanical Turk workers to write both a right ending and a wrong ending after eliminating original endings of given short stories. Both endings were required to make logical sense and include at least one character from the main story line. The published ROCStories dataset is constructed with ROCStories as a training set that includes 98,162 stories that exclude candidate wrong endings, an evaluation set, and a test set, which have the same structure (1 body + 2 candidate endings) and a size of 1,871.",,Incorporating Structured Commonsense Knowledge in Story Completion,https://arxiv.org/abs/1811.00625,,,
2706,RoomEnv-v0,RoomEnv-v0,RoomEnv-v0,"RoomEnv-v0, Multi-agent Reinforcement Learning",,,Methodology,roomenv-v0-on-roomenv-v0,MIT,https://github.com/humemai/room-env,https://paperswithcode.com/dataset/roomenv,"The Room environment - v0


We have released a challenging Gymnasium compatible
environment. The best strategy for this environment is to have both episodic and semantic
memory systems. See the paper for more information.

Prerequisites

A unix or unix-like x86 machine
python 3.10 or higher.
Running in a virtual environment (e.g., conda, virtualenv, etc.) is highly recommended so that you don't mess up with the system python.
This env is added to the PyPI server. Just run: pip install room-env

Data collection
Data is collected from querying ConceptNet APIs. For simplicity, we only collect triples
whose format is (head, atlocation, tail). Here head is one of the 80 MS COCO
dataset categories. This was kept in mind so that later on we can use images as well.

If you want to collect the data manually, then run below:

python collect_data.py

How does this environment work?
The Gymnasium-compatible Room environment is one big room with
N<sub>people</sub> number of people who can freely move
around. Each of them selects one object, among
N<sub>objects</sub>, and places it in one of the
N<sub>locations</sub> locations.
N<sub>agents</sub> number of agent(s) are also in this
room. They can only observe one human placing an object, one at a time;
x<sup>(t)</sup>. At the same time, they are given one question
about the location of an object; q<sup>(t)</sup>.
x<sup>(t)</sup> is given as a quadruple,
(h<sup>(t)</sup>,r<sup>(t)</sup>,t<sup>(t)</sup>,t),
For example, &lt;James’s laptop, atlocation, James’s desk, 42&gt; accounts
for an observation where an agent sees James placing his laptop on his
desk at t = 42. q<sup>(t)</sup> is given as a double,
(h,r). For example, &lt;Karen’s cat, atlocation&gt; is asking where
Karen’s cat is located. If the agent answers the question correctly, it
gets a reward of  + 1, and if not, it gets 0.

The reason why the observations and questions are given as
RDF-triple-like format is two folds. One is that this structured format
is easily readable / writable by both humans and machines. Second is
that we can use existing knowledge graphs, such as ConceptNet .

To simplify the environment, the agents themselves are not actually
moving, but the room is continuously changing. There are several random
factors in this environment to be considered:



With the chance of p<sub>commonsense</sub>,
   a human places an object in a commonsense location (e.g., a laptop
   on a desk). The commonsense knowledge we use is from ConceptNet.
   With the chance of
   1 − p<sub>commonsense</sub>, an object is
   placed at a non-commonsense random location (e.g., a laptop on the
   tree).



With the chance of
   p<sub>new_location</sub>, a human changes
   object location.



With the chance of p<sub>new_object</sub>, a
   human changes his/her object to another one.



With the chance of
   p<sub>switch_person</sub>, two people
   switch their locations. This is done to mimic an agent moving around
   the room.



All of the four probabilities account for the Bernoulli distributions.

Consider there is only one agent. Then this is a POMDP, where S<sub>t</sub> = (x<sup>(t)</sup>, q<sup>(t)</sup>), A<sub>t</sub> = (do something with x<sup>(t)</sup>, answer q<sup>(t)</sup>), and R<sub>t</sub> ∈ {0, 1}.

Currently there is no RL trained for this. We only have some heuristics. Take a look at the paper for more details.

RoomEnv-v0
```python
import gymnasium as gym

env = gym.make(""room_env:RoomEnv-v0"")
(observation, question), info = env.reset()
rewards = 0

while True:
    (observation, question), reward, done, truncated, info = env.step(""This is my answer!"")
    rewards += reward
    if done:
        break

print(rewards)
```

Every time when an agent takes an action, the environment will give you an observation
and a question to answer. You can try directly answering the question,
such as env.step(""This is my answer!""), but a better strategy is to keep the
observations in memory systems and take advantage of the current observation and the
history of them in the memory systems.

Take a look at this repo for an actual
interaction with this environment to learn a policy.

Contributing
Contributions are what make the open source community such an amazing place to be learn,
inspire, and create. Any contributions you make are greatly appreciated.


Fork the Project
Create your Feature Branch (git checkout -b feature/AmazingFeature)
Run make test &amp;&amp; make style &amp;&amp; make quality in the root repo directory,
   to ensure code quality.
Commit your Changes (git commit -m 'Add some AmazingFeature')
Push to the Branch (git push origin feature/AmazingFeature)
Open a Pull Request

Cite our paper
bibtex
@misc{https://doi.org/10.48550/arxiv.2204.01611,
  doi = {10.48550/ARXIV.2204.01611},
  url = {https://arxiv.org/abs/2204.01611},
  author = {Kim, Taewoon and Cochez, Michael and Francois-Lavet, Vincent and Neerincx,
  Mark and Vossen, Piek},
  keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences,
  FOS: Computer and information sciences},
  title = {A Machine With Human-Like Memory Systems},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

Authors

Taewoon Kim
Michael Cochez
Vincent Francois-Lavet
Mark Neerincx
Piek Vossen

License
MIT",2022,Paper,https://arxiv.org/abs/2204.01611,,,
2707,ROOR,Relation Extraction,Relation Extraction,"Relation Extraction, Reading Order Detection","Graph, Image",,Computer Vision,reading-order-detection-on-roor,CC-BY-4.0,https://github.com/chongzhangFDU/ROOR-Datasets,https://paperswithcode.com/dataset/roor,"ROOR is a reading order prediction (ROP) benchmark which annotates layout reading order as ordering relations. 

Layout reading order is typically formulated as a permutation of layout elements, i.e. a sequence containing all the layout elements. 
However, multiple cases have reflected that this formulation does not adequately convey the complete reading order information in the layout, which may negatively affect the utilization of this signal in downstream VrD tasks. 

Therefore, this work investigate the properties of layout reading order, conceptualizing it with terms Immediate Succession During Reading(ISDR) and Generalized Succession During Reading(GSDR), and formulate each of which as an ordering relation over layout elements. 
Then, ROOR provides the annotation of the ISDR relationship over layout segments, based on the layout annotation of EC-FUNSD. 
Overall, ROOR comprises 199 samples including 10,662 segments, 31,297 words and 10,967 annotated reading order linking pairs.
We hope the construction of this benchmark could facilitate the development of automated ROP methods of the improved task form.",,,,199 samples,,
2708,ROPES,Question Generation,Question Generation,"Question Generation, Reading Comprehension, Decision Making",Text,English,Natural Language Processing,,CC BY 4.0,https://allenai.org/data/ropes,https://paperswithcode.com/dataset/ropes,"ROPES is a QA dataset which tests a system's ability to apply knowledge from a passage of text to a new situation. A system is presented a background passage containing a causal or qualitative relation(s), a novel situation that uses this background, and questions that require reasoning about effects of the relationships in the back-ground passage in the context of the situation.",,,,,,
2709,RotoWire,Data-to-Text Generation,Data-to-Text Generation,"Data-to-Text Generation, Table-to-Text Generation","Tabular, Text",English,Natural Language Processing,"data-to-text-generation-on-rotowire-content-1, data-to-text-generation-on-rotowire-relation, table-to-text-generation-on-rotowire, data-to-text-generation-on-rotowire-content, data-to-text-generation-on-rotowire",,https://github.com/harvardnlp/boxscore-data,https://paperswithcode.com/dataset/rotowire,"This dataset consists of (human-written) NBA basketball game summaries aligned with their corresponding box- and line-scores. Summaries taken from rotowire.com are referred to as the ""rotowire"" data.  There are 4853 distinct rotowire summaries, covering NBA games played between 1/1/2014 and 3/29/2017; some games have multiple summaries. The summaries have been randomly split into training, validation, and test sets consisting of 3398, 727, and 728 summaries, respectively.",2014,Challenges in Data-to-Document Generation,https://arxiv.org/abs/1707.08052,,,
2710,rounD_Dataset,Trajectory Clustering,Trajectory Clustering,"Trajectory Clustering, Trajectory Planning, Trajectory Prediction, Trajectory Modeling, Trajectory Forecasting",Time Series,,Methodology,,Non-Commercial,https://levelxdata.com/round-dataset/,https://paperswithcode.com/dataset/round-dataset,"The rounD dataset introduces a fresh compilation of natural road user trajectory data from German roundabouts, gathered using drone technology to navigate past usual challenges such as occlusions inherent in traditional traffic data collection methods. It includes traffic data from three unique locations, capturing the movement and categorizing each road user by type. Advanced computer vision algorithms are applied to ensure high positional accuracy. This dataset is highly adaptable for a variety of applications, including predicting road user behavior, driver modeling, scenario-based safety evaluations for automated driving systems, and the data-driven creation of Highly Automated Driving (HAD) system components.",,,,,,
2711,RPCD,Aesthetics Quality Assessment,Aesthetics Quality Assessment,"Aesthetics Quality Assessment, Image Captioning, Aesthetic Image Captioning","Image, Text",English,Computer Vision,,,https://zenodo.org/record/6985507#.YvZ3bxuxVp8,https://paperswithcode.com/dataset/rpcd,"The Reddit Photo Critique Dataset (RPCD) contains tuples of image and photo critiques. RPCD consists of 74K images and 220K comments and is collected from a Reddit community used by hobbyists and professional photographers to improve their photography skills by leveraging constructive community feedback.

The proposed dataset differs from previous aesthetics datasets mainly in three aspects, namely


the large scale of the dataset and the extension of the comments criticizing different aspects of the image;
it contains mostly UltraHD images;
it can easily be extended to new data as it is collected through an automatic pipeline.",,,,74K images,,
2712,RR,Argument Pair Extraction (APE),Argument Pair Extraction (APE),Argument Pair Extraction (APE),,,Methodology,argument-pair-extraction-ape-on-rr,,https://github.com/LiyingCheng95/ArgumentPairExtraction,https://paperswithcode.com/dataset/rr,Review-Rebuttal (RR) dataset is introduced to facilitate the study of argument pair extraction in the peer review and rebuttal domain.,,,,,,
2713,RRG,Discourse Parsing,Discourse Parsing,Discourse Parsing,Text,English,Natural Language Processing,,cc-by 4.0,,https://paperswithcode.com/dataset/rrg,Parallel version of annotations in GUM RST v9.1.,,,,,,
2714,rrn-sudoku,Game of Sudoku,Game of Sudoku,Game of Sudoku,,,Methodology,,,https://www.dropbox.com/s/rp3hbjs91xiqdgc/sudoku-hard.zip,https://paperswithcode.com/dataset/rrn-sudoku,"A set of 180,000 Sudoku grids with a variable number of hints from the minimal number of 17 (extremely hard instances) to 34 (easy instances), with 10,000 instances per level of hardness.

Training how to play the hardest Sudoku instances is a bit of a challenge.",,,,000 instances,,
2715,RRS,Conversational Response Selection,Conversational Response Selection,Conversational Response Selection,,,Methodology,conversational-response-selection-on-rrs,,https://github.com/gmftbyGMFTBY/SimpleReDial-v1,https://paperswithcode.com/dataset/rrs,"|           | Train | Validation | Test    | Ranking Test |
| --------- | ----- | ---------- | ------- | ------------ |
| size      | 0.4M  | 50K        | 5K      | 800          |
| pos:neg   | 1:1   | 1:9        | 1.2:8.8 | -            |
| avg turns | 5.0   | 5.0        | 5.0     | 5.0          |

Ranking test set contains the high-quality responses that selected by some baselines, and their correlation with the conversation context are carefully annotated by 8 professional annotators (the average annotation scores are saved for ranking). For ranking test set, the metrics should be NDCG@3 and NDCG@5, since the correlation scores are provided. More details are available in the Appendix of the paper.",,,,,,
2716,RRS_Ranking_Test,Conversational Response Selection,Conversational Response Selection,Conversational Response Selection,,,Methodology,conversational-response-selection-on-rrs-1,,https://github.com/gmftbyGMFTBY/SimpleReDial-v1,https://paperswithcode.com/dataset/rrs-ranking-test,"|           | Train | Validation | Test    | Ranking Test |
| --------- | ----- | ---------- | ------- | ------------ |
| size      | 0.4M  | 50K        | 5K      | 800          |
| pos:neg   | 1:1   | 1:9        | 1.2:8.8 | -            |
| avg turns | 5.0   | 5.0        | 5.0     | 5.0          |

Ranking test set contains the high-quality responses that selected by some baselines, and their correlation with the conversation context are carefully annotated by 8 professional annotators (the average annotation scores are saved for ranking). For ranking test set, the metrics should be NDCG@3 and NDCG@5, since the correlation scores are provided. More details are available in the Appendix of the paper.",,,,,,
2717,RRT,End-to-End RST Parsing,End-to-End RST Parsing,"End-to-End RST Parsing, Discourse Parsing",Text,English,Natural Language Processing,,CC-BY-NC 4.0,https://rstreebank.ru/eng,https://paperswithcode.com/dataset/rurstreebank,"RST corpus for Russian.


233 documents 
2 genres (news & blogs) + relations annotation for academical texts
28k elementary discourse units
24 rhetorical relations",,,,233 documents,,
2718,RSBlur,Deblurring,Deblurring,Deblurring,,,Methodology,"deblurring-on-rsblur-trained-on-synthetic, deblurring-on-rsblur",,http://cg.postech.ac.kr/research/rsblur,https://paperswithcode.com/dataset/rsblur,"The RSBlur dataset provides pairs of real and synthetic blurred images with ground truth sharp images. The dataset enables the evaluation of deblurring methods and blur synthesis methods on real-world blurred images.  Training, validation, and test sets consist of 8,878, 1,120, and 3,360 blurred images, respectively.",,,,,"valuation of deblurring methods and blur synthesis methods on real-world blurred images.  Training, validation, and test sets consist of 8,878, 1,120, and 3,360 blurred images",
2719,RSICD,Scene Classification,Scene Classification,"Scene Classification, Cross-Modal Retrieval, Image Captioning, Text Retrieval, Image-to-Text Retrieval","Image, Text",English,Computer Vision,"text-retrieval-on-rsicd, cross-modal-retrieval-on-rsicd, image-to-text-retrieval-on-rsicd",,https://github.com/201528014227051/RSICD_optimal,https://paperswithcode.com/dataset/rsicd,"The Remote Sensing Image Captioning Dataset (RSICD) is a dataset for remote sensing image captioning task. It contains more than ten thousands remote sensing images which are collected from Google Earth, Baidu Map, MapABC and Tianditu. The images are fixed to 224X224 pixels with various resolutions. The total number of remote sensing images is 10921, with five sentences descriptions per image.",,,,,,
2720,RSITMD,Cross-Modal Retrieval,Cross-Modal Retrieval,Cross-Modal Retrieval,,,Methodology,cross-modal-retrieval-on-rsitmd,,,https://paperswithcode.com/dataset/rsitmd,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
2721,RSM-based_multi-objective_optimization_using_desir,Multiobjective Optimization,Multiobjective Optimization,Multiobjective Optimization,,,Methodology,,CC BY 4.0,,https://paperswithcode.com/dataset/simulation-inputs-outputs,The following files contains the simulation inputs and outputs for conducting the multi-objetive optimization of thermal comfort and dyalight with the Response Surface Methodology. This files feed are needed for running the R script/code as well as the datasets are contained in the Github repository.,,,,,,
2722,RST-DT,End-to-End RST Parsing,End-to-End RST Parsing,"End-to-End RST Parsing, Discourse Parsing",Text,English,Natural Language Processing,"discourse-parsing-on-rst-dt, end-to-end-rst-parsing-on-rst-dt-1",LDC,https://catalog.ldc.upenn.edu/LDC2002T07,https://paperswithcode.com/dataset/rst-dt,"The Rhetorical Structure Theory (RST) Discourse Treebank consists of 385 Wall Street Journal articles
from the Penn Treebank annotated with discourse structure in the RST framework along with
human-generated extracts and abstracts associated with the source documents.

In the RST framework (Mann and Thompson, 1988), a text's discourse structure can be
represented as a tree in four aspects:

(1) the leaves correspond to text fragments called elementary discourse units (the mininal discourse units);
(2) the internal nodes of the tree correspond to contiguous text spans;
(3) each node is characterized by its nuclearity, or essential unit of information; and
(4) each node is also characterized by a rhetorical relation between two or more non-overlapping, adjacent text spans. 

Data

The data in this release is divided into a training set (347 documents) and a test set (38 documents).
All annotations were produced using a discourse annotation tool that can be downloaded from http://www.isi.edu/~marcu/discourse.",1988,,,347 documents,,
2723,rt-inod-bias,Bias Detection,Bias Detection,Bias Detection,Image,,Computer Vision,bias-detection-on-rt-inod-bias,CC BY-SA 4.0,https://huggingface.co/datasets/innodatalabs/rt-inod-bias,https://paperswithcode.com/dataset/rt-inod-bias,"The Innodata Red Teaming Prompts aims to rigorously assess models’ factuality and safety. This dataset, due to its manual creation and breadth of coverage, facilitates a comprehensive examination of LLM performance across diverse scenarios.",,,,,,
2724,rt-inod-jailbreaking,Dialogue Safety Prediction,Dialogue Safety Prediction,Dialogue Safety Prediction,Time Series,,Methodology,dialogue-safety-prediction-on-rt-inod,CC BY-SA 4.0,https://huggingface.co/datasets/innodatalabs/rt-inod-jailbreaking,https://paperswithcode.com/dataset/rt-inod-jailbreaking,"The Innodata Red Teaming Prompts aims to rigorously assess models’ factuality and safety. This dataset, due to its manual creation and breadth of coverage, facilitates a comprehensive examination of LLM performance across diverse scenarios.",,,,,,
2725,RTE3-FR,RTE,RTE,"RTE, Natural Language Inference, Sentence-Pair Classification","Image, Text",English,Computer Vision,,,https://github.com/mskandalis/rte3-french,https://paperswithcode.com/dataset/rte3-fr,"RTE3-FR dataset is the French translation of the Textual Entailment English dataset used in the RTE-3 Challenge (https://nlp.stanford.edu/RTE3-pilot).

Like its English counterpart, the French RTE-3 dataset is composed of a development set and a test set, each containing 800 T/H pairs.

The dataset is annotated for a 3-way task with the following labels: entailment (0), neutral (1), contradiction (2).

RTE3-FR is available both in XML and TSV format.",,,,,,
2726,RTI_Rwanda_Drone_Crop_Types,Remote Sensing Image Classification,Remote Sensing Image Classification,Remote Sensing Image Classification,Image,,Computer Vision,,CC-BY-NC-SA 4.0,https://mlhub.earth/data/rti_rwanda_crop_type,https://paperswithcode.com/dataset/rti-rwanda-drone-crop-types,"RTI International (RTI) generated 2,611 labeled point locations representing 19 different land cover types, clustered in 5 distinct agroecological zones within Rwanda. These land cover types were reduced to three crop types (Banana, Maize, and Legume), two additional non-crop land cover types (Forest and Structure), and a catch-all Other land cover type to provide training/evaluation data for a crop classification model. Each point is attributed with its latitude and longitude, the land cover type, and the degree of confidence the labeler had when classifying the point location. For each location there are also three corresponding image chips (4.5 m x 4.5 m in size) with the point id as part of the image name. Each image contains a P1, P2, or P3 designation in the name, indicating the time period. P1 corresponds to December 2018, P2 corresponds to January 2019, and P3 corresponds to February 2019. These data were used in the development of research documented in greater detail in “Deep Neural Networks and Transfer Learning for Food Crop Identification in UAV Images” (Chew et al., 2020).",2018,,,,"training/evaluation data for a crop classification model. Each point is attributed with its latitude and longitude, the land cover type, and the degree of confidence the labeler had when classifying the point location. For each location there are also three corresponding image chips (4.5 m x 4.5 m in size) with the point id as part of the image name. Each image contains a P1, P2, or P3 designation in the name, indicating the time period. P1 corresponds to December 2018, P2 corresponds to January 2019, and P3 corresponds to February 2019. These data were used in the development of research documented in greater detail in “Deep Neural Networks and Transfer Learning for Food Crop Identification in UAV Images",
2727,RuBQ,Entity Linking,Entity Linking,"Entity Linking, Knowledge Base Question Answering, Question Answering",Text,English,Natural Language Processing,,,https://github.com/vladislavneon/RuBQ,https://paperswithcode.com/dataset/rubq,"The first Russian knowledge base question answering (KBQA) dataset. The high-quality dataset consists of 1,500 Russian questions of varying complexity, their English machine translations, SPARQL queries to Wikidata, reference answers, as well as a Wikidata sample of triples containing entities with Russian labels. The dataset creation started with a large collection of question-answer pairs from online quizzes. The data underwent automatic filtering, crowd-assisted entity linking, automatic generation of SPARQL queries, and their subsequent in-house verification.",,,,,,
2728,RuCoLA,Text Generation,Text Generation,"Text Generation, Linguistic Acceptability",Text,English,Natural Language Processing,linguistic-acceptability-on-rucola,,https://github.com/RussianNLP/RuCoLA,https://paperswithcode.com/dataset/rucola,The Russian Corpus of Linguistic Acceptability (RuCoLA) is built from the ground up under the well-established binary LA approach. RuCoLA consists of 9.8k in-domain sentences from linguistic publications and 3.6k out-of-domain sentence produced by generative models.,,RuCoLA: Russian Corpus of Linguistic Acceptability,https://arxiv.org/pdf/2210.12814v1.pdf,,,
2729,RuDaS,Inductive logic programming,Inductive logic programming,Inductive logic programming,,,Methodology,inductive-logic-programming-on-rudas,Apache-2.0,https://github.com/IBM/RuDaS,https://paperswithcode.com/dataset/rudas,"Logical rules are a popular knowledge representation language in many domains. Recently, neural networks have been proposed to support the complex rule induction process. However, we argue that existing datasets and evaluation approaches are lacking in various dimensions; for example, different kinds of rules or dependencies between rules are neglected. Moreover, for the development of neural approaches, we need large amounts of data to learn from and adequate, approximate evaluation measures. In this paper, we provide a tool for generating diverse datasets and for evaluating neural rule learning systems, including novel performance metrics.",,,,,,
2730,RUN,Vision and Language Navigation,Vision and Language Navigation,Vision and Language Navigation,Text,English,Natural Language Processing,,,https://github.com/OnlpLab/RUN,https://paperswithcode.com/dataset/run,"The RUN dataset  is based on OpenStreetMap (OSM). The map contains rich layers and an abundance of entities of different types. Each entity is complex and can contain (at least) four labels: name, type, is building=y/n, and house number. An entity can spread over several tiles. As the maps do not overlap, only very few entities are shared among them. The RUN dataset aligns NL navigation instructions to coordinates of their corresponding route on the OSM map.",,,,,,
2731,RuOpenBookQA,Logical Reasoning,Logical Reasoning,"Logical Reasoning, Question Answering",Text,English,Reasoning,question-answering-on-ruopenbookqa,Apache-2.0,http://tape-benchmark.com/datasets.html#ruopenbookqa,https://paperswithcode.com/dataset/ruopenbookqa,"RuOpenBookQA is a QA dataset with multiple-choice elementary-level science questions which probe the understanding of core science facts.

Motivation

RuOpenBookQA is mainly based on the work of (Mihaylov et al., 2018): it is a QA dataset with multiple-choice elementary-level science questions, which probe the understanding of 1k+ core science facts.

Very similar to the pipeline of the RuWorldTree, the dataset includes a corpus of factoids, factoid questions and correct answer. Only one fact is enough to find the correct answer, so this task can be considered easier.

```{
    'ID': '7-674', 

'question': 'If a person walks in the direction opposite to the compass needle, they are going (A) west (B) north (C) east (D) south',

'answer': 'D',

'episode': [11],

'perturbation': 'ru_openbook'

}```

Data Fields


ID: a string containing a unique question id
question: a string containing question text with answer options
answer: a string containing the correct answer key (A, B, C or D)
perturbation: a string containing the name of the perturbation applied to text. If no perturbation was applied, the dataset name is used
episode: a list of episodes in which the instance is used. Only used for the train set

Data Splits

The dataset consists of a training set with labeled examples and a test set in two configurations:


raw data: includes the original data with no additional sampling
episodes: data is split into evaluation episodes and includes several perturbations of test for robustness evaluation

Test Perturbations

Each training episode in the dataset corresponds to seven test variations, including the original test data and six adversarial test sets, acquired through the modification of the original test through the following text perturbations:


ButterFingers: randomly adds noise to data by mimicking spelling mistakes made by humans through character swaps based on their keyboard distance
Emojify: replaces the input words with the corresponding emojis, preserving their original meaning
EDAdelete: randomly deletes tokens in the text
EDAswap: randomly swaps tokens in the text
BackTranslation: generates variations of the context through back-translation (ru -> en -> ru)
AddSent: replaces one or more choice options with a generated one",2018,,,,,
2732,Ruralscapes,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Robot Navigation",Image,,Computer Vision,,,https://sites.google.com/site/aerialimageunderstanding/semantics-through-time-semi-supervised-segmentation-of-aerial-videos,https://paperswithcode.com/dataset/ruralscapes-dataset,A dataset with high resolution (4K) images and manually-annotated dense labels every 50 frames.,,,,,,
2733,RUSSE,Word Sense Disambiguation,Word Sense Disambiguation,"Word Sense Disambiguation, Reading Comprehension",,,Methodology,word-sense-disambiguation-on-russe,MIT,https://github.com/RussianNLP/RussianSuperGLUE,https://paperswithcode.com/dataset/russe,"WiC: The Word-in-Context Dataset A reliable benchmark for the evaluation of context-sensitive word embeddings.

Depending on its context, an ambiguous word can refer to multiple, potentially unrelated, meanings. Mainstream static word embeddings, such as Word2vec and GloVe, are unable to reflect this dynamic semantic nature. Contextualised word embeddings are an attempt at addressing this limitation by computing dynamic representations for words which can adapt based on context.

Russian SuperGLUE task borrows original data from the Russe project, Word Sense Induction and Disambiguation shared task (2018)

Task Type
Reading Comprehension. Binary Classification: true/false

Example
{
  ""idx"" : 8,
  ""word"" : ""дорожка"",
  ""sentence1"" : ""Бурые ковровые дорожки заглушали шаги"",
  ""sentence2"" : ""Приятели решили выпить на дорожку в местном баре"",
  ""start1"" : 15,
  ""end1"" : 23,
  ""start2"" : 26,
  ""end2"" : 34,
  ""label"" : false,
  ""gold_sense1"" : 1,
  ""gold_sense2"" : 2
}

How did we collect data?
All text examples were collected from Russe original dataset, already collected by Russian Semantic Evaluation at ACL SIGSLAV. Human assessment was carried out on Yandex.Toloka.

In version 2, we have manually collected in the same format testset.",2018,,,,,
2734,RuWiki-Good,Topic Models,Topic Models,Topic Models,,,Methodology,,,https://huggingface.co/datasets/TopicNet/RuWiki-Good,https://paperswithcode.com/dataset/ruwiki-good,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
2735,RuWorldTree,Logical Reasoning,Logical Reasoning,Logical Reasoning,,,Reasoning,logical-reasoning-on-ruworldtree,Apache-2.0,http://tape-benchmark.com/datasets.html#ruworldtree,https://paperswithcode.com/dataset/ruworldtree,"RuWorldTree is a QA dataset with multiple-choice elementary-level science questions, which evaluate the understanding of core science facts.

Motivation

The WorldTree dataset starts the triad of the Reasoning and Knowledge tasks. The data includes the corpus of factoid utterances of various kinds, complex factoid questions and a corresponding causal chain of facts from the corpus resulting in a correct answer.

The WorldTree design was originally proposed in (Jansen et al., 2018).

An example in English for illustration purposes:

```{
    'question': 'A bottle of water is placed in the freezer. What property of water will change when the water reaches the freezing point? (A) color (B) mass (C) state of matter (D) weight', 

'answer': 'C',

'exam_name': 'MEA',

'school_grade': 5,

'knowledge_type': 'NO TYPE',

'perturbation': 'ru_worldtree',

'episode': [18, 10, 11]

}```

Data Fields


text: a string containing the sentence text
answer: a string with a candidate for the coreference resolution
options: a list of all the possible candidates present in the text
reference: a string containing an anaphor (a word or phrase that refers back to an earlier word or phrase)
homonymia_type: a float corresponding to the type of the structure with syntactic homonymy
label: an integer, either 0 or 1, indicating whether the homonymy is resolved correctly or not
perturbation: a string containing the name of the perturbation applied to text. If no perturbation was applied, the dataset name is used
episode: a list of episodes in which the instance is used. Only used for the train set

Data Splits

The dataset consists of a training set with labeled examples and a test set in two configurations:


raw data: includes the original data with no additional sampling
episodes: data is split into evaluation episodes and includes several perturbations of test for robustness evaluation

We use the same splits of data as in the original English version.

Test Perturbations

Each training episode in the dataset corresponds to seven test variations, including the original test data and six adversarial test sets, acquired through the modification of the original test through the following text perturbations:


ButterFingers: randomly adds noise to data by mimicking spelling mistakes made by humans through character swaps based on their keyboard distance
Emojify: replaces the input words with the corresponding emojis, preserving their original meaning
EDAdelete: randomly deletes tokens in the text
EDAswap: randomly swaps tokens in the text
BackTranslation: generates variations of the context through back-translation (ru -> en -> ru)
AddSent: replaces one or more choice options with a generated one",2018,,,,,
2736,RWC,Language Modelling,Language Modelling,"Language Modelling, Music Information Retrieval, Chord Recognition","Audio, Image, Text",English,Computer Vision,,Custom (non-commercial),https://staff.aist.go.jp/m.goto/RWC-MDB/,https://paperswithcode.com/dataset/rwc,"The RWC (Real World Computing) Music Database is a copyright-cleared music database (DB) that is available to researchers as a common foundation for research. It contains around 100 complete songs with manually labeled section boundaries. For the 50 instruments, individual sounds at half-tone intervals were captured with several variations of playing styles, dynamics, instrument manufacturers and musicians.",,"RWC Music Database: Popular, Classical and Jazz Music Databases",http://ismir2002.ismir.net/proceedings/03-SP04-1.pdf,,,
2737,RWCP_Sound_Scene_Database,Sound Event Localization and Detection,Sound Event Localization and Detection,Sound Event Localization and Detection,"Audio, Image",,Computer Vision,sound-event-localization-and-detection-on-3,,http://www.openslr.org/13/,https://paperswithcode.com/dataset/rwcp-sound-scene-database,"The RWCP Sound Scene Database includes non-speech sounds recorded in an anechoic room, reconstructed signals in various rooms, impulse responses for a microphone array, speech data recorded with the same array, and recordings of background noises. It is intended for use when simulating sound scenes. It was developed by the Real Acoustic Environments Working Group of the Real World Computing Partnership (RWCP). The data was recorded from 1998 to 2000.",1998,,,,,
2738,RWF-2000,Activity Recognition,Activity Recognition,"Activity Recognition, Action Classification, Action Recognition","Image, Video",,Computer Vision,activity-recognition-on-rwf-2000,,https://github.com/mchengny/RWF2000-Video-Database-for-Violence-Detection,https://paperswithcode.com/dataset/rwf-2000,"A database with 2,000 videos captured by surveillance cameras in real-world scenes.",,,,,,
2739,RWSD,Logical Reasoning Reading Comprehension,Logical Reasoning Reading Comprehension,"Logical Reasoning Reading Comprehension, Common Sense Reasoning",,,Reasoning,common-sense-reasoning-on-rwsd,MIT,https://github.com/RussianNLP/RussianSuperGLUE,https://paperswithcode.com/dataset/rwsd,"A Winograd schema is a pair of sentences that differ in only one or two words and that contain an ambiguity that is resolved in opposite ways in the two sentences and requires the use of world knowledge and reasoning for its resolution. The schema takes its name from a well-known example by Terry Winograd.

The set would then be presented as a challenge for AI programs, along the lines of the Turing test. The strengths of the challenge are that it is clear-cut, in that the answer to each schema is a binary choice; vivid, in that it is obvious to non-experts that a program that fails to get the right answers clearly has serious gaps in its understanding; and difficult, in that it is far beyond the current state of the art.

Task Type
Logic and Reasoning, World knowledge. Binary Classification: true/false

Example
{
  ""text"": ""Кубок не помещается в коричневый чемодан, потому что он слишком большой.""
  ""label"": false,
  ""idx"": 5,
  ""target"": {
    ""span1_text"": ""чемодан"",
    ""span2_text"": ""он слишком большой"",
    ""span1_index"": 5,
    ""span2_index"": 8
  },
}

How did we collect data?
All text examples were collected manually translating and adapting original Winograd dataset for Russian. Human assessment was carried out on Yandex.Toloka.",,,,,,
2740,RxR,Vision and Language Navigation,Vision and Language Navigation,Vision and Language Navigation,Text,English,Natural Language Processing,vision-and-language-navigation-on-rxr,,https://github.com/google-research-datasets/RxR,https://paperswithcode.com/dataset/rxr,"Room-Across-Room (RxR) is a multilingual dataset for Vision-and-Language Navigation (VLN) for Matterport3D environments. In contrast to related datasets such as Room-to-Room (R2R), RxR is 10x larger, multilingual (English, Hindi and Telugu), with longer and more variable paths, and it includes and fine-grained visual groundings that relate each word to pixels/surfaces in the environment.",,,,,,
2741,S2B,Systematic Generalization,Systematic Generalization,Systematic Generalization,,,Methodology,,MIT,https://github.com/Near32/SymbolicBehaviourBenchmark,https://paperswithcode.com/dataset/s2b,"Suite of OpenAI Gym-compatible multi-agent reinforcement learning environment centered around meta-referential games to benchmark for behavioral traits pertaining to symbolic behaviours, as described in Santoro et al., 2021, ""Symbolic Behaviours in Artificial Intelligence"", with a primary focus on the following behavioural traits: 


receptive, 
constructive,
malleable, and 
separable.",2021,"Santoro et al., 2021, ""Symbolic Behaviours in Artificial Intelligence""",https://arxiv.org/abs/2102.03406,,,
2742,S2ORC,Text Summarization,Text Summarization,"Text Summarization, Citation Recommendation",Text,English,Natural Language Processing,"text-summarization-on-s2orc, citation-recommendation-on-s2orc",CC BY-NC 2.0,https://allenai.org/data/s2orc,https://paperswithcode.com/dataset/s2orc,"A large corpus of 81.1M English-language academic papers spanning many academic disciplines. Rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. Aggregated papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date.",,,,,,
2743,S3DIS,3D Instance Segmentation,3D Instance Segmentation,"3D Instance Segmentation, 3D Open-Vocabulary Instance Segmentation, Semantic Segmentation, Panoptic Segmentation, Few-shot 3D Point Cloud Semantic Segmentation, Generalized Zero-Shot Learning, 3D Object Detection, 3D Semantic Segmentation","3D, Image",,Computer Vision,"semantic-segmentation-on-s3dis, semantic-segmentation-on-s3dis-area5, panoptic-segmentation-on-s3dis-area5, 3d-instance-segmentation-on-s3dis, generalized-zero-shot-learning-on-s3dis, few-shot-3d-point-cloud-semantic-segmentation, 3d-object-detection-on-s3dis, 3d-open-vocabulary-instance-segmentation-on-2, panoptic-segmentation-on-s3dis, 3d-semantic-segmentation-on-s3dis",,http://buildingparser.stanford.edu/dataset.html,https://paperswithcode.com/dataset/s3dis,The Stanford 3D Indoor Scene Dataset (S3DIS) dataset contains 6 large-scale indoor areas with 271 rooms. Each point in the scene point cloud is annotated with one of the 13 semantic categories.,,Grid-GCN for Fast and Scalable Point Cloud Learning,https://arxiv.org/abs/1912.02984,,,
2744,S3E,Simultaneous Localization and Mapping,Simultaneous Localization and Mapping,Simultaneous Localization and Mapping,Image,,Computer Vision,,Apache-2.0 license,https://github.com/PengYu-Team/S3E,https://paperswithcode.com/dataset/s3e,"S3E is a novel large-scale multimodal dataset captured by a fleet of unmanned ground vehicles along four designed collaborative trajectory paradigms. S3E consists of 7 outdoor and 5 indoor scenes that each exceed 200 seconds, consisting of well synchronized and calibrated high-quality stereo camera, LiDAR, and high-frequency IMU data.",,S3E: A Large-scale Multimodal Dataset for Collaborative SLAM,https://arxiv.org/pdf/2210.13723v1.pdf,,,
2745,SA-1B,Image Segmentation,Image Segmentation,"Image Segmentation, Segmentation",Image,,Computer Vision,segmentation-on-sa-1b,,https://segment-anything.com,https://paperswithcode.com/dataset/sa-1b,"SA-1B consists of 11M diverse, high resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks.",,Segment Anything,https://arxiv.org/pdf/2304.02643v1.pdf,,,
2746,Saarbruecken_Voice_Database,Voice pathology detection,Voice pathology detection,Voice pathology detection,"Audio, Image",,Computer Vision,,N/A,https://stimmdb.coli.uni-saarland.de,https://paperswithcode.com/dataset/saarbruecken-voice-database,"Saarbruecken Voice Database contains voice and EGG recordings of patients diagnosed with voice disorder, as well as healthy persons.",,,,,,
2747,SafeEdit,Neural Network Security,Neural Network Security,Neural Network Security,Graph,,Methodology,,CC BY-NC-SA 4.2,,https://paperswithcode.com/dataset/safeedit,"SafeEdit encompasses 4,050 training, 2,700 validation, and 1,350 test instances. 
SafeEdit can be utilized across a range of methods, from supervised fine-tuning to reinforcement learning that demands preference data for more secure responses, as well as knowledge editing methods that require a diversity of evaluation texts.",,,,,,
2748,SAFIM,Text-to-Code Generation,Text-to-Code Generation,"Text-to-Code Generation, Code Generation, Code Completion",Text,English,Natural Language Processing,code-completion-on-safim,CC-BY-4.0,https://safimbenchmark.com,https://paperswithcode.com/dataset/safim,"Syntax-Aware Fill-in-the-Middle (SAFIM) is a benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. SAFIM has three subtasks: Algorithmic Block Completion, Control-Flow Expression Completion, and API Function Call Completion. SAFIM is sourced from code submitted from April 2022 to January 2023 to minimize the impact of data contamination on evaluation results.


Authors: Linyuan Gong, Sida Wang, Mostafa Elhoushi, Alvin Cheung
Paper: https://arxiv.org/abs/2403.04814
Huggingface Dataset: https://huggingface.co/datasets/gonglinyuan/safim
Leaderboard: https://safimbenchmark.com
Code & Submission Instructions: https://github.com/gonglinyuan/safim

The SAFIM benchmark is partially derived from problem descriptions and code solutions from https://codeforces.com. According to the license of CodeForces, you may publish the texts of Codeforces problems in any open sources, but you must preserve a direct link to the site.",2022,https://arxiv.org/abs/2403.04814,https://arxiv.org/abs/2403.04814,,,
2749,SaGA,Dialogue Evaluation,Dialogue Evaluation,Dialogue Evaluation,,,Methodology,,,https://www.researchgate.net/publication/266475499_The_Bielefeld_Speech_and_Gesture_Alignment_Corpus_SaGA,https://paperswithcode.com/dataset/saga,"The primary data of the SaGA corpus are made up of 25 dialogs of interlocutors (50), who engage in a spatial communication task combining direction-giving and sight description. Six of those dialogues with data only from the direction giver are available including audio (.wav) and video (.mp4) data. The secondary data consists of annotations (*.eaf) of gestures and speech-gesture referents, which have been completely and systematically annotated based on an annotation grid (cf. the SaGA documentation). The corpus is comprised of of 9881 isolated words and 1764 isolated gestures. The stimulus is a model of a town presented in a Virtual Reality (VR) environment. Upon finishing a ""bus ride"" through the VR town along five landmarks, a router explained the route as well as the wayside landmarks to an unknown and naive follower. The SaGA Corpus was curated for CLARIN as part of the Curation Project ""Editing and Integration of Multimodal Resources in CLARIN-D"" by the CLARIN-D Working Group 6 ""Speech and Other Modalities"".",,,,,,
2750,SAGC-A68,Graph Learning,Graph Learning,"Graph Learning, Node Classification","Graph, Image",,Computer Vision,,,https://doi.org/10.5281/zenodo.7805872,https://paperswithcode.com/dataset/sagc-a68,"The analysis of building models for usable area, building safety, and energy efficiency requires accurate classification data of spaces and space elements. To reduce input model preparation effort and errors, automated classification of spaces and space elements is desirable. Although existing space function classifiers use space adjacency or connectivity graphs as input, the application of Graph Deep Learning (GDL) to space layout element classification has not been extensively researched due to the lack of suitable datasets. To bridge this gap, we introduce a dataset named SAGC-A68, which comprises access graphs automatically generated from 68 digital 3D models of space layouts of apartment buildings designed or built between 1952 and 2019 in 13 countries. Each access graph contains nodes representing spaces and space elements and edges representing the connection between them. Nodes are uniquely identified and characterized by 16 features including “Position X”, “Position Y”, “Position Z”, “Width”, “Height”, “Depth”, “Area”, “Volume”, “Is_internal”, “Door_opening_quantity”, “Window_quantity”, “Max_door_width”,” Encloses_ws”, “Is_contained_in_ws”, ”bounding_box”, and “Label” (28 identified labels are shown in bold type in Table 1). Edges are identified by a unique ID and characterized by three features, including “Z_angle”, “Delta_z”, and “Length”. In total, the dataset comprises 4871 nodes and 4566 edges, including disconnected nodes representing shafts. It is suitable for developing GDL models for space element and space function classification in Building information modeling (BIM) authoring systems.",1952,,,,,
2751,Saint_Gall,Data Augmentation,Data Augmentation,"Data Augmentation, Handwriting Recognition, Handwritten Text Recognition","Image, Text",English,Computer Vision,handwritten-text-recognition-on-saint-gall,,,https://paperswithcode.com/dataset/saint-gall,"Saint Gall dataset contains handwritten historical manuscripts written in Latin that date back to the 9th century. It consists of 60 pages, 1 410 text lines and 11 597 words.",,,,,,
2752,SALICON,Few-Shot Transfer Learning for Saliency Prediction,Few-Shot Transfer Learning for Saliency Prediction,"Few-Shot Transfer Learning for Saliency Prediction, Saliency Prediction",Time Series,,Methodology,"few-shot-transfer-learning-on-salicon-2, few-shot-transfer-learning-on-salicon, few-shot-transfer-learning-on-salicon-1, saliency-prediction-on-salicon, few-shot-transfer-learning-on-salicon-3",Creative Commons Attribution 4.0 License,http://salicon.net/,https://paperswithcode.com/dataset/salicon,"The SALIency in CONtext (SALICON) dataset contains 10,000 training images, 5,000 validation images and 5,000 test images for saliency prediction. This dataset has been created by annotating saliency in images from MS COCO.
The ground-truth saliency annotations include fixations generated from mouse trajectories. To improve the data quality, isolated fixations with low local density have been excluded.
The training and validation sets, provided with ground truth, contain the following data fields: image, resolution and gaze.
The testing data contains only the image and resolution fields.",,DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations,https://arxiv.org/abs/1510.02927,,"training images, 5,000 validation images",
2753,Salient-KITTI,Saliency Prediction,Saliency Prediction,Saliency Prediction,Time Series,English,Methodology,,,https://github.com/Saixiaoma/SBA-SLAM,https://paperswithcode.com/dataset/salient-kitti,Salient-KITTI is a saliency map prediction dataset based on KITTI.,,https://arxiv.org/pdf/2012.11863.pdf,https://arxiv.org/pdf/2012.11863.pdf,,,
2754,SALMon,Language Modelling,Language Modelling,"Language Modelling, Acoustic Modelling","Audio, Text",English,Natural Language Processing,language-modelling-on-salmon,CC-BY-NC 4.0,https://pages.cs.huji.ac.il/adiyoss-lab/salmon/,https://paperswithcode.com/dataset/salmon,"The SALMon dataset and benchmark was introduced in the paper ""A Suite for Acoustic Language Model Evaluation"", with the goal of evaluating the modelling abilities of speech language models with regards to different kinds of acoustic elements.

It is built of several sub tasks, each task has 200 pairs of recordings - one considered positive and one negative. The positive recording is meant to be a more likely, realistic sample whereas the negative is less likely by some specific means.

The sub tasks can be categorised into two main categories: acoustic consistency and semantic-acoustic alignment. In semantic consistency, the positive sample is a real recording and the negative one is with the same spoken content but an acoustic feature (e.g. speaker) changes mid recording. In the alignment sub-task, the positive recording is one where the text matches the acoustic element (e.g sentiment) and the negative is where they don't.

See also the homepage or HuggingFace.",,A Suite for Acoustic Language Model Evaluation,https://arxiv.org/abs/2409.07437,,,
2755,SALSA,Gesture Recognition,Gesture Recognition,"Gesture Recognition, Dictionary Learning, Pose Estimation, Person Re-Identification","3D, Image",,Computer Vision,pose-estimation-on-salsa,CC BY-NC-SA 3.0,http://tev.fbk.eu/salsa,https://paperswithcode.com/dataset/salsa,A novel dataset facilitating multimodal and Synergetic sociAL Scene Analysis.,,,,,,
2756,SAMRS,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, The Semantic Segmentation Of Remote Sensing Imagery",Image,,Computer Vision,,,https://github.com/ViTAE-Transformer/SAMRS,https://paperswithcode.com/dataset/samrs,"SAMRS is a remote sensing segmentation dataset which provides object category, location, and instance information that can be used for semantic segmentation, instance segmentation, and object detection, either individually or in combination.",,Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model,https://arxiv.org/pdf/2305.02034v1.pdf,,,
2757,SAMSum,Text Summarization,Text Summarization,"Text Summarization, dialogue summary, Abstractive Text Summarization, Summarization, Federated Learning, Abstractive Dialogue Summarization",Text,English,Natural Language Processing,"dialogue-summary-on-samsum-corpus, summarization-on-samsum-1, summarization-on-samsum-corpus-a-human, text-summarization-on-samsum-corpus, abstractive-dialogue-summarization-on-samsum",,https://arxiv.org/src/1911.12237v2/anc,https://paperswithcode.com/dataset/samsum-corpus,A new dataset with abstractive dialogue summaries.,,Homepage,https://arxiv.org/src/1911.12237v2/anc,,,
2758,Santa_Clara_Reservoir_Levels,Time Series Analysis,Time Series Analysis,"Time Series Analysis, Irregular Time Series",Time Series,,Time Series,,MIT,https://github.com/davidanastasiu/NECPlus/tree/main/data,https://paperswithcode.com/dataset/santa-clara-reservoir-levels,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
2759,SARC,Sarcasm Detection,Sarcasm Detection,Sarcasm Detection,Image,,Computer Vision,"sarcasm-detection-on-sarc-all-bal, sarcasm-detection-on-sarc-pol-unbal, sarcasm-detection-on-sarc-pol-bal",,https://nlp.cs.princeton.edu/SARC/,https://paperswithcode.com/dataset/sarc,"This dataset was designed for contextual investigations, with related works making considerable usage of said context. The dataset was constructed by scraping Reddit comments; with sarcastic entries being self-annotated by authors through the use of the \s token, which indicates sarcastic intent on the website. Posts on Reddit are often in response to another comment; SARC incorporates this information through the addition of the parent comment and further child comments surrounding a post.",,DEEP AND DENSE SARCASM DETECTION,https://arxiv.org/pdf/1911.07474v2.pdf,,,
2760,Sarcasm_Corpus_V2,Common Sense Reasoning,Common Sense Reasoning,"Common Sense Reasoning, Sarcasm Detection",Image,,Reasoning,,,https://nlds.soe.ucsc.edu/sarcasm2,https://paperswithcode.com/dataset/sarcasm-corpus-v2,"The Sarcasm Corpus contains sarcastic and non-sarcastic utterances of three different types, which are balanced with half of the samples being sarcastic and half non-sarcastic.
The three types are:


Generic: 6,520 samples
Rhetorical Questions: 1,702 samples
Hyperbole: 1,164 samples",,Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue,https://arxiv.org/pdf/1709.05404.pdf,520 samples,,
2761,Satlas,Object Detection,Object Detection,"Object Detection, regression, Instance Segmentation, Semantic Segmentation",Image,,Computer Vision,,,https://satlas.allenai.org/,https://paperswithcode.com/dataset/satlas,"Satlas is a remote sensing dataset and benchmark that is large in both breadth, featuring all of the aforementioned applications and more, as well as scale, comprising 290M labels under 137 categories and 7 label modalities.",,"Satlas: A Large-Scale, Multi-Task Dataset for Remote Sensing Image Understanding",https://arxiv.org/pdf/2211.15660v1.pdf,,,137
2762,satnet-sudoku,Game of Sudoku,Game of Sudoku,Game of Sudoku,,,Methodology,,MIT,https://github.com/locuslab/SATNet,https://paperswithcode.com/dataset/satnet-sudoku,"A set of easy Sudoku instances used in the SATNet paper for training SatNet on how to learn to play Sudoku.

The instances are easy (plenty of hints) and it is therefore rather easy to get high accuracy on these. More challenging instances are available in the rrn-sudoku dataset.",,,,,,
2763,satp-zsm-stage1,Word Translation,Word Translation,Word Translation,Text,English,Natural Language Processing,,CC BY-NC-ND 4.0,https://doi.org/10.21979/N9/0NE37R,https://paperswithcode.com/dataset/satp-zsm-stage1,"This is the replication data for the paper: ""Crossing the Linguistic Causeway: A Binational Approach for Translating Soundscape Attributes to Bahasa Melayu"".


contains survey responses to a quantitative evaluation survey adopted from Watcharasupat et al., 2022",2022,,,,,
2764,satp-zsm-stage2,Word Translation,Word Translation,Word Translation,Text,English,Natural Language Processing,,CC BY-NC 4.0,https://doi.org/10.21979/N9/9AZ21T,https://paperswithcode.com/dataset/satp-zsm-stage2,"This is the replication data for the paper: ""Crossing the Linguistic Causeway: Ethnonational Differences on Soundscape Attributes in Bahasa Melayu"".",,,,,,
2765,SAVOIAS,Matrix Completion,Matrix Completion,Matrix Completion,,,Methodology,,,https://github.com/esaraee/Savoias-Dataset,https://paperswithcode.com/dataset/savoias,"A visual complexity dataset that compromises of more than 1,400 images from seven image categories relevant to the above research areas, namely Scenes, Advertisements, Visualization and infographics, Objects, Interior design, Art, and Suprematism. The images in each category portray diverse characteristics including various low-level and high-level features, objects, backgrounds, textures and patterns, text, and graphics.",,,,400 images,,
2766,SB20,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Instance Segmentation, Panoptic Segmentation, Neural Rendering",Image,,Computer Vision,,,http://agrobotics.uni-bonn.de/sugar_beet_2020_dataset/,https://paperswithcode.com/dataset/sb20,"Video sequences captured at a field on Campus Kleinaltendorf (CKA), University of Bonn, captured by BonBot-I, an autonomous weeding robot. The data was captured by mounting an Intel  RealSense D435i sensor with a nadir view of the ground.


RGB-D video sequences (Intel RealSense D435i cameras).
Robot odometry and IMU.
Crops and 8 different categories of weeds at different growth stages.
Different illumination conditions.
Three herbicide treatment regimes (30%, 70%, 100%), impacting weed density directly.
High quality sparese instance segmentation labels.",,,,,,
2767,SBD,Edge Detection,Edge Detection,"Edge Detection, Interactive Segmentation, Semantic Contour Prediction","Image, Time Series",,Computer Vision,"semantic-contour-prediction-on-sbd-val, interactive-segmentation-on-sbd, edge-detection-on-sbd",,http://home.bharathh.info/pubs/codes/SBD/download.html,https://paperswithcode.com/dataset/sbd,"The Semantic Boundaries Dataset (SBD) is a dataset for predicting pixels on the boundary of the object (as opposed to the inside of the object with semantic segmentation). The dataset consists of 11318 images from the trainval set of the PASCAL VOC2011 challenge, divided into 8498 training and 2820 test images. This dataset has object instance boundaries with accurate figure/ground masks that are also labeled with one of 20 Pascal VOC classes.",,Weakly Supervised Object Boundaries,https://arxiv.org/abs/1511.07803,11318 images,"trainval set of the PASCAL VOC2011 challenge, divided into 8498 training and 2820 test images",
2768,SBS_Figures,Chart Understanding,Chart Understanding,"Chart Understanding, Chart Question Answering",Text,English,Natural Language Processing,,,https://omron-sinicx.github.io/SBSFiguresPage/,https://paperswithcode.com/dataset/sbsfigures,"Building a large-scale figure QA dataset requires a considerable amount of work, from gathering and selecting figures to extracting attributes like text, numbers, and colors, and generating QAs. Although recent developments in LLMs have led to efforts to synthesize figures, most of these focus primarily on QA generation. Additionally, creating figures directly using LLMs often encounters issues such as code errors, similar-looking figures, and repetitive content in figures. To address this issue, we present SBSFigures (Stage-by-Stage Synthetic Figures), a dataset for pre-training figure QA. Our proposed pipeline enables the creation of chart figures with complete annotations of the visualized data and dense QA annotations without any manual annotation process. Our stage-by-stage pipeline makes it possible to create diverse topic and appearance figures efficiently while minimizing code errors. Our SBSFigures demonstrate a strong pre-training effect, making it possible to achieve efficient training with a limited amount of real-world chart data starting from our pre-trained weights.",,,,,,
2769,SBU___SBU-Refine,Skeleton Based Action Recognition,Skeleton Based Action Recognition,"Skeleton Based Action Recognition, Human Interaction Recognition, RGB Salient Object Detection, Shadow Detection","Image, Video",,Computer Vision,"shadow-detection-on-sbu, human-interaction-recognition-on-sbu, skeleton-based-action-recognition-on-sbu, salient-object-detection-on-sbu",,https://github.com/hanyangclarence/SILT/releases/tag/refined_sbu,https://paperswithcode.com/dataset/sbu,"SBU-Kinect-Interaction dataset version 2.0 comprises of RGB-D video sequences of humans performing interaction activities that are recording using the Microsoft Kinect sensor. This dataset was originally recorded for a class project, and it must be used only for the purposes of research. If you use this dataset in your work, please cite the following paper.
Kiwon Yun, Jean Honorio, Debaleena Chattopadhyay, Tamara L. Berg, and Dimitris Samaras, The 2nd International Workshop on Human Activity Understanding from 3D Data at Conference on Computer Vision and Pattern Recognition (HAU3D-CVPRW), CVPR 2012
SBU-Refine: SBU-Refine relabels the test set manually and refines the noise labels in training set by algorithm. H. Yang, T. Wang, X. Hu, and C.-W. Fu, “SILT: Shadow-aware iterative label tuning for learning to detect shadows from noisy labels,” in ICCV, 2023, pp. 12 687–12 698.",2012,,,,,
2770,SC2EGSet__StarCraft_II_Esport_Game_State_Dataset,Starcraft II,Starcraft II,"Starcraft II, Starcraft",,,Methodology,,CC BY 4.0 International,https://doi.org/10.5281/zenodo.5503997,https://paperswithcode.com/dataset/sc2egset-starcraft-ii-esport-game-state,"SC2EGSet: StarCraft II Esport Game State Dataset

Pre-processed data that was generated from the SC2ReSet: StarCraft II Esports Replaypack Set

Data Modeling

Our aplication programing interface (API) implementation supports downloading, unpacking, loading and data access features. Please refer to: https://github.com/Kaszanas/SC2EGSet_Dataset

License Information

This dataset is licensed under the following license: Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)",,,,,,
2771,SC2ReSet__StarCraft_II_Esport_Replaypack_Set,Starcraft II,Starcraft II,"Starcraft II, Starcraft",,,Methodology,,Custom,https://doi.org/10.5281/zenodo.5575796,https://paperswithcode.com/dataset/sc2reset-starcraft-ii-esport-replaypack-set,"Raw StarCraft II data is subject to processing under the Blizzard end user license agreement (EULA), and in special cases Blizzard AI and Machine Learning License may be applied. Please refer to the materials listed below.

The dataset contains data in MPQ format (.SC2Replay) that can be processed with multiple open-source libraries.


https://www.blizzard.com/en-us/legal/fba4d00f-c7e4-4883-b8b9-1b4500a402ea/blizzard-end-user-license-agreement
https://blzdistsc2-a.akamaihd.net/AI_AND_MACHINE_LEARNING_LICENSE.html",,,,,,
2772,SCAN,Semantic Parsing,Semantic Parsing,"Semantic Parsing, Systematic Generalization",Text,English,Natural Language Processing,,BSD License,https://github.com/brendenlake/SCAN,https://paperswithcode.com/dataset/scan,SCAN is a dataset for grounded navigation which consists of a set of simple compositional navigation commands paired with the corresponding action sequences.,,Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks,https://arxiv.org/pdf/1711.00350v3.pdf,,,
2773,ScanNet,3D Semantic Instance Segmentation,3D Semantic Instance Segmentation,"3D Semantic Instance Segmentation, 3D Instance Segmentation, Depth Estimation, Interactive 3D Instance Segmentation, Semantic Segmentation, Surface Normals Estimation, Zero-shot 3D Point Cloud Classification, 2D Panoptic Segmentation, Interactive 3D Instance Segmentation -Trained on Scannet40 - Evaluated on Scannet40, Panoptic Segmentation, Scene Segmentation, Generalized Zero-Shot Learning, Scene Recognition, 3D Object Detection, 3D Reconstruction, Unsupervised 3D Semantic Segmentation, Continual Semantic Segmentation","3D, Image",,Computer Vision,"3d-object-detection-on-scannetv2, 3d-semantic-instance-segmentation-on, depth-estimation-on-scannetv2, scene-segmentation-on-scannet, 3d-semantic-instance-segmentation-on-1, interactive-3d-instance-segmentation-trained, 3d-instance-segmentation-on-scannetv2, 2d-panoptic-segmentation-on-scannetv2, depth-estimation-on-scannet, 3d-instance-segmentation-on-scannet, generalized-zero-shot-learning-on-scannet, interactive-3d-instance-segmentation-on, panoptic-segmentation-on-scannet, semantic-segmentation-on-scannet, zero-shot-3d-point-cloud-classification-on-1, semantic-segmentation-on-scannetv2, panoptic-segmentation-on-scannetv2, scene-recognition-on-scannet, unsupervised-3d-semantic-segmentation-on, continual-semantic-segmentation-on-scannet, surface-normals-estimation-on-scannetv2, 3d-reconstruction-on-scannet",Custom,http://www.scan-net.org/,https://paperswithcode.com/dataset/scannet,"ScanNet is an instance-level indoor RGB-D dataset that includes both 2D and 3D data. It is a collection of labeled voxels rather than points or objects. Up to now, ScanNet v2, the newest version of ScanNet, has collected 1513 annotated scans with an approximate 90% surface coverage. In the semantic segmentation task, this dataset is marked in 20 classes of annotated 3D voxelized objects.",,A Review of Point Cloud Semantic Segmentation,https://arxiv.org/abs/1908.08854,,,20
2774,ScanNet200,3D Open-Vocabulary Instance Segmentation,3D Open-Vocabulary Instance Segmentation,"3D Open-Vocabulary Instance Segmentation, 3D Instance Segmentation, 3D Semantic Segmentation","3D, Image",,Computer Vision,"3d-open-vocabulary-instance-segmentation-on, 3d-semantic-segmentation-on-scannet200, 3d-instance-segmentation-on-scannet200",,http://kaldir.vc.in.tum.de/scannet_benchmark/,https://paperswithcode.com/dataset/scannet200,"The ScanNet200 benchmark studies 200-class 3D semantic segmentation - an order of magnitude more class categories than previous 3D scene understanding benchmarks. The source of scene data is identical to ScanNet, but parses a larger vocabulary for semantic and instance segmentation",,,,,,
2775,ScanObjectNN,Training-free 3D Point Cloud Classification,Training-free 3D Point Cloud Classification,"Training-free 3D Point Cloud Classification, 3D Point Cloud Classification, 3D Point Cloud Data Augmentation, 3D Point Cloud Linear Classification, Zero-shot 3D Point Cloud Classification, 3D Parameter-Efficient Fine-Tuning for Classification, Few-Shot Point Cloud Classification, Zero-shot 3D Point Cloud Classificationclassification, Supervised Only 3D Point Cloud Classification, Zero-Shot Transfer 3D Point Cloud Classification","3D, Image",,Computer Vision,"few-shot-point-cloud-classification-on-2, zero-shot-transfer-3d-point-cloud-2, zero-shot-3d-point-cloud, training-free-3d-point-cloud-classification-1, 3d-point-cloud-data-augmentation-on-1, 3d-point-cloud-linear-classification-on-1, 3d-parameter-efficient-fine-tuning-for, 3d-point-cloud-classification-on-scanobjectnn, supervised-only-3d-point-cloud-classification, zero-shot-3d-point-cloud-classification-on-3",Custom (research-only),https://hkust-vgd.github.io/scanobjectnn/,https://paperswithcode.com/dataset/scanobjectnn,"ScanObjectNN is a newly published real-world dataset comprising of 2902 3D objects in 15 categories. It is a challenging point cloud classification datasets due to the background, missing parts and deformations.",,A Self Contour-based Rotation and Translation-Invariant Transformation for Point Clouds Recognition,https://arxiv.org/abs/2009.06903,,,15
2776,ScanRefer_Dataset,Object Detection,Object Detection,"Object Detection, Object Localization, Sentence Embeddings",Image,,Computer Vision,,,https://github.com/daveredrum/ScanRefer,https://paperswithcode.com/dataset/scanrefer-dataset,"Contains 51,583 descriptions of 11,046 objects from 800 ScanNet scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D.",,,,,,
2777,SceneNet,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Scene Understanding, Single-View 3D Reconstruction, 3D Reconstruction, Object Detection, Surface Reconstruction","3D, Image",,Computer Vision,,CC BY-NC 4.0,https://robotvault.bitbucket.io/,https://paperswithcode.com/dataset/scenenet,"SceneNet is a dataset of labelled synthetic indoor scenes. There are several labeled indoor scenes, including:


11 Bedroom scenes with 428 objects
15 Office scenes with 1,203 objects
11 Kitchen scenes with 797 objects
10 Living Room scenes with 715 objects
10 Bathrooms with 556 objects",,,,,,
2778,SceneNN,3D Instance Segmentation,3D Instance Segmentation,3D Instance Segmentation,"3D, Image",,Computer Vision,3d-instance-segmentation-on-scenenn-1,,http://103.24.77.34/scenenn/home/,https://paperswithcode.com/dataset/scenenn,"SceneNN is an RGB-D scene dataset consisting of more than 100 indoor scenes. The scenes are captured at various places, e.g., offices, dormitory, classrooms, pantry, etc., from University of Massachusetts Boston and Singapore University of Technology and Design.
All scenes are reconstructed into triangle meshes and have per-vertex and per-pixel annotation. The dataset is additionally enriched with fine-grained information such as axis-aligned bounding boxes, oriented bounding boxes, and object poses.",,,,,,
2779,SCG,Product Relation Classification,Product Relation Classification,"Product Relation Classification, Production Forecasting, Demand Forecasting, Product Categorization, Product Relation Detection","Graph, Image, Time Series",,Computer Vision,"product-categorization-on-scg, product-relation-classification-on-scg, production-forecasting-on-scg, demand-forecasting-on-scg, product-relation-detection-on-scg",MIT,https://doi.org/10.5281/zenodo.13652826,https://paperswithcode.com/dataset/scg,"Abstract: Graph Neural Networks (GNNs) have recently gained traction in transportation, bioinformatics, language and image processing, but research on their application to supply chain management remains limited. Supply chains are inherently graph-like, making them ideal for GNN methodologies, which can optimize and solve complex problems. The barriers include a lack of proper conceptual foundations, familiarity with graph applications in SCM, and real-world benchmark datasets for GNN-based supply chain research. To address this, we discuss and connect supply chains with graph structures for effective GNN application, providing detailed formulations, examples, mathematical definitions, and task guidelines. Additionally, we present a multi-perspective real-world benchmark dataset from a leading FMCG company in Bangladesh, focusing on supply chain planning. We discuss various supply chain tasks using GNNs and benchmark several state-of-the-art models on homogeneous and heterogeneous graphs across six supply chain analytics tasks. Our analysis shows that GNN-based models consistently outperform statistical ML and other deep learning models by around 10-30% in regression, 10-30% in classification and detection tasks, and 15-40% in anomaly detection tasks on designated metrics. With this work, we lay the groundwork for solving supply chain problems using GNNs, supported by conceptual discussions, methodological insights, and a comprehensive dataset.",,,,,,
2780,Schiller,Data Augmentation,Data Augmentation,"Data Augmentation, Handwriting generation, Handwriting Recognition","Image, Text",English,Computer Vision,,,http://icfhr2018.org/competitions.html,https://paperswithcode.com/dataset/shiller,"Schiller contains handwritten texts written in modern German. Train sample consists of 244 lines, validation - 21 lines and test - 63 lines.",,,,,,
2781,Schirrmeister2017_MOABB,Within-Session Motor Imagery (right hand vs. feet),Within-Session Motor Imagery (right hand vs. feet),"Within-Session Motor Imagery (right hand vs. feet), Within-Session Motor Imagery (left hand vs. right hand), Within-Session Motor Imagery (all classes)",,,Methodology,"within-session-motor-imagery-all-classes-on-4, within-session-motor-imagery-right-hand-vs-6, within-session-motor-imagery-left-hand-vs-6",,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Schirrmeister2017.html,https://paperswithcode.com/dataset/schirrmeister2017-moabb,,,,,,,
2782,SciCite,Citation Intent Classification,Citation Intent Classification,"Citation Intent Classification, Sentence Classification",Image,,Computer Vision,"citation-intent-classification-on-scicite, sentence-classification-on-scicite, sentence-classification-on-sciencecite",,https://github.com/allenai/scicite,https://paperswithcode.com/dataset/scicite,SciCite is a dataset of citation intents that addresses multiple scientific domains and is more than five times larger than ACL-ARC.,,Structural Scaffolds for Citation Intent Classification in Scientific Publications,https://arxiv.org/abs/1904.01608,,,
2783,SciCo,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Cross Document Coreference Resolution",Text,English,Natural Language Processing,,Apache-2.0,https://scico.apps.allenai.org/,https://paperswithcode.com/dataset/scico,"SciCo is an expert-annotated dataset for hierarchical CDCR (cross-document coreference resolution) for concepts in scientific papers, with the goal of jointly inferring coreference clusters and hierarchy between them.",,,,,,
2784,SciDocs,Language Modelling,Language Modelling,"Language Modelling, Representation Learning, Re-Ranking, Text Retrieval, Document Classification, Zero-shot Text Search","Image, Text",English,Computer Vision,"zero-shot-text-search-on-scidocs, re-ranking-on-scidocs, representation-learning-on-scidocs, text-retrieval-on-scidocs",,https://github.com/allenai/scidocs,https://paperswithcode.com/dataset/scidocs,SciDocs evaluation framework consists of a suite of evaluation tasks designed for document-level tasks.,,,,,,
2785,ScienceQA,Open-Domain Question Answering,Open-Domain Question Answering,"Open-Domain Question Answering, Question Answering, Visual Commonsense Reasoning, Multimodal Deep Learning, Explainable Models, Science Question Answering, Visual Question Answering (VQA)","Image, Text",English,Computer Vision,science-question-answering-on-scienceqa,CC BY-NC-SA,https://scienceqa.github.io/,https://paperswithcode.com/dataset/scienceqa,"Science Question Answering (ScienceQA) is a new benchmark that consists of 21,208 multimodal multiple choice questions with diverse science topics and annotations of their answers with corresponding lectures and explanations. Out of the questions in ScienceQA, 10,332 (48.7%) have an image context, 10,220 (48.2%) have a text context, and 6,532 (30.8%) have both. Most questions are annotated with grounded lectures (83.9%) and detailed explanations (90.5%). The lecture and explanation provide general external knowledge and specific reasons, respectively, for arriving at the correct answer. To the best of our knowledge, ScienceQA is the first large-scale multimodal dataset that annotates lectures and explanations for the answers.

ScienceQA, in contrast to previous datasets, has richer domain diversity from three subjects: natural science, language science, and social science. Questions in each subject are categorized first by the topic (Biology, Physics, Chemistry, etc.), then by the category (Plants, Cells, Animals, etc.), and finally by the skill (Classify fruits and vegetables as plant parts, Identify countries of Africa, etc.). ScienceQA features 26 topics, 127 categories, and 379 skills that cover a wide range of domains.",,,,,,127
2786,SciERC,UIE,UIE,"UIE, Named Entity Recognition, Continual Pretraining, Joint Entity and Relation Extraction, Few-Shot Relation Classification, Relation Extraction, Named Entity Recognition (NER)","Graph, Image, Text",English,Computer Vision,"relation-extraction-on-scierc, named-entity-recognition-ner-on-scierc, uie-on-scierc, joint-entity-and-relation-extraction-on, continual-pretraining-on-scierc, few-shot-relation-classification-on-scierc, relation-extraction-on-scierc-sent, named-entity-recognition-on-scierc",,http://nlp.cs.washington.edu/sciIE/,https://paperswithcode.com/dataset/scierc,"SciERC dataset is a collection of 500 scientific abstract annotated with scientific entities, their relations, and coreference clusters. The abstracts are taken from 12 AI conference/workshop proceedings in four AI communities, from the Semantic Scholar Corpus. SciERC extends previous datasets in scientific articles SemEval 2017 Task 10 and SemEval 2018 Task 7 by extending entity types, relation types, relation coverage, and adding cross-sentence relations using coreference links.",2017,,,,,
2787,SciFact,Text Retrieval,Text Retrieval,"Text Retrieval, Zero-shot Text Search",Text,English,Natural Language Processing,"zero-shot-text-search-on-scifact, text-retrieval-on-scifact",CC BY-NC 2.0,https://allenai.org/data/scifact,https://paperswithcode.com/dataset/scifact,"SciFact is a dataset of 1.4K expert-written claims, paired with evidence-containing abstracts annotated with veracity labels and rationales.",,,,,,
2788,Scifi_TV_Shows,Event Expansion,Event Expansion,"Event Expansion, Story Completion, Story Generation",Text,English,Natural Language Processing,event-expansion-on-scifi-tv-shows,Creative Commons Attribution 4.0 International,https://huggingface.co/datasets/lara-martin/Scifi_TV_Shows,https://paperswithcode.com/dataset/scifi-tv-plots,"A collection of long-running (80+ episodes) science fiction TV show synopses, scraped from Fandom.com wikis. Collected Nov 2017. Each episode is considered a ""story"".

Contains plot summaries from :


Babylon 5 (https://babylon5.fandom.com/wiki/Main_Page) - 84 stories
Doctor Who (https://tardis.fandom.com/wiki/Doctor_Who_Wiki) - 311 stories
Doctor Who spin-offs - 95 stories
Farscape (https://farscape.fandom.com/wiki/Farscape_Encyclopedia_Project:Main_Page) - 90 stories
Fringe (https://fringe.fandom.com/wiki/FringeWiki) - 87 stories
Futurama (https://futurama.fandom.com/wiki/Futurama_Wiki) - 87 stories
Stargate (https://stargate.fandom.com/wiki/Stargate_Wiki) - 351 stories
Star Trek (https://memory-alpha.fandom.com/wiki/Star_Trek) - 701 stories
Star Wars books (https://starwars.fandom.com/wiki/Main_Page) - 205 stories
Star Wars Rebels - 65 stories
X-Files (https://x-files.fandom.com/wiki/Main_Page) - 200 stories

Total: 2276 stories

Dataset is ""eventified"" and generalized (see LJ Martin, P Ammanabrolu, X Wang, W Hancock, S Singh, B Harrison, and MO Riedl. Event Representations for Automated Story Generation with Deep Neural Nets, Thirty-Second AAAI Conference on Artificial Intelligence (AAAI), 2018. for details on these processes.) and split into train-test-validation sets for converting events into full sentences.",2017,,,,,
2789,SciGraphQA,Multimodal Text Prediction,Multimodal Text Prediction,"Multimodal Text Prediction, Visual Question Answering (VQA), Visual Question Answering","Image, Text, Time Series",English,Multimodal,,,https://github.com/findalexli/SciGraphQA,https://paperswithcode.com/dataset/scigraphqa,"SciGraphQA is a large-scale, open-domain dataset focused on generating multi-turn conversational question-answering dialogues centered around understanding and describing scientific graphs and figures. It contains over 300,000 samples derived from academic research papers in computer science and machine learning domains.

Each sample in ScFiGraphQA consists of a scientific graph image sourced from papers on ArXiv, accompanied by rich textual context including the paper's title, abstract, figure caption, and a paragraph from the paper referencing the figure. Using this comprehensive context, the dataset employs a  to produce multi-turn question-answer dialogues aimed at explaining the given graph in an interactive, conversational format. On average, each sample contains 2-3 turns of question-answer exchange.

The key motivation behind SciGraphQA is providing a large-scale resource to support research and development of multi-modal AI systems that can engage in informative, open-ended conversations about graphs and data visualizations. The multi-turn dialogue format presents a more natural and interactive setting compared to standard visual question answering datasets that use fixed sets of standalone questions.

Potential use cases of SciGraphQA include pre-training and benchmarking multi-modal conversational models for scientific graph comprehension, building AI assistants that can discuss data insights, and developing aids to help individuals understand complex figures and diagrams interactively. The academic source material also provides a way to evaluate model capabilities on expert-level graphs spanning diverse topics and complex visual encodings.",,,,000 samples,,
2790,SciHTC,Hierarchical Multi-label Classification,Hierarchical Multi-label Classification,"Hierarchical Multi-label Classification, Multi-Label Text Classification, Classification, Text Classification","Image, Text",English,Computer Vision,,,https://github.com/msadat3/SciHTC,https://paperswithcode.com/dataset/scihtc,"SciHTC is a dataset for hierarchical multi-label text classification (HMLTC) of scientific papers which contains 186,160 papers and 1,233 categories from the ACM CCS tree.",,Hierarchical Multi-Label Classification of Scientific Documents,https://arxiv.org/pdf/2211.02810v1.pdf,,,233
2791,SciQ,Text Generation,Text Generation,"Text Generation, Distractor Generation, Question Generation, Reading Comprehension",Text,English,Natural Language Processing,text-generation-on-sciq,,https://allenai.org/data/sciq,https://paperswithcode.com/dataset/sciq,"The SciQ dataset contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. For the majority of the questions, an additional paragraph with supporting evidence for the correct answer is provided.",,,,,,
2792,SciRepEval,regression,regression,"regression, Classification, Ad-Hoc Information Retrieval, Information Retrieval",Image,,Computer Vision,,Apache-2.0 license,https://github.com/allenai/scirepeval,https://paperswithcode.com/dataset/scirepeval,"SciRepEval is a comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search.",,SciRepEval: A Multi-Format Benchmark for Scientific Document Representations,https://arxiv.org/pdf/2211.13308v1.pdf,,,
2793,SciREX,Binary Relation Extraction,Binary Relation Extraction,"Binary Relation Extraction, 4-ary Relation Extraction, Event Extraction, Structured Prediction, Question Answering","Graph, Text, Time Series",English,Natural Language Processing,"4-ary-relation-extraction-on-scirex, binary-relation-extraction-on-scirex",,https://github.com/allenai/SciREX,https://paperswithcode.com/dataset/scirex,"SCIREX is a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. The dataset is annotated by integrating automatic and human annotations, leveraging existing scientific knowledge resources.",,SCIREX: A Challenge Dataset for Document-Level Information Extraction,https://arxiv.org/pdf/2005.00512v1.pdf,,,
2794,SciTLDR,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Extreme Summarization",Text,English,Natural Language Processing,,,https://github.com/allenai/scitldr,https://paperswithcode.com/dataset/scitldr,"A new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden.",,,,,,
2795,SciTSR,Table Detection,Table Detection,"Table Detection, Optical Character Recognition (OCR), Information Retrieval","Image, Tabular",,Computer Vision,,,https://github.com/Academic-Hammer/SciTSR,https://paperswithcode.com/dataset/scitsr,"SciTSR is a large-scale table structure recognition dataset, which contains 15,000 tables in PDF format and their corresponding structure labels obtained from LaTeX source files.",,,,,,
2796,ScreenSpot,Natural Language Visual Grounding,Natural Language Visual Grounding,Natural Language Visual Grounding,"Image, Text",English,Computer Vision,natural-language-visual-grounding-on,Apache-2.0,,https://paperswithcode.com/dataset/screenspot,"ScreenSpot Evaluation Benchmark
ScreenSpot is an evaluation benchmark for GUI grounding, comprising over 1,200 instructions from various environments, including iOS, Android, macOS, Windows, and Web. Each data point includes annotated element types (Text or Icon/Widget). For more details and examples, please refer to our paper.

Test Sample Details
Each test sample includes:


img_filename: The interface screenshot file.
instruction: Human-provided instruction.
bbox: The bounding box of the target element corresponding to the instruction.
data_type: The type of the target element, either ""icon"" or ""text"".
data_source: The interface platform, which could be iOS, Android, macOS, Windows, or Web (e.g., GitLab, Shop, Forum, Tool).",,paper,https://arxiv.org/abs/2401.10935,,"valuation benchmark for GUI grounding, comprising over 1,200 instructions from various environments, including iOS, Android, macOS, Windows, and Web. Each data point includes annotated element types (Text or Icon/Widget). For more details and examples",
2797,ScribbleKITTI,Weakly-Supervised Semantic Segmentation,Weakly-Supervised Semantic Segmentation,"Weakly-Supervised Semantic Segmentation, Weakly Supervised 3D Point Cloud Segmentation, LIDAR Semantic Segmentation, Semi-Supervised Semantic Segmentation, 3D Semantic Segmentation","3D, Image",English,Computer Vision,"3d-semantic-segmentation-on-scribblekitti, semi-supervised-semantic-segmentation-on-23",,https://github.com/ouenal/scribblekitti,https://paperswithcode.com/dataset/scribblekitti,ScribbleKITTI is a scribble-annotated dataset for LiDAR semantic segmentation.,,,,,,
2798,ScribbleSup,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Weakly-Supervised Semantic Segmentation, Instance Segmentation",Image,,Computer Vision,,,https://jifengdai.org/downloads/scribble_sup/,https://paperswithcode.com/dataset/scribblesup,"The PASCAL-Scribble Dataset is an extension of the PASCAL dataset with scribble annotations for semantic segmentation. The annotations follow two different protocols. In the first protocol, the PASCAL VOC 2012 set is annotated, with 20 object categories (aeroplane, bicycle, ...) and one background category. There are 12,031 images annotated, including 10,582 images in the training set and 1,449 images in the validation set.
In the second protocol, the 59 object/stuff categories and one background category involved in the PASCAL-CONTEXT dataset are used. Besides the 20 object categories in the first protocol, there are 39 extra categories (snow, tree, ...) included. This protocol is followed to annotate the PASCAL-CONTEXT dataset. 4,998 images in the training set have been annotated.",2012,,,031 images,"training set and 1,449 images",
2799,SCROLLS,Text Summarization,Text Summarization,"Text Summarization, Long-range modeling, Question Answering, Natural Language Inference",Text,English,Natural Language Processing,long-range-modeling-on-scrolls,MIT,https://www.scrolls-benchmark.com,https://paperswithcode.com/dataset/scrolls,"SCROLLS (Standardized CompaRison Over Long Language Sequences) is an NLP benchmark consisting of a suite of tasks that require reasoning over long texts. SCROLLS contains summarization, question answering, and natural language inference tasks, covering multiple domains, including literature, science, business, and entertainment. The dataset is made available in a unified text-to-text format and host a live leaderboard to facilitate research on model architecture and pretraining methods.

The SCROLLS benchmark contains the datasets GovReport, SummScreenFD, QMSum, QASPER, NarrativeQA, QuALITY and ContractNLI.",,,,,,
2800,SD-198,Skin Cancer Segmentation,Skin Cancer Segmentation,"Skin Cancer Segmentation, Semantic Segmentation, Lesion Segmentation, Skin Lesion Classification, Classification, Partial Label Learning",Image,,Computer Vision,"skin-lesion-classification-on-isic-2019, semantic-segmentation-on-ph2, partial-label-learning-on-isic-2019, skin-cancer-segmentation-on-ph2, lesion-segmentation-on-ph2, classification-on-isic-2019",,https://link.springer.com/chapter/10.1007/978-3-319-46466-4_13,https://paperswithcode.com/dataset/sd-198,"The SD-198 dataset contains 198 different diseases from different types of eczema, acne and various cancerous conditions. There are 6,584 images in total. A subset include the classes with more than 20 image samples, namely SD-128.""",,,,584 images,,
2801,SD-Eval,Spoken Dialogue Systems,Spoken Dialogue Systems,Spoken Dialogue Systems,,,Methodology,,CC BY-NC 4.0,https://github.com/amphionspace/SD-Eval,https://paperswithcode.com/dataset/sd-eval,"Speech encompasses a wealth of information, including but not limited to content, paralinguistic, and environmental information.
This comprehensive nature of speech significantly impacts communication and is crucial for human-computer interaction.
Chat-Oriented Large Language Models (LLMs), known for their general-purpose assistance capabilities, have evolved to handle multi-modal inputs, including speech.
Although these models can be adept at recognizing and analyzing speech, they often fall short of generating appropriate responses.
We argue that this is due to the lack of principles on task definition and model development, which requires open-source datasets and metrics suitable for model evaluation.
To bridge the gap, we present SD-Eval, a benchmark dataset aimed at multidimensional evaluation of spoken dialogue understanding and generation.
SD-Eval focuses on paralinguistic and environmental information and includes 7,303 utterances, amounting to 8.76 hours of speech data. The data is aggregated from eight public datasets, representing four perspectives: emotion, accent, age, and background sound.
To assess the SD-Eval benchmark dataset, we implement three different models and construct a training set following a process similar to that of SD-Eval. The training set contains 1,052.72 hours of speech data and 724.4k utterances. 
We also conduct a comprehensive evaluation using objective evaluation methods (e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the generated responses.
Models conditioned with paralinguistic and environmental information outperform their counterparts in both objective and subjective measures.
Moreover, experiments demonstrate that LLM-based metrics show a higher correlation with human evaluation compared to traditional metrics.
We open-source SD-Eval at https://github.com/amphionspace/SD-Eval.",,,,,,
2802,SD7K,Document Shadow Removal,Document Shadow Removal,"Document Shadow Removal, Image Shadow Removal, Shadow Removal","Image, Text",English,Computer Vision,,,https://cxh-research.github.io/DocShadow-SD7K/,https://paperswithcode.com/dataset/sd7k,"SD7K is the only large-scale high-resolution dataset that satisfies all important data features about document shadow currently, which covers a large number of document shadow images. Mean resolution is $2462 \times 3699$",,,,,,
2803,SDWPF,Prediction,Prediction,"Prediction, Time Series",Time Series,,Methodology,time-series-on-sdwpf,CC BY,https://aistudio.baidu.com/competition/detail/152/,https://paperswithcode.com/dataset/sdwpf,"The unique Spatial Dynamic Wind Power Forecasting dataset: SDWPF, which includes the spatial distribution of wind turbines, as well as the dynamic context factors. Whereas, most of
 the existing datasets have only a small number of wind turbines without knowing the locations and context information of wind turbines at a fine-grained time scale. By contrast, SDWPF provides the wind power data of 134 wind turbines from a wind farm over half a year with their relative positions and internal statuses.",,,,,,
2804,SE-PQA,Community Question Answering,Community Question Answering,Community Question Answering,Text,English,Natural Language Processing,,Creative Commons Attribution 4.0 International,https://github.com/pkasela/SE-PQA,https://paperswithcode.com/dataset/se-pqa,"Personalization in Information Retrieval is a topic studied for a long time. Nevertheless, there is still a lack of high-quality, real-world datasets to conduct large-scale experiments and evaluate models for personalized search. This paper contributes to fill this gap by introducing SE-PQA (StackExchange - Personalized Question Answering), a new resource to design and evaluate personalized models related to the two tasks of community Question Answering (cQA). The contributed dataset includes more than  1 million queries and 2 million answers,  annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. We describe the characteristics of SE-PQA and detail the features associated with both questions and answers. We also provide reproducible baseline methods for the cQA task based on the resource, including deep learning models and personalization approaches. The results of the preliminary experiments conducted show the appropriateness of SE-PQA to train effective cQA models; they also show that personalization improves remarkably the effectiveness of all the methods tested. Furthermore, we show the benefits in terms of robustness and generalization of combining data from multiple communities for personalization purposes.",,,,,,
2805,SearchQA,Open-Domain Question Answering,Open-Domain Question Answering,Open-Domain Question Answering,Text,English,Natural Language Processing,open-domain-question-answering-on-searchqa,,https://github.com/nyu-dl/dl4ir-searchQA,https://paperswithcode.com/dataset/searchqa,"SearchQA was built using an in-production, commercial search engine. It closely reflects the full pipeline of a (hypothetical) general question-answering system, which consists of information retrieval and answer synthesis.",,SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine,https://arxiv.org/pdf/1704.05179.pdf,,,
2806,SECO,Satellite Image Classification,Satellite Image Classification,"Satellite Image Classification, Unsupervised Pre-training",Image,,Computer Vision,,,https://github.com/ServiceNow/seasonal-contrast,https://paperswithcode.com/dataset/seco,Read more about the dataset here: https://github.com/ServiceNow/seasonal-contrast,,,,,,
2807,SECOND,Change Detection,Change Detection,"Change Detection, Building change detection for remote sensing images, Change detection for remote sensing images",Image,,Computer Vision,change-detection-on-second,,https://captain-whu.github.io/SCD/,https://paperswithcode.com/dataset/second,"SECOND is a well-annotated semantic change detection dataset. To ensure data diversity, we firstly collect 4662 pairs of aerial images from several platforms and sensors. These pairs of images are distributed over the cities such as Hangzhou, Chengdu, and Shanghai. Each image has size 512 x 512 and is annotated at the pixel level. The annotation of SECOND is carried out by an expert group of earth vision applications, which guarantees high label accuracy. For the change category in the SECOND dataset, we focus on 6 main land-cover classes, i.e. , non-vegetated ground surface, tree, low vegetation, water, buildings and playgrounds , that are frequently involved in natural and man-made geographical changes. It is worth noticing that, in the new dataset, non-vegetated ground surface ( n.v.g. surface for short) mainly corresponds to impervious surface and bare land. In summary, these 6 selected land-cover categories result in 30 common change categories (including non-change ). Through the random selection of image pairs, the SECOND reflects real distributions of land-cover categories when changes occur.",,,,,,
2808,SEDE,Text-To-SQL,Text-To-SQL,"Text-To-SQL, Semantic Parsing",Text,English,Natural Language Processing,text-to-sql-on-sede,,https://github.com/hirupert/sede,https://paperswithcode.com/dataset/sede,"SEDE is a dataset comprised of 12,023 complex and diverse SQL queries and their natural language titles and descriptions, written by real users of the Stack Exchange Data Explorer out of a natural interaction. These pairs contain a variety of real-world challenges which were rarely reflected so far in any other semantic parsing dataset. The goal of this dataset is to take a significant step towards evaluation of Text-to-SQL models in a real-world setting. Compared to other Text-to-SQL datasets, SEDE contains at least 10 times more SQL queries templates (queries after canonization and anonymization of values) than other datasets, and has the most diverse set of utterances and SQL queries (in terms of 3-grams) out of all single-domain datasets. SEDE introduces real-world challenges, such as under-specification, usage of parameters in queries, dates manipulation and more.",,,,,,
2809,SEED-Bench,Video Understanding,Video Understanding,Video Understanding,Video,,Methodology,,Apache-2.0,https://github.com/ailab-cvc/seed-bench,https://paperswithcode.com/dataset/seed-bench,"SEED-Bench consists of 19K multiple choice questions with accurate human annotations (~6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality.",,,,,,
2810,SEED,EEG Emotion Recognition,EEG Emotion Recognition,"EEG Emotion Recognition, Electroencephalogram (EEG), Emotion Recognition",Image,,Computer Vision,"emotion-recognition-on-seed, eeg-on-seed, eeg-on-seed-iv, eeg-emotion-recognition-on-seed-iv","Custom (research-only, non-commercial)",http://bcmi.sjtu.edu.cn/home/seed/index.html,https://paperswithcode.com/dataset/seed-1,"The SEED dataset contains subjects' EEG signals when they were watching films clips. The film clips are carefully selected so as to induce different types of emotion, which are positive, negative, and neutral ones.",,"Custom (research-only, non-commercial)",http://bcmi.sjtu.edu.cn/home/seed/resource/license/license.pdf,,,
2811,seeds,Clustering Algorithms Evaluation,Clustering Algorithms Evaluation,Clustering Algorithms Evaluation,,,Methodology,clustering-algorithms-evaluation-on-seeds,,https://archive.ics.uci.edu/ml/datasets/seeds,https://paperswithcode.com/dataset/seeds,"The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin.

The data set can be used for the tasks of classification and cluster analysis.",,,,,,
2812,SegTrack-v2,Unsupervised Video Object Segmentation,Unsupervised Video Object Segmentation,"Unsupervised Video Object Segmentation, Semantic Segmentation, Video Semantic Segmentation, Video Salient Object Detection, Unsupervised Object Segmentation, Video Object Segmentation, Video Segmentation","Image, Video",,Computer Vision,"video-object-segmentation-on-segtrack-v2-1, unsupervised-object-segmentation-on-segtrack, video-salient-object-detection-on-segtrack-v2, unsupervised-video-object-segmentation-on-3, video-segmentation-on-segtrack-v2",,https://web.engr.oregonstate.edu/~lif/SegTrack2/dataset.html,https://paperswithcode.com/dataset/segtrack-v2-1,SegTrack v2 is a video segmentation dataset with full pixel-level annotations on multiple objects at each frame within each video.,,,,,,
2813,SeizeIT1,Seizure Detection,Seizure Detection,Seizure Detection,Image,,Computer Vision,,Custom KU Leuven,https://rdr.kuleuven.be/dataset.xhtml?persistentId=doi:10.48804/P5Q0OJ,https://paperswithcode.com/dataset/seizeit1,"This dataset is obtained during an ICON project (2017-2018) in collaboration with KU Leuven (ESAT-STADIUS), UZ Leuven, UCB, Byteflies and Pilipili. The goal of this project was to design a system using Behind the ear (bhE) EEG electrodes for monitoring the patient in a home environment. This way, a nice balance can be found between sufficient accuracy of seizure detection algorithms (because EEG is used) and wearability (bhe EEG is relatively subtle, similar to a hear-aid device). The dataset acquired in the hospital during presurgical evaluation. During such presurgical evaluation, neurologists try to see if a specific part of the brain is causing the seizures, and if so, if that part of the brain can be removed during surgery. During the presurgical evaluation, patients are monitored using the vEEG for multiple days (typically a week). Patients are however restricted to move within their room because of the wiring and video analysis. In this dataset, following data is available per patient: • Full 10-20 scalp EEG data of the patient during the presurgical evaluation. • Behind-the-ear data (2 sensors positioned behind each ear) • Single-lead ECG data (typically lead II) Seizures are annotated by the clinicians based on the gold standard vEEG system. These seizure annotations are also available in the dataset. In total 82 patients were recorded between 23/01/2017 and 26/10/2018. From those patients, 54 were recorded with the bhe channels. Forty-two of those patients had seizures during their presurgical evaluation, while for twelve patients no seizure has been recorded. The number of seizures per patient ranged from 1 to 22, with a median of 3 seizures per patient. The duration of the seizures, the time difference of seizure EEG onset and end, varied between 11 and 695 seconds with a median of 50 seconds. 89% of the seizures were Focal Impaired Awareness seizures. 91% of the seizures originated from the (fronto-) temporal lobe. In the folder ’Data’ the raw data in the form of .edf, are provided with annotations for all the patients. The annotations are provided in .tsv (tab separated values) files. For every seizure the first column represents the starting point (in seconds) of the seizure, the second one the end point of the seizure, the third one the type of the seizure, while in the last column extra information are provided. The extra information includes the origin of the seizure, the hemisphere and if the seizure can be noted from the behind the ear channels (bhe:1 in that case). In the header section of every file information concerning the dataset and the annotations used are included. For every subject and for every session (even if no seizure is present) two different sets of annotations are provided. The ”a1”set of annotations is the annotations as provided by the doctors. The ”a2” set of annotations are the annotations used in [2] for training of the algorithm. The annotations provided from the doctors were not always perfectly aligned with the typical rhythmic ictal pattern, hence in ”a2” a refinement of the start of each annotation was performed visually by an engineer. Furthermore, in the annotations of the doctor the end point of some seizures was missing (”none”) in the ”a2” subset of annotations each seizure was considered with a stable length of 10 seconds.",2017,,,,,
2814,SEMAINE,Emotion Recognition in Conversation,Emotion Recognition in Conversation,Emotion Recognition in Conversation,Image,,Computer Vision,emotion-recognition-in-conversation-on-2,,https://ibug.doc.ic.ac.uk/resources/semaine-database2/,https://paperswithcode.com/dataset/semaine,"The SEMAINE videos dataset contains spontaneous data capturing the audiovisual interaction between a human and an operator undertaking the role of an avatar with four personalities: Poppy (happy), Obadiah (gloomy), Spike (angry) and Prudence (pragmatic). The audiovisual sequences have been recorded at a video rate of 25 fps (352 x 288 pixels). The dataset consists of audiovisual interaction between a human and an operator undertaking the role of an agent (Sensitive Artificial Agent). SEMAINE video clips have been annotated with couples of epistemic states such as agreement, interested, certain, concentration, and thoughtful with continuous rating (within the range [1,-1]) where -1 indicates most negative rating (i.e: No concentration at all) and +1 defines the highest (Most concentration). Twenty-four recording sessions are used in the Solid SAL scenario. Recordings are made of both the user and the operator, and there are usually four character interactions in each recording session, providing a total of 95 character interactions and 190 video clips.",,ROBUST MODELING OF EPISTEMIC MENTAL STATES,https://arxiv.org/abs/2005.13982,,,
2815,SemanticKITTI,Weakly supervised Semantic Segmentation,Weakly supervised Semantic Segmentation,"Weakly supervised Semantic Segmentation, 4D Panoptic Segmentation, LIDAR Semantic Segmentation, Panoptic Segmentation, 3D Semantic Scene Completion from a single RGB image, Generalized Zero-Shot Learning, Semi-Supervised Semantic Segmentation, 3D Semantic Scene Completion, Real-Time 3D Semantic Segmentation, 3D Semantic Segmentation","3D, Image",English,Computer Vision,"3d-semantic-segmentation-on-semantickitti, 3d-semantic-scene-completion-from-a-single-1, semi-supervised-semantic-segmentation-on-24, 4d-panoptic-segmentation-on-semantickitti, weakly-supervised-semantic-segmentation-on-7, 3d-semantic-scene-completion-on-semantickitti, real-time-3d-semantic-segmentation-on-1, generalized-zero-shot-learning-on, panoptic-segmentation-on-semantickitti, lidar-semantic-segmentation-on-semantickitti",CC BY-NC-SA 4.0,http://www.semantic-kitti.org/,https://paperswithcode.com/dataset/semantickitti,"SemanticKITTI is a large-scale outdoor-scene dataset for point cloud semantic segmentation. It is derived from the KITTI Vision Odometry Benchmark which it extends with dense point-wise annotations for the complete 360 field-of-view of the employed automotive LiDAR. The dataset consists of 22 sequences. Overall, the dataset provides 23201 point clouds for training and 20351 for testing.",,Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation,https://arxiv.org/abs/2008.01550,,,
2816,SemanticPOSS,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Weakly supervised Semantic Segmentation, 3D Semantic Segmentation, Autonomous Driving","3D, Image",,Computer Vision,"semantic-segmentation-on-semanticposs, weakly-supervised-semantic-segmentation-on-8",,http://www.poss.pku.edu.cn/semanticposs.html,https://paperswithcode.com/dataset/semanticposs,The SemanticPOSS dataset for 3D semantic segmentation contains 2988 various and complicated LiDAR scans with large quantity of dynamic instances. The data is collected in Peking University and uses the same data format as SemanticKITTI.,,,,,,
2817,Semantic_Question_Similarity_in_Arabic,Semantic Similarity,Semantic Similarity,Semantic Similarity,,,Methodology,,Attribution-NonCommercial 4.0 International,https://www.kaggle.com/c/nsurl-2019-task8/data,https://paperswithcode.com/dataset/semantic-question-similarity-in-arabic,"NSURL-2019 Shared Task 8: Semantic Question Similarity in Arabic

This dataset contains 11,997 pairs of questions in MSA Arabic that are assigned either a label of 0, for no semantic similarity, or 1 otherwise.",2019,NSURL-2019 Shared Task 8: Semantic Question Similarity in Arabic,https://aclanthology.org/2019.nsurl-1.1.pdf,,,
2818,Semantic_Scholar,Language Modelling,Language Modelling,"Language Modelling, Question Answering, Knowledge Graphs",Text,English,Natural Language Processing,,ODC-BY,https://allenai.org/data/open-research-corpus,https://paperswithcode.com/dataset/semantic-scholar,"The Semantic Scholar corpus (S2) is composed of titles from scientific papers published in machine learning conferences and journals from 1985 to 2017, split by year (33 timesteps).

Image Source: [http://s2-public-api-prod.us-west-2.elasticbeanstalk.com/corpus/] (http://s2-public-api-prod.us-west-2.elasticbeanstalk.com/corpus/)",1985,Learning Dynamic Author Representations with Temporal Language Models,https://arxiv.org/abs/1909.04985,,,
2819,SemEval-2010_Task-8,Relation Extraction,Relation Extraction,Relation Extraction,Graph,,Methodology,relation-extraction-on-semeval-2010-task-8,CC BY 3.0,http://www.kozareva.com/downloads.html,https://paperswithcode.com/dataset/semeval-2010-task-8,The dataset for the SemEval-2010 Task 8 is a dataset for multi-way classification of mutually exclusive semantic relations between pairs of nominals.,2010,,,,,
2820,SemEval-2014_Task-10,Word Embeddings,Word Embeddings,"Word Embeddings, Semantic Textual Similarity, Sentence Embeddings",,,Methodology,,,https://alt.qcri.org/semeval2014/task10/,https://paperswithcode.com/dataset/sts-2014,"SemEval 2014 is a collection of datasets used for the Semantic Evaluation (SemEval) workshop, an annual event that focuses on the evaluation and comparison of systems that can analyze diverse semantic phenomena in text. The datasets from SemEval 2014 are used for various tasks, including but not limited to:


Aspect-Based Sentiment Analysis (ABSA): This task is based on laptop and restaurant reviews. It involves identifying the aspects or features mentioned in a review and determining the sentiment expressed towards each aspect.
Text Classification: This task involves classifying text into predefined categories. Sub-tasks include text-scoring, natural language inference, and semantic-similarity-scoring.",2014,,,,,
2821,SemEval-2014_Task-4,Aspect Extraction,Aspect Extraction,"Aspect Extraction, Aspect-Based Sentiment Analysis (ABSA), Aspect-Based Sentiment Analysis, Aspect Category Detection, Aspect-oriented  Opinion Extraction","Image, Text",English,Computer Vision,"aspect-based-sentiment-analysis-on-semeval-10, aspect-based-sentiment-analysis-on-semeval, aspect-category-detection-on-semeval-2014-1, aspect-extraction-on-semeval-2014-task-4-sub-1, aspect-oriented-opinion-extraction-on-semeval",,https://alt.qcri.org/semeval2014/task4/,https://paperswithcode.com/dataset/semeval-2014-task-4-sub-task-2,"Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, regardless of the entities mentioned (e.g., laptops, restaurants) and their aspects (e.g., battery, screen; food, service). By contrast, this task is concerned with aspect based sentiment analysis (ABSA), where the goal is to identify the aspects of given target entities and the sentiment expressed towards each aspect. Datasets consisting of customer reviews with human-authored annotations identifying the mentioned aspects of the target entities and the sentiment polarity of each aspect will be provided.

Subtask 2: Aspect term polarity

For a given set of aspect terms within a sentence, determine whether the polarity of each aspect term is positive, negative, neutral or conflict (i.e., both positive and negative).

For example:

“I loved their fajitas” → {fajitas: positive}
“I hated their fajitas, but their salads were great” → {fajitas: negative, salads: positive}
“The fajitas are their first plate” → {fajitas: neutral}
“The fajitas were great to taste, but not to see” → {fajitas: conflict}",,,,,,
2822,SemEval-2017_Task-10,Keyword Extraction,Keyword Extraction,"Keyword Extraction, Keyphrase Extraction",,,Methodology,"keyphrase-extraction-on-semeval2017, keyword-extraction-on-semeval2017",,,https://paperswithcode.com/dataset/semeval2017,"We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities.",,,,,,
2823,SemEval-2018_Task-9,Hypernym Discovery,Hypernym Discovery,Hypernym Discovery,,,Methodology,"hypernym-discovery-on-music-domain, hypernym-discovery-on-medical-domain, hypernym-discovery-on-general",CC BY-SA 3.0,https://competitions.codalab.org/competitions/17119,https://paperswithcode.com/dataset/semeval-2018-task-9-hypernym-discovery,"The SemEval-2018 hypernym discovery evaluation benchmark (Camacho-Collados et al. 2018) contains three domains (general, medical and music) and is also available in Italian and Spanish (not in this repository). For each domain a target corpus and vocabulary (i.e. hypernym search space) are provided. The dataset contains both concepts (e.g. dog) and entities (e.g. Manchester United) up to trigrams.",2018,,,,,
2824,SemEval-2020_Task-8,Multimodal Sentiment Analysis,Multimodal Sentiment Analysis,"Multimodal Sentiment Analysis, Sentiment Analysis",Text,English,Multimodal,,,https://competitions.codalab.org/competitions/20629,https://paperswithcode.com/dataset/memotion-analysis,A multimodal dataset for sentiment analysis on internet memes.,,,,,,
2825,semi-indoor,Dynamic Point Removal,Dynamic Point Removal,Dynamic Point Removal,,,Methodology,dynamic-point-removal-on-semi-indoor,,https://github.com/KTH-RPL/DynamicMap_Benchmark,https://paperswithcode.com/dataset/dynamicmap,collected by one VLP-16 in a small vehicle (1m x 1m),,,,,,
2826,SemOpenAlex,Science Question Answering,Science Question Answering,"Science Question Answering, Joint Entity and Relation Extraction on Scientific Data, Scientific Document Summarization, Scientific Concept Extraction, Knowledge Graphs, Citation Recommendation, Knowledge-Aware Recommendation","Graph, Text",English,Natural Language Processing,,CC0,https://semopenalex.org/,https://paperswithcode.com/dataset/semopenalex,"SemOpenAlex is an extensive RDF knowledge graph that contains over 26 billion triples about scientific publications and their associated entities, such as authors, institutions, journals, and concepts. 
* SemOpenAlex is licensed under CC0, providing free and open access to the data. 
* We offer the data through multiple channels, including RDF dump files, a SPARQL endpoint, and as a data source in the Linked Open Data cloud, complete with resolvable URIs and links to other data sources (ISNI, DOI, ORCID, ROR, Scopus, DOAJ, Wikidata, 
* Moreover, we provide embeddings for knowledge graph entities using high-performance computing. 

SemOpenAlex enables a broad range of use-case scenarios, such as 
* exploratory semantic search via our website,
* large-scale scientific impact quantification, 
* other forms of scholarly big data analytics within and across scientific disciplines.
* enables academic recommender systems, such as recommending collaborators, publications, and venues, including explainability capabilities. 
* can serve for RDF query optimization benchmarks, 
* creating scholarly knowledge-guided language models, 
* as a hub for semantic scientific publishing.",,,,,,
2827,SEN12MS-CR-TS,Image Dehazing,Image Dehazing,"Image Dehazing, Image-to-Image Translation, Video Reconstruction, Image Reconstruction, Image Denoising, Cloud Removal","3D, Image, Text, Video",English,Computer Vision,cloud-removal-on-sen12ms-cr-ts,,https://patricktum.github.io/cloud_removal/,https://paperswithcode.com/dataset/sen12ms-cr-ts,SEN12MS-CR-TS is a multi-modal and multi-temporal data set for cloud removal. It contains time-series of paired and co-registered Sentinel-1 and cloudy as well as cloud-free Sentinel-2 data from European Space Agency's Copernicus mission. Each time series contains 30 cloudy and clear observations regularly sampled throughout the year 2018. Our multi-temporal data set is readily pre-processed and backward-compatible with SEN12MS-CR.,2018,,,,,
2828,SEN12MS-CR,Image Reconstruction,Image Reconstruction,"Image Reconstruction, Cloud Removal, Image Denoising","3D, Image",,Computer Vision,cloud-removal-on-sen12ms-cr,,https://patricktum.github.io/cloud_removal/,https://paperswithcode.com/dataset/sen12ms-cr,"SEN12MS-CR is a multi-modal and mono-temporal data set for cloud removal. It contains observations covering 175 globally distributed Regions of Interest recorded in one of four seasons throughout the year of 2018. For each region, paired and co-registered synthetic aperture radar (SAR) Sentinel-1 measurements as well as cloudy and cloud-free optical multi-spectral Sentinel-2 observations from European Space Agency's Copernicus mission are provided. The Sentinel satellites provide public access data and are among the most prominent satellites in Earth observation.",2018,,,,,
2829,SEN12MS,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Weakly-Supervised Semantic Segmentation, Scene Classification",Image,,Computer Vision,,,https://mediatum.ub.tum.de/1474000,https://paperswithcode.com/dataset/sen12ms,"A dataset consisting of 180,662 triplets of dual-pol synthetic aperture radar (SAR) image patches, multi-spectral Sentinel-2 image patches, and MODIS land cover maps.",,,,,,
2830,Sen4AgriNet,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Time Series Analysis, Crop Classification","Image, Time Series",,Computer Vision,,MIT,https://github.com/Orion-AI-Lab/S4A,https://paperswithcode.com/dataset/sen4agrinet,"A Sentinel-2 based time series multi country benchmark dataset, tailored for agricultural monitoring applications with Machine and Deep Learning. Sen4AgriNet dataset is annotated from farmer declarations collected via the Land Parcel Identification System (LPIS) for harmonizing country wide labels. Sen4AgriNet is the only multi-country, multi-year dataset that includes all spectral information. It is constructed to cover the period 2016-2020 for Catalonia and France, while it can be extended to include additional countries. Currently, it contains 42.5 million parcels, which makes it significantly larger than other available archives.",2016,,,,,
2831,SensatUrban,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Scene Understanding, 3D Semantic Segmentation","3D, Image",,Computer Vision,3d-semantic-segmentation-on-sensaturban,,https://github.com/QingyongHu/SensatUrban,https://paperswithcode.com/dataset/sensaturban,"The SensatUrbat dataset is an urban-scale photogrammetric point cloud dataset with nearly three billion richly annotated points, which is five times the number of labeled points than the existing largest point cloud dataset. The dataset consists of large areas from two UK cities, covering about 6 km^2 of the city landscape. In the dataset, each 3D point is labeled as one of 13 semantic classes, such as ground, vegetation, car, etc..",,,,,,
2832,Senseval-2,Language Modelling,Language Modelling,"Language Modelling, Word Sense Disambiguation, Word Embeddings",Text,English,Natural Language Processing,,,https://web.eecs.umich.edu/~mihalcea/senseval/,https://paperswithcode.com/dataset/senseval-2-1,"There are now many computer programs for automatically determining the sense of a word in context (Word Sense Disambiguation or WSD).  The purpose of SENSEVAL is to evaluate the strengths and weaknesses of such programs with respect to different words, different varieties of language, and different languages.",,,,,,
2833,Sentence_Compression,Text Generation,Text Generation,"Text Generation, Text Summarization, Sentence Compression",Text,English,Natural Language Processing,sentence-compression-on-google-dataset,Custom,https://github.com/google-research-datasets/sentence-compression,https://paperswithcode.com/dataset/sentence-compression,"Sentence Compression is a dataset where the syntactic trees of the compressions are subtrees of their uncompressed counterparts, and hence where supervised systems which require a structural alignment between the input and output can be successfully trained.",,,,,,
2834,SentEval,Linear-Probe Classification,Linear-Probe Classification,"Linear-Probe Classification, Semantic Textual Similarity",Image,,Computer Vision,"linear-probe-classification-on-senteval, semantic-textual-similarity-on-senteval",Various,https://arxiv.org/abs/1803.05449,https://paperswithcode.com/dataset/senteval,"SentEval is a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders.",,Homepage,https://arxiv.org/abs/1803.05449,,,
2835,SentimentArcs__Sentiment_Reference_Corpus_for_Nove,Twitter Sentiment Analysis,Twitter Sentiment Analysis,"Twitter Sentiment Analysis, Sentiment Analysis",Text,English,Natural Language Processing,,MIT,https://github.com/jon-chun/sentimentarcs_notebooks/,https://paperswithcode.com/dataset/sentimentarcs-sentiment-reference-corpus-for,"SentimentArcs’ reference corpus for novels consists of 25 narratives selected to create a diverse set of well recognized novels that can serve as a benchmark for future studies. The composition of the corpora was limited by the effect of copyright laws as well as historical imbalances. Most works were obtained from US and Australian Gutenberg Projects. The corpora is expected to grow in size and diversity over time.  

Several dimensions of diversity were considered for inclusion including popularity, period, genre, topic, style and author diversity. The first version of our corpus includes only English, although Proust and Homer are included in translation. SentimentArcs has processed a larger set of novels, including some in foreign languages. The initial reference corpus is in English since performance across all ensemble models was uneven in less resourced languages

In sum, the corpora includes (1) the two most popular novels on Gutenberg.org (Project Gutenberg, 2021b), (2) eight of the fifteen most assigned novels at top US universities (EAB, 2021), and (3) three works that have sold over 20 million copies (Books, 2021). There are eight works by women, two by African-Americans and five works by two LGBTQ authors. Britain leads with 15 authors followed by 6 Americans and one each from France, Russia, North Africa and Ancient Greece.",2021,,,,,
2836,Seoul_Bike_Sharing_Demand,regression,regression,regression,,,Methodology,,Creative Commons Attribution 4.0 International,,https://paperswithcode.com/dataset/seoul-bike-sharing-demand,"Data variables and description.
Parameters/Features                                 Abbreviation                             Type                                Measurement
Date                                                                      Date                                    year-month-day –
Rented Bike count                                          Count                                 Continuous                           0, 1, 2, .. ., 3556
Hour                                                                       Hour                                   Continuous                           0, 1, 2, .. ., 23
Temperature                                                      Temp                                   Continuous                            ◦C
Humidity                                                               Hum                                   Continuous                              %
Windspeed                                                           Wind                                  Continuous                              m/s
Visibility                                                                  Visb                                   Continuous                              10 m
Dew point temperature                                  Dew                                   Continuous                                ◦C
Solar radiation                                                     Solar                                  Continuous                              MJ/m2
Rainfall                                                                     Rain                                   Continuous                               Mm
Snowfall                                                                   Snow                                  Continuous                               cm
Seasons                                                                     Seasons                           Categorical                              Autumn, Spring, Summer, Winter
Holiday                                                                       Holiday                           Categorical                               Holiday, Workday
Functional Day                                                        Fday                                 Categorical                              NoFunc, Func
Week status                                                              Wstatus                          Categorical                             Weekday (Wday), Weekend (Wend)
Day of the week                                                       Dweek                            Categorical                              Sunday, Monday, .. ., Saturday",,,,,,
2837,Sepehr_RumTel01,Rumour Detection,Rumour Detection,Rumour Detection,Image,,Computer Vision,rumour-detection-on-sepehr-rumtel01,CC BY NC 3.0,https://doi.org/10.17632/jw3zwf8rdp,https://paperswithcode.com/dataset/sepehr-rumtel01,"The expansion of social networks has accelerated the transmission of information and news at every communities. Over the past few years, the number of users, audiences and social networking publishers, are increased dramatically too. Among the massive amounts of information and news reported on these networks, we are faced with issues that have not been verified which is called “rumors”. Identifying rumors on social networks is carried out in the form of rumor detection approaches; the massive amount of these news and information force to use the machine learning techniques. The most important problem with auto-detection approaches is the lack of a database of rumors. For that matter, in this article, a collection of rumors published on the social network “telegrams” have been collected. These data are gathered from five Persian-language channels that have specially reviewed this issue. The collected data set contains 3283 messages with 2829 attachments, having a volume of over 1.6 gigabytes. This dataset can also be used for different purposes of natural language processing.",,,,,,
2838,Sequence_Consistency_Evaluation__SCE__tests,Relational Reasoning,Relational Reasoning,"Relational Reasoning, Zero-Shot Learning, Visual Reasoning",Image,,Reasoning,,,https://github.com/Tomer-Barak/Naive-Few-Shot-Learning,https://paperswithcode.com/dataset/sequence-consistency-evaluation-sce-tests,Sequence Consistency Evaluation (SCE) consists of a benchmark task for sequence consistency evaluation (SCE).,,,,,,
2839,Seq_Funcat,Hierarchical Multi-label Classification,Hierarchical Multi-label Classification,Hierarchical Multi-label Classification,Image,,Computer Vision,hierarchical-multi-label-classification-on-6,,https://dtai.cs.kuleuven.be/clus/hmcdatasets/,https://paperswithcode.com/dataset/seq-funcat,Hierarchical-multilabel classification dataset for functional genomics,,,,,,
2840,SES,Multimodal Emotion Recognition,Multimodal Emotion Recognition,"Multimodal Emotion Recognition, Speech Emotion Recognition","Audio, Image",,Multimodal,,,,https://paperswithcode.com/dataset/ses,"Currently, an essential point in speech synthesis is the addressing of the variability of human speech. One of the main sources of this diversity is the emotional state of the speaker. Most of the recent work in this area has been focused on the prosodic aspects of speech and on rule-based formant synthesis experiments. Even when adopting an improved voice source, we cannot achieve a smiling happy voice or the menacing quality of cold anger. For this reason, we have performed two experiments aimed at developing a concatenative emotional synthesiser, a synthesiser that can copy the quality of an emotional voice without an explicit mathematical model.",,,,,,
2841,Set12,Grayscale Image Denoising,Grayscale Image Denoising,Grayscale Image Denoising,Image,,Computer Vision,"grayscale-image-denoising-on-set12-sigma15, grayscale-image-denoising-on-set12-sigma70, grayscale-image-denoising-on-set12-sigma50, grayscale-image-denoising-on-set12-sigma30, grayscale-image-denoising-on-set12-sigma25",,,https://paperswithcode.com/dataset/set12,Set12 is a collection of 12 grayscale images of different scenes that are widely used for evaluation of image denoising methods. The size of each image is 256×256.,,Designing and Training of A Dual CNN for Image Denoising,https://arxiv.org/abs/2007.03951,,,
2842,Set14,Blind Super-Resolution,Blind Super-Resolution,"Blind Super-Resolution, Image Super-Resolution",Image,,Computer Vision,"blind-super-resolution-on-set14-3x-upscaling, blind-super-resolution-on-set14-2x-upscaling, image-super-resolution-on-set14-8x-upscaling, image-super-resolution-on-set14, image-super-resolution-on-set14-3x-upscaling, image-super-resolution-on-set14-2x-upscaling, blind-super-resolution-on-set14-4x-upscaling, image-super-resolution-on-set14-4x-upscaling",,https://github.com/jbhuang0604/SelfExSR,https://paperswithcode.com/dataset/set14,The Set14 dataset is a dataset consisting of 14 images commonly used for testing performance of Image Super-Resolution models.,,,,14 images,,
2843,Set5,Image Rescaling,Image Rescaling,"Image Rescaling, Blind Super-Resolution, Image Super-Resolution, Compressive Sensing",Image,,Computer Vision,"image-rescaling-on-set5-2x, blind-super-resolution-on-set5-2x-upscaling, image-super-resolution-on-set5-3x-upscaling, image-super-resolution-on-set5-4x-upscaling, blind-super-resolution-on-set5-3x-upscaling, compressive-sensing-on-set5, image-super-resolution-on-set5-8x-upscaling, blind-super-resolution-on-set5-4x-upscaling, image-super-resolution-on-set5-2x-upscaling",,http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html,https://paperswithcode.com/dataset/set5,"The Set5 dataset is a dataset consisting of 5 images (“baby”, “bird”, “butterfly”, “head”, “woman”) commonly used for testing performance of Image Super-Resolution models.",,,,5 images,,
2844,SEVIR,Precipitation Forecasting,Precipitation Forecasting,"Precipitation Forecasting, Weather Forecasting",Time Series,,Methodology,"weather-forecasting-on-sevir, precipitation-forecasting-on-sevir",,https://registry.opendata.aws/sevir/,https://paperswithcode.com/dataset/sevir,"SEVIR is an annotated, curated and spatio-temporally aligned dataset containing over 10,000 weather events that each consist of 384 km x 384 km image sequences spanning 4 hours of time. Images in SEVIR were sampled and aligned across five different data types: three channels (C02, C09, C13) from the GOES-16 advanced baseline imager, NEXRAD vertically integrated liquid mosaics, and GOES-16 Geostationary Lightning Mapper (GLM) flashes. Many events in SEVIR were selected and matched to the NOAA Storm Events database so that additional descriptive information such as storm impacts and storm descriptions can be linked to the rich imagery provided by the sensors.",,https://proceedings.neurips.cc//paper/2020/file/fa78a16157fed00d7a80515818432169-Paper.pdf,https://proceedings.neurips.cc//paper/2020/file/fa78a16157fed00d7a80515818432169-Paper.pdf,,,
2845,SEWA_DB,Emotion Recognition,Emotion Recognition,"Emotion Recognition, Speech Emotion Recognition","Audio, Image",,Computer Vision,,,http://db.sewaproject.eu/,https://paperswithcode.com/dataset/sewa-db,"A database of more than 2000 minutes of audio-visual data of 398 people coming from six cultures, 50% female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies.",2000,,,,"valued valence, arousal, liking, agreement, and prototypic examples",
2846,SFCHD,Object Detection,Object Detection,"Object Detection, Open World Object Detection, Real-Time Object Detection, Small Object Detection",Image,,Computer Vision,object-detection-on-sfchd,,https://github.com/lijfrank-open/SFCHD-SCALE,https://paperswithcode.com/dataset/sfchd,"This work contributes a large, complex, and realistic high-quality safety clothing and helmet detection (SFCHD) dataset. The dataset comprises 12,373 images, covering 7 categories, with a total of 50,558 labeled instances. All images are captured from factory surveillance cameras, encompassing 40 different scenes across two chemical plants. It is worth noting that our SFCHD dataset not only provides a rich set of training samples but also serves as a benchmark for the evaluation of various detection tasks, such as small object detection, and high-low light object detection.",,,,373 images,,7
2847,SFD,Open-Ended Question Answering,Open-Ended Question Answering,"Open-Ended Question Answering, zero-shot long video question answering","Text, Video",English,Natural Language Processing,,CC-BY-NC-SA 4.0,https://shortfilmdataset.github.io/,https://paperswithcode.com/dataset/sfd,"The Short Film Dataset (SFD) is a long video question-answering benchmark. It consists of 1,078 movies and 4,885 questions, spanning 250 hours of video data (13 minutes per movie on average).",,,,,,
2848,SGD,Dialogue State Tracking,Dialogue State Tracking,"Dialogue State Tracking, Natural Language Understanding, Multi-domain Dialogue State Tracking, Slot Filling, Task-Oriented Dialogue Systems, Classification","Image, Text, Video",English,Computer Vision,"classification-on-sgd, multi-domain-dialogue-state-tracking-on-sgd, task-oriented-dialogue-systems-on-sgd",CC-BY-SA-4.0,https://github.com/google-research-datasets/dstc8-schema-guided-dialogue,https://paperswithcode.com/dataset/sgd,"The Schema-Guided Dialogue (SGD) dataset consists of over 20k annotated multi-domain, task-oriented conversations between a human and a virtual assistant. These conversations involve interactions with services and APIs spanning 20 domains, ranging from banks and events to media, calendar, travel, and weather. For most of these domains, the dataset contains multiple different APIs, many of which have overlapping functionalities but different interfaces, which reflects common real-world scenarios. The wide range of available annotations can be used for intent prediction, slot filling, dialogue state tracking, policy imitation learning, language generation, user simulation learning, among other tasks in large-scale virtual assistants. Besides these, the dataset has unseen domains and services in the evaluation set to quantify the performance in zero-shot or few shot settings.",,,,,,
2849,SGXSTest,Text Generation,Text Generation,"Text Generation, Language Modelling, Large Language Model, AI and Safety",Text,English,Natural Language Processing,,Apache-2.0,https://huggingface.co/datasets/walledai/SGXSTest,https://paperswithcode.com/dataset/sgxstest,"For testing refusal behavior in a cultural setting, we introduce SGXSTest — a set of manually curated prompts designed to measure exaggerated safety within the context of Singaporean culture. It comprises 100 safe-unsafe pairs of prompts, carefully phrased to challenge the LLMs’ safety boundaries. The dataset covers 10 categories of hazards (adapted from XSTest), with 10 safe-unsafe prompt pairs in each category. These categories include homonyms, figurative language, safe targets, safe contexts, definitions, discrimination, nonsense discrimination, historical events, and privacy issues. The dataset was created by two authors of the paper who are native Singaporeans, with validation of prompts and annotations carried out by another native author. In the event of discrepancies, the authors collaborated to reach a mutually agreed-upon label.",,,,,,10
2850,SHADR,Adversarial Robustness,Adversarial Robustness,"Adversarial Robustness, Classification, Ethics",Image,,Adversarial,,CC-BY-4.0,https://huggingface.co/datasets/m720/SHADR,https://paperswithcode.com/dataset/shadr,"SDoH Human Annotated Demoographic Robustness (SHADR) Dataset
Overview
The Social determinants of health (SDoH) play a pivotal role in determining patient outcomes. However, their documentation in electronic health records (EHR) remains incomplete. This dataset was created from a study examining the capability of large language models in extracting SDoH from the free text sections of EHRs. Furthermore, the study delved into the potential of synthetic clinical text to bolster the extraction process of these scarcely documented, yet crucial, clinical data.

Dataset Structure & Modification
To understand potential biases in high-performing models and in those pre-trained on general text, GPT-4 was utilized to infuse demographic descriptors into our synthetic data. 

For instance:
- Original Sentence: ""Widower admits fears surrounding potential judgment…""
- Modified Sentence: ""Hispanic widower admits fears surrounding potential judgment...""

Such demographic-infused sentences underwent manual validation. Out of these:
- 419 had mentions of SDoH
- 253 had mentions of adverse SDoH
- The remainder were tagged as NO_SDoH

Instructions for Model Evaluation

Initially, run your model inference on the original sentences.
Subsequently, apply the same model to infer on the demographic-modified sentences.
Perform comparisons for robustness.

For a detailed understanding of the ""adverse"" labeling, refer to https://arxiv.org/pdf/2308.06354.pdf. Here, the 'adverse' column demarcates if the label corresponds to an ""adverse"" or ""non-adverse"" SDoH.

Current Performance Metrics

Best Model Performance:
Any SDoH: 88% Macro-F1 

Adverse SDoH: 84% Macro-F1



Robustness Rate:


Any SDoH: 9.9%
Adverse SDoH: 14.3%


How to Cite:
@misc{guevara2023large,
      title={Large Language Models to Identify Social Determinants of Health in Electronic Health Records}, 
      author={Marco Guevara and Shan Chen and Spencer Thomas and Tafadzwa L. Chaunzwa and Idalid Franco and Benjamin Kann and Shalini Moningi and Jack Qian and Madeleine Goldstein and Susan Harper and Hugo JWL Aerts and Guergana K. Savova and Raymond H. Mak and Danielle S. Bitterman},
      year={2023},
      eprint={2308.06354},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}",2023,,,,,
2851,ShanghaiTech,Crowd Counting,Crowd Counting,"Crowd Counting, Video Anomaly Detection, Cross-Part Crowd Counting, Abnormal Event Detection In Video, Anomaly Detection In Surveillance Videos, Anomaly Detection","Image, Video",,Computer Vision,"crowd-counting-on-shanghaitech-b, anomaly-detection-on-shanghaitech, cross-part-crowd-counting-on-shanghaitech-b, cross-part-crowd-counting-on-shanghaitech-a, crowd-counting-on-shanghaitech-a, anomaly-detection-in-surveillance-videos-on-8, video-anomaly-detection-on-shanghaitech-4",,https://github.com/desenzhou/ShanghaiTechDataset,https://paperswithcode.com/dataset/shanghaitech,"The Shanghaitech dataset is a large-scale crowd counting dataset. It consists of 1198 annotated crowd images. The dataset is divided into two parts, Part-A containing 482 images and Part-B containing 716 images. Part-A is split into train and test subsets consisting of 300 and 182 images, respectively. Part-B is split into train and test subsets consisting of 400 and 316 images. Each person in a crowd image is annotated with one point close to the center of the head. In total, the dataset consists of 330,165 annotated people. Images from Part-A were collected from the Internet, while images from Part-B were collected on the busy streets of Shanghai.",,Iterative Crowd Counting,https://arxiv.org/abs/1807.09959,482 images,split into train and test subsets consisting of 300 and 182 images,
2852,ShanghaiTech_Campus,Weakly-supervised Anomaly Detection,Weakly-supervised Anomaly Detection,"Weakly-supervised Anomaly Detection, Video Understanding, Weakly-supervised Video Anomaly Detection, Video Anomaly Detection, Abnormal Event Detection In Video, Anomaly Detection In Surveillance Videos, Anomaly Detection","Image, Video",,Computer Vision,"weakly-supervised-video-anomaly-detection-on, video-anomaly-detection-on-shanghaitech, anomaly-detection-on-shanghaitech, anomaly-detection-on-shanghaitech-campus-2, anomaly-detection-in-surveillance-videos-on-8, video-anomaly-detection-on-hr-shanghaitech, anomaly-detection-in-surveillance-videos-on-1, video-anomaly-detection-on-shanghaitech-4",MIT,https://svip-lab.github.io/dataset/campus_dataset.html,https://paperswithcode.com/dataset/shanghaitech-campus,"The ShanghaiTech Campus dataset has 13 scenes with complex light conditions and camera angles. It contains 130 abnormal events and over 270, 000 training frames. Moreover, both the frame-level and pixel-level ground truth of abnormal events are annotated in this dataset.",,,,,,
2853,ShapeNet,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Training-free 3D Part Segmentation, 3D Object Reconstruction, Point Cloud Completion, Novel View Synthesis, 3D Part Segmentation, Single-View 3D Reconstruction, Point Cloud Generation, 3D Reconstruction","3D, Image, Text",English,Computer Vision,"point-cloud-generation-on-shapenet-car, single-view-3d-reconstruction-on-shapenet, point-cloud-completion-on-shapenet, novel-view-synthesis-on-shapenet-car, point-cloud-generation-on-shapenet, 3d-part-segmentation-on-shapenet-part, training-free-3d-part-segmentation-on, point-cloud-generation-on-shapenet-chair, novel-view-synthesis-on-shapenet-chair, 3d-object-reconstruction-on-shapenet, 3d-reconstruction-on-shapenet, point-cloud-generation-on-shapenet-airplane, semantic-segmentation-on-shapenet",Custom (non-commerical),https://www.shapenet.org/,https://paperswithcode.com/dataset/shapenet,"ShapeNet is a large scale repository for 3D CAD models developed by researchers from Stanford University, Princeton University and the Toyota Technological Institute at Chicago, USA. The repository contains over 300M models with 220,000 classified into 3,135 classes arranged using WordNet hypernym-hyponym relationships. ShapeNet Parts subset contains 31,693 meshes categorised into 16 common object classes (i.e. table, chair, plane etc.). Each shapes ground truth contains 2-5 parts (with a total of 50 part classes).",,A review on deep learning techniques for 3D sensed data classification,https://arxiv.org/abs/1907.04444,,,135
2854,ShapeNetCore,3D Reconstruction,3D Reconstruction,"3D Reconstruction, 3D Classification, Single-View 3D Reconstruction","3D, Image",,Computer Vision,single-view-3d-reconstruction-on-shapenetcore,Custom (non-commercial),https://shapenet.org/,https://paperswithcode.com/dataset/shapenetcore,"ShapeNetCore is a subset of the full ShapeNet dataset with single clean 3D models and manually verified category and alignment annotations. It covers 55 common object categories with about 51,300 unique 3D models. The 12 object categories of PASCAL 3D+, a popular computer vision 3D benchmark dataset, are all covered by ShapeNetCore.",,,,,,
2855,ShapenetRender,3D Reconstruction,3D Reconstruction,"3D Reconstruction, Single-View 3D Reconstruction",3D,,Methodology,,,https://github.com/Xharlie/ShapenetRender_more_variation,https://paperswithcode.com/dataset/shapenetrender,"ShapenetRenderer is an extension of the ShapeNet Core dataset which has more variation in camera angles. For each mesh model, the dataset provides 36 views with smaller variation and 36 views with larger variation. The resolution of the newly rendered images is 224x224 in contrast to the 137x137 original resolution. Additionally, each RGB image is paired with a depth image, a normal map and an albedo image.",,,,,,
2856,SHAPES,Time Series Classification,Time Series Classification,"Time Series Classification, Visual Question Answering (VQA), Visual Reasoning, Question Answering","Image, Text, Time Series",English,Time Series,time-series-classification-on-shapes,,https://github.com/ronghanghu/n2nmn#train-and-evaluate-on-the-shapes-dataset,https://paperswithcode.com/dataset/shapes-1,"SHAPES is a dataset of synthetic images designed to benchmark systems for understanding of spatial and logical relations among multiple objects. The dataset consists of complex questions about arrangements of colored shapes. The questions are built around compositions of concepts and relations, e.g. Is there a red shape above a circle? or Is a red shape blue?. Questions contain between two and four attributes, object types, or relationships. There are 244 questions and 15,616 images in total, with all questions having a yes and no answer (and corresponding supporting image). This eliminates the risk of learning biases.

Each image is a 30×30 RGB image depicting a 3×3 grid of objects. Each object is characterized by shape (circle, square, triangle), colour (red, green, blue) and size (small, big).",,Visual Question Answering: A Survey of Methods and Datasets,https://arxiv.org/abs/1607.05910,616 images,,
2857,ShapeStacks,Object Discovery,Object Discovery,"Object Discovery, Scene Understanding, Image Generation, Unsupervised Object Segmentation","Image, Text",English,Computer Vision,"unsupervised-object-segmentation-on, image-generation-on-shapestacks",,https://arxiv.org/pdf/1804.08018v2.pdf,https://paperswithcode.com/dataset/shapestacks,"A simulation-based dataset featuring 20,000 stack configurations composed of a variety of elementary geometric primitives richly annotated regarding semantics and structural stability.",,ShapeStacks: Learning Vision-Based Physical Intuition for Generalised Object Stacking,https://arxiv.org/pdf/1804.08018v2.pdf,,,
2858,shape_bias,Out-of-Distribution Generalization,Out-of-Distribution Generalization,"Out-of-Distribution Generalization, Object Recognition, Domain Generalization",Image,,Computer Vision,object-recognition-on-shape-bias,CC-BY 4.0,https://github.com/rgeirhos/texture-vs-shape,https://paperswithcode.com/dataset/shape-bias,"The 'shape bias' dataset was introduced in Geirhos et al. (ICLR 2019) and consists of 224x224 images with conflicting texture and shape information (e.g., cat shape with elephant texture). This is used to measure the shape vs. texture bias of image classifiers.",2019,,,224 images,,
2859,ShARC,Question Answering,Question Answering,"Question Answering, Reading Comprehension, Decision Making",Text,English,Natural Language Processing,,CC BY-SA 3.0,https://sharc-data.github.io/,https://paperswithcode.com/dataset/sharc,ShARC is a Conversational Question Answering dataset focussing on question answering from texts containing rules.,,https://arxiv.org/abs/1809.01494,https://arxiv.org/abs/1809.01494,,,
2860,ShARe_CLEF_2014__Task_2_Disorders,Negation Detection,Negation Detection,"Negation Detection, Entity Linking, Weakly-Supervised Named Entity Recognition, Weakly Supervised Classification, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,"weakly-supervised-classification-on-share, weakly-supervised-named-entity-recognition-on-7",,https://physionet.org/content/shareclefehealth2014task2/1.0/,https://paperswithcode.com/dataset/share-clef-2014-task-2-disorders,,,,,,,
2861,SheetCopilot,Robot Task Planning,Robot Task Planning,Robot Task Planning,,,Methodology,robot-task-planning-on-sheetcopilot,,https://sheetcopilot.github.io/,https://paperswithcode.com/dataset/sheetcopilot,"The SheetCopilot dataset contains 28 evaluation workbooks and 221 spreadsheet manipulation tasks that are applied to these workbooks. These tasks involve diverse atomic actions related to six task categories (i.e. Entry and manipulation, Formatting, Management, Charts, Pivot Table, and Formula).

Dataset statistics:



Each task possesses one or more ground truth solutions.



The lengths of the task instructions range from 20 to 530 characters, with most tasks between 80 and 110 characters.



The number of atomic actions required by each task ranges from 1 to 9.



Evaluation metrics:



Execution success rate, pass rate, and the number of used actions are evaluated to judge the functional correctness and efficiency of a method.



A submitted solution is considered correct if the properties to be checked match those of any of the GT solutions of the corresponding task.



Please download the full datasets in our Github Repo:

https://github.com/BraveGroup/SheetCopilot

Thanks for using our dataset!",,,,,,
2862,ShEMO,Speech Emotion Recognition,Speech Emotion Recognition,Speech Emotion Recognition,"Audio, Image",,Speech,speech-emotion-recognition-on-shemo,,https://github.com/pariajm/ShEMO,https://paperswithcode.com/dataset/shemo,"The database includes 3000 semi-natural utterances, equivalent to 3 hours and 25 minutes of speech data extracted from online radio plays. The ShEMO covers speech samples of 87 native-Persian speakers for five basic emotions including anger, fear, happiness, sadness and surprise, as well as neutral state.",,,,,valent to 3 hours and 25 minutes of speech data extracted from online radio plays. The ShEMO covers speech samples,
2863,SHHS,Sleep Stage Detection,Sleep Stage Detection,Sleep Stage Detection,Image,,Computer Vision,sleep-stage-detection-on-shhs,,https://sleepdata.org/datasets/shhs,https://paperswithcode.com/dataset/shhs,"The Sleep Heart Health Study (SHHS) is a multi-center cohort study implemented by the National Heart Lung & Blood Institute to determine the cardiovascular and other consequences of sleep-disordered breathing. It tests whether sleep-related breathing is associated with an increased risk of coronary heart disease, stroke, all cause mortality, and hypertension.  In all, 6,441 men and women aged 40 years and older were enrolled between November 1, 1995 and January 31, 1998 to take part in SHHS Visit 1. During exam cycle 3 (January 2001- June 2003), a second polysomnogram (SHHS Visit 2) was obtained in 3,295 of the participants. CVD Outcomes data were monitored and adjudicated by parent cohorts between baseline and 2011. More than 130 manuscripts have been published investigating predictors and outcomes of sleep disorders.",1995,,,,,
2864,Shifts,Weather Forecasting,Weather Forecasting,"Weather Forecasting, Machine Translation, Self-Driving Cars","Text, Time Series",English,Natural Language Processing,weather-forecasting-on-shifts,Multiple licenses,https://github.com/yandex-research/shifts,https://paperswithcode.com/dataset/shifts,"The Shifts Dataset is a dataset for evaluation of uncertainty estimates and robustness to distributional shift. The dataset, which has been collected from industrial sources and services, is composed of three tasks, with each corresponding to a particular data modality: tabular weather prediction, machine translation, and self-driving car (SDC) vehicle motion prediction. All of these data modalities and tasks are affected by real, `in-the-wild' distributional shifts and pose interesting challenges with respect to uncertainty estimation.",,,,,,
2865,ShiftySpeech,Synthetic Speech Detection,Synthetic Speech Detection,Synthetic Speech Detection,"Audio, Image",,Speech,,CC BY 4.0,https://huggingface.co/datasets/ash56/ShiftySpeech,https://paperswithcode.com/dataset/shiftyspeech,"ShiftySpeech: A Large-Scale Synthetic Speech Dataset with Distribution Shifts

🔥 Key Features

3000+ hours of synthetic speech
Diverse Distribution Shifts: The dataset spans 7 key distribution shifts, including:  
📖 Reading Style  
🎙️ Podcast  
🎥 YouTube  
🗣️ Languages (Three different languages)  
🌎 Demographics (including variations in age, accent, and gender)  
Multiple Speech Generation Systems: Includes data synthesized from various TTS models and vocoders.

Dataset can be downloaded from: Hugging Face",,,,,,
2866,Shin2017A_MOABB,Within-Session Motor Imagery (left hand vs. right hand),Within-Session Motor Imagery (left hand vs. right hand),Within-Session Motor Imagery (left hand vs. right hand),,,Methodology,within-session-motor-imagery-left-hand-vs-7,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Shin2017A.html,https://paperswithcode.com/dataset/shin2017a-moabb,,,,,,,
2867,ShoeV2,Sketch Recognition,Sketch Recognition,"Sketch Recognition, Sketch-Based Image Retrieval, Image Retrieval",Image,,Computer Vision,,,https://www.eecs.qmul.ac.uk/~qian/Project_cvpr16.html,https://paperswithcode.com/dataset/sketch-me-that-shoe,"ShoeV2 is a dataset of 2,000 photos and 6648 sketches of shoes. The dataset is designed for fine-grained sketch-based image retrieval.",,,,,,
2868,short-MetaWorld,Trajectory Planning,Trajectory Planning,Trajectory Planning,Time Series,,Methodology,,MIT,https://connecthkuhk-my.sharepoint.com/personal/liangzx_connect_hku_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fliangzx%5Fconnect%5Fhku%5Fhk%2FDocuments%2Fshort%2DMetaWorld&ga=1,https://paperswithcode.com/dataset/short-metaworld,"Overview
Short-MetaWorld is a dataset rendered from modified environment of Meta-World [1], which contains Multi-Task10(MT10) and Meta-Learning10(ML10) in total 20 tasks with 100 successful trajectories for each task. Each trajectory is padded to 20 steps.



File Structure
This directory contains 3 sub-directories.



└── short-MetaWorld
    ├── task_description.py           # language instructions for each task
    ├── img_only                      # rendered visual inputs for all tasks
    │   ├── button-press-topdown-v2   # task name
    │   │     ├── 0                   # trajectory id   
    │   │     │   ├── 0.jpg           # step 0 observation (224224)
    │   │     │   ├── 1.jpg           # step 1 observation
    │   │     │   └── ...
    │   │     ├── 1
    │   │     └── ...
    │   ├── door-open-v2
    │   └── ...
    ├── unprocessed            # trajectory actions with visual observations (256256)
    │   ├── unprocessed_MT10_20    # task name
    │   │   ├── data.pkl           # a file contains all 10 tasks
    │   │   ├── door-open-v2.pkl   # door-open task file
    │   │   └── ...
    │   └── unprocessed_ML10_20
    └── r3m-processed              # trajectory actions with visual obs processed by R3M [2]


Contact
If you have any questions, please contact liangzx@connect.hku.hk

[1] Yu, Tianhe, et al. ""Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning."" Conference on robot learning. PMLR, 2020.
[2] Nair, Suraj, et al. ""R3M: A Universal Visual Representation for Robot Manipulation."" Conference on Robot Learning. PMLR, 2023.",2020,,,,,
2869,SHREC,Skeleton Based Action Recognition,Skeleton Based Action Recognition,"Skeleton Based Action Recognition, 3D Object Recognition, Point Cloud Super Resolution, Hand Gesture Recognition, Gesture Recognition","3D, Image, Video",,Computer Vision,"gesture-recognition-on-shrec-2017-track-on-3d, skeleton-based-action-recognition-on-shrec, hand-gesture-recognition-on-shrec-2017-track, hand-gesture-recognition-on-dhg-14, hand-gesture-recognition-on-dhg-28, point-cloud-super-resolution-on-shrec15, hand-gesture-recognition-on-shrec-2017, 3d-object-recognition-on-shrec11-split16-4",,http://tosca.cs.technion.ac.il/book/shrec.html,https://paperswithcode.com/dataset/shrec,"The SHREC dataset contains 14 dynamic gestures performed by 28 participants (all participants are right handed) and captured by the Intel RealSense short range depth camera. Each gesture is performed between 1 and 10 times by each participant in two way: using one finger and the whole hand. Therefore, the dataset is composed by 2800 sequences captured. The depth image, with a resolution of 640x480, and the coordinates of 22 joints (both in the 2D depth image space and in the 3D world space) are saved for each frame of each sequence in the dataset.",,Exploiting Recurrent Neural Networks and Leap Motion Controller for Sign Language and Semaphoric Gesture Recognition,https://arxiv.org/abs/1803.10435,,,
2870,SI-HDR,Single-shot HDR Reconstruction,Single-shot HDR Reconstruction,"Single-shot HDR Reconstruction, HDR Reconstruction, Single-Image-Based Hdr Reconstruction","3D, Image",,Computer Vision,,Creative Commons 4.0,https://www.repository.cam.ac.uk/items/c02ccdde-db20-4acd-8941-7816ef6b7dc7,https://paperswithcode.com/dataset/si-hdr,"The dataset consists of 181 HDR images. Each image includes: 1) a RAW exposure stack, 2) an HDR image, 3) simulated camera images at two different exposures 4) Results of 6 single-image HDR reconstruction methods: Endo et al. 2017, Eilertsen et al. 2017, Marnerides et al. 2018, Lee et al. 2018, Liu et al. 2020, and Santos et al. 2020

Project web page
More details can be found at: https://www.cl.cam.ac.uk/research/rainbow/projects/sihdr_benchmark/

Overview
This dataset contains 181 RAW exposure stacks selected to cover a wide range of image content and lighting conditions. Each scene is composed of 5 RAW exposures and merged into an HDR image using the estimator that accounts photon noise [3] (code at HDRutils). A simple color correction was applied using a reference white point and all merged HDR images were resized to 1920×1280 pixels.

The primary purpose of the dataset was to compare various single image HDR (SI-HDR) methods [1]. Thus, we selected a wide variety of content covering nature, portraits, cities, indoor and outdoor, daylight and night scenes. After merging and resizing, we simulated captures by applying a custom CRF and added realistic camera noise based on estimated noise parameters of Canon 5D Mark III.

The simulated captures were inputs to six selected SI-HDR methods. You can view the reconstructions of various methods for select scenes on our interactive viewer. For the remaining scenes, please download the appropriate zip files. We conducted a rigorous pairwise comparison experiment on these images to find that widely-used metrics did not correlate well with subjective data. We then proposed an improved evaluation protocol for SI-HDR [1].

If you find this dataset useful, please cite [1].

References
[1] Param Hanji, Rafał K. Mantiuk, Gabriel Eilertsen, Saghi Hajisharif, and Jonas Unger. 2022. “Comparison of single image hdr reconstruction methods — the caveats of quality assessment.” In Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings (SIGGRAPH ’22 Conference Proceedings). [Online]. Available: https://www.cl.cam.ac.uk/research/rainbow/projects/sihdr_benchmark/

[2] Gabriel Eilertsen, Saghi Hajisharif, Param Hanji, Apostolia Tsirikoglou, Rafał K. Mantiuk, and Jonas Unger. 2021. “How to cheat with metrics in single-image HDR reconstruction.” In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops. 3998–4007.

[3] Param Hanji, Fangcheng Zhong, and Rafał K. Mantiuk. 2020. “Noise-Aware Merging of High Dynamic Range Image Stacks without Camera Calibration.” In Advances in Image Manipulation (ECCV workshop). Springer, 376–391. [Online]. Available: https://www.cl.cam.ac.uk/research/rainbow/projects/noise-aware-merging/",2017,,,,,
2871,SIBR,Key-value Pair Extraction,Key-value Pair Extraction,"Key-value Pair Extraction, Key Information Extraction",,,Methodology,key-value-pair-extraction-on-sibr,Apache-2.0,https://www.modelscope.cn/datasets/iic/SIBR/summary,https://paperswithcode.com/dataset/sibr,"SIBR是面向自然场景视觉信息抽取的数据集。

1）SIBR总的有1000张图片，400张测试，600张训练，包括中文、英文两种语言。
2）包含images.zip、label.zip、train.txt、test.txt四个文件，images.zip、label.zip中包含所有图片和标签，通过train.txt和test.txt区分训练和测试。
3）标注规则与FUNSD、XFUND一致，实体类别包括header、question、answer、other四类，但与之不同的是除了提供link id之外，还标注了link type。link type包括inter以及intra两种，inter表示实体间的link，也即是一对kv对之间的link；而intra表示同一个实体内部segment之间的link。通过intra将多行文字组成一个实体，通过inter将实体组成kv对。",,,,,,
2872,SIB_bioinformatics_SPARQL_queries,Text2Sparql,Text2Sparql,Text2Sparql,,,Methodology,,CC-BY 4.0 or MIT,https://github.com/sib-swiss/sparql-examples,https://paperswithcode.com/dataset/sib-bioinformatics-sparql-queries,"A large collection of human-written natural language questions and their corresponding SPARQL queries over federated bioinformatics knowledge graphs (KGs) collected for several years across different research groups at the SIB Swiss Institute of Bioinformatics. 
The collection comprises more than 1000 example questions and queries, including 65 federated queries. 
We propose a methodology to uniformly represent the examples with minimal metadata, based on existing standards. 
Furthermore, we introduce an extensive set of open-source applications, including query graph visualizations and smart query editors, easily reusable by KG maintainers who adopt the proposed methodology.",,A large collection of bioinformatics question-query pairs over federated knowledge graphs: methodology and applications,https://arxiv.org/abs/2410.06010,,,
2873,SICE-Mix,Image Enhancement,Image Enhancement,Image Enhancement,Image,,Computer Vision,image-enhancement-on-sice-mix,,https://drive.google.com/file/d/1gii4AEyyPp_kagfa7TyugnNPvUhkX84x/view,https://paperswithcode.com/dataset/sice-mix,A test dataset SICE_Mix image datasets to represent complex mixed over-/under-exposed scenes.,,,,,,
2874,SICK,Semantic Similarity,Semantic Similarity,"Semantic Similarity, Semantic Textual Similarity, Natural Language Inference, Tabular Data Generation","Tabular, Text",English,Natural Language Processing,"semantic-textual-similarity-on-sick-r-1, semantic-textual-similarity-on-sick, tabular-data-generation-on-sick, natural-language-inference-on-sick, semantic-similarity-on-sick",CC BY-NC-SA 3.0,http://marcobaroni.org/composes/sick.html,https://paperswithcode.com/dataset/sick,"The Sentences Involving Compositional Knowledge (SICK) dataset is a dataset for compositional distributional semantics. It includes a large number of sentence pairs that are rich in the lexical, syntactic and semantic phenomena. Each pair of sentences is annotated in two dimensions: relatedness and entailment. The relatedness score ranges from 1 to 5, and Pearson’s r is used for evaluation; the entailment relation is categorical, consisting of entailment, contradiction, and neutral. There are 4439 pairs in the train split, 495 in the trial split used for development and 4906 in the test split. The sentence pairs are generated from image and video caption datasets before being paired up using some algorithm.",,Multi-Label Transfer Learning for Multi-Relational Semantic Similarity,https://arxiv.org/abs/1805.12501,,,
2875,SICKLE,Sensor Fusion,Sensor Fusion,"Sensor Fusion, Super-Resolution, Plant Phenotyping, Crop Type Mapping, Harvesting Date Prediction, Panoptic Segmentation, Sowing Date Prediction, Crop Yield Prediction, Crop Classification, Transplanting Date Prediction, Multi-Task Learning, Synthetic Data Generation, 2D Semantic Segmentation","Image, Text, Time Series",English,Computer Vision,"crop-type-mapping-on-sickle, crop-yield-prediction-on-sickle, transplanting-date-prediction-on-sickle, harvesting-date-prediction-on-sickle, sowing-date-prediction-on-sickle",,https://sites.google.com/iiitd.ac.in/sickle/,https://paperswithcode.com/dataset/sickle,"The availability of well-curated datasets has driven the success of Machine Learning (ML) models. Despite greater access to earth observation data in agriculture, there is a scarcity of curated and labelled datasets, which limits the potential of its use in training ML models for remote sensing (RS) in agriculture. To this end, we introduce a first-of-its-kind dataset called SICKLE, which constitutes a time-series of multi-resolution imagery from 3 distinct satellites: Landsat-8, Sentinel-1 and Sentinel-2. Our dataset constitutes multi-spectral, thermal and microwave sensors during January 2018 - March 2021 period. We construct each temporal sequence by considering the cropping practices followed by farmers primarily engaged in paddy cultivation in the Cauvery Delta region of Tamil Nadu, India; and annotate the corresponding imagery with key cropping parameters at multiple resolutions (i.e. 3m, 10m and 30m). Our dataset comprises 2, 370 season-wise samples from 388 unique plots, having an average size of 0.38 acres, for classifying 21 crop types across 4 districts in the Delta, which amounts to approximately 209, 000 satellite images. Out of the 2, 370 samples, 351 paddy samples from 145 plots are annotated with multiple crop parameters; such as the variety of paddy, its growing season and productivity in terms of per-acre yields. Ours is also one among the first studies that consider the growing season activities pertinent to crop phenology (spans sowing, transplanting and harvesting dates) as parameters of interest. We benchmark SICKLE on three tasks: crop type, crop phenology (sowing, transplanting, harvesting), and yield prediction.",2018,,,370 samples,"training ML models for remote sensing (RS) in agriculture. To this end, we introduce a first-of-its-kind dataset called SICKLE, which constitutes a time-series of multi-resolution imagery from 3 distinct satellites: Landsat-8, Sentinel-1 and Sentinel-2. Our dataset constitutes multi-spectral, thermal and microwave sensors during January 2018 - March 2021 period. We construct each temporal sequence by considering the cropping practices followed by farmers primarily engaged in paddy cultivation in the Cauvery Delta region of Tamil Nadu, India; and annotate the corresponding imagery with key cropping parameters at multiple resolutions (i.e. 3m, 10m and 30m). Our dataset comprises 2, 370 season-wise samples",
2876,SID,Low-Light Image Enhancement,Low-Light Image Enhancement,"Low-Light Image Enhancement, Image Denoising, Denoising",Image,,Computer Vision,"image-denoising-on-sid-x300, low-light-image-enhancement-on-sid, image-denoising-on-sid-x100",,https://cchen156.github.io/SID.html,https://paperswithcode.com/dataset/sid,"The See-in-the-Dark (SID) dataset contains 5094 raw short-exposure images, each with a corresponding long-exposure reference image.
Images were captured using two cameras: Sony α7SII and Fujifilm X-T2.",,,,,,
2877,SIDD-Image,Image Classification,Image Classification,"Image Classification, Network Intrusion Detection","Graph, Image",,Computer Vision,network-intrusion-detection-on-sidd-a-large,CC-BY,https://www.kaggle.com/datasets/yuweisunut/sidd-segmented-intrusion-detection-dataset,https://paperswithcode.com/dataset/sidd-network,"This is the first image-based network intrusion detection dataset. This large-scale dataset included network traffic protocol communication-based images from 15 different observation locations of different countries in Asia. This dataset is used to identify two different types of anomalies from benign network traffic. Each image with a size of 48 × 48 contains multi-protocol communications within 128 seconds. The SIDD dataset can be to applied to a broad range of tasks such as machine learning-based network intrusion detection, non-iid federated learning, and so forth.",,,,,,
2878,SIDD,Noise Estimation,Noise Estimation,"Noise Estimation, Image Denoising, Image Restoration, Denoising","Audio, Image",,Computer Vision,"noise-estimation-on-sidd, image-denoising-on-sidd",MIT,https://www.eecs.yorku.ca/~kamel/sidd/,https://paperswithcode.com/dataset/sidd,"SIDD is an image denoising dataset containing 30,000 noisy images from 10 scenes under different lighting conditions using five representative smartphone cameras. Ground truth images are provided along with the noisy images.",,,,,,
2879,SidechainNet,Protein Structure Prediction,Protein Structure Prediction,Protein Structure Prediction,Time Series,,Methodology,,,https://github.com/jonathanking/sidechainnet,https://paperswithcode.com/dataset/sidechainnet,"SidechainNet is a protein structure prediction dataset that directly extends ProteinNet. Specifically, SidechainNet adds measurements for protein angles and coordinates that describe the complete, all-atom protein structure (backbone and sidechain, excluding hydrogens) instead of the protein backbone alone.",,,,,,
2880,SIDER,Graph Classification,Graph Classification,"Graph Classification, Drug Discovery, Molecular Property Prediction, Molecular Property Prediction (1-shot))","Graph, Image, Time Series",,Computer Vision,"drug-discovery-on-sider, molecular-property-prediction-on-sider-1, molecular-property-prediction-1-shot-on-sider, graph-classification-on-sider",,http://sideeffects.embl.de/,https://paperswithcode.com/dataset/sider,"SIDER contains information on marketed medicines and their recorded adverse drug reactions. The information is extracted from public documents and package inserts. The available information include side effect frequency, drug and side effect classifications as well as links to further information, for example drug–target relations.",,,,,,
2881,Siena_Scalp_EEG_Database,Seizure Detection,Seizure Detection,Seizure Detection,Image,,Computer Vision,,Creative Commons Attribution 4.0 International Public License,https://physionet.org/content/siena-scalp-eeg/1.0.0/,https://paperswithcode.com/dataset/siena-scalp-eeg-database,"The database consists of EEG recordings of 14 patients acquired at the Unit of Neurology and Neurophysiology of the University of Siena.  Subjects include 9 males (ages 25-71) and 5 females (ages 20-58). Subjects were monitored with a Video-EEG with a sampling rate of 512 Hz, with electrodes arranged on the basis of the international 10-20 System. Most of the recordings also contain 1 or 2 EKG signals. The diagnosis of epilepsy and the classification of seizures according to the criteria of the International League Against Epilepsy were performed by an expert clinician after a careful review of the clinical and electrophysiological data of each patient.",,,,,,
2882,SignalTrain_LA2A_Dataset,Audio Effects Modeling,Audio Effects Modeling,Audio Effects Modeling,Audio,,Audio,,Creative Commons Attribution 4.0 International,https://zenodo.org/record/3824876,https://paperswithcode.com/dataset/signaltrain-la2a-dataset,"LA-2A Compressor data to accompany the paper ""SignalTrain: Profiling Audio Compressors with Deep Neural Networks,"" https://arxiv.org/abs/1905.11928

Accompanying computer code: https://github.com/drscotthawley/signaltrain

A collection of recorded data from an analog Teletronix LA-2A opto-electronic compressor, for various settings of the Peak Reduction knob.  Other knobs were kept constant.  

Audio samples present in these files are either 'randomly generated', or downloaded audio clips with Create Commons licenses, or are property of Scott Hawley freely distributed as part of this dataset. 

Data taken by Ben Colburn, supervised by Scott Hawley

Dataset used in:



""Efficient neural networks for real-time analog audio effect modeling"" by C. Steinmetz & J. Reiss, 2021. https://arxiv.org/abs/2102.06200



“Exploring quality and generalizability in parameterized neural audio effects,"" by W. Mitchell and S. H. Hawley, 149th Audio Engineering Society Convention (AES), 2020.  https://arxiv.org/abs/2006.05584



""SignalTrain: Profiling Audio Compressors with Deep Neural Networks,"" 147th Audio Engineering Society Convention (AES), 2019. https://arxiv.org/abs/1905.11928",1905,,,,,
2883,SILD,Variable Disambiguation,Variable Disambiguation,"Variable Disambiguation, Variable Detection",Image,,Computer Vision,,,https://github.com/e-tornike/SIL,https://paperswithcode.com/dataset/sild,"This dataset contains a collection of texts from publications from a broad range of social science domains (e.g., economics, politics, psychology, etc.). The texts are annotated with labels for Survey Item Linking (SIL), an Entity Linking (EL) task. SIL is divided into two sub-tasks: Mention Detection (MD), a binary text classification task, and Entity Disambiguation (ED), a sentence similarity task. Sentences that mention survey items are labeled with the IDs of entities from a knowledge base (GSIM). SILD contains 20,454 sentences in English and German from 100 publications.",,,,454 sentences,,
2884,Silent_Speech_EMG,Electromyography (EMG),Electromyography (EMG),"Electromyography (EMG), Speech Synthesis",Audio,,Audio,,,https://github.com/dgaddy/silent_speech,https://paperswithcode.com/dataset/silent-speech-emg,Facial electromyography recordings during both silent and vocalized speech.,,,,,,
2885,Silhouettes,Variational Inference,Variational Inference,"Variational Inference, Density Estimation",,,Methodology,,,https://people.cs.umass.edu/~marlin/data.shtml,https://paperswithcode.com/dataset/silhouettes,"The Caltech 101 Silhouettes dataset consists of 4,100 training samples, 2,264 validation samples and 2,307 test samples. The datast is based on CalTech 101 image annotations. Each image in the CalTech 101 data set includes a high-quality polygon outline of the primary object in the scene. To create the CalTech 101 Silhouettes data set, the authors center and scale each outline and render it on a DxD pixel image-plane. The outline is rendered as a filled, black polygon on a white background. Many object classes exhibit silhouettes that have distinctive class-specific features. A relatively small number of classes like soccer ball, pizza, stop sign, and yin-yang are indistinguishable based on shape, but have been left-in in the data.",,1 Introduction,https://arxiv.org/abs/1506.04557,,"training samples, 2,264 validation samples",
2886,SILICONE_Benchmark,Sentiment Analysis,Sentiment Analysis,"Sentiment Analysis, Dialog Act Classification, Text Classification, Emotion Classification","Image, Text",English,Computer Vision,text-classification-on-silicone-benchmark,,https://huggingface.co/datasets/silicone,https://paperswithcode.com/dataset/silicone-benchmark,"The Sequence labellIng evaLuatIon benChmark fOr spoken laNguagE (SILICONE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems specifically designed for spoken language. All datasets are in the English language and covers a  large variety of domains (e.g daily life, scripted scenarios, joint task completion, phone call conversations, and televsion dialogue). Some datasets additionally include emotion and/or sentiment labels.",,,,,,
2887,SIMARA,Key Information Extraction,Key Information Extraction,"Key Information Extraction, Handwritten Text Recognition","Image, Text",English,Computer Vision,"handwritten-text-recognition-on-simara, key-information-extraction-on-simara",Creative Commons Attribution 4.0 International,https://zenodo.org/record/7868059,https://paperswithcode.com/dataset/simara,"Description
We propose a new database for information extraction from historical handwritten documents. The corpus includes 5,393 finding aids from six different series, dating from the 18th-20th centuries. Finding aids are handwritten documents that contain metadata describing older archives. They are stored in the National Archives of France and are used by archivists to identify and find archival documents. 

Each document is annotated at page-level, and contains seven fields to retrieve. The localization of each field is not available in such a way that this dataset encourages research on segmentation-free systems for information extraction.

The dataset is available at https://zenodo.org/record/7868059

Details for each series and entity type
| Series              | Train | Validation | Test | Total (%) |
| ------------------- | ----- | ---------- | ---- | --------: |
| E series            | 322   | 64         | 79   |       8.6 |
| L series            | 38    | 8          | 4    |       0.9 |
| M series            | 128   | 21         | 27   |       3.3 |
| X1a series          | 2209  | 491        | 469  |      58.8 |
| Y series            | 940   | 205        | 196  |      24.9 |
| Douët s'Arcq series | 141   | 22         | 29   |       3.5 |
| Total               | 3778  | 811        | 804  |       100 |

| Entities       | Train | Validation | Test  | Total (%) |
| -------------- | ----- | ---------- | ----- | --------: |
| date           | 8406  | 1814       | 1799  |      10.4 |
| title          | 35531 | 7495       | 8173  |      44.5 |
| serie          | 3168  | 664        | 676   |       3.9 |
| analysis       | 25988 | 5130       | 5602  |      31.9 |
| volume_number  | 3913  | 808        | 813   |       4.8 |
| article_number | 3181  | 665        | 678   |       3.9 |
| arrangement    | 644   | 122        | 153   |       0.8 |
| Total          | 80831 | 16698      | 17894 |       100 |

Data encoding
Transcriptions with entities are encoded in the labels.json JSON file. Special tokens are used to represent named entities. Please not that there are only opening NER tokens: each entity spans all words until the next entity starts. 

| Entities       | Special token | Symbol unicode |
| -------------- | ------------- | -------- |
| date           | ⓓ            | \u24d3 |
| title          | ⓘ            | \u24d8 |
| serie          | ⓢ            | \u24e2 |
| analysis       | ⓒ            | \u24d2 |
| volume_number  | ⓟ            | \u24df |
| article_number | ⓐ            | \u24d0 |
| arrangement    | ⓥ            | \u24e5 |

Cite us!
The dataset is presented in details in the following article:

bib
@article{simara2023,
    author = {Solène Tarride and Mélodie Boillet and Jean-François Moufflet and Christopher Kermorvant},
    title = {SIMARA: a database for key-value information extraction from full-page handwritten documents},
    year = {2023},
    journal={Proceedings of the 17th International Conference on Document Analysis and Recognition},
}",2023,,,,,
2888,SIMMC2.0,Response Generation,Response Generation,"Response Generation, Dialogue State Tracking","Image, Text, Video",English,Computer Vision,"response-generation-on-simmc2-0, dialogue-state-tracking-on-simmc2-0",Custom,https://github.com/facebookresearch/simmc2/tree/simmc2.0,https://paperswithcode.com/dataset/simmc2-0,"Next generation task-oriented dialog systems need to understand conversational contexts with their perceived surroundings, to effectively help users in the real-world multimodal environment. Existing task-oriented dialog datasets aimed towards virtual assistance fall short and do not situate the dialog in the user's multimodal context. To overcome, we present a new dataset for Situated and Interactive Multimodal Conversations, SIMMC 2.0, which includes 11K task-oriented user<->assistant dialogs (117K utterances) in the shopping domain, grounded in immersive and photo-realistic scenes.
The dialogs are collected using a two-phase pipeline: (1) A novel multimodal dialog simulator generates simulated dialog flows, with an emphasis on diversity and richness of interactions, (2) Manual paraphrasing of the generated utterances to collect diverse referring expressions. We provide an in-depth analysis of the collected dataset, and describe in detail the four main benchmark tasks we propose. Our baseline model, powered by the state-of-the-art language model, shows promising results, and highlights new challenges and directions for the community to study.",,,,,,
2889,SimpleQuestions,Knowledge Base Question Answering,Knowledge Base Question Answering,"Knowledge Base Question Answering, Question Answering",Text,English,Natural Language Processing,"knowledge-base-question-answering-on-2, question-answering-on-simplequestions",BSD-3-Clause License,https://github.com/davidgolub/SimpleQA/tree/master/datasets/SimpleQuestions,https://paperswithcode.com/dataset/simplequestions,"SimpleQuestions is a large-scale factoid question answering dataset. It consists of 108,442 natural language questions, each paired with a corresponding fact from Freebase knowledge base. Each fact is a triple (subject, relation, object) and the answer to the question is always the object. The dataset is divided into training, validation, and test  sets with 75,910, 10,845 and 21,687 questions respectively.",,Hierarchical Memory Networks,https://arxiv.org/abs/1605.07427,,,
2890,SimpleQuestionsWikiData,Semantic Parsing,Semantic Parsing,"Semantic Parsing, Knowledge Base Question Answering",Text,English,Natural Language Processing,knowledge-base-question-answering-on-4,CC License,https://github.com/askplatypus/wikidata-simplequestions,https://paperswithcode.com/dataset/simplequestionswikidata,"SimpleQuestionsWikidata maps SimpleQuestions to Wikidata.

It was proposed in the paper Question Answering Benchmarks for Wikidata by Diefenbach et al.",,Question Answering Benchmarks for Wikidata,https://ceur-ws.org/Vol-1963/paper555.pdf,,,
2891,SimplerEnv-Google_Robot,Robot Manipulation,Robot Manipulation,Robot Manipulation,,,Methodology,robot-manipulation-on-simpler-env,MIT,https://simpler-env.github.io/,https://paperswithcode.com/dataset/simpler-env,"Significant progress has been made in building generalist robot manipulation policies, yet their scalable and reproducible evaluation remains challenging, as real-world evaluation is operationally expensive and inefficient. We propose employing physical simulators as efficient, scalable, and informative complements to real-world evaluations. These simulation evaluations offer valuable quantitative metrics for checkpoint selection, insights into potential real-world policy behaviors or failure modes, and standardized setups to enhance reproducibility.

This repository's code is based in the SAPIEN simulator and the CPU based ManiSkill2 benchmark. We have also integrated the Bridge dataset environments into ManiSkill3, which offers GPU parallelization and can run 10-15x faster than the ManiSkill2 version. For instructions on how to use the GPU parallelized environments and evaluate policies on them, see: https://github.com/simpler-env/SimplerEnv/tree/maniskill3

This repository encompasses 2 real-to-sim evaluation setups:

Visual Matching evaluation: Matching real & sim visual appearances for policy evaluation by overlaying real-world images onto simulation backgrounds and adjusting foreground object and robot textures in simulation.
Variant Aggregation evaluation: creating different sim environment variants (e.g., different backgrounds, lightings, distractors, table textures, etc) and averaging their results.",,,,,,
2892,SimplerEnv-Widow_X,Robot Manipulation,Robot Manipulation,Robot Manipulation,,,Methodology,robot-manipulation-on-simplerenv-widow-x,MIT,https://simpler-env.github.io/,https://paperswithcode.com/dataset/simplerenv-widow-x,"Significant progress has been made in building generalist robot manipulation policies, yet their scalable and reproducible evaluation remains challenging, as real-world evaluation is operationally expensive and inefficient. We propose employing physical simulators as efficient, scalable, and informative complements to real-world evaluations. These simulation evaluations offer valuable quantitative metrics for checkpoint selection, insights into potential real-world policy behaviors or failure modes, and standardized setups to enhance reproducibility.

This repository's code is based in the SAPIEN simulator and the CPU based ManiSkill2 benchmark. We have also integrated the Bridge dataset environments into ManiSkill3, which offers GPU parallelization and can run 10-15x faster than the ManiSkill2 version. For instructions on how to use the GPU parallelized environments and evaluate policies on them, see: https://github.com/simpler-env/SimplerEnv/tree/maniskill3

This repository encompasses 2 real-to-sim evaluation setups:

Visual Matching evaluation: Matching real & sim visual appearances for policy evaluation by overlaying real-world images onto simulation backgrounds and adjusting foreground object and robot textures in simulation.
Variant Aggregation evaluation: creating different sim environment variants (e.g., different backgrounds, lightings, distractors, table textures, etc) and averaging their results.
We hope that our work guides and inspires future real-to-sim evaluation efforts.",,,,,,
2893,simply-CLEVR,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Question Answering, Feature Importance","Image, Text",English,Computer Vision,,,https://github.com/ahmedmagdiosman/simply-clevr-dataset,https://paperswithcode.com/dataset/simply-clevr,"The simply-CLEVR dataset aims to provide a benchmark dataset that can be used for transparent quantitative evaluation of explanation methods (aka heatmaps/XAI methods).
It is made of simple Visual Question Answering (VQA) questions, which are derived from the original CLEVR task, and where each question is accompanied by two Ground Truth Masks that serve as a basis for evaluating explanations on the input image.",,,,,,
2894,Sims4Action,Synthetic-to-Real Translation,Synthetic-to-Real Translation,"Synthetic-to-Real Translation, Action Classification, Transfer Learning, Domain Generalization, Unsupervised Domain Adaptation, Activity Recognition, Domain Adaptation","Image, Text, Video",English,Computer Vision,,,https://github.com/aroitberg/sims4action,https://paperswithcode.com/dataset/sims4action,"The Sims4Action Dataset: a videogame-based dataset for Synthetic→Real domain adaptation for human activity recognition.



Goal : Exploring the concept of constructing training examples for Activities of Daily Living (ADL) recognition by playing life simulation video games.  


 Sims4Action  dataset is created with the commercial game THE SIMS 4   by executing actions-of-interest within the game in a ""top-down"" manner. It features ten hours of video material of eight diverse characters and multiple environments.  Ten actions are selected to have a direct correspondence to categories covered in the real-life dataset Toyota Smarthome [2] to enable the research of Synthetic→Real transfer in action recognition.
Two benchmarks : Gaming→Gaming (training and evaluation on Sims4Action)  and Gaming→Real (training on Sims4Action, evaluation on the real Toyota Smarthome data [2]).
Main challenge: Gaming→Real domain adaptation
  While ADL recognition on gaming data is interesting from a theoretical perspective, the key challenge arises from transferring knowledge learned from simulated data to real-world applications. Sims4Action specifically provides a benchmark for this scenario since it describes a Gaming→Real challenge, which evaluates models on real videos derived from the existing Toyota Smarthome dataset . 

References
[1] Let's Play for Action: Recognizing Activities of Daily Living by Learning from Life Simulation Video Games.
Alina Roitberg, David Schneider, Aulia Djamal, Constantin Seibold, Simon Reiß, Rainer Stiefelhagen,
In International Conference on Intelligent Robots and Systems (IROS), 2021
(* denotes equal contribution.)

[2] Toyota smarthome: Real-world activities of daily living.
Srijan Das, Rui Dai, Michal Koperski, Luca Minciullo, Lorenzo Garattoni, Francois Bremond, Gianpiero Francesca,
In International Conference on Computer Vision (ICCV), 2019.",2021,Let's Play for Action: Recognizing Activities of Daily Living by Learning from Life Simulation Video Games.,http://arxiv.org/abs/2107.05617,,,
2895,Simulated_micro-Doppler_Signatures,Out-of-Distribution Detection,Out-of-Distribution Detection,"Out-of-Distribution Detection, Anomaly Detection, One-class classifier, Classification, 2D Semantic Segmentation",Image,,Computer Vision,,MIT,https://cloud.mbauw.eu/s/BPtk5HYkyBWAGLo,https://paperswithcode.com/dataset/simulated-micro-doppler-signatures,"Simulated pulse Doppler radar signatures for four classes of helicopter-like targets. The classes differ in the number of rotating blades each kind of target carries, thus each class translates into a specific modulation pattern on the Doppler signature. Doppler signatures are a typical feature used to achieve radar targets discrimination. This dataset was generated using a simple open-source MATLAB simulation code, which can be easily modified to generate custom datasets with more classes and increased intra-class diversity. 

Dataset can be easily used for supervised classification, out-of-distribution detection (near and far), unsupervised learning and modulation pattern segmentation. The code includes the generation of an SPD representation for each signature, thanks to the computation of a covariance matrix, thus allowing for second-order specific data processing (e.g. Riemannian neural network or tangent PCA).

Dataset used in the paper

Dataset generation code",,,,,,
2896,SinD,Human Behavior Forecasting,Human Behavior Forecasting,"Human Behavior Forecasting, Motion Forecasting, Trajectory Prediction, motion prediction","Image, Time Series, Video",,Computer Vision,,,https://github.com/SOTIF-AVLab/SinD,https://paperswithcode.com/dataset/sind,"The SIND dataset is based on 4K video captured by drones, providing information including traffic participant trajectories, traffic light status, and high-definition maps",,,,,,
2897,SINGA_PURA,Environmental Sound Classification,Environmental Sound Classification,"Environmental Sound Classification, Audio Classification, Audio Multiple Target Classification, Sound Event Detection, Audio Tagging","Audio, Image",,Computer Vision,,,https://researchdata.ntu.edu.sg/dataset.xhtml?persistentId=doi:10.21979/N9/Y8UQ6F,https://paperswithcode.com/dataset/singa-pura,"This repository contains the SINGA:PURA dataset, a strongly-labelled polyphonic urban sound dataset with spatiotemporal context. The data were collected via a number of recording units deployed across Singapore as a part of a wireless acoustic sensor network. These recordings were made as part of a project to identify and mitigate noise sources in Singapore, but also possess a wider applicability to sound event detection, classification, and localization. The taxonomy we used for the labels in this dataset has been designed to be compatible with other existing datasets for urban sound tagging while also able to capture sound events unique to the Singaporean context. Please refer to our conference paper published in APSIPA 2021 (which is found in this repository as the file ""APSIPA.pdf"") or download the readme (""Readme.md"") for more details regarding the data collection, annotation, and processing methodologies for the creation of the dataset.",2021,,,,,
2898,Single_Point_Corn_Yield_Data,Type prediction,Type prediction,Type prediction,Time Series,,Methodology,,Custom,https://data.mendeley.com/datasets/dkv6b3xj99/1,https://paperswithcode.com/dataset/single-point-corn-yield-data,"This data comprises processed weather, soil, yield, and cultivation area for corn yield prediction in Sub-Sahara Africa, with emphasis on Nigeria. The data was collected to design a corn yield prediction model to help smallholder farmers make smart farming decisions. However, the data can serve several other purposes through analysis and interpretation.

The reference study region in Africa is Nigeria. The focuses on corn crop because there are over 211.4 million people, of which a large percentage of the population are smallholder farmers. Nigeria [9.0820° N, 8.6753° E] is within an arable land area of 34 million hectares located on the west coast of Africa. The region comprises of 36 states with the most and least number of districts being 214 and 10, respectively. For each state, the environment data are collected as follows. 

Grid map climate data – This data spans spatial resolutions between ~1 km2 to ~340 km2 from the high spatial resolution WorldClim global climate database22. Each grid point on the map is monthly data from January to December between 1970 and 2000 years and records 8 climate variables. The variables are average temperature C0, minimum temperature C0, maximum temperature C0, precipitation (mm), solar radiation (kJ m^(-2) day(-1), wind speed (m s(-1)), and water vapor (kPa) taken at 30 seconds (s), 2.5 minutes m, 5 m, and 10 m. 

Grid map soil data – This data is obtained from 250 minutes of spatial resolution AfSIS soil data23 from year 1960 to 2012. The variables are wet soil bulk density, dry bulk density (kg dm-3), clay percentage of plant available water content, hydraulic conductivity, the upper limit of plant available water content, the lower limit of, organic matter percentage, pH, sand percentage (g 100 g-1), silt percentage (g 100 g-1) and, clay percentage (g 100 g-1), and saturated volumetric water content variables measured at depths 0–5, 5–10, 10–15, 15–30, 30–45, 45–60, 60–80, 80–100, and 100–120 measured in centimeters (cm). 

Corn yield data – This data is available on Kneoma Corporation website24. It ranged from years 1995 to 2006 and consisted of a corn yield of 1000 metric tonnes and a cultivation area of 1000 hectares.  

Geolocation coordinates (latitude and longitude) – The geolocation of each of the 36 states with their districts is sampled from Google Maps. The output feds into the Esri-ArcGIS 2.5, a professional geographical software, for extracting the point-cloud values of each environmental variable (weather and soil) at specific geolocation of the 36 states of Nigeria.

Other Descriptions:
Data type - Continous and Categorical
Dataset Characteristics - Tabular
Associated Tasks - Regression
Feature Type - Real
Number of Instances - 1828
Number of Features: 12",1970,,,,,
2899,Situation_Puzzle,Common Sense Reasoning,Common Sense Reasoning,"Common Sense Reasoning, Riddle Sense",,,Reasoning,,,https://github.com/chenqi008/LateralThinking,https://paperswithcode.com/dataset/situation-puzzle,"LLMs' lateral thinking capabilities remain under-explored and challenging to measure due to the complexity of assessing creative thought processes and the scarcity of relevant data. To address these challenges, we introduce SPLAT, a benchmark leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs. This benchmark, containing 975 graded situation puzzles across three difficulty levels.",,,,,,
2900,SiW,Face Anti-Spoofing,Face Anti-Spoofing,Face Anti-Spoofing,Image,,Computer Vision,face-anti-spoofing-on-siw-protocol-3,,https://cvlab.cse.msu.edu/siw-spoof-in-the-wild-database.html,https://paperswithcode.com/dataset/siw,"SiW provides live and spoof videos from 165 subjects. For each subject, we have 8 live and up to 20 spoof videos, in total 4,478 videos. All videos are in 30 fps, about 15 second length, and 1080P HD resolution. The live videos are collected in four sessions with variations of distance, pose, illumination and expression. The spoof videos are collected with several attacks such as printed paper and replay.",,,,,,
2901,SIZER,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, 3D Human Reconstruction, Multi-target regression","3D, Image",,Computer Vision,,,https://virtualhumans.mpi-inf.mpg.de/sizer/,https://paperswithcode.com/dataset/sizer,"Dataset of clothing size variation which includes  different subjects wearing casual clothing items in various sizes, totaling to approximately 2000 scans. This dataset includes the scans, registrations to the SMPL model, scans segmented in clothing parts, garment category and size labels.",2000,,,,,
2902,SKAB,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Outlier Detection, Change Point Detection",Image,,Computer Vision,"change-point-detection-on-skab, outlier-detection-on-skab",GNU General Public License v3.0,https://github.com/waico/SKAB,https://paperswithcode.com/dataset/skab,SKAB is designed for evaluating algorithms for anomaly detection. The benchmark currently includes 30+ datasets plus Python modules for algorithms’ evaluation. Each dataset represents a multivariate time series collected from the sensors installed on the testbed. All instances are labeled for evaluating the results of solving outlier detection and changepoint detection problems.,,,,,,
2903,Sketch,Continual Learning,Continual Learning,Continual Learning,,,Methodology,continual-learning-on-sketch-fine-grained-6,CC BY 4.0,http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/,https://paperswithcode.com/dataset/sketch,"The Sketch dataset contains over 20,000 sketches evenly distributed over 250 object categories.",,,,,,
2904,SketchGraphs,Program Synthesis,Program Synthesis,Program Synthesis,,,Methodology,,,https://github.com/PrincetonLIPS/SketchGraphs,https://paperswithcode.com/dataset/sketchgraphs,"SketchGraphs is a dataset of 15 million sketches extracted from real-world CAD models intended to facilitate research in both ML-aided design and geometric program induction.
Each sketch is represented as a geometric constraint graph where edges denote designer-imposed geometric relationships between primitives, the nodes of the graph.",,,,,,
2905,SkinCon,Interpretable Machine Learning,Interpretable Machine Learning,Interpretable Machine Learning,,,Methodology,,,https://SkinCon-dataset.github.io,https://paperswithcode.com/dataset/skincon,"SkinCon is a skin disease dataset densely annotated by dermatologists. SkinCon includes 3230 images from the Fitzpatrick 17k skin disease (Fitzpatrick Skin Tone) dataset densely labelled with 48 clinical concepts, 22 of which have at least 50 images representing the concept. The concepts used were chosen by two dermatologists considering the clinical descriptor terms used to describe skin lesions. Examples include ""plaque"", ""scale"", and ""erosion"".",,SkinCon: A skin disease dataset densely annotated by domain experts for fine-grained model debugging and analysis,https://arxiv.org/pdf/2302.00785v1.pdf,3230 images,,
2906,SKM-TEA,3D Medical Imaging Segmentation,3D Medical Imaging Segmentation,"3D Medical Imaging Segmentation, Medical Object Detection, MRI Reconstruction, MRI segmentation","3D, Image",,Medical,,Stanford University Dataset Research Use Agreement,https://github.com/StanfordMIMI/skm-tea,https://paperswithcode.com/dataset/skm-tea,"The SKM-TEA dataset pairs raw quantitative knee MRI (qMRI) data, image data, and dense labels of tissues and pathology for end-to-end exploration and evaluation of the MR imaging pipeline.  This 1.6TB dataset consists of raw-data measurements of ~25,000 slices (155 patients) of anonymized patient knee MRI scans, the corresponding scanner-generated DICOM images, manual segmentations of four tissues, and bounding box annotations for sixteen clinically relevant pathologies.

Challenge Tracks
DICOM Track: The DICOM benchmarking track uses scanner-generated DICOM images as the input for image segmentation and detection tasks.

Raw Data Track: The Raw Data benchmarking track uses raw MRI data (i.e. k-space) as the input for image reconstruction, segmentation and detection tasks.",,,,,"valuation of the MR imaging pipeline.  This 1.6TB dataset consists of raw-data measurements of ~25,000 slices (155 patients) of anonymized patient knee MRI scans, the corresponding scanner-generated DICOM images",
2907,Slakh2100,Music Transcription,Music Transcription,"Music Transcription, Semantic Segmentation, Data Augmentation, Music Source Separation, Multi-instrument Music Transcription","Audio, Image",,Computer Vision,"music-transcription-on-slakh2100, music-source-separation-on-slakh2100, multi-instrument-music-transcription-on",,http://www.slakh.com/,https://paperswithcode.com/dataset/slakh2100,"The Synthesized Lakh (Slakh) Dataset is a dataset for audio source separation that is synthesized from the Lakh MIDI Dataset v0.1 using professional-grade sample-based virtual instruments. This first release of Slakh, called Slakh2100, contains 2100 automatically mixed tracks and accompanying MIDI files synthesized using a professional-grade sampling engine. The tracks in Slakh2100 are split into training (1500 tracks), validation (375 tracks), and test (225 tracks) subsets, totaling 145 hours of mixtures.",,,,,,
2908,SLAM2REF,3D Pose Estimation,3D Pose Estimation,"3D Pose Estimation, Camera Pose Estimation, Camera Relocalization, Pose Estimation, Camera Localization, Simultaneous Localization and Mapping, Pose Tracking, Indoor Localization, 6D Pose Estimation using RGBD, Robot Pose Estimation, Semantic SLAM, 6D Pose Estimation, Object SLAM, 2D Pose Estimation, lidar absolute pose regression, Visual Odometry, Monocular Visual Odometry, Pose Retrieval","3D, Image, Video",,Computer Vision,,CC BY-NC,https://mediatum.ub.tum.de/1459256?show_id=1743877,https://paperswithcode.com/dataset/slam2ref,"This dataset comprehends the 3D building information model (in IFC and Revit formats), 
manually elaborated based on the terrestrial laser scanner of the sequence 2 of ConSLAM, 
and the refined ground truth (GT) poses (in TUM format) of sessions 2, 3, 4, and 5 of the open-access ConSLAM dataset (which provides camera, LiDAR, and IMU measurements).

This dataset enables the evaluation of pose estimation algorithms, such as localization, odometry, SLAM, and Bundle adjustment.
Given the differences between the real-world and the 3D reference BIM model (Scan-Map deviations), the dataset is appropriate for evaluating long-term localization and mapping capabilities, i.e., methods that are robust to changes in the environment. In particular, in a real-world construction environment with normal clutter (not as in the Hilti dataset).",,,,,,
2909,Slashdot,Link Sign Prediction,Link Sign Prediction,"Link Sign Prediction, Multi-Label Text Classification","Image, Text, Time Series",English,Computer Vision,"link-sign-prediction-on-slashdot, multi-label-text-classification-on-slashdot-1",,http://snap.stanford.edu/data/soc-sign-Slashdot090221.html,https://paperswithcode.com/dataset/slashdot,The Slashdot dataset is a relational dataset obtained from Slashdot. Slashdot is a technology-related news website know for its specific user community. The website features user-submitted and editor-evaluated current primarily technology oriented news. In 2002 Slashdot introduced the Slashdot Zoo feature which allows users to tag each other as friends or foes. The network cotains friend/foe links between the users of Slashdot. The network was obtained in February 2009.,2002,,,,,
2910,Sleep-EDF,Automatic Sleep Stage Classification,Automatic Sleep Stage Classification,"Automatic Sleep Stage Classification, Sleep Stage Detection, Multimodal Sleep Stage Detection",Image,,Computer Vision,"sleep-stage-detection-on-sleep-edf, automatic-sleep-stage-classification-on-sleep-1, multimodal-sleep-stage-detection-on-sleep-edf, multimodal-sleep-stage-detection-on-sleep-edf-1, sleep-stage-detection-on-sleep-edfx-single",ODC-By v1.0,https://www.physionet.org/content/sleep-edfx/1.0.0/,https://paperswithcode.com/dataset/sleep-edf,"The sleep-edf database contains 197 whole-night PolySomnoGraphic sleep recordings, containing EEG, EOG, chin EMG, and event markers. Some records also contain respiration and body temperature. Corresponding hypnograms (sleep patterns) were manually scored by well-trained technicians according to the Rechtschaffen and Kales manual, and are also available.",,,,,,
2911,SLNET,Language Modelling,Language Modelling,"Language Modelling, Graph Clustering, Code Completion","Graph, Text",English,Natural Language Processing,,CC BY,https://zenodo.org/record/5259648#.Ykn6VSTMJhE,https://paperswithcode.com/dataset/slnet,SLNET is collection of third party Simulink models. It is curated via mining open source repository (GitHub and Matlab Central) using SLNET-Miner (https://github.com/50417/SLNet_Miner).,,,,,,
2912,SlowFlow,Optical Flow Estimation,Optical Flow Estimation,"Optical Flow Estimation, Reflection Removal, Video Frame Interpolation",Video,,Methodology,,Custom,http://www.cvlibs.net/projects/slow_flow,https://paperswithcode.com/dataset/slowflow,SlowFlow is an optical flow dataset collected by applying Slow Flow technique on data from a high-speed camera and analyzing the performance of the state-of-the-art in optical flow under various levels of motion blur.,,,,,,
2913,SLTrans,Code Translation,Code Translation,"Code Translation, Code Generation",Text,English,Natural Language Processing,,CC-BY-NC-SA,https://huggingface.co/datasets/UKPLab/SLTrans,https://paperswithcode.com/dataset/sltrans,"The dataset consists of source code and LLVM IR pairs generated from accepted and de-duped programming contest solutions. The dataset is divided into language configs and mode splits. The language can be one of C, C++, D, Fortran, Go, Haskell, Nim, Objective-C, Python, Rust and Swift, indicating the source files' languages. The mode split indicates the compilation mode, which can be wither Size_Optimized or Perf_Optimized.",,,,,,
2914,SLURP,Spoken Language Understanding,Spoken Language Understanding,"Spoken Language Understanding, Slot Filling, Intent Classification","Image, Text",English,Computer Vision,"intent-classification-on-slurp, slot-filling-on-slurp",,https://github.com/pswietojanski/slurp,https://paperswithcode.com/dataset/slurp,"A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets.",,,,,,
2915,SMAC-Exp,Starcraft II,Starcraft II,"Starcraft II, Multi-agent Reinforcement Learning, SMAC+",,,Methodology,"smac-on-smac-def-armored-parallel, smac-on-smac-def-infantry-parallel, smac-on-smac-off-superhard-sequential, smac-on-smac-off-complicated-parallel, starcraft-ii-on-smac, smac-on-smac-def-outnumbered-parallel, multi-agent-reinforcement-learning-on-smac, smac-on-smac-off-near-parallel, smac-on-smac-off-superhard-parallel, smac-on-smac-def-outnumbered-sequential, smac-on-smac-off-distant-parallel, smac-on-smac-off-hard-parallel, smac-on-smac-def-armored-sequential",MIT,https://osilab-kaist.github.io/smac_exp/,https://paperswithcode.com/dataset/smac-plus,"The StarCraft Multi-Agent Challenges+ requires agents to learn completion of multi-stage tasks and usage of environmental factors without precise reward functions. The previous challenges (SMAC) recognized as a standard benchmark of Multi-Agent Reinforcement Learning are mainly concerned with ensuring that all agents cooperatively eliminate approaching adversaries only through fine manipulation with obvious reward functions. This challenge, on the other hand, is interested in the exploration capability of MARL algorithms to efficiently learn implicit multi-stage tasks and environmental factors as well as micro-control. This study covers both offensive and defensive scenarios. In the offensive scenarios, agents must learn to first find opponents and then eliminate them. The defensive scenarios require agents to use topographic features. For example, agents need to position themselves behind protective structures to make it harder for enemies to attack.",,,,,,
2916,SMAC,SMAC,SMAC,SMAC,,,Methodology,"smac-on-smac-mmm2-1, smac-on-smac-27m-vs-30m, smac-on-smac-corridor, smac-on-smac-6h-vs-8z-1, smac-on-smac-3s5z-vs-3s6z-1",,https://paperswithcode.com/task/smac,https://paperswithcode.com/dataset/smac,"The StarCraft Multi-Agent Challenge (SMAC) is a benchmark that provides elements of partial observability, challenging dynamics, and high-dimensional observation spaces. SMAC is built using the StarCraft II game engine, creating a testbed for research in cooperative MARL where each game unit is an independent RL agent.",,,,,,
2917,SMACv2,Reinforcement Learning (RL),Reinforcement Learning (RL),Reinforcement Learning (RL),,,Reinforcement Learning,,MIT,https://sites.google.com/view/smacv2,https://paperswithcode.com/dataset/smacv2,SMACv2 (StarCraft Multi-Agent Challenge v2) is a new version of the benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings (from the same distribution) during evaluation.,,SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning,https://arxiv.org/pdf/2212.07489v1.pdf,,,
2918,smallNORB,Image Classification,Image Classification,"Image Classification, Disentanglement",Image,,Computer Vision,image-classification-on-smallnorb,"Custom (research-only, non-commercial, attribution)",https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/,https://paperswithcode.com/dataset/smallnorb,"The smallNORB dataset is a datset for 3D object recognition from shape. It contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees).
The training set is composed of 5 instances of each category (instances 4, 6, 7, 8 and 9), and the test set of the remaining 5 instances (instances 0, 1, 2, 3, and 5).",,,,5 instances,,
2919,SMAP,Time Series Anomaly Detection,Time Series Anomaly Detection,"Time Series Anomaly Detection, Unsupervised Anomaly Detection","Image, Time Series",,Time Series,"unsupervised-anomaly-detection-on-smap, time-series-anomaly-detection-on-smap",,https://github.com/OpsPAI/MTAD,https://paperswithcode.com/dataset/smap,"Soil Moisture Active Passive (SMAP) dataset is a dataset of soil samples and telemetry information using the Mars rover by NASA. Originally published in https://arxiv.org/abs/1802.04431 and used for the unsupervised anomaly detection task in time series data. Later it was used in many popular anomaly detection methods and benchmarks that distribute it in their repositories e.g., https://github.com/OpsPAI/MTAD",,,,,,
2920,SMART-101,Common Sense Reasoning,Common Sense Reasoning,"Common Sense Reasoning, Visual Reasoning, Mathematical Question Answering, Arithmetic Reasoning, Logical Reasoning","Image, Text",English,Reasoning,,CC-BY-SA-4.0,https://smartdataset.github.io/smart101/,https://paperswithcode.com/dataset/smart-101,"Recent times have witnessed an increasing number of applications of deep neural networks towards solving tasks that require superior cognitive abilities, e.g., playing Go, generating art, ChatGPT, etc. Such a dramatic progress raises the question: how generalizable are neural networks in solving problems that demand broad skills? To answer this question, we propose SMART: a Simple Multimodal Algorithmic Reasoning Task (and the associated SMART-101 dataset) for evaluating the abstraction, deduction, and generalization abilities of neural networks in solving visuo-linguistic puzzles designed specifically for children of younger age (6--8). Our dataset consists of 101 unique puzzles; each puzzle comprises a picture and a question, and their solution needs a mix of several elementary skills, including pattern recognition, algebra, and spatial reasoning, among others. To train deep neural networks, we programmatically augment each puzzle to 2,000 new instances; each instance varied in appearance, associated natural language question, and its solution. To foster research and make progress in the quest for artificial general intelligence, we are publicly releasing our SMART-101 dataset, consisting of the full set of programmatically-generated instances of 101 puzzles and their solutions.

The dataset was introduced in our paper Are Deep Neural Networks SMARTer than Second Graders? by Anoop Cherian, Kuan-Chuan Peng, Suhas Lohit, Kevin A. Smith, and Joshua B. Tenenbaum, CVPR 2023",2023,,,,,
2921,SMC,Beat Tracking,Beat Tracking,Beat Tracking,"Image, Video",,Computer Vision,beat-tracking-on-smc,,,https://paperswithcode.com/dataset/smc,"A. Holzapfel, M. E. Davies, J. R. Zapata, J. L. Oliveira, and F. Gouyon, “Selective sampling for beat tracking evaluation,” Transactions on Audio, Speech, and Language Processing, vol. 20, no. 9, pp. 2539–2548, 2012",2012,,,,,
2922,SMD,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Time Series Analysis, Time Series Anomaly Detection","Image, Time Series",,Computer Vision,"anomaly-detection-on-smd, time-series-anomaly-detection-on-smd",,,https://paperswithcode.com/dataset/smd,a dataset of time-series anomaly detection,,,,,,
2923,SMDG,Optic Cup Segmentation,Optic Cup Segmentation,"Optic Cup Segmentation, Image Segmentation, Binary Classification, Optic Disc Segmentation, Retinal Vessel Segmentation",Image,,Computer Vision,,Other,https://www.kaggle.com/datasets/deathtrooper/multichannel-glaucoma-benchmark-dataset,https://paperswithcode.com/dataset/smdg,"Standardized Multi-Channel Dataset for Glaucoma (SMDG-19) is a collection and standardization of 19 public datasets, comprised of full-fundus glaucoma images, associated image metadata like, optic disc segmentation, optic cup segmentation, blood vessel segmentation, and any provided per-instance text metadata like sex and age. This dataset is the largest public repository of fundus images with glaucoma.",,,,,,
2924,SME,FS-MEVQA,FS-MEVQA,"FS-MEVQA, Explanatory Visual Question Answering","Image, Text",English,Computer Vision,fs-mevqa-on-sme,Apache-2.0,https://huggingface.co/datasets/LivXue/SME,https://paperswithcode.com/dataset/sme,"SME is a new dataset for Multi-modal Explanation for Visual Question Answering comprising 1,028,230 samples, with 1,656 visual objects requiring detection in explanations. To our knowledge, this is the first dataset where the explanations are in standard English with additional visual grounding tokens.",,,,230 samples,,
2925,SMHD,Depression Detection,Depression Detection,"Depression Detection, Feature Engineering, Text Classification","Image, Text",English,Computer Vision,,,http://ir.cs.georgetown.edu/data/smhd/,https://paperswithcode.com/dataset/smhd,A novel large dataset of social media posts from users with one or multiple mental health conditions along with matched control users.,,,,,,
2926,SMID,Low-Light Image Enhancement,Low-Light Image Enhancement,Low-Light Image Enhancement,Image,,Computer Vision,low-light-image-enhancement-on-smid,None,https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Seeing_Motion_in_the_Dark_ICCV_2019_paper.pdf,https://paperswithcode.com/dataset/smid,"This is the low-light image enhancement dataset collected by the CVPR 2018 paper ""Seeing Motion in the Dark"".",2018,Homepage,https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Seeing_Motion_in_the_Dark_ICCV_2019_paper.pdf,,,
2927,SMM4H,Language Modelling,Language Modelling,"Language Modelling, Multi-class Classification, Federated Learning","Image, Text",English,Computer Vision,,CC BY 4.0,https://data.mendeley.com/datasets/rxwfb3tysd/2,https://paperswithcode.com/dataset/smm4h,Social Media Mining for Health (SMM4H) Shared Task is a massive data source for biomedical and public health applications.,,,,,,
2928,SMS-WSJ,Audio Source Separation,Audio Source Separation,"Audio Source Separation, Speech Recognition","Audio, Image, Text",English,Audio,,,https://github.com/fgnt/sms_wsj,https://paperswithcode.com/dataset/sms-wsj,"Spatialized Multi-Speaker Wall Street Journal (SMS-WSJ) consists of artificially mixed speech taken from the WSJ database, but unlike earlier databases this one considers all WSJ0+1 utterances and takes care of strictly separating the speaker sets present in the training, validation and test sets.",,,,,,
2929,SMS_Spam_Collection_Data_Set,Ensemble Learning,Ensemble Learning,Ensemble Learning,,,Methodology,ensemble-learning-on-sms-spam-collection-data,,http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection,https://paperswithcode.com/dataset/sms-spam-collection-data-set,"This corpus has been collected from free or free for research sources at the Internet:


A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages.
A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available.
A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis.
the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages.",,,,,,
2930,SNAP,Link Prediction,Link Prediction,"Link Prediction, Graph Clustering, Community Detection","Graph, Image, Time Series",,Computer Vision,,,https://snap.stanford.edu/data/,https://paperswithcode.com/dataset/snap,"SNAP is a collection of large network datasets. It includes graphs representing social networks, citation networks, web graphs, online communities, online reviews and more.

Social networks : online social networks, edges represent interactions between people

Networks with ground-truth communities : ground-truth network communities in social and information networks

Communication networks : email communication networks with edges representing communication

Citation networks : nodes represent papers, edges represent citations

Collaboration networks : nodes represent scientists, edges represent collaborations (co-authoring a paper)

Web graphs : nodes represent webpages and edges are hyperlinks

Amazon networks : nodes represent products and edges link commonly co-purchased products

Internet networks : nodes represent computers and edges communication

Road networks : nodes represent intersections and edges roads connecting the intersections

Autonomous systems : graphs of the internet

Signed networks : networks with positive and negative edges (friend/foe, trust/distrust)

Location-based online social networks : social networks with geographic check-ins

Wikipedia networks, articles, and metadata : talk, editing, voting, and article data from Wikipedia

Temporal networks : networks where edges have timestamps

Twitter and Memetracker : memetracker phrases, links and 467 million Tweets

Online communities : data from online communities such as Reddit and Flickr

Online reviews : data from online review systems such as BeerAdvocate and Amazon

User actions : actions of users on social platforms.

Face-to-face communication networks : networks of face-to-face (non-online) interactions

Graph classification datasets : disjoint graphs from different classes",,,,,,
2931,Snips-SmartLights,Spoken Language Understanding,Spoken Language Understanding,Spoken Language Understanding,Text,English,Natural Language Processing,spoken-language-understanding-on-snips,,https://github.com/sonos/spoken-language-understanding-research-datasets,https://paperswithcode.com/dataset/snips-smartlights,"The SmartLights benchmark from Snipstests the capability of controlling lights in different rooms.
It consists of 1660 requests which are split into five partitions for a 5-fold evaluation.
 A sample command could be: “please change the [bedroom] lights to [red]” or “i’d like the [living room] lights to be at [twelve] percent”",,,,,,
2932,Snips-SmartSpeaker,Spoken Language Understanding,Spoken Language Understanding,Spoken Language Understanding,Text,English,Natural Language Processing,spoken-language-understanding-on-snips-1,,https://github.com/sonos/spoken-language-understanding-research-datasets,https://paperswithcode.com/dataset/snips-smartspeaker,"The SmartSpeaker benchmark tests the performance of reacting to music player commands in English as well as in French.
It has the difficulty of containing many artist or music tracks with uncommon names in the commands, like
“play music by [a boogie wit da hoodie]” or “I’d like to listen to [Kinokoteikoku]”.",,,,,,
2933,SNIPS,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Intent Discovery, Intent Detection, Slot Filling, Out of Distribution (OOD) Detection, Open Intent Discovery",Image,,Computer Vision,"zero-shot-learning-on-snips, intent-discovery-on-snips, intent-detection-on-snips, slot-filling-on-snips, open-intent-discovery-on-snips, out-of-distribution-ood-detection-on-snips",,https://github.com/sonos/nlu-benchmark,https://paperswithcode.com/dataset/snips,"The SNIPS Natural Language Understanding benchmark is a dataset of over 16,000 crowdsourced queries distributed among 7 user intents of various complexity:


SearchCreativeWork (e.g. Find me the I, Robot television show),
GetWeather (e.g. Is it windy in Boston, MA right now?),
BookRestaurant (e.g. I want to book a highly rated restaurant in Paris tomorrow night),
PlayMusic (e.g. Play the last track from Beyoncé off Spotify),
AddToPlaylist (e.g. Add Diamonds to my roadtrip playlist),
RateBook (e.g. Give 6 stars to Of Mice and Men),
SearchScreeningEvent (e.g. Check the showtimes for Wonder Woman in Paris).
The training set contains of 13,084 utterances, the validation set and the test set contain 700 utterances each, with 100 queries per intent.",,,,,,
2934,SNLI-VE,Visual Entailment,Visual Entailment,"Visual Entailment, Visual Reasoning, Visual Question Answering (VQA), Natural Language Inference","Image, Text",English,Computer Vision,"visual-entailment-on-snli-ve-val, visual-entailment-on-snli-ve-test",,https://github.com/necla-ml/SNLI-VE,https://paperswithcode.com/dataset/snli-ve,"Visual Entailment (VE) consists of image-sentence pairs whereby a premise is defined by an image, rather than a natural language sentence as in traditional Textual Entailment tasks. The goal of a trained VE model is to predict whether the image semantically entails the text. SNLI-VE is a dataset for VE which is based on the Stanford Natural Language Inference corpus and Flickr30k dataset.",,,,,,
2935,SNLI,Natural Language Inference,Natural Language Inference,Natural Language Inference,Text,English,Natural Language Processing,natural-language-inference-on-snli,CC BY-SA 4.0,https://nlp.stanford.edu/projects/snli/.,https://paperswithcode.com/dataset/snli,"The SNLI dataset (Stanford Natural Language Inference) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Flickr30k, while hypotheses were generated by crowd-sourced annotators who were shown a premise and asked to generate entailing, contradicting, and neutral sentences. Annotators were instructed to judge the relation between sentences given that they describe the same event. Each pair is labeled as “entailment”, “neutral”, “contradiction” or “-”, where “-” indicates that an agreement could not be reached.",,Breaking NLI Systemswith Sentences that Require Simple Lexical Inferences,https://arxiv.org/abs/1805.02266,,,
2936,Snopes,Fact Verification,Fact Verification,"Fact Verification, Fake News Detection, Stance Detection, Text Matching","Image, Text",English,Computer Vision,stance-detection-on-snopes,,https://github.com/nguyenvo09/EMNLP2020,https://paperswithcode.com/dataset/snopes,Fact-checking (FC) articles which contains pairs (multimodal tweet and a FC-article) from snopes.com.,,,,,,
2937,SOC,Salient Object Detection,Salient Object Detection,"Salient Object Detection, RGB Salient Object Detection",Image,,Computer Vision,salient-object-detection-on-soc,,http://dpfan.net/SOCBenchmark/,https://paperswithcode.com/dataset/soc,"SOC (Salient Objects in Clutter) is a dataset for Salient Object Detection (SOD). It includes images with salient and non-salient objects from daily object categories. Beyond object category annotations, each salient image is accompanied by attributes that reflect common challenges in real-world scenes.",,,,,,
2938,SoccerData,Game of Football,Game of Football,Game of Football,,,Methodology,,,https://github.com/AIS-Bonn/TemporalBallDetection,https://paperswithcode.com/dataset/soccerdata,A dataset of 4562 images of which 4152 images contain a soccer ball.,,,,4562 images,,
2939,SoccerNet-v2,Video Understanding,Video Understanding,"Video Understanding, Action Classification, Boundary Detection, Camera shot boundary detection, Video Object Tracking, Camera shot segmentation, Replay Grounding, Person Re-Identification, Action Spotting","Image, Video",,Computer Vision,"action-spotting-on-soccernet, camera-shot-boundary-detection-on-soccernet, camera-shot-segmentation-on-soccernet-v2, replay-grounding-on-soccernet-v2, action-spotting-on-soccernet-v2, person-re-identification-on-soccernet-v2, video-object-tracking-on-soccernet-v2",,https://soccer-net.org/,https://paperswithcode.com/dataset/soccernet-v2,"A novel large-scale corpus of manual annotations for the SoccerNet video dataset, along with open challenges to encourage more research in soccer understanding and broadcast production.",,,,,,
2940,Social-HM3D,Social Navigation,Social Navigation,Social Navigation,,,Methodology,,CC BY-NC,https://zeying-gong.github.io/projects/falcon/,https://paperswithcode.com/dataset/social-hm3d,"The scene derives from photo-realistic HM3D datasets. Our dataset offers a wide variety of environments especially for Social Navigation tasks, with carefully calibrated human density, incorporating realistic human motions and natural movement patterns. 
These features ensure balanced interaction dynamics across diverse scenes, facilitating the development of more effective social navigation algorithms.",,,,,,
2941,Social-MP3D,Social Navigation,Social Navigation,Social Navigation,,,Methodology,,CC BY-NC,https://zeying-gong.github.io/projects/falcon/,https://paperswithcode.com/dataset/social-mp3d,"The scene derives from photo-realistic MP3D datasets. Our dataset offers a wide variety of environments especially for Social Navigation tasks, with carefully calibrated human density, incorporating realistic human motions and natural movement patterns. 
These features ensure balanced interaction dynamics across diverse scenes, facilitating the development of more effective social navigation algorithms.",,,,,,
2942,SOD,Object Detection,Object Detection,"Object Detection, Salient Object Detection, RGB Salient Object Detection, Self-Driving Cars",Image,,Computer Vision,"salient-object-detection-on-sod-1, salient-object-detection-on-sod",None,https://github.com/LT1st/SmallObstacleDetection/blob/main/README.md,https://paperswithcode.com/dataset/sod,"Aiming
Detect small obstacles, like lost and found.

frames
3000+ picture.

3000+ claimed labelled.

1600 actually labelled.",,,,,,
2943,SODA,Dialogue Generation,Dialogue Generation,Dialogue Generation,Text,English,Natural Language Processing,,CC-BY-4.0,https://github.com/skywalker023/sodaverse,https://paperswithcode.com/dataset/soda,"SODA is a high-quality social dialogue dataset. In contrast to most existing crowdsourced, small-scale dialogue corpora, Soda distills 1.5M socially-grounded dialogues from a pre-trained language model (InstructGPT; Ouyang et al., ). Dialogues are distilled by contextualizing social commonsense knowledge from a knowledge graph (Atomic10x).",,SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization,https://arxiv.org/pdf/2212.10465v1.pdf,,,
2944,SoF,Dimensionality Reduction,Dimensionality Reduction,Dimensionality Reduction,,,Methodology,,,https://sites.google.com/view/sof-dataset,https://paperswithcode.com/dataset/sof,"The Specs on Faces (SoF) dataset, a collection of 42,592 (2,662×16) images for 112 persons (66 males and 46 females) who wear glasses under different illumination conditions. The dataset is FREE for reasonable academic fair use. The dataset presents a new challenge regarding face detection and recognition. It is focused on two challenges: harsh illumination environments and face occlusions, which highly affect face detection, recognition, and classification. The glasses are the common natural occlusion in all images of the dataset. However, there are two more synthetic occlusions (nose and mouth) added to each image. Moreover, three image filters, that may evade face detectors and facial recognition systems, were applied to each image. All generated images are categorized into three levels of difficulty (easy, medium, and hard). That enlarges the number of images to be 42,592 images (26,112 male images and 16,480 female images). There is metadata for each image that contains many information such as: the subject ID, facial landmarks, face and glasses rectangles, gender and age labels, year that the photo was taken, facial emotion, glasses type, and more.",,,,592 images,,
2945,Solar-Power,Time Series Analysis,Time Series Analysis,"Time Series Analysis, Correlated Time Series Forecasting, Univariate Time Series Forecasting",Time Series,,Time Series,"correlated-time-series-forecasting-on-solar, univariate-time-series-forecasting-on-solar",,https://www.nrel.gov/grid/solar-power-data.html,https://paperswithcode.com/dataset/solar-power,"Solar Power Data for Integration Studies
NREL's Solar Power Data for Integration Studies are synthetic solar photovoltaic (PV) power plant data points for the United States representing the year 2006.

The data are intended for use by energy professionals—such as transmission planners, utility planners, project developers, and university researchers—who perform solar integration studies and need to estimate power production from hypothetical solar plants.

Data Methodologies
The Solar Power Data for Integration Studies consist of 1 year (2006) of 5-minute solar power and hourly day-ahead forecasts for approximately 6,000 simulated PV plants. Solar power plant locations were determined based on the capacity expansion plan for high-penetration renewables in Phase 2 of the Western Wind and Solar Integration Study and the Eastern Renewable Generation Integration Study.

NREL generated the 5-minute data set using the Sub-Hour Irradiance Algorithm. The day-ahead solar forecast data for locations in the western United States were generated by 3TIER based on numerical weather predication simulations for Phase 1 of the Western Wind and Solar Integration Study. NREL generated the day-ahead solar forecast data in eastern U.S. locations using the Weather Research and Forecasting model.

The data are for specific years and should not be assumed to be representative of typical radiation levels for a site. These data should not generally be used for site-specific project development work.

Naming Convention
The naming convention of the state-wise solar power data (.csv files) from the Solar Integration Studies is as follows.

Data Type_Latitude_Longitude_Weather Year_PV Type_CapacityMW_Time Interval _Min.csv

Data Type
Actual: Real power output
DA: Day ahead forecast
HA4: 4 hour ahead forecast
Weather Year: The PV data is based on the particular year's known weather condition.
PV Type
UPV: Utility scale PV
DPV: Distributed PV

Note: The practical difference between UPV and DPV is in the configurations (UPV has single axis tracking while DPV is fixed tilt equaling to latitude) and the smoothing (both are run through a low-pass filter, the DPV will have more of the high frequency variability smoothed out).

Capacity: Installed capacity in MW
Time Interval: PV generation data reading interval in minutes.
Contact
Yingchen Zhang
Manager, Sensing, Measurement, and Forecasting Group
Yingchen.Zhang@nrel.gov
303-384-7090",2006,,,,,
2946,SoMeSci,Entity Linking,Entity Linking,"Entity Linking, Relation Extraction, Named Entity Recognition (NER), Entity Disambiguation","Graph, Image, Text",English,Computer Vision,,CC BY,https://data.gesis.org/somesci/,https://paperswithcode.com/dataset/somesci,"Knowledge about software used in scientific investigations is important for several reasons, for instance, to enable an understanding of provenance and methods involved in data handling. However, software is usually not formally cited, but rather mentioned informally within the scholarly description of the investigation, raising the need for automatic information extraction and disambiguation. Given the lack of reliable ground truth data, we present SoMeSci - Software Mentions in Science - a gold standard knowledge graph of software mentions in scientific articles. It contains high quality annotations (IRR: κ = .82) of 3756 software mentions in 1367 PubMed Central articles. Besides the plain mention of the software, we also provide relation labels for additional information, such as the version, the developer, a URL or citations. Moreover, we distinguish between different types, such as application, plugin or programming environment, as well as different types of mentions, such as usage or creation. To the best of our knowledge, SoMeSci is the most comprehensive corpus about software mentions in scientific articles, providing training samples for Named Entity Recognition, Relation Extraction, Entity Disambiguation, and Entity Linking. Finally, we sketch potential use cases and provide baseline results for the different tasks.",,,,,,
2947,Something-Something-100,Few Shot Action Recognition,Few Shot Action Recognition,Few Shot Action Recognition,"Image, Video",,Computer Vision,few-shot-action-recognition-on-something,,https://github.com/ffmpbgrnn/CMN/tree/master/smsm-100,https://paperswithcode.com/dataset/something-something-100,"Something-Something-100 is a dataset split created from Something-Something V2. A total of 100 classes are selected and each comprises 100 samples. The 100 classes were split into 64, 12, and 24 non-overlapping classes to use as the meta-training set, meta-validation set, and meta-testing set, respectively. Link to exactly selected samples can be found here: https://github.com/ffmpbgrnn/CMN/tree/master/smsm-100",,,,100 samples,split created from Something-Something V2. A total of 100 classes are selected and each comprises 100 samples,100
2948,Something-Something_V2,Video Prediction,Video Prediction,"Video Prediction, Action Classification, Action Recognition In Videos, Early Action Prediction, General Action Video Anomaly Detection, Text-to-Video Generation, Action Recognition, Video Classification","Image, Text, Time Series, Video",English,Computer Vision,"text-to-video-generation-on-something, video-classification-on-something-something-1, action-recognition-in-videos-on-something, action-classification-on-something-something-2, video-prediction-on-something-something-v2, general-action-video-anomaly-detection-on, action-recognition-in-videos-on-something-3, early-action-prediction-on-something-1",Custom,https://developer.qualcomm.com/software/ai-datasets/something-something,https://paperswithcode.com/dataset/something-something-v2,"The 20BN-SOMETHING-SOMETHING V2 dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 220,847 videos, with 168,913 in the training set, 24,777 in the validation set and 27,157 in the test set. There are 174 labels.

Source

Image Source",,,,,,174
2949,SOMOS,Text-To-Speech Synthesis,Text-To-Speech Synthesis,"Text-To-Speech Synthesis, Speech Synthesis","Audio, Text",English,Speech,,,https://innoetics.github.io/publications/somos-dataset/index.html,https://paperswithcode.com/dataset/somos,"The SOMOS dataset is a large-scale mean opinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS) samples. It can be employed to train automatic MOS prediction systems focused on the assessment of modern synthesizers, and can stimulate advancements in acoustic model evaluation. It consists of 20K synthetic utterances of the LJ Speech voice, a public domain speech dataset which is a common benchmark for building neural acoustic models and vocoders. Utterances are generated from 200 TTS systems including vanilla neural acoustic models as well as models which allow prosodic variations.",,,,,,
2950,Song_Describer_Dataset,Music Generation,Music Generation,"Music Generation, Text-to-Music Generation, Cross-Modal Retrieval, Music Captioning","Audio, Image, Text",English,Computer Vision,music-generation-on-song-describer-dataset,CC BY-SA 4.0,https://github.com/mulab-mir/song-describer-dataset,https://paperswithcode.com/dataset/song-describer-dataset,"The Song Describer Dataset (SDD) contains ~1.1k captions for 706 permissively licensed music recordings. It is designed for use in evaluation of models that address music-and-language (M&L) tasks such as music captioning, text-to-music generation and music-language retrieval.",,,,,,
2951,SONICS,Audio Deepfake Detection,Audio Deepfake Detection,"Audio Deepfake Detection, Fake Song Detection, Synthetic Speech Detection, DeepFake Detection, Synthetic Song Detection","Audio, Image",,Audio,,CC BY-NC 4.0,https://github.com/awsaf49/sonics,https://paperswithcode.com/dataset/sonics,"SONICS is a large-scale dataset comprising 97,164 songs — 48,090 real songs from YouTube and 49,074 fake songs from Suno & Udio — designed for synthetic song detection (SSD), also known as fake song detection (FSD). It addresses several limitations of existing datasets, such as the lack of end-to-end fake songs, limited diversity in music-lyrics, and insufficient long-duration songs. The average length of the songs in SONICS is 176 seconds, which enables the capture of long-context relationships. Moreover, SONICS provides open access to generated fake songs and is divided into 66,709 songs for training, 26,015 songs for testing, and 4,440 songs for validation. Additionally, the inclusion of song lyrics in SONICS dataset paves the way for future research in this field.",,,,,,
2952,SONYC-UST-V2,Environmental Sound Classification,Environmental Sound Classification,"Environmental Sound Classification, Audio Classification, Audio Tagging","Audio, Image",,Computer Vision,,CC BY 4.0,https://zenodo.org/record/3966543,https://paperswithcode.com/dataset/sonyc-ust-v2,"A dataset for urban sound tagging with spatiotemporal information. This dataset is aimed for the development and evaluation of machine listening systems for real-world urban noise monitoring. While datasets of urban recordings are available, this dataset provides the opportunity to investigate how spatiotemporal metadata can aid in the prediction of urban sound tags. SONYC-UST-V2 consists of 18510 audio recordings from the ""Sounds of New York City"" (SONYC) acoustic sensor network, including the timestamp of audio acquisition and location of the sensor.",,,,,,
2953,Sound-based_drone_fault_classification_using_multi,Audio Classification,Audio Classification,"Audio Classification, Classification, Fault Detection","Audio, Image",,Audio,classification-on-sound-based-drone-fault,Creative Commons Attribution 4.0 International,https://zenodo.org/record/7779574#.ZEVncnZBwQ-,https://paperswithcode.com/dataset/k-drone,"arxiv : https://arxiv.org/abs/2304.11708

Accepted at 29th International Congress on Sound and Vibration (ICSV29). 

The drone has been used for various purposes including military applications, aerial photography, and pesticide spraying. However, the drone is vulnerable to external disturbances, and malfunction in propellers and motors can easily occur. To improve the safety of drone operations, early detection of mechanical faults should be made in real-time. In this paper, we propose a sound-based deep neural network (DNN) fault classifier and drone sound dataset. The dataset was constructed by collecting the operating sounds of drones from microphones mounted on three different drones in an anechoic chamber. The dataset includes various operating conditions of drones, such as flight directions (front, back, right, left, clockwise, counter clockwise) and faults on propellers and motors. The drone sounds were then mixed with noises recorded in five different spots on the university campus, with a signal-to-noise ratio (SNR) varying from 10 dB to 15 dB. Using the acquired dataset, we train a DNN classifier, 1DCNN-ResNet, that classifies the types of mechanical faults and their locations from short-time input waveforms. We employ multitask learning (MTL) and incorporate the direction classification task as an auxiliary task to make the classifier learn more general audio features. The test over unseen data reveals that the proposed multitask model can successfully classify faults in drones and outperforms single-task models even with less training data. 

please reorganize the file directory like below

drone

ㄴA

ㄴB

ㄴC

For each drone type A, B, and C have 540002 files. (Here, 2 means stereo channel, you can find mic1 and mic2 in subdirectory) They are divided into train, valid, and test by a 6:2:2 ratio. For each file, recording information is labeled below.

{model_type}{maneuvering_direction}{fault}{drone_file_index}{background}{background_file_index}{SNR}

model_type: A, B, C

maneuvering_direction: F(Front), B(Back), R(Right), L(Left), C(Clockwise), CC(Counter-clockwise)

fault: N (Normal), MF1~4 (Moter Failure), PC1~4 (Propeller Cut) -> 1~4 means each motor/propeller of the quadcopter.

Dataset available under below ""Homepage"" ↓",,,,,,
2954,Soundscape_Attributes_Translation_Project__SATP__D,Word Translation,Word Translation,"Word Translation, Audio Emotion Recognition","Audio, Image, Text",English,Computer Vision,,CC BY 4.0,https://doi.org/10.5281/zenodo.6914433,https://paperswithcode.com/dataset/soundscape-attributes-translation-project,"The data and audio included here were collected for the Soundscape Attributes Translation Project (SATP). First introduced in Aletta et. al. (2020), the SATP is an attempt to provide validated translations of soundscape attributes in languages other than English. The recordings were used for headphones - based listening experiments.

The data are provided to accompany publications resulting from this project and to provide a unique dataset of 1000s of perceptual responses to a standardised set of urban soundscape recordings. This dataset is the result of efforts from hundreds of researchers, students, assistants, PIs, and participants from institutions around the world. We have made an attempt to list every contributor to this Zenodo repo; if you feel you should be included, please get in touch.",2020,,,,,
2955,Sound_of_Water_50,Physical Attribute Prediction,Physical Attribute Prediction,"Physical Attribute Prediction, Audio Tagging","Audio, Time Series",,Audio,physical-attribute-prediction-on-sound-of,MIT,https://bpiyush.github.io/pouring-water-website/,https://paperswithcode.com/dataset/sound-of-water-50,"We collect a dataset of 805 clean videos that show the action of pouring water in a container. Our dataset spans over 50 unique containers made of 5 different materials, 4 different shapes and with hot and cold water. 

The dataset can be used for the following tasks:
- Physical property understanding (e.g., predict size of container just from the sound of pouring)
- Audio generation (given video of pouring, generate a synchronised sound of pouring)
- Video generation",,,,,,
2956,Southern_California_Seismic_Network_Data,Seismic Detection,Seismic Detection,Seismic Detection,Image,,Computer Vision,seismic-detection-on-southern-california,,https://scedc.caltech.edu/data/deeplearning.html#phase_detection,https://paperswithcode.com/dataset/southern-california-seismic-network-data,"These files are supplementary material for “Generalized Seismic Phase Detection with Deep Learning” by Ross et al. (2018), BSSA (doi.org/10.1785/0120180080). The models were trained using keras and TensorFlow, and can be used with these libraries. The training dataset contains 4.5 million seismograms evenly split between P-waves, S-waves, and pre-event noise classes. We encourage the use of this hdf5 dataset for training deep learning models, and hope that it and the model architecture in the paper can serve as a benchmark for future studies. For additional information please contact Zachary Ross (zross@caltech.edu).",2018,,,,,
2957,SpaceNet_7,2D Semantic Segmentation,2D Semantic Segmentation,2D Semantic Segmentation,Image,,Computer Vision,,Creative Commons Attribution-ShareAlike 4.0 International License,https://spacenet.ai/sn7-challenge/,https://paperswithcode.com/dataset/spacenet-7,"Satellite imagery analytics have numerous human development and disaster response applications, particularly when time series methods are involved. For example, quantifying population statistics is fundamental to 67 of the 232 United Nations Sustainable Development Goals, but the World Bank estimates that more than 100 countries currently lack effective Civil Registration systems. The SpaceNet 7 Multi-Temporal Urban Development Challenge aims to help address this deficit and develop novel computer vision methods for non-video time series data. In this challenge, participants will identify and track buildings in satellite imagery time series collected over rapidly urbanizing areas. The competition centers around a new open source dataset of Planet satellite imagery mosaics, which includes 24 images (one per month) covering ~100 unique geographies. The dataset will comprise over 40,000 square kilometers of imagery and exhaustive polygon labels of building footprints in the imagery, totaling over 10 million individual annotations. Challenge participants will be asked to track building construction over time, thereby directly assessing urbanization.",,,,24 images,,
2958,SpaceSGG,Scene Graph Generation,Scene Graph Generation,Scene Graph Generation,"Graph, Text",English,Natural Language Processing,,Apache-2.0,https://huggingface.co/datasets/Endlinc/SpaceSGG,https://paperswithcode.com/dataset/spacesgg,"Scene Graph Generation (SGG) converts visual scenes into structured graph representations, providing deeper scene understanding for complex vision tasks. However, existing SGG models often overlook essential spatial relationships and struggle with generalization in open-vocabulary contexts. To address these limitations, we propose LLaVA-SpaceSGG, a multimodal large language model (MLLM) designed for open-vocabulary SGG with enhanced spatial relation modeling. To train it, we collect the SGG instruction-tuning dataset, named SpaceSGG. This dataset is constructed by combining publicly available datasets and synthesizing data using open-source models within our data construction pipeline. It combines object locations, object relations, and depth information, resulting in three data formats: spatial SGG description, question-answering, and conversation. To enhance the transfer of MLLMs' inherent capabilities to the SGG task, we introduce a two-stage training paradigm. Experiments show that LLaVA-SpaceSGG outperforms other open-vocabulary SGG methods, boosting recall by 8.6% and mean recall by 28.4% compared to the baseline. Our codebase, dataset, and trained models are publicly accessible on GitHub at the following URL: https://github.com/Endlinc/LLaVA-SpaceSGG.",,,,,,
2959,SpaGBOL,Visual Place Recognition,Visual Place Recognition,"Visual Place Recognition, Camera Localization, geo-localization, Cross-View Geo-Localisation, Visual Localization, Outdoor Localization, Image-Based Localization, Image Retrieval",Image,,Computer Vision,cross-view-geo-localisation-on-spagbol,,https://github.com/tavisshore/SpaGBOL,https://paperswithcode.com/dataset/spagbol,"Cross-View Geo-Localisation within urban regions is challenging in part due to the lack of geo-spatial structuring within current datasets and techniques. We propose utilising graph representations to model sequences of local observations and the connectivity of the target location. Modelling as a graph enables generating previously unseen sequences by sampling with new parameter configurations. 
SpaGBOL contains 98,855 panoramic streetview images across different seasons, and 19,771 corresponding satellite images from 10 mostly densely populated international cities. This translates to 5 panoramic images and one satellite image per graph node. Downloading instructions below.",,,,,,
2960,SPair-71k,Object Detection,Object Detection,"Object Detection, Colorization, Graph Matching, Semantic correspondence","Graph, Image",,Computer Vision,"graph-matching-on-spair-71k, semantic-correspondence-on-spair-71k",,http://cvlab.postech.ac.kr/research/SPair-71k/,https://paperswithcode.com/dataset/spair-71k,"SPair-71k contains 70,958 image pairs with diverse variations in viewpoint and scale. Compared to previous datasets, it is significantly larger in number and contains more accurate and richer annotations.",,,,,,
2961,SpanEX,Explanation Generation,Explanation Generation,"Explanation Generation, Explainable Artificial Intelligence (XAI)",Text,English,Natural Language Processing,,MIT,https://huggingface.co/datasets/copenlu/spanex,https://paperswithcode.com/dataset/spanex,"Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). We introduce SpanEx, a multi-annotator dataset of human-annotated span interaction explanations for two NLU tasks: NLI and FC.

SpanEx 
- consists of 7071 instances annotated for span interactions. 
- the first dataset with human phrase-level interaction explanations with explicit labels for interaction types.
- annotated by three annotators, which opens new avenues for studies of human explanation agreement -- an understudied area in the explainability literature.",,,,7071 instances,,
2962,Spanish_TimeBank_1.0,Joint Event and Temporal Relation Extraction,Joint Event and Temporal Relation Extraction,"Joint Event and Temporal Relation Extraction, Temporal Tagging","Graph, Time Series, Video",,Methodology,temporal-tagging-on-spanish-timebank-1-0,LDC User Agreement for Non-Members,https://catalog.ldc.upenn.edu/LDC2012T12,https://paperswithcode.com/dataset/spanish-timebank-1-0,"Spanish TimeBank 1.0 was developed by researchers at Barcelona Media and consists of Spanish texts in the AnCora corpus annotated with temporal and event information according to the TimeML specification language.

TimeML is a schema for annotating eventualities and time expressions in natural language as well as the temporal relations among them, thus facilitating the task of extraction, representation and exchange of temporal information. Spanish Timebank 1.0 is annotated in three levels, marking events, time expressions and event metadata. The TimeML annotation scheme was tailored for the specifics of the Spanish language. Temporal relations in Spanish present distinctions of verbal mood (e.g., indicative, subjunctive, conditional, etc.) and grammatical aspect (e.g., imperfective) which are absent in English. Spanish TimeBank 1.0 joins the family of TimeBank annotated corpora which includes languages such as English, Italian, French, Korean and Chinese. Through their common layer of annotation, these corpora provide resources useful for multilingual temporal extraction and processing, such as multilingual text entailment, opinion mining or question answering. Spanish Timebank 1.0 is the Spanish language complement to Catalan Timebank 1.0 LDC2012T10.",,LDC User Agreement for Non-Members,https://catalog.ldc.upenn.edu/license/ldc-non-members-agreement.pdf,,,
2963,SParC,Text-To-SQL,Text-To-SQL,"Text-To-SQL, Semantic Parsing, Data Augmentation",Text,English,Natural Language Processing,"text-to-sql-on-sparc, semantic-parsing-on-sparc",,https://github.com/taoyds/sparc,https://paperswithcode.com/dataset/sparc,"SParC is a large-scale dataset for complex, cross-domain, and context-dependent (multi-turn) semantic parsing and text-to-SQL task (interactive natural language interfaces for relational databases).",,https://arxiv.org/pdf/1906.02285.pdf,https://arxiv.org/pdf/1906.02285.pdf,,,
2964,SPaRCNet,Electroencephalogram (EEG),Electroencephalogram (EEG),"Electroencephalogram (EEG), Seizure Detection, EEG",Image,,Computer Vision,,BDSP Restricted Health Data License 1.0.0,https://bdsp.io/content/bdsp-sparcnet/1.1/,https://paperswithcode.com/dataset/sparcnet,"Seizures and seizure-like rhythmic and periodic brain activity known as “ictal-interictal-injury continuum” (IIIC) patterns are frequently detected during brain monitoring with electroencephalography (EEG) in patients with epilepsy or critical illness. Prior efforts to automate detection of IIIC patterns have been limited by lack of large well-annotated datasets to train/evaluate algorithms, and there have been only a few attempts to detect IIIC events other than seizures. The IIIC dataset includes  50,697 labeled EEG samples from 2,711 patients’ and 6,095 EEGs that were annotated by physician experts from 18 institutions. These samples were used to train SPaRCNet (Seizures, Periodic and Rhythmic Continuum patterns Deep Neural Network), a computer program that classifies IIIC events with an accuracy matching clinical experts. 

Prior efforts to automate seizure detection have been limited by lack of large well-annotated datasets to train and evaluate algorithms, and there have been only a few attempts to detect IIIC events other than seizures. To address this gap, we created a set of 50,697 IIIC and non-IIIC events from 2,711 patients’ (6,095 EEGs) and obtained independent annotations from 124 raters",,,,,"train/evaluate algorithms, and there have been only a few attempts to detect IIIC events other than seizures. The IIIC dataset includes  50,697 labeled EEG samples",
2965,Sparrow,Robot Navigation,Robot Navigation,Robot Navigation,,,Methodology,,MIT,https://github.com/XinJingHao/Sparrow-V0,https://paperswithcode.com/dataset/sparrow,"Sparrow-V0: A Reinforcement Learning Friendly Simulator for Mobile Robot

Features:
Vectorizable (Enable fast data collection; Single environment is also supported)
Domain Randomization (control interval, control delay, maximum velocity, inertia, friction, the magnitude of sensor noise and maps can be randomized while training)
Lightweight (Consume only 150~200 mb RAM or GPU memories per environment)
Standard Gym API with both Pytorch/Numpy data flow
GPU/CPU are both acceptable (If you use Pytorch to build your RL model, you can run your RL model and Sparrow both on GPU. Then you don't need to transfer the transitions from CPU to GPU anymore.)
Easy to use (30kb pure Python files. Just import, never worry about installation)
Ubuntu/Windows are both supported
Accept image as map (Customize your own environments easily and rapidly)
Detailed comments on source code.",,,,,,
2966,SPARTQA_-,Multi-hop Question Answering,Multi-hop Question Answering,"Multi-hop Question Answering, Relational Reasoning, Question Answering",Text,English,Natural Language Processing,,,https://github.com/HLR/SpartQA_generation,https://paperswithcode.com/dataset/spartqa-1,"We take advantage of the ground truth of NLVR images, design CFGs to generate stories, and use spatial reasoning rules to ask and answer spatial reasoning questions. This automatically generated data is called SpaRTQA.   https://aclanthology.org/2021.naacl-main.364/",2021,,,,,
2967,SpaRTUN,Transfer Learning,Transfer Learning,"Transfer Learning, Question Answering",Text,English,Natural Language Processing,,,https://github.com/HLR/SpaRTUN,https://paperswithcode.com/dataset/spartun,"SpaRTUN a dataset synthesized for transfer learning on spatial question answering (SQA) and spatial role labeling (SpRL).

Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is  less explored for spatial language. We provide two new data resources on multiple spatial language processing tasks. The first dataset is synthesized for transfer learning on spatial question answering (SQA) and spatial role labeling (SpRL). Compared to previous SQA datasets, we include a larger variety of spatial relation types and spatial expressions. Our data generation process is easily extendable with new spatial expression lexicons. The second one is a real-world SQA dataset with human-generated questions built on an existing corpus with SPRL annotations. This dataset can be used to evaluate spatial language processing models in realistic situations. We show pretraining with automatically generated data significantly improves the SOTA results on several SQA and SPRL benchmarks, particularly when the training data in the target domain is small.",,,,,,
2968,SPAVE-28G,Intelligent Communication,Intelligent Communication,Intelligent Communication,,,Methodology,,Creative Commons Attribution 4.0 International,https://doi.org/10.5281/zenodo.7178597,https://paperswithcode.com/dataset/spave-28g,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
2969,Spectre-v1,Code Classification,Code Classification,"Code Classification, Contextual Embedding for Source Code, Code Generation, Code Completion","Image, Text",English,Computer Vision,,,https://github.com/vernamlab/FastSpec,https://paperswithcode.com/dataset/spectre-v1,"Description

A dataset of assembly functions that are vulnerable to Spectre-V1 attack.


Motivation

Several techniques have been proposed to detect vulnerable Spectre gadgets in widely deployed commercial software. Unfortunately, detection techniques proposed so far rely on hand-written rules which fall short in covering subtle variations of known Spectre gadgets as well as demand a huge amount of time to analyze each conditional branch in software. Moreover, detection tool evaluations are based only on a handful of these gadgets, as it requires arduous effort to craft new gadgets manually.


Potential Use Cases

Generating assembly code.

Evaluating new detection tools.",,,,,,
2970,SpeechInstruct,Text-To-Speech Synthesis,Text-To-Speech Synthesis,"Text-To-Speech Synthesis, Automatic Speech Recognition (ASR)","Audio, Image, Text",English,Speech,,,https://github.com/0nutation/SpeechGPT,https://paperswithcode.com/dataset/speechinstruct,"SpeechInstruct is a large-scale cross-modal speech instruction dataset. It contains 37,969 quadruplets composed of speech instructions, text instructions, text responses, and speech responses.",,SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities,https://arxiv.org/pdf/2305.11000v1.pdf,,,
2971,SpeechMatrix,Translation,Translation,"Translation, Speech-to-Speech Translation","Audio, Text",English,Natural Language Processing,,CC-BY-NC 4.0,https://github.com/facebookresearch/fairseq/tree/ust/examples/speech_matrix,https://paperswithcode.com/dataset/speechmatrix,SpeechMatrix is a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech.,,,,,,
2972,speechocean762,Word-level pronunciation scoring,Word-level pronunciation scoring,"Word-level pronunciation scoring, Phone-level pronunciation scoring, Utterance-level pronounciation scoring",,,Methodology,"word-level-pronunciation-scoring-on, utterance-level-pronounciation-scoring-on, phone-level-pronunciation-scoring-on",Attribution 4.0 International (CC BY 4.0),https://www.openslr.org/101,https://paperswithcode.com/dataset/speechocean762,"speechocean762 is an open-source speech corpus designed for pronunciation assessment use, consisting of 5000 English utterances from 250 non-native speakers, where half of the speakers are children. Five experts annotated each of the utterances at sentence-level, word-level and phoneme-level. This corpus is allowed to be used freely for commercial and non-commercial purposes. To avoid subjective bias, each expert scores independently under the same metric",,,,,,
2973,Speech_Brown,Semantic Similarity,Semantic Similarity,"Semantic Similarity, Semantic Retrieval, Automatic Speech Recognition (ASR)","Audio, Image, Text",English,Computer Vision,,MIT,https://huggingface.co/datasets/llm-lab/SpeechBrown,https://paperswithcode.com/dataset/speechbrown,"Dataset Summary
Speech Brown is a comprehensive, synthetic, and diverse paired speech-text dataset in 15 categories, covering a wide range of topics from fiction to religion. This dataset consists of over 55,000 sentence-level samples.  

To train the CLASP model, we created this dataset based on the Brown Corpus. The synthetic speech was generated using the NVIDIA Tacotron 2 text-to-speech model.  

For more information about our proposed model, please refer to this paper. The dataset generation pipeline, along with code and usage instructions, is available on this GitHub page.  

Dataset Statistics

Total size: Approximately 30 GB.  
Number of samples: 55,173 pairs of speech and text.  
Average tokens per sample: 19.00.  
Maximum tokens in a sample: 48.  
Average characters per sample: 96.72.
Number of unique tokens: 50,667
Categories: 15 categories consist of adventure, belles_lettres, editorial, fiction, government, hobbies, humor, learned, lore, mystery, news, religion, reviews, romance, science_fiction.  

Dataset Structure
To ensure ease of use, the dataset is partitioned into 10 parts. Each part can be used independently if it meets the requirements of your task and model.  

Metadata Files

global_metadata: A JSON file containing metadata for all 55,173 samples.  
localized_metadata: A JSON file containing metadata for all samples, categorized into the 10 dataset partitions.  

Metadata Fields

id: The unique identifier for the sample.  
audio_file_path: The file path for the audio in the dataset.  
category: The category of the sample's text.  
text: The corresponding text of the audio file.

Usage Instructions
To use this dataset, download the parts and metadata files as follows:

Option 1: Manual Download
Visit the dataset repository and download all dataset_partX.zip files and the global_metadata.json file.

Option 2: Programmatic Download
Use the huggingface_hub library to download the files programmatically:

```python
from huggingface_hub import hf_hub_download
from zipfile import ZipFile
import os
import json

Download dataset parts
zip_file_path1 = hf_hub_download(repo_id=""llm-lab/SpeechBrown"", filename=""dataset_part1.zip"", repo_type=""dataset"")
zip_file_path2 = hf_hub_download(repo_id=""llm-lab/SpeechBrown"", filename=""dataset_part2.zip"", repo_type=""dataset"")

Download other parts...
Download metadata
metadata_file_path = hf_hub_download(repo_id=""llm-lab/SpeechBrown"", filename=""global_metadata.json"", repo_type=""dataset"")

for i in range(1, 11):
    with ZipFile(f'dataset_part{i}.zip', 'r') as zip_ref:
        zip_ref.extractall(f'dataset_part{i}')
    os.remove(f'dataset_part{i}.zip')

with open('global_metadata.json', 'r') as f:
    metadata = json.load(f)
metadata.keys()
```

Citations
If you find our paper, code, data, or models useful, please cite the paper:
@misc{abootorabi2024claspcontrastivelanguagespeechpretraining,
      title={CLASP: Contrastive Language-Speech Pretraining for Multilingual Multimodal Information Retrieval}, 
      author={Mohammad Mahdi Abootorabi and Ehsaneddin Asgari},
      year={2024},
      eprint={2412.13071},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.13071}, 
}

Contact
If you have questions, please email mahdi.abootorabi2@gmail.com or asgari@berkeley.edu.",2024,Paper,https://arxiv.org/abs/2412.13071,173 samples,,15
2974,Speech_Commands,Audio Classification,Audio Classification,"Audio Classification, Keyword Spotting, Speech Recognition, Federated Learning, Time Series Analysis","Audio, Image, Text, Time Series",English,Audio,"speech-recognition-on-speech-commands-2, audio-classification-on-speech-commands-1, time-series-on-speech-commands, keyword-spotting-on-google-speech-commands",CC BY,https://arxiv.org/abs/1804.03209,https://paperswithcode.com/dataset/speech-commands/,Speech Commands is an audio dataset of spoken words designed to help train and evaluate keyword spotting systems .,,Homepage,https://arxiv.org/abs/1804.03209,,,
2975,Speech_Robust_Bench,Adversarial Robustness,Adversarial Robustness,"Adversarial Robustness, Robust Speech Recognition","Audio, Image, Text",English,Adversarial,,,https://huggingface.co/datasets/mshah1/speech_robust_bench,https://paperswithcode.com/dataset/speech-robust-bench,"Click to add a brief description of the dataset (Markdown and LaTeX enabled).

Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset",,,,,,
2976,Spider-Realistic,Text-To-SQL,Text-To-SQL,"Text-To-SQL, Semantic Parsing",Text,English,Natural Language Processing,"text-to-sql-on-spider, semantic-parsing-on-spider",,https://zenodo.org/record/5205322#.YTts_o5Kgab,https://paperswithcode.com/dataset/spider-realistic,"Spider dataset is used for evaluation in the paper ""Structure-Grounded Pretraining for Text-to-SQL"". The dataset is created based on the dev split of the Spider dataset (2020-06-07 version from https://yale-lily.github.io/spider). We manually modified the original questions to remove the explicit mention of column names while keeping the SQL queries unchanged to better evaluate the model's capability in aligning the NL utterance and the DB schema. For more details, please check our paper at https://arxiv.org/abs/2010.12773.",2020,,,,,
2977,Spike-X4K,Image Reconstruction,Image Reconstruction,"Image Reconstruction, Event-based vision","3D, Image",,Computer Vision,image-reconstruction-on-spike-x4k,,https://github.com/bupt-ai-cz/SwinSF,https://paperswithcode.com/dataset/spike-x4k,"Overview
The Spike-X4K Dataset is a high-resolution image reconstruction resource tailored for the latest advancements in spike camera technology. It is designed to meet the demands of modern spike cameras with a resolution of 1000×1000 pixels, surpassing the capabilities of previous datasets like spike-REDS, which was limited to a resolution of 250×400 pixels.

Dataset Characteristics

Resolution: 1000×1000 pixels, aligning with state-of-the-art spike camera imaging standards.
Temporal Depth: The dataset captures the temporal dynamics of scenes with high-speed motion, providing a temporal sequence of spike frames.
Content: It includes both synthetic and real-world datasets, offering a diverse range of high-speed motion scenarios for training and testing image reconstruction models.
Pairs: The dataset comprises 1200 spike stream-ground truth image pairs for training and 45 pairs for testing, ensuring robust model evaluation.

Motivation and Content Summary
The development of the Spike-X4K Dataset was motivated by the need for a more representative and higher resolution dataset that could effectively train and evaluate image reconstruction models for spike cameras. Spike cameras, with their unique ability to capture photons independently at each pixel and generate binary spike streams, offer high temporal resolution and low latency, which are crucial for high-speed imaging. However, converting these spike streams into high-quality images requires sophisticated algorithms, and the Spike-X4K Dataset provides the necessary data to develop and refine these algorithms.

Potential Use Cases

Algorithm Development: For researchers and engineers working on spike image reconstruction algorithms specific to spike camera data.
Benchmarking: As a standard for benchmarking the performance of various image reconstruction models against state-of-the-art techniques.
Training and Testing: Providing a large and diverse set of data for training deep learning models to handle high-speed motion imaging.
Feature Extraction: Enabling the study and improvement of feature extraction techniques from spike streams, including spatial and temporal features.
Spike Image Reconstruction Tasks: Supporting a wide range of computer vision tasks that can benefit from high-resolution, high-speed imaging, such as motion analysis, object tracking, and event detection.",,,,,"train and evaluate image reconstruction models for spike cameras. Spike cameras, with their unique ability to capture photons independently at each pixel and generate binary spike streams, offer high temporal resolution and low latency, which are crucial for high-speed imaging. However, converting these spike streams into high-quality images",
2978,SPLASH,Text-To-SQL,Text-To-SQL,"Text-To-SQL, Semantic Parsing",Text,English,Natural Language Processing,,,https://github.com/MSR-LIT/Splash,https://paperswithcode.com/dataset/splash,"A dataset of utterances, incorrect SQL interpretations and the corresponding natural language feedback.",,,,,,
2979,SPoC,Program Synthesis,Program Synthesis,Program Synthesis,,,Methodology,"program-synthesis-on-spoc-testp, program-synthesis-on-spoc-testw",,https://sumith1896.github.io/spoc/,https://paperswithcode.com/dataset/spoc,"Pseudocode-to-Code (SPoC) is a program synthesis dataset, containing 18,356 programs with human-authored pseudocode and test cases.",,,,,,
2980,Spoken-SQuAD,Spoken Language Understanding,Spoken Language Understanding,"Spoken Language Understanding, Question Answering, Reading Comprehension, Speech Recognition","Audio, Image, Text",English,Computer Vision,spoken-language-understanding-on-spoken-squad,,https://github.com/chiahsuan156/Spoken-SQuAD,https://paperswithcode.com/dataset/spoken-squad,"In SpokenSQuAD, the document is in spoken form, the input question is in the form of text and the answer to each question is always a span in the document. The following procedures were used to generate spoken documents from the original SQuAD dataset. First, the Google text-to-speech system was used to generate the spoken version of the articles in SQuAD. Then CMU Sphinx was sued to generate the corresponding ASR transcriptions. The SQuAD training set was used to generate the training set of Spoken SQuAD, and SQuAD development set was used to generate the testing set for Spoken SQuAD. If the answer of a question did not exist in the ASR transcriptions of the associated article, the question-answer pair was removed from the dataset because these examples are too difficult for listening comprehension machine at this stage.",,,,,"training set was used to generate the training set of Spoken SQuAD, and SQuAD development set was used to generate the testing set for Spoken SQuAD. If the answer of a question did not exist in the ASR transcriptions of the associated article, the question-answer pair was removed from the dataset because these examples",
2981,SpokenSTS,Semantic Similarity,Semantic Similarity,Semantic Similarity,,,Methodology,,Creative Commons 4.0,https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:237533,https://paperswithcode.com/dataset/spokensts,Spoken versions of the Semantic Textual Similarity dataset for testing semantic sentence level embeddings. Contains thousands of sentence pairs annotated by humans for semantic similarity. The spoken sentences can be used in sentence embedding models to test whether your model learns to capture sentence semantics. All sentences available in 6 synthetic Wavenet voices and a subset (5%) in 4 real voices recorded in a sound attenuated booth. Code to train a visually grounded spoken sentence embedding model and evaluation code is available at https://github.com/DannyMerkx/speech2image/tree/Interspeech21,,,,,,
2982,Sports-1M,Action Recognition In Videos,Action Recognition In Videos,"Action Recognition In Videos, Action Recognition","Image, Video",,Computer Vision,"action-recognition-in-videos-on-sports-1m-1, action-recognition-in-videos-on-sports-1m",CC BY 3.0,https://cs.stanford.edu/people/karpathy/deepvideo/,https://paperswithcode.com/dataset/sports-1m,"The Sports-1M dataset consists of over a million videos from YouTube. The videos in the dataset can be obtained through the YouTube URL specified by the authors. Approximately 7% (as of 2016) of the videos have been removed by the YouTube uploaders since the dataset was compiled. However, there are still over a million videos in the dataset with 487 sports-related categories with 1,000 to 3,000 videos per category. The videos are automatically labelled with 487 sports classes using the YouTube Topics API by analyzing the text metadata associated with the videos (e.g. tags, descriptions). Approximately 5% of the videos are annotated with more than one class.",2016,Review of Action Recognition and Detection Methods,https://arxiv.org/abs/1610.06906,,,
2983,Sports10,Representation Learning,Representation Learning,"Representation Learning, Image Classification",Image,,Computer Vision,"representation-learning-on-sports10, image-classification-on-sports10",Apache-2.0,https://github.com/ChintanTrivedi/contrastive-game-representations,https://paperswithcode.com/dataset/sports10,"Games dataset containing 100,000 Gameplay Images of 175 Video Games across 10 Sports Genres - AMERICAN FOOTBALL, BASKETBALL, BIKE RACING, CAR RACING, FIGHTING, HOCKEY, SOCCER, TABLE TENNIS, TENNIS. 



Hand-curated images to remove menu/transition frames and only include gameplay sequences.



Games are divided into three visual styling categories: 
        RETRO (arcade-style, 1990s and earlier)
        MODERN (roughly 2000s)
        PHOTOREAL (roughly late 2010s).",,,,,,
2984,SportsMOT,Multiple Object Tracking,Multiple Object Tracking,"Multiple Object Tracking, Multi-Object Tracking","Image, Video",,Computer Vision,"multi-object-tracking-on-sportsmot, multiple-object-tracking-on-sportsmot",CC BY-NC 4.0,https://github.com/MCG-NJU/SportsMOT,https://paperswithcode.com/dataset/sportsmot,"Motivation
Multi-object tracking (MOT) is a fundamental task in computer vision, aiming to estimate objects (e.g., pedestrians and vehicles) bounding boxes and identities in video sequences.

Prevailing human-tracking MOT datasets mainly focus on pedestrians in crowded street scenes (e.g., MOT17/20) or dancers in static scenes (DanceTrack). 

In spite of the increasing demands for sports analysis, there is a lack of multi-object tracking datasets for a variety of sports scenes, where the background is complicated, players possess rapid motion and the camera lens moves fast.

To this purpose, we propose a large-scale multi-object tracking dataset named SportsMOT, consisting of 240 video clips from 3 categories (i.e., basketball, football and volleyball). 

The objective is to only track players on the playground (i.e., except for a number of spectators, referees and coaches) in various sports scenes. We expect SportsMOT to encourage the community to concentrate more on the complicated sports scenes.

Characteristics

Large scale
Fine Annotations
Player id consistency
No shot change
High and fixed resolution(1080P)
...

Focus

Diverse sports scenes
Complex motion patterns
Challenging re-id

Download
Examples
You can download the example for SportsMOT.


OneDrive
Baidu Netdisk, password: 4dnw

Official Dataset
Please Sign up in codalab, and participate in our competition. Download links are available in  Participate/Get Data.

News


SportsMOT is used for  DeeperAction@ECCV-2022. 



Refer to github repo: MCG-NJU/SportsMOT for the latest info.",2022,,,,,3
2985,SportsPose,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, 3D Pose Estimation, Pose Estimation, Pose Prediction, 2D Human Pose Estimation, 3D Absolute Human Pose Estimation","3D, Image, Time Series",,Computer Vision,,Custom,https://christianingwersen.github.io/SportsPose/,https://paperswithcode.com/dataset/sportspose,"Accurate 3D human pose estimation is essential for sports analytics, coaching, and injury prevention. However, existing datasets for monocular pose estimation do not adequately capture the challenging and dynamic nature of sports movements. In response, we introduce SportsPose, a large-scale 3D human pose dataset consisting of highly dynamic sports movements. With more than 176,000 3D poses from 24 different subjects performing 5 different sports activities, SportsPose provides a diverse and comprehensive set of 3D poses that reflect the complex and dynamic nature of sports movements. Contrary to other markerless datasets we have quantitatively evaluated the precision of SportsPose by comparing our poses with a commercial marker-based system and achieve a mean error of 34.5 mm across all evaluation sequences. This is comparable to the error reported on the commonly used 3DPW dataset. We further introduce a new metric, local movement, which describes the movement of the wrist and ankle joints in relation to the body. With this, we show that SportsPose contains more movement than the Human3.6M and 3DPW datasets in these extremum joints, indicating that our movements are more dynamic. The dataset with accompanying code can be downloaded from our website. We hope that SportsPose will allow researchers and practitioners to develop and evaluate more effective models for the analysis of sports performance and injury prevention. With its realistic and diverse dataset, SportsPose provides a valuable resource for advancing the state-of-the-art in pose estimation in sports",,,,,,
2986,Spot-the-diff,Natural Language Visual Grounding,Natural Language Visual Grounding,"Natural Language Visual Grounding, Image Retrieval","Image, Text",English,Computer Vision,,,https://github.com/harsh19/spot-the-diff,https://paperswithcode.com/dataset/spot-the-diff,"Spot-the-diff is a dataset consisting of 13,192 image pairs along with corresponding human provided text annotations stating the differences between the two images.",,Learning to Describe Differences Between Pairs of Similar Images,https://arxiv.org/pdf/1808.10584.pdf,,,
2987,SPOT,Sentiment Analysis,Sentiment Analysis,"Sentiment Analysis, Discourse Parsing, Multiple Instance Learning",Text,English,Natural Language Processing,,,https://github.com/EdinburghNLP/spot-data),https://paperswithcode.com/dataset/spot,"The SPOT dataset contains 197 reviews originating from the Yelp'13 and IMDB collections ([1][2]), annotated with segment-level polarity labels (positive/neutral/negative). Annotations have been gathered on 2 levels of granulatiry:


Sentences
Elementary Discourse Units (EDUs), i.e. sub-sentence clauses produced by a state-of-the-art RST parser

This dataset is intended to aid sentiment analysis research and, in particular, the evaluation of methods that attempt to predict sentiment on a fine-grained, segment-level basis.",,,,,,
2988,Spo_Funcat,Hierarchical Multi-label Classification,Hierarchical Multi-label Classification,Hierarchical Multi-label Classification,Image,,Computer Vision,hierarchical-multi-label-classification-on-7,,https://dtai.cs.kuleuven.be/clus/hmcdatasets/,https://paperswithcode.com/dataset/spo-funcat,Hierarchical-multilabel classification dataset for functional genomics,,,,,,
2989,Spring,Stereo Depth Estimation,Stereo Depth Estimation,"Stereo Depth Estimation, Stereo Disparity Estimation, Optical Flow Estimation, Scene Flow Estimation","3D, Video",,Methodology,"scene-flow-estimation-on-spring, stereo-depth-estimation-on-spring, optical-flow-estimation-on-spring",CC BY 4.0,https://spring-benchmark.org/,https://paperswithcode.com/dataset/spring,"Spring is a large, high-resolution and high-detail, computer-generated benchmark for scene flow, optical flow, and stereo. Based on rendered scenes from the open-source Blender movie ""Spring"", it provides photo-realistic HD datasets with state-of-the-art visual effects and ground truth training data.",,"Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo",https://arxiv.org/pdf/2303.01943v1.pdf,,,
2990,Sprites,Video Prediction,Video Prediction,"Video Prediction, Imputation, Disentanglement","Time Series, Video",,Methodology,"video-prediction-on-sprites, video-prediction-on-colored-dsprites, imputation-on-sprites",,https://github.com/YingzhenLi/Sprites,https://paperswithcode.com/dataset/sprites,"The Sprites dataset contains 60 pixel color images of animated characters (sprites). There are 672 sprites, 500 for training, 100 for testing and 72 for validation. Each sprite has 20 animations and 178 images, so the full dataset has 120K images in total. There are many changes in the appearance of the sprites, they differ in their body shape, gender, hair, armor, arm type, greaves, and weapon.",,Challenges in Disentangling Independent Factors of Variation,https://arxiv.org/abs/1711.02245,178 images,"training, 100 for testing and 72 for validation. Each sprite has 20 animations and 178 images",
2991,SQuAD,Open-Domain Question Answering,Open-Domain Question Answering,"Open-Domain Question Answering, Question Generation, Question Answering, Reading Comprehension, Data-free Knowledge Distillation",Text,English,Natural Language Processing,"question-answering-on-squad11, question-answering-on-squad20-dev, question-generation-on-squad, open-domain-question-answering-on-squad1-1, open-domain-question-answering-on-squad11, question-answering-on-squad-1, question-answering-on-squad11-dev, question-answering-on-squad-adversarial, question-generation-on-squad11, question-answering-on-squad20, data-free-knowledge-distillation-on-squad, question-answering-on-squad-v2",CC BY-SA 4.0,https://rajpurkar.github.io/SQuAD-explorer/,https://paperswithcode.com/dataset/squad,"The Stanford Question Answering Dataset (SQuAD) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones.",,Deep Learning Based Text Classification: A Comprehensive Review,https://arxiv.org/abs/2004.03705,,,
2992,SQUID,Image Dehazing,Image Dehazing,"Image Dehazing, Image Enhancement, Single Image Dehazing",Image,,Computer Vision,,,http://csms.haifa.ac.il/profiles/tTreibitz/datasets/ambient_forwardlooking/index.html,https://paperswithcode.com/dataset/squid,"A dataset of images taken in different locations with varying water properties, showing color charts in the scenes. Moreover, to obtain ground truth, the 3D structure of the scene was calculated based on stereo imaging. This dataset enables a quantitative evaluation of restoration algorithms on natural images.",,,,,,
2993,Squirrel__48__32__20__fixed_splits_,Node Classification on Non-Homophilic (Heterophilic) Graphs,Node Classification on Non-Homophilic (Heterophilic) Graphs,"Node Classification on Non-Homophilic (Heterophilic) Graphs, Node Classification",Image,,Graphs,"node-classification-on-non-homophilic-12, node-classification-on-squirrel-48-32-20",,,https://paperswithcode.com/dataset/squirrel-48-32-20-fixed-splits,Node classification on Squirrel with the fixed 48%/32%/20% splits provided by Geom-GCN.,,,,,,
2994,SR-Reg,Medical Image Registration,Medical Image Registration,Medical Image Registration,Image,,Medical,medical-image-registration-on-sr-reg,CC-BY-NC 4.0,https://drive.google.com/drive/folders/1qxUM-PuvWe1S6GvWudyKUXY8p_jnP_gN?usp=drive_link,https://paperswithcode.com/dataset/sr-reg,"SR-Reg is a brain MR-CT registration dataset, deriving from SynthRAD 2023 (https://synthrad2023.grand-challenge.org/). This dataset contains 180 subjects preprocessed images, and each subject comprises a brain MR image and a brain CT image with corresponding segmentation label. SR-Reg is first introduced in MambaMorph (https://arxiv.org/abs/2401.13934).",2023,,,,,
2995,SRD,Shadow Removal,Shadow Removal,Shadow Removal,,,Methodology,shadow-removal-on-srd,,http://vision.sia.cn/our%20team/JiandongTian/JiandongTian.html,https://paperswithcode.com/dataset/srd%0A,SRD is a dataset for shadow removal that contains 3088 shadow and shadow-free image pairs.,,,,,,
2996,SROIE,Key Information Extraction,Key Information Extraction,"Key Information Extraction, Task 2",,,Methodology,"key-information-extraction-on-sroie, task-2-on-sroie",,https://arxiv.org/pdf/2103.10213.pdf,https://paperswithcode.com/dataset/sroie,Consists of a dataset with 1000 whole scanned receipt images and annotations for the competition on scanned receipts OCR and key information extraction (SROIE).,,Homepage,https://arxiv.org/pdf/2103.10213.pdf,,,
2997,SRSD-Feynman__Easy_set_,Symbolic Regression,Symbolic Regression,Symbolic Regression,,,Methodology,,MIT,https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_easy,https://paperswithcode.com/dataset/srsd-feynman-easy-set,"Our SRSD (Feynman) datasets are designed to discuss the performance of Symbolic Regression for Scientific Discovery. We carefully reviewed the properties of each formula and its variables in the Feynman Symbolic Regression Database to design reasonably realistic sampling range of values so that our SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method con (re)discover physical laws from such datasets.

This is the Easy set of our SRSD-Feynman datasets.",,,,,,
2998,SRSD-Feynman__Hard_set_,Symbolic Regression,Symbolic Regression,Symbolic Regression,,,Methodology,,MIT,https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_hard,https://paperswithcode.com/dataset/srsd-feynman-hard-set,"Our SRSD (Feynman) datasets are designed to discuss the performance of Symbolic Regression for Scientific Discovery. We carefully reviewed the properties of each formula and its variables in the Feynman Symbolic Regression Database to design reasonably realistic sampling range of values so that our SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method con (re)discover physical laws from such datasets.

This is the Hard set of our SRSD-Feynman datasets.",,,,,,
2999,SRSD-Feynman__Medium_set_,Symbolic Regression,Symbolic Regression,Symbolic Regression,,,Methodology,,MIT,https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_medium,https://paperswithcode.com/dataset/srsd-feynman-medium-set,"Our SRSD (Feynman) datasets are designed to discuss the performance of Symbolic Regression for Scientific Discovery. We carefully reviewed the properties of each formula and its variables in the Feynman Symbolic Regression Database to design reasonably realistic sampling range of values so that our SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method con (re)discover physical laws from such datasets.

This is the Medium set of our SRSD-Feynman datasets.",,,,,,
3000,SSC,Audio Classification,Audio Classification,"Audio Classification, Classification, Audio Tagging","Audio, Image",,Audio,audio-classification-on-ssc,Creative Commons Attribution 4.0 International,https://zenkelab.org/resources/spiking-heidelberg-datasets-shd/,https://paperswithcode.com/dataset/ssc,"The SSC dataset is a spiking version of the Speech Commands dataset release by Google (Speech Commands). SSC was  generated using Lauscher, an artificial cochlea model. The SSC dataset consists of utterances recorded from a larger number of speakers under controlled conditions. Spikes were generated in 700 input channels, and it contains 35 word categories from a large number of speakers.

A full description of the dataset and how it was created can be found in the paper below. Please cite this paper if you make use of the dataset.

Cramer, B.; Stradmann, Y.; Schemmel, J.; and Zenke, F. ""The Heidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks"". IEEE Transactions on Neural Networks and Learning Systems 33, 2744–2757, 2022.",2022,,,,,
3001,SSCBench,3D Semantic Scene Completion,3D Semantic Scene Completion,"3D Semantic Scene Completion, 3D Semantic Scene Completion from a single RGB image","3D, Image",,Computer Vision,,,https://github.com/ai4ce/SSCBench,https://paperswithcode.com/dataset/sscbench,"SSCBench establishes a large-scale SSC benchmark in street views that facilitates the training of robust and generalizable SSC models. Overall, SSCBench consists of three subsets, including 38,562 frames for training, 15,798 frames for validation, and 12,553 frames for testing respectively, amounting totally to 66,913 frames.",,,,,,
3002,SSD,Task-Oriented Dialogue Systems,Task-Oriented Dialogue Systems,"Task-Oriented Dialogue Systems, SSTOD",,,Methodology,,,https://tianchi.aliyun.com/dataset/dataDetail?dataId=125708,https://paperswithcode.com/dataset/ssd-1,"SSD (Sub-slot Dialog) dataset: This is the dataset for the ACL 2022 paper ""A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots"". arxiv",2022,arxiv,https://arxiv.org/pdf/2203.10759.pdf,,,
3003,SSD_ID,Task-Oriented Dialogue Systems,Task-Oriented Dialogue Systems,"Task-Oriented Dialogue Systems, SSTOD",,,Methodology,,,https://tianchi.aliyun.com/dataset/dataDetail?dataId=125708,https://paperswithcode.com/dataset/ssd-id,"SSD (Sub-slot Dialog) dataset: This is the dataset for the ACL 2022 paper ""A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots"".",2022,,,,,
3004,SSD_NAME,Task-Oriented Dialogue Systems,Task-Oriented Dialogue Systems,"Task-Oriented Dialogue Systems, SSTOD",,,Methodology,sstod-on-ssd-name,,https://tianchi.aliyun.com/dataset/dataDetail?dataId=125708,https://paperswithcode.com/dataset/ssd-name,"SSD (Sub-slot Dialog) dataset: This is the dataset for the ACL 2022 paper ""A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots"".",2022,,,,,
3005,SSD_PHONE,Task-Oriented Dialogue Systems,Task-Oriented Dialogue Systems,"Task-Oriented Dialogue Systems, SSTOD",,,Methodology,,,https://tianchi.aliyun.com/dataset/dataDetail?dataId=125708,https://paperswithcode.com/dataset/ssd-phone,"SSD (Sub-slot Dialog) dataset: This is the dataset for the ACL 2022 paper ""A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots"".",2022,,,,,
3006,SSD_PLATE,Task-Oriented Dialogue Systems,Task-Oriented Dialogue Systems,"Task-Oriented Dialogue Systems, SSTOD",,,Methodology,,,https://tianchi.aliyun.com/dataset/dataDetail?dataId=125708,https://paperswithcode.com/dataset/ssd-plate,"SSD (Sub-slot Dialog) dataset: This is the dataset for the ACL 2022 paper ""A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots"".",2022,,,,,
3007,SSP-3D,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, 3D Human Reconstruction, 3D Human Shape Estimation","3D, Image",,Computer Vision,3d-human-shape-estimation-on-ssp-3d,,https://github.com/akashsengupta1997/SSP-3D,https://paperswithcode.com/dataset/ssp-3d,"SSP-3D is an evaluation dataset consisting of 311 images of sportspersons in tight-fitted clothes, with a variety of body shapes and poses. The images were collected from the Sports-1M dataset. SSP-3D is intended for use as a benchmark for body shape prediction methods. Pseudo-ground-truth 3D shape labels (using the SMPL body model) were obtained via multi-frame optimisation with shape consistency between frames, as described here.",,here,https://arxiv.org/abs/2009.10013,311 images,valuation dataset consisting of 311 images,
3008,SST-2,Sentiment Analysis,Sentiment Analysis,"Sentiment Analysis, Few-Shot Learning, Explanation Fidelity Evaluation, Classification, Text Classification","Image, Text",English,Computer Vision,"explanation-fidelity-evaluation-on-sst2, sentiment-analysis-on-sst-2-binary, few-shot-learning-on-sst-2-binary, text-classification-on-sst2, text-classification-on-sst-2, classification-on-sst-2",,https://github.com/YJiangcm/SST-2-sentiment-analysis,https://paperswithcode.com/dataset/sst-2,"The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.

Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary.",2005,,,,,
3009,SST-5,Explanation Fidelity Evaluation,Explanation Fidelity Evaluation,"Explanation Fidelity Evaluation, Sentiment Analysis, Few-Shot Text Classification","Image, Text",English,Computer Vision,"few-shot-text-classification-on-sst-5, sentiment-analysis-on-sst-5-fine-grained, explanation-fidelity-evaluation-on-sst-5",,https://github.com/doslim/Sentiment-Analysis-SST5,https://paperswithcode.com/dataset/sst-5,"The SST-5, also known as the Stanford Sentiment Treebank with 5 labels, is a dataset used for sentiment analysis. The SST-5 dataset consists of 11,855 single sentences extracted from movie reviews¹. It includes a total of 215,154 unique phrases from parse trees, each annotated by 3 human judges¹. Each phrase is labeled as either negative, somewhat negative, neutral, somewhat positive, or positive. This is why it's referred to as SST-5 or SST fine-grained.",,,,,,5
3010,SST,Sentiment Analysis,Sentiment Analysis,"Sentiment Analysis, Out-of-Distribution Detection, Few-Shot Learning, Explanation Fidelity Evaluation, Few-Shot Text Classification, Classification, Text Classification","Image, Text",English,Computer Vision,"explanation-fidelity-evaluation-on-sst2, sentiment-analysis-on-sst-2-binary, few-shot-text-classification-on-sst-5, few-shot-learning-on-sst-2-binary, text-classification-on-sst2, text-classification-on-sst-2, out-of-distribution-detection-on-sst, classification-on-sst-2, sentiment-analysis-on-sst-5-fine-grained, explanation-fidelity-evaluation-on-sst-5",,https://nlp.stanford.edu/sentiment,https://paperswithcode.com/dataset/sst,"The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a
complete analysis of the compositional effects of
sentiment in language. The corpus is based on
the dataset introduced by Pang and Lee (2005) and
consists of 11,855 single sentences extracted from
movie reviews. It was parsed with the Stanford
parser and includes a total of 215,154 unique phrases
from those parse trees, each annotated by 3 human judges.

Each phrase is labelled as either negative, somewhat negative, neutral, somewhat positive or positive.
The corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary.",2005,,,,,5
3011,ST-VQA,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Optical Character Recognition (OCR), Question Answering","Image, Text",English,Computer Vision,,,https://rrc.cvc.uab.es/?ch=11,https://paperswithcode.com/dataset/st-vqa,ST-VQA aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the VQA process.,,,,,,
3012,StackOverflow_CG,Open Intent Detection,Open Intent Detection,Open Intent Detection,Image,,Computer Vision,open-intent-detection-on-stackoverflow-cg,MIT,https://github.com/fangyihao/gptaug/tree/main/data/stackoverflow_cg,https://paperswithcode.com/dataset/stackoverflow-cg,"The dataset identifies the shortcomings of existing benchmarks in evaluating the problem of compositional generalization, which underscores the need for the development of datasets tailored to assess compositional generalization in open intent detection tasks.",,,,,,
3013,StackOverflow_MTPP,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Point Processes",Time Series,,Time Series,point-processes-on-stackoverflow-mtpp,Apache-2.0,https://huggingface.co/datasets/easytpp/stackoverflow,https://paperswithcode.com/dataset/stackoverflow-mtpp,The dataset has two years of user awards on a question-answering website: each user received a sequence of badges and there are 22 different kinds of badges in total.,,,,,,
3014,standard_atomic_contexts,Enumerative Search,Enumerative Search,Enumerative Search,,,Methodology,,,https://github.com/dimachine/closureseparation,https://paperswithcode.com/dataset/standard-atomic-contexts,The dataset contains standard contexts of the lattices of all atomic lattices in the Concept Explorer format.,,,,,,
3015,Stanford-ORB,Inverse Rendering,Inverse Rendering,"Inverse Rendering, Surface Normals Estimation, Novel View Synthesis, Image Relighting, Surface Reconstruction, Depth Prediction","3D, Image, Time Series",,Computer Vision,"surface-normals-estimation-on-stanford-orb, inverse-rendering-on-stanford-orb, image-relighting-on-stanford-orb, depth-prediction-on-stanford-orb, surface-reconstruction-on-stanford-orb",,https://stanfordorb.github.io/,https://paperswithcode.com/dataset/stanford-orb,"We introduce Stanford-ORB, a new real-world 3D Object inverse Rendering Benchmark. Recent advances in inverse rendering have enabled a wide range of real-world applications in 3D content generation, moving rapidly from research and commercial use cases to consumer devices. While the results continue to improve, there is no real-world benchmark that can quantitatively assess and compare the performance of various inverse rendering methods. Existing real-world datasets typically only consist of the shape and multi-view images of objects, which are not sufficient for evaluating the quality of material recovery and object relighting. Methods capable of recovering material and lighting often resort to synthetic data for quantitative evaluation, which on the other hand does not guarantee generalization to complex real-world environments. We introduce a new dataset of real-world objects captured under a variety of natural scenes with ground-truth 3D scans, multi-view images, and environment lighting. Using this dataset, we establish the first comprehensive real-world evaluation benchmark for object inverse rendering tasks from in-the-wild scenes, and compare the performance of various existing methods.",,,,,"valuating the quality of material recovery and object relighting. Methods capable of recovering material and lighting often resort to synthetic data for quantitative evaluation, which on the other hand does not guarantee generalization to complex real-world environments. We introduce a new dataset of real-world objects captured under a variety of natural scenes with ground-truth 3D scans, multi-view images",
3016,Stanford_Background,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Scene Labeling, Scene Parsing","Image, Text",English,Computer Vision,,Custom,http://dags.stanford.edu/projects/scenedataset.html,https://paperswithcode.com/dataset/stanford-background,The Stanford Background dataset contains 715 RGB images and the corresponding label images. Images are approximately 240×320 pixels in size and pixels are classified into eight different categories,,Unsupervised Total Variation Loss for Semi-supervised Deep Learning of Semantic Segmentation,https://arxiv.org/abs/1605.01368,,,
3017,Stanford_Cars,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Neural Architecture Search, Image Classification, Continual Learning, Image Clustering, Fine-Grained Image Classification, Image Generation, Prompt Engineering, Few-Shot Learning, Few-Shot Image Classification, Learning with coarse labels, Transductive Zero-Shot Classification","Image, Text",English,Computer Vision,"zero-shot-learning-on-stanford-cars, prompt-engineering-on-stanford-cars-1, image-generation-on-stanford-cars, few-shot-learning-on-stanford-cars, learning-with-coarse-labels-on-stanford-cars, image-classification-on-stanford-cars, transductive-zero-shot-classification-on-5, fine-grained-image-classification-on-stanford, neural-architecture-search-on-stanford-cars, continual-learning-on-stanford-cars-fine, few-shot-image-classification-on-stanford-3, image-clustering-on-stanford-cars, few-shot-image-classification-on-stanford-2",Custom (non-commercial),https://ai.stanford.edu/~jkrause/cars/car_dataset.html,https://paperswithcode.com/dataset/stanford-cars,"The Stanford Cars dataset consists of 196 classes of cars with a total of 16,185 images, taken from the rear. The data is divided into almost a 50-50 train/test split with 8,144 training images and 8,041 testing images. Categories are typically at the level of Make, Model, Year. The images are 360×240.",,"View Independent Vehicle Make, Model and Color Recognition Using Convolutional Neural Network",https://arxiv.org/abs/1702.01721,185 images,"train/test split with 8,144 training images",196
3018,Stanford_ECoG_library__ECoG_to_Finger_Movements,Brain Decoding,Brain Decoding,Brain Decoding,,,Methodology,brain-decoding-on-stanford-ecog-library-ecog,,https://searchworks.stanford.edu/view/zk881ps0522,https://paperswithcode.com/dataset/stanford-ecog-library-ecog-to-gestures,"Electrophysiological data from implanted electrodes in the human brain are rare, and therefore scientific access to it has remained somewhat exclusive. Here we present a freely-available curated library of implanted electrocorticographic (ECoG) data and analyses for 16 benchmark behavioral experiments, with 204 individual datasets from 34 patients made with the same amplifiers (at the same sampling rate and filter settings). In every case, electrode positions have been carefully registered to brain anatomy. A large set of fully-commented analysis scripts to interpret these data using modern techniques is embedded in the library alongside the data. All data, anatomic correlations, and analysis files (MATLAB code) are in a common, intuitive file structure at https://searchworks.stanford.edu/view/zk881ps0522. The library may be used as course material or serve as a starter package for researchers early in their career or for established groups, to modify the analyses and re-apply them in new settings.

Patients were cued with a word displayed on a bedside monitor to move individual fingers repetitively (contralateral to electrode array) during 2 s cue periods while finger position was recorded with a dataglove.
filename - fingerflex.zip",,,,,,
3019,Stanford_Light_Field,Super-Resolution,Super-Resolution,"Super-Resolution, Depth Estimation",3D,,Methodology,,,http://lightfield.stanford.edu/,https://paperswithcode.com/dataset/stanford-light-field,The Stanford Light Field Archive is a collection of several light fields for research in computer graphics and vision.,,,,,,
3020,Stanford_Online_Products,Image Classification,Image Classification,"Image Classification, Metric Learning, Fine-Grained Image Classification, Learning with coarse labels, Image Retrieval",Image,,Computer Vision,"learning-with-coarse-labels-on-stanford, image-retrieval-on-sop, fine-grained-image-classification-on-sop, metric-learning-on-stanford-online-products-1, image-classification-on-stanford-online",,https://cvgl.stanford.edu/projects/lifted_struct/,https://paperswithcode.com/dataset/stanford-online-products,"Stanford Online Products (SOP) dataset has 22,634 classes with 120,053 product images. The first 11,318 classes (59,551 images) are split for training and the other 11,316 (60,502 images) classes are used for testing",,Deep Metric Learning with Alternating Projections onto Feasible Sets,https://arxiv.org/abs/1907.07585,551 images,"split for training and the other 11,316 (60,502 images",634
3021,StaQC,Code Search,Code Search,"Code Search, Question Answering, Source Code Summarization",Text,English,Natural Language Processing,,CC BY 4.0,https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset,https://paperswithcode.com/dataset/staqc,"StaQC (Stack Overflow Question-Code pairs) is a large dataset of around 148K Python and 120K SQL domain question-code pairs, which are automatically mined from StackOverflow.",,https://arxiv.org/pdf/1803.09371v1.pdf,https://arxiv.org/pdf/1803.09371v1.pdf,,,
3022,StarCraft_II_Learning_Environment,Starcraft II,Starcraft II,"Starcraft II, Multi-agent Reinforcement Learning, Starcraft",,,Methodology,,,https://github.com/deepmind/pysc2,https://paperswithcode.com/dataset/starcraft-ii-learning-environment,"The StarCraft II Learning Environment (S2LE) is a reinforcement learning environment based on the game StarCraft II. The environment consists of three sub-components: a Linux StarCraft II binary, the StarCraft II API and PySC2. The StarCraft II API allows programmatic control of StarCraft II. It can be used to start a game, get observations, take actions, and review replays. PyC2 is a Python environment that wraps the StarCraft II API to ease the interaction between Python reinforcement learning agents and StarCraft II. It defines an action and observation specification, and includes a random agent and a handful of rule-based agents as examples. It also includes some mini-games as challenges and visualization tools to understand what the agent can see and do.",,,,,,
3023,StarData,Imitation Learning,Imitation Learning,"Imitation Learning, Real-Time Strategy Games, Starcraft",,,Methodology,,BSD License,https://github.com/TorchCraft/StarData,https://paperswithcode.com/dataset/stardata,"StarData is a StarCraft: Brood War replay dataset, with 65,646 games. The full dataset after compression is 365 GB, 1535 million frames, and 496 million player actions. The entire frame data was dumped out at 8 frames per second.",,,,,,
3024,STARE,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Video Polyp Segmentation, Cell Segmentation, Colorectal Gland Segmentation:, Optic Disc Segmentation, Dichotomous Image Segmentation, Retinal Vessel Segmentation","Image, Video",,Computer Vision,"retinal-vessel-segmentation-on-stare, optic-disc-segmentation-on-stare, video-polyp-segmentation-on-stare, dichotomous-image-segmentation-on-stare, cell-segmentation-on-stare, colorectal-gland-segmentation-on-stare, semantic-segmentation-on-stare",,https://cecas.clemson.edu/~ahoover/stare/,https://paperswithcode.com/dataset/stare,"The STARE (Structured Analysis of the Retina) dataset is a dataset for retinal vessel segmentation. It contains 20 equal-sized (700×605) color fundus images. For each image, two groups of annotations are provided..",,DPN: Detail-Preserving Network with High Resolution Representation for Efficient Segmentation of Retinal Vessels,https://arxiv.org/abs/2009.12053,,,
3025,STARSS22,Sound Event Localization and Detection,Sound Event Localization and Detection,Sound Event Localization and Detection,"Audio, Image",,Computer Vision,sound-event-localization-and-detection-on-1,MIT,https://zenodo.org/record/6387880#.Y1eqqezMJhE,https://paperswithcode.com/dataset/starss22,"The Sony-TAu Realistic Spatial Soundscapes 2022(STARSS22) dataset consists of recordings of real scenes captured with high channel-count spherical microphone array (SMA). The recordings are conducted from two different teams at two different sites, Tampere University in Tammere, Finland, and Sony facilities in Tokyo, Japan. Recordings at both sites share the same capturing and annotation process, and a similar organization. They are organized in sessions, corresponding to distinct rooms, human participants, and sound making props with a few exceptions.

Image https://arxiv.org/pdf/2206.01948v2.pdf",2022,https://arxiv.org/pdf/2206.01948v2.pdf,https://arxiv.org/pdf/2206.01948v2.pdf,,,
3026,STARSS23,Sound Event Localization and Detection,Sound Event Localization and Detection,Sound Event Localization and Detection,"Audio, Image",,Computer Vision,,MIT,https://zenodo.org/records/7880637,https://paperswithcode.com/dataset/starss23,"The Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23) dataset contains multichannel recordings of sound scenes in various rooms and environments, together with temporal and spatial annotations of prominent events belonging to a set of target classes. The dataset is collected in two different countries, in Tampere, Finland by the Audio Researh Group (ARG) of Tampere University (TAU), and in Tokyo, Japan by SONY, using a similar setup and annotation procedure. The dataset is delivered in two 4-channel spatial recording formats, a microphone array one (MIC), and first-order Ambisonics one (FOA). These recordings serve as the development dataset for the DCASE 2023 Sound Event Localization and Detection Task of the DCASE 2023 Challenge.",2023,,,,,
3027,Statcan_Dialogue_Dataset,Table Retrieval,Table Retrieval,"Table Retrieval, Retrieval, Dialogue Generation","Tabular, Text",English,Natural Language Processing,table-retrieval-on-statcan-dialogue-dataset,Custom,https://mcgill-nlp.github.io/statcan-dialogue-dataset/,https://paperswithcode.com/dataset/statcan-dialogue-dataset,"The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents

Xing Han Lu, Siva Reddy, Harm de Vries

EACL 2023


| | | | | |
| :--: | :--: | :--: | :--: | :--: |
| Code | Huggingface | Request on Dataverse | Paper | Website |",2023,The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents,https://arxiv.org/abs/2304.01412,,,
3028,State_Traversal_Observation_Tokens,Bayesian Inference,Bayesian Inference,Bayesian Inference,,,Methodology,,,,https://paperswithcode.com/dataset/state-traversal-observation-tokens,"When arriving at each state, each observation token gets a coin toss to see whether it will appear in the output observation string.  Numbers on the left are indices of observations, numbers on the right are indices of states.",,,,,,
3029,STDW,Table Detection,Table Detection,Table Detection,"Image, Tabular",,Computer Vision,table-detection-on-stdw,Apache-2.0,https://github.com/subex/STDW,https://paperswithcode.com/dataset/stdw,STDW is a diverse large-scale dataset for table detection with more than seven thousand samples containing a wide variety of table structures collected from many diverse sources.,,,,,,
3030,StepGame,Pancreas Segmentation,Pancreas Segmentation,"Pancreas Segmentation, Question Answering, Common Sense Reasoning","Image, Text",English,Computer Vision,question-answering-on-stepgame,,https://github.com/ZhengxiangShi/StepGame,https://paperswithcode.com/dataset/stepgame,A Benchmark for Robust Multi-Hop Spatial Reasoning in Texts,,,,,,
3031,StereoSet,Bias Detection,Bias Detection,Bias Detection,Image,,Computer Vision,bias-detection-on-stereoset-1,CC-BY-SA-4.0,https://github.com/moinnadeem/stereoset,https://paperswithcode.com/dataset/stereoset,"A large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion.",,,,,,
3032,STL-10,Out-of-Distribution Detection,Out-of-Distribution Detection,"Out-of-Distribution Detection, Self-Supervised Learning, Neural Architecture Search, Image Classification, Image Compression, Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly, Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly, Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly, Contrastive Learning, Anomaly Detection, Image Clustering, Fine-Grained Image Classification, Image Generation, Unsupervised Image Classification, Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly, Semi-Supervised Image Classification, Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly, Unsupervised Anomaly Detection","Image, Text",English,Computer Vision,"unsupervised-anomaly-detection-with-specified-20, unsupervised-anomaly-detection-on-stl-10, contrastive-learning-on-stl-10, fine-grained-image-classification-on-stl-10, unsupervised-anomaly-detection-with-specified-15, image-generation-on-stl-10, unsupervised-image-classification-on-stl-10, neural-architecture-search-on-stl-10, anomaly-detection-on-stl-10, image-compression-on-stl-10, unsupervised-anomaly-detection-with-specified, unsupervised-anomaly-detection-with-specified-5, image-clustering-on-stl-10, unsupervised-anomaly-detection-with-specified-8, image-classification-on-stl-10, semi-supervised-image-classification-on-stl, self-supervised-learning-on-stl-10, out-of-distribution-detection-on-stl-10",Custom (attribution + ImageNet license),https://cs.stanford.edu/~acoates/stl10/,https://paperswithcode.com/dataset/stl-10,"The STL-10 is an image dataset derived from ImageNet and popularly used to evaluate algorithms of unsupervised feature learning or self-taught learning. Besides 100,000 unlabeled images, it contains 13,000 labeled images from 10 object classes (such as birds, cats, trucks), among which 5,000 images are partitioned for training while the remaining 8,000 images for testing. All the images are color images with 96×96 pixels in size.",,Unsupervised Feature Learning with C-SVDDNet,https://arxiv.org/abs/1412.7259,000 images,"valuate algorithms of unsupervised feature learning or self-taught learning. Besides 100,000 unlabeled images",
3033,StockNet,Stock Market Prediction,Stock Market Prediction,"Stock Market Prediction, Stock Prediction",Time Series,,Methodology,stock-market-prediction-on-stocknet,,https://github.com/yumoxu/stocknet-dataset,https://paperswithcode.com/dataset/stocknet-1,"The StockNet dataset is a comprehensive dataset for stock movement prediction from tweets and historical stock prices.
It consists of two-year price movements from 01/01/2014 to 01/01/2016 of 88 stocks, coming from all the 8 stocks in the Conglomerates sector and the top 10 stocks in capital size in each of the other 8 sectors.",2014,,,,,
3034,Store_dataset,3D Pose Estimation,3D Pose Estimation,"3D Pose Estimation, Pose Estimation, 3D Multi-Person Pose Estimation","3D, Image",,Computer Vision,,,https://github.com/longcw/crossview_3d_pose_tracking,https://paperswithcode.com/dataset/store-dataset,"The Store Dataset is a dataset for estimating 3D poses of multiple humans in real-time. It is captured inside two kinds of simulated stores with 12 and 28 cameras, respectively.",,https://arxiv.org/abs/2003.03972,https://arxiv.org/abs/2003.03972,,,
3035,Story_Commonsense,Natural Language Inference,Natural Language Inference,"Natural Language Inference, Emotion Classification","Image, Text",English,Computer Vision,,,https://uwnlp.github.io/storycommonsense/,https://paperswithcode.com/dataset/story-commonsense,"Story Commonsense is a new large-scale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research.",,,,,,
3036,STPLS3D,3D Instance Segmentation,3D Instance Segmentation,"3D Instance Segmentation, Instance Segmentation, 3D Open-Vocabulary Instance Segmentation, Semantic Segmentation, 3D Semantic Segmentation","3D, Image",,Computer Vision,"3d-open-vocabulary-instance-segmentation-on-3, 3d-semantic-segmentation-on-stpls3d, 3d-instance-segmentation-on-stpls3d",,https://www.stpls3d.com/,https://paperswithcode.com/dataset/stpls3d,"Our project (STPLS3D) aims to provide a large-scale aerial photogrammetry dataset with synthetic and real annotated 3D point clouds for semantic and instance segmentation tasks.

Although various 3D datasets with different functions and scales have been proposed recently, it remains challenging for individuals to complete the whole pipeline of large-scale data collection, sanitization, and annotation (e.g., semantic and instance labels). Moreover, the created datasets usually suffer from extremely imbalanced class distribution or partial low-quality data samples. Motivated by this, we explore the procedurally synthetic 3D data generation paradigm to equip individuals with the full capability of creating large-scale annotated photogrammetry point clouds. Specifically, we introduce a synthetic aerial photogrammetry point clouds generation pipeline that takes full advantage of open geospatial data sources and off-the-shelf commercial packages. Unlike generating synthetic data in virtual games, where the simulated data usually have limited gaming environments created by artists, the proposed pipeline simulates the reconstruction process of the real environment by following the same UAV flight pattern on a wide variety of synthetic terrain shapes and building densities, which ensure similar quality, noise pattern, and diversity with real data. In addition, the precise semantic and instance annotations can be generated fully automatically, avoiding the expensive and time-consuming manual annotation process.  Based on the proposed pipeline, we present a richly-annotated synthetic 3D aerial photogrammetry point cloud dataset, termed STPLS3D, with more than 16 km^2 of landscapes and up to 18 fine-grained semantic categories. For verification purposes, we also provide a parallel dataset collected from four areas in the real environment.",,,,,,
3037,Strain_gauge_platforms__Time-lapse_microscopy_data,Beat Tracking,Beat Tracking,Beat Tracking,"Image, Video",,Computer Vision,,CC0 1.0 Universal (CC0 1.0) Public Domain Dedication,https://doi.org/10.5061/dryad.sqv9s4nbg,https://paperswithcode.com/dataset/strain-gauge-platforms-time-lapse-microscopy,"This dataset is a ""part I"" extension of the ""Engineered cardiac microbundle time-lapse microscopy image dataset"" and contains 732 experimental time-lapse image sequences of beating hiPSC-based cardiac microbundles using microbundle strain gauge platforms [1] (""Type1""). In ""part II"" extension, we include 808 experimental time-lapse image sequences of beating hiPSC-based cardiac microbundles using FibroTUG platforms [2] (""Type2""). 

Description of the data and file structure
We include here 732 examples of ""Type 1"" tissue. We share the raw videos in "".tif"" format and organize them based on: 1) platform type (""Type 1""), 2) experimental conditions under which the microbundles were grown (""condition_000"", ""condition_001"",...), and 3) the batch of differentiated CMs (""batch_000"", ""batch_001"",...) and is reflected in the folder naming.

Additionally, we include a metadata file (""metadata_type1.csv"") that includes the ""file_name"", ""experimental_conditions"", ""batch"", ""comments"", and ""included_in_plot"". While ""file_name"", ""experimental_conditions"", and ""batch"" are straightforward to comprehend, ""comments"" correspond to remarks regarding running each specific example with MicroBundlePillarTrack software available on GitHub. The ""included_in_plot"" column is used to indicate if the example was included (marked by ""1"") or not (marked by ""0"") in the summary output plot included in the main figure of the accompanying manuscript. We note that the keys for the number coded entries in ""experimental_conditions"" and ""comments"" are included in ""Type1_experimental_conditions.pdf"" and ""Comments_key.pdf"" files. 

Code/Software
We publish this dataset to accompany our pillar tracking software (MicroBundlePillarTrack) published on GitHub. In fact, we have tested our pipeline on all 1540 examples included in this dataset and the FibroTUG platforms: Time-lapse microscopy dataset of engineered cardiac microbundles dataset.

References
[1] Zhang, K., Cloonan, P. E., Sundaram, S., Liu, F., Das, S. L., Ewoldt, J. K., ... & Chen, C. S. (2021). Plakophilin-2 truncating variants impair cardiac contractility by disrupting sarcomere stability and organization. Science Advances, 7(42), eabh3995.

[2] DePalma, S. J., Davidson, C. D., Stis, A. E., Helms, A. S., & Baker, B. M. (2021). Microenvironmental determinants of organized iPSC-cardiomyocyte tissues on synthetic fibrous matrices. Biomaterials science, 9(1), 93-107.",2021,,,732 examples,tested our pipeline on all 1540 examples,
3038,StreetLearn,Cross-View Geo-Localisation,Cross-View Geo-Localisation,"Cross-View Geo-Localisation, Vision and Language Navigation",Text,English,Natural Language Processing,,,http://streetlearn.cc,https://paperswithcode.com/dataset/streetlearn,"An interactive, first-person, partially-observed visual environment that uses Google Street View for its photographic content and broad coverage, and give performance baselines for a challenging goal-driven navigation task.",,,,,,
3039,STREETS,Time Series Prediction,Time Series Prediction,"Time Series Prediction, Graph Generation","Graph, Text, Time Series",English,Time Series,,,https://databank.illinois.edu/datasets/IDB-3671567,https://paperswithcode.com/dataset/streets,"A novel traffic flow dataset from publicly available web cameras in the suburbs of Chicago, IL.",,,,,,
3040,Street_Dataset,Object Detection,Object Detection,"Object Detection, Federated Learning",Image,,Computer Vision,,,https://dataset.fedai.org/#/datasetfed,https://paperswithcode.com/dataset/street-dataset,"A real-world image dataset that contains more than 900 images generated from 26 street cameras and 7 object categories annotated with detailed bounding box. The data distribution is non-IID and unbalanced, reflecting the characteristic real-world federated learning scenarios.",,,,900 images,,
3041,STRING,Link Prediction,Link Prediction,"Link Prediction, Dimensionality Reduction",Time Series,,Methodology,,CC BY 4.0,https://string-db.org/,https://paperswithcode.com/dataset/string,STRING is a collection of protein-protein interaction (PPI) networks.,,,,,,
3042,Structured3D,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Metric Learning, Visual Localization, Image Generation, Indoor Localization, Room Layout Estimation","Image, Text",English,Computer Vision,semantic-segmentation-on-structured3d,Custom,https://github.com/bertjiazheng/Structured3D,https://paperswithcode.com/dataset/structured3d,"Structured3D is a large-scale photo-realistic dataset containing 3.5K house designs (a) created by professional designers with a variety of ground truth 3D structure annotations (b) and generate photo-realistic 2D images (c).
The dataset consists of rendering images and corresponding ground truth annotations (e.g., semantic, albedo, depth, surface normal, layout) under different lighting and furniture configurations.",,,,,,
3043,STS_Benchmark,Semantic Textual Similarity,Semantic Textual Similarity,Semantic Textual Similarity,,,Methodology,"semantic-textual-similarity-on-sts16, semantic-textual-similarity-on-sts13, semantic-textual-similarity-on-sts14, semantic-textual-similarity-on-sts-benchmark, semantic-textual-similarity-on-sts15, semantic-textual-similarity-on-sts12",,http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark,https://paperswithcode.com/dataset/sts-benchmark,"STS Benchmark comprises a selection of the English datasets used in the STS tasks organized in the context of SemEval between 2012 and 2017. The selection of datasets include text from image captions, news headlines and user forums.",2012,,,,,
3044,STURM-Flood,Flood Inundation Mapping,Flood Inundation Mapping,Flood Inundation Mapping,,,Methodology,,,https://doi.org/10.5281/zenodo.12748983,https://paperswithcode.com/dataset/sturm-flood,"The repository hosts the STURM-Flood dataset, an open-access resource designed for flood extent mapping using Sentinel-1 and Sentinel-2 satellite imagery. The dataset comprises 21,602 Sentinel-1 tiles and 2,675 Sentinel-2 tiles, each of size 128 × 128 pixels at a resolution of 10 meters, along with corresponding water masks covering 60 flood events globally. This curated dataset is optimized for deep learning applications and provides ground-truth data from the Copernicus Emergency Management Service to facilitate robust model development. We invite researchers and developers to utilize this resource for advancing flood mapping techniques in disaster management. For further details on the methodology, results, and implementation, please refer to our study published in Big Earth Data (2096-4471): https://doi.org/10.1080/20964471.2025.2458714 .",2096,,,,,
3045,StyleGallery,Style Transfer,Style Transfer,"Style Transfer, Video Style Transfer, Text Style Transfer, Image Generation","Image, Text, Video",English,Computer Vision,style-transfer-on-stylebench,,https://github.com/open-mmlab/StyleShot,https://paperswithcode.com/dataset/stylegallery,"We construct a style-balanced dataset, called StyleGallery, covering several open source datasets. Specifically, StyleGallery includes JourneyDB, a dataset comprising a broad spectrum of diverse styles derived from MidJourney, and WIKIART, with extensive fine-grained painting styles, such as pointillism and ink drawing, and a subset of stylized images from LAION-Aesthetics.",,,,,,
3046,StylePTB,Text Style Transfer,Text Style Transfer,Text Style Transfer,Text,English,Natural Language Processing,,,https://github.com/lvyiwei1/StylePTB/,https://paperswithcode.com/dataset/styleptb,"StylePTB is a fine-grained text style transfer benchmark. It consists of paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as compositions of multiple transfers which allow modelling of fine-grained stylistic changes as building blocks for more complex, high-level transfers.",,,,,,
3047,Stylized_ImageNet,Adversarial Robustness,Adversarial Robustness,"Adversarial Robustness, Image Classification, Domain Generalization",Image,English,Adversarial,adversarial-robustness-on-stylized-imagenet,,https://github.com/rgeirhos/Stylized-ImageNet,https://paperswithcode.com/dataset/stylized-imagenet,The Stylized-ImageNet dataset is created by removing local texture cues in ImageNet while retaining global shape information on natural images via AdaIN style transfer. This nudges CNNs towards learning more about shapes and less about local textures.,,Adversarial Examples Improve Image Recognition,https://arxiv.org/abs/1911.09665,,,
3048,SUBJ,Subjectivity Analysis,Subjectivity Analysis,Subjectivity Analysis,,,Methodology,subjectivity-analysis-on-subj,,http://www.cs.cornell.edu/people/pabo/movie-review-data/,https://paperswithcode.com/dataset/subj,"Available are collections of movie-review documents labeled with respect to their overall sentiment polarity (positive or negative) or subjective rating (e.g., ""two and a half stars"") and sentences labeled with respect to their subjectivity status (subjective or objective) or polarity.",,,,,,
3049,Subjective_Perception_of_Active_Noise_Reduction__S,Soundscape evaluation,Soundscape evaluation,Soundscape evaluation,,,Methodology,,CC-BY-NC 4.0,https://doi.org/10.21979/N9/SEGEFM,https://paperswithcode.com/dataset/subjective-perception-of-active-noise,"This repository contains replication data to the paper titled: ""Anti-noise window: subjective perception of active noise reduction and effect of informational masking""",,,,,,
3050,SubjQA,Word Sense Disambiguation,Word Sense Disambiguation,"Word Sense Disambiguation, Sentiment Analysis, Question Answering",Text,English,Natural Language Processing,,,https://github.com/megagonlabs/SubjQA,https://paperswithcode.com/dataset/subjqa,"SubjQA is a question answering dataset that focuses on subjective (as opposed to factual) questions and answers. The dataset consists of roughly 10,000 questions over reviews from 6 different domains: books, movies, grocery, electronics, TripAdvisor (i.e. hotels), and restaurants. Each question is paired with a review and a span is highlighted as the answer to the question (with some questions having no answer). Moreover, both questions and answer spans are assigned a subjectivity label by annotators. Questions such as ""How much does this product weigh?"" is a factual question (i.e., low subjectivity), while ""Is this easy to use?"" is a subjective question (i.e., high subjectivity).",,,,,,
3051,SubSumE,Text Summarization,Text Summarization,"Text Summarization, Extractive Document Summarization, Query-Based Extractive Summarization, Extractive Summarization, Extractive Text Summarization, Document Summarization",Text,English,Natural Language Processing,,CC-BY-4.0,https://github.com/afariha/SubSumE,https://paperswithcode.com/dataset/subsume,"SubSumE Dataset
This repository contains the SubSumE dataset for subjective document summarization. See the paper and the talk for details on dataset creation. Also check out our work SuDocu on example-based document summarization.

Dataset Files
Download the dataset from here.

The dataset contains :


Simplified text from 48 Wikipedia pages of the states in the US. Additionally, all the sentences in these documents
are put together in a single file processed_state_sentences.csv and are assigned a unique sentence id that 
is used in summary json files. 
Intent-based summaries created by human annotators.

Each datapoint file in the directory user_summary_jsons contains a json containing summaries of Wikipedia pages
of eight states with following keys:


intent : Summarization intent provided to human annotators for generating the summary
summaries: List of summary jsons for eight states assigned to the annotator. Each json in the list contains following keys:
state_name: Name of the state
sentence_ids: Global ids of sentences (wrt processed_state_sentences.csv) present in the summary
sentences: List of sentences present in the summary
use_keywords: Keywords used by the annotator to search the document when creating summaries",,,,,,
3052,SugarCrepe__,Semantic Similarity,Semantic Similarity,Semantic Similarity,,,Methodology,,CC BY 4.0,https://github.com/Sri-Harsha/scpp,https://paperswithcode.com/dataset/sugarcrepe,"The SUGARCREPE++ dataset evaluates the sensitivity of vision language models (VLMs) and unimodal language models (ULMs) to semantic and lexical alterations. 
Each sample in the SugarCrepe++ dataset consists of an image and a corresponding triplet of captions: a pair of semantically equivalent but lexically different positive captions and one hard negative caption. This poses a 3-way semantic (in)equivalence problem to the language models. The SUGARCREPE dataset consists of (only) one positive and one hard negative caption for each image. Relative to the negative caption, a single positive caption can either have low or high lexical overlap. The original SUGARCREPE only captures the high overlap case. To evaluate the sensitivity of encoded semantics to lexical alteration, we require an additional positive caption with a different lexical composition. SUGARCREPE++ fills this gap by adding an additional positive caption enabling a more thorough assessment of models’ abilities to handle semantic content and lexical variation.",,,,,,
3053,SUIM,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Scene Understanding, Saliency Prediction, Semi-Supervised Semantic Segmentation, Unsupervised Semantic Segmentation","Image, Time Series",,Computer Vision,"unsupervised-semantic-segmentation-on-suim, semi-supervised-semantic-segmentation-on-suim",,http://irvlab.cs.umn.edu/resources/suim-dataset,https://paperswithcode.com/dataset/suim,"The Segmentation of Underwater IMagery (SUIM) dataset contains over 1500 images with pixel annotations for eight object categories: fish (vertebrates), reefs (invertebrates), aquatic plants, wrecks/ruins, human divers, robots, and sea-floor. The images have been rigorously collected during oceanic explorations and human-robot collaborative experiments, and annotated by human participants.",,Semantic Segmentation of Underwater Imagery: Dataset and Benchmark,https://arxiv.org/abs/2004.01241,1500 images,,
3054,SuMe,Explanation Generation,Explanation Generation,"Explanation Generation, Text Summarization, Biomedical Information Retrieval",Text,English,Natural Language Processing,,,https://stonybrooknlp.github.io/SuMe/,https://paperswithcode.com/dataset/sume,"Can language models read biomedical texts and explain the biomedical mechanisms discussed? In this work we introduce a biomedical mechanism summarization task. Biomedical studies often investigate the mechanisms behind how one entity (e.g., a protein or a chemical) affects another in a biological context. The abstracts of these publications often include a focused set of sentences that present relevant supporting statements regarding such relationships, associated experimental evidence, and a concluding sentence that summarizes the mechanism underlying the relationship. We leverage this structure and create a summarization task, where the input is a collection of sentences and the main entities in an abstract, and the output includes the relationship and a sentence that summarizes the mechanism. Using a small amount of manually labeled mechanism sentences, we train a mechanism sentence classifier to filter a large biomedical abstract collection and create a summarization dataset with 22k instances. We also introduce conclusion sentence generation as a pretraining task with 611k instances. We benchmark the performance of large bio-domain language models. We find that while the pretraining task help improves performance, the best model produces acceptable mechanism outputs in only 32% of the instances, which shows the task presents significant challenges in biomedical language understanding and summarization.",,,,22k instances,,
3055,SumIPCC,Query-focused Summarization,Query-focused Summarization,Query-focused Summarization,Text,English,Natural Language Processing,,MIT,https://huggingface.co/datasets/ighina/SumIPCC,https://paperswithcode.com/dataset/sumipcc,"The dataset contains 140 paragraphs from climate change reports with associated aspect-based (i.e. query-focused) summaries, that were produced by experts especially for policy-makers.

The dataset's format is the one required by Hugginface's datasets library () and it can be loaded by looking at the available examples. The dataset consists of three possible configurations: ""AR6"" to download just the latest climate change report (70 paragraph/summaries), ""AR5"" to download just the first climate change report (70 paragraph/summaries) or ""ALL"" to download both (140 paragraph/summaries). 

The available features in the dataset are:
""full_paragraphs"": Full paragraphs to summarise,
 ""summary"": The current  query-focused ground truth summary,
                        ""summary_topic"": The topic of on which the current summary is focusing on,
                        ""paragraph_topic"": The macro-topic of the paragraph(s),
                        ""section_topic"": The high-level topic of the section from which the current summary originate,
                        ""source"": The source document (AR5/AR6),
                        ""paragraph_ids"": The id(s) of the current paragraph(s),
                        ""paragraph_titles"": The titles of each of the summarised paragraphs,
                        ""ID"": The row ID

All values above are strings. Please notice that each ground truth summary might refer to one or more target paragraphs from the target document, as the summaries transverse the reports according to specific topics (which is indicated in each case by the summary_topic variable). To better understand the format of this dataset the original IPCC reports and the accompanying summary for policymakers from which the ground truth summaries are taken can be consulted at: https://www.ipcc.ch/",,,,140 paragraphs,,
3056,Summaries_of_genetic_variation,Bayesian Inference,Bayesian Inference,Bayesian Inference,,,Methodology,,MIT,https://github.com/dennisprangle/abctools,https://paperswithcode.com/dataset/summaries-of-genetic-variation,"The dataset represents data generated from a commonly used model in population genetics. It comprises a matrix of 1,000,000 rows and 9 columns, representing parameters and summaries generated by an infinite-sites coalescent model for genetic variation. The first two columns encode the scaled mutation rate (theta) and scaled recombination rate (rho). The subsequent seven columns are data summaries: number of segregating sites (C1), standard uniform random noise acting as a distractor (C2), pairwise mean number of nucleotidic differences (C3), mean $R^2$ across pairs separated by <10% of the simulated genomic regions (C4), number of distinct haplotypes (C5), frequency of the most common haplotype (C6), number of singleton haplotypes (C7).

(this text is not original and adapted from https://journal.r-project.org/archive/2015-2/nunes-prangle.pdf).",2015,,,000 rows,,
3057,Summarizing_Source_Code_using_a_Neural_Attention_M,Source Code Summarization,Source Code Summarization,Source Code Summarization,Text,English,Natural Language Processing,,,https://github.com/sriniiyer/codenn,https://paperswithcode.com/dataset/summarizing-source-code-using-a-neural,"Presents a new dataset of code snippets with short descriptions, created using data gathered from Stackoverflow, a popular programming help website. Since access is open and unrestricted, the content is inherently noisy (ungrammatical, non-parsable, lacking content).",,,,,,
3058,SummZoo,Abstractive Dialogue Summarization,Abstractive Dialogue Summarization,"Abstractive Dialogue Summarization, Extreme Summarization",Text,English,Natural Language Processing,,SummZoo,https://github.com/microsoft/UniSumm,https://paperswithcode.com/dataset/summzoo,"SummZoo, a benchmark consists of 8 diverse summarization tasks with multiple sets of few-shot samples for each task, covering both monologue and dialogue domains.",,UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning,https://arxiv.org/pdf/2211.09783v1.pdf,,,
3059,SUN,Few-Shot Image Classification,Few-Shot Image Classification,"Few-Shot Image Classification, Long-tail learning with class descriptors, Zero-Shot Transfer Image Classification, Generalized Few-Shot Learning",Image,,Computer Vision,"few-shot-image-classification-on-sun-0-shot, long-tail-learning-with-class-descriptors-on-1, zero-shot-transfer-image-classification-on-2, generalized-few-shot-learning-on-sun",,,https://paperswithcode.com/dataset/sun,"When glancing at a magazine, or browsing the Internet, we are continuously being exposed to photographs. Despite of this overflow of visual information, humans are extremely good at remembering thousands of pictures along with some of their visual details. But not all images are equal in memory. Some stitch to our minds, and other are forgotten. In this paper we focus on the problem of predicting how memorable an image will be. We show that memorability is a stable property of an image that is shared across different viewers. We introduce a database for which we have measured the probability that each picture will be remembered after a single view. We analyze image features and labels that contribute to making an image memorable, and we train a predictor based on global image descriptors. We find that predicting image memorability is a task that can be addressed with current computer vision techniques. Whereas making memorable images is a challenging task in visualization and photography, this work is a first attempt to quantify this useful quality of images.",,,,,"train a predictor based on global image descriptors. We find that predicting image memorability is a task that can be addressed with current computer vision techniques. Whereas making memorable images is a challenging task in visualization and photography, this work is a first attempt to quantify this useful quality of images",
3060,SUN397,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Image Classification, Image Clustering, Fine-Grained Image Classification, Scene Recognition, Prompt Engineering, Few-Shot Learning, Transductive Zero-Shot Classification",Image,,Computer Vision,"prompt-engineering-on-sun397, transductive-zero-shot-classification-on-4, zero-shot-learning-on-sun397, image-clustering-on-sun397, scene-recognition-on-sun397, fine-grained-image-classification-on-sun397, image-classification-on-sun397, few-shot-learning-on-sun397",,https://vision.princeton.edu/projects/2010/SUN/,https://paperswithcode.com/dataset/sun397,"The Scene UNderstanding (SUN) database contains 899 categories and 130,519 images. There are 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition.",,,,519 images,,899
3061,SUNCG,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Visual Navigation, Depth Estimation","3D, Image",,Computer Vision,,,https://sscnet.cs.princeton.edu/,https://paperswithcode.com/dataset/suncg,"SUNCG is a large-scale dataset of synthetic 3D scenes with dense volumetric annotations.

The dataset is currently not available.",,,,,,
3062,SUN_Attribute,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Image Classification, Object Recognition, Generalized Zero-Shot Learning",Image,,Computer Vision,"zero-shot-learning-on-sun-attribute, generalized-zero-shot-learning-on-sun","Custom (research-only, non-commercial)",https://cs.brown.edu/~gmpatter/sunattributes.html,https://paperswithcode.com/dataset/sun-attribute,"The SUN Attribute dataset consists of 14,340 images from 717 scene categories, and each category is annotated with a taxonomy of 102 discriminate attributes. The dataset can be used for high-level scene understanding and fine-grained scene recognition.",,Zero-Shot Learning with Multi-Battery Factor Analysis,https://arxiv.org/abs/1606.09349,340 images,,
3063,SUN_RGB-D,Monocular 3D Object Detection,Monocular 3D Object Detection,"Monocular 3D Object Detection, Monocular Depth Estimation, Semantic Segmentation, Panoptic Segmentation, Scene Segmentation, Robust Semi-Supervised RGBD Semantic Segmentation, Panoptic Segmentation (PanopticNDT instances), Scene Recognition, Object Detection In Indoor Scenes, 3D Object Detection, Object Detection, Scene Classification (unified classes), Room Layout Estimation","3D, Image",,Computer Vision,"panoptic-segmentation-panopticndt-instances, 3d-object-detection-on-sun-rgbd-val, object-detection-in-indoor-scenes-on-sun-rgb, panoptic-segmentation-on-sun-rgbd, room-layout-estimation-on-sun-rgb-d, scene-segmentation-on-sun-rgbd, robust-semi-supervised-rgbd-semantic-1, semantic-segmentation-on-sun-rgbd, scene-classification-unified-classes-on-sun, object-detection-on-sun-rgbd-val, monocular-depth-estimation-on-sun-rgbd, 3d-object-detection-on-sun-rgbd, scene-recognition-on-sun-rgbd, monocular-3d-object-detection-on-sun-rgb-d",,https://rgbd.cs.princeton.edu/,https://paperswithcode.com/dataset/sun-rgb-d,"The SUN RGBD dataset contains 10335 real RGB-D images of room scenes. Each RGB image has a corresponding depth and segmentation map. As many as 700 object categories are labeled. The training and testing sets contain 5285 and 5050 images, respectively.",,Mix and match networks: multi-domain alignment for unpaired image-to-image translation,https://arxiv.org/abs/1903.04294,5050 images,training and testing sets contain 5285 and 5050 images,
3064,SuperMat,NER,NER,NER,,,Methodology,ner-on-supermat,Mixed,https://github.com/lfoppiano/SuperMat,https://paperswithcode.com/dataset/supermat,"A growing number of papers are published in the area of superconducting materials science. However, novel text and data mining (TDM) processes are still needed to efficiently access and exploit this accumulated knowledge, paving the way towards data-driven materials design. Herein, we present SuperMat (Superconductor Materials), an annotated corpus of linked data derived from scientific publications on superconductors, which comprises 142 articles, 16052 entities, and 1398 links that are characterised into six categories: the names, classes, and properties of materials; links to their respective superconducting critical temperature (Tc); and parametric conditions such as applied pressure or measurement methods. The construction of SuperMat resulted from a fruitful collaboration between computer scientists and material scientists, and its high quality is ensured through validation by domain experts. The quality of the annotation guidelines was ensured by satisfactory Inter Annotator Agreement (IAA) between the annotators and the domain experts.",,,,,,
3065,SupplyGraph,Relation Classification,Relation Classification,"Relation Classification, Graph Regression, Time Series Forecasting, Anomaly Detection, Temporal Relation Classification, Anomaly Forecasting, tabular-regression, Classification, Heterogeneous Node Classification","Graph, Image, Tabular, Time Series, Video",,Computer Vision,,LPGL-2.1,https://ciol-sust.github.io/works/SupplyGraph/,https://paperswithcode.com/dataset/supplygraph,"Graph Neural Networks (GNNs) have gained traction across different domains such as transportation, bio-informatics, language processing, and computer vision. However, there is a noticeable absence of research on applying GNNs to supply chain networks. Supply chain networks are inherently graphlike in structure, making them prime candidates for applying GNN methodologies. This opens up a world of possibilities for optimizing, predicting, and solving even the most complex supply chain problems. A major setback in this approach lies in the absence of real-world benchmark datasets to facilitate the research and resolution of supply chain problem using GNNs. To address the issue, we present a real-world benchmark dataset for temporal tasks, obtained from one of the leading FMCG companies in Bangladesh, focusing on supply chain planning for production purposes. The dataset includes temporal data as node features to enable sales predictions, production planning, and the identification of factory issues. By utilizing this dataset, researchers can employ GNNs to address numerous supply chain problems, thereby advancing the field of supply chain analytics and planning. 


Dataset GitHub
arXiv
PDF on arXiv

Read the paper to learn more details and data statistics.",,arXiv,https://arxiv.org/abs/2401.15299,,,
3066,SurgeGlobal_LaMini,Instruction Following,Instruction Following,Instruction Following,,,Methodology,,Apache-2.0,https://huggingface.co/datasets/SurgeGlobal/LaMini,https://paperswithcode.com/dataset/surgeglobal-lamini,"Overview
The LaMini Dataset is an instruction dataset generated using h2ogpt-gm-oasst1-en-2048-falcon-40b-v2. It is designed for instruction-tuning pre-trained models to specialize them in a variety of downstream tasks.

Dataset Generation

Base Model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2.
Seed Instructions: Sourced from databricks/databricks-dolly-15k dataset.
Generation Approach: Example-guided and topic-guided strategies.
Total Instructions: 1,504 unique instruction examples.

Dataset Sources

Repository: Bitbucket Project
Paper : Pre-Print

Structure
Each entry in the dataset contains:
- Instruction
- Response

Usage
The LaMini Dataset can be used to fine-tune language models to improve their ability to follow instructions and generate relevant responses.

Access
The dataset is available on HuggingFace at the following link: https://huggingface.co/datasets/SurgeGlobal/LaMini

Citation
If you find our work useful, please cite our paper as follows:
@misc{surge2024openbezoar,
      title={OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data}, 
      author={Chandeepa Dissanayake and Lahiru Lowe and Sachith Gunasekara and Yasiru Ratnayake},
      year={2024},
      eprint={2404.12195},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

Dataset Authors
Chandeepa Dissanayake, Lahiru Lowe, Sachith Gunasekara, and Yasiru Ratnayake",2048,Pre-Print,https://arxiv.org/abs/2404.12195,,,
3067,Survival_Analysis_of_Heart_Failure_Patients,Survival Prediction,Survival Prediction,"Survival Prediction, Survival Analysis",Time Series,,Methodology,,CC BY 4.0,https://plos.figshare.com/articles/dataset/Survival_analysis_of_heart_failure_patients_A_case_study/5227684/1,https://paperswithcode.com/dataset/survival-analysis-of-heart-failure-patients,The    dataset    contains cardiovascular medical records taken from 299 patients. The patient cohort comprised  of  105  women  and  194  men between  40  and  95  years  in age.  All  patients in  the  cohort were  diagnosed  with  the  systolic  dysfunction  of  the  left ventricle  and  had  previous  history  of  heart failures.  As  a result  of  their  previous  history  every  patient  was  classified into   either   class   III   or   class   IV   of   New   York   Heart Association  (NYHA)  classification for various stages  of heart  failure.,,,,,,
3068,SV-Ident,Cross-Lingual Paraphrase Identification,Cross-Lingual Paraphrase Identification,"Cross-Lingual Paraphrase Identification, Paraphrase Identification, Variable Disambiguation, Semantic Similarity, Information Retrieval, Variable Detection, Entity Linking, Multi-Label Text Classification, Text Classification","Image, Text",English,Computer Vision,"variable-disambiguation-on-sv-ident, variable-detection-on-sv-ident",Custom,https://vadis-project.github.io/sv-ident-sdp2022/,https://paperswithcode.com/dataset/sv-ident,"SV-Ident comprises 4,248 sentences from social science publications in English and German. The data is the official data for the Shared Task: “Survey Variable Identification in Social Science Publications” (SV-Ident) 2022. Sentences are labeled with variables that are mentioned either explicitly or implicitly. 

The dataset supports the following tasks:



Variable Detection: identifying whether a sentence contains a variable mention or not.



Variable Disambiguation: identifying which variable from a given vocabulary is mentioned in a sentence.",2022,,,248 sentences,,
3069,SVAMP,Math Word Problem Solving,Math Word Problem Solving,"Math Word Problem Solving, Mathematical Reasoning",,,Methodology,"math-word-problem-solving-on-svamp, math-word-problem-solving-on-svamp-1-n",MIT,https://github.com/arkilpatel/SVAMP,https://paperswithcode.com/dataset/svamp,"A challenge set for elementary-level Math Word Problems (MWP). An MWP consists of a short Natural Language narrative that describes a state of the world and poses a question about some unknown quantities.

The examples in SVAMP test a model across different aspects of solving MWPs: 1) Is the model question sensitive? 2) Does the model have robust reasoning ability? 3) Is it invariant to structural alterations?",,,,,,
3070,SVHN,Image Classification,Image Classification,"Image Classification, Novel Class Discovery, Sparse Representation-based Classification, Anomaly Detection, Unsupervised Image Classification, Semi-Supervised Image Classification, Domain Adaptation",Image,,Computer Vision,"domain-adaptation-on-svhn-to-mnist, semi-supervised-image-classification-on-svhn-1, semi-supervised-image-classification-on-svhn-4, semi-supervised-image-classification-on-svhn, semi-supervised-image-classification-on-svhn-3, anomaly-detection-on-svhn, sparse-representation-based-classification-on, image-classification-on-svhn, semi-supervised-image-classification-on-svhn-2, unsupervised-image-classification-on-svhn, novel-class-discovery-on-svhn, semi-supervised-image-classification-on-svhn-5",CC,http://ufldl.stanford.edu/housenumbers/,https://paperswithcode.com/dataset/svhn,"Street View House Numbers (SVHN) is a digit classification benchmark dataset that contains 600,000 32×32 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates. The cropped images are centered in the digit of interest, but nearby digits and other distractors are kept in the image. SVHN has three sets: training, testing sets and an extra set with 530,000 images that are less difficult and can be used for helping with the training process.",,Reading digits in natural images with unsupervised feature learning,http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf,000 images,"training, testing sets and an extra set with 530,000 images",
3071,SVIRO,3D Pose Estimation,3D Pose Estimation,"3D Pose Estimation, Intent Detection, Pose Estimation","3D, Image",,Computer Vision,,,https://sviro.kl.dfki.de/,https://paperswithcode.com/dataset/sviro,"Contains bounding boxes for object detection, instance segmentation masks, keypoints for pose estimation and depth images for each synthetic scenery as well as images for each individual seat for classification.",,,,,,
3072,SVT,Scene Text Recognition,Scene Text Recognition,Scene Text Recognition,"Image, Text",English,Computer Vision,scene-text-recognition-on-svt,,http://vision.ucsd.edu/~kai/svt/,https://paperswithcode.com/dataset/svt,"The Street View Text (SVT) dataset was harvested from Google Street View. Image text in this data exhibits high variability and often has low resolution. In dealing with outdoor street level imagery, we note two characteristics. (1) Image text often comes from business signage and (2) business names are easily available through geographic business searches. These factors make the SVT set uniquely suited for word spotting in the wild: given a street view image, the goal is to identify words from nearby businesses.

Note: the dataset has undergone revision since the time it was evaluated in this publication. Please consult the ICCV2011 paper for most up-to-date results.",,ICCV2011 paper,http://vision.ucsd.edu/~kai/pubs/wang_iccv2011.pdf,,,
3073,SVTP,Scene Text Recognition,Scene Text Recognition,Scene Text Recognition,"Image, Text",English,Computer Vision,scene-text-recognition-on-svtp,,,https://paperswithcode.com/dataset/svtp,"SVTP dataset stands for Scene Text Recognition Datasets. It is a collection of 4 popular Latin/English scene text recognition datasets, namely IIIT5K, SVT, SVTP, and CUTE-80. These datasets only provide case-insensitive annotations and no punctuation marks.",,,,,,
3074,SWAG,Question Answering,Question Answering,"Question Answering, Common Sense Reasoning",Text,English,Natural Language Processing,"question-answering-on-swag, common-sense-reasoning-on-swag",MIT,https://rowanzellers.com/swag/,https://paperswithcode.com/dataset/swag,"Given a partial description like ""she opened the hood of the car,"" humans can reason about the situation and anticipate what might come next (""then, she examined the engine""). SWAG (Situations With Adversarial Generations) is a large-scale dataset for this task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning.

The dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene. The correct answer is the (real) video caption for the next event in the video; the three incorrect answers are adversarially generated and human verified, so as to fool machines but not humans. The authors aim for SWAG to be a benchmark for evaluating grounded commonsense NLI and for learning representations.",,Zellers et al,https://arxiv.org/pdf/1808.05326v1.pdf,,,
3075,SweetRS,Matrix Completion,Matrix Completion,"Matrix Completion, Recommendation Systems",,,Methodology,,,https://github.com/kidzik/sweetrs-analysis,https://paperswithcode.com/dataset/sweetrs,Uses a  platform with 77 candies and sweets to rank. Over 2000 users submitted over 44000 grades resulting in a matrix with 28% coverage.,2000,,,,,
3076,SymphonyNet,Audio Generation,Audio Generation,Audio Generation,"Audio, Text",English,Audio,audio-generation-on-symphony-music,,https://symphonynet.github.io/,https://paperswithcode.com/dataset/symphonynet,First large-scale symphony generation dataset.,,,,,,
3077,SynD,Non-Intrusive Load Monitoring,Non-Intrusive Load Monitoring,Non-Intrusive Load Monitoring,,,Methodology,,CC0 1.0 Universal (CC0 1.0),https://springernature.figshare.com/collections/SynD_A_Synthetic_Energy_Dataset_for_Non-Intrusive_Load_Monitoring_in_Households/4716179,https://paperswithcode.com/dataset/synd,"SynD is a synthetic energy dataset with a focus on residential buildings. This dataset is the result of a custom simulation process that relies on power traces of household appliances. The output of simulations is the power consumption of 21 household appliances as well as the household-wide consumption (i.e. mains). Therefore, SynD's can be used for Non-Intrusive Load Monitoring, also referred to as Energy Disaggregation.",,,,,,
3078,SYNTH-PEDES,Representation Learning,Representation Learning,Representation Learning,,,Methodology,,MIT,https://github.com/Zplusdragon/PLIP,https://paperswithcode.com/dataset/synth-pedes,"SYNTH-PEDES is a large-scale person dataset with image-text pairs by far, which contains 312,321 identities, 4,791,711 images, and 12,138,157 textual descriptions.",,,,711 images,,
3079,SyntheticFur,Neural Rendering,Neural Rendering,Neural Rendering,,,Methodology,,Apache-2.0,https://github.com/google-research-datasets/synthetic-fur,https://paperswithcode.com/dataset/syntheticfur,"SyntheticFur is a dataset for neural rendering. Collecting and generating high quality fur images is an expensive and difficult process that requires content specialists to generate. By releasing this unique dataset with high quality lighting simulation via ray tracing, this can save time for researchers seeking to advance studies of fur rendering and simulation, without having to recreate this laborious process.

The dataset was used for neural rendering research at Google that takes advantage of rasterized image buffers and converts them into high quality raytraced fur renders. We believe that this dataset can contribute to the computer graphics and machine learning community to develop more advanced techniques with fur rendering.

It contains approximately 140,000 procedurally generated images and 15 simulations with Houdini. The images consist of fur groomed with different skin primitives and move with various motions in a predefined set of lighting environments.",,,,,,
3080,Synthetic_COVID-19_CXR_Dataset,Data Augmentation,Data Augmentation,"Data Augmentation, Image Classification, Synthetic Data Generation, COVID-19 Diagnosis","Image, Text",English,Computer Vision,,,https://github.com/hasibzunair/synthetic-covid-cxr-dataset,https://paperswithcode.com/dataset/synthetic-covid-19-cxr-dataset,"A public open dataset of synthetic chest X-ray images of COVID-19.

The dataset consists of 21,295 synthetic COVID-19 chest X-ray images. Images are generated using an unsupervised domain adaptation approach by leveraging class conditioning and adversarial training from source datasets RSNA Kaggle Dataset and COVID-19 Image Data Collection. 

Implementation of the algorithm is available here.",,,,,,
3081,Synthetic_Graph,Subgraph Counting - Tailed Triangle,Subgraph Counting - Tailed Triangle,"Subgraph Counting - Tailed Triangle, Subgraph Counting - C5, Subgraph Counting, Subgraph Counting - C4, Subgraph Counting - C6, Subgraph Counting - Triangle, Subgraph Counting - 3 Star, Subgraph Counting - 2 star, Subgraph Counting - K4, Subgraph Counting - Chordal C4",,,Methodology,"subgraph-counting-c6-on-synthetic-graph, subgraph-counting-k4-on-synthetic-graph, subgraph-counting-tailed-triangle-on, subgraph-counting-c4-on-synthetic-graph, subgraph-counting-triangle-on-synthetic-graph, subgraph-counting-2-star-on-synthetic-graph, subgraph-counting-3-star-on-synthetic-graph, subgraph-counting-c5-on-synthetic-graph, subgraph-counting-chordal-c4-on-synthetic",,,https://paperswithcode.com/dataset/synthetic-graph,"We include five substructure counting tasks: 3-stars, triangles, tailed triangles, chordal cycles and attributed triangles. 3-star is a subgraph-counting task while the remaining are induced-subgraph-counting.",,,,,,
3082,Synthetic_Keystroke,Inference Attack,Inference Attack,"Inference Attack, Domain Adaptation",,,Methodology,,,https://github.com/jlim13/keystroke-inference-attack-synthetic-dataset-generator-,https://paperswithcode.com/dataset/synthetic-keystroke,This dataset is a large-scale synthetic dataset to simulate the attack scenario for a keystroke inference attack.,,https://arxiv.org/abs/2009.05796,https://arxiv.org/abs/2009.05796,,,
3083,Synthetic_Object_Preference_Adaptation_Data,Industrial Robots,Industrial Robots,"Industrial Robots, Robot Navigation, Robot Task Planning",,,Methodology,,MIT,https://github.com/Alvinosaur/opa,https://paperswithcode.com/dataset/synthetic-object-preference-adaptation-data,"This dataset involves a 2D or 3D agent moving from a start to goal pose while interacting with nearby objects. These objects can influence position of the agent via attraction or repulsion forces as well as influence orientation via attraction to object's orientation. This dataset can be used to pre-train general policy behavior, which can be later fine-tuned quickly for a person's specific preferences. Example use-cases include:
- self-driving cars maintaining distance from other cars
- robot pick-and-place tasks with intermediate subtasks (ie: scanning factory items  before dropping them off)

Overall, pre-training initial policy behavior to be fine-tuned later is a powerful paradigm and is arguably essential for robots to handle changing environments and user preferences. This is compared to the paradigm of training on massive amounts of data and remaining fixed at test time, hoping that generalization alone will help the agent handle new scenarios.",,,,,,
3084,Synthetic_OD_Data,3D Point Cloud Reinforcement Learning,3D Point Cloud Reinforcement Learning,3D Point Cloud Reinforcement Learning,3D,,Reinforcement Learning,3d-point-cloud-reinforcement-learning-on,,,https://paperswithcode.com/dataset/synthetic-od-data,Synthetic OD data to mimic data showed in the application of the paper.,,,,,,
3085,Synthetic_Plant_Dataset,Synthetic Data Generation,Synthetic Data Generation,Synthetic Data Generation,Text,English,Natural Language Processing,,,https://gts.ai/dataset-download/synthetic-plant-dataset/,https://paperswithcode.com/dataset/synthetic-plant-dataset,"About Dataset
The File contains 3D point cloud data of a Fabricate plant with 10 sequences. Each sequence contains 0-19 days data at every growth stage of the specific sequence.

The Importance of Synthetic Plant Datasets
Synthetic Plant Datasets: Synthetic plant FIle are carefully curated collections of computer- Beginning images that mimic the diverse appearance and growth stages of real plants. 

Training and Evaluation: By  fabricated plant file, researchers can train and evaluate machine learning models in a controlled environment, free from the limitations of real-world data collection. This controlled setting enables more efficient code development and ensures consistent performance across various environmental conditions.

Applications in Agricultural Technology
Plant Phenotyping: Synthetic plant file enable researchers to analyze plant traits and characteristics on a large scale, facilitating plant phenotyping studies aimed at understanding genetic traits, environmental influences, and crop performance.

Crop Monitoring: With the rise of precision agriculture, fabricated plant file play a crucial role in developing remote sensing techniques for monitoring crop health, detecting pest infestations, and optimizing irrigation strategies.

Advancements in Computer Vision and Machine Learning
Object Detection: It serve as benchmarking tools for training and evaluating object detection algorithms tailored to identifying plants, fruits, and diseases in agricultural settings.

Future Directions and Challenges
Dataset Diversity: As the demand for more diverse and realistic  grows, researchers face the challenge of generating data that accurately reflects the variability observed in real-world agricultural environments.

Researchers continue to explore techniques for bridging the gap between synthetic and real data to enhance model robustness and applicability.

Conclusion
Synthetic plant datasets represent a cornerstone in the development of cutting-edge technologies for agricultural monitoring, plant phenotyping, and disease diagnosis. By harnessing the power of synthetic data generation and machine learning, researchers can unlock new insights into plant biology and revolutionize the future of agriculture.

This dataset is sourced from Kaggle.",,,,,,
3086,Synthetic_Speech_Attribution,Synthetic Speech Detection,Synthetic Speech Detection,Synthetic Speech Detection,"Audio, Image",,Speech,,MIT,https://www.kaggle.com/datasets/awsaf49/sp22-synthetic-dataset,https://paperswithcode.com/dataset/synthetic-speech-attribution,Synthetic Speech Attribution Dataset.,,,,,,
3087,SYNTHIA-AL,Object Detection,Object Detection,"Object Detection, Video Object Detection, Active Learning","Image, Video",,Computer Vision,,,http://synthia-dataset.net/downloads/,https://paperswithcode.com/dataset/synthia-al,Specially designed to evaluate active learning for video object detection in road scenes.,,,,,,
3088,SYNTHIA,Synthetic-to-Real Translation,Synthetic-to-Real Translation,"Synthetic-to-Real Translation, Image-to-Image Translation, Semantic Segmentation, Novel View Synthesis, Unsupervised Domain Adaptation, One-shot Unsupervised Domain Adaptation, Source-Free Domain Adaptation, Domain Adaptation","Image, Text",English,Computer Vision,"domain-adaptation-on-synthia-to-cityscapes, image-to-image-translation-on-synthia-to, unsupervised-domain-adaptation-on-synthia-to, semantic-segmentation-on-synthia, image-to-image-translation-on-synthia-fall-to, synthetic-to-real-translation-on-synthia-to-1, source-free-domain-adaptation-on-synthia-to, novel-view-synthesis-on-synthia-novel-view, semantic-segmentation-on-synthia-to, semantic-segmentation-on-synthia-cvpr16, one-shot-unsupervised-domain-adaptation-on-1",CC BY-NC-SA 3.0,https://synthia-dataset.net/,https://paperswithcode.com/dataset/synthia,The SYNTHIA dataset is a synthetic dataset that consists of 9400 multi-viewpoint photo-realistic frames rendered from a virtual city and comes with pixel-level semantic annotations for 13 classes. Each frame has resolution of 1280 × 960.,,Orientation-aware Semantic Segmentation on Icosahedron Spheres,https://arxiv.org/abs/1907.12849,,,13
3089,SynthSOD,Music Source Separation,Music Source Separation,Music Source Separation,Audio,,Audio,,Creative Commons Attribution Share Alike 4.0 International,https://zenodo.org/records/13759492,https://paperswithcode.com/dataset/synthsod,"The SynthSOD dataset contains more than 47 hours of multitrack music obtained by synthesizing orchestra and ensemble pieces from the Symbolic Orchestral Database (SOD) using Spitfire BBC Symphony Orchestra Professional Library. To synthesize the MIDI files from the SOD, we needed to fix the original files into the General MIDI standard, select a subsect of files that fitted into our requirements (e.g.,  containing only instruments that we could synthesize), and develop a new system to generate musically-motivated random annotations about tempo, dynamic, and articulation.",,,,,,
3090,SYSU-MM01-C,Cross-Modal Person Re-Identification,Cross-Modal Person Re-Identification,"Cross-Modal Person Re-Identification, Person Re-Identification, Generalizable Person Re-identification",Image,,Computer Vision,person-re-identification-on-sysu-mm01-c,,https://github.com/MinghuiChen43/CIL-ReID,https://paperswithcode.com/dataset/sysu-mm01-c,"SYSU-MM01-C is an evaluation set that consists of algorithmically generated corruptions applied to the SYSU-MM01 test-set.  These corruptions consist of Noise: Gaussian, shot,
impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions.",,,,,,
3091,SYSU-MM01,Cross-Modal  Person Re-Identification,Cross-Modal  Person Re-Identification,"Cross-Modal  Person Re-Identification, Person Re-Identification",Image,,Computer Vision,"cross-modal-person-re-identification-on-sysu, person-re-identification-on-sysu-mm01",Custom (non-commercial),https://github.com/wuancong/SYSU-MM01,https://paperswithcode.com/dataset/sysu-mm01,"The SYSU-MM01 is a dataset collected for the Visible-Infrared Re-identification problem. The images in the dataset were obtained from 491 different persons by recording them using 4 RGB and 2 infrared cameras. Within the dataset, the persons are divided into 3 fixed splits to create training, validation and test sets. In the training set, there are 20284 RGB and 9929 infrared images of 296 persons. The validation set contains 1974 RGB and 1980 infrared images of 99 persons. The testing set consists of the images of 96 persons where 3803 infrared images are used as query and 301 randomly selected RGB images are used as gallery.",1974,An Efficient Framework for Visible-Infrared Cross Modality Person Re-Identification,https://arxiv.org/abs/1907.06498,,"splits to create training, validation and test sets. In the training set, there are 20284 RGB and 9929 infrared images",
3092,Szeged_Corpus,Morphological Tagging,Morphological Tagging,"Morphological Tagging, Semantic Parsing, Part-Of-Speech Tagging, Lemmatization","Audio, Text",English,Natural Language Processing,,Custom,https://rgai.inf.u-szeged.hu/node/113,https://paperswithcode.com/dataset/szeged-corpus,"The Szeged Treebank is the largest fully manually annotated treebank of the Hungarian language. It contains 82,000 sentences, 1.2 million words and 250,000 punctuation marks. Texts were selected from six different domains, ~200,000 words in size from each. The domains are the following:

fiction
compositions of pupils between 14-16 years of age
newspaper articles (from the newspapers Népszabadság, Népszava, Magyar Hírlap, HVG)
texts in informatics
legal texts
business and financial news
The treebank exists in three versions:

Szeged Treebank 1.0 is annotated for noun phrases and clauses;
Szeged Treebank 2.0 contains a deep phrase-structured syntactic analysis for all sentences;
Szeged Dependency Treebank contains dependency-style annotation of all sentences.
A morphologically reannotated version of the corpus, Szeged Corpus 2.5 has just been released, where participles, causative, frequentative and model verbs are distinctively marked, and unknown or misspelled words have been corrected, along with some minor morphological modifications.
If you are interested in Szeged Corpus 2.5, please contact Veronika Vincze.",,,,000 sentences,,
3093,S_P_500_Intraday_Data,Algorithmic Trading,Algorithmic Trading,Algorithmic Trading,,,Methodology,,,https://www.kaggle.com/nickdl/snp-500-intraday-data,https://paperswithcode.com/dataset/s-p-500-intraday-data,"Technical Information
Dates range from 2017-09-11 to 2018-02-16 and the time interval is 1 minute.
This is a MultiIndex CSV file, to load in pandas use:

dataset = pd.read_csv('dataset.csv', index_col=0, header=[0, 1]).sort_index(axis=1)

Stocks that entered or exited the Index during the dataset time range are omitted.

Collection & Processing
These are the scripts used for collecting the data, and also utilities to clean & scale the dataset & convert it to a numpy array:
https://github.com/nickdl/alpha",2017,,,,,
3094,S_P_500_Pair_Trading,PAIR TRADING,PAIR TRADING,PAIR TRADING,,,Methodology,pair-trading-on-s-p-500-pair-trading,,https://github.com/chancefocus/trials,https://paperswithcode.com/dataset/s-p-500-pair-trading,"A pool of real stocks from S&P 500 for recent 21 years from 01/02/2000 to 12/31/2020.
We filter stocks that have missing data throughout the whole period, resulting in 150 stocks with 5,284 trading days.",2000,,,,,
3095,T-REx,Language Modelling,Language Modelling,"Language Modelling, Relation Extraction, Factual probe, Zero-shot Slot Filling, Question Answering","Graph, Text",English,Natural Language Processing,"factual-probe-on-t-rex, zero-shot-slot-filling-on-t-rex",Creative Commons Attribution-ShareAlike 4.0 International License,https://hadyelsahar.github.io/t-rex/downloads/,https://paperswithcode.com/dataset/t-rex,A dataset of large scale alignments between Wikipedia abstracts and Wikidata triples. T-REx consists of 11 million triples aligned with 3.09 million Wikipedia abstracts (6.2 million sentences).,,,,,,
3096,T2Dv2,Columns Property Annotation,Columns Property Annotation,"Columns Property Annotation, Table Type Detection, Table annotation, Row Annotation, Column Type Annotation","Image, Tabular",,Computer Vision,"columns-property-annotation-on-t2dv2, column-type-annotation-on-t2dv2, row-annotation-on-t2dv2, table-type-detection-on-t2dv2",Apache-2.0,http://webdatacommons.org/webtables/goldstandardV2.html,https://paperswithcode.com/dataset/t2dv2,"The T2Dv2 dataset consists of 779 tables originating from the English-language subset of the WebTables corpus. 237 tables are annotated for the Table Type Detection task, 236 for the Columns Property Annotation (CPA) task and 235 for the Row Annotation task. The annotations that are used are DBpedia types, properties and entities.

A subset of this dataset was annotated by Chen et al. for the Column Type Annotation (CTA) task where they annotate 236 tables with DBpedia types. In the papers it was used the subset was divided into training and testing splits and the evaluation was done on the testing split T2D-Te. This subset is available for download at their official Github repository.

Some characteristics for the different tasks are provided in the table below, where ""Annotations"" refers to the number of cells/rows/columns/tables annotated and ""Classes"" to the number of unique classes used for annotation.

|                | Annotations | Classes |
|----------------|---------|---------|
| Column Property Annotation  |   670  |  119 |
| Row Annotation |   26,106   | 13,975 |
| Table Type     | 237 | 41 |
| Column Type Annotation     | 411 | 37 |",,,,,,
3097,T2I-CompBench,Text-to-Image Generation,Text-to-Image Generation,Text-to-Image Generation,"Image, Text",English,Computer Vision,,MIT,https://karine-h.github.io/T2I-CompBench/,https://paperswithcode.com/dataset/t2i-compbench,"T2I-CompBench is a comprehensive benchmark for open-world compositional text-to-image generation, consisting of 6,000 compositional textual prompts from 3 categories (attribute binding, object relationships, and complex compositions) and 6 sub-categories (color binding, shape binding, texture binding, spatial relationships, non-spatial relationships, and complex compositions).",,,,,,3
3098,TabFact,Table-based Fact Verification,Table-based Fact Verification,"Table-based Fact Verification, Natural Language Inference","Tabular, Text",English,Natural Language Processing,"table-based-fact-verification-on-tabfact, natural-language-inference-on-tabfact",CC BY 4.0,https://tabfact.github.io/,https://paperswithcode.com/dataset/tabfact,"TabFact is a large-scale dataset which consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED. TabFact is the first dataset to evaluate language inference on structured data, which involves mixed reasoning skills in both symbolic and linguistic aspects.",,,,,,
3099,TableBank,Table Detection,Table Detection,Table Detection,"Image, Tabular",,Computer Vision,,,https://github.com/doc-analysis/TableBank,https://paperswithcode.com/dataset/tablebank,"To address the need for a standard open domain table benchmark dataset, the author propose a novel weak supervision approach to automatically create the TableBank, which is orders of magnitude larger than existing human labeled datasets for table analysis. Distinct from traditional weakly supervised training set, our approach can obtain not only large scale but also high quality training data.

Nowadays, there are a great number of electronic documents on the web such as Microsoft Word (.docx) and Latex (.tex) files. These online documents contain mark-up tags for tables in their source code by nature. Intuitively, one can manipulate these source code by adding bounding box using the mark-up language within each document. For Word documents, the internal Office XML code can be modified where the borderline of each table is identified. For Latex documents, the tex code can be also modified where bounding boxes of tables are recognized. In this way, high-quality labeled data is created for a variety of domains such as business documents, official fillings, research papers etc, which is tremendously beneficial for large-scale table analysis tasks.

The TableBank dataset totally consists of 417,234 high quality labeled tables as well as their original documents in a variety of domains.",,,,,,
3100,TACO-Code,Code Classification,Code Classification,"Code Classification, Code Generation","Image, Text",English,Computer Vision,code-generation-on-taco-code,Apache-2.0 license,https://github.com/FlagOpen/TACO/tree/main,https://paperswithcode.com/dataset/taco-topics-in-algorithmic-code-generation,"TACO (Topics in Algorithmic COde generation dataset) is a dataset focused on algorithmic code generation, designed to provide a more challenging training dataset and evaluation benchmark for the code generation model field. The dataset consists of programming competition problems that are more difficult and closer to real programming scenarios. It emphasizes improving or evaluating the model's understanding and reasoning abilities in practical application scenarios, rather than just implementing predefined function functionalities.",,,,,,
3101,TACO,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Instance Segmentation, One-Shot Learning",Image,,Computer Vision,,,https://github.com/pedropro/TACO,https://paperswithcode.com/dataset/taco,"TACO is a growing image dataset of waste in the wild. It contains images of litter taken under diverse environments: woods, roads and beaches. These images are manually labelled and segmented according to a hierarchical taxonomy to train and evaluate object detection algorithms. The annotations are provided in COCO format.",,,,,,
3102,TACoS_Multi-Level_Corpus,Activity Recognition,Activity Recognition,"Activity Recognition, Natural Language Moment Retrieval, Video Description","Image, Text, Video",English,Computer Vision,natural-language-moment-retrieval-on-tacos,,http://www.mpi-inf.mpg.de/tacos,https://paperswithcode.com/dataset/tacos-multi-level-corpus,Augments the video-description dataset TACoS with short and single sentence descriptions.,,,,,,
3103,TACRED-Revisited,Relation Classification,Relation Classification,"Relation Classification, Relation Extraction","Graph, Image",,Computer Vision,relation-extraction-on-tacred-revisited,LDC,https://github.com/DFKI-NLP/tacrev,https://paperswithcode.com/dataset/tacred-revisited,"The TACRED-Revisited dataset improves the crowd-sourced TACRED dataset for relation extraction by relabeling the dev and test sets using expert linguistic annotators. Relabeling focuses on the 5K most challenging instances in dev and test, in total, 51.2% of these are corrected. Published at ACL 2020.  

Paper (arXiv): https://arxiv.org/abs/2004.14855",2020,LDC,https://catalog.ldc.upenn.edu/license/ldc-non-members-agreement.pdf,,,
3104,TACRED,Relation Classification,Relation Classification,"Relation Classification, Relation Extraction","Graph, Image",,Computer Vision,"relation-classification-on-tacred-1, relation-extraction-on-tacred",,https://nlp.stanford.edu/projects/tacred/,https://paperswithcode.com/dataset/tacred,"TACRED is a large-scale relation extraction dataset with 106,264 examples built over newswire and web text from the corpus used in the yearly TAC Knowledge Base Population (TAC KBP) challenges. Examples in TACRED cover 41 relation types as used in the TAC KBP challenges (e.g., per:schools_attended and org:members) or are labeled as no_relation if no defined relation is held. These examples are created by combining available human annotations from the TAC KBP challenges and crowdsourcing.

Source: https://nlp.stanford.edu/projects/tacred/",,,,264 examples,,
3105,TAC_2010,Entity Disambiguation,Entity Disambiguation,Entity Disambiguation,,,Methodology,entity-disambiguation-on-tac2010,,https://tac.nist.gov//2010/Summarization/Guided-Summ.2010.guidelines.html,https://paperswithcode.com/dataset/tac-2010,"TAC 2010 is a dataset for summarization that consists of 44 topics, each of which is associated with a set of 10 documents. The test dataset is composed of approximately 44 topics, divided into five categories: Accidents and Natural Disasters, Attacks, Health and Safety, Endangered Resources, Investigations and Trials.",2010,Better Summarization Evaluation with Word Embeddings for ROUGE,https://arxiv.org/abs/1508.06034,10 documents,,
3106,Taiga_Corpus,Author Attribution,Author Attribution,"Author Attribution, Text Generation, Topic Classification, Chatbot, Reading Comprehension, Text Classification, Constituency Parsing","Image, Text",English,Computer Vision,,Apache-2.0 License,https://tatianashavrina.github.io/taiga_site/,https://paperswithcode.com/dataset/taiga-corpus,"Taiga is a corpus, where text sources and their meta-information are collected according to popular ML tasks.

Each text in corpus is represented in plain text and with morphological and syntactic annotation (UDPipe, homonymy resolved automatically) + has metainformation - date, theme, authorship, text difficulcy…etc (depending on source)

By now, about 5 billions of words are 77% literary texts (33 literary magazines), 19% of naive poetry, 2% of news (4 popular sites) and 2% of other (popular science, culture mags, social networks, amateur poems and prose), with documentation available.

Segments Info

| Genres           | Tokens, millions | %   |
|------------------|------------------|-----|
| News             | 92               | 1.5 |
| Literary Texts   | 4605             | 76  |
| Special datasets | 2.5              | 0.5 |
| Social media     | 80               | 1.5 |
| Subtitles        | 101              | 1.5 |
| Poems            | 1130             | 19  |

Annotation Example 
(CONLL-u):
```

newdoc
newpar
sent_id = 1
2003Armeniya.xml 1
text = В советский период времени число ИТ- специалистов в Армении составляло около десяти тысяч.
sent_id = 1
1   В   в   ADP _   _   3   case    3:case  _
2   советский   советский   ADJ _   Animacy=Inan|Case=Acc|Degree=Pos|Gender=Masc|Number=Sing    3   amod    3:amod  _
3   период  период  NOUN    _   Animacy=Inan|Case=Acc|Gender=Masc|Number=Sing   11  obl 11:obl  _
4   времени время   NOUN    _   Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing   3   nmod    3:nmod  _
5   число   число   NOUN    _   Animacy=Inan|Case=Acc|Gender=Neut|Number=Sing   11  obj 11:obj  _
6   ИТ  ит  PROPN   _   Animacy=Inan|Case=Nom|Gender=Neut|Number=Sing   8   compound    8:compound  SpaceAfter=No
7   -   -   PUNCT   _   _   6   punct   6:punct _
8   специалистов    специалист  NOUN    _   Animacy=Anim|Case=Gen|Gender=Masc|Number=Plur   5   nmod    5:nmod  _
9   в   в   ADP _   _   10  case    10:case _
10  Армении армения PROPN   _   Animacy=Inan|Case=Loc|Gender=Fem|Number=Sing    5   nmod    5:nmod  _
11  составляло  составлять  VERB    _   Aspect=Imp|Gender=Neut|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act   0   root    0:root  _
12  около   около   ADP _   _   14  case    14:case _
13  десяти  десять  NUM _   Case=Gen    14  nummod  14:nummod   _
14  тысяч   тысяча  NOUN    _   Animacy=Inan|Case=Gen|Gender=Fem|Number=Plur    11  nsubj   11:nsubj    SpaceAfter=No
15  .   .   PUNCT   _   _   14  punct   14:punct    _

```",,,,,,
3107,Talk2Car,Referring Expression Comprehension,Referring Expression Comprehension,"Referring Expression Comprehension, Autonomous Vehicles, Self-Driving Cars, Autonomous Driving",,,Methodology,referring-expression-comprehension-on-2,,https://github.com/talk2car/Talk2Car,https://paperswithcode.com/dataset/talk2car,"The Talk2Car dataset finds itself at the intersection of various research domains, promoting the development of cross-disciplinary solutions for improving the state-of-the-art in grounding natural language into visual space. The annotations were gathered with the following aspects in mind:
Free-form high quality natural language commands, that stimulate the development of solutions that can operate in the wild.
A realistic task setting. Specifically, the authors consider an autonomous driving setting, where a passenger can control the actions of an Autonomous Vehicle by giving commands in natural language.
The Talk2Car dataset was build on top of the nuScenes dataset to include an extensive suite of sensor modalities, i.e. semantic maps, GPS, LIDAR, RADAR and 360-degree RGB images annotated with 3D bounding boxes. Such variety of input modalities sets the object referral task on the Talk2Car dataset apart from related challenges, where additional sensor modalities are generally missing.",,,,,,
3108,TalkDown,Abusive Language,Abusive Language,Abusive Language,Text,English,Natural Language Processing,,,https://github.com/zijwang/talkdown,https://paperswithcode.com/dataset/talkdown,"TalkDown is a labelled dataset for condescension detection in context. The dataset is derived from Reddit, a set of online communities that is diverse in content and tone. The dataset is built from COMMENT and REPLY pairs in which the REPLY targets a specific quoted span (QUOTED) in the COMMENT as being condescending. The dataset contains 3,255 positive (condescend) samples and 3,255 negative ones.",,https://arxiv.org/pdf/1909.11272.pdf,https://arxiv.org/pdf/1909.11272.pdf,,,
3109,Talking_With_Hands_16.2M,Gesture Generation,Gesture Generation,Gesture Generation,Text,English,Natural Language Processing,,Attribution-NonCommercial 4.0 International,https://github.com/facebookresearch/TalkingWithHands32M,https://paperswithcode.com/dataset/talking-with-hands-16-2m,"This is a 16.2-million frame (50-hour) multimodal dataset of two-person face-to-face spontaneous conversations. This dataset features synchronized body and finger motion as well as audio data. It represents the largest motion capture and audio dataset of natural conversations to date. The statistical analysis verifies strong intraperson and interperson covariance of arm, hand, and speech features, potentially enabling new directions on data-driven social behavior analysis, prediction, and synthesis.",,,,,,
3110,Talk_the_Walk,Vision-Language Navigation,Vision-Language Navigation,"Vision-Language Navigation, Vision and Language Navigation, Visual Navigation","Image, Text",English,Computer Vision,,CC-BY-NC,https://github.com/facebookresearch/talkthewalk,https://paperswithcode.com/dataset/talk-the-walk,"Talk The Walk is a large-scale dialogue dataset grounded in
action and perception. The task involves two agents (a “guide” and a “tourist”)
that communicate via natural language in order to achieve a common goal: having
the tourist navigate to a given target location.",,,,,,
3111,TaL_Corpus,Speech Synthesis,Speech Synthesis,"Speech Synthesis, Speech Recognition","Audio, Image, Text",English,Speech,,CC-BY-NC 4.0,https://ultrasuite.github.io/data/tal_corpus/,https://paperswithcode.com/dataset/tal-corpus,"The Tongue and Lips (TaL) corpus is a multi-speaker corpus of ultrasound images of the tongue and video images of lips. This corpus contains synchronised imaging data of extraoral (lips) and intraoral (tongue) articulators from 82 native speakers of English.

The TaL corpus consists of two datasets:



TaL1 is a single-speaker dataset containing data of one professional voice talent, a male native speaker of English, over six recording sessions.



TaL80 is a multi-speaker dataset contains recording sessions of 81 native speakers of English without voice talent experience. Each speaker was recording over a single recording session.",,,,,,
3112,Tamil_Alpaca,Text Generation,Text Generation,"Text Generation, Instruction Following",Text,English,Natural Language Processing,,gpl-3.0,https://huggingface.co/datasets/abhinand/tamil-alpaca,https://paperswithcode.com/dataset/tamil-alpaca,"Dataset Card for ""tamil-alpaca""

This repository includes a Tamil-translated version of the Alpaca dataset. 

This dataset is part of the release of Tamil LLaMA family of models – an important step in advancing LLMs for the Tamil language. To dive deep into the development and capabilities of this model, please read the research paper and the introductory blog post (WIP)  that outlines our journey and the model's potential impact.

GitHub Repository: https://github.com/abhinand5/tamil-llama

Models trained using this dataset
| Model                    | Type                        | Data              | Base Model           | # Params | Download Links                                                         |
|--------------------------|-----------------------------|-------------------|----------------------|------|------------------------------------------------------------------------|
| Tamil LLaMA 7B Instruct  | Instruction following model | 145k instructions | Tamil LLaMA 7B Base  | 7B   | HF Hub |
| Tamil LLaMA 13B Instruct | Instruction following model | 145k instructions | Tamil LLaMA 13B Base | 13B  | HF Hub                       |

Meet the Developers
Get to know the creators behind this innovative model and follow their contributions to the field:


Abhinand Balachandran

Citation
If you use this model or any of the the Tamil-Llama datasets in your research, please cite:

bibtex
@misc{balachandran2023tamilllama,
      title={Tamil-Llama: A New Tamil Language Model Based on Llama 2}, 
      author={Abhinand Balachandran},
      year={2023},
      eprint={2311.05845},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}",2023,research paper,https://arxiv.org/abs/2311.05845,,,
3113,Tamil_Alpaca_Orca,Text Generation,Text Generation,"Text Generation, Instruction Following",Text,English,Natural Language Processing,,gpl-3.0,https://huggingface.co/datasets/abhinand/tamil-alpaca-orca,https://paperswithcode.com/dataset/tamil-alpaca-orca,"Dataset Card for ""tamil-alpaca""
This repository includes a Tamil-translated versions of the Alpaca dataset and a subset of OpenOrca dataset. 

This dataset is part of the release of Tamil LLaMA family of models – an important step in advancing LLMs for the Tamil language. To dive deep into the development and capabilities of this model, please read the research paper and the introductory blog post (WIP)  that outlines our journey and the model's potential impact.

GitHub Repository: https://github.com/abhinand5/tamil-llama

Models trained using this dataset
| Model                    | Type                        | Data              | Base Model           | # Params | Download Links                                                         |
|--------------------------|-----------------------------|-------------------|----------------------|------|------------------------------------------------------------------------|
| Tamil LLaMA 7B Instruct  | Instruction following model | 145k instructions | Tamil LLaMA 7B Base  | 7B   | HF Hub |
| Tamil LLaMA 13B Instruct | Instruction following model | 145k instructions | Tamil LLaMA 13B Base | 13B  | HF Hub                       |

Meet the Developers
Get to know the creators behind this innovative model and follow their contributions to the field:


Abhinand Balachandran

Citation
If you use this model or any of the the Tamil-Llama datasets in your research, please cite:

bibtex
@misc{balachandran2023tamilllama,
      title={Tamil-Llama: A New Tamil Language Model Based on Llama 2}, 
      author={Abhinand Balachandran},
      year={2023},
      eprint={2311.05845},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}",2023,research paper,https://arxiv.org/abs/2311.05845,,,
3114,Tamil_Memes,Meme Classification,Meme Classification,Meme Classification,Image,,Computer Vision,meme-classification-on-tamil-memes,,https://zenodo.org/record/4765573,https://paperswithcode.com/dataset/tamil-memes,"Social media are interactive platforms that facilitate the creation or sharing of information, ideas or other forms of expression among people. This exchange is not free from offensive, trolling or malicious contents targeting users or communities. One way of trolling is by making memes, which in most cases combines an image with a concept or catchphrase. The challenge of dealing with memes is that they are region-specific and their meaning is often obscured in humour or sarcasm. To facilitate the computational modelling of trolling in the memes for Indian languages, we created a meme dataset for Tamil (TamilMemes). We annotated and released the dataset containing suspected trolls and not-troll memes. In this paper, we use the a image classification to address the difficulties involved in the classification of troll memes with the existing methods. We found that the identification of a troll meme with such an image classifier is not feasible which has been corroborated with precision, recall and F1-score.",,,,,,
3115,TAMPAR,Object Detection,Object Detection,"Object Detection, Change Detection, Instance Segmentation, Keypoint Detection",Image,,Computer Vision,,Creative Commons Attribution 4.0 International,https://zenodo.org/records/10057090,https://paperswithcode.com/dataset/tampar,"TAMPAR is a real-world dataset of parcel photos for tampering detection with annotations in COCO format. For details see the paper and for visual samples the project page. Features are: 




900 annotated real-world images with >2,700 visible parcel side surfaces



6 different tampering types
6 different distortion strengths",,paper,https://arxiv.org/abs/2311.03124,,,
3116,TAO,Robot Navigation,Robot Navigation,"Robot Navigation, Object Tracking, Multi-Object Tracking","Image, Video",,Computer Vision,multi-object-tracking-on-tao,,http://taodataset.org/,https://paperswithcode.com/dataset/tao,"TAO is a federated dataset for Tracking Any Object, containing 2,907 high resolution videos, captured in diverse environments, which are half a minute long on average. A bottom-up approach was used for discovering a large vocabulary of 833 categories, an order of magnitude more than prior tracking benchmarks. 

The dataset was annotated by labelling tracks for objects that move at any point in the video, and giving names to them post factum.",,,,,,833
3117,Taobao__TGN_Style_,Dynamic Link Prediction,Dynamic Link Prediction,Dynamic Link Prediction,Time Series,,Methodology,,CC BY-NC-SA 4.0,https://www.kaggle.com/datasets/chenxi1228/taobao-tgn-style,https://paperswithcode.com/dataset/taobao-tgn-style,Taobao dataset which is pre-processed in TGN Style.,,,,,,
3118,TAP-Vid,Point Tracking,Point Tracking,"Point Tracking, Visual Tracking","Image, Video",,Computer Vision,point-tracking-on-tap-vid,Creative Commons Attribution 4.0 International,,https://paperswithcode.com/dataset/tap-vid,"TAP-Vid is a benchmark which contains both real-world videos with accurate human annotations of point tracks, and synthetic videos with perfect ground-truth point tracks. This is designed for a new task called tracking any point.",,,,,,
3119,TAPOS,Generic Event Boundary Detection,Generic Event Boundary Detection,"Generic Event Boundary Detection, Action Parsing, Action Recognition","Image, Text, Video",English,Computer Vision,generic-event-boundary-detection-on-tapos,,https://sdolivia.github.io/TAPOS/,https://paperswithcode.com/dataset/tapos,"TAPOS is a new dataset developed on sport videos with manual annotations of sub-actions, and conduct a study on temporal action parsing on top. A sport activity usually consists of multiple sub-actions and that the awareness of such temporal structures is beneficial to action recognition.

TAPOS contains 16,294 valid instances in total, across 21 action classes. These instances have a duration of 9.4
seconds on average. The number of instances within each class is different, where the largest class high jump has over
1,600 instances, and the smallest class beam has 200 instances. The average number of sub-actions also varies
from class to class, where parallel bars has 9 sub-actions on average, and long jump has 3 sub-actions on average. All instances are split into train, validation and test sets, of sizes 13094, 1790, and 1763, respectively.",,,,600 instances,,
3120,TAPVid-3D__A_Benchmark_for_Tracking_Any_Point_in_3,Point Tracking,Point Tracking,Point Tracking,"Image, Video",,Computer Vision,,"Apache 2.0, with additional restrictions",https://tapvid3d.github.io/,https://paperswithcode.com/dataset/tapvid-3d-a-benchmark-for-tracking-any-point,"TAPVid-3D is a dataset and benchmark for evaluating the task of long-range Tracking Any Point in 3D (TAP-3D). The dataset consists of 4,000+ real-world videos and 2.1 million metric 3D point trajectories, spanning a variety of object types, motion patterns, and indoor and outdoor environments.",,,,,,
3121,TartanAir,Visual Odometry,Visual Odometry,"Visual Odometry, Multi-Task Learning",Image,,Computer Vision,,,http://theairlab.org/tartanair-dataset/,https://paperswithcode.com/dataset/tartanair,"A dataset for robot navigation task and more. The data is collected in photo-realistic simulation environments in the presence of various light conditions, weather and moving objects.",,,,,,
3122,TASD,Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),Text,English,Natural Language Processing,aspect-based-sentiment-analysis-absa-on-tasd,,https://github.com/sysulic/TAS-BERT,https://paperswithcode.com/dataset/tasd,"Aspect-based sentiment analysis (ABSA) aims to detect the targets (which are composed by continuous words), aspects and sentiment polarities in text. Published datasets from SemEval-2015 and SemEval-2016 reveal that a sentiment polarity depends on both the target and the aspect. However, most of the existing methods consider predicting sentiment polarities from either targets or aspects but not from both, thus they easily make wrong predictions on sentiment polarities. In particular, where the target is implicit, i.e., it does not appear in the given text, the methods predicting sentiment polarities from targets do not work. To tackle these limitations in ABSA, this paper proposes a novel method for target-aspect-sentiment joint detection. It relies on a pre-trained language model and can capture the dependence on both targets and aspects for sentiment prediction. Experimental results on the SemEval-2015 and SemEval-2016 restaurant datasets show that the proposed method achieves a high performance in detecting target-aspect-sentiment triples even for the implicit target cases; moreover, it even outperforms the state-of-the-art methods for those subtasks of target-aspect-sentiment detection that they are competent to.",2015,,,,,
3123,Task2Dial,Task-Oriented Dialogue Systems,Task-Oriented Dialogue Systems,Task-Oriented Dialogue Systems,,,Methodology,,CC,https://huggingface.co/datasets/cstrathe435/Task2Dial,https://paperswithcode.com/dataset/task2dial,"A novel dataset of document-grounded task-based dialogues, where an Information Giver (IG) provides instructions (by consulting a document) to an Information Follower (IF), so that the latter can successfully complete the task. In this unique setting, the IF can ask clarification questions which may not be grounded in the underlying document and require commonsense knowledge to be answered.",,,,,,
3124,Taskmaster-1,Language Modelling,Language Modelling,"Language Modelling, Speech Recognition, Dialogue State Tracking","Audio, Image, Text, Video",English,Computer Vision,,CC BY-SA 4.0,https://research.google/tools/datasets/taskmaster-1/,https://paperswithcode.com/dataset/taskmaster-1,"Taskmaster-1 is a dialog dataset consisting of 13,215 task-based dialogs in English, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations.",,https://arxiv.org/pdf/1909.05358v1.pdf,https://arxiv.org/pdf/1909.05358v1.pdf,,,
3125,Taskography,Robot Task Planning,Robot Task Planning,Robot Task Planning,,,Methodology,,CC BY,https://github.com/taskography/pddlgym,https://paperswithcode.com/dataset/taskography,PDDL dataset of Rearrangement tasks in large-scale 3D scene graphs.,,Rearrangement,https://arxiv.org/abs/2011.01975,,,
3126,Taskonomy,Surface Normals Estimation,Surface Normals Estimation,"Surface Normals Estimation, Depth Estimation",3D,,Methodology,"surface-normals-estimation-on-taskonomy, depth-estimation-on-taskonomy",,http://taskonomy.stanford.edu/,https://paperswithcode.com/dataset/taskonomy,"Taskonomy provides a large and high-quality dataset of varied indoor scenes.


Complete pixel-level geometric information via aligned meshes.
Semantic information via knowledge distillation from ImageNet, MS COCO, and MIT Places.
Globally consistent camera poses. Complete camera intrinsics.
High-definition images.
3x times big as ImageNet.",,,,,,
3127,Tasksource,Multiple-choice,Multiple-choice,"Multiple-choice, Natural Language Inference, Multi-task Language Understanding, Multi-Task Learning, Classification","Image, Text",English,Computer Vision,,,https://github.com/sileod/tasksource,https://paperswithcode.com/dataset/tasksource,"Huggingface Datasets is a great library, but it lacks standardization, and datasets require preprocessing work to be used interchangeably. tasksource automates this and facilitates reproducible multi-task learning scaling.

Each dataset is standardized to either MultipleChoice, Classification, or TokenClassification dataset with identical fields. We do not support generation tasks as they are addressed by promptsource. All implemented preprocessings are in tasks.py or tasks.md. A preprocessing is a function that accepts a dataset and returns the standardized dataset. Preprocessing code is concise and human-readable.",,,,,,
3128,TAT,Speech-to-Speech Translation,Speech-to-Speech Translation,"Speech-to-Speech Translation, Voice Query Recognition","Audio, Image, Text",English,Speech,speech-to-speech-translation-on-tat,CC BY-NC 4.0,https://sites.google.com/nycu.edu.tw/speechlabx/tat_s2st_benchmark?authuser=0,https://paperswithcode.com/dataset/tat,"Taiwanese Across Taiwan (TAT) corpus is a Large-Scale database of Native Taiwanese Article/Reading Speech collected across Taiwan. This corpus contains native Taiwanese speech of various accent across Taiwan. The corpus is annotated twice for use in voice recognition research. The corpus contains recording from 100 native speakers, each with length of 30 minutes making a total of 100 hours of speech data.",,,,,,
3129,Tatoeba,Translation fry-eng,Translation fry-eng,"Translation fry-eng, Language Modelling, Translation deu-eng, Translation nds-eng, Translation eng-spa, Translation afr-nld, Translation eng-tur, Translation afr-eng, Translation hrx-deu, Translation afr-deu, Translation deu-nld, Translation nld-afr, Translation ara-eng, Translation nld-deu, Cross-Lingual Transfer, Translation nds-nld, Translation eng-afr, Translation nld-fry, Translation nld-eng, Translation eng-ara, Translation hrx-eng, Sentence Embedding, Translation eng-nld, Translation nds-deu, Translation deu-afr, Translation ltz-eng, Translation tur-eng, Translation fry-nld, Translation ltz-deu, Translation eng-deu, Translation nld-nds, Translation ltz-nld, Machine Translation",Text,English,Natural Language Processing,"translation-hrx-eng-on-tatoeba-test-v2021-08, translation-nld-deu-on-tatoeba-test-v2021-08, translation-tur-eng-on-tatoeba-test-v2021-08, translation-ara-eng-on-tatoeba-test-v2021-08, translation-eng-deu-on-tatoeba-test-v2021-08, translation-ltz-nld-on-tatoeba-test-v2021-08, machine-translation-on-tatoeba-en-to-el, translation-nds-nld-on-tatoeba-test-v2021-08, translation-deu-eng-on-tatoeba-test-v2021-08, translation-nld-fry-on-tatoeba-test-v2021-08, translation-eng-tur-on-tatoeba-test-v2021-08, translation-deu-nld-on-tatoeba-test-v2021-08, translation-fry-eng-on-tatoeba-test-v2021-08, translation-afr-deu-on-tatoeba-test-v2021-08, translation-afr-eng-on-tatoeba-test-v2021-08, translation-nld-afr-on-tatoeba-test-v2021-08, translation-eng-spa-on-tatoeba-test-v2021-08, translation-ltz-eng-on-tatoeba-test-v2021-08, translation-eng-nld-on-tatoeba-test-v2021-08, translation-deu-afr-on-tatoeba-test-v2021-08, translation-nds-deu-on-tatoeba-test-v2021-08, translation-nds-eng-on-tatoeba-test-v2021-08, machine-translation-on-tatoeba-el-to-en, translation-eng-ara-on-tatoeba-test-v2020-07, translation-eng-afr-on-tatoeba-test-v2021-08, translation-afr-nld-on-tatoeba-test-v2021-08, translation-fry-nld-on-tatoeba-test-v2021-08, translation-nld-nds-on-tatoeba-test-v2021-08, translation-ltz-deu-on-tatoeba-test-v2021-08, translation-nld-eng-on-tatoeba-test-v2021-08, translation-hrx-deu-on-tatoeba-test-v2021-08",CC–BY 2.0 FR,https://tatoeba.org,https://paperswithcode.com/dataset/tatoeba,"Tatoeba is a free collection of example sentences with translations geared towards foreign language learners. It is available in more than 400 languages. Its name comes from the Japanese phrase “tatoeba” (例えば), meaning “for example”. It is written and maintained by a community of volunteers through a model of open collaboration. Individual contributors are known as Tatoebans.",,,,,,
3130,TAU-NIGENS_Spatial_Sound_Events_2020,Sound Event Localization and Detection,Sound Event Localization and Detection,"Sound Event Localization and Detection, Sound Event Detection, Direction of Arrival Estimation","Audio, Image",,Computer Vision,,,https://zenodo.org/record/4064792,https://paperswithcode.com/dataset/tau-nigens-spatial-sound-events-2020,"The TAU-NIGENS Spatial Sound Events 2020 dataset contains multiple spatial sound-scene recordings, consisting of sound events of distinct categories integrated into a variety of acoustical spaces, and from multiple source directions and distances as seen from the recording position. The spatialization of all sound events is based on filtering through real spatial room impulse responses (RIRs), captured in multiple rooms of various shapes, sizes, and acoustical absorption properties. Furthermore, each scene recording is delivered in two spatial recording formats, a microphone array one (MIC), and first-order Ambisonics one (FOA). The sound events are spatialized as either stationary sound sources in the room, or moving sound sources, in which case time-variant RIRs are used. Each sound event in the sound scene is associated with a trajectory of its direction-of-arrival (DoA) to the recording point, and a temporal onset and offset time. The isolated sound event recordings used for the synthesis of the sound scenes are obtained from the NIGENS general sound events database. These recordings serve as the development dataset for the DCASE 2020 Sound Event Localization and Detection Task of the DCASE 2020 Challenge.",2020,,,,,
3131,TAU-NIGENS_Spatial_Sound_Events_2021,Sound Event Localization and Detection,Sound Event Localization and Detection,"Sound Event Localization and Detection, Sound Event Detection, Audio Classification, Direction of Arrival Estimation","Audio, Image",,Computer Vision,sound-event-localization-and-detection-on-tau-1,,https://zenodo.org/record/4844825#.YNv3h-gzZPY,https://paperswithcode.com/dataset/tau-nigens-spatial-sound-events-2021,"The TAU-NIGENS Spatial Sound Events 2021 dataset contains multiple spatial sound-scene recordings, consisting of sound events of distinct categories integrated into a variety of acoustical spaces, and from multiple source directions and distances as seen from the recording position. The spatialization of all sound events is based on filtering through real spatial room impulse responses (RIRs), captured in multiple rooms of various shapes, sizes, and acoustical absorption properties. Furthermore, each scene recording is delivered in two spatial recording formats, a microphone array one (MIC), and first-order Ambisonics one (FOA). The sound events are spatialized as either stationary sound sources in the room, or moving sound sources, in which case time-variant RIRs are used. Each sound event in the sound scene is associated with a single direction-of-arrival (DoA) if static, a trajectory DoAs if moving, and a temporal onset and offset time. The isolated sound event recordings used for the synthesis of the sound scenes are obtained from the NIGENS general sound events database. These recordings serve as the development dataset for the DCASE 2021 Sound Event Localization and Detection Task of the DCASE 2021 Challenge.",2021,,,,,
3132,TAU_Urban_Acoustic_Scenes_2019,Keyword Spotting,Keyword Spotting,"Keyword Spotting, Image Classification, Scene Classification, Acoustic Scene Classification","Audio, Image",,Computer Vision,"keyword-spotting-on-tau-urban-acoustic-scenes, acoustic-scene-classification-on-tau-urban",Other (Non-Commercial),https://zenodo.org/record/2589280,https://paperswithcode.com/dataset/tau-urban-acoustic-scenes-2019,"TAU Urban Acoustic Scenes 2019 development dataset consists of 10-seconds audio segments from 10 acoustic scenes: airport, indoor shopping mall, metro station, pedestrian street, public square, street with medium level of traffic, travelling by a tram, travelling by a bus, travelling by an underground metro and urban park. Each acoustic scene has 1440 segments (240 minutes of audio). The dataset contains in total 40 hours of audio.",2019,,,,,
3133,TaxiBJ,Fine-Grained Urban Flow Inference,Fine-Grained Urban Flow Inference,Fine-Grained Urban Flow Inference,,,Methodology,"fine-grained-urban-flow-inference-on-taxibj-1, fine-grained-urban-flow-inference-on-taxibj-3, fine-grained-urban-flow-inference-on-taxibj-2, fine-grained-urban-flow-inference-on-taxibj",,https://github.com/TolicWang/DeepST/tree/master/data/TaxiBJ,https://paperswithcode.com/dataset/taxibj,"TaxiBJ consists of trajectory data from taxicab GPS data and meteorology data in Beijing from four time intervals: 1st Jul. 2013 - 30th Otc. 2013, 1st Mar. 2014 - 30th Jun. 2014, 1st Mar. 2015 - 30th Jun. 2015, 1st Nov. 2015 - 10th Apr. 2016.",2013,https://arxiv.org/pdf/1701.02543.pdf,https://arxiv.org/pdf/1701.02543.pdf,,,
3134,TbD-3D,Video Super-Resolution,Video Super-Resolution,Video Super-Resolution,Video,,Methodology,video-super-resolution-on-tbd-3d,,https://github.com/rozumden/fmo-deblurring-benchmark,https://paperswithcode.com/dataset/tbd-3d,,,,,,,
3135,TbD,Video Super-Resolution,Video Super-Resolution,Video Super-Resolution,Video,,Methodology,video-super-resolution-on-tbd,,https://github.com/rozumden/fmo-deblurring-benchmark,https://paperswithcode.com/dataset/tbd,,,,,,,
3136,TCAB,Sentiment Analysis,Sentiment Analysis,"Sentiment Analysis, Adversarial Attack, Text Classification","Image, Text",English,Computer Vision,,Apache-2.0 license,https://github.com/react-nlp/tcab_generation,https://paperswithcode.com/dataset/tcab,"Text Classification Attack Benchmark (TCAB) is a dataset for analyzing, understanding, detecting, and labeling adversarial attacks against text classifiers. TCAB includes 1.5 million attack instances, generated by twelve adversarial attack targeting three classifiers trained on six source datasets for sentiment analysis and abuse detection in English. The process of generating attacks is automated, so that TCAB can easily be extended to incorporate new text attacks and better classifiers as they are developed.",,TCAB: A Large-Scale Text Classification Attack Benchmark,https://arxiv.org/pdf/2210.12233v1.pdf,,,
3137,TCGA,Cancer type classification,Cancer type classification,"Cancer type classification, Classification, Multiple Instance Learning",Image,,Computer Vision,"multiple-instance-learning-on-tcga, classification-on-tcga, cancer-type-classification-on-tcga",,https://portal.gdc.cancer.gov/,https://paperswithcode.com/dataset/tcga,,,,,,,
3138,tdcommons,TDC ADMET Benchmarking Group,TDC ADMET Benchmarking Group,TDC ADMET Benchmarking Group,,,Methodology,tdc-admet-benchmarking-group-on-tdcommons,MIT,https://tdcommons.ai,https://paperswithcode.com/dataset/tdcommons,"Therapeutics Data Commons is an open-science initiative with AI/ML-ready datasets and AI/ML tasks for therapeutics, spanning the discovery and development of safe and effective medicines. TDC provides an ecosystem of tools, libraries, leaderboards, and community resources, including data functions, strategies for systematic model evaluation, meaningful data splits, data processors, and molecule generation oracles. All resources are integrated via an open Python library.",,,,,,
3139,TE141K,Text Effects Transfer,Text Effects Transfer,"Text Effects Transfer, Style Transfer",Text,English,Natural Language Processing,,,https://daooshee.github.io/TE141K/,https://paperswithcode.com/dataset/te141k,"A new text effects dataset with 141,081 text effect/glyph pairs in total. The dataset consists of 152 professionally designed text effects rendered on glyphs, including English letters, Chinese characters, and Arabic numerals.",,,,,,
3140,TED_Gesture_Dataset,Gesture Generation,Gesture Generation,Gesture Generation,Text,English,Natural Language Processing,gesture-generation-on-ted-gesture-dataset,BSD-3,https://github.com/youngwoo-yoon/youtube-gesture-dataset,https://paperswithcode.com/dataset/ted-gesture-dataset,"Co-speech gestures are everywhere. People make gestures
when they chat with others, give a public speech, talk on a
phone, and even think aloud. Despite this ubiquity, there are
not many datasets available. The main reason is that it is
expensive to recruit actors/actresses and track precise body
motions. There are a few datasets available (e.g., MSP
AVATAR [17] and Personality Dyads Corpus [18]), but their
sizes are limited to less than 3 h, and they lack diversity in
speech content and speakers. The gestures also could be
unnatural owing to inconvenient body tracking suits and acting
in a lab environment.

Thus, we collected a new dataset of co-speech gestures: the
TED Gesture Dataset. TED is a conference where people share
their ideas from a stage, and recordings of these talks are
available online. Using TED talks has the following
advantages compared to the existing datasets:

• Large enough to learn the mapping from speech to
gestures. The number of videos continues to grow.
• Various speech content and speakers. There are
thousands of unique speakers, and they talk about their
own ideas and stories.
• The speeches are well prepared, so we expect that the
speakers use proper hand gestures.
• Favorable for automation of data collection and
annotation. All talks come with transcripts, and flat
background and steady shots make extracting human
poses with computer vision technology easier.",,,,,,
3141,Teeth3DS_,3D Classification,3D Classification,"3D Classification, 3D Point Cloud Classification, 3D Part Segmentation","3D, Image",,Computer Vision,,CC BY-NC-ND 4.0,https://crns-smartvision.github.io/teeth3ds,https://paperswithcode.com/dataset/teeth3ds,"Intraoral 3D scans analysis is a fundamental aspect of Computer-Aided Dentistry (CAD) systems,
playing a crucial role in various dental applications, including teeth segmentation, detection, labeling,
and dental landmark identification. Accurate analysis of 3D dental scans is essential for orthodontic and prosthetic treatment planning, as it enables automated processing and reduces the need for
manual adjustments by dental professionals. However, developing robust automated tools for these
tasks remains a significant challenge due to the limited availability of high-quality public datasets and
benchmarks. This article introduces Teeth3DS+, the first comprehensive public benchmark designed
to advance the field of intraoral 3D scan analysis. Developed as part of the 3DTeethSeg 2022 and
3DTeethLand 2024 MICCAI challenges, Teeth3DS+ aims to drive research in teeth identification, segmentation, labeling, 3D modeling, and dental landmarks identification. The dataset includes at least
1,800 intraoral scans (containing 23,999 annotated teeth) collected from 900 patients, covering both
upper and lower jaws separately. All data have been acquired and validated by experienced orthodontists and dental surgeons with over five years of expertise. Detailed instructions for accessing the
dataset are available at https://crns-smartvision.github.io/teeth3ds",2022,,,,,
3142,TekGen,Joint Entity and Relation Extraction,Joint Entity and Relation Extraction,Joint Entity and Relation Extraction,Graph,,Methodology,joint-entity-and-relation-extraction-on-9,CC BY-SA,https://github.com/google-research-datasets/KELM-corpus#part-1-tekgen-training-corpus,https://paperswithcode.com/dataset/tekgen,"The Dataset is part of the KELM corpus

This is the Wikipedia text--Wikidata KG aligned corpus used to train the data-to-text generation model. Please note that this is a corpus generated with distant supervision and should not be used as gold standard for evaluation.

It consists of 3 files:

https://storage.googleapis.com/gresearch/kelm-corpus/updated-2021/quadruples-train.tsv
https://storage.googleapis.com/gresearch/kelm-corpus/updated-2021/quadruples-validation.tsv
https://storage.googleapis.com/gresearch/kelm-corpus/updated-2021/quadruples-test.tsv

Each file contains one example per line. Each example is a json object with three fields:

triples: A list of triples of the form (subject, relation, object). eg. (Person X, award received, Award Y). If the triple has a subproperty, then it is quadruple instead. eg. (Person X, Award Y, received on, Date Z).

serialized triples: triples concatenated together as used for input to T5. The format is ""&lt;subject&gt; &lt;relation&gt; &lt;object&gt;"" where some subjects have multiple relations, e.g. ""&lt;subject&gt; &lt;relation1&gt; &lt;object1&gt; &lt;relation2&gt; &lt;object2&gt; &lt;relation3&gt; &lt;object3&gt;"". For more details on how these relations are grouped, please refer to the paper.

sentence: The wikipedia sentence aligned to these triples.

The names, aliases and Wikidata Ids of the entities can be found in https://storage.googleapis.com/gresearch/kelm-corpus/updated-2021/entities.jsonl.",2021,,,,,
3143,TempEval-3,Temporal Information Extraction,Temporal Information Extraction,"Temporal Information Extraction, Temporal Tagging","Time Series, Video",,Methodology,"temporal-information-extraction-on-tempeval-3, temporal-tagging-on-tempeval-3",CC-BY,https://aclanthology.org/S13-2001/,https://paperswithcode.com/dataset/tempeval-3,"Within the SemEval-2013 evaluation exercise, the TempEval-3 shared task aims to advance research on temporal information processing. It follows on from TempEval-1 and -2, with: a three-part structure covering temporal expression, event, and temporal relation extraction; a larger dataset; and new single measures to rank systems – in each task and in general.

We present TempEval-3 Silver data, with 666K words, and TempEval-3 Platinum, an evaluation set with 6K words. Documents are annotation with EVENT and TIMEX3 spans and also TLINKs, following the TimeML standard.",2013,,,,,
3144,Temporal_Logic_Video__TLV__Dataset,Video Temporal Consistency,Video Temporal Consistency,"Video Temporal Consistency, Temporal Complex Logical Reasoning","Time Series, Video",,Methodology,,MIT,https://github.com/UTAustin-SwarmLab/Temporal-Logic-Video-Dataset,https://paperswithcode.com/dataset/temporal-logic-video-tlv-dataset,"The Temporal Logic Video (TLV) Dataset addresses the scarcity of state-of-the-art video datasets for long-horizon, temporally extended activity and object detection. It comprises two main components:

Synthetic datasets: Generated by concatenating static images from established computer vision datasets (COCO and ImageNet), allowing for the introduction of a wide range of Temporal Logic (TL) specifications.

Real-world datasets: Based on open-source autonomous vehicle (AV) driving datasets, specifically NuScenes and Waymo.",,,,,,
3145,TEP,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Change Point Detection",Image,,Computer Vision,change-point-detection-on-tep,Custom,https://github.com/YKatser/CPDE/tree/master/TEP_data,https://paperswithcode.com/dataset/tep,"The original paper presented a model of the industrial chemical process named Tennessee Eastman Process and a model-based TEP simulator for data generation. The most widely used benchmark consists of 22 datasets, 21 of which (Fault 1–21) contain faults and 1 (Fault 0) is fault-free. It is available in repository. All datasets have training (500 samples) and testing (960 samples) parts: training part has healthy state observations, testing part begins right after training, and contains faults which appear after 8 h since the training part. Each dataset has 52 features or observation variables with a 3 min sampling rate for most of all.",,,,500 samples,training (500 samples,
3146,TexRel,Relation Classification,Relation Classification,"Relation Classification, Visual Relationship Detection, Relational Pattern Learning, Few-Shot Relation Classification, Relation Extraction, Spatial Relation Recognition, Relational Reasoning","Graph, Image",,Computer Vision,,MIT,https://github.com/asappresearch/texrel,https://paperswithcode.com/dataset/texrel,"Green family of datasets for emergent communications on relations.

By comparison with other relations datasets, TexRel provides rapid training and experimentation, whilst being sufficiently large to avoid overfitting in the context of emergent communications.",,,,,,
3147,TextBox_2.0,Text Simplification,Text Simplification,"Text Simplification, Text Summarization, Paraphrase Generation, Text Style Transfer, Text Generation, Question Generation, Question Answering, Machine Translation",Text,English,Natural Language Processing,,MIT,https://github.com/RUCAIBox/TextBox,https://paperswithcode.com/dataset/textbox-2-0,"TextBox 2.0 is a comprehensive and unified library for text generation, focusing on the use of pre-trained language models (PLMs). The library covers 13 common text generation tasks and their corresponding 83 datasets and further incorporates 45 PLMs covering general, translation, Chinese, dialogue, controllable, distilled, prompting, and lightweight PLMs.",,TextBox 2.0: A Text Generation Library with Pre-trained Language Models,https://arxiv.org/pdf/2212.13005v1.pdf,,,
3148,TextCaps,Optical Character Recognition (OCR),Optical Character Recognition (OCR),"Optical Character Recognition (OCR), Visual Reasoning, Image Captioning","Image, Text",English,Computer Vision,image-captioning-on-textcaps-2020,,https://textvqa.org/textcaps,https://paperswithcode.com/dataset/textcaps,"Contains 145k captions for 28k images. The dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects.",,,,28k images,,
3149,TextComplexityDE,Text Simplification,Text Simplification,"Text Simplification, Text Complexity Assessment (GermEval 2022)",Text,English,Natural Language Processing,text-complexity-assessment-germeval-2022-on,,https://drive.google.com/drive/folders/1xStj_KNlapHsgPPECv9SyvwfJCptGDc_,https://paperswithcode.com/dataset/textcomplexityde,"TextComplexityDE is a dataset consisting of 1000 sentences in German language taken from 23 Wikipedia articles in 3 different article-genres to be used for developing text-complexity predictor models and automatic text simplification in German language. The dataset includes subjective assessment of different text-complexity aspects provided by German learners in level A and B. In addition, it contains manual simplification of 250 of those sentences provided by native speakers and subjective assessment of the simplified sentences by participants from the target group. The subjective ratings were collected using both laboratory studies and crowdsourcing approach.",,,,1000 sentences,,
3150,TextOCR,Scene Text Detection,Scene Text Detection,"Scene Text Detection, Optical Character Recognition (OCR), Scene Text Recognition","Image, Text",English,Computer Vision,,CC BY 4.0,https://textvqa.org/textocr,https://paperswithcode.com/dataset/textocr,"TextOCR is a dataset to benchmark text recognition on arbitrary shaped scene-text. TextOCR requires models to perform text-recognition on arbitrary shaped scene-text present on natural images. TextOCR provides ~1M high quality word annotations on TextVQA images allowing application of end-to-end reasoning on downstream tasks such as visual question answering or image captioning.

Dataset statistics:


28,134 natural images from TextVQA
903,069 annotated scene-text words
32 words per image on average",,,,,,
3151,TextSeg,Style Transfer,Style Transfer,"Style Transfer, Text Style Transfer, self-supervised scene text recognition","Image, Text",English,Computer Vision,self-supervised-scene-text-recognition-on-1,,https://github.com/SHI-Labs/Rethinking-Text-Segmentation,https://paperswithcode.com/dataset/textseg,"TextSeg is a large-scale fine-annotated and multi-purpose text detection and segmentation dataset, collecting scene and design text with six types of annotations: word- and character-wise bounding polygons, masks and transcriptions.",,,,,,
3152,TextVQA,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Question Answering, Visual Question Answering","Image, Text",English,Computer Vision,"visual-question-answering-vqa-on-textvqa, visual-question-answering-on-textvqa-test-2, visual-question-answering-on-textvqa-test-1",CC BY 4.0,https://textvqa.org/,https://paperswithcode.com/dataset/textvqa,"TextVQA is a dataset to benchmark visual reasoning based on text in images.
TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions.

Statistics
* 28,408 images from OpenImages
* 45,336 questions
* 453,360 ground truth answers",,,,408 images,,
3153,TextWorld_KG,text-based games,text-based games,"text-based games, Knowledge Graphs",Text,English,Natural Language Processing,,,https://github.com/MikulasZelinka/textworld_kg_dataset,https://paperswithcode.com/dataset/textworld-kg,"TextWorld KG is a dynamic Knowledge Graph (KG) extraction dataset. It is based on a set of text-based games generated using. That framework allows to extract the underlying partial KG for every state, i.e., the subgraph that represents the agent’s partial knowledge of the world – what it has observed so far. All games share the same overarching theme: the agent finds itself hungry in a simple modern house with the goal of gathering ingredients and cooking a meal.",,https://arxiv.org/abs/1910.09532,https://arxiv.org/abs/1910.09532,,,
3154,TextZoom,self-supervised scene text recognition,self-supervised scene text recognition,"self-supervised scene text recognition, Image Super-Resolution, Super-Resolution","Image, Text",English,Computer Vision,self-supervised-scene-text-recognition-on,,https://github.com/JasonBoy1/TextZoom,https://paperswithcode.com/dataset/textzoom,TextZoom is a super-resolution dataset that consists of paired Low Resolution – High Resolution scene text images. The images are captured by cameras with different focal length in the wild.,,,,,,
3155,Text_VPH,Text Categorization,Text Categorization,"Text Categorization, text annotation, Text Classification","Image, Text",English,Computer Vision,,MIT,https://www.kaggle.com/dsv/6460567,https://paperswithcode.com/dataset/comentarios-vacuna-vph,"Este conjunto de datos consiste en comentarios de publicaciones del MINSA (Perú) en Facebook sobre la vacuna contra el VPH entre los años 2019 y 2020. Se leyó cuidadosamente cada uno de los comentarios, luego se procedió a clasificarlos de manera manual. Para esta clasificación se interpretó los mensajes de las personas, por lo que se analizó los hilos (comentarios y respuestas) por separado y se procedió a etiquetarlos por temas ""Topic"" .  Un profesional de salud realizó una segunda clasificación y las discrepancias se resolvieron con un tercer profesional.  Luego, se seleccionaron subcategorías que hacían referencia directa a las vacunas contra el VPH. La clasificación se realizó utilizando las siguientes categorías ""topic_c"" :


0: El comentario tiene una postura contraria a la vacuna contra el VPH (antivacuna)
1: El comentario tien una postura a favor de la vacuna contra el VPH (provacuna)
2: El comentario refleja una duda o dudas relacionada con la vacuna contra el VPH
3: El comentario habla de cualquier otra cosa

Citar:


Lewis De La Cruz, Lucy Cordova, &amp; Esperanza Reyes. (2023). <i>Text_VPH</i> [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/6460567",2019,,,,,
3156,Texygen_Platform,Text Generation,Text Generation,"Text Generation, Adversarial Text, Imitation Learning",Text,English,Natural Language Processing,,,https://github.com/geek-ai/Texygen,https://paperswithcode.com/dataset/texygen-platform,"Texygen is a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and facilitate the sharing of fine-tuned open-source implementations among researchers for their work. As a consequence, this would help in improving the reproductivity and reliability of future research work in text generation.",,,,,,
3157,TFix_s_Code_Patches_Data,Program Repair,Program Repair,"Program Repair, Transfer Learning, Multi-Task Learning, Code Generation",Text,English,Natural Language Processing,program-repair-on-tfix-s-code-patches-data,,https://github.com/eth-sri/TFix,https://paperswithcode.com/dataset/tfix-s-code-patch-data,"The dataset contains more than 100k code patch pairs extracted from open source projects on GitHub. Each pair comes with the erroneous and the fixed version of the corresponding code snippet. Instead of the whole file, the code snippets are extracted to focus on the problematic region (error line + other lines around it). For each sample, the repository name, the commit id, and the file names are provided so that one can access the complete files in case of interest.

The dataset only has JavaScript programs and the error are detected by the popular static code analyzer ESLint. The dataset can be used in the fields of: program repair, code generation, bug finding, transfer learning and many more fields related to machine learning for code",,,,,,
3158,TGIF-QA,TGIF-Frame,TGIF-Frame,"TGIF-Frame, TGIF-Transition, Zero-Shot Video Question Answer, Zeroshot Video Question Answer, TGIF-Action, Question Answering, Video Question Answering, Visual Question Answering (VQA)","Image, Text, Video",English,Computer Vision,"zeroshot-video-question-answer-on-tgif-qa, tgif-transition-on-tgif-qa, zeroshot-video-question-answer-on-tgif-qa-1, tgif-frame-on-tgif-qa, visual-question-answering-on-tgif-qa, video-question-answering-on-tgif-qa, tgif-action-on-tgif-qa",Custom (research-only),https://github.com/YunseokJANG/tgif-qa,https://paperswithcode.com/dataset/tgif-qa,The TGIF-QA dataset contains 165K QA pairs for the animated GIFs from the TGIF dataset [Li et al. CVPR 2016]. The question & answer pairs are collected via crowdsourcing with a carefully designed user interface to ensure quality. The dataset can be used to evaluate video-based Visual Question Answering techniques.,2016,,,,,
3159,THCHS-30,Speech Synthesis,Speech Synthesis,"Speech Synthesis, Speech Recognition, Multi-Task Learning","Audio, Image, Text",English,Speech,,"Custom (research-only, non-commercial)",http://data.cslt.org/thchs30/README.html,https://paperswithcode.com/dataset/thchs-30,"THCHS-30 is a free Chinese speech database
THCHS-30 that can be used to build a full-fledged
Chinese speech recognition system.",,,,,,
3160,TheoremQA,Natural Questions,Natural Questions,"Natural Questions, Question Answering, Mathematical Reasoning",Text,English,Natural Language Processing,natural-questions-on-theoremqa,MIT,https://github.com/wenhuchen/TheoremQA,https://paperswithcode.com/dataset/theoremqa,"We propose the first question-answering dataset driven by STEM theorems. We annotated 800 QA pairs covering 350+ theorems spanning across Math, EE&CS, Physics and Finance. The dataset is collected by human experts with very high quality. We provide the dataset as a new benchmark to test the limit of large language models to apply theorems to solve challenging university-level questions. We provide a pipeline in the following to prompt LLMs and evaluate their outputs with WolframAlpha.",,,,,,
3161,Thermal_Face_Database,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Facial Emotion Recognition, Facial Action Unit Detection, Facial Landmark Detection","Image, Video",,Computer Vision,,BSD-3-Clause license,https://github.com/marcinkopaczka/thermalfaceproject,https://paperswithcode.com/dataset/thermal-face-database,"High-resolution thermal infrared face database with extensive manual annotations, introduced by Kopaczka et al, 2018. Useful for training algoeithms for image processing tasks as well as facial expression recognition. The full database itself, all annotations and the complete source code are freely available from the authors for research purposes at https://github.com/marcinkopaczka/thermalfaceproject.

Please cite following papers for the dataset:
[1] M. Kopaczka, R. Kolk and D. Merhof, ""A fully annotated thermal face database and its application for thermal facial expression recognition,"" 2018 IEEE International Instrumentation and Measurement Technology Conference (I2MTC), 2018, pp. 1-6, doi: 10.1109/I2MTC.2018.8409768.
[2] Kopaczka, M., Kolk, R., Schock, J., Burkhard, F., & Merhof, D. (2018). A thermal infrared face database with facial landmarks and emotion labels. IEEE Transactions on Instrumentation and Measurement, 68(5), 1389-1401.",2018,BSD-3-Clause license,https://transfer.lfb.rwth-aachen.de/IRDatabase/LfB_License.pdf,,,
3162,ThermoScenes,3D Reconstruction,3D Reconstruction,"3D Reconstruction, Infrared And Visible Image Fusion, Novel View Synthesis, Neural Rendering","3D, Image",,Computer Vision,,,https://github.com/SchindlerEPFL/thermo-nerf,https://paperswithcode.com/dataset/thermoscenes,Dataset of paired thermal and RGB images comprising ten diverse scenes—six indoor and four outdoor scenes— for 3D scene reconstruction and novel view synthesis (e.g. with NeRF).,,,,,,
3163,The_BioScope_Corpus,Negation and Speculation Scope resolution,Negation and Speculation Scope resolution,"Negation and Speculation Scope resolution, Negation Scope Resolution, Negation and Speculation Cue Detection, Speculation Scope Resolution",Image,,Computer Vision,"negation-scope-resolution-on-bioscope-full, negation-scope-resolution-on-bioscope, speculation-scope-resolution-on-bioscope-full, speculation-scope-resolution-on-bioscope, negation-and-speculation-cue-detection-on",,https://rgai.inf.u-szeged.hu/node/105,https://paperswithcode.com/dataset/the-bioscope-corpus,"It is a  freely available resource for research on handling negation and uncertainty in biomedical texts . The corpus consists of three parts, namely medical free texts,biological full papers and biological scientific abstracts. The dataset contains annotations at the token level for negative and speculative keywords and at the sentence level for their linguistic scope. The annotation process was carried out by two independent linguist annotators and a chief annotator – also responsible for setting up the annotation guidelines – who resolved cases where the annotators disagreed.",,,,,,
3164,The_China_Physiological_Signal_Challenge_2018,Arrhythmia Detection,Arrhythmia Detection,Arrhythmia Detection,Image,,Computer Vision,arrhythmia-detection-on-the-china-1,,http://2018.icbeb.org/Challenge.html,https://paperswithcode.com/dataset/the-china-physiological-signal-challenge-2018,The China Physiological Signal Challenge 2018 aims to encourage the development of algorithms to identify the rhythm/morphology abnormalities from 12-lead ECGs. The data used in CPSC 2018 include one normal ECG type and eight abnormal types.,2018,,,,,
3165,The_COLOSSEUM,Robot Manipulation Generalization,Robot Manipulation Generalization,Robot Manipulation Generalization,,,Methodology,robot-manipulation-generalization-on-the,,https://robot-colosseum.github.io/,https://paperswithcode.com/dataset/the-colosseum,"To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions. Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup.

We present Colosseum, a novel simulation benchmark,with 20 diverse manipulation tasks, that enables systematical evaluation of models across 12 axes of environmental perturbations. These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, and camera pose. Using Colosseum, we compare 4 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50% across these perturbation factors.

When multiple perturbations are applied in unison, the success rate degrades > 75%. We identify that changing the number of distractor objects, target object color, or lighting conditions are the perturbations that reduce model performance the most. To verify the ecological validity of our results, we show that our results in simulation are correlated (R2 = 0.614) to similar perturbations in real-world experiments. We open source code for others to use Colosseum, and also release code to 3D print the objects used to replicate the real-world perturbations. Ultimately, we hope that Colosseum will serve as a benchmark to identify modeling decisions that systematically improve generalization for manipulation.",,,,,,
3166,The_ComMA_Dataset_v0.2,Abusive Language,Abusive Language,"Abusive Language, Hate Speech Detection, Aggression Identification","Audio, Image, Text",English,Computer Vision,,CC-BY-NC-SA 4.0,https://competitions.codalab.org/competitions/35482,https://paperswithcode.com/dataset/the-comma-dataset-v0-2,"The ComMA Dataset v0.2 is a multilingual dataset annotated with a hierarchical, fine-grained tagset marking different types of aggression and the ""context"" in which they occur. The context, here, is defined by the conversational thread in which a specific comment occurs and also the ""type"" of discursive role that the comment is performing with respect to the previous comment. The initial dataset, being discussed here (and made available as part of the ComMA@ICON shared task), consists of a total 15,000 annotated comments in four languages - Meitei, Bangla, Hindi, and Indian English - collected from various social media platforms such as YouTube, Facebook, Twitter and Telegram. As is usual on social media websites, a large number of these comments are multilingual, mostly code-mixed with English.",,,,,,
3167,The_Game_of_2048,Playing the Game of 2048,Playing the Game of 2048,Playing the Game of 2048,,,Methodology,2048-on-2048,MIT,https://github.com/rgal/gym-2048,https://paperswithcode.com/dataset/2048,The 2048 game task involves training an agent to achieve high scores in the game 2048 (Wikipedia),2048,,,,,
3168,The_Little_Prince,AMR Parsing,AMR Parsing,"AMR Parsing, AMR-to-Text Generation",Text,English,Natural Language Processing,"amr-parsing-on-the-little-prince, amr-to-text-generation-on-the-little-prince",,https://amr.isi.edu/,https://paperswithcode.com/dataset/the-little-prince,"This corpus is an annotation of the novel The Little Prince by Antoine de Saint-Exupéry, published in 1943. We were inspired by the UNL project to include this novel, so that different groups could compare representations on the same text.",1943,,,,,
3169,The_Mafia_Dataset,Text Generation,Text Generation,"Text Generation, Deception Detection, Dialogue Understanding, Text Classification","Image, Text",English,Computer Vision,,,https://github.com/omonida/mafia-dataset,https://paperswithcode.com/dataset/the-mafia-dataset,"The Mafia Dataset was created to model the behavior of deceptive actors in the context of the Mafia game, as described in the paper “Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia”. We hope that this dataset will be of use to others studying the effects of deception on language use.",,,,,,
3170,The_Pile,Language Modelling,Language Modelling,Language Modelling,Text,English,Natural Language Processing,language-modelling-on-the-pile,,https://pile.eleuther.ai/,https://paperswithcode.com/dataset/the-pile,"The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together.

Datasheet: Datasheet for the Pile",,,,,,
3171,The_QUAERO_French_Medical_Corpus,Medical Named Entity Recognition,Medical Named Entity Recognition,"Medical Named Entity Recognition, Named Entity Recognition (NER), Nested Named Entity Recognition","Image, Text",English,Medical,,GFDL 1.3,https://quaerofrenchmed.limsi.fr/,https://paperswithcode.com/dataset/the-quaero-french-medical-corpus,"A vast amount of information in the biomedical domain is available as natural language free text. An increasing number of documents in the field are written in languages other than English. Therefore, it is essential to develop resources, methods and tools that address Natural Language Processing in the variety of languages used by the biomedical community. In this paper, we report on the development of an extensive corpus of biomedical documents in French annotated at the entity and concept level. Three text genres are covered, comprising a total of 103,056 words. Ten entity categories corresponding to UMLS Semantic Groups were annotated, using automatic pre-annotations validated by trained human annotators. The pre-annotation method was found helful for entities and achieved above 0.83 precision for all text genres. Overall, a total of 26,409 entity annotations were mapped to 5,797 unique UMLS concepts.",,,,,,
3172,The_RBO_Dataset_of_Articulated_Objects_and_Interac,3D Object Recognition,3D Object Recognition,"3D Object Recognition, 3D Object Reconstruction, 3D Object Retrieval, 3D Object Tracking","3D, Image, Video",,Computer Vision,,CC BY 4.0,https://tu-rbo.github.io/articulated-objects,https://paperswithcode.com/dataset/the-rbo-dataset-of-articulated-objects-and,"The RBO dataset of articulated objects and interactions is a collection of 358 RGB-D video sequences (67:18 minutes) of humans manipulating 14 articulated objects under varying conditions (light, perspective, background, interaction). All sequences are annotated with ground truth of the poses of the rigid parts and the kinematic state of the articulated object (joint states) obtained with a motion capture system. We also provide complete kinematic models of these objects (kinematic structure and three-dimensional textured shape models). In 78 sequences the contact wrenches during the manipulation are also provided.",,,,,,
3173,The_ULS23_Challenge_Test_Set,Tumor Segmentation,Tumor Segmentation,"Tumor Segmentation, Segmentation",Image,,Computer Vision,tumor-segmentation-on-the-uls23-challenge,,https://uls23.grand-challenge.org/evaluation/test-phase-leaderboard/leaderboard/,https://paperswithcode.com/dataset/the-uls23-challenge-test-set,"The ULS23 test set contains 725 lesions from 284 patients of the Radboudumc and JBZ hospitals in the Netherlands. It is intended to be used to measure the performance of 3D universal lesion segmentation models for Computed Tomography (CT). To prepare the data, radiological reports from both participating institutions where searched using NLP tools identifying patients with measurable target lesions, indicating that these lesions were clinically relevant. A random sample of patients was selected, 56.3% of which were male and with diverse scanner manufacturers. The lesions were annotated in 3D by expert radiologists with over 10 years of experience in reading oncological scans. ULS23 is an open benchmark, and we invite ongoing submissions to advance the development of future ULS models.",,,,,,
3174,The_Write___Improve_Corpus_2024,Grammatical Error Detection,Grammatical Error Detection,"Grammatical Error Detection, Automated Essay Scoring, Grammatical Error Correction",Image,,Computer Vision,,Cambridge University Press & Assessment,https://englishlanguageitutoring.com/datasets/write-and-improve-corpus-2024,https://paperswithcode.com/dataset/the-write-improve-corpus-2024,"We present a new annotated corpus of written learner English, derived from essays submitted to the learning platform Write & Improve (W&I). Users of W&I are presented with automated scoring and feedback on grammatical errors, and are encouraged to act on their error feedback, submitting multiple versions of their essays for any given prompt. We build the corpus on this interplay between users and prompts, collecting sets of essays submitted by users for a selected list of 50 popular prompts. The prompts include 20 aimed at beginner learners of English, 20 aimed at intermediate learners, and 10 at advanced learners. This distribution reflects the greater use of W&I by beginner and intermediate learners of English. We ensured that the prompts were not likely to elicit personal information and covered a broad range of tasks and topics. This list of prompts enabled us to identify 5050 essay sets written by 766 users, forming the basis for the Write & Improve Corpus, which is being made available for non-commercial use on the ELiT website. We describe the steps we took to ensure the corpus contains appropriate texts, does not include personal information, and will come with annotations relating to Common European Framework of Reference (CEFR) level and grammatical errors. All essays were submitted between 2020 and 2022 by registered users of W&I who have supplied their first language (L1) in an optional questionnaire. In total, there are more than 23K essays containing more than 3.5 million word tokens. The final versions of each essay set amount to 762K word tokens. There are 22 different L1s in the corpus, with the most common being Spanish, Portuguese, Japanese, Arabic and Vietnamese. We present some descriptive statistics for the corpus, and consider some research use cases.",2020,,,,,
3175,This_is_not_a_Dataset,Commonsense Causal Reasoning,Commonsense Causal Reasoning,"Commonsense Causal Reasoning, Binary text classification, Common Sense Reasoning, Negation Detection, Zero-Shot Text Classification, Common Sense Reasoning (Zero-Shot), Text Classification","Image, Text",English,Reasoning,"zero-shot-text-classification-on-this-is-not, text-classification-on-this-is-not-a-dataset",apache-2.0,https://github.com/hitz-zentroa/This-is-not-a-Dataset,https://paperswithcode.com/dataset/this-is-not-a-dataset,"We introduce a large semi-automatically generated dataset of ~400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms that we use to evaluate LLMs.

Image:  ThisIsNotaDataset",,,,,,
3176,Three-view_Synthetic_data,Multiview Detection,Multiview Detection,Multiview Detection,Image,,Computer Vision,,MIT,https://patternkps.github.io,https://paperswithcode.com/dataset/three-view-synthetic-data,"10000 instances of three-view numerical data set with 4 clusters and 2 feature components are considered. The data points in each view are generated from a 2-component 2-variate Gaussian mixture model (GMM) where their mixing proportions $\alpha_1^{(1)}=\alpha_1^{(2)}=\alpha_1^{(3)}=\alpha_1^{(4)}=0.3$;  $\alpha_2^{(1)}=\alpha_2^{(2)}=\alpha_2^{(3)}=\alpha_2^{(4)}=0.15$; $\alpha_3^{(1)}=\alpha_3^{(2)}=\alpha_3^{(3)}=\alpha_3^{(4)}=0.15$  and $\alpha_4^{(1)}=\alpha_4^{(2)}=\alpha_4^{(3)}=\alpha_4^{(4)}=0.4$. The means  $\mu_{ik}^{(1)}$ for the first view are $[-10 ~-5)]$,$[-9 ~ 11]$, $[0~ 6]$   and $[4~0]$;  The means  $\mu_{ik}^{(2)}$ for the view 2 are $[-8 ~-12]$,$[-6 ~ -3]$, $[-2~ 7]$   and $[2~1]$; And the means  $\mu_{ik}^{(3)}$ for the third view are $[-5 ~-10]$,$[-8 ~ -1]$, $[0~ 5]$   and $[5~-4]$. The covariance matrices for the three views are $\Sigma_1^{(1)}=\Sigma_1^{(2)}=\Sigma_1^{(3)}=\Sigma_1^{(4)}=\left[ \begin{array}{cc} 1 & 0\0&1\end{array}\right]$; $\Sigma_2^{(1)}=\Sigma_2^{(2)}=\Sigma_2^{(3)}=\Sigma_2^{(4)}=3 \left[ \begin{array}{cc} 1 & 0\0&1\end{array}\right]$;  $\Sigma_3^{(1)}=\Sigma_3^{(2)}=\Sigma_3^{(3)}=\Sigma_3^{(4)}=2 \left[ \begin{array}{cc} 1 & 0\0&1\end{array}\right]$; and $\Sigma_4^{(1)}=\Sigma_4^{(2)}=\Sigma_4^{(3)}=\Sigma_4^{(4)}=0.5 \left[ \begin{array}{cc} 1 & 0\0&1\end{array}\right]$. These $x_1^{(1)}$  and $x_2^{(1)}$  are the coordinates for the view 1,  $x_1^{(2)}$  and  $x_2^{(2)}$ are the coordinates for the view 2,  $x_1^{(3)}$ and $x_2^{(3)}$ are the coordinates for the view 3. While the original distribution of data points for cluster 1, cluster 2, cluster 3, and cluster 4 are 1514, 3046, 3903, and 1537, respectively.",,,,10000 instances,,
3177,ThreeDWorld_Transport_Challenge,Motion Planning,Motion Planning,Motion Planning,Video,,Methodology,,,http://tdw-transport.csail.mit.edu/,https://paperswithcode.com/dataset/threedworld-transport-challenge,"ThreeDWorld Transport Challenge is a visually-guided and physics-driven task-and-motion planning benchmark. In this challenge, an embodied agent equipped with two 9-DOF articulated arms is spawned randomly in a simulated physical home environment. The agent is required to find a small set of objects scattered around the house, pick them up, and transport them to a desired final location. Several containers are positioned around the house that can be used as tools to assist with transporting objects efficiently. To complete the task, an embodied agent must plan a sequence of actions to change the state of a large number of objects in the face of realistic physical constraints. 

This benchmark challenge has been built using the ThreeDWorld simulation: a virtual 3D environment where all objects respond to physics, and where can be controlled using fully physics-driven navigation and interaction API.",,,,,,
3178,THUMOS14,Zero-Shot Action Detection,Zero-Shot Action Detection,"Zero-Shot Action Detection, Few Shot Temporal Action Localization, Action Classification, Action Recognition In Videos, Online Action Detection, Weakly-supervised Temporal Action Localization, Weakly Supervised Action Localization, Temporal Action Localization, Weakly Supervised Temporal Action Localization, Semi-Supervised Action Detection, Action Recognition, Action Detection, Temporal Action Proposal Generation, Zero-Shot Action Recognition","Image, Text, Time Series, Video",English,Computer Vision,"weakly-supervised-action-localization-on-4, weakly-supervised-temporal-action-5, few-shot-temporal-action-localization-on-1, zero-shot-action-detection-on-thumos-14, temporal-action-proposal-generation-on-thumos, temporal-action-localization-on-thumos-14, temporal-action-localization-on-thumos14-2, weakly-supervised-action-localization-on-8, weakly-supervised-action-localization-on, weakly-supervised-temporal-action, temporal-action-localization-on-thumos14, action-classification-on-thumos14, action-recognition-in-videos-on-thumos14-1, action-detection-on-thumos-14, action-recognition-in-videos-on-thumos14, action-classification-on-thumos-14, weakly-supervised-action-localization-on-5, online-action-detection-on-thumos-14, action-recognition-on-thumos14, semi-supervised-action-detection-on-thumos-14, zero-shot-action-recognition-on-thumos-14",,http://crcv.ucf.edu/THUMOS14/home.html,https://paperswithcode.com/dataset/thumos14-1,"The THUMOS14 (THUMOS 2014) dataset is a large-scale video dataset that includes 1,010 videos for validation and 1,574 videos for testing from 20 classes. Among all the videos, there are 220 and 212 videos with temporal annotations in validation and testing set, respectively.",2014,Learning to Localize Actions from Moments,https://arxiv.org/abs/2008.13705,,,20
3179,THYME-2016,Temporal Information Extraction,Temporal Information Extraction,"Temporal Information Extraction, Named Entity Recognition (NER), Weakly Supervised Classification, Negation Detection","Image, Text, Time Series, Video",English,Computer Vision,weakly-supervised-classification-on-thyme,,,https://paperswithcode.com/dataset/thyme-2016,,,,,,,
3180,Thyroid,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Tabular Data Generation","Image, Tabular, Text",English,Computer Vision,anomaly-detection-on-thyroid,,https://archive.ics.uci.edu/ml/datasets/thyroid+disease,https://paperswithcode.com/dataset/thyroid,"Thyroid is a dataset for detection of thyroid diseases, in which patients diagnosed with hypothyroid or subnormal are anomalies against normal patients. It contains 2800 training data instance and 972 test instances, with 29 or so attributes.",,Deep Reinforcement Learning for Unknown Anomaly Detection,https://arxiv.org/abs/2009.06847,,,
3181,tida-gcn-data,Sequential Recommendation,Sequential Recommendation,Sequential Recommendation,,,Methodology,,,https://bitbucket.org/jinyuz1996/tida-gcn-data/src/master/,https://paperswithcode.com/dataset/tida-gcn-data,"The datasets of ""Time Interval-enhanced Graph Neural Network for Shared-account Cross-domain Sequential Recommendation"" (TNNLs 2022)",2022,,,,,
3182,tieredImageNet,Few-Shot Image Classification,Few-Shot Image Classification,"Few-Shot Image Classification, Image Classification, Unsupervised Few-Shot Image Classification",Image,English,Computer Vision,"few-shot-image-classification-on-tiered-3, image-classification-on-tiered-imagenet-5-way, few-shot-image-classification-on-tiered, unsupervised-few-shot-image-classification-on-3, unsupervised-few-shot-image-classification-on-2, few-shot-image-classification-on-tiered-1, few-shot-image-classification-on-tiered-2",Custom (non-commercial),https://github.com/yaoyao-liu/tiered-imagenet-tools,https://paperswithcode.com/dataset/tieredimagenet,"The tieredImageNet dataset is a larger subset of ILSVRC-12 with 608 classes (779,165 images) grouped into 34 higher-level nodes in the ImageNet human-curated hierarchy. This set of nodes is partitioned into 20, 6, and 8 disjoint sets of training, validation, and testing nodes, and the corresponding classes form the respective meta-sets. As argued in Ren et al. (2018), this split near the root of the ImageNet hierarchy results in a more challenging, yet realistic regime with test classes that are less similar to training classes.",2018,https://arxiv.org/pdf/1803.00676.pdf,https://arxiv.org/pdf/1803.00676.pdf,165 images,,608
3183,TikTok_Dataset,Instance Segmentation,Instance Segmentation,"Instance Segmentation, Depth Estimation, Human Detection, Human Part Segmentation, 3D Human Reconstruction, Human Instance Segmentation, Video Segmentation, Human Dynamics","3D, Image, Video",,Computer Vision,,Please use the dataset only for the research purpose.,https://www.yasamin.page/hdnet_tiktok,https://paperswithcode.com/dataset/tiktok-dataset,"We learn high fidelity human depths by leveraging a collection of social media dance videos scraped from the TikTok mobile social networking application. It is by far one of the most popular video sharing applications across generations, which include short videos (10-15 seconds) of diverse dance challenges as shown above. We manually find more than 300 dance videos that capture a single person performing dance moves from TikTok dance challenge compilations for each month, variety, type of dances, which are moderate movements that do not generate excessive motion blur.   For each video, we extract RGB images at 30 frame per second, resulting in more than 100K images. We segmented these images using Removebg application, and computed the UV coordinates from DensePose.  

Download TikTok Dataset:



Please use the dataset only for the research purpose.



The dataset can be viewed and downloaded from the Kaggle page. (you need to make an account in Kaggle to be able to download the data. It is free!)



The dataset can also be downloaded from here (42 GB). The dataset resolution is: (1080 x 604)



The original YouTube videos corresponding to each sequence and the dance name can be downloaded from here (2.6 GB).",,,,100K images,,
3184,TimeBank,Timex normalization,Timex normalization,"Timex normalization, Temporal Information Extraction","Time Series, Video",,Methodology,"timex-normalization-on-timebank, temporal-information-extraction-on-timebank",,https://catalog.ldc.upenn.edu/LDC2006T08,https://paperswithcode.com/dataset/timebank,"Enriches the TimeML annotations of TimeBank by adding information about the Topic Time in terms of Klein (1994). The annotations are partly automatic, partly inferential and partly manual. The corpus was converted into the native format of the annotation software GraphAnno and POS-tagged using the Stanford bidirectional dependency network tagger.",1994,,,,,
3185,TimeBankPT,Multi-Labeled Relation Extraction,Multi-Labeled Relation Extraction,"Multi-Labeled Relation Extraction, Relation Extraction, Joint Event and Temporal Relation Extraction, Temporal Tagging","Graph, Time Series, Video",,Methodology,temporal-tagging-on-timebankpt,Coming soon.,http://nlx-server.di.fc.ul.pt/~fcosta/TimeBankPT/,https://paperswithcode.com/dataset/timebankpt,"TimeBankPT is a corpus of Portuguese text with annotations about time. The annotation scheme used is similar to TimeML. TimeBankPT is the result of adapting the English corpus used in the first TempEval challenge to the Portuguese language.

In what regards the temporal relaiton type, the corpus was annotated with six labels, namely: BEFORE, AFTER, OVERLAP, BEFORE-OR-OVERLAP,
OVERLAP-OR-AFTER and VAGUE.

|           | Train | Test |
|-----------|-------|------|
| Documents | 162 | 20 |
| Sentences | 2281  | 351  |
| Words     | 60782 | 8920 |
| Events    | 6790  | 1097 |
| Timexs    | 1244  | 165  |
| Tlinks    | 5781  | 758  |",,,,,,
3186,Timers_and_Such,Spoken Language Understanding,Spoken Language Understanding,"Spoken Language Understanding, Speech Recognition","Audio, Image, Text",English,Computer Vision,spoken-language-understanding-on-timers-and,,https://zenodo.org/record/4623772,https://paperswithcode.com/dataset/timers-and-such,"Timers and Such is an open source dataset of spoken English commands for common voice control use cases involving numbers. The dataset has four intents, corresponding to four common offline voice assistant uses: SetTimer, SetAlarm, SimpleMath, and UnitConversion. The semantic label for each utterance is a dictionary with the intent and a number of slots. 

All recordings were converted from their original formats to single-channel 16,000 Hz .wav files.",,,,,,
3187,TimeTravel,Text Generation,Text Generation,"Text Generation, Music Auto-Tagging, Counterfactual Inference, Counterfactual Detection","Audio, Image, Text",English,Computer Vision,music-auto-tagging-on-timetravel,,https://github.com/qkaren/Counterfactual-StoryRW,https://paperswithcode.com/dataset/timetravel,"TimeTravel contains 29,849 counterfactual rewritings, each with the original story, a counterfactual event, and human-generated revision of the original story compatible with the counterfactual event.",,,,,,
3188,TIMIT,Speech Separation,Speech Separation,"Speech Separation, Lip Reading, Clustering, Speech Recognition, Speech Enhancement, Speaker-Specific Lip to Speech Synthesis","Audio, Image, Text",English,Speech,"lip-reading-on-tcd-timit-corpus-mixed-speech, speech-recognition-on-timit, speaker-specific-lip-to-speech-synthesis-on-1, speech-separation-on-tcd-timit-corpus-mixed, speech-enhancement-on-tcd-timit-corpus-mixed",,https://catalog.ldc.upenn.edu/LDC93S1,https://paperswithcode.com/dataset/timit,The TIMIT Acoustic-Phonetic Continuous Speech Corpus is a standard dataset used for evaluation of automatic speech recognition systems. It consists of recordings of 630 speakers of 8 dialects of American English each reading 10 phonetically-rich sentences. It also comes with the word and phone-level transcriptions of the speech.,,Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/abs/1207.0580,,,
3189,TinyChirp,Bird Audio Detection,Bird Audio Detection,Bird Audio Detection,"Audio, Image",,Audio,,CC BY-NC-SA 4.0,https://www.kaggle.com/datasets/zhaolanhuang/tinychirp,https://paperswithcode.com/dataset/tinychirp,"TinyChirp dataset for model training, validation and testing",,,,,,
3190,TinyPerson,Object Detection,Object Detection,"Object Detection, Image Restoration, Human Detection",Image,,Computer Vision,,,http://vision.ucas.ac.cn/resource.asp,https://paperswithcode.com/dataset/tinyperson,"TinyPerson is a benchmark for tiny object detection in a long distance and with massive backgrounds. The images in TinyPerson are collected from the Internet. First, videos with a high resolution are collected from different websites. Second, images from the video are sampled every 50 frames. Then images with a certain repetition (homogeneity) are deleted, and the resulting images are annotated with 72,651 objects with bounding boxes by hand.",,https://arxiv.org/abs/1912.10664,https://arxiv.org/abs/1912.10664,,,
3191,Tiny_ImageNet,Image Classification,Image Classification,"Image Classification, Weakly-Supervised Object Localization, Image Clustering, Personalized Federated Learning, Conditional Image Generation, Clean-label Backdoor Attack (0.05%), Self-Supervised Learning","Image, Text",English,Computer Vision,"image-classification-on-tiny-imagenet-2, image-classification-on-tiny-imagenet-1, weakly-supervised-object-localization-on-tiny, image-clustering-on-tiny-imagenet, conditional-image-generation-on-tiny-imagenet, clean-label-backdoor-attack-0-05-on-tiny, personalized-federated-learning-on-tiny, self-supervised-learning-on-tiny-imagenet",,https://www.kaggle.com/c/tiny-imagenet,https://paperswithcode.com/dataset/tiny-imagenet,"Tiny ImageNet contains 100000 images of 200 classes (500 for each class) downsized to 64×64 colored images. Each class has 500 training images, 50 validation images and 50 test images.",,Embedded Encoder-Decoder in Convolutional Networks Towards Explainable AI,https://arxiv.org/abs/2007.06712,100000 images,"training images, 50 validation images",200
3192,Tiny_Images,Image Classification,Image Classification,"Image Classification, Quantization, Image Retrieval",Image,,Computer Vision,,,https://groups.csail.mit.edu/vision/TinyImages/,https://paperswithcode.com/dataset/tiny-images,"The image dataset TinyImages contains 80 million images of size 32×32 collected from the Internet, crawling the words in WordNet. 

The authors have decided to withdraw it because it contains offensive content, and have asked the community to stop using it.",,Implementing Randomized Matrix Algorithms in Parallel and Distributed Environments,https://arxiv.org/abs/1502.03032,,,
3193,TIP_2018,Image Enhancement,Image Enhancement,Image Enhancement,Image,,Computer Vision,image-enhancement-on-tip-2018,,https://huggingface.co/datasets/zxbsmk/TIP-2018,https://paperswithcode.com/dataset/tip-2018,"The first large demoire dataset. The dataset contains 135,000 image pairs, each containing an image contaminated with moire patterns and its corresponding uncontaminated reference image.",,,,,,
3194,TiROD,Object Detection,Object Detection,"Object Detection, 2D Object Detection, Continual Learning, TiROD",Image,,Computer Vision,tirod-on-tirod,CC BY 4.0,https://pastifra.github.io/TiROD/,https://paperswithcode.com/dataset/tirod,"Dataset to benchmark Continual Learning for Object Detection in a Tiny Robotics settings.


10 Continual learning tasks
5 environments (1 indoor and 4 outdoor)
2 illumination conditions
14 object categories

Dataset Website

Dataset video

Code

Paper",,Paper,https://arxiv.org/pdf/2409.16215,,,
3195,TITAN,Autonomous Vehicles,Autonomous Vehicles,"Autonomous Vehicles, Trajectory Prediction, Autonomous Driving",Time Series,,Methodology,,Custom,https://usa.honda-ri.com/titan,https://paperswithcode.com/dataset/titan,"TITAN consists of 700 labeled video-clips (with odometry) captured from a moving vehicle on highly interactive urban traffic scenes in Tokyo. The dataset includes 50 labels including vehicle states and actions, pedestrian age groups, and targeted pedestrian action attributes that are organized hierarchically corresponding to atomic, simple/complex-contextual, transportive, and communicative actions.",,,,,,50
3196,Titanic,Survival Prediction,Survival Prediction,"Survival Prediction, Binary Classification, Synthetic Data Generation, Synthetic Data Evaluation","Image, Text, Time Series",English,Computer Vision,"synthetic-data-evaluation-on-titanic, survival-prediction-on-titanic",CC BY-SA 3.0,https://kaggle.com/competitions/titanic,https://paperswithcode.com/dataset/titanic,"Titanic Dataset Description
Overview
The data is divided into two groups:
- Training set (train.csv):
  Used to build machine learning models. It includes the outcome (also called the ""ground truth"") for each passenger, allowing models to predict survival based on “features” like gender and class. Feature engineering can also be applied to create new features.
- Test set (test.csv):
  Used to evaluate model performance on unseen data. The ground truth is not provided; the task is to predict survival for each passenger in the test set using the trained model.

Additionally, gender_submission.csv is provided as an example submission file, containing predictions based on the assumption that all and only female passengers survive.


Data Dictionary
| Variable   | Definition                                | Key                                              |
|------------|------------------------------------------|-------------------------------------------------|
| survival   | Survival                                 | 0 = No, 1 = Yes                                 |
| pclass     | Ticket class                             | 1 = 1st, 2 = 2nd, 3 = 3rd                       |
| sex        | Sex                                      |                                                 |
| age        | Age in years                             |                                                 |
| sibsp      | # of siblings/spouses aboard the Titanic |                                                 |
| parch      | # of parents/children aboard the Titanic |                                                 |
| ticket     | Ticket number                            |                                                 |
| fare       | Passenger fare                           |                                                 |
| cabin      | Cabin number                             |                                                 |
| embarked   | Port of Embarkation                      | C = Cherbourg, Q = Queenstown, S = Southampton  |


Variable Notes

pclass: Proxy for socio-economic status (SES):
1st = Upper
2nd = Middle
3rd = Lower
age:  
Fractional if less than 1 year.  
Estimated ages are represented in the form xx.5.
sibsp: Defines family relations as:
Sibling: Brother, sister, stepbrother, stepsister.
Spouse: Husband, wife (excluding mistresses and fiancés).
parch: Defines family relations as:
Parent: Mother, father.
Child: Daughter, son, stepdaughter, stepson.
  Some children traveled only with a nanny, so parch = 0 for them.",,,,,,
3197,TLDR9_,Extreme Summarization,Extreme Summarization,Extreme Summarization,Text,English,Natural Language Processing,extreme-summarization-on-tldr9,,https://github.com/sajastu/reddit_collector,https://paperswithcode.com/dataset/tldr9,"TLDR9+ is a large-scale summarization dataset containing over 9 million training instances extracted from Reddit discussion forum. This dataset is specifically gathered to perform extreme summarization (i.e., generating one-sentence summary in high compression and abstraction) and is more than twice larger than the previously proposed dataset. With the help of human annotations, a more fine-grained dataset is distilled by sampling High-Quality instances from TLDR9+ and call it TLDRHQ. dataset.",,https://arxiv.org/pdf/2110.01159v1.pdf,https://arxiv.org/pdf/2110.01159v1.pdf,,,
3198,TLUnified-NER,NER,NER,NER,,,Methodology,,GNU GPL v3,https://huggingface.co/datasets/ljvmiranda921/tlunified-ner,https://paperswithcode.com/dataset/tlunified-ner,"We present the development of a Named Entity Recognition (NER) dataset for Tagalog. This corpus helps fill the resource gap present in Philippine languages today, where NER resources are scarce. The texts were obtained from a pretraining corpora containing news reports, and were labeled by native speakers in an iterative fashion. The resulting dataset contains ~7.8k documents across three entity types: Person, Organization, and Location. The inter-annotator agreement, as measured by Cohen's κ, is 0.81. We also conducted extensive empirical evaluation of state-of-the-art methods across supervised and transfer learning settings. Finally, we released the data and processing code publicly to inspire future work on Tagalog NLP.",,,,8k documents,,
3199,TNCR_Dataset,Object Detection,Object Detection,"Object Detection, Table annotation, Table Detection, Table Recognition","Image, Tabular",,Computer Vision,,MIT,https://github.com/abdoelsayed2016/TNCR_Dataset,https://paperswithcode.com/dataset/tncr-dataset,"We present TNCR, a new table dataset with varying image quality collected from free open source websites. TNCR dataset can be used for table detection in scanned document images and their classification into 5 different classes.

TNCR contains 9428 high-quality labeled images. In this paper, we have implemented state-of-the-art deep learning-based methods for table detection to create several strong baselines. Cascade Mask R-CNN with ResNeXt-101-64x4d Backbone Network achieves the highest performance compared to other methods with a precision of 79.7%, recall of 89.8%, and f1 score of 84.4% on the TNCR dataset. We have made TNCR open source in the hope of encouraging more deep learning approaches to table detection, classification and structure recognition.",,,,,,
3200,TNL2K,Visual Tracking,Visual Tracking,"Visual Tracking, Visual Object Tracking","Image, Video",,Computer Vision,"visual-tracking-on-tnl2k, visual-object-tracking-on-tnl2k",Custom,https://sites.google.com/view/langtrackbenchmark/,https://paperswithcode.com/dataset/tnl2k,"Tracking by Natural Language (TNL2K) is constructed for the evaluation of tracking by natural language specification. TNL2K features:



Large-scale: 2,000 sequences, contains 1,244,340 frames, 663 words, 1300 / 700 for the train / testing respectively 



High-quality: Manual annotation with careful inspection in each frame 



Multi-modal: Providing visual and language annotation for each sequence 



Adversarial-samples: Randomly adding adversarial samples for research on adversarial attack and defence 



Significant-appearance-variation:  Containing videos with cloth/face change for pedestrian 



Heterogeneous: Containing RGB, thermal, Cartoon,  Synthetic data 



Multiple-baseline: Tracking-by-BBox, Tracking-by-Language, Tracking-by-Joint-BBox-Language",,,,,,
3201,TOMG-Bench,Description-guided molecule generation,Description-guided molecule generation,"Description-guided molecule generation, Text-based de novo Molecule Generation",Text,English,Natural Language Processing,description-guided-molecule-generation-on,,https://phenixace.github.io/tomgbench/,https://paperswithcode.com/dataset/tomg-bench,"In this paper, we propose Text-based Open Molecule Generation Benchmark (TOMG-Bench), the first benchmark to evaluate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses a dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each task further contains three subtasks, with each subtask comprising 5,000 test samples. Given the inherent complexity of open molecule generation, we have also developed an automated evaluation system that helps measure both the quality and the accuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the current limitations and potential areas for improvement in text-guided molecule discovery. Furthermore, with the assistance of OpenMolIns, a specialized instruction tuning dataset proposed for solving challenges raised by TOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5\% on TOMG-Bench.",,,,,"valuate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses a dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each task further contains three subtasks, with each subtask comprising 5,000 test samples",
3202,ToN_IoT,Speech Synthesis - Gujarati,Speech Synthesis - Gujarati,Speech Synthesis - Gujarati,Audio,,Speech,speech-synthesis-gujarati-on-ton-iot,CC BY 4.0,https://ieee-dataport.org/documents/toniot-datasets,https://paperswithcode.com/dataset/ton-iot,"The TON_IoT datasets are new generations of Internet of Things (IoT) and Industrial IoT (IIoT) datasets for evaluating the fidelity and efficiency of different cybersecurity applications based on Artificial Intelligence (AI). The datasets have been called ‘ToN_IoT’ as they include heterogeneous data sources collected from Telemetry datasets of IoT and IIoT sensors, Operating systems datasets of Windows 7 and 10 as well as Ubuntu 14 and 18 TLS and Network traffic datasets. The datasets were collected from a realistic and large-scale network designed at the IoT Lab of the UNSW Canberra Cyber, the School of Engineering and Information technology (SEIT), UNSW Canberra @ the Australian Defence Force Academy (ADFA). 

The datasets were gathered in a parallel processing to collect several normal and cyber-attack events from IoT networks. A new testbed was developed at the IoT lab to connect many virtual machine, physical systems, hacking platforms, cloud and fog platforms, IoT and IIoT sensors to mimic the complexity and scalability of industrial IoT and Industry 4.0 networks.

Different hacking techniques, such as DoS, DDoS and ransomware against, were launched against web applications, IoT gateways and computer systems across the IIoT network.",,,,,,
3203,ToolBench,Trajectory Planning,Trajectory Planning,Trajectory Planning,Time Series,,Methodology,trajectory-planning-on-toolbench,Apache-2.0 license,https://github.com/OpenBMB/ToolBench,https://paperswithcode.com/dataset/toolbench,"ToolBench is an instruction-tuning dataset for tool use, which is created automatically using ChatGPT. Specifically, the authors collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub, then prompt ChatgPT to generate diverse human instructions involving these APIs, covering both single-tool and multi-tool scenarios.",,,,,,49
3204,Topic_modeling_topic_coverage_dataset,Topic coverage,Topic coverage,Topic coverage,,,Methodology,topic-coverage-on-topic-modeling-topic-2,CC BY 4.0,https://github.com/dkorenci/topic_coverage,https://paperswithcode.com/dataset/topic-modeling-topic-coverage-dataset,"A prevalent use case of topic models is that of topic discovery.
However, most of the topic model evaluation methods rely on abstract metrics such as perplexity or topic coherence.
The topic coverage approach is to measure the models' performance by matching model-generated topics to topics discovered by humans.
This way, the models are evaluated in the context of their use, by essentially simulating
topic modeling in a fixed setting defined by a text collection and a set of reference topics.

Reference topics represent a ground truth that can be used to evaluate both topic models and other measures of model performance.
The coverage approach enables large-scale automatic evaluation of both existing and future topic models.

The topic coverage dataset consists of two text collections and two sets of reference topics.
These two sub-datasets correspond to two domains (news text and biological text) 
where topic models are used for topic discovery in large text collections.
The reference topics consist of model-generated topics inspected, selected, and curated by humans.

Each dataset contains a corpus of preprocessed (tokenized) texts and a set of reference topics, 
each represented by a list of words and text documents.
The dataset details, including the instruction for the use of the data and supporting code, are here:
https://github.com/dkorenci/topic_coverage/blob/main/data.readme.txt

The coverage measures that can be used to evaluate topic models are described in the accompanying paper, 
whereas the code and the instructions can be found in the github repo.",,,,,,
3205,TopLogo-10,Incremental Learning,Incremental Learning,"Incremental Learning, Traffic Sign Recognition, Metric Learning, One-Shot Learning",Image,,Computer Vision,traffic-sign-recognition-on-toplogo-10,,http://www.eecs.qmul.ac.uk/~hs308/qmul_toplogo10.html/,https://paperswithcode.com/dataset/toplogo-10,Collected from top 10 most popular clothing/wearable brandname logos captured in rich visual context.,,,,,,
3206,TOPv2,Semantic Parsing,Semantic Parsing,"Semantic Parsing, Task-Oriented Dialogue Systems",Text,English,Natural Language Processing,,CC BY-NC 4.0,https://fb.me/TOPv2Dataset,https://paperswithcode.com/dataset/topv2,"Task Oriented Parsing v2 (TOPv2) representations for intent-slot based dialog systems.

Provided under the CC-BY-SA license. Please cite the accompanying paper when
using this dataset -

@inproceedings{chen-etal-2020-low-resource,
    title={Low-Resource Domain Adaptation for Compositional Task-Oriented
        Semantic Parsing},
    author={Xilun Chen and Asish Ghoshal and Yashar Mehdad and Luke Zettlemoyer
        and Sonal Gupta},
    booktitle={Proceedings of the 2020 Conference on Empirical Methods in
        Natural Language Processing (EMNLP)},
    year={2020},
    publisher = ""Association for Computational Linguistics""
}

CHANGELOG:<br>
03/10/2021 (V1.1): Added the low-resource splits used in the paper.<br>
09/18/2020 (V1.0): Initial release.

TOPv2 is a multi-domain task-oriented semantic parsing dataset. It is an extension to the TOP dataset (http://fb.me/semanticparsingdialog) with 6 additional domains and 137k new samples.

In total, TOPv2 has 8 domains (alarm, event, messaging, music, navigation, reminder, timer, weather) and 180k samples randomly split into train, eval, and test sets for each domain. Please refer to the paper for more data statistics.
Note: As TOPv2 data is provided on a per-domain basis, the UNSUPPORTED utterances in the original TOP dataset were removed as they could not be mapped to any domain.

The training, evaluation and test sets for each domain are provided as tab-separated value (TSV) files with file names of ""domain_split.tsv"".
The first row of each file contains the column headers, while each following row is of the format:
domain <tab> utterance <tab> semantic_parse
where the semantic_parse follows the same format as the original TOP dataset.

e.g. event <tab> Art fairs this weekend in Detroit <tab> [IN:GET_EVENT [SL:CATEGORY_EVENT Art fairs ] [SL:DATE_TIME this weekend ] in [SL:LOCATION Detroit ] ]

The low-resource splits used in our experiments are provided in the
low_resource_splits subdirectory, including training and validation sets from the reminder and weather domains under 10, 25, 50, 100, 250, 500 and 1000 SPIS.",2020,,,180k samples,,
3207,TORCS,Decision Making,Decision Making,"Decision Making, Imitation Learning, Autonomous Driving",,,Methodology,,GPLv2,https://sourceforge.net/projects/torcs/,https://paperswithcode.com/dataset/torcs,"TORCS (The Open Racing Car Simulator) is a driving simulator. It is capable of simulating the essential elements of vehicular dynamics such as mass, rotational inertia, collision, mechanics of suspensions, links and differentials, friction and aerodynamics. Physics simulation is simplified and is carried out through Euler integration of differential equations at a temporal discretization level of 0.002 seconds. The rendering pipeline is lightweight and based on OpenGL that can be turned off for faster training. TORCS offers a large variety of tracks and cars as free assets. It also provides a number of programmed robot cars with different levels of performance that can be used to benchmark the performance of human players and software driving agents. TORCS was built with the goal of developing Artificial Intelligence for vehicular control and has been used extensively by the machine learning community ever since its inception.",,MADRaS : Multi Agent Driving Simulator,https://arxiv.org/abs/2010.00993,,,
3208,Toronto-3D,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Scene Understanding, 3D Semantic Segmentation, Autonomous Driving","3D, Image",,Computer Vision,"3d-semantic-segmentation-on-toronto-3d, semantic-segmentation-on-toronto-3d-l002",,https://github.com/WeikaiTan/Toronto-3D,https://paperswithcode.com/dataset/toronto-3d,"Toronto-3D is a large-scale urban outdoor point cloud dataset acquired by an MLS system in Toronto, Canada for semantic segmentation. This dataset covers approximately 1 km of road and consists of about 78.3 million points. Point clouds has 10 attributes and classified in 8 labelled object classes.",,,,,,
3209,Toronto_NeuroFace_Dataset,Facial Landmark Detection,Facial Landmark Detection,"Facial Landmark Detection, Clinical Concept Extraction",Image,,Computer Vision,,,https://slp.utoronto.ca/faculty/yana-yunusova/speech-production-lab/datasets/,https://paperswithcode.com/dataset/toronto-neuroface-dataset,"Toronto NeuroFace Dataset: A New Dataset for Facial Motion Analysis in Individuals with Neurological Disorders

Toronto NeuroFace Dataset is a public dataset with videos of oro-facial gestures performed by individuals with oro-facial impairment due to neurological disorders, such as amyotrophic lateral sclerosis (ALS) and stroke. Perceptual clinical scores from trained clinicians are provided as metadata. Manual annotation of facial landmarks is also provided for a subset of over 3300 frames.",,,,,,
3210,Torque,Abusive Language,Abusive Language,"Abusive Language, Question Answering, Natural Language Inference, Graph Generation","Graph, Text",English,Natural Language Processing,question-answering-on-torque,,https://allennlp.org/torque.html,https://paperswithcode.com/dataset/torque,Torque is an English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships.,,,,,,
3211,TotalCapture,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Pose Estimation, 3D Absolute Human Pose Estimation","3D, Image",,Computer Vision,"3d-human-pose-estimation-on-total-capture, 3d-absolute-human-pose-estimation-on-total-1","Custom (research-only, non-commercial, attribution)",https://cvssp.org/data/totalcapture/,https://paperswithcode.com/dataset/totalcapture,"The TotalCapture dataset consists of 5 subjects performing several activities such as walking, acting, a range of motion sequence (ROM) and freestyle motions, which are recorded using 8 calibrated, static HD RGB cameras and 13 IMUs attached to head, sternum, waist, upper arms, lower arms, upper legs, lower legs and feet, however the IMU data is not required for our experiments. The dataset has publicly released foreground mattes and RGB images. Ground-truth poses are obtained using a marker-based motion capture system, with the markers are <5mm in size. All data is synchronised and operates at a framerate of 60Hz, providing ground truth poses as joint positions.",,Semantic Estimation of 3D Body Shape and Pose using Minimal Cameras,https://arxiv.org/abs/1908.03030,,,
3212,ToTTo,Data-to-Text Generation,Data-to-Text Generation,Data-to-Text Generation,Text,English,Natural Language Processing,data-to-text-generation-on-totto,,https://github.com/google-research-datasets/totto,https://paperswithcode.com/dataset/totto,"ToTTo is an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description.

During the dataset creation process, tables from English Wikipedia are matched with (noisy) descriptions. Each table cell mentioned in the description is highlighted and the descriptions are iteratively cleaned and corrected to faithfully reflect the content of the highlighted cells.",,,,,,
3213,Touchdown_Dataset,Style Transfer,Style Transfer,"Style Transfer, Vision and Language Navigation, Text Style Transfer",Text,English,Natural Language Processing,vision-and-language-navigation-on-touchdown,,https://github.com/lil-lab/touchdown,https://paperswithcode.com/dataset/touchdown-dataset,"Touchdown is a corpus for executing navigation instructions and resolving spatial descriptions in visual real-world environments. The task is to follow instruction to a goal position and there find a hidden object, Touchdown the bear.",,,,,,
3214,Tough_Tables,Table annotation,Table annotation,"Table annotation, Cell Entity Annotation, Column Type Annotation",Tabular,,Methodology,"cell-entity-annotation-on-toughtables-wd, column-type-annotation-on-toughtables-dbp, cell-entity-annotation-on-toughtables-dbp, column-type-annotation-on-toughtables-wd",CC BY 4.0,https://zenodo.org/record/6211551,https://paperswithcode.com/dataset/tough-tables,"The ToughTables (2T) dataset was created for the SemTab challenge and includes 180 tables in total. The tables in this dataset can be categorized in two groups: the control (CTRL) group tables and tough (TOUGH) group tables. 

The CTRL group contains 60 tables generated by querying the DBpedia SPARQL endpoint and tables collected from Wikipedia and their characteristic is that they are easy to annotate. The TOUGH group contains 120 tables mainly scraped from the web, some containing misspelled words and nicknames/homonyms and their characteristic is that they are hard to annotate. In both groups some tables were generated by the authors where they added noise to the collected tables.

The dataset was annotated for two tasks using DBpedia (DBP) types and entities and WikiData (WD): Column Type Annotation (CTA) and Cell Entity Annotation (CEA). In the table below the number of columns annotated for the CTA and number of cells annotated for the CEA task as well as the number of classes used are listed.

|     | Annotations| Classes |
|-----|---------|---------|
| DBP-Column Type Annotation | 540     | 39 |
| DBP-Cell Entity Annotation | 663,656 | 16,023  |
| WD-Column Type Annotation| 540 | 276 |
| WD-Cell Entity Annotation | 667,244 | 24,653 |",,,,,,
3215,Tox21,Graph Classification,Graph Classification,"Graph Classification, Molecular Property Prediction, Molecular Property Prediction (1-shot)), Drug Discovery, Graph Regression","Graph, Image, Time Series",,Computer Vision,"molecular-property-prediction-1-shot-on-tox21, drug-discovery-on-tox21, graph-classification-on-tox21, graph-regression-on-tox21, molecular-property-prediction-on-tox21-1",,http://bioinf.jku.at/research/DeepTox/tox21.html,https://paperswithcode.com/dataset/tox21-1,"The Tox21 data set comprises 12,060 training samples and 647 test samples that represent chemical compounds. There are 801 ""dense features"" that represent chemical descriptors, such as molecular weight, solubility or surface area, and 272,776 ""sparse features"" that represent chemical substructures (ECFP10, DFS6, DFS8; stored in Matrix Market Format ). Machine learning methods can either use sparse or dense data or combine them. For each sample there are 12 binary labels that represent the outcome (active/inactive) of 12 different toxicological experiments. Note that the label matrix contains many missing values (NAs). The original data source and Tox21 challenge site is https://tripod.nih.gov/tox21/challenge/.",,,,,training samples and 647 test samples,
3216,ToxiGen,Hate Speech Detection,Hate Speech Detection,Hate Speech Detection,"Audio, Image",,Speech,,,https://github.com/microsoft/TOXIGEN,https://paperswithcode.com/dataset/toxigen,"A large-scale and machine-generated dataset of 274,186 toxic and benign statements about 13 minority groups. 

This dataset uses a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and
benign text with a massive pre-trained language model (GPT-3). Controlling machine generation in this way allows TOXIGEN to cover implicitly toxic text at a larger scale,
and about more demographic groups, than previous resources of human-written text. TOXIGEN can be used to fight human-written and machine-generated
toxicity.",,,,,,
3217,ToyADMOS,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Unsupervised Anomaly Detection, Open Set Learning",Image,,Computer Vision,,,https://github.com/YumaKoizumi/ToyADMOS-dataset,https://paperswithcode.com/dataset/toyadmos,"ToyADMOS dataset is a machine operating sounds dataset of approximately 540 hours of normal machine operating sounds and over 12,000 samples of anomalous sounds collected with four microphones at a 48kHz sampling rate, prepared by Yuma Koizumi and members in NTT Media Intelligence Laboratories. The ToyADMOS dataset is designed for anomaly detection in machine operating sounds (ADMOS) research. It is designed for three tasks of ADMOS: product inspection (toy car), fault diagnosis for fixed machine (toy conveyor), and fault diagnosis for moving machine (toy train).",,,,000 samples,,
3218,Toyota_Smarthome_Dataset,Action Detection,Action Detection,"Action Detection, Activity Detection, Action Classification","Image, Video",,Computer Vision,"action-classification-on-toyota-smarthome, action-detection-on-tsu",,https://project.inria.fr/toyotasmarthome/,https://paperswithcode.com/dataset/toyota-smarthome,A large scale dataset with daily-living activities performed in a natural manner.,,,,,,
3219,TR-News,Toponym Resolution,Toponym Resolution,Toponym Resolution,,,Methodology,,MIT,https://github.com/ehsk/CHF-TopoResolver/,https://paperswithcode.com/dataset/tr-news,"This dataset is collected from various global and local news sources. Toponyms are manually annotated in the articles with the corresponding entries from GeoNames. In total, the dataset
consists of 118 articles.",,,,,,
3220,TrackingNet,Visual Tracking,Visual Tracking,"Visual Tracking, Visual Object Tracking","Image, Video",,Computer Vision,"visual-object-tracking-on-trackingnet, visual-tracking-on-trackingnet",Apache-2.0,https://tracking-net.org/,https://paperswithcode.com/dataset/trackingnet,"TrackingNet is a large-scale tracking dataset consisting of videos in the wild. It has a total of 30,643 videos split into 30,132 training videos and 511 testing videos, with an average of 470,9 frames.",,Learning the Model Update for Siamese Trackers,https://arxiv.org/abs/1908.00855,,,
3221,TRADES-LOB,Stock Price Prediction,Stock Price Prediction,"Stock Price Prediction, Stock Trend Prediction",Time Series,,Methodology,,MIT,https://github.com/LeonardoBerti00/DeepMarket/tree/main/data/TRADES-LOB,https://paperswithcode.com/dataset/trades-lob,"TRADES-LOB comprises simulated TRADES market data for Tesla and Intel, for 29/01 and 30/01. Specifically, the dataset is structured into four CSV files, each containing 50 columns. The initial six columns delineate the order features, followed by 40 columns that represent a snapshot of the LOB across the top 10 levels. The concluding four columns provide key financial metrics: mid-price, spread, order volume imbalance, and Volume-Weighted Average Price (VWAP), which can be useful for downstream financial tasks, such as stock price prediction. In total, the dataset is composed of 265,986 rows and 13,299,300 cells, which is similar in size to the benchmark FI-2010 dataset.",2010,,,986 rows,,
3222,Traditional_and_Context-specific_Spam_Twitter,Spam detection,Spam detection,"Spam detection, Traditional Spam Detection, Context-specific Spam Detection",Image,,Computer Vision,"spam-detection-on-context-specific-spam, traditional-spam-detection-on-context, context-specific-spam-detection-on-context",GNU General Public License v3.0,https://github.com/GU-DataLab/context-spam,https://paperswithcode.com/dataset/context-specific-spam,"This data set is being released to support the spam and context-specific spam detection tasks on Twitter data.

There are three sets of tweets, parenting-related, #MeToo-related (a social movement focused on tackling issues related to sexual harassment and sexual assault of women), and gun-violence-related tweets. Each set contains 5,000 tweets. These tweets are original tweets in English. There are no retweets, quoted tweets or non-English tweets.",,,,,,
3223,Traffic,GLinear,GLinear,"GLinear, Multivariate Time Series Forecasting",Time Series,,Methodology,"glinear-on-traffic, multivariate-time-series-forecasting-on-45",Creative Commons Attribution 4.0 International,https://archive.ics.uci.edu/ml/datasets/Traffic+Flow+Forecasting,https://paperswithcode.com/dataset/traffic,"Abstract: The task for this dataset is to forecast the spatio-temporal traffic volume based on the historical traffic volume and other features in neighboring locations.

| Data Set Characteristics | Number of Instances | Area     | Attribute Characteristics | Number of Attributes | Date Donated | Associated Tasks | Missing Values |
| ------------------------ | ------------------- | -------- | ------------------------- | -------------------- | ------------ | ---------------- | -------------- |
| Multivariate             | 2101                | Computer | Real                      | 47                   | 2020-11-17   | Regression       | N/A            |

Source:
Liang Zhao, liang.zhao '@' emory.edu, Emory University.

Data Set Information:
The task for this dataset is to forecast the spatio-temporal traffic volume based on the historical traffic volume and other features in neighboring locations. Specifically, the traffic volume is measured every 15 minutes at 36 sensor locations along two major highways in Northern Virginia/Washington D.C. capital region. The 47 features include: 1) the historical sequence of traffic volume sensed during the 10 most recent sample points (10 features), 2) week day (7 features), 3) hour of day (24 features), 4) road direction (4 features), 5) number of lanes (1 feature), and 6) name of the road (1 feature). The goal is to predict the traffic volume 15 minutes into the future for all sensor locations. With a given road network, we know the spatial connectivity between sensor locations. For the detailed data information, please refer to the file README.docx.

Attribute Information:
The 47 features include: (1) the historical sequence of traffic volume sensed during the 10 most recent sample points (10 features), (2) week day (7 features), (3) hour of day (24 features), (4) road direction (4 features), (5) number of lanes (1 feature), and (6) name of the road (1 feature).

Relevant Papers:
Liang Zhao, Olga Gkountouna, and Dieter Pfoser. 2019. Spatial Auto-regressive Dependency Interpretable Learning Based on Spatial Topological Constraints. ACM Trans. Spatial Algorithms Syst. 5, 3, Article 19 (August 2019), 28 pages. DOI:[Web Link]

Citation Request:
To use these datasets, please cite the papers:

Liang Zhao, Olga Gkountouna, and Dieter Pfoser. 2019. Spatial Auto-regressive Dependency Interpretable Learning Based on Spatial Topological Constraints. ACM Trans. Spatial Algorithms Syst. 5, 3, Article 19 (August 2019), 28 pages. DOI:[Web Link]",2020,,,,,
3224,Tragic_Talkers,3D Reconstruction,3D Reconstruction,"3D Reconstruction, Active Speaker Localization","3D, Audio, Image",,Computer Vision,,,https://cvssp.org/data/TragicTalkers/,https://paperswithcode.com/dataset/tragic-talkers,"Tragic Talkers is an audio-visual dataset consisting of excerpts from the ""Romeo and Juliet"" drama captured with microphone arrays and multiple co-located cameras for light-field video. Tragic Talkers provides ideal content for object-based media (OBM) production. It is designed to cover various conventional talking scenarios, such as monologues, two-people conversations, and interactions with considerable movement and occlusion, yielding 30 sequences captured from a total of 22 different points of view and two 16-element microphone arrays.",,Tragic Talkers: A Shakespearean Sound- and Light-Field Dataset for Audio-Visual Machine Learning Research,https://arxiv.org/pdf/2212.01892v1.pdf,,,
3225,TrajAir__A_General_Aviation_Trajectory_Dataset,Trajectory Forecasting,Trajectory Forecasting,"Trajectory Forecasting, Multi-future Trajectory Prediction, Trajectory Prediction, Multi Future Trajectory Prediction",Time Series,,Methodology,trajectory-prediction-on-trajair-a-general,CC BY 4.0,https://theairlab.org/trajair/,https://paperswithcode.com/dataset/trajair,"This dataset contains aircraft trajectories in an untowered terminal airspace collected over 8 months surrounding the Pittsburgh-Butler Regional Airport [ICAO:KBTP], a single runway GA airport, 10 miles North of the city of Pittsburgh, Pennsylvania. The trajectory data is recorded using an on-site setup that includes an ADS-B receiver. The trajectory data provided spans days from 18 Sept 2020 till 23 Apr 2021 and includes a total of 111 days of data discounting downtime, repairs, and bad weather days with no traffic. Data is collected starting at 1:00 AM local time to 11:00 PM local time. The dataset uses an Automatic Dependent Surveillance-Broadcast (ADS-B) receiver placed within the airport premises to capture the trajectory data. The receiver uses both the 1090 MHz and 978 MHz frequencies to listen to these broadcasts. The ADS-B uses satellite navigation to produce accurate location and timestamp for the targets which is recorded on-site using our custom setup. Weather data during the data collection time period is also included for environmental context. The weather data is obtained post-hoc using the METeorological Aerodrome Reports (METAR) strings generated by the Automated Weather Observing System (AWOS) system at KBTP. The raw METAR string is then appended to the raw trajectory data by matching the closest UTC timestamps.

We also provide processed data that filters, interpolates and transforms data from a global frame to an airport-centred inertial frame. The inertial frame is centred at one end of the runway with the x-axis along the runway. Trajectories are filtered with aircrafts under 6000 ft MSL and around a 5km radius around the airport origin. We also remove duplicates and interpolate data every second. The proceed files also contain wind-data; a crucial factor in decision-making; separated in components along and perpendicular to the runway direction.",2020,,,,,
3226,TrajNet,Trajectory Forecasting,Trajectory Forecasting,"Trajectory Forecasting, Trajectory Prediction, Decision Making",Time Series,,Methodology,"trajectory-prediction-on-trajnet, trajectory-forecasting-on-trajnet",,http://trajnet.stanford.edu/,https://paperswithcode.com/dataset/trajnet-1,"The TrajNet Challenge represents a large multi-scenario forecasting benchmark. The challenge consists on  predicting 3161 human trajectories, observing for each trajectory 8 consecutive ground-truth values (3.2 seconds) i.e., t−7,t−6,…,t, in world plane coordinates (the so-called world plane Human-Human protocol) and forecasting the following 12 (4.8 seconds), i.e., t+1,…,t+12. The 8-12-value protocol is consistent with the most trajectory forecasting approaches, usually focused on the 5-dataset ETH-univ + ETH-hotel + UCY-zara01 + UCY-zara02 + UCY-univ. Trajnet extends substantially the 5-dataset scenario by diversifying the training data, thus stressing the flexibility and generalization one approach has to exhibit when it comes to unseen scenery/situations. In fact, TrajNet is a superset of diverse datasets that requires to train on four families of trajectories, namely 1) BIWI Hotel (orthogonal bird’s eye flight view, moving people), 2) Crowds UCY (3 datasets, tilted bird’s eye view, camera mounted on building or utility poles, moving people), 3) MOT PETS (multisensor, different human activities) and 4) Stanford Drone Dataset (8 scenes, high orthogonal bird’s eye flight view, different agents as people, cars etc. ), for a total of 11448 trajectories. Testing is requested on diverse partitions of BIWI Hotel, Crowds UCY, Stanford Drone Dataset, and is evaluated by a specific server (ground-truth testing data is unavailable for applicants).",,Transformer Networks for Trajectory Forecasting,https://arxiv.org/abs/2003.08111,,,
3227,Travel,Tabular Data Generation,Tabular Data Generation,Tabular Data Generation,"Tabular, Text",English,Natural Language Processing,tabular-data-generation-on-travel,CCO,https://www.kaggle.com/datasets/tejashvi14/tour-travels-customer-churn-prediction,https://paperswithcode.com/dataset/travel,"A Tour & Travels Company Wants To Predict Whether A Customer Will Churn Or Not Based On Indicators Given Below.
Help Build Predictive Models And Save The Company's Money.
Perform Fascinating EDAs.
The Data Was Used For Practice Purposes And Also During A Mini Hackathon, Its Completely Free To Use",,,,,,
3228,TREC-COVID,Text Retrieval,Text Retrieval,"Text Retrieval, Information Retrieval, Zero-shot Text Search",Text,English,Natural Language Processing,"text-retrieval-on-trec-covid, zero-shot-text-search-on-trec-covid",,https://ir.nist.gov/trec-covid/,https://paperswithcode.com/dataset/trec-covid,"TREC-COVID is a community evaluation designed to build a test collection that captures the information needs of biomedical researchers using the scientific literature during a pandemic. One of the key characteristics of pandemic search is the accelerated rate of change: the topics of interest evolve as the pandemic progresses and the scientific literature in the area explodes. The COVID-19 pandemic provides an opportunity to capture this progression as it happens. TREC-COVID, in creating a test collection around COVID-19 literature, is building infrastructure to support new research and technologies in pandemic search.",,,,,,
3229,TREK-150,Video Object Tracking,Video Object Tracking,"Video Object Tracking, Object Tracking, Visual Object Tracking","Image, Video",,Computer Vision,,,https://machinelearning.uniud.it/datasets/trek150/,https://paperswithcode.com/dataset/trek150,TREK-150 is a benchmark dataset for object tracking in First Person Vision (FPV) videos composed of 150 densely annotated video sequences.,,,,,,
3230,TripClick,Text Retrieval,Text Retrieval,"Text Retrieval, Click-Through Rate Prediction, Information Retrieval, Biomedical Information Retrieval","Text, Time Series",English,Natural Language Processing,,,https://tripdatabase.github.io/tripclick/,https://paperswithcode.com/dataset/tripclick,"TripClick is a large-scale dataset of click logs in the health domain, obtained from user interactions of the Trip Database health web search engine. 

Provide:


Approximately 5.2 million user interactions
IR evaluation benchmark
Trainin data for deep learning IR models",,,,,,
3231,TriviaHG,Hint Generation,Hint Generation,"Hint Generation, Large Language Model, Question Answering, Information Retrieval",Text,English,Natural Language Processing,,MIT,https://github.com/DataScienceUIBK/TriviaHG,https://paperswithcode.com/dataset/triviahg,"Nowadays, individuals tend to engage in dialogues with Large Language Models, seeking answers to their questions. In times when such answers are readily accessible to anyone, the stimulation and preservation of human’s cognitive abilities, as well as the assurance of maintaining good reasoning skills by humans becomes crucial. This study addresses such needs by proposing hints (instead of final answers or before giving answers) as a viable solution. We introduce a framework for the automatic hint generation for factoid questions, employing it to construct TriviaHG, a novel large-scale dataset featuring 160,230 hints corresponding to 16,645 questions from the TriviaQA dataset. Additionally, we present an automatic evaluation method that measures the Convergence and Familiarity quality attributes of hints. To evaluate the TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals to annotate 2,791 hints and tasked 6 humans with answering questions using the provided hints. The effectiveness of hints varied, with success rates of 96%, 78%, and 36% for questions with easy, medium, and hard answers, respectively. Moreover, the proposed automatic evaluation methods showed a robust correlation with annotators’ results. Conclusively, the findings highlight three key insights: the facilitative role of hints in resolving unknown questions, the dependence of hint quality on answer difficulty, and the feasibility of employing automatic evaluation methods for hint assessment.",,,,,,
3232,TriviaQA,Question Generation,Question Generation,"Question Generation, Open-Domain Question Answering, Question Answering, Reading Comprehension",Text,English,Natural Language Processing,"question-answering-on-triviaqa, question-generation-on-triviaqa, open-domain-question-answering-on-triviaqa, open-domain-question-answering-on-kilt-2",,http://nlp.cs.washington.edu/triviaqa/,https://paperswithcode.com/dataset/triviaqa,"TriviaQA is a realistic text-based question answering dataset which includes 950K question-answer pairs from 662K documents collected from Wikipedia and the web. This dataset is more challenging than standard QA benchmark datasets such as Stanford Question Answering Dataset (SQuAD), as the answers for a question may not be directly obtained by span prediction and the context is very long. TriviaQA dataset consists of both human-verified and machine-generated QA subsets.",,Episodic Memory Reader: Learning What to Rememberfor Question Answering from Streaming Data,https://arxiv.org/abs/1903.06164,662K documents,,
3233,TrMor2018,Morphological Analysis,Morphological Analysis,Morphological Analysis,,,Methodology,,,https://github.com/ai-ku/TrMor2018,https://paperswithcode.com/dataset/trmor2018,A new high accuracy Turkish morphology dataset.,,,,,,
3234,TRN,Graph Generation,Graph Generation,Graph Generation,"Graph, Text",English,Natural Language Processing,graph-generation-on-toulouse-road-network,,https://github.com/davide-belli/toulouse-road-network-dataset,https://paperswithcode.com/dataset/trn,"The Toulouse Road Network dataset describes patches of road maps from the city of Toulouse, represented both as spatial graphs G = (A, X) and as grayscale segmentation images. 

The TRN dataset contains 111,034 data points (map tiles), of which: 80,357 are in the training set (around 72.4%), 11,679 are in the validation set (around 10.5%), 18,998 are in the test set (around 17.1%). 

Each tile represents a squared region of side 0.001 degrees of latitude and longitude on the map, which corresponds to a square of around 110 meters. The semantic segmentation of each patch is represented as a 64 × 64 grayscale image. 

The dataset is generated starting from publicly available data from OpenStreetMap. More details on the dataset characteristic and generation methods are available in our blogpost.",,https://arxiv.org/pdf/1910.14388.pdf,https://arxiv.org/pdf/1910.14388.pdf,,,
3235,Trojans_Against_Trojans__TAT_,backdoor defense,backdoor defense,backdoor defense,,,Methodology,,Custom,https://github.com/vimal-isi-edu/tat,https://paperswithcode.com/dataset/trojans-against-trojans-tat,"The dataset contains 1,200 trained ViT-B-16 models, trained on ImageNet. Half of the models are benign. The other half constitutes Trojan models, each trained with a randomly generated trigger that makes the model predict a specific target class, chosen at random for each Trojan model.",,,,,,
3236,TruthfulQA,Text Generation,Text Generation,"Text Generation, Question Answering",Text,English,Natural Language Processing,"text-generation-on-truthfulqa-0-shot, text-generation-on-truthfulqa-tr, text-generation-on-truthfulqa, question-answering-on-truthfulqa",,https://github.com/sylinrl/TruthfulQA,https://paperswithcode.com/dataset/truthfulqa,"TruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. The authors crafted questions that some humans would answer falsely due to a false belief or misconception.",,https://arxiv.org/pdf/2109.07958v1.pdf,https://arxiv.org/pdf/2109.07958v1.pdf,,,38
3237,TruthGen,Reinforcement Learning,Reinforcement Learning,"Reinforcement Learning, Text Classification","Image, Text",English,Reinforcement Learning,,CC BY 4.0,https://huggingface.co/datasets/wwbrannon/TruthGen,https://paperswithcode.com/dataset/truthgen,"TruthGen is a dataset of generated true and false statements, intended for research on truthfulness in reward models and language models, specifically in contexts where political bias is undesirable. This dataset contains 1,987 statement pairs (3,974 statements in total), with each pair containing one objectively true statement and one false statement. It spans a variety of everyday and scientific facts, excluding politically charged topics to the greatest extent possible. The dataset is particularly useful for evaluating reward models trained for alignment with truth, as well as for research on mitigating political bias while improving model accuracy on truth-related tasks.",,,,,,
3238,TruthQuest,Logical Reasoning,Logical Reasoning,Logical Reasoning,,,Reasoning,,CC-BY-4.0,https://huggingface.co/datasets/mainlp/TruthQuest,https://paperswithcode.com/dataset/truthquest,"A benchmark for suppositional reasoning based on the principles of knights and knaves puzzles. Knights and knaves problems represent a classic genre of logical puzzles where characters either tell the truth or lie. The objective is to logically deduce each character's identity based on their statements. The challenge arises from the truth-telling or lying behavior, which influences the logical implications of each statement.

TruthQuest encompasses problems of varying complexity, considering both the number of characters and the types of logical statements involved. The dataset comprises 2,400 instances. Each instance has a unique solution.",,,,400 instances,,
3239,TSAC,Sentiment Analysis,Sentiment Analysis,"Sentiment Analysis, Transliteration",Text,English,Natural Language Processing,,,https://github.com/fbougares/TSAC,https://paperswithcode.com/dataset/tsac,Tunisian Sentiment Analysis Corpus (TSAC) is a Tunisian Dialect corpus of 17.000 comments from Facebook.,,,,,,
3240,TSFM-ScalingLaws-Dataset,Time Series Prediction,Time Series Prediction,Time Series Prediction,Time Series,,Time Series,,,https://huggingface.co/datasets/Qingren/TSFM-ScalingLaws-Dataset,https://paperswithcode.com/dataset/tsfm-scalinglaws-dataset,"TSFM-ScalingLaws-Dataset

This is the dataset for the paper Towards Neural Scaling Laws for Time Series Foundation Models.

We selected some high-quality (SNR > 20), domain-balanced, and low-redundancy data from Lotsa and UTS datasets for TSFM pre-training. These datasets contain three sizes: 16B, 1B, 100M, and 10M.

Code: https://github.com/Qingrenn/TSFM-ScalingLaws

Well-trained models: https://huggingface.co/PeacefulData/TSFM-ScalingLaws-Checkpoints",,,,,,
3241,TSSB,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Change Point Detection, Time Series Analysis","Image, Time Series",,Computer Vision,change-point-detection-on-tssb,BSD-3-Clause License,https://github.com/ermshaua/time-series-segmentation-benchmark,https://paperswithcode.com/dataset/tssb,"The time series segmentation benchmark (TSSB) currently contains 75 annotated time series (TS) with 1-9 segments. Each TS is constructed from one of the UEA & UCR time series classification datasets. We group TS by label and concatenate them to create segments with distinctive temporal patterns and statistical properties. We annotate the offsets at which we concatenated the segments as change points (CPs). Addtionally, we apply resampling to control the dataset resolution and add approximate, hand-selected window sizes that are able to capture temporal patterns.",,,,,,
3242,TTE-A_O,Travel Time Estimation,Travel Time Estimation,"Travel Time Estimation, regression",,,Methodology,travel-time-estimation-on-tte-a-o,MIT,https://github.com/Eighonet/GCT-TTE,https://paperswithcode.com/dataset/tte-a-o,"The dataset includes two parts corresponding to the cities of Abakan (65524 nodes, 340012 edges) and Omsk (231688 nodes, 1149492 edges). Along with the road network graph, it includes trip records represented as sequences of visited nodes (making the dataset suitable both for path-blind and path-aware settings). There are two types of target values for a regression task: real travel time and real length of a trip.",,,,,,
3243,TUH_EEG_Seizure_Corpus,Seizure Detection,Seizure Detection,"Seizure Detection, Seizure prediction","Image, Time Series",,Computer Vision,seizure-detection-on-tuh-eeg-seizure-corpus,,https://isip.piconepress.com/projects/tuh_eeg/index.shtml,https://paperswithcode.com/dataset/tuh-eeg-seizure-corpus,"Our goal is to enable deep learning research in neuroscience by releasing the largest publicly available unencumbered database of EEG recordings. This ongoing project currently includes over 30,000 EEGs spanning the years from 2002 to present. Data collected can be used for both research and commercialization purposes.

Iyad Obeid and Joseph Picone. The temple university hospital eeg data corpus. Frontiers in neuroscience,10:196, 2016.",2002,,,,,
3244,TUM_Kitchen,Action Segmentation,Action Segmentation,"Action Segmentation, Temporal Action Localization, Action Recognition","Image, Time Series, Video",,Computer Vision,,,https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data,https://paperswithcode.com/dataset/tum-kitchen,"The TUM Kitchen dataset is an action recognition dataset that contains 20 video sequences captured by 4 cameras with overlapping views. The camera network captures the scene from four viewpoints with 25 fps, and every RGB frame is of the resolution 384×288 by pixels. The action labels are frame-wise, and provided for the left arm, the right arm and the torso separately.",,Temporal Human Action Segmentation via Dynamic Clustering,https://arxiv.org/abs/1803.05790,,,
3245,TUM_monoVO,Visual Odometry,Visual Odometry,"Visual Odometry, Monocular Visual Odometry, Simultaneous Localization and Mapping",Image,,Computer Vision,,,https://vision.in.tum.de/data/datasets/mono-dataset,https://paperswithcode.com/dataset/tum-monovo,"TUM monoVO is a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes.
All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence.
In contrast to existing datasets, all sequences are photometrically calibrated: the dataset creators provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting).",,,,,,
3246,TUM_RGB-D,Visual Odometry,Visual Odometry,"Visual Odometry, Simultaneous Localization and Mapping, Depth Estimation","3D, Image",,Computer Vision,,CC BY 4.0,https://vision.in.tum.de/data/datasets/rgbd-dataset,https://paperswithcode.com/dataset/tum-rgb-d,TUM RGB-D is an RGB-D dataset. It contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640x480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz).,,,,,,
3247,TUN-EL,Hate Speech Detection,Hate Speech Detection,"Hate Speech Detection, Hate Span Identification","Audio, Image",,Speech,,CC BY (Citation Required),https://github.com/NabilBADRI/Towards-Automatic-Detection-of-Inappropriate-Content-in-Multi-dialectic-Arabic-Text,https://paperswithcode.com/dataset/tun-el,"Arabic multi-dialectal hate speech dataset. It consists of 23,033 tweets labelled for Hate, Abusive and Normal in three Arabic dialects Egyptian, Lebaneese and Tunisian.

How to cite this work: Badri, Nabil, Ferihane Kboubi, and Anja Habacha Chaibi. ""Towards automatic detection of inappropriate content in multi-dialectic Arabic text."" International Conference on Computational Collective Intelligence. Cham: Springer International Publishing, 2022.",2022,,,,,
3248,TuPyE-Dataset,Multilabel Text Classification,Multilabel Text Classification,"Multilabel Text Classification, Binary Classification, Hate Span Identification","Image, Text",English,Computer Vision,,CC-BY-4.0,https://huggingface.co/datasets/Silly-Machine/TuPyE-Dataset,https://paperswithcode.com/dataset/tupye-dataset,"TuPyE, an enhanced iteration of TuPy, encompasses a compilation of 43,668 meticulously annotated documents specifically selected for the purpose of hate speech detection within diverse social network contexts. This augmented dataset integrates supplementary annotations and amalgamates with datasets sourced from Fortuna et al. (2019), Leite et al. (2020), and Vargas et al. (2022), complemented by an infusion of 10,000 original documents from the TuPy-Dataset.

In light of the constrained availability of annotated data in Portuguese pertaining to the English language, TuPyE is committed to the expansion and enhancement of existing datasets. This augmentation serves to facilitate the development of advanced hate speech detection models through the utilization of machine learning (ML) and natural language processing (NLP) techniques.",2019,,,,,
3249,Turing_Change_Point_Dataset,Change Point Detection,Change Point Detection,Change Point Detection,Image,,Computer Vision,,MIT,https://github.com/alan-turing-institute/TCPD,https://paperswithcode.com/dataset/turing-change-point-dataset,"Specifically designed for the evaluation of change point detection algorithms, consisting of 37 time series from various domains.",,,,,,
3250,TurkCorpus,Text Simplification,Text Simplification,Text Simplification,Text,English,Natural Language Processing,text-simplification-on-turkcorpus,,https://github.com/cocoxu/simplification/tree/master/data/turkcorpus/GEM,https://paperswithcode.com/dataset/turkcorpus,"TurkCorpus, a dataset with 2,359 original sentences from English Wikipedia, each with 8 manual reference simplifications.
The dataset is divided into two subsets: 2,000 sentences for validation and 359 for testing of sentence simplification models.",,,,000 sentences,,
3251,Turkish_Punctuation_Restoration,Punctuation Restoration,Punctuation Restoration,Punctuation Restoration,,,Methodology,,,https://github.com/uygarkurt/Turkish-Punctuation-Restoration/tree/main/multitarget-ted,https://paperswithcode.com/dataset/turkish-punctuation-restoration,"we have prepared a dataset using publicly available TED Talks transcripts [27] and selected the Turkish corpus. The resulting Turkish punctuation restoration dataset currently consists of 146K sentences and 1.8M tokens. The ratio of the train, validation, and test splits are 0.8, 0.1, and 0.1, respectively. Data files contain two columns. The first column has the tokens separated by white space. The second column includes tags for each token.",,,,146K sentences,,
3252,TURL,Paraphrase Identification,Paraphrase Identification,Paraphrase Identification,,,Methodology,paraphrase-identification-on-turl,,https://github.com/lanwuwei/Twitter-URL-Corpus,https://paperswithcode.com/dataset/twitter-news-url-corpus,"Twitter News URL Corpus is a human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification.",,https://arxiv.org/pdf/1708.00391v1.pdf,https://arxiv.org/pdf/1708.00391v1.pdf,,,
3253,TUT-SED_Synthetic_2016,Language Modelling,Language Modelling,"Language Modelling, Sound Event Detection","Audio, Image, Text",English,Computer Vision,,,https://webpages.tuni.fi/arg/paper/taslp2017-crnn-sed/tut-sed-synthetic-2016,https://paperswithcode.com/dataset/tut-sed-synthetic-2016,"TUT-SED Synthetic 2016 contains of mixture signals artificially generated from isolated sound events samples. This approach is used to get more accurate onset and offset annotations than in dataset using recordings from real acoustic environments where the annotations are always subjective.
Mixture signals in the dataset are created by randomly selecting and mixing isolated sound events from 16 sound event classes together. The resulting mixtures contains sound events with varying polyphony. All together 994 sound event samples were purchased from Sound Ideas. From the 100 mixtures created, 60% were assigned for training, 20% for testing and 20% for validation. The total amount of audio material in the dataset is 566 minutes.
Different instances of the sound events are used to synthesize the training, validation and test partitions. Mixtures were created by randomly selecting event instance and from it, randomly, a segment of length 3-15 seconds. Between events, random length silent region was introduced. Such tracks were created for four to nine event classes, and were then mixed together to form the mixture signal. As sound events are not consistently active during the samples (e.g. footsteps), automatic signal energy based annotation was applied to obtain accurate event activity within the sample. Annotation of the mixture signal was created by pooling together event activity annotation of used samples.",2016,https://arxiv.org/abs/1702.06286,https://arxiv.org/abs/1702.06286,,"training, validation and test partitions. Mixtures were created by randomly selecting event instance and from it, randomly, a segment of length 3-15 seconds. Between events, random length silent region was introduced. Such tracks were created for four to nine event classes, and were then mixed together to form the mixture signal. As sound events are not consistently active during the samples",
3254,TUT_Acoustic_Scenes_2017,Scene Classification,Scene Classification,"Scene Classification, Acoustic Scene Classification","Audio, Image",,Computer Vision,acoustic-scene-classification-on-tut-acoustic,Other (Non-Commercial),https://zenodo.org/record/400515,https://paperswithcode.com/dataset/tut-acoustic-scenes-2017,The TUT Acoustic Scenes 2017 dataset is a collection of recordings from various acoustic scenes all from distinct locations. For each recording location 3-5 minute long audio recordings are captured and are split into 10 seconds which act as unit of sample for this task. All the audio clips are recorded with 44.1 kHz sampling rate and 24 bit resolution.,2017,Ensemble of deep neural networks for acoustic scene classification,https://arxiv.org/abs/1708.05826,,,
3255,TUT_Sound_Events_2017,Language Modelling,Language Modelling,"Language Modelling, Sound Event Detection","Audio, Image, Text",English,Computer Vision,,Other (Non-Commercial),https://zenodo.org/record/400516,https://paperswithcode.com/dataset/tut-sound-events-2017,"The TUT Sound Events 2017 dataset contains 24 audio recordings in a street environment and contains 6 different classes. These classes are: brakes squeaking, car, children, large vehicle, people speaking, and people walking.",2017,Language Modelling for Sound Event Detection with Teacher Forcing and Scheduled Sampling,https://arxiv.org/abs/1907.08506,,,
3256,TUT_Sound_Events_2018,Sound Event Detection,Sound Event Detection,"Sound Event Detection, Acoustic Scene Classification, Self-Driving Cars","Audio, Image",,Computer Vision,,Other (Non-Commercial),https://zenodo.org/record/1237793,https://paperswithcode.com/dataset/tut-sound-events-2018,The TUT Sounds Event 2018 dataset consists of real-life first order Ambisonic (FOA) format recordings with stationary point sources each associated with a spatial coordinate. The dataset was generated by collecting impulse responses (IR) from a real environment using the Eigenmike spherical microphone array. The measurement was done by slowly moving a Genelec G Two loudspeaker continuously playing a maximum length sequence around the array in circular trajectory in one elevation at a time. The playback volume was set to be 30 dB greater than the ambient sound level. The recording was done in a corridor inside the university with classrooms around it during work hours. The IRs were collected at elevations −40 to 40 with 10-degree increments at 1 m from the Eigenmike and at elevations −20 to 20 with 10-degree increments at 2 m.,2018,https://www.cs.tut.fi/~mesaros/pubs/mesaros_eusipco2016-dcase.pdf,https://www.cs.tut.fi/~mesaros/pubs/mesaros_eusipco2016-dcase.pdf,,,
3257,TUT_Urban_Acoustic_Scenes_2018,Acoustic Scene Classification,Acoustic Scene Classification,Acoustic Scene Classification,"Audio, Image",,Computer Vision,acoustic-scene-classification-on-tut-urban,,,https://paperswithcode.com/dataset/dcase-2018-task-1a,"The dataset for this task is the TUT Urban Acoustic Scenes 2018 dataset, consisting of recordings from various acoustic scenes. The dataset was recorded in six large european cities, in different locations for each scene class. For each recording location there are 5-6 minutes of audio. The original recordings were split into segments with a length of 10 seconds that are provided in individual files. Available information about the recordings include the following: acoustic scene class, city, and recording location.",2018,,,,,
3258,TVQA,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Video Question Answering, Zero-Shot Video Question Answer","Text, Video",English,Natural Language Processing,"zero-shot-video-question-answer-on-tvqa, zero-shot-learning-on-tvqa, video-question-answering-on-tvqa",,http://tvqa.cs.unc.edu/,https://paperswithcode.com/dataset/tvqa,"The TVQA dataset is a large-scale video dataset for video question answering. It is based on 6 popular TV shows (Friends, The Big Bang Theory, How I Met Your Mother, House M.D., Grey's Anatomy, Castle). It includes 152,545 QA pairs from 21,793 TV show clips. The QA pairs are split into the ratio of 8:1:1 for training, validation, and test sets. The TVQA dataset provides the sequence of video frames extracted at 3 FPS, the corresponding subtitles with the video clips, and the query consisting of a question and four answer candidates. Among the four answer candidates, there is only one correct answer.",,Two-stream Spatiotemporal Feature for Video QA Task,https://arxiv.org/abs/1907.05006,,,
3259,TVQA_,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Zero-Shot Video Question Answer, Video Question Answering, Visual Question Answering (VQA), Question Answering","Image, Text, Video",English,Computer Vision,"zero-shot-video-question-answer-on-tvqa, zero-shot-learning-on-tvqa, video-question-answering-on-tvqa",,https://github.com/jayleicn/TVQAplus,https://paperswithcode.com/dataset/tvqa-1,"TVQA+ contains 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers.",,TVQA+: Spatio-Temporal Grounding for Video Question Answering,https://arxiv.org/pdf/1904.11574v2.pdf,,,
3260,TVRecap,Story Generation,Story Generation,Story Generation,Text,English,Natural Language Processing,"story-generation-on-tvmegasite-test, story-generation-on-fandom-dev, story-generation-on-tvmegasite-dev, story-generation-on-fandom-test",,https://github.com/mingdachen/TVRecap,https://paperswithcode.com/dataset/tvrecap,"TVRecap a story generation dataset that requires generating detailed TV show episode recaps from a brief summary and a set of documents describing the characters involved. Unlike other story generation datasets, TVRecap contains stories that are authored by professional screenwriters and that feature complex interactions among multiple characters. Generating stories in TVRecap requires drawing relevant information from the lengthy provided documents about characters based on the brief summary. In addition, by swapping the input and output, TVRecap can serve as a challenging testbed for abstractive summarization.",,,,,,
3261,TVSeries,Action Detection,Action Detection,"Action Detection, Online Action Detection, Action Anticipation","Image, Video",,Computer Vision,online-action-detection-on-tvseries,,https://homes.esat.kuleuven.be/psi-archive/rdegeest/TVSeries.html,https://paperswithcode.com/dataset/tvseries,"A realistic dataset composed of 27 episodes from 6 popular TV series. The dataset spans over 16 hours of footage annotated with 30 action classes, totaling 6,231 action instances.",,,,,,
3262,TvSum,Video Summarization,Video Summarization,"Video Summarization, Highlight Detection, Supervised Video Summarization, Unsupervised Video Summarization","Image, Text, Video",English,Computer Vision,"highlight-detection-on-tvsum, supervised-video-summarization-on-tvsum, unsupervised-video-summarization-on-tvsum, video-summarization-on-tvsum",Creative Commons CC-BY (v3.0) license,https://github.com/yalesong/tvsum,https://paperswithcode.com/dataset/tvsum,"Introduced by Song et al. in TVSum: Summarizing web videos using titles.

The TVSum dataset comprises 50 videos, with durations ranging from 1 to 11 minutes. These videos belong to 10 different categories associated with the TRECVid MED task, with 5 videos in each category, and were collected from YouTube. The video categories include various activities like changing a vehicle tire, making a sandwich, and flash mob gatherings. For annotation, each video was reviewed and rated by 20 users, who assigned frame-level importance scores on a scale from 1 (not important) to 5 (very important).",,,,,,
3263,Tweebank,Part-Of-Speech Tagging,Part-Of-Speech Tagging,"Part-Of-Speech Tagging, Dependency Parsing","Audio, Text",English,Speech,"part-of-speech-tagging-on-tweebank, dependency-parsing-on-tweebank",,https://github.com/Oneplus/Tweebank,https://paperswithcode.com/dataset/tweebank,"Briefly describe the dataset. Provide:


a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset

If the description or image is from a different paper, please refer to it as follows:",,,,,,
3264,TweetEval,Language Modelling,Language Modelling,"Language Modelling, Sentiment Analysis",Text,English,Natural Language Processing,sentiment-analysis-on-tweeteval,,https://github.com/cardiffnlp/tweeteval,https://paperswithcode.com/dataset/tweeteval,TweetEval introduces an evaluation framework consisting of seven heterogeneous Twitter-specific classification tasks.,,https://arxiv.org/pdf/2010.12421v2.pdf,https://arxiv.org/pdf/2010.12421v2.pdf,,,
3265,TwinSynths,Fake Image Detection,Fake Image Detection,"Fake Image Detection, Image Classification, Binary Classification, DeepFake Detection",Image,,Computer Vision,,,https://huggingface.co/datasets/koooooooook/TwinSynths,https://paperswithcode.com/dataset/twinsynths,"The TwinSynths dataset is a novel benchmark designed to overcome common limitations found in earlier synthetic image datasets, such as low image quality, inadequate content preservation, and limited class diversity. TwinSynths generates pairs of images where each synthetic image is visually identical to its real counterpart, ensuring that the essential content remains intact while showcasing the unique architectural features of the generative models used. TwinSynths comprises two subsets:

TwinSynths-GAN This subset uses a GAN generator architecture which trained from scratch on individual real images using a mean-squared error loss to ensure pixel-level fidelity. By fixing the latent vector input, the method produces synthetic images that closely mirror the original content. The GAN subset consists of 8,000 generated images spanning 80 classes selected from ImageNet.

TwinSynths-DM For the diffusion model-based subset, DDIM inversion is used to maintain the content integrity of the original images. By applying a noise-adding forward process followed by a text-conditioned denoising procedure (using class name prompts), this generates synthetic images that are highly similar to their real counterparts. The same set of ImageNet classes is used as in the GAN subset, allowing for a consistent evaluation across different generative methods.",,,,,"trained from scratch on individual real images using a mean-squared error loss to ensure pixel-level fidelity. By fixing the latent vector input, the method produces synthetic images",80
3266,TwinViews-13k,Bias Detection,Bias Detection,"Bias Detection, Fairness, Political Salient Issue Orientation Detection",Image,,Computer Vision,,CC BY 4.0,https://huggingface.co/datasets/wwbrannon/twinviews-13k/,https://paperswithcode.com/dataset/twinviews-13k,"TwinViews-13k is a dataset of 13,855 pairs of left-leaning and right-leaning political statements, each pair matched by topic. It was created to study political bias in reward and language models, with a focus on understanding the interaction between model alignment to truthfulness and the emergence of political bias. The dataset was generated using GPT-3.5 Turbo, with extensive auditing to ensure ideological balance and topical relevance. This dataset can be used for various tasks related to political bias, natural language processing, and model alignment, particularly in studies examining how political orientation impacts model outputs.",,,,,,
3267,twitch-gamers,Node Classification on Non-Homophilic (Heterophilic) Graphs,Node Classification on Non-Homophilic (Heterophilic) Graphs,"Node Classification on Non-Homophilic (Heterophilic) Graphs, Node Classification",Image,,Graphs,"node-classification-on-twitch-gamers, node-classification-on-non-homophilic-15",,,https://paperswithcode.com/dataset/twitch-gamers,node classification on twitch-gamers,,,,,,
3268,Twitter-HyDrug-UR,Hypergraph Contrastive Learning,Hypergraph Contrastive Learning,"Hypergraph Contrastive Learning, Node Classification",Image,,Computer Vision,hypergraph-contrastive-learning-on-twitter,,https://github.com/graphprojects/HyGCL-AdT,https://paperswithcode.com/dataset/twitter-hydrug-ur,"This benchmark hypergraph dataset, Twitter-HyDrug-UR,  is derived from Twitter-HyDrug by HyGCL-DC. Twitter-HyDrug-UR is a real-world hypergraph data that describes the drug trafficking on Twitter. Unlike HyGCL-DC, which targets a drug trafficking community detection task (a multi-label node classification), we aim to identify drug user roles in drug trafficking activities on social media. To this end, we categorize node labels into four distinct roles: drug seller, drug buyer, drug user, and drug discussant, and each node is assigned to one and only one label. Consequently, we frame the problem for Twitter-HyDrug-UR as a multi-class node classification task.",,,,,,
3269,Twitter_job_title_prediction,Job classification,Job classification,Job classification,Image,,Computer Vision,,cc-by-sa-4.0,,https://paperswithcode.com/dataset/twitter-dataset-for-job-title-prediction,"We introduce a dataset consisting of 1314 samples, including users’ tweets and bios. The user’s job title is found using Wikipedia crawling. The challenge of multiple job titles per user is handled using a semantic word embedding and clustering method. Then, a job prediction method is introduced based on a deep neural network and TF-IDF word embedding. We also use hashtags and emojis in the tweets for job prediction. Results show that the job title of users in Twitter could be well predicted with 54% accuracy in nine categories.",,,,1314 samples,,
3270,Twitter_POS,POS Tagging,POS Tagging,POS Tagging,,,Methodology,pos-tagging-on-twitter-pos,,,https://paperswithcode.com/dataset/twitter-pos,"K. Gimpel, N. Schneider, B. O’Connor, D. Das, D. Mills, J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan, and N. A. Smith, “Part-of-speech tagging for Twitter: Annotation, features, and experiments”, in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2011, pp. 42–47.",2011,,,,,
3271,Twitter_Sentiment_Analysis,Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),"Aspect-Based Sentiment Analysis (ABSA), Twitter Sentiment Analysis, Text Classification","Image, Text",English,Computer Vision,text-classification-on-twitter-sentiment-1,CC0: Public Domain,https://www.kaggle.com/jp797498e/twitter-entity-sentiment-analysis,https://paperswithcode.com/dataset/twitter-sentiment-analysis,"This is an entity-level Twitter Sentiment Analysis dataset. For each message, the task is to judge the sentiment of the entire sentence towards a given entity. For example, A outperforms B is positive for entity A but negative for entity B. The dataset contains ~70K labeled training messages and 1K labeled validation messages. It is available online for free on Kaggle.",,,,,,
3272,Twitter_Stance_Election_2020,Stance Detection,Stance Detection,"Stance Detection, Stance Detection (US Election 2020 - Biden), Stance Detection (US Election 2020 - Trump), Text Classification","Image, Text",English,Computer Vision,"stance-detection-us-election-2020-biden-on, stance-detection-us-election-2020-trump-on",GNU General Public License v3.0,https://github.com/GU-DataLab/stance-detection-KE-MLM,https://paperswithcode.com/dataset/twitter-stance-election-2020,"The data set contains 2500 manually-stance-labeled tweets, 1250 for each candidate (Joe Biden and Donald Trump). These tweets were sampled from the unlabeled set that our research team collected English tweets related to the 2020 US Presidential election. Through the Twitter Streaming API, the authors collected data using election-related hashtags and keywords. Between January 2020 and September 2020, over 5 million tweets were collected, not including quotes and retweets.

Paper: Knowledge Enhanced Masked Language Model for Stance Detection",2020,,,,,
3273,TYC_Dataset,Representation Learning,Representation Learning,"Representation Learning, Instance Segmentation, Unsupervised Image Segmentation, Unsupervised Pre-training, Panoptic Segmentation, Cell Segmentation",Image,,Computer Vision,,CC BY 4.0,https://christophreich1996.github.io/tyc_dataset/,https://paperswithcode.com/dataset/tyc-dataset,"We introduce the trapped yeast cell (TYC) dataset, a novel dataset for understanding instance-level semantics and motions of cells in microstructures. We release $105$ dense annotated high-resolution brightfield microscopy images, including about $19$k instance masks. We also release $261$ curated video clips composed of $1293$ high-resolution microscopy images to facilitate unsupervised understanding of cell motions and morphology.",,,,,,
3274,TyDiQA-GoldP,Cross-Lingual Question Answering,Cross-Lingual Question Answering,"Cross-Lingual Question Answering, Question Answering, Cross-Lingual Transfer, Meta-Learning",Text,English,Natural Language Processing,cross-lingual-question-answering-on-tydiqa,,https://github.com/google-research-datasets/tydiqa,https://paperswithcode.com/dataset/tydiqa-goldp,"TyDiQA is the gold passage version of the Typologically Diverse Question Answering (TyDiWA) dataset, a benchmark for information-seeking question answering, which covers nine languages. The gold passage version is a simplified version of the primary task, which uses only the gold passage as context and excludes unanswerable questions. It is thus similar to XQuAD and MLQA, while being more challenging as questions have been written without seeing the answers, leading to 3× and 2× less lexical overlap compared to XQuAD and MLQA respectively.",,XTREME,https://arxiv.org/pdf/2003.11080.pdf,,,
3275,U-10__United-10_COVID19_CT_Dataset,3D Classification,3D Classification,3D Classification,"3D, Image",,Computer Vision,3d-classification-on-u-10-united-10-covid19,,https://zenodo.org/records/14064172,https://paperswithcode.com/dataset/u-10-united-10-covid19-ct-dataset,"This dataset supports the research detailed in the pre-print ""Virtual Imaging Trials Improved the Transparency and Reliability of AI Systems in COVID-19 Imaging."" The study employs both clinical and simulated CT data to evaluate AI models for COVID-19 diagnosis. By leveraging the Virtual Imaging Trials (VIT) framework, the research addresses reproducibility and generalizability issues prevalent in medical imaging AI models.

The dataset includes:

Clinical CT Data: Drawn from 10 publicly available datasets, comprising over 12,000 volumes. These datasets span diverse populations, imaging protocols, and scanner configurations. Each of the 10 zip files contains pre-processed CT TFRecords (Train/Validation/Test)  used in the study. Detailed information about the data sources, pre-processing steps, and the inclusion and exclusion criteria can be found in the manuscript (https://arxiv.org/abs/2308.09730).
Simulated CT Data: Generated using computational anatomical phantoms from the XCAT model and imaged with the DukeSim simulation framework. This synthetic dataset allows controlled experiments that isolate the effects of imaging physics and patient-specific factors. Can be available upon request through Center For virtual Imaging Trial Portal at https://cvit.duke.edu/
The accompanying study analyzes the performance of lightweight convolutional neural networks on both real and synthetic data, comparing results across multiple internal and external validation scenarios. Insights into factors such as infection severity, imaging dose, and modality type are explored.

For further details, visit our Project Page: https://fitushar.github.io/ReviCOVID.github.io/
The full source code is available on gitHub and GitLab
GitHub: https://github.com/fitushar/CVIT_ReviCOVID19
GitLab : https://gitlab.oit.duke.edu/cvit-public/cvit_revicovid19

Citation:  When using this dataset, please cite the manuscript () and the original data-source.
Tushar et al., ""Virtual Imaging Trials Improved the Transparency and Reliability of AI Systems in COVID-19 Imaging"", arXiv:2308.09730.

Contact:  fakrulislam.tushar@duke.edu",,,,,,
3276,U-DIADS-Bib,Handwritten Document Recognition,Handwritten Document Recognition,"Handwritten Document Recognition, document understanding, Semantic Segmentation, Document Layout Analysis, 2D Semantic Segmentation","Image, Text",English,Computer Vision,document-layout-analysis-on-u-diads-bib,,https://ai4ch.uniud.it/udiadsbib/,https://paperswithcode.com/dataset/u-diads-bib,"U-DIADS-Bib is a proprietary dataset developed through the collaboration of computer scientists and humanities at the University of Udine. It is composed of 200 images, 50 for each of the 4 different manuscripts that characterize it. These handwritten books were selected in collaboration with humanist partners considering both the complexity of their layout and the presence of significant and semantically distinguishable elements. In particular, the images of the four manuscripts were collected from the digital library Gallica. All manuscripts are Latin and Syriac Bibles published between the 6th and 12th centuries A.D.",,,,200 images,,
3277,U2OS,Cell Segmentation,Cell Segmentation,Cell Segmentation,Image,,Computer Vision,,,https://murphylab.web.cmu.edu/data/2009_ISBI_Nuclei.html,https://paperswithcode.com/dataset/u2os,The archive contains original images from U2OS cells stained with Hoechst 33342 as PNG files. It also contains images (as Photoshop and GIMP files) showing hand-segmentation of the Hoechst images into regions containing single nuclei.,,,,,,
3278,UA-DETRAC,Object Detection,Object Detection,"Object Detection, Multiple Object Tracking, Object Tracking","Image, Video",,Computer Vision,object-detection-on-ua-detrac,,,https://paperswithcode.com/dataset/ua-detrac,"Consists of 100 challenging video sequences captured from real-world traffic scenes (over 140,000 frames with rich annotations, including occlusion, weather, vehicle category, truncation, and vehicle bounding boxes) for object detection, object tracking and MOT system.",,,,,,
3279,UA-GEC,Grammatical Error Correction,Grammatical Error Correction,Grammatical Error Correction,,,Methodology,grammatical-error-correction-on-ua-gec,CC-BY-4.0,https://github.com/grammarly/ua-gec,https://paperswithcode.com/dataset/ua-gec,UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language,,,,,,
3280,UAV-Human,Pedestrian Attribute Recognition,Pedestrian Attribute Recognition,"Pedestrian Attribute Recognition, Skeleton Based Action Recognition, Pose Estimation, 3D Action Recognition, Person Re-Identification, Action Recognition, 2D Human Pose Estimation","3D, Image, Video",,Computer Vision,"skeleton-based-action-recognition-on-uav, pedestrian-attribute-recognition-on-uav-human, person-re-identification-on-uav-human, action-recognition-on-uav-human, pose-estimation-on-uav-human",,https://github.com/SUTDCV/UAV-Human,https://paperswithcode.com/dataset/uav-human,"UAV-Human is a large dataset for human behavior understanding with UAVs. It contains 67,428 multi-modal video sequences and 119 subjects for action recognition, 22,476 frames for pose estimation, 41,290 frames and 1,144 identities for person re-identification, and 22,263 frames for attribute recognition. The dataset was collected by a flying UAV in multiple urban and rural districts in both daytime and nighttime over three months, hence covering extensive diversities w.r.t subjects, backgrounds, illuminations, weathers, occlusions, camera motions, and UAV flying attitudes. This dataset can be used for UAV-based human behavior understanding, including action recognition, pose estimation, re-identification, and attribute recognition.",,,,,,
3281,UAVDT,Object Detection,Object Detection,"Object Detection, Object Tracking, Multi-Object Tracking","Image, Video",,Computer Vision,"object-detection-on-uavdt, multi-object-tracking-on-uavdt",Custom (research-only),https://sites.google.com/view/grli-uavdt/%E9%A6%96%E9%A1%B5,https://paperswithcode.com/dataset/uavdt,"UAVDT is a large scale challenging UAV Detection and Tracking benchmark (i.e., about 80, 000 representative frames from 10 hours raw videos) for 3 important fundamental tasks, i.e., object DETection
(DET), Single Object Tracking (SOT) and Multiple Object Tracking (MOT).

The dataset is captured by UAVs in various complex scenarios. The objects of
interest in this benchmark are vehicles. The frames are manually annotated with bounding boxes and some useful attributes, e.g., vehicle category and occlusion. 

The UAVDT benchmark consists of 100 video sequences, which are selected
from over 10 hours of videos taken with an UAV platform at a number of locations in urban areas, representing various common scenes including squares, arterial streets, toll stations, highways, crossings and T-junctions. The videos
are recorded at 30 frames per seconds (fps), with the JPEG image resolution of 1080 × 540 pixels.",,,,,,
3282,UAVid,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Scene Understanding, Scene Segmentation, Autonomous Driving",Image,,Computer Vision,"scene-segmentation-on-uavid, semantic-segmentation-on-uavid",,https://uavid.nl/,https://paperswithcode.com/dataset/uavid,"UAVid is a high-resolution UAV semantic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. The UAV dataset consists of 30 video sequences capturing 4K high-resolution images in slanted views. In total, 300 images have been densely labeled with 8 classes for the semantic labeling task.",,,,300 images,,8
3283,UBFC-rPPG,Heart Rate Variability,Heart Rate Variability,"Heart Rate Variability, Photoplethysmography (PPG), Heart rate estimation, Photoplethysmography (PPG) heart rate estimation, Photoplethysmography (PPG) beat detection",Image,,Computer Vision,photoplethysmography-ppg-heart-rate-1,,https://sites.google.com/view/ybenezeth/ubfcrppg,https://paperswithcode.com/dataset/ubfc-rppg,"We introduce here a new database called UBFC-rPPG (stands for Univ. Bourgogne Franche-Comté Remote PhotoPlethysmoGraphy) comprising two datasets that are focused specifically on rPPG analysis. The UBFC-RPPG database was created using a custom C++ application for video acquisition with a simple low cost webcam (Logitech C920 HD Pro) at 30fps with a resolution of 640x480 in uncompressed 8-bit RGB format. A CMS50E transmissive pulse oximeter was used to obtain the ground truth PPG data comprising the PPG waveform as well as the PPG heart rates. During the recording, the subject sits in front of the camera (about 1m away from the camera) with his/her face visible. All experiments are conducted indoors with a varying amount of sunlight and indoor illumination. The link to download the complete video dataset is available on request. A basic Matlab implementation can also be provided to read ground truth data acquired with a pulse oximeter.",,,,,,
3284,UBI-Fights,Abnormal Event Detection In Video,Abnormal Event Detection In Video,"Abnormal Event Detection In Video, Semi-supervised Anomaly Detection, Anomaly Detection In Surveillance Videos, Anomaly Detection, Weakly Supervised Classification, Semi-Supervised Video Classification, Unsupervised Anomaly Detection","Image, Video",,Computer Vision,"abnormal-event-detection-in-video-on-ubi, semi-supervised-anomaly-detection-on-ubi",Public,http://socia-lab.di.ubi.pt/EventDetection/,https://paperswithcode.com/dataset/ubi-fights,"UBI-Fights - Concerning a specific anomaly detection and still providing a wide diversity in fighting scenarios, the UBI-Fights dataset is a unique new large-scale dataset of 80 hours of video fully annotated at the frame level. Consisting of 1000 videos, where 216 videos contain a fight event, and 784 are normal daily life situations. All unnecessary video segments (e.g., video introductions, news, etc.) that could disturb the learning process were removed.",,,,,,
3285,UBIRIS.v2,Iris Segmentation,Iris Segmentation,"Iris Segmentation, Iris Recognition",Image,,Computer Vision,,,http://iris.di.ubi.pt/ubiris2.html,https://paperswithcode.com/dataset/ubiris-v2,"The UBIRIS.v2 iris dataset contains 11,102 iris images from 261 subjects with 10 images each subject. The images were captured under unconstrained conditions (at-a-distance, on-the-move and on the visible wavelength), with realistic noise factors.",,Constrained Design of Deep Iris Networks,https://arxiv.org/abs/1905.09481,10 images,,
3286,Ubuntu_IRC,Conversational Response Selection,Conversational Response Selection,Conversational Response Selection,,,Methodology,conversational-response-selection-on-ubuntu-3,,https://github.com/jkkummerfeld/irc-disentanglement,https://paperswithcode.com/dataset/ubuntu-irc-1,"The Ubuntu IRC dataset is a valuable resource for research in natural language understanding and dialogue systems. Let me provide you with some details:



Ubuntu Dialogue Corpus:


This dataset contains almost 1 million multi-turn dialogues, comprising over 7 million utterances and 100 million words.
It serves as a unique resource for building dialogue managers based on neural language models that can leverage large amounts of unlabeled data³.
The dialogues are sourced from IRC (Internet Relay Chat) conversations related to Ubuntu, a popular open-source operating system.
Researchers can use this corpus to explore various aspects of dialogue understanding and generation.



Specifics of the Ubuntu IRC Dataset:


The dataset includes 77,563 annotated messages from IRC.
Most of these messages originate from the Ubuntu IRC Logs for the #ubuntu channel.
Additionally, a smaller subset is a re-annotation of data from the #linux channel, which was originally collected by Elsner and Charniak in 2008².
You can find this dataset in the kummerfeld/data folder of the repository².



In summary, the Ubuntu IRC dataset provides a rich collection of dialogues that researchers can use to advance the field of natural language processing and dialogue modeling. 🌐🗣️

(1) The Ubuntu Dialogue Corpus: A Large Dataset for Research in .... https://arxiv.org/abs/1506.08909.
(2) GitHub - amarazad/DSRNet: Meta-Context Transformers for Domain-Specific .... https://github.com/amarazad/DSRNet.
(3) Ubuntu Dialogue Corpus | Kaggle. https://www.kaggle.com/datasets/rtatman/ubuntu-dialogue-corpus.
(4) GitHub - jkkummerfeld/irc-disentanglement: Dataset and model for .... https://github.com/jkkummerfeld/irc-disentanglement.",,,,,,
3287,UCCS,Open Set Learning,Open Set Learning,"Open Set Learning, Face Detection, open-set classification, Face Recognition",Image,,Computer Vision,,,https://vast.uccs.edu/Opensetface/,https://paperswithcode.com/dataset/uccs,"Unconstrained Face Detection and Open-Set Face Recognition Challenge


Paper: https://arxiv.org/abs/1708.02337
Official website: https://vast.uccs.edu/Opensetface
UnOfficial website: https://exposing.ai/uccs

Face detection and recognition benchmarks have shifted toward more difficult environments. The challenge presented in this paper addresses the next step in the direction of automatic detection and identification of people from outdoor surveillance cameras. While face detection has shown remarkable success in images collected from the web, surveillance cameras include more diverse occlusions, poses, weather conditions and image blur. Although face verification or closed-set face identification have surpassed human capabilities on some datasets, open-set identification is much more complex as it needs to reject both unknown identities and false accepts from the face detector. We show that unconstrained face detection can approach high detection rates albeit with moderate false accept rates. By contrast, open-set face recognition is currently weak and requires much more attention.",,https://arxiv.org/abs/1708.02337,https://arxiv.org/abs/1708.02337,,,
3288,UCF-CC-50,Crowd Counting,Crowd Counting,"Crowd Counting, Density Estimation, Human Detection",Image,,Computer Vision,,Custom (non-commercial),https://www.crcv.ucf.edu/data/ucf-cc-50/,https://paperswithcode.com/dataset/ucf-cc-50-1,"UCF-CC-50 is a dataset for crowd counting and consists of images of extremely dense crowds. It has 50 images with 63,974 head center annotations in total. The head counts range between 94 and 4,543 per image. The small dataset size and large variance make this a very challenging counting dataset.",,Active Crowd Counting with Limited Supervision,https://arxiv.org/abs/2007.06334,50 images,,
3289,UCF-Crime,Video Anomaly Detection,Video Anomaly Detection,"Video Anomaly Detection, Anomaly Detection In Surveillance Videos, Anomaly Detection, Multiple Instance Learning, Small Data Image Classification","Image, Video",,Computer Vision,"anomaly-detection-on-ucf-crime-1, video-anomaly-detection-on-ucf-crime-2, anomaly-detection-in-surveillance-videos-on",,https://www.crcv.ucf.edu/research/real-world-anomaly-detection-in-surveillance-videos/,https://paperswithcode.com/dataset/ucf-crime,"The UCF-Crime dataset is a large-scale dataset of 128 hours of videos. It consists of 1900 long and untrimmed real-world surveillance videos, with 13 realistic anomalies including Abuse, Arrest, Arson, Assault, Road Accident, Burglary, Explosion, Fighting, Robbery, Shooting, Stealing, Shoplifting, and Vandalism. These anomalies are selected because they have a significant impact on public safety. 

This dataset can be used for two tasks. First, general anomaly detection considering all anomalies in one group and all normal activities in another group. Second, for recognizing each of 13 anomalous activities.",1900,,,,,
3290,UCF101,Action Recognition In Videos,Action Recognition In Videos,"Action Recognition In Videos, Self-Supervised Action Recognition, Open Set Action Recognition, Image Clustering, Prompt Engineering, Zero-Shot Action Recognition, Transductive Zero-Shot Classification, Few Shot Action Recognition, Video Frame Interpolation, Video Generation, Human Activity Recognition, Action Recognition, Self-Supervised Action Recognition Linear, Early Action Prediction, Text-to-Video Generation, Zero-Shot Learning, Skeleton Based Action Recognition, Action Classification, Temporal Action Localization, Few-Shot Learning, Self-supervised Video Retrieval","Image, Text, Time Series, Video",English,Computer Vision,"prompt-engineering-on-ucf101, text-to-video-generation-on-ucf-101, video-generation-on-ucf-101-16-frames, action-recognition-in-videos-on-ucf101-2, self-supervised-action-recognition-on-ucf101-1, open-set-action-recognition-on-ucf101-mitv2, zero-shot-action-recognition-on-ucf101, human-activity-recognition-on-ucf-101, image-clustering-on-ucf101, action-recognition-in-videos-on-ucf101, action-recognition-on-ucf-101, action-recognition-in-videos-on-ucf-101, action-classification-on-ucf101, early-action-prediction-on-ucf101, self-supervised-action-recognition-on-ucf101, few-shot-action-recognition-on-ucf101, skeleton-based-action-recognition-on-ucf101, video-frame-interpolation-on-ucf101-1, self-supervised-action-recognition-linear-on, self-supervised-video-retrieval-on-ucf101, video-generation-on-ucf-101, few-shot-learning-on-ucf101, zero-shot-learning-on-ucf101, video-generation-on-ucf-101-16-frames-64x64, video-generation-on-ucf-101-16-frames-128x128, transductive-zero-shot-classification-on-9",MIT,https://www.crcv.ucf.edu/data/UCF101.php,https://paperswithcode.com/dataset/ucf101,"UCF101 dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 × 240.",,Two-stream Collaborative Learning with Spatial-Temporal Attention for Video Classification,https://arxiv.org/abs/1711.03273,,,101
3291,UCI_Machine_Learning_Repository,Density Estimation,Density Estimation,"Density Estimation, Core set discovery, Gaussian Processes, Time Series Classification, Multivariate Time Series Imputation, Synthetic Data Generation","Image, Text, Time Series",English,Computer Vision,"density-estimation-on-uci-miniboone, core-set-discovery-on-uci-gas, density-estimation-on-uci-power, density-estimation-on-uci-gas, density-estimation-on-uci-hepmass, gaussian-processes-on-uci-power, time-series-classification-on-uci-epileptic, multivariate-time-series-imputation-on-uci, synthetic-data-generation-on-uci-epileptic-1",,http://archive.ics.uci.edu/ml/datasets.php,https://paperswithcode.com/dataset/uci-machine-learning-repository,UCI Machine Learning Repository is a collection of over 550 datasets.,,,,,,
3292,UCLA_Aerial_Event_Dataset,Bayesian Inference,Bayesian Inference,"Bayesian Inference, Trajectory Prediction",Time Series,,Methodology,,,http://www.stat.ucla.edu/~tianmin.shu/AerialVideo/AerialVideo.html,https://paperswithcode.com/dataset/ucla-aerial-event-dataset,"The UCLA Aerial Event Dataest has been captured by a low-cost hex-rotor with a GoPro camera, which is able to eliminate the high frequency vibration of the camera and hold in air autonomously through a GPS and a barometer. It can also fly 20 ∼ 90m above the ground and stays 5 minutes in air. 

This hex-rotor has been used to take the set of videos in the dataset, captured in different places: hiking routes, parking lots, camping sites, picnic areas with shelters, restrooms, tables, trash bins and BBQ ovens. By detecting/tracking humans and objects in the videos, the videos can be annotated with events.

The original videos are pre-processed, including camera calibration and frame registration. After pre-processing, there are totally 27 videos in the dataset, the length of which ranges from 2 minutes to 5 minutes. Each video has annotations with hierarchical semantic information of objects, roles, events and groups in the videos.",,,,,,
3293,UCR_Anomaly_Archive,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Time Series Anomaly Detection","Image, Time Series",,Computer Vision,"anomaly-detection-on-ucr-anomaly-archive, time-series-anomaly-detection-on-ucr-anomaly",,https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/,https://paperswithcode.com/dataset/ucr-anomaly-archive,"The UCR Anomaly Archive is a collection of 250 uni-variate time series collected in human medicine, biology, meteorology and industry. The collected time series contain a few natural anomalies though the majority of the anomalies are artificial . The dataset was first used in an anomaly detection contest preceding the ACM SIGKDD conference 2021.
Each of the time series contains exactly one, occasionally subtle anomaly after a given time stamp. The data before that timestamp can be considered normal.
The time series collected in the UCR Anomaly Archive can be categorized into 12 types originating from the four domains human medicine, meteorology, biology and industry. The distribution across the domains is highly imbalanced with around 64% of the times series being collected in human medicine applications, 22% in biology, 9% in industry and 5% being air temperature measurements. The time series within a single type (e.g. ECG) are not completely unique, but differ in terms of injected anomalies or a modification of the original time series through added Gaussian noise and wandering baselines. 

The downloadable archive contains, among other supplemental material, a set of slides explaining the injected anomalies with examples.",2021,,,,,
3294,UCR_Time_Series_Classification_Archive,Audio Classification,Audio Classification,"Audio Classification, Time Series Averaging, ECG Classification, Time Series Clustering, Time Series Classification","Audio, Image, Time Series",,Audio,"ecg-classification-on-ucr-time-series, audio-classification-on-ucr-time-series",,https://www.cs.ucr.edu/~eamonn/time_series_data_2018/,https://paperswithcode.com/dataset/ucr-time-series-classification-archive,"The UCR Time Series Archive - introduced in 2002,
has become an important resource in the time series data mining
community, with at least one thousand published papers making
use of at least one data set from the archive. The original
incarnation of the archive had sixteen data sets but since that
time, it has gone through periodic expansions. The last expansion
took place in the summer of 2015 when the archive grew from
45 to 85 data sets. This paper introduces and will focus on the
new data expansion from 85 to 128 data sets. Beyond expanding
this valuable resource, this paper offers pragmatic advice to
anyone who may wish to evaluate a new algorithm on the archive.
Finally, this paper makes a novel and yet actionable claim: of the
hundreds of papers that show an improvement over the standard
baseline (1-nearest neighbor classification), a large fraction may
be misattributing the reasons for their improvement. Moreover,
they may have been able to achieve the same improvement with
a much simpler modification, requiring just a single line of code.",2002,,,,,
3295,UCY,Trajectory Forecasting,Trajectory Forecasting,"Trajectory Forecasting, Trajectory Prediction, motion prediction","Time Series, Video",,Methodology,trajectory-prediction-on-ucy,,https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data.html,https://paperswithcode.com/dataset/ucy,"The UCY dataset consist of real pedestrian trajectories with rich multi-human interaction scenarios captured at 2.5 Hz (Δt=0.4s). It is composed of three sequences (Zara01, Zara02, and UCY), taken in public spaces from top-view.",,Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data,https://arxiv.org/abs/2001.03093,,,
3296,Udacity,Steering Control,Steering Control,Steering Control,,,Methodology,steering-control-on-udacity,,https://github.com/udacity/self-driving-car/tree/master/datasets,https://paperswithcode.com/dataset/udacity,"The Udacity dataset is mainly composed of video frames taken from urban roads. It provides a total number of 404,916 video frames for training and 5,614 video frames for testing. This dataset is challenging due to severe lighting changes, sharp road curves and busy traffic.",,Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks,https://arxiv.org/abs/1811.02759,,,
3297,UDC,Answer Selection,Answer Selection,"Answer Selection, Conversational Response Selection, Dialogue Generation",Text,English,Natural Language Processing,"dialogue-generation-on-ubuntu-dialogue-tense, conversational-response-selection-on-ubuntu-2, answer-selection-on-ubuntu-dialogue-v2, dialogue-generation-on-ubuntu-dialogue-cmd, conversational-response-selection-on-ubuntu-1, dialogue-generation-on-ubuntu-dialogue, dialogue-generation-on-ubuntu-dialogue-entity, answer-selection-on-ubuntu-dialogue-v1",,https://github.com/npow/ubottu,https://paperswithcode.com/dataset/ubuntu-dialogue-corpus,"Ubuntu Dialogue Corpus (UDC) is a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter.",,The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems,https://arxiv.org/pdf/1506.08909v3.pdf,,,
3298,UDE-DomainNet,Unsupervised Domain Expansion,Unsupervised Domain Expansion,Unsupervised Domain Expansion,,,Methodology,unsupervised-domain-expansion-on-ude,,https://github.com/li-xirong/ude,https://paperswithcode.com/dataset/ude-domainnet,"Different from the setting of domain adaptation which uses all labeled source and unlabeled target domain examples for training,  domain examples should be divided into two disjoint parts: training and test.  UDE-DomainNet is built from DomainNet, so the performance of domain-adapted or domain-expanded models on the source and target domain can be evaluated.",,,,,"training,  domain examples",
3299,UDE-Office-Home,Unsupervised Domain Expansion,Unsupervised Domain Expansion,Unsupervised Domain Expansion,,,Methodology,unsupervised-domain-expansion-on-ude-office,,https://github.com/li-xirong/ude,https://paperswithcode.com/dataset/ude-office-home,"Different from the setting of domain adaptation which uses all labeled source and unlabeled target domain examples for training,  domain examples should be divided into two disjoint parts: training and test.  UDE-Office-Home is built from Office-Home, so the performance of domain-adapted or domain-expanded models on the source and target domain can be evaluated.",,,,,"training,  domain examples",
3300,UDED,Edge Detection,Edge Detection,Edge Detection,Image,,Computer Vision,edge-detection-on-uded,,https://github.com/xavysp/UDED,https://paperswithcode.com/dataset/uded,"This dataset is a collection of 1, 2, or 3 images from: BIPED, BSDS500, BSDS300, DIV2K, WIRE-FRAME, CID, CITYSCAPES, ADE20K, MDBD, NYUD, THANGKA, PASCAL-Context, SET14, URBAN10, and the camera-man image. The image selection process consists on computing the Inter-Quartile Range (IQR) intensity value on all the images, images larger than 720×720 pixels were not considered. In dataset whose images are in HR, they were cut. We thank all the datasets owners to make them public. This dataset is just for Edge Detection not contour nor Boundary tasks.",,,,3 images,"value on all the images, images",
3301,UFPR-ALPR,License Plate Detection,License Plate Detection,"License Plate Detection, Optical Character Recognition (OCR), License Plate Recognition, Object Tracking, Scene Text Recognition","Image, Text, Video",English,Computer Vision,license-plate-recognition-on-ufpr-alpr,Research Only,https://web.inf.ufpr.br/vri/databases/ufpr-alpr/,https://paperswithcode.com/dataset/ufpr-alpr,"This dataset includes 4,500 fully annotated images (over 30,000 license plate characters) from 150 vehicles in real-world scenarios where both the vehicle and the camera (inside another vehicle) are moving.

The images were acquired with three different cameras and are available in the Portable Network Graphics (PNG) format with a size of 1,920 × 1,080 pixels. The cameras used were: GoPro Hero4 Silver, Huawei P9 Lite, and iPhone 7 Plus.

We collected 1,500 images with each camera, divided as follows:

- 900 of cars with gray license plates;
- 300 of cars with red license plates;
- 300 of motorcycles with gray license plates.

The dataset is split as follows: 40% for training, 40% for testing and 20% for validation. Every image has the following annotations available in a text file: the camera in which the image was taken, the vehicle’s position and information such as type (car or motorcycle), manufacturer, model and year; the identification and position of the license plate, as well as the position of its characters.",,,,500 images,,
3302,UFPR-Periocular,Iris Recognition,Iris Recognition,"Iris Recognition, Mobile Periocular Recognition",Image,,Computer Vision,,Research Only,https://web.inf.ufpr.br/vri/databases/ufpr-periocular/,https://paperswithcode.com/dataset/ufpr-periocular,"The UFPR-Periocular dataset has 16,830 images of both eyes (33,660 cropped images of each eye) from 1,122 subjects (2,244 classes).

All the images were captured by the participant using their own smartphone through a mobile application (app) developed by the authors.
There are 15 samples from each subject's eye, obtained in 3 sessions (5 images per session) with a minimum interval of 8 hours between the sessions.

The images were collected from June 2019 to January 2020 and have several resolutions varying from 360×160 to 1862×1008 pixels – depending on the mobile device used to capture the image. In total, the dataset has images captured from 196 different mobile devices.

Each subject captured their images using the same device model. This dataset's main intra- and inter-class variability are caused by lighting variation, occlusion, specular reflection, blur, motion blur, eyeglasses, off-angle, eye-gaze, makeup, and facial expression.

The authors manually annotated the eye corner of all images with 4 points (inside and outside eye corners) and used it to normalize the periocular region regarding scale and rotation. All the original and cropped periocular images, eye-corner annotations, and experimental protocol files are publicly available for the research community (upon request).

The paper contains information about images' distributions by gender, age, resolution, and other experiments' details and benchmarks.",2019,,,830 images,,244
3303,UGIF,Instruction Following,Instruction Following,Instruction Following,,,Methodology,,,https://github.com/google-research/google-research/tree/master/ugif,https://paperswithcode.com/dataset/ugif,"UGIF is a multi-lingual, multi-modal UI grounded dataset for step-by-step task completion on the smartphone. It contains 523 natural language instructions with paired sequences of multilingual UI screens and actions that show how to execute the task in eight languages.",,UGIF: UI Grounded Instruction Following,https://arxiv.org/pdf/2211.07615v1.pdf,,,
3304,UHDM,Image Restoration,Image Restoration,"Image Restoration, Image Enhancement",Image,,Computer Vision,image-restoration-on-uhdm,,https://xinyu-andy.github.io/uhdm-page/,https://paperswithcode.com/dataset/uhdm,"The first ultra-high-definition image demoireing dataset,  consisting of 4,500 4K resolution training pairs and 500 standard 4K resolution validation pairs.",,,,,,
3305,UHGEvalDataset,Text Generation,Text Generation,"Text Generation, Hallucination Evaluation",Text,English,Natural Language Processing,,Apache-2.0 license,https://github.com/IAAR-Shanghai/UHGEval,https://paperswithcode.com/dataset/xinhuahallucinations,UHGEvalDataset contains over 5000 news items. It can be used in hallucination evaluation or detection tasks.,,,,,,
3306,UIEB,Object Detection,Object Detection,"Object Detection, Saliency Prediction, Image Enhancement, Single Image Dehazing","Image, Time Series",,Computer Vision,single-image-dehazing-on-uieb,,https://li-chongyi.github.io/proj_benchmark.html,https://paperswithcode.com/dataset/uieb,"Includes 950 real-world underwater images, 890 of which have the corresponding reference images.",,,,,,
3307,UIT-ViSFD,Opinion Mining,Opinion Mining,"Opinion Mining, Aspect-Based Sentiment Analysis (ABSA), Sentiment Analysis (Product + User)",Text,English,Natural Language Processing,,CC BY-NC-SA 4.0,https://sites.google.com/uit.edu.vn/kietnv/datasets,https://paperswithcode.com/dataset/uit-visfd,"UIT-ViSFD is a Vietnamese Smartphone Feedback Dataset as a new benchmark corpus built based on strict annotation schemes for evaluating aspect-based sentiment analysis, consisting of 11,122 human-annotated comments for mobile e-commerce, which is freely available for research purposes.",,,,,,
3308,UJ-CS_Math_Phy,Definition Modelling,Definition Modelling,"Definition Modelling, Definition Extraction",,,Methodology,,,https://github.com/jeffhj/CDM,https://paperswithcode.com/dataset/uj-cs-math-phy,"Definitions of jargon/terms in computer science, mathematics, and physics",,,,,,
3309,UKP,Language Modelling,Language Modelling,"Language Modelling, Argument Mining, Decision Making",Text,English,Natural Language Processing,,,https://www.informatik.tu-darmstadt.de/ukp/research_6/data/argumentation_mining_1/argument_annotated_essays/index.en.jsp,https://paperswithcode.com/dataset/ukp,The UKP Argument Annotated Essays corpus consists of argument annotated persuasive essays including annotations of argument components and argumentative relations.,,https://www.aclweb.org/anthology/C14-1142.pdf,https://www.aclweb.org/anthology/C14-1142.pdf,,,
3310,ULP_Dataset,backdoor defense,backdoor defense,backdoor defense,,,Methodology,,,https://umbcvision.github.io/Universal-Litmus-Patterns/,https://paperswithcode.com/dataset/ulp-dataset,"Hundreds of clean and poisoned models per dataset for Tiny-ImageNet, CIFAR10",,,,,,
3311,Ultra-processed_Food_Dataset,Predictive Process Monitoring,Predictive Process Monitoring,"Predictive Process Monitoring, Causal Discovery",,,Methodology,,,https://github.com/giovanniMen/CPCaD-Bench,https://paperswithcode.com/dataset/ultra-processed-food-dataset,"The raw data are obtained from an industrial plant for ultra-processed food production. The sampling was carried
out every 5 minutes while the total production cycle takes
approximately 3 hours, from raw ingredients to final semi-
finished products. The extracted data represent approximately
80 days of production.  Variables 2 − 14 belonging to 4 specific phases of the process and
influence the qualitative variable 17. Variables 15 and 16 are
external variables not controlled by the process which affect
the final product. It should also be noted that some variation
may be due to changes in raw materials, in production flow
(variable 1) or to possible reconfiguration between weeks.
However while the magnitude of effects may change between
weeks, the causal relationships are dictated by the plant and
process dynamics and are consistent (at the best of potential
un-cofounder and faults) throughout the production .",,,,,,
3312,UMDFaces,Face Detection,Face Detection,"Face Detection, Face Verification, Face Recognition",Image,,Computer Vision,,,https://www.umdfaces.io/,https://paperswithcode.com/dataset/umdfaces,"UMDFaces is a face dataset divided into two parts:


Still Images - 367,888 face annotations for 8,277 subjects.
Video Frames - Over 3.7 million annotated video frames from over 22,000 videos of 3100 subjects.

Part 1 - Still Images

The dataset contains 367,888 face annotations for 8,277 subjects divided into 3 batches. The annotations contain human curated bounding boxes for faces and estimated pose (yaw, pitch, and roll), locations of twenty-one keypoints, and gender information generated by a pre-trained neural network.

Part 2 - Video Frames

The second part contains 3,735,476 annotated video frames extracted from a total of 22,075 for 3,107 subjects. The annotations contain the estimated pose (yaw, pitch, and roll), locations of twenty-one keypoints, and gender information generated by a pre-trained neural network.",,,,,,
3313,UML_Classes_With_Specs,Model extraction,Model extraction,Model extraction,,,Methodology,model-extraction-on-uml-classes-with-specs,,https://github.com/XsongyangX/uml-classes-and-specs,https://paperswithcode.com/dataset/uml-classes-with-specs,"Repository for UML-English data
This repository contains the data used for ""Extraction of UML Class Diagrams from Natural Language Specification"" (Yang et al. 2022)

Getting the dataset
To get the entire dataset, you must download the release containing dataset.tar.gz.

Structure of the dataset

dataset.tar.gz: archive that contains all the files
fragments.csv: file that lists UML fragments and their characteristics
labels.csv: file that contains the labels received in the crowdsourcing effort
models.csv: file that lists UML class diagrams and their characteristics
zoo/: folder that contains all the UML data itself, such as pictures and UML encodings

Making use of the dataset
Unzip the tarball first.

Opening the image of a certain UML model
Open models.csv to read the list of available models. Copy its name and search in the zoo/ folder for .png files starting with that name. For example, the ACME model has an image in the zoo/ folder called ACME.png.

bash
ls zoo/ACME.png
code zoo/ACME.png # any other image visualizer

Opening the image of a certain fragment
Fragment files are named in the following pattern.

Class fragments:
(ModelName)_(class)(number).png

Relationship fragments:
(ModelName)_(rel)(number).png

Similarly, you can visualize them.
bash
code zoo/CFG_class0.png

Finding the image of a fragment starting from a label

Browse through labels.csv and find the line that has the label of interest.
Every label has a fragment_id, which can be indexed in fragments.csv. Find the ID for the label of interest.
Inside fragments.csv, search for the line where the column value of unique_id equals fragment_id from Step 2.
Proceed like in the previous section",2022,,,,,
3314,UMVM,Multi-modal Entity Alignment,Multi-modal Entity Alignment,"Multi-modal Entity Alignment, Entity Alignment, Knowledge Graphs",,,Methodology,"multi-modal-entity-alignment-on-umvm-dbp-zh, multi-modal-entity-alignment-on-umvm-oea-en, multi-modal-entity-alignment-on-umvm-dbp-ja, multi-modal-entity-alignment-on-umvm-oea-d-w-1, multi-modal-entity-alignment-on-umvm-oea-en-1, multi-modal-entity-alignment-on-umvm-oea-d-w, multi-modal-entity-alignment-on-umvm-dbp-fr",MIT,https://github.com/zjukg/UMAEA,https://paperswithcode.com/dataset/mmea-umvm,"We present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM.

To create our MMEA-UMVM(uncertainly missing visual modality) datasets, we perform random image dropping on MMEA datasets. Specifically, we randomly discard entity images to achieve varying degrees of visual modality missing, ranging from 0.05 to the maximum $R_{img}$ of the raw datasets with a step of 0.05 or 0.1. 
Finally, we get a total number of 97 data split. 

Refer to the following paper for more details: Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment",,Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment,https://arxiv.org/abs/2307.16210,,,
3315,Unbalance_Classification_Using_Vibration_Data,Time Series Analysis,Time Series Analysis,"Time Series Analysis, Fault Detection","Image, Time Series",,Time Series,,CC BY 4.0,https://fordatis.fraunhofer.de/handle/fordatis/151.2,https://paperswithcode.com/dataset/unbalance-classification-using-vibration-data,"This dataset contains vibration data recorded on a rotating drive train. This drive train consists of an electronically commutated DC motor and a shaft driven by it, which passes through a roller bearing. With the help of a 3D-printed holder, unbalances with different weights and different radii were attached to the shaft. Besides the strength of the unbalances, the rotation speed of the motor was also varied. This dataset can be used to develop and test algorithms for the automatic detection of unbalances on drive trains. Datasets for 4 differently sized unbalances and for the unbalance-free case were recorded. The vibration data was recorded at a sampling rate of 4096 values per second. Datasets for development (ID ""D[0-4]"") as well as for evaluation (ID ""E[0-4]"") are available for each unbalance strength. The rotation speed was varied between approx. 630 and 2330 RPM in the development datasets and between approx. 1060 and 1900 RPM in the evaluation datasets. For each measurement of the development dataset there are approx. 107min of continuous measurement data available, for each measurement of the evaluation dataset 28min.",1900,,,,,
3316,Uncertainty_Quantification_for_Underwater_Object_S,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Uncertainty Quantification",Image,,Computer Vision,,,https://iaminullah.com/publications/,https://paperswithcode.com/dataset/uncertainty-quantification-for-underwater,"This dataset extends the  Semantic Segmentation of Underwater Imagery: Dataset and Benchmark, adding an uncertainty evaluation component. 
To facilitate uncertainty analysis, the test set incorporates a comprehensive range of perturbations, inspired by Benchmarking Neural Network Robustness to Common Corruptions and Perturbations, applied at four intensity levels. These perturbations, which preserve the original ground truth labels, encompass variations in Brightness and Contrast (simulating diverse lighting and object coloration), Gaussian and Shot Noise (reflecting low-light and discrete light properties), and Impulse Noise (resulting from bit errors). Additionally, Defocus, Motion, and Zoom Blurs are included, along with Elastic Transformations, Pixelation from upscaling, and JPEG Compression artifacts. This enhanced dataset enables an in-depth evaluation of model robustness, providing valuable insights into performance under a wide range of challenging, real-world underwater conditions.",,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,https://arxiv.org/abs/1903.12261,,,
3317,Uncorrelated_Corrupted_Dataset,Cross-Modal Person Re-Identification,Cross-Modal Person Re-Identification,"Cross-Modal Person Re-Identification, Multimodal Deep Learning, Infrared And Visible Image Fusion, Person Re-Identification",Image,,Computer Vision,,,https://github.com/art2611/MREiD-UCD-CCD,https://paperswithcode.com/dataset/ucd,"Uncorrelated Corrupted Dataset is an evaluation set that consists of realistic visible-infrared (V-I) corruptions allowing for models' corruption robustness evaluation. Initially proposed for multimodal person re-identification, our dataset can also be used for the evaluation of V-I cross-modal approaches. Corruptions of the visible modality are the twenty corruptions proposed by Chen & al. in the ""Benchmarks for Corruption Invariant Person Re-identification"" paper. Corruptions of the infrared modalities have been proposed in our paper, introducing 19 corruptions that respect the infrared modality encoding.  In practice, the corruptions are applied randomly and independently to the visible and the infrared cameras, making it more suited to a not co-located camera setting.",,,,,,
3318,UNDD,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Scene Parsing, Autonomous Driving","Image, Text",English,Computer Vision,,,https://github.com/sauradip/night_image_semantic_segmentation,https://paperswithcode.com/dataset/undd,"UNDD consists of 7125 unlabelled day and night images; additionally, it has 75 night images with pixel-level annotations having classes equivalent to Cityscapes dataset.",,,,,,
3319,Underwater_Object_Detection_Dataset,Underwater Image Restoration,Underwater Image Restoration,"Underwater Image Restoration, object-detection",Image,,Computer Vision,,,https://gts.ai/dataset-download/underwater-object-detection-dataset/,https://paperswithcode.com/dataset/underwater-object-detection-dataset,"Description:

<a href=""https://gts.ai/dataset-download/underwater-object-detection-dataset/"" target=""_blank"">👉 Download the dataset here</a>

This dataset is designed for advanced underwater object detection and classification. It provides a comprehensive collection of images featuring underwater objects, each precisely annotated with bounding boxes. The dataset aims to support a wide range of research applications, from environmental monitoring to underwater robotics.

Download Dataset

Classes:

Fish (individual and grouped)

Crab

Human Diver

Trash (marine pollution)

Jellyfish

Coral Reef

Sea Turtle

Starfish

Dataset Structure:

Training Set (70%): A robust sample for building detection models.

Validation Set (10%): Used to fine-tune model performance.

Test Set (20%): A carefully selected set of images for evaluating model accuracy.

Pre-processing Techniques:

Auto-Orientation: Ensures all images are correctly aligned.

Resizing: Images are scaled to 640×640 pixels for uniformity.

Brightness Normalization: Corrects for underwater lighting conditions.

Contrast Stretching: Enhances visibility for objects in murky or low-contrast scenes.

New Annotation Techniques:

Polygonal Segmentation: Introduces more precise segmentation for irregular shapes such as coral reefs.

3D Depth Mapping: For enhanced understanding of object placement in underwater space.

Dataset Use Cases:

Marine Ecology: Assessing species diversity and tracking the impact of environmental changes.

Pollution Analysis: Detecting and classifying marine trash, aiding in cleanup efforts.

Underwater Robotics: Training AUVs to recognize and navigate around complex underwater structures like coral reefs or large groups of fish.

Conclusion:

The expanded Underwater Object Detection provides a rich resource for researchers, environmentalists, and engineers working on underwater object detection and classification. Its enhanced classes, precise annotations, and preprocessing techniques make it a valuable asset for developing robust models in marine exploration and conservation.

This dataset is sourced from Kaggle.",,,,,Test Set (20%): A carefully selected set of images,
3320,UNER_v1,Cross-Lingual NER,Cross-Lingual NER,"Cross-Lingual NER, Named Entity Recognition (NER), Multilingual Named Entity Recognition","Image, Text",English,Computer Vision,"named-entity-recognition-ner-on-uner-v1-3, cross-lingual-ner-on-uner-v1-pud-chinese, cross-lingual-ner-on-uner-v1-tagalog-t, cross-lingual-ner-on-uner-v1-pud-swedish, named-entity-recognition-ner-on-uner-v1-5, cross-lingual-ner-on-uner-v1-croatian, named-entity-recognition-ner-on-uner-v1-pud, named-entity-recognition-ner-on-uner-v1-1, cross-lingual-ner-on-uner-v1-chinese-1, named-entity-recognition-ner-on-uner-v1-2, named-entity-recognition-ner-on-uner-v1-pud-3, cross-lingual-ner-on-uner-v1-chinese, named-entity-recognition-ner-on-uner-v1-pud-1, cross-lingual-ner-on-uner-v1-pud-portuguese, cross-lingual-ner-on-uner-v1-cebuano, cross-lingual-ner-on-uner-v1-swedish, named-entity-recognition-ner-on-uner-v1-4, cross-lingual-ner-on-uner-v1-pud-russian, cross-lingual-ner-on-uner-v1-danish, cross-lingual-ner-on-uner-v1-portuguese, named-entity-recognition-ner-on-uner-v1-8, cross-lingual-ner-on-uner-v1-slovak, cross-lingual-ner-on-uner-v1-pud-german, cross-lingual-ner-on-uner-v1-english, named-entity-recognition-ner-on-uner-v1-6, named-entity-recognition-ner-on-uner-v1-pud-2, cross-lingual-ner-on-uner-v1-tagalog-u, named-entity-recognition-ner-on-uner-v1-7, cross-lingual-ner-on-uner-v1-pud-english, cross-lingual-ner-on-uner-v1-serbian, named-entity-recognition-ner-on-uner-v1",CC BY-SA 4.0,https://www.universalner.org,https://paperswithcode.com/dataset/uner-v1,"UNER v1 adds an NER annotation layer to 18 datasets (primarily treebanks from UD) and covers 12 geneologically and ty- pologically diverse languages: Cebuano, Danish, German, English, Croatian, Portuguese, Russian, Slovak, Serbian, Swedish, Tagalog, and Chinese4. Overall, UNER v1 contains nine full datasets with training, development, and test splits over eight languages, three evaluation sets for lower-resource languages (TL and CEB), and a parallel evaluation benchmark spanning six languages.",,,,,,
3321,uniD_Dataset,Trajectory Clustering,Trajectory Clustering,"Trajectory Clustering, Trajectory Planning, Trajectory Prediction, Trajectory Modeling, Trajectory Forecasting",Time Series,,Methodology,,Non-Commercial,https://levelxdata.com/unid-dataset/,https://paperswithcode.com/dataset/unid-dataset,"The uniD dataset is an innovative collection of naturalistic road user trajectories, captured within the RWTH Aachen University campus using drone technology to address common challenges such as occlusions found in traditional traffic data collection methods. It meticulously documents the movement and classifies each road user by type. Employing cutting-edge computer vision algorithms, the dataset ensures high positional accuracy. Its utility spans various applications, from predicting road user behavior and modeling driver actions to conducting scenario-based safety checks for automated driving systems and facilitating the data-driven design of Highly Automated Driving (HAD) system components.",,,,,,
3322,UniMiB_SHAR,Multi-Goal Reinforcement Learning,Multi-Goal Reinforcement Learning,"Multi-Goal Reinforcement Learning, Activity Recognition, Federated Learning","Image, Video",,Reinforcement Learning,,,http://www.sal.disco.unimib.it/technologies/unimib-shar/,https://paperswithcode.com/dataset/unimib-shar,"Includes 11,771 samples of both human activities and falls performed by 30 subjects of ages ranging from 18 to 60 years. Samples are divided in 17 fine grained classes grouped in two coarse grained classes: one containing samples of 9 types of activities of daily living (ADL) and the other containing samples of 8 types of falls. The dataset has been stored to include all the information useful to select samples according to different criteria, such as the type of ADL, the age, the gender, and so on.",,,,771 samples,,
3323,UniMorph_4.0,Morpheme Segmentaiton,Morpheme Segmentaiton,"Morpheme Segmentaiton, Morphological Inflection",,,Methodology,morpheme-segmentaiton-on-unimorph-4-0,CC-BY-SA,https://unimorph.github.io/,https://paperswithcode.com/dataset/unimorph,"The Universal Morphology (UniMorph) project is a collaborative effort to improve how NLP handles complex morphology in the world’s languages. The goal of UniMorph is to annotate morphological data in a universal schema that allows an inflected word from any language to be defined by its lexical meaning, typically carried by the lemma, and by a rendering of its inflectional form in terms of a bundle of morphological features from our schema. The specification of the schema is described here and in Sylak-Glassman (2016).",2016,,,,,
3324,UNIPD-BPE,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, 3D Human Pose Tracking, 3D Human Shape Estimation, 3D Action Recognition, 3D Multi-Person Human Pose Estimation","3D, Image, Video",,Computer Vision,,CC0,https://doi.org/10.17605/OSF.IO/YJ9Q4,https://paperswithcode.com/dataset/unipd-bpe,"The University of Padova Body Pose Estimation dataset (UNIPD-BPE) is an extensive dataset for multi-sensor body pose estimation containing both single-person and multi-person sequences with up to 4 interacting people
A network with 5 Microsoft Azure Kinect RGB-D cameras is exploited to record synchronized high-definition RGB and depth data of the scene from multiple viewpoints, as well as to estimate the subjects’ poses using the Azure Kinect Body Tracking SDK.
Simultaneously, full-body Xsens MVN Awinda inertial suits allow obtaining accurate poses and anatomical joint angles, while also providing raw data from the 17 IMUs required by each suit.
All the cameras and inertial suits are hardware synchronized, while the relative poses of each camera with respect to the inertial reference frame are calibrated before each sequence to ensure maximum overlap of the two sensing systems outputs.

The setup used allowed to record synchronized 3D poses of the persons on the scene both via Xsens’ inverse kinematics algorithm (inertial motion capture) and by exploiting the Azure Kinect Body tracking SDK (markerless motion capture), simultaneously.
The additional raw data (RGB, depth, camera network configuration) allow the user to assess the performance of any custom markerless motion capture algorithm (based on RGB, depth, or both).
Further analyses can be progressed by varying the number of cameras being used and/or their resolution and frame rate.
Moreover, raw angular velocities, linear accelerations, magnetic fields, and orientations from each IMU allow to develop and test multimodal BPE approaches focused on merging visual and inertial data.
Finally, the precise body dimensions of each subject are provided.
They include body height, weight, and segment lengths measured before the beginning of a recording session.
They were used to scale the Xsens biomechanical model, and also constitute a ground truth for assessing the markerless BPE accuracy on estimating each subject’s body dimensions.

The recorded sequences include 15 participants performing a set of 12 ADLs (e.g., walking, sitting, and jogging).
The actions were chosen to present different challenges to BPE algorithms, including different movement speeds, self-occlusions, and complex body poses.
Moreover, multi-person sequences, with up to 4 people performing a set of 7 different actions, are provided.
Such sequences offer challenging scenarios where multiple self-occluded persons move and interact in a restricted space.
They allow assessing the accuracy of multi-person tracking algorithms, focused on maintaining frame-by-frame consistent IDs of each detected person.

A total of 13.3h of RGB, depth, and markerless BPE data are present in the dataset, corresponding to over 1,400,000 frames obtained from a calibrated network with 5 RGB-D cameras.
The inertial suits, on the other hand, allowed to record 3h of inertial motion capture data, corresponding to a total of over 600,000 frames recorded by each of the 17 IMUs used by every suit.",,,,,,
3325,Universal_Dependencies,Language Identification,Language Identification,"Language Identification, Part-Of-Speech Tagging, Dependency Parsing, Cross-Lingual POS Tagging, Cross-lingual zero-shot dependency parsing","Audio, Text",English,Natural Language Processing,"language-identification-on-universal, part-of-speech-tagging-on-ud, dependency-parsing-on-universal-dependencies, cross-lingual-pos-tagging-on-universal, cross-lingual-zero-shot-dependency-parsing-on",Various (see link),https://universaldependencies.org/,https://paperswithcode.com/dataset/universal-dependencies,"The Universal Dependencies (UD) project seeks to develop cross-linguistically consistent treebank annotation of morphology and syntax for multiple languages. The first version of the dataset was released in 2015 and consisted of 10 treebanks over 10 languages. Version 2.7 released in 2020 consists of 183 treebanks over 104 languages. The annotation consists of UPOS (universal part-of-speech tags), XPOS (language-specific part-of-speech tags), Feats (universal morphological features), Lemmas, dependency heads and universal dependency labels.",2015,"Evaluating Contextualized Embeddings on 54 Languagesin POS Tagging, Lemmatization and Dependency Parsing",https://arxiv.org/abs/1908.07448,,,
3326,University_of_Waterloo_skin_cancer_database,Skin Lesion Segmentation,Skin Lesion Segmentation,"Skin Lesion Segmentation, Lesion Segmentation, Local Color Enhancement",Image,,Computer Vision,"skin-lesion-segmentation-on-university-of, lesion-segmentation-on-university-of-waterloo, local-color-enhancement-on-university-of",,https://uwaterloo.ca/vision-image-processing-lab/research-demos/skin-cancer-detection,https://paperswithcode.com/dataset/university-of-waterloo-skin-cancer-database,"The dataset is maintained by VISION AND IMAGE PROCESSING LAB, University of Waterloo.
The images of the dataset were extracted from the public databases DermIS and DermQuest, along with manual segmentations of the lesions.

The dataset was used in the following journal publication.
[1] Glaister, J., A. Wong, and D. A. Clausi, ""Automatic segmentation of skin lesions from dermatological photographs using a joint probabilistic texture distinctiveness approach"", IEEE Transactions on Biomedical Engineering
[2] Amelard, R., J. Glaister, A. Wong, and D. A. Clausi, ""High-level intuitive features (HLIFs) for intuitive skin lesion descriptionpdf"", IEEE Transactions on Biomedical Engineering, vol. 62, issue 3, pp. 820-831, October, 2015.
[3] Glaister, J., R. Amelard, A. Wong, and D. A. Clausi, ""MSIM: Multi-Stage Illumination Modeling of Dermatological Photographs for Illumination-Corrected Skin Lesion Analysis"", IEEE Transactions on Biomedical Engineering, vol. 60, issue 7, pp. 1873 - 1883, November, 2013.",2015,,,,,
3327,UnrealEgo,3D Pose Estimation,3D Pose Estimation,"3D Pose Estimation, Pose Estimation, Egocentric Pose Estimation","3D, Image",,Computer Vision,egocentric-pose-estimation-on-unrealego,,https://4dqv.mpi-inf.mpg.de/UnrealEgo/,https://paperswithcode.com/dataset/unrealego,"UnrealEgo is a dataset that provides in-the-wild stereo images with a large variety of motions for 3D human pose estimation. The in-the-wild stereo images are stereo fisheye images and depth maps with a resolution of 1024×1024 pixels each with 25 frames per second and a total of 450k  (900k images) are captured for the dataset. Metadata is provided for each frame, including 3D joint positions, camera positions, and 2D coordinates of reprojected joint positions in the fisheye views.",,,,900k images,,
3328,UNSW-NB15,Intrusion Detection,Intrusion Detection,"Intrusion Detection, Network Intrusion Detection, Synthetic Data Generation","Graph, Image, Text",English,Computer Vision,"synthetic-data-generation-on-unsw-nb15, intrusion-detection-on-unsw-nb15, network-intrusion-detection-on-unsw-nb15",Custom (research),https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/,https://paperswithcode.com/dataset/unsw-nb15,"UNSW-NB15 is a network intrusion dataset. It contains nine different attacks, includes DoS, worms, Backdoors, and Fuzzers. The dataset contains raw network packets. The number of records in the training set is 175,341 records and the testing set is 82,332 records from the different types, attack and normal.

Paper: UNSW-NB15: a comprehensive data set for network intrusion detection systems",,Evaluation of Adversarial Training on Different Types of Neural Networks in Deep Learning-based IDSs,https://arxiv.org/abs/2007.04472,341 records,,
3329,UPFD,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Graph Classification, Fake News Detection, Misinformation","Graph, Image",,Computer Vision,"graph-classification-on-upfd-gos, graph-classification-on-upfd-pol",Apache-2.0,https://github.com/safe-graph/GNN-FakeNews,https://paperswithcode.com/dataset/upfd,"For benchmarking, please refer to its variant UPFD-POL and UPFD-GOS.

The dataset has been integrated with Pytorch Geometric (PyG) and Deep Graph Library (DGL). You can load the dataset after installing the latest versions of PyG or DGL. 

The UPFD dataset includes two sets of tree-structured graphs curated for evaluating binary graph classification, graph anomaly detection, and fake/real news detection tasks. The dataset is dumped in the form of Pytorch-Geometric dataset object. You can easily load the data and run various GNN models using PyG.

The dataset includes fake&real news propagation (retweet) networks on Twitter built according to fact-check information from Politifact and Gossipcop.
The news retweet graphs were originally extracted by FakeNewsNet.
Each graph is a hierarchical tree-structured graph where the root node represents the news; the leaf nodes are Twitter users who retweeted the root news.
A user node has an edge to the news node if he/she retweeted the news tweet. Two user nodes have an edge if one user retweeted the news tweet from the other user. 

We crawled near 20 million historical tweets from users who participated in fake news propagation in FakeNewsNet to generate node features in the dataset.
We incorporate four node feature types in the dataset, the 768-dimensional bert and 300-dimensional spacy features 
are encoded using pretrained BERT and spaCy word2vec, respectively.
The 10-dimensional profile feature is obtained from a Twitter account's profile.
You can refer to profile_feature.py for profile feature extraction.
The 310-dimensional content feature is composed of a 300-dimensional user comment word2vec (spaCy) embedding plus a 10-dimensional profile feature.

The dataset statistics is shown below:

| Data  | #Graphs  | #Fake News| #Total Nodes  | #Total Edges  | #Avg. Nodes per Graph  |
|-------|--------|--------|--------|--------|--------|
| Politifact | 314   |   157    |  41,054  | 40,740 |  131 |
| Gossipcop |  5464  |   2732   |  314,262  | 308,798  |  58  |

Please refer to the paper for more details about the UPFD dataset.

Due to the Twitter policy, we could not release the crawled user's historical tweets publicly.
To get the corresponding Twitter user information, you can refer to the news lists under \data in our github repo
and map the news id to FakeNewsNet.
Then, you can crawl the user information by following the instruction on FakeNewsNet.
In the UPFD project, we use Tweepy and Twitter Developer API to get the user information.",,paper,https://arxiv.org/pdf/2104.12259.pdf,,,
3330,UQ_NIDS_Datasets__FlowMeter_Format_,Network Intrusion Detection,Network Intrusion Detection,Network Intrusion Detection,"Graph, Image",,Computer Vision,,Custom,https://staff.itee.uq.edu.au/marius/NIDS_datasets/,https://paperswithcode.com/dataset/uq-nids-datasets-flowmeter-format,CICFlowMeter format of the datasets are made up of 83 features.,,,,,,
3331,UR-FUNNY,Humor Detection,Humor Detection,"Humor Detection, Opinion Mining, Sentence Embedding",Image,,Computer Vision,,,https://github.com/ROC-HCI/UR-FUNNY/blob/master/UR-FUNNY-V1.md,https://paperswithcode.com/dataset/ur-funny,For understanding multimodal language used in expressing humor.,,,,,,
3332,UR5_Tool_Dataset,3D Object Classification,3D Object Classification,"3D Object Classification, Object Categorization, Object Recognition","3D, Image",,Computer Vision,,,https://www.eecs.tufts.edu/~gtatiya/pages/2023/UR5_Tool_Dataset.html,https://paperswithcode.com/dataset/ur5-tool-dataset,"In this dataset UR5 robot used 6 tools: metal-scissor, metal-whisk, plastic-knife, plastic-spoon, wooden-chopstick, and wooden-fork to perform 6 behaviors: look, stirring-slow, stirring-fast, stirring-twist, whisk, and poke. The robot explored 15 objects: cane-sugar, chia-seed, chickpea, detergent, empty, glass-bead, kidney-bean, metal-nut-bolt, plastic-bead, salt, split-green-pea, styrofoam-bead, water, wheat, and wooden-button kept cylindrical containers. The robot performed 10 trials on each object using a tool, resulting in 5,400 interactions (6 tools x 6 behaviors x 15 objects x 10 trials). The robot records multiple sensory data (audio, RGB images, depth images, haptic, and touch images) while interacting with the objects.",,,,,"split-green-pea, styrofoam-bead, water, wheat, and wooden-button kept cylindrical containers. The robot performed 10 trials on each object using a tool, resulting in 5,400 interactions (6 tools x 6 behaviors x 15 objects x 10 trials). The robot records multiple sensory data (audio, RGB images",
3333,Urban100,Color Image Denoising,Color Image Denoising,"Color Image Denoising, Image Super-Resolution, Grayscale Image Denoising, Blind Super-Resolution, Image Denoising, Joint Demosaicing and Denoising, Compressive Sensing",Image,,Computer Vision,"grayscale-image-denoising-on-urban100-sigma15-1, blind-super-resolution-on-urban100-3x, grayscale-image-denoising-on-urban100-sigma25, color-image-denoising-on-urban100-sigma50, blind-super-resolution-on-urban100-4x, blind-super-resolution-on-urban100-2x, image-super-resolution-on-urban100-16x, grayscale-image-denoising-on-urban100-sigma70, image-super-resolution-on-urban100-4x, color-image-denoising-on-urban100-sigma70, image-super-resolution-on-urban100-3x, image-super-resolution-on-urban100-2x, color-image-denoising-on-urban100-sigma25, grayscale-image-denoising-on-urban100-sigma50, image-denoising-on-urban100-sigma15, compressive-sensing-on-urban100-2x-upscaling, image-super-resolution-on-urban100-8x, grayscale-image-denoising-on-urban100-sigma10, joint-demosaicing-and-denoising-on-urban100, color-image-denoising-on-urban100-sigma15-1, color-image-denoising-on-urban100-sigma30, image-denoising-on-urban100-sigma50, grayscale-image-denoising-on-urban100-sigma30, color-image-denoising-on-urban100-sigma10",,https://github.com/jbhuang0604/SelfExSR,https://paperswithcode.com/dataset/urban100,The Urban100 dataset contains 100 images of urban scenes. It commonly used as a test set to evaluate the performance of super-resolution models.,,,,100 images,,
3334,UrbanSound8K,Environmental Sound Classification,Environmental Sound Classification,"Environmental Sound Classification, Audio Classification, Data Augmentation, Environment Sound Classification, Zero-shot Audio Classification","Audio, Image",,Computer Vision,"zero-shot-audio-classification-on, environment-sound-classification-on, environmental-sound-classification-on",CC BY-NC 3.0,https://urbansounddataset.weebly.com/urbansound8k.html,https://paperswithcode.com/dataset/urbansound8k-1,"Urban Sound 8K is an audio dataset that contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. All excerpts are taken from field recordings uploaded to www.freesound.org.",,,,,,10
3335,Urdu_News_Headlines_Dataset,Text Clustering,Text Clustering,"Text Clustering, Information Retrieval",Text,English,Natural Language Processing,text-clustering-on-urdu-news-headlines,,,https://paperswithcode.com/dataset/urdu-news-headlines-dataset,"Urdu News Headlines Dataset with VOA and BBC
An Urdu news headlines dataset is a collection of news headlines in the Urdu language, typically scraped from news websites and social media platforms. These datasets can be valuable for researchers and developers working on a variety of tasks, such as:

Machine translation: Training machine translation models to translate between Urdu and other languages.
Text summarization: Developing algorithms for automatically summarizing Urdu news articles.
Natural language processing: Studying the structure and grammar of the Urdu language.
Clustering: Grouping news articles into similar categories based on their headlines.
Benefits of using a dataset with VOA and BBC headlines
There are several benefits to using a dataset that includes news headlines from VOA and BBC:

High quality: VOA and BBC are well-respected news organizations known for their high-quality journalism. This means that the headlines in the dataset are likely to be accurate and unbiased.
Diversity of topics: VOA and BBC cover a wide range of topics, including politics, business, sports, and entertainment. This means that the dataset will be representative of the different types of news that are available in the Urdu language.
Large size: VOA and BBC have been publishing news in Urdu for many years. This means that there is a large amount of data available, which can be used to train machine learning models and to conduct research.
Here are some examples of how a Urdu news headlines dataset with VOA and BBC headlines can be used:

A machine translation model could be trained on the dataset to translate Urdu news headlines into English. This could be used to make Urdu news more accessible to a wider audience.
A text summarization algorithm could be trained on the dataset to automatically generate summaries of Urdu news articles. This could be used to save people time and to help them to stay informed.
A clustering algorithm could be used to group Urdu news articles into similar categories based on their headlines. This could be used to create personalized news feeds for users.
Here are some limitations of using a Urdu news headlines dataset with VOA and BBC headlines:

Limited scope: VOA and BBC are primarily English-language news organizations. This means that the dataset may not be representative of the full range of Urdu news that is available.
Bias: VOA and BBC are both Western news organizations. This means that the dataset may be biased towards Western perspectives.
Limited access: The data may not be readily available to everyone.",,,,,,
3336,Urdu_Text_Scene_Images,Image to text,Image to text,"Image to text, Text-Variation","Image, Text",English,Computer Vision,,,https://gts.ai/dataset-download/urdu-text-scene-images/,https://paperswithcode.com/dataset/urdu-text-scene-images,"Description:

<a href=""https://gts.ai/dataset-download/urdu-text-scene-images/"" target=""_blank"">👉 Download the dataset here</a>

Urdu text extraction in natural scenes poses unique challenges, largely due to the lack of publicly available datasets. To address this, we offer an enriched dataset containing 500 high-quality images of Urdu text captured in real-world environments. These images represent diverse settings, lighting conditions, and backgrounds, making it ideal for researchers and developers working on Urdu Optical Character Recognition (OCR) systems.

Dataset Structure

The dataset is structured as follows:

Training Set (Training Raw): Contains raw images featuring Urdu text for model training.

Test Set (Test Raw): A separate set of images for model testing and validation.

Non-Text Set (Non-Text Raw): Scene images with no Urdu text to prevent false positives and enhance text classification models.

Download Dataset

Applications

This dataset is specifically designed to support the following applications:

Urdu Text Detection & Recognition: Building and fine-tuning OCR models for Urdu script in natural scenes.

Multilingual OCR Systems: Extending existing text recognition systems to include Urdu, especially for South Asian languages with similar script structures.

Autonomous Driving & Navigation Systems: Recognizing Urdu text in street signs, direction boards, and public places, improving functionality in Urdu-speaking regions.

Augmented Reality (AR) Applications: Real-time Urdu text translation or interpretation in natural scenes for tourists or native speakers.

Potential Use Cases

Multilingual Document Digitization: This dataset can be integrated into systems designed for multilingual digitization, where recognizing Urdu text in complex backgrounds is critical.

Urban Planning & Smart Cities: The dataset can aid in the development of systems that recognize text in public areas for smart city initiatives and urban planning efforts.

Mobile Applications: Can be used to enhance mobile apps that need to extract and recognize Urdu text for translation or user interaction.

Future Enhancements

Further dataset releases could include a broader array of text instances, incorporating more variations in fonts, languages (including mixed-language scenarios), and additional annotations like bounding boxes for character-level recognition. This would extend its utility in fields like document analysis, smart OCR solutions, and advanced multilingual systems.

This dataset is sourced from Kaggle.",,,,,,
3337,URMP,Music Generation,Music Generation,"Music Generation, Music Transcription, Multi-instrument Music Transcription, Music Information Retrieval","Audio, Text",English,Natural Language Processing,"music-transcription-on-urmp, multi-instrument-music-transcription-on-urmp",,http://www2.ece.rochester.edu/projects/air/projects/URMP.html,https://paperswithcode.com/dataset/urmp,"URMP (University of Rochester Multi-Modal Musical Performance) is a dataset for facilitating audio-visual analysis of musical performances. The dataset comprises 44 simple multi-instrument musical pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece the dataset provided the musical score in MIDI format, the high-quality individual instrument audio recordings and the videos of the assembled pieces.",,,,,,
3338,USNA-Cn2__long-term_,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Time Series Regression",Time Series,,Time Series,time-series-regression-on-usna-cn2-long-term,Creative Commons Attribution 4.0,https://github.com/cdjellen/otbench,https://paperswithcode.com/dataset/usna-cn2-long-term,"The USNA long-term scintillation study is a continuing effort to characterize and measure optical turbulence in the near-maritime boundary layer.

Summary
The field campaign utilized a ScinTec BLS450 scintillometer to characterize the strength of optical turbulence over a link approximately 3 [m] above the river’s surface between January 1\textsuperscript{st}, 2020, and July 26\textsuperscript{th}, 2023. This field campaign included a range of meteorological and oceanographic measurements from nearby NOAA, National Data Buoy Center (NDBC), and Academy weather stations.

The long-term dataset covers the period between January 1\textsuperscript{st}, 2020 and July 14\textsuperscript{th}, 2022. This data set includes local measurements made with a Davis Vantage Pro2 weather station, as well as water conditions from the NDBC Thomas Point data buoy.

Details
Additional Information
 * Frequency: 1 minute
 * Language: English

Categories
 * Surface

Platforms
 * National Data Buoy Center station TPLM2

Instruments
 * ScinTec BLS450
 * Davis Vantage Pro2

Temporal coverage
 * Begin datetime: 2020-01-01 00:00:00
 * End datetime: 2022-07-14 00:00:00

Spatial coverage
 * Maximum (North) Latitude: 38.98
 * Minimum (South) Latitude: 38.98
 * Minimum (West) Longitude: -76.48
 * Maximum (East) Longitude: -76.48

Primary contact information
 * author: Chris Jellen &#99;&#106;&#101;&#108;&#108;&#101;&#110;&#64;&#109;&#105;&#99;&#114;&#111;&#115;&#111;&#102;&#116;&#46;&#99;&#111;&#109;",2020,,,,,
3339,USNA-Cn2__short-duration_,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Time Series Regression",Time Series,,Time Series,"time-series-regression-on-usna-cn2-short, time-series-forecasting-on-usna-cn2-short",Creative Commons Attribution 4.0,https://github.com/cdjellen/otbench,https://paperswithcode.com/dataset/usna-cn2-short-duration,"The USNA long-term scintillation study is a continuing effort to characterize and measure optical turbulence in the near-maritime boundary layer.

Summary
The field campaign utilized a ScinTec BLS450 scintillometer to characterize the strength of optical turbulence over a link approximately 3 [m] above the river’s surface between January 1\textsuperscript{st}, 2020, and July 26\textsuperscript{th}, 2023. This field campaign included a range of meteorological and oceanographic measurements from nearby NOAA, National Data Buoy Center (NDBC), and Academy weather stations.

The short-duration dataset includes measured $C_n^2$ along with atmospheric and oceanographic parameters from a co-located NOAA COOPS observation station between June 1\textsuperscript{st}, 2021, and September 1\textsuperscript{st}, 2021.

Details
Additional information
 * Frequency: 6 minute
 * Language: English

Categories
 * Surface

Platforms
 * NOAA Coastal Observation Station 8575512

Instruments
 * ScinTec BLS450
 * Davis Vantage Pro2

Temporal coverage
 * Begin datetime: 2021-06-01 00:00:00
 * End datetime: 2021-09-01 00:00:00

Spatial coverage
 * Maximum (North) Latitude: 38.98
 * Minimum (South) Latitude: 38.98
 * Minimum (West) Longitude: -76.48
 * Maximum (East) Longitude: -76.48

Primary contact information
 * author: Chris Jellen &#99;&#106;&#101;&#108;&#108;&#101;&#110;&#64;&#109;&#105;&#99;&#114;&#111;&#115;&#111;&#102;&#116;&#46;&#99;&#111;&#109;",2020,,,,,
3340,USPS,Image Clustering,Image Clustering,"Image Clustering, Deep Clustering, Domain Adaptation",Image,,Computer Vision,"domain-adaptation-on-usps-to-mnist, domain-adaptation-on-mnist-to-usps, image-clustering-on-usps, deep-clustering-on-usps",,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps,https://paperswithcode.com/dataset/usps,"USPS is a digit dataset automatically scanned from envelopes by the U.S. Postal Service containing a total of 9,298 16×16 pixel grayscale samples; the images are centered, normalized and show a broad range of font styles.",,Hallucinating Agnostic Images to Generalize Across Domains,https://arxiv.org/abs/1808.01102,,,
3341,USPTO-50k,Single-step retrosynthesis,Single-step retrosynthesis,"Single-step retrosynthesis, Chemical Reaction Prediction",Time Series,,Methodology,single-step-retrosynthesis-on-uspto-50k,,https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.6b00564/suppl_file/ci6b00564_si_002.zip,https://paperswithcode.com/dataset/uspto-50k,"Subset and preprocessed version of Chemical reactions from US patents (1976-Sep2016) by Daniel Lowe.
It includes 50K randomly selected reactions that was later classified into 10 reaction classes by Nadine Schneider et al.",1976,,,,,
3342,USR-PersonaChat,Dialogue Evaluation,Dialogue Evaluation,Dialogue Evaluation,,,Methodology,dialogue-evaluation-on-usr-personachat,,http://shikib.com/usr,https://paperswithcode.com/dataset/usr-personachat,"This dataset was collected with the goal of assessing dialog evaluation metrics. In the paper, USR: An Unsupervised and Reference Free Evaluation Metric for Dialog (Mehri and Eskenazi, 2020), the authors collect this data to measure the quality of several existing word-overlap and embedding-based metrics, as well as their newly proposed USR metric.",2020,,,,,
3343,USR-TopicalChat,Dialogue Evaluation,Dialogue Evaluation,Dialogue Evaluation,,,Methodology,dialogue-evaluation-on-usr-topicalchat,,http://shikib.com/usr,https://paperswithcode.com/dataset/usr-topicalchat,"This dataset was collected with the goal of assessing dialog evaluation metrics. In the paper, USR: An Unsupervised and Reference Free Evaluation Metric for Dialog (Mehri and Eskenazi, 2020), the authors collect this data to measure the quality of several existing word-overlap and embedding-based metrics, as well as their newly proposed USR metric.",2020,,,,,
3344,UT-Kinect,Multimodal Activity Recognition,Multimodal Activity Recognition,"Multimodal Activity Recognition, Skeleton Based Action Recognition","Image, Video",,Multimodal,"skeleton-based-action-recognition-on-ut, multimodal-activity-recognition-on-ut-kinect",,https://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html,https://paperswithcode.com/dataset/ut-kinect,"The UT-Kinect dataset is a dataset for action recognition from depth sequences. The videos were captured using a single stationary Kinect. There are 10 action types: walk, sit down, stand up, pick up, carry, throw, push, pull, wave hands, clap hands. There are 10 subjects, Each subject performs each actions twice. Three channels were recorded: RGB, depth and skeleton joint locations. The three channel are synchronized. The framerate is 30f/s.",,,,,,
3345,UT-Zappos50K,Compositional Zero-Shot Learning,Compositional Zero-Shot Learning,Compositional Zero-Shot Learning,,,Methodology,,,https://vision.cs.utexas.edu/projects/finegrained/utzap50k/,https://paperswithcode.com/dataset/ut-zappos50k-1,"UT Zappos50K (UT-Zap50K) is a large shoe dataset consisting of 50,025 catalog images collected from Zappos.com. The images are divided into 4 major categories — shoes, sandals, slippers, and boots — followed by functional types and individual brands. The shoes are centered on a white background and pictured in the same orientation for convenient analysis.
This dataset is created in the context of an online shopping task, where users pay special attentions to fine-grained visual differences. For instance, it is more likely that a shopper is deciding between two pairs of similar men's running shoes instead of between a woman's high heel and a man's slipper. GIST and LAB color features are provided. In addition, each image has 8 associated meta-data (gender, materials, etc.) labels that are used to filter the shoes on Zappos.com.
We introduced this dataset in the context of a pairwise comparison task, where the goal is to predict which of two images more strongly exhibits a visual attribute. When given a novel image pair, we want to answer the question, “Does Image A contain more or less of an attribute than Image B?” Both training and evaluation are performed using pairwise labels.
However, the usefulness of this dataset extends beyond the comparison task that we’ve demonstrated. The meta-data labels and the large size of the dataset makes it suitable for other tasks as well, such as:

category/brand classification
fine-grained attribute learning with rationales
gender-specific style matching
zero-shot learning",,,,,,
3346,UTKFace,Race/Bias-conflicting,Race/Bias-conflicting,"Race/Bias-conflicting, Fairness, Race/Unbiased, Age/Bias-conflicting, Age Estimation, Age/Unbiased, Multi-Task Learning, Facial Attribute Classification",Image,,Computer Vision,"age-unbiased-on-utkface, age-estimation-on-utkface, multi-task-learning-on-utkface, age-bias-conflicting-on-utkface, race-unbiased-on-utkface, facial-attribute-classification-on-utkface, fairness-on-utkface, race-bias-conflicting-on-utkface","Custom (research-only, non-commercial)",https://susanqq.github.io/UTKFace/,https://paperswithcode.com/dataset/utkface,"The UTKFace dataset is a large-scale face dataset with long age span (range from 0 to 116 years old). The dataset consists of over 20,000 face images with annotations of age, gender, and ethnicity. The images cover large variation in pose, facial expression, illumination, occlusion, resolution, etc. This dataset could be used on a variety of tasks, e.g., face detection, age estimation, age progression/regression, landmark localization, etc.",,,,,,
3347,UTRSet-Real,Irregular Text Recognition,Irregular Text Recognition,"Irregular Text Recognition, Optical Character Recognition (OCR), Scene Text Recognition, Printed Text Recognition","Image, Text",English,Computer Vision,printed-text-recognition-on-utrset-real,CC BY-NC-ND,https://abdur75648.github.io/UTRNet/,https://paperswithcode.com/dataset/utrset-real,"The UTRSet-Real dataset is a comprehensive, manually annotated dataset specifically curated for Printed Urdu OCR research. It contains over 11,000 printed text line images, each of which has been meticulously annotated. One of the standout features of this dataset is its remarkable diversity, which includes variations in fonts, text sizes, colours, orientations, lighting conditions, noises, styles, and backgrounds. This diversity closely mirrors real-world scenarios, making the dataset highly suitable for training and evaluating models that aim to excel in real-world Urdu text recognition tasks.

The availability of the UTRSet-Real dataset addresses the scarcity of comprehensive real-world printed Urdu OCR datasets. By providing researchers with a valuable resource for developing and benchmarking Urdu OCR models, this dataset promotes standardized evaluation and reproducibility and fosters advancements in the field of Urdu OCR. Further, to complement the UTRSet-Real for training purposes, we also present UTRSet-Synth, a high-quality synthetic dataset closely resembling real-world representations of Urdu text. For more information and details about the UTRSet-Real & UTRSet-Synth datasets, please refer to the paper ""UTRNet: High-Resolution Urdu Text Recognition In Printed Documents""",,"""UTRNet: High-Resolution Urdu Text Recognition In Printed Documents""",https://arxiv.org/abs/2306.15782,,,
3348,UTRSet-Synth,Irregular Text Recognition,Irregular Text Recognition,"Irregular Text Recognition, Optical Character Recognition (OCR), Scene Text Recognition","Image, Text",English,Computer Vision,,CC BY-NC-ND,https://abdur75648.github.io/UTRNet/,https://paperswithcode.com/dataset/utrset-synth,"The UTRSet-Synth dataset is introduced as a complementary training resource to the UTRSet-Real Dataset, specifically designed to enhance the effectiveness of Urdu OCR models. It is a high-quality synthetic dataset comprising 20,000 lines that closely resemble real-world representations of Urdu text.

To generate the dataset, a custom-designed synthetic data generation module which offers precise control over variations in crucial factors such as font, text size, colour, resolution, orientation, noise, style, and background, was employed. Moreover, the UTRSet-Synth dataset tackles the limitations observed in existing datasets. It addresses the challenge of standardizing fonts by incorporating over 130 diverse Urdu fonts, which were thoroughly refined to ensure consistent rendering schemes. It overcomes the scarcity of Arabic words, numerals, and Urdu digits by incorporating a significant number of samples representing these elements. Additionally, the dataset is enriched by randomly selecting words from a vocabulary of 100,000 words during the text generation process. As a result, UTRSet-Synth contains a total of 28,187 unique words, with an average word length of 7 characters.

The availability of the UTRSet-Synth dataset, a synthetic dataset that closely emulates real-world variations, addresses the scarcity of comprehensive real-world printed Urdu OCR datasets. By providing researchers with a valuable resource for developing and benchmarking Urdu OCR models, this dataset promotes standardized evaluation, and reproducibility, and fosters advancements in the field of Urdu OCR. For more information and details about the UTRSet-Real & UTRSet-Synth datasets, please refer to the paper ""UTRNet: High-Resolution Urdu Text Recognition In Printed Documents""",,"""UTRNet: High-Resolution Urdu Text Recognition In Printed Documents""",https://arxiv.org/abs/2306.15782,,,
3349,UTSD,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Time Series Forecasting, Imputation","Image, Time Series",,Computer Vision,,,https://huggingface.co/datasets/thuml/UTSD,https://paperswithcode.com/dataset/utsd,"Unified Time Series Dataset (UTSD) includes 7 domains with up to 1 billion time points with hierarchical capacities to facilitate research of large models in the field of time series. It is meticulously assembled from a blend of publicly accessible online data repositories and empirical data derived from real-world machine operations. We analyze each dataset within the collection, examining the time series through the lenses of stationarity and forecastability to allows us to characterize the level of complexity inherent to each dataset.

All datasets are classified into seven distinct domains by their source: Energy, Environment, Health, Internet of Things (IoT), Nature, Transportation, and Web with diverse sampling frequencies. UTSD is constructed with hierarchical capacities, namely UTSD-1G, UTSD-2G, UTSD-4G, and UTSD-12G, where each smaller dataset is a subset of the larger ones. A larger subset means greater data difficulty and diversity, allowing you to conduct detailed scaling experiments.

See the paper and codebase for more information.",,paper,https://arxiv.org/pdf/2402.02368,,,
3350,UT_Zappos50K,Image Captioning,Image Captioning,"Image Captioning, Image-to-Image Translation, Image Generation, Few-Shot Image Classification, Compositional Zero-Shot Learning","Image, Text",English,Computer Vision,"compositional-zero-shot-learning-on-ut, few-shot-image-classification-on-ut-zappos50k","Custom (academic, non-commercial)",http://vision.cs.utexas.edu/projects/finegrained/utzap50k/,https://paperswithcode.com/dataset/ut-zappos50k,"UT Zappos50K is a large shoe dataset consisting of 50,025 catalog images collected from Zappos.com. The images are divided into 4 major categories — shoes, sandals, slippers, and boots — followed by functional types and individual brands. The shoes are centered on a white background and pictured in the same orientation for convenient analysis.",,,,,,
3351,UV6K,Image Segmentation,Image Segmentation,"Image Segmentation, Satellite Image Classification, The Semantic Segmentation Of Remote Sensing Imagery",Image,,Computer Vision,the-semantic-segmentation-of-remote-sensing,,https://zenodo.org/record/8404754,https://paperswithcode.com/dataset/uv6k,"UV6K is a high-resolution remote sensing urban vehicle segmentation dataset.


Images: 6,313
Vehicle: 245,141
Resolution: 0.1m
Image Size: 1024x1024",,,,,,
3352,UW_Indoor_Scenes__UW-IS__Occluded_dataset,Object Recognition,Object Recognition,"Object Recognition, Instance Segmentation, 6D Pose Estimation using RGBD, 6D Pose Estimation, 6D Pose Estimation using RGB","3D, Image",,Computer Vision,,CC BY 4.0,https://doi.org/10.6084/m9.figshare.20506506,https://paperswithcode.com/dataset/uw-indoor-scenes-uw-is-occluded-dataset,"UW Indoor Scenes (UW-IS) Occluded dataset is curated using commodity hardware (Intel RealSense D435) to reflect real world robotics scenarios. It consists of two completely different indoor environments. The first environment is a lounge where the objects are placed on a tabletop. The second environment is a mock warehouse setup where the objects are placed on a shelf. For each of these environments, we have RGB-D images from 36 videos comprising five to seven objects each, taken from distances up to approximately 2m. The videos cover two different lighting conditions, three different levels of object separation for three different object categories (i.e., kitchen objects, food items, and tools/miscellaneous). The first level of object separation is such that there is no object occlusion. The second level of object separation is such that some occlusion occurs, while the third level is where the objects are placed extremely close together. Overall, the dataset considers 20 object classes and consists of 8,456 images, which have a total of 42,902 object instances. We also provide instance segmentation masks and 6D pose annotations for all the images generated using LabelFusion (Marion et al., 2018)",2018,,,456 images,,
3353,UZLF,Artery/Veins Retinal Vessel Segmentation,Artery/Veins Retinal Vessel Segmentation,"Artery/Veins Retinal Vessel Segmentation, Retinal Vessel Segmentation",Image,,Computer Vision,"retinal-vessel-segmentation-on-uzlf, artery-veins-retinal-vessel-segmentation-on",,https://rdr.kuleuven.be/dataset.xhtml?persistentId=doi:10.48804/Z7SHGO,https://paperswithcode.com/dataset/uzlf,"The Leuven-Haifa dataset contains 240 disc-centered fundus images of 224 unique patients (75 patients with normal tension glaucoma, 63 patients with high tension glaucoma, 30 patients with other eye diseases and 56 healthy controls) from the University Hospitals of Leuven. The arterioles and venules of these images were both annotated by master students in medicine and corrected by a senior annotator. All senior segmentation corrections are provided as well as the junior segmentations of the test set. An open-source toolbox for the parametrization of segmentations was developed. Diagnosis, age, sex, vascular parameters as well as a quality score are provided as metadata. Potential reuse is envisioned as the development or external validation of blood vessels segmentation algorithms or study of the vasculature in glaucoma and the development of glaucoma diagnosis algorithms. The dataset is available on the KU Leuven Research Data Repository (RDR).",,,,,,
3354,UzWordnet,Lexical Analysis,Lexical Analysis,Lexical Analysis,,,Methodology,,Attribution-NonCommercial 4.0 International,https://uzwordnet.ldkr.org,https://paperswithcode.com/dataset/uzwordnet,"UzWordnet is a lexical-semantic database, or a “word-net”, for the (Northern) Uzbek language (native: O’zbek till) compatible with Princeton Wordnet. By providing it open source (see License), we aim to motivate, support, and increase the application of database and knowledge graphs principles and techniques to the study of computational aspects of the (Northern) Uzbek language and, more generally, the usability of Uzbek within IT applications and the Internet.",,,,,,
3355,V-D4RL,Reinforcement Learning (RL),Reinforcement Learning (RL),Reinforcement Learning (RL),,,Reinforcement Learning,,MIT,https://huggingface.co/datasets/conglu/vd4rl,https://paperswithcode.com/dataset/v-d4rl,"V-D4RL provides pixel-based analogues of the popular D4RL benchmarking tasks, derived from the dm_control suite, along with natural extensions of two state-of-the-art online pixel-based continuous control algorithms, DrQ-v2 and DreamerV2, to the offline setting.",,,,,,
3356,V2VBench,Video Editing,Video Editing,"Video Editing, Text-to-Video Editing","Text, Video",English,Natural Language Processing,,,https://huggingface.co/datasets/Wenhao-Sun/V2VBench,https://paperswithcode.com/dataset/v2vbench,"V2VBench is a comprehensive benchmark designed to evaluate video editing methods. It consists of:
- 50 standardized videos across 5 categories, and
- 3 editing prompts per video, encompassing 4 editing tasks: Huggingface Datasets
- 8 evaluation metrics to assess the quality of edited videos: Evaluation Metrics

For detailed information, please refer to the accompanying paper.",,,,,,5
3357,V4V,Heart rate estimation,Heart rate estimation,"Heart rate estimation, Physiological Computing",,,Methodology,,,https://vision4vitals.github.io/index.html,https://paperswithcode.com/dataset/v4v,"Over the past few years a number of research groups have made rapid advances in remote PPG methods for estimating heart rate from digital video and obtained impressive results. How these various methods compare in naturalistic conditions, where spontaneous behavior, facial expressions, and illumination changes are present, is relatively unknown. To enable comparisons among alternative methods, the Vision for Vitals dataset was introduced. It is a novel dataset containing high-resolution videos time-locked with varied physiological signals from a diverse population.

It contains more than 150+ subjects with over 1300+ videos along with ground truth heart rate and respiration rate annotations. It also includes blood pressure waveform signals as part of its physiological data.",,Unknown,https://vision4vitals.github.io/V4V_EULA-ICCV2021.pdf,,,
3358,ValNov_Subtask_A,ValNov,ValNov,ValNov,,,Methodology,valnov-on-valnov-subtask-a,,https://phhei.github.io/ArgsValidNovel/,https://paperswithcode.com/dataset/valnov-subtask-a,Binary labels for Validity and Novelty respectively are given for each Conclusion.,,,,,,
3359,ValNov_Subtask_B,ValNov,ValNov,ValNov,,,Methodology,valnov-on-valnov-subtask-b,,https://phhei.github.io/ArgsValidNovel/,https://paperswithcode.com/dataset/valnov-subtask-b,"Validity and Novelty are determined in a comparative setting between two conclusions at a time. For Validity and Novelty possible labels are ""Conclusion 1 is better"", ""tie"" and ""Conclusion 2 is better"", for Validity and Novelty respectively.",,,,,,
3360,VALSE,image-sentence alignment,image-sentence alignment,image-sentence alignment,Image,,Computer Vision,"image-sentence-alignment-on-valse-counting, image-sentence-alignment-on-valse-actant-swap, image-sentence-alignment-on-valse, image-sentence-alignment-on-valse-counting-2, image-sentence-alignment-on-valse-coreference-1, image-sentence-alignment-on-valse-foil-it, image-sentence-alignment-on-valse-plurality, image-sentence-alignment-on-valse-existence, image-sentence-alignment-on-valse-action, image-sentence-alignment-on-valse-counting-1, image-sentence-alignment-on-valse-spatial, image-sentence-alignment-on-valse-coreference",,https://github.com/Heidelberg-NLP/VALSE,https://paperswithcode.com/dataset/valse,"We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible.
We expect VALSE to serve as an important benchmark to measure future progress of pretrained V&L models from a linguistic perspective, complementing the canonical task-centred V&L evaluations.",,,,,,
3361,VALUE,Text-to-video search,Text-to-video search,"Text-to-video search, Video Captioning, Video Question Answering","Image, Text, Video",English,Computer Vision,,Multiple licenses,https://value-leaderboard.github.io/,https://paperswithcode.com/dataset/value,"VALUE is a Video-And-Language Understanding Evaluation benchmark to test models that are generalizable to diverse tasks, domains, and datasets. It is an assemblage of 11 VidL (video-and-language) datasets over 3 popular tasks: (i) text-to-video retrieval; (ii) video question answering; and (iii) video captioning. VALUE benchmark aims to cover a broad range of video genres, video lengths, data volumes, and task difficulty levels. Rather than focusing on single-channel videos with visual information only, VALUE promotes models that leverage information from both video frames and their associated subtitles, as well as models that share knowledge across multiple tasks. 

The datasets used for the VALUE benchmark are: TVQA, TVR, TVC, How2R, How2QA, VIOLIN, VLEP, YouCook2 (YC2C, YC2R), VATEX",,,,,,
3362,VAST,Stance Detection,Stance Detection,Stance Detection,Image,,Computer Vision,stance-detection-on-vast,,https://github.com/emilyallaway/zero-shot-stance,https://paperswithcode.com/dataset/vast,"VAST consists of a large range of topics covering broad themes, such as politics (e.g., ‘a Palestinian state’), education (e.g., ‘charter schools’), and public health (e.g., ‘childhood vaccination’). In addition, the data includes a wide range of similar expressions (e.g., ‘guns on campus’ versus ‘firearms on campus’). This variation captures how humans might realistically describe the same topic and contrasts with the lack of variation in existing datasets.",,Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic Representations,https://arxiv.org/pdf/2010.03640v1.pdf,,,
3363,VBR,Visual Place Recognition,Visual Place Recognition,"Visual Place Recognition, Stereo Disparity Estimation, Point Cloud Registration, Monocular Depth Estimation, Point Cloud Super Resolution, Stereo Image Super-Resolution, Visual Localization, Vehicle Pose Estimation, Image to 3D, Image Registration, Monocular Visual Odometry, Stereo Matching, Novel View Synthesis, Vehicle Speed Estimation, Novel LiDAR View Synthesis, 3D Reconstruction, 3D Pose Estimation, Visual Tracking, Motion Estimation, Point Cloud Completion, Stereo Depth Estimation, Image to Point Cloud Registration, Pose Estimation, Optical Flow Estimation, Scene Flow Estimation, Pose Tracking, Stereo-LiDAR Fusion, Visual Odometry, 3D Place Recognition","3D, Image, Video",,Computer Vision,,CC BY-SA 4.0 DEED,https://rvp-group.net/slam-dataset.html,https://paperswithcode.com/dataset/vbr,"This dataset presents a vision and perception research dataset collected in Rome, featuring RGB data, 3D point clouds, IMU, and GPS data. We introduce a new benchmark targeting visual odometry and SLAM, to advance the research in autonomous robotics and computer vision. This work complements existing datasets by simultaneously addressing several issues, such as environment diversity, motion patterns, and sensor frequency. It uses up-to-date devices and presents effective procedures to accurately calibrate the intrinsic and extrinsic of the sensors while addressing temporal synchronization. During recording, we cover multi-floor buildings, gardens, urban and highway scenarios. Combining handheld and car-based data collections, our setup can simulate any robot (quadrupeds, quadrotors, autonomous vehicles). The dataset includes an accurate 6-dof ground truth based on a novel methodology that refines the RTK-GPS estimate with LiDAR point clouds through Bundle Adjustment. All sequences divided in training and testing are accessible through our website.",,,,,,
3364,VC-Clothes,Unsupervised Person Re-Identification,Unsupervised Person Re-Identification,"Unsupervised Person Re-Identification, Person Re-Identification",Image,,Computer Vision,"unsupervised-person-re-identification-on-vc, person-re-identification-on-vc-clothes",Custom,https://wanfb.github.io/dataset.html,https://paperswithcode.com/dataset/vc-clothes,"Person re-identification (Reid) is now an active research topic for AI-based video surveillance applications such as specific person search, but the practical issue that the target person(s) may change clothes (clothes inconsistency problem) has been overlooked for long. For the first time, this paper systematically studies this problem. We first overcome the difficulty of lack of suitable dataset, by collecting a small yet representative real dataset for testing whilst building a large realistic synthetic dataset for training and deeper studies. Facilitated by our new datasets, we are able to conduct various interesting new experiments for studying the influence of clothes inconsistency. We find that changing clothes makes Reid a much harder problem in the sense of bringing difficulties to learning effective representations and also challenges the generalization ability of previous Reid models to identify persons with unseen (new) clothes. Representative existing Reid models are adopted to show informative results on such a challenging setting, and we also provide some preliminary efforts on improving the robustness of existing models on handling the clothes inconsistency issue in the data. We believe that this study can be inspiring and helpful for encouraging more researches in this direction. The dataset is avaliable on the project website: https://wanfb.github.io/dataset.html.",,,,,,
3365,VCG_112K,Video Understanding,Video Understanding,"Video Understanding, Zeroshot Video Question Answer, Video Question Answering, Video-based Generative Performance Benchmarking, Question Answering","Text, Video",English,Natural Language Processing,,MIT,https://huggingface.co/datasets/MBZUAI/VCG-plus_112K,https://paperswithcode.com/dataset/vcg-112k,"Video-ChatGPT introduces the VideoInstruct100K dataset, which employs a semi-automatic annotation pipeline to generate 75K instruction-tuning QA pairs. To address the limitations of this annotation process, we present \ourdata~dataset developed through an improved annotation pipeline. Our approach improves the accuracy and quality of instruction tuning pairs by improving keyframe extraction, leveraging SoTA large multimodal models (LMMs) for detailed descriptions, and refining the instruction generation strategy.",,,,,,
3366,VCR,Explanation Generation,Explanation Generation,"Explanation Generation, Visual Commonsense Reasoning, Visual Question Answering (VQA)","Image, Text",English,Computer Vision,"visual-question-answering-on-vcr-q-ar-dev, visual-commonsense-reasoning-on-vcr-qa-r-dev, visual-question-answering-on-vcr-q-ar-test, visual-commonsense-reasoning-on-vcr-q-a-test, visual-commonsense-reasoning-on-vcr-q-ar-dev, visual-commonsense-reasoning-on-vcr-q-ar-test, visual-commonsense-reasoning-on-vcr-q-a-dev, visual-commonsense-reasoning-on-vcr-qa-r-test, visual-question-answering-on-vcr-q-a-test, visual-question-answering-on-vcr-qa-r-test, explanation-generation-on-vcr, visual-question-answering-on-vcr-qa-r-dev, visual-question-answering-on-vcr-q-a-dev",Custom,https://visualcommonsense.com/,https://paperswithcode.com/dataset/vcr,"Visual Commonsense Reasoning (VCR) is a large-scale dataset for cognition-level visual understanding. Given a challenging question about an image, machines need to present two sub-tasks: answer correctly and provide a rationale justifying its answer. The VCR dataset contains over 212K (training), 26K (validation) and 25K (testing) questions, answers and rationales derived from 110K movie scenes.",,Visual Commonsense R-CNN,https://arxiv.org/abs/2002.12204,,,
3367,VCTK,Bandwidth Extension,Bandwidth Extension,"Bandwidth Extension, Directional Hearing, Real-time Directional Hearing, Audio Super-Resolution, Voice Conversion",Audio,,Audio,"audio-super-resolution-on-voice-bank-corpus-1, real-time-directional-hearing-on-vctk, voice-conversion-on-vctk, directional-hearing-on-vctk, bandwidth-extension-on-vctk, audio-super-resolution-on-vctk-multi-speaker-1",Creative Commons License: Attribution 4.0 International,https://datashare.is.ed.ac.uk/handle/10283/2651,https://paperswithcode.com/dataset/vctk,"This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald & Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, ""The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,"" https://doi.org/10.1109/ICSDA.2013.6709856. The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf. All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed.",2013,,,400 sentences,,
3368,VD-Ref,Phrase Grounding,Phrase Grounding,Phrase Grounding,,,Methodology,,Apache-2.0 license,https://github.com/izhx/Phrase-Grounding-with-Pronoun,https://paperswithcode.com/dataset/vd-ref,"VD-Ref is a dataset with ground-truth mappings from both noun phrases and pronouns to image regions. This dataset contains a set of 10k complete sets from the VisDialog dataset, and uses the StanfordCoreNLP tool to tokenize the sentences, making it proper for the succeeding human annotation.",,Extending Phrase Grounding with Pronouns in Visual Dialogues,https://arxiv.org/pdf/2210.12658v1.pdf,,,
3369,Vega-Lite_Chart_Collection,Chart Question Answering,Chart Question Answering,Chart Question Answering,Text,English,Natural Language Processing,,MIT,https://hyungkwonko.info/chart-llm/,https://paperswithcode.com/dataset/vega-lite-chart-collection,"We present a new collection of 1,981 Vega-Lite specifications, which is used to demonstrate the generalizability and viability of our NL generation framework. This collection is the largest set of human-generated charts obtained from GitHub to date. It covers varying levels of complexity from a simple line chart without any interaction to a chart with four plots where data points are linked with selection interactions. Compared to the benchmarks, our dataset shows the highest average pairwise edit distance between specifications, which proves that the charts are highly diverse from one another. Moreover, it contains the largest number of charts with composite views, interactions (e.g., tooltips, panning & zooming, and linking), and diverse chart types (e.g., map, grid & matrix, diagram, etc.).",,,,,,
3370,Vehicle_Claims,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Unsupervised Anomaly Detection, Fraud Detection",Image,,Computer Vision,"unsupervised-anomaly-detection-on-vehicle, anomaly-detection-on-vehicle-claims",,https://github.com/ajaychawda58/UADAD/blob/main/Code/Notebooks/create_dataset.ipynb,https://paperswithcode.com/dataset/vehicle-claims,"The code to create the dataset is available here.
The dataset used in the paper is available on github


Maker - Categorical - The brand of the vehicle.
GenModel - Categorical - The model of the vehicle.
Color - Categorical - Colour of the vehicle.
Reg_Year - Categorical - Year of Registration.
Body_Type - Categorical - Eg. SUV, Convertible.
Runned_Miles - Numerical - Distance covered by the vehicle.
Engin_Size - Categorical - Size of engine.
GearBox - Categorical - Automatic, Manual.
FuelType - Categorical - Petrol, Diesel.
Price -  Numerical - Price of vehicle.
Seat_num - Numerical - Number of seats.
Door_num -  Numerical - Number of Doors.
issue - Categorical - Type of damage.
issue_id - Categorical - Specific damage.
repair_complexity - Categorical - Difficulty to repair the vehicle.
repair_hours -  Numerical - Time required to finish the job.
repair_cost - Numerical - Cost of repair.

Other attributes are not used for evaluation in this work. 
breakdown_date and repair_date were added with the idea of inserting anomalies based on the number of days required to repair the vehicle.",,,,,,
3371,VeRi-Wild,Vehicle Re-Identification,Vehicle Re-Identification,"Vehicle Re-Identification, Metric Learning, Person Re-Identification",Image,,Computer Vision,"vehicle-re-identification-on-veri-wild-large, vehicle-re-identification-on-veri-wild-medium, vehicle-re-identification-on-veri-wild-small",,https://github.com/PKU-IMRE/VERI-Wild,https://paperswithcode.com/dataset/veri-wild,"Veri-Wild is the largest vehicle re-identification dataset (as of CVPR 2019). The dataset is captured from a large CCTV surveillance system consisting of 174 cameras across one month (30× 24h) under unconstrained scenarios. This dataset comprises 416,314 vehicle images of 40,671 identities. Evaluation on this dataset is split across three subsets: small, medium and large; comprising 3000, 5000 and 10,000 identities respectively (in probe and gallery sets).",2019,Vehicle Re-Identification: an Efficient Baseline Using Triplet Embedding,https://arxiv.org/abs/1901.01015,,"trained scenarios. This dataset comprises 416,314 vehicle images",
3372,Verifee,News Classification,News Classification,News Classification,Image,,Computer Vision,,,https://verifee.ai/research/,https://paperswithcode.com/dataset/verifee,"Verifee is a dataset of news articles with fine-grained trustworthiness annotations. It contains over 10, 000 unique articles from almost 60 Czech online news sources. These are categorized into one of the 4 classes across the credibility spectrum we propose, raging from entirely trustworthy articles all the way to the manipulative ones.",,Fine-grained Czech News Article Dataset: An Interdisciplinary Approach to Trustworthiness Analysis,https://arxiv.org/pdf/2212.08550v1.pdf,,,4
3373,Verse,Word Sense Disambiguation,Word Sense Disambiguation,"Word Sense Disambiguation, Image Retrieval, Action Recognition","Image, Video",,Computer Vision,,,https://github.com/spandanagella/verse,https://paperswithcode.com/dataset/verse,Verse is a new dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels.,,Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings,https://www.aclweb.org/anthology/N16-1022.pdf,,,
3374,VESUS,Emotion Recognition,Emotion Recognition,"Emotion Recognition, Voice Conversion","Audio, Image",,Computer Vision,,,https://engineering.jhu.edu/nsa/vesus/,https://paperswithcode.com/dataset/vesus,"The Varied Emotion in Syntactically Uniform Speech (VESUS) repository is a lexically controlled database collected by the NSA lab. Here, actors read a semantically neutral script of words, phrases, and sentences with different emotional inflections. VESUS contains 252 distinct phrases, each read by 10 actors in 5 emotional states (neutral, angry, happy, sad, fearful).",,,,,,
3375,VGG-Sound,Speaker Recognition,Speaker Recognition,"Speaker Recognition, Audio Classification, Multi-modal Classification, Speaker Verification, Image Classification, Video-to-Sound Generation, Zero-shot Audio Classification","Audio, Image, Text, Video",English,Computer Vision,"video-to-sound-generation-on-vgg-sound, zero-shot-audio-classification-on-vgg-sound, multi-modal-classification-on-vgg-sound, audio-classification-on-vggsound",CC-BY 4.0,http://www.robots.ox.ac.uk/~vgg/data/vggsound/,https://paperswithcode.com/dataset/vgg-sound,Consists of more than 210k videos for 310 audio classes.,,,,,,
3376,VGGFace2,Image Super-Resolution,Image Super-Resolution,"Image Super-Resolution, Image Attribution, Facial Inpainting",Image,,Computer Vision,"facial-inpainting-on-vggface2, image-attribution-on-vggface2, image-super-resolution-on-vggface2-8x",,https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,https://paperswithcode.com/dataset/vggface2-1,"VGGFace2 is a large-scale face recognition dataset. Images are downloaded from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession. VGGFace2 contains images from identities spanning a wide range of different ethnicities, accents, professions and ages. All face images are captured ""in the wild"", with pose and emotion variations and different lighting and occlusion conditions. Face distribution for different identities is varied, from 87 to 843, with an average of 362 images for each subject.",,,,362 images,,
3377,VGGFace2_HQ,Face Swapping,Face Swapping,"Face Swapping, Face Generation, Face Transfer, Face Age Editing, Image Generation, Age Estimation","Image, Text",English,Computer Vision,,Attribution-NonCommercial 4.0 International,https://github.com/NNNNAI/VGGFace2-HQ,https://paperswithcode.com/dataset/vggface2-hq,"A high-resolution version of VGGFace2 for academic face editing purposes.
This project uses GFPGAN for image restoration and insightface for data preprocessing (crop and align).",,,,,,
3378,VGGSound-Sparse,Video Synchronization,Video Synchronization,"Video Synchronization, Audio Classification, Audio-Visual Synchronization","Audio, Image, Video",,Computer Vision,,,https://v-iashin.github.io/SparseSync,https://paperswithcode.com/dataset/vggsound-sparse,"The dataset uses VGG-Sound which consists of 10s clips collected from YouTube for 309 sound classes. A subset of ‘temporally sparse’ classes is selected using the following procedure: 5–15 videos are randomly picked from each of the 309 VGGSound classes, and manually annotated as to whether audio-visual cues are only sparsely available. As a result, 12 classes are selected (∼4 %) or 6.5k and 0.6k videos in the train and test sets, respectively. The classes include 'dog barking', 'chopping wood', 'lion roaring', 'skateboarding' etc.",,,,,,12
3379,VGG_Face,,,", Face Verification, Domain Adaptation, Face Recognition",Image,,Computer Vision,on-vgg-face,CC BY-NC 4.0,https://www.robots.ox.ac.uk/~vgg/data/vgg_face/,https://paperswithcode.com/dataset/vgg-face-1,"The VGG Face dataset is face identity recognition dataset that consists of 2,622 identities. It contains over 2.6 million images.",,http://www.bmva.org/bmvc/2015/papers/paper041/paper041.pdf,http://www.bmva.org/bmvc/2015/papers/paper041/paper041.pdf,,,
3380,VGMIDI,Music Generation,Music Generation,"Music Generation, Music Emotion Recognition","Audio, Image, Text",English,Computer Vision,,,https://github.com/lucasnfe/vgmidi,https://paperswithcode.com/dataset/vgmidi,"VGMIDI is a dataset of piano arrangements of video game soundtracks. It contains 200 MIDI pieces labeled according to emotion and 3,850 unlabeled pieces. Each labeled piece was annotated by 30 human subjects according to the Circumplex (valence-arousal) model of emotion using a custom web tool.",,,,,,
3381,Vi-Fi_Multi-modal_Dataset,Multimodal Association,Multimodal Association,Multimodal Association,,,Multimodal,,,https://sites.google.com/winlab.rutgers.edu/vi-fidataset/home,https://paperswithcode.com/dataset/vi-fi-multi-modal-dataset,"A large-scale multi-modal dataset to facilitate research and studies that concentrate on vision-wireless systems.
The Vi-Fi dataset is a large-scale multi-modal dataset that consists of vision, wireless and smartphone motion sensor data of multiple participants and passer-by pedestrians in both indoor and outdoor scenarios. In Vi-Fi, vision modality includes RGB-D video from a mounted camera. Wireless modality comprises smartphone data from participants including WiFi FTM and IMU measurements.

The presence of Vi-Fi dataset facilitates and innovates multi-modal system research, especially, vision-wireless sensor data fusion, association and localization.

(Data collection was in accordance with IRB protocols and subject faces have been blurred for subject privacy.)",,,,,,
3382,Vibrating_Plates,PDE Surrogate Modeling,PDE Surrogate Modeling,PDE Surrogate Modeling,,,Methodology,,CC BY-NC 4.0,https://doi.org/10.25625/UWF7RB,https://paperswithcode.com/dataset/vibrating-plates,"We present a structured benchmark dataset for a representative vibroacoustic problem: Predicting the frequency response for vibrating plates. The vibrating plates benchmark dataset consists of in total 12,000 varied plate designs and accompanying vibration patterns, when the plates are excited by a harmonic force. These vibration platterns give the vibration velocity at every location of the plate orthogonal to its surface. The plate designs incorporate randomly placed beadings, indentations in the plate surface. The beadings stiffen the plates and completely change the resulting vibration patterns. Additionally, the size, thickness and damping loss factor of the plates are varied.

The dataset is intended to further the development of surrogate modeling methods for partial differential equations in the field of vibroacoustics. It contains two settings, G-5000 and V-5000. Both incorporate the beading pattern variation but only G-5000 additionally includes the variation of plate size, thickness and damping loss factor.",,,,,,
3383,VibraVox__forehead_accelerometer_,Automatic Phoneme Recognition,Automatic Phoneme Recognition,"Automatic Phoneme Recognition, Speaker Verification, Bandwidth Extension, Automatic Speech Recognition (ASR)","Audio, Image, Text",English,Computer Vision,"automatic-phoneme-recognition-on-vibravox-1, bandwidth-extension-on-vibravox-forehead, speaker-verification-on-vibravox-forehead",CC-BY-4.0,https://huggingface.co/datasets/Cnam-LMSSC/vibravox,https://paperswithcode.com/dataset/vibravox-forehead-accelerometer,"This is the forehead accelerometer variant of the VibraVox dataset.

VibraVox aims at serving as a valuable resource for advancing the field of body-conducted speech analysis and facilitating the development of robust communication systems for real-world applications. This dataset, available on HuggingFace can be used for various audio machine learning tasks :


Automatic Speech Recognition (ASR) (Speech-to-Text , Speech-to-Phoneme)
Audio Bandwidth Extension (BWE)
Speaker Verification (SPKV) / identification
Voice cloning
etc ...

The VibraVox dataset speech corpus has been released in July 2024. It includes french speech recorded simultaneously using multiple audio and vibration sensors : a forehead miniature vibration sensor, an in-ear comply foam-embedded microphone, an in-ear rigid earpiece-embedded microphone, a temple vibration pickup, a headset microphone located near the mouth, and a laryngophone. VibraVox has been recorded with 200 participants under various acoustic conditions imposed by a 5th order ambisonics spatialization sphere.",2024,,,,,
3384,VibraVox__headset_microphone_,Automatic Phoneme Recognition,Automatic Phoneme Recognition,"Automatic Phoneme Recognition, Speaker Verification, Automatic Speech Recognition (ASR)","Audio, Image, Text",English,Computer Vision,"speaker-verification-on-vibravox-headset, automatic-phoneme-recognition-on-vibravox",CC-BY-4.0,https://huggingface.co/datasets/Cnam-LMSSC/vibravox,https://paperswithcode.com/dataset/vibravox-headset-microphone,"This is the reference headset microphone variant of the VibraVox dataset.

VibraVox aims at serving as a valuable resource for advancing the field of body-conducted speech analysis and facilitating the development of robust communication systems for real-world applications. This dataset, available on HuggingFace can be used for various audio machine learning tasks :


Automatic Speech Recognition (ASR) (Speech-to-Text , Speech-to-Phoneme)
Audio Bandwidth Extension (BWE)
Speaker Verification (SPKV) / identification
Voice cloning
etc ...

The VibraVox dataset speech corpus has been released in July 2024. It includes french speech recorded simultaneously using multiple audio and vibration sensors : a forehead miniature vibration sensor, an in-ear comply foam-embedded microphone, an in-ear rigid earpiece-embedded microphone, a temple vibration pickup, a headset microphone located near the mouth, and a laryngophone. VibraVox has been recorded with 200 participants under various acoustic conditions imposed by a 5th order ambisonics spatialization sphere.",2024,,,,,
3385,VibraVox__rigid_in-ear_microphone_,Bandwidth Extension,Bandwidth Extension,"Bandwidth Extension, Automatic Speech Recognition (ASR), Automatic Phoneme Recognition, Speaker Verification, Speech-to-Phoneme","Audio, Image, Text",English,Computer Vision,"speaker-verification-on-vibravox-rigid-in-ear, speech-to-phoneme-on-vibravox-headset, bandwidth-extension-on-vibravox, automatic-phoneme-recognition-on-vibravox-3",CC-BY-4.0,https://huggingface.co/datasets/Cnam-LMSSC/vibravox,https://paperswithcode.com/dataset/vibravox,"This is the in-ear rigid earpiece-embedded microphone variant of the VibraVox dataset.

VibraVox aims at serving as a valuable resource for advancing the field of body-conducted speech analysis and facilitating the development of robust communication systems for real-world applications. This dataset, available on HuggingFace can be used for various audio machine learning tasks :


Automatic Speech Recognition (ASR) (Speech-to-Text , Speech-to-Phoneme)
Audio Bandwidth Extension (BWE)
Speaker Verification (SPKV) / identification
Voice cloning
etc ...

The VibraVox dataset speech corpus has been released in July 2024. It includes french speech recorded simultaneously using multiple audio and vibration sensors : a forehead miniature vibration sensor, an in-ear comply foam-embedded microphone, an in-ear rigid earpiece-embedded microphone, a temple vibration pickup, a headset microphone located near the mouth, and a laryngophone. VibraVox has been recorded with 200 participants under various acoustic conditions imposed by a 5th order ambisonics spatialization sphere.",2024,,,,,
3386,VibraVox__soft_in-ear_microphone_,Automatic Phoneme Recognition,Automatic Phoneme Recognition,"Automatic Phoneme Recognition, Speaker Verification, Bandwidth Extension, Automatic Speech Recognition (ASR)","Audio, Image, Text",English,Computer Vision,"bandwidth-extension-on-vibravox-soft-in-ear, automatic-phoneme-recognition-on-vibravox-2, speaker-verification-on-vibravox-soft-in-ear",CC-BY-4.0,https://huggingface.co/datasets/Cnam-LMSSC/vibravox,https://paperswithcode.com/dataset/vibravox-soft-in-ear-microphone,"This is the in-ear comply foam-embedded microphone variant of the VibraVox dataset.

VibraVox aims at serving as a valuable resource for advancing the field of body-conducted speech analysis and facilitating the development of robust communication systems for real-world applications. This dataset, available on HuggingFace can be used for various audio machine learning tasks :


Automatic Speech Recognition (ASR) (Speech-to-Text , Speech-to-Phoneme)
Audio Bandwidth Extension (BWE)
Speaker Verification (SPKV) / identification
Voice cloning
etc ...

The VibraVox dataset speech corpus has been released in July 2024. It includes french speech recorded simultaneously using multiple audio and vibration sensors : a forehead miniature vibration sensor, an in-ear comply foam-embedded microphone, an in-ear rigid earpiece-embedded microphone, a temple vibration pickup, a headset microphone located near the mouth, and a laryngophone. VibraVox has been recorded with 200 participants under various acoustic conditions imposed by a 5th order ambisonics spatialization sphere.",2024,,,,,
3387,VibraVox__temple_vibration_pickup_,Automatic Phoneme Recognition,Automatic Phoneme Recognition,"Automatic Phoneme Recognition, Speaker Verification, Bandwidth Extension, Automatic Speech Recognition (ASR)","Audio, Image, Text",English,Computer Vision,"automatic-phoneme-recognition-on-vibravox-5, bandwidth-extension-on-vibravox-temple, speaker-verification-on-vibravox-temple",CC-BY-4.0,https://huggingface.co/datasets/Cnam-LMSSC/vibravox,https://paperswithcode.com/dataset/vibravox-temple-vibration-pickup,"This is the temple vibration pickup variant of the VibraVox dataset.

VibraVox aims at serving as a valuable resource for advancing the field of body-conducted speech analysis and facilitating the development of robust communication systems for real-world applications. This dataset, available on HuggingFace can be used for various audio machine learning tasks :


Automatic Speech Recognition (ASR) (Speech-to-Text , Speech-to-Phoneme)
Audio Bandwidth Extension (BWE)
Speaker Verification (SPKV) / identification
Voice cloning
etc ...

The VibraVox dataset speech corpus has been released in July 2024. It includes french speech recorded simultaneously using multiple audio and vibration sensors : a forehead miniature vibration sensor, an in-ear comply foam-embedded microphone, an in-ear rigid earpiece-embedded microphone, a temple vibration pickup, a headset microphone located near the mouth, and a laryngophone. VibraVox has been recorded with 200 participants under various acoustic conditions imposed by a 5th order ambisonics spatialization sphere.",2024,,,,,
3388,VibraVox__throat_microphone_,Automatic Phoneme Recognition,Automatic Phoneme Recognition,"Automatic Phoneme Recognition, Speaker Verification, Bandwidth Extension, Automatic Speech Recognition (ASR)","Audio, Image, Text",English,Computer Vision,"bandwidth-extension-on-vibravox-throat, speaker-verification-on-vibravox-throat, automatic-phoneme-recognition-on-vibravox-4",CC-BY-4.0,https://huggingface.co/datasets/Cnam-LMSSC/vibravox,https://paperswithcode.com/dataset/vibravox-throat-microphone,"This is the throat microphone (laryngophone) variant of the VibraVox dataset.

VibraVox aims at serving as a valuable resource for advancing the field of body-conducted speech analysis and facilitating the development of robust communication systems for real-world applications. This dataset, available on HuggingFace can be used for various audio machine learning tasks :


Automatic Speech Recognition (ASR) (Speech-to-Text , Speech-to-Phoneme)
Audio Bandwidth Extension (BWE)
Speaker Verification (SPKV) / identification
Voice cloning
etc ...

The VibraVox dataset speech corpus has been released in July 2024. It includes french speech recorded simultaneously using multiple audio and vibration sensors : a forehead miniature vibration sensor, an in-ear comply foam-embedded microphone, an in-ear rigid earpiece-embedded microphone, a temple vibration pickup, a headset microphone located near the mouth, and a laryngophone. VibraVox has been recorded with 200 participants under various acoustic conditions imposed by a 5th order ambisonics spatialization sphere.",2024,,,,,
3389,Vid2RealHRI_online_video_and_results_dataset,Social Navigation,Social Navigation,Social Navigation,,,Methodology,,CC0,https://dataverse.tdl.org/dataset.xhtml?persistentId=doi:10.18738/T8/KAHJIB,https://paperswithcode.com/dataset/vid2realhri-online-video-and-results-dataset,"Introduction
This dataset was gathered during the Vid2RealHRI study of humans’ perception of robots' intelligence in the context of an incidental Human-Robot encounter. The dataset contains participants' questionnaire responses to four video study conditions, namely Baseline, Verbal, Body language, and Body language + Verbal. The videos depict a scenario where a pedestrian incidentally encounters a quadruped robot trying to enter a building. The robot uses verbal commands or body language to try to ask for help from the pedestrian in different study conditions. The differences in the conditions were manipulated using the robot’s verbal and expressive movement functionalities.

Dataset Purpose
The dataset includes the responses of human subjects about the robots' social intelligence used to validate the hypothesis that robot social intelligence is positively correlated with human compliance in an incidental human-robot encounter context. The video based dataset was also developed to obtain empirical evidence that can be used to design future real-world HRI studies.

Dataset Contents

Four videos, each corresponding to a study condition.
Four sets of Perceived Social Intelligence Scale data. Each set corresponds to one study condition
Four sets of compliance likelihood questions, each set include one Likert question and one free-form question
One set of Godspeed questionnaire data.
One set of Anthropomorphism questionnaire data.
A csv file containing the participants demographic data, Likert scale data, and text responses.
A data dictionary explaining the meaning of each of the fields in the csv file.

More details and access to this data are available from the Texas Data Repository. 

This research is part of the Vid2Real project, supported by NSF Award #2219236 and Good Systems, a Grand Research Challenge at the University of Texas at Austin.",,,,,,
3390,Vid4,Video Super-Resolution,Video Super-Resolution,"Video Super-Resolution, Space-time Video Super-resolution, Video Frame Interpolation, Key-Frame-based Video Super-Resolution (K = 15), Federated Learning (Video Super-Resolution)",Video,,Methodology,"video-frame-interpolation-on-vid4-4x, video-super-resolution-on-vid4-4x-upscaling, key-frame-based-video-super-resolution-k-15, federated-learning-video-super-resolution-on-1",,,https://paperswithcode.com/dataset/vid4,"The Vid4 dataset is generally used for testing video super-resolution. It consists of four sequences: walk (740x480, 47 frames), foliage (740x480, 49 frames), city (704x576, 34 frames), and calendar (720x576, 41 frames).",,,,,,
3391,Video-MME,Zero-Shot Video Question Answer,Zero-Shot Video Question Answer,Zero-Shot Video Question Answer,Video,,Methodology,"zero-shot-video-question-answer-on-video-mme, zero-shot-video-question-answer-on-video-mme-1",,https://video-mme.github.io,https://paperswithcode.com/dataset/video-mme,"Video-MME stands for Video Multi-Modal Evaluation. It is the first-ever comprehensive evaluation benchmark specifically designed for Multi-modal Large Language Models (MLLMs) in video analysis¹. This benchmark is significant because it addresses the need for a high-quality assessment of MLLMs' performance in processing sequential visual data, which has been less explored compared to their capabilities in static image understanding.

The Video-MME benchmark is characterized by its:
1. Diversity in video types, covering 6 primary visual domains with 30 subfields for broad scenario generalizability.
2. Duration in the temporal dimension, including short-, medium-, and long-term videos ranging from 11 seconds to 1 hour, to assess robust contextual dynamics.
3. Breadth in data modalities, integrating multi-modal inputs such as video frames, subtitles, and audios.
4. Quality in annotations, with rigorous manual labeling by expert annotators for precise and reliable model assessment¹.

The benchmark includes 900 videos totaling 256 hours, manually selected and annotated, resulting in 2,700 question-answer pairs. It has been used to evaluate various state-of-the-art MLLMs, including the GPT-4 series and Gemini 1.5 Pro, as well as open-source image and video models¹. The findings from Video-MME highlight the need for further improvements in handling longer sequences and multi-modal data, which is crucial for the advancement of MLLMs¹.

(1) [2405.21075] Video-MME: The First-Ever Comprehensive Evaluation .... https://arxiv.org/abs/2405.21075.
(2) Video-MME. https://video-mme.github.io/home_page.html.
(3) Video-MME: Welcome. https://video-mme.github.io/.
(4) undefined. https://doi.org/10.48550/arXiv.2405.21075.",,,,,,
3392,Video2GIF,Sentence Embedding,Sentence Embedding,"Sentence Embedding, Emotion Recognition, Video Summarization","Image, Text, Video",English,Computer Vision,,,https://github.com/gyglim/video2gif_dataset,https://paperswithcode.com/dataset/video2gif,"The Video2GIF dataset contains over 100,000 pairs of GIFs and their source videos. The GIFs were collected from two popular GIF websites (makeagif.com, gifsoup.com) and the corresponding source videos were collected from YouTube in Summer 2015. IDs and URLs of the GIFs and the videos are provided, along with temporal alignment of GIF segments to their source videos. The dataset shall be used to evaluate GIF creation and video highlight techniques.

In addition to the 100K GIF-video pairs, the dataset contains 357 pairs of GIFs and their source videos as the test set. The 357 videos come with a Creative Commons CC-BY license, which allowed the authors to redistribute the material with appropriate credit to make the results on test set reproducible even when some of the videos become unavailable.",2015,,,,,
3393,VideoCube,Video Object Tracking,Video Object Tracking,"Video Object Tracking, Object Tracking, Visual Object Tracking","Image, Video",,Computer Vision,visual-object-tracking-on-videocube,CC BY-NC-SA 4.0,http://videocube.aitestunion.com/downloads,https://paperswithcode.com/dataset/videocube,VideoCube is a high-quality and large-scale benchmark to create a challenging real-world experimental environment for Global Instance Tracking (GIT). MGIT is a high-quality and multi-modal benchmark based on VideoCube-Tiny to fully represent the complex spatio-temporal and causal relationships coupled in longer narrative content.,,,,,,
3394,VideoForensicsHQ,Metric Learning,Metric Learning,"Metric Learning, Face Swapping",Image,,Computer Vision,,,http://gvv.mpi-inf.mpg.de/projects/VForensicsHQ/,https://paperswithcode.com/dataset/videoforensicshq,"VideoForensicsHQ is a benchmark dataset for face video forgery detection, providing high quality visual manipulations.  It is one of the first face video manipulation benchmark sets that also contains audio and thus complements existing datasets along a new challenging dimension. VideoForensicsHQ shows manipulations at much higher video quality and resolution, and shows manipulations that are provably much harder to detect by humans than videos in other datasets. 

VideoForensicsHQ contains 1,737 videos of speaking faces (44% male, 56% female), with 8 different emotions, most of them of “HD” resolution. The videos amount to 1,666,816 frames.",,VideoForensicsHQ: Detecting High-quality Manipulated Face Videos,https://arxiv.org/abs/2005.10360,,,
3395,VideoInstruct,Video-based Generative Performance Benchmarking (Consistency),Video-based Generative Performance Benchmarking (Consistency),"Video-based Generative Performance Benchmarking (Consistency), Video-based Generative Performance Benchmarking (Detail Orientation)), Video-based Generative Performance Benchmarking (Temporal Understanding), Video Question Answering, VCGBench-Diverse, Video-based Generative Performance Benchmarking (Contextual Understanding), Video-based Generative Performance Benchmarking, Video-based Generative Performance Benchmarking (Correctness of Information)","Text, Time Series, Video",English,Natural Language Processing,"video-based-generative-performance-4, vcgbench-diverse-on-videoinstruct, video-based-generative-performance, video-based-generative-performance-1, video-based-generative-performance-3, video-based-generative-performance-5, video-based-generative-performance-2",Creative Commons Attribution 4.0,https://mbzuai-oryx.github.io/Video-ChatGPT,https://paperswithcode.com/dataset/videoinstruct,"Video Instruction Dataset is used to train Video-ChatGPT. It consists of 100,000 high-quality video instruction pairs. employs a combination of human-assisted and semi-automatic annotation techniques, aiming to produce high-quality video instruction data. These methods create question-answer pairs related to


Video summarization
Description-based question-answers (exploring spatial, temporal, relationships, and reasoning concepts)
Creative/generative question-answers",,,,,,
3396,VideoLQ,Video Super-Resolution,Video Super-Resolution,"Video Super-Resolution, Video Denoising",Video,,Methodology,video-denoising-on-videolq,,https://github.com/ckkelvinchan/RealBasicVSR,https://paperswithcode.com/dataset/videolq,"VideoLQ consists of videos downloaded from various video hosting sites such as Flickr and YouTube, with a Creative Common license.",,,,,,
3397,VidHOI,Action Detection,Action Detection,"Action Detection, Human-Object Interaction Anticipation, Human-Object Interaction Detection, Spatio-Temporal Action Localization","Image, Time Series, Video",,Computer Vision,"human-object-interaction-anticipation-on, human-object-interaction-detection-on-vidhoi",,https://github.com/coldmanck/VidHOI,https://paperswithcode.com/dataset/vidhoi,VidHOI is a video-based human-object interaction detection benchmark. VidHOI is based on VidOR which is densely annotated with all humans and predefined objects showing up in each frame. VidOR is also more challenging as the videos are non-volunteering user-generated and thus jittery at times.,,,,,,
3398,VIDIT,Image Relighting,Image Relighting,"Image Relighting, SSIM, Image-to-Image Translation, Domain Adaptation","Image, Text",English,Computer Vision,image-relighting-on-vidit20-validation-set,,https://github.com/majedelhelou/VIDIT,https://paperswithcode.com/dataset/vidit,"VIDIT  is a reference evaluation benchmark and to push forward the development of illumination manipulation methods. VIDIT includes 390 different Unreal Engine scenes, each captured with 40 illumination settings, resulting in 15,600 images. The illumination settings are all the combinations of 5 color temperatures (2500K, 3500K, 4500K, 5500K and 6500K) and 8 light directions (N, NE, E, SE, S, SW, W, NW). Original image resolution is 1024x1024.",,,,600 images,"valuation benchmark and to push forward the development of illumination manipulation methods. VIDIT includes 390 different Unreal Engine scenes, each captured with 40 illumination settings, resulting in 15,600 images",
3399,VIENA2,Human motion prediction,Human motion prediction,"Human motion prediction, Action Anticipation, motion prediction","Image, Time Series, Video",,Computer Vision,,,https://sites.google.com/view/viena2-project/home,https://paperswithcode.com/dataset/viena2,"Covers 5 generic driving scenarios, with a total of 25 distinct action classes. It contains more than 15K full HD, 5s long videos acquired in various driving conditions, weathers, daytimes and environments, complemented with a common and realistic set of sensor measurements. This amounts to more than 2.25M frames, each annotated with an action label, corresponding to 600 samples per action class.",,,,600 samples,,
3400,ViGGO,Data-to-Text Generation,Data-to-Text Generation,Data-to-Text Generation,Text,English,Natural Language Processing,data-to-text-generation-on-viggo-1,,https://nlds.soe.ucsc.edu/viggo,https://paperswithcode.com/dataset/viggo,"The ViGGO corpus is a set of 6,900 meaning representation to natural language utterance pairs in the video game domain. The meaning representations are of 9 different dialogue acts.",,,,,,
3401,ViHOS,Sequence-to-sequence Language Modeling,Sequence-to-sequence Language Modeling,"Sequence-to-sequence Language Modeling, Hate Span Identification","Text, Time Series",English,Natural Language Processing,sequence-to-sequence-language-modeling-on-46,,https://github.com/phusroyal/ViHOS,https://paperswithcode.com/dataset/vihos,The first human-annotated corpus containing 26k spans on 11k comments,,,,,,
3402,Vimeo90K,Video Prediction,Video Prediction,"Video Prediction, Video Super-Resolution, Image Super-Resolution, Optical Flow Estimation, Video Frame Interpolation","Image, Time Series, Video",,Computer Vision,"video-frame-interpolation-on-vimeo90k, video-prediction-on-vimeo90k, video-super-resolution-on-vimeo90k",,http://toflow.csail.mit.edu/,https://paperswithcode.com/dataset/vimeo90k-1,"The Vimeo-90K is a large-scale high-quality video dataset for lower-level video processing. It proposes three different video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.",,https://arxiv.org/pdf/1711.09078.pdf,https://arxiv.org/pdf/1711.09078.pdf,,,
3403,ViMQ,Named Entity Recognition (NER),Named Entity Recognition (NER),"Named Entity Recognition (NER), Intent Classification","Image, Text",English,Computer Vision,,,https://github.com/tadeephuy/vimq,https://paperswithcode.com/dataset/vimq,"ViMQ is a Vietnamese dataset of medical questions from patients with sentence-level and entity-level annotations for the Intent Classification and Named Entity Recognition tasks. It contains Vietnamese medical questions crawled from the consultation section online between patients and doctors from www.vinmec.com, a website of a Vietnamese general hospital. Each consultation consists of a question regarding a specific health issue of a patient and a detailed respond provided by a clinical expert. The dataset contains health issues that fall into a wide range of categories including common illness, cardiology, hematology, cancer, pediatrics, etc. We removed sections where users ask about information of the hospital and selected 9,000 questions for the dataset.",,ViMQ: A Vietnamese Medical Question Dataset for Healthcare Dialogue System Development,https://arxiv.org/pdf/2304.14405v1.pdf,,,
3404,Vinoground,Counterfactual Reasoning,Counterfactual Reasoning,"Counterfactual Reasoning, Temporal Relation Extraction, Video Compression","Graph, Time Series, Video",,Reasoning,temporal-relation-extraction-on-vinoground,,https://huggingface.co/datasets/HanSolo9682/Vinoground,https://paperswithcode.com/dataset/vinoground,A temporal counterfactual dataset composing of 1000 short and natural video-caption pairs.,,,,,,
3405,VIPeR,Patch Matching,Patch Matching,"Patch Matching, Metric Learning, Person Re-Identification",Image,,Computer Vision,,,https://github.com/KaiyangZhou/deep-person-reid/blob/master/torchreid/data/datasets/image/viper.py,https://paperswithcode.com/dataset/viper,"The Viewpoint Invariant Pedestrian Recognition (VIPeR) dataset includes 632 people and two outdoor cameras under different viewpoints and light conditions. Each person has one image per camera and each image has been scaled to be 128×48 pixels. It provides the pose angle of each person as 0° (front), 45°, 90° (right), 135°, and 180° (back).",,PaMM: Pose-aware Multi-shot Matching for Improving Person Re-identification,https://arxiv.org/abs/1705.06011,,,
3406,VIPL-HR,Heart Rate Variability,Heart Rate Variability,"Heart Rate Variability, Heart rate estimation",,,Methodology,heart-rate-estimation-on-vipl-hr,,http://vipl.ict.ac.cn/view_database.php?id=15,https://paperswithcode.com/dataset/vipl-hr,"VIPL-HR database is a database for remote heart rate (HR) estimation from face videos under less-constrained situations. It contains 2,378 visible light videos (VIS) and 752 near-infrared (NIR) videos of 107 subjects. Nine different conditions, including various head movements and illumination conditions are taken into consideration. All the videos are recorded using Logitech C310, RealSense F200 and the front camera of HUAWEI P9 smartphone, and the ground-truth HR is recorded using a CONTEC CMS60C BVP sensor (a FDA approved device).",,,,,,
3407,VIRDO_Dataset,Deformable Object Manipulation,Deformable Object Manipulation,"Deformable Object Manipulation, Surface Reconstruction",3D,,Methodology,,,https://github.com/MMintLab/VIRDO/,https://paperswithcode.com/dataset/virdo-dataset,"From https://github.com/MMintLab/VIRDO/blob/master/data/dataset_readme.txt,



DESCRIPTION: This dataset is written in 'dtype=torch.float64'. This dataset consists of total 144 deformation scenes from 6 different objects generated through MATLAB. It is divided into 'train' and 'test' dataset, where data['train'][OBJECT IDX = i][DEFORM IDX = j ] and data['test'][OBJECT IDX = i][DEFORM IDX = j ] indicates the same scene, but they are two different subsets of query points.



STRUCTURE: The dataset structure is as follows:
VIRDO_simul_dataset = {
'train':{
    <OBJECT IDX>: {
        'nominal': {
            'coords': tensor([1, M, 3]),
            'normals': tensor([1, M, 3]),
            'gt': tensor([1, M, 3]),
            'scale': float
            },
        <DEFORM IDX>: {
            'coords': tensor([1, M, 3]),
            'contact': tensor([1, M_c, 3]),
            'normals': tensor([1, M, 3]),
            'gt': tensor([1, M, 3]),
            'scale': float,
            'reaction': tensor([1,3]
            },
        },

},
'test':{
<OBJECT IDX>: {
    'nominal': {
        'coords': tensor([1, M, 3]),
        'normals': tensor([1, M, 3]),
        'gt': tensor([1, M, 3]),
        'scale': float
        },
    <DEFORM IDX>: {
        'coords': tensor([1, M, 3]),
        'contact': tensor([1, M_c, 3]),
        'normals': tensor([1, M, 3]),
        'gt': tensor([1, M, 3]),
        'scale': float,
        'reaction': tensor([1,3]
        },
    },

},
}


<OBJECT IDX> = Interger from 0 ~ 5. Each number indicates different object.
<DEFORM IDX> = Unique integer for each deformation.
M = total points (on-surface + off-surface)
M_c = a subset of on-surface points that are in contact
[:,i,:] elements of 'coords', 'normals', and 'gt' refers ith query point of a scene. To get on-surface points of data_def = data['train'][<OBJECT IDX>][<DEFORM IDX>], you should do data_def['coords'][:,torch.where(data_def['gt'] == 0)[1],:].",,,,,,
3408,VirtualHome2KG,Multi-modal Knowledge Graph,Multi-modal Knowledge Graph,Multi-modal Knowledge Graph,Graph,,Methodology,,MIT,https://github.com/aistairc/VirtualHome2KG,https://paperswithcode.com/dataset/virtualhome2kg,"VirtualHome2KG is a system for constructing and augmenting knowledge graphs (KGs) of daily living activities using virtual space. We also provide an ontology to describe the structure of the KGs. We used VirtualHome as a platform of virtual space simulation. Thus, this repository is an extension of the virtualhome. Please see the original repository of the virtualhome for details of the Unity simulation.",,,,,,
3409,Virtual_KITTI,Monocular 3D Object Detection,Monocular 3D Object Detection,"Monocular 3D Object Detection, Depth Estimation, Optical Flow Estimation, Semantic Segmentation, Monocular Depth Estimation, Simultaneous Localization and Mapping, Stereo Matching, Object Tracking, Multi-Object Tracking, Visual Odometry, Autonomous Driving","3D, Image, Video",English,Computer Vision,"monocular-3d-object-detection-on-virtual, monocular-depth-estimation-on-virtual-kitti-2",Creative Commons Attribution-NonCommercial-ShareAlike 3.0,https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/,https://paperswithcode.com/dataset/virtual-kitti,"Virtual KITTI is a photo-realistic synthetic video dataset designed to learn and evaluate computer vision models for several video understanding tasks: object detection and multi-object tracking, scene-level and instance-level semantic segmentation, optical flow, and depth estimation.

Virtual KITTI contains 50 high-resolution monocular videos (21,260 frames) generated from five different virtual worlds in urban settings under different imaging and weather conditions. These worlds were created using the Unity game engine and a novel real-to-virtual cloning method. These photo-realistic synthetic videos are automatically, exactly, and fully annotated for 2D and 3D multi-object tracking and at the pixel level with category, instance, flow, and depth labels (cf. below for download links).",,https://arxiv.org/pdf/1605.06457.pdf,https://arxiv.org/pdf/1605.06457.pdf,,,
3410,Virtual_KITTI_2,Monocular 3D Object Detection,Monocular 3D Object Detection,"Monocular 3D Object Detection, Depth Estimation, Semantic Segmentation, Monocular Depth Estimation, Simultaneous Localization and Mapping, Stereo Matching, Object Tracking, Multi-Object Tracking, Visual Odometry","3D, Image, Video",English,Computer Vision,"monocular-3d-object-detection-on-virtual, monocular-depth-estimation-on-virtual-kitti-2",Creative Commons Attribution-NonCommercial-ShareAlike 3.0,https://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds,https://paperswithcode.com/dataset/virtual-kitti-2,"Virtual KITTI 2 is an updated version of the well-known Virtual KITTI dataset which consists of 5 sequence clones from the KITTI tracking benchmark. In addition, the dataset provides different variants of these sequences such as modified weather conditions (e.g. fog, rain) or modified camera configurations (e.g. rotated by 15◦). For each sequence we provide multiple sets of images containing RGB, depth, class segmentation, instance segmentation, flow, and scene flow data. Camera parameters and poses as well as vehicle locations are available as well. In order to showcase some of the dataset’s capabilities, we ran multiple relevant experiments using state-of-the-art algorithms from the field of autonomous driving. The dataset is available for download at https://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds.",,,,,,
3411,VisDA-2017,Semi-supervised Domain Adaptation,Semi-supervised Domain Adaptation,"Semi-supervised Domain Adaptation, Universal Domain Adaptation, Semantic Segmentation, Partial Domain Adaptation, Unsupervised Domain Adaptation, Source-Free Domain Adaptation, Domain Adaptation",Image,,Computer Vision,"unsupervised-domain-adaptation-on-visda-2017-1, source-free-domain-adaptation-on-visda-2017, unsupervised-domain-adaptation-on-visda2017, domain-adaptation-on-visda2017, universal-domain-adaptation-on-visda2017, partial-domain-adaptation-on-visda2017, semi-supervised-domain-adaptation-on",Custom,http://ai.bu.edu/visda-2017/,https://paperswithcode.com/dataset/visda-2017,"VisDA-2017 is a simulation-to-real dataset for domain adaptation with over 280,000 images across 12 categories in the training, validation and testing domains. The training images are generated from the same object under different circumstances, while the validation images are collected from MSCOCO..",2017,Gradually Vanishing Bridge for Adversarial Domain Adaptation,https://arxiv.org/abs/2003.13183,000 images,"training, validation and testing domains. The training images",12
3412,VisDial,Common Sense Reasoning,Common Sense Reasoning,"Common Sense Reasoning, Question Answering, Chat-based Image Retrieval, Visual Dialog, Visual Question Answering (VQA)","Image, Text",English,Reasoning,"visual-dialog-on-visdial-v10-test-std, chat-based-image-retrieval-on-visdial, common-sense-reasoning-on-visual-dialog-v09, visual-dialog-on-visdial-v09-val, common-sense-reasoning-on-visual-dialog-v0-9, visual-dialog-on-visual-dialog-v1-0-test-std",Creative Commons Attribution 4.0 International,https://visualdialog.org/,https://paperswithcode.com/dataset/visdial,"Visual Dialog (VisDial) dataset contains human annotated questions based on images of MS COCO dataset. This dataset was developed by pairing two subjects on Amazon Mechanical Turk to chat about an image. One person was assigned the job of a ‘questioner’ and the other person acted as an ‘answerer’. The questioner sees only the text description of an image (i.e., an image caption from MS COCO dataset) and the original image remains hidden to the questioner. Their task is to ask questions about this hidden image to “imagine the scene better”. The answerer sees the image, caption and answers the questions asked by the questioner. The two of them can continue the conversation by asking and answering questions for 10 rounds at max.

VisDial v1.0 contains 123K dialogues on MS COCO (2017 training set) for training split, 2K dialogues with validation images for validation split and 8K dialogues on test set for test-standard set. The previously released v0.5 and v0.9 versions of VisDial dataset (corresponding to older splits of MS COCO) are considered deprecated.",2017,Granular Multimodal Attention Networks for Visual Dialog,https://arxiv.org/abs/1910.05728,,"training set) for training split, 2K dialogues with validation images",
3413,VisDrone,Object Detection,Object Detection,"Object Detection, Optical Flow Estimation","Image, Video",,Computer Vision,object-detection-on-visdrone-det2019-1,,https://github.com/VisDrone/VisDrone-Dataset,https://paperswithcode.com/dataset/visdrone,"VisDrone is a large-scale benchmark with carefully annotated ground-truth for various important computer vision tasks, to make vision meet drones. The VisDrone2019 dataset is collected by the AISKYEYE team at Lab of Machine Learning and Data Mining, Tianjin University, China. The benchmark dataset consists of 288 video clips formed by 261,908 frames and 10,209 static images, captured by various drone-mounted cameras, covering a wide range of aspects including location (taken from 14 different cities separated by thousands of kilometers in China), environment (urban and country), objects (pedestrian, vehicles, bicycles, etc.), and density (sparse and crowded scenes). Note that, the dataset was collected using various drone platforms (i.e., drones with different models), in different scenarios, and under various weather and lighting conditions. These frames are manually annotated with more than 2.6 million bounding boxes of targets of frequent interests, such as pedestrians, cars, bicycles, and tricycles. Some important attributes including scene visibility, object class and occlusion, are also provided for better data utilization.",,https://arxiv.org/pdf/2001.06303.pdf,https://arxiv.org/pdf/2001.06303.pdf,,,
3414,Visiting_Card___ID_Card_Images___Hindi-English,Optical Character Recognition (OCR),Optical Character Recognition (OCR),"Optical Character Recognition (OCR), Multilingual Machine Comprehension in English Hindi",Image,,Computer Vision,,,https://www.kaggle.com/datasets/dataclusterlabs/visiting-card-id-card,https://paperswithcode.com/dataset/visiting-card-id-card-images-hindi-english,"This dataset is an extremely challenging set of over 2000+ original Visiting card/ID card images captured and crowdsourced from over 300+ urban and rural areas, where each image is manually reviewed and verified by computer vision professionals at Datacluster Labs.

Dataset Features

Dataset size   : 2000+
Captured by  : Over 150+ crowdsource contributors
Resolution     : 100% images HD and above (1920x1080 and above)
Location        : Captured with 300+ cities accross India
Diversity        : Covers a wide variety of things such as reflective paper, different fonts, different color and type of cards, etc.
Device used  : Captured using mobile phones in 2020-2021
Usage            : Visiting card detection, card edge detection, paper edge detection, ID card OCR, Hindi OCR, etc.

Available Annotation formats
COCO, YOLO, PASCAL-VOC, Tf-Record

To download full datasets or to submit a request for your dataset needs, please ping us at sales@datacluster.ai Visit www.datacluster.ai to know more.  

Note:
All the images are manually captured and verified by a large contributor base on DataCluster platform",2000,,,,,
3415,ViSpamReviews,Spam detection,Spam detection,Spam detection,Image,,Computer Vision,,,https://github.com/sonlam1102/vispamdetection,https://paperswithcode.com/dataset/vispamreviews,This dataset is used for spam review detection (opinion spam reviews) on Vietnamese E-commerce website,,,,,,
3416,VisPro,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Natural Language Understanding, Visual Dialog","Image, Text",English,Computer Vision,,,https://github.com/HKUST-KnowComp/Visual_PCR,https://paperswithcode.com/dataset/vispro,"VisPro dataset contains coreference annotation of 29,722 pronouns from 5,000 dialogues.",,,,,,
3417,VIST-E,Image-guided Story Ending Generation,Image-guided Story Ending Generation,Image-guided Story Ending Generation,"Image, Text",English,Computer Vision,image-guided-story-ending-generation-on-vist,,,https://paperswithcode.com/dataset/vist-e,"VIST-E consists of 49,913 training samples, 4,963 validation samples and 5,030 test samples, which is modified from VIST dataset. As every sample in VIST contains a story of five sentences, each sample in VIST-E contains the story ending, the ending-related image and the first four sentences in the story as the story context. Additionally, each sentence is trimmed down to a maximum of 40 words.",,,,,"training samples, 4,963 validation samples",
3418,VIST-Edit,Visual Storytelling,Visual Storytelling,Visual Storytelling,Image,,Computer Vision,,,https://github.com/tingyaohsu/VIST-Edit,https://paperswithcode.com/dataset/vist-edit,"The dataset, VIST-Edit, includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions.",,Visual Story Post-Editing,https://arxiv.org/pdf/1906.01764v1.pdf,,,
3419,VIST,Story Continuation,Story Continuation,"Story Continuation, Visual Storytelling",Image,,Computer Vision,"story-continuation-on-vist, visual-storytelling-on-vist",,https://visionandlanguage.net/VIST/dataset.html,https://paperswithcode.com/dataset/vist,"The Visual Storytelling Dataset (VIST) consists of 210,819 unique photos and 50,000 stories. The images were collected from albums on Flickr. The albums included 10 to 50 images and all the images in an album are taken in a 48-hour span. The stories were created by workers on Amazon Mechanical Turk, where the workers were instructed to choose five images from the album and write a story about them. Every story has five sentences, and every sentence is paired with its appropriate image. The dataset is split into 3 subsets, a training set (80%), a validation set (10%) and a test set (10%). All the words and interpunction signs in the stories are separated by a space character and all the location names are replaced with the word location. All the names of people are replaced with the words male or female depending on the gender of the person.",,"Stories for Images-in-Sequence by using Visual and Narrative Components This research was partially funded by Pendulibrium and the Faculty of computer science and engineering, Ss. Cyril and Methodius University in Skopje.",https://arxiv.org/abs/1805.05622,50 images,,
3420,Vistas-NP,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Image Classification, Outlier Detection",Image,,Computer Vision,,,https://github.com/matejgrcic/Vistas-NP,https://paperswithcode.com/dataset/vistas-np,"The Vistas-NP dataset is an out-of-distribution detection dataset based on the Mapillary Vistas dataset. The original Vistas dataset consists of 18,000 training images and 2,000 validation images with 66 classes. In Vistas-NP the human classes are used as outliers due to their dispersion across scenes and visual diversity from other objects. The dataset is created by excluding all images with class person and the three rider classes to the test subset. Consequently, the dataset has 8,003 train images and 830 validation images. The test set contains 11,167.",,,,,"training images and 2,000 validation images",66
3421,Visual_Commonsense_Immorality_benchmark,Visual Commonsense Reasoning,Visual Commonsense Reasoning,Visual Commonsense Reasoning,Image,,Reasoning,,,https://github.com/ku-vai/zero-shot-visual-commonsense-immorality-prediction,https://paperswithcode.com/dataset/visual-commonsense-immorality-benchmark,"Visual Commonsense Immorality benchmark is a benchmark designed to evaluate commonsense immorality. It contains 2,172 immoral images for general and extensive immoral image detection.",,Zero-shot Visual Commonsense Immorality Prediction,https://arxiv.org/pdf/2211.05521v1.pdf,,"valuate commonsense immorality. It contains 2,172 immoral images",
3422,Visual_Genome,Multi-label Image Recognition with Partial Labels,Multi-label Image Recognition with Partial Labels,"Multi-label Image Recognition with Partial Labels, Scene Graph Generation, Phrase Grounding, Visual Relationship Detection, Dense Captioning, Bidirectional Relationship Classification, Image Generation from Scene Graphs, Unbiased Scene Graph Generation, Unsupervised semantic parsing, Scene Graph Detection, Scene Graph Classification, Predicate Classification, Object Detection, Visual Question Answering (VQA), Unsupervised KG-to-Text Generation, Layout-to-Image Generation","Graph, Image, Text",English,Computer Vision,"unsupervised-semantic-parsing-on-vg-graph, visual-question-answering-on-visual-genome-1, phrase-grounding-on-visual-genome, scene-graph-classification-on-visual-genome, scene-graph-generation-on-visual-genome, scene-graph-detection-on-visual-genome, predicate-classification-on-visual-genome, visual-relationship-detection-on-visual, layout-to-image-generation-on-visual-genome-2, image-generation-from-scene-graphs-on-visual, layout-to-image-generation-on-visual-genome-3, unbiased-scene-graph-generation-on-visual, dense-captioning-on-visual-genome, bidirectional-relationship-classification-on, visual-question-answering-on-visual-genome, object-detection-on-visual-genome, layout-to-image-generation-on-visual-genome-4, multi-label-image-recognition-with-partial-2, unsupervised-kg-to-text-generation-on-vg",CC BY 4.0,https://homes.cs.washington.edu/~ranjay/visualgenome/index.html,https://paperswithcode.com/dataset/visual-genome,"Visual Genome contains Visual Question Answering data in a multi-choice setting. It consists of 101,174 images from MSCOCO with 1.7 million QA pairs, 17 questions per image on average. Compared to the Visual Question Answering dataset, Visual Genome represents a more balanced distribution over 6 question types: What, Where, When, Who, Why and How. The Visual Genome dataset also presents 108K images with densely annotated objects, attributes and relationships.",,RaAM: A Relation-aware Attention Model for Visual Question Answering,https://arxiv.org/abs/1903.12314,174 images,,
3423,Visual_Question_Answering_v2.0,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Visual Question Answering","Image, Text",English,Computer Vision,"visual-question-answering-on-vqa-v2-test-std, visual-question-answering-on-vqa-v2-val, visual-question-answering-on-vqa-v2-test-std-1, visual-question-answering-on-vqa-v2-val-1, visual-question-answering-on-vqa-v2-test-dev, visual-question-answering-on-vqa-v2-1, visual-question-answering-on-vqa-v2-test-dev-1",,https://visualqa.org/,https://paperswithcode.com/dataset/visual-question-answering-v2-0,"Visual Question Answering (VQA) v2.0 is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. It is the second version of the VQA dataset.


265,016 images (COCO and abstract scenes)
At least 3 questions (5.4 questions on average) per image
10 ground truth answers per question
3 plausible (but likely incorrect) answers per question
Automatic evaluation metric

The first version of the dataset was released in October 2015.",2015,,,016 images,,
3424,Visual_Relationship_Detection_Dataset,Visual Relationship Detection,Visual Relationship Detection,Visual Relationship Detection,Image,,Computer Vision,,,https://cs.stanford.edu/people/ranjaykrishna/vrd/,https://paperswithcode.com/dataset/visual-relationship-detection-dataset,"A dataset containing 5000 images with 37,993 thousand relationships. The dataset contains 100 object categories and 70 predicate categories connecting those objects together.",,,,5000 images,,
3425,Visual_Wake_Words,Neural Architecture Search,Neural Architecture Search,"Neural Architecture Search, Image Classification, Quantization, Hardware Aware Neural Architecture Search",Image,,Computer Vision,"image-classification-on-visual-wake-words, hardware-aware-neural-architecture-search-on",Custom,https://github.com/Mxbonn/visualwakewords,https://paperswithcode.com/dataset/visual-wake-words,"Visual Wake Words represents a common microcontroller vision use-case of identifying whether a person is present in the image or not, and provides a realistic benchmark for tiny vision models.",,Visual Wake Words Dataset,https://arxiv.org/pdf/1906.05721.pdf,,,
3426,Visual_Writing_Prompts,Text Generation,Text Generation,"Text Generation, Story Generation, Visual Storytelling","Image, Text",English,Computer Vision,,Apache-2.0,https://vwprompt.github.io/,https://paperswithcode.com/dataset/visual-writing-prompts,"Hugging Face Datasets (New!)  | Website | Github Repository | arXiv e-Print

<!-- Provide a quick summary of the dataset. -->


The Visual Writing Prompts (VWP) dataset contains almost 2K selected sequences of
movie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which are collected via crowdsourcing given the image sequences and up to 5  grounded characters from the corresponding image sequence.

Dataset Details
Links
<!-- Provide the basic links for the dataset. -->



TACL 2023 Paper: Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences

Dataset Description
<!-- Provide a longer summary of what this dataset is. -->


The Visual Writing Prompts (VWP) dataset is designed to facilitate the development and testing of natural language processing models that generate stories based on sequences of images. This dataset comprises nearly 2,000 curated sequences of movie shots, each sequence containing between 5 to 10 images. These images are meticulously selected to ensure they depict coherent plots centered around one or more main characters, enhancing the visual narrative structure for story generation. Aligned with these image sequences are approximately 12,000 stories, which were written by crowd workers using Amazon Mechanical Turk. This setup aims to provide a rich, visually grounded storytelling context that helps models generate more coherent, diverse, and engaging stories.


Curated by: Xudong Hong, Asad Sayeed, Khushboo Mehra, Vera Demberg, Bernt Schiele
Funded by: See Acknowledgments in our paper
Language(s) (NLP): English
License: Apache License 2.0

Dataset Structure
<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->


The dataset is in a CSV file. The explanation of each column is in this table.

Uses
<!-- Address questions about how the dataset is intended to be used. -->


Direct Use
<!-- This section describes suitable use cases for the dataset. -->


The dataset is intended for use in natural language processing tasks, particularly for the development and evaluation of models designed to generate coherent and visually grounded stories from sequences of images.

Out-of-Scope Use
<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->


The copyrights of all movie shots belong to the original copyright holders which can be found in the IMDb page of each movie. The IMDb page is indicated by the index in the imdb_id column. For example, for the first row of our data, the imdb_id is tt0112573 so the corresponding imdb page is https://www.imdb.com/title/tt0112573/companycredits/. Do not violate the copyrights while using these images. The usage of these images is limited to academic purposes.

Dataset Creation
Curation Rationale
<!-- Motivation for the creation of this dataset. -->


The dataset was curated to improve the quality of text stories generated from image sequences, focusing on visual storytelling with coherent plots and character grounding.

Source Data
<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->


Data Collection and Processing
<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->


The source data consists of image sequences extracted from the movie shots from the MovieNet dataset (https://opendatalab.com/OpenDataLab/MovieNet/tree/main/raw), ensuring a coherent plot around one or more main characters.

Who are the source data producers?
<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->


The images were initially produced by movie production companies and extracted by authors of MovieNet. The stories are written by crowd workers. Then the stories are compiled and refined by the authors.

Annotations
<!-- If the dataset contains annotations that are not part of the initial data collection, use this section to describe them. -->


Annotation process
<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->


Crowdworkers were asked to write stories that fit the provided image sequences. The annotation process included reviewing these stories for coherence, grammatical correctness, and alignment with the images. More details are in our paper.

Who are the annotators?
<!-- This section describes the people or systems who created the annotations. -->


The annotators were five graduate students from Saarland University. Two are native English speakers. The other three are proficient in English.

Personal and Sensitive Information
<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->


We do not collect personal or sensitive information. Personal information like worker IDs are not released. Our anonymization process is described in our paper.

Bias, Risks, and Limitations
<!-- This section is meant to convey both technical and sociotechnical limitations. -->


The stories in this dataset are in English only. Although we have tried our best to filter the images and review the stories, it is not possible to go through all the stories. There could still be biased or harmful content. Please use the dataset carefully.

Citation
Xudong Hong, Asad Sayeed, Khushboo Mehra, Vera Demberg, and Bernt Schiele. 2023. Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences. Transactions of the Association for Computational Linguistics, 11:565–581.

BibTeX:

latex
@article{10.1162/tacl_a_00553,
author = {Hong, Xudong and Sayeed, Asad and Mehra, Khushboo and Demberg, Vera and Schiele, Bernt},
title = ""{Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences}"",
journal = {Transactions of the Association for Computational Linguistics},
volume = {11},
pages = {565-581},
year = {2023},
month = {06},
issn = {2307-387X},
doi = {10.1162/tacl_a_00553},
url = {[https://doi.org/10.1162/tacl\\\\_a\\\\_00553](https://doi.org/10.1162/tacl%5C%5C%5C%5C_a%5C%5C%5C%5C_00553)},
eprint = {[https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\\\_a\\\\_00553/2134487/tacl\\\\_a\\\\_00553.pdf](https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl%5C%5C%5C%5C_a%5C%5C%5C%5C_00553/2134487/tacl%5C%5C%5C%5C_a%5C%5C%5C%5C_00553.pdf)},
}

Dataset Card Authors
Xudong Hong

Dataset Card Contact
xLASTNAME@coli.uni-saarland.de

Disclaimer:
All the images are extracted from the movie shots from the MovieNet dataset (https://opendatalab.com/OpenDataLab/MovieNet/tree/main/raw). The copyrights of all movie shots belong to the original copyright holders which can be found in the IMDb page of each movie. The IMDb page is indicated by the index in the imdb_id column. For example, for the first row of our data, the imdb_id is tt0112573 so the corresponding imdb page is https://www.imdb.com/title/tt0112573/companycredits/. Do not violate the copyrights while using these images. We only use these images for academic purposes. Please contact the author if you have any questions.",2023,arXiv e-Print,https://arxiv.org/abs/2301.08571,10 images,"testing of natural language processing models that generate stories based on sequences of images. This dataset comprises nearly 2,000 curated sequences of movie shots, each sequence containing between 5 to 10 images",
3427,VISUELLE,New Product Sales Forecasting,New Product Sales Forecasting,New Product Sales Forecasting,Time Series,,Methodology,new-product-sales-forecasting-on-visuelle,MIT,https://github.com/HumaticsLAB/GTM-Transformer,https://paperswithcode.com/dataset/visuelle,"VISUELLE is a repository build upon the data of a real fast fashion company, Nunalie, and is composed of 5577 new products and about 45M sales related to fashion seasons from 2016-2019. Each product in VISUELLE is equipped with multimodal information: its image, textual metadata, sales after the first release date, and three related Google Trends describing category, color and fabric popularity.

Download  <a href=""https://drive.google.com/file/d/11Bn2efKfO_PbtdqsSqj8U6y6YgBlRcP6/view?usp=sharing"">here</a>",2016,https://arxiv.org/pdf/2109.09824v1.pdf,https://arxiv.org/pdf/2109.09824v1.pdf,,,
3428,VISUELLE2.0,Product Recommendation,Product Recommendation,"Product Recommendation, Popularity Forecasting, Time Series Forecasting, New Product Sales Forecasting, Short-observation new product sales forecasting",Time Series,,Methodology,"new-product-sales-forecasting-on-visuelle2-0, short-observation-new-product-sales",,https://humaticslab.github.io/forecasting/visuelle,https://paperswithcode.com/dataset/visuelle2-0,"Visuelle 2.0 is a dataset containing real data for 5355 clothing products of the retail fast-fashion Italian company, Nuna Lie. Specifically, Visuelle 2.0 provides data from 6 fashion seasons (partitioned in Autumn-Winter and Spring-Summer) from 2017-2019, right before the Covid-19 pandemic. Each product is accompanied by an HD image, textual tags and more. The time series data are disaggregated at the shop level, and include the sales, inventory stock, max-normalized prices (for the sake of confidentiality} and discounts. Exogenous time series data is also provided, in the form of Google Trends based on the textual tags and multivariate weather conditions of the stores’ locations. Finally, we also provide purchase data for 667K customers whose identity has been anonymized, to capture personal preferences. With these data, Visuelle 2.0 allows to cope with several problems which characterize the activity of a fast fashion company: new product demand forecasting, short-observation new product sales forecasting, and product recommendation.",2017,,,,,
3429,Visuomotor_affordance_learning__VAL__robot_interac,Offline RL,Offline RL,Offline RL,,,Methodology,,,https://sites.google.com/view/val-rl,https://paperswithcode.com/dataset/visuomotor-affordance-learning-val-robot,"This data contains about 2500 trajectories (with images and actions) of a Sawyer robot interacting with various objects.

Examples from the dataset are shown in the adjacent video. We provide two versions of the VAL dataset - one with low-res images (1.4 GB) and one with high-res images (162 GB). The data quantity and format is the same between these two versions; the difference is only the image observation quality.

The smaller dataset, with 48x48x3 images which can be used for eg. offline RL, is available for direct download: https://drive.google.com/file/d/1UuWANkVtWLg4egIK2LB_YCKuF87rMQ1H/view?usp=sharing

The larger dataset, with 480x640x3 which might be preferred for eg. representation learning, is available at this Google drive folder:  https://drive.google.com/drive/folders/1kD9kyP7-RlIrSnuN7rpEASAGWp5qnNov?usp=sharing

To download the larger dataset, we suggest using https://rclone.org/

The data is sorted into several folders. There are a total of 300 files and 2500 trajectories.
- fixed_drawer - Human-controlled demonstration data opening and closing drawers. (~10%)
- fixed_pnp - Human-controlled demonstration data picking up objects. (~10%)
- fixed_pot - Human-controlled demonstration data interacting with a pot and a lid. (~10%)
- fixed_tray - Human-controlled demonstration data picking up objects and placing it in a tray. (~10%)
- general - Further human-controlled demonstration data collected with the most diversity and variation. (~40%)
- onpolicy_eval - Evaluation data collected by an RL policy. (~10%)
- onpolicy_expl - Exploration data collected by an RL policy. (~10%)",,,,3 images,VAL dataset - one with low-res images (1.4 GB) and one with high-res images,
3430,VitaminC,Fact Checking,Fact Checking,"Fact Checking, Fact Verification, Conditional Text Generation",Text,English,Natural Language Processing,,CC BY-SA 3.0,https://github.com/TalSchuster/VitaminC,https://paperswithcode.com/dataset/vitaminc,"The VitaminC dataset contains more than 450,000 claim-evidence pairs for fact verification and factual consistent generation. Based on over 100,000 revisions to popular Wikipedia pages, and additional ""synthetic"" revisions.",,,,,,
3431,VizDoom,Q-Learning,Q-Learning,"Q-Learning, Scene Generation, Game of Doom, Image Generation, Decision Making, Starcraft","Image, Text",English,Computer Vision,"game-of-doom-on-vizdoom-basic-scenario, image-generation-on-vizdoom, scene-generation-on-vizdoom",Custom (multiple),http://vizdoom.cs.put.edu.pl/,https://paperswithcode.com/dataset/vizdoom,"ViZDoom is an AI research platform based on the classical First Person Shooter game Doom. The most popular game mode is probably the so-called Death Match, where several players join in a maze and fight against each other. After a fixed time, the match ends and all the players are ranked by the FRAG scores defined as kills minus suicides. During the game, each player can access various observations, including the first-person view screen pixels, the corresponding depth-map and segmentation-map (pixel-wise object labels), the bird-view maze map, etc. The valid actions include almost all the keyboard-stroke and mouse-control a human player can take, accounting for moving, turning, jumping, shooting, changing weapon, etc. ViZDoom can run a game either synchronously or asynchronously, indicating whether the game core waits until all players’ actions are collected or runs in a constant frame rate without waiting.",,Arena: a toolkit for Multi-Agent Reinforcement Learning,https://arxiv.org/abs/1907.09467,,,
3432,VizNet-Sato,Table annotation,Table annotation,"Table annotation, Column Type Annotation",Tabular,,Methodology,"column-type-annotation-on-viznet-sato-full, column-type-annotation-on-viznet-sato-1",,https://github.com/megagonlabs/sato/#original-tables,https://paperswithcode.com/dataset/viznet-sato,"VizNet-Sato is a dataset from the authors of Sato and is based on the VizNet dataset. The authors choose from VizNet only relational web tables with headers matching their selected 78 DBpedia semantic types.  The selected tables are divided into two categories: Full tables and Multi-column only tables. The first category corresponds to 78,733 selected tables from VizNet, while the second category includes 32,265 tables which have more than one column.  The tables of both categories are divided into 5 subsets to be able to conduct 5-fold cross validation: 4 subsets are used for training and the last for evaluation.

The headers of the columns act as semantic annotations for the Column Type Annotation (CTA) task. Some statistics about both categories of tables are provided in the table below, where ""Columns"" refers to the number of annotated columns and ""Classes"" to the number of unique DBpedia semantic types used for annotation.

|                 | Columns | Classes |
|-----------------|---------|---------|
| Full | 120,609 | 78  |
| Multi-column | 74,141 | 78 |",,,,,,
3433,VizWiz-Classification,Image Classification,Image Classification,"Image Classification, Multi-Label Image Classification, Multi-Label Classification, Domain Generalization",Image,,Computer Vision,"domain-generalization-on-vizwiz, image-classification-on-vizwiz-classification, multi-label-image-classification-on-vizwiz",,https://vizwiz.org/tasks-and-datasets/image-classification/,https://paperswithcode.com/dataset/vizwiz-classification,"Our goal is to improve upon the status quo for designing image classification models trained in one domain that perform well on images from another domain. Complementing existing work in robustness testing, we introduce the first test dataset for this purpose which comes from an authentic use case where photographers wanted to learn about the content in their images. We built a new test set using 8,900 images taken by people who are blind for which we collected metadata to indicate the presence versus absence of 200 ImageNet object categories. We call this dataset VizWiz-Classification.",,,,900 images,"trained in one domain that perform well on images from another domain. Complementing existing work in robustness testing, we introduce the first test dataset for this purpose which comes from an authentic use case where photographers wanted to learn about the content in their images",
3434,VizWiz-FewShot,Object Detection,Object Detection,"Object Detection, Few-Shot Object Detection, Instance Segmentation, Semantic Segmentation",Image,,Computer Vision,,Creative Common CC-BY 4.0,,https://paperswithcode.com/dataset/vizwiz-fewshot,"VizWiz-FewShot is a a few-shot localization dataset originating from photographers who authentically were trying to learn about the visual content in the images they took. It includes nearly 10,000 segmentations of 100 categories in over 4,500 images that were taken by people with visual impairments.",,,,500 images,,100
3435,VizWiz,Visual Question Answering,Visual Question Answering,"Visual Question Answering, Visual Question Answering (VQA), Image Captioning","Image, Text",English,Computer Vision,"image-captioning-on-vizwiz-2020-test-dev, image-captioning-on-vizwiz-2020-test, visual-question-answering-on-vizwiz-2018-1, visual-question-answering-on-vizwiz-2018, visual-question-answering-on-vizwiz-2020-vqa, visual-question-answering-on-vizwiz-2020, visual-question-answering-on-vizwiz-1",CC BY 4.0,https://vizwiz.org/tasks-and-datasets/vqa/,https://paperswithcode.com/dataset/vizwiz,"The VizWiz-VQA dataset originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. The proposed challenge addresses the following two tasks for this dataset: predict the answer to a visual question and (2) predict whether a visual question cannot be answered.",,,,,,
3436,VLOG_Dataset,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Video Semantic Segmentation, Optical Flow Estimation","Image, Video",,Computer Vision,,,https://web.eecs.umich.edu/~fouhey/2017/VLOG/index.html,https://paperswithcode.com/dataset/vlog-dataset,A large collection of interaction-rich video data which are annotated and analyzed.,,,,,,
3437,VMD,Short-Text Conversation,Short-Text Conversation,"Short-Text Conversation, Large Language Model, text annotation, User Simulation",Text,English,Natural Language Processing,,CC BY-SA,https://github.com/dimits-ts/synthetic_moderation_experiments/blob/master/data/dataset.zip,https://paperswithcode.com/dataset/vmd,"This dataset contains synthetically generated discussions and annotations using exclusively LLM agents. Discussions are performed between randomly selected users, with a LLM moderator featuring various moderation strategies.

Each LLM user and annotator use a distinct Sociodemographic Background. User-agents also have different intents when joining the discussion. Each discussion consists of 28 comments - 14 participant comments and 14 moderator interventions. Each comment is annotated by 10 different LLM annotators for toxicity and argument quality. 

Designed to analyze moderation strategies and synthetic online discussion simulation but can also be used for LLM moderator finetuning. Available in both CSV (61,147 × 33) and JSON formats.",,,,,,
3438,VNAT,Security Studies,Security Studies,"Security Studies, Classification",Image,,Computer Vision,,,https://www.ll.mit.edu/r-d/datasets/vpnnonvpn-network-application-traffic-dataset-vnat,https://paperswithcode.com/dataset/vnat,"This dataset is a collection of labelled PCAP files, both encrypted and unencrypted, across 10 applications, as well as a pandas dataframe in HDF5 format containing detailed metadata summarizing the connections from those files. It was created to assist the development of machine learning tools that would allow operators to see the traffic categories of both encrypted and unencrypted traffic flows. In particular, features of the network packet traffic timing and size information (both inside of and outside of the VPN) can be leveraged to predict the application category that generated the traffic.",,,,,,
3439,VOC-MLT,Zero-Shot Learning,Zero-Shot Learning,"Zero-Shot Learning, Multi-Label Image Classification, Long-tail Learning",Image,,Computer Vision,"zero-shot-learning-on-voc-mlt, long-tail-learning-on-voc-mlt",,,https://paperswithcode.com/dataset/voc-mlt,"We construct the long-tailed version of VOC  from its 2012 train-val set. It contains 1,142 images from 20 classes, with a maximum of 775 images per class and a minimum of 4 images per class.  The ratio of head, medium, and tail classes after splitting is 6:6:8. We evaluate the performance on VOC2007 test set with 4952 images.",2012,,,142 images,"train-val set. It contains 1,142 images",20
3440,VocalSet,Singer Identification,Singer Identification,Singer Identification,,,Methodology,,Creative Commons Attribution 4.0 International,,https://paperswithcode.com/dataset/vocalset,"VocalSet is a a singing voice dataset consisting of 10.1 hours of monophonic recorded audio of professional singers demonstrating both standard and extended vocal techniques on all 5 vowels. Existing singing voice datasets aim to capture a focused subset of singing voice characteristics, and generally consist of just a few singers. VocalSet contains recordings from 20 different singers (9 male, 11 female) and a range of voice types.  VocalSet aims to improve the state of existing singing voice datasets and singing voice research by capturing not only a range of vowels, but also a diverse set of voices on many different vocal techniques, sung in contexts of scales, arpeggios, long tones, and excerpts.",,,,,,
3441,VocalSound,Audio Tagging,Audio Tagging,"Audio Tagging, Audio Classification, Speech Recognition","Audio, Image, Text",English,Audio,audio-classification-on-vocalsound,CC BY-SA,https://groups.csail.mit.edu/sls/downloads/vocalsound/,https://paperswithcode.com/dataset/vocalsound,"VocalSound is a free dataset consisting of 21,024 crowdsourced recordings of laughter, sighs, coughs, throat clearing, sneezes, and sniffs from 3,365 unique subjects. The VocalSound dataset also contains meta-information such as speaker age, gender, native language, country, and health condition.",,,,,,
3442,VOCASET,Talking Face Generation,Talking Face Generation,"Talking Face Generation, 3D Face Animation","3D, Image, Text",English,Computer Vision,3d-face-animation-on-vocaset,,https://voca.is.tue.mpg.de/,https://paperswithcode.com/dataset/vocaset,VOCASET is a 4D face dataset with about 29 minutes of 4D scans captured at 60 fps and synchronized audio. The dataset has 12 subjects and 480 sequences of about 3-4 seconds each with sentences chosen from an array of standard protocols that maximize phonetic diversity.,,,,,,
3443,VoiceBank___DEMAND,Speech Enhancement,Speech Enhancement,Speech Enhancement,Audio,,Speech,speech-enhancement-on-demand,CC BY 4.0,https://datashare.ed.ac.uk/handle/10283/2791,https://paperswithcode.com/dataset/demand,"VoiceBank+DEMAND is a noisy speech database for training speech enhancement algorithms and TTS models. The database was designed to train and test speech enhancement methods that operate at 48kHz. A more detailed description can be found in the paper associated with the database. Some of the noises were obtained from the Demand database, available here: http://parole.loria.fr/DEMAND/ . The speech database was obtained from the Voice Banking Corpus, available here: http://homepages.inf.ed.ac.uk/jyamagis/release/VCTK-Corpus.tar.gz .",,,,,,
3444,Volleyball,Group Activity Recognition,Group Activity Recognition,"Group Activity Recognition, Sports Ball Detection and Tracking, Action Recognition","Image, Video",,Computer Vision,"sports-ball-detection-and-tracking-on-1, action-recognition-in-videos-on-volleyball, group-activity-recognition-on-volleyball",,https://github.com/mostafa-saad/deep-activity-rec#dataset,https://paperswithcode.com/dataset/volleyball,Volleyball is a video action recognition dataset. It has 4830 annotated frames that were handpicked from 55 videos with 9 player action labels and 8 team activity labels. It contains group activity annotations as well as individual activity annotations.,,,,,,
3445,VOT2014,Video Object Tracking,Video Object Tracking,"Video Object Tracking, Object Tracking, Visual Tracking, Visual Object Tracking","Image, Video",,Computer Vision,visual-object-tracking-on-vot2014,,https://www.votchallenge.net/vot2014/dataset.html,https://paperswithcode.com/dataset/vot2014,"The dataset comprises 25 short sequences showing various objects in challenging backgrounds. Eight sequences are from the VOT2013 challenge (bolt, bicycle, david, diving, gymnastics, hand, sunshade, woman). The new sequences show complementary objects and backgrounds, for example a fish underwater or a surfer riding a big wave. The sequences were chosen from a large pool of sequences using a methodology based on clustering visual features of object and background so that those 25 sequences sample evenly well the existing pool.",,,,,,
3446,VOT2016,Visual Object Tracking,Visual Object Tracking,Visual Object Tracking,"Image, Video",,Computer Vision,visual-object-tracking-on-vot2016,,https://www.votchallenge.net/vot2016/dataset.html,https://paperswithcode.com/dataset/vot2016,"VOT2016 is a video dataset for visual object tracking. It contains 60 video clips and 21,646 corresponding ground truth maps with pixel-wise annotation of salient objects.",,Video Saliency Detection by 3D Convolutional Neural Networks,https://arxiv.org/abs/1807.04514,,,
3447,VOT2017,Object Tracking,Object Tracking,"Object Tracking, Visual Object Tracking","Image, Video",,Computer Vision,"visual-object-tracking-on-vot2017, visual-object-tracking-on-vot201718",,https://www.votchallenge.net/vot2017/dataset.html,https://paperswithcode.com/dataset/vot2017,VOT2017 is a Visual Object Tracking dataset for different tasks that contains 60 short sequences annotated with 6 different attributes.,,GradNet: Gradient-Guided Network for Visual Object Tracking,https://arxiv.org/abs/1909.06800,,,
3448,VOT2018,Object Tracking,Object Tracking,"Object Tracking, Visual Object Tracking","Image, Video",,Computer Vision,visual-object-tracking-on-vot2018,,https://www.votchallenge.net/vot2018/dataset.html,https://paperswithcode.com/dataset/vot2018,VOT2018 is a dataset for visual object tracking. It consists of 60 challenging videos collected from real-life datasets.,,Remove Cosine Window from Correlation Filter-based Visual Trackers: When and How,https://arxiv.org/abs/1905.06648,,,
3449,VOTChallenge,Semi-Supervised Video Object Segmentation,Semi-Supervised Video Object Segmentation,"Semi-Supervised Video Object Segmentation, Visual Tracking, Video Object Tracking, Visual Object Tracking, Object Tracking","Image, Video",,Computer Vision,"semi-supervised-video-object-segmentation-on-15, visual-object-tracking-on-vot2019, visual-object-tracking-on-vot2018, visual-object-tracking-on-vot2022, visual-object-tracking-on-vot2016, visual-object-tracking-on-vot2014, visual-object-tracking-on-vot201718, visual-object-tracking-on-vot2017",,https://www.votchallenge.net/,https://paperswithcode.com/dataset/vot,The Visual Object Tracking (VOT) dataset is a collection of video sequences used for evaluating and benchmarking visual object tracking algorithms. It provides a standardized platform for researchers and practitioners to assess the performance of different tracking methods.,,,,,,
3450,Voxceleb-3D,3D Face Modelling,3D Face Modelling,3D Face Modelling,"3D, Image",,Computer Vision,3d-face-modelling-on-voxceleb-3d,MIT,https://choyingw.github.io/works/Voice2Mesh/index.html,https://paperswithcode.com/dataset/voxceleb-3d,"A dataset for voice and 3D face structure study. It contains about 1.4K identities with their 3D face models and voice data. 3D face models are fitted from VGGFace using BFM 3D models, and voice data are processed from Voxceleb",,,,,,
3451,VoxCeleb1,,,", Learning with noisy labels, Speaker Recognition, Talking Head Generation, Speaker Verification, Video Reconstruction, Speaker Identification, Few-Shot Audio Classification","3D, Audio, Image, Text, Video",English,Computer Vision,"speaker-verification-on-voxceleb, talking-head-generation-on-voxceleb1-8-shot, video-reconstruction-on-voxceleb, speaker-recognition-on-voxceleb1, on-voxceleb1, talking-head-generation-on-voxceleb1-1-shot, few-shot-audio-classification-on-voxceleb1, talking-head-generation-on-voxceleb1-32-shot, speaker-verification-on-voxceleb1, speaker-identification-on-voxceleb1",,https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html,https://paperswithcode.com/dataset/voxceleb1,"VoxCeleb1 is an audio dataset containing over 100,000 utterances for 1,251 celebrities, extracted from videos uploaded to YouTube.",,,,,,
3452,VoxCeleb2,Talking Head Generation,Talking Head Generation,"Talking Head Generation, Speaker Verification, Speech Separation","Audio, Text",English,Natural Language Processing,"talking-head-generation-on-voxceleb2-1-shot, speech-separation-on-voxceleb2, talking-head-generation-on-voxceleb2-8-shot, talking-head-generation-on-voxceleb2-32-shot, speaker-verification-on-voxceleb2",CC BY 4.0,https://www.robots.ox.ac.uk/~vgg/data/voxceleb/,https://paperswithcode.com/dataset/voxceleb2,"VoxCeleb2 is a large scale speaker recognition dataset obtained automatically from open-source media. VoxCeleb2 consists of over a million utterances from over 6k speakers. Since the dataset is collected ‘in the wild’, the speech segments are corrupted with real world noise including laughter, cross-talk, channel effects, music and other sounds. The dataset is also multilingual, with speech from speakers of 145 different nationalities, covering a wide range of accents, ages, ethnicities and languages. The dataset is audio-visual, so is also useful for a number of other applications, for example – visual speech synthesis, speech separation, cross-modal transfer from face to voice or vice versa and training face recognition from video to complement existing face recognition datasets.",,VoxCeleb2: Deep Speaker Recognition,https://arxiv.org/pdf/1806.05622v2.pdf,,,
3453,VoxForge,Keyword Spotting,Keyword Spotting,"Keyword Spotting, Spoken language identification, Accented Speech Recognition, Language Identification","Audio, Image, Text",English,Computer Vision,"language-identification-on-voxforge, accented-speech-recognition-on-voxforge-3, keyword-spotting-on-voxforge, spoken-language-identification-on-voxforge, accented-speech-recognition-on-voxforge, spoken-language-identification-on-voxforge-2, accented-speech-recognition-on-voxforge-1, accented-speech-recognition-on-voxforge-2, spoken-language-identification-on-voxforge-1",,http://www.voxforge.org/home,https://paperswithcode.com/dataset/voxforge,"VoxForge is an open speech dataset that was set up to collect transcribed speech for use with Free and Open Source Speech Recognition Engines (on Linux, Windows and Mac).",,,,,,
3454,VOXLINGUA107,Spoken language identification,Spoken language identification,"Spoken language identification, Language Identification",Text,English,Natural Language Processing,"language-identification-on-voxlingua107-1, spoken-language-identification-on",,,https://paperswithcode.com/dataset/voxlingua107,Language Identification Dataset,,,,,,
3455,VoxPopuli,Automatic Speech Recognition,Automatic Speech Recognition,"Automatic Speech Recognition, Speech Recognition, Automatic Speech Recognition (ASR)","Audio, Image, Text",English,Speech,"automatic-speech-recognition-on-voxpopuli-1, automatic-speech-recognition-on-voxpopuli",,https://github.com/facebookresearch/voxpopuli,https://paperswithcode.com/dataset/voxpopuli,VoxPopuli is a large-scale multilingual corpus providing 100K hours of unlabelled speech data in 23 languages. It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16 languages and their aligned oral interpretations into 5 other languages totaling 5.1K hours.,,,,,,
3456,VPCD,Audio-Visual Active Speaker Detection,Audio-Visual Active Speaker Detection,Audio-Visual Active Speaker Detection,"Audio, Image",,Audio,audio-visual-active-speaker-detection-on-vpcd,CC BY 4.0,https://www.robots.ox.ac.uk/~vgg/data/Video_Person_Clustering/,https://paperswithcode.com/dataset/vpcd,"VPCD contains multi-modal annotations (face, body and voice) for all primary and secondary characters from a range of diverse TV-shows and movies. It is used for evaluating multi-modal person-clustering. It contains body-tracks for each annotated character, face-tracks when visible, and voice-tracks when speaking, with their associated features.

It consists of more than 30,000 face and body tracks of 300+ characters, from over 23 hours of video.",,,,,,
3457,VQA-HAT,Visual Question Answering (VQA),Visual Question Answering (VQA),"Visual Question Answering (VQA), Probabilistic Deep Learning, Question Answering","Image, Text",English,Computer Vision,,,https://computing.ece.vt.edu/~abhshkdz/vqa-hat/,https://paperswithcode.com/dataset/vqa-hat,VQA-HAT (Human ATtention) is a dataset to evaluate the informative regions of an image depending on the question being asked about it. The dataset consists of human visual attention maps over the images in the original VQA dataset. It contains more than 60k attention maps.,,,,,,
3458,VQG,Text Generation,Text Generation,"Text Generation, Question Generation, Visual Question Answering (VQA)","Image, Text",English,Computer Vision,question-generation-on-visual-question,,https://www.microsoft.com/en-us/download/details.aspx?id=53670,https://paperswithcode.com/dataset/vqg,VQG is a collection of datasets for visual question generation. VQG questions were collected by crowdsourcing the task on Amazon Mechanical Turk (AMT). The authors provided details on the prompt and the specific instructions for all the crowdsourcing tasks in this paper in the supplementary material. The prompt was successful at capturing nonliteral questions. Images were taken from the MSCOCO dataset.,,What BERT Sees: Cross-Modal Transfer for Visual Question Generation,https://arxiv.org/pdf/2002.10832v3.pdf,,,
3459,VRD,Visual Relationship Detection,Visual Relationship Detection,"Visual Relationship Detection, Scene Graph Detection, Relationship Detection, Scene Graph Generation","Graph, Image, Text",English,Computer Vision,"relationship-detection-on-vrd, scene-graph-generation-on-vrd, visual-relationship-detection-on-vrd, visual-relationship-detection-on-vrd-phrase, scene-graph-detection-on-vrd, visual-relationship-detection-on-vrd-2, visual-relationship-detection-on-vrd-1",,https://cs.stanford.edu/people/ranjaykrishna/vrd/,https://paperswithcode.com/dataset/vrd,"The Visual Relationship Dataset (VRD) contains 4000 images for training and 1000 for testing annotated with visual relationships. Bounding boxes are annotated with a label containing 100 unary predicates. These labels refer to animals, vehicles, clothes and generic objects. Pairs of bounding boxes are annotated with a label containing 70 binary predicates. These labels refer to actions, prepositions, spatial relations, comparatives or preposition phrases. The dataset has 37993 instances of visual relationships and 6672 types of relationships. 1877 instances of relationships occur only in the test set and they are used to evaluate the zero-shot learning scenario.",,Compensating Supervision Incompleteness with Prior Knowledge in Semantic Image Interpretation,https://arxiv.org/abs/1910.00462,4000 images,,
3460,VRMocap__VR_Mocap_Dataset_for_Pose_Reconstruction,Motion Synthesis,Motion Synthesis,"Motion Synthesis, Pose Estimation, Pose Tracking, Pose Prediction, 6D Pose Estimation","3D, Image, Time Series, Video",,Computer Vision,,Creative Commons Attribution 4.0 International,https://zenodo.org/records/8427980,https://paperswithcode.com/dataset/vrmocap-vr-mocap-dataset-for-pose,"Data used for the paper SparsePoser: Real-time Full-body Motion Reconstruction from Sparse Data

It contains over 1GB of high-quality motion capture data recorded with an Xsens Awinda system while using a variety of VR applications in Meta Quest devices.",,,,,,
3461,VR_Mocap_Dataset_for_Pose_Orientation_Prediction,Pose Tracking,Pose Tracking,"Pose Tracking, Pose Estimation, Pose Prediction","3D, Image, Time Series, Video",,Computer Vision,,CC BY-NC-SA 4.0,https://zenodo.org/record/7048601#.YzHUWXZBxhE,https://paperswithcode.com/dataset/vr-mocap,"Data used for the paper Combining Motion Matching and Orientation Prediction to Animate Avatars for Consumer-Grade VR Devices.

MMData.zip contains the necessary files to execute the project in Unity. Use only following the instructions on the GitHub project.

MMVR_Dataset.zip contains all .bvh files used for training the orientation prediction network. All files are captured with an Xsens Awinda motion capture system while using Virtual Reality. Visit GitHub for more information.",,,,,,
3462,VSPW,Video Semantic Segmentation,Video Semantic Segmentation,Video Semantic Segmentation,"Image, Video",,Computer Vision,video-semantic-segmentation-on-vspw,,https://www.vspwdataset.com/,https://paperswithcode.com/dataset/vspw,A Large-scale Dataset for Video Scene Parsing in the Wild,,,,,,
3463,VSR,Visual Entailment,Visual Entailment,"Visual Entailment, Visual Reasoning",Image,,Computer Vision,visual-reasoning-on-vsr,Apache-2.0,https://github.com/cambridgeltl/visual-spatial-reasoning,https://paperswithcode.com/dataset/vsr,"The Visual Spatial Reasoning (VSR) corpus is a collection of caption-image pairs with true/false labels. Each caption describes the spatial relation of two individual objects in the image, and a vision-language model (VLM) needs to judge whether the caption is correctly describing the image (True) or not (False).",,,,,,
3464,VSTaR-1M,visual instruction following,visual instruction following,visual instruction following,Image,,Computer Vision,,,https://huggingface.co/datasets/orrzohar/Video-STaR,https://paperswithcode.com/dataset/vstar-1m,"VSTaR-1M is a 1M instruction tuning dataset, created using Video-STaR, with the source datasets: 
* Kinetics700
* STAR-benchmark
* FineDiving

The videos for VSTaR-1M can be found in the links above. 

VSTaR-1M is built off of diverse task with the goal of enhancing video-language alignment in Large Video-Language Models (LVLMs).


kinetics700_tune_.json - Instruction tuning QA pairs for the Kinetics700 source dataset. Good for increasing diversity and for more fine-grained activity recognition.  
starb_tune_.json - Instruction tuning QA pairs for the STAR-benchmark source dataset. Good for temporal reasoning.
finediving_tune_.json - Instruction tuning QA pairs for the FineDiving source dataset. Example of adapting LVLMs for novel tasks (Olympic diving judge).",,,,,,
3465,VT5000,Object Detection,Object Detection,"Object Detection, Salient Object Detection, RGB Salient Object Detection",Image,,Computer Vision,,,https://github.com/lz118/RGBT-Salient-Object-Detection,https://paperswithcode.com/dataset/vt5000,Includes 5000 spatially aligned RGBT image pairs with ground truth annotations. VT5000 has 11 challenges collected in different scenes and environments for exploring the robustness of algorithms.,,,,,,
3466,Vulnerability_Java_Dataset,Vulnerability Detection,Vulnerability Detection,Vulnerability Detection,Image,,Computer Vision,vulnerability-detection-on-vulnerability-java,,https://github.com/rmusab/vul-llm-finetune/tree/main/Datasets,https://paperswithcode.com/dataset/vulnerability-java-dataset,"The dataset consists of two versions: $X_1$ with $P_3$ and $X_1$ without $P_3$, where $P_3$ represents a set of random unchanged functions from vulnerability fixing commits. This dataset is designed for finetuning large language models to detect vulnerabilities in code. It can be used for training and evaluating models in automated vulnerability detection tasks.",,,,,,
3467,Vulnerable_Verified_Smart_Contracts,Classification,Classification,"Classification, Vulnerability Detection",Image,,Computer Vision,,,https://doi.org/10.6084/m9.figshare.21990287,https://paperswithcode.com/dataset/vulnerable-verified-smart-contracts,"Vulnerable Verified Smart Contracts is a dataset of real vulnerable Ethereum smart contracts. Based on the manually labeled Benchmark dataset of Solidity smart contracts. A total of 609 vulnerable contracts are provided, containing 1,117 vulnerabilities.",,,,,,
3468,VulScribeR,Vulnerability Detection,Vulnerability Detection,Vulnerability Detection,Image,,Computer Vision,vulnerability-detection-on-vulscriber,,,https://paperswithcode.com/dataset/vulscriber,Datasets are listed in the repository's readme file. This one is extra and yields 20K+ items after filtering with a fuzzy parser.,,,,,,
3469,Wallhack1.8k,Human Activity Recognition,Human Activity Recognition,Human Activity Recognition,"Image, Video",,Computer Vision,,,https://zenodo.org/records/13950918,https://paperswithcode.com/dataset/wallhack1-8k,"The Wallhack1.8k dataset comprises 1,806 CSI amplitude spectrograms (and raw WiFi packet time series) corresponding to three activity classes: ""no presence,"" ""walking,"" and ""walking + arm-waving."" WiFi packets were transmitted at a frequency of 100 Hz, and each spectrogram captures a temporal context of approximately 4 seconds (400 WiFi packets).

To assess cross-scenario and cross-system generalization, WiFi packet sequences were collected in LoS and through-wall (NLoS) scenarios, utilizing two different WiFi systems (BQ: biquad antenna and PIFA: printed inverted-F antenna). The dataset is structured accordingly:

LOS/BQ/ <- WiFi packets collected in the LoS scenario using the BQ system
LOS/PIFA/ <- WiFi packets collected in the LoS scenario using the PIFA system
NLOS/BQ/ <- WiFi packets collected in the NLoS scenario using the BQ system
NLOS/PIFA/ <- WiFi packets collected in the NLoS scenario using the PIFA system

These directories contain the raw WiFi packet time series (see Table 1). Each row represents a single WiFi packet with the complex CSI vector H being stored in the ""data"" field and the class label being stored in the ""class"" field. H is of the form [I, R, I, R, ..., I, R], where two consecutive entries represent imaginary and real parts of complex numbers (the Channel Frequency Responses of subcarriers). Taking the absolute value of H (e.g., via numpy.abs(H)) yields the subcarrier amplitudes A.",,,,,,
3470,WANDS,Information Retrieval,Information Retrieval,"Information Retrieval, Product Recommendation",,,Methodology,,MIT,https://github.com/wayfair/WANDS,https://paperswithcode.com/dataset/wands,"The dataset contains:


42,994 candidate products with data comprising product class, title, description, attributes, category hierarchy, average rating, and number of reviews
480 search query strings with predicted product class
233,448 (query string, product) human relevance judgments with labels (exact match, partial match, irrelevant)

The purpose of the dataset is to evaluate retrieval models for product search in the e-commerce domain using expert judgment of whether a product is relevant to a given query. It can be used to benchmark different retrieval against each other. As of its publication in 2022, it was to the best of our knowledge the biggest such public dataset.

The accompanying publication describes in depth the annotation guidelines and process used to collect the dataset. It also includes a measure of the quality of the annotation and experimentally compares the dataset's ability to discriminate the effectiveness of different retrieval models vs other comparable evaluation datasets.",2022,,,,,
3471,Wang2016_MOABB,Within-Session SSVEP,Within-Session SSVEP,Within-Session SSVEP,,,Methodology,within-session-ssvep-on-wang2016-moabb,,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Wang2016.html,https://paperswithcode.com/dataset/wang2016-moabb,,,,,,,
3472,Warblr,Bird Audio Detection,Bird Audio Detection,Bird Audio Detection,"Audio, Image",,Audio,,,http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/,https://paperswithcode.com/dataset/warblr,"Warblr is a dataset for the acoustic detection of birds. The dataset comes from a UK bird-sound crowdsourcing research spinout called Warblr. From this initiative the authors collected over 10,000 ten-second smartphone audio recordings from around the UK. The audio totals around 28 hours duration.

The audio covers a wide distribution of UK locations and environments, and includes weather noise, traffic noise, human speech and even human bird imitations. It is directly representative of the data that is collected from a mobile crowdsourcing initiative. Annotations of the Warblr dataset are performed by a network of volunteers.",,,,,,
3473,warpPIE10P,Image/Document Clustering,Image/Document Clustering,Image/Document Clustering,"Image, Text",English,Computer Vision,image-document-clustering-on-warppie10p,,https://jundongl.github.io/scikit-feature/,https://paperswithcode.com/dataset/warppie10p,face dataset,,,,,,
3474,washed_contract,Code Search,Code Search,Code Search,,,Methodology,,,,https://paperswithcode.com/dataset/washed-contract,Dataset contains about 48K contracts which are open source on Etherscan.,,,,,,
3475,Washington_RGB-D,Object Detection,Object Detection,"Object Detection, Object Recognition, 3D Object Recognition","3D, Image",,Computer Vision,,,https://rgbd-dataset.cs.washington.edu/,https://paperswithcode.com/dataset/washington-rgb-d,"Washington RGB-D is a widely used testbed in the robotic community, consisting of 41,877 RGB-D images organized into 300 instances divided in 51 classes of common indoor objects (e.g. scissors, cereal box, keyboard etc). Each object instance was positioned on a turntable and captured from three different viewpoints while rotating.",,Learning Deep Visual Object Models From Noisy Web Data: How to Make it Work,https://arxiv.org/abs/1702.08513,300 instances,"testbed in the robotic community, consisting of 41,877 RGB-D images",51
3476,Watch-n-Patch,Action Segmentation,Action Segmentation,"Action Segmentation, Human-Object Interaction Detection, Action Recognition","Image, Video",,Computer Vision,,Custom,,https://paperswithcode.com/dataset/watch-n-patch,"The Watch-n-Patch dataset was created with the focus on modeling human activities, comprising multiple actions in a completely unsupervised setting. It is collected with Microsoft Kinect One sensor for a total length of about 230 minutes, divided in 458 videos. 7 subjects perform human daily activities in 8 offices and 5 kitchens with complex backgrounds. Moreover, skeleton data are provided as ground truth annotations.",,Head Detection with Depth Images in the Wild,https://arxiv.org/abs/1707.06786,,,
3477,WaterBench,LLM-generated Text Detection,LLM-generated Text Detection,LLM-generated Text Detection,"Image, Text",English,Computer Vision,,,https://github.com/THU-KEG/WaterBench,https://paperswithcode.com/dataset/waterbench,Multi-level  Benchmark of Watermarks for Large Language Models,,,,,,
3478,WaterScenes,Instance Segmentation,Instance Segmentation,"Instance Segmentation, Semantic Segmentation, Line Segment Detection, Panoptic Segmentation, Point Cloud Segmentation, Object Detection, 2D Semantic Segmentation","3D, Image",,Computer Vision,"2d-semantic-segmentation-on-waterscenes, object-detection-on-waterscenes",,https://waterscenes.github.io,https://paperswithcode.com/dataset/waterscenes,"A Multi-Task 4D Radar-Camera Fusion Dataset for Autonomous Driving on Water Surfaces description of the dataset 


WaterScenes, the first multi-task 4D radar-camera fusion dataset on water surfaces, which offers data from multiple sensors, including a 4D radar, monocular camera, GPS, and IMU. It can be applied in multiple tasks, such as object detection, instance segmentation, semantic segmentation, free-space segmentation, and waterline segmentation.
Our dataset covers diverse time conditions (daytime, nightfall, night), lighting conditions (normal, dim, strong), weather conditions (sunny, overcast, rainy, snowy) and waterway conditions (river, lake, canal, moat). An information list is also offered for retrieving specific data for experiments under different conditions.
We provide 2D box-level and pixel-level annotations for camera images, and 3D point-level annotations for radar point clouds. We also offer precise timestamps for the synchronization of different sensors, as well as intrinsic and extrinsic parameters.
We provide a toolkit for radar point clouds that includes: pre-processing, labeling, projection and visualization, assisting researchers in processing and analyzing our dataset.",,,,,,
3479,Water_Footprint_Recommender_System_Data,Food recommendation,Food recommendation,Food recommendation,,,Methodology,,,https://www.kaggle.com/datasets/turconiandrea/water-footprint-recommender-system-data,https://paperswithcode.com/dataset/water-footprint-recommender-system-data,"It contains data from two different realities: Food.com, a well-known American recipe site, and Planeat, an Italian site that allows you to plan recipes to save food waste. The dataset is divided into two parts: embeddings, which can be used directly to execute the work and receive suggestions, and raw data, which must first be processed into embeddings.",,,,,,
3480,WavCaps,Text to Audio Retrieval,Text to Audio Retrieval,"Text to Audio Retrieval, Audio captioning","Audio, Image, Text",English,Audio,,,,https://paperswithcode.com/dataset/wavcaps,A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research.,,,,,,
3481,WaveFake,DeepFake Detection,DeepFake Detection,DeepFake Detection,Image,,Computer Vision,,,https://github.com/rub-syssec/wavefake,https://paperswithcode.com/dataset/wavefake,WaveFake is a dataset for audio deepfake detection. The dataset consists of a large-scale dataset of over 100K generated audio clips.,,,,,,
3482,Waymo_Open_Dataset,3D Human Pose Estimation,3D Human Pose Estimation,"3D Human Pose Estimation, Video Object Detection, Multiple Object Tracking, 3D Object Detection, Object Detection, 3D Object Detection From Monocular Images, 3D Multi-Object Tracking, 3D Semantic Segmentation, Autonomous Driving","3D, Image, Video",,Computer Vision,"3d-object-detection-on-waymo-open-dataset, 3d-object-detection-on-waymo-vehicle, 3d-object-detection-on-waymo-cyclist, 3d-human-pose-estimation-on-waymo-open, 3d-object-detection-from-monocular-images-on-6, 3d-multi-object-tracking-on-waymo-open, object-detection-on-waymo-2d-detection-all-ns, 3d-multi-object-tracking-on-waymo-open-1, 3d-object-detection-on-waymo-pedestrian, object-detection-on-waymo-2d-detection-all-ns-1, multiple-object-tracking-on-waymo-open, video-object-detection-on-waymo-open-dataset, 3d-object-detection-on-waymo-all-ns, 3d-semantic-segmentation-on-waymo-open, object-detection-on-waymo-open-dataset",Custom (non-commercial),https://waymo.com/open,https://paperswithcode.com/dataset/waymo-open-dataset,"The Waymo Open Dataset is comprised of high resolution sensor data collected by autonomous vehicles operated by the Waymo Driver in a wide variety of conditions. 

The Waymo Open Dataset currently contains 1,950 segments. The authors plan to grow this dataset in the future. Currently the datasets includes:


1,950 segments of 20s each, collected at 10Hz (390,000 frames) in diverse geographies and conditions
Sensor data
1 mid-range lidar
4 short-range lidars
5 cameras (front and sides)
Synchronized lidar and camera data
Lidar to camera projections
Sensor calibrations and vehicle poses


Labeled data
Labels for 4 object classes - Vehicles, Pedestrians, Cyclists, Signs
High-quality labels for lidar data in 1,200 segments
12.6M 3D bounding box labels with tracking IDs on lidar data
High-quality labels for camera data in 1,000 segments
11.8M 2D bounding box labels with tracking IDs on camera data",,,,,,
3483,Waymo_Open_Motion_Dataset,Motion Forecasting,Motion Forecasting,"Motion Forecasting, Trajectory Prediction, motion prediction","Time Series, Video",,Methodology,,,https://waymo.com/open,https://paperswithcode.com/dataset/waymo-open-motion-dataset,"As autonomous driving systems mature, motion forecasting has received increasing attention as a critical requirement for planning. Of particular importance are interactive situations such as merges, unprotected turns, etc., where predicting individual object motion is not sufficient. Joint predictions of multiple objects are required for effective route planning. There has been a critical need for high-quality motion data that is rich in both interactions and annotation to develop motion planning models. In this work, we introduce the most diverse interactive motion dataset to our knowledge, and provide specific labels for interacting objects suitable for developing joint prediction models. With over 100,000 scenes, each 20 seconds long at 10 Hz, our new dataset contains more than 570 hours of unique data over 1750 km of roadways. It was collected by mining for interesting interactions between vehicles, pedestrians, and cyclists across six cities within the United States. We use a high-accuracy 3D auto-labeling system to generate high quality 3D bounding boxes for each road agent, and provide corresponding high definition 3D maps for each scene. Furthermore, we introduce a new set of metrics that provides a comprehensive evaluation of both single agent and joint agent interaction motion forecasting models. Finally, we provide strong baseline models for individual-agent prediction and joint-prediction. We hope that this new large-scale interactive motion dataset will provide new opportunities for advancing motion forecasting models.",,,,,,
3484,WBM,Band Gap,Band Gap,"Band Gap, Structured Prediction, Formation Energy",Time Series,,Methodology,,,https://tddft.org/bmg/data.php,https://paperswithcode.com/dataset/wbm,"We propose an efficient high-throughput scheme for the discovery of stable crystalline phases. Our approach is based on the transmutation of known compounds, through the substitution of atoms in the crystal structure with chemically similar ones. The concept of similarity is defined quantitatively using a measure of chemical replaceability, extracted by data-mining experimental databases. In this way we build 189,981 possible crystal phases, including 18,479 that are on the convex hull of stability. The resulting success rate of 9.72% is at least one order of magnitude better than the usual success rate of systematic high-throughput calculations for a specific family of materials, and comparable with speed-up factors of machine learning filtering procedures. As a characterization of the set of 18,479 stable compounds, we calculate their electronic band gaps, magnetic moments, and hardness. Our approach, that can be used as a filter on top of any high-throughput scheme, enables us to efficiently extract stable compounds from tremendously large initial sets, without any initial assumption on their crystal structures or chemical compositions.",,,,,,
3485,WCEP,Multi-Document Summarization,Multi-Document Summarization,"Multi-Document Summarization, Document Summarization",Text,English,Natural Language Processing,multi-document-summarization-on-wcep,,https://github.com/complementizer/wcep-mds-dataset,https://paperswithcode.com/dataset/wcep,"The WCEP dataset for multi-document summarization (MDS) consists of short, human-written summaries about news events, obtained from the Wikipedia Current Events Portal (WCEP), each paired with a cluster of news articles associated with an event. These articles consist of sources cited by editors on WCEP, and are extended with articles automatically obtained from the Common Crawl News dataset.",,,,,,
3486,WDC-Dialogue,Dialogue Understanding,Dialogue Understanding,"Dialogue Understanding, Dialogue Generation",Text,English,Natural Language Processing,,,https://github.com/thu-coai/EVA,https://paperswithcode.com/dataset/wdc-dialogue,"WDC-Dialogue is a dataset built from the Chinese social media to train EVA. Specifically, conversations from various sources are gathered and a rigorous data cleaning pipeline is designed to enforce the quality of WDC-Dialogue. 

The dataset mainly focuses on three categories of textual interaction data, i.e., repost on social media, comment / reply on various online forums and online question and answer (Q&A) exchanges. Each round of these textual interactions yields a dialogue session via well-designed parsing rules.",,,,,,
3487,WDC-PAVE,Attribute Value Extraction,Attribute Value Extraction,Attribute Value Extraction,,,Methodology,attribute-value-extraction-on-wdc-pave,,http://webdatacommons.org/structureddata/wdc-pave/,https://paperswithcode.com/dataset/wdc-pave,"The datasets contains 1,420 human annotated product offers, systematically selected from the Web Data Commons Product Matching Corpus, featuring 24,582 annotated attribute-value pairs, making it a valuable resource for both product attribute-value extraction and product matching tasks. 
The normalized gold standard contains the standardized attribute value pairs as described below.",,,,,,
3488,WDC_Block,Blocking,Blocking,"Blocking, Data Integration",,,Methodology,"blocking-on-wdc-block-small, blocking-on-wdc-block-medium, blocking-on-wdc-block-large",,https://webdatacommons.org/largescaleproductcorpus/wdc-block/,https://paperswithcode.com/dataset/wdc-block,"WDC Block is a benchmark for comparing the performance of blocking methods that are used as part of entity resolution pipelines.

Entity resolution aims to identify records in two datasets (A and B) that describe the same real-world entity. Since comparing all record pairs between two datasets can be computationally expensive, entity resolution is approached in two steps, blocking and matching. Blocking applies a computationally cheap method to remove non-matching record pairs and produces a smaller set of candidate record pairs reducing the workload of the matcher. During matching a more expensive pair-wise matcher produces a final set of matching record pairs. 

Existing benchmark datasets for blocking and matching are rather small with respect to the Cartesian product AxB for comparing all records and the vocabulary size. If blockers are evaluated only on these small datasets, effects resulting from a high number of records or from a large vocabulary size (large number of unique tokens that need to be indexed) may be missed. The Web Data Commons Block (WDC-Block) is a new blocking benchmark that provides much larger datasets and thus requires blockers that address these scalability challenges. WDC Block features a maximal Cartesian product of 200 billion pairs of product offers which were extracted form 3,259 e-shops.  Additionally, we provide three development sets with different sizes (~1K pairs, ~5K pairs & ~20K pairs) to experiment with different amounts of training data for the blockers.",,,,,,
3489,WDC_LSPM,Entity Resolution,Entity Resolution,Entity Resolution,,,Methodology,"entity-resolution-on-wdc-computers-small, entity-resolution-on-wdc-computers-xlarge, entity-resolution-on-wdc-watches-small, entity-resolution-on-wdc-watches-xlarge",,http://webdatacommons.org/largescaleproductcorpus/v2/,https://paperswithcode.com/dataset/wdc-products,"Many e-shops have started to mark-up product data within their HTML pages using the schema.org vocabulary. The Web Data Commons project regularly extracts such data from the Common Crawl, a large public web crawl. The Web Data Commons Training and Test Sets for Large-Scale Product Matching contain product offers from different e-shops in the form of binary product pairs (with corresponding label ""match"" or ""no match"") for four product categories, computers, cameras, watches and shoes. 

In order to support the evaluation of machine learning-based matching methods, the data is split into training, validation and test sets. For each product category, we provide training sets in four different sizes (2.000-70.000 pairs). Furthermore there are sets of ids for each training set for a possible validation split (stratified random draw) available. The test set for each product category consists of 1.100 product pairs. The labels of the test sets were manually checked while those of the training sets were derived using shared product identifiers from the Web via weak supervision. 

The data stems from the WDC Product Data Corpus for Large-Scale Product Matching - Version 2.0 which consists of 26 million product offers originating from 79 thousand websites.",,,,,,
3490,WDC_Products,Entity Resolution,Entity Resolution,"Entity Resolution, Data Integration",,,Methodology,"entity-resolution-on-wdc-products-80-cc-seen-1, entity-resolution-on-wdc-products, entity-resolution-on-wdc-products-50-cc, entity-resolution-on-wdc-products-80-cc-seen",BSD-3-Clause license,http://webdatacommons.org/largescaleproductcorpus/wdc-products/,https://paperswithcode.com/dataset/wdc-products-1,"WDC Products is an entity matching benchmark which provides for the systematic evaluation of matching systems along combinations of three dimensions while relying on real-word data. The three dimensions are 

i) amount of corner-cases 

ii) generalization to unseen entities, and 

iii) development set size

WDC Products contains 11715 product offers describing in total 2162 product entities belonging to various product categories.",,WDC Products: A Multi-Dimensional Entity Matching Benchmark,https://arxiv.org/pdf/2301.09521v1.pdf,,,
3491,WDC_SOTAB,Columns Property Annotation,Columns Property Annotation,"Columns Property Annotation, Table annotation, Column Type Annotation, Data Integration",Tabular,,Methodology,"columns-property-annotation-on-wdc-sotab, column-type-annotation-on-wdc-sotab",,http://webdatacommons.org/structureddata/sotab/,https://paperswithcode.com/dataset/wdc-sotab,"WDC SOTAB is a benchmark that features two annotation tasks: Column Type Annotation and Columns Property Annotation. The goal of the Column Type Annotation (CTA) task is to annotate the columns of a table with 91 Schema.org types, such as telephone, duration, Place, or Organization. The goal of the Columns Property Annotation (CPA) task is to annotate pairs of table columns with one out of 176 Schema.org properties, such as gtin13, startDate, priceValidUntil, or recipeIngredient. The benchmark consists of 59,548 tables annotated for CTA and 48,379 tables annotated for CPA originating from 74,215 different websites. The tables are split into training-, validation- and test sets for both tasks. The tables cover 17 popular Schema.org types including Product, LocalBusiness, Event, and JobPosting. The tables originate from the Schema.org Table Corpus.

Some characteristics for the different tasks are provided in the table below, where ""Columns"" refers to the number of columns/column pairs labeled and ""Classes"" to the number of unique classes used for annotation.

|                | Columns| Classes |
|----------------|---------|---------|
| Column Property Annotation  |  174,998  |  176 |
| Column Type Annotation     | 162,351 | 91 |",,,,,,
3492,WDC_SOTAB_V2,Columns Property Annotation,Columns Property Annotation,"Columns Property Annotation, Table annotation, Column Type Annotation, Data Integration",Tabular,,Methodology,"columns-property-annotation-on-wdc-sotab-v2, column-type-annotation-on-wdc-sotab-v2",,https://webdatacommons.org/structureddata/sotab/v2/,https://paperswithcode.com/dataset/wdc-sotab-v2,"SOTAB V2 features two annotation tasks: Column Type Annotation (CTA) and Columns Property Annotation (CPA). The goal of the Column Type Annotation (CTA) task is to annotate the columns of a table using 82 types from the Schema.org vocabulary, such as telephone, Duration, Mass, or Organization. The goal of the Columns Property Annotation (CPA) task is to annotate pairs of table columns with one out of 108 Schema.org properties, such as gtin, startDate, priceValidUntil, or recipeIngredient. The benchmark consists of 45,834 tables annotated for CTA and 30,220 tables annotated for CPA originating from 55,511 different websites. The tables are split into training-, validation- and test sets for both tasks. The tables cover 17 popular Schema.org types including Product, LocalBusiness, Event, and JobPosting.

Some characteristics for the different tasks are provided in the table below, where ""Columns"" refers to the number of columns/column pairs labeled and ""Classes"" to the number of unique classes used for annotation.

|     | Train  |         | Validation |         | Test   |         | Classes |
|-----|--------|---------|------------|---------|--------|---------|---------|
|     | Tables | Columns | Tables     | Columns | Tables | Columns |         |
| CTA | 44,769 | 116,887 | 456        | 1,769   | 609    | 1,851   | 82      |
| CPA | 29,158 | 109,994 | 497        | 2,459   | 565    | 2,340   | 108     |",,,,,,
3493,WEAR,Action Detection,Action Detection,"Action Detection, Temporal Action Localization, Human Activity Recognition","Image, Time Series, Video",,Computer Vision,,CC BY-NC-SA 4.0,https://mariusbock.github.io/wear/,https://paperswithcode.com/dataset/wear,"WEAR is an outdoor sports dataset for both vision- and inertial-based human activity recognition (HAR). The dataset comprises data from 22 participants performing a total of 18 different workout activities with untrimmed inertial (acceleration) and camera (egocentric video) data recorded at 11 different outside locations. Unlike previous egocentric datasets, WEAR provides a challenging prediction scenario marked by purposely introduced activity variations as well as an overall small information overlap across modalities.",,,,,,
3494,Weather,GLinear,GLinear,"GLinear, Time Series, Time Series Forecasting, Multivariate Time Series Forecasting",Time Series,,Methodology,"time-series-forecasting-on-weather-192, time-series-on-weather-720, time-series-forecasting-on-weather-720, glinear-on-weather-720, glinear-on-weather, multivariate-time-series-forecasting-on-46, glinear-on-weather-192, time-series-forecasting-on-weather-336, time-series-forecasting-on-weather-96, time-series-forecasting-on-weather-1",,https://www.bgc-jena.mpg.de/wetter/,https://paperswithcode.com/dataset/weather-ltsf,"Weather is recorded every 10 minutes for the 2020 whole year, which contains 21 meteorological indicators, such as air temperature, humidity, etc. The dataset in CSV format can be downloaded at https://drive.google.com/file/d/1Tc7GeVN7DLEl-RAs-JVwG9yFMf--S8dy/view?usp=share_link.",2020,,,,,
3495,Weather2K,Time Series Forecasting,Time Series Forecasting,"Time Series Forecasting, Spatio-Temporal Forecasting, Weather Forecasting","Time Series, Video",,Time Series,"time-series-forecasting-on-weather2k1786-192, time-series-forecasting-on-weather2k79-192, time-series-forecasting-on-weather2k850-720, time-series-forecasting-on-weather2k79-336, time-series-forecasting-on-weather2k79-720, time-series-forecasting-on-weather2k1786-336, time-series-forecasting-on-weather2k114-96, time-series-forecasting-on-weather2k114-720, time-series-forecasting-on-weather2k850-96, time-series-forecasting-on-weather2k850-192, time-series-forecasting-on-weather2k1786-96, time-series-forecasting-on-weather2k79-96, time-series-forecasting-on-weather2k114-192, time-series-forecasting-on-weather2k850-336, time-series-forecasting-on-weather2k114-336, time-series-forecasting-on-weather2k1786-720",,https://github.com/bycnfz/weather2k/,https://paperswithcode.com/dataset/weather2k,A multivariate spatio-temporal benchmark dataset for meteorological forecasting based on real-time observation data from ground weather stations.,,,,,,
3496,WeatherBench,Weather Forecasting,Weather Forecasting,Weather Forecasting,Time Series,,Methodology,,,https://github.com/pangeo-data/WeatherBench,https://paperswithcode.com/dataset/weatherbench,"A benchmark dataset for data-driven medium-range weather forecasting, a topic of high scientific interest for atmospheric and computer scientists alike.",,,,,,
3497,WEATHub,Bias Detection,Bias Detection,Bias Detection,Image,,Computer Vision,,CC-BY-4.0,https://huggingface.co/datasets/iamshnoo/WEATHub,https://paperswithcode.com/dataset/weathub,"WEATHub is a dataset containing 24 languages. It contains words organized into groups of (target1, target2, attribute1, attribute2) to measure the association target1:target2 :: attribute1:attribute2. For example target1 can be insects, target2 can be flowers. And we might be trying to measure whether we find insects or flowers pleasant or unpleasant. The measurement of word associations is quantified using the WEAT metric in our paper. It is a metric that calculates an effect size (Cohen's d) and also provides a p-value (to measure statistical significance of the results). In our paper, we use word embeddings from language models to perform these tests and understand biased associations in language models across different languages.",,,,,,
3498,WebKB,Imputation,Imputation,"Imputation, Node Clustering, Bayesian Inference, Node Classification, Relational Reasoning",Image,,Computer Vision,"node-classification-on-cornell, node-classification-on-texas, node-clustering-on-cornell, node-clustering-on-texas, node-classification-on-wisconsin, node-clustering-on-wisconsin",,http://www.cs.cmu.edu/~webkb/,https://paperswithcode.com/dataset/webkb,"WebKB is a dataset that includes web pages from computer science departments of various universities. 4,518 web pages are categorized into 6 imbalanced categories (Student, Faculty, Staff, Department, Course, Project). Additionally there is Other miscellanea category that is not comparable to the rest.",,Using Fuzzy Logic to Leverage HTML Markup for Web Page Representation,https://arxiv.org/abs/1606.04429,,,
3499,WebLINX,Text Generation,Text Generation,"Text Generation, Conversational Web Navigation, Vision and Language Navigation",Text,English,Natural Language Processing,conversational-web-navigation-on-weblinx,,https://mcgill-nlp.github.io/weblinx/,https://paperswithcode.com/dataset/weblinx,WebLINX is a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. It covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios.,,,,,,
3500,WebNLG,Data-to-Text Generation,Data-to-Text Generation,"Data-to-Text Generation, Joint Entity and Relation Extraction, Graph-to-Sequence, Unsupervised semantic parsing, Relation Extraction, KG-to-Text Generation, Unsupervised KG-to-Text Generation, Table-to-Text Generation","Graph, Tabular, Text, Time Series",English,Natural Language Processing,"unsupervised-semantic-parsing-on-webnlg-v2-1, kg-to-text-generation-on-webnlg-2-0, kg-to-text-generation-on-webnlg-2-0-1, graph-to-sequence-on-webnlg, kg-to-text-generation-on-webnlg-seen, data-to-text-generation-on-webnlg-en, joint-entity-and-relation-extraction-on-1, table-to-text-generation-on-webnlg-seen, data-to-text-generation-on-webnlg-full-1, unsupervised-kg-to-text-generation-on-webnlg, kg-to-text-generation-on-webnlg-all, relation-extraction-on-webnlg, table-to-text-generation-on-webnlg-unseen, data-to-text-generation-on-webnlg, kg-to-text-generation-on-webnlg-unseen, table-to-text-generation-on-webnlg-all, joint-entity-and-relation-extraction-on-8",CC BY-NC-SA 4.0,https://webnlg-challenge.loria.fr/,https://paperswithcode.com/dataset/webnlg,"The WebNLG corpus comprises of sets of triplets describing facts (entities and relations between them) and the corresponding facts in form of natural language text. The corpus contains sets with up to 7 triplets each along with one or more reference texts for each set. The test set is split into two parts: seen, containing inputs created for entities and relations belonging to DBpedia categories that were seen in the training data, and unseen, containing inputs extracted for entities and relations belonging to 5 unseen categories.

Initially, the dataset was used for the WebNLG natural language generation challenge which consists of mapping the sets of triplets to text, including referring expression generation, aggregation, lexicalization, surface realization, and sentence segmentation.
The corpus is also used for a reverse task of triplets extraction.

Versioning history of the dataset can be found here.

It's also available here: https://huggingface.co/datasets/web_nlg
Note: ""The v3 release (release_v3.0_en, release_v3.0_ru) for the WebNLG2020 challenge also supports a semantic parsing task.""",,Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation,https://arxiv.org/abs/1904.03396,,,
3501,WebQSP,Entity Linking,Entity Linking,"Entity Linking, Knowledge Base Question Answering",Text,English,Natural Language Processing,"knowledge-base-question-answering-on-webqsp-1, entity-linking-on-webqsp-wd, knowledge-base-question-answering-on-webqsp",,,https://paperswithcode.com/dataset/webqsp,,,,,,,
3502,WebQuestions,KG-to-Text Generation,KG-to-Text Generation,"KG-to-Text Generation, Open-Domain Question Answering, Question Answering, Knowledge Base Question Answering",Text,English,Natural Language Processing,"question-answering-on-webquestions, kg-to-text-generation-on-webquestions, knowledge-base-question-answering-on-3, open-domain-question-answering-on",,https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a,https://paperswithcode.com/dataset/webquestions,"The WebQuestions dataset is a question answering dataset using Freebase as the knowledge base and contains 6,642 question-answer pairs. It was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. The original split uses 3,778 examples for training and 2,032 for testing. All answers are defined as Freebase entities.

Example questions (answers) in the dataset include “Where did Edgar Allan Poe died?” (baltimore) or “What degrees did Barack Obama get?” (bachelor_of_arts, juris_doctor).",,Question Answering with Subgraph Embeddings,https://arxiv.org/abs/1406.3676,778 examples,"split uses 3,778 examples",
3503,WebQuestionsSP,Entity Linking,Entity Linking,"Entity Linking, Semantic Parsing, Knowledge Base Question Answering, Question Answering",Text,English,Natural Language Processing,"knowledge-base-question-answering-on-webqsp, semantic-parsing-on-webquestionssp, entity-linking-on-webqsp-wd, knowledge-base-question-answering-on-1, question-answering-on-webquestionssp",,https://www.microsoft.com/en-us/download/details.aspx?id=52763,https://paperswithcode.com/dataset/webquestionssp,"The WebQuestionsSP dataset is released as part of our ACL-2016 paper “The Value of Semantic Parse Labeling for Knowledge Base Question Answering” [Yih, Richardson, Meek, Chang & Suh, 2016], in which we evaluated the value of gathering semantic parses, vs. answers, for a set of questions that originally comes from WebQuestions [Berant et al., 2013]. The WebQuestionsSP dataset contains full semantic parses in SPARQL queries for 4,737 questions, and “partial” annotations for the remaining 1,073 questions for which a valid parse could not be formulated or where the question itself is bad or needs a descriptive answer. This release also includes an evaluation script and the output of the STAGG semantic parsing system when trained using the full semantic parses. More detail can be found in the document and labeling instructions included in this release, as well as the paper.",2016,,,,,
3504,WebVid-CoVR,Composed Image Retrieval (CoIR),Composed Image Retrieval (CoIR),"Composed Image Retrieval (CoIR), Composed Video Retrieval (CoVR), Zero-Shot Composed Image Retrieval (ZS-CIR)","Image, Video",,Computer Vision,composed-video-retrieval-covr-on-covr,,https://imagine.enpc.fr/~ventural/covr/,https://paperswithcode.com/dataset/webvid-covr,"The WebVid-CoVR dataset is a collection of video-text-video triplets that can be used for the task of composed video retrieval (CoVR). CoVR is a task that involves searching for videos that match both a query image and a query text. The text typically specifies the desired modification to the query image. 

The WebVid-CoVR dataset is automatically generated from web-scraped video-caption pairs, using a language model to generate the modification text. The dataset contains 1.6 million triplets, with diverse content and variations. The dataset also includes a manually annotated test set of 2.5K triplets, which can be used to evaluate CoVR models.",,,,,,
3505,WebVid,Video-Text Retrieval,Video-Text Retrieval,"Video-Text Retrieval, Video Captioning, Text-to-Video Generation, Video Generation, Video Retrieval","Image, Text, Video",English,Computer Vision,text-to-video-generation-on-webvid,Custom,https://github.com/m-bain/webvid,https://paperswithcode.com/dataset/webvid,"WebVid contains 10 million video clips with captions, sourced from the web. The videos are diverse and rich in their content.

Both the full 10M set and a 2.5M subset is available for download:
https://github.com/m-bain/webvid-dataset",,,,,,
3506,WebVision,Image Classification,Image Classification,"Image Classification, Learning with noisy labels",Image,,Computer Vision,"image-classification-on-webvision, image-classification-on-mini-webvision-1-0, image-classification-on-webvision-1000, learning-with-noisy-labels-on-mini-webvision",Custom,https://data.vision.ee.ethz.ch/cvl/webvision/dataset2017.html,https://paperswithcode.com/dataset/webvision-database,"The WebVision dataset is designed to facilitate the research on learning visual representation from noisy web data. It is a large scale web images dataset that contains more than 2.4 million of images crawled from the Flickr website and Google Images search. 

The same 1,000 concepts as the ILSVRC 2012 dataset are used for querying images, such that a bunch of existing approaches can be directly investigated and compared to the models trained from the ILSVRC 2012 dataset, and also makes it possible to study the dataset bias issue in the large scale scenario. The textual information accompanied with those images (e.g., caption, user tags, or description) are also provided as additional meta information. A validation set contains 50,000 images (50 images per category) is provided to facilitate the algorithmic development.",2012,,,000 images,"trained from the ILSVRC 2012 dataset, and also makes it possible to study the dataset bias issue in the large scale scenario. The textual information accompanied with those images",
3507,WEC-Eng,Cross Document Coreference Resolution,Cross Document Coreference Resolution,Cross Document Coreference Resolution,Text,English,Natural Language Processing,,,https://github.com/AlonEirew/extract-wec,https://paperswithcode.com/dataset/wec-eng,"WEC-eng is a cross-document event coreference resolution dataset extracted from English Wikipedia. Coreference links are not restricted within predefined topics. The training set includes 40,529 mentions distributed into 7,042 coreference clusters.",,,,,,
3508,Weibo-Douban,Anchor link prediction,Anchor link prediction,"Anchor link prediction, Record linking, Entity Alignment, Entity Resolution",Time Series,,Methodology,,,https://github.com/ChenBaiyang/MAUIL,https://paperswithcode.com/dataset/weibo-douban,"This dataset is used for user identity linkage across two online social networks in Chinese. It contains two popular Chinese social platforms: Sina Weibo\footnote{https://weibo.com} and Douban\footnote{https://www.douban.com}. 

Details:
* 9,714 users and 117,218 relations in Weibo; 9,526 users and 120,245 relations in Douban; 1,397 pair of matched users.
* Approximate power-law degree distribution and high aggregation coefficient.
* Multiple text attributes available, including username, geographical location and recent (text) posts of the users.
* Construction time: April 2020.",2020,,,,,
3509,Weibo2014_MOABB,Within-Session Motor Imagery (right hand vs. feet),Within-Session Motor Imagery (right hand vs. feet),"Within-Session Motor Imagery (right hand vs. feet), Within-Session Motor Imagery (left hand vs. right hand), Within-Session Motor Imagery (all classes)",,,Methodology,"within-session-motor-imagery-left-hand-vs-8, within-session-motor-imagery-right-hand-vs-7, within-session-motor-imagery-all-classes-on-5",,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Weibo2014.html,https://paperswithcode.com/dataset/weibo2014-moabb,,,,,,,
3510,Weibo21,Fake News Detection,Fake News Detection,Fake News Detection,Image,,Computer Vision,,,https://github.com/kennqiang/MDFEND-Weibo21,https://paperswithcode.com/dataset/weibo21,"Weibo21 is a benchmark of fake news dataset for multi-domain fake news detection (MFND) with domain label annotated, which consists of 4,488 fake news and 4,640 real news from 9 different domains.",,,,,,
3511,WeiboPolls,Question Generation,Question Generation,"Question Generation, Answer Generation, Poll Generation",Text,English,Natural Language Processing,"answer-generation-on-weibopolls, question-generation-on-weibopolls, poll-generation-on-weibopolls",,https://github.com/polyusmart/Poll-Question-Generation,https://paperswithcode.com/dataset/weibopolls,"Dataset Description
The dataset described in the provided text is focused on social media polls collected from Weibo, a popular Chinese microblogging platform. The dataset aims to empirically study social media polls and analyze user engagement patterns.

Characteristics of the Dataset

Size: The dataset consists of 20,252 polls collected from 1,860 users on Weibo.
Data Collection: The polls were obtained by sampling Weibo posts containing polls and examining the posting history of the authors. The dataset also includes comments on each post.
Sparsity: The dataset faces the challenge of sparse distribution of polls on Weibo, as less than 0.1% of the randomly gathered posts contained polls.
Content: The dataset includes user-generated polls with questions, answer choices, and corresponding votes. The polls often incorporate trendy hashtags to attract user attention and cover various topics, including social events, public emergencies (such as the COVID-19 outbreak), entertainment topics (celebrities, TV shows), and more.

Motivations and Summary
The motivation behind collecting this dataset is to explore social media polls on Weibo and analyze user engagement patterns. The study aims to understand how users interact with polls, the influence of polls on user engagement, and the types of topics that are more likely to contain polls.

The dataset provides insights into user behavior on Weibo by examining factors such as the length of posts, comments, questions, and answers. It also highlights the preference for voting over commenting as a means of expressing opinions. The dataset's analysis suggests that posts with polls tend to attract more comments, likes, and reposts compared to posts without polls.

Potential Use Cases
This dataset can be useful for various research and practical applications, such as:


Social Media Analysis: Researchers can analyze the characteristics and dynamics of social media polls, understanding how they influence user engagement and the types of topics that attract poll creation.
User Engagement Studies: The dataset allows for the exploration of user behavior and preferences when it comes to interacting with polls, providing insights into the factors that drive user engagement on social media platforms.
Trend Analysis: By examining the hashtags associated with polls, the dataset can contribute to understanding social events, public emergencies, and entertainment trends on Weibo.
Marketing and Advertising: The dataset can assist marketers and advertisers in understanding user preferences and interests, enabling them to create targeted campaigns based on the popular topics identified in the dataset.
Please note that the actual contents and specific applications of the dataset may vary based on further analysis and research conducted by users.",,,,,,
3512,Weibo_NER,Chinese Named Entity Recognition,Chinese Named Entity Recognition,"Chinese Named Entity Recognition, Fake News Detection","Image, Text",English,Computer Vision,"fake-news-detection-on-weibo-ner, chinese-named-entity-recognition-on-weibo-ner",,https://github.com/OYE93/Chinese-NLP-Corpus/tree/master/NER/Weibo,https://paperswithcode.com/dataset/weibo-ner,The Weibo NER dataset is a Chinese Named Entity Recognition dataset drawn from the social media website Sina Weibo.,,Chinese NER Using Lattice LSTM,https://arxiv.org/abs/1805.02023,,,
3513,Well-being_Dataset,Anxiety Detection,Anxiety Detection,"Anxiety Detection, Depression Detection",Image,,Computer Vision,"anxiety-detection-on-well-being-dataset, depression-detection-on-well-being-dataset",Private,,https://paperswithcode.com/dataset/well-being-dataset,The dataset is a private dataset collected for automatic analysis of psychological distress. It contains self-reported distress labels provided by human volunteers. The dataset consists of 30-min interview recordings of participants.,,,,,,
3514,Western_Mediterranean_Wetlands_Birds_-_Version_2,Bird Audio Detection,Bird Audio Detection,Bird Audio Detection,"Audio, Image",,Audio,,Creative Commons Attribution 4.0 International Public License,https://doi.org/10.5281/zenodo.5093173,https://paperswithcode.com/dataset/western-mediterranean-wetlands-bird-dataset,"The Western Mediterranean Wetlands Bird Dataset is a collection of birds' vocalizations of different lengths that primarily consists of 5,795 labelled audio clips derived from 1,098 recordings, totalling 201.6 minutes or 12,096 seconds alongside with corresponding annotations. It also comes with Mel spectrogram version of the data, where an image represents a 1-second window of the original audio, resulting in a total of 17,536 spectrographic images. These are stored in matrix form within .npy files. These are the species covered:


Acrocephalus arundinaceus
Acrocephalus melanopogon
Acrocephalus scirpaceus
Alcedo atthis
Anas strepera
Anas platyrhynchos
Ardea purpurea
Botaurus stellaris
Charadrius alexandrinus
Ciconia ciconia
Circus aeruginosus
Coracias garrulus
Dendrocopos minor
Fulica atra
Gallinula chloropus
Himantopus himantopus
Ixobrychus minutus
Motacilla flava
Porphyrio porphyrio
Tachybaptus ruficollis

Can be considered a subset retrieved from the Xeno-Canto citizen science portal which is a database containing 716,298 recordings of bird sound at the time of writing this.",,,,,,
3515,WHAMR_,Audio Source Separation,Audio Source Separation,"Audio Source Separation, Speech Separation, Speech Dereverberation, Speech Enhancement",Audio,,Audio,"speech-enhancement-on-whamr, speech-dereverberation-on-whamr, speech-separation-on-whamr",,https://wham.whisper.ai/,https://paperswithcode.com/dataset/whamr,"WHAMR! is a dataset for noisy and reverberant speech separation. It extends WHAM! by introducing synthetic reverberation to the
speech sources in addition to the existing noise. Room impulse responses were generated and convolved using pyroomacoustics. Reverberation times were chosen to approximate domestic and classroom environments (expected to be similar to the restaurants and coffee shops where the WHAM! noise was collected), and
further classified as high, medium, and low reverberation based on a
qualitative assessment of the mixture’s noise recording.",,,,,,
3516,WHAMR_ext,Speech Denoising,Speech Denoising,"Speech Denoising, Speech Separation, Speech Dereverberation, Speech Extraction, Speech Enhancement",Audio,,Speech,speech-dereverberation-on-whamr-ext,Creative Commons Zero v1.0 Universal,https://github.com/jwr1995/WHAMR_ext,https://paperswithcode.com/dataset/whamr-ext,WHAMR_ext is an extension to the WHAMR corpus with larger RT60 values (between 1s and 3s),,,,,,
3517,WHAM_,Audio Source Separation,Audio Source Separation,"Audio Source Separation, Speech Separation, Speech Dereverberation, Speech Enhancement",Audio,,Audio,"speech-separation-on-wham, speech-enhancement-on-wham, speech-enhancement-on-whamr, speech-dereverberation-on-whamr, speech-separation-on-whamr",CC BY-NC 4.0,http://wham.whisper.ai/,https://paperswithcode.com/dataset/wham,"The WSJ0 Hipster Ambient Mixtures (WHAM!) dataset pairs each two-speaker mixture in the wsj0-2mix dataset with a unique noise background scene. It has an extension called WHAMR! that adds artificial reverberation to the speech signals in addition to the background noise.

The noise audio was collected at various urban locations throughout the San Francisco Bay Area in late 2018. The environments primarily consist of restaurants, cafes, bars, and parks. Audio was recorded using an Apogee Sennheiser binaural microphone on a tripod between 1.0 and 1.5 meters off the ground.",2018,,,,,
3518,WHO-COVID19_Dataset,COVID-19 Modelling,COVID-19 Modelling,COVID-19 Modelling,,,Methodology,covid-19-modelling-on-who,,https://covid19.who.int/,https://paperswithcode.com/dataset/who,COVID19 Data from the World Health Organization,,,,,,
3519,WHOI-Plankton,Fine-Grained Visual Recognition,Fine-Grained Visual Recognition,"Fine-Grained Visual Recognition, Gaussian Processes",Image,,Computer Vision,,,https://github.com/hsosik/WHOI-Plankton,https://paperswithcode.com/dataset/whoi-plankton,"WHOI-Plankton is a collection of annotated plankton images. It contains > 3.5 million images of microscopic marine plankton, organized according to category labels provided by researchers at the Woods Hole Oceanographic Institution (WHOI). The images are currently placed into one of 103 categories.",,,,,,103
3520,WHOOPS_,Image Captioning,Image Captioning,"Image Captioning, Visual Commonsense Tests, Common Sense Reasoning, Visual Commonsense Reasoning, Image-to-Text Retrieval, Image Generation, Explanation Generation, Visual Question Answering (VQA)","Image, Text",English,Computer Vision,"image-captioning-on-whoops, visual-question-answering-vqa-on-whoops, explanation-generation-on-whoops, image-to-text-retrieval-on-whoops",CC-By 4.0,https://whoops-benchmark.github.io/,https://paperswithcode.com/dataset/whoops,"WHOOPS! Is a dataset and benchmark for visual commonsense. The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney. It contains commonsense-defying image from a wide range of reasons, deviations from expected social norms and everyday knowledge.",,,,,,
3521,WHU-RS19,Remote Sensing Image Classification,Remote Sensing Image Classification,Remote Sensing Image Classification,Image,,Computer Vision,,,https://captain-whu.github.io/BED4RS/,https://paperswithcode.com/dataset/whu-rs19,"WHU-RS19 is a set of satellite images exported from Google Earth, which provides high-resolution satellite images up to 0.5 m. Some samples of the database are displayed in the following picture. It contains 19 classes of meaningful scenes in high-resolution satellite imagery, including airport, beach, bridge, commercial, desert, farmland, footballfield, forest, industrial, meadow, mountain, park, parking, pond, port, railwaystation, residential, river, and viaduct. For each class, there are about 50 samples. It’s worth noticing that the image samples of the same class are collected from different regions in satellite images of different resolutions and then might have different scales, orientations and illuminations.",,,,50 samples,,19
3522,WHU_-_Audio_ENF,ENF (Electric Network Frequency) Extraction,ENF (Electric Network Frequency) Extraction,"ENF (Electric Network Frequency) Extraction, ENF (Electric Network Frequency) Detection","Graph, Image",,Computer Vision,,,https://github.com/ghua-ac/ENF-WHU-Dataset/tree/master/ENF-WHU-Dataset,https://paperswithcode.com/dataset/whu-audio-enf,"The Whu dataset is an audio only dataset thought for testing ENF
detection. It is divided into two parts: one with recordings containing ENF traces
(H1) and one without them (H0). The recordings with ENF traces are coupled
with the corresponding ENF reference (H1_ref). The dataset consists of 60 real-world
audio recordings captured around Wuhan University campus, featuring a diverse
range of environments and conditions. The recordings were made at a sampling
rate of 44.1 kHz with 16-bit quantization and mono channel. Among the 60
recordings, 50 were found to have captured and verified ENF signals, which
were confirmed by comparing the recording times with a reference database. The
remaining 10 recordings were made in open exterior environments with strong
noise and interference, and were often affected by Doppler effects due to the user
walking while recording. The final dataset has 130 audio recordings in H1 and 40
in H0, obtained by randomly cropping them to durations varying from 5 to 270
seconds. In our experiments we don’t consider H0 bacause we needed recordings
coupled with ENF reference.",,,,,,
3523,WHU_Building_Dataset,Change Detection,Change Detection,"Change Detection, Extracting Buildings In Remote Sensing Images, Building change detection for remote sensing images",Image,,Computer Vision,"building-change-detection-for-remote-sensing-1, extracting-buildings-in-remote-sensing-images-2, change-detection-on-whu-building-dataset",,http://gpcv.whu.edu.cn/data/building_dataset.html,https://paperswithcode.com/dataset/whu-building-dataset,"We manually edited an aerial and a satellite imagery dataset of building samples and named it a WHU building dataset. The aerial dataset consists of more than 220, 000 independent buildings extracted from aerial images with 0.075 m spatial resolution and 450 km2 covering in Christchurch, New Zealand. The satellite imagery dataset consists of two subsets. One of them is collected from cities over the world and from various remote sensing resources including QuickBird, Worldview series, IKONOS, ZY-3, etc. The other satellite building sub-dataset consists of 6 neighboring satellite images covering 550 km2 on East Asia with 2.7 m ground resolution.",,,,,,
3524,WI-LOCNESS,Variational Inference,Variational Inference,"Variational Inference, Grammatical Error Correction",,,Methodology,"grammatical-error-correction-on-bea-2019-test, grammatical-error-correction-on-wi-locness-1",,https://www.cl.cam.ac.uk/research/nl/bea2019st/,https://paperswithcode.com/dataset/locness-corpus,"WI-LOCNESS is part of the Building Educational Applications 2019 Shared Task for Grammatical Error Correction. It consists of two datasets:


LOCNESS: is a corpus consisting of essays written by native English students. 
Cambridge English Write & Improve (W&I): Write & Improve (Yannakoudakis et al., 2018) is an online web platform that assists non-native English students with their writing. Specifically, students from around the world submit letters, stories, articles and essays in response to various prompts, and the W&I system provides instant feedback. Since W&I went live in 2014, W&I annotators have manually annotated some of these submissions and assigned them a CEFR level.",2019,,,,,
3525,WiC-TSV,Entity Linking,Entity Linking,"Entity Linking, Word Sense Disambiguation",,,Methodology,"word-sense-disambiguation-on-wic-tsv, entity-linking-on-wic-tsv",MIT,https://github.com/semantic-web-company/wic-tsv,https://paperswithcode.com/dataset/wic-tsv,"WiC-TSV is a new multi-domain evaluation benchmark for Word Sense Disambiguation. More specifically, it is a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as a binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the dataset highly flexible for the evaluation of a diverse set of models and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the model.",,,,,,
3526,WiC,Language Modelling,Language Modelling,"Language Modelling, Word Sense Disambiguation, Word Embeddings, Classification","Image, Text",English,Computer Vision,classification-on-wic,CC BY-NC 4.0,https://pilehvar.github.io/wic/,https://paperswithcode.com/dataset/wic,"WiC is a benchmark for the evaluation of context-sensitive word embeddings. WiC is framed as a binary classification task. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not. In fact, the dataset can also be viewed as an application of Word Sense Disambiguation in practise.",,,,,,
3527,WIDER,Face Detection,Face Detection,"Face Detection, Image Captioning, Blind Face Restoration, Multi-Task Learning","Image, Text",English,Computer Vision,blind-face-restoration-on-wider,Custom (research-only),http://yjxiong.me/event_recog/WIDER/,https://paperswithcode.com/dataset/wider,"WIDER is a dataset for complex event recognition from static images. As of v0.1, it contains 61 event categories and around 50574 images annotated with event class labels.",,,,50574 images,,
3528,WiFiCam,Image Reconstruction,Image Reconstruction,"Image Reconstruction, Image Generation","3D, Image, Text",English,Computer Vision,,,https://zenodo.org/records/11554280,https://paperswithcode.com/dataset/wificam,"WiFiCam dataset for through-wall imaging based on WiFi channel state information. The corresponding source code repository is located at: https://github.com/StrohmayerJ/wificam

Dataset Structure:

/wificam

├── j3

└── 320  <-- 320x240 resolution subset

└── csi.csv <-- raw WiFi packet sequence recorded with the ESP32-S3

└── csiComplex.npy <-- complex CSI sequence (cache)

└── 92108.png <-- 320x240 RGB image

└── 92112.png

└── ...

└── 640 <-- 640x480 resolution subset

└── csi.csv <-- raw WiFi packet sequence recorded with the ESP32-S3

└── csiComplex.npy <-- complex CSI sequence (cache)

└── 154.png <-- 640x480 RGB image

└── 155.png

└── ...

├── statistics320.csv <-- per-channel means and standard deviations for 320x240 images

├── statistics640.csv <-- per-channel means and standard deviations for 640x480 images",,,,240 images,,
3529,Wiki-40B,Language Modelling,Language Modelling,"Language Modelling, Quantization, Benchmarking",Text,English,Natural Language Processing,"benchmarking-on-wiki-40b, quantization-on-wiki-40b, language-modelling-on-wiki-40b",,https://research.google/pubs/pub49029/,https://paperswithcode.com/dataset/wiki-40b,A new multilingual language model benchmark that is composed of 40+ languages spanning several scripts and linguistic families containing round 40 billion characters and aimed to accelerate the research of multilingual modeling.,,,,,,
3530,Wiki-One,Knowledge Graph Completion,Knowledge Graph Completion,"Knowledge Graph Completion, Few-Shot Learning",Graph,,Methodology,,Apache-2.0,https://github.com/xwhan/One-shot-Relational-Learning,https://paperswithcode.com/dataset/wiki-one,"This dataset is a Wikipedia dump, split by relations to perform Few-Shot Knowledge Graph Completion. 

\begin{table}[]
\begin{tabular}{@{}lllccl@{}}
\textbf{Dataset}  & \textbf{# Ent}    & \textbf{# Rel} & \textbf{# Triplets}                 & \textbf{Train/Dev/Test}               \ 
Wiki-One & 4,838,244 & 822                              & 5,829,240                   & 133/16/34                   \
\end{tabular}
\caption{Datasets used in the experiments. }
\end{table}",,,,,,
3531,WikiANN,Token Classification,Token Classification,"Token Classification, UIE, Cross-Lingual NER, Word Embeddings, Named Entity Recognition (NER), Cross-Lingual Transfer","Image, Text",English,Computer Vision,"uie-on-wikiann, cross-lingual-ner-on-wikiann-ner, token-classification-on-wikiann",,https://tensorflow.google.cn/datasets/catalog/wikiann,https://paperswithcode.com/dataset/wikiann-1,"WikiANN, also known as PAN-X, is a multilingual named entity recognition dataset. It consists of Wikipedia articles that have been annotated with LOC (location), PER (person), and ORG (organization) tags in the IOB2 format¹². This dataset serves as a valuable resource for training and evaluating named entity recognition models across various languages.

For instance, it includes information about notable individuals, places, and organizations mentioned in Wikipedia articles. Researchers and practitioners can use WikiANN to develop and improve natural language processing systems that identify and classify named entities in text.

(1) wikiann · Datasets at Hugging Face. https://huggingface.co/datasets/wikiann.
(2) wikiann | TensorFlow Datasets. https://tensorflow.google.cn/datasets/catalog/wikiann.
(3) wikiann · Datasets at Hugging Face. https://huggingface.co/datasets/wikiann/viewer/en.
(4) WikiAnn Dataset | Papers With Code. https://paperswithcode.com/dataset/wikiann-1.",,,,,,
3532,WikiArt,Style Transfer,Style Transfer,"Style Transfer, Continual Learning",,,Methodology,"style-transfer-on-wikiart, continual-learning-on-wikiart-fine-grained-6",Custom (non-commercial),https://github.com/cs-chan/ArtGAN/blob/master/WikiArt%20Dataset/README.md,https://paperswithcode.com/dataset/wikiart,WikiArt contains painting from 195 different artists. The dataset has 42129 images for training and 10628 images for testing.,,Adding New Tasks to a Single Network with Weight Transformations using Binary Masks,https://arxiv.org/abs/1805.11119,42129 images,training and 10628 images,
3533,WikiChurches,imbalanced classification,imbalanced classification,"imbalanced classification, Fine-Grained Visual Recognition, Fine-Grained Image Classification, Image Classification with Label Noise, Small Data Image Classification, Fine-Grained Image Recognition","Audio, Image",,Computer Vision,,CC BY-SA,https://doi.org/10.5281/zenodo.5166986,https://paperswithcode.com/dataset/wikichurches,"WikiChurches is a dataset for architectural style classification, consisting of 9,485 images of church buildings. Both images and style labels were sourced from Wikipedia. The dataset can serve as a benchmark for various research fields, as it combines numerous real-world challenges: fine-grained distinctions between classes based on subtle visual features, a comparatively small sample size, a highly imbalanced class distribution, a high variance of viewpoints, and a hierarchical organization of labels, where only some images are labeled at the most precise level.

In addition, we provide 631 bounding box annotations of characteristic visual features for 139 churches from four major categories. These annotations can, for example, be useful for research on fine-grained classification, where additional expert knowledge about distinctive object parts is often available.",,,,485 images,,
3534,WikiConv,Question Answering,Question Answering,"Question Answering, Abuse Detection, Knowledge Graphs","Image, Text",English,Computer Vision,,,https://github.com/conversationai/wikidetox/tree/master/wikiconv,https://paperswithcode.com/dataset/wikiconv,"A corpus that encompasses the complete history of conversations between contributors to Wikipedia, one of the largest online collaborative communities. By recording the intermediate states of conversations---including not only comments and replies, but also their modifications, deletions and restorations---this data offers an unprecedented view of online conversation.",,,,,,
3535,Wikidata-Disamb,Entity Disambiguation,Entity Disambiguation,Entity Disambiguation,,,Methodology,,,https://github.com/ContextScout/ned-graphs,https://paperswithcode.com/dataset/wikidata-disamb,"The Wikidata-Disamb dataset is intended to allow a clean and scalable evaluation of NED with Wikidata entries, and to be used as a reference in future research.",,Named Entity Disambiguation using Deep Learning on Graphs,https://arxiv.org/pdf/1810.09164.pdf,,,
3536,Wikidata5M-SI,Inductive Link Prediction,Inductive Link Prediction,Inductive Link Prediction,Time Series,,Methodology,inductive-link-prediction-on-wikidata5m-si,,https://github.com/uma-pi1/wikidata5m-si,https://paperswithcode.com/dataset/wikidata5m-si,"Semi-inductive link prediction (LP) in knowledge graphs (KG) is the task of predicting facts for new, previously unseen entities based on context information. Although new entities can be integrated by retraining the model from scratch in principle, such an approach is infeasible for large-scale KGs, where retraining is expensive and new entities may arise frequently. In this paper, we propose and describe a large-scale benchmark to evaluate semi-inductive LP models. The benchmark is based on and extends Wikidata5M: It provides transductive, k-shot, and 0-shot LP tasks, each varying the available information from (i) only KG structure, to (ii) including textual mentions, and (iii) detailed descriptions of the entities. We report on a small study of recent approaches and found that semi-inductive LP performance is far from transductive performance on long-tail entities throughout all experiments. The benchmark provides a test bed for further research into integrating context and textual information in semi-inductive LP models.",,,,,,
3537,WikiDataSets,Relational Reasoning,Relational Reasoning,Relational Reasoning,,,Reasoning,,,https://graphs.telecom-paris.fr/Home_page.html#wikidatasets-section,https://paperswithcode.com/dataset/wikidatasets,"Topical subsets of WikiData, assembled using the WikiDataSets python library. Extracted from Wikidata in April 2020.",2020,,,,,
3538,WikiDetox,Toxic Comment Classification,Toxic Comment Classification,"Toxic Comment Classification, Aggression Identification",Image,,Computer Vision,,Apache-2.0,https://github.com/ewulczyn/wiki-detox,https://paperswithcode.com/dataset/wikidetox,"An annotated dataset of 1m crowd-sourced annotations that cover 100k talk page diffs (with 10 judgements per diff) for personal attacks, aggression, and toxicity.",,,,,,
3539,WikiEvents,Zero-shot Named Entity Recognition (NER),Zero-shot Named Entity Recognition (NER),"Zero-shot Named Entity Recognition (NER), Event Argument Extraction, Event Extraction","Image, Text",English,Computer Vision,"event-argument-extraction-on-wikievents, zero-shot-named-entity-recognition-ner-on-3",,https://github.com/raspberryice/gen-arg,https://paperswithcode.com/dataset/wikievents,WikiEvents is a document-level event extraction benchmark dataset which includes complete event and coreference annotation.,,,,,,
3540,WikiFactDiff,knowledge editing,knowledge editing,knowledge editing,,,Methodology,,CC BY-SA 4.0,https://huggingface.co/datasets/Orange/WikiFactDiff,https://paperswithcode.com/dataset/wikifactdiff,"WikiFactDiff is a dataset designed as a resource to perform atomic factual knowledge updates on language models, with the goal of aligning them with current knowledge. It describes the evolution of factual knowledge between two dates, named T_old and T_new,​ in the form of semantic triples. To enable the possibility of evaluating knowledge algorithms (such as ROME, MEND, MEMIT, etc.), these triples are verbalized and neighbor facts are determined to check for eventual bleedover.",,,,,,
3541,WikiGraphs,KG-to-Text Generation,KG-to-Text Generation,"KG-to-Text Generation, Graph Representation Learning, Conditional Text Generation, Graph Generation","Graph, Text",English,Natural Language Processing,kg-to-text-generation-on-wikigraphs,Apache-2.0,https://github.com/deepmind/deepmind-research/tree/master/wikigraphs,https://paperswithcode.com/dataset/wikigraphs,"WikiGraphs is a dataset of Wikipedia articles each paired with a knowledge graph, to facilitate the research in conditional text generation, graph generation and graph representation learning. Existing graph-text paired datasets typically contain small graphs and short text (1 or few sentences), thus limiting the capabilities of the models that can be learned on the data. 

WikiGraphs is collected by pairing each Wikipedia article from the established WikiText-103 benchmark with a subgraph from the Freebase knowledge graph. This makes it easy to benchmark against other state-of-the-art text generative models that are capable of generating long paragraphs of coherent text. Both the graphs and the text data are of significantly larger scale compared to prior graph-text paired datasets.",,,,,,
3542,WikiHop,Paraphrase Identification,Paraphrase Identification,"Paraphrase Identification, Question Answering",Text,English,Natural Language Processing,"paraphrase-identification-on-wikihop, question-answering-on-wikihop",CC BY-SA 3.0,http://qangaroo.cs.ucl.ac.uk/,https://paperswithcode.com/dataset/wikihop,"WikiHop is a multi-hop question-answering dataset. The query of WikiHop is constructed with entities and relations from WikiData, while supporting documents are from WikiReading. A bipartite graph connecting entities and documents is first built and the answer for each query is located by traversal on this graph. Candidates that are type-consistent with the answer and share the same relation in query with the answer are included, resulting in a set of candidates. Thus, WikiHop is a multi-choice style reading comprehension data set. There are totally about 43K samples in training set, 5K samples in development set and 2.5K samples in test set. The test set is not provided. The task is to predict the correct answer given a query and multiple supporting documents.

The dataset includes a masked variant, where all candidates and their mentions in the supporting documents are replaced by random but consistent placeholder tokens.",,Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs,https://arxiv.org/abs/1905.07374,43K samples,"training set, 5K samples",
3543,WikiHow,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Text Summarization",Text,English,Natural Language Processing,"text-summarization-on-wikihow, abstractive-text-summarization-on-wikihow",CC-BY-NC-SA,https://github.com/mahnazkoupaee/WikiHow-Dataset,https://paperswithcode.com/dataset/wikihow,"WikiHow is a dataset of more than 230,000 article and summary pairs extracted and constructed from an online knowledge base written by different human authors. The articles span a wide range of topics and represent high diversity styles.",,,,,,
3544,WikiLarge,Text Simplification,Text Simplification,"Text Simplification, Lexical Simplification",Text,English,Natural Language Processing,,,https://github.com/XingxingZhang/dress,https://paperswithcode.com/dataset/wikilarge,"WikiLarge comprise 359 test sentences, 2000 development sentences and 300k training sentences. Each source sentences in test set has 8 simplified references",2000,Semi-Supervised Text Simplification with Back-Translation and Asymmetric Denoising Autoencoders,https://arxiv.org/abs/2004.14693,,,
3545,WikiLingua,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Text Summarization, Cross-Lingual Abstractive Summarization, Document Summarization",Text,English,Natural Language Processing,"document-summarization-on-wikilingua-tr-en, cross-lingual-abstractive-summarization-on, cross-lingual-abstractive-summarization-on-1, cross-lingual-abstractive-summarization-on-2, cross-lingual-abstractive-summarization-on-3",,https://github.com/esdurmus/Wikilingua,https://paperswithcode.com/dataset/wikilingua,WikiLingua includes ~770k article and summary pairs in 18 languages from WikiHow. Gold-standard article-summary alignments across languages are extracted by aligning the images that are used to describe each how-to step in an article.,,Ladhak et al,https://arxiv.org/pdf/2010.03093v1.pdf,,,
3546,WikiMatrix,Word Embeddings,Word Embeddings,"Word Embeddings, Sentence Embeddings",,,Methodology,,CC BY-SA 4.0,https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix,https://paperswithcode.com/dataset/wikimatrix,"WikiMatrix is a dataset of parallel sentences in the textual content of Wikipedia for all possible language pairs. The mined data consists of:


85 different languages, 1620 language pairs
134M parallel sentences, out of which 34M are aligned with English",,,,,,
3547,WikiNEuRal,Token Classification,Token Classification,"Token Classification, Cross-Lingual NER, Multilingual Named Entity Recognition, Word Embeddings, Named Entity Recognition (NER), Cross-Lingual Transfer","Image, Text",English,Computer Vision,,CC BY-SA-NC 4.0,https://github.com/Babelscape/wikineural,https://paperswithcode.com/dataset/wikineural,WikiNEuRal is a high-quality automatically-generated dataset for Multilingual Named Entity Recognition.,,,,,,
3548,WikiNews_Dataset,Arabic Text Diacritization,Arabic Text Diacritization,Arabic Text Diacritization,Text,English,Natural Language Processing,,,https://github.com/kdarwish/Farasa,https://paperswithcode.com/dataset/wikinews-dataset,"The WikiNews Arabic Diacritization dataset is a test set composed of 70 WikiNews articles (majority are from 2013 and 2014) that cover a variety of themes, namely: politics, economics, health, science and technology, sports, arts, and culture.
The articles are evenly distributed among the different themes (10 per theme).
The articles contain 18,300 words with around 400 different sentences (Each line is considered as a sentence).",2013,,,,,
3549,WikiOFGraph,Data-to-Text Generation,Data-to-Text Generation,"Data-to-Text Generation, Graph-to-Sequence","Graph, Text, Time Series",English,Natural Language Processing,data-to-text-generation-on-wikiofgraph,llama3,,https://paperswithcode.com/dataset/wikiofgraph,"a high-level explanation of the dataset characteristics
We introduce WikiOFGraph, a novel large-scale, domain-diverse dataset synthesized by LLMs, ensuring superior graph-text consistency to advance general-domain graph-to-text generation.



explain motivations and summary of its content
The scarcity of high-quality, general-domain G2T generation datasets restricts progress in the generaldomain G2T generation research. To address this issue, we introduce Wikipedia OntologyFree Graph-text dataset (WikiOFGraph), a new large-scale G2T dataset generated using a novel method that leverages Large Language Model (LLM) and Data-QuestEval.



potential use cases of the dataset
General Domain Knowledge Graph-to-Text generation",,,,,,
3550,WIKIOG,Structured Prediction,Structured Prediction,Structured Prediction,Time Series,,Methodology,,,https://arxiv.org/pdf/1905.10039.pdf,https://paperswithcode.com/dataset/wikiog,WIKIOG is a public collection which consists of over 1.75 million document-outline pairs for research on the OG task.,,Outline Generation: Understanding the Inherent Content Structure of Documents,https://arxiv.org/pdf/1905.10039,,,
3551,WikipediaGS,Table annotation,Table annotation,"Table annotation, Cell Entity Annotation, Column Type Annotation",Tabular,,Methodology,"column-type-annotation-on-wikipediags-cta, cell-entity-annotation-on-wikipediags",,https://springernature.figshare.com/articles/dataset/Evaluating_Web_Table_Annotation_Methods_From_Entity_Lookups_to_Entity_Embeddings/5229847,https://paperswithcode.com/dataset/wikipediags,"The WikipediaGS dataset was created by extracting Wikipedia tables from Wikipedia pages. It consists of 485,096 tables which were annotated with DBpedia entities for the Cell Entity Annotation (CEA) task.

Additionally, a subset of these tables was annotated by Chen et al. for the Column Type Annotation (CTA) task and includes 604 tables, where selected columns were annotated using DBpedia types. This subset is available for download at their official Github repository.

The table below shows the number of annotated cells/columns for each task and the number of different classes used for the annotation.

|     | Annotations | Classes   |
|-----|-------------|-----------|
| CEA | 4,453,329   | 1,222,358 |
| CTA | 620         | 31        |",,,,,,
3552,Wikipedia_Generation,Text Generation,Text Generation,"Text Generation, Multi-Document Summarization, Document Summarization",Text,English,Natural Language Processing,,,https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum,https://paperswithcode.com/dataset/wikipedia-generation,Wikipedia Generation is a dataset for article generation from Wikipedia from references at the end of Wikipedia page and the top 10 search results for the Wikipedia topic.,,,,,,
3553,Wikipedia_Person_and_Animal_Dataset,KB-to-Language Generation,KB-to-Language Generation,"KB-to-Language Generation, Data-to-Text Generation, Table-to-Text Generation","Tabular, Text",English,Natural Language Processing,"data-to-text-generation-on-wikipedia-person, table-to-text-generation-on-wikipedia-person, kb-to-language-generation-on-wikipedia-person",MIT,https://eaglew.github.io/dataset/narrating,https://paperswithcode.com/dataset/wikipedia-person-and-animal-dataset,"This dataset gathers 428,748 person and 12,236 animal infobox with descriptions based on Wikipedia dump (2018/04/01) and Wikidata (2018/04/12).",2018,,,,,
3554,Wikipedia_Title,Morphological Analysis,Morphological Analysis,"Morphological Analysis, Document Classification, Text Classification","Image, Text",English,Computer Vision,,,https://github.com/frederick0329/Wikipedia_title_dataset,https://paperswithcode.com/dataset/wikipedia-title,"Wikipedia Title is a dataset for learning character-level compositionality from the character visual characteristics. It consists of a collection of Wikipedia titles in Chinese, Japanese or Korean labelled with the category to which the article belongs.",,https://arxiv.org/abs/1704.04859,https://arxiv.org/abs/1704.04859,,,
3555,WikiSem500,Word Embeddings,Word Embeddings,"Word Embeddings, Outlier Detection, Sentiment Analysis","Image, Text",English,Computer Vision,,,https://github.com/belph/wiki-sem-500,https://paperswithcode.com/dataset/wikisem500,"The WikiSem500 dataset contains around 500 per-language cluster groups for English, Spanish, German, Chinese, and Japanese (a total of 13,314 test cases).",,https://arxiv.org/abs/1611.01547,https://arxiv.org/abs/1611.01547,,,
3556,WikiSplit,Split and Rephrase,Split and Rephrase,"Split and Rephrase, Text Simplification, Sentence Fusion",Text,English,Natural Language Processing,split-and-rephrase-on-wikisplit,,https://github.com/google-research-datasets/wiki-split,https://paperswithcode.com/dataset/wikisplit,"Contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the WebSplit corpus introduced by Narayan et al. (2017) as a benchmark for this task.",2017,,,,,
3557,WikiSQL,Semantic Parsing,Semantic Parsing,"Semantic Parsing, Code Generation, Sql Chatbots, Question Answering, SQL-to-Text",Text,English,Natural Language Processing,"question-answering-on-wikisql, sql-to-text-on-wikisql, code-generation-on-wikisql, semantic-parsing-on-wikisql-1",BSD 3-Clause License,https://github.com/salesforce/WikiSQL,https://paperswithcode.com/dataset/wikisql,"WikiSQL consists of a corpus of 87,726 hand-annotated SQL query and natural language question pairs. These SQL queries are further split into training (61,297 examples), development (9,145 examples) and test sets (17,284 examples). It can be used for natural language inference tasks related to relational databases.",,SQL-to-Text Generation with Graph-to-Sequence Model,https://arxiv.org/abs/1809.05255,297 examples,"split into training (61,297 examples",
3558,WikiSRS,Entity Disambiguation,Entity Disambiguation,"Entity Disambiguation, Entity Typing",,,Methodology,,,https://github.com/OSU-slatelab/WikiSRS,https://paperswithcode.com/dataset/wikisrs,"WikiSRS is a novel dataset of similarity and relatedness judgments of paired Wikipedia entities (people, places, and organizations), as assigned by Amazon Mechanical Turk workers.",,,,,,
3559,WikiSum,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Multi-Document Summarization, Document Summarization",Text,English,Natural Language Processing,,,https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum,https://paperswithcode.com/dataset/wikisum,"WikiSum is a dataset based on English Wikipedia and suitable for a task of multi-document abstractive summarization. In each instance, the input is comprised of a Wikipedia topic (title of article) and a collection of non-Wikipedia reference documents, and the target is the Wikipedia article text. The dataset is restricted to the articles with at least one crawlable citation. The official split divides the articles roughly into 80/10/10 for train/development/test subsets, resulting in 1865750, 233252, and 232998 examples respectively.",,Generating Wikipedia by Summarizing Long Sequences,https://arxiv.org/pdf/1801.10198.pdf,232998 examples,"split divides the articles roughly into 80/10/10 for train/development/test subsets, resulting in 1865750, 233252, and 232998 examples",
3560,WikiTableQuestions,Semantic Parsing,Semantic Parsing,"Semantic Parsing, Question Answering",Text,English,Natural Language Processing,"semantic-parsing-on-wikitablequestions, question-answering-on-wikitablequestions",CC-BY-SA-4.0,https://ppasupat.github.io/WikiTableQuestions/,https://paperswithcode.com/dataset/wikitablequestions,"WikiTableQuestions is a question answering dataset over semi-structured tables. It is comprised of question-answer pairs on HTML tables, and was constructed by selecting data tables from Wikipedia that contained at least 8 rows and 5 columns. Amazon Mechanical Turk workers were then tasked with writing trivia questions about each table. WikiTableQuestions contains 22,033 questions. The questions were not designed by predefined templates but were hand crafted by users, demonstrating high linguistic variance. Compared to previous datasets on knowledge bases it covers nearly 4,000 unique column headers, containing far more relations than closed domain datasets and datasets for querying knowledge bases. Its questions cover a wide range of domains, requiring operations such as table lookup, aggregation, superlatives (argmax, argmin), arithmetic operations, joins and unions.",,Explaining Queries over Web Tables to Non-Experts,https://arxiv.org/abs/1808.04614,8 rows,,
3561,WikiTables-TURL,Cell Entity Annotation,Cell Entity Annotation,"Cell Entity Annotation, Data Integration, Columns Property Annotation, Table annotation, Column Type Annotation",Tabular,,Methodology,"cell-entity-annotation-on-wikitables-turl-cea, columns-property-annotation-on-wikitables, column-type-annotation-on-wikitables-turl-cta",,https://buckeyemailosu-my.sharepoint.com/personal/deng_595_buckeyemail_osu_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fdeng%5F595%5Fbuckeyemail%5Fosu%5Fedu%2FDocuments%2FBuckeyeBox%20Data%2FTURL&ga=1,https://paperswithcode.com/dataset/wikitables-turl,"The WikiTables-TURL dataset was constructed by the authors of TURL and is based on the WikiTable corpus, which is a large collection of Wikipedia tables. The dataset consists of 580,171 tables divided into fixed training, validation and testing splits. Additionally, the dataset contains metadata about each table, such as the table name, table caption  and column headers. 

406,706 of these tables are annotated for the Column Type Annotation (CTA) task, 55,970 tables for the Columns Property Annotation (CPA) task and 200,744 tables for the Cell Entity Annotation (CEA) task. As classes for the CTA and CPA, Freebase's types and relations were used, whereas for the CEA task entities from Freebase were used. The table below lists the total annotated columns (or cells in the case of CEA) for each split and for each task  as well as the number of classes used for annotation.

|     | Training | Validation | Testing | Classes |
|-----|--------|----------|-------|-------|
| CTA | 628,254 |13,391| 13,025 | 255 |
| CPA | 62,954| 2,175 | 2,072 | 121 |
| CEA | 1,264,217 | 76,720  | 225,777 | 1,787,737 |

The authors have made the dataset and its variants publicly available for download.",,,,,,
3562,WikiText-103,Text Generation,Text Generation,"Text Generation, Language Modelling",Text,English,Natural Language Processing,"language-modelling-on-wikitext-103, text-generation-on-wikitext-103",CC BY-SA 3.0,https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/,https://paperswithcode.com/dataset/wikitext-103,"The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.

Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.",,,,,,
3563,WikiText-2,Text Generation,Text Generation,"Text Generation, Language Modelling",Text,English,Natural Language Processing,"language-modelling-on-wikitext-2, language-modelling-on-wikitext-103, text-generation-on-wikitext-103",CC BY-SA 3.0,https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/,https://paperswithcode.com/dataset/wikitext-2,"The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.

Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.",,,,,,
3564,Wild-Places,3D Place Recognition,3D Place Recognition,"3D Place Recognition, Sequential Place Recognition","3D, Image",,Computer Vision,,Creative Commons Attribution Noncommercial-Share Alike 4.0 Licence,https://csiro-robotics.github.io/Wild-Places/,https://paperswithcode.com/dataset/wildplaces,"Many existing datasets for lidar place recognition are solely representative of structured urban environments, and have recently been saturated in performance by deep learning based approaches. Natural and unstructured environments present many additional challenges for the tasks of long-term localisation but these environments are not represented in currently available datasets. 
To address this we introduce Wild-Places, a challenging large-scale dataset for lidar place recognition in unstructured, natural environments.  Wild-Places contains eight lidar sequences collected with a handheld sensor payload over the course of fourteen months, containing a total of 63K undistorted lidar submaps along with accurate 6DoF ground truth.  This dataset contains multiple revisits both within and between sequences, allowing for both intra-sequence (i.e., loop closure detection) and inter-sequence (i.e., re-localisation) tasks.  We also benchmark several state-of-the-art approaches to demonstrate the challenges that this dataset introduces, particularly the case of long-term place recognition due to natural environments changing over time.  Our dataset and code is available at https://csiro-robotics.github.io/Wild-Places",,,,,,
3565,Wild-Time,Ensemble Learning,Ensemble Learning,"Ensemble Learning, Continual Learning, Domain Generalization, Self-Supervised Learning",,,Methodology,,MIT,https://github.com/huaxiuyao/Wild-Time,https://paperswithcode.com/dataset/wild-time,"Wild-Time is a benchmark of 5 datasets that reflect temporal distribution shifts arising in a variety of real-world applications, including patient prognosis and news classification. On these datasets, we systematically benchmark 13 prior approaches, including methods in domain generalization, continual learning, self-supervised learning, and ensemble learning.",,Wild-Time: A Benchmark of in-the-Wild Distribution Shift over Time,https://arxiv.org/pdf/2211.14238v1.pdf,,,
3566,WildDeepfake,Metric Learning,Metric Learning,"Metric Learning, DeepFake Detection, Face Swapping",Image,,Computer Vision,,,https://github.com/deepfakeinthewild/deepfake-in-the-wild,https://paperswithcode.com/dataset/wilddeepfake,"WildDeepfake is a dataset for real-world deepfakes detection which consists of 7,314 face sequences extracted from 707 deepfake videos that are collected completely from the internet. WildDeepfake is a small dataset that can be used, in addition to existing datasets, to develop more effective detectors against real-world deepfakes.",,,,,,
3567,Wildtrack,Multiview Detection,Multiview Detection,"Multiview Detection, Multi-Object Tracking","Image, Video",,Computer Vision,"multi-object-tracking-on-wildtrack, multiview-detection-on-wildtrack",,https://www.epfl.ch/labs/cvlab/data/data-wildtrack/,https://paperswithcode.com/dataset/wildtrack,"Wildtrack is a large-scale and high-resolution dataset. It has been captured with seven static cameras in a public open area, and unscripted dense groups of pedestrians standing and walking. Together with the camera frames, we provide an accurate joint (extrinsic and intrinsic) calibration, as well as 7 series of 400 annotated
frames for detection at a rate of 2 frames per second. This results in over 40 000 bounding boxes delimiting every person present in the area of interest, for a total of more than
300 individuals.",,,,,,
3568,WiLI-2018,Language Identification,Language Identification,"Language Identification, Dependency Parsing, Natural Language Inference",Text,English,Natural Language Processing,,ODC Open Database License v1.0,https://doi.org/10.5281/zenodo.841984,https://paperswithcode.com/dataset/wili-2018,"WiLI-2018 is a benchmark dataset for monolingual written natural language identification. WiLI-2018 is a publicly available, free of charge dataset of short text extracts from Wikipedia. It contains 1000 paragraphs of 235 languages, totaling in 23500 paragraphs. WiLI is a classification dataset: Given an unknown paragraph written in one dominant language, it has to be decided which language it is.",2018,,,1000 paragraphs,,
3569,Win-Fail_Action_Understanding,Video Understanding,Video Understanding,"Video Understanding, Action Classification, Action Recognition In Videos, Video Recognition, Human-Object-interaction motion tracking, Action Understanding, Action Recognition, Video Classification, Video Retrieval, Text-to-video search, Human-Object Interaction Detection","Image, Text, Video",English,Computer Vision,"action-recognition-on-win-fail-action, action-understanding-on-win-fail-action",,https://github.com/ParitoshParmar/Win-Fail-Action-Recognition,https://paperswithcode.com/dataset/win-fail-action-understanding,"First  of  its  kind paired win-fail action understanding dataset with samples from  the  following  domains:  “General  Stunts,”  “Internet Wins-Fails,”  “Trick  Shots,”  &  “Party  Games.” The task is to identify successful and failed attempts at various activities. Unlike existing  action  recognition  datasets,  intra-class  variation is  high  making  the  task  challenging,  yet  feasible.",,,,,,
3570,Wine,Graph Classification,Graph Classification,"Graph Classification, Image/Document Clustering, Feature Importance, General Classification, Incremental Constrained Clustering","Graph, Image, Text",English,Computer Vision,"image-document-clustering-on-wine, graph-classification-on-wine, incremental-constrained-clustering-on-wine, classification-on-wine, feature-importance-on-wine",,https://archive.ics.uci.edu/ml/datasets/Wine,https://paperswithcode.com/dataset/wine,These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.,,,,,,
3571,WinoBias,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Word Embeddings, Fairness",,,Methodology,,MIT,https://uclanlp.github.io/corefBias/overview,https://paperswithcode.com/dataset/winobias,"WinoBias contains 3,160 sentences, split equally for development and test, created by researchers familiar with the project. Sentences were created to follow two prototypical templates but annotators were encouraged to come up with scenarios where entities could be interacting in plausible ways. Templates were selected to be challenging and designed to cover cases requiring semantics and syntax separately.",,,,160 sentences,,
3572,WinoGrande,Common Sense Reasoning,Common Sense Reasoning,"Common Sense Reasoning, Natural Language Understanding, parameter-efficient fine-tuning, Text Generation, Natural Language Inference, Question Answering",Text,English,Reasoning,"text-generation-on-winogrande-5-shot, common-sense-reasoning-on-winogrande, parameter-efficient-fine-tuning-on-winogrande, text-generation-on-winogrande-tr",CC-BY,http://winogrande.allenai.org/,https://paperswithcode.com/dataset/winogrande,"WinoGrande is a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations.",,,,,,
3573,Winoground,Visual Reasoning,Visual Reasoning,"Visual Reasoning, Image Captioning","Image, Text",English,Reasoning,visual-reasoning-on-winoground,,https://huggingface.co/datasets/facebook/winoground,https://paperswithcode.com/dataset/winoground,"Winoground is a dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning. Given two images and two captions, the goal is to match them correctly -- but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set of fine-grained tags to assist in analyzing model performance.",,,,,,
3574,WinoPron,Coreference Resolution,Coreference Resolution,"Coreference Resolution, coreference-resolution",,,Methodology,,CC BY-SA,,https://paperswithcode.com/dataset/winopron,"WinoPron is a novel dataset of Winogender-like template pairs in English, which fixes inconsistencies in Winogender Schemas and contains balanced template pairs for pronoun forms in 3 grammatical cases, which we find impacts performance and bias evaluation.",,,,,,
3575,WinSyn,Synthetic-to-Real Translation,Synthetic-to-Real Translation,"Synthetic-to-Real Translation, 3D Depth Estimation, 2D Semantic Segmentation","3D, Image, Text",English,Computer Vision,,CC BY-NC-ND 3.0,https://twak.github.io/winsyn/,https://paperswithcode.com/dataset/winsyn,75k photos of windows + 21k synthetic renders of building windows.,,,,,,
3576,WiRe57,Open Information Extraction,Open Information Extraction,Open Information Extraction,,,Methodology,open-information-extraction-on-wire57,,https://github.com/rali-udem/WiRe57,https://paperswithcode.com/dataset/wire57,"We manually performed the task of Open Information Extraction on 5 short documents, elaborating tentative guidelines for the task, and resulting in a ground truth reference of 347 tuples. [section 1]

A small corpus of 57 sentences taken from the beginning of 5 documents in English was used as the source text from which to extract tuples. Three documents are Wikipedia articles (Chilly Gonzales, the EM algorithm, and Tokyo) and two are newswire articles (taken from Reuters, hence the Wi-Re name). [section 3.1]",,,,57 sentences,,
3577,Wisconsin_60__20__20__random_splits_,Node Classification on Non-Homophilic (Heterophilic) Graphs,Node Classification on Non-Homophilic (Heterophilic) Graphs,Node Classification on Non-Homophilic (Heterophilic) Graphs,Image,,Graphs,node-classification-on-non-homophilic-1,,,https://paperswithcode.com/dataset/wisconsin-60-20-20-random-splits-1,Node classification on Wisconsin with 60%/20%/20% random splits for training/validation/test.,,,,,,
3578,WITS,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Sarcasm Detection","Image, Text",English,Computer Vision,"abstractive-text-summarization-on-wits, sarcasm-detection-on-wits",,https://github.com/LCS2-IIITD/MAF,https://paperswithcode.com/dataset/wits,"This dataset is an extension of MASAC, a multimodal, multi-party, Hindi-English code-mixed dialogue dataset compiled from the popular Indian TV show,
‘Sarabhai v/s Sarabhai’.  WITS was created by augmenting MASAC with natural language explanations for each sarcastic dialogue. The dataset consists of the transcribed sarcastic dialogues from 55 episodes of the TV show, along with audio and video multimodal signals. It was designed to facilitate Sarcasm Explanation in Dialogue (SED), a novel task aimed at generating a natural language explanation for a given sarcastic dialogue, that spells out the intended irony. Each data instance in WITS is associated with a corresponding video, audio, and textual transcript where the last utterance is sarcastic in nature. All the final selected explanations contain the following attributes:

• Sarcasm source: The speaker in the dialog who is being sarcastic.
• Sarcasm target: The person/ thing towards whom the sarcasm is directed.
• Action word: Verb/ action used to describe how the sarcasm is taking place.  e.g. mocking, insults, taunts, etc.
• Description: A description of the scene to help contextualize the sarcasm.",,,,,,
3579,Wizard-of-Oz,Dialogue State Tracking,Dialogue State Tracking,Dialogue State Tracking,"Image, Video",,Computer Vision,dialogue-state-tracking-on-wizard-of-oz,,https://arxiv.org/pdf/1606.03777.pdf,https://paperswithcode.com/dataset/wizard-of-oz,"The WoZ 2.0 dataset is a newer dialogue state tracking dataset whose evaluation is detached from the noisy output of speech recognition systems. Similar to DSTC2, it covers the restaurant search domain and has identical evaluation.

Description from NLP Progress",,Homepage,https://arxiv.org/pdf/1606.03777.pdf,,,
3580,Wizard_of_Wikipedia,Visual Dialog,Visual Dialog,Visual Dialog,Image,,Computer Vision,visual-dialog-on-wizard-of-wikipedia,,https://parl.ai/projects/wizard_of_wikipedia/,https://paperswithcode.com/dataset/wizard-of-wikipedia,Wizard of Wikipedia is a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. It is used to train and evaluate dialogue systems for knowledgeable open dialogue with clear grounding,,,,,,
3581,WMT_2014,Translation deu-eng,Translation deu-eng,"Translation deu-eng, Unsupervised Machine Translation, Machine Translation, Translation eng-deu",Text,English,Natural Language Processing,"unsupervised-machine-translation-on-wmt2014-3, unsupervised-machine-translation-on-wmt2014, machine-translation-on-wmt2014-english-french, unsupervised-machine-translation-on-wmt2014-2, translation-deu-eng-on-newstest2014-deen, unsupervised-machine-translation-on-wmt2014-1, machine-translation-on-wmt2014-english-czech, translation-eng-deu-on-newstest2014-deen, machine-translation-on-wmt2014-english-german, machine-translation-on-wmt2014-german-english, machine-translation-on-wmt2014-french-english",,http://www.statmt.org/wmt14/index.html,https://paperswithcode.com/dataset/wmt-2014,"WMT 2014 is a collection of datasets used in shared tasks of the Ninth Workshop on Statistical Machine Translation. The workshop featured four tasks:


a news translation task,
a quality estimation task,
a metrics task,
a medical text translation task.",2014,https://www.aclweb.org/anthology/W14-3302.pdf,https://www.aclweb.org/anthology/W14-3302.pdf,,,
3582,WMT_2016,Sequence-to-sequence Language Modeling,Sequence-to-sequence Language Modeling,"Sequence-to-sequence Language Modeling, Translation deu-eng, Translation eng-deu, Unsupervised Machine Translation, Machine Translation","Text, Time Series",English,Natural Language Processing,"machine-translation-on-wmt2016-english-german, sequence-to-sequence-language-modeling-on-1, unsupervised-machine-translation-on-wmt2016-2, machine-translation-on-wmt2016-english-czech, translation-eng-deu-on-newstest2016-ende, machine-translation-on-wmt2016-romanian, machine-translation-on-wmt2016-finnish, machine-translation-on-wmt2016-english-1, translation-deu-eng-on-newstest2016-deen, machine-translation-on-wmt2016-russian, machine-translation-on-wmt2016-czech-english, machine-translation-on-wmt2016-english, unsupervised-machine-translation-on-wmt2016-5, unsupervised-machine-translation-on-wmt2016, machine-translation-on-wmt2016-german-english, unsupervised-machine-translation-on-wmt2016-3, unsupervised-machine-translation-on-wmt2016-1, machine-translation-on-wmt2016-english-french",,http://www.statmt.org/wmt16/index.html,https://paperswithcode.com/dataset/wmt-2016,"WMT 2016 is a collection of datasets used in shared tasks of the First Conference on Machine Translation. The conference builds on ten previous Workshops on statistical Machine Translation.

The conference featured ten shared tasks:


a news translation task,
an IT domain translation task,
a biomedical translation task,
an automatic post-editing task,
a metrics task (assess MT quality given reference translation).
a quality estimation task (assess MT quality without access to any reference),
a tuning task (optimize a given MT system),
a pronoun translation task,
a bilingual document alignment task,
a multimodal translation task.",2016,,,,,
3583,WMT_2016_Biomedical,Automatic Post-Editing,Automatic Post-Editing,"Automatic Post-Editing, Multimodal Machine Translation, Machine Translation",Text,English,Natural Language Processing,,,http://www.statmt.org/wmt16/index.html,https://paperswithcode.com/dataset/wmt-2016-biomedical,"The Biomedical Translation Shared Task was first introduced at the First Conference of Machine Translation. The task aims to evaluate systems for the translation of biomedical titles and abstracts from scientific publications. The data includes three language pairs (English ↔ Portuguese, English  ↔ Spanish, English  ↔ French) and two sub-domains of biological sciences and health sciences.

The training data consists mainly of the Scielo corpus, a parallel collection of scientific publications composed of either titles, abstracts or title and abstracts which were retrieved from the Scielo database. For the Scielo corpus, a parallel documents are provided for all language pairs in the two sub-domains, except for the English  ↔ French, where only health was considered, as there were inadequate parallel documents available for biology in that pair. The training data was aligned using the GMA alignment tool. Additionally, a corpus of parallel titles from MEDLINEⓇ for all three language pairs were provided as well as monolingual documents for the four languages, retrieved from the Scielo database. These consist of documents in the Scielo database which have no corresponding document in another language.

The test set consisted of 500 documents (title and abstract) for each of the two directions of each language pair. None of the test documents was included in the training data and there is no overlap of documents between the test sets for any language pair, translation direction and sub-domain.",,https://www.aclweb.org/anthology/W16-2301.pdf,https://www.aclweb.org/anthology/W16-2301.pdf,500 documents,,
3584,WMT_2016_IT,Automatic Post-Editing,Automatic Post-Editing,"Automatic Post-Editing, Multimodal Machine Translation, Machine Translation",Text,English,Natural Language Processing,,,http://www.statmt.org/wmt16/index.html,https://paperswithcode.com/dataset/wmt-2016-it,"The IT Translation Task is a shared task introduced in the First Conference on Machine Translation. Compared to WMT 2016 News, this task brought several novelties to WMT:


4 out of the 7 langauges of the IT task are new in WMT,
adaptation to the IT domain with its specifics such as frequent named entities (mostly menu items, names of products and companies) and technical jargon,
adaptation to translation of answers in helpdesk service setting (many of the sentences are instructions with imperative verbs, which is very rare in the News translation task).

The test set consisted of 1000 answers from the Batch 3 of the QTLeap Corpus. The in-domain training data contained 2000 answers from the Batches 1 and 2 and also localization files from several open-source projects (LibreOffice, KDE, VLC) and bilingual dictionaries of IT-related terms extracted from Wikipedia. The out-of-domain training data contained all the corpora from the WMT 2016 News, plus PaCo2-EuEn Basque-English corpus and SETimes with Bulgarian-English parallel sentences. “Constrained” systems were restricted to use only these training data provided by the organizers.

The task was evaluated on the following language pairs:


English → Bulgarian
English → Czech
English → German
English → Spanish
English → Basque
English → Dutch
English → Portuguese",2016,Bojar et al,https://www.aclweb.org/anthology/W16-2301.pdf,,,
3585,WMT_2016_News,Automatic Post-Editing,Automatic Post-Editing,"Automatic Post-Editing, Word Alignment, Unsupervised Machine Translation, Machine Translation",Text,English,Natural Language Processing,"machine-translation-on-wmt2016-romanian, machine-translation-on-wmt2016-finnish, machine-translation-on-wmt2016-english-1, machine-translation-on-wmt2016-russian, machine-translation-on-wmt2016-english, unsupervised-machine-translation-on-wmt2016-5, unsupervised-machine-translation-on-wmt2016-3, unsupervised-machine-translation-on-wmt2016-2",,http://www.statmt.org/wmt16/index.html,https://paperswithcode.com/dataset/wmt-2016-news,"News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Czech, German, Finnish, Romanian, Russian, Turkish) and additional 1500 sentences from each of the 5 languages translated to English. For Romanian a third of the test set were released as a development set instead. For Turkish additional 500 sentence development set was released. The sentences were selected from dozens of news websites and translated by professional translators.
The training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.
Some training corpora were identical from WMT 2015 (Europarl, United Nations, French-English 10⁹ corpus, Common Crawl, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (CzEng v1.6pre, News Commentary v11, monolingual news data). Additionally, the following new corpora were added: Romanian Europarl, SETIMES2 from OPUS for Romanian-English and Turkish-English, Monolingual data sets from CommonCrawl.",2015,https://www.aclweb.org/anthology/W16-2301.pdf,https://www.aclweb.org/anthology/W16-2301.pdf,1500 sentences,,
3586,WN18,Link Prediction,Link Prediction,"Link Prediction, Ancestor-descendant prediction, Knowledge Graph Completion, Dynamic Link Prediction","Graph, Time Series",,Methodology,"ancestor-descendant-prediction-on-wn18rr, link-prediction-on-wn18-filtered, link-prediction-on-wn18rr, dynamic-link-prediction-on-wn18-filtered, knowledge-graph-completion-on-wn18rr, link-prediction-on-wn18",,https://everest.hds.utc.fr/doku.php?id=en:transe,https://paperswithcode.com/dataset/wn18,"The WN18 dataset has 18 relations scraped from WordNet for roughly 41,000 synsets, resulting in 141,442 triplets. It was found out that a large number of the test triplets can be found in the training set with another relation or the inverse relation. Therefore, a new version of the dataset WN18RR has been proposed to address this issue.",,,,,,
3587,WN18RR,Link Prediction,Link Prediction,"Link Prediction, Ancestor-descendant prediction, Knowledge Graph Completion","Graph, Time Series",,Methodology,"knowledge-graph-completion-on-wn18rr, ancestor-descendant-prediction-on-wn18rr, link-prediction-on-wn18rr",,https://download.microsoft.com/download/8/7/0/8700516A-AB3D-4850-B4BB-805C515AECE1/FB15K-237.2.zip,https://paperswithcode.com/dataset/wn18rr,"WN18RR is a link prediction dataset created from WN18, which is a subset of WordNet. WN18 consists of 18 relations and 40,943 entities. However, many text triples are obtained by inverting triples from the training set. Thus the WN18RR dataset is created to ensure that the evaluation dataset does not have inverse relation test leakage. In summary, WN18RR dataset contains 93,003 triples with 40,943 entities and 11 relation types.",,End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion,https://arxiv.org/abs/1811.04441,,,
3588,WNUT_2017,UIE,UIE,"UIE, Few-shot NER, Named Entity Recognition (NER)","Image, Text",English,Computer Vision,"uie-on-wnut-2017, named-entity-recognition-on-wnut-2017",CC-BY 4.0,https://noisy-text.github.io/2017/emerging-rare-entities.html,https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity,"This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarisation), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for example the tweet “so.. kktny in 30 mins?” - even human experts find entity kktny hard to detect and resolve. This task will evaluate the ability to detect and classify novel, emerging, singleton named entities in noisy text.

The goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities.",,,,,,
3589,WoodScape,Object Detection,Object Detection,"Object Detection, Semi-Supervised Semantic Segmentation, Autonomous Driving, Semantic Segmentation",Image,,Computer Vision,semi-supervised-semantic-segmentation-on-45,,https://github.com/valeoai/WoodScape,https://paperswithcode.com/dataset/woodscape,"Fisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of its prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images.
WoodScape is an extensive fisheye automotive dataset named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images.",1906,,,000 images,"valence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images",40
3590,Word_Sense_Disambiguation__a_Unified_Evaluation_Fr,Word Sense Disambiguation,Word Sense Disambiguation,"Word Sense Disambiguation, Quantization",,,Methodology,"quantization-on-knowledge-based, word-sense-disambiguation-on-knowledge-based, word-sense-disambiguation-on-supervised",,http://lcl.uniroma1.it/wsdeval/,https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified,"The Evaluation framework of Raganato et al. 2017 includes two training sets (SemCor-Miller et al., 1993- and OMSTI-Taghipour and Ng, 2015-) and five test sets from the Senseval/SemEval series (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2013; Moro and Navigli, 2015), standardized to the same format and sense inventory (i.e. WordNet 3.0).

Typically, there are two kinds of approach for WSD: supervised (which make use of sense-annotated training data) and knowledge-based (which make use of the properties of lexical resources).

Supervised: The most widely used training corpus used is SemCor, with 226,036 sense annotations from 352 documents manually annotated. All supervised systems in the evaluation table are trained on SemCor. Some supervised methods, particularly neural architectures, usually employ the SemEval 2007 dataset as development set (marked by *). The most usual baseline is the Most Frequent Sense (MFS) heuristic, which selects for each target word the most frequent sense in the training data.

Knowledge-based: Knowledge-based systems usually exploit WordNet or BabelNet as semantic network. The first sense given by the underlying sense inventory (i.e. WordNet 3.0) is included as a baseline.

Description from NLP Progress",2017,,,352 documents,,
3591,WorldCuisines,Multiple-choice,Multiple-choice,"Multiple-choice, Open-Ended Question Answering, Visual Question Answering (VQA), Visual Question Answering","Image, Text",English,Computer Vision,,Creative Commons Attribution-ShareAlike 4.0 International License,https://worldcuisines.github.io/,https://paperswithcode.com/dataset/worldcuisines,"Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images along with the VQA data.

The dataset can be found in https://huggingface.co/datasets/worldcuisines/vqa",,,,60k instances,"valuate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images",
3592,WOS,Image Classification,Image Classification,"Image Classification, Document Classification, Hierarchical Multi-label Classification, Text Classification","Image, Text",English,Computer Vision,"hierarchical-multi-label-classification-on-16, document-classification-on-wos-46985, document-classification-on-wos-5736, document-classification-on-wos-11967",,https://data.mendeley.com/datasets/9rw3vkcfy4/6,https://paperswithcode.com/dataset/web-of-science-dataset,"Web of Science (WOS) is a document classification dataset that contains 46,985 documents with 134 categories which include 7 parents categories.",,,,985 documents,,134
3593,WOST,Scene Text Recognition,Scene Text Recognition,Scene Text Recognition,"Image, Text",English,Computer Vision,scene-text-recognition-on-wost,,,https://paperswithcode.com/dataset/wost,"The Weakly Occluded Scene Text (WOST) dataset is a public dataset for scene text segmentation. It is used to generate pixel-level annotations in scene text images 1. The dataset is designed to contain weakly annotated images, which means that the images are not fully annotated with pixel-level labels.",,,,,,
3594,WritingPrompts,Text Generation,Text Generation,"Text Generation, Language Modelling, Story Generation, Natural Language Understanding",Text,English,Natural Language Processing,story-generation-on-writingprompts,MIT,https://www.kaggle.com/ratthachat/writing-prompts,https://paperswithcode.com/dataset/writingprompts,WritingPrompts is a large dataset of 300K human-written stories paired with writing prompts from an online forum.,,Hierarchical Neural Story Generation,https://arxiv.org/pdf/1805.04833v1.pdf,,,
3595,WS353,Word Similarity,Word Similarity,Word Similarity,,,Methodology,word-similarity-on-ws353,,,https://paperswithcode.com/dataset/ws353,"Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, pages 406–414",2001,,,,,
3596,WSC,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Classification",Image,,Computer Vision,"coreference-resolution-on-winograd-schema, classification-on-wsc",CC BY 4.0,https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html,https://paperswithcode.com/dataset/wsc,"The Winograd Schema Challenge was introduced both as an alternative to the Turing Test and as a test of a system’s ability to do commonsense reasoning. A Winograd schema is a pair of sentences differing in one or two words with a highly ambiguous pronoun, resolved differently in the two sentences, that appears to require commonsense knowledge to be resolved correctly. The examples were designed to be easily solvable by humans but difficult for machines, in principle requiring a deep understanding of the content of the text and the situation it describes.

The original Winograd Schema Challenge dataset consisted of 100 Winograd schemas constructed manually by AI experts. As of 2020 there are 285 examples available; however, the last 12 examples were only added recently. To ensure consistency with earlier models, several authors often prefer to report the performance on the first 273 examples only. These datasets are usually referred to as WSC285 and WSC273, respectively.",2020,https://arxiv.org/pdf/2004.13831.pdf,https://arxiv.org/pdf/2004.13831.pdf,285 examples,"Test and as a test of a system’s ability to do commonsense reasoning. A Winograd schema is a pair of sentences differing in one or two words with a highly ambiguous pronoun, resolved differently in the two sentences, that appears to require commonsense knowledge to be resolved correctly. The examples",
3597,WSJ0-2mix-extr,Speech Extraction,Speech Extraction,Speech Extraction,Audio,,Speech,speech-extraction-on-wsj0-2mix-extr,,https://github.com/xuchenglin28/speaker_extraction_SpEx/tree/master/data/wsj0_2mix,https://paperswithcode.com/dataset/wsj0-2mix-extr,WSJ0-2mix-extr is a speech extraction dataset,,,,,,
3598,WSJ0-2mix,Adversarial Attack,Adversarial Attack,"Adversarial Attack, Audio Source Separation, Speech Separation",Audio,,Adversarial,"adversarial-attack-on-wsj0-2mix, speech-separation-on-wsj0-2mix-16k, speech-separation-on-wsj0-2mix",,https://www.merl.com/demos/deep-clustering,https://paperswithcode.com/dataset/wsj0-2mix-1,WSJ0-2mix is a speech recognition corpus of speech mixtures using utterances from the Wall Street Journal (WSJ0) corpus.,,,,,,
3599,WSJ_POS,POS Tagging,POS Tagging,POS Tagging,,,Methodology,pos-tagging-on-wsj-pos,,,https://paperswithcode.com/dataset/wsj-pos,"M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz, “Building a large annotated corpus of English: The Penn Treebank”, Computational Linguistics, vol. 19, no. 2, pp. 313–330, 1993. [Online]. Available: https://aclanthology.org/J93-2004",1993,,,,,
3600,WSRD_,Shadow Removal,Shadow Removal,Shadow Removal,,,Methodology,shadow-removal-on-wsrd,,,https://paperswithcode.com/dataset/wsrd,A version of the WSRD Dataset will be used as a benchmark for the NTIRE24 Challenge on Image Shadow Removal.,,,,,,
3601,X-SRL,Machine Translation,Machine Translation,"Machine Translation, Semantic Role Labeling",Text,English,Natural Language Processing,,,https://github.com/Heidelberg-NLP/xsrl_mbert_aligner,https://paperswithcode.com/dataset/x-srl,"SRL is the task of extracting semantic predicate-argument structures from sentences. X-SRL is a multilingual parallel Semantic Role Labelling (SRL) corpus for English (EN), German (DE), French (FR) and Spanish (ES) that is based on English gold annotations and shares the same labelling scheme across languages.",,Daza et al,https://arxiv.org/pdf/2010.01998v1.pdf,,,
3602,x-stance,Argument Mining,Argument Mining,"Argument Mining, Stance Detection",Image,,Computer Vision,,,https://github.com/ZurichNLP/xstance,https://paperswithcode.com/dataset/x-stance,"A large-scale stance detection dataset from comments written by candidates of elections in Switzerland. The dataset consists of German, French and Italian text, allowing for a cross-lingual evaluation of stance detection. It contains 67 000 comments on more than 150 political issues (targets).",,,,,,
3603,XAI-Bench,Explainable artificial intelligence,Explainable artificial intelligence,Explainable artificial intelligence,,,Methodology,,,,https://paperswithcode.com/dataset/xai-bench,"XAI-Bench is a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate ground-truth Shapley values and other metrics. The synthetic datasets released offer a wide variety of parameters that can be configured to simulate real-world data.",,,,,,
3604,XAlign,KG-to-Text Generation,KG-to-Text Generation,"KG-to-Text Generation, Data-to-Text Generation",Text,English,Natural Language Processing,data-to-text-generation-on-xalign,MIT,https://github.com/tushar117/XAlign,https://paperswithcode.com/dataset/xalign,"It consists of an extensive collection of a high quality cross-lingual fact-to-text dataset in 11 languages: Assamese (as), Bengali (bn), Gujarati (gu), Hindi (hi), Kannada (kn), Malayalam (ml), Marathi (mr), Oriya (or), Punjabi (pa), Tamil (ta), Telugu (te), and monolingual dataset in English (en). This is the Wikipedia text <--> Wikidata KG aligned corpus used to train the data-to-text generation model. The Train & validation splits are created using distant supervision methods and Test data is generated through human annotations.

Data Format
Dataset is publicly available here. Each directory contains language specific dataset (refered through language ISO code) and contains of three files:


train.jsonl
test.jsonl
val.jsonl

Data stored in the above files are of JSON Line (jsonl) format.

Record structure (JSON structure)
Each record consist of the following entries:


sentence (string) : Native language wikipedia sentence. (non-native language strings were removed.) 
facts (List[Dict]) : List of facts associated with the sentence where each fact is stored as dictionary.
language (string) : Language identifier.

The facts key contains list of facts where each facts is stored as dictionary. A single record within fact list contains following entries:


subject (string) : central entity.
object (string) : entity or a piece of information about the subject.
predicate (string) : relationship that connects the subject and the object.
qualifiers (List[Dict]) : It provide additional information about the fact, is stored as list of 
qualifier where each record is a dictionary. The dictionary contains two keys: qualifier_predicate to represent property of qualifer and qualifier_object to store value for the qualifier's predicate. 

Examples
Example from English dataset
{
  ""sentence"": ""Mark Paul Briers (born 21 April 1968) is a former English cricketer."",
  ""facts"": [
    {
      ""subject"": ""Mark Briers"",
      ""predicate"": ""date of birth"",
      ""object"": ""21 April 1968"",
      ""qualifiers"": []
    },
    {
      ""subject"": ""Mark Briers"",
      ""predicate"": ""occupation"",
      ""object"": ""cricketer"",
      ""qualifiers"": []
    },
    {
      ""subject"": ""Mark Briers"",
      ""predicate"": ""country of citizenship"",
      ""object"": ""United Kingdom"",
      ""qualifiers"": []
    }
  ],
  ""language"": ""en""
}
Example from one of the low-resource languages (i.e. Hindi)
{
  ""sentence"": ""बोरिस पास्तेरनाक १९५८ में साहित्य के क्षेत्र में नोबेल पुरस्कार विजेता रहे हैं।"",
  ""facts"": [
    {
      ""subject"": ""Boris Pasternak"",
      ""predicate"": ""nominated for"",
      ""object"": ""Nobel Prize in Literature"",
      ""qualifiers"": [
        {
          ""qualifier_predicate"": ""point in time"",
          ""qualifier_subject"": ""1958""
        }
      ]
    }
  ],
  ""language"": ""hi""
}",1968,,,,,
3605,xBD,Disaster Response,Disaster Response,"Disaster Response, Semantic Segmentation, Data Augmentation, Extracting Buildings In Remote Sensing Images, 2D Semantic Segmentation",Image,,Computer Vision,"extracting-buildings-in-remote-sensing-images, 2d-semantic-segmentation-on-xbd",CC BY-NC-SA 4.0,https://github.com/DIUx-xView/xview2-baseline,https://paperswithcode.com/dataset/xbd,"The xBD dataset contains over 45,000KM2 of polygon labeled pre and post disaster imagery. The dataset provides the post-disaster imagery with transposed polygons from pre over the buildings, with damage classification labels.",,Gupta et al,https://arxiv.org/pdf/1911.09296.pdf,,,
3606,xCodeEval,Code Translation,Code Translation,"Code Translation, Retrieval, Program Repair, Code Classification, Program Synthesis","Image, Text",English,Computer Vision,,CC-BY-NC 4.0,https://github.com/ntunlp/xCodeEval,https://paperswithcode.com/dataset/xcodeeval,"xCodeEval is one of the largest executable multilingual multitask benchmarks consisting of 17 programming languages with execution-level parallelism. It features a total of seven tasks involving code understanding, generation, translation, and retrieval, and it employs an execution-based evaluation instead of traditional lexical approaches. It also provides a test-case-based multilingual code execution engine, ExecEval that supports all the programming languages in xCodeEval.",,,,,,
3607,XCOPA,Cross-Lingual Transfer,Cross-Lingual Transfer,Cross-Lingual Transfer,,,Methodology,cross-lingual-transfer-on-xcopa,,https://github.com/cambridgeltl/xcopa,https://paperswithcode.com/dataset/xcopa,The Cross-lingual Choice of Plausible Alternatives (XCOPA) dataset is a benchmark to evaluate the ability of machine learning models to transfer commonsense reasoning across languages. The dataset is the translation and reannotation of the English COPA (Roemmele et al. 2011) and covers 11 languages from 11 families and several areas around the globe. The dataset is challenging as it requires both the command of world knowledge and the ability to generalise to new languages.,2011,,,,,
3608,XED,Opinion Mining,Opinion Mining,"Opinion Mining, Sentiment Analysis",Text,English,Natural Language Processing,,,https://github.com/Helsinki-NLP/XED,https://paperswithcode.com/dataset/xed,"XED is a multilingual fine-grained emotion dataset. The dataset consists of human-annotated Finnish (25k) and English sentences (30k), as well as projected annotations for 30 additional languages, providing new resources for many low-resource languages.",,,,,,
3609,XFORMAL,Text Style Transfer,Text Style Transfer,Text Style Transfer,Text,English,Natural Language Processing,,,https://github.com/Elbria/xformal-FoST,https://paperswithcode.com/dataset/xformal,"XFORMAL is a multilingual formal style transfer benchmark of multiple formal reformulations of informal text in Brazilian Portuguese, French, and Italian.",,,,,,
3610,XGLUE,Natural Language Understanding,Natural Language Understanding,"Natural Language Understanding, Cross-Lingual NER, Part-Of-Speech Tagging, Cross-Lingual Natural Language Inference, Natural Language Inference, Cross-Lingual POS Tagging, Few-shot NER","Audio, Text",English,Natural Language Processing,"part-of-speech-tagging-on-xglue, few-shot-ner-on-xglue",Custom (non-commercial),https://microsoft.github.io/XGLUE/,https://paperswithcode.com/dataset/xglue,"XGLUE is an evaluation benchmark XGLUE,which is composed of 11 tasks that span 19 languages. For each task, the training data is only available in English. This means that to succeed at XGLUE, a model must have a strong zero-shot cross-lingual transfer capability to learn from the English data of a specific task and transfer what it learned to other languages. Comparing to its concurrent work XTREME, XGLUE has two characteristics: First, it includes cross-lingual NLU and cross-lingual NLG tasks at the same time; Second, besides including 5 existing cross-lingual tasks (i.e. NER, POS, MLQA, PAWS-X and XNLI), XGLUE selects 6 new tasks from Bing scenarios as well, including News Classification (NC), Query-Ad Matching (QADSM), Web Page Ranking (WPR), QA Matching (QAM), Question Generation (QG) and News Title Generation (NTG). Such diversities of languages, tasks and task origin provide a comprehensive benchmark for quantifying the quality of a pre-trained model on cross-lingual natural language understanding and generation.",,,,,,
3611,XImageNet-12,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Explainable Artificial Intelligence (XAI), Classification",Image,English,Computer Vision,classification-on-ximagenet-12,Custom,https://sites.google.com/view/ximagenet-12/home,https://paperswithcode.com/dataset/ximagenet-12,"Enlarge the dataset to understand how image background effect the Computer Vision ML model. With the following topics: Blur Background / Segmented Background / AI generated Background/ Bias of tools during annotation/ Color in Background / Dependent Factor in Background/ LatenSpace Distance of Foreground/ Random Background with Real Environment!

We introduce XIMAGENET-12, an explainable benchmark dataset with over 200K images and 15,600 manual semantic annotations. Covering 12 categories from ImageNet to represent objects commonly encountered in practical life and simulating six diverse scenarios, including overexposure, blurring, color changing, etc.,

Our research builds upon the foundation laid by ""Noise or Signal: The Role of Image Backgrounds in Object Recognition"" (Xiao et al., ICLR 2022), ""Explainable AI: Object Recognition With Help From Background"" (Qiang et al., ICLR Workshop 2022), reinforced the notion that models trained solely on backgrounds can substantially improve accuracy. One noteworthy discovery highlighted in their studies is that more accurate models tend to rely less on backgrounds.",2022,,,200K images,,12
3612,XL-BEL,Entity Linking,Entity Linking,"Entity Linking, Cross-Lingual Entity Linking",,,Methodology,,,https://github.com/cambridgeltl/sapbert,https://paperswithcode.com/dataset/xl-bel,XL-BEL is a benchmark for cross-lingual biomedical entity linking (XL-BEL). The benchmark spans 10 typologically diverse languages.,,,,,,
3613,XL-Sum,Abstractive Text Summarization,Abstractive Text Summarization,"Abstractive Text Summarization, Text Summarization",Text,English,Natural Language Processing,,Custom (non-commercial),https://github.com/csebuetnlp/xl-sum,https://paperswithcode.com/dataset/xl-sum,"XL-Sum is a comprehensive and diverse dataset for abstractive summarization comprising 1 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation.",,,,,,
3614,XLCoST,Code Search,Code Search,Code Search,,,Methodology,,Apache-2.0 license,https://github.com/reddy-lab-code-research/XLCoST,https://paperswithcode.com/dataset/xlcost,"XLCoST is a benchmark dataset for cross-lingual code intelligence. The dataset contains fine-grained parallel data from 8 languages (7 commonly used programming languages and English), and supports 10 cross-language code tasks.",,XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence,https://arxiv.org/pdf/2206.08474v1.pdf,,,
3615,XMIDI,Music Generation,Music Generation,"Music Generation, Music Genre Recognition, Music Classification, Music Emotion Recognition","Audio, Image, Text",English,Computer Vision,,,https://github.com/xmusic-project/XMIDI_Dataset,https://paperswithcode.com/dataset/xmidi,"XMIDI is a comprehensive, large-scale symbolic music dataset that includes accurate emotion and genre labels, consisting of 108,023 MIDI files. The average duration of the music pieces is approximately 176 seconds, yielding a total dataset length of around 5,278 hours.",,,,,,
3616,XNLI,Chinese Sentence Pair Classification,Chinese Sentence Pair Classification,"Chinese Sentence Pair Classification, Cross-Lingual Natural Language Inference, Natural Language Inference","Image, Text",English,Computer Vision,"natural-language-inference-on-xnli-chinese, cross-lingual-natural-language-inference-on-4, chinese-sentence-pair-classification-on-xnli, chinese-sentence-pair-classification-on-xnli-1, cross-lingual-natural-language-inference-on-3, cross-lingual-natural-language-inference-on, natural-language-inference-on-xnli-chinese-1, cross-lingual-natural-language-inference-on-1, natural-language-inference-on-xnli-french",Attribution-NonCommercial 4.0 International,https://github.com/facebookresearch/XNLI,https://paperswithcode.com/dataset/xnli,"The Cross-lingual Natural Language Inference (XNLI) corpus is the extension of the Multi-Genre NLI (MultiNLI) corpus to 15 languages. The dataset was created by manually translating the validation and test sets of MultiNLI into each of those 15 languages. The English training set was machine translated for all languages. The dataset is composed of 122k train, 2490 validation and 5010 test examples.",,CamemBERT: a Tasty French Language Model,https://arxiv.org/abs/1911.03894,,"validation and test sets of MultiNLI into each of those 15 languages. The English training set was machine translated for all languages. The dataset is composed of 122k train, 2490 validation and 5010 test examples",
3617,xP3,Coreference Resolution,Coreference Resolution,"Coreference Resolution, Text Summarization, Sentence Completion, Translation",Text,English,Natural Language Processing,,,https://github.com/bigscience-workshop/xmtf,https://paperswithcode.com/dataset/xp3,xP3 is a multilingual dataset for multitask prompted finetuning. It is a composite of supervised datasets in 46 languages with English and machine-translated prompts.,,Crosslingual Generalization through Multitask Finetuning,https://arxiv.org/pdf/2211.01786v1.pdf,,,
3618,XQuAD,Language Modelling,Language Modelling,"Language Modelling, Question Answering, Reading Comprehension, Cross-Lingual Question Answering",Text,English,Natural Language Processing,cross-lingual-question-answering-on-xquad,CC BY-SA 4.0,https://github.com/deepmind/xquad,https://paperswithcode.com/dataset/xquad,"XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question answering performance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. Consequently, the dataset is entirely parallel across 11 languages.",2016,https://arxiv.org/pdf/1910.11856v3.pdf,https://arxiv.org/pdf/1910.11856v3.pdf,240 paragraphs,,
3619,xSID,Spoken Language Understanding,Spoken Language Understanding,"Spoken Language Understanding, Slot Filling, Zero-Shot Cross-Lingual Transfer, Intent Classification","Image, Text",English,Computer Vision,,,https://bitbucket.org/robvanderg/xsid,https://paperswithcode.com/dataset/xsid,"xSID, a new evaluation benchmark for cross-lingual (X) Slot and Intent Detection in 13 languages from 6 language families,  including a very low-resource dialect, covering Arabic (ar), Chinese (zh), Danish (da), Dutch (nl), English (en), German (de), Indonesian (id), Italian (it), Japanese (ja), Kazakh (kk), Serbian (sr), Turkish (tr) and an Austro-Bavarian German dialect, South Tyrolean (de-st).",,,,,,
3620,XSum,Text Summarization,Text Summarization,"Text Summarization, Summarization, Extreme Summarization",Text,English,Natural Language Processing,"text-summarization-on-xsum-2, extreme-summarization-on-xsum, summarization-on-xsum, text-summarization-on-x-sum",,https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset,https://paperswithcode.com/dataset/xsum,"The Extreme Summarization (XSum) dataset is a dataset for evaluation of abstractive single-document summarization systems. The goal is to create a short, one-sentence new summary answering the question “What is the article about?”. The dataset consists of 226,711 news articles accompanied with a one-sentence summary. The articles are collected from BBC articles (2010 to 2017) and cover a wide variety of domains (e.g., News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts). The official random split contains 204,045 (90%), 11,332 (5%) and 11,334 (5) documents in training, validation and test sets, respectively.",2010,https://arxiv.org/pdf/1808.08745.pdf,https://arxiv.org/pdf/1808.08745.pdf,,,
3621,XTREME,Token Classification,Token Classification,"Token Classification, Zero-Shot Cross-Lingual Transfer",Image,,Computer Vision,"token-classification-on-xtreme, zero-shot-cross-lingual-transfer-on-xtreme",,https://sites.research.google/xtreme,https://paperswithcode.com/dataset/xtreme,"The Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark was introduced to encourage more research on multilingual transfer learning,. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics.

The languages in XTREME are selected to maximize language diversity, coverage in existing tasks, and availability of training data. The languages in XTREME are selected to maximize language diversity, coverage in existing tasks, and availability of training data. Among these are many under-studied languages, such as the Dravidian languages Tamil (spoken in southern India, Sri Lanka, and Singapore), Telugu and Malayalam (spoken mainly in southern India), and the Niger-Congo languages Swahili and Yoruba, spoken in Africa.",,,,,,
3622,xView,Image Super-Resolution,Image Super-Resolution,"Image Super-Resolution, Disaster Response, Object Detection In Aerial Images, Geophysics, Object Detection",Image,,Computer Vision,object-detection-in-aerial-images-on-xview,Custom,http://xviewdataset.org/,https://paperswithcode.com/dataset/xview,"xView is one of the largest publicly available datasets of overhead imagery. It contains images from complex scenes around the world, annotated using bounding boxes. It contains over 1M object instances from 60 different classes.",,,,,,
3623,xView3-SAR,Dense Object Detection,Dense Object Detection,"Dense Object Detection, Representation Learning, regression, Holdout Set, Object Detection, Decision Making Under Uncertainty",Image,,Computer Vision,holdout-set-on-xview3-sar,CC BY-NC-SA 4.0,https://iuu.xview.us/,https://paperswithcode.com/dataset/xview3-sar,"Unsustainable fishing practices worldwide pose a major threat to marine resources and ecosystems. Identifying vessels that do not show up in conventional monitoring systems---known as ``dark vessels''---is key to managing and securing the health of marine environments. With the rise of satellite-based synthetic aperture radar (SAR) imaging and modern machine learning (ML), it is now possible to automate detection of dark vessels day or night, under all-weather conditions. SAR images, however, require a domain-specific treatment and are not widely accessible to the ML community. Maritime objects (vessels and offshore infrastructure) are relatively small and sparse, challenging traditional computer vision approaches. We present the largest labeled dataset for training ML models to detect and characterize vessels and ocean structures in SAR imagery. xView3-SAR consists of nearly 1,000 analysis-ready SAR images from the Sentinel-1 mission that are, on average, 29,400-by-24,400 pixels each. The images are annotated using a combination of automated and manual analysis. Co-located bathymetry and wind state rasters accompany every SAR image. We also provide an overview of the xView3 Computer Vision Challenge, an international competition using xView3-SAR for ship detection and characterization at large scale. We release the data  (\href{https://iuu.xview.us/}{https://iuu.xview.us/}) and code (\href{https://github.com/DIUx-xView}{https://github.com/DIUx-xView}) to support ongoing development and evaluation of ML approaches for this important application.",,,,,"training ML models to detect and characterize vessels and ocean structures in SAR imagery. xView3-SAR consists of nearly 1,000 analysis-ready SAR images",
3624,XWikiRef,Text Generation,Text Generation,"Text Generation, Cross-Lingual Abstractive Summarization, Unsupervised Extractive Summarization",Text,English,Natural Language Processing,cross-lingual-abstractive-summarization-on-4,MIT,https://github.com/DhavalTaunk08/XWikiGen,https://paperswithcode.com/dataset/xwikiref,"We provide a new data set XWikiRef for the task of Cross-lingual Multi-document Summarization. This task aims at generating Wikipedia style text in Low Resource languages by taking reference text as input. Overall, the data set contains 8 different languages: bengali (bn), english (en), hindi (hi), marathi (mr), malayalam (ml), odia (or), punjabi (pa) and tamil (ta). It also contains 5 domains: books, films, politicians, sportsman and writers.

Data Format
Dataset is publicly available here. Each directory contains language specific data subset having 1 json file per domain. In each file, each line denotes one article. It contains the following set of keys:


Article title
Sections
section title 1
section text 1
list of reference texts 1
.....
.....
.....
section title n
section text n
list of reference texts 1",,,,,,
3625,YAGO,Link Prediction,Link Prediction,"Link Prediction, Triple Classification, Time-interval Prediction","Image, Time Series",,Computer Vision,"triple-classification-on-yago39k, link-prediction-on-yago39k, link-prediction-on-yago15k-1, link-prediction-on-yago37, time-interval-prediction-on-yago11k, link-prediction-on-yago3-10, link-prediction-on-yago11k",CC BY 3.0,https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/,https://paperswithcode.com/dataset/yago,"Yet Another Great Ontology (YAGO) is a Knowledge Graph that augments WordNet with common knowledge facts extracted from Wikipedia, converting WordNet from a primarily linguistic resource to a common knowledge base. YAGO originally consisted of more than 1 million entities and 5 million facts describing relationships between these entities. YAGO2 grounded entities, facts, and events in time and space, contained 446 million facts about 9.8 million entities, while YAGO3 added about 1 million more entities from non-English Wikipedia articles. YAGO3-10 a subset of YAGO3, containing entities which have a minimum of 10 relations each.",,"Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches",https://arxiv.org/abs/1904.01172,,,
3626,YASO,Aspect-Based Sentiment Analysis (ABSA),Aspect-Based Sentiment Analysis (ABSA),"Aspect-Based Sentiment Analysis (ABSA), Aspect Extraction, Aspect Term Extraction and Sentiment Classification","Image, Text",English,Computer Vision,aspect-extraction-on-yaso-yelp,,https://github.com/IBM/yaso-tsa,https://paperswithcode.com/dataset/yaso,"YASO is a crowd-sourced TSA evaluation dataset, collected using a new annotation scheme for labeling targets and their sentiments. The dataset contains 2,215 English sentences from movie, business and product reviews, and 7,415 terms and their corresponding sentiments annotated within these sentences.",,,,,,
3627,YCB-Slide,Robot Manipulation,Robot Manipulation,"Robot Manipulation, Object SLAM",,,Methodology,,CC BY-SA 4.0,https://github.com/rpl-cmu/YCB-Slide,https://paperswithcode.com/dataset/ycb-slide,"The YCB-Slide dataset comprises of DIGIT sliding interactions on YCB objects. We envision this can contribute towards efforts in tactile localization, mapping, object understanding, and learning dynamics models. We provide access to DIGIT images, sensor poses, RGB video feed, ground-truth mesh models, and ground-truth heightmaps + contact masks (simulation only). This dataset is supplementary to the MidasTouch paper, a CoRL 2022 submission.",2022,,,,,
3628,YCB-Video,Pose Estimation,Pose Estimation,"Pose Estimation, 6D Pose Estimation using RGBD, 6D Pose Estimation, Occluded 3D Object Symmetry Detection, Symmetry Detection, 6D Pose Estimation using RGB","3D, Image",,Computer Vision,"6d-pose-estimation-on-ycb-video-2, 6d-pose-estimation-on-ycb-video, symmetry-detection-on-ycb-video, 6d-pose-estimation-using-rgbd-on-ycb-video, occluded-3d-object-symmetry-detection-on-ycb",,https://rse-lab.cs.washington.edu/projects/posecnn/,https://paperswithcode.com/dataset/ycb-video,"The YCB-Video dataset is a large-scale video dataset for 6D object pose estimation. provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames.",,,,,,
3629,YCBInEOAT_Dataset,6D Pose Estimation,6D Pose Estimation,"6D Pose Estimation, Pose Estimation, Pose Tracking","3D, Image, Video",,Computer Vision,,,https://github.com/wenbowen123/iros20-6d-pose-tracking,https://paperswithcode.com/dataset/ycbineoat-dataset,A new dataset with significant occlusions related to object manipulation.,,,,,,
3630,Yeast,Community Detection,Community Detection,"Community Detection, Q-Learning, Node Classification",Image,,Computer Vision,,,http://vlado.fmf.uni-lj.si/pub/networks/data/bio/Yeast/Yeast.htm,https://paperswithcode.com/dataset/yeast,"Yeast dataset consists of a protein-protein interaction network. Interaction detection methods have led to the discovery of thousands of interactions between proteins, and discerning relevance within large-scale data sets is important to present-day biology.",,Topological structure analysis of the protein-protein interaction network in budding yeast,http://www.imb-jena.de/jcb/ppi/PPI_PDF_free/bu2003.pdf,,,
3631,Yelp-Fraud,Anomaly Detection,Anomaly Detection,"Anomaly Detection, Fraud Detection, Graph Mining, Node Classification","Graph, Image",,Computer Vision,"fraud-detection-on-yelp-fraud, node-classification-on-yelpchi",Apache-2.0,https://github.com/YingtongDou/CARE-GNN,https://paperswithcode.com/dataset/yelpchi,"Yelp-Fraud is a multi-relational graph dataset built upon the Yelp spam review dataset, which can be used in evaluating graph-based node classification, fraud detection, and anomaly detection models.


Dataset Statistics

| # Nodes  |  %Fraud Nodes (Class=1) |
|-------|--------|
|  45,954 | 14.5   | 

| Relation  | # Edges |
|--------|--------|
  |   R-U-R    |  49,315 |
 |  R-T-R  |  573,616  |
|  R-S-R  |  3,402,743 |
 |  All |  3,846,979  |


Graph Construction

The Yelp spam review dataset includes hotel and restaurant reviews filtered (spam) and recommended (legitimate) by Yelp. We conduct a spam review detection task on the Yelp-Fraud dataset which is a binary classification task. We take 32 handcrafted features from SpEagle paper as the raw node features for Yelp-Fraud. Based on previous studies which show that opinion fraudsters have connections in user, product, review text, and time, we take reviews as nodes in the graph and design three relations: 1) R-U-R: it connects reviews posted by the same user; 2) R-S-R: it connects reviews under the same product with the same star rating (1-5 stars); 3) R-T-R: it connects two reviews under the same product posted in the same month. 

To download the dataset, please visit this Github repo. For any other questions, please email ytongdou(AT)gmail.com for inquiry.",,SpEagle,http://shebuti.com/wp-content/uploads/2016/06/15-kdd-collectiveopinionspam.pdf,,,
3632,Yelp,Sentiment Classification,Sentiment Classification,"Sentiment Classification, Document Classification, SQL Parsing, Text Classification, Paraphrase Identification, Sequential Recommendation, Unsupervised Opinion Summarization, Graph Mining, Aspect Extraction, Link Prediction, Anomaly Detection, Recommendation Systems, Node Classification, Text Style Transfer, Unsupervised Text Style Transfer, Sentiment Analysis, Cross-Domain Document Classification, Collaborative Filtering, Fraud Detection, Multibehavior Recommendation","Graph, Image, Text, Time Series",English,Computer Vision,"text-style-transfer-on-yelp-review-dataset-1, sql-parsing-on-yelp, unsupervised-text-style-transfer-on-yelp2018, recommendation-systems-on-yelp2018, multibehavior-recommendation-on-yelp, unsupervised-opinion-summarization-on-yelp, paraphrase-identification-on-yelp, sentiment-analysis-on-yelp-binary, sentiment-analysis-on-yelp-fine-grained, collaborative-filtering-on-yelp2018, recommendation-systems-on-yelp, text-classification-on-yelp-5, unsupervised-text-style-transfer-on-yelp, sequential-recommendation-on-yelp, node-classification-on-yelpchi, link-prediction-on-yelp, aspect-extraction-on-yaso-yelp, fraud-detection-on-yelp-fraud, document-classification-on-yelp-14, text-style-transfer-on-yelp-review-dataset, cross-domain-document-classification-on-yelp, text-classification-on-yelp-2",,https://www.yelp.com/dataset,https://paperswithcode.com/dataset/yelp,"The Yelp Dataset is a valuable resource for academic research, teaching, and learning. It provides a rich collection of real-world data related to businesses, reviews, and user interactions. Here are the key details about the Yelp Dataset:
Reviews: A whopping 6,990,280 reviews from users.
Businesses: Information on 150,346 businesses.
Pictures: A collection of 200,100 pictures.
Metropolitan Areas: Data from 11 metropolitan areas.
Tips: Over 908,915 tips provided by 1,987,897 users.
Business Attributes: Details like hours, parking availability, and ambiance for more than 1.2 million businesses.
Aggregated Check-ins: Historical check-in data for each of the 131,930 businesses.",,,,,,
3633,Yelp2018,Link Prediction,Link Prediction,"Link Prediction, Unsupervised Text Style Transfer, Collaborative Filtering, Recommendation Systems","Text, Time Series",English,Natural Language Processing,"collaborative-filtering-on-yelp2018, recommendation-systems-on-yelp2018, unsupervised-text-style-transfer-on-yelp2018",,,https://paperswithcode.com/dataset/yelp2018,The Yelp2018 dataset is adopted from the 2018 edition of the yelp challenge. Wherein local businesses like restaurants and bars are viewed as items. We use the same 10-core setting in order to ensure data quality.,2018,,,,,
3634,YFCC100M,Image Classification,Image Classification,"Image Classification, Image Retrieval",Image,,Computer Vision,,,https://bit.ly/yfcc100mf,https://paperswithcode.com/dataset/yfcc100m,"YFCC100M is a that dataset contains a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos, all of which carry a Creative Commons license. Each media object in the dataset is represented by several pieces of metadata, e.g. Flickr identifier, owner name, camera, title, tags, geo, media source. The collection provides a comprehensive snapshot of how photos and videos were taken, described, and shared over the years, from the inception of Flickr in 2004 until early 2014.",2004,YFCC100M: The New Data in Multimedia Research,https://arxiv.org/pdf/1503.01817v2.pdf,,,
3635,YouCook2,Zero-Shot Video Retrieval,Zero-Shot Video Retrieval,"Zero-Shot Video Retrieval, Long Video Retrieval (Background Removed), Action Classification, Dense Video Captioning, Video Captioning, Zero-Shot Video-Audio Retrieval, Video Retrieval, Zero-shot dense video captioning","Audio, Image, Text, Video",English,Computer Vision,"video-retrieval-on-youcook2, dense-video-captioning-on-youcook2, long-video-retrieval-background-removed-on, action-classification-on-youcook2, zero-shot-video-retrieval-on-youcook2, zero-shot-video-audio-retrieval-on-youcook2, zero-shot-dense-video-captioning-on-youcook2, video-captioning-on-youcook2",Custom,http://youcook2.eecs.umich.edu/,https://paperswithcode.com/dataset/youcook2,"YouCook2 is the largest task-oriented, instructional video dataset in the vision community. It contains 2000 long untrimmed videos from 89 cooking recipes; on average, each distinct recipe has 22 videos. The procedure steps for each video are annotated with temporal boundaries and described by imperative English sentences (see the example below). The videos were downloaded from YouTube and are all in the third-person viewpoint. All the videos are unconstrained and can be performed by individual persons at their houses with unfixed cameras. YouCook2 contains rich recipe types and various cooking styles from all over the world.",2000,,,,,
3636,YourMT3_Dataset,Drum Transcription in Music (DTM),Drum Transcription in Music (DTM),"Drum Transcription in Music (DTM), Drum Transcription, Music Transcription, Multi-instrument Music Transcription",Audio,,Audio,,"Various, MIT, CC-BY",https://zenodo.org/records/10009959,https://paperswithcode.com/dataset/yourmt3-dataset,"We redistribute a suite of datasets as part of the YourMT3 project. The license for redistribution is attached.

YourMT3 Dataset Includes:

Slakh
MusicNet (original and EM)
MAPS (not used for training)
Maestro
GuitarSet
ENST-drums
EGMD
MIR-ST500 Restricted Access
CMedia Restricted Access
RWC-Pop (Bass and Full) Restricted Access
URMP
IDMT-SMT-Bass",,,,,,
3637,YouTube-100M,Instrument Recognition,Instrument Recognition,"Instrument Recognition, Audio Classification, Music Information Retrieval","Audio, Image",,Computer Vision,,,https://arxiv.org/pdf/1609.09430.pdf,https://paperswithcode.com/dataset/youtube-100m,"The YouTube-100M data set consists of 100 million YouTube videos: 70M training videos, 10M evaluation videos, and 20M validation videos. Videos average 4.6 minutes each for a total of 5.4M training hours. Each of these videos is labeled with 1 or more topic identifiers from a set of 30,871 labels. There are an average of around 5 labels per video. The labels are assigned automatically based on a combination of metadata (title, description, comments, etc.), context, and image content for each video. The labels apply to the entire video and range from very generic (e.g. “Song”) to very specific (e.g. “Cormorant”).
Being machine generated, the labels are not 100% accurate and of the 30K labels, some are clearly acoustically relevant (“Trumpet”) and others are less so (“Web Page”). Videos often bear annotations with multiple degrees of specificity. For example, videos labeled with “Trumpet” are often labeled “Entertainment” as well, although no hierarchy is enforced.",,https://arxiv.org/pdf/1609.09430.pdf,https://arxiv.org/pdf/1609.09430.pdf,,,871
3638,YouTube-UGC,Video Quality Assessment,Video Quality Assessment,Video Quality Assessment,Video,,Methodology,video-quality-assessment-on-youtube-ugc,CC-BY,https://media.withyoutube.com/,https://paperswithcode.com/dataset/youtube-ugc,This YouTube dataset is a sampling from thousands of User Generated Content (UGC) as uploaded to YouTube distributed under the Creative Commons license. This dataset was created in order to assist in the advancement of video compression and quality assessment research of UGC videos.,,,,,,
3639,YouTube-VIS_2019,Semantic Segmentation,Semantic Segmentation,"Semantic Segmentation, Video Instance Segmentation, Instance Segmentation","Image, Video",,Computer Vision,"video-instance-segmentation-on-youtube-vis-1, video-instance-segmentation-on-youtube-vis",CC BY 4.0,https://github.com/youtubevos/MaskTrackRCNN,https://paperswithcode.com/dataset/youtubevis,"YouTubeVIS is a new dataset tailored for tasks like simultaneous detection, segmentation and tracking of object instances in videos and is collected based on the current largest video object segmentation dataset YouTubeVOS.",,,,,,
3640,YouTube-VOS_2018,Semi-Supervised Video Object Segmentation,Semi-Supervised Video Object Segmentation,"Semi-Supervised Video Object Segmentation, Video Inpainting, Visual Object Tracking, One-shot visual object segmentation, Video Object Segmentation","Image, Video",,Computer Vision,"video-object-segmentation-on-youtube-vos-2019-2, visual-object-tracking-on-youtube-vos-1, video-object-segmentation-on-youtube-vos-1, video-inpainting-on-youtube-vos-1, video-object-segmentation-on-youtube-vos, one-shot-visual-object-segmentation-on, visual-object-tracking-on-youtube-vos, video-inpainting-on-youtube-vos, one-shot-visual-object-segmentation-on-1, semi-supervised-video-object-segmentation-on-18",CC BY 4.0,https://youtube-vos.org/,https://paperswithcode.com/dataset/youtube-vos,"Youtube-VOS is a Video Object Segmentation dataset that contains 4,453 videos - 3,471 for training, 474 for validation, and 508 for testing. The training and validation videos have pixel-level ground truth annotations for every 5th frame (6 fps). It also contains Instance Segmentation annotations. It has more than 7,800 unique objects, 190k high-quality manual annotations and more than 340 minutes in duration.",,YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark,https://arxiv.org/pdf/1809.03327.pdf,,,
3641,YouTube8M-MusicTextClips,Music Recommendation,Music Recommendation,"Music Recommendation, Music Captioning","Audio, Image, Text",English,Computer Vision,,"Research-only, non-commercial Adobe Research License",https://zenodo.org/record/8040754,https://paperswithcode.com/dataset/youtube8m-musictextclips,"The YouTube8M-MusicTextClips dataset consists of over 4k high-quality human text descriptions of music found in video clips from the YouTube8M dataset.

For each selected YouTube music video, we extracted 10 second clips at the middle of the video for annotation. We provided annotators with only the audio corresponding to this clip. Thus, text annotations describe audio alone, not the visual content of the clip.

The dataset annotations are divided into train and test split files. As the dataset is meant mainly for evaluation, there are 3169 annotated clips from the test set and only 1000 annotated clips from the train set.",,,,,,
3642,Youtubean,Opinion Mining,Opinion Mining,"Opinion Mining, Sentiment Analysis, Domain Adaptation",Text,English,Natural Language Processing,,,https://github.com/epochx/opinatt,https://paperswithcode.com/dataset/youtubean,Youtbean is a dataset created from closed captions of YouTube product review videos. It can be used for aspect extraction and sentiment classification.,,https://arxiv.org/pdf/1708.02420.pdf,https://arxiv.org/pdf/1708.02420.pdf,,,
3643,YTSeg,Text Segmentation,Text Segmentation,"Text Segmentation, Headline Generation","Image, Text",English,Computer Vision,"text-segmentation-on-ytseg, headline-generation-on-ytseg",CC BY-NC-SA 4.0,https://huggingface.co/datasets/retkowski/ytseg,https://paperswithcode.com/dataset/ytseg,"We present YTSeg, a topically and structurally diverse benchmark for the text segmentation task based on YouTube transcriptions. The dataset comprises 19,299 videos from 393 channels, amounting to 6,533 content hours. The topics are wide-ranging, covering domains such as science, lifestyle, politics, health, economy, and technology. The videos are from various types of content formats, such as podcasts, lectures, news, corporate events \& promotional content, and, more broadly, videos from individual content creators. We refer to the paper for further information.",,paper,https://arxiv.org/abs/2402.17633,,,
3644,YUP__,Scene Recognition,Scene Recognition,"Scene Recognition, Optical Flow Estimation, Action Recognition","Image, Video",,Computer Vision,scene-recognition-on-yup,,http://vision.eecs.yorku.ca/research/dynamic-scenes/,https://paperswithcode.com/dataset/yup,A new and challenging video database of dynamic scenes that more than doubles the size of those previously available. This dataset is explicitly split into two subsets of equal size that contain videos with and without camera motion to allow for systematic study of how this variable interacts with the defining dynamics of the scene per se.,,,,,,
3645,ZeroKBC,Knowledge Base Completion,Knowledge Base Completion,Knowledge Base Completion,,,Methodology,,MIT,https://github.com/brickee/ZeroKBC,https://paperswithcode.com/dataset/zerokbc,ZeroKBC is comprehensive benchmark that covers all scenarios of zero-shot Knowledge Base Completion (KBC) task. It has 3 zero-shot scenarios with 8 fine-grained settings.,,ZeroKBC: A Comprehensive Benchmark for Zero-Shot Knowledge Base Completion,https://arxiv.org/pdf/2212.03091v1.pdf,,,
3646,ZESHEL,Entity Linking,Entity Linking,"Entity Linking, Entity Retrieval",,,Methodology,"entity-retrieval-on-zeshel, entity-linking-on-zeshel",CC-BY-SA,https://github.com/lajanugen/zeshel,https://paperswithcode.com/dataset/zeshel,"ZESHEL is a zero-shot entity linking dataset, which places more emphasis on understanding the unstructured descriptions of entities to resolve the ambiguity of mentions on four unseen domains.

This dataset was constructed using Wikias from FANDOM.",,,,,,
3647,ZEST,Systematic Generalization,Systematic Generalization,Systematic Generalization,,,Methodology,,,https://github.com/allenai/zest,https://paperswithcode.com/dataset/zest,A new English language dataset structured for task-oriented evaluation on unseen tasks.,,,,,,
3648,Zhou2016_MOABB,Within-Session Motor Imagery (right hand vs. feet),Within-Session Motor Imagery (right hand vs. feet),"Within-Session Motor Imagery (right hand vs. feet), Within-Session Motor Imagery (left hand vs. right hand), Within-Session Motor Imagery (all classes)",,,Methodology,"within-session-motor-imagery-all-classes-on-6, within-session-motor-imagery-left-hand-vs-9, within-session-motor-imagery-right-hand-vs-8",,http://moabb.neurotechx.com/docs/generated/moabb.datasets.Zhou2016.html,https://paperswithcode.com/dataset/zhou2016-moabb,,,,,,,
3649,ZINC,Molecular Graph Generation,Molecular Graph Generation,"Molecular Graph Generation, Graph Regression, Graph Ranking","Graph, Text",English,Natural Language Processing,"molecular-graph-generation-on-zinc, graph-regression-on-zinc-500k, graph-ranking-on-zinc, graph-regression-on-zinc, graph-regression-on-zinc-full, graph-regression-on-zinc-100k",Custom,http://zinc15.docking.org/,https://paperswithcode.com/dataset/zinc,"ZINC is a free database of commercially-available compounds for virtual screening. ZINC contains over 230 million purchasable compounds in ready-to-dock, 3D formats. ZINC also contains over 750 million purchasable compounds that can be searched for analogs.",,,,,,
3650,_chinahate,Twitter Sentiment Analysis,Twitter Sentiment Analysis,"Twitter Sentiment Analysis, Hate Speech Detection","Audio, Image, Text",English,Computer Vision,,,https://github.com/JINHXu/how-much-hate-with-china,https://paperswithcode.com/dataset/chinahate,"#chinahate dataset contains a total of 2,172,333 tweets hashtagged #china posted during the time it was collected. It is designed for the task of hate speech detection.",,,,,,
3651,_O_2_Perm,Graph Regression,Graph Regression,Graph Regression,Graph,,Methodology,,,https://github.com/liugangcode/GREA/tree/main/data/o2_prop/raw,https://paperswithcode.com/dataset/o2perm,"The $O_2$Perm dataset is created from the Membrane Society of Australasia portal. It uses monomers as polymer graphs to predict the property of oxygen permeability. It has he limited size (595 polymers), which brings great challenges to the property prediction.",,,,,,
